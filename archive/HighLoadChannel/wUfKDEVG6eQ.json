{
  "video_id": "wUfKDEVG6eQ",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "хочу передать слово следующему нашему спикеру Владимир руководитель компании quy labs специализируется на распределённых SQ движках Владимир тебе слово коллеги Всем добрый день сегодня мы поговорим про то каким образом устроены устроены ряд иных движков которые обрабатывают данные с озёр данных и мы поговорим Почему вообще кому-то в голову пришла идея отправлять SQL к озёрам в принципе Какая в этом польза И самое главное Какие ключевые оптимизации РМ движков за этим стоят и как как с ними соответственно как их использовать чтобы получить все все преимущества такого такого рода инструмента если кратко про меня то я руковожу компанию qus делаем сные движки всевозможные и среди прочего мы делаем продукт называется это Open Source это коммерческий движок на основе Open Source проекта Рина поэтому поря Ну уже порядка 10 лет я занимаюсь распределённый SQ и Давайте сначала поговорим про небольшая подводка Почему вообще Нам нужен SQL и в дата лейка если мы посмотрим на путь развития аналитических систем в различных компаниях как в рамках проектов так и глобально в рамках больших платформ данных то на протяжении там многих лет мы сталкиваемся с одной и той же проблемой что данных сначала не очень много потом их количество увеличивается потом бизнес говорит Давайте принимать решение На основе данных все поголовно Заходите к нам в дата веха и запускайте запросы и это приводит к тому что увеличивается количество данных количество пользователей появляются разнородные нагрузки как мы на это отвечаем естественно когда данных мало справляется и монолитные системы А когда данных становится больше мы часто используем системы на основе shn архитектуры Green plam ветика рота и прочее которые призваны обеспечить нам горизонтальную масштабируемость но по мере того как нагрузки становятся ещё больше о на Становится всё более разноплановый то в современном мире всё чаще используется другой подход это так называемое отделение компью от стод которое заключается в том что у вас есть некие вычислительные узлы они находятся отдельно есть какие-то данные которые находятся отдельно и такой подход конечно Наверняка вы знаете очень сильно стал на слуху после появления сноуфлейк потому что они Именно таким образом работают да данные вст соответственно компьютер это там компью амазона но такому же принципу следует и огромное количество других бешков Спарк В каком-то смысле проявления такого же подхода тот же самый три на пре про которой мы будем говорить многие вендоры классических дата хаусов идут в эту сторону ветика века Да хаха S3 они вот вчера колеги с Яндекса рассказывали про отделение ко в гнп правильно То есть все очень много кто движется в эту сторону Давайте поговорим чем это связано связано Это с тем что архитектура Несмотря на то что она обеспечивает достаточную масштабируемость так или иначе обладает рядом недостатков ключевой из них как раз таки большая часть недостатков как раз таки вытекает из того что вычисление и хранение объединены а это значит что хотите вы изменить размер кластера вам нужно данные перемещать Это довольно слож процессов при выполнении которых ваши там кхд могут терять устойчивость например не очень понятно как это в облако переносить не очень понятно как обеспечить там Scale Up Scale Down например А и конечно же это приводит к тому что данные хранятся Ну в формата каких-то вендоров конкретных Да положили в нпм только нпм их понимает положили в роту только терадата эти данные понимает и смарт на это приводит Ну в общем-то по мере вовлечения всё большего количества пользователей это этим становится не очень тяжело не очень просто управлять Если же мы говорим про подход с отделённых компьютером от Режа то он во-первых обеспечивает действительно эластичную масштабируемость потому что вы можете очень быстро новые узлы добавлять в кластер убирать с кластера накручивать туда кубернетес с авто слингом Ну общем любые такие приколюхи делать И этот же подход позволяет вам стартовать множество кластеров к одним и тем же данным под каждую нагрузку Да там отдел продаж в этот кластер отдел маркетинга в этот кластер или же Он позволяет разным продуктам треть на одни и те же данные положили паркет в dat Lake сюда пошли с парком в в эти данные для ба для батч процессинга в эти же данные пошли линком и отдельно пошли трина То есть это позволяет организовывать достаточно гибкие архитектуры под любое окружение под любое требование и естественно если мы говорим про всякие хайповые тренды то это отлично ложится на вот то что сейчас модно Называть хауза да это концепция Когда вы строите такую слоистую инфраструктуру Где у вас есть внизу Ну условно говоря там ну некие там классический dake hdfs aone там S3 совместимый Ред Облачный что угодно сверху вы накручивается какие-то форматы хранения начиная с простейшего CSV и заканчивая Там паркетом орком сверху если надо вы добавляете там транзакции люции схемы с помощью айсберга и наконец сверху этого всего вы можете подключать различные бишки в зависимости от того с какой нагрузкой вы сталкиваетесь хотите подключили спар Холи хотите там Лин и прочее Вот это всё достаточно привлекательно но коне и среди всех вот этих продуктов можно выделить вот группу движков которые направлены на интерактивный анализ данных в дата лейка такими движками Ну на текущем этапе наверно Самыми популярными осор является Рина и дре это продукты которые появились Ну плюс-минус в одно время в недрах там больших корпораций Потом пошли в Open Source потом куча всяких коммерческих фендеров на них оброс и мы там к ним при сосались В общем и так или иначе этики отлично справляются с быстрой эффективной интерактивной обработкой данных в дата лейка за счёт чего это происходит за счёт того что эти движки Ну естественно отделяют копь storage потому что они только обрабатывают данные а сами данные находятся в вашем дата лейке они естественно являются колоночные массивно параллельными там и прочим и прочим то есть весь классический набор для аналитических движков Но конечно же учитывая то что компьютер реджи отделён и учитывая то что мы работаем с открытыми форматами данных вроде паркета и орка которые не всегда легко обрабатывать А их надо там декодировать какие-то перегонять свои форматы возникает вопрос Насколько быстро в принципе такие движки могут работать и могут можно ли их вообще всерьёз рассматривать как замену полноценных кхд Ну по крайней мере в некоторых случаях А И на самом деле технологии эти активно идут на рынок и появляется уже история успеха вот мы буквально на днях с коллегами записали очень интересное видео где коллеги савита рассказывают как они планомерно мигрируют с верти довольно большого петабайт кластера на комбинацию трина и Цефа цеф для хранения трина для обработки данных и ну этот доклад знаете особо интересен тем что можно следовать просто хайпу да Ой это там серебряная пуля всё сейчас быстренько сделаем а можно подходить Прагматично постепенно мигрировать понимая Какие плюсы минусы вы получите это вот очень хороший пример так что это в каком-то смысле история успеха где большая компания большие нагрузки но тем не менее переезд на такой архитектуру сденм эльм дком который входит в приводит к успеху и наконец теперь мы поговорим про то как же эти движки работают и почему это в принципе может более-менее адекватно по производительности работать опять же значит что необходимо Отметить все движки такого плана как правило внутри себя имеют очень мощные оптимизаторы запросов внутри эти оп представляют Запрос который вы отправляете в виде некого реализаци дерева которым очень легко вращать и из которого можно выводить очень большое количество всевозможных оптимизаций а для нас для целей Вот именно Далеков самыми важными оптимизация являются следующее Первое - это конечно прувинг неиспользуемых колонок сама оптимизация реализуется достаточно просто идея такая у нас там допустим в аналитических нагрузках часто бывают очень широкие таблицы но конкретные запросы трогают только небольшое количество колонок в них поэтому конечно же мы хотим понять какие именно колонки задействованы и обращаться только к ним поэтому оптимизатор там как-то крутит вращать планы в конечном счёте понимает что только небольшой набор колонок требуется Ну вроде бы эта вещь достаточно там Ну достаточно понятная Да но тем не менее С практической точки зрения это значит что если вы используете такого рода движки то вы должны данные в вашем даке хранить в колоночной формате вот такой вот Внезапный совет но тем не менее на практике мы встречаем ситуации когда допустим каким-то компаниям интересно попробовать такие подходы но они говорят А мы там начнём наш Пилот с Джейсона там или CSV Ну если вы Пилот свой будете так строить то никакого производительности вы не увидите потому что колонки ниться не будут вы будете все данные подтягивать и получится непонятно что поэтому вроде бы капитанство но тем не менее тем не менее на практике это встречается и такого Надо избегать Вы должны понимать что если вы используете такой рожки они колоночные ваши данные должны быть колоно тогда вы будете получать э хорошую производительность дальше Вторая важнейшая оптимизация - это конечно же а фильтр pushdown да то есть она используется в большом количестве движков в разных а ипостасях а для нас когда мы работаем с дата лейками фильтр pushdown направлен в первую очередь на то чтобы сканировать как можно меньше данных это подход который в целом в литературе обычно называется Дета спин В чём он заключается вы в вашем запросе задаёте какой какие-то фильтры каким-то колонкам дальше мы опять же в процессе оптимизации эти фильтры как-то преобразовывается спускаем вниз к операторам сканирования причём какие-то фильтры могут быть тривиальные вроде там дата равно вот какому-то числу Ну с ними всё понятно какие-то фильтры могут быть завёрнутые функции из которых конкретные там ограничения надо ещё вывести Вот например здесь вот мы используем фильтр по колонке Date где мы указываем конкретный год но вот так вот в лоп движки часто не смогут этот фильтр понять поэтому что они делают они предварительно такого рода функции разворачивают какие-то более понятные выражения тут у меня приведён вообще тривиальный пример на самом деле довольно много это операторы like операторы truncate там больше-меньше по колонкам там на самом деле довольно много всего и в конечном счёте к чему это приводит если мы получаем такой понятный фильтр то если этот фильтр относится к полонке фу полон колонке партиционирование или колонке бакеро Нея ваших данных то мы можем его использовать чтобы отбрасывать какие-то партишн ие бакеты и не обращаться к ним вовсе это чрезвычайно мощный техника и на практике это означает то что конечно же если вы организуете ваши данные в далеке то понятное дело вы задумываетесь при работе с большими таблицами партиционирование пакетирование Если вы это грамотно сделали то и такого рода движки вроде там три на просторе прекрасно будут это использовать Но вместе с тем это же означает и то что вы должны Если вы партиционирование так чтобы движок понял что вы действительно это что вы действительно хотите от этой колонки какие-то движки могут делать там продвинутые разные трансформации на не очень понятными фильтрами какие-то с этим не справляются поэтому здесь а важно понимать что чем более в понятном виде вы фильтры записали тем с большей вероятностью он будет использован это очень похоже на самом деле на вот подход знаете вот часто об этом говорят sarg был там sarb аргументы или sar был запросы вот в контексте скольз сервера и прочих Да что вы написали понятный фильтр у вас используется индекс всё работает Быстро Здесь также написали понятный фильтр у вас там прутся парны всё хорошо Вот и на практике возможности движков по переписыванию вот этих фильтров очень сильно разнятся поэтому конечно в идеале надо поэкспериментировать Да И вот этот пепер он очень интересн он показывает насколько далеко можно продвинуться там отдельная глава есть про выведение фильтров хотя на самом деле сам пепер вообще про другое вот но тем не менее в любом случае надо экспериментировать стремится писать фильтры максимально Понятно тогда всё будет классно у вас отфильтровать мало данных мы читаем соответственно быстро отвечаем на ваши запросы далее ещё одна естественно там супер важная оптимизация в такого ро движках - это так называемые ранта фильтры или динамические фильтры Ну по-разному они называются Но идея примерно одна и та же допустим у вас есть какая-то Большая таблица фактов А который вы Джой нете домены и пишете фильтры по шем а обычная ситуация да Ну какая там ну схема условно говоря звезда какая-то и вот к к листвы операторам вы пишете фильтры а явных фильтров на таблице фактов нету Если мы будем в лоб исполнять такой запрос он обратить внимание буквально получает продажи за один день А эти Ну это может быть там доля процента от всех данных в нашей таблице то Если мы будем исполнять такой запрос в лоб вот я вот это конкретно исполнял на трина на tpc там сл фактор Ну 1 терабайт данных то вот видно что например в лоб исполняя запрос мы там из таблицы продаж тянем в моём случае почти 3 млр записей только для того чтобы потом большая часть была брошена в процессе выполнения оператора Jo и Ну время запроса там какое-то вот условно 4 секунды Но что можно сделать альтернативно вместо этого можно сначала отсканировать понимая что реально равны значения в этом условии и таким образом сформировать прямо в ран тайме дополнительный фильтр который будет направлен к оператору сканирования большой таблицы И тем самым соответственно прочитать только небольшое количество записей и вот в моём случае когда я включаю такую оптимизацию конкретно в трина то было 3 млрд записей теперь я сканирую всего 9 млн записей время выполнения запрос конечно радикально сокращается вот поэтому все движки которые так или иначе с дата лейками они такого рода оптимизации в том или ином виде имеют потому что без неё никак иначе вам бесконечно придётся там сканировать большие куски данных при этом что стоит отметить тоже будет немножко капитанское замечание надо документацию почитать Почему Потому что дело в том что эти динамические фильтры они устроены довольно сложно там потому что вы сначала сканирует вот правую часть джоина потом вы эти статистики собираете потом их надо разослать на другие узлы и оказывается что какие-то движки может быть не всё из этого умеют делать в каких-то случаях может оказаться Так что вы допустим собираете вот этот динамический фильтр справа а он оказывается не селективным или таблица справа тоже очень большой оказывается из-за этого в каких-то там ну там крайних случаях запрос даже замедляется на самом деле может Поэтому в идеале конечно хотелось бы чтобы движки решения о том ставить динамические фильтры здесь или нет принимали на основе Ну каких-то оценок стоимости там статистик и прочего по факту этого обычно не происходит поэтому они или ставятся или нет в зависимости от того что вы указали в конфигурации и поэтому чтобы не нарываться на проблемы надо внимательно вообще ну то есть понимая то что как-то устроено надо внимательно почитать документацию потому что там вы можете видеть например что иногда эти оптимизации там почему-то автоматически отключаются их надо может быть вручную включить Иногда вы можете сконфигурировать например тайм-ауты да то есть допустим ждём сканирование таблиц справа не более 3 секунд в противном случае просто продолжаем будто бы фильтра нету И такого рода соображения эти фильтры очень большие получаются потому что очень много Дин значений движок может отказаться их пробрасывается чувствовать это нормальная вещь в любых сложных движках это Так естественно системы типа трина стремятся выбрать какие-то адекватные дефолты но тем не менее Доверяй но но проверяй хорошо поговорили про подан вычислений теперь немножко про параллелизм мы говорили да что кора движки являются массивно параллельными сейчас если твой движок не имеет в своём кратком описании слово массивно параллельные Никто с вами серьёзно разговаривать не будет да и значит Каким образом обеспечивается параллелизм в такого рода движках конечно же всё начинается с того что нам нужно эффективно параллельно данные читать и оказывается что в даках это дело достаточно легко Почему Потому что если у вас есть Большая таблица вы её побили на партишен на бакеты а или же в процессе записи этой таблицы не знаю там через парк указали небольшие размеры там R Group или ещё чего-то у вас вероятно за большой таблицей находится большое количество файлов эти все файлы можно в общем-то читать параллельно более того у файлов а особенно там структурированных вроде паркета орка есть ещё внутренняя структура своя например которая описывает по каким офсета находятся групы А ну и Ну какие-то там другие элементы и всю эту информацию можно учитывать для того чтобы спланировать такое массивно параллельное чтение с по большому количеству узлов в кластере и вот например трина устроен таким образом что мы делаем листинг директории в дата лейке который описывает текущую таблицу понимаем какие там есть файлы если файл очень ну если файлы файлы не очень большие то один файл будет прочитан в один поток если файлы большие Мы попытаемся в несколько потоков их прочитать и в дальнейшем соответственно задачи на чтение фав Прошу прощения которые мы называем Сплит мы их называем сплиты эти задачи распределяются по воркер злам ну и воркеры соответственно Независимо друг от друга начинают читать Вот отдельные части вашей таблицы потом они естественно между собой данные обмениваются чтобы промежуточные операторы исполнять Ну и так далее Что здесь важно понимать Это то что Руту Фарук минус соотноситься с возможностями вашего кластера Если Вы взяли и за огромной таблице записали один огромный файл с одной огромной группой Никакого параллелизма не будет у вас и будет работать Всё не очень хорошо Если же вы наоборот взяли и вашу таблицу побили на маленький партишн там по несколько килобайт то вы там больше потеряете на накладных расходах пока вы там си вызов какой-то сделаете и получите какую-то совсем небольшую небольшое количество данных поэтому здесь для того чтобы это всё работало быстро надо чтобы файлы были Ну как-то п плюс-минус адекватно побиты и здесь Наверное хорошей практика является отталкиваться вот от базовых рекомендаций которые обычно движки такого плана дают Ну это там знаете группы там по 64 МБ 128 там 256 Мб Ну относительно они большие да чем у вас больше узлов в кластере тем может быть более мелкие группы дадут вам больше параллелизм но только до какого-то определённого предела потому что потом вы будете уже получать больше накладных расходов поэтому тут надо достаточно внимательно Ну за этим смотреть но тем не мене Если вы будете отталкиваться от таких общепринятых дефолтов скорее всего у вас всё будет плюс-минус хорошо а тем не менее также стоит отметить что если у вас вот таких сплитов довольно много то какие-то движки могут принимать там консервативные стратегии по тому как ске долить эти сплиты на узлы Ну вот например мы берём преста и оказывается что преста какой бы вы мощный воркер не поставили он в рамках одного воркера будет сканировать по умолчанию не более чем в 16 пото токов у вас там 128 ядер допустим колоссальный сетевой канал а прос просто на дефолт такой стоит он просто по чуть-чуть эти данные тянет вот поэтому на самом деле если у вас очень большие таблицы то также встаёт остро вопрос правильной конфигурации скели Ну вот для трина я привёл пару примеров где можно почитать о чём речь но у каждого движка там есть свои особенности поэтому это тоже нельзя упускать и обычно сигналом к тому что что-то может быть таблицы не очень хорошо побиты Или может быть там скер неправильно работает сигнал к этому является то что вы видите недостаточную утилизацию сети недостаточную утилизацию CPU в процессе сканирования Поэтому вот Конечно если вы вот занимаетесь администрированием системы настройки то надо на это внимательно посмотреть Ну из коробки как правило плюс-минус адекватно срабатывают далее ещё одна очень важная оптимизация - это то что движки Ну так как у нас компьютер лён Мы в отличие от классических кхд не обязаны допустим находится у нас вообще нет понятия Мы можем виртуально такое партиционирование делать сводится Это к тому что мы можем посмотреть получить список всех наших Батов и по какому-то принципу их сгруппировать по узлам например мы можем сказать давайте мы сделаем так чтобы все сплиты которые относятся к парну там вот за дату 2 марта пойдут на один и тот для раден системах требуется шалить данные под собой для того чтобы ну чтобы правильно исполнить оператор Это относится к агрегатам Да потому что все ключи все записи с одним и тем же ключом агрегации должны оказаться на одном ворке правильно это относится кнам Это относится к Window функциям Ну к очень большому количеству операторов и вот в нашем случае если мы например сканируем нашу таблицу которая парова по дате и по дате же делаем агрегацию ЕС как сплиты группировать то получается что мы сначала сканируем таблицу потом мы может быть делаем Преда агрегацию потом мы делаем шал чтобы уже финальные агрегаты посчитать потому что именно после шафла все записи с одной и той же датой окажутся на одном узле Но если же мы заранее каким-то образом эти самые наши сплиты сгруппируй то кажется что мы можем избежать шафла потому что мы уже сплиты так рассылаем по узлам что все записи с одной и той же даты находятся на одном узле и в конечном счёте это приводит к тому что у вас структура вашего дерево операторов Может у прощаться меньше данных по сети воркеры друг другу передают и запрос исполняются быстрее вот то есть вроде бы классно но на самом деле не особо-то не всегда это классно Почему Потому что как только мы жёстко привязываем конкретные части данных к конкретным узлам мы получаем ровно ту ту же самую историю что в классических кхд там горячие партишен D SQ и прочее да потому что оказалось так что у нас один партишен гораздо больше другого он прилетел на конкретный узел и теперь скорость допустим нси вашего запроса будет Вот гнаться за вот этим самым Ну будет упираться в самый медленный партишен вот поэтому на самом-то деле такого решения по-хорошему тоже надо принимать в Ну на основе костов и такое на Ну вот такое Кост Бей решение группировать или нет движки на самом деле либо не делают Вообще либо делают очень-очень так ну поверхностно что ли Ну например вот в спа есть оптимизация да Когда у вас есть там горячий проти он может сделать в фронтами репти и там ну как-то принять другое решение по там количеству баке тов Да в трина там Dre PR Ну вообще особо ничего такого не заимку но на самом деле движок вас там ну немножко подстраховать но не особо сильно Вот поэтому если вы работаете с таблицами то с одной стороны в каком-то случае если вы если у вас данные плюс-минус равномерно распределены и парнов вот этих больше чем узлов то тогда Вы весьма вероятно для многих запросов получите ускорение потому что не будет вот этих Шаов паразитных А вот если у вас есть SQ то наоборот Вы можете получить замедление поэтому здесь Но тем не менее всё-таки что хочется отметить постро с классическими кхд что здесь у нас по крайней мере есть Свобода выбора хотим используем хотим не используем а Возьмите условный у вас побили на парти и побили и всё вот поэтому такого рода оптимизации надо использовать но я опять поиграю в капитана надо документацию почитать сначала дальше ну и наконец Да вот последний такой групп оптимизаций важных про которую хотел бы рассказать это всевозможной кэширование кэширование является очень актуальным при работе с датами Да и вообще в принципе с аналитическими ми потому что мы как правило работаем с медленно изменяемыми данными например Мы записали парше который представляет собой например продажа за последний день мы их записали в dat Lake Ну или даже там не знаю в кхд и скорее всего вы их уже особо не будете изменять правильно Ну может быть вы как-то их там потом заору ете там от срочно но в целом это неизменяемые данные и это значит что Когда мы будем многократно к ним отправлять льные запросы то как правило вы будете делать одни и те же повторяющиеся вычисления снова снова снова снова а данные одни и те же возникает вопрос понимая то что данные изменяется мало можем ли мы это как-то использовать для того чтобы такого рода вычисления ускорить и оказывается можем и значит ускорение можно получить за счёт кэширования Мета информации и за счёт кэширования непосредственно самих данных если мы поговорим про Мета информацию то в контексте Ну вернее даже давайте так посмотрим как вот обычно идт обработка конкретного файла в далеке в первую очередь мы хотим узнать что это за файл каков его размер Да поэтому мы идём в и делаем там условный там запрос три там получаем какой-то статус или входу статус получаем файла да и у нас в руках Оказывается там локейшен его блоков Да там модификации размер что очень важно и проще далее если мы работаем с пакетами орками например то у них есть футе которы мо читать уже Мета информацию о внутренней структуре файла поэтому мы сначала получили статус потом мы снова пошли в dat Lake получили там какой-то кусочек футера например его проанализировали и только после этого мы понимаем там где находятся конкретные колонки там индексы блон фильтры там что угодно и начинаем непосредственно данные читать поэтому обычно да это вот таких вот три шага при этом все эти шаги обычно Нам отдают одну и ту же информацию Потому что данные медленно изменяются Значит первое что мы можем делать Это кэшировать непосредственно метаданные такая вот достаточно прямолинейная оптимизация заключается в том что давайте будем кэшировать например фуры с паркета или орка Ну какую-то Мета информацию о внутренней структуре файлов реализовано это может быть таким образом что мы каждый раз идём в Да и смотрим Что файл не изменился с прошедшего чтения Если это так то мы просто зашиваю вот это ме информаци перес внутри воке правило занимает много ме и поэтому её можно даже просто в памяти хранить и ну и тем самым уменьшить количество сетевых вызовов к вашему дата лейку вот такого рода оптимизации есть вот в пре у нас в движке вот в цедру это есть Ну и она ну где-то н там уменьшает но это не что-то там супер интересное можно использовать более агрессивные стратегии и вообще кэшировать даже информацию о там статусах файлов или там о листинге директорий то есть мы однократно идм в видим там какой-то файл и предполагаем что этот файл будет там в неизменном виде всегда это уже немножко Как вы понимаете более такая агрессивная стратегия потому что А что если кто-то его изменил да то есть движок думает что всё нормально тут пришли пользователи Говорят у вас там Айсберг он побит на маленькие кусочки медленно работает заору ите Ну администраторы ЗАО Зро А ваш движок по-прежнему думает что там какие-то другие файлы и поэтому это надо использоваться опаской такого рода оптимизации есть не везде Но вот например в пре оно есть вот по ссылке можно почитать Это ссылка которая ну там такой блогпост от Фейсбука который рассказывает про там всевозможные виды Шей которые они сделали в Престо и среди прочего вот каширование статуса файлов там тоже упоминается ну и наконец Ну в любом случае даже если это мы сделаем как правило если у вас действительно большие данные наибольше время мы так или иначе тратим непосредственно на чтение самих данных что можно с ними сделать Т при чтении данных у нас возникает два вызова на самом деле первое данные находятся удалённо мы ком делили на всё круто и данные далеко значит их читать дорого по крайне мере во многих случаях значительно дороже чем Да если данные были просто под ногами у вас Да как там в система мы если данные хранятся в формате оптимизировано для конкретного движка потому что мы тогда эти данные пря мгновенно в движок передаём и начинаем их перемалывать но если мы работаем с открытыми форматами вроде паркета орка то эти данные надо сначала декодировать паркет надо Ну условно говоря разжать Может быть о там зашифрован расшифровать и потом ещё и походить по его там группам посмотреть на эти всякие тамле там вот это ВС прочее и только потом получить какой-то Вектор значения колон кою уже Джок будет обрабатывать и на это на самом деле ЕС Вы по профили то увидите что на это тратится довольно значительная доля CPU на самом-то деле Вот поэтому вот не только проблема в сетке А проблема ещё и в том что даже когда вы данные получили вам надо ещё потратить значительное количество усилий чтобы их движок смог дальше переживать Каким образом с этим можно бороться одно из решений заключается в том что давайте мы на воркер узлах будем кэшировать наши данные не вникая в их внутреннюю структуру например вот есть вот Ну тоже в и вот бложик можете почитать про просто А есть проект alio который реализует фактически каширу ую файловую систему То есть у вас допустим есть дуп Ну вы с ним работаете как с файловой системой а вы можете сверху поставить alio Как такую впер над там любой файловой системы дупа И что будет этот впер делать он будет брать и просто прозрачно для вас кэшировать узлах Каширова по границам каких-то своих внутренних блоков или там по какому-то фиксированному размеру но не вдаваясь подробности А что за данные под ним находятся если мы используем такой подход то это помогает нам избегать сетевых вызовов потому что данные теперь у нас под ногами находится но проблема с де кодин у нас остаётся потому что мы по-прежнему ничего не понимаем про структуру этих данных это просто для нас какие-то произвольные байти поэтому мы их зашивали но ВС рано перенять ний формат вишка нам требуется но тем не менее это уже очень очень неплохо Ну и прежде чем я пойду дальше также стоит отметить что все эти кэши вы их можете там произвольным образом терять потому что это просто дог копия данных и поэтому умер у вас тамр или там не знаю диск сломался ничего страшного просто пойдём в озеро и заново прочитаем То есть это тоже такая интересная особенность теперь возвращаюсь к проблеме значит Как можно избавиться от декон чтобы избавиться от декон мы можем не просто кэшировать абстрактные бинарные файлы А мы можем углубиться в том углубиться в структуру конкретных данных декодировать там однократно тот же самый Паркер и уже готовые вектора ну которые готовы к потреблению движком сохранить У нас под ногами это другой подход который Ну вот мы у себя реализуем в нашем продукте по сравнени с трина и который вот есть Очень популярный проект раньше назывался сечас это называется Speed израильский стартап раньше был который вот берёт и так вот прозрачно для трина сохраняет вот эти зашивает преимущество получаем мы не тратим CPU на этот самый декодинг это классно во-вторых когда вы такие данные Когда вы понимаете что именно вы каширу ете Вы можете строить дополнительны структуры данных например Вы можете индексы построить можете фильтры добавить Какие рада например там даже люсиновский индексы умеет строить по строкам и прямо сразу же вместе с ними сохранять или же Вы можете эти данные например отсортировать потому что вы знаете что у вас приходит фильтр там а больше чем что-то если данные отсортированы то вы можете только там часть этих данных на самом деле считать и что-то быстро отбрасывать поэтому как только вы понимаете структуру данных у вас появляется очень большой потенциал для дополнительных оптимизаций Не говоря уже о том что вы экономить на CPU потому что вам не приходится делать там декодинг де компрессию и прочего Но если большая проблема сам деле потому что данные в паркете там или ворке они Классно сжимаются очень правильно А вот если вы храните просто вектора то у них размер может быть на самом деле очень большой Ну для примера вы берёте tcds там 1тб сгенерировать данные он у вас и будет 1 теб Ну примерно вы их сжали в паркет Ну перегнали в паркет и сжали там каким-нибудь спе у вас получись 3то атом себя обратно прочитаете и Закаев лоб то у вас это будет снова 1 ТБ потому что там ну вы потеряли сжатие вы потеряли там всякие оптимизации паркета и прочего А и тут ну это знаете вот есть известная такая штука ну забыла привести Пепе что вот есть вот apch Arrow может быть слышали Да проект который про как раз-таки колоночные представление данных вот у ER одна из больших проблем что если лоб хранить он очень много места занимает Ну здесь вот один в один проблема потому что внутри такой род движки они плюс-минус формате использует Ну не прямо его Но плюс-минус одно и то же Поэтому если вы такие данные храните локально то их в идеале надо ещё а сжимать А если вы сжимается значит вы снова будете CPU тратить Но если вы подберёте нормальный алгоритм плюс-минус оптимальный то может быть не очень много будете тратить например вот что мы у нас вот в цедру се делаем мы сжимаем данные с помощью zstd и Ну с точки зрения CPU Ну это плюс-минус нормально это сжимается Он практически также как как там сжатый паркет ну не не так же конечно там на 20-30 про больше но в целом это более-менее адекватно поэтому тут вот такого рода соображение есть но ещё одну тем не менее проблему не совсем очевидную стоит отметить что как только вы начинаете данные кэшировать на воркер узлах возникает вопрос А как сделать так чтобы этот кэш был эффективным Потому что если вы отправляете допустим на на конкретный узел задачу на каширование Фу на чтение фа вы на этом ворке файл зашивали то когда следующий запрос придёт на чтение этого же файла в идеале вы хотите его отправить на этот же самый узел Ну потому что там уже они зашивал А если вы не будете это делать то в конечном счёте после большого-большого количества запуска кажется что у вас полные датасеты находятся на каждом возле столько дисков у вас нету скорее всего и в общем-то как-то нехорош получается абы на один и тот же узел на чтение одного и того же файла Вы по сути снова привязывается как бы в каком-то смысле компьют к сторедж правильно Ну потому что это ж именно оно и есть А и поэтому вот возникает такая интересная вещь что мы как бы многократно с вами говорим Как как классный компьютер сторедж отделять а тем не менее это уже второй пример где мы его мы их обратно схлопывается вот так а и значит что обычно движки делают а движки используют некие стратегии которые позволяют м привязывать конкретные там сканы конкретных файлов или часте файлов к конкретному воркер узлу при этом это привязка не является строгой То есть например движок может сказать если этот воркер не загружен и там данные зарова то отправьте запрос туда если вокер перегружен то давайте отправим запрос в другое место то есть мы тогда всё равно пойдём в сетку там будем декодировать эти паркеты что вот эта проблема совмещения основы перед нами появляется по-прежнему мы не обязаны ей слепо следовать мы можем динамически на уровне движка принимать решение Хотим мы эти зашивать или нет Это тоже нам даёт определённую гибкость Ну и А вам это даёт дополнительную сложность в том как это правильно сконфигурировать вот ну и в качестве алгоритмов тут естественно используются алгоритмы которые позволяют сделать так чтобы маппинг с конкретного файла на узел не поплыл сильно если у вас изменилось топология кластера Потому что если мы будем делать там какой-нибудь там зашивали там имя файла и взяли остаток отделения то тогда у вас Один узел ушёл и у вас привязка всех файлов ну изменилась и теперь у вас по сути весь кэш который у вас был становится бесполезным всё кэшируется заново а если использовать алгоритмы в котором Даже при изменении количества узлов в кластере привязка файла к узлу остаётся то тогда такого рода спецэффектов не будет происходить ну здесь обычно используется конечно же consent н алгоритм ну или там какие-то алгоритмы со схожими свойствами Вот Но тем не менее важно понимать что вот это совмещение компьютера Ред тут опять наносит ответный удар так сказать А что ещё можно Каширова ещё можно Каширова на самом деле не только файлы А можно Каширова промежуточные результаты запросов потому что например Ну вот так например делает пре В некоторых случаях вы допустим вы сканирует какой-то файл может там как-то фильтрацию добавляете а потом тут же считаете агрегат э чтобы постоянно это всё не пересчитывать если агрегаты Ну достаточно хорошо схлопывается то мы можем взять и прямо результат расчёта этого агрегата сохранить Ну в случае преста они хранят это в памяти а и М что происходит мы берём наш план у него есть некая там ну уникальная сигнатура которая описывает что план делает у нас есть файл который мы хотим прочитать мы это всё объединяем в некий ключ и это становится ключом в кэше а UE этого кэша является непосредственно то что получилось после запуска Ну запуска операций получается что вы многократно делаете там одни и те же агрегаты но по факту мы можем их памяти сохранять и просто вам отдавать готовый результат не сканируя даже файл не прогоняете агрегации заново и конечно же здесь нам надо тоже делать Вот эти перепроверка что файлы не изменились Там и так далее но в целом это срабатывает но проблема конкретно в Престо например в том что она может Каширова Ну по крайне мере сейчас только очень просты планы данные там должны быть де нормализованные потому что там джоны не поддерживаются например ну поэтому там это всё достаточно так ограниченно действует Но а вот смотря на род мапы продуктов видно что скорое там будет всё в большей степени такого рода кэши появляться потому что там можно на самом деле пойти очень далеко Можно кэшировать более сложные части под дерево под деревьев можно анализировать нагрузку которая приходит от пользователей выделять Като повторяе части и широва именно их Ну по что ВС в кэш у вас не влезет Да но если Вы внимательно смотрите что вот это часть чаще всего повторяется и она там самая тяжёлая и больше всего там схлопывается и кто-то уже это делает кто-то будет делать в будущем но тем не менее как только это в движке появляется для некоторых нагрузок си усет и конечно же последнее о ЧМ хочется поговорить Это материализованные представления также как и в и там в обычных там кхд материализованные вьюшки можно использовать их можно строить и здесь У вас есть под ногами там есть какие-то таблицы Вы можете эти данные прочитать и в виде материализованные на повторяющихся вычислениях а вместе с тем вы в дата лейка Также можно достаточно легко отслеживать изменения файлов да то есть если вы хотите например сделать так чтобы ваша материализованная вьюшка всегда давала актуальные данные то например Вы можете либо перепроверять каждый раз что файлы на основе которых была построена матюшка совпадает с теми файлами которые представляет текущая таблица Если вы используете табличные форматы вроде там Del Lake или айсберга то вы можете сохранить допустим айди ники снапшота в айсберга по которым были построены построены материализованная вьюшка и перепроверять что они по-прежнему не изменились да этим самым Можно не только Да вот эти вычисления материализовать но и также гарантировать что вы всегда получите актуальный ответ Это очень классно и такого рода вещи поддерживаются Практически во всех движках там вроде три на Престо дреме но можно пойти ещё дальше на самом деле А и есть вот одна очень интересная Проблема в том что если вы сделали материализованное представление а пользователь о НМ не знает и отправляет какой-то Запрос к оригинальным таблицам но на этот запрос можно было бы ответить с помощью материализованное пользователя напрямую к авь Вот это сложно оптимизация это комбинатор сложная Задача В общем случае име один с материализованное представления один от пользователя Найти пересечение это ну это реально очень сложно поэтому многие продукты Это не умеют делать например прот и наш движок мы не умеем это делать а какие-то движки умеют делать например Это умеет делать Dre он там полагается Частично на пач Кольт Частично на там собственной оптимизации то есть там происходит много шагов по некой нормализации планов там и прочего и в конечном сч какие-то запрос автоматически перенаправляет Мы у себя тоже такую вещь собираемся сделать вот поэтому это тоже интересно потому что администраторы сами принимают решение о том что они будут какие вьюшки делать допустим на основе анализа нагрузок от пользователей а пользователи работают с обычными таблицами И думают ой Как всё быстро как всё классно какие у нас все ну какой классный продукт мы используем вот поэтому м материализованное представление - это это мощь но как я сказал на текущем этапе не все движки м могут их потенциал целиком раскрыть но так или иначе они движутся в эту сторону вот поэтому на этом на самом деле коллеги У меня всё я наверное рассказал про ключевые такие оптимизации которые используются чтобы данные обрабатывать быстро и поэтому на самом деле можно смело их использовать множество историй успеха переезда с кхд на такого рода решения они Вполне себе неплохо себя проявляют Спасибо Владимир Спасибо большое за доклад У нас есть ещё где-то 5 минут 5-7 минут на вопросы поэтому скорее всего все задать не успеем Давайте начнём вот Сергей вижу начнём с него Добрый день Вопрос немножко как сказали религиозный все эти оптимизации про которые вы рассказывали они уже Ну примерно так 5-10 лет Как сделаны в оракле Почему на ваш взгляд играют в эти кубики Ну Вместо того чтобы спокойно пользоваться какой-нибудь ЭКТО или облачным оралом Ну это действительно религиозный вопрос потому что ну Для чего там вообще там нам нужны далеки и что-то поверх них запускать тут ответов много и каждый из них там в каком-то смысле спорный Да потому что первое Вы можете однократно положить данные допустим в и разными движками заточены под разные нагрузки к ним приходить их Ну делать их анализ Если вы orcal это загрузили то Вот то с чем окл хорошо справляется вы делаете А хотите что-то другое сделать чего у него не хватает Вы там данные копируете например есть такого рода вещи поэтому на самом деле вот знаете наверное то о чём вы спрашиваете это больше вопрос К тому А почему вот эти все знаете хайповые термины леус и прочее Почему они ну вызывают интерес Да и там объяснение обычно такое что перейдёте на новую там инфраструктуру у вас там уменьшится tco Ну иногда уменьшится иногда Не уменьшится иногда понятно Как как работать иногда специалистов Нет ну понимаете да это всё очень сложные проблемы и поэтому Ну мы например со своей стороны ни в коем случае не говорим что это серебряная пуля какая-то Да но тем не менее для кого-то это нормально срабатывает ну и плюс Oracle эту в России сейчас купите тоже там интересный вопрос Следующий вопрос пожалуйста вот с левой стороны Второй второй ряд спасибо очень классный был доклад можно задать миллион вопросов но я пожалуй спрошу вот что можно пожалуйста поближе микрофон держать чтобы было слышно в трансляции тоже вопрос хорошо я пожалуй спрошу вот что есть история успеха Как на сноуфлейк построить ДВК и вот собственно вопрос позволяют ли подобные движки собрать там свой dat не знаю якорную модель насколько успешно можно это осуществить ну вы знаете я наверно на свом вя ли СГУ как-то там ну обширно ответить я могу сказать что у нас например сейчас мы общаемся с одной компанией которая как раз-таки прямо сейчас мигрирует со сно флейка на самом деле Ну в наше там в в отечественное окружение Вот и я думаю что Ну в целом это Ну давайте так в целом системы к этому наверное скорее готовы чем нет но тем не менее есть много нюансов Ну вот там вы знаете что когда вы переезжаете там в далей из классической ккд вы там транзакции теряете например да эволюция схемы не очень понятно как делается Да и появляется всё больше проектов которые направлены на решение например такого рода проблем но их скажем так уровень их готовности для сложных задач не всегда ну не не всегда достаточен например вы используете Айсберг а там транзакции только на одной таблице Если вы хотите сделать даже с каждым месяцем со всё большим количеством нагрузок они справляются такого рода Ну подход Да но далеко не всегда и далеко не совсем именно это наверное сейчас мы просто находимся в такой точке что какой-то критический набор функционала накопился поэтому очень много кто активно В этой идёт даже в России там и банки и Телеком и Ритейл вот походите поспрашивайте крупного бизнеса все Сейчас вот в эту сторону идут ну ладно не все многие вот поэтому это знаете Это некий такой растянутый процесс взросления и даже там всякие там модные меми это это взрослости не хватает поэтому тут ситуация динамически изменяется так сказать поэтому надо следить в оба Ну не знаю насколько ответил или нет ш вопрос так коллеги у нас ещё времени наверное на один или два вопроса вот пожалуйста с последнего ряда комы если что в дискуссии обм безусловно после вопросов будет время е в дискуссионной зоне пообщаться Владимир Спасибо за доклад хотел уточнить вот достаточно популярно если мы используем данные в дайке часто используется SQL Вот можете немножко сопоставить производительность вот этих движков и собственно гово SQ смотрите Ну идеологически там Спарк так или иначе он похож Ну по смыслу да потому что это тоже вычисление Там отдельный где-то которы читат данные вот Если сравнивать см то допустим три на дрем они больше про интерактивный анализ то есть они стремятся в вычисление в памяти удерживать стремятся стримить данные между узлами но ценой отказа устойчивости да то есть для для Вполне себе нормально если аналитик например отправил запрос ждёт ответа там не более там минуты пару минут и допустим в середине запрос упал потому что там узел там допустим Ну там узел вы там администратору убили вот мы просто перезапуска и всё всё хорошо а в спарке Если вы запускаете там многосоюзие вас Ну у вас там что-то упало вы вы вы хотели бы продолжить выполнение да то есть там вот больше про то как обеспечить м отказоустойчивость очень тяжёлых Джов и поэтому если вы в лоб погоняет одни и те же запросы там Спарк SQ допустим и трина то на интерактивных Ну На таких вот ну трина обычно будет быстрее но она будет там чаще падать Ну вот грубо говоря но при этом я бы также отметил следующее Что сейчас идёт активное размытие вот э того как эти движки себя позиционируют Потому что например в трина он исторически был движком который всё в памяти обрабатывает памяти не хватило Ну либо вы там спилить на диск ну что там сильно замедляет либо же падаете что они сделали они сделали Сейчас ну буквально там год назад специальный режим называется F tolerant execution который позволяет как раз таки промежуточные стадии вычислений сохранять на диске Ну в распределённой файловой системе И это прямо Шаг в сторону спарка они как только это сделали сказали вот теперь это через нас гоняет тяж SQ там ите там и спарка с ним и так далее вот Ну конечно это всегда надо скепсиса воспринимать но тем не менее это показывает то что все эти движки это некий такой спектр и положение конкретного движка на вот это там оси координат оно со временем может изменяться коллеги у нас к сожалению время подходит к концу Давайте вручим подарок за лучши вопрос Владимир Выбирай вопро гию вам пожалуйста подарок от нашего партнёра Газпромнефть за вопрос про религию Владимир У нас для тебя также есть подарок Спасибо большое тебе за доклад за твоё выступление развитие продукта"
}