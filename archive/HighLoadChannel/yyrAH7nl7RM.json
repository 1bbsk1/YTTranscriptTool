{
  "video_id": "yyrAH7nl7RM",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Антон Губарев Авита Всем привет меня зовут Антон сегодня будем говорить об опыте реализации межсервисный авторизации в рамках платформы в Авито Я являюсь фич дом этого проекта и и Одним из основных разработчиков а также инженером вти платформа сервис и прежде чем переходить непосредственно к разговору об авторизации хотел бы погрузить вас немножко в контекст и вкратце рассказать непосредственно про саму платформу платформа позволяет разработчикам в духовых командах не тратить время на вникаю в особенности инфраструктуры Как работает как устроен прод сколько там дата-центров своих в облаках и так далее решения все эти реализованы для всей компании централизовано и никто не проходит по одним и тем же граблям разрабатывая какие-то там библиотеки Вари и так далее тут эта штука называется дашборд мы называем это UI интерфейс платформы тут всё что нужно для управления сервисом это деплой в разные окружения информация о потреблении ресурсов деплой манифесты в кубернетес и так далее Так помимо всего прочего есть также блок со связями который какие другие сервисы в платформе и Откуда приходят запрос в этот сер какие другие сервисы в него ходят для ме взаимодействия мы используем свой формат он называется бриф это наша собственная разработка по требовани дуж нашей платформы и эта штука очень похожа на проба но за исключением того что она заточена под нас Онам упрощена и поэтому врем пония ври минимальная разрабы пишут брифы для своих сервисов и затем реализуют ручки те которые они писали они Собственно уже реализуют на том языке программирование на котором они пишут свой сервис во время деплоя схемы регистрируются в специальном сервисе схем и потом другие разработчики с помощью код генерации могут наге неть клиентов также под свой язык программирования Ну по аналогии с grpc и тоже клиенты также заточены под особенности нашей п зарегистрированные схемы тоже можно увидеть на дашборде Вот так это выглядит в U это по сути графическое отображение того же самого брифа за исключением того что здесь ещё можно видеть Кто ходит на конкретную ручку и даже какие поля используются технически это всё rpc то есть это пост запросы J с каким-то чиком в теле и можно даже взять и руками подр какие-то ручки там что-то посмотреть у нас примерно 2 сервисов На текущий момент Так примерно и все они тесно связаны между собой этим самым форматом и вот наступил момент когда потребовалось реализовать авторизацию под эти ручки на этом большом и живом организме что собственно требовалось то есть мы собрали требования опросили тех кому это было в первую очередь нужно и определили какую задачу изначально мы решаем Ну конечно это контроль доступа к ручкам потому что часть сервисов или отдельных ручек они содержат какие-то чувствительные данные это могут быть данные пользователей это какая-то там финансовая информация и так далее и нам необходимо контролировать доступ к ним и быть собственно уверенными кто и как их использует а никакая Ну любая система безопасности она теряет большую часть своей своего своей пользы если мы не знаем кто и когда эти изменения вносил и собственно Для чего чтобы можно было потом по истории посмотреть поэтому логирование это также важное было требование Ну и конечно же не сломать связи большого количества сервисов которые связи уже существуют и любая из них может в принципе привести к деградации продакшена поскольку сервисов много и не хотелось бы заставлять сотни разработчиков вносить изменения какие-то в свои сервисы и адаптировать их под новую фичу которую мы им предложим и также есть не все конечно но есть сервис которые очень чувствительны 1020 миллисекунд ним добавить это уже достаточно критично и этого допустить тоже было нельзя Ну и к тому моменту пока мы пришли к этому проекту некоторые команды уже начали делать какие-то свои решения использовать какие-то там токены что-то ещё городить и необходимо было дать главное централизованное решение поскольку это всё-таки платформа чтобы этим никому приходи зани ри из Вот первый вариант - это было запилить Вари для языков которые используются здесь там не полный список ну этот вариант там не до конца подходил потому что во-первых реализация будут различаться То есть как не круте всё равно это разные языки разные библиотеки с разными реализация это достаточно сложно обновлять то есть если вдруг что-то Нужно обновить и надо перевыпускать сервисов это достаточно дорогая операция и тратить время разработчиков при том что у нас есть продвинутая система обновления библиотек есть бот который в случае обновления библиотек он может делать автоматические пиары в сервисы Как показывает практика не всегда это можно сделать быстро Ну и также инфра протекает сервисы и был ещё второй вариант у нас есть сеш и можно было реализовать на нём казалось что он избавляет от всех проблем которые перечислено выше реализация всегда только одна и она на инфраструктур слое обновлять ког мы посчитаем это нуж когда есть необходимость Вот и инфра останется собственно в инф также немаловажная деталь - это то что нагрузка только на команду па и нет необходимости идти в продуктовой команды нагружать их Работай дальше как это будет выглядеть для пользователя это одно из тоже немаловажных вопросов этот момент не менее важен даже чем Техническая реализация и не менее сложен можно в принципе наградить ЮА Вот Но у нас уже был на это время подход конфигурации как код есть Файлик aptal который есть в каждом сервисе на основании этого файлика собственно вся наша машинерия платформы катит сервис таким образом каким его хотят видеть разработчики то есть Нужное количество реплик переменно окружения запуски кранов там много чего ещё всё это в простом понятном формате поть сделать который будет в таком же формате и он даёт нам возможность логировать изменение потому что этот Файлик будет в гите Также можно на него настроить авы То есть через настройки кодре можно нужных людей там политики накрутить также с помощью речного деплоя Можно выкатывать и смотреть что там убедиться что ничего не отвалилось Что работает мы ожидаем и также погружать разработчиков рели есть как это работает под капотом Ну и просто все привыкли к Да ещ его нельзя переуло потому что тот же самый я если взять там можно накрутить наследование слияние там много разных вещей вот в том это сделать достаточно сложнее это немаловажно при поддержке потом этого формата структура получилась вот такой файл состоит из двух основных секций секци пони можно описать одну единственную эту секцию дефолта и она будет работать на все ручки чтобы не переписывать все ручки и есть полиси уже задаёт правила для конкретных ручек на них уже дефолт перестаёт распространяться при этом на всё остальное продолжает То есть можно узнать можно указать общие правила и потом конкретизировать их для каких-то отдельных уже ручек а политики как политик может быть в файле A сколько угодно То есть в принципе можно описывать их нужное количество Самое главное чтобы не повторялись ручки в одно и то же политики чтобы не было наложений Ну это контролирует валидатор в политиках мы перечисляем клиентов клиентами то есть Клиенты те кто может ходить в эти ручки клиенты полностью перекрывают дефолт и чтобы их описать нужно перечислить всех клиентов даже если они уже описаны в дефолте на самом деле в последствии мы доработали еде возможность использовать переменные то есть мы бем какую-то переменную описываем там какую-то группу клиентов которые используется дальше там несколько раз и можно здесь её уже указывать но к сожалению по времени я про все такие штуки не расскажу но так хотя бы вкратце постараюсь упомянуть сервисы Какие могут быть сервисы сервисы могут быть это во-первых это постные сервисы всегда можно указать звёздочку обозначая что любой пас сервис может иметь сюда доступ либо можно указать конкретный название конкретного сервиса пользователи польз Также можно использовать звёздочку можно использовать конкретного пользователя его логин который у нас ну на всю компанию распространён и каждый знает свой плагин и также можно указать весь Юнит то есть люди могут в юниты приходить уходить перемещаться между ними каждый раз актуализировать Ну достаточно мутона штука поэтому реализована была Вот такая возможность и есть ещё внешне зависимости это по сути всё остальное это могут быть какие-то вебхуки от жиры какие-то боты то есть всё что угодно что не сервис и не пользователь но у них тоже есть свой сертификат всё вроде выглядит неплохо но есть проблемы у некоторых сервисов Есть десятки ручек и десятки потребителей и переписывать всё это вручную достаточно может быть сложной операции А И помимо этого можно ещё и ошибиться в названиях что тоже потом выкатил что-то не то перевырити у нас есть лишка которая работает в общем на всю компанию любого на ноуте есть эта клишках которой можно выполнять различные функции в платформе коды генерировать валидировать бриф интерактивно добавлять зависимости деплоить Ну практически всё что угодно что там доступно с дашборда и мы добавили туда возможность генерации собственно этого aal а он берёт потребителей из сервиса схем которые известны который зарегистрирован который работает в проде а и м добавляет их в секцию default а дальше уже разработчик может Исходя из этого уже как-то это что-то менять Там подкручивать под себя также помимо этого он добавляет туда меню доку которая позволяет Ну чтобы не лезть в документацию можно быстренько посмотреть как работает какие есть типы клиентов в общем прямо всё документировать для быстрого старта так скажем теперь когда мы определились с продуктовой частью можно уже непосредственно переходить к реализации к технической реализации как я уже говорил мы решили использовать серс он у нас сейчас на данный момент работает на и до это у нас было самосе Решение вот когда ещё и был давно-давно ещё не настолько зрелым и не отвечал всем функциям которые нам были нужны Вот теперь мы уже практически полностью перешли на и и он отвечает даёт всё что нам нужно Ну конечно ценой этого является его сложность основ принцип любого сервис - это перехват входящего в под и зарули его на Сайка то есть есть в каждый пот где нужен сервис smh добавляется Сайка Куда приходит этот трафик и дальше уже он через трафик прокси ется в сервис и точно также работает и обратно сервис не шлёт трафик напрямую он сначала слёта в этот прокси а прокси уже отправляет туда куда надо и внутри этого прокси реализуются различные штуки Вот Но прежде чем реализовывать авторизацию сначала нужно знать имя клиента то есть сначала нужна аутентификация для этого у нас реализован mtls mtls нужен был он уже был реализован до этого и он нужен для защиты во-первых от несанкционированного доступа например по IP пода не всей компании нужен доступ ко всем сервисам и поэтому такая возможность закрыт то есть чтобы постучаться в сервис Вам нужен сертификат в котором есть именной сертификат и сервис уже может управлять доступа к себе зная имя также он нужен для шифрования чувствительных данных То есть даже если предположить что трафик внутри кластера как будто бы безопасный на самом деле это не так потому что там работает куча сторонних библиотек каких-то и у разных сервисов В общем факторов много и разновидности Атак что тоже может повлиять поэтому внутри кластера трафик тоже шифруется Ну и конечно же он нужен для авторизации но чтобы эта штука работала нужны сертификаты сертификаты можно выдавать пользователям владельцам сервисов вручную каждому чтобы с сертификат А можно это дело автоматизировать потому что вручную достаточно сложно для этого уже тоже есть готовые решения мы используем spire spire - это штука которая реализует стандарт сфи описывающий подход к идентификации и работе с секретами в инфраструктуре Как это работает на практике на практике этот состоит из двух компонентов это сервер и Агент рр отвечает за выпуск новых сертификатов а Агент запускается на каждой ноде конфигурирован нек с другими с другими сатками такин посредством mtls протокола и присылает конфигурацию Иво Каким образом ему и где получать сертификаты и таким образом знает что нужно пойти к Агенту и получить сертификат выдан собственно с помощю которого начать шифровать трафик как это в итоге работает м по итогу Сайка шифрует входящий трафик и расшифровывает входящий одновременно извлекая имя клиента из сертификата в каждом есть это либо серве ну сервис в данном случае если мы говорим про внутри кластерный трафик и помещает имя клиента в заголовок X Source в header типичный хедер таким образом сервис имеет доступ к этому хеде и может с этим заголовком что-то делать то есть понимать кто к нему пришёл и таким образом что-то там как-то разруливает ему или нельзя а и этому заголовку можно доверять но есть ещё и пользовательский трафик то есть пользователи иногда могут захотеть постучаться на какую-то ручку рло или чем-то ещё А И это хоть уже и не совсем межсервисный взаимодействие но всё-таки его тоже было необходимо поддержать У нас есть балансер один общий внутренний который доступен через VPN и с помощью него можно получить доступ к какому-то из сервисов то на попасть одного из сервисов некоторые ручки так и дёргаются пото некоторые пря не имеют U и разработаны служебные для этого и таким образом этот функционал разбора сертификата работает на балансе то есть внутри там функци позволяет па сертификат из клиенты точно так же появляется зало который дальше уже идёт в сервис как я уже говорил использует в качестве ври за проходит через цепочку фильтров что-то вроде Вари и дальше идёт в целевой порт внутри внутри пода а может не пойти то есть фильтр может его заблочить а может там и Бади поменять Может хедеры поменять вообще там всё что угодно может в принципе сделать существует огромный набор фильтров уже готовых но в принципе можно делать ещё и свои там на го лу в асме и плюсах чём-то ещё и конечно же Нашёлся фильтр для авторизации готовой коробочной что он делает это фильтр делает запрос на какую-то внешнюю штуку которая говорит можно или нельзя спрашивает То есть можноли это запрос лететь дальше либо нет Если штука блокирует запрос то соответственно ивой сразу отвечает 403 и запрос до сервиса даже не доходит то есть сервис даже не в курсе что к нему кто-то пытался постучаться Осталось только найти эту штуку которая это всё делает и такая штука уже тоже есть готовая вот называется она Open policy Agent это cncf дипломированный проект он развивается с шестнадцатого года достаточно старый Несмотря на то что у него версия мажорная ноль ещё пока что э но сервис smh - это одно из его назначений А и у него особенности он использует собственный язык для описания политик это как и плюс и минус я сейчас чуть дальше об этом скажу а и есть у него множество готовых интеграций и конечно же с такой популярной штукой как anid тоже есть а она может эта штука может работать в двух вариантах Первый вариант - это Демон Да это Гош ная разработка поэтому Бинар под любую платформу и с ашкой который которой можем отправить запрос спрашивать можно или нельзя либо Гош библиотека которую можно встраивать в какие-то свои инструменты в этот этому Агенту мы передаём политики которые мы написали на и также передаём какие-то данные Ну в данном случае это могут быть данные которые могут использоваться в политиках например данные об орк структуре Компани им-то е как его осва это декларативный язык для выражения политик над сложными иерархически структурами данных его особенность в том что описывает политики простыми способами и сокращённым синтаксисом позволяет обрабатывать сложные какие-то выражения коротко Ну буквально в одно выражение у нас опять же Это данные о ор структуре мы же хотим Да мы хотим как я уже до этого показывал давать возможность указывать не пользователя а допустим целый Юнит сонно нам нужно перемалывать эти структурные штуки Кто в каком юните в какой команде вот это вот всё А Reg выглядит примерно вот так в начале может показаться достаточно сложным но когда разберёшься то всё становится понятнее Хотя конечно как и в любом другом инструменте тут своих приколов и подводах камней тоже хватает и особенностей А читабельность вот пример как может выглядеть прогон многомерного массива на питоне и как это то же самое вещь та же самая задача решается с помощью рего Ну справа читать гораздо быстрее очень жаль что этой возможности нет в тех языках которые я использую Вот потому что регор зарабатывал Сим под программированию доступов и в поддержке потом это можно просто взять и глазами быстро прочитать и гораздо быстрее понять Чем дебажить какой-то там своё решение пытаться разобраться Почему читабельность разобрались дальше расширяемость а в рего поскольку написано нагой его достаточно просто расширить добавляя разрабатывая плагины например для похода в какие-то ваши внутренние опиш какие-то внутренние базы данных если это нужно для того чтобы правила заработали тестирование deb тоже одна из возможностей интересных которая нам показалось важной потому что ну можно брать правила одного сервиса правила другого сервиса и сравнивать их на совместимость что ничего не отваливается и осталось только сделать интерпретатор который будет собственно А переводить в это оказалось даже проще чем казалось получилось всего там пару Строн строк кода вместе с шаблонами полиси ре в итоге итоговый получился Примерно вот такой он состоит из двух основных частей первая часть - это общая для всех которая забирает там нужные входные данные определяет тип клиента по умолчанию какие-то делает вещи например по умолчанию переменная lao равна fse То есть то что нельзя вот а во второй части уже Мы проходим по политикам которые сгенерировал зау мла и если хоть одна из политик говорит что можно соответственно доступ есть то достаточно чтобы сработала только одна политика сами правила работают в принципе тоже Ну максимально плоско есть какой-то путь э если клиент - это пользователь то соответственно ищем его в массиве пользователей Юнита это значит тогда будет Т политика срабатывает если все условия внутри неё истины то есть каждая строчка если истинна тогда политика срабатывает то есть Таким образом мы просто с Аса Тола переводим все те же самые условия вот сюда как итоговая схема как работает запрос попадает на Сайка Сайка спрашивает у агента скажи можно ли мне вот пришёл ко мне запрос Вот с таким вот вот на Такой путь он пришёл вот такой вот у него xce такой клиент Можно ли мне его пустить дальше Агент обрабатывает рего правила и отвечает да или нет Если нет то соответственно просто 403 и всё но потом мы захотели померить сколько миллисекунд мы добавили к запросам померили время которое Т на собственно Запрос к Агенту и у нас получилась Вот такая картина там местами было там до 10 миллисекунд и это было много то есть в большинстве случаев Возможно это Окей Вот Но у нас для некоторых сервисов Это был бы блокер для использования авторизации метрик кстати говоря в начале не было мы даже запили Пиа и он даже в github его приняли То есть были метрики на http вот а на grpc по которому Иво агентом работает Трик не было Вот пришлось их добавлять нагружен сервисом нельзя больше 2 миллисекунд добавлять так вообще прямо в идеале вот а у нас было до 10 и это было прямо много при этом сам Агент отвечает меньше чем за 2 миллисекунды то есть непосредственно сами правила обрабатываются очень быстро Вот и эти основные потери они были где-то между то есть где-то между на локло Хосте где-то между сайд карами что было достаточно та удивительно до 8 миллисекунд там терялось И что самое главное мы тратили время на одинаковые во То есть получается что один и тот же клиент всегда получит один и тот же ответ для одной и той же ручки То есть он первый раз говорить что нельзя Значит все запросы тоже Бут нельзя до тех пор пока сервис не будет переп пока не будут изменения в внесены поэтому мы решили посмотреть в сторону кэширования как я говорил у Иво есть возможность делать свои расширение рассмотрели четы варианта Первый вариант снм интерфейс для ланга был Ещё достаточно сырой потому что а он менялся даже в минорных версиях и это было очень очень критично потому что любой а попытка обновления сайт Кара попытка обновления ISO приводило бы к тому что пришлось бы переписывать вообще целиком это расширение Ну как бы не вариант а второй вариант vasm он во-первых это виртуальная машина которая работает рядом и она добавляет там свой оверхед по ленси и в принципе нивелирует во-вторых там урезанная Гошка и там какие-то проблемы с с htp запросами что даже в общем не подходило никак можно было расчехлил тоже скилы но не очень хотелось этим заниматься и был лу с которым был уже опыт Ну достаточно простой как палка предоставлял возможность делать htp запросы мы запили прототип в принципе тоже много времени не заняло быстро посмотрели нагрузочные тесты и метрики и оказалось что прибавка не больше 1но миллисекунды как хотели то есть условно запрос первый раз он делает запрос идёт на Агент второй раз этот запрос Уже не идёт потому что он просто хранится в памяти а дальше валидация недостаточно просто включить авторизацию вот самое главное никого не сломать и причём не сломать не только технически но ещё и логически а допустим ситуация у сервиса А есть ручка Get User на неё ходит сервис B а при этом сервис а закрывает к ней доступ и выкатывает то есть в ауле говорит что э руч больше никто уходить не может выкатывает это сервис B сломан и соответственно мы получили деградацию Поэтому нам необходимо было проверять закрытие существующих связей То есть те которые уже существовали на данный момент а потом необходимо было проверять корректность самого амла то есть это что одна ручка указана только один раз что клиенты существуют которые там указаны что вообще-то есть такие сервисы есть такие пользователи что сервис включен вообще в принципе TS потому что без него ничего работать не будет Вот и не допускать впро с ошибками то есть просто блочить деплой как я уже говорил у нас есть не некий реестр брифа то есть того формата по котором сервисы взаимодействуют которые позволяет аналогичные проверки делать для брифа например что нельзя сломать обратную совместимость ручки или удалить поле которое уже кто-то использует то есть такое изменение просто не выкати впт мы добавили аналогичную штуку для амла То есть когда сервис выкатывается и его шные правила складываются в реестр этих столов и там собственно хранятся а затем во время перед тем как сервис деплоить Он к этим связям имеет доступ что эти правила работают то есть та самая возможность тестировать правила которые я говорил до этого А это даёт возможность получить собственно актуальный а для для любого сервиса на самом деле мы храним потому что ну могут меняться интерпретатор меняется и нам важно именно Тот рего который хранится который сейчас работает в проде вот и получается мы знаем кто ходит наш сервис куда ходит этот сервис Вот и на самом деле это работает не только при деплой но ещё и локально можно запустить проверку Там же в крышке запили дополнительно нте и во время создания Пиара то есть на C Он отрабатывает наравне с другими литерами и прочими проверками как работает Проверка зависимости катим сервис мы смотрим папку в которой лежат Клиенты есть бриф тех клиентов которы ходит сервис и непосредственно интерфейс кото мы катим мы берм этих зависимостей из прода есть в который он ходит включа даже канареечный рата се может быть сломать то то Тестируем данными теку Клиенты у этом сервис то есть кто в него ходит берём текущий рего сервиса и проверяем доступ у клиентов то есть также передаём путь передаём название клиента и убеждаемся что доступ есть А вот если упал бит что упал Билд Что делать дальше А во-первых можно оставить связь как есть то есть Наверное мы просто наткнулись на связь которая нужна разработчики идут друг другу и договариваются можно побить как декей что связь она ещё есть но она на самом деле уже не нужна Просто тем давно забыта и надо её просто выпилить пока и поставили задачу на то чтобы выпилить её Скоро или нашли Да просто ненужную связь что в итоге какое мы оценку В итоге сделали результата что получили И какие выводы с точки зрения технической реализация реализации тут можно оценить новый продукт который внедрили это Open polic во-первых Он дал возможность не писать и не поддерживать какое-то сво решение понятная и читаемая политика на выходе для платформенной команды удобная работа с оргструктуры со всеми вложенными вложенными какими-то иерархически данными и также возможность тестирования доступов что оказалось крайне немаловажным с точки зрения платформы с точки зрения пользователей которые платформу используют это простое понятное решение А у проще некуда Любой человек Берт и сразу про начинает этим пользоваться сервисы не затронуты то есть не пришлось вносить никакие изменения то есть нужно просто добавить один ещё один просто Файлик конфигурации А и связи не сломаны самое главное существующие связи у нас пока не было Э ни одного инцидента чтобы существующая связь сломалась То есть все валидатор отрабатывает как и планировалось А ну и выяснили что просто добавить авторизацию недостаточно А и необходимо добавлять валидации генерации и кучу про дополнительного DVX обвеса спасибо вопросы раз Антон Спасибо тебе большое что-то У меня микрофон не работает О заработало Спасибо большое случилась магия и у меня заработал микрофон так у нас традиция традиция давать подарочки за лучший вопрос поэтому поднимайте руки пожалуйста держите их будем распределять пом Вот вот вот там Спасибо большое Меня зовут Александр У меня есть один хитрый вопрос Подскажите пожалуйста А после внедрения авторизации насколько сильно увеличилось потребление ресурсов всей схемой каждый этот дополнительный все системы сейчас не скажу точно я могу сказать что каждый дополнительный сака он е примерно 0с в кубовых энология вот а по памяти что-то около там меньше 20 Мб потребляет вот Ну пока ещё не замеряли сервисы у которых очень много потребителей там где будет кэш Ну по идее должно быть немного потому что там достаточно простой кэш вот я не думаю что тоже будет там несколько мегабат спасибо Вот тамм был вопрос можно уточнить ши есть доступ нет доступ он на стороне сервера происходит или клиента на стороне ия то есть нане на А как смотрить вот допустим у нас есть политика рега позволяющая клиенту ходить и тут мы ему говорим тебе нельзя ходить как происходит обновление вот этого кэша у клиента пере деплой полный то есть потому что А он как бы лежит в коде чтобы его поменять Нужно пере деплоить сервис Окей понял Да вот Ну на самом деле сейчас тоже времени не хватило бы есть механика по которой можно делать это на горячую Вот Но для некоторых сервисов потому что есть сервис которые долго сложно катить Вот вот такой там достаточно болезненный деплой вот нуно сюда уже это просто не влазит Вот потому что у агента есть возможность делать Она с помощью полинга с помощью кол плейна делает обновление данных таким образом обновляем данные по структуре по юнитам и в том числе можно обновлять политике вот там вот потом вот сюда вот переместимся Добрый день спасибо за доклад конечно Володя Меня зовут вопрос такой больше продуктовый Как вы вообще пришли к этой схеме То есть как продукт этот организовался вы ваши потребители по сути да это внутренние команды разработки Да вы как-то обратную связь собирали как-то анкеты не знаю что вам надо Что не надо как вам отвечали команды вот интересн процесс вот сбора этого информации Да я понял Ну в начале есть стейкхолдеры которые мы опросили что им нужно там отдел безопасности вот все кто непосредственно заинтересованы лица они написали требования которые в начале там указал Вот Но мы внедряли это не сразу пря Бах и всё вот вам сначала было этап бета-тестирования там желающие присоединялись это было Некоторое количество сервисов они начинали это дело использовать Вот и пока не использовали у них там миллион вопросов возникал а как это как то и благодаря на самом деле этой обратной связи Вот такому м мягкому входу мы дорабатываем такие фичи как А вот группы клиентов чтобы не переписывать их А вот это вот горячее обновление тоже выяснили что оно на самом деле много кому нужно вот и прочие прочие штуки И за счёт общения вот с этой с этими первыми Бета тестерами вот дальше продвигалась это вот развитие продукта вот таким образом так вот здесь вопрос потом вот сюда вот переходим Здравствуйте я здесь я тут Привет А спасибо большое за доклад У меня на самом деле больше вопрос про ивой Если что мы можем потом в кулар Подскажите удавалось ли вам с помощью НВО L4 трафик балансировать и приходилось ли и использовали ли Q Fans Ну это нас сейчас не совсем к теме доклада можем можем тогда лучше после хорошо Спасибо хороший вопрос Спасибо Давайте Вот вот архитектор решений А у меня такой вопрос А почему не Почему не Кипер EK да да эти решения есть Ну на момент когда мы это начинали внедрять ещё год назад был Сват вот ну для тех кто не знает это такая штука которая от Open решение кото можно это реализовывать вот во-вторых уя система деплоя в которую кипера было бы сложнее достаточно впихнуть а в-третьих гет keer не давал возможность правила тестировать то есть там сложности были с этим вот и ещё какое-то четвёртое почему его не стали сейчас вот так уже не могу вспомнить я потом вспомню потом вам скажу ещё что-то четвёртый было Точно Почему не его брали а AK clow - это штука которая вообще для аутентификации по-моему Да если я не правильно помню тоже моторизация но в основном для стов Да ну специфика ваших сервисов в том что у вас не только растапи да у вас наверное ещё какие-нибудь там jpc например или нет У нас основной протокол бриф вот этот вот но он по сути rpc Спасибо Ребят а перед тем как Следующий вопрос Я вас попрошу сделать одну вещь Достаньте пожалуйста свои телефончики Достаньте Ну Достаньте Достаньте Достаньте сделайте приятное спикеру направьте на этот QR код и Оставьте отзыв и оценку это очень помогает спикеру очень помогает нам программному комитету для вас сделать коре СБО вам большое И вот дальше здесь вопрос Здравствуйте спасибо за доклад Я хотел спросить Они рассматривали ли более динамические схемы изменения контроля доступа вот некоторые пишут свои сервисы там это атрибутный ролевой контроль Чтобы в них можно было ходить их тюнить через ручки У угу А ну некоторые решения платформенные как бы люди пишут свои вот эти вот сервисы которые обеспечивают контроль доступа у которых есть менеджмент ручки чтобы можно было схему в динамике менять чтобы прямо на ходу Ну на ходу можно кого-нибудь сломать тогда что-нибудь поменяло и что-нибудь отвалилось и прот упал и Вы получили недоступности и кто-нибудь премию не получит И второй вопрос не совсем про авторизацию про метрики rpc метрики что-то готово использовали ли структуру свою то есть Вот например у Open есть prus Есть ли что-то у rpc у нет мы вообще используем у нас Victor Matrix для сбора Рик был раньше проу но он перестал там какой-то момент вывозить вот а Виктория лучше все показывает вот мы все метрики собираем там вот это то есть Т нуй даёт какой-то метрик готовых которые можно ходить и собирать Ну не все потому что на самом деле Вот он не даёт возможности замерять сколько работает каждый фильтр Вот это тоже отдельная история Как замеряли скорость работы в этого лу Вот это отдельно там приходилось пушить там вообще могу потом после рассказать как это всё вообще Виктория Так где у нас ещё вот вот рука Поднимите руки где ещё у нас будут вот потом там да спасибо за доклад вопрос вы мтлс используете только там меж взаимодействие или внутре и внеш и внутре а вообще для всего вообще везде Кошмар Спасибо Почему Кошмар Зато безопасно раньше было без ТЛ и тоже постепенно это внедрять сервисы постепенно включали а потом просто включили всем сертификаты пользовательские получаются тоже автоматизировано они не выдаются вручную то есть это та же самая клишках директории свое Вот и у сервисов тоже с помою спара тоже автомати человек хотел услышать полегче а ты всю вселенную сломал давай там Привет Спасибо за доклад круто смотри вопрос про консистентность Да у тебя есть структура человека уволили у вас только через деплой контекст обновляется как мы ему закроем право Да я вот до этого как раз про это упоминал чуть-чуть вкратце просто уже не влазит в доклад есть Control Plane который собирает данные из наших учётных систем по структуре в этот Control Plan ходит каждый Сайка раз там в минуту допустим там в две и забирает актуальные данные То есть это коробочная возможность у Open польс Агента вот а и собственно таким образом обновляются данные по юнитам Ну смотри вот у тебя у сервиса Может быть 100 подов Да консистентность не будет Ну то есть пока они все обновят ну какой-то пользователь в течение 2 минут не будет иметь доступ не ну главно не будет иметь будет иметь Ну в этом же проблема Ну то есть он что-то имел какой-то доступ Да это там не знаю хакер какой-то да про Да тут всё от тре зави можно поставить каждые 5 секунд обновлять вообще Если уж настолько критично прямо интересен кейс Когда нам за 2 минуты критично отозвать доступ к куда-то там в таком случае можно у него сертификат отобрать если какой-то польз комментировал можно просто им тело сертификат отбирать централизован Всё если вот тут вот вопросик А спасибо я Андрей Большое спасибо за доклад Я немножко запутался в ране слайда Когда речь шла про авторизацию живых человеков то есть Я так понял что в норме авторизация МТС от Сайка до сакара соответственно в случае живых людей там стоит какой-то н который Термини ет на своей стороне mtls и видимо дальше он ходит уже просто с хим А как будто бы в этом месте типа вка тогда можно с улицы прийти с хром и без сертификата всё заработает или как оно тут устроено Нет нет нет Н он туда в этот во-первых это внутренний который только под впм доступен вот во-вторых туда приходишь в этот ин тоже с сертификатом он его парсит ну есть коробочный модуль который может парсить Вот и он смотрит оттуда забирает имя он не делает сам авторизацию Он просто говорит что это запрос Вот с таким именем и дальше шлёт его в ингресс а Доверие от инса до дальше конкретного сервиса Как там Там отдельный общий сертификат которым шифруется все запросы до это О спасибо вот там вот ещё вопросы Спасибо за доклад зовут Сергей скажите Я возможно пропустил момент небольшой после внедрение кша первый запрос также оставили там условно медленным который до 10 миллисекунд то есть не стали рес момент да да ну он ни на что не влияет первый запрос 10 миллисекунд вообще это его даже даже никто не увидит на графиках Да просто вы изначально сказали что это как бы не укладывается в рамки но в итоге решили оставить укладывается если это постоянная картина если на нашем высоком нле там 9995 высокий перцентиль там не укладывается вот если вообще такая картина А если первый запрос его там даже будет не видно на этом графике То есть он вообще ни на что не влияет вот при Прим на более низких нх то есть там на 99 там и 95 например вот там картина совсем другая там меньше 2 миллисекунд Ну просто тут уже Кому Какие у кого какие требования какие критерии вот у нас нас не устраивало 95 нам нужно вот 95 Спасибо Давайте там ещё один вопросик был а дальше дискуссию переведём в зону Спасибо за доклад Александр у меня пару вопросов первое по ролевой модели я правильно понимаю что на стороне самих сервисов остались роли роли какие нет ничего не осталось А ну допустим группа или роли вот которые может человек задавал вопрос про клок который можно настроить вот допустим в клоке нет никаких групп нет можно указать если касаемо людей можно указать либо конкретного человека либо Юнит в котором он работает то есть весь Юнит всё так понял второй вопрос PK движок отзыв сертификатов необходим в такой схеме когда межсервисный авторизация отзы сертификатов Нет зачем аутентификация Она до авторизации стоит как бы она окей Тогда вопрос про Vol отпадает и движок Ну и собственно говоря последний момент про какую так Или это уже не относится я хотел про ubk спросить Может ли пользователь с кеем прийти Ну сертификатом закрытым ключом на ке Ну у нас Вроде только для авторизации ВПН и как бы всё получается Он не влияет вот на дальше на вот эти вот сервисы всё понял спасибо спасибо большое Ребят а Почётная миссия твоя выбрать лучший вопрос был второй вопрос я вот оттуда я забыл только о чём он был прямо был самый лучший вопрос второй по ходу да вот дадада Вот вот да молодой человек напомни пожалуйста о чём обновле Вот точно про горячее обновление кше про горячее обновление кэше соответственно вот молодому человеку уходит подарочек от нашего партнёра за лучшие вопрос Антон А тебя Мы благодарим Спасибо тебе большое за отличный доклад и на память от компании онтика от нас э тебе как это памятные ценные призы чтобы ты к нам чаще приходил и делал доклады Спасибо большое L"
}