{
  "video_id": "lzdQzQ5jOA0",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Виктор Бурцев из Яндекса Я думаю что ребята отлично знают как строить сервисную архитектуру встречаем под Ваши аплодисменты раз-раз Всем привет Меня зовут Виктор я чик в Яндексе департамент Яндекс Go направление такси команда кор такси и примерно год назад в на в нашу команду вообще наша команда занимается тем что мы занимаемся циклом заказа такси и примерно год назад к нам приходит задача обычная таска о том что нужно сделать такую штуку которая называется Что такое Life Activity а Apple анонсировал с версией 16.1 вот эту фичу так и назвал её Life Activity её особенность в том что а она позволяет давать информацию о вашем приложении даже на заблокированном экране телефона а и в том числе обновлять эту информацию также на лог скрине а мы такие Всё круто задача выглядит классно Давайте делать собира понимаем в компании в продуктовой разработке ещ никто ну с подобной задачей не сталкивался вообще в мире около двух компаний на тот момент занон сили у себя вообще реализацию этой фичи мы значит собираемся с нашими смежники Давайте обсуждать форматы как мы вообще будем вместе работать нно ской штукой Почему Потому что как раз она работает на заблокированном экране когда я делаю заказ я заказываю такси делаю заказ заблокировал телефон положил его в карман всё у меня клиент полностью потерял возможность опрашивать сервер в формате запрос ответ и тогда мы понимаем что всю логику на себя должен брать энд в этой всей истории А ещё оказалось такая юва Это pns appa я в дальнейшем для контекста буду говорить просто apla а App appa принимает этот токен по которому понимает Какое Устрой У какого устройства нужно обновлять Life Activity и в отличие от пуш токена которые А мы можем получить на момент инсталл из стора А по которому мы отправляем обычные пуши для Life Activity токен создаётся конкретно в момент создания Life Activity и создаётся кстати тоже асинхронно А вообще про что я тут буду рассказывать Я буду рассказывать о реализации фичи которую синхронно в принципе невозможно было сделать но для начала Давайте вообще дадим понятие А синхронности вот Википедия нам говорит о том что асинхронная Ну вот концепция асинхронного программирования - это такая концепция при которой результат работы функции доступен нам не сразу а в виде некого асинхронного то есть нарушающего обычный порядок выполнения вызова но конкретно в рамках доклада я буду говорить об асинхронно как о большом количестве фоновых задач каждая из которых запускает новые фоновые задачи какие-то ещё фоновые задачи перезапускает уже существующие фоновые задачи фоновые задачи для апдейта данных для других фоновых задач Ну в общем о большом количестве фоновых тасо А вообще расскажу с чем столкнулись С какими проблемами как их решали на что обратили внимание А на что не обратили стоило бы обратить и расскажу о том как бы вообще что бы отличалось если бы у нас была возможность синхронно реализовать эту задачу и такой сразу спойлер особо-то никаких проблем в отличии в проблемах не было бы а теперь расскажу как мы вообще всю эту историю запили во-первых нам нужно было получить токен токен по которому мы обновляем и вая была такая о том что есть ручка создания заказа А и есть база данных в котором мы храним данные о наших заказах передавать просто токен в эту ручку сохранять в базе и потом этим токеном пользоваться но проблема была в том что токен создаётся асинхронно А мы не могли позволить себе удерживать создание заказа пока у нас не создался токен соответственно эту реализацию мы отме таем и что мы делаем параллельно с созданием заказа Мы также вызываем ручку вообще другого сервиса который когда-нибудь сохранит этот самый токен вообще заказ в Яндек такси представляет собой йт машину это по сути машина состояний Это ожидание вообще это поиск машины Когда вы просто только нажали заказать такси Это ожидание когда машина приедет к вам ожидание водителем вас сам процесс поездки и завершение всего пять статусов Ну на самом деле больше но вот пять основных статусов эти статусы меняются из водительского приложения когда водитель жмёт соответствующие кнопки То есть он может нажать там что я ну принял заказ поменялся стоит он приезжает жмёт Я на месте стоит поменялся А И у нас есть специальный сервис который называется процессинг мы можем в наших сервисах подписываться но на состояние на изменение стейта и как раз про он грит эти события на сервисы которые на него подписаны соответственно чтобы инициализировать первый а мы подписываемся на процессинг получаем данные о том что й поменялся идём в нашу базу и инициализирует обновлять для это Кози это та по сути представляет из себя бесконечный цикл самой себя она отрабатывает собирает данные из базы и перезапускает Точно такую же задачу на секунд вперёд Но поскольку та цикл ничего не знает о все который уходит в базу собирает эти данные и отправляет в ещё на есть вот такие вот кнопки это мы можем связаться с водителем мы можем сказать иму что я уже выхожу мы можем оценить поездку по сути эти кнопки являются ликами в приложении но когда юзер с ними прокон поня поскольку это пленки мы вызываем ручки соответствующих сервисов которые заставляют отложенную тас запуститься прямо сейчас а то есть таска стояла на сколько-то секунд вперёд Нам нужно поменять стейт мы говорим ей Запускай се сейчас а более низкоуровневая реализация у нас такая а вообще все таски у нас хранятся в монге это позволяет нам иметь пропускную способность около 20.000 фоновых задач в секунду и Life Activity обрабатывается на нескольких машинках они работают через Агент очередей то есть каждая машинка может пойти в Агент очередей сказать что Добавить запись в базу и на каждой машинке работают воркеры воркеры забирают задачи из монг Ну то есть вообще воркеры они просто полят базу постоянно пытаясь забрать какую-то часть задач из себя и на рынке примерно два аналогичных реше два аналогичных решения это амазонок а Simple Q серс и Гугловский Cloud tasks Аа сама таска в монге представляет из себя Ну если очень упрощённость из айди Ника название очереди каких-то входных аргументов и очень важного параметра ята estimate который говорит о том когда воркер должен брать тас в работу а Ну как я сказал ворке работает по принципу полинга он просто ходит раз в секунду в базу говорит Дай мне те таски у которых Эй уже начался и если у нас есть отложенная задача то и нам нужно её перезапустить нам достаточно просто поменять е задачи и тогда воркер возьмёт все таски у которых ета начался прямо сейчас в работу а и вот на тот момент архитектура общая выглядела вот так сразу вопрос есть предположение всё ли было нормально С такой архитектурой или были с ней какие-то проблемы так Какие было логично упущенные задачи дублирование упущенное дублирование токены так воркеры пода Так а что ещё раз в целом Так что поговорим про фейлы А вообще первый фейл был в том что мы неправильно рассчитали нагрузку А мы изначально расчёт был в принципе правильный А и когда мы запустили фичу на команду из 20 человек оно всё хорошо работало то есть вообще прямо отлично А мы раскатываем на весь Яндекс и В целом все довольны у всех всё работает А после этого Мы открываем фичу на 20% юзеров графики идут вверх выживают примерно 90% железа мы добавляем железяк на каждую машинку и всё становится опять Нормально мы катимся на 50% а потом к нам начинают приходить о том что пишут юзеры у меня значит на А время 10 минут в приложении смотрю 5 минут и таких репортов много мы понимаем что проблема в том что время апдейта таски цикла у нас слишком большое мы занижаем до раз в 30 секунд Наступает вечер пятницы и у нас раз в минуту приходит мину % через минуту 90 потом уже криты пошли что мы делаем Ну мы просто откаты откаты на 20% юзера и идём считать нагрузку по новой и нам очень помогли конфиги обычные конфиги которыми мы начали регулировать А как часто мы должны обновлять вот такую вот таку в цикле вот этот вот самый бесконечный цикл который постоянно ходит в базу добавляем и меняем в м время не подумали про мониторинги у нас да у нас были стандартные мониторинги сервисов То есть это какие-то железо графики всего бэнда но на клиенте не работали на клиенте фича Просто иногда отказывалась работать и мы понимаем что у нас Ну нету инструмента который нам скажет А всё ли у нас вообще нормально и вообще на самом деле тут такая проблема конкретно с от что Apple он в принципе не даёт много возможностей как-то мониторить систему именно клиента При заблокированном экране то есть мы до сих пор пытаемся справиться с этой проблемой решали мы на тот момент тем что добавляли просто клиентские метрики которые пытаемся отправлять Когда всё-таки у нас связь с сервером появляется А следующий фейл был в том что мы использовали Cor сервис для получения актуальных данных а ну как я говорю мы ходим фоновой таско именно в и когда мы когда я говорил что мы просчитались в rps чтобы вы понимали цифры просчёта изначально мы посчитали примерно 23000 пса в итоге мы получаем 7000 п на таку цикла 1000ps на таку апдейты 1000 п на контакты с активити и 1000 на различные перезапуске и в часы Пика мы получаем 15000 фоновых задач в секунду и когда мы 15 к нам приходят ребята Изра и говорят что мы ВС понимаем это всё красиво и так далее но мы тут предоставляем услуги такси поэтому Собирайтесь и уходите от нас куда-нибудь там в другое место что мы делаем мы инициализирует цикле она ходит в этот самый дис и забирает данные и для контактов с Activity Мы также добавляем ещё одну фоновую тас так и называемую R update которая обновляет данные в редисе и Форт перезапустить таку прямо сейчас а и Последний фейл был в том что мы не обрабатывали все возможные ответы сервисов иногда перезапускать фоновые задачи не стоит если например ваши корсер висы не отвечают в этом случае нужно просто Её убить или наоборот если смежные сервисы прислали что-то непонятное то может быть есть смысл трая попробовать получить данные снова вообще такой мини вывод о том что очень полезно читать все возможные ответы ваших смежники и уже оттуда решать Какие для вас приемлемы какие Нет вообще какие проблемы И их решение фоно асинхронных задачах А вот первая и основная проблема - это отладка А ну вот как я говорил вообще на самом деле у нас есть ещё два сервиса которые а представляют собой полный путь от клиента до пла когда мы работаем с Live Activity Ну это просто для контекста А И вот во всей вот этой схеме если у вас много сервисов и они работают со синхронной у вас таски просто теряются и именно вам как разработчикам придётся понимать где они потерялись и как что вообще нам для этого поможет Первое - это трассировка то есть всегда ещё и клиентские метрики Если вы у вас клиент-серверная архитектура а пишите на клиентах о том что вы эти данные принимаете следующая проблема - это балансировка таких задач А у нас был случай когда на нашем балансе вот а те самые воркеры о которых я говорил есть настройки этих воркеров А И у нас было указано брать Ну слишком большое число брать 6.000 задач на ноду у нас было несколько НОД всего 7.000 задач первая нода хватает на себя 6.000 вторая нода хватает тысячу а вторые вообще ничего не делают потому что первые две просто забрали всё А ну и первая нода Просто алер то что я сейчас умру А ну что мы сделаем мы просто перебан сирова указали не слишком большое число а там буквально по 200 тасо на ноду всё стало хорошо следующее - это управление фно та вообще стоит завести конфиги всегда стоит заводить конфиги когда мы работаем с Тами асинхронно это позволит нам дать хоть какой-то контроль над ними то есть контролировать можно вообще всё это могут быть количество ваших та какие-то тайм-ауты если у вас в предложении предусмотрен режим деда подумать о том насколько эти фоновые задачи ваш Ну важны для для вашего бизнеса в целом для приложения то есть А в нашем случае Life Activity - это хоть и довольно видная штука но она никак не влияет на основной цикл заказа такси поэтому А в случае если у нас всё плохо в приложении нам проще убить Life Activity и дать а нашим Железяка там выдохнуть чуть-чуть А ну то есть конфигов у нас было много мы могли конфигурировать вообще всё то есть это и максимальное количество ТВ Это таймаут для каждого статуса заказа и даже мы могли конфигурировать алгоритмы вычисления там оставшегося пути машинки и последняя проблема - это оповещение то есть алерты они вообще в принципе важны вообще определитесь Как вы хотите получать алерты то есть там СМСками в телеграме на почту звонком Ну и Нужно следить за ресурсами да то есть следить за железом своим смотреть на графики следить за метриками Если вы не можете посчитать нагрузку как мы не могли да тогда имеете возможность постепенно раскатывать но и имеете также возможность откатиться и последняя проблема - это завершение таких фоновых задач нужно всегда подумать о том по каким условиям мы их долж завершать есть этото но завершение когда всё нормально завершение по каким-то конкретным условиям то есть как у нас это изменение йт машины это может быть аварийное завершение и тогда нужно опять же подумать что делать с ними ретра или убивать эти задачи И какие выводы из всего этого можно сделать Ну в целом В асинхронных задачах точно такие же проблемы что и в синхронных что и вообще в реализации именно синхронной есть небольшие юан что таски могут теряться могут работать не так как нам хочется для этого лучше оставлять себе лазейки в виде тех же конфигов то есть мы чтобы иметь хоть какой-то контроль над ними чтобы иметь возможность вмешаться в ход выполнени этих задач А вообще определитесь является ли вот эта вот фича критичной для вашего приложения Ну как на нашем примере она не является таковой поэтому мы можем просто себе позволить убить е если у вас фоновые таски прямо Крит прямо бизнеса бизнесов важны тогда продумывать свои варианты А ну и такой дополнительный вывод о том что вот пожалуйста всегда согласовывать всё с вашими смежники Если вы Ну приходите к ним и говорите что мы вам сейчас принесём 15.000 ПС И вам как вообще такое нормально или нет А у меня всё спасибо Спасибо Виктор очень классный доклад ребят у нас по традиции секция вопросов ответов мне сообщили что у нас даже будет Два подарка за самый лучший вопрос сейчас поэтому а жду ваших рук поднимайте руки у кого есть вопросы первый ряд сразу микрофон идёт Да добрый день Я хотел вот уточнить Там картинка была интересная с масштабированием задач между нодами и вот докладчик указал на то что всё это с помощью конфигурационных файлов каких-то регулируется хотелось бы узнать почему конфигурационные файлы а не какое-то динамическое регулирование нагрузки вы вот про это а да да вообще вот конкретно балансировка фоновых задач у нас А ну это самописная по сути да это настройки для воркеров для каждой ноды и у вас действительно в этих решениях есть возможность указать балансировку не такую жёсткую что я там хочу брать разом до 200 задач А и различные стратегии балансировки использовать То есть как робины там какие-то с Весами Вот это всё но мы конкретно используем вот эту потому что у нас машинки все одинаковые нам достаточно их балансировать равномерно Не ну понятно допустим под новый код там или ещё когда-нибудь ну нагрузка то постоянно меняется То есть у вас там сидит человек и постоянно эти коэффициенты что ли меняет нет а тут наоборот мы вообще заметили что как раз-таки Чем меньше указываем задач на каждую ноду тем лучше а это как раз вышло Вот из этого а то есть было указано изначально 6.000 когда первая нода взяла 6.000 и остальные ничего не делали поэтому мы там 200 потом даже до 100 опускали в какой-то момент Но нет у нас нету мы не применяем как раз автоматически автоматическую балансировку задач Ну потому что так выгоднее но такая возможность есть да вот второй доклад я слушаю из Яндекса каждый раз это какое-то очередное колесо Почему всё пишете зано что есть треки которые больше за объясняют нам какие-то стандартные кейсы Да но если вы работаете в Яндексе Я думаю что вам не избежать каких-то уникальных задач на которые просто не подойдут какие-то стандартные решения поэтому Ну и во-первых с другой стороны это же намного интереснее написать своё колесо чем просто использовать стандартное решение Хорошо У кого ещ есть вопросы Я вижу довольно много рук да Да здравствуйте Спасибо за доклад А скажите со смежники чтобы их не долбить с пул на пуш Не хотите переходить перейти А что ещё раз просить спл на пуш то есть чтобы они там через вебхуки к вам отправляли вообще хотим А И на самом деле у нас заведён а тикет в котором уже полгода идёт как раз выяснение от отношений о том что давайте переделывать вот эту архитектуру но тут проблема в том что вот э вот переделать архитектуру вот такой штуки вообще больно поэтому вообще очень хотим Да основной RS вообще даёт вот эта вот штука то есть она вот так вот крутится сама собой А Но вообще ой следующий вопрос направо отдал чтобы мы Чтобы ты немножко походил размял А да у меня буквально простенький вопрос как раз по поводу этого слайда а Task update Time А чему равен И как вы его меняете По какому принципу и как это время вообще получили а вообще у нас а пять конфигов пять статусов А это ну вот как я говорил стоит машина из пяти статусов шесть конфигов есть дефолтное время апдейта она так и называется Time и для каждого статуса там sech Ну если для для какого-то статуса мы не можем найти значение конфига мы берём дефолт как мы это время получали По большому сч мы выставляли несколько минут две смотрели что железо живёт если железо жило то уменьшали если становилось плохо увеличивали и в зависимости от некоторых статусов на самом деле есть смысл вообще убирать время апдейта например когда водитель ждёт вас он приехал сказал Я на месте нам уже не нужно изменять State Life Activity Ну потому что ничего уже не произойдёт и в этом случае мы вообще не циклим то есть там стоит Значение но сути опытным пум смотря на графики и на метрики Понятно спасибо У нас есть вопросы теперь слева пока мы передаём микрофон я хотел бы напомнить что у нас есть QR код и попросить оценить доклад для нас это очень важно фидбек особенно конструктивный всегда приветствуется Спасибо за доклад Меня зовут Михаил меня очень интересует вопрос по артам Как вы контролируете их корректность при изменении И вообще скажем так актуальное состояние и есть ли какой-то мониторинг за мониторингом но алерты у нас вообще есть стандартные алерты Когда у нас сервис Раскаты Ну вот только Новый сервис мы создаём Да там через тоже тикеты у нас вешаются стандартные мониторинги Они во-первых каждый мониторинг за каждой отдельно ручко сервиса мониторинг за логами если логи выбрасывают какие-то ошибки или предупреждения мониторинги за какими-то фоновыми процессами что касается клиента на клиенте собираются данные Нет я уточнял про то как вы контролируете что мониторинг не просто мониторинг именно алерты не сломались Да важна составляющая А ну в том смысле что сами мониторинги живы Ну чтоле мониторинги гко визуально сто контроли тяжело визуально Есть ли что-то а Ну вообще У нас там много мониторингов начинается всё с того что приходит Арт в Telegram если мы ловим алер в наших внутренних мониторинга приходит Арт в Telegram если на него никто не реагирует то начинает звонить бот не А если алер не пришёл блин Если честно я даже Пока не могу ответить потому что у меня ни разу не было такого что Арт не приходил Обычно он Арт начинает звонить если не может дозвониться до тебя он звонит следующему и так пока не задол бит всю команду А если не смог дозвониться до команде то звонит директору А директор звонит уже всей команде подоб истори делали в фане Да но вопрос как раз Как контролировать по поводу По поводу контроля у нас был первый доклад про точке отказа там было чуть больше информации по поводу инцидент-менеджмента Я не знаю были ли вы на Первом докладе но советую тоже его глянуть У нас есть вопрос вот дальше чуть выше Третий третий ряд всё Виктор спасибо большое за доклад было очень интересно Владимир а Яндекс такси м заказ это такая большая система и вы показали как вот наращивали этот функционал вот Конкретно этот слайд мне интересно это ну с организационной точки зрения это делала отдельная команда потому что выглядит как большой Объём работ сервис и работ с базой данных и так далее То есть там был ли отдельный продакт-менеджер и так далее или же это реализовало как просто дополнительная фича к данному сервису Спасибо А с точки зрения организации у нас приходит менеджер и говорит вот такую штуку хотим приносит её в команду и назначается один человек ну мы называем его там тех ли дом он начинает писать а грубо говоря документацию сразу техническую там какую-то ну более менеджер и после этого у нас много людей заходит Смотрят на эту документацию начинают её согласовывать Если кому-то не нравится он может написать что мне не нравится Давайте вот так вот подправить А ну в целом Весь процесс вот так устроен А как раз-таки человек который отвечает за разработку он пишет всё то есть архитектуру используя конечно же уже какие-то существующие сервисы То есть он пишет Я планирую взять там вот эту штуку например и её заем вот для реализации потом призываются смежники и допустим смежники говорит что мне не нравится то что мы используем вот это вы начинаете там спорить Да кто Ну как бы пока все смежники не согласуют вашу архитектуру то есть процесс вот так устроен Спасибо ещё есть вопрос от человек рядом со звуко оператором и потом мы вернёмся в эту сть зала Здравствуйте СБО поте пожалуста Если у человека два допустим Айфона он заказывает на одном как это отображается на другом и если заказ оформляется допустим на другого пользователя такая возможность в Яндек такси тоже есть там секция фейлы у нас была но вообще да изначально мы этот момент не подумали потом начали о нём думать а когда у вас запускается телефон Ну вот ваше приложение стартует мы пытаемся действительно сейчас восстановить активити если он А был допустим не вами сделан Аа или сделан условно с другого телефона Да а и Да в момент создания мы сходим А вот здесь вот и посмотрим есть ли уже какой-то Live активити если он есть мы вам попробуем его восстановить на вашем втором телефоне то есть у эпла по токену проблем никаких не будет если он создаётся с другого устройства А там работает принцип подписки а то есть я говорю что вот такой-то такой-то девайс и вот к нему Ещё токен ещё девайс Вот вот ещё девайс и Вот токен спасибо спасибо Пока мы перемещаемся в эту часть зала У меня есть вопрос один из онлайна человек спрашивает почему не не решили не использовать очереди А вместо записей в монго и сервиса какие-то стандартные MQ решения как бы тут правильно сказать Ну во-первых потому что в принципе вот этот механизм он чем-то похож на очереди Да но он не является механизмом очередей Почему не использовали Ну наверное потому что это у нас какой-то принятый стандарт уже внутри конкретно го использовать вот эту нашу штуку которую мы разработали А ну тут действительно не тестировали просто эффективно работает вас это устраивает как работает и в принципе во-первых там у нас очень много своих уже мониторингов которые следят за каждой вообще каждым движением этой штуки плюс Это довольно легко отлаживать плюс Ну это действительно проверенный временем У нас механизм который мы на бенчмарках сравнивали с какими-то системами очередей уже существующими они ну не проигрывали им Ну значит исследования какие-то в какой-то момент времени были хорошо спасибо тогда вопрос Спасибо за доклад такой вопрос а научились ли вы падать условно та чтото устаревшее А вообще началось с того что или учились в процессе чему-то мы что-то мы предусмотрели чему-то мы учились в процессе А чему-то так и не научились пытаемся научиться сейчас вообще когда мы инициализирует вот прямо вот на этом моменте мы уже ставим сейчас задачу на 6 часов вперёд которая говорит что вот она через 6 часов запустится и посмотрит Если активити ещё живой она его просто прихлопнут А И то не все иногда может не отработать Но вообще да У нас был момент что активити просто зависает то есть вроде бы как будто бы всё работает но она не работает мы когда мы всё искали Вот это почему не работает оказалось что мы обрабатываем сменив как раз не все их ответы и вот как вот в процессе то есть увидели что там долго долго деба увидели вот эту ошибку Мы ещё не отловили А давайте там мы обработаем что-то с ней сделаем то есть базовый сценарий того как падать вроде бы предусмотрели сначала какие-то граничные случаи нет и учились этому в процессе Спасибо вопрос посередине право секци Ага да спасибо за доклад Алексей ты оди хотел бы спросить а альтернатива редис рассматривалась какая-нибудь или сразу выбрали всё Ну вообще У нас создаётся тикет когда вот нужно изменить прямо архитектуру сильно тикет называется архревю ну архитектурное ревью и как раз-таки в в это архревю я залетел с предложением редиса других не рассматривал Ну как бы залетел говорю вот есть редис быстрая штука нам подходит И вот именно в этом моменте в а RW проблем не было То есть все такие Да круто поставили свои овые мы такие Да Алексей А что бы предложили вместо Диса не Ну просто интересно Просто интересно Нет я просто мне тоже интересно есть ли какой-то Ещё вариант Потому что сейчас kdb там вот эти все истории Окей Ну и ещё вопрос У вас СН машин используете для для жизненного цикла что использовалось сприн стоит машин для жизненного цикла заказа имеете в виду ну да На чём у вас движок вот эта вот штука как раз процессинг она а именно база имеете в виду а монго а монго всё спасибо есть у нас ещё вопросы есть вопрос первый ряд вот здесь Здравствуйте спасибо за доклад У меня вопрос Следующий вот на основании той истории который вы рассказали при выводе этой фичи на пром вот я не услышал что использовалось где-то в этой истории нагрузочное тестирование то есть Есть ли у вас стенды используются ли потому что большинство проблем которые вы описали Вот мне кажется бы они вскрылись бы сначала бы на этом стенде Да и соответственно вы бы их порешали там скажем не отлавливать их там на 20% пользователей Ну и например вот эта тема с бесконечным реше Дулин это как бы выглядит очень крайне рисковой историе изначально спасибо а нагрузочного тестирования мы как такового М проводим железо А ну которая на тот момент нас не выявила чего-то такого вообще у нас принцип Ну как бы обычно мы как делаем мы раскатываем Постепенно У нас каждая фича выезжает под экспериментом эксперимент Может там катиться на команду на одного человека на пять Ну различные условия и Да и кстати Ну проблема с нагрузка она оказалась в тот момент когда мы именно уменьшили шилинг время апдейта вот этого самого дунга ну то есть под наручным тестированием поднима понимаем эксперименты как раз эксперимент на команду на компанию на город там на страну А А насчёт того что идея с Риши Дулин выглядит рискованный м ну да да но на тот момент как-то ничего лучше наверное в голову не пришло А да и сейчас не очень-то приходит Ну просто когда у нас собственно говоря запущен стенд там минимум на 8 часов например да и ну симулировать отказ с одного из сервисов за счёт чего например нотификации не будут у вас отправляться да у вас накопится соответственно огромные очереди которые там либо упадут фоновые процесс которые их обрабатывают либо база данных закончится в которых они хранятся и так далее и так далее ну то есть как бы история правильная на самом деле Хороший вопрос вообще можем поговорить об этом прямо я я предлагаю Да перейти со следующими вопросами перейти в кулуары для обсуждение но для начала нам нужно выбрать Два лучших вопроса вопросов было очень много есть ли у тебя кандидаты Да если у кого-то есть возможность поднять руки чтобы визуально просто Виктор мог вспомнить кто задавал вопросы Сделайте это ещё раз пожалуйста вот Ну вообще вот последний вопрос как раз про нагрузочное тестирования так хорошо один подарок у нас отправляется сюда и ещё один У нас есть Подарок Ещё один вопрос м м Ну да наверное вопрос так к кто кому вот а вот мужчина уходит А мужчина Всё я понял Отлично подарки почти все раздали у нас есть ещё э подарок для нашего спира спикера Виктор спасибо большое ещё раз Аплодисменты спасибо все Кто пришёл Да спасибо N"
}