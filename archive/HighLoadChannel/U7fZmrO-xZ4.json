{
  "video_id": "U7fZmrO-xZ4",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "В общем Меня зовут Рома А я пришёл в компанию Яндекс в 2021 году э и одним из первых внедряли весный до обучения больших языковых моделей а а сегодня я старший разработчик в команде Яндекс gpt и занимаюсь задачами ускорения Ирен А собственно сегодня хочется обсудить наше внедрение нашей этой модельки Яндекс gpt со стороны как раз оптимизации ресурсов думаю все или может быть почти все из вас когда-то слышали что современные большие языковые модели и построение продукта на них в реальном продакшене это Очень дорогое удовольствие вот думаю большинство из вас понимает как устроен этот процесс Ну вообще несколько стадий это Трей большой языковой модели а затем нам нужно взять умную обученную нашу модельку и дооб учить её на Реальную продуктовую задачу После чего выделить под это дело сколько-то карточек и зарезать в прод даже в таком наивно плане уже можно посчитать а примерные косты значит первый этап самый дорогой и сложный - это пре добже большой языковой модели Вот и тут нам понадобится прямо много то есть примерно 1.000 видеокарт где-то на месяц придётся поставить считаться для обучения какой-нибудь миллиардный модели на следующем этапе мы берём нашу умную эту языковую модельку Да и её файюм самый простой способ до обучения модели он стоит естественно сильно дешевле нам понадобится Примерно 100 видеокарт может быть на 1-2 дня но тут проблема заключается в том что в бой компании таких параллельных внедрений может быть очень много десяток два и в каждом из них мы активно проверяем гипотезы что может вылиться в сотни параллельных обучений и оно всё вот так взрывается и с этим что-то нужно делать затраты становится большие ну и наконец на релизе стоит отметить две вещи мы берём наши карточки сколько это карточек и по сути хороним хороним в прость уход и держит какой-то теоретически неограниченный сверху поток который возможно потребуется ещ сильнее увеличивать наши ресурсы добавлять туда ещё видеокарт Ну как бы и вообще в целом эти ресурсы Нужно оптимизировать потому что это очень дорого Ну и также да стоит отметить что после релиза мы дожать какой-то ланси наш UI он должен быть отзывчивым а а наши модельки они большие сложно вычислительные а требуют много ресурсов с этим надо что-то делать а Итого в реальности план э А вот оно всё я понял хорошо Итого в реальности наш реальный план а по э созданию продукта он дополняется тремя важными подпунктами Первый из них - это оптимизация скорости обучения в нашем трейне на втором этапе нам нужно делать какой-то эффективный Файн тюнинг и желательно тоже тут какие-то ресурсы сэкономить а в конце когда мы будем рези перед этим ускорить наш иренс естественно вычисления в трейне они очень тяжёлые мы все это прекрасно понимаем и все эльки они могут разбираться в наших блоках из которых состоит нерока и оптимизировать каждый из них по отдельности это можно делать архитектурно опти вычисление в каждой из видеокарт и собственно мы это делаем вот так примерно выглядит типичный профиль нашего обучения Он такой страшный и каждый блок как бы можно оптимизировать как я уже сказал в рамках архитектуры либо совет вычислений но Давайте посмотрим на кое е элу это коммуникации коммуникации между видеокартами они работают сообща и внутри них как бы происходит обмен информации между картами и эта операция занимает довольно значительную часть Во время исполнения нашего обучения и довольно часто там возникают проблемы в чём собственно заключается эта проблема иногда эти коммуникации зависают случаются рестар изза этого чисто в теории такие реста они могут ть по огромному ряду причин например отказы в железе отказы в сети между хостами какие-то софтов проблемы вот однажды мы обнаружили что такие зависания они возникают детерминировано и приводят к рестарта м то есть что происходило на фиксированной какой-то модельке достаточно большого размера это важно через какой-то фиксированный промежуток времени детерминированный у нас случа рестарт и зависание буквально итерация итера как показано на слайде вот оказалось что это происходит вот в фреймворке реворк распределенных коммуникаций и такие зависания они приводили нас к огромным поте потерям в вычислениях порядка 10 тире 30% нашего обучения уходило просто на то чтобы рестартнуть ничего не сделать такие большие потери Они вообще возникают того во врем должно много чего произойти на слайде примерно показано что происходит все эти операции они не быстрые они сложные и требует какого-то времени с этой проблемой сталкивались не только мы например великие Могучие ужасные Open тоже они когда-то репортинг сложно но что тут важно понимать есть небольшой нюанс этот фреймворк довольно сложный но разрабатывает примерно один человек 80% коммитов в нём э сделаны вот этим товарищем А это проблема маленький бас Фактор в общем мы решили что эту ситуацию отпускать нельзя 10 тире 30% времени обучения это прямо много да и начали копать вообще как бы то как мы нашли и локализовали эту проблему это тема в целом отдельного доклада на час Я думаю у нас такого хронометража нет сейчас поэтому мы как это немножко пропустим этот этап и как бы скажем что много дней спустя мы всё-таки эту проблему локализовали и где же она была она на самом деле находится в коде ля который отвечает за коммуникации между нашими хостами по проколу это такой очень быст уровень связанности между машинками а для смотрящих в записи сейчас есть отличный момент Остановить запись и поискать проблему на слайде код тут достаточно простой А ну и остальным тоже есть какое-то время м но сейчас я покажу собственно В чём она заключалась а красными строчками подсвеченный счётчик Вот такая детская болячка приводила к огромным к огромным затратам которые очень сложно найти и мы её починили вот Пир собственно мы уже замёрзли он вле проблема у нас больше не воспроизводилось отрад видеть что это ещё возникало только на больших обучения Когда вы максимально волнуетесь по этому поводу вам нужно прямо сейчас что-то обучить у вас значит какие-то зависания происходят допустим какой-то трейн у нас всё-таки получилось обучить с этим дальше нужно что-то делать до обучать его то есть на наши конкретные продуктовые применения и как я уже раньше говорил в компании может быть огромное количество применений разных огромное количество планов на новый запуск нового продукта все они тестируют гипотезы перебирают что-то обучают В общем активно жгут железу и всё оно в итоге выглядит как на слайде общем сложно непонятно дорого в общем не очень для такой ситуации нам бы отлично помогли какие-нибудь другие новые методы до обучения эффективные лёгкие там которые не тратят не жгут железо под конкретные продуктовые задачи Да тут стоит отметить важный момент не всегда эти эффективные лёгкие до обучения о которых я буду рассказывать Дальше они действительно сохраняют качество бы Серебряной пули нет fune без него иногда вот совсем никак одни из самых популярных этих эффективных методов до обучения то есть параметры Fish and тюн сокращённо пеф это тюнинг и lora наверное кто-то из них про них слышал Давайте вкратце расскажу идею э в тюнин мы заменяем нашу руками скрафчу подводку которую мы обычно отдаём там чат gpt на обучаемые вектора и учим их на маленькой выборке А в Лора соответственно мы э дооб Буча не вот эту подводку а дооб Буча маленькие низкорамный перемножения вот оба эти метода у нас Отлично себя показывают довольно часто сохраняют всё необходимое качество но при этом тратят ресурсов в десятки или в сотни раз меньше подробнее про эти методы можно почитать по вот этим ссылочка если кому-то интересно сейчас есть небольшой момент просканировать вот даю секунду ч Хорошо давайте идти дальше а приятной особенностью этих ФТ методов является возможность их хостинга как бы нескольких сервисов нескольких бета запусков на одной и той же айпишник байты их можно отправлять в прямо вот вместе с запросом ходить по сети соответственно у нас модель на литу будет переключаться с одной задачи на другую хорошо Таким образом мы решили проблему с запуском наших бета тестов как бы но тут есть одно большое ограничение железа у нас больше не стало как мы видим и большой поток в такой Коммунальной опиш мы тянуть не в состоянии Вот Но для тестирования вполне Сойдёт Во время этого тестирования обычно мы огромный Круг гипотез сужаем до какого-то небольшого до каких-то плановых запусков и выглядит Оно обычно всё Примерно вот так у нас есть план какой-то таймлайны и в этих запусках уже совершенно другие требования там будет гораздо больший поток нужно давать более строгие гарантии нашим пользователям в плане скорости и вообще потенциальная нагрузка и вычислительные ресурсы там сильно выше тут как бы нам наша Коммунальная пишка как я уже сказал совершенно не подходит нужно прямо по-честному ускорять иренс то есть время работы наших диктовку они у нас всегда ограничены и для чего это сделано для того чтобы просто у наших сервисов сходила экономика невозможно тратить столько много ресурсов а соответственно как мы оцениваем сколько нам вообще нужно видео карточек при релизе в Production Ну для этого есть стандартные метрики наверное Многие из них знают многие из вас знают их это rps И latency latency мы обычно Мерием а либо на запрос либо на токен потому что сейчас уже очень популярна а стриминг технология когда мы видим как моделька токен за токеном генерирует ответ А обычно у нас в сервисе есть какое-то жёсткое ограничение сверху на латен и мы стараемся При этом при этом условии наш rps максимизировать вот что тут ещё важно отметить на графике Есть конкретные как бы данные для пользовательских запросов rps Они прилетают очень неравномерно есть много разных сезонное И пиковый rps он может очень сильно отличаться от среднего про это тоже стоит помнить с другой стороны когда вот мы что-то начинаем оценивать А у нас всегда есть жёсткое ограничение на количество видеокартой просто не может предоставить Нам всё железо мира а для больших запусков их может просто не хватать когда поток большой Ну и в Маленьких тоже ресурсы Нужно экономить а потому что систематически это делая как бы затраты сэкономленные прямо ощутимы Ну что давайте на каком-нибудь примерке разберёмся с собственно в чём проблема что за дела допустим у нас существует какой-то сервис у которого примерная нагрузка следующая 2.000 запросов прилетает в секунду и тенси необходимый который вообще нормально держать там это 3 секунды Ну допустим ещё 20% из этого потока большого мы можем зашивать таким образом 1600 запросов нам нужно держать вот прямо по-честному в Реал тайме которые Ну по-честному в Пике ещё могут вырасти до 10000 дальше мы берём нашу модельку и нашу видюшка конкретно замеряем вот сколько одна видеокарта может держать оказывается что вот 6 rps при максимальном как бы нашем лансе который мы можем себе позволить 3 секунды О'кей м Значит на весь сервис нам понадобится сколько Ну 300 видеокарт всё вроде Понятно К сожалению тут возникает проблема компания нам может доставить только 200 В общем ресурсы надо как-то оптимизировать модель ускорять с этим нам как раз поможет справиться ускорение Ире его методы Их существует пре великое множество Ну давайте с базовых самое основное Наверное - это дистилляция знаний в такой базовой постановке задачи что у нас имеется у нас имеется огромная очень умная модель обычно мы называем её учитель она очень долгая мы не можем её себе позволить выкатить Например размер может быть 3 как у ламы а также у нас есть модель студента она сильно глупее уж тут греха Но работает зато быстро мы можем её себе позволить прямо сейчас прямо сегодня покати и вот основная постановка в данной задаче - это перенести наши знания из умного учителя в студента это можно делать по-разному Ну вот какая-то формул есть на слайде там сравниваются предсказание просто модели Окей самый как-то свежий метод который мы используем в нашей компании - это crl дистилляция как бы на пальцах что там происходит у нас есть какие-то примеры на которых мы хотим обучаться входы для моделей мы берём нашего учителя и начинаем генерировать для каждого примера несколько ответов из Учителя они по идее должны быть верными После чего мы берём наши ответы и начинаем их сортировать сортировать с помою какой-то модели про не позже выбра самый лучший вот этот наш ответ мы берём его и на этих парах начинаем дооб Буча В тупую нашим студентам вот собственно весь этот метод тут как бы очень важно напомнить что Откуда берётся есть так называемая саба Метрика что в ней происходит вот у нас есть какой-то запрос и есть два ответа от разных моделей возможно и мы стараемся их сравнить и решить Кто выиграл итоговый скор модели по такой метрике это бы процент выигрышей по какой-то выборке то есть там модель выиграла в 62 про случаях одна а другая там в тридцати восьми Я думаю Понятно именно на вот эту выборку когда мы сравниваем два каких-то ответа обучается наша Вар модель и помогает выбрать нам лучший с прошлого слайда А хорошо собственно мы применили всё это дело страшное к на самом деле нет к реальной задаче к задаче генерации наших быстрых ответов в поиске в честном Реал тайме в данной задаче очень важно помимо метрики S by S не врать то есть быть фактологический Вот и замеряем какую-то метрику схожести между нашим ответом и исходным источником В общем в этой задаче две метрики подтверждён какие у нас В итоге получились результа в этой задаче у нас была какая-то очень хорошая миллиардная модель и мы её захотели сливать нашим методом в полутора миллиард и у нас это получилось вот Итого как бы ускорение при переходе из т в полтора получилось примерно 12 по раз это очень много при этом с чем тут Важно сравниться по качеству на самом деле не с большой моделью Это не наша цель нужно Сравни слайм который мы можем выкатить бы без ции это миллиардная модель и вот если с ним сравниваться то меньшая модель вот эта заливая которая в 2 с поно раза меньше полутора миллиардная она с большим отрывом его выигрывает получается 61 на 39 вот что очень круто хорошо вроде с Деля как-то разобрались другой подход который поможет нам ускорять иренс - это квантизация значит основная идея какая тут у нас есть у нас есть модель большая и она делается вычисление в каком-то типе правильно обычно это fp16 вот Давайте попробуем сжать её веса или может быть ещё какие-то там активации в этой сети в тип с меньшей битность например в int 4 или int8 а формул для этого собственно есть на слайде а они посередине Давайте по ним как бы быстренько разберёмся чтобы было как это понятней Потому что те кто не знаком наверняка тут как этот В общем выглядит странно э Значит у нас есть на входе какой-то X это первая формула которая верхня имы его хотим переместить в какой-то диапазон от Ну до 2 стеми это вот эти как раз наши возможные значения для этого делим на округляем получаем и сдвигаем на какой-то затем делаем клинг в наш который можем себе позволить естественно в такой операции происходит большая потеря информации где она заключена как раз в этом округления наша модель в этот момент не отупела вот мы это замеряем на бенчмарках Естественно для такой операции есть обратная Ну почти обратная она не может восстановить X каким он был до этого Но примерно его восстанавливает нам нужно просто умножить Точнее сначала сдвинуть а потом умножить всё понятно Я думаю вот в чём выигрыш у такой операции мы можем наши вычисления проводить прям реально по-честному в инх и потом как-то кванти естественно псов у видеокарты в инх сильно больше там прямо в разы таким образом наши матричные перемножения из которых состоит Вся сеть они сильно ускоряются А при этом качество как оказывается сохраняется на том же уровне примерно вообще квантова можно только веса в модели А можно и веса и активации это влияет на то как мы как-то в реальности будем производить наши вычисления Ну а в это не стоит сейчас супер глубоко погружаться давайте перем к чему-то интересному Давай придумаем Вот и тому как мы применили нашу модельку вообще это диалоговый ассистент в котором можно пообщаться на произвольную тему я просто напоминаю что это И квантизация у нас даёт здесь примерно следующие результаты сравнивались Мы в этой задаче по метрике СБС опять же вот чей Ответ Лучше то и победи снить оригинальную би то Получается примерно следующий счёт Да значимая просадка в качестве небольшая есть 5 52 на 48 Это всё-таки победа оригинальной модели Но небольшая скажем так на тоненького Вот и собственно наш эти потери нам показались абсолютно приемлемыми и вот почему всего нам получилось ускорить модель в два расть утива моде в два раза лучше а на графике изображены соответственно сейчас вспомню медиана средняя и девяносто пятый девяносто девятый квантили и настолько же то есть примерно в два раза у нас получилось сократить наши требуемые ресурсы то есть мы просто порезали количество карточек в два раза сэкономили кучу денег вот а опять же QR коды для интересующихся тут можно почитать про обзорные статьи тема Очень обширная в неё можно очень долго копать там куча всего интересного Ну вот две статьи которые помогут войти в эту тему Ну и вообще в целом сейчас после доклада Я готов ответить на все ваши вопросы которые возникают и в кулуарах В общем буду рад обсудить Ну и как бы я рассказал вообще не про все методы можно задавать вопросы Что ещё существует там как это можно пробовать сколько это денег приносит и так далее на этом моменте Я уже начинаю потихонечку закругляться и хочу вот подчеркнуть следующий момент что сегодня Мик он богат огромным разнообразием инфраструктурных и оптимизационных задач удивительно на самом деле то насколько близко сейчас распределённые вычисления и машинное обучение сфера ставит для нас с вами то есть разработчиков инф и щиков очень Челлендж тяжёлые инженерные задачи которые полностью скрыты от глаз обычного пользователя который видит простой вот сценарий генеративного там генерации то есть Давай придумаем вот я хочу вам всем сказать спасибо что за этот короткий полёт который мы сейчас совершили мы вместе заглянули на обратную сторону Луны вот если кому-то интересно почитать ещё какие-то мои мысли я оставляю опять же на Telegram канал В общем присоединяйтесь я туда что-то буду писать на этом всем спасибо задавайте Вопросы Спасибо ющий доклад Я всё вспоминал мем про то как из куска провода Как из из куска провода сделать Центральный процессор и запустить на нём большую текстовую модель конечно пока железа надо мы тебя нанимаем давай нам нравится такой поход Давайте переходить к вопросам по существу Добрый день Меня Владимир зовут Спасибо за доклад классно подскажите вот после квантификация когда перешли на инты там объём модели уменьшился как-то то есть для этого теперь не нужно там 100 карт или там четыре условно для на 100 млрд которая было Да конечно Это очень важный вопрос видеопамять прямо по-честному уменьшается как уменьшается и битность то есть там Было 16 стало четыре размер модели в этот момент он как раз в четыре ра режется на видеопамяти то есть надо конкретно по видеопамяти там вообще супер честно прям вот умножаешь и всё то есть Теперь я могу запускать на своих 4 1080 Ну если сильно постараешься то да рабан Спасибо за выступление очень такой своеобразный вопрос все знают Open у неё хорошая модель все дела они двигаются вперёд насколько возможно использовать её как учителя для своей внутренней нейронной сети Ну в общем я не юрист прям супер точно тебе ответить на этот вопрос не могу но как я это вижу как у нас в компании происходит Нея по-хорошему вот кто-то должен прийти и предъявить да возможно не придёт как бы такое возможно но что точно можно делать как бы В Рес целях ты всегда можешь проверить какую-то гипотезу да то есть ты можешь Может дистиллировать её знания куда-то и посмотреть как хорошо оно работает Если полученное качество тебя прямо устраивает и твой сервис прям вот прямо сейчас может зарабатывать деньги на этом Ну ты можешь пройти некоторый процесс ты можешь собрать такие же да по-честному то есть без помощи Open обучить на этом какую-нибудь модельку твою или там оную ну и собственно всё Профит НС Спасибо большое Сергей тарасом спасибо за доклад Ну такой вопрос она на каких данных Вот вы обучали и модель насколько она огромная вот Ну так сказать была сама вот ну исходные данные Да это первый тако второе вот а сравнивали вы свою модель с какими-то другими вот там аналогами Ну точнее она отвечает или хуже Я не знаю там с нашумевших чатом gpt и так далее Я не знаю вот проводили такие исследования конечно да проводили но боюсь я тут не всё могу прямо рассказать потому что информация наверное ин Даш вот короткий ответ да всё проводили всё замеряли И тут нужно задать ещё как бы уточняющий вопрос на самом деле для каждой модели количество пройденных данных количество потраченных ресурсов они очень зависит от её конфига то есть для вот большого гиганта вроде Open и их 175 миллиардные Модели там одни масштабы Но для продакшена на самом деле в реальности который там нужен Яндекс или ещё кому-нибудь ещё можно обойтись сильно меньшими масштабами вот я могу на этот вопрос ответить как бы но меня запрещает компания вот если коротко Ну а хотя бы это да откуда брали из интернета чисто который Вам доступно у вас в поисковике есть или может ещё откуда-то я не знаю там на чём учили свой модель Да конечно Тут на самом деле очень глубокий тоже вопрос у нас этим занимаются целые там команды отделы А если коротко естественно там есть обычный дикий интернет но его нужно чистить а потому что если походить в целом по урла особенно собираем их пауком а там бывают иногда урл на которых одна буква написана и повторена 100.000 раз потому что они защищаются от парсинга мы Чистим данные из дикого интернета Мы добавляем туда руками отобранные источники Возможно там какие-то книги открытые которые можно добавлять не защищённые там копирайта а в общем максимально засовываю туда релевантные источники информации чтобы модель становилась умнее У нас есть Прямо целая панель бенчмарках бенчмарком на которых мы оцениваем качество и для каждого из них какие-то знания типа полезны какие-то нет вот там для химии не нужно считать Достоевского Скорее всего я понял последнее такой А вы моральные нормы какие-то накладывается на свою модель типа ну нельзя ругаться матом или такие выводить ответы вот конечно да вот тут Мы ничем не отличаемся от индустрии у нас много этапное обучение в середине есть как раз вот тот самый алайт как уча gpt RF вот это вот всё когда мы учим модель собственно быть полезной а не ругаться с нами Вот помимо этого в конце у нас всегда есть какой-нибудь классификатор который проверяет что мы сейчас плохого что-то не сказали вот если бы как это рекламирую Давай придумаем можно туда зайти и убедиться что Как развести нашу модель на какое-нибудь оскорбление довольно сложно в контексте последней книжки самой новой книжки Пелевина там русская классика в исходных данных есть Достоевский Там должна быть Ну я считаю что должна быть как бы точно проверить сейчас тебе не смогу за секунду но должна быть привествую Спасибо большое что расширили моё личное понимание того как можно модели сжимать вот очень интересно но у вас был слайд такой интересный Как вы внедряет модели в разных сервисах и мой вопрос о том а как вы принимаете решение о том что именно в этих сервисах эти сжатые модели будут эффективно работать то есть как вы вообще начинаете сам процесс и говорите да Это тго хороший вопрос на самом ще индустрия сейчас в таком состоянии когда Ну всё бурлит огромный мы стараемся внедрить как бы искуственный интеллект вот это вот всё и там существуют вполне понятные задачи которые Ну как бы экономически если погрузиться в в бизнес-сервис вот понятно что тут есть проблема она оценивается там какой-то там может быть рынок там не знаю миллиарды не миллиарды но наверное миллионы и мы как бы понимаем что эту задачу можно решить строится Прототип на наших моделях мы смотрим на его качество если оно в принципе удовлетворяет клиента то всё эту модель можем катить неважно В каком варианте То есть если потом её получится оптимизировать Вообще круто типа ещё и как бы у нас улучшилась экономика в нашем сервисе но по умолчанию мы принимаем решение только тогда когда пользователи доволен нашей технологией Вот то есть не бояться и пробовать Да ну так всегда Да спасибо было очень интересно Можно вопрос такой популистский входящее и исходящее токенов ско для каждого конкретного применения это очень разные числа Вот например в поиске там есть какой-то источник информации и там могут быть тысячи токенов то есть мы сначала прочитали большую страничку с текстом Ну там часть книжки может быть а потом ответили на короткий вопрос соответственно токенов у нас там ну 100-200 ну иногда вот например как Давай придумаем Может быть там тоже 1000 токенов когда нас пользователь попросил придумать там сочинение за него написать мы такие Ну ладно напишем Ну это Верхняя цифра тысячи правильно нет И второй вопрос опиш Когда откроете вообще сейчас насколько я знаю в клауде уже что-то можно попробовать вот в Яндекс клауде смыс Спасибо я вижу там ещё вот вопрос есть микрофон Да Большое спасибо за было очень интересно у меня такой вопрос наверное с какой-то стороны обучать на больших данных легко потому что в индустрии есть очень много способов вот а что делать если я хочу обучить на маленьком количестве данных То есть я хочу чтобы моя модель умело отвечать на вопросы про какую-то конкретную документацию какой-то конкретной библиотеки как такой задачки поступаться да Отличный вопрос А вообще я чуть-чуть про него говорил там где-то Лано Ладно ээ вот параметры efficient fing он частично для этого если тебе нужно до обучиться например имитировать какой-то стиль например сухо Отвечать по документации ты можешь взять какое-то небольшое количество примеров и на них до оббурдон не нужно там терабайты данных хватает буквально Ну 100.000 может быть максимум 10.000 примеров Вот то есть для конкретного твоего сценария это подходит Но помимо этого вот прошлый докладчик который тут был он рассказывал про R retal augmented Generation А это новые подходы это новый Челлендж когда мы к нашей модельке помимо её внутреннего знания которое она выучила на трейне или там на до обучении а пытаемся добавить ещё что-то то есть какую-то книжку в которую она может быстренько сходить например там поиск или просто база вот документации а подсмотреть это эту информацию добавить к себе в inp и на этом уже сгенерировать ответ Вот надеюсь ответил Спасибо Рома давай теперь займёмся интересной задачей нам нужно все вопросы отсортировать по релевантности выбрать самый клёвый вопрос который тебе больше всего понравился мы там подарок подарим их было очень много а давай сделаем сэмплирование и выберем самые лучшие из выборки которую ты запомнил да Я так и делаю сейчас ты что-нибудь новое услышал знаешь типа ага надо над этим подумать на работе на самом деле Хороший вопрос был про длинны входов и выходов потому что я про это явно не говорил а это очень сильно влияет вообще на все процессы отлично А кто этот вопрос задавал вот тогда от партнёра Газпром Нефть вам подарок а Спасибо Рома тебе тоже подарок от конференции круто Давайте поблагодарим Рому Спасибо было приятно что-то рассказать L"
}