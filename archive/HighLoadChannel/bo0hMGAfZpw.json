{
  "video_id": "bo0hMGAfZpw",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Всем привет Ирония состоит в том что два остальных претендента были ребята из моей команды почему-то победил руководитель вот совпадение не думаю на самом деле это реально была Случайность Видимо просто был интересен более такой верхнеуровневый взгляд о чём мы поговорим сегодня обс платформе Альфабанка централизованной большой А мы глубоко верим что она сейчас лучшее на рынке и можем про это аргументированно рассказать немного расскажу о себе Я не депс а дата сайентист и риско Вик 15 лет Я работаю в банках а начинал карьеру райфа кредитбанк основное моё достижение вот в тех западных банках это комплексная модель принятия решения в юникредите Она работает уже больше 10 лет по всем физическим лицам это наверное не очень хорошо Ну в принципе 10 лет модель работать но это так Поэтому если вы когда-либо получали кредит как физическое лицо в юникредите Скорее всего вы получали его по одной из моей моих моделей А дальше я занимался моделированием в казначействе финансовые модели строил в бизнесе Альфабанка это было первый раз когда я пришёл в альфа-банк м в уралсибе отвечал за валидацию корпоративное моделирование внедрение Базель ских стандартов МСФО и так далее в открытии курировал корпоративное моделирование и портфельный анализ вначале потом централизованное моделирование и вот там мы сделали как кажется нам классную MS платформу централизованную и сейчас в альфабанке Я тоже вместе со своими ребятами делаю такую же платформу даже лучше о чём мы поговорим ой собственно Крат нашей инфраструктуры цели и задачи нашего проекта а чего мы достигли совсем кратко А дальше классные фишки которые как кажется делают нас лидерами рынка и которые вы можете переиспользовать наверное а из чего состоит наша инфраструктура у нас централизованная инфраструктура Это довольно сильно отличает нас от большинства конкурентов 80% всех моделей Альфабанка даже уже 85 работает в нашей экосистеме а у нас не только централизованная платформа разработки и применения моделей машинного обучения У нас централизованный dat Lake Hub пока сейчас идёт миграция на новое решение ss3 как это всё работает внешние данные и внутренние данные складываются в dat Lake над дайком есть наш фи Store супермаркет фиче то есть данных для моделей над стоит наша mdp Mod development Plat единая подчеркну единая среда для разработки моделей все дата сайентист Ну там 95% датасаентистом банка работает на этой платформе дальше у нас есть единый пайплайн поставки моделей впд необходимые сред тестирования которые доводят модель до Сима Сим среда исполнения моделей наша среда применения моделей основные модели bch онлайн и немного стриминговых моделей ба модели получают данные из далей и кладут результат в dake онлайн модели внутренние данные также получают через dat Lake через горячее хранилище они подкладывают А вот данные которые приходят с процесса Они получают из онлайн бизнес-приложений И отдают обратно в такие же онлайн бизнес-приложения основные цели нашего проекта Первое - это конечно сокращение Time to maret и Как нам кажется по бенчмарка которые дают нам наши консультанты мы лидеры на рынке по этому показателю в среднем 2-д с по недели занимает внедрение бач и онлайн модели Это полный цикл внедрения модели у наших конкурентов побольше могу потом рассказать насколько и при этом мы сохраняем качество То есть у нас уровень стабильности моделей отказа устойчивости - это там 29 уровень Mission Critical системы А и соответственно мы не можем как некоторые наши конкуренты прямо вбх куча куча денег в инфраструктуру в железке вот поэтому нам актуально экономное расходование железа соответственно основные наши как бы показатели Мы реально масштабировать кратно масштабировать в прошлом году в 10 раз увеличили количество моделей сейчас уже в 15 раз Ну то есть вот как бы сейчас у нас порядка 310 моделей это активные модели всего в альфабанке моделей то есть большая часть в нашей инфраструктуре остальная переезжает скорость вывода модели Мы также кратно ускорили и у нас кратно увеличилось Ну не кратно в два раза увеличилось количество датасаентистом в нашей среде разработки Согласно яко партнёрам это мази мы вот по инвестициям сильно отстаём от конкурентов здесь ещё как бы слинг немного неправильны потому что за инвестициями сбера тяжело УГ развитие инфраструктуры мы вот вроде как в лидерах Да это что означает это Согласно таблице Google Вот анкета Google - это базовые на самом деле принцип C CD которые Ну не знаю вот мы называем не мешать там C CD процессу нормально работать при этом а это достигается за счёт в первую очередь централизации и как нам кажется правильно выстроенных процессов при этом реально лидерство в MS и в инфраструктуре искусственно интеллекта как кажется обеспечивают сейчас важные проекты о которых я поговорю а я буду говорить на примере так называемый стандартной модели так как это площадка в первую очередь для айтишников Я хочу показать пример стандартной модели У нас есть самые разные модели Мы внедряем в нашу инфраструктуру большие языковые модели сложные нейросети и так далее и так далее вот самая простая модель - это какая-нибудь бизнесов модель склонности пример модель склонности юр лица взять и использовать бизнес-карты карточку которую он может платить со своего счёта со своих оборотов а целевая переменная то что он начнёт её активно использовать вот здесь приведены я не буду озвучивать критерии они по и либо или сочетаются а основные работающие переменные Ну либо фичи Да здесь Наверное тоже никакого секрета нету это собственно то насколько компания долго живёт уровень её оборотов тип деятельности кому-то это актуально а кому-то совсем не актуально Ну в общем всё довольно стандартно с точки зрения инфраструктуры что собой представляет процесс обучения и применения такой модели Train sample маленький порядка ца 40 ГБ inference - это бачо история Да иренс витрина порядка гигабайта например на ежедневной основе читаемое Long list факторов маленький это 500 штук самые большие Ну мы LM в сторону отводим Да может быть до 10.000 наверное да здесь это 500 штук а впт уходит в шорли 14 штучек да А И вот само количество строк это собственно компании широкого рынка компании среднего бизнеса по сути наверное плюс-минус все там за счёт за исключением каких-то фильтров порядка 400 штук вот ну и Таргет Рей очень маленький да то есть вот класс который м предсказываем меньше процента потому что Ну понятное дело всего рынка часть клиентов являются клиентами Альфабанка и за заданный период они активируют карту Вот пример стандартной модели держите её в голове когда я буду рассказывать об каких-то инструментах А первое о чём я хочу рассказать - это инструменты АВ ML комплексные инструменты HTML а как нам кажется Сейчас мы здесь тоже задаём тренд А какие предпосылки были для работы и создания таких инструментов у нас кратно выросло покрытие а бизнес-процессов моделями кратно как я уже говорил выросло само количество моделей и у нас есть отдельное подразделение которое нам предоставляет новые данные оно работает очень хорошо и у нас большой выбор э новых источников с которыми мы можем работать Какая возникает проблема для всего этого не хватает датасаентистом модели протух ет любая модель со временем протух ет да то есть у неё снижается разделяющая способность по сравнению с А тем какая она была на старте её нужно мониторить переобуть и внедрять перев недр на это требуется ресурс дата сайентиста Аа на тестирование источников также требуется ресурс датасаентистом и так далее соответственно вот такой кратный рост моделей если ничего не делать не автоматизировать приводит Ну по сути к кратному росту датасаентистом что очень дорого и вряд ли пройдёт без снижения качества потому что ресурс очень дефицитный Извините что людей называю ресурсом наши конкуренты это понимают и тоже занимаются автоматизацией а весь а ML можно разделить на четыре основных инструмента Первое - это бейслайн автообучение модели ну здесь каждый дата сайентист уважающий себя имеет свой собственный инструмент белай какие-то свои сохранённые процедуры а методы и так далее да поэтому это есть у всех основных наших конкурентов такое институциализация второе Ай вот что здесь важно сказать что у централизованная история из вот представленных компаний есть у нас и у ВТБ вот как проводил анализ Яков и партнёры Да по всем остальным компаниям они смотрели если вот у них в одном из решений Да есть подобное решение то они ставили галочку Вот соответственно ВТБ как централизованная инфраструктура ещё не сделала мы сделали у остальных конкурентов это где-то есть авто кажется это очень важная история Да есть у нас только я сейчас про него подробнее расскажу и у Яндекса Вот в одном из проектов было бы интересно узнать в каком и автоматизированное тестирование фиче это то к чему все идут у Яндекса это опять же есть Молодцы А давайте посмотрим на жизненные цикл модели с точки зрения потенциала автоматизации Первое - это постановка задачи ну здесь как бы хочешь не хочешь наверное не автоматизирует следу это подготовка данных понятное дело есть промышленный фича Store в котором есть Ну там дай Бог все фичи и встаёт как раз вопрос подключения новых источников данных соответственно если у нас есть по каждому направлению тридер модели Ну модель которая как бы показывает в принципе олицетворяет собой класс моделей и мы на этой модели можем протестировать качество нового источника например нам дали данные о поведении клиента в интернете либо о том как часто он заказывает пиццу и если эти данные улучшают предсказательная способность модели и Мы это можем проверить автоматизировано Это значительно упрощает процесс тестирования внешних источников потому что обычно тебе сэмпл дают бесплатно Ну то есть вот как бы попроб улучшает либо не улучшает он твои модели А дальше тебе нужно пройти довольно большую дорожку покупки обоснование финансовой эффективности этого источника его закупки и промышленного подключения автоматизируем упрощаем следующий шаг как я говорил это собственно разработка модели baseline сейчас у нас уже внедрено это по сути вот сейчас обязательная история что дата сайентист перед разработкой любой стандартной подчёркивая стандартной модели должны прогнать её через белай это является бенчмарком в случае если м данные модели показатели её качества автоматически разработаны устраивают Ну D sst может дальше модель не разрабатывать вот если всё-таки ему это надо Ну в большинстве случаев это так потому что всё-таки а даже там 2-3 про рока аука играют и это сотни миллионов рублей То он идёт в собственную разработку также А инструмент белай используется валидации как инструмент альтернативного моделирования А вдруг просто нажав кнопочку получится построить модель лучше чем сделал её дата сайентист и соответственно а retrain модель внедрила её мониторят качество ухудшается что АВ делает он автоматически производит мониторинг модели в случае если он видит деградацию статистически значимо ниже трешхолд инициализирует разработку модели сверяет лучше ли первая версия модели но новая версия старой и если лучше автоматически заменяет это уже внедрено это промышленный инструмент вот мы его внедрили в мае и активно масштабируемой поставлено на автообучение ну суммарно У нас их 300 вот и к концу года 80% бач моделей должно стоять на авто переобучение помимо экономии ресурсов дата сайентиста есть ещё и прямой бизнес эффект от автотрейда В чём он состоит обычный ручной цикл переработки модели состоит в том что вот год она живёт как-то если не произошло ничего экстраординарного потом её переобучить и внедрили Вот соответственно она деградирует если её переобуть регулярно Ну в принципе автоматический инструмент можно хоть переобуть Ну вот здесь мы протестировали это реальные данные на примере как раз похожей модели склонности как я показывал четыре раза в год каждый квартал её переобулись должен быть Target Store то есть э хранилище целевых значений а которые модель предсказывает которая должен должна наполняться это важный очень реквизит вот у нас должен быть качественный fure Store автоматически обновляемой само собой У нас должна быть Промышленная система мониторинга вот у нас её пока нету вот поэтому мы весь мониторинг размещаем в наших пайплайн чуть подробнее Расскажу позже соответственно у нас есть специальный пайплайн Для внедрения модели и автоматической постановки её на автопечек я рассказал да то есть автоматически монитори и переобувается в случае необходимости таким образом А где-то К августу Мы хотим реализовать полный цикл автоматизации жизненного цикла модели Извините за тавтологию можно модель автоматически обучить автоматически внедрить Ну белай поставить на автообучение и она сама будет переобуться и перев недр и это является пролог собственно к cen дантисту когда мы прикручиваем сверху всего этого добра UI и отдаём вовне не датасаентистом для того чтобы там дата аналитики сами разрабатывали модели история Спорная и очень такая А дискуссионная в части того насколько это может быть эффективно Да люди не занимающиеся дата сансом строящие модели вот это как раз нам предстоит узнать в начале двадцать пятого года немного про технологическую сторону соответственно у нас есть три пайплайн первый пайплайн Ирен в который монтированная это давы airflow проводит вычисления А в случае если метрики упали он дёргает й пайплайн й pipeline прямо в продин То есть он вот не проходит все эти процессы внедрения по pipeline тестирования и так далее которые у нас как я говорил занимают 2 недели прямо в проде обучает модель и считает метрики Дальше он дёргает третий pipeline Quality Gate pipeline Quality Gate pipeline сравнивает результаты старой модели новые модели сравнивает их на стат значимость если лучше новая модель это ожидаемое поведение что модель будет лучше он автоматически перев недр ет модель старую версию архивируемых и выводящее у нас модель внедряет дата S при помощи наших ML инженеров и ставит её на авт всё и так собственно можно делать хоть Каждый день а что нужно чтобы это всё было соответственно как я сказал F Store Target Store вот мониторинг Вот сейчас как бы мы не дождались единой системы мониторинга и реализовали в каждой модели в каждом палай модели собственный Мониторинг это хорошо потому что мы промышленный инструмент стратегически Это не очень хорошо Почему когда у нас будут модели тысячи а до 1.000 моделей Мы точно дойдём у нас точно будет потребность поменять частоту мониторинга какой-то группы моделей Ну например вот все модели склонности их там 100 штук почаще мониторить сейчас это централизованно сделать невозможно В идеале Это должен быть дашборд ты кнопку нажал и все модели Встали на другой порядок мониторинга сейчас нужно будет залезать в каждый пайплайн и а менять частоту мониторинга моделей пачев модель саму это не очень хорошо вот и очень важная История - это как раз пересборка образа модели непосредственно в проде вот а на основе базового образа чуть подробнее расскажу попозже второй инструмент для бач моделей - это инструменты патентного вывода моделей уж вот начали говорить про модели склонности их очень много А реально их порядка 100-150 штук у нас ну то есть что это такое Вот мы оцениваем склонность какого-то клиента взять либо взять например либо если это физик взять ипотеку потреб соответственно если все эти модели внедрять нашими пайплайн было сделать инструмент чтобы модель можно было внедрить разом Вот соответственно как модели внедряются Согласно вот нашему инструменту у нас выделяется пнг модели понг моде и вот формируется такой блок моде дальше эти блоки можно друг с другом соединять соч и строить по сути любые ансамбли это всё реализовано в pyon СК выглядит это вот следующим образом соответственно прописывается блочно модели элементы вот узлы эти блоки можно исключать из моделе по сути строить Ну ансамбли любой конфигурации что мы получаем что мы можем потно быстро внедрять модели мы можем делать разные архитектуры моделей Ну практически любые вот э и Это значительно нам помогает около 40% всех моделей которые мы внедрили мы внедрили каскадами они внедряются очень быстро среднее время одной модели внедрения полдня Ну то есть у нас в среднем 2 недели а здесь полдня при учёте там прохождения всего тестирования и так далее это как бы мёд был да Теперь давайте поговорим про ложку дёгтя вот ну то есть это реально Мы очень этим гордимся и это действительно большой Шаг вперёд а какие есть нюансы нюансы есть первый очередь с airflow airflow - это отраслевой стандарт все его используют Какие минусы вот Первое - это большие проблемы с ролевой моделью airflow Active directory нормально прикрутить можно но очень тяжело и у нас сейчас нету нормальной изоляции дагов Ну то есть грубо говоря у нас человек из одного хаба Может поменять расписание Дага другого хаба моделирования само собой Это неправильно во-вторых вот сделав такую супер-пупер каскадную модель Ну это библиотека Да А мы столкнулись с тем что её очень тяжело поддерживать Она реально неповоротливые не все её понимают как она работает вот и она работает медленно Вот и у нас есть большие проблемы с отказоустойчивость всей этой истории напомню у нас инфраструктура максимально Промышленная поэтому отказоустойчивость у нас очень болезненна что мы хотим сделать вот сейчас конкретно наши тех Лиды прорабатывают и гмт эту историю уже пилоти на самом деле пилотируемые комьюнити простота относительная Да вот Наверное в принципе и всё AR workflow позволяет конфигурировать Ну да да Через Я файлы что значительно проще и позволяет уйти от этой огромной библиотеке вот позволяет проще ставить на шедулер UI кто-то говорит что лучше Work кто-то для это небо принципиальная ирия Сосем работают Инне очень важна га workflow Она работает отлично можно строить супер сложные конфигурации моделей warg workflow очень важная История - это собственно работа с деплоймент на предыдущую версию на три версии назад и так далее Потому что сейчас это делается с большой болью вот а это довольно частая история когда надо что-то пошло не так особенно в пилотных историях и нужно откатиться назад там другой старой версии модели и конечно Ну вот наверное последнее оно самое главное - это вопрос отказа устойчивости у нас папки логов и собственно список дагов хранятся на pvc это Long Horn Но это единое сетевое хранилище соответственно если какой-то процесс его забивает у нас вся структура дагов весь Дулин может рухнуть Вот это очень плохо да и соответственно при дальнейшем масштабировании при увеличении количества дагов несёт очень большие риски Вот пример библиотеки это кусочек Вот она действительно большая Вот в общем выглядит страшно а обещал рассказать про то как мы деплой шаг в упрощении и пути к масштабированию системы А у нас есть соответственно й дальше есть четыре контура де контур на котором поднимается inference модели среда для нагрузочного тестирования и функцио среда для интеграционного тестирования и собственно прод Раньше у нас модель собиралась целиковы образом дата сайентист фигачит мог был быть образ Ирен модели средние 15 всё равно это очень много это приводило к переполнения артефакто Да проблема с артефакто тоже есть потому что сейчас проблемы с поддержкой В общем всё понятно не очень хорошо она тоже оказывает ребятами из Индии это приводило к зависаний В общем было действительно много жалоб что мы сделали мы выделили базовые образы базовые образы привязаны к версии Mi Python Почему мы пользуемся разными версиями потому что мы активно используем dataframe Spark и так получается что именно Python вот последней поддерживаемой версии по сути 3.8 лучше всего работает со спарм вот поэтому у нас есть несколько версий Пана Вот это гки образы они весят порядка 2 ГБ мы собираем модель и саму модель в виде пикл файла её окружения архивируемых Иви это хранилище Да в соответственно образ вот тем самым мы обеспечиваем пересборка про каскады оффлайн моделей что по онлайн моделям собственно та же самая проблематика нам потребовалось реализовать инструмент для многоуровневой архитектуры онлайн моделей но решали мы чуть другую проблему в случае офлайна нам в первую очередь нужно было обеспечить большое промышленное количество моделей быстро их вывести в случае онлайн истории нам нужно именно обеспечить вот многомодульная это важно а в рисках Да в банковских рисках в принципе является стандартом для онлайн модели принятия решения модель которая в моменте определяет давать вам кредит либо не давать а строить несколько моделей в зависимости от источников данных обычно используется большое количество внешних источников это данные там внешние кредитные истории либо данные арбитражей Ну в общем вот все данные которые ты о клиенте можешь из вне собрать и для каждого источника строится своя модель для чего это делается в случае если источник станет Плохо работать либо выключиться чтобы можно было просто модуль убрать а всю структуру модели сохранить не надо было переобуть модели потому что Понятное дело что можно все фичи в одну модель засунуть и обучить но Тогда придётся Каждый раз при изменении источника пере обучать весь ансамбль В общем это является рыночной практикой и вот есть у нас такая многомодульная модель у неё там 16 источников это модули первого уровня они выдают вероятность дефолта потом есть модули второго уровня которые уже как бы считают финальную вероятность дефолту которые модель и должна предсказывать дефолт - это собственно не платёж по обязательствам соответственно а нужно было нам сделать такой пайплайн как мы его реализовали а это набор микросервисов вот в кубернетес А пресловутые мы делаем через А в привязки ккку управляются и дипка настраиваются эти каскады мы их можем делать любой конфигурации и любую архитектуру моделе задавать через Python sdk Фаста соответственно обеспечивает доступ к metastore хранит Мета информация ну здесь всё понятно да Вот как раз эти лёгкие образы живут в артефакто скрипты сами в бит батике gent отвечает за сборку вот основное Да как микросервисы обмениваются друг с другом обмениваются Они через бротер сообщений Ну и Ключевое хранили мы выбирали между редисом и rit MQ вообще как бы стандартно нужно использовать rit MQ Вот но он более тяжёлый более специализированный и он актуален для больших сообщений у нас микросервисы вот эти мини модельки обмениваются маленькими сообщениями Ну факторы которые они посчитали результат работы модели плюс дис у нас в принципе используется в нашей экосистеме для высоконагруженных моделей вот поэтому мы остановились на редисе в принципе не пожалели S отвечает за многопоточность асинхронность и работу в фоновом режиме вот классная библиотека вот нашим техдом очень нравится Вот соответственно каждый Каскад можно дёргать по апе и все артефакты сохраняются в виде логов в S3 а соответственно что мы получили Да возможность дипка настраивать каскады и разделять их по средам это очень важно потому что нам нужно чтобы в каждой среде у нас было одно и тоже вот изоляция каскадов которое мы в оффлайновый истории не смогли добиться Вот и ролевая модели что нужно нам сделать дальше сделать так чтобы во-первых эти каскады можно было прогонять в цикли это актуально как раз для оценки вот этих корпоративных моделей потому что бывают Группы компаний и ты должен по каждой компании сделать расч вероятности невозврата кредита находя Ну это просто вот нужно кастомизировать jone сделать так чтобы модель обладала памятью это тоже специфичность банковского кредитного процесса когда один кредит в случае ручного процесса такой тоже есть может 50 расчётов прогнать и выбрать тридцать пятый расчёт по компании Да нужно чтобы всё это сохранялось И бизнес нас просит добавить соответственно jpc подключение нам кажется что это честно говоря излишней текущим уровнем Вот и как бы формата более чем достаточно И последнее что я хочу рассказать это про нашу среду разработки моделей часто в компаниях есть отдельные команды разработчиков которые сидят на отдельном железе более того Каждый сайентист хочет чтобы у него было выделенное железо чтобы он ни с тем не конкурировать но в принципе вот по тем замерам которые мы делали это действительно приводит к перерасходу к простава железу И шарить железо лучше Да у нас вся среда разработки отказа устойчиво живёт в mdp в единой среде разработке но возникают некоторые проблемы при вот таком шаринг ресурсов это что главное - это обеспечение дата сайентиста необходимыми ресурсами при том Мы помним что модели у нас абсолютно разного профиля нагрузки и сложности разрабатываются есть как вот реально простые модели пример которые я приводил так и сложные свёрточные нейросети вот сейчас там модель думаем до обучать Ну там всё-таки лучше Наверно протива Но тем не менее В общем задачи самые разные А что мы сделали Мы пересобрать спаунер стандартный спаунер Питер хаба он завязан на профиле то есть есть несколько профилей например Малый объём ресурсов средний и большой чем такая история плоха например Малый средний объём профиля 50 ГБ оперативки А Большой 100 ГБ оперативки дата сайентист нужно 60 Ну не то не сё он должен выбирать большой профиль Ну чтобы поместиться в объём и соответственно там 40 ГБ простаивает мы себе такое позволить не можем нас финансы контролируют вот довольно крепко с точки зрения закупки железа поэтому мы доработали спаунер Юпитера и эн чтобы можно было ползунками выбрать Ну точное значение о выбираешь точное значение следующее что мы сделали это сделали Слава видеокарт Ну видеокарты мы используем стандартные Тесла 180 ГБ Вот в общем делим их пополам можно их по-разному порезать Ну вот это как бы опросили всех наиболее удобный объём по 40 ГБ вс-таки активно работаем с мми поэтому вроде как достаточно мое Кост видем У нас есть как бы стандартный объём модели который должен покрывать 80% всех необходимых ресурсов на разработку это 16 виртуальных цпу А и 200 ГБ оперативки если нужно больше то мы отправляем пользователя на выделенную ноду эта нода тоже в рамках общего кластера mdp она живёт в общем кубернетес но она приорита для каждого из хабов моделирования всего у нас их пять разных хабов вот а там нету никаких ограничений на объём Вот в общем можешь использовать всю память и есть ролевая модель а также дата сайентист доступен мониторинг ресурсов в фане Вот то есть он может посмотреть сейчас Сколько занимает ресурсов каждый хаб насколько загружена каждая нода а также смотреть сколько насколько загружены выделенные ноды А И кто собственно их грузит вот ну там если это речь про выделенную но ноду а не про общагу он может прийти к своему коллеге и сказать друг ты занимаешь всю выделенную ноду у меня задача важнее пойдём к тем леду Пусть он решит кто должен пользоваться этой нодом что мы сделали вот как бы день ноль Да что у нас было Питер Hub статичные сессии а и вот эти большие образы неповоротливые которые приводили к большому количеству зависаний реально при выстраивании поддержки это наверное было топ о жалоб у нас зависает Юпитер Сделайте что-нибудь это как раз было из-за больших тяжёлых образов образы облегчили зависать перестало тем не менее некоторые Проблемы остались какие это проблемы проблема состоит в том что сейчас в случае если мы хотим поменять какие-то настройки например добавить новую версию пайтона либо поменять правила распределения видеокарт слай наме более мелко и хотим это внедрить в спаунер нам нужно полностью перегружать UB сейчас у нас где-то 150 ПС а на самом деле уже по 200 пользователей и ты никогда не найдёшь техническое окно чтобы не нарушить бизнес-процесс это реально у нас проблема И мы за 2 недели стараемся согласовать как это можно решить Вот мы сейчас видим два основных пути первый путь - это пойти пум сбера и пойти в глубокую сохранением сессий когда по сути после обновления Можно резко эти сессии которые не были разорваны восстановить вторая история которую мы видим более перспективной это в принципе написать свой с нуля самописный uui прикрутить его к ингресс и перейти на ow То есть в принципе уйти от хаба предоставляет по сути мощный очень инструментарий для моде Вот и соответственно управлять поднятием сессии выделения им ресурса непосредственно при коннектив к конгресс кажется эта история перспективная вот сейчас мы только начинаем на неё смотреть наверное ответ как бы действительно пойдёт Эта история либо нет мы дадим ближе к концу года собственно на этом доклад закончен Подписывайтесь на наш канал ADV это канал нашего централизованного моделирования ВМ рассказываем про все наши успехи и неудачи и прикольные события митапы и так далее вот вакансии размещаем в общем много чего интересного Если хотите связаться со мной Вот QR код моего Telegram канала и оценивайте доклад Всем большое спасибо готов ответить на вопросы Напоминаю что вопросы можно задать через попасть через QR код на экране там вопросы задаются быстрее и более гарантированно но начнём мы с зала Давайте молодой человек в проходе на четвёртом ряду Да спасибо большое за доклад захари ИРР Ну отдельную боль Конечно я про ролевую модель услышал сам занимаясь разработкой инструментов безопасности больших данных актуально интересен первый вопрос как происходит до оббурдон непосредственно на кластер дупа и там обучается либо разделены слои compute and storage и как-то вливаются данные и ещё момент как контролируется в литие данных То есть если модель относительно публичная то её же нельзя учить на Ну как минимум данные должны быть обезличенные либо должны быть какие-то данные не конфиденциальные как обеспечивается этот процесс Угу Да спасибо за вопрос вот такой банковский вопрос А смотри на первый вопрос отвечаю у нас не происходит обучение модели на нода дупа у нас происходит в нашем кластере кубернетес на дупе у нас происходит только подготовка данных то есть компьют фича инжиниринга вот а поэтому всё происходит в нашем кластере в случае переобучения автотрейда мы действительно переобуть модель в проде вот чтобы сократить цикл поставки это мы согласовали с нашими никами вот ну то есть по сути потому что у нас есть два кластера это продоете на котором модели функционирует и а кластер для обучения - это по сути standby прода с репликации данных Т -1 вот поэтому Ну в общем что мы на Т -1 обучаем что на текущих данных разницы никакой нет и дкб Ники нам это согласовали в части масти Нея данных у нас мосру основные данные необходимые Вот ну как в общем pers Data Да ну честно говоря она практически не входит как фичи в модели иногда она фигурирует как ключи Но это вот уровня там ну а реально такой перс дата Да вот поэтому сейчас больших проблем при моделировании с Марова ней не возникает потому что у нас всё соответственно внутри Банка в облака мы ничего не выносим вот если мы пойдём в облака это большая история которой сейчас мы думаем Вот она перспективная с точки зрения там бесконечных возможностей масштабирования сути Вот но здесь большой вопрос С марони всего Вот и Это основное как бы препятствие ну у нас есть инструмент если интересно и нет не в клауде пока не в клауде И второй вопрос Вот как раз по поводу обеспечения ролевой модели изоляции дагов пробовали ли вы какие-то готовые инструменты для обеспечения ролевой модели использовать например apch Ranger или как-то кастомизировать то есть там допустим Полинки не умеют но можно доработать то есть да вот сейчас как раз Ну вот мы сейчас мы выкатили вот этот инструмент потому что нам нужно было Ну обеспечить скорость бизнеса выкатить всё да сейчас задумались как раз об изоляции то есть сначала сделали потом как бы стали думать над ролевой моделью вот а как раз сейчас мы рассматриваем два инструмента Вот честно говоря не скажу каких это технические лидеры прорабатывают а но они приводят к ещё большему утяжеления вот этой библиотеки потому что они должны вызывать за деплой где-то инструмент он должен там А по апе отдавать какой-то ответ В общем это какая-то вот ещё Костыль над костылём Да поэтому мы в принципе хотим с этого всего уйти через а workflow большую ставку на это делаем всё большое спасибо спасибо Следующий вопрос у нас из чата Смирнов Dan be Devices Что является триггером Для начало переобучения моделе и насколько должна просе Метрика для начала процесса Ну как бы на общий вопрос общий ответ вот да что снижение должно быть значимо на самом деле зависит от модели потому что ну модели у нас много а для А ну поясню для модели рисковой Да например принятия решения риско Ну это батч модели предов например да просаживается зависит конкретно от статистической значимости там той или иной модели которая определяется потоком на котором она применяется Ну в общем на общий вопрос общий ответ к сожалению Спасибо вопрос из зала второй ряд Спасибо большое хотел задать такой вопрос есть ли у вас модели в которых применяются ручные асессоры и Если есть то как это встроить в пайплайн вот в автоматический это метка у нас статьей модели есть собственно У нас есть большое подразделение разметки А хороший вопрос мы сейчас к этому как раз хотим прийти То есть как у нас это работает у нас отдельно есть разметчики вот которые генерируют нам сэмплы Вот соответственно Дальше они есть специальное по в котором эта разметка проводится потом это всё выплёвывает в хадуп Ну а из хадуп уже забирается при процессе обучения а соответственно процесс до обучения он должен строиться с приоритетными задачами на процессоров Ну то есть ты должен быстро что-то разметить так чтобы Ну не компрометировать весь процесс переобучения если это будет слишком долго то мы вот потеряем всю ту экономию времени которую получаем с точки зрения автотрейда пока мы вот это следующий шаг как раз перевод сложных моделей на автотрейн мы не уверены что это вообще пойдёт и получится ну потому что если мы не сможем управлять бэклогом ручных асессоров эффективно А сейчас мы в начале вообще пути и они размечают всё вот в основном работа с текстами сейчас уже пош пошли изображения и так далее то мы может быть в принципе не будем это использовать в автоперевод А привет Спасибо большое за доклад очень интересно я Эльвира Дета сайентист ВКонтакте очень заинтересовал момент про выделение железа Мне вот интересно А как много времени проходит между тем когда сайентист нажимает на кнопку и ему выделяется ресурс И как вы следите за тем что дата сайентист не выделяют ресурсов больше чем им на самом деле нужно А ну да Давай отвечу выделяется мгновенно вот Собственно как бы спаунер делает это мгновенно при поднятии пода вот а не выделяется больше чем ему нужно смотри Ну как это это делается путём статистики вот я там показывал слайд мониторинг был а мониторинг был А да вот слайд с мониторингом он же как бы работает в две стороны да то есть это слайд ну этот мониторинг который онлайн доступен для дата сайентиста чтобы он видел нагрузку также у нас мониторинг сохраняется в логах Да он правда вот сейчас он был доступен 7 дней сейчас мы Пытаемся до полугода расширить окно и дальше собирать статистику То есть если Дантист регулярно заказывает ресурсы я не знаю на пол терабайта а использовать 2 терабайта но просто приходить к нему к тем Лиду Ну это не приходить это регулярные рассылки с пере потреблением То есть это делается административными методами пока мы не научились Как делать по-другому это автоматически или это человек Смотри сейчас вот сейчас что делается сейчас раз в неделю сотрудники нашей поддержки поднимают лоди Почему раз в неделю потому что потом они исчезнут и смотрит кто не утилизировали И если это происходит часто они пишут Чувак это ты что-то неправильно оцениваешь вот мы хотим это сделать автоматически Для начала нужно расширить окно чтобы мы могли собирать статистику Ну неделя не показатель поняла Спасибо Спасибо Давайте вот там у нас микрофон Спасибо за доклад вопрос тоже по про переобучение моделе когда они деградируют А что если эта модель в Каскаде участвует ведь не гарантия что мы улучшим модельку где-то посередине и весь последующий Каскад не посыпаться и сюда же вопрос а с фичами вы ну в деградации только модели обвиняете и их переобуть или фичи ретрейдинг аэ в части Каскада Вот как раз сейчас в процессе мы технологически прикручиваем а возможность переобучения каскадов да то есть мы в это верим абсолютно а на самом деле по Каскада у нас замеряют метрики а общей производительности Каскада и производительности в разрезе а а каждого модуля а действительно у нас может случится Так что переобучение одного модуля может не повлиять либо повлиять даже там в обратную сторону из-за корреляции в на производительность всего Каскада Вот мы сейчас хотим накопить статистику потому что в принципе кажется что всё равно Это скорее чёрный лебедь Ну то есть грубо говоря если тако если у нас а для любой модели автопечек результат что переобучение новая модель хуже либо не значимо лучше старой версии всё равно это идёт на ручной разбор вот мы посовещались Дато с сайентиста вроде сейчас кажется что вот этот СН алон переобучение оно валидно если оно не будет валидно если мы увидим очень много будет идти на ручной разбор но мы буквально Скоро это увидим да Тогда мы начнём переобуть весь Каскад сразу просто здесь мы исключительно экономим вычислительные ресурсы потому что моделей много переобуть их хотят часто и тупо будут жать очень много ресурса в части переобучения фичей Смотрите это вот прямо вы п Пять шагов вперёд сделали очень интересная идея сейчас как мы хотим делать сейчас мы переобулись на Лонг листе на Лонг листе Мы переобуйся хотим начать переобуть на всём домене фиче Да ну то есть у нас вот есть H your L у него есть свой домен и попытаться туда впихнуть вообще все фичи А какой есть основной риск здесь что модель начнёт эволюционировать неожиданным образом то есть грубо говоря та модель которая была отваливалась через 15 этапов переобучения то может быть вообще другая модель и возможно у неё стабильность потеряется а а вот дальше вы говорите а вы давайте не только вот все фичи вставлять А ещё и фичи Ну если я правильно понял логику менять это очень интересно но это прямо через шаг Поэтому вот пока мы хотим поиграться с расширением А списка фичей Хотя идея очень интересная Спасибо за неё Спасибо за интересный вопрос есть у нас уже микрофон у кого-то в зале Давайте Здраствуйте Меня зовут Николай Никитин Я из ИТМО сам занимаюсь исследованиями в области hml в частности в рамках осного фреймворка Т поэтому тема очень интересная Вопрос такой вообще hml обычно порождает очень сложную постановку задачи оптимизации то есть там поиск структуры ансамбля параметров и так далее вот используете ли вы какие-то примы для именно оптимизации Там суррогатную оценку качества моделей какие-то приёмы Мета оптимизации особенно интересно думаете ли сторону использования больших языковых моделей для такой Мета оптимизации Потому что сейчас про это очень много работ Спасибо Угу Да спасибо ну как бы быстрый ответ Пока нет да потому что сейчас мы пользуемся по сути базовыми библиотеками для обучения и переобучения и ну инфраструктур их внедряем Да насколько я знаю как раз Альфабанка Альфабанк смо активно взаимодействует вот через наших датасаентистом и возможно на дата сайентист нам принесут что-то более сложное и интересное Да пока мы внедряем базу Вот Но вот в части именно а больших языковых моделей Да ну их и их использования для автопечек процесса жизненного цикла сейчас мы смотрим в сторону разметки Да чтобы соответственно м модели делали разметку А наши ручные асессоры сверять с этим вот а ещё ну как бы так сейчас больше за инфраструктуру отвечаю да вот инфраструктурная мке мы используем например сейчас играемся с тем чтобы разворачивать например образы в кубернетес вот да пока наверное а и автоматизация кот ревю и автотестов то есть Э вот на C review кстати прямо Co review кода модели Я имею в виду это обязательный этап в нашем пайплайн мы используем ламу вот прямо хорошо идёт вот сейчас у нас Пилот уже практически закончился и а Лама сейчас является советчиком ML инженера в будущем мы планируем для класса моделей определённых вот простых как раз в принципе отказаться от ручного Корею Поэтому в каких-то точечка мы вот это используем Да но с точки зрения внедрения сложных оптимизационных алгоритмов Бей ну либо в А пока нет друзья если в этой части зала вопросы да давайте первый ряд Здравствуйте тоже вот внедряя модели тоже как бы есть какой-то опыт вот интересно такой то есть если всё автоматически делается с помощью а ML Я только не понял какой именно а ML используется а кто-то отвечает что модель Ну вот после переобучения она пошла там хуже стала ещё работать вот ответственность она наком потому что вроде как всё автоматизировано всем пофиг грубо говоря там что-то работает и кто должен поднимать панику да давайте расскажу вкратце В принципе на ком ответственность Давайте вот в простом процессе Для начала да у нас вот много готов про это говорить модели внедряют дата сайентист Мне кажется это абсолютно правильно это ломка парадигмы некоторая для дата сайентиста Но это вот приводит к тем ускорения о которых я говорю и отсутствует вот эта Пятая нога в виде ML инженеров которые там Ну классически всегда присутствовали при внедрении модели А у нас ответственность за внедрение модели внесёт дата сайентист в части кода модели в обычном жизни Да в ручном процессе ML инженеры псы псы Ну то есть мы отвечаем за то чтобы максимально просто дата Санти смог модель внедрить и показатели скорости внедрения модели и стабильности - это мои tpi непосредственно мои моей команды эсеров там дата инженер отвечают за свой кусок в части данных эта конструкция работает хорошо теперь отвечаю на ваш вопрос про автоматизированное решение А за постановку модели всё равно отвечает дата Sci потому что все библиотеки открыты более того библиотеки а ML у нас модульные Вот вы там просили Какие конкретно библиотеки используются Да сейчас врать не хочу потому что их несколько их несколько и они вставляются в пайплайн более того каждый хаб может использовать свою библиотеку которая прошла наш rvw в этом и смысл то есть сама логика переобучения задаётся поставкой дата сайентиста мы как инфраструктура предоставляем пайплайн чтобы он мог свои методы применять автоматизировано вот поэтому ответ на друзья Большое спасибо вопросы те которые не успели задать можно обсудить после доклада в дискуссионной зоне сейчас Павел нужно выбрать лучший вопрос и обладателя эксклюзивной матрёшки А мне понравился вопрос коллеги из этмо Вот про собственно перспективные методы Вот наверное Большое спасибо за вопросы приз уходит коллеги из ИТМО с также у нас есть подарок для Павла Павел Большое спасибо за доклад L"
}