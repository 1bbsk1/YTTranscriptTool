{
  "video_id": "8jtAUQYcUYI",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Всем привет меня действительно зовут Евгений Я из МТС Digital давайте для начала с вами немного Познакомимся а дальше уже перейдём к самой сути доклада я сетево в центре управления клиентскими данными это Big MTS Digital и имею учёную степень кандидат изит наук я стараюсь делиться своими знаниями своими своим опытом и сделать некоторый вклад в развитие нашего it сообщества поэтому в качестве Доцента ВУЗа прощаю молодое поколение в области it и участвую в разработке курса для даты инженеров также стараюсь принимать участие в различных профильных конференциях в роли спикера и эксперта Теперь давайте к тому что вас сегодня здесь ожидает Для начала я вам расскажу немного контекста про рекламные компании МТСа расскажу про данные и про кейсы которые мы хотим обрабатывать расскажу как нам здесь можем помочь стриминг процессинг и сформулируем целиком задачу которую хотим решить с её требованиями и ограничения дальше под эти требования ограничения мы подберём инструментарий исходя из нашего опыта и я покажу решение с точки зрения архитектуры и применимости тех самых выбранных нами инструментов также покажу некоторые интересные детали реализации нашего решения и Даже покажу характеристики железа на котором всё это дело работает и утилизацию ресурса для вашего понимания Ну и в конце будут выводы которые можем с вами обсудить уже после самого доклада Итак поехали МТС большая компания со своей экосистемой и МТС имеет множество различных источников данных одним из самых важных источников данных для нас является информация который мы получаем а от оборудования расположенного на базовых станциях или проще говоря сотовых вышек вся эта информация которая к нам поступает она поступает от многообразия Флоры и фауны различного оборудования и это многообразие Флоры и фауны этого оборудования порождают множество логических и физических форматов данных которые путешествуют у нас по множеству различных протоколов но в принципе все эти данные можно разделить на несколько категорий или как мы их называем домены это информация по звонкам по эсэмэска классический Клик Stream То есть это поток урловской признаков от базовых станций И нахождение абонентов в ронге тоже В отдельной категории А что можно с этими данными делать эти данные к нам поступают в режиме реального времени и мы можем делать с ними в принципе всё что угодно и такие данные можно и нужно монетизировать и один из кейсов монетизации Таких данных - это э применение рекламных кампаний э все вот эти рекламные компании должны быть максимально эффективными чтобы приносить действительно какую-то выгоду и максимальную э конверсию от этих рекламных компаний можно получить Именно тогда когда мы коммуницирует Ну или проще говоря да с клиентом с нашим э в подходящей для него момент времени то есть тогда когда ему действительно это нужно давайте рассмотрим небольшой пример допустим у нас есть какой-то абонент наш клиент которого от которого мы получаем в режим реального времени э поток данных по геопозиции по урла и на основании этих данных мы понимаем что человек в данный момент ищет себе новый смартфон и знаем даже что в данный момент человек направляется домой и мы знаем его маршрут он ходит практически каждый день с работы до дома и с дома до работы и знаем что по пути исследования его находится какой-то ТЦ в котором есть наш ретейл партнёр и у этого Ритейл партнёра э есть как раз-таки тот самый смартфон причём выгодном приложению мы можем сделать контакт по этой рекламной кампании Но именно в тот Самый подходящий момент когда человек проходит мимо этого ТЦ он знает что вот он уже идёт домой и ничего не стоит зайти там буквально за метров 50 в этот ТЦ Хотя Раньше он этого мог не замечать соответственно получает предложение заходит в ТЦ покупает смартфон по выгодному приложению и счастливой идёт домой факт заключается в том что мы проком с абонентом в Самый подходящий для него момент времени мы быстро среагировали и Нам нужен был Триггер для этой реакции если мы сделали это позже то абонент бы Просто ушёл дальше от этого ТЦ и сделал бы ну сказал бы что нам это не нужно нам это не интересно и вообще забыл бы про предложение кейсов таких может быть много тут просто нужно проявить фантазию это лишь один из примеров Теперь давайте более конкретно про кейсы и проводные данные и что вообще нам нужно сделать в этой задаче нам нужно обработать большой поток данных это более мл событий в секунду или если смотреть на объём то более 1 гиба в секунду данных источником для нас является внешний кластер кавка достаточно большой более 185 топиков и более 20.000 партиции нам нужна будет система триггеров которая Как раз-таки будет срабатывать по поведению пользователя сети и отсылать вот эти самые триггеры на коммуникацию с клиентом и соответственно Нам нужен некий функционал сопоставления входного события на параметр рекламных кампаний также мы понимаем что у нас должен быть один выходной объект Это должен быть Триггер но при этом у нас может быть запущено множество рекламных компаний одновременно и соответственно одно событие может генерировать множество триггеров и соответственно у нас выходной объект - Это Триггер будет сообщением в кавка может быть в формате Триф может быть в формате G всё как захочет продукт потребитель по нефункциональный требованиям что от нас требуется мы считаем что данные которые старше 15 минут для уже являются неактуальными Потому что эти данные уже устарели морально потоковые данные Они вообще скоропорт все мы должны уметь быстро и легко масштабироваться например горизонтально потому что у нас происходит прес данных по определённым особым дням такие как Новый год Праздники там и какие-нибудь чёрные пятницы также нам нужна Надёжность и доступность 24 на 7 напомню у нас поток более там 10 млн событий в секунду и простой даже в несколько минут лишит нас огромного среза данных который не дойдёт до продуктов потребителей и они просто не смогут принять решение о том как коммуницировать с абонентом и это может даже нести какие-то репутационные риски также мы должны иметь очень детальный мониторинг системы Дело в том что потоковые системы они очень капризные и они требуют очень много внимания и расследовать инциденты на таком скорости обработки данных и с таким объёмом это очень трудо затратные задача и очень сложная задача особенно там из разряда Почему вы нам не прислали Триггер или наоборот Почему вы нам прислали этот Триггер и такая одно из важных требований для нас является это максимальная утилизация имеющихся у нас ресурсов железа Дело в том что стриминг процессинг - это на самом-то деле вещь дорогая и хочется получить максимальную выгоду с того что мы сделали и как-то это дело монетизировать и такое как цель тире требование - это вот достигнуть нашей максимума пропускной способности при минимальных затратах на все ресурсы которые у нас есть а Всё бы ничего и эта задача спокойно и классно решается если убрать последние два пункта но э по моему опыту могу сказать что э э без этих двух пунктов всё делается очень просто берём любой берём любой популярный инструмент Framework такие как например Spark streaming flink Можно даже взять ksql там на определённых потоках можно э если хотите там экзотично взять какую-нибудь реактивную модель там э с акторно моделью тоже связаные такие как а или если есть если есть проблемы с лицензией Пек или может быть vertx можно взять самза можно вообще взять всё что угодно всё что вам больше подходит и с точки зрения продукта и с точки зрения ваших компетенций опыта но дело в том что я уже много лет занимаюсь подобными задачами задачами связанными со стриминг процессинговый опыт который я трансформировал в доклады курсы один из докладов был на хайлоу про обзор инструментов для построения стриминговых пайла делал на смарт дате также рассказывал про реактив сиину с применением ако стримов напоите и э Если хотите подробности Конечно же можно посмотреть эти доклады Если э коротко то максимальную утилизацию ресурсов и получить максимальную степень эффективности с учётом того что мы хотим тратить мало ресурсов вот эти фреймворки нам сделать не позволят и если сформулировать несколько кратких тезисов из этих докладов относительно текущей темы то фреймворки несут Нам несколько минусов которые не позволяют их использовать для нашей конкретной задачи во-первых фреймворки в большинстве своём стараются быть универсальными и с точки зрения фреймворков это хорошо они стараются решить задачу широкого спектра то есть попробовать применить этот фреймворк ко многим задачам с разных сторон но это не означает что это будет эффективно то есть любая ваша специфичная задача потребует дополнительных ресурсов чтобы это работало хорошо также фреймворки и инструменты популярные предлагают много избыточности в большинстве своём тот функционал который вы используете вы используете на 30 примерно 40% то есть остальной функционал системы он для вас не нужен для вашей специфичной задачи там поток обработки данных тоже не особо пригождается и это добавляет сложности в эксплуатации такой системы плюс вам потребуется конечно же дополнительные компетенции Либо вы будете нанимать этих людей на рынке которые будут дороже стоить либо взращивать свои компетенции что тоже стоит и времени и денег Ну и конечно же фреймворке имеет свою определённую волатильность то есть есть история где у фреймворков могут появляться некоторые уязвимости которые могут не сразу там чиниться могут появляться отставания от языка программирования которым они написаны то есть и нельзя уже будет применять там какие-то свежие фичи которые бы могли бы для вас стать например Киллер фич могут меняться лицензии Зато комьюнити Ну и ряд других не очень приятных вещей и всё это накладывает определённые ограничения на эти фреймворки счётом всего этого мы просто решили что мы не будем использовать фреймворки и мы просто их вычеркнули из своего списка Э мы посмотрели что у нас есть подругой посмотрели на свой опыт на свои компетенции поняли что нам в принципе достаточно и небольшого набора инструментов и что мы сделали мы взяли просто язык программирования в качестве основного языка основного инструмента разработки движка потока обработки данных это было для нас Java причём Java без каких-либо дополнительных тяжёлых фреймворков по типу спрингар конечно же у нас есть но он есть в не особо нагруженных модулях которые занимаются просто управлением этой системой просто для удобства чтобы писать можно было ошки также нам Понадобился плос город для хранения параметров рекламной компании м также мы использовали достаточно большой кластер Аро Спайка для того чтобы работать в потоке с большими объёмами данных и взяли кафку собственную инсталляцию Ну грубо говоря собственный промежуточный кластер для как в Каче интеграционной шины данных и некоторого промежуточного хранения Ну и некоторый набор инструментов для управления ресурсами И CCD палай Ну и набор инструментов для сборки логов и мониторинга здесь можно со мной подискутировать по поводу выбора некоторых инструментов в том числе и с управления ресурсом с точки зрения там сборки и мониторинга системы Но основной движок у нас на Java ВС остальное уже вторично с инструментами допустим разобрались с требованиями разобрались давайте рассмотрим что нам нужно сделать с данными в потоке нам нужно данные прочитать и я поясню почему это вообще отдельный квадратик и что там на самом деле всё не так уж просто данные нужно нам стилизовать достать из например рифтовые формата это sequence by формат и разложить данные из различных протоколов по их доменам то есть звонки к звонкам СМС к эмм и так далее после чего нужно выполнить сложную обработку да данных То есть это нужно сопоставить входной поток большой входной поток с параметрами рекламных компаний после чего нужно а данные отфильтровать по сегментам абонентов с которым хотим провести коммуникацию это тоже сложная Задачка и при необходимости данные обогатить если в этих данных есть пропуски или нужна полезная информация дополнительная После чего мы данные преобразовывать в формат триггера и по Контакту с продуктом потребителя думаем дупли или нет то есть Нужна ли э ну нуж нужны ли э частые триггеры продукта потребителя Ну и помним что э для нескольких рекламных компаний одно входящее событие может быть может порождать несколько триггеров А значит эти триггеры мы должны разослать всем владельцам этих рекламных компаний немножко Давайте про сегменты Что такое сегмент сегмент - это готовый э Набор э или список номеров который отобран по определённым признакам например это мог быть э среди наших абонентов сотрудники it компании которые например живут в санкт-петербурге и которые посещали последний там месяц сайт SA hl вот их пересечение кругов этих Лера - это готовый список абонентов и таких абонентов может быть много на самом деле и мы могли бы сейчас завести рекламную кампанию где если в текущее время наши абоненты находятся в данный момент времени вот здесь на докладе э то можно подойти например на стен МТСа и получить какой-то интересный мерч но мы такого делать не будем а по пайлан в принципе понятно пайлан не очень большой и в крупную клетку Давайте посмотрим на архитектурные решение которые нам здесь могут помочь и что в первую очередь к нам приходит на ум во-первых это Монолит самое простое быстрое решение с точки зрения VP который можно привести действительно результат и потом его дорабатывать с учётом своего опыта разработки грубо говоря и как бы выглядел на самом деле Монолит в нашей системе задача кавка ту кавка мы выделяем отдельный компонент кон сюра монолите И говорим что коню будет вычитывать данные и раскладывать эти данные по потокам мы пишем на Java соответственно у нас многопоточная модель хорошо работает и соответственно в каждом потоке будет свой па плано обработке данных какие здесь есть проблемы у нас есть два очень сложных этапа обработки данных это сопоставление большого потока данных с параметрами рекламной компании и фильтрации по сегментам в сегменте тоже могут быть много абонентов это могут быть десятки миллионов абонентов и на когда данные приходят от консьюмер вот в этот поток обработки мы здесь можем Тор Для нас это может выступить бас фактором и здесь будет Вот такое бутылочной горош который хотелось бы вот пинцетом грубо говоря расширить добавить всего лишь чуть-чуть ресурсов на конкретный функционал но сделать нам этого не получится мы можем запустить дополнительную реплику с монолитом но мы вместе с этим монолитом зарезервировать скажем так переизбытка то есть у нас будет система в данный момент времени Негру А если мы не добавим этот эту реплику то она будет перегружена и в любой момент времени у нас получается так что система не утилизирует максимально те ресурсы которые ей предоставлены и не получает максимальну пропускную способность Как можно попробовать решить э проблему Можно попробовать можно попробовать распилить части сложные по обработке на отдельные потоки и сделать так называемую Ну более похожее на что-то на вертикальное масштабирование выделить в отдельный поток например process и по сегментам а всё остальное в третий грубо говоря поток но здесь появляется другая проблема по другому оверхед синхронизация между вот этими потоками будет у нас не бесплатно это достаточно дорогостоящая операция плюс у нас появляется большой орх по передаче данных между этими потоками И если бы я этот доклад грубо говоря рассказывал несколько лет назад то я бы ещё сказал что здесь появляется оверхед или издержки по переключению контекста ядра но опытные системные программисты заверили мня что современных процессорах уже такого конечно же не наблюдается если у вас там не огромная туча потоков э в любом случае у нас получается так что Монолит нам не совсем подходит не подходит то вот под каким причинам э у нас Получилось не очень эффективное масштабирование ни горизонтальное ни вертикальное и не то не та утилизация ресурсов который бы мы хотели плюс мы получили неизолированной кода Где в поток обработки данных Для нас это больше хуже чем хорошо то есть в любое внесение в любой этап обработки данных может нести за собой последствия в других этапах обработки данных и прежде чем выводить это всё дело в релиз на нам бы пришлось сделать всевозможные сценарии кейсы нагрузочного тестирования чтобы проверить весь пайплайн а не ухудшило сли этой истории мы не можем выделить маленький кусочек который мы изменили и только его протестить на то что изменилось по производительности также мы э в монолите обнаружили что у нас будет много потерь промежуточных вычислений опять же жирные этап обработки данных закончили свою работу вроде какие-то действия сделали но там при рестар нашего приложения при там возникновении инцидентов мы эти данные потеряем нам придётся заново вы и свки учитывать огромный кусок данных и это э обрабатывать А у нас данные естественным образом начнут устаревать и их уже можно будет просто выкинуть э соответственно можно сделать вывод на самом деле что Монолит то будет работать просто для достижения конкретно наших целей по достижению именно максимальной там эффективности утилизации ресурсов это просто решение не очень подходит другое естественно популярно решение это микросервисы но я очень осторожно всегда подхожу к теме микросервисов это очень скользкая история и могу по опыту сказать что даже в идеальном своём представлении спецификация микросервисов для потока обработки данных она не очень-то и подходит она в принципе вот эта вот спецификация микросервисов не везде подходит И по моему мнению сейчас микросервисы используются во многих системах где они не только там не приносят пользу но и могут даже навредить но я несколько не умоляю там заслуги микросервисной архитектуры действительно есть польза этой архитектуры и её можно правильно и нужно правильно применять и я думаю что вы без труда найдёте доклады о пользе вреде микросервисов даже здесь я думаю и в зале и на площадке есть куча экспертов связанных с микросервисами вы их можете легко найти там поднять в толпе руку сказать что Монолит лучше и вас обязательно найдут чтобы в этом переубедить Ну это всё шутки всё на самом деле хорошо какие есть проблемы с микросервисами во-первых возникает вопрос А что использовать в качестве публичного интерфейса интеграции И сколько этих вообще публичных интерфейс интеграции между микросервисами должны быть это может быть какой-то брокер сообщения это может быть вообще какой-нибудь реактивное А это там не знаю Могут быть очереди в редисе может быть что-то другое что вы там используете само описано написали это вызывает очень много вопросов и может разрастись и зоопарк и дополнительный ресурс на поддержание вот этих вот очередей плюс э возникает вопрос А я вообще нормально декомпозировать всю свою систему на микросервисы Ну то есть насколько сильно нужно декомпозировать насколько вот эта гранулярность мне поможет и это получится сделать только эмпирическим путём А потоковая обработка данных она не очень-то стабильна потоки у нас могут быть как и там по синус ходе ходить а могут очень резко взрываться и здесь очень сложно подобрать нужную грануляции что же в итоге получается у нас получается что от микросервисов мы получаем дополнительные операции практически на каждый сервис их очень много есть дополнительная поддержка очередей на обработку в каждом сервисе это тоже дополнительные ресурсы требует также у нас естественно получается по передаче данных между сервисами То есть у нас 10 млн событий в секунду от одного сервиса пришёл потом к другому 10 мл событий и так далее Потому что не на каждом этапе у нас могут отсека какой-то средст по событиям и конечно же нам дополнительно на каждый сервис потребуется некоторые механизмы надёжности по типу dlq чекпоинтов спотов но здесь тоже придётся искать компромисс между количеством и качеством восстановленных данных и скорости этой работы в итоге получается что нам-то не очень не подходит Ни микросервисы нам нужно какое-то гибридное решение может быть какой-то найти компромисс где мы сможем нивелировать вот этот большой оверхед по микросервиса и по всем их недостаткам но и при этом не укрупнить это всё в одном сервисе попробуем декомпозировать всю весь наш пап обработки данных на отдельные этапы но при этом совместить в некоторых сервисах обработку нивелиру Вот это орх по передаче данных Давайте пойдём по этому по порядку и обсудим каждый вот этап обработки данных что у нас получается я помню у нас источник данных и нам надо из этих данных из этого источника данные Вычитать большой кластер кавка э более там 185 топиков более 20.000 партиции и здесь э важно то что мы не имеем доступа к этим э к модификации этого кластера То есть если у нас есть в топке 10 партиции мы ничего с этим сделать не можем мы не можем увеличить большую там сделать большую степень парализации И на самом деле с чтением данных ивки Казалось бы простая задача Но когда у вас объёмы данных вырастают там в разы и это вот такие объёмы которые у нас то у вас будет на самом деле куча проблем и как при кон сюмин так и при продюсировать парализации у вас это не получается для этого на самом деле есть решение я его нашёл и я об этом рассказываю в отдельном докладе я его тоже покажу есть проблема с постоянными равномерным распределением данных то есть смотрите данные вроде как в топиках распределены равномерно и опять же приведу пример с дею партиями девять из них работает хорошо Но одна из них постоянно копит лак И это для нас тоже проблема мы будем постоянно тормозить и данные состаренные например постоянные ребаланс низкая пропускная способность не подтверждение записи или очень долгое подтверждение записи сообщений на продюсера эти и другие проблемы я э погружался в них очень детально я разбирал их до эксп и э говорил о том какие есть причины возникновения этих проблем и как вообще эти проблемы решать всё это есть в отдельном моём докладе который э рассказывает о поток обработки данных с учётом того что у нас данные именно Big Data А что у нас получается у нас на входе получается большой поток данных и это поток данных ээ по факту сообщения в формате Триф но только читать сообщение мы не можем это Ну не совсем эффективно получается нужно докинути это должно быть ещё сама лизация то есть мы должны из различных протоколов данные собрать и сформировать из них объекты и уже эти объекты распределить по доменам ну и соответственно в качестве после лизации мы должны проверить качество этих данных какую-то делать первичную валидацию например хотя бы что в объекте звонка номер телефона действительно валидный Ну и сонно эти данные уже в какой-то промежуточной топик каки где они будут разделены по по доменам данных в общем смысле получается такой отдельный компонент который вот занимается по факту только чтением данных валидации и транспортировкой до промежуточного топика что нам позволяет сделать Почему нам это удобно потому что мы во-первых можем настроить и затюнить очень тонко вот этот вот компонент на максимальную пропускную способность потому что это самай нагруженный с точки зрения входного потока данных компонент так как мы пишем на Java можем построить например можно настроить многопоточность мы можем пот gc на максимальную пропускную способность и это будет штука работать плюс мы можем это дело отдельно масштабировать а отдельный кластер кавка помогает нам более-менее равномерно данные распределять в зависимости от их бизнес нагрузки ну и соответственно мы можем как угодно масштабироваться добавляя партиции брокеры Топки и так далее мы этот компонент назвали шринкер это такое кодовое слово чтобы нам было легче ориентироваться Итак у нас есть тепер после ри намдо параметры рекламных кампаний и с мачи входной поток на рекламной кампании для этого у нас э есть отдельные инсталляция постгрес и в постгрес по параметру рекламной компании заливают отдельные люди они называются аккаунт менеджеры и они делают это через наш выделенный сервис comp менеджер он написано на Java п Spring здесь никакой нагрузки Нет Всё сделано просто для удобства Что нужно сделать нужно сделать отдельный компонент который мы назвали кой процессор который будет вычитывать входной поток данных и применять ээ к этому входному потоку данных сопоставление с параметра рекламных кампаний и на выходе должны быть смачные сообщения по рекламной кампании которые можно уже роти на наших потребителей я говорил много про параметры рекламной компаний но не говорил что это такое что включает в себя параметр рекламной кампании это может быть набор геоточка причём наборы гео точек может быть десятки там и сотни тысяч геопозиции это может быть список урловска одной компании могут списки абонентов списки номеров и конечно же сегменты в качестве сегментов тоже могут быть многомиллионные аудитории на которые нужно сделать рассылку помимо всего вот этих основных параметров могут быть ещё и дополнительные параметры например для звонков это исходящий или входящий звонок или там успешный неуспешный и так далее В итоге что у нас получается Давайте пример вот по геоданным например нам нужно завести рекламную кампанию по геоданным и у нас например сотни тысяч событий в секунды и только вот поео данные загружаются из шринкер в койн Процессор при этом кой процессор должен Вычитать все параметры рекламных компаний одновременно может быть запущено более сотни рекламных компаний и в каждой этой рекламной компании десятки тысяч гео точек которые надо сопоставить с входными данными После чего мы сгенерить как минимум один Триггер э для рекламной компании задача на самом деле не очень простая и здесь потребовалось множество различных хитростей в виде многоуровневых Шей и нестандартных структур данных далее нам нужно данные отфильтровать по сегментам как эти сегменты у нас появляются опять же есть аккаунт-менеджер он заливает данные в кафку э одна рекламная компания - это один сегмент но и в принципе рекламные компании могут быть и бессетт-кеннеди э потом список уже рекламных компаний которые можно сопоставлять рекламных компаний может быть больше сотни далее У нас есть отдельный компонент который называется Точнее не есть компонент А мы думали как раз-таки его сделать сегмент фильтр которы должен отфильтровать этот поток данных по вот этим вот рекламным компаниям Всё бы ничего но здесь большой оверхед получается то есть если мы Отмотай назад мы получаем что после кампейн ме после кампейн процессора входной поток который нашёл он не сильно-то уменьшился за счёт триггеров он мог даже увеличиться и мы получится от одного компонента в другой будем перекладывать большие объёмы данных поэтому мы решили что в принципе вот эти две функциональности мачин рекламной компании фильтрации под сегментом будет у нас в рамках одного компонента ко процессор просто ему выделим грубо говоря в палане это отдельную историю Ну и дальше Просто смачные сообщения будут отправляться уже в промежуточные топик как промежуточное хранилище в целом получилось несложная в этом плане архитектура далее что нам нужно сделать нам нужно сделать так чтобы мы зародилися рекламных кампаний и Для этого нам потребовался небольшой но такой достаточно ёмкий по функциональности DSL то есть мы хотели бы чтобы мы без изменения кода Могли бы просто в конфигурациях написать свои кастомные функции из разряда там нум вот этот дишни и взять Вот этот дишни который содержит имя вот такой-то рекламной кампании и перенаправить её по маршруту нескольких владельцев этих рекламных компаний ну или в свой там собственный топик прида для дебага и в функционале роутинг мы получаем множество различных триггеров и которые могут дублировать друг друга и чтобы не ддосить грубо говоря продукт потребителей мы можем сделать там дупликации причём это сделать локально как это можно сделать локально и почему это вообще будет работать локально мы все данные отправляем в по партиции и в качестве ключа используем номер абонента А значит консмед вычитывает данные конкретных партиции будет получать всегда все события от одного абонента поэтому мы сделали дупликации не отдельном какой-то хранилище базе данных а прямо в кэше в раме грубо говоря самого роутера в итоге у нас получилась такая скромная архитектура То есть у нас есть приёмник данных shrinker есть выход роутера и есть основная часть кой процессора и это основная нагруженная часть Ну и также дополнительные штуки связанные с управлением рекламной компаний и загрузкой сегментов Теперь давайте посмотрим на некоторые интересные детали реализации этой системы и напомню про компоненты У нас есть шринкер приёмник данных компа процессор роутер кой менеджер и сегмент Loader который загружает все эти сегменты что мы сделали интересного в шринкер Он должен был у нас Дели зовать сообщения проверять на пропуске данных и проверки делать на актуальность данных помимо всего этого мы добавили ещё шринкер функциональность по заполнению пропусков то есть до богаче этого компонента данными более-менее статичными такими справочными Но их очень много это порядка 120 млн объектов соответственно мы загрузили эти данные в aerospike и поняли что наш шринкер который работает на приёме там у него большой поток данных он с этим вроде как справляется но при всплеска данных начинает копить лак то есть Мы заметили во-первых overhead по сети постоянно ходим в AOS Spike и заметили уже рост lency при вычитки данных из внешнего кластера что сделали решили что нам нужен всё-таки Горячий кэш поис следовали несколько структур данных которые нам могли бы подойти и воспользовались chronic Map который отлично работает в раме может реализовать кэш Может быть как персистентный так и не персистентный оказалась для нас хорошая штука Поэтому если не знаете что это такое то советую завести задачку себе в backlog на r&d этой штуке А в кай процессоре тоже есть свои нюансы мы делали в этом компоненте ма компании и фильтрацию по сегментам и здесь у нас возникли некоторые сложности коню который работает в компа процессоре очень сильно нагружен и мы по своему опыту сделали фиксированный тред пол где указали количество потоков по количеству цпу и коню просто равномерно по блокам данные лял по этим потокам и оказалось что у нас с каким-то промежутком времени начинает лаг по данным расти то есть мы начинаем не успевать учитывать данные более того э Мы заметили что у нас появились постоянные ребаланс на консьюмер то есть консьюмер то отпада то обратно возвращались в группу как оказалось данные это мы распределили равномерно но бизнес нагрузка на эти данные и вообще хит попадания э сложная часть этих данных он оказался в сосредоточен грубо говоря в одном потоке и мы это контролировать никак не можем то есть мы всегда пов получаем что один поток загружен сильно А другие потоки не сильно это И нагружены решение которое к нам пришло и которое нам получилось сделать более эффективно мы воспользовались более динамическим распределением данных по потокам воспользовались параллельным стримами for Jo Pool и у нас получилось что данные которые приходят на обработку и которые с точки зрения эвристики сложнее обрабатывать они делились на For таки делились до тех пор пока не получалось более-менее равномерно распределить эту нагрузку А другие большие блоки спокойно обрабатывали в других потоках соответственно у нас получилось более-менее равномерно распределить эту нагрузку и более эффективно по утилизировать те самые ресурсы а помимо всего этого мы смогли потютьков консьюмер и кавка продюсер даже как мы это сделали тоже У меня есть в отдельном докладе там много технических деталей поэтому советую в этом плане посмотреть как результат хочу показать такую картинку я Заранее извиняюсь за качество этой картинки потому что это картинка из рабочего чата но там есть три зоны Красная синяя и Зелёная вот в красном происходит полный Хаус там консьюмер данные вычитывает очень неравномерно то Впадают то выпадают всё очень плохо и в ситий зоне Мы чуть-чуть потютьков консьюмер начинали то выпадать то ещё что-нибудь с ними приходило что-то плохое и дальше мы просто потюленить что мы пишем на Java там есть некоторые нюансы мы Поти рч коллектор и у нас стало всё хорошо данные стали читаться равномерно консьюмер перестали выпадать вообще из консьюмер групп ещё есть отдельная тема с урла урл У нас очень длинные в потоках и поиск под строки в таком длинном урлейка это сложная задача поэтому мы стали искать структуры данных с алгоритмы которые нам в этом могут помочь нам больше всего понравился алгоритм Аха карасика он ищет под строку строки используя структуру данных Бор и строя конечный рованный аппарат такой же алгоритм подобие его используется например в утилите гп роутере всё тоже для нас оказалось достаточно просто Нам нужен был DSL поэтому воспользовались готовым генератором логических парс логических выражений langage recn и для роутера также мы воспользовались кофеином как самый эффективный на наш взгляд структурой для работы с Шом по монито я как я уже говорил стриминг процессинг требует очень много и деталей и хочется его постоянно мониторить поэтому вывели отдельные конечно же метрики на consumer Delay то есть хотим понимать Насколько быстро мы данные вычитывать хотим понимать насколько В общем папланетам как быстро мы мачи компании рекламные на входной поток и на отдельной такой борде Мы мониторим в разрезе каждого из инстан сов утилизацию ресурсов и соответственно настра на это скейлинг ну и советую также мониторить историю равномерного распределения данных по партиции Кафки и в разрезе их объёма также очень важные метрики для нас оказались коню lck в рамках конкретных кон сюмейе подов Ну и очень важная Метрика тоже Для нас это время последнего ребаланса в идеале она у вас всегда должна быть линейной то есть ребаланс никогда не должны происходить если ребаланс происходит то это что-то не очень хороший не очень хороший ваш ше системе происходит Ну и смотрим конечно же за общей утилизацией ресурсов на всём кластере и советуем смотреть как за нодами за пейсами за всеми сервисами в целом На каком железе всё это работает у нас есть такой гетерогенный кластер у него есть множество различных характеристик но в совокупности это порядка там 1500 ядер и примерно там 18 ТБ рама и данный кластер на данный момент утилизируется порядка там 20% цпу и 25% рама что оставляет доста большую ка для масштабирования и больших всплесков данных также у нас есть отдельный кластер кавка который мы используем как промежуточные Топки это нот совокупность там 180 ядер и более там примерно 2 рабата рама по выводам что мы какие выводы Мы из этого можем сделать во-первых жизнь потоковой обработки данных с условиях то есть большими объёмами данных и достаточно надёжно и быстро можно жить и без фреймворков и этого не стоит бояться то есть это можно сделать и достаточно просто не нужно придумывать сложных архитектур это будет действительно работать также э нужно понимать что фреймворки за вас делают некоторые всё-таки оптимизации и если вы не используете фреймворки то вам самостоятельно Придётся делать э оптимизации тюнить отдельные компоненты использовать нестандартные структуры данных и делать возможное нестандартные подходы к обработке данным плюс А мы должны понимать что нам нужен всегда детальный мониторинг системы нам Это жизненно необходимо чтобы быстро реагировать на инциденты те же самые выводы наверное в краткой форме Если кому-то так будет удобнее в тезисах А у меня на этом всё спасибо вам большое за внимание Я очень рад что вы пришли ко мне на доклад Спасибо Спасибо Это было великолепно друзья поднимайте руки у кого есть вопросы к вам уже бегут с микрофонами смотрите все кто онлайн мы ваши вопросы тоже спасибо за доклад Антон и на тех вопрос к выбору архитектуры компонентов поз вы выбрали Скажите насколько вам нужны транзакции и как бы вроде бы и нет Если нет то не смотрели ли вы какие-то нереляционные варианты да Э спасибо большое за вопрос действительно нам транзакции здесь не не нужны особо Мы в это не углублялась и Да мы в принципе рассматриваем и другие варианты У нас есть рост базы то есть есть рост и задела на то чтобы использовать э большее количество рекламных кампаний и больше у них параметров поэтому есть э Задача В бэклог на исследование других леционе задачки последовать Как нам в этом может помочь Касандра Сыла и ээ Другие может быть колоночные решения Спасибо Добрый день Андрей Сбер А вот вы на слайде показали что у вас один кавка коню затем распараллеливание данные в кафку и какую стратегию вообще использовали для гарантированной обработки данных потому что при переходе от однопоточный нюансов Да спасибо большое за вопрос Это очень хороший на самом деле вопрос потому что очень много вопросов с этим связано как обеспечить вообще Надёжность обработки этих данных и как закоммитить то что мы уже обработали соответственно Да у нас есть Ну в разных сервисах происходит комит по-разному Где например вход который у нас используется Да шринкер компонент который получает вход более 10 там миллионов событий в секунду очень большой э поток там мы стараемся всё делать средствами э самой капки то есть используем колбеки э на консьюмер и отложенные какие-то вещи связанные с Ну то есть это всё включено механизм retri и прочие составляющий То есть если мы какое-то сообщение не добрата оно добавляется в отдельный там dlq и из этого dlq потом вычитывает и потом если мы понимаем что он успешно отработал то мы можем уже его закоммитить и в некоторых сервисах рассматривали вариант где мы комети уже не за счёт отдельного Ну не за счёт механизма Кафки А например сохраняем это всё в базу данных и там это тоже неплохо работает но в принципе склоняюсь к тому чтобы сделать везде за счёт именно кака механизм комита Женя Большое спасибо за доклад у меня появился такой вопрос Как вы оценивали Что шринкер удалось реализовать достаточно оптимально То есть например место сообщений в ваших форматах использовать проб потому что в нём удобно часть полей только парсить Это значительно ускоряет обработку и например вместо джавы использовать плюсы Возможно это тоже будет производительнее да спасибо большое как раз таки за этот вопрос Это хороший вопрос тут тоже стоит такой витаю в воздухе мнение что в принципе мы могли бы использовать и другие языки программирования это не Java может быть даже не в первую очередь Ja быть даже и плюсы это могут быть ust или go в своём понимании и здесь jav во-первых мы выбрали из-за того что у нас уже есть компетенции с этим нам не нужно было развивать свой отдельный там центр компетенций нанимать людей других плюс мы думали о том насколько будет бюджетный этот продукт насколько он будет максимально эффективный и на тот момент мы поняли что нам найти на рынке проще джав которые имеют скажем грубо говоря нужен нам скоп компетенции чем найти например плюсов или ребят которые помогли бы нам это сделать на Go второй вопрос про ш rinker мы сравнили с другой Legacy системой которая очень похожа на то что сделали Мы она использовала Spark стриминг и она была написана на Скала и мы сравнили результаты и получили примерно 40% прирост по эффективности ресурсов но при этом максимальная професия Способность у нас Даже возросла а если учитывать ещ масштаби то масштабирование контейнерное у Спарк стриминга оно работает Ну даже с динамической локацией хорошо но ресурсы тратится намного больше и мы в Пике выиграли 70% ресурсов при масштабировании в моменте А про пробы Почему А про пробу Да у нас внешний источник данных который сильно засел на рифта и здесь мы ничего не могли с этим сделать мы либо должны были смириться и жить по схеме с рифта но в дальнейшем потом их как-то перекладывают может быть в прото буфы но посчитали что это дополнительная какая-то для на Ну дополнительная для нас функциональность которая не будет востребована у других продуктов потребителей для них тоже важен там трифт а внут себя сначала переводить в Триф потом сначала переводить из рифта в пробу а потом из пробу в трифт оказалось накладным Спасибо Женя спасибо за доклад Сергей Сбер Вопрос такой наверно прикладной больше ты сказал в своём докладе что вы раскладывается по кафке по номеру телефона чтобы потом коню могли получить по номеру телефона абонента ТС все события А что делать если у клиента несколько телефонов МТС И кто отвечает за коммуникационную политику и выверка лучших компаний Для клиента Да спасибо большое за вопрос Это вопрос из разряда управления клиентскими данными то есть такой часть CDP и за это отвечают на самом деле отдельные продукты то есть этот отдельный продукт в себе содержит ядро управления клиентскими данными там он формирует некий такой виртуальный аккаунт которому цепляются грубо говоря многие э номера телефонов не только номера телефонов у МТСа много индустрий тот же самый аккаунт Юрен и так далее Все эти индустрии могут прийти за нужными данными и уже завести рекламную кампанию по нужным им номерам телефонов то есть там уже продукт потребитель точно знает что вот эти два номера телефона приходят к конкретному одному человеку и соответственно он может разрезов контактную политику по рекламной кампании пятый ряд пожалуйста сечас секунду а Евгений здравствуйте Меня зовут Владимир ловцов ты он ГП посмотрел твои прошлые доклады они были хорошие понравился этот доклад он очень близко по архитектуре там с нашим решением и хотел тебя спросить первый вопрос насколько оказался эффективнее текущий подход чем если бы ты использовал флин А да смотрите мы сравнивали эту историю у нас как я уже говорил была Legacy система на Спарк стриминге и мы пытались сразу же сделать подобное что-то на флин Мне флин очень нравится как инструмент если бы вот мне последние два требования которых я указывал Я бы прямо 100% скорее всего использовал флин гибкий хороший инструмент который нормально и масштабируется всё остальное делается мы попробовали посмотрели на него он действительно выигрывает в моменте у Спарк стриминга но когда возникают большие потоки данных его масштабирование и поддержка вот этого Т стриминга Да когда у тебя летен там миллисекунды оно выходит дороже а вот тут стриминг который здесь есть у нас он не достигает вот этой н миллисекунд и он в принципе нам не нужен поэтому нам эффективнее было бы использовать вот чисто написанно Java по эффективности утилизации ресурсов флинк Просто стал чуть-чуть дороже процентов на 10 и мы плюс не стали это делать на линке потому что нам бы потребовались ещё компетенции в линке И вообще чтобы там хорошо писать во линке нужно всё-таки в него немножко погрузиться и что-то уже правильно с ним делать то есть грубо говоря полного исследования вы не делали Нет мы увидели вот mvp решение и то как уже утилизировали ресурс как вот всё происходит сказали Нет давайте вот у нас есть другое решение которое Нам очень подходит А сколько если не секрет ушло времени на разработку Так подожди третий вопрос Это платный всё Пардон Спасибо Спасибо сейчас если время останется пожалуй Привет Евгений Яндекс Если я правильно понял то для вас продуктово важно чтобы были трие А как вы тогда обеспечиваете это при рестарта кампейн процессора нет здесь у нас требований к EX Lance нет опять же мы стараемся сделать так чтобы повторных событий было как можно меньше Но продукты потребителей сами на себя берут ответственность по каким триггером они хотят взаимодействовать То есть если произошёл например дубль какой-то то они сами решают с точки зрения бизнеса это дубль и не дубль мы освещаем только технические триггеры Даже если мы технический дубль отправили они всё равно могут для себя решить хотят они контактировать с Поня Спасибо Евгений Привет Спасибо за доклад Василий Зайцев честный знак Ты очень быстро сказал про то что тюли гц вот ну и очевидно да Раз уж выбрали дву Вот соответственно Ну что что сделали какой гц выбрали и какой прирост Это дало Да спасибо за вопрос зна кто это Это этом сырке QR кодик и надо сырке тепер одинако С знаю Я думаю ты про конкретного человека говорит на каждо сырке не не не давай давай да а Спасибо за вопрос Это действительно прикладной вопрос с точки зрения Java про Garbage колектор и это на самом деле тема отдельного разговора там не всё так просто если сказать коротко то в части компонентов мы выбрали zgc по пропускной способно в части оставили Garbage One Garbage Колек но по тюни несколько параметров которые нам позволили увеличить пропускную способность и сократить э фонову Зару цпу на это дело а вы как-то прогрева их прежде чем запустить и вс-таки в процентах Сколько получилось добиться перформанса процентов перформанса чего до тюнинга и после тю на 40% мы улучшили да то есть стандартно вот когда мы использу когда мы пишем что-то на Java без применения каких-то тюнингов Да мы просто запускаем на том что есть на то что есть в основном в данный момент времени Это колектор воет множество различных ситуаций в которых именно для потока обработки данных где всё сначала работает хорошо Но ты поработал неделю и уже спустя недели это прямо вот начинается какой-то ад И вот подход где мы рестар каждую ночь это не мой подход но он бы тоже работал Поэтому я решил вложиться именно в тюнинг gc а не в рестарт каждую ночь спасибо доб Привет Спасибо за доклад рассказывал что вы используете огромное количество топиков входных Они разнородны и так далее и там вроде бы было звонки А можешь чуть подробнее рассказать про privacy и вот типа Какие данные вы можете не можете работать как и так далее Ну да у меня как тому сотрудника который работает с этими данными У меня есть подписанные Тайны связ с этим юридически всё хорошо Вот но до продуктов потребителей мы не отдаём именно вот конкретно вот эти данные мы формируем тригер Где содержится номер телефона без привязки к его личности к его персональным данным и э саму расшифровку разговора мы не знаем и не видим в большинстве своём мы видим и нам важны э важно понимать признак исходящего входящего звонка Кто куда звонил например молодые мамы звонят там например куда-то чтобы искать там детские товары или кто-то ещё куда-то звонит и соответственно из-за этого можно как-то каким-либо образом сделать рекламу Э мы не занимаемся там расшифровкой того что ходит в эсэмэска что ходит в звонках Нас интересуют именно технические детали поведения нашего абонента именно в сети То что приходится с оборудование на базовых станциях Спасибо А сейчас в жизни любого спикера наступает момент когда надо выбрать кому подарить матрёшку а кому э сумку с мтсовский кто задавал вопросы а так да был вопрос про комит сообщений э э кто вот да человек а у тебя уже есть матрёшка Да нет тогда Забирай теперь будет Матрёшка Отлично Да за вклад в сообщество получаешь уникальную матрёшку мы тут бьёмся за них так и теперь пара опционов МТСа кому так А скажите пожалуйста ещё раз Поднимите руки кто поднимал я зрительно помню Да вот человек спрашивал задавал Ну ты запретил задавать три вопроса они все были хорошие Да я Ду тий вопрос в кулуарах сейчас Бет не считай билеты на тебе тоже памятный сувенир друзя если это не сделали Оцените пожалуйста доклад Все доклады надо оценивать чтобы програмному комитету подложить соломку следите за программой определяйте где вам быть на следующем докладе"
}