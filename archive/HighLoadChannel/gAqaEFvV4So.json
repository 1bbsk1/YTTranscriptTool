{
  "video_id": "gAqaEFvV4So",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "представитель вконтактами Всем привет Меня зовут Стёпа я руководитель команды алгоритмов ленты и рекомендаций товаров во Вконтакте в компании работаю уже больше 5 лет за это время успел позаниматься самыми разными рекомендатель системами Вот одна из них как раз Лента рекомендаци о ней будет мой доклад про ленту рекомендации в цифрах это вторая вкладка после основной умной ленты в ней 10 млн да около 6000 ПС примерно 20.000 сетевых вызовов происходит в рекомендациях на одно построение и можно сказать 45 движков содержат данные для этих самых рекомендаций мой доклад будет про плиточный режим этой ленты но такой она была не всегда Давайте вспомним как Лента для вас выглядела ранее выглядела она как-то так представляла из себя последовательность постов которые можно скроллить вертикально Ну в принципе как и в основной Ленте сейчас она выглядит вот так представляет из себя множество плиток где плиточка являются посты или клипы а также механики которые происходит при клике на один из этих айтемов так при клике на клип открывается Лента рекомендованных клипов а при клике на полента на Клик мой доклад будет непосредственно про построение этой ленты от её предыдущего варианта к текущему Какие алгоритмы нужно применить Как построить Эн архитектуру и аналитику вокруг такой большой достаточно фичи обо всём этом Далее в докладе доклад получится можно сказать ФК поэтому Уверен что каждый вынесет для себя что-то инте Первое это раздел обзор с рекомендациями медиаконтента самые плиточки и второе подразделы по кликам на клип и фото наш план будет состоять из следующих частей сперва поговорим про иную часть задачи затем Про бедную часть и закончим аналитической конечно в такой большой продуктовой фиче есть ещё и клиентская часть и непосредственно продуктовая Но это мы оставим за рамками данного доклада сть ша она будет состоить из двух больших пунктов витрина плиточки и похожие посты каждая из этих частей из четырёх подпунктов состоит нам нужно персонально подобрать контент затем его проранжировать сделать его разнообразным и качественным приступаем к первой части А как я сказал у нас на руках есть предыдущая версия ленты рекомендаций нам нужно сделать её плиточной поэтому мы возьмём её в ней у нас были уже клипы мы возьмём движок рекомендованных клипов он умеет отдавать собственно персонализированное клипы для каждого пользователя и таким образом мы подберём клипы на новой витрине а для того чтобы отобрать А фотографии в этом плиточном режиме мы опять же возьмём предыдущую версию лент рекомендаций в ней было очень много самого разного формата контента там фото видео всякие лонгриды музыка сниппеты и так далее но оставим только тот который удовлетворяет нашему ограничению то есть посты с хотя бы одной фотографией Так мы на этом этапе подберём персональный контент дальше нужно его поран Ну для того чтобы что-то ранжировать э нам щика обычно нужен датасет У нас dataset представляет из себя пары User po А где каждой паре соответствует множество фичей и каких-то таргетом фичами мы э используем это фичи юзера фичи поста фичи автора фичи пары юзер автор И пары юзер тематика поста А в качестве таргетом мы берём логи предыдущего ранжирования То есть все те взаимодействия которые пользователь производил с лентой и используем их для обучения моделей самый интересный вопрос здесь какие именно сигналы брать с предыдущего ранжирования и в каком количестве как их использовать то есть здесь возникает задача Таргет инжиниринга Ну для того чтобы ответить на этот вопрос нам нужно посмотреть на нашу витрину и понять что пользователь может с ней совершить во-первых он очевидно может кликнуть на плиточку это хороший Таргет во-вторых он на самом деле может производить всякие разные действия внутри ленты похожих постов которая рождена вот кликом на плиточку там он может лайкать комментировать подписываться там смотреть фотографии совершать долгие просмотры и так далее В общем может много делать разного рода действий возм эти знания и итоговый набор таргетом тогда сделаем следующий первый - это будет Клик на плиточку Ну Достаточно мощный Таргет затем можно взять длину сессии которая порождений и так далее разных действий которые пользователь может совершать в лентах например ещё можно взять продолжительность сессии по времени Ну и так далее здесь такая Инженерная задача возникает можно разные таргеты придумать остановимся на этих теперь можно взять обучить модель которая работает именно вот с но критериальной оптимизацией то есть сразу с - таргета мы во ВКонтакте используем для такой задачи xgboost ль mget prv а именно его распределён ную реализацию на спарке о ней подробнее рассказывал Мой коллега можно посмотреть его доклад перейдя р коду Ну я повторю что там было в принципе стандартное решение такое задачи Когда у нас есть н таргетом образом обучают по классификатору на каждый Таргет и затем взвешивает их с какими-то Весами получают итоговый скор и считают его той самой релевантность которая отражает интерес пользователя к атему у такого подхода есть ряд минусов во-первых это неэффективный расход ёмкости модели а во вторых модель не может выучить сложные зависимости таргетом подходе и время инса растёт линейно с числом таргетом другое решение который нивелирует все эти минусы вместо Point мы используем Pair то есть такой Лос который умеет именно упорядочивать айтемы внутри сессии и не обращать внимания на как бы их релевантность с точки зрения вероятности и затем мы взвешиваем такие лосы с некоторыми весами В итоге получаем во-первых эффективный расход ёмкости модели У ней нет ненужных свойств для нас таких как например предсказывали вероятности время инфе не зависит от числа таргетом у нас во ВКонтакте это очень важное свойство потому что мы используем обычно больше дети таргетом и соответственно это свойство позволяет нам ускорять inf моделей и самое интересное что сложные зависимости таргетом уже учтены на обучении а теперь хочется рассказать о том как можно его применить на практике достаточно просто нужно взять любую библиотеку которая умеет работать Сва лом любую стандартную там Boost Неважно а для каждого таргета собрать по независимому датасет затем склеить их с Весами и вызвать Фит и вы получите все те самые приятные свойства с предыдущего слайда у нас во ВКонтакте из-за того что очень много данных нам пришлось поработать над оптимизация мы сделали свой Билд и буста на спарке и ускорили обучение таким образом что таргеты и веса задали уже как массив ну и соответственно Внутри там всё работает Быстро за счёт того что он бустинг оперирует одним датасета а не датасета это повлияло только на ускорение оно в десятки раз ускорилось Ну там вашей задаче не обязательно Это ВС делать можно использовать простой кейс что касается я тут до этого момента ВС время говорил про какие-то веса взвешиваем мы или лосы или вероятности это тоже очень большая и на самом деле сложная задача Обычно она решается Просто экспертно То есть как самый опытный разработчик или менеджер говорит какие веса нужно подставить и с такими весами модели обучается мы во ВКонтакте написали свой алгоритм жадного подбора весов который подбирает их таким образом чтобы они были на Парето фронте таргетом то есть всегда один из наших таргетом максимизировать идея СПИД контроллерами была О'кей то есть На данном этапе Мы научились подбирать контент и его ранжировать если больше ничего не делать то можно заметить что иногда в Ленте попадаются Ну какие-то некрасивые некачественные картиночки вот на той самой витрине с этим конечно же нужно бороться контент у нас контак очень-очень много поэтому А у нас много и контентной с ним что мы делаем мы умеем с помощью нейронокс вые неприятные не эстетичные фотографии какие-то посты с оскорбительными фразами в тексте посты с большим количеством текста на фотографии посты запрещённой тематики фотографии с плохим разрешением и так далее А всё это является отдельной большой задачей на которую обучается нейронка а но у всех них на самом деле есть один недостаток мир меняется достаточно быстро наши авторы придумывают всё новые и новые виды трешака и для того чтобы тоже быстро на это реагировать мы используем трюк под названием фильтрация через пром о нём и хочется рассказать сейчас а для того чтобы о нём рассказать Нужно вспомнить что такое модель Clip от Open Ну предполагаю что многие в курсе повторюсь это такая модель которая получает текст изображения может получать их эмбеддинг то есть она переводит их в одно векторное пространство между которыми можно мерить расстояние то есть сравнивать то насколько текст по Похож на изображение вот мы берём эту модель Кроме этого ещё берём механику под названием zer Shot classification А что это такое Это такая техника которая позволяет намм классифицировать объект на какой-либо класс который ранее не был на обучении то есть вообще абсолютно Рандомный какой-то класс А мы можем с помощью этой техники получить его вероятность для некоторых вот айтемов как мы используем это у нас вот у нас есть выдача та самая а уже персонализированная мы берём какие-то фиксированные некрасивые картинки а называем это промт постами и фильтруем по косинусное таким образом мы из выдачи убираем те посты которые визуально похожи на какие-то заранее фиксированные некрасивые изображения а далее конечно же хочется сделать Аналогично и Для текста тут можно взять Вектор клипа текста но мы берём другую модель под названием use Universal sentence encoder потому что она просто работает лучше с русским языком чем клип вот и всё Поэтому взяли её и сделали Всё тоже самое только вместо фиксированной картинки неприятный такой не эстетичный примеров которых конечно же не будет Вот мы берём фиксированные оскорбительные фразы и делаем всё тоже самое если остановиться на этом моменте то выдача у нас уже будет персональной а качественной но всё ещё в ней будут проблемы Например иногда могут попадаться плиточки с примерно одинаковыми изображениями или в лентах похожих постов может быть такое не хватает нужно поработать над разнообразным контентом что мы делаем мы предлагаем следующий алгоритм достаточно простой вот есть наша качественная выдача мы проходим по ней каким-то окном размера М И каждый раз берём такой пост у которого максимально следующая величина это его скор разделить на о плюс штраф умножить на Альфа где штраф мы вычисляем как количество уже отобранных ранее с такой тематикой постов а тематику Мы берм из тех самых нейронокс очень много то есть мы умеем определять тематики для всех постов Ну Альфа здесь какой-то гипер параметр а далее мы на самом деле всё ещё можем столкнуться с проблемами контентный дубликатов Что это такое Ну это когда вот в лентах похожих постов или на витрине очень такие семантические одинаковые изображения или вообще которые на равно равно совпадают А хочется конечно же их убирать из выдачи что мы Для этого делаем мы используем алгоритм под названием клипш Ну по сути мы считаем хэши для каждого изображения и фильтруем плиточки с одинаковыми хэша Ну хочется тут именно заострить внимание на алгоритме Clip хш это алгоритмы статьи которые можно прочитать перейдя по QR коду его суть следующая для того чтобы его использовать нам нужны клип вектора изображения мы их берём затем мы считаем pca разложение клип вектора берём первый K а компонент этого PC разложения где ка это какой-то гипер параметр и далее переводим Каждую компоненту из нормального распределения с каким-то ожиданием и дисперсии с которого она породила на самом деле по определению pca в один из 16 бинов которые кодируем Как шестнадцатеричное число преимущество этого подхода по сравнению с другими алгоритмами хэширования изображений Кох на самом деле достаточно много следующие мы выделяем по крайней мере такие во-первых можно выбрать любую длину хша от одного до 512 512 размерность клип вектора мы на практике берм затем расстояние между Шами несёт какую-то семантическую нагрузку то есть мы можем мерить хэши не только на равно равно А и считать расстояние между Шами и вот похожие по смыслу изображения будут иметь близкие хэши например мы используем расстояние хэмминга здесь далее выглядит как посты с одним стилем на самом деле Интересно что можно сделать прямо в онлайне обратное преобразование то есть вернуться от хэшей к исходным векторам Ну с какими-то небольшими потерями но они будут достаточно похожие А ещё этот алгоритм на самом деле можно использовать для поиска дубликатов любых у которых есть какие-либо им бедин из любых доменных областей Ну благо эмбеддинг сейчас есть У всего там у видео у текста У музыки и так далее О'кей Мы закончили с первой частью с витриной переходим к похожим постам На данном этапе у нас совсем ничего нет чтобы могли переиспользовать предыдущий вариант ленты для вас Давайте что-то придумывать Ну в качестве белай можно взять просто посты того же автора вот пользователь кликает на плиточку показываем посты от того же автора Ну окей да это будет работать но конечно не то чего мы хотим а мы же хотим чтобы там были посты похожие Как бы как по контенту так и по коллаборативная это на самом деле является стандартной задачей любой рекомендательной системы которая состоит из двух подзадач глобальных это поиск кандидатов и дальше их ранжирование и нам предстоит это решить для того чтобы показывать ленты похожих постов А ну давайте вспоминать как в принципе можно искать кандидатов методов достаточно много всякие коллаборативные фильтрации там и простые основанные на корреляция на разложения матриц гибрид всякие методы основаны на нейронка в том числе на контентные обычно для таких задач используем факторизации матриц сразу нескольких а или подходы с нейронными сетями конкретно в этой задаче в похожих лентах Мы в итоге остановились на ворту веке он показал себя лучше всего с точки зрения метрик а пользовательских метрик интересно тут понять Как именно Мы его использовали Ну опять же предполагаю что все в курсе что такое концепция Word To века там есть какие-то тексты а состоящие из токенов для которых мы получаем динги и можем сравнивать друг с другом мы взяли в нашей задаче в качестве токенов паблики а в качестве предложений мы взяли сессии пользователей того как они гуляют Во ВКонтакте вот по паблика вне всяких рекомендательных систем просто переходит одного паблика в другой и так далее там соответственно если сессия получается то мы её используем в качестве предложения и на таком датасете нам нужно было обучить Word и тут мы опять столкнулись с проблемой из-за того что у нас датасеты гигантские никакие стандартные реализации здесь не применимы типа генма поэтому мы написали свою реализацию СП на спарке выложили её на github а она работает абсолютно с любым количеством данных и абсолютно точно подойдёт под вашу задачу тоже рекомендую воспользоваться Итак в качестве поиска кандидатов мы использовали Word tow по сессия переходов по паблика А мы показываем посты всё-таки Они паблики что касается постов то мы взяли просто все посты которые были за последние N дней от найденных пабликов Теперь нужно их как-то проранжировать хочется чтобы при этом ранжирование было вот как бы такое семантически похожее А как раз-таки модель клип она была у нас про какую-то семантику такую похожа с точки зрения изображений Так давайте её используем для того чтобы сделать ранжирование постов мы вспомнили как мы делали контентные фильтрации и затем сделали Всё тоже самое только вместо фильтрации по косинусу мы по косинусу А между постами и уже фиксированного какого-то поста якоря на который был Клик это привело к тому что в лентах похожих постов у нас сверху именно такие семантические одинаковые интересные изображения которые пользователь пользователю интересно листать и проводить время в таких лентах оно тут на самом деле можно пойти дальше и уже сделали топ пересортица качественного и разнообразного контента качественный контент мы сделали по аналогии применили всё тоже самое алгоритм разнообразия тоже аналогичный В витрине но с другими коэффициентами штрафа а поиск дубликатов здесь уже работает немножко иначе так как у нас в лентах похожих уже есть вектора клипа в онлайне по которым мы сортируем то мы можем фильтровать дубликаты и по ним это работает чуть медленнее чем клипш Ну понятное дело качественные потому что пш всё равно теряет какой-то какие-то какую-то информацию в себе при этом на витрине мы не можем это использовать потому что там очень много плиток в одной выдаче и пользователь не дождётся свою ленту если мы будем загружать для каждой векто размера 512 льной частью что можно из неё вынести во-первых это практическую реализацию многокритериальной оптимизации через градиентный бустинг Ну всё то что я рассказал про склеивание датасет pwi Target и вызов метода Fit а затем можно забрать с собой Zero Shot classification для быстрой генерации контентный фильтров тоже отличная вещь подойдёт в любой области сможет сделать Вам выдачу более качественной А дальше интересный алгоритм ш который применим для любых доменных областей у которых есть динги И последнее - это распределенную реализацию Word на спарке которая эффективно работает с любым количеством данных и даже на наших датасета огромных за там пару часов ёт качественные динги окей Как вы поняли на этом эта достаточно много получилось разного е и быстро работало чтобы пользователь дожидался свою ленту за сотни миллисекунд нужно придумать какую-то backend архитектуру при этом под де Милн дау и таким количеством пса ну с другой стороны чего тут думать вот идёт запрос За лентой дальше мы идём в движки рекомендаций постов и клипов возвращаем результат и всё можно остановиться на этом ну оно конечно не будет работать точнее Поль будет ждать свою ленту уже какие-то секунды поэтому нужны оптимизации первое самое простое понятное - это мы распределили запросы за рекомендациями постов и клипов затем а опытный кендер наверняка хочет захочет что-нибудь за Каширова Так у меня презентация лагает Угу угу ладно расскажу спойлер про прот немножечко прилёг сейчас починим Пока без спойлера буде Да такое случается Мы тоже айтишники и у нас тоже есть свой продакшн и он называется ло аж с двумя плюсами считаем оп почти сохранены четыре девятки заработало да как я сказал хочется что-нибудь закро мы добавили кэш на 3 часа потом А хочется чтобы в этом кэше всегда были какие-нибудь свежие данные чтобы могли их сразу обновлять э отдавать пользователю мы сделали крон который периодически в фоне генерирует нам всю выдачу складывают её в кэш и тогда если пользователь приходит За лентой данные уже сразу есть Кроме этого сделали ещё следующий трюк когда стартует приложение ВКонтакте вместе с запросом за основной умной ленты идёт запрос ещё из-за ленты рекомендаций параллельно в фоне которая опять же или попадает в кэш или генерируют её заново но так или иначе если пользователь делает свайп вправо у него уже сразу присутствует контент Окей А что делать если происходит какой-то форсированный запрос За лентой который по определению должен обходить все кэши например Pull to Refresh а действительно обходит почти все кэши мы здесь сделали следующее предположение если запрос пришёл не позже 10 минут с момента генерации последнего кэша то можем его взять и убрать просто отфильтрованные плиточки а убрать плиточки которые пользователь уже ранее видел и тем са и вернуть результат а далее при клике на пост у нас появляется Лента похожих постов Да мы её конечно же тоже зашивали А ещё на самом деле всё это происходит на клиентах потому что именно клиенты общаются с нашим кэндо и мы добавили кэш и локальный на клиентах Вот примерно так и выглядит наша архитектура ленты для вас которая на второй вкладке что можно вынести отсюда Ну во-первых сложные вычисления льные всякие разные там Старайтесь выносить в оффлайн контур затем Конечно же можно использовать параллелизм в онлайн контуре как можно чаще и что касается кэша то можно оттуда читать данные до тех пор пока в нём лежат относительно свежие данные где это определение Но каждый для себя может понять исходя из какого-то продуктового вижена или Ой тепер каже понятно что и вот внутри бэнда каких-то ещё параметров например там времени кэша и так далее очень много всего для того чтобы за всеми этими параметрами гипер параметрами следить как бы контролируемого к цели к росту метрик в при переходе наплечный режим Мы каждый наш шаг проверяли через а тесты Ну просто для того чтобы объективно оценивать метрики а не катить фичи на жене небольшое напоминание про а тесты Что это такое Ну в целом это такая механика а представляет из себя следующее Когда вы хотите выкатить какую-нибудь новую фичу и объективно оценить её результат на ваших пользователях вы берёте ваших пользователей делите их на две части контрольные тестовые контрольно ничего не делаете на тестовой части применяете вашу фичу затем ждёте какое-то количество времени а собираете данные считаете метрики и смотрите на их зна если они растут в положительную сторону то вы такую фичу выкатывает если они падают стат значимо то вы не катите если у вас табличка Как говорится белая то каждый тут принимает решение для себя катить фичу или нет теория говорит что нельзя катить белые таблички Ну на практике Тут каждый для себя решает Мы в каждый момент времени проводили достаточно много разных а тестов как с Элем так и с гипер параметрами бэнда и вот для для того чтобы они все не стояли по времени друг друга не стояли в очереди придумали следующую схему то есть стандартная схема как я сказал 50 на 50 мы сделали Иначе мы взяли всех наших пользователей разделили их случайным образом на 10 частей далее взяли части 6 78 и разделили их ещё раз случайным образом на 10 частей и там получили 10 групп размером 3% нашей аудитории дальше мы взяли части 1 2 3 4 5 ещё раз их Поша влили случайно на пять групп размером 10% аудитории а часть номер девять мы выделили в качестве обратной группы она размером 10% аудитории это такая группа на которую не катится никакие изменения в рамках этой фичи перехода на плиточный режим ленты рекомендаций Ну для того чтобы мы в каждый момент времени могли оценить по Метрика Как именно мы далеко продвинулись Вот там какой алгоритм правильный завели и так далее а последнюю часть мы объединились с частями 1 2 3 4 5 и разделили ещё на две части случайным образом и получили Там две группы размером 30% аудитории то есть Прямо очень большие группы как итог у нас было пять групп размером 10% аудитории 10 групп размером 3% аудитории две группы размером 30% и вот одна обратная группа Зачем нужно такое сложно разбиение во-первых чтобы каждый пользователь случайным образом мог попасть в любую из груп важно чтобы всегда была обратная группа вот на таком большом продуктовом изменении затем чтобы мы могли Независимо шаф мелкие средние вот большие группы относительно друг друга то есть менять Там соль каждый раз пользователи перемешиваются И мы продолжаем ставить наши эксперименты а далее чтобы в каждый момент времени у нас не стояло слишком много тестов вот по этой фичи перехода наплечный режим лент рекомендаций там 10-15 тестов максимум Иначе аналитику просто уже сложно За всеми следить понимать где Что происходит даже если там одинаковые примерно тесты прибираются какие-то гиперпараметры по одной под фиче всё равно уже сложно за этим следить и правильные выводы делать Ну и далее нужно чтобы хватало чувствительности метрик поэтому размеры именно такие а Что касаемо метрик то они у нас бывают самые разные там метрики всякие ключевые контрольные метрики просмотров лайки комментарии глубины и так далее представляют они из себя у нас во-первых это табличку примерно как на экране только в ней намного больше метрик сотни которые вот значимо прокрашиваются в одну из сторон после того как мы применяем стат тест на них а также мы ещё умеем строить графики которые показывают нам динамику метрик там день ко дню динами относительного эффект абсолютного значения и так далее метрики проводя атест мы знаем их таксономию мы знаем какие метрики самые главные какие нет достаточно быстро принимаем решение о том следует катить изменения или нет Что можно забрать с собой из блока про аналитику во-первых делайте обратные группы к большим продуктовым фича Ну речь именно про большие фичи если у вас какая-нибудь мелкая фича то не обязательно оставлять группу пользователе на которых мы е не катим затее неви одновременно сного затем стараться как-то за ними следить когда их больше 10 это уже становится невозможно и конечно же регулярно делайте шал пользователей в ваших группах чтобы эффекты не застаивалась небольшой бонус трек в конце понятно что теперь Лента рекомендации выглядит непривычно для пользователей раньше там были посты обычные тепер КАТО чки и для того чтобы а для того чтобы как бы угодить всем Мы в том числе прокачали и наши посты может быть интересно в основной умной ленте мы сделали для них отдельную ме модель которая отбирает их персонализированного в умную ленту увеличили количество таких постов и улучшили наши модели фильтрации по вероятности скрытия такого контента А на этом У меня всё спасибо Спасибо большое Стёпа слушайте У меня вопрос к залу А у меня одного сейчас вот на предпоследнем слайде Там сверху показалось что там написано ES Котен да А ну то есть всё я понял хорошо так друзья достаём телефончики достаём двойные листочки помните эту фразу вот доставим телефончики и вот QR код пожалуйста оценок и комментарии нашему спикеру нам это очень-очень-очень важно чтобы сделать круто дальше А ваши ручки вверх вот вот вот Давайте вот сюда пока вот сюда потом туда во второй Здравствуйте Можете включить работа А спасибо за доклад А подскажите а на вот это вот ранжирование ленты влияет допустим поведение там моих друзей там кто что лайкал по похожей тематике и так далее да влияет у нас в фи чах зашита информация о том как ведут себя ваши друзья Ну и то есть она мне точно так же по поведени информация о втором круге она влияет Всё спасибо да привет Спасибо за доклад я хотел узнать лишь один момент вот смотри ты сказал что вы строили новую ленту для вас на основе событий фактически по старому алгоритму а почему Или возможно вы использовали событийные данные там из ленты которая вот Вот это умная с постами из тех на кого-то подписан Почему вы её не использовали если использовали то как бы вопроса нет А если да Это хороший вопрос а мы использовали в том числе и данные которые льются Из основной умной ленты то как пользователь взаимодействует там с не подписочный контентом Да мы это использовали тоже для того чтобы как-то сделать наши таргеты более соответственно персонами и качественными Да вот там вот дальше Привет Да У меня вопрос по поводу дубликатов видео Вот вы говорите что вы используете клип но клип - это всё-таки про картинки как вы агрегирующие берм от них из этого сервиса и вот используем нашей Ленте то есть мы не работаем с видео внутри нашей команды вот поэтому тут не смогу подсказать можно я добавлю фан факт доклад про Котен ID в видео не прошёл строгий фильтр програмного программного комитета лода будут готовиться лучше Поэтому ждите следующих выпусков Приходите наталку и вот там вот ещ был Да можно я пока да у меня на самом деле семь вопросов сколько можно задать один самый лучший зде ровал Да подготовился Меня зовут Марк работару тоже много рекомендациями занимаюсь а ты очень много рассказывал про разнообразие А как-то мерите какие метрики разнообразия используете Да конечно метрики разнообразия есть самые очевидные это Например как много постов там одной тематики идут подряд внутри одной выдачи которые группируется по там по сессия или по пользователям в том числе как много одинаковых авторов идут подряд Ну то есть все такие продуктовые метрики которые понятны менеджерам которые менеджер смотрит э и видит А ну да там авторов стало меньше в одной выдаче там в окне пять отлично примерно Вот такие мы используем онлайн имено метрики Угу а офлайн там всякие серендипность и всё остальное оффлайн А ну вот такие метрики мы особо не используем Но потому что этот алгоритм если вспомнить он скорее применялся в онлайне то есть мы прямо в онлайне подобрали гипер параметр и посмотрели на а тесты вот на эти самые метрики на то как там меняются наши основные продуктовые kpi метрики танты и на то как они там Ну понятное дело они будут падать с ростом разнообразия потому что релевантность выдачи падает и нашли вот такой такую приемлемую для нас величину понял спасибо там был вопрос да давайте можешь рассказать Почему Каб в качестве ранжиру куда смотреть сюда Ага да Привет Да могу рассказать а не не россеть А у нас niked Boost у нас xg Boost а использовался могу рассказать почему Ну да я имею в виду градиентный бустинг а не Да а потому что фичи у нас в основном табличные табличные данные а с ними же бусты там Кэт бусты работают лучше чем нейронные сети У нас есть много персонализированных фичей под пользователя там то как он взаимодействует с тематика какие-то затухающие сиры в том числе то есть такая табличная табличные данные вот и нейронки их особо победить не могут в онлайне Ну можно же к табличном добавить не табличные данные тексты картинки и так далее да всё это мы используем тоже конечно же Но опять же не в качестве сырых эмдин А каких-то фичей вот Ну пока на практике не получилось нейронка победить бустинг вот такой ответ Ну если получится мы обязательно переедем на Мне кажется это серии не Почему конкретно Да что вот вот здесь вот такие плюссы а здесь такие минусы А здесь ответ находится на стыке непосредственно своего опыта отрицательного возможного опыта реализации вот использования нейронных сетей вот и всё да спасибо за доклад вопрос про кандидата генерацию Если я правильно понял то обучаются им бедин по переходам по паблика в рамках сессии Ну тако быстренько вопрос как бьётся всё по сессия а второй вопрос содержательный получается что у нас кандидата генерации обучается на текущее поведение пользователя А наверняка ваши Ну то есть как вы боретесь с обратной связью вот такой вопрос Да первая про сессии Стандарт в индустрии использовать 30 минут То есть если вот div между действиями больше 30 минут то это Две разных сессии Ну это мы тоже взяли это все используют А пробе loop тут вот я как раз сказал такую фразу что мы использовали данные которые как пользователи гуляли по паблика вне всяких рекомендательных алгоритмов То есть просто как они гуляют по ВКонтакте самостоятельно там перемещаются по паблика вне лент Вот и именно это и убрала тот самый бас друзья ещё вопросы Давайте вот там Привет Спасибо за доклад У меня вопрос почасти касательно фильтрации негативного контента что чтобы отойти от разметки вы начали использовать клип но при этом вы выбрали какой-то приан набор промпто и картинок с которыми мате контент вот как Вы убедились в том что этих наборов достаточно чтобы фильтровать всё распределение негативного контента и не думали ли вы посмотреть в сторону может быть СБО основная суть фильтрации через промт в том что мы их можем быстро добавлять в онлайне в том числе То есть это такой был итеративный процесс один раз добавили потом живём прилетает какая-нибудь жалоба Или мы сами видим что выдача в выдаче появился некрасивый контент мы берём и используем либо его в качестве промто То есть это быстро через конфиги мы добавляем его через конфиг соответственно все выдачи сразу ну в рамках там отдельных а тестов меняются Вот то есть именно в этом Ну конечно же в оффлайне перед этим ВС равно там Мы считали стандартные метрики наши prision на фильтрация но тут основной как бы поит в том что можно быстро через конфиги добавлять такие некрасивые пром вот а по второму вопросу ПРО модель Повторите как она называется кажется я с ней не знаком Open кото принима от на вопро по изображению Мне кажется мы ещ не успели просто использовать можем с вами обсудить позже Спасибо Давайте ещё один вопросик есть у нас нет вопросы кончились Ну тогда стёб Выбирай Кому достанется всей прекрасный хомяк и подарок от нашего спонсора мне понравился вопро прозвание дан вот я предлагаю хомяка отдать мяка да спасибо за вопрос Да вот туда Ага Во а второй подарочек а второй можно про дейс про то как мы обучали наши модели Да вот всё спасибо большое друзья мои Ваши аплодисменты спикеру ёпа Спасибо тебе большое мы от лица программного Комитета и онтика тебе тоже дарим небольшой сувенирчик сда пожалуйста делай ещё более крутые доклады Приходи к нам Мы очень буду рад тебя видеть ребята Спасибо всем Спасибо ещё раз"
}