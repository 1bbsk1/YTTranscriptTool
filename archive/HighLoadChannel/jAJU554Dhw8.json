{
  "video_id": "jAJU554Dhw8",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Всем привет расскажу вам сегодня в этом докладе зачем нам вообще Понадобился хадуп в азоне Почему мы растянули его на три дата-центра как это сделать у себя дома не прибегая к огнетушителю и что у нас из этого Из всего получилось вот где мы тушили пожары Где мы страдали Где падали и всё тому подобное для начала надо рассказать Кто является клиентами дупа внутри озона Это примерно 30 Может чуть больше сейчас точно больше команд и которые представляются dat Science аналитиками которые считают при помощи apch Spark какие-то свои агрегаты предикты для того чтобы толкать бизнес вперёд вот там предсказание спроса поисковые машины для этого для всего нужен ач SPAR и конечно дата платформа которая для всех этих ребят поддаётся данные доставляет из кучи баз данных там каких-то мастер систем информацию из стриминговая интеграции и всё тому подобное для того чтобы объяснить зачем мы вообще пошли в три дата центра надо рассказать небольшую предысторию э до 20 второго года мы постоянно переезжали у нас грубо говоря есть дата Cent 1 в нём тусуется наш кластер дупа команда туда сабмит свои задачи дата инженера подвозят данные и в какой-то момент дата-центр должен закрыться у нас либо заканчивается контракт либо нам больше не хватает места так или иначе нам надо переезжать Итак двадцать первый год у нас примерно 6 петабайт данных 35 серверов У нас есть какой-то свой дуц в котором есть клиентс процессинг со спарм кон данные от дата инженеров то что считают наши dat Science и вот примерный план переезда как мы переезжали в двадцать первом году мы начинаем с того что сетам новый дата-центр сетам Тачки сетам новый дуп и это самая самая самая самая самая быстрая часть того что мы делаем потому что потом нам надо начинать двигать данные мы запускаем дис ЦП пытаемся не уложить сеть и медленно но верно двигаем из одного ДЦ в другой там несколько петабайт данных откопировать историю Молодцы после этого нам необходимо повторить Ну несколько сотен интеграции из различных постгрес сов из различных вертик из кафо все эти стриминги нужно повторить в новый дата-центр в новый кластер А это там ну очень много интеграций после этого всего когда мы настроили двойную доставку данных нам надо произвести ревизию от этого ну тоже никуда не уйдёшь вам надо сверять потому что иначе у вас все отчёты поедут не туда вот после этого Когда у вас есть чистый красивый новый кластер с данными вы начинаете мигрировать клиентов так как у них там есть тоже какие-то кумулятивные агрегаты и прое туки им тоже их расчётов вы это тоже начинаете двигать через дис ЦП потом запускаете клиентский процессинг в Дубль напомню там 30 команд сотни расчётов на спарке вы это запускаете в Дубль И после этого сверяет агрегаты где-то уже после сверки каждого клиента вы начинаете Ну консмед на новый дата-центр на новый кластер Вот и чтобы ну представить снова масштаб проблемы у вас 30 команд там в чати дупа у нас типа чуть больше тысячи человек вот у вас сотни интеграции Дато инженеров А дагов в общей сложно сумме где-то 7000 и добрая половина из них дагов ну как-то завязано находу и вам нужно это повторить что вот мы получили новый дата-центр закрыли старый увеличили объём дупа сделали всем хорошо но есть огромная проблема и проблема эта заключается в тому что это невероятно долго то есть Представьте что мы где-то полгода тащим процессинг и данные из одного дупа в другой это усно нужно понто Свер не доехало Ну для этого нет никаких платформенных решений готовых скриптов всего тому подобного это невероятно Дорого потому что дата Science дата инженеры ходу повы админы это ну дорогие ребята и они вместо того чтобы толкать продукт вперёд делать какие-то инновационные крутые штуки они занимаются инфраструктурой они там повторяют свои и вот не при пользу Компани это жутко неинтересно потому что это ну суровая операционка и заниматься и не сказать что прям классно гораздо интереснее какие-то новые технологии тестировать читать документацию на тот же самый хадуп это и применять всё это в продакшене это веселей А вот заниматься операционной клиентов Это невесело А самое страшное что Ну через год или два это снова повторится нам снова не хватит место мы не сможем масштабироваться горизонтально приехали всё и наступает момент в двадцать втором году когда надо что-то с этим делать потому что понятно что вот там тратить четверть времени на поддержание инфраструктуры узников никакого желания нет Мы только-только закончили переезд людей которые пользуются хопом Стало ещё больше но есть новое вводное и это новое вводное заключается в том что Озон начал с двадцать второго года жить в трёх дата центрах Аа и нам выдали новую пачку серверов сказали ребята Придумайте архитектуру как мы будем жить так чтобы вот больше вот этой Катавасия не заниматься мы начали накидывать цели и думать что мы вообще хотим от всего этого в первую очередь мы себе сказали Мы хотим дуб который будет отказу устойчив при выходе из строя одного из дата-центров упал метеорит оборвали кабель всё что угодно ходу должен продолжать отдавать получать данные жабы в ер должны собти А у нас об этом Ну просто не должна болеть голова мы хотели масштабироваться горизонтально на несколько дата-центров внутри дата-центра не иметь практически никаких ограничений ну его физические пределы не закончатся и Ну желательно чтобы их был не один мы больше не хотели никогда заниматься переездами потому что это ну дичь полная и мы точно не хотели сильно менять стек с учётом того Сколько у нас всего было построено на спарке с интеграция в хадуп сколько других аналитических баз было завязано на интеграцию в дуп сколько сервисов на него завязано мы вот явно не хотели от дупа отказываться раз и навсегда поэтому начали прикидывать варианты Ну выбрать новые технологии и выкинуть ходу звучало сначала логично взять какой-нибудь S3 поверх кубера запускать Спарк и жить как классное облачко нотт Назар кластеров вариант номер два но здесь возникает вопрос А как мне распределить данные между тремя кластерами так чтобы и не обидеть никого чтобы потребители этих данных ну получили всё в конечном счёте и сеть не нагнуть чтобы при копировании данных из одного дата центра в другой постоянно контролировать это так ещ и получается по месту огромные пробле клиентские данные которые трекер как все его называют Весит примерно треть всего дупа получается Если я раскинут кластера это будет с реплика Фактор 3 x9 от исходного формата звучит страшно дорого нехорошо и нам приходит вариант а Дава Поу прикидывать как бы это нам дело провернуть в архитектурной части обсудим с вами Каким образом такое дело вообще построить и как оно будет переживать DC -1 для hdf и для ерна поговорим сначала про hdfs Это единый кластер просто мы в каждом дата-центре сетам примерно одинаковое количество дано Желательно чтобы по Капа они были одинаковые у нас всего один Ну AC Ма как и в любом отказ устойчивом дупе Их всего три мастера два висят в стендбай клиенты ходят за метаданными всегда только в одну ноду в активную примерно прикинули трафик который будет туда идти поняли что это совсем копейки нас это вообще волнует такой трафик на Мета информацию в Если со всех кластеров будут ходить в одну нам ноду не страшно что делать с данными А с данными мы придумали следующее в дупе есть такая штука Как рек Вы можете помечать стойки говоря что эти сервера находятся в одной стойке для того чтобы у вас м при выходе из строя стойки не происходила полная потеря данных Умно раскидывает блоки Мы подумали А почему нам к не обозвать дата центром мы берём даце говорим что этон 1 и применяем блок placement polic tolerance которая раскидывает в нашем случае блоки на каждый сть старатся равномерно размазать все блоки всем реком таким образом сейчас Простите таким образом каждый блок у нас находится в каждом дата-центре в единой копии получается что локальность чтения данных у нас гарантирована то есть если клиент приходит из дата-центра 2 он замета информации сходил в активную нейм ноду Где бы она ни была а за блоком пойдёт точно локально с этим проблем у нас нет когда клиент ходит за данными в свой дата-центр мы получается минимизируем oss DC трафик чего от нас собственно требуют сетевики и oss DC у нас Остаётся только на запись То есть когда мы записываем Ну как ни крути трафик по сети надо гонять Что нужно иметь Какие пререквизиты для того чтобы такую инсталляцию сварганить у вас должен быть хороший канал Вы должны с сетевика заранее договориться о нём Вот и стараться его не превышать У вас должны быть низкие задержки между дата центрами хождения пакетов Потому что если вы будете ходить в мету долго и с задержками у вас будут проблемы если у вас будут проблемы с репликации вам Ну у вас тоже низкие высокие сетевые задержки вам с этим не помогут сетевые задержки и желателен равный капасити в равный капасити объёма дата noe Почему как и в любой распределённой системе у вас получается суммарный объём вашего кластера он не равен сумме всех дата node объёма сумме объёма всех дата noe он получается равен вашему самому маленькому дата центру умноженному на 3 в данной схеме Потому что если вы заб один из дата-центров то ничего хорошего с вашим ходу пом не будет как ведут себя дата ноды когда они приближаются к 95% утилизации капацитет спросить любого админ дупа Ничего хорошего Вы там не получите поэтому у вас должны быть одни и те же Простите одни и одни и те же количество серверов желательно с одними и теми же дисками Disaster Recovery имеем схему три дата-центра падает метеорит оторвало сеть ВС идт как и в обычном кластере у вас активная нано переезжает в другой дата-центр получается мету вы уже отдаёте А клиенты которые выжили в дата-центре 2 и в дата центре 3 они спокойно ходят за блоками которые Ну у них остались потому что вы гарантировали это при помощи Даше Яр есть примерно такой же механизм как и у hdfs пометки Note Вы можете сказать что там есть у каждого ресурс менеджера есть лейл вы говорите Это йл dc1 это л dc2 раскидали его и при помощи лейблов Вы можете по дефолту закидывать клиента ерна в нужный вам дата-центр то есть Для клиента всё это прозрачно получается клиент просто сабмит что-то в ярн у нас в рне для его очереди есть настройка В какой дата-центр должна полететь Джаба и она туда летит при выходе из строя дата-центра здесь вообще всё просто То есть если там у вас происходят какие-то переключения нады вам нужно вс-таки за этим следить Здесь вам нужно поменять только один конфиг вы меняете конфиг шелера ерна настройки очередей говорите что эти очереди теперь должны работать в других дата-центра товарищи которые запускают у вас задачи просто сделали мит в ярн и даже не зная того что происходит закинули уже на другое железо свои задачи что мы получили в реальности у нас было были две глобальные проблемы Первое - это когда мы решили подменить дата-центр а вторая Ну собственно на учениях Сейчас я вам о них расскажу у нас есть получается три дата-центра живём себе хорошо в уз не дуем приходит новость с дата центром оди нам придётся распрощаться у нас будет новый дата-центр дата-центр 4 никаких проблем добавляем туда сетам ноды раскатываем ходу добавляем его в кластер в единый и говорим Дан 1 ты уходишь на как только мы это сделали А сделали Мы это скажем так резко зади комисси сразу 100 нот в дата-центре о данные полетели из2 и3 реплицировать в D4 резонно нам сказали и радуемся жизни приходят менеджеры говорят Ребята надо ускориться ваши прогнозы в 2 недели репликации этих там 15 Пит Нас не устраивают давайте-ка побыстрее никаких проблем смотрим за трафиком увеличиваем пороги по идее после до комиссии просто Выключаем D1 и живём вообще потрясающе то есть админа дупа ничего кроме того что Натали Тачки Натали ходу прогнали включили до комиссию не делают как было в реальности у нас ограничение для дупа чтобы между дата центрами У нас не было трафика больше чем в 200 Гигабит в секунду наш Крос DC лимит приходят люди и говорят ребята поему проблем нет Вот график вы сами Нам его построили Мы молодцы Мы хорошие э но 2 дня изысканий наших сетевиков и мы увидели следующую картину Если изменить рейд то мы видим Как ведёт себя ходу при репликации 2 дня Ребята искали боль ужас э а хадуп просто делает что он при репликации имеет определённый порог времени и порог по объёму он говорит я за вот этот объём Ой вот это время могу проп нуть вот такой объём данных Он резко выплёвывает в сеть всё что может забивая канал а потом стоит курит оставшееся время что мы и видим На прекрасном графике в результате репликация дупа у нас скажем так сделала больно рядом стоящим сервисом Итого что надо делать внимательно смотрим за настройками троттлинга внимательно относимся к мониторингу сети не допускаем стов и собственно какой мы сделали вывод маркировать сеть дупа отдельно отдельно её Шепитько в кучу с другими сервисами это приводит к проблемам потому их сложно бывает отловить вот мы перевезли получается hdfs что у нас сном а сном у нас вообще просто Красота и лепота он нас не подвёл для того чтобы сейчас Простите есть у нас получается 3dc в в нём крутятся какие-то очереди какие-то клиенты для клиентов которые тусуются ВНТ мы что делаем мы просто меняем настройку expr То есть это то в какой лейл отправляется задача клиента в рне поменяли dc1 на dc4 всё проблема решена выкинули dc1 клиент живёт себе спокойно Аа да перее с ерм вообще не вызвал никаких проблем здесь Красота лепота и радость Итак кейс номер два учение Нам отключили 1 Что произошло ВС правильно переехала активная Неда но дата ноды по запросу нено начали слать в ноду блок репорты почему так происходит у вас пропала треть кластера 100 из 300 иф ту Все ли блоки вообще живы потому что диски могут выходить из строя сами собой и не всегда дано не всегда это может отследить не всегда об этом знает Потому что если у вас пропала реплика нано нужно срочно всё это дело актуализировать что мы получили в итоге мы получили дикие тайминги у5 ярн задач если у вас таймаута dfs то РНУ будет плохо Простите как это выглядело это два графика один показывает время время ответа очереди нейм ноды пишно а второй количество Under блоков получается где-то в 030 мы увидели что блоки начали пропадать НОД вышла из строя дата нода дано начали слать репорты забили нам всю очередь и у клиентов были проблемы потом получается интервал в час во время учений без одного дата-центра всё прошло отлично никаких проблем а вот когда нам вернули наши Потерянные дата ноды мы получили примерно тотже самый эффект Потому что со начали лететь блок репорты что с этим делать мы выкинули фо очередь и сделали не сделали А за использовали Мы за использовали для обработки запросов на он работает вопрос высылает клиент тем ниже у него приоритет Чем ниже приоритет тем меньше за интервал времени обрабатывается клиентских запросов получается Если внутренний польза с Date засе вам кучу блок репортов они будут обрабатываться С задержкой таким образом клиенты которые зас мало запросов будут свою мету отды получать быстро с этим проблем не будет используем четыре приоритетные очереди обработки rpc запросов раскидываю клиентов на четыре очереди нам помогло но мы пока в процессе тюнинга всех этих настроек так выводы схема Рабочая кластер переживает потерю одного дата-центра через 25 минут мы смогли спокойно сабмит задачи данные были доступны как писалось так и читалось куча Under блоков на этом всё итоги dat Sci супер довольны потому что вместо того чтобы заниматься инфраструктурой они пилят свои задачи платформа данных довольна потому что больше не надо постоянно мигрировать эксплуатация тоже довольна потому что это единый один большой единый кластер и его ну проще админить чем много-много маленьких сейчас у нас кластер в 3 дата центра 368 серва ков 67 пиб когда использовать это если вы не можете растить больше свой хадуп в одном дата-центре стоит присмотреться к растянутой архитектуре она вполне Рабочая если вам нужно ДЦ ми1 для дупа Если у вас есть какие-то крити критичные расчёты тоже можно использовать если вам нужно балансировать расчётную нагрузку между дата центрами тоже классная схема Зачем внимательно следить за сетью при репликации страшная вещь и rpc нагрузка на МНО тоже за этим стоит Ну внимательно следить Когда у вас один кон кластер на всю компанию Спасибо за внимание Да пожалуйста тебе спасибо а Итак а Давайте приступим к вопросам пока они кластери в центре зала и вый ещ водички У меня тоже сегодня горло першило это прям тяжело Здравствуйте меня зовут Владимир и у меня ходу протяну на 7 дц Так что я прекрасно понимаю все ваши боли мы переживали примерно тоже самое У меня к вам много вопросов но я задам только парочку небольших первоя сколько занимал вывод всего ремиссию моим подсчётом Это должно занять ну недельку как минимум Спасибо за вопрос мы сильно задушили репликацию И это где-то заняло месяц Ну чтобы Ну для того чтобы не убить сеть для того чтобы не повредить клиентам Окей спасибо и второй тоже Маленький вопрос Вы вот говорите что вы по факту Яр делили на на три зоны в этих трёх зонах у вас по там какому-то количеству там ядер памяти и вы там запускаете клиентом получается что ярно у вас разбит по факту на три независимых кластера Вот это справедливо для всех ваших клиентов Или например аналитикам можно там послать только в один кластер а если там задача важная нужно запустить какой-нибудь большой пересчёт то вы можете забить наик и и запустить жабу в нескольких ДЦ или это железное правило Нет это железное правило мы не запускаем задачу сразу на несколько дата-центров Потому что так как это клиентский код он может сгенерить всё что угодно шалы могут быть страшные поэтому мы от такого точно отказываемся а ревю не проходит разве нет печально Следующий вопрос тоже будет из центра пожалуйста записывай кратенько О чём вопросы Ставь свои пометки потому что тебе нужно выбрать два и это всегда очень тяжело прошу Спасибо за доклад Фёдор Васильев итек Андрей я если я тебя пра может я не понял но вот хочу уточнить Если я правильно понял то кафи это система один сейчас ещё раз кафи один То есть Ну а если у тебя один дата-центр вышел из строя то всё работает как будто он есть да нет ну в смысле его нет да но продолжает работать Смотри я к чему клоню к тому что если у меня часть данных я команда у меня часть данных на одном ДЦ часть на другом но размазала вот этот момент мне непонятен Хорошо Давайте ещё раз пройдём у вас блоки блоки которые вы ну раскидывает в каждом дата-центре в одну копию прибит центру Нет по расчётам Да вы начинаете что-то считать только в одном дата-центре Вы записываете первый блок летит в этот дата-центр а два других летят в второй и третий дата-центр то есть в каждом дата-центре у вас по одной копии ваших блоков они одинаковые Да конечно То есть это полный зеркалирование Ну это реплика Фактор 3 Ага это реплика Фактор в каждый Дант поно репли получается общее капасити равно наименьшему capacity DC всё именно так имен Просто ты сказал что о умножается на три я вот это тогда не понял Нет наоборот Ну общее капасити равно минимальный дата-центр умноженный на ТН Почему умноженный натто Потому что сейчас хорошо потому что Представь что у тебя дата-центры 5 ТБ 10 ТБ 10 ТБ Ну какой общей капасити п 15 Почему 15 хорошо с учётом реплики Ну реплика яп записал всё у меня на это об этом говорю я понял с учётом реплики и без Ну типа мы хорошо ладно то ты оптимистична я пессимистично Ну я гда просто на железо смотрю а не на размер с сыя мне-то интересно данные обсчитать Поня СБО Следующий вопрос тоже центр зала Спасибо за доклад меня Бондаренко Михаил зовут Я компания МТС вот такой может быть прикольный вопрос как известно самый быстрый способ перенести данные из одного дата-центра в другой - это просто снять жёсткие диски погрузить их в самолёт и воткнуть в другом дата-центре не рассматривали такую возможность рассматривали смотрите здесь есть сложности с этим делом это классный вопрос почему мы не таскали данные именно физически во-первых это другое железо Не факт то что ваши серваки Нормально быстро доедут А если таскать кусками то у вас всегда есть возможность э для некоторых данных получить блоки Шрёдингера потому что изначально мы использовали стандартный Block placement polic и смотрите что получается сейчас где он пампампампам все данные жат стандартном Поли в котором у вас одна один одна реплика прикапывать произвольной Представьте что вы вытаскивает и везёте его в другой дата-центр не факт что блок в реке а на самом деле сейчас ДОУ с см момен перевоза железа блок всё его нет пока вы не привезёте его туда вести его туда могут там день два коммутировать проблем с сетью вот там именование Тачки решение каких-то ещё проблем и всё это время у вас блок недоступен Что происходит с процессинга они ну падают ничего хорошего в этом нет а когда у вас Ну большой хадуп у вас вероятность потери какого-то блока случайного она Ну есть мы от это отказались Следующий вопрос передняя часть зала лево Добрый день Меня зовут Андрей меня два вопроса Первый это не пытались ли вы растягивать какие-то другие компоненты экосистемы ме любимый многими и или наме поэто собственно не пробовали смотрите все у нас получается дуб в нём живёт рядом есть zer рядом есть Хаф всё рядом есть Ranger он тоже отказа устойчивый при выходе дата-центра То есть все компоненты которые там вот нужны для того чтобы нормально запускать Спарк джаб у нас реплицировать слишком много файлов становится не превысили ли ваши потребители такой неприятный естественно проблема большого количества файлов всегда стоит мы справляемся с этим просто квотирования То есть у нас есть один большой верхаус на всех который сегменти ется по клиентам каждый клиент получает свою квоту по объёму по количеству файлов вот мы её рассчитываем исходя из объёма оперативной памяти ноды и каких-то там ну вменяемых э цифр типа ну не нужно вам 50 млн файлов в HD фсе Вот не надо и всё значит вы неправильно забиваете этим инструментом гвозди Вот вы взяли не молоток Следующий вопрос Центральная часть зала Спасибо за доклад А у меня вопрос Следующий если выходит один из дата-центров вообще из строя А у нас получается да фактор репликации два становится но в конфигах прописан три нач ли Ду на одном из кластеров две реплики держать он должен по своим политикам нача это делать я понял политика не начнёт писать в два Ну в новые файлы Да новые файлы начнут записываться с реплика Фактор 3 но старые реплицировать не начнут а не Привет ли это к тому что по размеру хорошо смотрите дата Cent -1 когда он выходит из строя Это довольно редкое событие и здесь можно Ну пойти двумя путями либо сказать О'кей у нас хватает капасити И мы живём в реплика Фактор 3 в двух дата центрах Либо мы экстренно говорим Найда на всё что у нас есть реплика Фактор 2 игнорирую клиентские конфиги всегда реплика Фактор 2 вот ну то есть это такие уже детали интересные в которые надо погружаться Когда это случилось Когда дата-центр вышел из строя вот а на моей памяти у нас была всего одна авария в Озоне и как раз после которой началась вся эпопея с ДЦ -1 учениями и через несколько часов собственно кластер дата-центр вернулся обратно если Вы точно знаете что он не вернётся если это метеорит вот тогда стоит об этом подумать Если нет то Давайте продолжим считать данные на оставшихся дата-центра 23 А этот просто подождём и не будем там типа резко везде rf2 вводить записывай помечать вопросы следующие пара вопросов будут из нашего Telegram чата Были ли случаи когда совпадали выход из строя одного дата-центра и проблемы во втором были случаи когда Ну делаем дми делаете мину е одном случается А да делаем ДЦ -1 и проблемы с какой-нибудь дата ноды у которой пропали э блоки слава Богу миссинг блоков у нас при этом не было Э мы там в некоторые файлы жили с блоке с rf1 но не сказать что прям критично быстро отрепьева дальше принято блиц Вопрос какую версию дуп используете 336 в ней появилась как раз Call для rpc очереди Следующий вопрос средний часть зала слева Андрей Спасибо за доклад такой вопрос Когда у нас полностью площадка уехала это понятно А что происходит когда у нас половина какой-то площадки уехала то есть блоки локально потерялись и компьют поехал по Меж цду в соседние цоды это уже ручками получается вам нужно либо ярн вытащить оттуда то есть всех клиентов Яна попросить сабмит в дата цент 23 если первый вылетел либо очень тоненько тоненько мониторить межсетевой трафик здесь как бы ну два выхода на автопати обязательно поднимем бокалы за тонкий монитори межсервисного трафика Следующий вопрос из центральной части зала Да спасибо за доклад Меня зовут Андрей и у меня такой просто вопрос а очень много конфигов возникает особенно если есть клиентское приложение где-нибудь там в спарке прописаны как это менеджери или всё влезает в один универсальный конфиг где всё прописано все настройки ярно а нейм ноды переключения или возникает проблема нескольких конфигов и особенно с клиентскими приложениями Ну прокш понятно там быстро поменяли А вот как пользователей Не знаю нужно ли им как-то уть поо Хороший вопрос так как получается к дупу мы Коннект набор Мелик генерируем аблом откладываем в S3 по-моему и у нас есть питоновая либо которая говорит либо Дай мне конфиг для этого кластера и она генерирует вам этот конфиг складывает там в ТМП и прописывает в экспорт вашего Баша пожалуйста Используйте вот сервисы юзают это Любу либо могут сходить в S3 забрать конфиги А когда Мы производим какой-то Main эти конфиги там и обновляются при помощи абла как раз выкатывают туда вот сабмит задач У нас есть э свой сервис то есть клиентам не нужно помнить про спарко вые конфиги они вшиты в сервис самита с парковых задач как-то так угу у меня будет ещё вопрос а мы поговорили про сеть между дата-центра Окей Вот про железо все вот дата-центры Они как-то между собой получается вы стремитесь к какому-то сказать по нагрузке по диску по процессорам В общем какой-то к одному Золотому стандарту ли же дата-центр различаются чтобы не было перекоса особенно в пересчёте если какой-нибудь он блок не доехал то ходу возьмёт из другого более медленного или более быстрого В общем вот политика по выбору желе есть какая-то или без разни Ну смотрите оно должно быть примерно одинаковым в рамках одного ДЦ именно по цпу по Memory для того чтобы вам у вас задачи внутри дата-центра получали одно и то же железку при сабмит Вот вы можете дать разное железо в разных дата-центра но вот внутри Желательно чтобы они были похожи по диском нас интересует в первую очередь капасити если какой-то из дата-центров начинает проседать там эж вот вне зависимости от того что происходит Мы просто находим виновника либо оптимизируем его задачи либо Если это невозможно то можем мигрировать его в другой дата-центр где нагрузка меньше таким образом как-то отбалансировать как-то помечать вопросы Следующий вопрос из чата Скажи пожалуйста Правильно ли тебя поняли слушатели что в рате сервиса ты не до утилизирует дата-центров в надежде на то что один из них отвалится и у тебя всё продолжит работать Хороший вопрос это прямо классный вопрос я его сразу поме недо утилизация смотрите по-хорошему да вам нужно прикопать 33% ресурсов в каждом дата-центре но здесь есть нюанс У вас есть куча задач вот в продуктовых компаниях есть куча задач от хока когда вам нужно просто что-то там аналитику или Data Science прийти посчитать проверить гипотезу И после этого уже там как-то тащить это в продакшн прок может занимать там 50% нагрузки А может и меньше так вот чтобы в такой критической ситуации выхода из строя у вас было отлично вы зае ОТК и спокойно жите с продано дата центра Вот и когда дата центр о вернётся из своего летаргического сна возвращаете всё на место от хок получает свои ресурсы ребята считают Всё дальше продо принято давай ещё пару вопросов и завершаем прошу Да извините не могу остановиться У меня на самом деле такой вопрос возник А Вы не боитесь ситуации когда к вам приходит высокий менедж и говорит у нас гениальная идея А давай-ка мы ещё один четвёртый дата-центр развернёт тоже запустим чтобы ну там более комфортно нагрузку распи потому что у меня ощущение что все ваши предположения они в этом случае разваливаются что в рне что в дупе у меня контур вопрос Как вы живёте в 7 дц Я очень хочу про это в кулуарах потом поговорить рах Да смотрите D4 или там 5 6 7 как на мой взгляд можно это планировать Вы можете во-первых выставлять реплика фактор там больше трёх для горячих данных для того чтобы они равномерно размазали холодные данные там Рид вот уже насколько не жалко Вам потому что если у вас холодные данные вы их редко читаете У вас есть audit Log hdf Вы можете спокойно посмотреть предсказать Будут ли эти данные читаться там в течение месяца наперёд потому что у вас Ну стандартные процессы будут с оден Теном вы для них Заря реплика фактор их не читают крод трафика нет ВС классно горячие данные повышаем реплика фактор Крос ДЦ трафика нет поэтому теоретически Возможно провернуть на мой взгляд хинт к обсуждению их двоих в ларах три против семи можно и нужно присоединяться и Давайте завершающий вопрос из зала Спасибо за доклад вы упомянули также что вы реплицируемый либо гасим стор как сервис либо Режем его по сети тамм продолжают клиенты роти на оставшиеся два Нет проблем Ну что ж теперь самое сложное для всех спикеров Мы все технари когда говорят А вот теперь Выбери Два лучших вопроса в голове такая звенящая пустота лучши вопросы вопросы кто я где я давай в начале вопрос за который ты подаришь подарок от озона вот здесь лично в студии прямо вот руками а хорошо э мне очень понравился вопрос про грузовичок про перевозку Почему диски не кто задал вопрос про грузовичок выходи на сцену сейчас мы тебя одаривать будем Спасибо Спасибо а теперь наша матрёшка она может уйти в онлайн в офлайн куда угодно Выбирай кому мне очень понравился вопрос про мне поможет команда хелперов в онлайне был вопрос про не доули заю от Дмитрия Безрукова Аплодисменты Дмитрию мы его найдём и обязательно подарим ему матрёшку ну и Аплодисменты нашему спикеру и обязательный подарок от нас Спасибо тебе большое спасибо L"
}