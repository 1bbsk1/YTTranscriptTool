{
  "video_id": "OEd2Vwc6PMQ",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "сегодня мы поговорим о каких вызовах в баннерной демоне мы столкнулись при росте нагрузки так не работает кликер вот Ну давайте познакомимся Меня зовут Артём Букин и я ведущий разработчик ВК реклама пока мые и я боюсь он сломается сейчас и нам придётся купить новый О заработало А значит я являюсь ведущим программистом ВК реклама в частности в проекте баннерное Мона занимаюсь инфраструктурными Задачка и очень люблю дружить с админами девопса Ну естественно с сре потому что нагрузка большая бывают разные приколы которые надо решать и на самом деле на этом слайде могла бы быть Ваша реклама вот здесь наша реклама о чём мы сегодня поговорим в первую очередь мы поговорим о том как мы делили трафик по ролям Для более гибкой конфигурации кластеров также как ускоряли процесс запуска демонов в условиях роста объектов и потребления памяти а также как выносили индексный в отдельный демон и кэширование запросов во время роста этих запросов чтобы снизить утилизацию цпу за 20 рост рекламы произошёл достаточно сильно за счёт того что запускались достаточно крупные проекты в частности новый кабинет ВК реклама и рост количества компаний у нас произошёл в несколько раз соответственно так как Выросли количество рекламных кампаний то и потребление памяти выросло со 150 ГБ до 250 и выше на каждый сервер утилизация цпу за счёт прихода новых площадок и увеличение РПС выросло с 30% до 70% что тоже достаточно серьёзный рост а также из-за большого количества объектов и связанных с ними объектов выросло А время запуска Демона что усложняет раскладку новых версий и новых фичей чтобы держать потихонечку данные объёмы по компаниям по псу и всего остального первоначально также начинался и рост количество серверов У нашего кластера в котором обрабатываются запросы на рекламу и происходит подбор баннерный демон принципиально встроен в достаточно большой крупный наш инфраструктурный проект вообще ВК рекламы и мы являемся так называемым кэндо в рекламной сети рекламна про рекламную сеть рассказывал Мой коллега Миша вчера и соответственно высокая нагрузка которая поступает на рекламную сеть дальше переваривается и преобразовывается в те запросы которые будут приходить уже в баннерный демо на подбор конкретно рекламы Это уже такой баннерный движок который включает в себя вот такие фичи То есть это монолитный монолитный сервис обработку профилей пользователей работу с ML моделями ставки компании рассчитываются автобит также сам подбор рекламы и проведение аукциона и форматирование уже в различных выдачи банеров которые мы будем отдавать непосредственно уже на показ и запрос рекламы у баннерное следующий пайплайн это первичная обработка параметров запроса чтобы определить какие фичи или что будет у нас перекладывать либо потом дальше в пиксель либо влиять на фильтры на таргетинги сбор профиля пользователя чтобы делать персонализированную рекламу также стадия это применение индексных таргетингов стадия Select - это применение уже итеративный таргетингов который будет последовательно для подобранных компаний и производиться проведение аукциона для подбора уже конкретного количества банеров для выдачи и форматирование уже самого ответа то есть мы можем представить рекламу в различных форматах Это для площадок это могут быть а упакованные данные для конкретных уже форматов могут быть даже форматированного Их тысячи компаний сотни тысяч компаний на серверах соответственно потребление у нас достаточно большое и 250 Гб на сервер может даже не помещаться в одну Ну и это тоже серьёзный Такой вызов поэтому из за это сокращение количества памяти потребляемых на объекта хранящихся в базе данных не в баз данных а в памяти одного Демона А в памяти Демона у нас хранится это в виде двух in ров которые если мы говорим сейчас об 250 гиба то это по 125д стора которые работают по какому принципу один р обрат работает на обработку запросов чтобы от начала до конца каждого запроса а второй работает и он работает Кстати он в режиме то второй р принимает обновление по протоколу кратко что это такое то есть каждый баннерный демон является репликой и получает апдейты в соответствии с протоколом соответственно рост количества компании в несколько раз только в этом году подразумевает что мы начинаем выходить за 250 Гб И дальше то есть нам надо модернизировать а сервера добавлять памяти и естественно мы уже не лезем ни в какие куберы и всё остальное а помимо компании естественно компания как ключевой момент память потребляет и места размещения рекламы то есть по цепочке и индексы которые оперируют которые строятся для этих компаний а также модели которые будет с ними работать и а как мы будем во-первых ограничивать эти компании и вообще что такое в нашем понимании внутреннем роль то есть роль изначально для таргета Таргет Вот кто не знает это у нас был проект My Target который стал как раз частью частью ВК рекламы и соответственно с разных площадок может идти разный профиль трафика с разных источников то есть Есть трафик который приходит на большое количество выдачи а есть на большое количество подборов грубо говоря Мы на разных стадиях обработки запроса можем контролировать нагрузку То есть у нас есть две фазы мы по них немножко говорили то есть когда сбор профиля и обработка самого запроса там есть первичный подбор на котором мы можем контролировать трафик То есть это уже подводит нас к тому что когда приходит из разных источников то есть из внутренних площадок с внешних площадок трафик либо из других рекламных систем что нам необходимо иметь возможность конфигурировать разные Тачки по-разному то есть их собирать в какие-то кластера для этого было введено понятие роль и для каждой роли укрупнённой что это будет различные конфигурации то есть мы можем выделять наборы тачек в роль и каждая роль будет получать на свой астрим свой трафик и также для каждой роли мы можем настраивать свои фичи можем настраивать свои эксперименты как это будет у нас работать соответственно каждая роль балансирует отдельно то есть реклама сеть балансирует на нас запросы на своей стороне и мы тоже внутри роли будем балансировать и раскладка каждой роли тоже очень важно раскладка мониторинг производится тоже можно делать отдельно только не только весь кластер но и каждую роль По отдельности зачастую например катится какой-то эксперимент может произойти нештатная ситуация то есть инцидент и мы видим что это отвалилась одна роль то есть сразу локализовано в каком месте то есть мы можем искать это мы Ром по экспериментам по этих экспериментов кто что катил чтобы найти эти проблемы соответственно важным моментом которым мы подходим к снижению потребления памяти это рекламные компании могут на ролях отличаться как решаем эту задачу Для этого ввели такое понятие как шардирование компаний Это как мы будем делить на каждой роли на либо на отдельных тачках какие компании будут крутиться и соответственно какой будет набор данных на них обрабатываться это маска 64 которую мы можем выделять сегменты бит и это могут быть например младший сегмент выбираем там для юлы например и так далее для каждой роли для каждого кластера мы выделяем отдельный диапазоны бит чтобы потом делить компании то есть каждая компания Потом тоже имеет свою маску UN 64 и на уровне заведения рекламной компании либо проведения экспериментов мы можем всегда для той компании задать те биты если мы хотим чтобы это попало например компании которые входят в кластер л они попадают в младшую часть Ну естественно обычно фасетная а распределение бит и мы можем также компанию указать что она будет находиться в нескольких ролях Таким образом мы делим компании паролям на тачках можем сразу контролировать Сколько памяти у нас будет э потребляет резкий скачок на роль прихода компаний то что делать мы будем в этом случае для А ну хотелось бы ещё отметить как исключение для есть отдельная роль для API серверов и соответственно она включает в себя полностью весь всё покрытие компании чтобы технически различные запросы данные можно было с них брать и вот встаёт вопрос Если у нас резкий всплеск количества компаний то есть у нас резкий рост как мы будем сдерживать рост памяти чтобы не деградировать сразу по памяти по пому не начать валиться для этого был реализован механизм автоматического шардирование то есть мы берём кластер с заданной маской шардирование выбираем те биты которые будут у нас являться для автоматического распределения потом дальше групп То есть например говорим что у нас будет 4 бита на автоматическое распределение компаний внутри этой роли и маска компаний которые будет уже делиться будет в соответствие с алгоритмом Ну условно инверно вырва каждого там первого ре чет в зависимости от количества групп на которое мы будем деть эти биты Таким образом мы можем в случае если мы чувствуем Что мы начинаем не помещаться по памяти мы можем банально включить автоматическое распределение компаний по шардам и сдержать деградацию по памяти чтобы не свалиться в ом и за это время уже решать как мы будем жить дальше что это за компания Почему про ро то есть если это было неконтролируемое и незапланированный приход большого количества компаний очень хороший кейс Когда необходимо сдерживать такие резкие выбросы следующим моментом о котором бы хотелось поговорить Это как мы раскладываем и как мы грузимся во-первых начнём с того что баннерный демон - это критический узел вообще ре палане вообще подбора рекла Поэтому до прода Лучше бы чтобы ничего плохого не доезжают свои новый функционал на бете и это бета это обычные продо Тачки на которой Форвард Форвард те же самые все запросы проходит полный пайплайн подбора рекламы но ответ никуда не уходит Если всё окей то мы отправляем это всё выкладываем на несколько тачек с выкладкой на чтобы у на новый смотрели что и как у нас уже на большем окне времени И если всё О'кей тогда начинаем уже катиться в прот выкладываем первую часть и смотрим Что всё ли у нас хорошо на против уже Настей бле мы начинаем отвечать нормальными запросами и можно уже оценить и продуктовые метрики и технические метрики и когда покатилась уже первая часть первой частью мы сейчас поговорим сколько это тачек то Если всё О'кей у нас по раскладке то выкладываются все остальные и только уже на этапе конечной выкладки мы будем мжа а в структура Ну очень важный момент что при росте компаний вообще объектов загрузка одного баннерное достаточно много около 40 минут если мы сейчас когда поговорим о структуре проде прода во время мониторинга будем за этим всем наблюдать или стопить другие какие-то процессы выкладки то это могло занимать порядка ТХ суток при том количестве серверов которые у нас сейчас есть наконец дре года Это было 1350 там серверов то есть хотя там условно говоря в 2 там дем году ковидном когда пошёл ещё уже рост нас было там достаточно меньше это в районе 300 серверов то есть уже произошёл рост когда эти оптимизационные задачи начали решаться и из них 56 серверов уходит на прий 500 серверов в пачке то есть по 500 серверов мы будем расклады и на текущий момент уже 1 планируется Е 200 серверов до конца года и как бы достаточно серьёзный вызов для того чтобы сократить пайплайн выкладки и загрузку одного Демона реализовали рр протокол быстрой загрузки уже готовых данных на загруженной тачке потому что с базы данных это стало про неприлично долготе вею и смотрим как у нас сервис живёт они грузятся по классике с базы данных долго Окей но их немного дальше первая пачка которую мы пинаем на загрузку она начинает грузиться уже р Top с прила и потом дальше уже лавинообразно ну это не лавинообразно это контролируемое естественно за счёт того что у нас есть metastore в это CD и каждый момент загрузки каждая тачка туда анонси сколько с неё сейчас грузится мы поговорим это чуть-чуть подальше и раскладка начинает у нас идти Ну грубо говоря это ладно не лавино веером То бишь друг с друга начинают загружаться все машинки в проде в hcd у нас пишется количество приёмников версия которая будет демо на которой мы сейчас раскладываем а также маска шардирование загрузка таким образом за счёт реализации р Top у нас теперь сократилось до 15 минут Ну это такой худший вариант 15 минут то То есть если у нас компаний мало то мы загрузим уже с быстрой загрузкой за 5 минут а без быстрой загрузки за 15 минут таким образом что у нас получилось что общий пайплайн выкладки теперь занимал у нас максимум один рабочий день Ну если мы не начали катить вечером то Ну там можно сказать технически что это 2 дня Но на самом деле всё равно не более суток Теперь мы выкладываем новую версию на весь прод ориф загрузки подразумевает под собой что необходимо первоначально Загрузи захватить источник для Р Top и захват источника нужен для чего чтобы заблокировать стор мы говорили в самом начале что у нас два стора и один обрабатывает второй получает апдейты и немножко тогда не сказал один важный момент что каждый так называемый swop интервал мы меняем местами указатели на эти сторы соответственно Таким образом мы меняем уже на адены данные с теми которые были и потом применяем апдейты на тот старый стор и вот чтобы данные во время загрузки от начала до конца когда с него грузится другая тачка были консистентные мы блокируем с помощью захвата источника этот стор и с него забираем нужные данные для этого в два этапа всё делается первое формируется параллельным загрузчиком множество запросов то есть всех типов которые поддерживаются для протокола быстрой загрузки и если вдруг что-то пошло не так то мы можем на базу данных и уже дальше загрузиться то есть в любом случае мы загрузим Ну если не упали в корку Вот обработка запросов вее ответов которые пришли происходит на втором этапе там уже конечно с лком будет сложнее На этом этапе чтобы у нас прод не лёг если мы вдруг там всё-таки до прода довезли что-то непонятное то естественно у нас идёт автоматический анализ упавших корок мы сразу поймаем такую ситуацию и будем откатывать источник освобождаются сторы начинают по очереди меняться апдейты начинают влиться и всё красиво Все очень рады очень классный вариант который можно использовать для ваших проектов Когда вы ещё не знаете насколько далеко у вас будет рост условно говоря Но вам нужно себе дать время там на 2Т п там всё зависит от того насколько будет бурный рост у вас потребления памяти или времени загрузки вашего сервиса выбор источника Может быть как автоматический так и ручной автоматически менедж через at CD мы уже немножко про это поговорили а то есть из CD мы берём все анонсы которые были для всего прода Ну условно и фильтруем Какие источники У нас есть с этой версией с этими фичами Ну не фичами а грубо говоря масками и данной роли и статус загружен он или не загружен после чего ранжиру тот который самый свободный и дальше по классике захват загрузка и освобождение и забираем для того чтобы ВС было красиво и чтобы это было не занимало много места параллельно с разработкой orm на c+ Plus разрабатывался формат arbin это бинарный формат для упаковки данных который позволяет у нас из нашей внутренней Умки на плюсах написанной сразу же сериализовать и де сериализовать обекты отличается тем что он поддерживает версию объекта Как вариант обработки То есть у нас есть отдельное всегда поле которое говорит о том что есть дополнительная информация для стерилизатора и дестри затора ну в самом для дестреза Тора то есть простые типы могут ездить без версии а яркий пример применения версии когда мы хотим запаковать какой-то например контейнер то в зависимости от размера данного контейнера мы можем указывать в этом формате Ну либо какой-то Magic который будет говорить как мы будем этот В контейнер паковать распаковывать либо это может быть Ну банально А количество байт на длину данного контейнера А дальше уже в контейнере мы уже следующий указываем размер этого контейнера и данные то есть мы можем маленький контейнеры упаковывать грубо говоря голова формат версия и 1 байт и там до 255 Да объектов длинные можем 4 байта и дальше Сколько нам надо данная задачка очень классно решила вопрос загрузки и выкладки мы смогли теперь катиться не один раз в неделю А до трёх раз в неделю всё в зависимости от того количество Какое количество фичей мы к нам принёс бизнес и мы их начинаем катить следующий момент для который хотелось бы подсветить это как мы ещё больше сокращались бы рассказать что такое таргетинг в общем виде таргетинг это просто а одно или несколько условий по которым будет подбираться реклама То бишь персонализированная либо там по месту то есть яркий пример а геолокация то есть соц дем таргетинги такие как H пол там ну вот всё что с этим связано и вот те те моменты когда то есть мы можем сразу видно что у нас таргетинги бывают как попадание в критерии так и не попадание в критерий то есть негативный таргетинги это обычная инверсия от может быть просто инвертированный таргетинг А может задаваться как отдельный негативный таргетинг и дальше комбинироваться с правилами похожими как в в языках программирования типы таргетингов бывают ремаркетинга которые в прошлом произошли а фильтры которые фильтруются для банеров на площадках иерархические которые собираются и привязываются к компании либо к пакету либо мультан которые собираются могут обозначать разные сущности но собираться в битке теперь для чего это было нужно что мы сразу видим что у нас есть варианты таргетингов которые будут применяться последовательности для компаний а есть которые мы можем убедить в обычные индексы в тма и соответственно можем на этом тоже неплохо сэкономить поэтому баннерный демон был поделен потихонечку Мы хотим вс-таки Малик распиливать и разъезжаться на маленькие де манки там как в рамках конечно разумного и выделили теперь из функционал построения бит масок и индексов выделили в отдельный баннерный так называемый индексный баннерный стал и поисковый баннерный пайплайн обработки запроса в баннерной демоне стал немножко теперь отличаться то есть мы также обрабатываем запрос а дальше мы начинаем уже ходить за профилем во внешние сервисы а также в тарантулы А ну в тарантулы скажу сразу мы ходили изначально то есть чтобы потом готовить профиль но теперь профиль у нас можно готовить в отдельном сервисе потом мы их Можем сходить в индексной взять тма и после этого уже применять индексные таргетинги итеративный таргетинги аукцион и отрендерить результат соответственно ключевым моментом здесь оптимизации будет И вообще уже потихонечку распиливание это поход во внешний сервис за тем чтобы отфильтровать компании и обработка в индексной демоне это обычный API который уже в зависимости от запроса выдаёт тма тех адиши компаний по которым у нас которые проходят индексные таргетинги для данного запроса по сом таргетингам А что позволил нам вытащить индексный индексный нам дал скроить от 50 до 100 ГБ памяти в зависимости от толщины индекса то есть количество компаний которые у нас на этой роли есть это очень клёвый результат плюс один индексный У нас баннерный оди к 12 с поисковыми то есть выделяем отдельную роль То есть за счёт того что роли уже есть мы можем индексный затащить в отдельные роли и распределять и балансировать уже трафик по индексным теперь момент а что же с РПС с РПС Это работа которая позволила нам за счёт кэширования позволит нам снизить утилизацию пу каширования во-первых у нас всё-таки могут Несмотря на то что есть механизмы троттлинга у рекламной сети могут приходить из внутренних ресурсов повторные запросы перезаказать и профиль пользователя тоже достаточно статичен Это важный тоже момент он меняется не там в секундах а бывает там минутах часах днях поэтому можно Каширова и по этому критерию а также параметры шаблонов меняются достаточно редко То есть это то то уже куда не лезет нени пользователь это уже под капотом внутренней системы соответственно если это рассматривать в виде такой схемы в виде вороночник уже применения индексов и походов в профиле и тарантулы мы уже получаем 10.000 компаний после этого А уже начинается тяжёлая работа внутри мозгов баннерное мы делим на разные подзадач и делим по количеству компаний на эту каждую подзадач и применяем итеративный уже таргетинги взвешивания в результате чего у нас на подбор на финальный аукцион остаётся там порядка 500 компаний где мы уже проведём финальные проверки сайд эффекты и вот на данном моменте мы можем этот запрос положить в кэш соответственно дальше нам в кэш не надо ходить э вернее не надо на Повторный запрос не надо ходить и делать те же самой процедуры мы можем сходить в локальный кэш внутри каждого баннерное и А где ключом выступает Hit пад Ну пад ID - это место размещения и хэш реквест значением будет у нас как раз список этих Адик которые мы в конце на фи на финале уже сохранили в кэш и на стадии Final мы их просто положили и возьмём уже пропустив поход в индексный и применение индексных таргетингов инвалидация кэша производит либо пору либо по тлю либо если у нас фале произошли какие-то там другие процедурки есть критерии соответственно теперь отсе вшм те же самые 50к на профиле до тарантулы мы сходим чтобы получить профиль а уже в из индексного нам не надо ходить и мы уже сразу пропустим половину селекта останется сразу из кша взять готовые наши 500 банеров финальные проверки сайд эффекты и какой мы получили Профит с 300 миллисекунд в худшем случае мы уходим для худшего случая в 90 миллисекунд потому что всё-таки подбор рекламы - Это с очень таким маленьким Лан должна быть система и цпу сократилась тоже утилизация практически в три раза для каждой роли мы могли можем сейчас прикинуть хирейд то есть для внутренних площадок это хрей на ролях порядка 60% а для ВК это может быть 40 50% Ну и уже для общего трафика либо внешних систем это порядка 20% что тоже достаточно хороший результат на котором мы можем скроить количество Ну вообще утилизацию цпу что в итоге мы получили мы получили Таких хороших э четыре кейса которые дали нам сдержать нагрузку и не деградировать и дали нам хорошее время на дальнейшее развитие проекта это гибкость за счёт настройки по ролям и выкладки экспериментов и экспериментов либо льных экспериментов Чего угодно реализован хороший механизм для сдерживания потребления памяти в случае чего и вообще мы можем прогнозировать в зависимости от запусков есть например Тачки к нам не успевают заезжать мы сразу уже готовимся как мы будем с этим жить загрузка которую можно да уже развивать на будущее и очень хорошо снижает время выкладки и выделение индексных то есть Старт распиливание Монолита это тоже классный шаг потому что многие компании долго-долго мучаются перед тем как начинаем пили на микросервисы за счёт чего мы выиграли неплохо памяти и снизили утилизацию пу за сч чего Требования по ка СТ по надёжности чтобы мы вс-таки держали определённые там X2 X3 нагрузки и утилизацию цпу естественно надо потихонечку снижать какие у нас дальше перспективы а перспективы что хотим реализовывать уже паттерн cdc потихонечку то поделить апдейты на быстрые и медленные чтобы п оставить только в контуре подготовки данных а быстрые данные уже могли ходить например там по кафке загрузку перевести на СРО чтобы прям дово медленные данные снапшота и подниматься там или не готовить эти данные на каждом из баннерный и готовить только на определённом кластере потихонечку выносить mail логику аукцион и другие кусочки в отдельные сервисы Ну естественно в пределах разумного и шардирование интересные кейсы Где вы можете применить данные подходы алгоритмы то милости просим для этого мы и рассказывали а на этом всё Мы очень рады вам всё это дело рассказать приглашаем всех на стенд у нас кстати очень интересный стенд ВК и за самый классный вопрос Мы даже подарим классную игрушку можно показать Кстати какую игрушку два вопроса за два вопроса хороших нет погоди не путай меня за два вопроса точнее два приза у нас от тебя и ещё от лода класс То есть получается игрушка И матрёшка вот попробуй теперь выбрать из вопросов которые будут Выбирай показывай Куда микрофон здравствуйте Да спасибо за доклад во-первых у меня такой Чуть более продуктовый наверное вопрос послед сть сши Вот это хеширование приводит к перекрута и ну и насколько это вообще как бы бороться с крутами стоит дешевле чем не хаширова так Ну давайте смотреть во-первых мы говорим что всё-таки кэш у нас достаточно краткосрочный и перекрута у нас могут тут в данном случае появится когда запросы уже немножко разнесу что мы отдадим там в течение Ну вот возьмём среднее средне в рекламных системах реакция пользователя на пока Ну целевого события после выдачи рекламы в районе одной секунды в пределах одной секунды что мы отдадим там 10 пикселей и всего остального то есть мы не покажем как раз лишнюю лишний пиксель Ага ну окей ладно понял да то есть по идее не будет у нас там Тоти ес ну пому если как бы косми смотреть возможно будет нет если много много систем если за Каширова на многих но мы же вот как раз кашированный запрос у нас он почитается как один а ну окей понятно Да Всё тогда о Понятно Так похоже мне дали микрофон Можно говорить Я просто не по туну сбое у меня сначала Маленький вопрос компания - это равно реклама То есть то что я кликнуть сайт зайду и мне выдаст баннер А ну условно говоря компания - это бюджет рекламы что мы хотим показать и набор критериев по которым мы хотим эту рекламу Ну то есть определяем целевую аудиторию на которую будет она расходиться А хорошо а можно вот слайд с кэшем так вот то есть наверное где вороночник это 50к компании которые могут быть подобраны изначально которые попадают в воронку подбора и соответственно дальше мы уже в зависимости то есть от активной компании которые крутятся условно говоря Да это вот активные компании на сервере То есть их естественно там не 50к Это мы так для примера говорим дальше из этих активных компаний нужно подобрать в зависимости от того запроса который пришёл профиль этого пользователя его собрать и применить таргетинги для этих компаний и уже сделать отсев который выдаст нам условно там 10 банеров и 10 банеров мы уже для там 10 компаний там или там пяти компаний за того ну Будем считать что одна компания один баннер Ну так чтобы было проще то так и отдадим То есть это чтобы из вот этого ПК добраться там до де или до одного допустим до дети которые будут участвовать в аукционе за в аукционе будет там 500 Ага То есть в аукцион побольше попадут ну или там меньше в зависимости от того как на хорошо вот где будут использоваться данные которые вот ну мы в итоге получим вот после этого всего прогона то есть это 10 баннеров поте Ага это финал Да я понял это рассчитывается для одного конкретного пользователя что допустим он зашёл на 10 сайтов Да И вот конкретно для него вот тут как система настроена Какая интеграция То есть это может быть один пользователь который склит ленту ему надо показать в пределах скро баннера это может быть там что угодно это может быть внешний аукцион которому нужно вылить набор банеров И дальше по определённым по определённому запросу и мы их отдадим они уже будут с ним работать спасибо понятно Ух сколько рук Ну давайте а вот с этой стороны Давайте потом девушке дадим Спасибо Спасибо большое за доклад впечатляющий движок У меня вопрос сза протестировано несколько версий То есть это несколько версий движка по машинам делится или у вас внутри кода свич на альтернативную логику я понял Значит у нас реализована своя админка экспериментов Где мы можем по ролям по тачкам раскладывать довозит опции конфига и непосредственно на процент тачек пароли либо на конкретные Тачки можем они уже внутри кода должны быть изначально заложены Ну либо если это приходится то там отдельный мир в который подгружается модели и уже с фичами там работает то есть это как бы к барном уже не относится так да вот девушки Здравствуйте спасибо за доклад в продолжении вашего ответа проде используете вы их в подборе баров Во вот это таргетингам и так далее И если используете Есть ли какие-то подходы к ускорению их использования для того чтобы они не аффективным проектами и вопросы внутренности меля то есть ML используется в процессе подбора Это точно и он используется в два этапа как минимум Ну то есть это первый этап - это ранг чтобы ещ больше уменьшить А дальше уже во время подбора А какие алгоритмы там по улучшению эффективности подбора как это всё устроено это есть отдельная команда которая занимается мелем для баннерное эти ребята ну то есть в плане инфраструктуры вы его используете просто как запрос ответ и здесь никакой оптимизации нет в его использовании именно не не запрос отвес ходит прям в баннерный и для каждого запроса есть ну как бы своё крутится то есть in Memory там в пайплайн короче ль есть и есть проект чтобы частично уже ходится и во внешний сервис меля чтобы можно было и сделать единый ль через который уже ещ ещё лучше улучшить подбор ну а там дальше в зависимости от того какие у нас будет Требования по оттуда мы будем уже плясать либо будем использовать внутренний движок либо ходить во внешний Спасибо за ответ Так вот девушка в белой футболке Привет Спасибо за доклад У меня скорее вопрос уточнения А первое я правильно понимаю вот загрузка баннерное что это идёт обновление компании именно то есть не обновление сервиса своего пло и выгод Вот именно это именно обновление сервиса Ну то есть это холодный старт условно говоря холодный старт с базы Горячий старт То есть это вот прям деплой новой версии получается у вас компании они вместе с риом Вот именно за нет компании тянутся с базы И дальше Потом все обновления ещё по псву доезжают нон-стопом и когда мы грузимся у нас в соответствии с протоколом Lips у нас всегда есть последний тит в логе с которого мы стопнули и мы с него потом дорум эти апдейты то есть они не потеряются апдейты которые были Нет мне просто интересно у вас получается компании вот не автоматически сразу вот обновляются когда кто-то там из аналитиков или кто бизнесом занимается обновляет компанию У вас компания обновляются вместе с загрузкой банер те нет автоматически они доедут по Ну как быстро как медленные условно говоря данные по механизмам репликации Ага а вот по шардирование хотела бы уточнить ещё получается Суть в том что вы трафик делите на некие и потом под это подстраивать инфраструктуру именно Да именно железо Я так понимаю ну по сути да да А можно ещё вот про кэш запросов можно уточнить что конкретно Вот именно кэшируется то есть именно запросы в БД именно запросы к нам на подбор То есть когда вот условно говоря у вас есть телефон и вы стае ленту и вы вот прям делаете вот так два ра естен могут запросить для своего внутреннего к кэша сделать два запроса одновременно практически и соответственно нас два раза грузит второй раз в холостую мы можем второй скипнуть и с первого уже всё отдать или вы когда крутанула Ну по определённым критериям проходит под кэширование тогда если мы его закрут дадим одно и тоже но уже площадка из этих дети например с нас спрашивают каждый раз по 10 нам что спрашивать с нас один что 10 одно и тоже с нас могут спросить 10 три раза спросить и мы отдадим эти 10 и уже с них они пароля все 10 покажут зависит от того как есть на коротком участке уже целевое действие будет от первого от первой выдачи от первого расчёта Понятно спасибо Так вот наверное тут недалеко и потом на ту сторону Да вот у меня такой вопрос У вас приходит запрос и он подбирает там 500 компаний которые кэшируется да для конкретно вот пользователя ашки Да компании Как долго кэш этот живёт он недолго живёт потому что у нас достаточно быстрое изменения есть а ну я сейчас так не скажу но в районе там минуты двух вот так Ага Ну вопрос-то вот в чём то есть он кэшируется в это время произошло вот изменение компаний и всё-таки всё инвалидация кэш из кэша Она инвалид во время апдейта то есть если мы в кэше такие изменения есть у нас же там вот как раз мы говорили что может быть на инвалидизация кэша по изменениям изменения приходят они по псву за детектив эти запросы и мы их инвалиди всё то есть дальше подбор будет заново всё то есть такая ситуация что нам пришёл пользователь и что-то ему не ту отдали она исключена такой задачи у вас не было То есть у вас пользователю ушёл ответ подобрался какая-то реклама и Вы точно можете сказать как бы состояние базы то есть по каким компаниям конкретно производился подбор или или такой нету задач у вас ещё раз Ну у вас там в базе 50.000 компаний Да вот к вам прилетает запрос на основе этих вот 50.000 произвели все все фильтры и вернули Да вот вы по этому ответу как бы можете сказать в каком состоянии то есть база была то есть какие там значения были или или такой нету задачи чтобы понять какие там фильтры были Почему такой запрос отдался Нет смотрите А значит есть механизмы логирования Ну как сказать логирование но это логирование по запросу то есть грубо говоря когда в саппорте могут быть вопросы или что-то ещё то Ну например где-то есть какой-то какая-то проблема там компания не крутится Да условно то можно сделать тестовый запрос который сохранит Лог прохождения фильтров и посмотреть по каком Вот например компания не крутится обычный кейс службы поддержки если он до нас добрался и посмотрели что в компании там ну там визуально что ну как бы всё ок И пришли к разрабом тогда мы можем сделать дбай запрос который соберёт вот эту последовательность фильтров чего мы проходили посмотрим почему это может быть не так то есть какие критерии какие-то какая-то бизнес логика там настроено неправильно и уже Вторая линия поддержки этот вопрос исправит то есть но на постоянку такой де Бах никакой не идёт то есть и никакой аналитику bigdata это всё ни в коем случае никуда не идёт Спасибо Давайте молодой человек Привет можете пару кликов сделать вперёд там где будет время работы Да вот тут вы сказали что худшее время упало с 300 миллисекунд до 90 да А можете объяснить почему может я не понимаю потому что по идее худшее время - это как раз когда кэш пустой и оно должно оставаться таким же как и до того как кэш внедрили Нет в смысле вот смотрите когда мы у нас есть запрос и когда кэш полный Мы не ходили в индексный и не делали взвешивание то есть ЕС тут мы в худшем случае кэш пустой и мы не пошли Никуда мы сходили в индексный мы пере применили маску перебрали Все итеративный таргетинги После этого провели вот э фильтрацию и у нас больше процедур есть в пайплайн Когда у нас был в кэше приходит запрос мы смотрим Что есть в кэш мы не идём во внешний сервис мы не применяем первичную всю фильтрацию часть стадии селекта и соответственно берём уже заготовленные ашки готовые дишни то есть считай треть палана даже бо 2/3 пайплайн это со всеми другими улучшениями о которых вы рассказывали ещ до этих с А если вместе с кэшем там 09 или там 099 кванти Ну насколько выигрыш в таком плане смотрите средний у нас среднее время ответа где-то в районе 50 миллисекунд но на тяжёлых ролях А мы можем ходить за 100 мсн то есть в зависимости от того какая роль А если было у нас требование 100 миллисекунд вообще итоговое то там в районе 30 миллисекунд 50 миллисекунд ответ Если у нас есть требования повыше там 400 миллисекунд там то есть тяжёлый подбор тяжёлая компания много таргетингов то там мы укладываемся в районе 200 МС то есть всегда у нас есть запас в районе X2 до края требований Угу я понял Спасибо Артём можем взять один вопрос ещё так Ну тут вот молодой человек хотел очень давайте мы в кулуарах вы уже зададите Вопрос вот молодой человек хотел он уж руку держить Достаточно давно Спасибо за доклад У меня вопрос про фильтрацию и сегменты баннеров ты упоминал что у тебя все баннеры построены Так что это какое-то бинарное типа да нет отнесение к сегменту и вся логика там на принадлежности к каким-то Да нет сегментам нет нет нет нет нет значит на тему сегментов это отдельная история как раз Вот ремаркетинг и таргетинги они построены на сегментах то есть заводится сегмент и уже от сегмента появляются иерархические определённые структуры по цепочке которые можно настроить на компанию можно настроить на несколько сегментов то есть вот это как раз итеративный таргетинги а Да нет это индексные ну и индексные плюс те которые подходят без профиля то есть которые идут в сом я понял чуть-чуть углублять этот вопрос нет ли продуктовой Ну в общем у меня был вопрос в основном в том что вот есть да нет сегменты А нет ли целесообразности использовать какую-то там диапазон величины Я не знаю ну такие тоже есть такие таргетинги есть но они идут Как бинарные всё равно к вам в сервис нетт есть прям Ну как сказать критерии которые задаются и диапазонами и Нама Ну то есть разные есть варианты и стринге то есть бывают совершенно различные варианты таргетингов Спасибо так и за доклад можно проголосовать по Q коду он достаточно Давно висит Так Спасибо Артём Смотри будь прям ответственен потому что у нас два приза сегодня и один приз от ВК да И насколько я помню это вообще супе ограниченный жаби дроп вкш най для разрабов в среду и вот это вот всё А ещё для этой жабы нужно сшить Штаны а его обладателем куча мемов короче Вокруг этой жабы сложилась в экосистеме в ВК а Поэтому да выбирай ты помнишь очень много вопросов было да да я честно говоря я уже первый забыл А ну а что делать Не ну я думаю что жабу мы подарим вот молодому человеку в Зелёной футболке не Вот который в Зелёной футболке вот а матрёшку Честно говоря мне очень понравился вопрос про сегменты Хотя он продуктовый а не инфраструктурный но как бы вот молодому человеку последнему который докладывал вернее задавал вопрос Спасибо большое"
}