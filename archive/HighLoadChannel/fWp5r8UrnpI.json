{
  "video_id": "fWp5r8UrnpI",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Всем добрый день Уважаемые коллеги собственно говоря сегодня я буду рассказывать про языковые модели и в целом про NLP ввиду того что эта область мне близка я NLP reser и там на своём проекте вт1 Я как раз отвечаю за языковые модели А вкратце вы можете посмотреть на план который предстоит он достаточно длинный Мы пробежимся через основы вплоть до современных архитектур и те направления которые на мой взгляд будут развиваться в ближайшее время если кратко Что такое обработка естественного языка там А у нас есть некий текст и мы должны какие-то задачи решить это либо выделение сущности нер либо задачи там генерации когда по предыдущим токенам словам Суслова мы предсказываем следующее слово а либо классификации и так далее ну неважно всё что касается текста и задач перевод текста в какие-то понятные вектора для модели это NLP раньше как примерно работали Ну если мы говорим про классический перевод У нас есть какая-то большая база данных переводов и мы что делаем мы набираем статистику следующего слова от предыдущего используем тере B и за счёт этого предсказываем но я думаю многие помнят какой был переводчик года до 2011 и он работал ну не сказать что прям хорошо и начиная с Ох годов крупные компании там Google Яндекс начали использовать именно языковые модели для перевода и первыми модели были это рекуррентные есть разные виды там классическая рекуррентная сеть СТМ Гру соответственно ячейки Вы можете видеть на слайде и это был первый важный прорыв но у них была одна большая важная проблема связанная с тем что они забывают то есть они не улавливать длинный контекст это была большая проблема соответственно в году четырнадцатом пятнадцатом появился механизм внимания и на самом деле статья внимание всё что вам нужно от 17 года от Google они переиспользовать механизм Анна потому что до этого был двунаправленный lstm encoder и там однонаправленный декодер в механизмы перевода либо текст to текст в задачах и идея В чём что А давайте мы построим специальную функцию которая будет предсказывать некий скор относительно наших токе и это позволило бороться соответственно сбываемость не до конца Но кривая забывает начала расти убывать Прошу прощения и в знаменитой статье внимание всё что вам нужно впервые было представлено архитектура трансформер которая собственно говоря сейчас и породила там все современные большие языковые модели Вот и Ключевая идея в том что высокий параллелизм то есть мы можем множить блоки сколько угодно и самое главное там внимание и многоголового внимание отличие самого внимания от многоголового что естественным образом возникает гипотеза о том что а возможно разные внимания должны отвечать за разные признаки и Давайте вот у нас приходит какой-то большой Вектор мы его Разобьём на субб и отдельно механизм внимания будет а Подробно как работает внимание если кто не знаком Мы после доклада можем обсудить потому что там специфическое умножение который позволяет это предсказывать Но следующее что родилось очень важное А у Гугла механизм encoder декодера и очень сложно его обучать и так далее и openi в своё время выпустили gpt 1 они говорят слушайте Ну энкодер нам не нужен Давайте оставим только декодер там хитрым образом мы треугольную маску внимание добавим А и всё будет работать и на самом деле а Вот это развитие gpt 1 это было на хайпе от статьи 2017 года которая в своё время переиспользовать вообще механизма других разработчиков и gpt 1 особо не взлетела она была всего порядка там 100 млн параметров она уже более-менее что-то генерированию вышло две очень важных работы это gpt2 и gpt-3 а которые по сути ключевым образом отличались только числом параметром там немного с нормировки изменили архитектуру но ключевых не было изменений И вот gt3 уже обладая 175 миллиардами параметров Решало очень много задач и очень хорошо однако мы сейчас немного вернёмся на пару лет назад от gp3 была архитектура Т5 вот Ну на самом деле архитектурно это всё тот же ванильный трансформер 201 года од у разработчиков в вопрос у нас в тексте очень много различных задач мы можем там требовать саммари зации мы можем требовать перевод мы можем куча всего если до этого мы тренировали модели на разные домены Либо мы такие вот тебе технический токен который отвечает за задачу естественным образом возникает вопрос Если человек понимает что такое А какая именно задача лежит в основе а вопроса то почему бы не попробовать модель так потренировать Тем самым появилась архитектура Т5 э Где просто тренировали на многих задачах и модель сама в контексте понимает что за задачи требуется Исходя из этого родилось очень важное направление которое определяет на самом деле что не только архитектуру мы должны каким-то образом подобрать но и как мы обучаем не менее важно а соответственно появились подходы зшт шт флан и так далее идея В чём а zer Shot - это когда мы не даём подсказки а просто в лоб спрашиваем модель там как решить эту задачу и она должна ответить и оцениваем Однако оказалось если мы в промтек примеру хотя бы одного решения это One SH Learning то модель существенно лучше начинает отвечать то есть она по контексту начинает Ну условно говоря понимать что от неё хотят Вот и Исходя из этого Получилось что мы можем добавлять примеры и модели существенно лучше справляется А дальше появилась идея флан что мы не просто Аа спрашиваем модель Мы ещё и объясняем что мы от неё хотим подробно в разных задачах и вот это всё привело к тому что появилось gpt 3,5 идея В чём А изначально вышла статья instruct gpt где а показали что если мы даём очень подробную инструкцию модели что мы от неё хотим и на этом её обучаем то у неё хороший высокий результат А дальше логичное развитие возникло есть отдельная область машинного обучения которая называется обучение с подкреплением вот а где мы можем тренировать модели когда мы не можем однозначно сказать Какой вывод лучше но можем каким-то образом сопоставить результат модели с неким скором то есть мы можем оценить насколько это хорошо либо плохо мало того уже ну там А по-моему в четырнадцатом или тринадцатых годах Ганы появились когда одна моделька тренирует другую и логичное продолжение было возможно ли построить какой-то механизм такой чтобы он обучал нашу модель то есть делал Всё лучше и лучше на самом деле не Open предложил этот подход Несмотря на то что у них очень много прорывных работ в области ля до них вышла статья первая по р с фидбека от человека но они взяли есть арий позвони без верных ответов и идея в ЧМ мы берм нашу какую-то мку мы берм датасет размеченный данных плохой хороший ответ грубо говоря если упрощенно берём вторую модельку которая тренируется предсказать А вот этот ответ хороший либо плохой из того что моделька сгенерировал и дальше мы включаем классический механизм хорошо изученный имы тренируем языковую модель больших человеческих усилий И на самом деле что касается железа тоже немного потрачено мы можем модель обучить таким образом чтобы она максимально правдоподобно Вела там либо диалог либо Решала какую-то задачу Да это не исключает проблем галлюцинации и так далее но однако модель начинает существенно лучше генерировать следующим шагом марта выла gt4 ровным см ничего кроме того что она мультимодальная вот на самом деле если вы Изучите там технический репорт то кроме того насколько там высокий скоро в разных задачах про неё больше ничего не известно и не раскрывается до сих пор никакая информация Хотя недавно в октябре был новый релиз до сих пор непонятно Кроме того что она мультимодальная Ну мультимодальной честности ради опять же не Open впервые вела Вот Но вот как так это было немного важной теории впереди для того чтобы мы сейчас поговорили о том что мы вообще практически решаем и как это можно хитрым образом решить сейчас существует большое количество открытых больших языковых моделей которые мы можем использовать в своих задачах даже с коммерциализацией и так далее но Понятное дело что они тренировались на других доменах соответственно нам нужно е как-то доочить это называется и какие есть хитрые подходы а к сожалению обучение моделей особенно больших языковых моделей штука вычислительное то есть и очень много сил сейчас тратится для того чтобы А с минимальным использованием ресурсов мы могли дооб учить нашу модель чтобы она Решала конкретно наши задачи и вот первое что расскажу это как можно экономить ресурсы именно в момент обучения соответственно есть разные подходы связанные с даб учением смотрите есть работа которые показывают сильную зависимость условной разумности языковых моделей от числа параметров и сейчас некоторые минимальный Эталон - это порядка 7 миллиардов параметров это вот там Лама Куна альпака Ну там большое семейство на самом деле моделей Мистраль тот же самый который Недавно вышел но 7 млр параметров если мы будем хранить в весах в 32 бита это дорого мало того если мы ещё вдобавок начнём её обучать то нам на каждый вес нужно хранить состояние нужно хранить значение градиента если мы используем например Adam либо Adam W нам ещё нужно хранить а момент и дисперсию соответственно нашего оптимизатора и просто на каждый параметр у нас получается там от пяти до семи параметров сверху надо хранить в зависимости от типа обучения соответственно видео оперативно память начинает очень быстро заканчиваться а как Но мы понимаем что у нас-то модель обучалась на широком домене и нам совсем немного её надо как-то поправить поэтому родилось условно говоря три главных направления это адаптеры lora и соответственно прек Learning и идея В чём а давайте мы разным способом добавим маленькие части новые и будем обучать только их грубо говоря под модель будет править ответы большой модель чтобы полностью отвечать нашему домену задач Вот И тем самым мы можем обучая всего примерно 10-12 прото от общего числа параметров соответственно мы сильно экономим это в видеопамяти но решаем наши задачи да Мы конечно на ифин потом проигрываем потому что в конечном счёте число параметров возникает увеличиваются Однако это существенно позволяет сократить трудозатраты в обучении более того а были даже теоретические работы показывающие что на самом деле Вот это прироста там в 10-12 про новых параметров достаточно чтобы перенести с одного домена на другой Вот соответственно более детально опять же мы можем обсудить В дискуссионной части После доклада следующий вариант экономии видео ресурсов опять же мы говорили что там Фло 32 бита - это дорого а мало того там куда и тензорные там ядра позволяют определенные вычисления там в Половин точности вести Однако возникает вопрос А можем ли мы ещё дальше пойти и оказывается можем есть различные варианты квантизация мы можем хитрым образом наши матрицы по сути Ну она сетка это большие матрицы которые умножаются пересчитать в целые числа е с меньшей точностью Так что результ умножения матри поэтому там появилась квантизация сначала вн8 бит потом в 4 сейчас даже есть одно битная квантизация Понятное дело что с каждым этапом мы немного теряем в точности но из-за того что сети на самом деле современные там ну не только современная архитектура глубоких сетей не требует абсолютной точности в вычислениях что если мы немного изменим веса результат чаще всего тоже не сильно меняется Вот и за сч вот этого мы можем уменьшить разрядность наших данных именно вот в битовой части и там на слайде есть Вот пример когда мы flot Point переводим сопоставляя матрицу целых чисел и ну здесь можно просто в голове прикинуть Если мы с 32 бит переходим до че бита соответственно Мы в восемь раз экономим видеопамять И на самом деле это вот очень важно момент потому что там графические вычислители с большим объёмом видео оперативной памяти они достаточно дорогие То есть если пользовательские там видеокарты условно доступны и самый большой объём - это у 490 24 ГБ Если мы говорим от NVIDIA но серверные карты там а6000 48 ГБ вот резко есть ещё А100 h100 соответственно там 80 Гб а мы понимаем что современные вычисления - это классическая всё-таки архитектура фон Неймана что есть вычислитель есть память И если мы будем там либо с жёсткого диска либо с оперативной памяти мы начнём Ну перекачивать веса либо какие-то элементы вычислений мы начнём терять время на именно переносе данных а к сожалению большие языковые модели чаще всего учится долго и за счёт этого мы можем просто много потерять Окей идём дальше что ещё может позволить сэкономить нам вычисление либо ускорить вычисление соответственно есть такой очень важный момент механизм внимания о котором мы уже говорили ранее а он когда-то был реализован понятное дело мы реализуем это всё в питоне есть фреймворки которые дружат с кудой Однако как оказалось алгоритм вычисления написан не совсем ну максимально оптимально соответственно переписали его сделали более оптимально чтобы А хорошо использовать кэш память видеокарты и появился первый ш идея в том что так как когда мы умножаем матрицу у нас много независимых операций и в конце мы потом их складываем А давайте мы Разобьём это всю часть на под части будем хитрым образом хранить в кэше вычислять быстро и к примеру там у нас функция активации софт Мак когда мы экспоненту делим на сумму экспонент оказывается нам не обязательно так-то прям дожидаться результат всего мы можем посчитать часть а потом суммировать и вот это позволило родиться Flash Attention 1 сейчас вышел уже Flash Attention 2 он ещё более сильно оптимизирует вычисления ускоряет и банально здесь мы можем получить прирост порядка 30% просто используя хорошо написанный на куда алгоритм вычислении Вот и дальше опять же мы сейчас приведём ряд техник которые позволяют делать более оптимальные вычисления на малых ресурсах вот на уровне пользовательских и там Первое - это аккумуляция градиента которая заключается в следующем во время обучения У нас же есть какой-то пакет вот а пакет этот весь опять же надо хранить в памяти соответственно мы пропорционально пакету грубо говоря нашу модельку будем множить это дорого вот Однако вычислять на Малом пакете данных Мы можем столкнуться с тем что шум градиентного шага будет настолько большой что мы не будем двигаться в сторону там локального экстрема который мы хотим достичь Наша задача во время обучения достичь какой-то точки минимума нашей ошибки и логика такая А давайте мы наш батч Разобьём ещё на более маленькие части будем на них вычислять мы будем складывать после каждого вычисления усреднять наш Градиент И за счёт этого мы избавимся от шума Да у нас не решается проблема связанна со временем потому что во времени этого увеличивается пропорционально тому как мы Разобьём Однако мы можем экономить видео оперативную память есть другой хитрый приём у нас модельки чаще всего большие соответственно на каждом слое нам нужно хранить Градиент и идея в том что А иногда мы можем Градиент заново посчитать И это не так дорого во времени идея в том что когда мы делаем обратное распространение оши вы А давайте мы часть градиентов не будем хранить в памяти а мы как до этого точки дойдём мы заново посчитаем и это позволяет существенно сэкономить видеопамять и во времени только увеличивая примерно там на 30% вычисление также как уже ранее мы упоминали у нас есть возможность использовать специфические вычислители что касается ускорителей N помимо классических кудер тензорные ядра которые позволяет матричное умножение делать эффективно именно на аппаратном уровне используя так называемый смешанную точность это как раз позволяет часть весов хранить в 32 битах часть 16 и это позволяет А быстрее считать потому что меньше битовых операций б соответственно Мы ещё немного экономим видеопамять есть совсем специфические техники которые позволяют обучать на там домашнем грубо говоря на домашнем компьютере что У нас есть большая модель но у нас оперативной памяти Ну как бы чаще всего больше Мало более того она более доступна либо у нас есть быстрые жёсткие диски там 2 либо другие и Давайте вот мы какие-то вычисления когда сделали Мы выгрузить в обычную память и как до востребования будем там хранить мы освободим видеокарту но опять же начнём терять на переносе данных туда-сюда поэтому обучать с нуля там трансформер таким образом неэффективно потому что и так в принципе у нас во времени всё размажет это дорогая сложная операция А если мы ещё таким образом поступим она существенно увеличится Но для дооб учени для тонкой настройки операции периодически подходят и совсем специфические техники связаны как раз Тем А из-за того что а во многом фреймворке писались там для датасаентистом которые не следили в первую очередь за эффективностью вычислений потому что ну у нас задачи другие чтобы удобно было построить модель удобно построить Граф вычислений чтобы по графу вычислений хорошо текли градиенты и так далее то сейчас активно развиваются способы параллелизма различные области и как раз есть тензорный палим когда у нас есть бач длина последовательности внутренние фичи и Давайте мы вот этот тензор будем разбивать на субтельний и есть различные фреймворки которые позволяют делать подобные вычисления и собственно говоря ускорять там тот или иной процесс А это такая около теоретическая часть была практических кейсов следующий кейс о котором я хотел бы сказать это соответственно в компании стала задача Нужно обрабатывать очень много текста вот из интернета вот Понятное дело что языковые модели сейчас очень хорошо с этим справляются никуда отправлять Ну не стоит такой сильно закрытый контур Однако на всех задачах использовать большие языковые модели опять же как мы говорили это дорого Вычислите то есть нужен специфические серве вот если сервера если мы даём большую нагрузку её надо множить А сейчас и в виду санкции сложно строить такие цоды и так далее И какой есть вариант а давайте вот если мы какой-то текст принимаем А давайте и после оценим насколько он хороший вообще подходит нам или нет и оказалось вот эти модельки которые просто говорят ответ есть тут что-то хороши либо нет они сильно проще Ив вычислениях постро сначала ряд моделей которые делают какие-то специфические операции оценивают вообще Стоит ли отдавать этот фрагмент или нет позволило сильно ускорить анализировать Ну просто колоссальное число текстов из интернета и извлекать полезную информацию которую непосредственно нужно заказчику Вот и третий кейс - это соответственно чисто научная работа который сейчас на самом деле многие компании начинают в этом соревноваться языковой модели начали решать задачи которые до этого казалось что они не должны решать оказалось что творческие задачи уже достаточно хорошо решаются то есть пишут стихи Ну там с тем или иным качество рассказы эссе и так далее а Однако если что мы касаемся кода либо там математики то до сих пор они там не особо хорошо Сея показывают соответственно там мною было созда определённая архитектура Трансформера с неким модулем глобальной памяти абстрактной которую на самом деле посмотрел у нейрофизиолог И в скором времени выйдет полноценная статья и моделька выйдет тоже в открытый доступ которая было обучена именно на специфических научных текстах и соответственно на слайде можно увидеть пример что она прямо генерирует используя факты ссылаясь на те или иные работы и ну используя формулы в нормальной кодировке вот собственно говоря В общем если что следить за публикациями и вот этот важный момент что вот этот модуль абстрактной памяти позволил немного побороться с проблемой галлюцинации которой многие сталкиваются те которые Ну используют языковые модели в каких-то задачах где нужна жёсткая фактология вот Какие ещё есть интересные модели Трансформеров Ну мы уже немного коснулись мульда когда говорили про gp4 и сейчас активно развивается что естественно текст конечно хорошо А давайте мы ещё попробуем прикрутить туда изображение звук либо видео и на самом деле Оказалось это идейно несложно вот ну там вычислительное с данными чуть сложнее опять же с набором данных и по сути в ядре лежит та же языковая моделька только для тек и для видео мы используем разные способы представления перевода обычной информации в специфическую понятную для модели вот также одна из интересных архитектур Трансформеров это формер которая позволяет обрабатывать очень большие длины последовательности текста и почему специальная моделька нужна А на самом деле когда мы пробуем обработать большой текст единый единовременно отправляет вам модельку для этого требуется два важных условия первое что моделька должна была учиться на таких длинных текстах второе у нас опять же должно хватить видеопамяти для того чтобы это всё вычис без выгрузки там на цпу либо используя другие приёмы выгрузки И на самом деле не так много моделей которые обрабатывают очень длинные последовательности формер как раз позволяет решить эту задачу опять же вкратце скажем про дополнительные интересные модели это Лама Lama 2 я думаю многие знакомы они сделали важную вещь Мета они добавили ротационный эмбеддинг во-первых а во-вторых специфические функции активации и опять же подробно мы можем обсудить после доклада особенно хитек Мистраль которая всего на 7 млр параметров себя хорошо показывает и там важны особенностью является тот факт как они обучали соответственно там оконное внимание используется опять же есть специфический архитектуры для длинных последовательностей это вот Фоме есть иф опять же когда в принципе бесконечную длину может обрабатывать но опять же сдека также есть интересная архитектура это опять же от Google потому что Google во многом сделали очень важные вещи в мире NLP это смесь экспертов И если бы вот мы говорили про многоголового внимание что каждый голова внимания отвечает за специфический признак который сами обучаются а то для смеси экспертов идея была в следующем А давайте у нас есть вот разные ветви Трансформера и давайте у нас есть некий роутер который принимает решение Какая из ветвей либо пропорционально Какая из ветвей будет сейчас работать а для предсказания того или иного токена и это позволило обучить модельку на 540 млрд параметров Palm и Palm 2 там можно загуглить но она закрытая то есть весов в открытом доступе нет и это тоже один из способ экономии ресурсов во время обучения и использования также отдельно хочу отметить работу которую вышла в этом году а которая называется интегральные нейронные сети а и там очень красивая идея на самом деле возникло что а веса которые вот есть в сети а давайте мы не дискретно будем хранить А может быть попробуем аппроксимировать некоторые непрерывные функции потому что параметры этой функции проще хранить и мало того мы ещё с разным уровнем точности аппроксимации можем делать выкидывать всё больше и больше данных и если верить результатам выводов в работе то там выкидывание даже 40% параметров позволяет сохранить точность и там на мой взгляд это такое очень важное направление когда Там сильная математическая часть переходит в практический ML Потому что сейчас во многом все новые архитектуры принимаются следующим образом что-то придумывается проводится эксперимент он показал себя лучше значит сохраняем И говорим что э хорошая вещь без определённых там доказательств Ну и кратко под конец скажем какие есть На мой взгляд пути развития а соответственно дальше будет развиваться мультимодальной домены дополняют друг друга и подобно человеку что у нас есть зрение у нас есть слух осязание и это позволяет больше объём данных получать от мира соответственно модели по идее примерно также ну Должны развиваться но опять же это такая естественная гипотеза которая возникает изза анализа данных а сейчас также начинает активно развиваться мультиагентной мультиагентной модельке назначаем роли и заставляем их взаимодействовать друг с другом вот и это позволяет частично решить главную проблему современных моделей ту которую сейчас как бы сформулировать современные модели не умеют планировать в отличие от естественного интеллекта Когда мы можем построить строгий план исследовать ему современная архитектура не умеет так делать Вот мультиагентной направление математики которое называется статистическая машина обучения Где используется формализм функционального анализа и других подходов которые там позволят доказывать вещи и во многом мировое там сообщество ждёт определённых математических прорыв которые хотя бы позволит решить Какое оптимальное соотношение ширины и Глубины Трансформера для обучения потому что до сих пор это на интуитивном уровне по эксперименту проводится и решается как будет именно выглядеть трансформер опять же сейчас активно развиваются нейроны вычислители которые Базирон на импульсных нейронных сетях и возможно в ближайшем времени Появится новая архитектура опять же скопировано с естественного интеллекта либо что-то подобное которое позволит улучшить современные архитектуры вот также активно развивается использование реальных биологических систем в качестве вычислителе и не так давно выходила работа Когда использовали реальные клетки мозга обучали с помощью обучения с подкреплением играть в пинг-понг и вот оказалось что это работает и а достаточно хорошо а тут же важно что там к примеру человеческий мозг Он потребляет не так много энергии там по-моему порядка 30 ВТ Однако делает колоссальное число операций в секунду и это очень энергоэффективный вычислитель Вот опять же как мы уже говорили что активно развивается направления связанное с экономией ресурсов и там главная цель мирового сообщества наверное дата саентистом компьютере на доступном железе без специфических серверов и обучать в том числе Ну и возможно появится какой-то принципиально новый подход поэтому условно названо это инновациями и позволит там сделать какой-то определённый скачок в сторону там либо общего интеллекта либо качества современных вычислительных архитектур Ну собственно говоря у меня всё вот попрошу там отсканировать QR код и поставить оценку там жду вопросов Спасибо Саша очень интересный обзорный доклад у нас будет целых четыре приза за лучшие вопросы поэтому Давайте подготовьтесь добрый день Добрый день Але Спасибо за доклад Иванов Алексей два вопроса если Разрешите первый Вот вы перечислили тренды и всё-таки Как вы считаете вот GT5 он на что будет похож то есть вот он вки бизнес какая-то модель которая для бизнеса используется уже по большему счёту вот какой основной тренд который они себе возьмут Слушай Т сложно Ответь на вопрос потому Ниго не4 но gpt 5 на мой взгляд будет ну там логичным продолжением gpt 4 и там 3 она будет ещё более разумной скажем так ну и будет куча дополнительных вещей связанны с генерацией изображения обработкой изображени и так далее Вот они сейчас четвёртую добавили вставку изображений прямо восхитительная вещь Я теперь с ней только скриншотами общаюсь перестал печатать Ну мульти модальность Да так Ну у них там хитры скорее всего опять же сложно сказать как это устроено либо это архитектура react когда вызывается то есть модель сама генерирует вызов AP и обращается к другой модели либо это именно встроено вот угу понятно спасибо И второй вопрос всё-таки вот для домашнего использования вот хотел бы я поиграться с этими Лель модемами Есть ли сейчас какая-то модель которую вы можете порекомендовать вот поставить дома поиграться или это вообще невозможно А да вот есть вот тот же Мистраль Лама вторая хорошие есть э фреймворк Колос lae который позволяет сильно экономить ресурсы как раз которые всё что мы перечисляли использовать и у них есть свой colosal Chat вот он правда с русским не дружит Но работает великолепно спасибо спасибо Вот там был вопрос по-моему да наверху и ещё вопрос можно задавать в чате зала и под трансляцией есть кнопочка там тоже можно задать вопрос эти вопросы тоже будут озвучены раз раз Александр привет Спасибо за доклад хотел узнать вот вот эти подходы практики используются ли они в реальных продуктах белай и если используются или будут использоваться то Можно ли назвать не знаю топ-3 самых таких топовых бизнесом сможем вот вот это всё увидеть слушайте честности ради как сотруднику Т1 сложно ответить зала прошу проще сам Боше ва используется в реальных бизнес-задачи Ну например там чат Ботом сейчас по-моему никого не удивить дела дискретные чат-боты это сложно потому что инженер должен продумать все вот эти деревья решения и так далее мало того ещё люди не любят следовать им они любят задать вопрос на естественном языке и желая получить ответ как от реального оператора и там вот чат-боты они ну по-моему сейчас почти у всех есть вот экономия ресурсов сейчас очень актуальна особенно благодаря санкциям Вот потому что закупка железа очень сложно технически стала Ну и я не знаю вот что касается обработки текста сейчас по-моему все про это говорят и все пытаются Делать продукты то есть как как минимум очень много решений появляется связанные даже ну вти с помощью в генерации кода То есть если до этого ишки использовали всякие ста чтобы понять вот здесь Ты там скобку не закрыл здесь точку запятой не поставил то сейчас есть помощники кода которые какие-то рутины задачи позволяет Там сгенерировать те там 20-50 строчек кода за 15 секунд это вещи простые понятны как бы а что бы не использовать это сильно оптимизирует время оставляя там разработчикам время думать над теми задачами которые Ну пока не покоряются для языковых моделей спасибо спасибо там вот было а потом в ниже Владимир рубитех У меня два вопроса таких первый такой больше исторический наверное да как повлиял отказ от энкодера в чат гпт на производительность обучения Слушайте а отказ от энкодера тут не то что на производительность повлиял А у Google вышло две статьи Universal Learning ul1 и ul2 они там с разнице в пару лет где они как раз особо рассуждают во-первых Что лучше encoder декодер либо decoder Only архитектура А во-вторых а как правильно энкодер декодер обучать то есть либо мы подаём там первую последовательность в энкодер требуя сгенерировать вторую последовательность декодеры либо опять же мы там через аналог маскирования Там есть денойзер разные способы обучени и здесь никак не получается сойтись когда из одного домена в другой то есть мы там тексту код либо более Ну какие-то подобные задачи да там энкодер декодер Понятно Как обучать специфически на двух существенно разных доменах Ну или переводчик когда с одного языка на другой а но что касается обычной там генерация общей какой-то текста то decoder Only он просто понятнее То есть как его обучать там ты даёшь просто последовательность текста даёшь треугольную маску и всё тебе не надо больше Заботиться об этом и во многом как бы это прорыв был Open что это просто понять не проще второй такой больше наверно технический вопрос микрофон из вы сказали что основная тенденция того что сейчас идёт чтобы запуск Шёл на маломощных так сказать системах А почему не решается задача в коммерческом плане в создании сверхмощных графических так сказать аппаратных средств там условно говоря ту же оперативную память можно 2 ТБ поставить и так далее То есть почему графические ограничения серверных коммерческих видеокарт 48 ГБ не 80 А ну я по слушал 48 которые это боле более популярно так сказать А вот именно там взять терабайт терабайт данных распараллелить на несколько ядер графических не А и В коммерческом плане использовать для обучения такие системы не так честности ради Ну я про это не упомянул вот ну у нас есть условный монополист для щиков в плане видео ускорителе это там NVIDIA и так они представили суперкомпьютер по-моему супер как он супер Хопер кажется назвается называется ну в общем идея В чём Там специальный цпу пушки кото H серверные мало того что они внутри одного кластера Ну вернее одного модуля объединены через специфическую шину которая высокоскоростная то ещё все эти модули соединены оптоволокном Infinity Band который там по-моему 900 Гб в секунду передаёт данных и там как раз вот у них максимально по-моему 256 ячеек объединяется в одну большую видеокарту то есть там получается по-моему 9 50 ТБ видео оперативки то есть ну как бы это разработать Но это недёшево вот это дешевле чем пушные суперкомпьютеры Ну и B h100 и А100 запрещены в поставку в РФ сейчас а тем более такой специфический вычислитель ну нереально достать короче мы его сейчас не увидим Ну не вот тут ещё был вопрос можно ещё один вопрос давай передадим дальше потом в ларах вот молодо человек с блестящей причёской колега Доброе утро Спасибо большо за интересный очень доклад у меня такой вопрос презентации бы сеть Я видел я читал Просто интересно Просто ваша экспертная оценка о том что расширение идт мульти с камер предположим но также и данны из датчиков для того чтобы они учат роботов даных роботов для того чтобы он бы не программировал а именно учится то есть там запоминается Вот соответственно Я так и понимаю там хранятся временные ряды от этих сенсоров и каких-то вот актуаторов вот и даже вели какой-то специальный термин неть котоно хотим подробно что-то рассказать потому что только в новостях вот такие такие-то краткие были упоминания Может быть вы что-то там более подробно можете рассказать Слушайте а вот все все модели которые обрабатывают вот сигналы с реальных датчиков вот есть отдельное направление в ML называется Digital Signal Processing DSP Где используется очень хитрые преобразование фе и отдельные модели действительно там существует и это прямо отдельно большая тема которую мы можем там позже обсудить А ну активно развивается особенно медицина же требует как минимум У нас есть кг Вот и мед карточка тоже разные модальности вот которые позволяют побороть самое главное усталость человека не Я немножко про другое Я понимаю что всякие вот специализированные россети они давно уже существуют и там мы сами там использовали ещё 4 года назад для там таких для расчётов стандартный метод M тоже не не россети просто Вопрос в том что получается Здесь у вас если текст у вас там пространство смыслов дети то токены появляются точками в неком там пространстве смысла в котором вы можете там приобретать векторные какие-то измерения соответственно если у вас там есть текст видео там звук они уже Да сейчас они там сохранены как тоже некие такие пространства соответственно Может ли быть вот более низкие сенсоры там от датчиков и прочее также быть осмыслены условно этими точками также являтся в этом большом пространстве где можно было проводить векторные операции то есть условно Мы же сейчас уже можем сморовоз на текст видео на текст а почему Например последовательности каких-то эмоциональных датчиков напри человека снятых там кг тот же самое Да тоже не наложить на какой-то текст и мы потом текстом могли бы в обратную сторону перевести там понять Вот вот эта часть отвечает там за его такую-то эмоциональность он там не знаю там Гнев Ну какие-то вот такие ассоциативные текстовые вещи то есть не просто специализированные сети Да это понятно там существует А вот именно в большом вот пространстве смыслов которою приобретают именно Ну вот эти ламки которых можно в Едином как бы формате работать не только с текстом видео ну как вот другими формами но и вот более низкими там информационными уровнями типа потоков от сенсоров Нет так это там давайте честность ради то есть любая моделька - Это от поражения из одного пространства в другое вот а там если будет до сет То есть пока я не вижу там проблем обучить и так сделать тем более Ну известные же задачи оценки там качества текста насколько он там негативный позитивный и так далее тем более Ну вот компания любит же автоматически Оцени не вот всякие комментарии отзывы и так далее так что ну выглядит ну вполне Ну как бы реу то чески можно этим заниматься Да просто я вот не очень как бы сам не дентист я проше архитектор Да мне просто интересно вот теоретически в эту схему можно идти или это ну смотрите технически я ограничений не вижу а там зависит от того кто финансирует вас вот Понятно спасибо большое спасибо Вот здесь вопрос Здравствуйте спасибо залили очень крутой доклад Меня зовут зовут ПТ компания русал хотел спросить вопрос про перспективы развития у вас на одном из последних слайдов было написано там про мультимодального агентное А вот в наших практических задачах мы сейчас видим возможность ну скажем так сократить нагрузку на модель путём использования каких-то других приложений которые выполняют простые функции например контекстный поиск и подобные вещи Вот видите ли вы развитие В неком симбиозе существующих технологий и моделей как тоже как некоторые перспективы Слушайте а тут Важно отметить Спасибо это Великолепный вопрос а ЛМ И вообще глубокое обучение и машинное обучение - это не Панацея то есть а есть очень простой пример это авиация есть старый добрый пид-регулятор он великолепно работает он Понятно работает и его заменяет там какой-то сложной моделькой Ну то есть не так но и Как говорится есть микроскоп не надо им гвозди забивать И действительно очень много задач прекрасно решаются классическими алгоритмами просто есть задачи Вот Ну вот как творчество вот там генерации текстов там заменить анров которые будут Ну там грубо говоря анализировать там отзывы на сеть это просто экономически выгоднее точность плюс-минус похожая будет А вот какието специфические вещи Ну там на мой взгляд нет смысла Да Кажется что сейчас просто технологии находится на пике там кривой хайпа по гарднеру вот когда наиболее распространённая везде цитируемые и её пытаются везде всунуть для решения каких-то задач которые на самом деле решаются иначе ну честности ради просто с глубокими сетями пару лет назад Ну не пару лет там в году в четырнадцатом похожая ситуация была но вот что касается симбиоза есть работа называется Фоме и вот как раз идея в том что Ну вот делает плохо сеть А давайте её будем учить делать разные а запросы где она от классических алгоритмов получит ответ А дальше будет генерировать нужным образом Да спасибо спасибо Вот там ещё сзади был вопрос да Александр Спасибо за доклад Меня зовут Артём компания technolog вопрос короткий на самом деле Вот вы перечислили большое количество оных моделей стра и иное количество и мы видим та тенденцию что они сейчас ну все пытаются соревноваться с решениями от Open которые являются сечас со решениями и Как вы считаете Наступит ли такая точка в которой Open Source догонит Open и настанет у них такая вот адекватная конкуренция или же это не наступит и что на это будет влиять так а Вопрос хороший но вот даже качество Ну мы уже вроде Санти математики тут определиться стоит по многим бенчмарка как бы открытой модели обгоняют и gpt 3,5 некоторые gpt 4 поэтому там в некоторых задачах догнали всё хорошо некоторых задачах Да ещё отстают ну я думаю что Вполне догонят опять же А там если мы сравниваем там на 7 млрд параметров модельку с gpt 4 про который непонятно сколько там ну как минимум 3 с поно 175 млрд параметров логично что её аппроксимирующая способность существенно выше вот однако если сравнить там ламу в 70 млрд параметров она уже не сказать что сильно уступает особенно Вторая Лама которая обучалась на большем числе токенов Так что Ну я думаю там всё догнали вот просто у GP у openi есть удобные AP есть вычислительные центры где ты быстро получаешь ответ за Разумное время И это главное достижение как и Chat gpt А там же Революции не было Кроме того что дали понятный интерфейс удобный ещё и бесплатный вот Ну это на мой сугубо взгляд Спасибо Саша надо четыре вопроса выбрать и подарить подарки людям ту вспомнить бы всех первый вопрос Але Да что ему дарим Так у нас есть подарок от организаторов Да вот от организаторов давайте это вот сюда а вот эти вот да два вопроса добрый человек задал к сожалению был так это это туда вот так А вопрос про с роботами очень хороший был как раз я не включил его в рассказ это вот блестящая причёска как у нас с тобой Ну да так и последний Блин я не всех а можете Напомнить руки поднять Кто спрашивал А ну как очень простое вычисление про Пётр Да который кажется да Да правильно я понял да да Всё спасибо Тебе тоже подарки от организаторов конференции Приходи к нам ещё доклад был классный действительно Спасибо большое"
}