{
  "video_id": "JFZt3tAMMOo",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Приветствуем Вадима Привет и друзья у нас сбоку на экранах есть QR код где можете присоединиться к нам онлайн и задать вопросы в чате стесняетесь Почему сказать это устный микрофон Вадим тебе слово Всем привет привет лот Меня зовут Вадим селютин Я старший исследователь компании visionlabs сегодня поделюсь с вами как мы обучаем быстрые А главное точные модели по распознаванию лиц для инфе на мобильных устройствах сегодня в моём докладе Мы сначала кратко посмотрим как вообще формулируются задача по распознаванию лиц далее перейдём к примерам применения конкретным мобильного распознавания лиц затем обсудим мобильные архитектуры которые помогут вам решить эту задачу обсудим Какие проблемы возникают при обучении на действительно больших объёмах данных и затем обсудим как можно ещё выжимать максимум качества при фиксированных данных и архитектуре начнём с формулировки задачи в распознавании лиц Мы обычно разделяем две разные задачи это верификация и идентификация в задаче ци вам на входе Дана пара фотографий людей и вы должны дать ответ один ли человек представлен в этой паре А в задаче идентификации у вас заранее есть какая-то уже сохранённая база людей и вам на вход поступает фото человека Вы должны ответить принадлежит ли он этой базе или нет если ваша база людей фиксировано например вы делаете какую-то проходную в ваш офис у вас фиксированное число сотрудников и вы знаете что никто не добавится и не уйдёт то здесь всё просто вы просто по входной фотографии определяете при помощи вашей модели айдини этого человека и делайте вывод на основании этого айдини То есть например в задаче верификации Вы можете просто сравнить два номера и дать ответ один ли это человек или разные люди Однако в реальной жизни всё обстоит иначе Здесь всё сложнее у нас база людей как правило изменяется сотрудники приходят уходят и мы не можем обучить нашу сетку на всех людях в мире чтобы она всех могла узнать Давайте подумаем как вообще решается эта проблема Здесь нам поможет Такая сущность как дескриптор по входной фотографии Давайте теперь извлекать такой информативный Вектор который назовём дескриптор который по сути будет являться некоторым набором чисел который будет описывать фотографию размер этого вектора может быть например 256 512 или 1024 и теперь ответ уже для задач верификации идентификации мы будем строить на основе не меток не номеров ID а на основе расстояния между извлечённый дескриптора Теперь давайте посмотрим как вообще строится пайплайн по распознаванию лиц сначала лицо на фотографии нужно найти это называется детекция после того как лицо найдено его нужно нормализовать мы его немного разворачиваем в 2D плоскости то есть выравниваем мы его приближаем и обрезаем лишние детали на фоне это делается для того чтобы сети было как можно проще распознать лицо и она не тратила свои репрезентативные способности на фон и повороты лица отлично после такого кропа мы можем теперь извлечь дескриптор при помощи какой-то модели А может быть даже нескольких и последним этапом идёт матчинг дескрипторов между собой когда мы считаем расстояние между двумя дескриптора и сравниваем его с каким-то порогом это может быть например косинусное расстояние или евклидово если это расстояние превышает какой-то порог то мы говорим что два дескриптора далеки друг от друга и это разные люди а если дескрипторы близки то соответственно это один человек теперь обсудим метрики как вообще померить качество модели по распознаванию лиц на примере задачи верификации у нас на входе фото пары людей здесь в зелёные рамки обведено так называемые положительные пары Когда у нас представлен один и тот же человек в этой паре А отрицательные пары красные - это разные люди так вот Давайте введём такое такую метрику как True positive Rate она считается как доля истинно положительных ответов где наша модель сказала что это один человек и была пра к доле К числу всех положительных пар где представлен один человек и соответственно fse это доля числа ответов где наша модель сказала что это один человек и была неправа К числу всех отрицательных пар Однако при измерении уже качества модели мы стараемся оптимизировать значение True POS при каком-то очень маленьком фиксированном значении для того чтобы наша модель была как можно более точной Однако Практически никого не пропускала в базу если он не зарегистрирован Если вы хотите подготовить по-настоящему сильную модель по распознаванию лиц то вы должны стараться поддерживать в балансе три столпа это данные архитектура и функция в качестве архитектуры Вы можете взять в лом глубокого обучения например свёрточные просто чтобы она принимала на вход фотографию и извлекает этот Face anding или дескриптор размера о над во время обучения сети мы по входной фотографии извлекаем дескриптор и пытаемся его классифицировать то есть мы решаем задачу классификации Однако как я сказал во время Ирен мы будем свать п между собой и здесь возникает вопрос как же нам отойти от задачи классификации и перейти к задаче сравнения дескрипторов что мы должны сделать для этого сейчас будет немного математики Здесь нам поможет Лос функция Лос функция - это такая Метрика ошибки модели оптимизируя то есть уменьшая которую мы можем обновлять веса нашей глубокой архитектуры делая е ВС более точной слайде представлен типичный Лос кросс энтропии Давайте посмотрим что здесь есть по обозначениям X - это дескриптор лица который мы извлекли по входной фотографии Y и - это метка его класса Ну по сути номер айдини wgt - это жия колонка обучаемый матрицы весов последнего классификационного слоя модели то есть после свто сети у нас идёт линейный слой который должен дескриптор классифицировать отнести его к какому-то классу и колонку этой матрицы W можно неявно рассматривать как центр класса то есть по сути мы во время решения задачи классификации пытаемся максимально прибить дескриптор к своему центру класса Теперь давайте применим следующий трюк мы спроецировать над экспонентой в терминах скалярного произведения через нормы векторов и косинуса угла между ними далее Далее нормы векторов wgt и X ит приведём к единичной норме а бас просто за нули после такого преобразования у нас Остаётся только косинус и это очень хорошо потому что теперь мы можем сравнивать расстояние между векторами в терминах углов между ними мы можем пойти ещё дальше и ввести усложнение задачи то есть марны м12 и м3 которые появились вот в формуле над экспонентой эти марджи нужны для того чтобы искусственно увеличить расстояние между дескрипторов его в м раз больше чем расстояние между тем же самым дескрипторов и не родными для него центрами классов посмотрим к чему это приводит на игрушечном примере допустим у нас есть два класса жёлтая класс о розовая класс 2 сверху их распределение в в D плоскости А в Нижней строке будет появляться распределение с точки зрения углов между этими классами сейчас видно что при использовании обычного Лоса классы в целом линейно разделим Однако если мы хотим сравнивать именно вектора между ними расстояние между векторами то этого недостаточно Мы хотим чтобы эти облака были более кучми и когда мы нормализуя Лос функцию то есть переходим к косинусу Мы видим что облака сужаются мы этого и добиваемся в общем-то А если мы ещё добавим усложнение задачи и введём марны М1 М2 и м3 то вы видите что расстояние между классами стало велико а сами облака сузились то есть внутри классовое расстояние теперь мало И это очень хорошее свойство для сравнения расстояний дескрипторов разных классов между собой Отлично Теперь вы знаете как формулируется задача по распознаванию лиц теперь можем обсудить т конкретные примеры его применения в терминах мобильных устройств наиболее частым примером использования мобильного распознавания лиц является системой контроля удалённого доступа как я уже рассказывал Например у вас есть какая-то проходная в офис и вы хотите решать задачу идентификации то есть понимать сотрудник есть в базе надо ли его пропускать или нет другим примером является оплата по биометрии у нашей компании visionlabs в этом есть довольно большая экспертиза поскольку мы запустили например оплату по лицу на кассах X5 ril Group или в Московском метрополитене да А теперь поговорим про мобильные архитектуры подходящие для этой задачи тут нам понадобится ввести такое определение как Дева свёртка которая является вообще основным блоком при построении мобильных архитектур на примере трёх канального изображения которое представлено на слайде посмотрим как работает обычная свёртка Она накладывается на это трёхканальный с ним и потом мы агрегирующие свёртки сначала обрабатывает каждый канал Независимо То есть у нас число каналов сохраняется после этой операции и затем применяется свёртка один на один которая агрегирующие гка свёртка о на о первым известным таким примером применения deps свёртки является архитектура Mobile net1 которая показала довольно хорошее качество на imet для мольной архитектуры и здесь важно Отметь что свка это вообще-то частный случай групповой СВТ когда разбива вход Кана на ГРУ и каждая группа обрабатывается как будто бы своей сврт независима Ну а результат мы контини дальше выходила архитектура Mobile net V2 которая отличается от V1 тем что авторы добавили inverted residual Block если в обычном residual блоке у нас на вход поступает фи мапа с большим числом каналов и потом мы его уменьшаем чтобы применить тяжёлую сврт 3 на3 то в ined resid блоке мы сврт 3 на3 заменили на лёгкую сврт deps И теперь мы можем позволить себе наоборот увеличить число каналов чтобы е применить тем уменьшить обратно А в статье mobilenet V3 авторы пошли ещё дальше и к второй версии добавили squ excitation модуль который по сути адаптивно перевзбила нелинейность Hard swish которая в целом является быстрой и хорошо зарекомендовала себя в мобильных архитектура и дальше другой архитектурой которую хотелось бы обсудить является гнет при её формировании авторы взяли за основу архитектуру ренет в качестве основы и начали семейство архитектур постепенно модифицировать добавляя различные модификации так например на слайде они начали семейство архитектур довольно широкого обозначенного буквой A и добавляя модификации улучшая это семейство сужа постепенно до семейства B и C тем самым постепенно получая всё более сильное И точное семейство моделей так были получены сетки regnet X и regnet Y где regnet y - это по сути всё тот же regnet X с squ excitation модулем который уже обсудили другой архитектурой для мобильных устройств является ghostnet в первой версии авторы добавили Ghost модуль который позволяет адаптивно сначала на входной фир мапе применить сврт и затем за счёт лёгких линейных операций которые обозначены F1 F2 и FK обработать полученный результат Ну и плюс ещё пробрасывается Входная фир мапа конкатенировать контекст во второй версии этой архитектуры авторы ещё добавили механизм внимания который обращает внимание на детали по вертикальной и горизонта входного изображения тем самым тоже имитируя механизм внимания в 2 вром году вышла архитектура коне в ней авторы опять же взяли за основу семейства т и начали его итеративности и получили сетки Next которые уже довольно сильно отличались от обычных Т блоков но однако в этой статье авторами были представлены только сетки довольно большого размера которые не подходят для мобильного применения и здесь нам на помощь пришла библиотека Mod которая Представила сетки мобильного размерам по работе с этими архитектура здесь Первая колонка - это собственно модели которую мы обсудили второе - это число параметров каждой модели чем больше параметров тем сетка медленнее и жирнее третья колонка - это вычислительная сложность модели Чем больше тем опять же медленнее архитектура и последние три колонки - это результаты На тестах Чем больше тем лучше мы можем заметить что семейства хоть являтся довольно точными тем не менее у них наибольшее число параметров Что делает их действительно тяжеловесными сети mobilenet показали в целом довольно посредственно точность и здесь мы делаем наш выбор в пользу архитектур regnet y regnet x и Con Next важно заметить что эти результаты - это наши исторические максимумы Которые мы получали для этих архитектур Независимо то есть мы обучали их с разными параметрами обучения с точки зрения замеров А это последние две колонки замеры на CPU и GPU здесь представлено FPS или скорость обработки кадров здесь опять же подтвердилась наша гипотеза то что golnet является медленной архитектурой если посмотреть на скорость на CPU и э сетка Con V Next показала тоже довольно посредственно скорость из чего мы делаем Вывод что сети regnet Y и regnet X являются одновременно точными и быстрыми моделями также есть и другие способы как можно ускорить архитектуры на девайсах например Это специализированные чипы и платформы но есть нюанс то что под каждую платформу вам скорее всего придётся писать соответствующий эн код для инса другим способом является квантизация вашей архитектуры при переходе в вычисление на меньшей точности Однако этот момент мы оставляем за рамками этого доклада Какие архитектуры нам подойдут Давайте посмотрим на данные напомню что при обучении сетки по распознаванию лиц мы решаем задачу классификации То есть у нас каждый ID человека является отдельным классом и если вы хотите натренировать действительно сильную сетку по распознаванию лиц то вы заинтересованы в том чтобы она видела действительно много людей во время обучения сейчас на слайде представлена статистика по числу айдини в Open Source датасета из Академии по распознаванию лиц Вы видите что их десятки тысяч или даже миллионы Давайте посмотрим к чему это приводит во время обучения мы входное изображение отправляем на вход свёрток получаем дескриптор который Затем идёт на классификационную голову которая имеет матрицу весов размера D на N где D - это размер дескриптора это Например порядка 000 и N - это число как раз уже ашнико потому чтом классифицировать дескриптор то есть отнести к какому-то классу и вот здесь как раз-таки возникает сложность нам нужно эту матрицу которая имеет размерность возможно миллионы как-то хранить во время обучения на графическом процессоре и здесь типичные методы типа Data parallel или distributed Data parallel которые используются при обучении модели уже не помогут потому что они создают реплику весов на каждом процессе наглядно проблему можно проиллюстрировать следующим образом здесь на графике представлена по оси Y нагрузка на на графический процессор GPU при хранении Как раз-таки этой матрицы весов последнего линейного слоя для классификации модели и по оси X представлено число классов в датасете Мы видим что на датасете с дм миллионами классов то есть с 2м миллионами айдини уже возникает проблема переполнения памяти на карте 2080 T от NVIDIA А если мы захотим слить наш датасет то надо как-то эту проблему решать предложение следующее Давайте перейдём к параллельному ускорению и разделим эту матрицу весов на чанки равного размера по как раз-таки размерности которая равняется число дишни ков У нас есть несколько пушек Давайте на каждом пушке на каждую ГПУ на каждый процесс положим соответственно свой Чан этой матрицы W выглядеть это будет следующим образом посмотрим на Форвард шаг на Форвард шаге мы подаём на вход сетки изображения и хотим его просто классифицировать получить Лос У нас есть несколько ГПУ на каждой ГПУ лежит свой чанк этой матрицы сначала нам нужно собрать дескрипторы со всех процессов то есть мы агрегирующие мы можем на каждом процессе посчитать произведение всего боча с локальным чанко этой матрицы и когда у нас будут на руках такие произведения мы можем их агрегировать и посчитать итоговое значение Лоса супер теперь посмотрим как происходит бэквард шаг вард шаг - это когда имея на руках значение Лоса мы считаем градиенты чтобы обновить веса нашей архитектуры начинается всё с того что мы считаем как раз-таки эти градиенты локальным гим то есть по локальным произведениям и это позволяет нам обновить как раз таки локальные чанки которые хранятся на каждом процессе После этого мы можем сагре также градиенты по дескриптора и обновить экземпляр архитектуры на каждом процессе посмотрим как такое параллельное ускорение влияет на хранение этой матри при обучении здесь опять же по о отложено чис классов нагрузка на процессор для хранения этой матрицы Весов и чёрная кривая это если мы не используем технику параллельного ускорения которой я только что рассказал А синяя кривая если используем Мы видим что в целом сложность и там и там растёт линейно Но эффект наблюдается довольно драматичный кажется что техника работает отлично тепер когда мы выбрали архитектуру данные обсудим как ещ можно выжать максимум качества из сетки по распознаванию лиц Если она имеет довольно маленькие размеры тут нам поможет такой приём как дистилляция знаний допустим у вас есть бюджет для обучения какой-то крупной медленной архитектуры по распознаванию лиц и вы про неё точно знаете что она не пойдёт впро Но она слишком медленная назовём эту сетку сетью учителя и будем обучать как обычно для решения задач классификации то есть мы будем использовать в качестве ответов метки классов после того как мы обучили такую сетку Давайте начнём обучать уже маленькую архитектуру мобильную Которую потом планируем задеплоить в продакшн зовём эту архитектуру сеть студента но также во время обучения мы будем использовать сеть учителя особенность в том что мы эту сеть учителя она никак не будет обновляться А в качестве ответов для студента мы будем уже использовать не метки классов а по сути распределение ответов или предсказаний от учителя однажды у нас был конкретный кейс когда к нам пришёл заказчик и попросил сделать совместимыми большую крупную медленную сетку и маленькую мобильную сетку для того чтобы можно было сравнивать их дескрипторы между собой с точки зрения расстояния и получать правильные валидные ответы с точки зрения решения задач верификации и идентификации Давайте посмотрим как можно это провернуть будем опять-таки решать эту задачу при помощи дистилляции знаний но с одной особенностью мы то есть теперь у нас заморожены учитель и эти матрицы если мы будем обучаться в таком сепе то у нас студент по сути выучит распределение дескриптора от учителя и они станут совместимыми по результатам это будет выглядеть следующим образом если здесь у нас в первой строке представлены результаты большого крупного учителя resnet 101 а во второй строке представлены результаты его студента маленькой сети regnet X то обратите внимание на третью строку здесь представлены результаты кросман этих моделей между собой то есть мы берём дескрипторы от учителя один сравниваем их с дескриптора от учителя 2 то есть считаем расстояние между ними и отправляем это на тестирование для предсказания ответа Мы видим что точность При таком кросман она где-то между результатами студента и учителя из чего мы делаем Вывод что эти модели в принципе совместимы чем хотелось бы поделиться и что хотелось бы подытожить во-первых архитектура regnet X по нашим результатам демонстрирует по-настоящему хороший баланс скорости и качества во-вторых то самое разделение весов матрицы последнего линейного слоя архитектуры - это сейчас де-факто основа для обучения современных моделей по распознаванию лиц которую рано или поздно приходится использовать если вы обучается на действительно больших датасета и третья особенность здесь это то что мобильная сеть учителя и сеть студента могут быть совместимы между собой если вы используете дистилляции и голову от учителя спасибо у меня на этом всё голосуйте за мой доклад Вадим тебе спасибо от всех учителей иро Малей низа есть Уже вопросы в зале прошу не забывать голосовать за доклад это обратная связь она очень важна всем А пока несу микрофон Вадим А сколько у вас ушло времени на эту разработку разработка эта разработка по сути начала появляться ещ с началом компании поскольку с началом появления компании поскольку то есть получа ГТО с 2014 года То есть получается лет почти примерно да то есть распознавание лиц это в целом довольно конкурентная отрасль и здесь борьба Идёт уже за доли сотых про точности Ну такое бывает только Наде то что вы делали 9 лет и рассказал нам за 35 минут и у нас может быть это освещали мне интересен вопрос А по поводу дооб на конкретных Ну на конкретных задачах То есть у нас есть там не знаю 100 сотрудников Мы хотим распознавать их а как вот вопрос до обучения в вот в контексте вот этих вот сетей и там так далее Спасибо за вопрос Если вы заинтересованы в том чтобы ваша сетка была робастность всех людей вы отделите эти 100 сотрудников От данных обучения и Попробуйте а набрать Как можно больший набор данных чтобы ваша сетка в целом имела хорошую обобщающую способность Однако если вы точно уверены что ваша база зафиксировано то можно в том числе и дефайн тюни на этой сотне людей Ну я имею в виду что вот у нас есть там 5 млн людей чтобы хорошая была классификация вот я хочу добавить ещё своих 100 э Ну и как-то динамически это дооб Навля То есть как Какая какой подход архитектурный вот для того чтобы это конкретно на А ну на конкретные задачи для распознавания там сотрудников компании например применять Ну как вариант например Вы можете либо до обучиться конкретно на этой сотне людей с каким-то маленьким Нин рейм чтобы не сильно сбить веса вашей архитектуры Или например добавить просто эту сотню людей Ну например сотню классов в обучение при основном этапе обучения вашей архитектуры Спасибо И у нас следующий вопрос Здравствуйте спасибо за доклад было очень интересно хотел спросить по поводу работы именно на мобильных девайсах Может быть вы подскажете какие сейчас движки для инфе наверное актуальны именно для инфе вот на устройстве пользователя в частности на мобильном потому что с десктопа вроде как проблем особо нет спасибо Спасибо за вопрос у нашей компании есть собственный фреймворк для Ирен на мобильных устройствах который оптимизирован под c+ Однако в целом существует сейчас довольно большое количество конкурентно способных фреймворков например или Open которые позволяют вам довольно просто задеплоить вашу архитектуру вот такой вопрос Open он же ну на инские процессоры в основном заточен А вот это ну я не знаю Я знаю вот слайд он уже вообще не в ходу уже или есть какие-то может китайские движки для этого всего здесь боюсь что не смогу подсказать Однако действительно Да здесь есть такая особенность что под каждый девайс и платформу вам скорее всего придётся использовать свой Кэн код какое-то своё окружение Спасибо И у нас следующий вопрос Да Вадим Спасибо за доклад а такой вопрос есть ли какие-то супер прорывные статьи за последние 2-3 года в распознавании лиц Хороший вопрос как правило в распознавании лиц наиболее популярные статьи - это статьи про Лос функции которые разному по-разному пытаются усложнить задачу при обучении сейчас современные лосы они базируются на использовании нормы дескриптора то есть каком-то понятии о качестве этого дескриптора например maf AD Face и тому подобное также по распознаванию лиц выходит довольно большое количество по дистилляции и по архитектура которые тоже в целом позволяют конкретно для этой задачи получить улучшение а Спасибо за ответ я узнал второе значение слова дистилляция это важно и у нас следующий вопрос Вадим Скажите пожалуйста вы сказали что для достижения требова качества распознавания необходимо соблюсти баланс модель это архитектура данные и Лос функция рассматривали вы четвёртый аспект увеличение разрешающей способности патча то есть выровненного изображения лица допустим до 224 Ну или выше Да действительно то есть типичное разрешение в сетках по распознаванию лиц - это как раз 224 и тут нужно уникально под вашу задачу пытаться пробовать увеличивать это разрешение или уменьшать бывает в целом и такое что вы его увеличиваете но качество у вас не растёт в целом есть довольно известный трюк Когда вы например обучается сначала на картинках маленького разрешения какого-то а потом на иренс используете большое разрешение То есть это как некоторое усложнение задачи у вас работает Спасибо И у нас последний вопрос Да Вадим Спасибо большое за доклад был очень интересно у меня два Да вопроса Первый про один вопрос хорошо тогда давайте наверное лучше про дистилляции ты сказал что вроде каких-то много статей новых пробовали вы дистиллировать как-то более подробно сетку чем просто про гиты основных сетей потому что кажется что там дистиллировать какие-то промежуточные слои выглядят более эффективной подходом да Это хороший вопрос есть разные способы например действительно какие-то промежуточные гиты пытаться дистиллировать Однако в распознавании лиц очень много статей выходит именно на тему дистилляции последних репрезентаций то есть именно на уровне дескрипторов скоров классификации и там например помимо типичного к Лоса можно использовать например L2 Лос между дескрипторов задать вопрос У нас есть зона Куров прямо выходе из зала где мы сможем все продолжить дискуссию и те самый интересный вопрос Выбери лучший вопрос мне понравился вопрос про прорывные статьи которые выходили за последнее время От молодого человека в белой толстовке Прошу выйти на сцену молодого человека в белой толстовке и с оранжевой ленточкой Поздравляю Спасибо за за доклад Спасибо что пришли Спасибо кто с нами онлайн не переключайтесь"
}