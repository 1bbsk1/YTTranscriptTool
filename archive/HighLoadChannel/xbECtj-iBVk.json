{
  "video_id": "xbECtj-iBVk",
  "channel": "HighLoadChannel",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Михаил Жилин Павел конотопов Готовьте вопросы и Давайте Пошумим Привет привет зал Я рад вас всех приветствовать на highload п+ 2023 мы сегодня поговорим в первую очередь конечно же про постгрес А во-вторых про такую довольно специфичны вещь как малти мастер Меня зовут Михаил Жилин Я уже порядка десяти с плюсом лет занимаюсь анализом производительности и в нашем сегодняшнем докладе мне будет помогать Павел конотопов который уже 10 с лишним лет занимается тем что строит кластера на постгрес на не постгрес всего вокруг парой Господи не будем ВС перечислять Вот Но как я уже сегодня сказал наш сегодняшний доклад будет про постгрес и конкретно про такую вещь под названием молти мастер Что такое ще молти мастер да Казалось бы постепенно Мы про него поговорим как он появлялся что это за технология что в ней заложено Какие механизмы для чего они вож и самое главное поговорим про вообще какие реализации его существуют как его можно применять и как его не стоит применять И вообще поговорим про самое главное про производительность мультимастер потому что ну если база данных медленнее черепахи то такая база данных наверное всё-таки не база данных уже а просто черепаха вот Итак начнём с такого простой вещи что такое же молти мастер Ну как быстр ну Мастер значит несколько мастеров мастер мастер 2 мастер 3 И зачем нам нужен несколько серверов Ну для того чтобы организовать некоторое кластерное отказ устойчивое решение которое бы умело там масштабироваться быть надёжным можно было его раскидать на несколько дата-центров всё было бы хорошо и самое интересное что эта технология совсем не новая то есть мы тут не первый раз на сцене Хайда рассказываем просте до этого вот последний раз рассказывал Илья 2017 году добрые хорошие сладкие времена когда всё было Open Source только зажигали и всё такое но после этих докладов може могло показаться что тимар - это в общем-то технология которая Пока ещё настолько сырая что она не может масштабироваться она не точно никак не про производительность очень довольно сложно её использовать применять вообще е практически нигде не существует это такая какая-то така редкий звер в природе который занен в Красную книгу и его лучше не трогать а тем более уже на продакшены ни на какие не пускать так в общем-то и покажется что в общем-то мамае это какой-то такой баз о котором стоит рассказывать только на лоде вот правильно Так вот был Был мама самого начала можно сказать существования постгрес и перед тем как вообще перейти к мультимастер и описанию данной технологии надо наверное вспомнить какие-то базовые вещи которые есть в постгрес в первую очередь это конечно физическая репликация её придумали Достаточно давно мы с одного узла на другой узел передам изменения то есть физи эти изменения применяем на втором узле но здесь конечно в данном подходе мультимастер на такой технологии не построить потому что у нас во-первых реплицируемый масштабировать нашу систему то в данной схеме мы можем писать только в один уест приду вки репликация непосредственно транзакций из одного узла в другой но мы тут можем увидеть что тут достаточно много гораздо больше компонент чем физической репликации Тут ещё появляется логическое декодирование это более гибкий инструмент модель которую логическая репликация реализует это публикатор и Подписчик и мы в данном случае из одной базы данных в другую базу данных и не ВС целиком А даже какие-то объекты и ещ по ходу дела их фильтровать но тем не менее логическая репликация у нас это всё равно однонаправленный однонаправленный поток изменений и опять-таки на этой технологии мультимастер полноценно не построишь сообщество тоже на месте не стоит и разработчиков было встроенной логической репликации в постгрес была написана такая вещь как PG logical и поверх неё была написана вещь которая называется bdr двунаправленная логическая репликация но с более поздних версий это закрытый коммерческий код А вот логическая репликация встроенная в постгрес появилась в дей версии как раз вот в том году когда Илья делал свой доклад а встроенная двунаправленная репликация появилась только в шестнадцатом постгрес ваниле и это встроенная двунаправленная репликация и тут сообщество и энтузиасты сказали Ну что вот он мультимастер нашли Нет но всё хорошо но здесь вот мы со своей стороны если подумать чего бы нам хотелось от мультима то на есть к нему требования и пожелания да то есть мы записываем изменения в несколько узлов и мы хотим сделать так чтобы у нас всё что Мы записали было консистентность с этим было достаточно просто то есть для приложения данная вещь должна быть достаточно прозрачной во-первых мы должны сохранять порядок транзакций во-вторых мы должны каким-то образом следить за блокировками и их разрешать или предотвращать в-третьих Мы хотим сделать так что если у нас будет какое-то разделение то не было бы Сплит Брейна чтобы наш кластер был доступен А ещё Мы хотим чтобы если вдруг узел с ним что-то случилось и он сломался когда мы его починили чтобы он автоматически восстановился и стал заново рабочим узлом кластера Ну и в схеме с логической репликации должна быть защита от повторного применения транзакций и вот собственно мультимастер с нашей точки зрения должен выглядеть так что это должны быть какие-то внутри постгрес дополнительные фоновые процессы которые вот все те требования о которых мы сейчас сказали отслеживают управляют постгрес координируют наши транзакции параллельно их применяют для ускорения если это возможно собственно разрешают определяют конфликт и заниматся тем что присматривают То есть это какой-то специальный или набор специальных компонент внутри постгрес и вот если с этой точки зрения посмотреть на постгрес который на мультимастер который сделали поверх логической репликации А он такой есть то у него соответственно есть определённые минусы то есть Open Source породил два примера которые Ну нам известно На текущий момент один называется смешно трактор другой называется тоже В общем достаточно весело но при ближайшем рассмотрении этих проектов там вы не обнаружите ни строгой согласованности данных не отказа устойчивости узлов нету автоматического восстановления и главное что нет транзакционных всего кластера собственно Спок что по названию я не мог пройти мимо картинки это расширение которое вс-таки реализует какой-то постгрес Он поддерживает логическую репликацию из нескольких баз данных он какие-то конфликты автоматически разрешает об этом мы поговорим попозже используя различные стратегии э между прочим там реализована такая вещь как бесконфликтный типы данных и реплицируемый руками эти конфликты не Разрешите дальше дело не пойдёт другой мультимастер поверх логической репликации так по крайней мере заявлено разработчиками это вообще внешний сервис который базируется на том что следит за логом постгрес и если там обнаруживается какой-то конфликт он идёт в постгрес и начинает перезапускать логическую репликацию или требовать разрешения собственно Ну собственно что мы с семнадцатого года как-нибудь проэволюшн конечно устроен гораздо сложнее собственно это два коммерческих продукта Вот первый продукт у нас от компании Enterprise db и второй продукт от компании постгрес профессиональный обе эти компании являются крупнейшими контрибьютор исходного кода в по SQL паче И что же было сделано в обоих продуктах по сравнению с тем что есть сейчас в опенсорс во-первых была добавлена ddl репликация вот когда мы говорим про логическую репликацию которая есть в постгрес все понимают что она умеет переносить данные из базы о в базу 2 но она никак не умеет переносить изменения в схеме данных из одной базы в другую то есть если вы добавляете атрибутик в некоторую табличку то вся логическая репликация превращается К сожалению в тыкву следующее Что было добавлено это возможность организовать согласованный распределённый комит между различными двумя инстамикс Ну и дополнительно подумали немножко о производительности добавили вспомогательных воркеров чтобы они выполняли нам работу связанную с переносом данных и их применени думали огромное количество алгоритмов для того чтобы решать конфликты если они такие возникают И самое главное что реализовали канал управления между различными нодами для того чтобы была возможность кластер Как собрать так и понять что он развалился как только управляющий канал прекращает сво существование такой вот честный мультимастер должен на наш взгляд состоять из вот нескольких вот дополнительных процессов Вот отдельный процесс мониторинга которые связаны друг с другом между различными нодами отдельные процессы так называемые dmq receive и dmq S они должны выполнять как раз-таки согласовано применение коммитов чтобы у нас не получилось так что вот тут данные одни А здесь данные другие во времени разошлись всё Беда печаль несогласованные данные читаешь из одной базы а там совсем другое что в другой базе вот дополнительно ещё несколько процессов связанные с тем чтобы обрабатывать потерю например узла или восстановление узла если после там рестарта решили наша нода вернуться в кластер всё бы это хорошо только все эти новости новые фичи они вносят свои ограничения Вот почему это просто так взять нельзя отнести патчами ванилу и сказать ребята Ну возьмите же уже наконец-то и мы не будем мучиться Тут ничего писать код больше не будем вот а ограничения очень простые во-первых мы можем реплицировать только одну базу данных то есть молти мастер поверх нескольких баз данных пока что не реализован ни в погосте профессиональном ни в Enterprise db А это довольно-таки неудобно когда Ты создаёшь целую коммуналку баз данных в одном инстанс и хочешь чтобы они все хорошо работали как бы мастер с мастером вот опять же е больше огромное количество ограничений связаных с ддм нельзя делать конкурентно пересоздание индексов все уже настолько привыкли по-моему то ли с онай то ли с двенадцатой версии сделаешь concurrent И вот тебе он на либо же если у тебя там табличка раздулась то запускаешь к который тоже кстати использует concurrent ндекс и у тебя всё хорошо становится ещё ограничения связа с тем что если ты созда табличку Тод добр создай пожалуста ключ А это не всегда довольно-таки простая операция иногда кто-то ленится а иногда ктото действительно просто не может придумать какой же первичный ключ создать на таблицу такие сложные данные вот ну и если переходить всё же к плюсам дополнительным которые дают тимар по сравнению с теми минусами которые у нас сейчас есть это же во-первых возможность практически бев обновлять узлы кластера То есть вы можете снять нагрузку с одной ноды выключить её обновить версию минор запустить обратно нода вернулась в кластер всё никто даже не заметил свич овера Вуаля красота можно масштабироваться по чтению пото читать с каждой ноды но если мы же пишем в одну ноду то эти же записи должны перенести в другую ноду а значит по количеству псов мы к сожалению здесь ничего не выигрываем и теперь самый главный вопрос и как же это всё нам завести А завести это очень просто во-первых входит в профессиональный ED начиная с тридцатой версии чтобы его поставить достаточно нам создать этен под названием тимар на каждой ноде постгрес А после вызвать Командо на одном из узлов и перечислив адреса других это Командо сходит кот всех в оди еди Дружный сте да можно будет посмотреть как они друг с другом дружат ноды с помою команды статус или вот на примеру на слайде сейчас показано уже собранный мастер хной спокойно себе работает в ВС практически тоже самое те же самые шаги ставим вызываем вспомогательную служебную команду которая собирает PR db пошли ещё и дальше они создали специальный инструмент под названием trusted pog architect вообще вот название самое лучшее инструмент для развёртывания малти мастеров в контейнере в облаке На физических ресурсах какой хочешь всё прекрасно Ну хорошо Ну вот развернулись мы где-то там в контейнерах в Докера и так далее И что дальше мы же вот как бы понимаем что просто так развернув инс Ну вот например с физической репликации ничего само управляться не будет правильно не будет Ну вот смотрите вот типичная схема кластера патроне взята же сайта самого же патрони то есть Нам нужен дополнительный tcd кластер для согласованности между различными нодами Нам нужен КСИ для того чтобы ходить на правильный узел Потому что если мы пойдём на реплику а захотели записать данные то мы получим ошибку А мы не хотим чтоб наше приложение получало какие-то ошибки поэто в луча Ну согласованный консенсус у нас ноды умеют уже сами устанавливать мы сказали что там есть специальные процессы которые за это отвечают поэтому мы берём и hcd выбрасываем Ура У нас минус три виртуалки или мину три контейнера Кому как и нравится но главное у нас освободились ещё некоторые ресурсы память процессор и тому подобное идём дальше нужен ли нам балансировщик Ну самый главный наверно вопрос Нужен Ну как надо же правильно понимать где Какие адреса или нет Да здесь вообще вопрос вопрос вопрос большой вопрос Давайте по немножко идти Итак все они у нас мастера можно ходить к любому можно подключиться приложениям и начать с ним работать но вообще-то у нас у каждого узла бывают состояния кроме Я хорошо Я работаю я Мёртвый есть ещё промежуточное состояние состояние восстановления оно называется либо чап либо Recovery в профессиональном Одно название в Enterprise db другое но суть одна и та же это фаза восстановления И если мы подключим наше приложение к такому узлу то приложение получит ошибку под названием что-то типа m из on или из Вот и наше приложение которое работает через там jdbc или psql с после того как соединение пакет подключится авторизируется ВС нормально оно к такой ноде может подключиться это же логическая репликация нам Никто не запрещает типа парень стоп и попробует сделать какое-нибудь дополнительное действие такое какое проставление сессионных переменных А какая у нас самая популярная сессионный переменная Ну работа с FL переменными сколько мне цифей всегда их выставляет всегда выставляет сенные переменные получат Опа бум нельзя работать с такой Ной Беда печаль но нет Есть простое довольно спасение Давайте просто перечислим все Хосты через запятую в конек стринге тогда когда наш драйвер будет подключаться к какому-нибудь хосту который сейчас ну не в том состояни состоянии восстановления то драйвер получит После подключения к нему ошибку и пот до тех пор пока не найдёт рабочий хост тем самым мы можем научить наше приложение ходить минуя всякие балансировщик напрямую в базу данных как и говорили вот эти парни Правильно же вот всё выбрасываем оставляем только постгрес и только приложение всё Вот оно счастье простота и радость ВС над хороо или нет ну самое важное конечно в любой базе данных как мы понимаем надёжно сохранить данные иначе вообще зачем база данных нужна и поэтому как вот при такой распределённой модели нам гарантировать строгую консистентность и как нам автоматически разрешать конфликты большой вопрос И вообще это история про достаточно большие академические прежде всего исследования которые вылились в определённые практики и легли в основу модели по разрешению конфликтов и как можно утверждать конфликты вопервых бывают разные Когда у нас две транзакции самые распространённые наверное их толкование когда две транзакции обновляют одни и те же данные и совестно могут друг друга Взаимно заблокировать помимо этого ещ и другие бывают конфликты и какой-то вот универсальный стратегии для всех вообще в принципе возможных конфликтов её не существует и конечно возникает вопрос что нам делать с конфликтами в мультимастер Ну первое Конечно надо стараться их не создавать это отно прежде всего к Диза приложения с рабо с базой данных и с таким распределенным кластером мы можем писать только в Один узел мультимастер читать данные через другие узлы и помимо этого ещё использовать так называемый бесконфликтный тип данных ну и сейчас будет двух минутка энциклопедии потому что вот эту вещь её не так давно изобрели есть некая серия даже трудов на эту тему собственно У нас есть конвергентные коммутативный типы данных Собственно сам бесконфликтный тип это когда у нас одновременное обновление коммутативный то есть мы передаём только дельты изменений неважно В каком порядке они к нам приходят но финальный результат в какой-то момент времени он всегда будет одинаков есть Да гарантированно сходятся при том что обновления выполняются на каждой реплике и вот такие примеры можно привести во-первых хотел сказать что мы можем передавать либо изменения либо состояние когда у нас есть некое состояние целое число или вектор и собственно каждый узел он у нас какой-то конкретный момент времени при транзакции распространяет состояние на другой узел выборка нашего состояния применению его к другой базе данных к другой реплике заключается в том что мы выбираем наибольшее значение Ну например по времени как здесь приведено типичный пример из нашей жизни это разного рода датчики которые посылают нам либо одно изменение либо Вектор измерений и соответственно нам важно чтобы у нас всегда когда мы Обращаемся к базе данных у нас всегда было послед состояние Мы из неё могли извлечь собственно другой тип данных - это когда мы реплицируемый Наверное вариант распространённый который приходит в голову Это счётчик мы обновляем счётчик на нескольких узлах но мы знаем что это некоторая последовательность она монотонно увеличивается соответственно Нам нужно только сделать одно если мы получили в какую-то какой-то узел базы данных это изменение тут же передать его на другой а там сумму и прибавить пришедшие изменения в конце и собственно и получится то что вот изображено на картинке но тем не менее нам всё равно надо выявлять каким-то образом конфликты и они всё равно появляются что тут не делай и вот что делать как жить да вот мы говорим конфликты конфликты конфликты А что это вообще такое для пост есть строчки есть Запроси А такое конфликты лит надо рассматривать с точки зрения работы логической репликации А как она работает во-первых у нас все изменения переносится с одной стороны на другую сторону построчно То есть изменилась одна строка в базе данных мы передали на другую сторону со словами Найди вот эту строчку у себя в базе данных и Поменяй колонку X на значение y то есть нам для того чтобы найти эту строку на стороне при к и мы это первичный ключ используем при поиске проверяем строка вообще сейчас заблокированы или нет потому что если кто-то ещё пытается уже на приёмнике поменять эту строчку то он наверняка придёт к нам в наш в нашу ноду мамае и тоже попробует эту строчку поменять и тут у нас уже получается что две транзакции существуют кото менят од теже данные чадо уже реть данный конфликт Ладно мы можем немножко проигнорировать эту блокировку попробовать заблокироваться всё-таки в попытаться схватить эту блокировку встать некоторое в ожидание Пока так транзакции не закоммитить на то что у нас рано или поздно не случится большой дедлок глобальный и всё пройдёт успешно такое тоже бывает транзакция отка идм строчка за строчкой и вот в эти моменты когда мы проверяем блокировки или берём блокировки и ждём как раз и происходит вот этот конфликт детек и Conflict resu либо же уже асинхронно когда уже всё сохранили ребята вот у нас здесь были две транзакции что-то пошло не так но самый Теперь главный вопрос Окей мы нашли конфликт А что же дальше с ним делать постгрес Ну в первую очередь надо принять Какая из транзакций в конфликте права Ну приходим в суд и судья у нас должен решить ты ну первый или второй прав используется при а таком вот вынесении вердикта различная информация например откуда пришла строка Origin или же используется версия строки если такая возможно но так или иначе финальное решение Либо мы возвращаем ошибку данной транзакции данному в данную сессию либо уже Мы пропускаем данные изменения Ну ладно типа всё нормально Лучше ничего не блокировать обновить данну встре профессионального есть специальная даже настройка которая контролирует данный вот процесс проверки и лю называется Ну для того чтобы в конце не словить это совсем уже печально сложно резолвится но самое любопытное Не это а в том что можно е професи его пока нету который позволяет двум транзакциям бесконфликтной ту же строчку но только разные колоночки каким это образом реализовано в каждую колоночная метка и с помощью каждому значению сопоставляется именно время когда последний раз модифицировал и уже на основе и в результате получить Вполне себе консистентные данные вот но всё это хорошо в теории на практике реализовали запаковать Но мы же не уверены что всё-таки в двух разных базах данных допустим они размером по терабайта случайно не прилетел какой-нибудь излучение из космоса и не испортило нам тихенько данные вот кстати Паш ты знаешь вот есть тако веь процент правильно зна ты промили Да знаешь ну наверно там все знают Какие промили надо не превышать вот А знаешь как называется следующая часть Ося про что нет оказывается это называется базисный пункт Итак ребята Из алибабы недавно совсем вот месяц назад опубликовали статью в которой проверили свои дата-центры на то как них Прим они проверили и диски и процессора и выяснили что оказывается несколько базисных пунктов процессоров неправильно считают чекс сумму на данных представляешь ксму как можно доверять процессорам которые неправильно чек суммы считают а тут ты говоришь вон там конфликты правильно резот наверняка что-то они рано или позно что-нибудь неправильно цен Вот они вот они гамма излучения уже пошли микрофоны поэтому попадание какого-нибудь космического луча в конкретную ищейку памяти наиболее вероятно но чтобы нам как-то убедиться всё-таки в том что всё в порядке У нас есть вещь которую мы называем которая внутри баз данных называется утилитами проверки согласованности данных отдельный продукт который называется comp который может сравнивать несколько баз данных именно кластера мультимастер целиком на предмет каких-то аномалий вообще в документации Enterprise db там сказано что вот аномалии расхождения данных самые наверное тяжёлые потому что их наиболее тяжело устранять мы должны иметь какой-то критерий на основе которого мы будем одеть у на актуальные последние данные в нашей базе данных Как вы вторая история про есть отдельная функция которая называется и она в состоянии сделать снимок на всм кластере мультимастер И после этого сравнить данные таблиц в пределах этого одного снимка в этом смысле снимок у нас транзакционный Спасибо большое Но помимо того что мы что-то сравниваем между собой У нас есть ещё и другие стратегии разрешения конфликтов реализована такая вещь как панер или SM собственно у нас каждый узел в кластере он имеет представление о том какие транзакции происходили и какие статусы у них были в соседнем узле для этого эта конфигурация для двух узлов и соответственно Если вдруг у нас какая-то транзакция откатилась или наоборот ожидается какой-то конфликт то другой узел посмотрит в память соседнего узла и примет решение на основе этого знания есть такая вещь которая называется Комо это comed Most то есть один раз мы что-то должны закоммитить собственно представим себе что у нас есть приложение оно делает комит а ответа не дожидается Вот и соответственно что мы должны предпринять в следующий раз мы должны заново повторить А вдруг эта транзакция закоментить а соответственно тогда у нас будет конфликт в данном случае приложение это как бы третий участник нашего кластера в том смысле что оно тоже принимает участие в консенсус в комит транзакции знает о судьбе предыдущего комита потому что например того узла который живой она может получить сведение что комит произошёл успешно есть ещё разрешение на основе такого протокола как как Егерь Когда мы можем определить потенциально конфликтующие транзакции и вообще мы можем настроить даже триггеры для какого-то кастомного разрешения У нас какая-то есть особенность в нашей конструкции нашей СУБД и в нашей схеме и мы можем специальный триггер на это написать помимо всего прочего известные протоколы устойчивости к сбоям используются для того чтобы у нас был глобальный комит это собственно протоколы консенсуса это в посте профессиональном это па двухфазный комит и вот различные комбинации но вот если посмотреть на вот эту схему которую вы видите то здесь можно увидеть что значительны оверхед принесут нам протоколы консенсуса потому что это поход между узлами пот собственно вот Следующий вопрос который я буду адресовать Михаилу он как раз у нас производительностью занимается пока я последние это то что собственно Чего как это всё счастье-то пить Ну замечательно У вас есть три узла на них данные Мы очень надеемся что они одни и те же и собственно мы Какой узел резервно копировать и потом как там вообще весь этот кластер восстановить внутри кластера мультимастер узел и соответственно до получаем данные с другого узла собственно из этого мы знаем что фитор этого узла будет один и тотже для всех узлов кластера Поэтому собственно исходя из того что всё у нас хорошо работает и надёжно мы думаем что у нас на каждом узле содержится и уверено даже в этом идентичный набор данных и соответственно восстановление узла оно если мы не клонируем с других узлов кластера то мы просто восстанавливаем каталог базы данных узла запускаем СУБД удаляем метаданные собственно из Ново поднятого узла и м просто узел в кластер собственно вот и всё И тогда узел просто остаётся дождаться когда он получит накопившиеся изменения за то время пока он отсутствовал и конечно вот увидев историю про то как мы разрешаем транзакции разрешаем конфликты как у нас происходит комит особенно глобальный конечно встаёт вопрос о том насколько это вообще всё быстро и не не просто быстро А где мы вообще можем применить потому что явно мультимастер не про скорость без границ ми Да Дада я чувствую уже что мне пора посыпать голову пеплом Итак мы подошли к самому вкусному а именно к производительности вот всё что перед этим мы сказали это классный механизм они предоставляют нам Надёжность но как любая Надёжность это сильно замедляет производительность так вот к нам пришёл один клиент который очень хотел чтобы это его решение работала на мультимастер вых ихних тестах с мастером результат был просто катастрофи в сравнении с мультимастер это было в три в четыре раза медленней просто ужасно 800 миллисекунд против ДК п и они к нам пришли и сказали ребята что это вы тут вообще продаёте вот ваш мультимастер вот из-за вашей надёжности из-за вашего тут вот супер-пупер фич он просто черепаха Простите меня Ну давайте понемножку разбираться в чём же он черепаха в чём же он притормаживает Ну во-первых мы включили дополнительно логирование валы с информацией для логическо декодирования если честно это всего там пару записей и сильно прям валы не раздуваются в разы Ну блин А где здесь три разы Ну нету здесь трёх раз идём дальше Ну проигрывать надо изменения на всех нода да мы на одной ноде проиграли теперь побежали на вторую но Подождите на второй ноде то мы проиграем это всё всё равно бы за то же самое время что и на первой откуда три раза опять же не может быть Ну как бы там Ну чуть-чуть ещё притормозить хорошо у нас там ещё появилось голосование да то есть все ноды должны ещё в конце проголосовать по паксус договориться что все у всех всё хорошо можем производить комит Да ещё дополнительно немножко времени потратили но опять же ну ну немножко же времени потратили транзакции та где я вот Пакс и как оказалось что вообще-то есть не только внутренние механизмы которые в самом мультимастер притормаживает но ещё есть некоторые грабли с них-то Мы с граблей то как раз и начнём первое что убивает производительность в мультимастер это таблички без первичного Ключа если к нам прилетает строчка на изменения на другую сторону там нету первичного ключа нам приходится что делать полное сканирование таблицы искать Где же наша любимая строчка которая поменялась в малти Мастере по профессиональном Мы даже специальную настрое сделали чтобы игнорировать такие вот таблички чтобы они не реплицировать Но это уж совсем для параноиков Но главное чтобы производительность работала здесь Первое правило что первичные ключи вторая грабля вот совсем абсолютно реальная Лог ситуации которая произошла в пре продакшене в один прекрасный момент логическая репликация которая выбирала данные из мамае начала затыкать на той стороне на стороне преемника уже другого постгрес что-то пошло не так на той ноде на которой было подключено логической репликации начали копиться валы валы копились копились копились пока диск конечно же закончился как только закончился диск первая нода превратилась в тыкву вторая нода честная такая добрая ждёт когда на первой ноде что-нибудь произойдёт нормальное она вернётся в нормальное состояние копит для него валы копит копит копит копит пока у неё у самой через час тоже не заканчиваются данные диск и всё весь малти масте превращается в одну большую тыкву и здесь есть простое решение есть в постгрес в ванильном валов до того момента пока Слот не взять и не прибить То есть если вдруг валов накопилось больше чем X ГБ то пожалуйста найди Слот который нам заставляет держать эти валы и прибей его Пускай лучше отвалится эта логическая репликация Но наш мультимастер продолжит жить спокойно поэтому выставляем этот параметр и наступает по идее бы счастье но надо ещё немножко поработать во-первых нам надо глянуть А что же у нас вообще с котами вот взяли валы вынули из валов информацию когда происходили изменения когда начинался так называемые распределённый комит когда он закончился и выяснили вот на простом примере Что существовало множество транзакций которые поменяли всего лишь одну строчку и при этом время глобального комита было Ну в 10 раз больше чем время самой транзакции посмотрели в одну строчку в одну транзакцию печаль посмотрели в другую транзакцию такая же печаль пошли к разработчикам Мы спрашиваем Ребят вы что транзакции по одной записи меняете у вас вроде НТО на 20 Мб прилетел Вы же должны были это всё объединить ребята такие блин точно мы забыли у нас же это в Джаве автокомис был выставлен в True и вот мы каждый каждая dml у нас создавала дополнительную транзакцию они выключили автокот и счастье неожиданно мастер начал летать время глобального комита резко сократилось и оно стало уже не там 99% времени занимать от всей транзакции А всего лишь 28 что уже сравнимо с работой физической репликации матер сй хорошо Мы ещё немножко поколдовать Кроме того чтобы глобальный комит произошёл быстро нам нужно чтобы сеть отмечала практически мгновенно А мы любим все виртуальные машины бла вот мир такой виртуализации к нам пришёл а в любой виртуальной машине кему есть виртуальный сетевой интерфейс и сетевой этот интерфейс по умолчанию однопоточный комит начал упираться просто в сеть мы выставили вместо одной очереди четыре и время тоже резко сократилось сле и в итоге что главное Надо запомнить быстрой производительности мастера надо держать первичные ключи пытаться делать более объёмные транзакции чтобы был более дешёвый глобальный комит настраивать настройку масай от того чтобы у нас мультимастер просто не превращался в тыкву но ещё дополнительно можно упомянуть что пытаться избежать конфликтов на разных нотах потому что резвин конфликтов - Это тоже не дешёвая вещь И как вы этот резвин настроите настолько сложным Он и будет поскольку у нас используются глобальные распределённые коммиты которые построены поверх raft Пакс которые используют по себе временные метки то желательно чтобы время было синхронно на различных нода если начнётся расхождение начнутся дополнительные жите данных транзакций бойтесь безопасни ков эти ребята любят всегда поставить Антивирус а антивирус у нас всегда лезет во что в сетевую активность у нас же наверно сетевого периметра атаковать и вот и специально начинает находить вредоносный трафик и обязательно найдёт его в глобальном коммите тоже этот вредоносный трафик начнёт его замедлять и последнее Если у вас есть огромная миграция там сотни тысячи миллиардов строчек то постарайтесь её делить на части потому что когда вы сделаете одну большую транзакцию её надо потом логически декодировать в памяти а это приведёт к киллеру Вот чем же закончилась наша вся тем что в итоге время транзакции упало с 800 до 300 система у слава Богу успешно прошла все тестирования и перешла в промышленную эксплуатацию А мы получили огромный ценный опыт знаем Теперь какие настроечные надёжны Павел надёжный надёжны вот как микрофоны Да как как микрофоны Ну почти надёжны у нас же малти мастер вот два человека могут всегда подхватить вот главное что они нам позволяют добиться высокой надёжности по сравнению с любом кластерным решением на физической репликации То есть у нас всегда будет доступна наша база без всяких там времени на восстановление и главное что это можно использовать в промышленных решениях Да ура мультимастер я бы сказал последней версии уже достаточно проверенное решение которое можно использовать в продукти раньше мы упоминали о том что Попро мультимастер - это закрытый код но это не значит что мы не можем о нём прежде чем решить нужен он нам или не нужен Ничего узнать для изучения У нас есть во-первых абсолютно бесплатные учебные курсы и Ну про это Многие знают что на сайте по Pro они есть но последний курс - это именно по по Pro Enterprise и внутри этого курса есть отдельный раздел синхронный кластер мультимастер вот на него ссылочка есть а как пощупать мультимастер для этого есть учебная виртуальная машина которую можно себе загрузить запустить мультимастер и попробовать ВС то о ЧМ мы сегодня говорили для вдумчиво изучения Ну тут вот большое количество ссылок довольно-таки это разного рода доклады про разные технологии это если кому интересно В нашем докладе это есть Ну и последнее ставьте лайки делайте репосты голосуйте за нас доклад Спасибо всем за внимание голосуй сечас вопрос Вопросы мы задам А вот прямо слева смотри уже первый вопрос ле Ну давай вот вы говорили что под капотом там логическая репликация а проигрывание валов у нас идёт в один поток А их генерация скажем так во много потоков То есть это как-то допили или мы просто не упирается ОТВ е на приёмную сторону в результате проблема утыкание в один процесс решена и в Enterprise db и в по профессиональный так Следующий вопрос пожалуйста Добрый день раз раз вы говорили что вы тестировали мультимастер инсталляции Вт узла правильно в 2п о и Вт вот существует как минимум Некоторое количество организаций которые Рива с вами только там от 1.000 узлов например А как у вас распределённой транзакция на таком количестве узлов вот для решения на там тысячах узлах у нас отдельный продукт называется ard Man шардирование не мультимастер вопрос был именно про распределенную транзакцию будет ли она работать на таком количестве узлов работает в нашем шарма по крайней мере до 100 узлов Мы точно масштабировать а 1000 узлов Ну мы же не Яндекс со своими дата-центра чтобы хопани Скажите пожалуйста со всеми ли типами стерилизации в транзакциях эта вся штука будет работать да ели какие-то были подводные камни при разработке этого всего дела Это первый маленький вопрос а второй вы показали создание получается мультимастер кластера да то есть Мы создали и всё есть ли какая-то динамическая составляющая То есть если мы хотим Здесь и сейчас добавить брать что-то с этим сделать СБО нас изоляции транзакций улу работает работает а второе то что мы просто просто добавляется узел с помо интерфейс вот так как было на слайде отражено вот ровно также и в жизни Единственно Только что ну надо дождаться пока этот у ещ вопросы смотрите вот там справа просто уже слышно слышно мы слышим вопрос Вы говорите что мультимастер запись попробуй не держать во это не про масштабирование на запись это про масштабирование на чтение и про доступность да да А чем это это решение лучше нежели встроенная ВГ репликация и в случае если кто-то хохочет читать Пусть читает с репли хорошо но на реплику А нельзя писать а иногда очень-очень хочется и Б то что когда у нас проходит све мастер становится недоступным некоторое время несколько там секунд минут Слава богу не часов будет ещ и восстанавливаться пока не станет мастером То есть у вас есть некоторое окно Когда у вас база по сути в состоянии восстановления и ничего с ней не сделаешь ну то есть это скорость переключение фактически Да спасибо Для этого даже придумали термин который называется Always On р Always миллисекунд на переключение всё кака красивая фамилия Авин да давайте последний вопрос и убегаем в кулуары потому что время закончилось нам надо проветрить ещё тут но в дискуссионной зоне Это после ковидная практика а не намёк на то что вы душнилы давайте так нормально В дискуссионной зоне можно будет всё это до обговорить Пожалуйста подскажите вот есть решение нпм они примерно тоже в этой же сфере В чём ключевые отличи Когда нужно нам применять п когда когда рованный по В чём отличие от гпма в части удержания консистентность Ну ГТО это больше про шардирование и про аналитику то есть это решение которое способно хранить большие большие данные на разных узлах эти наборы разные это про расе транзакции иее это про достаточно относительно небольшие размеры баз данных и про одинаковость данных на всех узлах поэтому для вот задачи Когда вам есть пул приложений который должен читать одни и те же данные с какого-то количества узлов это одна задача А когда вы должны запускать аналитические запросы на терабайтах данных - Это другая задача Ну вот собственно и отсюда и вытекает применение всё А теперь кому кому отдадим за вопрос кстати понра давай честно честно вот я когда смотрю всегда на затрудняются докладчики за какой-то вопрос э определить какой самый лучший А я вот а мне вот понравился вопрос про уровни изоляции да Ну да вообще это актуально ээ про Ага приз да поможи А поможите рукой А вот отлично сейчас вам сейчас вам помощники всё выдадут мы Я помощник это Т Нет Это у нас приз от Газпромнефть класс давай Да просто Выходи сюда будем фотографироваться Да и ребята вам тоже за доклад памятные призы Да спасибо большое спасибо великолепно"
}