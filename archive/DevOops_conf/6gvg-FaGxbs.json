{
  "video_id": "6gvg-FaGxbs",
  "channel": "DevOops_conf",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Привет друзья Привет всем кто здесь в зале привет всем кто слушает нас в онлайне Меня зовут Лёша кирпичников Я работаю в контуре и в программном комитете конференции devops Прежде чем я познакомлю вас с нашим докладчиком я должен Напомнить вам про три вещи три вещи во-первых тем кто сидит Здесь в зале я Напоминаю что нужно отсканировать этот QR код смотрите я первым делом это сделал чтобы вы успели пока я тут трещ отсканировать QR код присоединиться к чату этого доклада и иметь возможность задать вопросы Потому что эти вопросы если у нас останется время после доклада обсудим здесь если не останется мы их Обязательно Обсудим в дискуссионной второе собственно дискуссионного вечера мы сможем Женю мучить Вот и третье это важно как раз для тех кто в онлайне находится не тех кто здесь тех кто в онлайне у вас после доклада выскочит форма оценки доклада собственно нужно в ней соответствующие действия выполнить оценить доклад Прошу вас об этом от души всё три вещи я вспомнил теперь мне нужно представить докладчика со мной на сцене здесь сегодня человек без которого в общем-то меня на этой сцене наверное бы не было потому что найти без карты дгис Ленинградский проспект я бы не смог просто это Женя хрв Он Лид инфраструктуры в компании 2ГИС наверняка вам хорошо известный Всем привет Вот и Да можно по аплодировать очередной конференции devops я сразу же мы начинаем базу докладчиков смотреть и я сразу же говорю так Женя я напишу сам Ну во-первых потому что это как бы лёгкая карма В смысле что Женя согласится придёт сделать классный доклад мне ничего особенно делать не надо вот я там карму заработал отлично Лёша Молодец отличный член программного комитета Ну если серьёзно то конечно А я чувствую просто себя с Женей в одной лодке В каком-то смысле потому что ну я как мы оба страдальцы которые сорт свои частные облака внутри инфраструктуры компании и много докладов Женя об этом раньше делал У нас в том числе насе и про кубернетес на своём железе и про котиков и про сосуществование железа и облако В общем мне всегда на самом деле интересно послушать что же будет дальше и сейчас я предлагаю прямо этим и заняться пожалуйста поприветствуйте Лёш Спасибо большое за представление за приглашение было очень приятно получить от тея ли поэтому я сразу при доклад и мы тут все собрались Спасибо что вы собрались Давайте начнём я Женя из двагис лит инфраструктуры расскажу сегодня про стейт фулы Давайте кто вообще крутит стейт улы в кубернетес есть такие люди о довольно много Надеюсь будет что-то новое для вас Давайте расскажу про Команду вообще ну то есть Кто мы такие Почему я здесь сегодня рассказываю а инфраструктурная команда крутит кубер дсе Всё дозировано у нас Мы предоставляем р как сервис внутри ДСА также мы предоставляем базы данных как сервис внутри ДСА это кандра наши любимые логи стандартные мониторинг через Викторию потому что протей уже не справился любим что-то походить на питоне нанг и всякие обвязки и 160.000 rps 68 млн активных пользователей в день с уд до 35 ТБ Вот примерно наш дневной трафик как он идёт о чём мы поговорим поговорим мы вообще о Почему нам нужны фулы в кубе что мы жили 20 лет нормально на виртуалка Зачем что-то менять всё хорошо работало разберём различия между локальными и сетевыми вома проговори какие-то особенности для приложений которые учитывать когда мы запускаем stateful приложение в кубернетес И я расскажу интересные случаи из нашей конкретной жизни На что мы попадались как мы это боролись и что у нас сейчас всё хорошо давайте чуть-чуть синхронизируется У нас есть й к которому мы 100 лет в обед привыкли и запускаем в кубе уже там 5-8 лет приложения не хранят никакие данные запускаются всегда с чистого листа и на любой запрос отвечают Ну так же как и раньше ий это приложение которое внутри себя хранят данные эти данные можно изменять И при каждом следующем запросе у вас может быть совершенно другой ответ зачем мы вообще йф в кубернетес захотели для нас п - это та платформа которая позволяет экономить нашу операционку то есть запуская приложение в куртис мы перестаём о некоторых вещах и просто занимаемся именно запуском приложения либо развитием платформы для примера это экономия на мониторинге то есть запустив приложение сразу мы получаем алерты дашборды метрики всё всё всё всё готово из коробки логи просто Просто добавь воды просто Отправь в всё логи в эластики можешь пользоваться наша платформа особенно тем что у нас есть входящий который легко настраивается для приложений буквально в ации вва добавили приложение добавили аннотации трафик можно получать И самое главное это масштабирование если мы говорим о том что масштаби на виртуальных машинах это что-то ещё там сценарии которые по полчаса прогоняю то в кубе с 2 3 5 и через 3 минуты у вас уже дальше ш поднялся Главное чтоб ме в платформе было какс это выглядит со йм У нас есть два типа Ред это есть локальные есть сетевые реджи вот для примера У нас есть Касандра эластик довольно жирные по 5 по 35 ТБ это всё крутится уже в Кубе есть небольшой прот на постгрес ха мы начали запускать через ха оператор вроде тоже работает и от моги мы всё пытаемся избавиться но но не получается и сетевые диски мы используем в основном это для какого-то окружения неважно запу главное запуститься и чтобы там дам памяти скидывать потому что Неважно где тоже запустить а скидывать довольно дёшево поговорим мы сегодня про конкретные три вот эти вещи и на для дела и на их примерах разберём что зачем нам нужно про локальные время ответа то есть сетевой диск нам не даст время ответа там 600 наносекунд для Кандры он только 3 миллисекунды будет гонять по сети вот эти пакетики и мы устанем ждать и пользователь уйдёт И будет всем всем грустно поэтому локальные диски мы используем Когда нам вот Касандра ластик или там по двухсот гигабайт базой надо сейчас погонять Вот это туда Ну Собственно как и на виртуалка тоже раньше было и на железка локальные диски мы применяем для больших обв есть в схд иметь 48 теб диск Да это можно если у вас богатая компания и вы готовы покупать вот эти огромные схд за миллиарды рублей и потом раздавать их по сети ещё по гигабитной сети не знаю до виртуалок доно Куба но у нас такого нет поэтому мы спокойно ставим в каждый сервер один или два диска там 4 теба Эски и подключаем их вот просто подключаем их как раздел Когда мы можем использовать локальные мы нам нужно Быть готовым к тому что мы можем потерять один под одну реплику нашу и ничего не должно произойти к этому должно быть готово приложение для примера наш возьмём эластик или кассандру Ну собственно Касандра вот одат нашу карту Веги который вы видите это ВС отда из по по ключу мы отдаём туда ключ Касан возвращает за 600 наносекунд нам ответ ну там в течение дня там растёт до до тысячи на на секунду ответ но в принципе очень быстро на сети мы бы так не получили а фишка Конкретно что Касандра что эластика мы можем потерять ноду на день и проблем не будет Ну в случае с ластиком он сам будет ждать пока нода не вернётся Либо мы пока тот пот не выгоним с конкретной локальной ноды в случае с Касандра Ну например мы можем реплику перетащить тоже на другую ноду Она подтянет себе шарды которые вот были потеряны А да это как раз про самостоятельное восстановление то есть мы потеряли железку всех выселили вообще с этой железки все стартанул на соседние ноги и самостоятельно что эластик что кандр они поднялись подтянули себе данные из тех кто остался Ну то есть у нас мто конечно же есть и всё продолжает работать Всё о'кей и никакой нашей операционки в это не требуется то есть нам не надо там не знаю а ПБУ прогонять чтобы поднять виртуалку или терраформ не знаю в Асе или что-то ещё такое делать ничего нет мы просто как бы стандартной операции удаляем ноду из Куба очень крутая штука с шардирование она есть У Касандра у эластика она чуть сложнее у нас есть например из п мы такие Давай поке до се она такая Ой я полилась и в процессе старта она просто переливает на эти ноды Новые те данные которые она считает что надо перелить То есть у неё какое-то автоматическое шардирование есть вот тот пример Почему мы в кубе это крутим и нам это помогает очень сильно экономить операционку крутим мы всё на вот этой вот страшной схеме обсуждать мы не будем Потому что ина тут ВС время выйдет имы устам Я постараюсь схему чуть-чуть попроще Рассказать своими словами продукт называется топ LM ссылка на него будет презентация будет сразу после доклада как это выглядит в кубе мы стартуем какой-то под и просим создай нам пожалуйста какой-то ВОМ это pvc это запрос на persistent Vol этот запрос уходит в топ оператор который собственно управляет этими вома оператор идёт на какую-то ноду ВМ раздел который есть и нарезает там нужный нам ВОМ нужного намм размеры дальше этот ВОМ монтируется к поду и вот самая магия которая здесь есть это вот это это когда мы под привязываем к ноде и делает это топ оператор через affinity для конкретного пода он делает это что-то типа и при старте при перезапуске этого пода Э вы всегда старта на одной и той же ноде и таким образом э у вас есть проблема что если нода вышла то вы не стартон Но с другой стороны у вас нет проблемы Что если удалили под и он уехал на ноду с пустыми данными этого нету ну типа тут всё О'кей А из плюсов этой штуки она умеет в динамический провижен а захотели гигабайт захотели два захотели 10 захотели 200 Главное чтобы было место на вашем сервере она умеет очень лёгкое расширение То есть вы попросили увеличить мня persistent Volume с 10 до 20 ГБ она прямо на литу его увеличила lvm раздел и файлу тоже расширила даже поды перезапускать не нужно а сразу из коробки есть какие-то метрики по потреблению persistent вома Ну и быстры довольно быстро про происходит то есть никаких заковраш ноду Куба вам нужно создать Ну создать задел подм разделы то есть что там ПВ ВГ создать и вс Оста чтобы оператор смог с этим работать про локальный примерно Понятно пойдём к сетевым мы их применяем Когда нам нужны какие-то небольшие размеры Когда де Stage тест что-то ещё то есть не Сильно критично к проду мы ожидаем что на проде мы это тоже можем применять потому что нам нужна Надёжность этих данных например какая-то пасова база для какого-то стартапа который только вот внутри два гса Родился он готов там бэкапить свои данные э готов ожидать что э ну то есть то что если локальный диск умрёт то не ну эти данные не потеряются просто стартануть в другом месте и сетевой ВОМ неважен Где где можно запускаться То есть это довольно хороший кейс для крон Job мы можем НБУ с каким-то pers ВОМ там не знаю на гигабайт над запускать что-то с данными обрабатывать и она запустится где угодно в кластере для примера у нас по стандартные две реплики Master slave при перезапуске пода он тут же стартует На соседней ноде То есть это минимальный простой с Сва и логи которые пишутся в ма они не успевают протухнуть То есть у вас репликация не разваливается даже в варианте из коробки который мы мы используем спил оператор для пост госа в кубе а там варианте из коробки два валока и при перезапуске пода обычно этого даже хватает то есть ничего кастомизировать не нужно а схем подключения сетевых люмов А примерно похожая это Мы также делаем запрос на persistent Volume уже запрос идёт в какой-то схд оператор их на самом деле там около сотни уже пона Придумано и для облаков и для локальных и для всего остального в нашем случае это ф есть такая большая коробочка в цеф мы идём к нему спрашиваем Дай нам пожалуйста нужный размер Он нам его даёт всё здорово подключили к поду и они теперь связаны Из плюсов сетевых люмов Ну тот же самый динамический про сколько запросили Столько получили это не такое лёгкое но тоже простое расширение А мы увеличиваем размер нам надо перезапустить под при перезапуске он просто А расширит сам файл Ну сам диск и файловую систему А и быстрое восстановление то что мы получим сразу же э при дрейнеры при удалении пода мы сразу стартам на соседний и всё продолжит работать а есть минусы они довольно критичны А во-первых вам нужна очень быстрая сеть потому что ну если вы хотите перегонять много данных вам нужны быстрые диски под Сефа если в нашем случае это Сашки которые там стоят уже 100 лет это Ну так себе быстрая сеть быстрые диски и надо учитывать что есть ещё деградация схд То есть если не знаю добавили но добавили диск цеф удалили диск ф там с фом что-то происходит это всё отражается на вас на ваших приложениях если у Вас завязано например там 100 приложений на это диско хранилище ним происходят какие-то проблемы у вас происходят проблемы в 100 приложениях тут тоже такое если это прод то все начинают бегать прыгать Ну не очень приятная ситуация поговорим про лы именно особенности для приложений на что нужно обратить внимание когда мы хотим завести лы в чнм с локальных это наш Ну несколько вещей которые пря нужно сделать а нужно настроить пот Анти affinity все знают что такое пот Анти affinity Окей под афти нам рассказывает о том что поды нельзя размещать рядом на одной железке То есть если вы запустили ваших два постр или две Кассандры и они обе упали на одну железку с локальным диском ну естественно как только железка выпала Всё спасибо До свидания следующий поэтому настраиваем под А чтобы поды друг с другом м рядом не дулисьма Если вы ушли на обслуживание например на полчасика А в это время кто-то железку занял в цпу в памяти ещё в чём-то вы не можете зашили А вы только на неё и можете зашили то здесь нужно что-то придумать чтобы ну зашили не знаю выгнать соседей Вы не можете потому что это коммуналка и ты пока пойдёшь соседей искать там пройдёт ещ полдня поэтому мы специально для локальных дисков сделали хум установку приоритета на поды выше чем у всех остальных То есть если вы вышли на обслуживание полчаса где-то погуляли возвращаетесь ваше место заняты и ваш под с локальными дисками просто раздвигает тех кто есть на ноде и всё Дули остальные уезжают на другие ноды всё О'кей под distr bget он в принципе важен не только для локальных дисков но и для вообще всех подов в кубе Он позволяет нам нормально обслуживать кубес когда идёт Ну команда обслуживания делает Рон апдейт всех НОД в кубе И вот она знает что следующую ноду я могу обновить потому что у меня все удовлетворяют условия под есть ещё особенности жизни с локальными дисками в коммуналке в том что если вы заедете дм какими-то жирными А подами на одну ноду Вы можете получить его вей заехала Касандра которая очень много отдаёт и эластик который очень много пишет Ну возможно ваш диск не справится аэ ещё одна фишка мы можем не квотирования там Дайте мне 5 Гб Дайте мне 10 Да Заведите мне тикет короче нет этого нету не надо Всё всё О'кей мы ограничены просто диском если ты запросил 8 ТБ у тебя в кластери максимум 4 ну сорян а ну классика stateful приложение минимум три реплики чтобы стандартный кворум был в принципе со Спи оператором с посо можно и две две делать он тоже нормально работает ну и йф и локальные волю - это не Панацея поэтому не забываем про бэкапы Кто делает бэкапы что-то Не все Да от ракета никто не застрахован поэтому не забывайте про бэкапы А ещё лучше проверяйте их что их можно восстановить Давайте про сетевые воу Посмотрим по аффинити для сетевых лемов в принципе важен но уже не так обязателен даже если у нас два пода заехали на одну ноду то под БАД котором будет дальше он не даст выгнать оба пода сразу ну да мы можем потерять если нода там как-то не знаю По памяти умерла там кто-нибудь питание в дата-центре выключил а типа в этот момент мы потеряем работу с этой ноды обоих подов но при graceful шатдаун ноды мы Не сломаем Вот как раз о нём а то есть что-то из этого точно должно быть а с сетевыми волюма ситуация такая что если мы делаем много операций с дисками мы 100% рвёмся на eade Либо это будет eade в сеть Либо это будет её в диск мы увидим и как бы будем страдать поэтому тут нужно такую градацию иметь То есть если вы можете запуститься на сетевом волюме какой-то mvp proof of Concept своего приложения то потом надо отслеживать вот а дальше Моё приложение пошло в прото оно точно здесь должно остаться или мы всё-таки на локальные переедем вот здесь надо подумать и следить за графиками а для нас сетевые воу - это конечный ресурс поэтому мы их квоти и имеем какие-то ограничения из коробки потому что во-первых ну их не так много нам их выделяют по запросу и мы каждой команде выдали по 50 ГБ вот если надо больше больше то Приходите к нам обсудим потому что придут к нам мы пойдём к другой команде она нам выдаст ещё и вот это вот всё поэтому здесь ограничение обязательно и у нас есть омит поэтому здесь тоже можно нарваться с квотирование нужны ещ и алерты если вы предоставляете сетевые воу о заботьтесь метриками и артами чтобы неожиданно не оказалось что вы забили весь сетевой ВОМ стандартная история про слы и реплики Ну и про бэкапы тоже тут как бы что там что тут Давайте про наш опыт поделюсь про интересные какие-то штуки Когда у вас вы начинаете работать со фми и вома у вас возникает вот этот процесс Мы например запустили наше приложение оно в двух репликах на фе эти реплики это база данных или там что угодно Короче говоря и мы вот на фе мы его подключили оно подключено к нашему Фу вроде он сный быстрый но нам уже не хватает производительности и мы такие Окей кубер давай короче вот Поменяй сфа на топ А он такой А я не могу этор Делай что хочешь придумывай своё и приходится придумывать Вот это капец Какая Страшная штука мы берём и удаляем йф Set чтобы нам поменять один несчастный параметр нам приходится рисковать и удалять йф Set благо куб предусмотрел вот эту опцию кафан Она позволяет оставить сиротами поды которые Ну собственно были под управлением этим фетом по себе скажу короче первый раз удалять страшно во второй уже нормально выполнили и у нас вот вот такая ситуация и вот здесь короче главное не не продол бать вот этот момент когда вы такие удалили СТС и вас там не знаю на обед позвали На митинг ещё куда-нибудь А в это время обслуживание Куба идёт и у вас Э Ваш пот удаляется и всё и не стартует короче вот вот здесь надо делать быстро удалили а следом мы создаём такой же stateful сет э вот прямо в точности Мы же можем выгрузить то что у нас есть и загрузить А лучше если это будет в C будет вообще топчик и главное что меняем Stage кла уже на топ lvm лейблы оставляем те же самые подхватывает он те же поды всё дальше продолжает работать Всё Огонёк что дальше чтобы нам поменять Цефа Мы выполняем ещё одну такую приятную операцию начинаем удалять свои поды и удаляем Не только поды но и удаляем ещё и пвц Ну чтобы он нам новую создал и новый диск выдал удалили шедуле видит несправедливость и пере создат собственно под и ПЦ к нему и мы получаем уже опа на новом вообще огонь Здорово один уже работает Быстро второй медленно повторяем операцию со следующим Всё мы переехали там всё нормально миграция данных обычно должна чеками и всем остальным контролироваться если этого Конечно нет да надо подождать определённое время которое вы даже не узна Но это оставим для дискуссии давайте к следующей задач когда к нам Прибежал ко мне как-то один из разработчиков говорит Жень мне надо 200 Гб короче скачать обсчитать и начать раздавать людям каждый день так надо делать дайм дай-ка мне виртуалку я такой Что Зачем Почему тебе виртуалка нужна Ну то есть Вот задачка такая что он получает данны как-то их готовит ИТ ве день ли че не умею настраивать я там не знаю как это делается поэтому я только в куб имею в кубе Задачка это решается так она не очень типичная но в принципе решается у нас есть вот три наших сущности это есть собственно который будет раздавать весь день наши Данные есть калькулятор который данные умеет забирать и обсчитывают крон Джаба который раз в день запускается Э мы знаем что мы привязаны к одной конкретной ноде и мы не можем монтировать мы не можем монтировать локальный волюм короче расщепить его на две железки там на три на пять на всё остальное А и не можем монтировать в один локальный волюм э два пода на запись нав Ну у нас так система конечно позволит но там Хаки неудобства Короче так не надо поэтому мы наш Джинс монтируем именно к этому вому как на просто на чтение а ДБУ которая запускается раз в день мы монтируем к нему на запись И всё И главное там свести каталоги к одному месту Джаба вам данные подготавливает сливает в каталоге жин их раздаёт все счастливы А выглядит это так основная магия здесь то что вот этот калькулятор который мы стартуем у него есть по affinity То есть он должен стартовать рядом с нашим нксо и всё и он там ста стартует здесь есть одна проблема Это в одном экземпляре Ну не очень удобно красиво это работает самый лучший вариант вместо Local pv использовать S3 но там был кейс что типа нам от вендора пришло мы ничего не умеем переделывать надо срочно всё поехали идём дальше идём дальше к моему любимому эластик у кого эластик тоже любимый Здорово ша Я помню Лёша рассказывал мне сво про свой эластик эластик когда-то Дожил до того что ему надо переехать в кубер мы перевезли все наши эластики в кубер но перед этим были какие-то с ним вопросы это вот история про них эластик - это jav приложение мы как умные Маши знаем Как настраивать кучу в Джаве выделили 16 Гб как было и На прошлой железке мы знаем что реквесты по памяти для джавы обычно стоит выделять чуть-чуть больше кучи потому что это же Java она только в куче своей возится зачем что-то ещё надо поэтому выделили реквесты по памяти чу чуть больше э ну а так как ей больше памяти не надо Ну давайте выделим ей лимиты по памяти ещё на гигабайт больше Боже Пусть там свои 2 ГБ съест не знаю что с ними сделает а всего у нас железка Вот такая ну типа 64 гига мы выделили 18 давайте для упрощение Будем считать что вот эластик пока что единственный потребитель на этой железке мы такие запустили Что может пойти не так запустили трафик и тут короче всё сломалось просто диск в полку ишкат чтото постоянно с диска поднимается считается смотрим вот железки вот дру ле эк настроен одинаково единственно вот ти и появился что происходит что не так начали копать и короче дошли до того что на диске ластик был там порядка 200 гиб это нода и он просто на отдачу не успевал нам из выдавать данто с более популярный и он эти Данные постоянно в память в несчастные вот эти 2 ГБ лимита памяти которые мы ему с Барского плеча отсыпали Он пытался в их втиснуть и ему не хватало полечить всё довольно просто это были проблема с Шом Мы просто не ну нам не хватало памяти для него мы увеличили лимиты по памяти уже чуть-чуть расширили и всё короче всё прошло всё замечательно всё заработало кайф лимиты на ничего не стоит увеличить ну по моему мнению ничего не стоит потому что Linux как-то сам разберётся там дальше как вот этим Шом рулить и более приоритетно туда складывать тем кому нужно поэтому не не зажимай jav которая умеет хочет много работать с диском лимитов по памяти Да и в принципе всем остальным приложениям тоже а идём дальше есть такая штука как generics это развитие ров но с гарантией с гарантией размера хранилище Ну которое мы Запроси то есть нам надо 100 ГБ 200 там 5 А в этого может быть не быть появилось не так давно там в 123 кубе что ли в зашла не помню уже в общем довольно новая под нам нужно было стартовать с гарантией наличия места то есть вот э вот крон Джаба которая должна была скачать 100 гиб данных обсчитать их и где-то разложить на диски Вот это гарантия места Ну то есть её можно было решить так А ещё одна из Ещё ещё одна штука которая ещё genic закрывает нашу потребность это в том что мы можем стартовать в любой ситуации на любой ноге вот эти волю они не перманентные они э Ну временны просто отработал умер новый запустился новый создался и нам самое важное было от них что это быстрые диски То есть это локальные воу и получается нам нужно где-то на локальных вома искать место под задачу У нас по они подошли под какие-то крон джобы где надо что-то скачать рассчитать положить назад в S3 или куда-то в ещё хранили подра довольно хорошо подошли это просто У нас тоже в Кубе У нас есть пул машинок там стартуют поды поды выступают в линеров и вот у тебя сколько есть места столько он забивает кшм потом умирает стартует новый кэш чистый и поехали и какие-то такие приложения которые при старте начинают скачивать себе данные что-то с ними делают и ну там либо они раздают Либо они кэши себе скачивают Ну что-то такое происходит и мы даже если мы теряем эти поды то они опять с данного стартануть опять скачаю все эти кши Это займёт там 30 секунд но проблем с этим нет Главное что они стартануть где угодно и могут работать с Шами на локальном диске это уже следующая тема кто узнаёт картину у кого Такие бывают про эопы собственно в кубе я уже говорил о том что у нас два пода с локальными дисками могут вызвать повышение Ну собственно что иногда и бывает как это у нас вообще было мы порвались на этом один раз то есть вот когда у нас реально кандр с ластиком сошлись на одной ног они там начали бороться за диск ну сервер слегка умер Мы их вот эту конкретную кассандру с ластиком разделили через по некрасиво неправильно беспокоимся не беспокоимся ещ Почему Потому что у нас сейчас все локальные диски на сдшка А в чем году сдш уже с ме примерно по стоимости очень похожи и мы ожидаем что в д чертом мы уже начнём переходить на Н И эта ситуация всё реже и реже будет повторяться ничего не делаем потому что ещё костыли это ВС решение это реально довольно много работы то есть там месяцев работы неко скорее всего мы закопаем как у сообщества Мы реально ждём чтобы сообщество это сделал а мы себе внедрили нативные инструменты очень здорово что в куб просочились все группы вторые То есть все группа вторых уже ограничение по иоп сам есть это можно за использовать но проблема в том что разрабы Куба ещё это ну не внедрили у себя и на э тему у них есть по крайней мере видение of которое они хотят и там про ИОС и потребление диска уже есть и в докладе от Лёши Мне вчера понравилось что он тикет про eop лимиты тоже тоже тоже включил то есть мы все натыкается короче говоря на одно и тоже тикеты более-менее живые То есть им где-то 2-3 года но общение там идёт поэтому есть шанс что не знаю к Кубу 130 может быть это всё продвинется начинаем заканчивать Давайте расскажу Вот про какие pers Во мы вообще используем чтобы нашу инфраструктуру не нагружать мы используем вот этот оператор О котором я показывал и он у нас в двух видах это во-первых наши стандартные диски на SSD которые мы раздаём всем и недавно у нас появились диски на HDD это ну реально терабайтный диски на каждой ноде куба где данных хранится очень много а потребителей у них довольно мало Ну это внутренняя команда там из знаю 50 человек которые работает с этими данными поэтому нас Вполне устраивает производительность сетевых люмов У нас два У нас есть медленное но большое и мы можем там не знаю терабайт из него забрать и можем с ними что-то работать либо надувные SSD цефы которые в каждом дата-центре У нас есть если вам нужно что-то запустить можете использовать их и вот э штука о которой я даже говорить не хочу вслух потому что го прон с костылями и всем остальным вот единствен короче Лер фича это вот это то то ради чего стоит её взять если она вам нужна но лучше Используйте S3 просто S3 интерфейс в который вы будете пушить свои данные Итак какой у нас правильный получается в любом случае когда мы запускаем приложение вбе мыж думать ОВО есть это не од реплика кою мы потели данными и приложение легло и всё будет плохо Хотя бы три две две в случае если у вас умеет вот этот файвер какой-то делать 3 5 7 и так далее Э мы должны быть готовы к тому что выпадение одного пода - это вообще нормально ну типа настройте как я говорю три буквы п под disruption Budget под priority под Anti affinity вот настроили и вы вы должны уметь с этим работать ну то есть ваше приложение должно переживать выпадение одного пода если не переживают давайте пока Притормози с кубом если мы используем больше типов persistent Вов мы более не знаю более качественно используем нашу инфраструктуру то есть разработчика надо донести что у нас есть несколько под разные задачи давайте выберем с вами разработчиками конкретный persistent Vol который будем использовать топ можем советовать вообще годная штука и есть подозрение что сам куб движется вот туда и возможно Этот проект скоро окажется какой-то частью Куба и Ну вы сразу будете с ним работать мы больше не поднимаем никакие виртуалки если кто-то к нам приходит мы я теперь буду им показывать доклад И всё И пусть заезжают в кубы Ну и не забываем про бэкапы Спасибо Спасибо Женя у нас есть совершенно шикарные 6 минут на вопросы Из зала но же да вы готов вте вопрос сейчас будем но я тут Звал всех в чат И в чате есть вопрос поэто Ты ты мне подложил Короче я его написал Олег Вознесенский спрашивает почему как раз про топ лвм сейчас говорил и про то что он идёт к тому что возможно часть кубер будет Почему топ LM а не Local Stage у нас был я не знаю как но при исследовании какой тип локальных дисков взять топ просто выпал из глаз Ну типа его не было и мы взяли нативные локальные стод от куба с случайно пропустили я не знаю как может он через месяц после исследования родился Может что-то ещ произошло его не было в первоначальном исследовани здесь нормально работает но с такими костылями о которых я даже вспоминать не хочу То есть мы что делали мы создавали на каждой ноде через UN deploy loop девайсы заданного размера и потом их раздавали как реджи короче слава Богу мы это забыли и больше не пользуемся всё понятно был вопрос где-то вот там Да вот вот есть рука во втором ряду Да добрый Спасибо за доклад а эластик используется по классике для логов нет эластик используется для нескольких целей и как для логов по классике и как хранилище которое раздаёт очень много данных и внутри него происходят поиски То есть это прямо как база данных а вот это классическое партицирование Hot warm Cold оно присутствует и пробовали ли вы загиб что Hot warm лежат допустим на Local pv а колды уезжают уже на шару сетевая шара довольно дорогая Для нас по крайней мере поэтому мы в таком виде его не используем У нас есть ход и и ход и варм ноды А колм у нас будет S3 хранилище Когда мы будем бэкапить индексы и складывать их и потом если надо разворачивать оттуда вот будет да там ещё вот возле стены есть вопрос в дальних рядах Хорошо ли оттуда видно Да здравствуйте Спасибо за доклад У меня вопрос там про S3 несколько раз прозвучало Вы проне S3 используете или это просто в контексте что б капчики туда льются это просто в контексте что туда короче лучше использовать для работы с файлами Ну так более нативно для ДТИ факторных приложений и бэкапы мы тоже будем складывать в Если необходимо прора как такового у нас нету у нас есть он предоставляет два интерфейса rbd и S3 и вот мы rbd подключили к Кубу а с S3 работаем по Репе А понял спасибо А здесь в последнем ряду здесь есть вопрос и так сюда мало смотрю есть вопросы да у кого-то Ага всё увидел увидел спасибо за доклад Вопрос такой максимальный размер кластера не знаю эластика в подах подах да подах эластика по-моему штук 18 чтото такое там сами диски не очень большие они помоему гигабайт то ли 200 то или 300 А вот именно расчётная часть у него тяжёлая Поэтому приходится горизонтально масштабироваться просто кажется что отказ одной защититься от отказа одной реплики недостаточно в этом случае потому что вс-таки железки если считать что каждый пот на своей летно достаточно велика Я согласен Когда у нас становится много получается реплик на каждой реплике У нас есть какой-то шарт в эластики у эластика есть реплики на шарды И мы обычно делаем минимум одну а в этих случаях мы уже больше делаем типа ДТП потому что ну там там вопрос к эку когда мы делаем на него запрос остается считать по всем шардам которы уго Чем больше шадов ты сделаешь и больше эпли к ним тем сильнее ты разма нагрузку поэтому у нас вот вос подовой конфигурации там 100% не одна реплика там три или четыре должно быть не пробовали выкинуть нап и сделать фс У вас есть ф уже я заходил с ффм к ребятам раз пять меня тряпками короче гнали оттуда мы пользовались бом об работает нормально Я читал что и в Кубе есть какой-то оператор который предоставляет cfs на локальных дисках Вот это умеет Но короче инженеры пока не готовы к этому понял Спасибо Давайте мы успеем один вопрос здесь в последнем ряду возле колонны а остальные видимо переведём в дискуссионная Спасибо за доклад у меня такой вопрос какие инструменты используете и используете ли для предотвращения Ну Сплит брейв условно если у нас есть Ну кластер ПД Да и одна ну Мастер он не падает просто теряет там не знаю по сети связь с со свом и приложения Ну разные несколько экземпляров Да одного и того же приложения начинают писать одновременно в разные инстансы то есть Есть ли какой-то механизм который бы при обнаружении вот такого Ну сетевого м ну сетевого недоступности сбрасывал бы ну соединение к одному из инстан сов Да спасибо за вопрос в деталях я ответить не смогу я скажу Так мы доверяем тем операторам которые мы плом в куб То есть у нас есть спила для постгрес который в принципе мастер слей плюс патроне внутри вот мы доверяем патроне что он сам сам эту ситуацию обру и сделает а мы доверяем Стрим оператору где у нас кавка внутри и она как-то сама короче вот умеет переживать падение может быть кто-то из моих ребят сможет детали рассказать но вот я к сожалению не могу то есть проблем со слит Брейном за вот 5 лет что мы йф крутим У нас не было спасибо большое друзья что пришли на доклад мы сейчас пойдём в дискуссионную комнату А пока предлагаю поблагодарить ещё раз Женю да"
}