{
  "video_id": "By-DEHiH5mg",
  "channel": "DevOops_conf",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "доброе утро доброе утро родной питер как сказали меня зовут антон или как называют меня друзья and white я приехал к вам из израиля но до того как поселился в израиле я родился и 16 лет прожил в этом замечательном городе чем я конечно очень приятно стоять на этой сцене за что большое спасибо организаторам конференции отдельное спасибо баруха в израиле последние 18 лет я работаю в области информационных технологий последние три из них являюсь соучредителем и генеральным консультантом консалтинга который называется томата softer мы занимаемся тем что помогаем большим и маленьким компаниям быстрее доставлять качественный софт и вот как бы а быстрой доставки софта мы и будем с вами разговаривать называться наш разговор будет как вы видите тупые сервисы в умных сетях и обсуждать мы будем как нам деплоить как настоящим ниндзя при помощи сервисного сито что такое деплоить как ниндзя для начала ниндзя по-японски кто знает что значит нельзя по-японски это тот кто прячется тот кого никто не видит значит деплоить как нинзя значит деплоить так чтобы никто ничего не заметил пока не стало слишком поздно но вторых почему тупые сервисы в умных сетях дело в том что если верить статистике то интеллектом выше среднего обладают лишь пять процентов населения планеты а это значит что остальные 95 процентов из нас как бы нам не хотелось верить в обратное включая нас находящихся на этой конференции обладают интеллектом среднем и ниже среднего то есть по простому мы все с вами друзья иногда тупим ну кроме пяти процентов я так этим занимаюсь просто постоянно и задав следующие выводы на взгляд во первых тупить это не позор но потому что большинство из нас этим занимается во вторых и даже если мы тупим мы можем все еще деплоить как ниндзя в третьих возможно это среднестатистическая тупость является причиной повального увлечения нашей индустрии микро сервисными архитектурами вот кому тут уже довелось счастье строить и поддерживать микро сервисной системы поднимите руки чудесно руки есть значит есть кому выразить свое соболезнование потому что микро сервиса у них есть один очень важный признак да они несут с собой макро болью почему почему так больно ну потому что с откатом разломали наша система на много маленьких сервисов которые общаются между собой через сетку мы неминуемо получаем что распределенную систему правильно и тут сразу в игру вступает все так называемые ошибки распределенных систем которые еще в девяносто четвертом году прошлого века определил л питер дочь со своими товарищами мы люди плохо понимаем распределенные системы они не укладываются нам в голову сложность таких систем уже больше не находятся в действие каждого отдельного компонента сложность лежит во взаимодействии между этими компонентами соответствием большую система частей тем сложнее становится этого взаимодействия и там начинают происходить всякие такие штуки которые мы не можно предусмотреть мы поставим наступаем на одни и те же грабли до возникают новые виды отказов новые виды ошибок которые мы так называем да кстати вот когда я делал доклад буквально два дня назад социальные сети подкинули очень соответствующий men так вот там ступа возникают так называемые грабли распределенных систем вот некоторые примеры до таких кораблей например неправильная настройка запасного варианта если сервис которым мы обращаемся недоступен или например а кстати у меня же есть clicker шквал повторно копыта подсоединения дата называет так называемая грохочущие стадо если мы неправильно настроили тайм-аут или возможно как раз из-за того что мы неправильно настроили тайм-аут или по любой другой причине какая-то нисходящая зависимость недоступна и из-за этого все восходящие зависимости не знают что им делать и обязательно в системе кроется какая-нибудь единая точка сбоя возможно именно этот сервис который мы построили которые мы выкатили мы просто об этом ничего не знали и в какой-то момент это сбу . сбоя система обязательно обвалить но мы наступаем на грабли уже не первый год до сервис на ориентированной архитектуры приносят больно много лет еще до того как мы придумали новые хипстерская слова микро сервисы поэтому мы для себя выработали некоторые техники защиты до вас того как по набивали шишек кстати все грабли распределенных систем описаны вот здесь это манифест хаос на инженерии которая буквально месяца четыре назад мы перебирали на русский язык так что заходите читайте интересуйтесь паттерны и устойчивости ну такие как например пола подключений детектора ошибок разнообразные стратегии откаты и дублирования такие как прерыватели x3 национальный откат конечно же балансировщик и куда без них и также различные стратегии ограничения трафика такие как как кэширование ограничения по чистоте то есть появилось огромное количество новых забот о которых нам нужно думать помимо бизнес-логики нашей собственно самой по себе информационной системой помимо этого из-за того что наш распределенная система они не просто распределенные системы они еще и облачные эфемерные виртуальные появилась еще энное количество забот ну например как найти сервис на просторах нашего облака да нам нужен сервис discovery и теперь когда у нас много инстанциях каждого сервиса чтобы понять что вообще происходит системе нам нужны новые подходы наблюдаемости такие как с определенные трассирования агрегация логов специалистов и безопасности говорят если наши сервисы так активно общаются между собой то теперь уже недостаточно защищать по периметру нужно чтобы ещё все сообщение между сервисами тоже была защищена мучил tls вопрос спорный но когда специалисты по безопасности чего-то хотят они как правило этого добиваются так что нам приходится соответствовать ну и в конце концов ради чего мы вообще устроили себе эту головную боль и зачем мы разломали систему на много маленьких кусочков как правило причиной до основных 2 мы хотим добиться большей гибкости и нет возможности непрерывной доставки быстрой доставки с авто поэтому нам необходимо поддерживать новые стратегии непрерывные непрерывной заливки такие как как минимум роллинга птиц да но в идеале было бы неплохо также иметь возможность делать dark ланч и поддерживать blue гривен и канареечный диплом и возможность отзеркаливать трафик с продакш ананас ты джига обратно все что что мы называем деплоить как mandy так как же мы это делаем как мы помогаем нашим систему которую и так в общем то занимаются много чем еще вот помнить обо всем этом ну пионерами краси разных систем те же самый netflix google twitter нет изначально делали библиотеками да они написали для себя потом за open source или мы тоже стали пользоваться за что им большое спасибо но у библиотек есть одна проблема на самом деле две первая проблема заключается то что библиотеки привязана к определённому языку и фреймворка значит они неминуемо отменяют такое преимущество микросистем как по legacy возможность строить каждый сервис на том языке и free мурки которым больше подходит во вторых в тот момент когда все сервисы зависит от одних и тех же библиотек то нам нужно управлять версии метит зависимости кто любит управлять версиями зависимости никто потому что это нудная кропотливая работа и еще большая головная боль соответственно если мы не хотим управлять версиями зависимость это все сервисы должны зависеть от одних и тех же версий библиотек и тогда наша прекрасная гибкая микро сервисный система превращается самый ужасный вид монолита которыми на себе представить распределенный монолит для того чтобы этого не происходило умные парни сегодня предлагают и девчонки предлагают новое решение сервисная сито или сервис меж по-английски подход изначально популяризирован и такими проектами к клинкер де и онго это другая легковесный очень гибко конфигурируемые прокси это что сегодня пришло проект о котором мы с вами будем разговорит под названием i still часть сенсеев foundation так что же нам даёт истину во первых можно сказать из коробочки умные роутинга балансировка во вторых подробные telemetry я тоже прямо из коробочки независимость от платформы и языка по крайне мере декларируемые независимо страшно сегодня если все еще очень сильно привязан к бернетт асу да можно запускать на наводи на клауд foundry в дальнейшем обещается что можно будет запускать где угодно даже в космосе но пока чтобы есть привязка впрочем для нас кубер нас уже собственно говоря дом родной так что чего же плохого встроенная безопасность о которой никто все-таки думаете особенно не хочет но приходится так что было бы неплохо если бы решение было такое чтобы прямо коробочное и централизованный контроль над этим всем хорошо это или плохо мы об этом поговорим дальше все это самое главное без изменения в коде приложения ну почти без изменений мы сейчас увидим какие небольшие изменения все-таки возможно придется сделать что получить некоторые из этих пряников как естественно это делает вот архитектура но по большому счету как мы сказали много легковесных прокси из тела подсаживает каждому нашему сервису прокси тот самый on guy on work выпущенный компанией лифт испытанный вроде испытаны в бою уже обработав ши и миллиарды и миллиарды request of в их продакшене он садится рядом с каждым сервисом перехватывает весь трафик который входит и выходит из сервиса и что-то с ним делает как он решает что с ним делать за эти решения отвечает контроль плейлисте у которой находится здесь внизу состоит из трех основных компонентов это пайлот собственно тот кто отвечает за конфигурацию из arrow thing это миксер это такой мозг в центральный мозг который там сидит все весь трафик которые возникают сначала прокси идет на миксер и спрашивать его что с трафиком делать какую политику к нему применить кого пущать кого не пущать и куда и зачем после того как соединение произошло всю информацию об общении между сервисами опять же на мой сливает на миксер для того чтобы миксер мог это передать дальше большому брату системы сбора телеметрии вот ну и последняя компоненты цитадель отвечает за security всего этого сообщения то есть по большому счету просто генерит сертификаты и раздает их на прокси достаточно просто все в теории давайте посмотрим как это все работает на практике смотреть будем на примере такой маленькой дома аппликация который не делает ничего кроме того что сервиса общаются между собой спрашивают как друг друга зовут есть один фронт-энд 2 бэг-энда вот все они между собой разговоре то есть и будет этим управлять давайте смотреть как это выглядит вот так итак у нас как видите здесь есть кластер в котором три но да замечательно и здесь уже у нас предустановлен из тела который по умолчанию устанавливается в свой собственный namespace под названием истину систем давайте посмотрим что у нас там бежит ну вот здесь вот бегут все те аккуме вам рассказывал вот цитадель вот пайлот вот тут есть даже несколько кодов которые принадлежат миксера 2 пода которые обслуживают полисе один по телеметрии кроме этого есть еще всякие сопровождающие сервиса которых мы на минутку поговорим есть вот здесь один интересный под который называется сайт карен доктор тоже через пару минут и вот этот вот компонент который появился буквально кстати недавно есть вышла первая production ради версия 100 сейчас уже есть 102 и вот там появился этот компонент который называется голей который в общем-то и является 1 ступень как от связывание este от кубер не tissot на нынешний момент и стью общается с кубинцем посредством большого количества кастом ресурсов если мы посмотрим то увидим что их реально немало даже вот можно посчитать целых 50 штук нормально кстати видно код на на экране все на задних рядах видно не видно скажите так вот для того чтобы не приходилось всем компонентам и сил общаться с купер нити сам вот галей будет таким слоем абстракции который абстрагируется все остальное и стирку вернется и кроме этого будет являться отличным контроллером который будет отвечать за все проверки вот пока он только частично это делает замечательно для того значит у нас есть аппликация для того чтобы посмотреть как она работает нам желательно конечно же запустить какое-то приложение для этого мы пойдём сюда и запустим скрипт с который делает диплом wall давайте пока он бежит посмотрим что мы здесь делаем мы создаем нашем кластере новый namespace мы ставим на этот план м space лейбл и стивен джексон равняется не был как мы сказали работаете осуществляется прокси который загоняется рядом с каждым сервисом то есть по большому счету сайт карт катар который находится в каждом поди как он туда попадает либо можно туда его загнать ручку и при помощи команды утилиты есть у кота называется и стейси тел либо так как это делают настоящие ниндзя которые хотят чтоб все было автоматически незаметно мы ставим лейбл на каждый namespace который нас интересует этот самый сайт к ranger который мы видели до этого заботиться о том чтобы в момент создания кода в нем появился добавочный контейнер и потом мы вот собственно говорят тепло им всю нашу все наше приложение отдельно каждый сервис плюс добавляем еще маленький под который называется laughter который позволит нам создавать нагрузку на наше приложение давайте посмотрим как выглядит ну вот например наш сервис под названием алеф как мы видим простенький сервис открыты на 2 портах и очень тоже простенький deployment один контейнер версия 0.2 и на нем стоят лейблы и пеппи алеф который позволяет а кстати да почему бы к называется алеф и бет это 2pac две первые буквы еврейского лобби то если у кого-то возникла возник вопрос так вот олив привязан к сервису олив и есть добавочный лейбл ввержен в 02 обратите на это внимание нам это будет важно замечательно значит приложение побежала мы можем теперь увидеть что в тот момент когда оно создалось у нас действительно вместо того же одного контейнера для дипломанта который мы попросили говорится я же просил один контейнер а получил два контейнера да и вот именно и так мы видим что в каждом нашем поле появился маленький прокси замечательно приложение бежит что мы хотим первую очередь сделать когда приложение бежит мы хотим убедиться что она работает на че нам нужно на него посмотреть данный момент действительно есть прокси но они пока еще ничего не делают они не от конфигурирование давайте мы не дать им какую-то первоначальную конфигурацию сделаем это при помощи вот это вот я амалика с мы сначала запустим а потом посмотрим как он выглядит вот он побежал и вот так вот он выглядит что мы здесь с вами создали мы создали объект истек от r называется гей твой как мы обычно открываем сервис снаружи из кластера мы либо создаем нод порт не было балансирного пользуемся ingress он до каким-нить основаны там на engine.exe на трафике на чем угодно и steel заменяет привычное нам ingress со своим объектам под названием гей твой который более гибки более конфигурирование привязан его собственным и ресурсам и вот мы создаем дед твой под названием games gateway и привязываем к нему другой ресурс истек который называется virtual сервис вершил сервис называется games вот он здесь привязан gipfel и он говорит весь трафик не важен не важно какой который заходит мы перенаправляем на сервис и сервис registry купюрница который называется фронт namespace и games также мы создаем вертел сервисы очень просты для бэг-энда алиф и б н д bad просто которой говорят если зашел сервис на хост алеф перенаправляя его на олив . games и связь и кластер local то же самое мы говорим для бета все все сервисы между собой разговаривают и steel следит за всем трафиком нам осталось посмотреть на аппликация что он на на приложение что нам для этого нужно сделать для этого нам нужно узнать где же бежит тот самый gateway нашего из тела мы сейчас обнаружим как у него адрес не пей вот у него адрес и сейчас самый волнительный момент потому что сейчас либо сработает либо не сработает сработала ура вот наше приложение очень простенько сейчас я немножко увеличит что было лучше видно мы видим что по большому счету как я вам обещал приложение ничего не делать просто показывает какие у нее есть бренды как их зовут и в какой версии они на нынешний момент бегут осуществляется это все вот этим вот простеньким кодом все кусочки приложения все сервисы написаны на питоне вот этот конкретно and point backend просто здесь бежит цикл который опрашивает оба бэг-энда спрашивает какая у них версия и как их зовут список брендов получает из вот это вот конфигурационного файлика и теперь по поводу тех изменений в коде которые мы сказали все таки немножечко необходимы если мы хотим получать отвести услуги распределенного трассирования то нам необходимо чтобы каждый request который проходит между сервисами каким-то образом был помечен на нынешний момент это делается при помощи федоров которые генерируются в начале на входе на на gate вы из тела и потом просто каждый сервис отвечает за то чтобы передавать всем своим и нисходящим зависимостям этих ядер и вот для этого это имплементировать это вот функция здесь здесь вот список этих и доров которые все являются частью спецификации up and racing и таким образом можно передавать это на системы трассирование которые поддерживают эту спецификацию как например егерь который у нас из из тебя ведёт прямо в коробочке можно сказать и вот он у нас здесь то есть тот момент когда мы начали обслуживать наши сервисы eesti у нас появилась возможность пользоваться егерем и мы можем например пойти и сказать я хочу посмотреть весь трафик который зашел на и still in грыз gateway и что с ним произошло вот мы немножко создадим там трафика не скажем найди-ка мне все trace и заходим сюда и у нас замечательное совершенно отслеживание всего что произошло вот зашел трассе трафик на ноге и твой пошел на фронт мы видим как все сливается на полисе и на миксер для проверки и для телеметрии потом с фронта на алеф здесь тоже здесь фронта на обед по поводу каждого вызова мы видим сколько времени он занял в общем все отслеживается где-то что-то застряло очень удобно анализировать кроме этого есть еще пряники ну во-первых вся эта информация также всех соединений сливается в prometheus и prometheus у подключена графа нас уже готовыми даже бардами можно заходить сюда смотреть вот например сервиса лев в реальном времени создадим ещё немножечко трафика посмотрим как граф она нам это отображает или не отображает но должна в какой-то момент начать реагировать да вот начала реагировать и еще такой небольшой но очень приятный пряник это вот этот вот сервис граф который нам дает графическое представление того что происходит вот наверху у нас ingress гей твой с него идет трафик на сервис фронт бед алеф все видно все сливается на есть его policy is still телеметрии красота все видно но это не то чего мы хотели мы же хотели деплоить как ниндзя да просто смотрите замечательно совершенно конечно отслеживать тоже замечательно мы хотим что-то предпринимать поэтому давайте что-нибудь предпримем мы за тепло и новую версию для начала два чтоб понять с чем это отличается от mi mente мы сначала с дипломы и просто вот по простому для этого мы пойдем вот сюда и за тепло им новую версию версию нет нету версию 3 версия побежала замечательно пока оно создалось давайте посмотрим что же это мы такой там создали очень простенький deployment с другим названием это уже олив версия 03 обратите внимание тот же самый и пеппи олив нос новым лейблом virgin 2 0 3 другой имидж соответственно к чему это приводит к тому что сейчас у нашего сервиса лев получается 2 бэг-энда реальных по две реплики на карте соответственно каждый раз когда мы будем теперь заходить на наше приложение мы будем в 50 процентов случаев получать backend 02 50 процентов случаев получать backend 03 приблизительно фифти-фифти но это ж не то чего мы хотели мы же хотели за тепло и так чтобы никто ничего не заметил поэтому было бы неплохо осуществить такой подход как темный запуск dark lunch как мы это можем сделать при помощи из что такой дар к ланчу во-первых да это когда приложение бежит в продакшене но об этом не знает никто кроме доверенных людей например в идеале тот самый разработчик который за это за эту we новую версию отвечает который в ней виноват как вы это делаем делаем это следующим образом мы запускаем вот такой вот я облек нас который называется dark loncin что здесь происходит мы перри конфигурируем виртуальный сервис олив и говорим следующие теперь мы смотрим что у него там фидерах если в кадрах у него есть cookies которая говорит нам о том что user который зашел это девелопер там и этом удивило прямо во время о еде кота на свою версию 03 смотри что с ней происходит всем остальным мы говорим оставайтесь на версии 02 не беспокойтесь идет работа каким образом они идут на эти версии да они обращаются какой-то подгруппе некий sap сад возникает откуда он возникает для того чтобы возник сапсан мы пользуемся другим ресурсам если у которой на за destination ролл destination ро определяет различные наборы политик правил которые применяются к роутинга и вот в том числе здесь он говорит о следующем говорит есть две подгруппы в 02 обращается к тем потом на которой стоит лейбл в virgin 2 0 2 2 0 3 соответственно тем которых на стоит на которых стоит лейбл 2 0 3 после того как мы это запустим а мы ещё не запустили если мне не изменяет память dark ланч результат будет следующим теперь как обычный пользователь который ни о чем не подозревает мы будем постоянно получать версию 0.2 но стоит нам зайти как виноватый во всем девелопер и мы получим версию 03 что и требовалось доказать стоит нам выйти как мы опять получим версию 02 красота очень хорошо девелопер может заходить смотреть но это не то чего мы на самом деле хотим потому что мы знаем что главное испытание для новой версии это ядовитый воздух рода и пока она там не побывала мы не можем быть уверены в том что не сломается девелопер никогда не будет издеваться над своим приложением так как над ним будут издеваться настоящий юзеры поэтому мы хотим имплементировать такой подход как канареечный дипл и что такое канареечный deploy когда мы выпускаем версию в пруд и потихонечку потихонечку начинаем пускать к ней ядовитые газы prada и смотреть как она себя чувствует если канарейка чувствует себя хорошо мы даем ними еще еще яду и пока не убеждаемся в том что оно достаточно сильно чтобы уже выпускать ее к юзерам замечательно как мы это делаем при помощи истер у нас здесь есть другой облик который называется канария ну окей здесь мы опять перри конфигурируем виртуальный сервис олив в данном случае мы берем следующий у нас есть два destination один называется подгруппа production и здесь мы уже определяем какой вес того трафика которую он начинает получать ну например мы могли бы для начала определить что 99 процентов трафика идет на прот и один маленький процентик идет низу на на канарейку так мы обычно и будем делать но сейчас для наглядности давайте определим что у нас 80 процентов идет на прот и например 30 на канарейку определяется эти сосед а кстати опять destination дролам который мы тоже перри конфигурируем теперь мы говорю у нас есть сосед и которые не привязаны конкретным версиям то есть они привязаны конкретным версиям сейчас но потом можно будет их перенаправить на другие версии продакшн и камере и так вы сделали 8 130 как что вы думаете сработает нет мы проверим итак мы говорим ну-ка сделай камни canary не получилось и всё говорит нам 110 процентов это бывает только у вас очень исполнительных людей у нас машин сто процентов это сто процентов как бы никуда не денешься поэтому пожалуйста соответствуете ну и мы будем соответствовать мы скажем хорошо уговорил 7030 запусти к нам такую конфигурацию получилось и но здесь мы по большому счету можем ничего не увидеть нам придется покликать некоторое количество роз поэтому чтобы действительно увидеть это наглядно у нас здесь есть прямо там же как мы сказали в нашем namespace и есть погиб который нам позволяет создавать трафик и мы сейчас его откроем и с него создадим трафик который нам понадобится и в дальнейшем здесь у нас сидит маленький скрипт тиц который умеет создавать нагрузку на тот ural который ему дают побежали мы видим что действительно там в 3 случаях из 10 приблизительно мы получаем 0 3 а в остальных получаем 0 2 замечательно работает но есть проблема кто проблема увидел проблема заключается в том что нам приходится писать огромное количество яму да как мы уже сказали я мол стал бичом нашей индустрии я думаю что если есть такой человек пускай поднимет руку которая когда представлялся как будет инженером в будущем представлялся пишущим яму и есть оказаться один человек счастливый человек что вы рассказать всю жизнь мечтала земли замечательно но для большинства это не так мы обычно не довольны тем что происходит поэтому не хотим писать яму настоящий ниндзя не пользуются яман они сделают все каким-то образом более хитрым автоматически что можно сделать мы для этого предлагаем пользоваться подходом который мы для себя называем следующем называем его кубер не this at the meters да то есть вы возможно слышали о паттерне который называется кабинете супер reuters до перейти рс это подход который говорит следующее мы вместо того чтобы описать яму и мы пишем программки которые ну такие контроллеры которые разговаривают непосредственно с и 5 кубер не tissot и помогают нам управлять определенным приложением определенным сервисом как-то например postgres и или prometheus или или вообще ну ваше приложение которое вы написали о том maker с немножко другой подход он горит мы не занимаемся определенными приложений мы просто автоматизируем то как мы управляем нашим кластером то есть кубер нить создает нам набор абстракции данный не говорит как с ними работать ничего не предписывают а мы вот здесь начинаем предписывать то как автоматически происходит процесс и есть примеры таких вещей например есть такой проект организуется drain о который позволяет автоматически дел drain аноды есть у booking.com некий проект под названием schipper который тоже имплементировать разнообразные стратегии deployment а мы разрабатываем свои собственные у the maker и сегодня я вам представляю эту maker который называется это не здесь который называется бёрд watch или по-русски птиц осмотр как устроен птиц осмотр ну вот он сидит в вашем кластере и ждет канарейку канарейка определяется вот эти мутко стэм ресурсом который называется бёрд watch в тот момент когда канарейка появляется птиц смотрит о нашел канарейку он узнает от канарейки о каком сервисе идет речь какая версия этой к на речке и начинает выпускать на канарейку трафик каждый раз выпускает трафик проверяет как она себя чувствует если канарейка здорово он дает еще яду и еще я да еще и так пока нас благополучно не выходит в прод или благополучно не задыхается и тогда он ее убивает нафиг или может быть делать что-то другое давайте посмотрим как это работает итак вот у нас здесь в нашем кластере уже по секрету вам скажу установлен этот кастом ресурс и вот он здесь и здесь же у нас уже бежит наш птиц осмотр вот он здесь так что давайте мы сразу откроем его логе и будем смотреть чем же это он там занимается мы видим что он там активировал какой-то снег бот как нам и завещал джон вылезла пользуетесь с лаком мы пользуемся и ждет птичку говорит почему добиться вот флаг у нас находится вот прямо вот здесь вот у нас есть канал deployment а нынешний момент в нем ничего не происходит давайте дадим птиц осмотру птичку это произойдет нас здесь здесь окей печку мы ему дадим при помощи но все-таки все еще ямада явно придется писать меньше но как-то же нужно определить что заходит на кластер просто нам потом мне нужно будет конфигурировать и стью и так что мы делаем мы заходим вот сюда и здесь у нас есть еще небольшие ямки теперь запускать канарейку и смотреть как она была получена выходит в продакшен интересно не очень интересно что гораздо интереснее запускать канарейку смотреть как ее убивают правильно убивать убийство навсегда гораздо зрелищнее поэтому для начала давайте запустим больную канарейку который изначально мы знаем что она больная она в 50 процентах случаев возвращает нам 500 итак мы сделаем да давайте посмотрим как оно признано потом запустим уже что было совсем зрелищно находится она у нас вот здесь ну пей мы создаем новый deployment который называется алеф багета есть в нем есть бак мы ставим на него лейбл версия в 04 по то что действительно его версия у нас здесь есть имидж который действительно баги и к нему мы добавляем вот этот вот к снам ресурс который определил следующим образом он говорит следующее речь идет о виси под названием алеф окей версия этого сервиса и 04 если канарейка не здорово то мы делаем ей рубок каждый раз мы добавляем трафика production на эту канарейку один процентик то есть по чуть-чуть мы с не обходимся нежно и ждем пять секунд что посмотреть здорово канарейка или больно как мы знаем здорово она или нет у нас тут есть метрика которую мы можем вытаскивать из prometheus и кстати обратите внимание что метрика вытаскиваться из того про метился который идет систем прямо в коробочке то есть нам не нужно ни инструмен тировать наш сервис не устанавливать prometheus сервис может быть полностью тупым он может вообще ничего не знать о том как он общается с другими сервисами вот и потом мы говорим что отклонение допустимое отклонение от от той метрики от здорового состояния является 0,2 ну что ж я думаю что мы готовы запускать больную канарейку канарейка полетела да конечно же дать правильный и канарейка полетела что происходит мы сразу видим что наш птиц осмотр обнаружил что появился новый deployment для сервиса олив версии 04 и начинает опрашивать про метил спрашивать его ну как там канарейка здоровый или нет тем временем добавляет на канарейку все люди по процентик у трафика и все время спрашивает как у нее дела мы также видим что он передал сообщение своему другу которого зовут отто что вот нам написал флаг о том что есть новая канарейка которая пришла для сервиса олив и но вот да вот мы пропустили такой трагический момент но сейчас мы посмотрим на него в логах в какой то момент когда на канарейки было 4 процента трафика он сказал канарейка заболела это мы тоже я думаю не успели посмотреть здесь в лоб влогах мы бы увидели что возвращались у нас ответы 500 короче когда он увидел что канарейка не здорово он сказал канарейка больна мы ее убиваем замечательно канарейка убита можно расходиться нет этого недостаточно мы теперь хотим посмотреть на менее зрелищную но более счастливый сценарий того как здоровая канарейка вылетает в продакшен так мы и сделаем мы для начала сотрем остатки мертвой канарейки из нашего кластера чтобы нам не было грустно после чего мы за тепло им версию 03 которую как мы уже знаем здоровым и уже на нее смотрели ну запускали на нее 3 проц 30 процентов трафика и она нормально себя чувствовал посмотрим выдержит ли она больше чем 30 полетела канареечка опять же нам сообщает наш птицы смотр что есть новая канарейка теперь версия 03 канарейка признана в этот раз таким образом что мы ее выпускаем быстрее да мы говорим каждый раз добавляем 5 процентов трафика на канарейку и ждем всего 2 секунды метрику измеряем ту же самую обратите внимание что птицы смотр поддержит еще такую стратегию если канарейка заболел это может сделать ей фриз и таким образом можно потом зайти посмотреть почему она так все плохо себя чувствует и вот все происходит бежит уже 55 процентов идет на канарейку 60 мы можем посмотреть здесь влоги увидеть что часть request of уже возвращается с версии 03 канарейка чувствовать себя хорошо 75 процентов 80 вы в напряжении я точно напряжение моё дело мне за него отвечать 90 процентов 95 100 ура мы выпустили канарейку в прод вот и от то нам об этом тоже празднично сообщает тут не хватает иконки там вот это вот есть такое какая-то фейерверки и праздник надо добавить в дальнейшем следующей версии итак все благополучно закончилось слава богу можно вздохнуть несколько вопросов во первых сито это замечательно она дает нам совершенно замечательный контроль у меня возникает вопрос если застрять не за средних и в сети devops да то есть мы всегда говорю что devops это о сотрудничестве разрабов iops of и что разрабы должны иметь возможность беспрепятственно приходить в прод но тут у нас получается у нас есть тупые сервисы разраба вообще ничего не знают о том как сервисы сообщаются между собой есть какие то люди которые пишут yabla которые всем этим централизовано управляют возникает вопрос не возникнут у нас тут проблема общения не получится ли так что когда есть одни люди которые пишут joomla другие которые пишут код между ними нет достаточного сообщений в сети что-то застревает это раз второй вопрос если мы действительно говорим что мы ведем себя как настоящий ниндзя и мы пишем меньше ям love и мы даем совсем простенький яблоко с которыми даже разрабы могут справиться и пишем от amateur это кто же будет идти от amateurs писать писать их нетривиально должны сказать потому что ну во первых там просто готланд задокументированных оберните не очень python клайн задокументирован еще хуже python клиент например дырявый потому что пока мы писали этот этого птиц осмотра нам пришлось его пропатчить в четырех местах как минимум все нетривиально не всех захотят этим заниматься это большая работа но в другой стороны работа в ту сторону уже ведется вот мы что-то написали есть еще наши коллеги из компании би-рекс которая сейчас тоже занимаются тем что строят новый слой абстракции над кистью так чтобы обрести ничего не нужно было знать в общем но вопросы все еще возникает если мы хотим делать что-то ведь у всех же deployment и работает как-то по-другому ничего стандартного нет в общем вопрос остается ну и третий вопрос который возможно некоторых из вас возник почему мы написали наше нашего а томатора на питоне они на гол потому что вроде как стандарт написать контроль под кубер нити все-таки на гол потому что как выяснилось для того чтобы управлять кастом ресурсами из go клиента нужно описывать все эти ресурсы в своем городе истине предоставлять свои объекты описанные для кубер не tissot и поэтому это бы значило что просто брать все объекты из тела и переписывайте в своем коде много-много нудные неприятные работы как выяснилось потом те же самые наши коллеги из девиц эту работу все-таки проделали и даже есть на гитхабе можно найти и взять просто эти готовые файлы но мы к тому моменту уже закончились птицы смотрим вот давайте подведем итоги во-первых сервисная сито это новое лекарство от граблей она решает проблему общих библиотек это хорошо если вам нравится централизованный контроль я всегда сомневаюсь до преимущества централизованного контроля когда мы говорим о больших масштабах потому что ну как бы любому человеку который об этом когда-то думал понятно что централизованный контроль имеет ограничение по масштабируемости и тут возникает вопрос хорошо это или плохо умная считает а значит что сервиса ничего не знают о своем общении значит это значит тупые сервис и это опять же это ломает один из первых постулатов микро сервисов до вспомним мартина фаулера умные сервисы тупые трубы тут у нас трубы становятся очень умными вопрос не превратится ли сервисная сета в тот же самый enterprise service bass если не пользоваться а томатора метать это скорее всего засориться но пока мере так кажется нам то что мы видим да пока что сложно сказать потому что никто почти истек продакшене пользуется пока что что происходит это то что все ходят и друг друга спрашивает ну вы уже пользуетесь из тел и мы мы конкретно сейчас в одном проекте уже на стадии очень-очень продвинутого приусе мы уже готовы к выходу в продакшен но пока на маленьком кусочке то есть опять же о результатах и влиянии этого на организацию в целом пока говорить достаточно рано если сито засориться а в нем тупые сервис и вообще можно представить какая то будет катастрофа такие тупые слепые мыши которые ты каются во все стороны и не знают куда пойти ну и поэтому мы конечно предлагаем что если вы хотите быть настоящими нильзы что вы пользовались а то моторами потому что так гораздо интереснее и веселее есть ли кроме из тела варианты есть во первых есть контент компании буян тех самых которые написали свое милен гербе они написали теперь другой прокси и об обертку вокруг него управляющую все это они написали на расти есть такое преимущество до 1 это язык который очень эффективно обращается с память обещает что нигде никаких утечек памяти не будет очень быстрый очередь перформанс пока не смотрели но возможно будет хорошо дальше компания hаши карп добавила фичер свой любимый многими из нас концу который называется консул connect который постепенно добавляет все фичер и которым которых мы ждем от сервисного сета вот буквально пару дней назад они выпустили новую версию в которой они поддержат даже уже вброс тех же самых on вой прокси при помощи их каких-то утилит внутрь кодов на кубер найтись и так что это тоже альтернатива кроме этого уже появился коммерческий вариант под названием mass примешь это какой-то инкубатор компания в 5 который просто заворачивает и все в красивый enterprise обертку ну и конечно же у нас с вами появилась новая игрушка вокруг который мы можем понастроить вообще всего чего только угодно новый подход новые решения интересная жизнь чего в общем-то я вам желаю пользуйтесь и там чувствуйте себя настоящими ninja и не скучать антонова из еще раз спасибо большое за доклад ребят у нас есть несколько минут на вопросы поэтому давайте вы будете понимать свои руки а я буду вам приносить микрофон встаем представляемся и задаем вопрос добрый день и спасибо за доклад лёша меня зовут вот когда рассказывают про из тела и подобные системы по умолчанию считается что протокол общения между микро сервисами the http но это не всегда так и что делать если это не ешьте теперь ну есть ли какие-то у него решения во первых как выяснил посмотрим здесь об этом я умолчал как они говорят о том что когда начинает поддерживает сужают степи 2g рпц просто как бы голый tcp и сейчас по моему последней версии они уже даже какие-то поддержки для манга протокола появились то есть это постепенно добавляется опять же проект open source на кто хочет добавить обработку протоколы вперед тем более что миксер он полностью плава был на самом деле вся система полностью плодов можно даже онго и заменить на все что угодно другое поставить другой прокси конфигурировать его по-другому и миксер позволяет то есть как миксер осуществляет вообще все политики до когда онго его спрашивает что делать он может пойти наружу и спросить у любой другой системы что с этим трафиком делать и как его обработать понятно ну то есть я так понимаю что программировать не то чтобы много ну чуть чуть надо ну естественно очевидно становится ну да да это пока все так сказать косточки надо на них наращивать мясо божьего спасибо большое еще вопросы вы просто поднимаете руку дай привет спасибо большое было интересно скажите брался вы уже переживали при с одной версии на другую ну нет потому что вот только вышел первое так сказать production игры версия и как я сказал переживали в в пенси они поменяли действительно вот буквально совсем недавно гей твой объект поменялся системы роутинга поменялось понятное дело но вот сейчас версия 1 0 видимо переходы будут более гладкими следует надеяться кстати то что происходит сейчас сейчас идет активное обсуждение выясню о том чтобы сделать свой собственный синай плагин то есть чтобы не туркам если управляла сам ufc обернитесь и потому что на нынешний момент а что происходит для того чтобы прокси могли вообще перехватывать трафик они должны поднимать там и нет контейнер который бежит в приведу в моде для того чтобы переписать ай-петри bus сейчас они говорят о том что будет вместо этого свой диман который будет управлять не туркам в каберне тисе и и тогда есть они придется делать вот это вот насилия я очень долго развернутый ответ я понимаю ну короче так проект очень свежий поэтому переход с версии на версию может быть ухабистым если это отвечать здравствуйте меня зовут антон у меня такой вопрос по поводу а цитадель а вы что-то пробовали тестировали интересует вопрос например хранения большого количества сср сертификат как с этим работает папа опять же нету нету никакой статистики обычный масштабных дипломе нтов поэтому конечно вы не такого но то что мы делали там не не не делали бенчмарки на на большое количество над стандартном количество мы не видели никаких там ни на чем не затыкалась другой вопрос возникает вообще смягчил тел с этого это боль да как потом достучаться до этих сервисов если нужно со стороны как они стучатся наружу если эти сертификаты все внутренние там есть посты в блогах которые говорят о том что это все достаточно проблематично и есть вопросы нерешенные спасибо спасибо за доклад дмитрий меня зовут вопрос о вашем вот эту автоматизация на гитхабе да да да да да во первых все презентации вот у меня под здесь нету ленка но я добавлю link когда вы будет выложена презентации там все будет а так очень круто конечно спасибо до презентации появится ровненько через день после окончания конференции то и завтра к вечеру уже все быть на сайте еще вопросы вера здравствуйте спасибо за доклад на а хотел уточнить а как вообще с этим работать если больше одного дата-центра то здесь много трафика там прокси миксер телеметрия как собирается лучше вообще делать как фидера этот кластер а вообще полностью независима и или если допустим в разных концах города 2d до центра это все можно в один забубенить и она будет нормально жить какие другие может нюанс есть вот у меня нету пока ответы на этот вопрос есть сейчас какие-то тоже bios на тему того как сделать так чтобы из те управляла фидера этот кластерами и и управлял тем что происходит везде но не могу сказать как это работает я понимаешь там есть ещё какие-то сложности ну потому что просто нужно понять как как для начала все эти кластера между собой общаются например вы в своей практике там в каком-то амазоне или в гугле делали конт на на весь регион один кластер аду я хотя бы в разных в любите зонах она в принципе живет или или только в рамках одной визе не в разных нормально работает если типа один кластер разбросанный новосиба и после вопрос но давайте как-то вот поможем друг другу добрый день меня зовут дима и я хотел бы уточнить вот такой вопрос вообще пробовали запускать систему стандартные николесис если в этом смысл ну по большому счету смысл исчезает потому что все полисе которые нужно он уже применяет более того в последней версии к лику они добавили поддержку об аппликационной блэка тивных network policy которые завязаны на есть нужно поставить есть и к лику обращается ко ести и позвольте еще один вопрос вот вы пробовали канарейка привязывать еще horizontal под авто скейлера и заработали они не пробовали пока честно говоря ну можно заставить работать все что угодно на самом деле как бы ну то что вы видите здесь понятное дело что это прототип там можно найти 25 дырок или 100"
}