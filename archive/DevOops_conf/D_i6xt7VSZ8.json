{
  "video_id": "D_i6xt7VSZ8",
  "channel": "DevOops_conf",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "всем привет меня зовут евгений я работаю в компании 2гис я инфраструктуру и команды наша команда занимается основной и делает поддержка кого из кластеров для всей компании мы очень любим поговорить смысла много живем на доке с ним и не самообман внутри компании помимо cooper у нас еще есть работа с базами данных это подъем в основном несколько небольших костров все все как обычно у других логин и собираем мелко стеком в параметре метрики складываем чтобы хранить англизировал этих на протяжении нескольких лет наша команда также поддерживает очереди rabbit кафка базу кассандру нас иногда тоже поддерживается если слегка сеансов под нее и для компании вы нашей территории охране система хранения кода в виде кислота тоже тоже администрировать помимо все то инфраструктур шины вся наша команда любит ходить на чем-нибудь на лонге на питание на других языках я понадобится и вот или мы занимаемся собственное еще компании сегодня так вот наш в своем докладе расскажу о том как мы готовим поговорить на лучше лезет почему у нас это голое железо появилась какое мы выбрали что мы для себя придумали то есть какие у него есть проблемы этого железо тут не будет прямого сравнения с облаками мы просто постараемся я постараюсь рассказать о плюсах и минусах конкретной работы на железе скутером а вы уже самостоятельно можете принять то есть может будете ли вы справляться с этими проблемами страшны ли они вам если у вас такие же проблемы в блоках или нет и можете сделать выбор что дамы приведена железо потому что этого например дешевле обслуживаем меча облака или нет мы останемся на облака потому что вот эти проблемы с железом нас пока что нет времени их решать собственно не только о проблемах мы будем говорить но и об их решениях как что мы сделали чтобы их решить сколько это боли или радости нам принесло про планирование ресурсов поговорим потому что железно это не так чтобы это щелкнуть пальцами и у тебя сразу же появилось плюс 200 серверов нет это нужно заранее спланировать заранее проговориться всеми командами что у нас ожидается в будущем через месяц через два через год например какие были и собственно проговорим как то что мы запланировали или то начал у нас сейчас придется приложение как это всё считать то есть там есть несколько вариантов подсчетов то что у вас приложение крутятся каких-то выделяют железках или приложения придется на в общем пули вы прогрел этот вопрос как как отделять общей например частного закупленного железо конкретно подвода эту задачу я больше никто не работает давайте начнем конкретно почему вообще у нас появилась компании железо мы до как его сказали сибирские ребята мы начинали в новосибирске то есть и у нас основная аудитория это за уралом новосибирска сибирский регион дальний восток потом мы выходили урал екатеринбург европейская часть россии буквально там 80 лет назад мы появились москве и дело в том что за уралом у нас много пользователей но дата-центров заняло очень много очень очень мало нормальных тут нет таких возможности что ты через записку закажешь себе виртуальные ресурсы тут ты можешь наткнуться на что ты пойдешь арендовать железо упасть он скажет прости у меня больше нет я ничего не могу с этим поделать или есть дата центре где есть железо на например они не устраивает отказоустойчивость по доступности по каналам связи и вот эти вот все компромиссы надо как-то соединять и пытаться выходить из них выход поэтому мы пришли к тому что мы покупаем свои сервера свои сетевые сетевые железки арендуем канала связи арендуем питания и берем локейшн нескольких дата-центров и по себе и теперь уже в европейской части россии и в европе то есть у нас изначально такая политика что она исторически сложилась жизнь на своих серверах и дальше мы и продолжаем имамата принципе нравится и исходя из какого-то опыта работы на серверах на собственных исходя из бюджетов на их закупку на обслуживание стоимость сравнения стоимости колокейшене сравнения стоимости аренды ресурсов в облаках или конкретных тут же родились еще и требований от компании то что свои сервера калатеи чем это на самом деле намного дешевле чем аренда железо или об облачных ресурсов другие там доходят до того что свой сервер может окупиться за один полтора года аренды а про работает у тебя еще лет пять или шесть и таким образом ты можешь спокойно начать своей емкости не сильно затрачивается раз мы режимом своем железе наряды всем интересно узнать о какой вообще у нас железа находится чтобы сравнить своей ситуации или сравнить имеют собственные которых они арендует в облаках ну и для себя выбрали два типа железо которые масло мы закупаем у нас есть два больших компонентов нашей компании это облако вопрос так и для облака pan stik где собственно могут и кости c виртуальных ресурсов для улицы не закупаем довольно таки жирный сервера это больше с вами коррекции куда терабайта памяти и локальных дисков именно туда мы создали покупаем мы не режется сетевых дисков нам локальное как-то ближе работает быстрее если нужно отказоустойчивость вы или подобными эти данные используйте те программные решения которые самостоятельно раскидывают данные потому что берутся то есть для пан степан жирные машинки на которых мы можем посетить те же самые 300 музыку вернется и удавка вид конечно же использовать для кубера мы пришли потому что мы покупаем довольно таки маленькие сравнивают до 16 еды полу они мало ядер но они очень быстро то здесь медленнее 320 нет например мульти ядерной они довольно медленно и четыре большие снимки для cooper у нас все же микро сервиса днем придется памяти много не надо поэтому 6 4 гигов памяти на каждое ружье не хватает от деда должны 32 арест диски куда мы складываем по вопросам оттуда то есть если мы видим что там большинство приложений и the states но и серьезное количество дисках там не требуется поэтому мы просто засыпаем обычно по меню и работаем с ними буквально полгода назад мы начали использовать с российским стороне и начали закупать чуть-чуть побольше диски туда начинают там сто-двести не капать на каждой машине стоит он хранит в принципе это возможно с этим проблем нет дальше продолжаем железо когда вы собственно уже работаете в нем купер на нем у вас возникает ситуация что вы запустили там 100 микро сервисов заняли там почти все железо знаете что через какие-то полгода вас придет новый график потребуется еще ресурсы и появится новое нас будет поэтому вы желез так вот триггерами покупке нового железа мы считаем для себя что пора задуматься новом железе и когда у нас мы знаем что новый график обведет это либо новый сервис разработается либо линейный рост трафика просто-напросто произойдет и мы будем вынуждены через полгода например держать больше ресурсов также к нам могут прийти продукты и сказать что мы разрабатываем новое приложение у него будет примерно 10000 abs нагрузка давайте ребята прикинем сколько можно купить новое железо в какие-то обозримые сроки ну и конечно же новое железо мы покупаем для резервирования железом ситуация такая то что когда вы заказываете железо вам его нельзя заказывать в тот притык можно сказать к тому что у вас запрашивают потому что с железом всегда ситуация такая то что вы заказали его она вам пришло но процентов 50 случаев несколько серверов из тех что вы заказали отправляются назад вендор заказали потому что быть либо проблемы с памятью либо проблемы с материнской платы что ты еще точно нужно заказать чуть чуть больше да это будет стоить дороже но зато вы точно потребность бизнеса закрою эти в том чтобы хостить новое приложение держать начиная ps когда мы говорим вообще о закате железо то у нас возникает проблема как бы если бы мы жили в облаке мы бы открыли ей или попишешь кубы запрос составили сказали что нам надо замазано из гугла и зайчон и пожалуйста там три виртуальных машин и по 8-е 116 памяти и диском я там чуть чуть по насыпи и у нас там через три минуты уже появились железо появились ресурс а когда мы спустимся на собственных серверах тут возникает такая небольшая проблема как timeline доставки железо то есть доставки его установки то есть нам нужно понимать вообще когда нам его надо заказывать чтобы это понимать у нас такая ситуация то есть у нас тоже должно произойти какое-то событие событие этого поднимается что новый трафик приезжают или ожидается прирост трафика или новый клиент пришел который давать новые приложения хвастаться может быть на принимаешь нам нужно зарезервировать чуть побольше нужно что-то еще произойдет в общем должен должно произойти какое-то событие когда вы начнёте задумываться о том чтобы купить новое железо то есть вот он первый день время отчета к покупке нового железа потом вы смотрите на то что вообще произойдет то есть почему вы покупаете новое железо подбираете спецификации какие вы хотите купить это должны быть нам нужно много ядерного нужно быстро видео графства к нам памяти нужно аттестаты не то сколько каналов связи сетевая доступность там или что такое то есть вы садитесь с разработки с отделами по закупке железо например садитесь покайтесь спецификацию которую надо отправить вендору и что можем и словом счет явится согласование произошло как только в этом все подобрали эндер вам преславу информацию о том сколько это все стоит тут вы собственно идете к финансистам и бюджетное тоже вес тегеран кстати может быть еще ситуация что например конец года мы планируем железо на следующий год у нас есть возможность купить сейчас прямо железо давайте примерно за планируется что нам потребуется его когда-то игра на пяти часам та же самая печати равале бы эту если как бы не бюджетирование во что или кто бывает такая ситуация то что согласование финансов на покупку нового железа это тоже процесс занимает время то есть вполне себе два месяца она может средства на закате или на понимать и как деньги на новые железо и самый интересный момент получение нового железа это его поставка то есть сервера к сожалению в россии еще не собираются все себя которые собираются когда мы покупаем по хранению все американское американские supermicro не к нам приезжают и все поставщики которые нам их продают минимальный срок поставки и заявляют в шесть недель то есть сорок два дня вы после того как оплатили просто ждете пока о них вам придут и в это время вам еще надо жить на имеющихся ресурсов и чтобы сервисы были выстрелы и работали все было нормально и вот эта поставка в 42 дня она на самом деле может спокойно растянуться до 90 дней на 120 то есть это такая величина которая не ограничено с дальней стороны можно сказать но по крайней мере мы не знаем об этих ограничениях и на не могут влиять внешние факторы текущие проблемы с эпидемии по всему миру ну и после того как железок нам наконец придет совместно с днями работает это центр его оставят вы накатим на него систему кубера накатим проверим какие то приемочные тесты проведем и выкатим в продакшен любую систему на это железо то есть если посмотреть что когда вообще пора заказывать железо когда мы хотим использовать нам нужно начинает заказывать за два или три месяца до того как она нам нужно то есть если вы живете на своем железе и всегда должны учитывать вот этот гэп зная что завтра вас эти ресурсы не неизвестно откуда они появятся вам нужно их чуть-чуть заранее продумать и заказать и этот процесс надо провести через всю компанию которая требует от вас размещение своих сервисов но конечно же бывают ситуации такие то что железо закончилась но тот момент триггер пропустили железного нас кончилась простирается к ним все прибывают и прибывают и нам надо где-то размещать чтобы у нас есть старый сервис работали и новые клиенты уже начали отдаваться и что вообще с этим делом что что делать в таком случае нету такого сказать что прости бизнес давай подождем там пару месяцев потом ты продолжишь нет так делать нельзя поэтому тут два основных правила которые вы для себя определили это именно потушить тот пожар который разгорелся с отсутствием ресурсов сейчас и после того как мы потушили пожар спокойной обстановке со всеми причастными лицами кто участвовал в поджоге скажем так провести посмотрим чтобы определить как надо жить дальше чтобы такого не происходило чтобы мы не тушили пожары каждый раз когда у нас что-то новое или зиц это ненормальная ситуация если говорить про тушении пожаров вот мы ставим сразу брать об этих ситуации тушении пожаров надо вообще определить а что у нас конкретно закончилась то есть какие ресурсы центральный процессор памятью все диски кончились может быть у нас проводим связи какое-то нехорошее или между москвой и но сибирском екатеринбурге excavator что-то там наделал нехорошие на канале связи у нас там что-то случилось может нам нужен поставщик но в общем мы должны определить какая проблема у нас случилось и как только мы определили какая проблема случилась на понять насколько она вообще нужны чтобы закрыть эту проблему чтобы не просто и и даже закрыть а закрытие и иметь некоторые вверх чтобы она у нас нестабильного сразу на следующий день мы закрыли потому что часто бывает что вы закрываете потребность в ресурсах а на следующий день там смотрят а ресурсы есть ли тут такой парк я пришел вас попросить и все что вы закрыли уже 59 и если говорить конкретно про купюрница то нас интересуют два конкретных ресурса это и память потому что по ним рассчитывается конкретно вместимость кластеров куда имеется купим вам не позволит саши давит на ноготь больше приложений чем вы поставите коды на эти приложения то есть вас есть квота на ногу с восьмиядерным примерно 70 половиной доступно и подал каждое приложение все они суммируются все приложения на ноги и какой-то запас остается там чтобы зачем мы для себя определили что для нас самый дефицитный ресурс по которому мы горим чаще всего это центральной процесса потому что у нас ноты и многогранные но быстрые и поэтому мы стараемся выдавать поменьше на request супругу приложением но скажу так сканируем их горизонтально потому что но можно сделать больше по памяти же у нас на самом деле проблем нету у нас полно им а основная масса приложений это питон galant java script и они с памятью в более-менее дружат и много не надо java скала у нас тоже есть да у нас есть положение которое я там по 4 по 6 гигабайт памяти по 8 но этого хватает то есть тех но которые 32 гиг бывает на память и хватает для размещения вот этих вот ищущих память приложения и дефицита магния не испытывая и память всегда можно намного проще память докупить маленькие сервера чем поменять особенно кинкстра действующие xeon и которые каждый год если дальше углубляться в пожары то как вообще можно потушить пожар и когда вас кончились ресурсы когда вам прибежала с кучей микро сервисов пришли люди с ключами коснемся вам надо разместить самый быстрый способ тушения пожара в этом случае чтобы закрыть его и начать думать и конструктивно что за этой ситуации делает это воспользоваться тем запасом которые у вас есть где-то на черный день у нас такая заначка находится в нашем уроке я постиг то есть там медленные ядра но их много и мы всегда можем оттуда взять там создать две три виртуальных машин и докинуть 2030 ягер кластер разнести по ним все приложения которые например быстродействие цб не требуется и таким образом высвободить железные ноды для обработки именно критические бизнес критически важных приложений и наш диплом должен быть и наш и ваш диплом должен быть очень-очень быстро готов принимать эти ноты глубины вашей заначки то есть реальная ситуация реально ситуация то что добавление одно из таких диплом должно добавлять одну новую верните и после этого она автоматически должно это она должна включаться в пластина ненужным и кататься все сервисные приложения и туда сразу начнутся какие-то пользовательские приложения чтобы разгружать ваш власти и это то есть к этому надо готовить ваш тепло и дату и самих себя чтобы когда у вас что-то загорится вы знали что вот этот способ сто процентов рабочий и можно пользоваться но тут как говорится дело такое это очень хороший способ он очень быстрый и очень лениво от него отказываться и нельзя чтобы вот это временное решение как превратилась в постоянно то есть как только кризисная ситуация миновала как только вы раскидали с пожаром вам нужно работать план найти где-то ресурсы под размещение вот этих двух приложений новые приложения на эти ресурсы передвинуть и те ноты что вы создали потушить назад и отдать их назад конспект чтобы в следующий раз когда у вас не дай бог случится такая ситуация могли этим воспользоваться потому что если вы каждый раз будете из-за пустяка собирать ресурсы когда вас приезжают новые приложения то вы можете напороться на то что у вас и он стеки закончится ресурс и что-то это делать вопросы жует становится более интересным для разбирательств поэтому как только вы потушили свой пожар заначка работали что делать дальше заначку назад положили в карман еще один из способов решения пожаров он не о том что у вас физически кончились ресурсы властей он о том что у вас кончилась конкретно квота на нотах кластер тут бывает такое то что ну-ка то на ногах например это 800 велико в 8000 велико 8-я мы когда начинали работать с купюрница в шестнадцатом году вы были зеленые неопытные имеющие вид на себя что вот мы поставим 100 миль и core request of цикл на каждый контейнер и наверняка все будет хорошо и мы собственно так и начинали делать и шестнадцатом году у нас по дефолту вы всей разработки сказали что вот кто заезжает является номере корм и каждый контейнер ставим и работаем потом у нас случился 18 год когда у нас немного подзадержалась железо и нам нужно было где-то размещать свои свои приложение и окон строку нас было немного под забиты еще тоже очень легко и быстро заехать и сделать еще плюс кучу иисуса мы сели проанализировали все графики графики потребления циклу приложениями трафике того что запрошенному приложением и от того что она потребляет самое главное что то двигает ваше приложение мы в этом селе посмотрели нескольким приложениям уменьшили квест отсюда меньших значение в восемнадцатом году было тридцать миль и core посмотрели на боюсь что это все работает нормально и никаких приседания psn и нету и вот он исходя из опыта маш deform на новых приложений у нас единица мы пришли к тому что 30 миллионам приложения это совершенно достаточно для того чтобы она работала потом к нам пришло железом и чуть-чуть расслабились и двинулись в двадцатый год двадцатом году у нас тоже опять немного подзадержалась железо и начали 30 миль или хорошо а чан может быть мы еще может где-то с экономить начали провели подобный же анализ оказалось что у нас несколько приложений уже по инициативе самих разработчиков используют меньше и коста цикл и они работают нормально то есть мы опять снизил эту планку на дефолтные с 30 до 10 миль и core то есть если бы мы начинали в шестнадцатом году с этим знаниям то вы могли бы разместить можно сказать там если прям прямо смотреть это в 10 раз больше не красивой узнав о наших на наших на чем через ну конечно если бы не получилось но процентов и десятку наверняка вы игры это это это то есть вот этот способ он подходит к тому что если бы кто-то заранее знал как подбирать правильно ресурсы он покупал железо еще меньше чем требуется и на этом компании konami лариса судить рост более рационально тут же еще возникает перейдя ситуация где можно еще выиграть ресурсы и разместить ваше новое приложение новейшим сочились тут важно принять такое знание что то что вас запросили для приложения это совершенно не одно тому что потребляется то есть у нас есть входные и выходные view новых компонентов где вы прорыва и мы просто сколько ваше приложение будет потреблять для этого приложения разворачиваться на первых он ты на них на гоняется какой-то трафик прогнозируемый зависимости от того что это за сервис и там высчитывается сколько цикл памяти примерно потребляет приложение эти цифры мы заносим в табличку и когда приложение релизиться наборе они же используются но бывают такие ситуации что в процессе разработки приложения где то что то можно оптимизировать то есть разработчики могли оптимизировать какой-то метод он стал употребляться пунктом несколько раз меньше и работать быстрее они это сделали порадовались никого у него ведом или и просто напросто выкатились на бой уже с теми ресурсами которые были или просто по ходу работы по ходу жизни приложения обошли оптимизацию исправили ее также они меня ресурсы просто это нормальная ситуация и на таких штуках надо экономить то есть надо составлять отчеты кто из приложение потребляют меньше чем запросил и это даже относится не к дефолтным ресурсом который там десять миль и пор 30 листа это могут быть ресурсы она там 200 нелегко давайте на примере посмотрим у нас есть конкретное приложение мы видим его дневной пик это там 12 часов дня например и приложение дефолтные квесты 200 миль и core то есть мы когда-то они договорились разработчики их поставили используют приложения работы мы пока у нас проблем с ее собственных властей нет во внимание обращая но если присмотреться внимательно topic потребление этого приложения это реально оставили core то есть вот эта разница с каждого по да мы ее можем экономить если проходили в приложении обходе жизненный пример ходе review по потреблению ресурсов и по оптимизации что такое можно было бы сэкономить несколько десятков я dragonica с каждого кластер но это все как говорится ручной процесс и от руками отслеживать все вот эти все например 500 или 1000 микро сервисов которые масс крутятся это нереально то есть это просто затраты на разработку не понятно что то что ты будешь просто просматривать монитор каждое приложение опробовать выше нижнюю мир со принимать это ненормальная ситуация поэтому вы подумали что надо бы это на автоматику доспехами пусть она этим самостоятельно занимаются разработчики куда больше внимания уделяют чем и этот ручной процесс меня иисусов его надо автоматизировать мы очень хотели уже собрались написать этот дела самостоятельно но слава богам нашли на просторах интернета оказывается уже есть ванильных тоске лера написали само сообщество кубинская и продвигает его вы его попробовали подключили к нескольким приложениям ну точнее к нашим 40 микро сервисам которые придется под нашим управлением посмотрели что он работает нормально проблем если нет все вроде хорошо но все-таки один момент в нем нам где мне понравился и моего фармлю себе внутри написали то что конкретно нам не понравилось и сейчас использованию в таком виде патча приводить здесь не буду новая расскажи логику которую мы руководствовались и что она мне понравилась вот смотрите если говорить про ресурс вообще то есть нас у нас болит наш самый дефицитный ресурс это цикл то есть мы говорим сейчас request и лимитах в вашем случае это может быть память но один диски там особо не лимитируется еще подумаете поэтому и памяти на ресурсах которых может болеть голова если перегореть вертикальных тасс келлер то у нас есть ты состоянии это те ресурсы которые задает разработчик на создание приложения например 100 request of 1000 лимитов и он считает что с этими ресурсами приложение будет работать нормально и сто процентов там где близится request как только вы к этому приложению подключаем вертикальный авто скейлер выпал здесь назначен то вертикальных тасс келлер исходя из статистики работы приложения за какой-то промежуток времени он просто смотрит что это приложение почти ничего не потребляет и таким образом снижает ему request и до минимума которые оказали бы вы указали велико но в то же время вертикальный of those келлер снижает пропорционально и лимиты по цене до 100 м то есть в 10 раз он снизил request a 10 раз он снизу и лимит и вот здесь кроется самая большая проблема и карману наш взгляд потому что вот эти лимиты в одно ядро которая заболел задал изначально цепочек один наверно все таки были заданный не зря могут быть ситуации когда этому приложению в какой-то короткий промежуток времени например там пять минут за одну неделю нужно именно вот этого 1 просто маната либо запустилась либо какую-то задачу внутри себя выполняла либо что то еще с ним произошло и если авто скейлер у нас трогает и лимиты по циклу то значит он реально занижает производительность приложения и тут могут возникнуть такие ситуации что приложение будет либо отрабатывать в десятки раз больше а если она отрабатывать десятки раз дольше там can плане справа на может убить за это сказать что меня не отвечаешь на протяжении одну минуту а давай-ка это перезагрузись пожалуйста и в этом проблема а вдруг на старте приложения оно тоже должно выполнить какую-то ресурс емкую задача в течение 30 секунд а как только мы в 10 раз занизили его производительность выполняет не 30 секунд атеиста онлайн испробовать поймал видом постоит и все и у вас приложение никогда не запустятся и вот этим как это нам не понравился тот of the skin который предоставляется сообществом вы собственно когда мы его форк 0 и мы на самом деле оставили практически весь функционал его без изменений но отвязались от вот этого пропорционального изменения лимитов request то есть мы управляем вертикальных тоске ли в москву большей части управляет только емкостью которая требуется под приложения и он экономит нам емкость снова упасть и подвергается а лимитер поступало которые конкретно ограничивают работу этого приложения мы оставили как есть об этом а я них лучше на самом деле знает разработчик они их лучше знают команду нагрузочного тестирования колядками этому относятся относимся к более менее адекватные считаем что это нормальная ситуация то есть вертикальный артас келлерман давно помогает нам экономить емкость платья значка купят меньше собственного железо играть вечно но в то же время он силится и износ не ограничивает и проблем за вот эти где-то полгода или семь месяцев мысли больше не знаем увидим если после говорить про авто скрининге то конечно нельзя пройти мимо и горизонтальных с керлинг которыми распространенных больше если вертикальную вам помогает экономить емкость исходя из скажем так экономить емкость на всем к власти во все во все время его работы то горизонтальный of the skilled вам помогает экономить емкость например не в часы пик когда у нас пользовательский трафик минимален то есть на ночь мы можем сдавать наше приложение и тем самым освобождать какое-то большое количество ресурсов которые могут быть заполнены какими крон задачи бэкап с задачами задачами по подсчетам метрик составление отчетов что-нибудь такое то есть это нормальный рабочий процесс вам не нужно покупать под это дело железо но зато ночью вас этого железа больше чем днем а днем горизонтальных каскелен он повышает ваши подать повышает количество ваших кодов и не дает просто дайте свои ваших продуктов то есть ваш ipod и отвечают именно в то время в какое вы договорились с менеджером также мы пришли к тому что горизонтально скелет у нас распространил мимо всех продуктов а только на реально больших есть вот реальные часы пик когда у вас трафик ночью в десять раз например меньше чем днем и вам нужно там если ночью вы держите 23 года то надежных там 910 кода ими что такое вот это вот очень хорош и экономит то есть если вы будете горизонтально каскелен подключать ко всем продуктам то возможно вы устанете потому что продукта довольно таки много и во-вторых возможного профит они получат и потому что минимальное количество кодов 72 от и или четыре кому-то может и не потребоваться поэтому я бы вполне себе можно жить если вас на микрофинансов небольшие с еще тут такой момент что горизонтальных тоске линк с вертикальным сочетается не очень хорошо потому что оба этих друга они работают с одними и теми же ресурсами то есть и памятью по умолчанию получению работает с ответили в связи с этим может возникнуть такая ситуация что вам в течение дня горизонтального каскелен он поддерживает так чтобы потребление spool каждого пода не больше шести процентов в течение дня вертикальных тоске ленка эту статистику набирает смотрят а а у этого подали больше 50 процентов а давай-ка ты поступаешь он их понижает потом это оказывается то что у вас как только вы понизили ведь вас там сотни до 50 например для горизонтального авто скейлера у нас планка опустилась ну и можно до 50 процентов от request of держать и она допустила с 50 мм до 25 и подрастала больше и по итогу они могут очень сильно подраться так что вертикальная скейлер например зависит ресурс по максимуму но зато у вас в приложениях будет просто 200 экземпляров по всему пластин и это на самом деле проблема и никто не советует использовать скин вместе в простом таком случае мы для себя решили что мы их можем использовать вместе но они должны стелятся по по разным метрикам то есть вертикальная скейлер мы также оставляем спилить и памяти горизонтально мы увеличиваем уменьшаем количество тонов исходя из пример fps от антиподы то есть мы на первичном контуре узнали сколько у нас держат конкретное приложение 1 под и поставили триггер что если на один приходится больше тогда пожалуйста добавь еще один под а вертикально авто скейлера уже сам будет подбирать идеальные ресурсы для этих для всего для всех позах приложений чуть-чуть графиков про горизонтально каскелен как он помогает нам ночью например сэкономить ресурсы обмен их занять чтобы поддержать самый простой пример вот у нас утро-вечер у нас отрабатывает изначально с кейлин и получается ночью масс стандартное состояние два кода на предложения а днем 3 чтобы поддержать или на время ответа и ночью мы экономим сто миль и корда которая может быть пришить их либо еще какую-то трон задачу либо вычислительную либо что то еще произойти ну собственно пожарные вроде потушили то есть есть способы способы которые позволяют нам найти ресурсы для приложения уже имеющим сожалеть даже без заказа на нового потому что заказ нового как вы помните два-три месяца ожидания это не щелчок пальцев поэтому пожарные потушили надо провести посмотрим и посмотрим собственно учиться планирует сам процесс во всем нецивилизованно называется capacity planning и собственно напоила тоже мне дает можно компания чтобы больше нам пожарить и шить что вообще мы под этим подразумеваем себя копатель онлайн как бы подразумеваем у себя то что мы должны знать вот то мы мы должны четко определить тот 3-е который скажет нам о том что пора заказывать железу или пара оптимизировать работу текущих приложений и тут же когда мы знаем что пора нам нужно знать а сколько нам нужно заказывать железно нам нужно четко и конкретно цифрой нам не нужно вычислять ее он неделю или месяц мы должны зайти на какой-то график посмотреть сколько реально можно сказать сколько у нас недостаток сколько вы лишний увидели что вообще произошло мы для себя определили к пасте клиент как и придерживаемся его в таком такой формулировке такой ключе тут бы я хотел попросить или все задания опрос на всех чтобы понимать вообще кого вообще есть capacity planning как у тебя его используют не используют может быть кто-то на облаках живет и там нет этой проблемы а может быть она есть да же я опрос уже отправил я думаю что пока мы собираем на него ответ и мы можем как раз на пару вопросов из chateau ответить готов да конечно давай попробуем смотри здесь есть вопрос который в общем возникли у меня в самом начале упомянул что вы сейчас железо и в том числе и для versys там старриджа используете а вот здесь вопрос такой все ли ваши приложения state вас если нет то как вы храните данные c флинн стар или что-то еще и мне тоже очень интересно потому что я тоже кластер повернуться на железе и как вы делаете persistent storage расскажи пожалуйста давай попробуй как сестринство джефф у нас несколько видов мы начинали самого простого у нас есть компании большое цех plaster он предоставляет как с 3 хранилища так и в сторону и с ты это собственно программный уровень 2 разработчики самостоятельно куда заливают файлы вычитывают оттуда это стандартно то есть мы соседняя команда просто раздает ты командам они сами их используют а rbd мы позволяем подключать конкретного дата центра то есть в каждом дата-центре мы подняли по маленькому цехов на ssd дисков и просто напросто подключаем их к кпд девайсы конкретную года стоит фон которые нужны обычно ну например нас такой артефакт и живет внутри где просто в виде стоит он сам сам сам под крутится 0 0 и виде стать flow это в конце это момент единство врача также мы недавно начали делать локальные стороны чтобы можно было на общих но так держать стоит у нас процентов процентов 90 приложения the state is или даже 95 примерно так и 510 это стоит состоит из собственно стоит тут заключается в двух моментах это либо конкретно данные хранить которые надо стараться не потерять либо это хрень какие-нибудь каши например из которых надо быстро восстановиться если под уме и бывает такая ситуация что нам нужен очень быстро street full в данном случае нам помогает локальный сторож пока local перед которых подумать и локальный диск 90 создашь ну да у нас получается приложения привязана к какой-то конкретной нодди но но работает не очень быстр и таких каким приложением конечно же есть требования чтобы они работали не в одном экземпляре это классная система чтобы выход одного из я не приводил деградации мне удалось отведен вопрос да мне кажется что удалось еще вопрос из чата скажи пожалуйста ты говорил про лимитирование цели матирования там памяти а в чате из опроса имитируете ли вы как-то сетевую активность приложений хороший вопрос нет сетевую активность приложениями никак не лимитированным я честно не могу вспомнить если возможностью кадр меняется и или ее лимитировать в новых версиях может быть уже появилась у нас 117 сегодня она будет катиться надо будет посмотреть потому что на самом деле лимитированной сетевой активности тоже дело хорошее потому что бывают ситуации когда тот рейс разработки ошибиться и тебя вся гигабитная сеточка на вот этих маленьких лодок на которых придется приложение она просто вся забивается и у тебя возникают проблемы с фантомной недоступностью какого-то каких то приложений лимитировать я надо но действенных способов я пока что не видел если кто-то их подскажет в чате например это будет очень здорово я думаю что это отличная тема для разговора в discussion ки после после твоего доклада а сейчас давай попробую тебе еще один вопрос успеть задать им и двинуться дальше хорошо да вопрос про утилизацию высвобождаемых ресурсов ночью можно сказать вот у вас наверное какой-то дневной паттерн дневные пике трафика до днем вас больше пользователей ночью меньше а что вы делаете с кучей ресурсов которые освобождаются ночью хорошо спасибо да у нас пике пользовательская трафика это рабочие часы в городах то есть реальный график с 9 утра до семи восьми вечера исходя из зеленой зоны для подбора проспать где засыпает ночью нас возбуждается процентов на 20 от кластер как минимум и вы их забиваем либо это backup задачи либо эта задача по расчетам у нас есть внутренние расчеты например математику посчитать мат модель об учителе еще что то такое у нас тоже это запускается в кубе также у нас есть большие задачи когда нам надо обновить данные на наших сферах и это несколько сотен job которые запускаются в параде или власти и колбасит в течение пары часов и да не задач утилизируют эти высвобожденные ресурсы спасибо я тебе готов результаты опроса рассказать две трети ответивших у нас живут в облаках и бед не знают даже не 2 3 тоже 70 процентов а примерно 30 процентов заказывают ресурсы по факту и ждут что думаешь насчет этих результатов ну правда к я не сомневался да что там большее количество воды в те 30 процентах мы недавно были все впереди но это на вещи люди которых бат до те люди которые наибольшую пользу твоего доклада сейчас получат я поэтому предлагаю не останавливаться и продолжать рассказывать что у тебя еще есть спасибо алексей да я тоже надеюсь что мой доклад принесет пользу и тем кто заказывает ресурсы по фактам так давайте попробуем продолжить про capacity planning как вообще обычно делается capacity planning если вы хотите его начинать дело то есть что происходит но самый стандартный верный способ capacity planning а вот такой просто и прикладываете пальцем не был говорить финансовом нам нужно на 200 миллионов и тогда все будет хорошо финансовым аманды отдавайте вот вам сто например у нас тоже все будет хорошо и вы такие но давайте в общем это самый герой capacity planning с которого начинают все надо их начать и попробовать посмотреть как пойдет у нас что-то подобное же было у нас было еще немного чуть-чуть хуже чуть чуть хуже ситуация которая сейчас расскажу мы выдавали квоты командам кодами приложение 1 квоты на namespace и в кабинете вы их избивали исходя из того сколько команда у нас просили осло информации сколько из этих крот используются на тех ресурсов которые можно планировать потому что есть ресурсы которые реально надо планировать крутится 90 процентов 80 процентов приложения есть 20 процентов приложений которые улице вообще на отдельных ресурсов но команды могут иметь доступ к тем или другим приложениям и не надо обучить команда делаем 20 namespace of чтобы они там раскидывали эти ресурсы между этими разными ситуациями поэтому мы выдавали каждому по своей плоти и сложно было понять кого у нас используются на общем сколько на локальных вот это вот изоляция которые то что сказал она никак не учитывалась отдых они были общие и так как у нас было сгруппировано вот этого получается общие ресурсы и какие-то часто не ресурсам и не могли понять насколько у нас реально какая утилизация и от кого и там несколько разных команд через какой ну прям пойти поговорить об улучшении по улучшению утилизации или с такой каким подразделением пойти поговорить сказать чтобы например не себе новую сервер купите она до закажите какое-то железо в общей клуб потому что уже здесь потребляет много вот эти вопросы были было очень сложно решить и у нас не было это инструмент и тут еще такой момент что так вот и выдавали исходя из того что у нас запросят вы старались не выдавать там миллионов от сразу потому что с этим не было проблем а что-нибудь один например заняла пять секунд а все остальные были сидели и думали почему мое приложение не курица plaster вроде бы я любил явные успехи есть коды на то чтобы заправить новое приложение новую мечту листа поставь власти и конь через вот и таким образом мы вручную моделировали кому сколько этих ресурсов и команды ожидали нашу команду были такие между нами в очереди в поликлинике когда педиатр освободиться следующего запустил кому можно еще и суставы это как бы эффект тела и на нас и на команды и общем это все лишнее турка поэтому надо было обе эти проблему решать и вы решили попробовать их вместе решить 1 отлично что эта проблема планирование ресурсов то есть как это как вообще вот выглядела та ситуация которая рассказал есть у нас общие баньши ресурсов это в основном состоятельность сервисы это какой-то большой путь серверов на него там 80 процентов приложений есть у нас какие-то стоит full и под которыми мы нарезаем на приеме ритуальные мужчины и окон стейки под конкретную задачу или заказывать железо под конкретную задачу с конкретными диска например и конечно размещается вот эти стихи конкретно от этих новых это все в куре в одном озере и просто в этих нот разные 0 и селектор есть еще например строителя с приложение но они такие встретились что они просто колбаса 24 на 7 на 365 дней в году постоянно в 16 ядер и не дай бог свое маленькое приложение за едет туда дома никогда в жизни не запустится потому что не продохнуть не дадут и в этом тоже проблема а ресурсе раздавались вот так вот то есть есть у нас какой-то namespace ему нужен estate фу ему нужны хостить приложения и на общих ресурсов и чтобы разработчики не метались между к ученным space of где там стоит где там стоит нас тут на запутали зачем вообще об этом знать вы просто выдали квоту на общее все они шагу лица просто не знаю что у стоит валаноре селектор такое-то устроителя сообщи так а это все как бы вот вот это единственное знание которыми переда охота обще это для них очень здорово но для нас на самом деле это проблема потому что они не знают сколько вот так вот а обрезана с одной стороны сколько другой и то же самое состоит леса то есть как команда которая передать и ресурсоемкие приложения на счету литых туда но она в то же время там держит десяток микро сервисов каких-то общих ресурсов которые там прп с внутренних держат и и вообще нормально то есть и ее тоже не надо учить не надо обучить своих разработчиков вот эти novak лишними разделения нужно просто нормально инфраструктура строить ресурсы отделились вот на два типа получается вот это стоит флавиусом своем kinesis и это не те ресурсы которым от планировать это те ресурсы которые планируется один раз по факту то есть вам говорят что у вас мы разработаем вот этот сервис он будет сто процентов ресурсоемкий не надо чтобы его кто-то мешал и не надо чтобы он кому-то вещал и эти ресурсы либо из облака pan stik а вычленяются либо сервера под них находят либо какой то еще случае и ты их один раз сделал можно сказать туда больше не заглядываешь ну максимум ты можешь команда можешь прийти тебя сказать что вот у нас вы становитесь рпс вырастут давай-ка пальчик ресурсов а вот именно конкретно общие ресурсы которые нам интересны при планировании вот они их надо думать головой больше но ситуация какая то что вот год и не делятся между общими ресурсами и частным и поэтому нам сложно было планировать общие ресурсы потому что мы не понимали сколько чего где находятся вот в этом была проблема планирования и была проблема в том сколько чего заказать и вторую проблему мы решили тут же решать то есть они примерно об одном и том же и можно решить их способами как вообще ног от угла выдавали то есть вот вы подавали хату чисто на те приложения которые крутятся и фото на apple и потому что дефолт на вот эта граница это жесткая а то вы не можете за нее выйти то есть нельзя вот это вот новое приложение которое бы подъехала оно не могло бы зашить улица если бы она вышла за вот и проблема возникала в том что вот это новое приложение это все ручные операции по написанию там циферок увеличение код для конкретного мира по запросу jira вот весь pipelines просто какой-то которую еще надо решать asap потому что как бы ты не должен быть слабым звеном процессе доставки половине как я вроде уже 2020 год на дворе ты все равно доставляешь это дело медленно и чески это ручные операции вот от этого очень хотелось избавиться углу и собственно эти проблемы надо было решать решаемых всеми любимых способов коми конечно же мы пишем что-то свое в данном случае мы решились на написание собственного контроллера код до убирается которая была бы не жестким по квотам а мягким которыми можно было бы превышать и вот как это все выглядело смотрите нас есть стандартная связка до стандартные компоненты вывернется это принимается агрессивно в нем хранятся информации о квотах о приложениях ресурсах разных типов ресурсов а ногах обо всем короче говоря я думаю все знают описями и к ним уже есть рядышком schedule a которые собственно выбирает куда за schedule этот под который ресурсы попросил и можно ли его вообще захочет делителей нельзя мы решили какой соски подключиться то есть мы написали свой небольшой контроллер который работает с кастомными ресурсами которые мы сами себя завели позволяет пастырском склада то есть это там по моему ресурс код это дефолт вы сделали к стрессу скотт это наш собственный и он не жёсткие и мягкие the soft кода которую можно превысить и как только то что лишь приложение вот те request и лимиты что ты поставил они записываются также не только стандартные стандартную из воска воду но ее в нашу вот у меня у нее в состоянии и исходя из этого состояния мы можем получить какие-то метрики из этой кости ресурс квоты чтобы мы же можем превысить состоянии там оно такое условное то есть вы там просто прописали сколько сколько доступно и сколько используется сейчас но нам но так команды могут превысить эту колоду нам нужно какой-то инструмент контроля за ними получить чтобы понимать вообще а как это все используется то есть может быть мы всем коду отключили все такие довольные пошли весь кластер забили у нас еще все закончилось виде вся идея с контролем год закончился через неделю после его релиза поэтому нам нужно как-то за этим делом следить и что-то делать вы собственно писали не большой экспортер который ходит как и получает вот этот ресурс себе для понимания вообще что что что происходит с кодами в конкретного приложения ну это дело он просто прими то и забирает и на него schedule отца олег ты какие метрики есть ряд и к этим мид по этим метрикам и случае если это и превышаешь тусовка то этот аят приходит тебе и количество ты привысил ее пожалуйста развесь что произошло так же мы предполагали что еще один компонент напишем но впоследствии от него отказались который вы запрещал например щит улиц особо особо пьяными разработчикам кучу микро сервисом за один раз например али как кот и или как так вот она превышать там в 10 раз например ты привысил засов вот-вот делать а все-таки надо запретить чтобы ты обратил на это внимание что то сделал вам по факту вы поняли что пока что этот компонент не нужен ли можно обойтись без него у нас остался только компонент который свою софт под у поддерживает то есть изменяет ее состояние исходя того что исходя из того что поменяем спейсер домена и на каких но that'sa schedule in a самое главное вот что и с противо которые эти метрики просто в пробитые передает на основе чего можно анализировать что происходит эта штука не очень простая для понимания возможно на словах алексея тебе как мально понятно что то еще были сказать слушай в воде вроде понятно давай я попробую основную идею своими словами сказать то есть я правильно понял что вы во мне подошли жесткие квоты которые есть для namespace афк обернитесь и потому что вы устали от того что к вам разработчики приходится ими в жиру и просят им накинуть еще еще еще но совсем их убрать нельзя потому что тогда весь кластер займут да и получается что вы сделали такую систему которая вы выставили эти квоты жесткие в бесконечность по сути и сделали свою систему которая allure feat если если вы разработчик приблизился к алексей все верно это как раз такие решения проблемы жду на когда люди ждали своего ты я возможно до конца здесь не сказал тут есть еще решение проблемы то где эти квоты используются то есть вот этот ресурс контроллер он привязан не только к общим потом ну то есть он признан он связан еще и канада то есть каждая надо у нас обозначается как то там много общая но до под кассандра много попов газ надо под что-то еще и вот этот контроллер он знает где приложения защиту лились и он поставляет вот это потребляемую квоту прелата schedule иного приложения конкретно к той ноги на котором она задумывалась таким образом мы знаем сколько у нас из всей от и защиту ли восход конкретно на на общий но так такое более гранулярный управление ресурсами чем-то которое есть uber не даси из коробки да да именно так и она тут же не напрягает команды с тем чтобы использовать десятки in space of которые например скидывали их только на конкретные нот на ну я кажется уловил основную идею давай продолжать поехали дальше спасибо едем дальше но собственно вот контроллер нам нам позволил теперь пиридатель выдаваемые под и конкретной многие селектора и теперь мы понимаем сколько у нас на общих ресурсов используется этих этих ресурсов сколько запрошена сколько используется сколько у нас есть еще запасся сколько нет и конечно чем освободили наша очередь поликлиники когда больше никто к нам не приходит а со стасом задачами и что нам нужно за релизиться срочно пятницу в пять вечера потому что product и пообещали что в субботу все будет на bayou но все происходили ребята вестись на ваш страх и риск если что-то произойдет может быть мы поможем может быть нет потому что мониторинг мы вам только в понедельник накрутим например что-то такое и вот эта задача скоттами чтобы alert на команду не мешали мы просто собираем и два в месяц решаем просто их пачками чтобы не переключая контекста между текущими задачами вот этими задачами на год мы просто собрались и решили их за один раз и проблем с этим и каких нет когда мы прилетели тут такая ситуация как бы нам же нельзя полностью отказываться от того чтобы люди не указывали request и или метро и мы просто она просто выдали им реально по 10000 к вто 40 терабайт памяти и особо активные пользователи нашего поднимается которые интересуются очень удивились потому что теперь столько можно делать и почти испортила весь сюрприз но с ними говорить как вообще вот эта вся схема выглядит исходя например не за иисуса раковиной ца вот тока чему мы пришли в обычной котте то есть это я по лимитам prequest там 10000 несколько тысяч или сотен много секретов и бац и здесь уже мы в процессе смотрим сколько можно нельзя и нужно ли здесь повышать обычно стены с ними никаких проблем нет 0 с другими типами ресурсов мы тоже поступаем их можно как и подсох плату и как так имеющим какая по бы под обычную хардкот а вот как выглядит наш собственный ресурс который мы написали и за которым дни теперь следим исходя из которых график смотри у нас здесь есть имя вот это например kazimova горки она говорит о том что он но не селектор здесь стоит на вот эту воду применяя на все ноды у которых лейбл это роли worker этого бага нашей общей но так обозвали они себе используются и на роли например вот из тех десяти тысяч цвету которые в харькове об 146 выданное на общее ноты и потом сто двенадцать штук нурик вас такое-то количество и таким образом вот команда казино она либо говоря имеет несколько таких вот то есть и есть квота на общие ресурсы где они их используют вы знаем сколько они реальные из них используют и у них есть несколько других вот к примеру смотрите помните картинку где бы дела нас есть разные типы но с разными типами использования но разные норм space может на нескольких надо сразу же быть приложение в одном и space могут располагаться сразу на нескольких надо вот сейчас это картина выглядит так что кто-то по она конкретно общие ресурсы и мы знаем сколько казино у нас потребляет на общих ресурсов также мы знаем сколько примерно стоит у нас но потребляет мы выделили какую-то кода туда небольшие ли вообще можно имитировать код можно сказать что типа не лимитирует так бы все и нам не важно потому что эти ресурсы не изменится их никто не займет главная славу казино просто-напросто какие-то адекватные дмитрий костров приложения поставить она на флоте за этим следить не обязательно что мы то знаем что ничего не изменится и добавлять вот туда будем только по запросу и также мы сделали небольшую защиту от ошибок для команд чтобы например они не чтобы они могли заехать на те годы на которые им заезд не предназначен но их вы об этом предупредили потому что раньше об этом никто не предупреждал если ты не указан или селектор то для проблема ты можешь заехать это мой своем настоятельства который топит круглосуточная и постоянно и средь разбираться почему меня приложения ответ она работает хотя на моей локальной машине вроде все хорошо а вот тут на серверов чат очень плохо непонятная ситуация а тут мы можем поставить в этом году так что на всех нотах кроме там общих ресурсов estate тебе можно что на всех нотах кроме общих ресурсов и стив холланд тебе можно 0 пода ты как бы защиту лица с марта так это превысило над усов прототип реликты полетит alleged вот это приложение засаду лилась на те моды где тебя дозволено мной под давайте разберись пожалуйста что у тебя произошло и таким образом как бы и разработки мы подсказываем что они сделали что-то не так потому что тепло это не и вот этих вот разбирательств почему она не работает где она вообще очутилась что происходит их просто перестала быть никто на это а так по графикам давайте пройдемся по быстренькому как это вообще было 68 ну вот то есть при общей когда у нас были общие кожи то у нас было все команды учишься 8-я запрошен set 4 использовался 14 когда мы в недели нашу схему оказалось что из этих 68 на общих только реально 53 запрошенные за действие 343 а используется всего лишь 10 то есть мы знаем что горизонт планирования здесь расширился например там с двух месяцев до четырех потому что наверно используется ночи вещь но самое интересное то это наверно про емкости кластер а то есть мы теперь знаем что у нас есть ёмкость есть фото которую запросили на общие ресурсы есть сколько реально и зато куда запросили сколько реально используются и мы знаем что у нас есть водород вдв 30 процентов когда у нас когда у нас остается меньше тридцати процентов от емкости кластером мы должны что-то делать то есть вот он триггер когда пора заказывать новые железо вот это самое интересное что вы получили для планирования ну собственно теперь мы знаем что когда кот стало больше чем железо когда выглядели то есть мы могли раньше выдать ниже больше чем вообще возможна жизнь а у нас жене можно эти apple у нас осталось совсем мало времени я предлагаю но мы в основную часть доклада уже закончили и осталась вот только сделать выводы и понять что же мы теперь знаем давай мы это унесем с тобой в дискуссионную зону чтобы слушатели могли кому интересно дослушать там а все остальные пошли на следующие доклады спасибо тебе большое за рассказ я всех кто нас слушает приглашаю а сейчас discussion q было супер кучи вопросов давайте пойдём туда по силе"
}