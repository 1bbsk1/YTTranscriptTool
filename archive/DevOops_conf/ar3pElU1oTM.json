{
  "video_id": "ar3pElU1oTM",
  "channel": "DevOops_conf",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "первый доклад на девуксе меня зовут Антон картунов и я техлид в инфраструктуре Яндекса доклад про переезд Минского контура нам сегодня будет рассказывать смотриненко Всем привет Мы сборе очень давно уже знакомы давно вместе работаем бой Расскажи пожалуйста ты был нормальным админом же раньше как ты докатился до жизни такой да использовали мы надежные технологии 10 лет плюс наливались руками жили на железе но все меняется мир двигается вперед и даже в таких низкоуровневых местах как админской контура Мы тоже решили начать это движение Что такое вообще админинский контур сейчас же Облака Вот это вот все причем тут админы админинский контур Да я согласен не очень понятно что это значит на первый взгляд речь о чем есть внутренняя инфраструктура обычно есть какой-то внутренний Клауд можно назвать это базовой инфраструктурой в компании она должна работать но чтобы она работала нужна сеть А чтоб работала сеть нужен наш админский контур очевидно он не может быть завязан на общую инфраструктуру и за кольцевые зависимости и вот собственно поэтому выделены что же нужно идти глубже и погнали да давайте расскажу еще немного о себе Я занимаюсь системным администрированием разработкой и в принципе сфере IT наверное уже лет 15 если не больше начиналось все с раздачи интернета в общежитии наверное много кто так начинал вот в итоге потом я оказался в Яндексе там занимались и облачными решениями виде Вас решение кокаин возможно кто-то слышал можно сказать облака тогда только начинались Вот Потом я занимался обслуживанием сторожа S3 нашего внутреннего Вот и сейчас я занимаюсь сетью Я работаю службе сетевой разработки и мы обсудим чем вообще занимается служба сетевой разработки Зачем нужны разработчики в сервисах связанных с сетью Вот Потом мы обсудим Почему мы решили снять хоть мы это обсудили уже немножечко но затронем это более подробно вот посмотрим какие варианты мы рассматривали а рассматривали Мы не только cubernetis как это ни странно посмотрим Почему Кстати мы рассматривали другие решения Вот какие условия были для реализации задачи а именно они и пододвинули нас к решению использовать cubernet вот посмотрим Вот как раз а инфраструктуре то есть у нас есть внутреннее облако почти все сервисы в Яндексе и в Яндекс клауди используют какие-то внутренние облака никто уже не живет на железе Как вы знаете у нас это некубер натис Но это неважно важно что для этого облака требуется сеть и эту сеть как раз мы организуем и сетевые сервисы Чем занимается наша служба наша служба занимается любой автоматизацией связанной сетью там даже наливка свечей мониторинг клиентов мониторинг би джиписи найти сервисов мониторинга у нас ну наверное десятки если не сотня Вот Потом мы обслуживаем все сервисы которые выдают какие-то сетевые ресурсы доступы какие-то майпинги между сетями заказ макросов Firewall многие его части то есть Firewall у нас выглядит как много разных частей и большинство из них мы в каком-то виде обслуживаем конкретно моя группа делает инфраструктуру для вот этих сервисов Мы хотим модифицироваться поэтому большинство этих сервисов селятся в наш вот этот маленький админский контур от которого зависит все остальное большое и в Яндексе и в Яндекс клауди и теперь возможно и за пределами вот так как это выглядит в реальности в реальности У нас есть пять дата-центров и у нас есть какое-то количество серверов в районе 100 они размазаны по этим дата-центрам они размазаны не совсем равномерно где-то поскребли там нарыли сервера где-то у другого сервиса взяли и все равно для понимания у нас опять дата центров мы должны переживать отключение хотя бы одного лучше больше плюс мы такой базовый сервис если где-то сломалась сеть наши сервисы обязаны работать чтобы все это починить и как это выглядит Ну как это выглядело вот в самом зачаточном состоянии я не говорю там про прошлые года это уже наверное лет 10 назад такого плана можно сказать на старте вообще Яндекса люди хотели в нашем окружении сделать код Новый сервис а Не находили сервера просто вот там попросил у кого-то что-то еще наливали его как получится не всегда автоматически Ну конечно с флешки с диска никто не наливался в дата-центр никто не ездил но понятно что автоматизация там никакой не было стали туда все вручную могли код собрать на сервере операционную систему тоже выбирали какую удобнее У нас есть не только Linux либо SD все таки Ну мы ориентированы на сеть Вот и было не было почти никакой автоматизации в том числе починки оборудования нужно было создавать тикеты что-то сломалось заметили уже пользователи пошли починили Ну вот как-то так естественно так не продолжалось долго можно сказать это такой стартап режим и это превратилось Немного более культурный вид то есть добавился был конфиги стали храниться там приезжать на машинки централизованно можно уже было переиспользовать какие-то Вот сервисы которые используются для заведения новых Ростов те же конфиги какого-то Демона и при этом все равно часть могли поставить вручную особенно когда какой-то Новый сервис только появлялся как я сказал их на самом деле у нас много вот и к этому добавилось централизованная система мониторинга в том плане что и конфиги хранились вместе И у нас был свой контур забикса то есть получилось такая частичная унификация но разброда все равно много сервера все равно могут выглядеть по-разному они банально могут принадлежать разным командам внутри нашего отдела Что случилось дальше это уже относительно недавние времена Вот Мы перешли на гипервизоры то есть до этого на самом деле все было поднято на хостах даже без контейнеров мы перевалили все машины под LX Ну там Алексей lxd неважно то есть теперь все живет в контейнерах машина наливается автоматически новый хост там вводится буквально в пару кликов в работу настраивается все теперь через Alt потому что мы лучше его умеем варить вот и получилось такое неофициальный подход к созданию контейнеров контейнер тоже создается в пару кликов ну пару кликов конечно в консоли Вот и единственное контейнеры не переезжает и все равно то есть подход получился не в сырный но никакой автоматизации переезда машин или может быть Увеличение количества контейнеров автонастройки их ограничений по по цепу по памяти ничего такого нету Почему мы вообще решили идти дальше Ну кроме того что очевидно все так делают и кажется стоит сюда двигаться количество сервисов наших растет к сети предъявляются новые требования и количество разработчиков на соответственно растет поэтому это обслуживает становится вот эту схему с lxd все сложнее Ну мы хотели поэтому достичь большей автоматизации двигаться в этом направлении Вот ну как вы знаете сначала коронавируса у нас кризис не только в России а вообще в мире железо становится дороже достать его сложнее хочется утилизировать то что есть более эффективно более эффективно это значит плотнее селить плотнее утилизировать авто скилл переезд возможно подав в нашей схеме этого нет мы хотели не закупать новые сервера если можно что-то поменять также мы хотели и абстрагироваться от инфраструктуры в целом потому что опять же новые времена несут новые вызовы раньше никто не мог подумать что интернет может закончиться в том плане что отрубили внешние интернет от России еще какие-то такие проблемы и возникают вопросы что нужны бэкапные контура Возможно стоит выживать на одном дата-центре вообще изолированном хотелось абстрагироваться от железа чтобы иметь возможность запуска Вне нашей инфраструктуры все-таки наш сейчас контур который был алексейшный он сильно завязан на внутренней особенности Ну конечно упрощение найма потому что наверное уже каждый диплоился в куберетесь как минимум сетом на мини кубе но уже далеко не каждый наливал сервер вот при этом мы не хотели закопать много усилий в эту задачу мы все-таки не сервис который делает облако вот Хотя облако хотелось получить поэтому у нас было условно полтора разработчика мы хотели видеть модульную архитектуру чтобы можно было использовать и внутренние компоненты наши индексовые какие-то как-то они должны были легко прикручиваться к этому всему Но можно было бы жить и без них при этом конечно же мы хотели максимально надежности схема которая есть сейчас ее плюс в том что нет Центральной точки отказа даже один сервер в каком-то виде будет работать ничего не умрет никакой автоматизации под никуда не переедет это звучит надежно хоть и по старому вот надежность должна сохраниться понятно что любая автоматизация она может как-то что-то сломать Но это воздействие должно быть минимальным Вот и опыт использования cubernet многие из нас разворачивали какие-то сервисы в кубернете использовали менеджер может губернатис или там Mini Coop Но на самом деле Далеко не каждый делал большой кластер при этом чтобы он был доступен на несколько локаций и и чтобы проблем потом в дальнейшем с этим не было с минимальными усилиями опыт такого У нас не было Вот какие системы мы вообще рассматривали при выборе мы рассматривали какие-то надстройки опенсорсные над lxl XD над кв м Если сходу погуглить можно идти же найти тот же прокс Макс там есть морда можно тыкать контейнеры виртуалки вроде бы удобно но надо рассмотреть и другое мы рассматривали и отдельный контур который нам бы могли сделать люди коллеги у нас компании которые делают базовую инфраструктуру базовый Клауд но там еще не было к этому ничего готового плюс мы бы не смогли отвязаться от своей инфраструктуры чтобы не очень удобно Велосипед велосипед всегда рассматриваем все это любят дальнейшая автоматизация над тем что у нас есть добавить немного больше немного скриптов еще чего-то вот уже начинает переезжать что-то мы этим но понятно ограничено Ну и конечно мы рассматривали cubernetis а то этого доклада бы не было вот какой подход к развертыванию кабинет мы все-таки использовали А мы выбрали его ну разработчиков у нас мало поэтому пробуем все сделать максимально стандартно пытаемся ничего не менять ничего не дописывать по минимуму пачить если патчем то мы не меняем исходник накладываем Диф для удобства обновления при этом мы должны полностью понимать что там происходит Мы не должны нажать две кнопки развернулся кластер Но это для нас Черная коробка когда там что-то сломалось мы приходим и начинаем усиленно Гугли читать Help и что-то как это дебашить как чинить какая-то магия вот этого не должно произойти Ну хай выбилити конечно любая компонент который диплоится как я сказал нас опять дата-центров на самом деле нас еще больше есть выделенные площадки под наши железо и в итоге не должно создаться ситуации что один дата-центр отвалился по какой-то причине экскаватор перекопал дорогу э-э чуть-чуть заделал оптику и Всё хана у нас всё сломалось сеть лежит и также мы решили изначально был такой план сделать несколько кластеров а не один в продакшене Почему Потому что ну как я рассказывал мы хотели максимально надежности и единая точка отказа это все-таки пугает Мы решили сделать несколько Ну переделать легко Пусть так с самого начала будет сеть у нас были отдельные ограничения тут отдельная наша проблема Почему все наши дата-центры они ipv6 там нет в четвертой адресации для железа ну вообще просто в 4 в 4 связанности внутри дата-центра нету только на выход из дата центра на границе вот при этом это усугубляется еще тем что у нас 3 настойку схема то есть два сегмент не выходит за пределы стойки Switch в стойке он выступая выступает в роли маршрутизатора в нашем случае чему это помешало это помешало тому что мы не можем взять и заносить bgp из одной стойки в другое То есть в рамках одной стойки какие-то анонсы у нас Работают но между стоек уже нет и поэтому часть overlay решение по организации сети внутри они не могут работать вот поэтому в том числе мы хотели какой-то простой вариант решения проблем с сетью условно чтобы кабинете просто раздавал адреса нашим подам без какой-то изоляции без оверлея Ну так на основе также примерно как у нас работал в LX на бриджах нам не было у нас нужды в какой-то в каком-то переусложнении и для начала выглядела уместно нужно было использовать внутренние электробалансировщика тоже никакого другого решения для привоза трафика сервиса пока нету вот ну Engine привыкли у нас свои разработчики его есть вот Антон например мой Эксперт поэтому мы ингресс другой не рассматривали решили мы сделать прототип почему потому что все-таки были сомнения говорю опыта у нас было не так много сомнения были в том что мы можем пойти по этому пути упереться в какую-то проблему которую не то чтобы невозможно решить а она заставит нас потратить очень много усилий у нас получится какой-то кастом который будет сложно синкой с обстримом поддерживать поэтому мы решили сделать прототип сделали прототип такой на коленке все руками естественно никакой автоматизации конечно во время запиливания прототипа рассматривались компоненты решения которые можно было бы уже использовать в продакшене стало более понятно что потребуется для появления продакшна но он был сделан и стало понятно что мы не упремся ни во что подобное наверное еще года два назад Судя по багам в трекере Так бы и было но сейчас действительно все получится э-э может быть прям наши проблемы с той же сетью и не встречались у других людей но они были очень похожи и решить все можно было подкрутить конфиги что-то где-то чуть-чуть подпачить и поехали но стало также понятно что кубернетис это не вот набор каких-то компонент которые мы поставили и у нас готов кластер а на самом деле все эти компоненты надо выбрать губернатис это такое дерево без листьев и веток и нужно погуглить Какой компонент конкретном месте подойдет лучше это было но некоторые проблемы потому что распространение достигло такого масштаба что компонент уже каждый второй запиливает и это все нужно выбирать Нужно исследовать А времени мало Мы все-таки как я сказал не группа облачная Ну и стало понятно что нужно менять концепцию У нас не было каких-то монолитов больших Но даже сервис в котором условно два локейшна в инжинкс это уже была проблема потому что Ingress он унифицирован и также гибко настроить Ingress как engings но не получается легко подкручиваем rewrite он появляется во всех Location А нам нужен в одно Ну вот просто уже проблема на ровном месте нужно менять подход к разработке Какие проблемы выбора стояли стояла выбора стояла Проблема выбора базовой наливки то есть чем мы поднимаем базовую часть cubernates куб спрей мы не стали использовать потому что ну не то что мы прям не любим Анти был но там очень много чего есть и это получился бы черный ящик что-то мы нажали что-то появилось но по-хорошему надо сесть и почитать все эти рецепты время на это не было поэтому использовали он разворачивает базовую часть но инфраструктура нестандартная но нужно попачить И вот эти дефолтные конфиги нужно было что-то придумать как это сделать не хотелось это там через кубка отель Эдит делать все должно было быть воспроизводимым легко разворачиваться и в следующий раз мы использовали кубиан патч забегая вперед но это кажется очевидным Но изначально мы даже не знали что такое есть вот в чем проблема Ну конечно же была проблема Как базовая настроить на весь сервера Но это мы уже умели прошлое наша инфраструктура собственно и была об этом также мы думали о том где будут жить мастера отдельной Тачки не хотелось под это выделять было бы слишком жирно плюс у нас Хай в обилиетин уже не один мастер нужно 3 Нужно несколько кластеров нужен тестинг и получается что там не знаю 10 процентов железа у нас мастера это было неприемлемо поэтому попробовали завести мастера в LX и это получилось пришлось конечно добавить там прав контейнеру подгрузить пару модулей но все заработало вот мы видим значит запущен мастер Он запущен Galaxy при этом понятно что и первое время и какой-то последующая часть LX контейнеров точнее гипервизоров остается и нет большой проблемы Что там запущены мастера продолжаем проблемы с сетью проблема сетью оказались даже больше чем мы ожидали на самом деле я думал что будет несколько проще их решить и об этом мы поговорим дальше также была проблема выбора базовых компонент которые уже диплоид непосредственно приложение в cubernets то есть как-то нужно было эти ярлыки создавать первоначально мы сделали их руками и кто-то может сказать что Да есть же вот Hill написал чарт Вот и готово Но на самом деле там хоть и есть шаблоны но ту же версию нового приложения кто-то должен туда закомить и тут нужно было выбрать У нас есть своя монореп в ней Мы в итоге использовали его вот и Ну Собственно как приложение писать нам Хилл костомайс Ну этот тут выбирать не пришлось слава Богу тут можно сказать однозначное решение кастомась мы используем для внешних компонент если там нет Hilt charter накладываем какой-то патч чтобы было проще обновляться То есть если тот же Ingress engines мы скачали с Интернета Яндекс попачили все все хорошо нужно было выбрать какое-то решение для CD Понятно есть пул пуш модель на самом деле базово надо выбрать хотя бы это потом посмотреть не знаю какое решение использовать чаще всего вообще в целом мы подходили к вопросу выбора Так что если решение какое-то использовать чаще всего Это скорее всего у нас будет там меньше проблем со своими кастомными особенностями и поэтому мы пробовали использовать его ну и секреты стали отдельные проблемой наверное у каждого в компании есть какое-то хранилище для секретов и в кубернатисе принято что есть секрет оператор Он берет оттуда секрет подпихивает корешок куда-то Однако наш хранилище секретов не имеет оператора для cubinate тоже внутренний Клауд он не cubernatis вот и это стало отдельной проблемой которую пришлось решать Это было несколько неожиданно Вот но и отдельно пришлось подумать про метрики как я и сказал если что-то у нас немного нестандартное Например у нас нет прометеоса это внезапно проблема потому что почти в каждой компоненте cubernet люди продукт подумали про метрики Но они такие думают Как какие метрики нам сделать естественно сделать формат А у нас свой формат у нас свое хранилище метрик и это проблема об этом нужно думать вот логин нужно как-то собрать Ну это мелочи но об этом нужно тоже подумать Вот и пришлось подумать о том как скрутить Ingress с нашим балансировщиком как-то их надо подружить как-то нужно приземлить трафик именно туда куда нужно И при этом не хочется изобрести сразу велосипед написать целый оператор тут решение тоже было найдено о нем я расскажу чуть позже вот ну и registry контейнеры хранить Понятно вас проблема Это скорее всего коснется но у нас свой ragistry поэтому такой проблемы не было Вот что сетью А пиво 6 Only Казалось бы 2к22 но мир не готов к ipv6 все еще мы видим докер иоризолинг и там нет 6 адреса мы не можем пойти и спорить Калика Imagine Например у нас уже губернатиз даже базовые не запускается вот эту проблему стоило решить об этом будет дальше но такая проблема она Возникала почти на каждом шагу вот справа опять же есть кусочек билете и мы видим что редис слушает у нас только четверку если у нас в поле шестерка у нас ничего не работает все ну берем кастом Айс и поехали вот что с выбором контроллера для сети мы использовали Калика Мы даже особо не изучали некие другие потому что как я сказал времени было мало большинство используют Калика и ожидалось меньше проблем если мы пойдем по этому пути IP туннели мы не рассматривали это не очень удобно на наш взгляд большие расходы на передачу трафика от этого отказались сразу я посмотрел оверлей у нас не заработал как я рассказал раньше при этом вы Ислан не заработал на ipv6 транспорте то есть там именно такая особенность именно в Калика на пиво 6 транспорте он не заработал Ну и мы пришли к тому к чему Мы в принципе и хотели прийти использование Нон мода то есть в этом моде калька только раздает адреса подам по умолчанию он раздает то что вы указали ему при нити но из-за того что у нас каждая worker нода но у нее есть своя подсветка нам Пришлось написать свой скрипт который прибивал к подсветкам просто ноду соответственно Калика выдает из этой подсветки IP адрес дальше все это маршрутизируется нет никакой изоляции но все работает и работает так как мы привыкли в LX можно назвать от переходным переводом но нас это устраивает сейчас можно и более гранулярно выделить вот эти подсветки скрипт этот переделать там не знаю с крана нормальный оператор и будет уже решение не вот так на коленочная а Хорошие то есть понятно что такое тех долго у нас будет относительно много когда мы только развернули всю эту схему Но что самое важное в долге важно чтобы его можно было легко устранить На мой взгляд совсем без тех долго что-то сделать новое вот куб прокси мод нам пришлось выбирать Казалось бы это тоже не должно быть проблемой есть opitables есть ipvs использовать petables и даже не знаю что это epitables потому что все работает но iptables Mod не заработался пиво 6 Only там он пыталась создаться по два сервиса на кон сервис и соответственно все ломалось На этом но я нашел даже еще на гитхабе оно было уже починено К тому моменту но новый версии cubernet еще не было она не была зарежена поэтому используя pvs Но на самом деле мы хотели использовать ipvs потому что он более производительным и к нему как сетевики привыкли вот отдельная проблема стал DNS DNS на самом деле всегда проблема И почти все сервисы которые с ним связаны рано или поздно страдают Но для нас это был отдельной проблемой потому что нам нужно использовать DNS 64 совместно сна 6 4 это решение которое позволяет нам сходить в четвертый интернет получить в четвертую связность Как это работает так извиняюсь обновляться не будем Как это работает мы подсовываем специальный DNS сервер при резоре адреса нам возвращается не оригинальный 4 а хитро замапленный шестой в него замаприл 4 эти адреса маршрутизируются вот этим нашим свечом стойке специальным образом попадают на отдельные сервера коробки которые стоят на границе дата-центра Мы кстати частично участвуем в обслуживании этих машин которые называются декабсуляторы Вот И там шестой пакетик снимается 4 выплевается в сеть в итоге мы можем сходить из V6 или дата-центров 4 мир и там не знаю спорить имидж Вот Но в чем проблема у нас сетевые сервисы иногда нам нужно оригинальную запись Что делать в этой ситуации мы поняли два Local cash То есть у нас есть 2dmance это на каждый Ну то есть на каждой машине по два пота 2dmance это один использую dns64 другое использует обычные DNS и слава Богу там в кубернете можно указать какой DNS Использовать можно использовать кастомный соответственно вот эти сервисы которые нужно оригинальная запись они используют обычные DNS которым нужно почему-то сходить в мир и по умолчанию они используют DNS 64 Да можно заметить что рано или поздно возникнет проблема Когда нам нужно сходить в четверку но и получить оригинальный адрес Хоста пока такого не было будем думать что делать вот так отдельная проблемой стала для нас драка запитывалс что я имею ввиду под этим внутри дата-центра опять же мы используем хвост Best Firewall Что это значит Это значит что внутри хостов в нашей сети нету фаервола большого трафик идет напрямую от машины машины сейчас можно подумать что безопасники этому очень рады но решение есть выглядит на следующим образом каждая машина сама себе фаервол наш сервис который как раз мы обслуживаем он раздает эти правила на все машины Яндекса они приходят к нам Мы отдаем ruset он применяется в итоге Машина сама себя фаерводит безопасником остается только следить за тем чтобы мы не раздавали что-то странное и чтобы администраторы серверов не отключили эту автоматику Понятно за то что у нас Клауд внутренней его обслуживает не так много людей и следить за этим не сложно Почти никто аккуратно голом железе но какую проблему это создает почти все сервисы которые хотят поправить много эпитеба с правил они создают большой Файлик с правилами потом делают iptables Restore для больших гранулярности для простоты применения Однако если таких сервисов у нас несколько то возникает понятная проблема один перетирает правила другого и в кубернетесе и калику и прокси они как-то используют эпите вас они добавляют туда свои правила добавляют над что-то меняют а наш ход Best Firewall делает то же самое и они начинают вот эту драку один перетирает правила другого этот перетирает их обратно и у нас получается такой кластер Шрёдингера то все работает то не работает то работает с каким-то другим образом и пришлось как-то решать эту проблему мы начали хост-бэст Firewall делали там защищенные бранчи которые надо трогать это учитывалось какие-то исключения также пришлось покрутить настройки и в куберетесь чтобы он пытался не менять правила делал это пореже делал меньше правил которые Ну при нашем использовании просто не нужны Но вот такая проблема возникла и она на самом деле даже не решена до конца иногда возникают какие-то подземные штуки правило нас очень много сеть большая и рано или поздно всплывает что вот тут еще какое-то место где у нас вот эта гонка Так что такая проблема может быть на самом деле она кажется уникальной для нас Но есть много тузови которые например какие-то тузой на безопасник их от безопасников которые могут следить за вот этим firewall-ом что-то туда добавлять и вас тоже может постичь такая проблема отдельная проблем как я говорил стала проблема того чтобы скрестить наш балансер с ингрессом То есть как работает наш балансер он приземляет трафик на машинке можно сказать вот на эти машинки нам нужно получить трафик туда прилетают пакетик но там поднимается виртуальный адрес туда прилетает пакетики мы пошли по простому пути заставили ГРЭС Во первых стать Diamonds этом запуститься на всех нодах и слушать сеть То есть это у нас единственный на самом деле из наших кодов которые слушают сеть контейнера сеть самой worker ноты соответственно в него Понятно начал прилетать трафик вроде бы проблема решена но понятно что есть ограничения если у нас 40 воркер нот при этом у нас трипода самого приложения получается относительно странно трафик прилетает на все машины потом ротятся в три машины которые могут оказаться и в другом дата-центре и Да это такая особенность неприятная с одной стороны с другой стороны это значит что сервис какой-то маленький и тут нет много трафика это не вызывает много проблем При этом если сервис большой то скорее всего много кодов и опять же все выглядит относительно хорошо более того можно сделать один балансер и дальше Просто разруливать в инженксигами всю эту историю вот отдельной проблемой стала настрой конгресса Ну я думал инженксин Джинкс что ж мы не настраивали Джинкс но наши конфиги относительно сложные мы используем и рейд лимитеры и бывает rewrite и что-то еще соответственно все это нужно было приделать конгресс а ингрессу но нефицированный бывает же не только engines в cubernet все унифицировано И это создало отдельную проблему Пришлось сделать немного странные конфиги тут стало очевидным что надо сервисы дальше распиливать на микросервисы не должно быть ничего Что делает больше чем что-то одно действие все остальное должно отделяться в отдельные поды по-хорошему какой кластер получился В итоге понятно мы используем Hero для своих приложений мы используем кастомайз для того что взяли с интернета где не было чарта свой Sei свою репу сейчас к ней добавился гид интерфейс что очень упростила жизнь потому что мы взяли argo CD опять же как один из самых распространенных сетей для cubernational модель нам понравилось Можно сказать она превзошла наши ожидания потому что она никак не завязана на базовую инфраструктуру спулил гид и спулил Ну тут не может быть проблем тут неважно что там дальше вот более того Арго сиди у нас катит сам себя и это работает он даже обновляет сам себя и это работает Это было очень приятно удивлением вот использовал свой рейджестре и насчет секретов как я сказал это стало проблемой и было найдено следующее решение все приложения все ямки у нас катятся через argocd но секреты сбоку сбоку Почему Потому что наш солт-стек который мы наливаем сервера он умеет ходить в нашу местную секретницу и мы решили не усложнять для начала сделать так чтобы псал забирал секрет подпихивал в ямник и дипломов губернатиз а тут на удивление оказалось что салцтек на самом деле как антибал у него есть губернатис плагин и он умеет такое делать да схема получилась такая немного сбоку это неудобно сложно сделать бесшовный диплоид если секрет меняется но у кого часто меняется секреты Мне кажется это не очень распространено и конкретно у нас сейчас это не вызывает проблем Хотя конечно же этих долг вот над этим всем как я сказал мы используем jeeps and gress при этом проблемы которые я описал это не Единственная проблема которая у нас с ним возникли опять же нам нужны метрики сен-джинкса Это можно сказать базу и метрики почти всех сервисов почти все сервисы их трафик идет через МГС и очень удобно видеть эти метрики Однако в инженексе люди подумали про это тоже и сделали юниста от ручку которая дает эти метрики но там Прометей формат он не очень нам подходит Пришлось написать свой модуль engings Это не просто на самом деле много тесте она туда уже включено там вообще очень много луа намного больше чем просто вот мы подсунули туда свой модуль он просто Файлик монтируется в определенную папочку Все работает и у нашего инжинса получилось ручка которая дает метрике в нужном нам формате очень удобно получилось недорого не пришлось менять Ingress опять же чуть-чуть кастомайза один Файлик Все Готово Вот R3 балансер использовали как я и рассказывал но используемой пиваску прокси мод и опять же насчет метрик как я сказал Нет прометеоса мы не можем использовать прометеос оператор но еще до всей этой истории Мы научились использовать телеграф мы научили телеграф свой формат телеграф вообще достаточно гибкий Я думаю много кто его использовал и как оказалось есть телеграф оператор мы чуть-чуть подкинули туда вот своих модулей которые умеет наш формат и все заработало В итоге телеграф оператор поднимает нам поды с телеграфом которые прилетают садкаром к нашим приложениям и все работает Ну и мы сделали Даймонд сет для коллектингалогов То есть он подключается к уберу коллектит все логи там есть свои решения потому что опять же у нас не Кафка не какие-то решения которые приняты в опенсорсе а свое хранилище логов и пришлось там чуть-чуть добавить конфига в керси слогу RC слов очень гибкий и здесь у нас все получилось настроить именно на нем вот что мы получили и какие в сумме проблемы будут вас ждать если вы попробуете сделать свой кластер на в том числе нестандартной инфраструктуре базовая настройка Понятно об этом нужно подумать нужно выбрать вот все эти компоненты многие мы выбирали просто самые распространенный потому что не было времени изучать другие решения может быть стоит это сделать но компонент больше чем можно ожидать Если ты не занимался этим раньше разбивка на кластера тоже вызывает проблему мы сделали 2-3 кластера вот мы решили сделать именно такое количество что во-первых не упало все если кто-то где-то накосячил но при этом может быть стоило сделать один об этом нужно подумать если нет опыта Но придется смотреть ходить на конференции так Ну вам нужно выбрать CD как минимум пул пуш-модель обновление как я говорил мы не должны получить черную коробку не должно быть такого что через год обсуждать нашего кластера выходит новая версия cubernation первый раз мы пытаемся обновиться и все ломается потому что мы не подумали об этом заранее этого не сделали Мы специально сели и попробовали обновлять с этим слава Богу все хорошо можно дренить ноды при обновлении перевыпускаются сертификаты и Ну об этом явно подумали но рекомендую всем это протестировать заранее про секреты логи метрики Я сказал об этом нужно подумать это нужно не забыть когда делаешь Прототип этого всего Скорее всего нет но выбора там тоже много так ну отдельной проблемой стоит Сеть опять же Возможно у нас это вызвало больше проблем из IP V6 но коснуться вас тоже это может если ваша инфраструктура какая-то достаточно странная не три сервера которые стоят в одной стойке тоже стало понятно что нужно менять парадигму разработки И когда вы приезжаете на кабернетис и делаете свой кластер тоже об этом нужно подумать заранее не пройдет такой вариант что вы сделали нового облака и все ваши приложения переехали наверное есть куча докладов и на этой конференции Я уже видел доклады про переезд в кибернетис Вот это именно об этом Многие думают что сделали свое облачко хоп переехали все готово нет придется менять и подход к разработке Вот но и дальнейшей проблемы сетью нужно не забыть про DNS даже если вы не используете DNS 64 нужно сделать кэш в базовой версии cubernetis DNS там все 2-3 пода есть Трейд лимитер это по сути корневой DNS сервер его вы быстро убьете не забываем делать кэш Какие итоги возможно прозвучит странно потому что все уже привыкли что cubernet стандарт де-факто Но я считаю что это относительно молодой проект он динамично развивается и баги в нем все еще есть то есть нет такой ситуации Что как с алексе который мы никогда не обновляем соответственно Мы не наступим на новую проблему здесь такие проблемы могут случиться но при этом их оперативно решают они не критичные обычно документации очень много и на вашу проблему скорее всего уже кто-то наступил то есть вот это динамичное развитие и семимильные шаги Ну они несут какие-то риски Но эти риски нивелируются тем что пользователи очень много и все кто-то зарепортит раньше вас вот при этом из-за этого же очень много модулей архитектура очень модульная в базе почти ничего нет Вы можете приделать что угодно и даже если какое-то решение еще не существует есть операторы вы можете написать свой это не сложно и получить что угодно из своего кластера Ну и про надежность слава Богу разработчики подумали Об этом я пробовал ломать кластер разными путями и ломать мастера что-то еще не происходит Вот каких-то неприятностей типа сломался мастер все под и поступались на всем кластер такого нет но развитие динамично как я уже сказал не прокатит тема сделали контейнеры и еще пять лет не трогаем это дело нужно трогать есть уникальное решение это круто операторы операторы позволяют нам сделать свой дебасс на коленке и многое другое это прямо то что кубернете сильно отличает от других решений удобная релизный цикл но как я сказал про обновление подумали при этом совершаются какие-то другие обслуживающие действия просто об этом Нужно заранее почитать Нужно заранее знать нужно Быть готовым что если Вы обслуживаете кластер это придется делать но и есть много тузов и способов за дебажить что происходит в этой условно черной коробке удобно почитать и логи и есть логи события любой компоненты понятные связи Ну не возникает такой ситуации Что что-то где-то сломалось и мы сидим и мы не знаем что делать непонятно вот короче туши туши все заявление на увольнение уходим нет такого нет понятно что дебажи чуть сложнее чем просто железку там себе дамб уже условно разработчик не может зайти запустить легко Но если он захочет это тоже можно сделать Ну и в итоге унификация стандартизация cubernet стандарт де-факто получаем этот плюс что в итоге наши работы получилось полгода мы делали там прототип в условно свободное время еще полгода мы делали Production времени было немного разработчиков было немного но этот условно год не прошел зря половина железа из там условной 100 серверов было перинолита под cubernetics оно работает в продакшене есть два кластера есть отдельные тестовые кластер там уже крутится Реальное приложение отвечает на запросы но сейчас совместно с железом То есть сейчас схема даже сложнее часть кодов в кабинете часть в LX и балансер натянут на все это я думаю что мы получили даже больше чем хотели наверное речь идет в основном про операторы потому что это реально крутая штука ну и опять же больше чем хотели мы получили В некоторых случаях с тем же Арго сиди Я не ожидал что есть такая удобная гитаб с практика реализованная которая у нас заработает и полностью нас устроит нам почти ничего не придется пачкать Ну и велосипедов Мы запилили даже наверно меньше чем ожидали причем все эти велосипеды они не выглядят как сторонним решением они как-то прикручены к стандартному либо это патч либо это не знаю максимум Свой контейнер который наследуется от базового то есть в итоге мы можем обновиться обновиться легко Нам не нужно держать целую группу людей которые будут вот этой поддержкой заниматься Ну и конечно Тут этих долго осталось многое то чего хотелось попробовать это вот и операторы для баз данных сейчас у нас в основном Стоит ли с приложения то есть нужно заводить стейку Вот и другие сетевые контроллеры возможно решает часть наших проблем которые пришлось решать нам вот но и сделать секрета оператор для нашей секретницы как я сказал тоже пригодится тут немного мемасов короче я думаю что мы получили то что хотели путь оказался тернистым но тоже не настолько насколько могло бы получиться Спасибо всем коллегам которые помогали в реализации этой задачи всем кто пришел послушать доклад Ну и всем тем кто использует нашего облака И вообще наши сервисы будет приятно продолжить общение и в комнате и сейчас я готов поотвечать на вопросы Боже Спасибо тебе большое за доклад Мне кажется всем понравилось смотрите вопросы Из зала Наверное мы не будем сейчас спрашивать для того чтобы услышали их трансляцию пишите их пожалуйста в телеграм здесь qr-код куда вы можете написать и Пойдемте в дискуссионную комнату где мы сможем подискутировать у нас там неограниченное количество времени практически сейчас у нас ну буквально пару минут есть давай вот наверное самый первый вопрос который был вопрос звучит так железные машины где-то остались или все уехала в кулер и ты говорил что половину машин перелили что с оставшейся половиной получится ли все переехать не получится ну половина Ну во-первых конечно есть риски то что они отстоялась полгода не может считаться надежным поэтому не только по этой причине половина железа в кубери но опять же не все наши предложения встретились их несколько сложнее перевести чтобы перевести требуется время со стороны разработки нужно попробовать операторы баз Ну то есть есть ограничение со стороны разработки и с нашей стороны есть какие-то доделки которые хочется решить где-то добавить мониторинга Ну изначально на самом деле мы планировали оставить часть lx7 машин Потому что всегда есть какие-то кейсы страны которые на кубернете относительно сложно решить но сейчас я думаю что большинство мы все-таки перенальем и тот же State сейчас отлично ложится на cubernetics он может туда тащить есть особенности нашей сетевые например часть контейнеров их ip-адреса записаны в целях свечей чтобы их туда пропускали то есть файервола там но там есть и Firewall но есть и некоторые доступы которые привязаны к ip-шнику соответственно пот не может переехать он должен жить на какой-то конкретной машине такие проблемы придется решить либо прокси у это сделать но я думаю что все проблемы решаема в итоге не останется и за этим будущем вот ну что же Отлично мы тогда завершаем нашу дискуссию сердца вопросов ответов здесь Пойдемте в дискуссионную комнату с вами подискутируем по пути можете кофе себе налить и всем спасибо думайте интересные вопросы"
}