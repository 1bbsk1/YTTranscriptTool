{
  "video_id": "EcBOzkoiYT4",
  "channel": "DevOops_conf",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Всем доброе утро Да всегда есть где-то утро Сегодня у нас весьма животрепещущая тема А да вы видите ее на экране это пирамида потребности мультитанансистемы вот прекрасного Константина малого поприветствуем его Спасибо я понимаю что это такая история как бедная но все-таки ты немножко будешь О другом говорить давай мы до наших слушателей и в онлайне и здесь донесем все-таки мысль про что это будет и на кого прежде всего доклад нацелен чтобы очертить нашу целевую аудиторию а да название это на самом деле метафора отсылка к пирамиде Маслоу в том плане что если вы не закрываете какие-то потребности сервиса то рано или поздно вы не сможете расти дальше Да не сможете придумывать новые фичи потому что все есть ваши силы будут уходить на проблемы с которыми вы столкнетесь из-за того что не закрыли какие-то потребности вашего сервиса вот мы представили это некой виде пирамиды все-таки на кого Прежде всего это нацелена это исключительно история вокруг процессов и для Тим лидов менеджеров или это в том числе для архитекторов и рядовых например разработчиков Это скорее автобиография сервиса Когда вы хотите построить очень большую систему с какими проблемами вы столкнетесь в процессе роста ваших кластеров сперва там от сотен потом тысяч машин А сейчас у нас уже и десятки тысяч машин в отдельных кластерах проблемах которые вас будут от того что ваших кластеров стало больше что ваша команда стала больше что пользователи вас больше стало и поддержки стало сложнее Это скорее автобиография нашего сервиса я услышал много слов больше сложнее и сильнее то есть так или иначе здесь и про хайлоут и в том числе условно про Big Data Я думаю надо сначала рассказать что такое выйти заурус чтобы было понятно Ну тогда не будем спойлерить И прежде чем начнем Я Напоминаю что данного скажем так зала есть свой чатик в котором вы сможете задать свои вопросы Сейчас вы видите его на экране Так что не стесняемся понятно что какие-то вопросы будут в онлайне какие-то отовсюду здесь после чего в самом конце У Вас в любом случае будет возможность помучить Константина в дискуссионной зоне ну Кость давай начинать не Пуха черту смотрите вот нашей метафоре нашей пирамиде потребности должен быть надежный фундамент хороший сервис который хорошо масштабируется отказоустойчивой и для нас это выйти замуж что такое давай Что такое выйти замуж Я в двух словах вам расскажу это распределенная система хранения обработки очень больших объемов данных разработаны В Яндексе недавно выдали в Open Source можно представить что эти запрос состоит из двух частей первая часть отвечает за представление и хранение данных а также запуск операции поверх этих данных в ней есть мастера которые хранят в памяти дерево объектов кластера даты ноды на которых хранятся данные пользователи exenode на которых выполняется пользовательский код Map regis операций есть планировщики которые распределяют между пользователями ресурсы кластера и запускают их операции контроллергенты которые отслеживают уже выполнение конкретных операций и http и rpc прокси являюсь являющейся входной точкой в кластера звучит очень похоже на ходуп на самом деле это и есть аналог который разработал Яндекс для себя место ходупа а вторая часть мы Ее называем динамические таблицы это keywayustorage распределенный с поддержкой транзакций он чем-то похож на кассандру или эти части используют общий общий слой хранения Кроме того если говорить про дюс то в 2023 году в общем-то не программисты не аналитики не любят писать мапрыгюсы вот совсем они предпочитают использовать какое-нибудь Сколь подобный сервис например в мире ходу поэта Хайф у нас а для этого есть отдельный сервис под названием якуэль он преобразует искуэль запросы подобные запросы в цепочку мы определимся операций которые выполняются на кластерах и ukl позволяет обрабатывать большие объемы данных это десятки и сотни терабайт иногда даже петабайт в одну превью с операцию а если у вас небольшой dataset и вы хотите получить ответ как можно быстрее потому что все-таки мап reduce операция не занимают время для небольших объемов данных Я думаю это десятки и сотни гигабайт У нас есть два специальных проекта под названием Speed и чит на самом деле это Spark overwatch И крик Хаус войти То есть можно запускать прям внутри выйти заурс кластера небольшие кластера спорта и Хаоса использовать их для доступа к данным в Яндексе несколько десятков инсталляций уйти заурск кластеров они очень разные есть совсем маленькие кластера там десяток машин в основном это кластера в сотню или несколько тысяч машин есть два больших кластера в которых больше 10 тысяч нот в каждом и на них можно хранить больше данных пользователям доступно больше миллиона вычислительных ядер есть несколько тысяч ГПУ которые можно использовать в расчетах скажу так аккуратно кажется что в этих кластерах хранятся все проекты Яндекса А это более 2000 домашних каталогов и каждый день с этими кластерами работают тысячи тысячи активных пользователей Вот в такой сервис 7 лет назад я пришел в команду которая занималась опциями Да мы тогда жили не во внутреннем облаке А на голом железе управлялись чифом нас было шесть человек число машин у нас наверное уже перевалило за 10 тысяч и Если бы у нас не было подспорье виде специальной системы управления железом нам было бы уже плохо то есть вот если посмотреть а слайды Да у нас уже был вопрос не про пару серверов да скорее вот вторая картинка про грустного котика которому надо там тысячу серверов поставить за раз вот кстати рекламирую шедевром отличный способ на генерить что-нибудь для презентации так вот в докладе эту систему называю hardwarry Management System это часть инфраструктуры Яндекса общий для всех поэтому я не сильно буду про нее рассказывать она не специфична давайте заурса в двух словах Что она делает она умеет чинить машины и наливать их вот если совсем высокого уровня говорить причем чинить она их умеет двумя способами первый способ она может перегрузить машину специальный образ И там будут запускаться различные нагрузочные тестирования процессор память диски и если какой-то из тестов упадет то она сама напишет тикет дата-центр о том что надо исправить машинку будет отслеживать состояние этикета когда он будет закрыт она еще раз запустит тест и соответственно машинка рано или поздно вернется но такое такое починка она с точки зрения времени достаточно дорогая потому что что-то конкретное сломалось вам надо выполнить очень большой набор тестов и не факт что это один из первых поймает вашу неизвестную вашу проблему вот а у этой системы был второй способ починки машинок это более точечный вариант на каждом Хосте был Специальный агент который постоянно отправлял состояние различных проверок если что-то ломалось он сообщал об этом системе управления железом это соответственно запускала запускала починки конкретный неисправности Вот например Это да немножко промахнулся как мог выглядеть pipeline починки например перегрева сервера Агент фиксировал перегрев сообщал об этом системе управления железом то выключала машинку по соответственно создавала ticket в дата-центр сразу про перегрев инженеры шел чинить машинку hms периодически спрашивал состояние ticket когда он закрывался она соответственно возвращала машину в работу вот в чем проблема мы не могли использовать вот этот точечную систему починки Почему покажу на примере допустим у нас три дата ноды в разных доменах отказа и на них на всех одновременно загорелась проверка перегрева hms чинит машинки понятно что не параллельно а не последовательность а параллельно какими-то большими пачками Вот соответственно Вполне может Взять все эти треноды Выключите начать чинить а у нас там данные все три реплики на этих машинах и в общем-то происходит беда мы система хранения данных которые потеряли данные вот поэтому нам приходилось использовать hms скорее как систему мониторинга Мы смотрели какие проверки загорались на машинах потом руками выводили сервера из работы из кластеров запускали преднастройку которая искала все возможные проблемы вот после починки мы бы руками возвращали машины в кластер соответственно мы уже вшестером тратили очень много времени на обслуживание железа и даже думали о том что она может еще и 7 и 8 человек при этом битву железом мы проигрывали железо становилось все больше а нас в общем-то не сильно больше вот все больше наших машинок простаивала ожидая когда же у нас дойдут до них руки причем самое обидное то что в hms уже был функционал который позволял бы развязать эту проблему Единственное что надо было сделать это донести до hms знания сервисах которые работают на машинах до сервисов знания о том что с какими-то из их машин будут производиться работы вот такой такая система у нас называется кластер менеджмент System или cms ее может реализовать при себя каждый сервис и подключить к своим проектам внутри системы управления железом Как она может работать вот тот же самый pipoint Pro перегрев только теперь отличие hms первым делом идет cms и говорит Я хочу такую-то машинку забрать в ремонт отвечает Нет нет сейчас нет ни в коем случае нельзя и начинает потихонечку готовить эту машину к выводу из эксплуатации в какой-то момент она это доделывает переводит у себя заявку в состояние что я готова отдать машину hms в очередной раз приходят проверить как там дела видит что Да теперь машина можно чинить и дальше чинит до состояния исправности последний момент когда она ее дочинила Она опять идет cms со словами У нас заявка закрыта Забирай машину назад и соответственно сама cms возвращает ее в работу в принципе вот я сейчас рассказываю какие-то очень простые вещи но они дали грандиозный эффект на самом деле в какой-то момент мы собрались силами перестали заниматься рутиной и написали первую реализацию cms а это позволило нам во-первых перестать наращивать наш ОПС отдел людьми которые занимаются исключительно рутиной починки машин мы стали писать больше года заниматься там различные аналитикой мониторингами В общем развязались себе руки с другой стороны число машин которые мы не успевали бы починить резко сократилось Вот например смотрите это не очень хорошо видно Наверное да это Лог запросов заявок cms за последний месяц один из Больших наших мастеров вот тут видно что было почти 3000 заявок про блоки на дисках вот я не про одну эту заявку на самом деле ничего не знаю они все выполнились автоматически вот 3000 машинок за месяц подчинились без моего участия я просто знаю о порядке цифр Починок в месяц в качестве завершения это части хочу сказать следующее если у вас десяток машин да Или сотни машин то скорее всего в этом месте не нужна никакая автоматизация все и так вас хорошо ваш диопс инженеры они смогут и руками такое число машин обслуживать без проблем если у вас уже тысячи машин то наверное вам нужна уже какая-то система как минимум наливок А если вы Вы имеете парк машин в десятки тысяч то вам нужна Вот такая сквозная автоматизация от соответственно железа до сервисов иначе вы рискуете тем что часть машин будет постоянно простаивать потому что вы не будете успевать их чинить и ваш отдел который занимается Вот исключительно опцией починками Да он будет расти до бесконечности продолжить теперь другой кейс рутина и хаос ударили нас неожиданной стороны а именно со стороны мониторингов у нас в Яндексе есть достаточно много систем которые занимаются мониторингом есть событийным системы есть timeline Time Series который позволяют описать на Type серии данных для событийной для событийного мониторинга у нас был антибал плагин и в принципе он позволял нам конфигурировать все наши кластера разом А вот для Time Series система мониторинга К сожалению приходилось все накликивать в юа-интерфейсе и это было в принципе ничего пока у нас было там с десяток кластеров но когда их стало два десятка Потом три десятка Оказалось что в общем масштабировать эту историю очень сложно более того мы начали путаться в том где Какая версия мониторинга правильная вот а самое худшее иногда мы находили какой-то кейс на одном из кластеров правили мониторинг и в следующий раз какой-то там плохое событие Мы бы уже отловили на этом кластере но не успевали применить этот мониторинг на другом кластере он ломался причем ломался так обидно потому что мы уже знали как он мог так сломаться это с одной стороны с другой стороны у нас росла команда и мы стали обосабливаться то есть появилась группа которая занимается отдельным мастерами отдельно нодами там планировщиками людей стало больше не стало больше времени на написание различных мониторингов они стали Лучше понимать если они более специализировано стали заниматься каждый из этих подсистем соответственно стали больше писать мониторингов на специфичные кейсы но дело в том что система мониторинга знала 2-3 человека Остальные в общем-то не очень горели желанием самостоятельно это настраивать из-за достаточно высокого порога входа в настройки и получилось что вот эти два три человека стали басфактором управления мониторингами потому что к ним приходят Там 40 человек со словами сделаем не такой-то мониторинг сделали такой-то мониторинг при этом как правило это люди которые занимаются задачами плохо понимают как работают компоненты отдельные с которыми их просят для которых их просят написать мониторинг и получается что каждый мониторинг надо делать несколько итераций за испорченного телефона в какой-то момент мы стали понимать что мы проваливаемся в этом болото и с мониторингами надо что-то делать чтобы нам хотелось во-первых нам бы хотелось чтобы источником Правды о конфигурации мониторинга была какая-то единая точка из которой можно было легко бы настроить сразу все кластера это раз два нам бы хотелось упростить работу с мониторингами чтобы каждый из нашей команды не особо разбираясь в конкретной реализации той или иной системы мониторинга Бог написать какой-то высокоуровневое описание Я хочу чтобы мониторилась Примерно вот так вот понимая как бы эти требования Мы начали искать решение Сначала мы хотели попробовать написать свой правильный плагинцы был потому что тот который мы использовали до этого нам предоставила сама система мониторинга они его не очень поддерживали В общем и так приходилось править и почему бы его не исправить настолько чтобы написать самим Мы думали о тира форме но некоторые коллеги справедливо заметили что мы вот порог входа в мониторинг только ухудшим тем самым во-первых придется сдать как работают конкретные системы А еще и разбираться Как работает танцы был и его плагин и писать вот поэтому мы стали думать о другом подходе и О каком-то шаблонизаторе да потому что был незатор на самом деле поправить намного проще чем написать провайдер или что-то в нем подправить это совпало с тем что мы в команде начали активно применять и поэтому выбрали шаблонизатор из мира а именно а чем хорош кьюланг во-первых он написанного и нам хотелось использовать инструменты нога во-вторых в нем есть схема данных соответственно можно описать те данные которые он должен генерировать с некими правилами их ограничить и соответственно Если вы делаете что-то не то он сам подскажет что вы вот тут схему нарушили и Давайте лучше все-таки заново вот а с проблемой входа в знания как работают те или Насти системы мониторинга мы пришли к такому решению а Давайте попробуем описать наши мониторинги вот буквально русским языком Я хочу чтобы мониторинг из такой-то из такой-то системы работал только в рабочее время Ну и брал данные из не знаю там из другой системы Давайте покажу как на примере Вот например сверху черная квадратик это декоративное описание мониторинга он говорит я хочу создать мониторинг снапшот валидейшен данные о событиях и оборуду убрать из системы под названием один Я не хочу чтобы мне по этой проверки приходили звонки или СМС но я хочу чтобы по ней отправлялись события в telegram-чатек Ну он деск Чен Это я тут немножко схалтурил на самом деле надо было бы написать он description change Act какой-нибудь Ну типа срабатывай не только на изменение статуса Но и на изменение description то есть получилось такая схема У нас есть некая декларативное описание мониторинга которые мы в качестве входных данных Передаем в шаблонизатор а он уже генерирует конкретные данные конкретные системы мониторинга если посмотреть слева это вот как раз описание который сгенерится из верхней из верхнего описания Правда Конечно не со всеми мониторингами так получилось например ставим Series а все-таки из-за того что там надо написать и сам запрос и способ его обработки не совсем декоративное описание получилось но зато мы перевели все наши мониторинги в такой формат у нас было много примеров соответственно человек новый мог прийти посмотреть как выглядит тот или иной уже работающий мониторинг и просто скопировать его и поправить один из минусов подхода А дальше было написано специальный слой склейки который брал какой-то Джейсон сгенерированной конфигурации применял уже конкретно система мониторинга с помощью тех API которые они нам предоставляли до внедрения вот этой схемы мы часто шутили что нам нужен нужна группа применение мониторингов Спустя год после слишком после ее применения это так и осталось шуткой вся команда пользуется вот этим подходом Я как автор шаблонизатора очень редко соответственно слышу какие-то вопросы о нем Вот соответственно Этот уровень мы закрыли вот таким подходом и очень довольны мы получили то что наши мониторинге хранятся в репозитории в Едином месте из него можно сконфигурировать сразу все кластера ушла проблема с разными версиями разных кластеров Да и за счет такого декларативного описания мы смогли упростить всей команде работы с мониторингом последняя часть про которую хочу рассказать это масштабирование поддержки самом деле поддержка это та часть которая может по моему похоронить в рутине какие-то миллионы человека часов и мы столкнулись с этим же причем мы были очень наивны предполагая что нашими силами собственно нашей командой и удобным клип для управления кластеров мы решим все проблемы Давайте вам немножко расскажу как у нас устроена поддержка и с какими проблемами именно мы столкнулись Итак у нас есть один дежурный который в течение дня следит за мониторингами если что-то происходит плохое то он пишет анонсы связывает между собой команды такой работой занимается заодно он отвечает на пользовательский запросы в трекер специально очередь приходит вопросы или какие-то просьбы изменения и соответственно дежурное их применяет Когда у нас были небольшие кластера и не так много пользователей Это хорошо работало например Какие заявки могли приходить Первое это на систему доступа то есть мог прийти Человек со словами что я хочу теперь получить доступ к этому кластеру Вот мы смысле к этой к этим данным на этом кластеру Вот мой руководитель он подтверждает в тикете моего звали вот пожалуйста выполните команду мы выполняли одну клей команду прописывали новая цель для этого пользователя и он соответственно заявка выполнялась или приходил новый пользователь мы его добавляли в группу и соответственно тоже все было хорошо ситуация усугубилась когда мы поняли что мы стали дата лэйком для всего Яндекса и пользователи начали между собой общаться У кого какие данные есть и запрос стал превращаться в такой вот там есть какой-то каталог я не знаю кто у него владельцы но там очень интересные данные для моего подразделения Пожалуйста дайте мне доступ соответственно дежурный искал ответственного за эти данные звала в тикет пользователь с ним начинали там тикете общаться кто-то зачем тебе Вот она дежурный все это ждал ждал когда ему скажут надо применять команду или не надо а и в общем с десяток таких заявок в день могли сильно испортить Кому угодно настроение похожая была ситуация с управлениями дисковыми квотами То есть у каждого аккаунта У каждого сервиса есть своя дисковая квота там прописаны сколько он может использовать там дисков Сколько может создать объектов кластера Вот соответственно пользователи приходили запросами типа ну вы знаете мы тут не учли все что происходит происходило за последний год А может быть вы нам выдадите 100 ТБ До получки Ой простите до поставки Вот и соответственно дежурный писал нашему техническому менеджеру что Пожалуйста отметь вот этот вот Друг у нас запросил лишнее лишнюю квоту и выполнял задачу хуже было когда приходили два пользователя со словами вот Пожалуйста передай из одного сервиса квоту в другой и начиналась вся та же самая история типа А давайте между собой пообщаетесь там договоритесь что Сколько кому вот были более технические заявки из серии Вот мой превью с не работает Пожалуйста покажите мне СТД или там не знаю какой-то трейс как он упал А на каких данных он упал Вот а если корка ну и соответственно дежурный шел на сервера крепологии все это дело предоставлял Ну тоже В общем не самая веселое занятие Хотя дежурный с этим справлялся пока у нас было не так много пользователей Ну и последняя тоже из этой же серии Здравствуйте моя мама превью с операция сегодня выполняется в 10 раз дольше чем вчера что происходит Какой кошмар Ну дежурный шел систему нашего мониторинга Times Series до показывал графики говорит что дорогой пользователь вчера Ты запустил свою мать с операцию вашим вычислительном поле один вот и все ресурсы это вычислительного поля Пула были выданы тебе А сегодня ты Твои коллеги запустили 10 операций ну и соответственно в 10 раз она работает Извини дольше ничего тут не поделаешь или вот как-то вот или ресурсов больше просить или как-то договариваетесь внутри кто когда что запускает вот что объединяет все эти заявки что на самом деле наш дежурный тут был совершенно лишним Ему надо было сделать какое-то легкое легко автоматически автоматизированные действия и в какой-то момент мы поняли что дальше так жить нельзя что в общем-то пользователи должны делать свою работу сами А мы должны дать просто такие инструменты для этого для каждого из случаев у нас было свое решение для целей Мы решили подключить внешнюю систему управления доступами Вот она у нас называется иденти-менеджмент менеджер или Эдем А у неё очень простая система работы та система которая не подключается экспортирует к ней свою систему ролей на самом деле это просто название и набор полей который с этой ролью связан а также экспортирует так называемый workflow на самом деле это просто питон скрипт который можно прямо загрузить систему он определяет В какой последовательности должны подтверждаться Эти роли условно говоря когда человек запрашивает дата доступ каким-то данным какому-то пути из этого пути понимается что это за каталог кто за него отвечает соответственно вот самой Эдем появляется запрос о том что вот эти люди должны подтвердить эту заявку если они ее подтверждают через апек нам уже идет запрос о том что надо прописать эти цели более того Эта система позволяла достаточно гибкие настройки вот этого подтверждения сделать например подтверждает владелец данных и его руководитель или владелец данных или службы информационной безопасности то есть мы как бы подключились заодно расширили свою безопасность и возможности для пользователей например сейчас я ради интереса посмотрел Сколько в среднем выполняется заявок и DM за день на больших кластерах и обнаружил там в районе 500 различных действий на одном кластере Я представляю что было бы с дежурным которым свалилось 500 тикетов за день кажется нам пришлось дежурных нанимать много или психологов проблему с аккаунтами Мы решили тоже подключением внешней системы и тут я должен как это порекламировать коллег У нас есть внутри специальный сервис сервис каталог и вот когда я где-то читал Про Сервис каталоге или видел их реализацию некоторых компаний это выглядело скорее как не знаю некие Желтые страницы в которых было бы написано что есть такой сервис вот там контакты но иногда подключена Вики Да с какими-то Да с какими-то описаниями как он работает вот команда которая занимается внутренними сервисами сделала наоборот из сервис каталога нашего такого достаточно активная сущность и для нас там в первую очередь были интересны функционал управления квотами мы вынесли все управление квотами в этот сервис у каждого сервиса есть свои настройки Он может передавать данные свою квоту может передавать между сервисами друг другу она просто приходит сообщение о том что вот примени пожалуйста это изменение Ну и заодно сервисный каталог позволил нам расширить модель доступа и DM потому что вот первоначально когда мы первый раз ее подключили мы использовали в качестве групп чтобы не каждого человека отдельно подключать использовали структуру организации но это не очень гибкая модель потому что человек может быть какой-то группе в организации заниматься там 10 сервисами и те кто владеет этими сервисами не очень хотят чтобы все люди из его подразделения тоже имели туда доступ правильнее нарезать роли на основании собственно самих сервисов Вот соответственно Мы тоже получили такое расширение для пользователей которые нас спрашивали что где как упала мы просто добавили для каждой выполняемые жабы информацию о том с каким статус ходом она завершилась если с т.др его можно было прям сразу скачать входные данные которые в этом отправлялись он их тоже мог скачать и отладить если Джаба падала в корку то можно было бы прям в спеке Операции указать путь на кластере в которой бы сохранялись эти корки соответственно мы дали Все инструменты для такой отладки и еще добавили Shell в работающую джабу соответственно человек мог залогиниться и увидеть прям как оно работает получите стресс ГТБ вот все стандартные инструменты для отладки для ответа на вопросы Что У пользователя происходит Мы добавили графики во все места UI где только смогли например для операций человек может сразу видеть сколько операций его потребляет ресурсов а какая-то процент от пола который ему выделены Ну и что там в поле тоже происходит Как я рассказывал историю про пользователю которого в 10 раз медленнее Все работает Ну конечно только часть пользователей стала этими графиками пользоваться не всего нам пишут заявки что происходит но зато теперь дежурному достаточно просто открыть операцию у него все графики перед глазами то есть мы тоже ускорили свою работу за счет того что добавили информацию происходящем во все инструменты вот этот подход пользователь должен делать свою работу сам ему надо дать только такие инструменты Мне кажется он единственный возможный для больших мультитан от систем иначе вы будете опять же наращивать поддержку А мы вот например думали о первой линии поддержки но тем не менее Вот с применением вот этих вот всех внешних систем и этой автоматизации первую линию Мы так не сделали У нас все еще один дежурный Хотя с момента когда мы начали об этом думать наши кластера там стали вас пять больше пользователей тоже примерно столько же стало больше Вот теперь наверное в качестве заключения хочу сказать следующее все эти проблемы старались решить не заливая не заливая их людьми до которые бы занимались какой-то рутиной мы предпочитали нанимать инженеров или находить свои силы которые бы писали код автоматизирующий различные аспекты работы да и убирающие рутину но и сколько опыта мы знаем что любая автоматизация рано или поздно взрывается и поэтому начинаю проектировать какую-то новую систему но первым делом думаем Как она может взорваться и какой будет радиус поражения а поняв это начинаем вставлять различные стоп-краны в эти системы Вот например cms которая управляет отдачей кластера отдачи хостов есть несколько таких стоп кранов Первое это здоровье кластера там есть определенные метрики по которым она судит что во-первых у нас не потеряны данные и мы их точно не потеряем сейчас а если есть какие-то предпосылки она перестает в автоматическом режиме отдавать машинки хотя есть способ ручного подтверждения заявки в системе управления железом в принципе всего Яндекса есть тоже похожий стоп-кран если в нее поступают слишком много сигналов каких-то поломках она предпочитает отключить автоматику и позвать кого-то из людей которые за нее отвечают чтобы просто не ломать дров например мы предполагали что а вдруг она нам сообщит что надо Удалить все доступы поэтому мы делаем снапшоты состояния Если что мы можем откатиться На прошлой на прошлое состояние Вот это такая ложка дегтя в рассказе про автоматизацию Ну а теперь соответственно бочка меда выйти заурус не так давно вышел в Open Source и теперь вы можете попробовать эту систему самостоятельно вот здесь вот несколько qr-кодов я вот сегодня почти ничего не рассказывал технического Потому что если честно пробовал добавить доклад хоть что-то про масштабирование отказа устойчивость выйти заурса но как бы в тайминг я не вписывался никак поэтому здесь вот есть qr-код на конференции где такая информация есть на наш сайт и github Спасибо Спасибо большое кость это было интересно это было местами нестандартно с точки зрения реализации в том числе из-за того что вы в таком маленьком количестве рук сделали действительно много и в том числе предусмотрели вот эти точки отказа там где это было возможно это достойно восхищение тут есть вопросики но сначала давайте наверное по людям которые здесь у нас присутствуют а потом уже перейдем к вопросам из чатика поднимайте руки если у кого-то есть вопросики Константину Большое спасибо Константину за доклад вопрос Представьтесь пожалуйста меня Назар зовут непонятно чем создание новой системы ими cms помогло вам освободить рабочие руки А вот этой связи не понял вас был в какой-то у выходных задач потом появилась дополнительная система cms и вас потом появилось время и непонятно как это матчится то есть появилась новая система но при этом не больше покажу на примере Вот например загорается проверка Ну не знаю там памятина на сервере Да его нельзя просто выключить тут же выключить это потенциального вы потеряете данные поэтому инженер должен пойти в консоль сказать я эту машину вывожу из эксплуатации потом подождать пока она выйдет из эксплуатации переключиться еще раз на эту задачу через какое-то время потом пойти в систему управления железом сказать Ага машина готова я ее забанил в кластере чтобы если что она там не включилась посередине проверок каких-нибудь пойти в систему управления железом запустить преднастройку то есть запуск всех тестов неизвестно сколько они займут они могут тут же найти неисправность а могут работать там 8 часов или могут через день соответственно будет вернуться к этой машине опять инженеру Да убедиться что она вернулась и опять пойти в консоль и включить ее назад кластер Вот куча переключений причем при парке машин в десяток тысяч вас будут таких переключений в день очень много а тут вот этот Весь процесс за вас делает автоматика инженер не вписано не вмешивается вообще никак в этом соответственно плюс понял Большое спасибо больше коллеги Есть еще вопросы не стесняемся давайте все-таки диалог устроим и не монолог Ну кто-нибудь нет Да Добрый день Михаил Сбербанк А подскажите пожалуйста вот все-таки по доступом чуть поподробнее вот в этизаурус используется рейнджер или вот что-то подобное то есть с точки зрения доступа если например человек закажете какой-то доступ то например менеджер данных человек который как бы Ок нет что ему туда можно может ответить там через две недели условно то есть вот не будет ли за счет этого очень долгий Процесс получения доступа это правда конечно Такое возможно поэтому во-первых мы советуем всем пользователям добавлять как минимум два три подтверждающихся Один в отпуске вряд ли что-то подтвердит в течение или как происходит но это какая-то трекинг системе или где это происходит это на самом деле можно заказать специальную роль в том же идеме да в которой роль подтверждающих соответственно она в конце концов резолбится в набор Логинов которые потом будут использоваться Но у тебя наверное получается это идет что-то вроде кот апрувелларов да при условно мертв request в репозитории оно примерно также Да выглядит просто сюда еще Рыбак добавляется условный условно да То есть Вам нужен набор людей и определенный порядок то есть там может быть первый человек подтвердит одна группа должна подтвердить потом вторая и вот когда вот все группы подтвердили вот тогда через apium к нам уже спускается этот запрос А у вас какой-нибудь чатов или что-то встроено туда чтобы минимизировать время отклика по вот этой всей цепочке Потому что если много людей то как ты правильно заметил можно прям ждать Хорошо к сожалению вот здесь вот есть что еще дорабатывать вот еще вопрос еще один вопрос хотел спросить вы рассказывали когда запуск идет на кластерах это Но очевидно деф система какая-то да то есть ну продакшене уже скорее всего эти потоки все запускаются автоматом на эти вот у вас есть какая-то система которая потоки запускают по событию еще как-нибудь я не очень Простите не очень расслышал Я имею в виду вот смотрите То есть у вас запускает сама придется пользователей Да например 100 потоков день запускается Вот они запускаются руками вот здесь вот вашем кейсе то есть скорее всего это среда а Нет смотрите есть какой-то автоматизатор который эти потоки по событиям запускается смотрите у нас есть специальный тип пользователей роботные Да И вот пользователи для своих систем могут настроить запуск через роботов А как они это сделают но собственно это уже Автодом то есть конечно какие-то куча процессов они запускаются какими-то автоматами пользователю больше доната разница у вас условно там определенный слой А если пользователь уже хочет что-то сверху навесить нас лапки вы там в рамках квота и то есть пользователи могут запустить свои процессы как хотят Может быть у них какой-то крон или какой-то пайплайн на события Да но это уже внутри их систем которые вне нашего контура находится то есть общую какую-то не писали там там сетей какой Control там запуск потоков таких Казалось бы для странно такую систему писать потому что у нас очень разные пользователи с очень разные нагрузкой сложно было бы ее написать Скорее это все-таки пользователям Ну и здесь важно заметить что все-таки Константин больше про ядро системы говорил и все-таки абстракция пользователя поэтому и дается ему на отку Потому что слишком универсальное что-то сделать овчинка может виделки не стоить то есть они ребята закопают туда скорее всего много ресурсов отдаст ли это реальный Профит и учтет все нюансы и специфику различных систем пользовательские духи там Апачи Например я бы сказал что не было таких запросов от пользователей видимо их текущая ситуация все устраивает а Вы не думайте перспектива развивать эту историю вот продолжение вопроса коллеги условно с точки концепта платформы Когда у вас есть различные модули которые там fitches лагами или итогами включаются со стороны пользователя и таким образом в принципе вот то что коллега говорит частично покроет все такие запросы и большую часть времени пользователь будет длиннее будет тратить определенная работа в этом направлении есть но я не готов сейчас рассказывать спойлеров это будет следующий Спасибо за вопрос есть еще какие-то вопросики или мы перейдем потихонечку чат но и в том числе сходную зону у нас меньше минутки осталось Если кто-то хочет буквально один вопросик еще в эфире Ну что ж тогда я предлагаю перемещаться в дискуссионную зону потому что в чате Вопросов много Спасибо тебе большое Давайте поаплодируем"
}