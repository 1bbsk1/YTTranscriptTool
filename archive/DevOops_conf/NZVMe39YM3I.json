{
  "video_id": "NZVMe39YM3I",
  "channel": "DevOops_conf",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "Добрый день друзья а есть поговорка не можешь победить возглавь Я представляю своих коллег Максима Чудновского и Александра Козлова которые решили превратить проверки в код Ну что ж друзья Всем привет давайте начнём как это проверки как код сканируйте кокод хороший Каламбур с него и начнём Смотрите кто мы А мы с Сашей работаем в сбертехе занимаемся разработкой решение serv мы делаем Это Достаточно давно это зрелый продукт у него очень широкий AD То есть это сотни кластеров десятки тысяч клодо которые он обслуживает и дополнительно мы разрабатываем много чего полезного вокруг вокруг куне и сегодня как раз об одном из наших сайт проектов поговорим детально всё про нас можно найти на сайте gss заходите всех там буде будем ждать с удовольствием и Давайте начинать и поговорим с вами такое поняти как когда Ну становится понятно что мы говорим про какое-то управление контроле политике и применительно кластеру куберто какие политики могут быть но первое что приходит на ум это сразу вопрос с безопасностью Потому что когда мы диплом в кубернетес Мы хотим чтобы наше приложения там не использовали ресурсы хостой машины не использовали ресурсы хостой сети запускались с минимальными привилегиями и не под рутом И это всё можно проконтролировать заранее соответственно сюда же идёт история с сетью Мы хотим чтобы у нас были Network poc в кластере чтобы было грамотная вибили между например пейсами какое-то внутри кластерное нро Если есть serv Man то включён mutual tation ну такие вещи сюда же идёт в история со скели и топологией потому что чаще всего кластера у нас бывают неоднородные если например у вас есть кластер в котором есть узлы с ресурсами GPU которые полезны для клодо то логично чтобы на таких узлах скели только те приложения которые Их используют А все другие скидывали где-нибудь в другом месте для того чтобы ну ресурсы не утилизировали зазря и сюда же идёт история с configuration constraints здесь я имею в виду то что наверное мы бы не хотели чтобы кто-нибудь из псов или разработчиков пробрасывается виде чтобы он всё-таки использовал секреты или что-то более секюр Что предоставляет ему платформа и всё это нужно проверять И всё это нужно контролировать для того чтобы жизнь кубернетес была проста понятна и удобна дополнительно сюда идут разные платформенные расширения такие как Service smh какие-то хранилищ чувствительной информации э решения по аудиту по журналированием вот эти три решения объединяют то что чаще всего они делаются через паттерн Сай кар контейнеров соответственно этих Сайка контейнеров будет много темплейт их будет много версий их Будет ещё больше всё это нужно как-то меджи и лучше бы это контролировалась как-нибудь централизованно потому что иначе это превращается в конфигурационный ад Таким образом получается что есть много аспектов кластера кубернетес которые нужно контролировать и я хочу с вами обсудить Ну какие способы для этого бывают потому что выглядит так что функциональность полезна и первое решение которое я называю О так даваемое решение от команды В чём суть смотрите у нас есть команда разработчиков они Как ни странно разрабатывают какое-то приложение пишут код и в принципе им по силам написать манифесты развёртывания под кубернетес соответственно они могут сделать какой-то шаблон сделать прокиды параметров и туда упаковать все правила все Сайка контейнеры все политики сразу рядышком со своим приложением и в таком виде дется в cuber соответственно много команд каждая из них пишет эти конфигурации Все живут счастливо это такое Я бы сказал менеджерские решение Давайте внедрив процесс написания конфигурации внедряем как процесс есть Никто не счастлив потому что нужно много конфигов писать а в итоге это вырождается в то что пишутся темплейты эти темплейты когда мы добавляем туда какие-то стандартные вещи типа Network polic или Сай каров становятся очень большими А дальше это всё переходит на стадию эксплуатации мы заполняем val потом ещ и ещё в общем я думаю идея понятна что это всё превращается в я ад и разработчики вместо Как это там Java разработчиков или го разработчиков становится ямо разработчиками никто от этого не хэпи получается что это решение оно максимально простое потому что мы просто говорим людям Живите так и они начинают это делать и нам ничего не нужно модифицировать но одновременно с этим оно несёт ряд минусов во-первых в командах всех нужно обучить кубер чтобы каждый разработчик помимо своей технологии понимал что такое кубернетес Какие подвижные штуки в нём есть как там Что крутится поворачивается чтобы уметь это грамотно настраивать вместе со своим приложением приложения начнут друг другу мешать потому что разработчики делают политики Независимо и в этот же момент платформенная команда Она же всё равно будет обновлять кубернетес добавлять какие-то новые фишки туда и это приведёт к тому что меняются системные API на кластере и всем командам нужно это дело поддерживать соответственно они постоянно будут обновлять приложение чтобы догонять релизы это не очень удобно поэтому есть второе решение которое я бы назвал такое рсе соответственно для того чтобы объяснить это решение на сцену выйдет Мистер дженкинс который вы все знаете и суть здесь в чём если инженер настраивает кове то он может в этом конвейере имплементировать всю логику goverment имплементировать туда дефолтные политики какие-то модули переиспользование и будет это вот так выглядеть соответственно есть у нас какой-то пайплайн он берёт Исходный код какие-то базовые манифесты рендерит из них какие-то композитные манифесты потом накатывает туда политики накатывает туда какую-то валидацию и всё это как-то работает в этом релизом конвейере а если не любите дженкинс пожалуйста можно взять что-нибудь более подходящее под кубер в нотации gitops суть от этого не меняется Всё равно в релиз конвейере у нас будут Ну некоторые проблемы с которыми будет сложно жить в итоге получается что это решение не требует вовлечения команд потому что всё делает devs иннер в релиз конвейере CCD Однако этот релизный конвейер становится монструозный и его нужно постоянно поддерживать развивать догонять платформу вместе с релизами PL Team и всё равно он будет не до конца полным Потому что кубес так устроен что очень часто а результирующий манифест например спецификация пода которая будет запускаться в кластере будет известна только в кластере потому что её будет потому что её создаст какой-нибудь контроллер который там работает и мы на CCD всё равно прямо все политики откатать не сможем и не успеем поэтому есть ещё одно решение платформенной А я в целом платформенный разработчик это решение моё любимое потому что я делаю платформы их Надо как-то продавать вот платформенные решение смотрите самое классное В чём суть если платформа предоставляет Нам очень много возможностей для того чтобы разворачивать наше приложение управлять его скели гомбо безопасностью то пусть она как-нибудь сама отрут моменты когда нам нужно контролировать в политики безопасности и сделать так чтобы разработчики думали просто про код а платформа думала про всё остальное если кто-то из вас сейчас подумал что запахло вебхуками то в целом так и есть потому что когда мы применяем какой-то манифест в кластер кубернетес делаем п apply - F то в работу вступают сначала такие сущности как admission контролеры которые выполняют некоторую предобработка ресурса прежде чем он сохранится а в кластерный репозиторий и будет уже обработан соответствующим контроллером Ну и там запустится нужная логика соответственно с эти среди этих admission контроллеров есть два mutation и validation несложно догадаться что mation позволяет изменить исходный ресурс а validation позволяет его ну как-то проверить на соответствие каким-то практикам и через технологию веков можно построить достаточно контролируемую систему когда сам кластер кубернетес будет проверять всё что к нему приходит в автоматическом режиме если мы хотим быть добрыми полицейскими мы будем мутировать и не позволять разработчику ошибаться если мы хотим быть пожёстче то мы возьмём validation admission будем жёстко от говорить ему дорогой друг у тебя здесь запуск приложения под рутом в нашем кластере таких гостей не любят Пожалуйста иди переделай свой манифест развёртывания и получается что вот это решение построено на нативных технологиях кубернетес оно действительно неплохое потому что оно не требует участия команд не нужно переделывать исходные манифесты не нужно даже особо думать в этих манифестах потому что они всё равно преобразуются в целевые в момент деплоя не нужно ничего делать на Ар И тем более на не модифицировать релизный конвейер Потому что опять-таки кластер сам разберется как ему нужно быть безопасным как он это сделает владельца кластера который отвечает за его целостность и соответствие практикам которые приняты в организации будет централизован репозиторий там у него описаны политики он собственно этот кластер контролирует один раз надписал согласовал и спит спокойно сложно модифицировать нужны расширенные права если модифицировать не очень хорошо и не очень понимая как работают вебхуки то кластер может сломаться можно полностью остановить пло и так далее и соответственно конфигурировать всю эту историю на горячую при изменении политик через нативные механизмы кубернетес не очень хорошо это большая проблема если оставлять это так как есть и мы в компании постарались решить эту проблему и придумали такой и появился такой сайт проект он сначала был сайт проект он теперь уже самостоятельный который мы назвали латы когда я говорю про латы то мне задают два вопроса первое что это такое и второе Почему вы это так назвали начну с конца А у проекта есть слоган он звучит как Enjoy cuber каты соответственно идея самого проекта в том чтобы вы пользовались кубернетес не разбираясь в том что там внутри чтобы вам не нужно было погружаться в смысл деплой в configuration писать какие-то Network polic кластер разберётся сам потому что платформа достаточно умная и чтобы реализовать этот паттерн нам нужно решение класса policy Engine соответственно это решение которое позволяет централизованно управлять всеми кластерны аспектами А через набор необходимых политик А что такое policy Engine смотрите известная схема про вебхуки которую мы с вами разобрали Если мы переходим от нативных механизмов куберто policy Engine получаем Вот такую картину соответственно вместо мешанина вебхуков у нас получается некоторый оператор который как-то конфигурируется и сам собой представляет просто настраиваемый webhook сервер соответственно мы можем теперь получать все амины реквесты на мутацию на валидацию и обрабатывать их по тем правилам которые гибко настраиваются через API этого решения то есть через Custom Resource definitions через crd это позволяет нормально решить вопрос с анонсирование разрулить как-то по зонам выстроить иерархию конфигурации накатить туда ролевую модель в общем добавить полноценное управление политиками и вот когда я обычно рассказываю Ну вот смотрите ребята Cu БТА это policy Engine мне задают второй вопрос А почему вы начали это делать почему вы ну не посмотрели на какой-то готовый он Source который уже принят в коммьюнити и в принципе это очень хороший вопрос а потому что а действительно такие решения есть и самый главный момент здесь в том что нам не хватало функциональности потому что на момент когда мы это делали уже были люди которые используются пользуются пером который частично закрывает требования частично нет соответственно их нужно как-то онбординг в общем функциональности не хватало это привело к тому что пришлось строить своё собственное решение и решать проблему с вот этим вот Ну понятно что Всё дело нужно зарегистрировать во всех реестрах получить всевозможные сертификации самое главное это будет Вот твой код которым ты максимально владеешь Это очень хорошо и здесь есть момент вот часто на конференциях говорят смотрите ребята вот проблема там хаотичный кластер куберто Вот её решение policy Engine вот мы в компании сделали вот так-то если вы хотите это повторить Пройдите весь наш путь или Погугли что есть в интернете осорно Мы решили так не делать Мы решили поделиться проектом с Community соответственно проект ты доступен в Open Source на Отечественной площадке gww это площадка с для хостинга гит репозиториев там есть репозиторий ты он вам доступен вот здесь Кстати на скриншот попал в комит Александра который здесь он что-то там хорошее делал так проект живой развивается или нет Это очень важно потому что вы можете Юда зайти поделиться какими-то идеями по его развитию или поделиться этим в форме порек что максимально приветствует ссылочка кликабельная в презентации материалах конференции она у вас будет И теперь Раз уж это осный инструмент то давайте посмотрим как им пользоваться и какой результат можно получить исходя из того что он предоставляет соответственно среди его API есть ресурс который называется и Вы не поверите это соответственно описывает всю структуру образ результата который мы хотим получить То есть это плоский файл который описывает целевую спецификацию пода configuration Map или Network policy которая должна быть либо получиться в результате мутации а либо получиться в результате генерации э ну в общем каким-то образом получиться если плоской структуры не хватает то понятно поддерживается шаблонизатор писать любого уровня шаблоны наверное примерно Похоже на то как вы привыкли к этому Когда делаете Ну чарты либо какие-то другие похожие вещи также есть ресурс Триггер Триггер определяет точку мутации То есть когда она должна сработать приходит ресурс например деплоймент Мы хотим добавить ему секюрити контекст Поэтому в триггере будет написано пожалуйста когда пришёл деплоймент Запусти мутацию Запусти её с вот таким-то шаблоном будет ссылочка на шаблон и соответственно будет стратегия применения этой мута понят из ная это знат что поми мутации запустится какой-то фоновый процесс который может сделать что-то полезное например не просто мутировать исходный ресурс а сгенерировать пачку подходящих хороший пример генерация конф Но об этом Саша вам сегодня подробнее расскажет есть ещ один спецификацию в сколько шесть строчек и она почти всегда такая вот это всегда ямлихан ассоциирован с каким-то шаблоном и через этот шаблон будут сгенерирован будет очень полезен в качестве якоря для встроенной в куберто уборки мура потому что там простра что в принципе хорошая практика соответственно при удалении автоматически прир и удалит все сгенерирован для это мутация для того чтобы что-то изменить есть валидация для того чтобы что-то проваливать проверить и дать результат соответственно здесь мы определяем точку начала валидации мы говорим если пришёл деплоймент пожалуйста проверь что у него правильно заполнен sec контекст или правильно заполнены рек лимиты чтобы ну обеспечить нужный для пода здесь можно писать правила можно использовать можно их писать на языке рега Если вы любите его любите Если нет то есть встроенный DSL который позволяет это сделать Ну несколько попроще и таким образом получается чтоб покрывает очень множество кейсов и Ну есть прям множество решений которые можно через него реализовать и самая большая проблема в этом множестве как раз-таки в том что оно очень большое и мы попытались немного структурировать и поговорить о конкретных практических кейсах где будет полезен будет полезно решение polic policy Engine и соответственно на примере лата это сейчас расскажем я передаю слово Саше и он сейчас с вами поделится этой информацией Да спасибо Максим большое за такой подробный экскурс в то как всё это работает устроено на самом деле мы не зря вам рассказали про эти три решения и соответственно из чего в принципе кубер состоит И на что хотелось бы обратить внимание потому что мы на самом деле всё это дело проходили чно опыте Вот и набрали достаточно большое количество всевозможных ситуаций здесь мы про все конечно не расскажем но расскажем про Некоторые из них за которыми Было бы неплохо следить Ну первое начнём мы по нашим стопам про которые Максим уже рассказывал И начнём мы с нашей любимой безопасности здесь на самом деле просто клади всего самого возможного всё что может происходить на вашем кластере и компрометировать можно со всех сторон И начнём наверное с самой частой истории это запуск привилегированного пода а Делается это очень легко и просто почему это часто делается потому что чаще всего копируют где-нибудь из каких-нибудь общественных источников спецификацию деплоймент подов и тому подобное и там часто есть Security контекст который запускает наши контейнеры привилегированными А в чём собственно Ну вот эта вот строчка которая делает нам злом если мы это де не осознанно а делаем просто так мы даём инструмент злоумышленнику получить доступ к процессу на нашей ноде под повышенными павами то есть грубо говоря мы можем уже из-под этого процесса получить физический доступ и к сети и может быть если не дай Бог на этой ноде где-нибудь кусочек кластера dcd заложен то мы ещё и в него можем залезть в etcd вообще всё в открытую хранится и соответственно скомпрометировать вообще всё что только можно поэтому так делать не надо и соответственно что нам нужно для этого сделать это соответственно написать правила которые говорят о том что Не загружайте пожалуйста деплоймент коды и тому подобное в кластер которые запускаются с повышенными правами конечно В некоторых случаях это требуется и можно настроить исключение Но это прямо нужно делать осознанно нужно понимать зачем мы это несём а во всех остальных случаях мы просто блокируем создание таких подов собственно и не допускаем лишних уязвимостей на кластере А как мы жили до появления policy Engine Ну на самом деле Всё достаточно сложно было у нас было да и до сих пор есть куча разных сканеров которые смотрят что Какие ресурсы есть на кластерах э там что и как запущено потом приходят всякие страшные письма командам о том что так нельзя делать и тому подобное Вот и собственно всё это дело точечно Понятное дело что в большой распределённой системе это ВС очень сложно контролировать это практически невозможно ну и соответственно как бы так как опять же многие делали это методом копипаста то это плодись и размножать кластерах очень очень даже быстро с кулата собственно Всё пошло гораздо проще У нас появился набор правил вот в котором включено как ки контро подов под привилегиями вот И теперь мы можем и генерить исключение Когда нам действительно это необходимо и контролировать С какими привилегиями был запущен под и соответственно в каких-то общих случаях вообще вырубать эту возможность всё стало достаточно просто и соответственно главный управляемым дальше мы посмотрим что мы с сетями сделали Здесь тоже очень интересная история потому что проектов у нас в кластерах плодится много вот часто бывают формируются разные зоны сраз уровнем доверия и необходимо делать сетевую видимость С тех или иных проектов чтобы соответственно сервисы друг с другом коммуницировать опять же как бы если это отдавать на откуп разработчикам то нередко Бывает так что либо срабатывают человеческие факторы открывается слишком много либо срабатывают человеческие факторы и не открывается вообще ничего и к их сервису получить доступ нельзя Вот Но в любом случае как бы всегда что-то получается ИТ не так вот нужно соответственно делать Вот такие ресурсы вот при создании проектов где нужно будет настраивать входящий входящий исходящий трафик соответственно какие-то разрешения делать и тому подобное если ты ещё и не знаешь как у тебя кластер устроен и где у тебя в кластере какие-нибудь там средства мониторинга например расположены то соответственно и вки за которы за это ответственный Ну в общем куча времени на это всё тратить вот и этих соответственно э так сказать ресурсов Network policy нужно будет делать не одну штуку их ча чаще всего под разные сервисы э сделано там много вот которые разрешают доступ к определённым точкам в кластере э обо всех этих точках надо каким-то образом знать Вот когда у вас вашим кластером пользуются несколько сотен команд Ну как бы Попробуйте рассказать им всем Вот но на самом деле ВС это можно сделать опять же проще вот можно сделать темплейты для кубиты вот в котором описать базовые настройки NW polic посадить их на определённые метки которые требуются Вы можете на своём деплой указать что мне нужен такой такой и такой сервис и вам всё равно будет где эти сервисы расположены потому что собственно остальное остальные конфиги прине вам с помощью темпле вы описываете с помощью триггера вы описываете метки которые ждт лата на вашем ресурсе для того чтобы создать необходимые конфигурации Собственно как это было до этого ну как я уже рассказывал это всё происходило вручную каждая команда которая заказывала проект несла с собой эти ресурсы и соответственно нередко бывали случаи когда организ появлялась куча саппортов потому что не работают и приходилось разбираться почему же всё на самом деле было просто просто не было организовано сетевой видимости Слата же всё это дело автоматизировал Вот ребята теперь не занимаются доставкой сетевых политик они просто знают что им они взаимодействуют с этими сервисами это собственно прикладная уже история и с помощью меток подключают всё что необходимо дальше посмотрим что у нас со слингом из топологии Тут тоже такая история интересная с связанная с часто с уязвимостями вот Ну на самом деле там есть несколько вариантов есть иногда потребность располагать поды какие-то критичные на определённых нотах которые специально для этого были созданы у них есть определённые политики определённые ограничения контроль и тому подобное и вот такой сервис можно поднимать только там вот а иногда соответственно в более общих случаях мы например хотим контролировать использование хостой сети потому что хостой сеть и подключение соответственно хост Network в вашем подике напрямую позволяет вашему поду подключиться к сети к физической сети на ноде А это значит Соответственно что с помощью вот этой вот строчки Вы можете засни всё что происходит на ноде весь трафик который в кубе идёт который в Куба идёт который в etcd идёт и тому подобное а так как большая часть его не зашифрована то опять же можете скомпрометировать вообще всё что угодно на на вашем кластере Короче говоря так лучше делать не надо Поэтому такие вещи мы опять же тоже Валиди говорим что э запускать такие поды не надо но опять же можно сделать исключение в том случае когда это действительно необходимо и у вас там есть например какой-то очень низкоуровневый шлюз Вот который должен контролировать весь трафик на ноде и соответственно это надо делать Прямо действительно осознано опять же та же история что часто ребята делают просто приходят копипаст то есть где-то нашли спеку которая плюс-минус удовлетворяет потребности Мы вроде бы создали деплоймент создали подик и всё запустило и на этом успокоились вот поэтому эти флажки всплывали в самых неожиданных местах вот Ну и конечно же не было стандартно в кубе не было никакой возможности это дело контролировать конечно р чему-то начинает учиться в этом плане но всё ещё достаточно так сказать не зрело Слата опять же появился контроль исключений то есть мы для наших гетов смогли настроить политики которые разрешают это использовать Ну а во всех остальных случаях мы просто запретили использование этого флажок на подах Вот и очень кстати любимая История это монтирование так сказать и использование всяких разных конфиг маповская фишка как бы при монтировании при монтирование секрет у нас распаковывается и в открытом виде попадает в переменное окружение Ну соответственно Любой кто может получить доступ к непосредственно нашему контейнеру сможет получить кре вот делается это собственно таким образом я думаю очень многие видели данную конструкцию в спека часто где-то часто очень её можно увидеть вот практически на всех ресурсах которые вам помогают что-либо настроить в кубе Ну вот вот так вот делать очень опасно и желательно вообще не делать я потом попозже схо просто запретим использование секрет рефов вот опять же у нас есть валидация правило вот в чём вообще смысл как раз-таки всего вот описания этих кейсов к тому что вы с помощью валидации можете в точечно очень настраивать Законность нахождения ресурсов которые у вас в кластере Ну вот и соответственно Если вы понимаете Какие аспекты вам нужны лучше оставить только их а запретить всё остальное Потому что это не раз спасёт вашу сказать ва ваши нервные клетки и время ночное А собственно опять же что мы делали когда у нас не было полиси менеджера А ну когда самого начала опять же история с агентами которые скани наш кластера Вот это очень сложная история была потому что размещение этих кластеров было в разных сегментах соответственно нужно было каким-то образом агентам давать туда доступ сама инфраструктура этих кластеров была разная вот всё что на них было запущено тоже было разное и в результате короче сопровождать это было ещё сложнее чем в принципе ту пользу которая это дело всё приносило вот ну и соответственно была вероятность компрометации когда кто-то получает доступ к контейнеру вот ВС можно было соответственно получить пря виде вот сейчас же соответственно у нас запрещено прямое монтирование вот мы снизили комплементации живём в чуть-чуть в более безопасном мире вот а давайте теперь посмотрим на более так сказать сложные вещи которые умеет делать наш полиси менеджер и здесь уже на самом деле история такая более комплексного характера которые но тоже очень часто встречается вот у нас большой большие кластера большие сегменты и часто всякие разные внешние сервисы вынесены либо вообще в отдельные там на отдельной виртуалке либо в отдельные зоны в кластере и каким-то образом нужно нашему прикладному проекту указать доступ там к кафке к панголин к Погаре к ресу к ещё к чему-нибудь вот которые находится не в нашем проекте так как у нас везде используется сервис что по умолчанию если маршрута не построены то доступ запрещён и команде каким-то образом нужно было например узнавать Где находятся все брокеры кластера кавка Вот для этого там куча всяких разных нужно было сделать сервисов если мы по-простому общаемся С какой там наплевали мы на безопасность нам это не нужно мы вообще хотим просто что-нибудь туда складывать то например сес entry для нам достаточно но чаще всего это не так и мы хотим работать с сертификатами Мы хотим безопасный Коннект и тому подобное поэтому у нас этих правил становится сильно больше вот а чаще всего для каждого брокера там чуть ли не свою пачку нужно генерить вот и в результате этих конфигураций очень много становится а брокеры - это такая история которая чаще всего динамическая Вот и на момент деплоя вашего сервиса Может быть одна конфигурация Через час будет другая конфигурация и каким образом доносить дополнительные ресурс для того чтобы по взаимодействовать с новыми брокерами Вот а если не дай Бог как бы кавка решит перебан сирова ваш вас на новый брокер который подключился А у вас доступа и маршрута нету И всё сервис как бы нагибается из-за этого Соответственно что мы сейчас делаем мы берём темплейты и в эти темплейты засовывая все конфигурации которые нам необходимы для того чтобы построить маршрут естественно это уже делается разово Вот это уже делается человеком который умеет настраивать Стью знает где находятся брокеры В общем это там админ сопровождение подобное который знает что собственно сюда писать вот потом делает Триггер Вот который говорит что да Я вот ожидаю сейчас например который говорит что я хочу там подключиться к аке и автоматически все маршруты строятся Вот и теперь как бы кулата всё необходимое принесла вот ваш сервис может обращаться в кафку вы не знаете где эта кавка находится вам пофигу что там как какая топология вообще всего какие там динамические конфигурации и тому подобное ВС это автоматически принес уже из самого кластера что у нас было до этого на самом деле мы до сих пор как бы сталкиваемся с этой историей что каждую команду которая выходит в наш кластер нам приходится обучать конфигурации и как его настраивать как вообще с ним работать и тому подобное это неотъемлемая сть уже всех наших ровном как у нас было очень много людей очень много команд то это было очень сложно количество поддержки было просто неописуемый вот опять же нужно знать топологию нужно уметь опять же работать с сертификатами их выпускать и тому подобное это всё вручную раньше выполнялось И это всё достаточно болезненно и долго сейчас опять же вы не знаете что такой конфиг есть вам это не надо вы их не настраиваете вам не надо знать топологию стенда потому что вы вы за неё не отвечаете вам не нужно ручного админа иметь которому постоянно писать где что у нас расположено и дай мне все Аники вот Ну и конечно же уже секюр безопасность этих соединений она тоже автоматически прилетает как приятная опция вне зависимости от того например что вам может быть вы какие-нибудь ток телеметрию какую-нибудь в каку кладёте и шифровать её необязательно Конечно вы можете и без этого если вопрос производительности стоит но по умолчанию всё секюр вот дальше как раз-таки история с подключением уже секретов А это у нас через вольты сейчас происходит очень нужная и полезная история но тоже очень не просто настраивается во-первых это огромная пачка аннотаций которую Вам нужно будет указать в деплоймент который настроит Агент Вольта скажет ему откуда брать секреты куда их монтировать что с ними делать и тому подобное это всё соответственно даже выглядит страшно Это причём не вся пачка это я там только кусо скопировал выглядит это там ну Сток стро наверное и соответственно помимо всего этого нужно будет ещё и конфиги ию сделать для того чтобы подключиться к вольту вот опять же та же самая история топологии где он расположен как к нему подключиться и тому подобное всё это нам надо было знать что мы сейчас делаем мы сейчас соответственно можем сделать ресурсы для подключения это маршрута ИО и соответственно мутационный ресурс который принесёт те самые аннотации нашего Вольта в деплоймент то есть опять же мы просто вешаем аннотацию на наш код о том что нам нужно с интегрироваться С Вольтом всё остальное нам прилетает из кластера количество конечно ручных действий здесь сокращается просто в разы Ну для того чтобы всё это дело заработало нам нужно описать мето на которые мы будем смотреть с помощью триггера вот триггера для создания и триггера для мутации в принципе это может быть вообще в целом одна метка могут быть разные метки в зависимости уже от того где-то нужно маршруты строить где-то не нужно маршруты строить и тому подобное В общем настройка Вольта сама по себе непростая история обучить ей каждым каждую команду тоже очень тяжело очень дорого и очень сложно вот очень много аннотаций очень много кода Нужно настраивать и конфигурировать соответственно параметров тоже очень много легко ошибиться вообще человеческий фактор здесь постоянно торчит Ну и конечно же опять же топология это самая с кулата всё получилось гораздо проще у нас опять же заработала вся автоматизация с генерации конфигов вот построились маршруты прилетели аннотации всё замутила с а самое главное что мы теперь можем менять а нашего Вольта э и соответственно особо не говорить об этом никому потому что как только приложение приклада задеплоить новая конфигурация с новым А и команде не нужно будет об этом вообще Париться Вот теперь история опять же с мониторингом тоже очень хорошая полезная история Когда у нас сервисов много вот у нас получается так что чаще всего в кластере приложение не одно за деплоя у нас какая-нибудь микросервисная архитектура где сервисов там целая стая вот всех их нужно подключить к мониторингу у нас распределённая система у нас много пользователей ВС это за трафиком надо следить мало ли что-то идт не так и тому подобное естественно так сказать нужно чтобы кто-то это сделал и хочется это не делать вручную потому что я уже рассказывал что нужно знать где расположены все точки подключения там маршруты построить и тому подобное вот если мы просто попытаемся туда обратиться то ничего не получится вот ну и опять к чему мы приходим мы приходим к автоматизации всей этой истории У нас есть специальный Сайка который умеет читать логи который там может там собрать определённую структуру в универсальном формате из ваших логов там и положить их куда-то там Соответственно в этом темплейт мы описываем создание конфиг мапы этого сайт Кара где есть определённые настройки дальше мы описываем шаблон Как именно уже этот контейнер выглядит вот что какие воу примонтировать как их принти подобно в какой-то момент просто у любого пода появляется сло агентом который снимает всё необходимое с этого контейнера и уже отправляет ресурсы куда нам надо Вот соответственно мы описали Триггер и мето по которым всё это дело подключать и теперь выглядит вся эта история Таким образом у нас трафик заработал по мап по необходимости они например там в сервис мыше построены и в принципе команда тоже уже ничего дополнительного не делает она не знает где там эти сервера настроены где там не знаю какая-нибудь Виктория расположена где там может быть или кавка для сбора телеметрии или ещё что-нибудь всё просто улетает автоматически команда об этом не парится какие мы болячки здесь поле опять же конечно вопрос с топологии здесь наверно даже ещ больше вставали потому что система мониторинга - это очень живой организм который видоизменяется очень быстро и сильно в зависимости от размера вашего кластера и поэтому всё время требовало держать актуальную конфигурацию всего этого дела Вот вручную всё это всё синхронизировать эти конфиги строить маршруты наши админы просто вешали от этого Поэтому вручную с этим работать лучше не надо вот ну всё это дело удалось автоматизировать Получи мы подключаем все наши приложения по мето вот всё в принципе работает а главное что мы можем ещё и единообразно настроить передачу логов вот ну и соответственно последний вариант - Это у нас ещё и куча всяких разных операторов в кластере например тот же самый Rate limit и раньше команде необходимо было знать опиш этого й лимите вот знать его версию если соответственно там она изменилась её актуализировать вот а в разных стендах ещё и разные значения возможно в этих опиш и настройках были поэтому опять же мы всё это дело перенесли на наши шаблоны Вот потому что соответственно команда теперь не знает вообще как там подключать Рей лимите что в этом кластере такой лимитер в этом кластере такой просто всё это дело подключается через мето вот И теперь у нас всё необходимое прилетает уже из нашего любимого кулата вот без кулата нужно было знать непосредственно вообще набор всех возможностей которые есть в кластере есть там вообще этот лимитер или нет Вот и соответственно следить за актуальностью ошки А с автоматизацией уже всё стало гораздо проще Вот потому что скрыта за нашими мы просмотрели это конечно Просто краткая выдержка из всего того с чем мы столкнулись Но на самом деле вещей закрыто гораздо больше вот подключайтесь к тверсу подключайтесь соо ответственно к развитию всей этой истории Добавляйте свои собственные кейсы Я думаю это собственно полезная вещь будет не только нам но и вообще всему сообществу потому что управлять политиками нужно и важно вот ну и краткие итоги соответственно как я уже сказал управлять политиками это очень важно потому что мы живём очень так сказать в активно развивающемся микросервисной мире этих конфигураций становится очень много сервисов становится очень много и с ними надо соответственно что-то делать их как-то валидировать как-то это нужно автоматизировать вручено это уже просто невозможно Ну и policy Engine - Это хороший паттерн который подходит для решения этих задач есть не только наш есть Конечно есть Вот Но в нашей стране разрешено использовать только наш пока что так что Пользуйтесь Спасибо большое верни назад коллеги я напоминаю про ссылку по которой можно писать вопросы в чат Да и прежде чем мы отдадим вопросы в зал один вопрос из чата а когда я вижу какое-то красивое решение мне всегда интересно А сколько оно ест поэтому первый вопрос который был в чате Максим Сколько потребляет ресурсов решения И что будет с решением если найм спейса в кластере будет 1.000 это прекрасный вопрос А смотрите все policy Engine почти все а а именно наш Вот это Stat L веб-приложение по факту оно обрабатывает http запросы которые Ну это обычный Джейсон который называется admission request соответственно Ну понятно что это приложение потребляет минимальное количество ресурсов оно в принципе нагрузка слабо коррелирует с количеством объектов в кластере потому что ну оно производительно более того Так как это й оно хорошо горизонтально масштабируется средствами самого кубернетес поэтому такой проблемы в принципе не стоит вот это большой плюс Потому что есть альтернативный подход к реализации управление политиками которые Ну так называемые кэширует текущего ресурса кэширует себя состояние кластера чтобы проводить ретроспективный анализ и вот если в таком режиме вы что-то запускаете то там будут у вас гигабайты или десятки гигабайт по памяти и очень большая нагрузка в общем поэтому многие не любят тот же пер мы сознательно от этого отказались поэтому по ресурсам очень требования спасибо всё спасибо всем большое спасибо большое"
}