{
  "video_id": "nBWRIlnszmY",
  "channel": "DevOops_conf",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "всем привет меня зовут артем семенов я руковожу направлением клауд инжиниринг компания лан технолоджи который производит медицинское оборудование и сегодня я являюсь экспертом на докладе баллы селиванова из вики клауд solution пропала я могу сказать что это человек который видел кубер но тут еще пять лет назад и пользовался им паша до коллеги всем привет всех приветствую собственно рад что вы подключились к моему докладу если вы к нему подключились а если не подключились очень напрасно спасибо доклад на сегодня действительно необычный наверное все знают фразу это одним выстрелом убить двух зайцев на самом деле эта фраза присутствует во многих культурах многих языках по разному в английском это килты bros вызван stone но смысл тот же самый когда мы пытаемся получить два каких-то хороших результатов делов одно действие обычно в жизни так не бывает но сегодня мы поговорим про случай когда так как раз можно сделать павел расскажет про то как можно сделать ваши кубинского стира более надежными и одновременно ещё имеешь платит за них денег в за облачным в структуру которых поддерживает так бывает и давайте послушаем как же этого достичь да давай сразу поедем презентацию собственно коллеги на самом деле действительно я собираюсь вам рассказать о том как повысить отказоустойчивость класть ираку вернитесь и как снизить расходы на этот самый кластер кубер метиса но так получилось что при подготовке этого доклада я еще заодно зацепил другие темы в общем у мне надоело знаете что я решил вам сегодня выдать все тайны облачных провайдеров сегодня вам вот точно расскажу в докладе что все облачные провайдеры вам всем пруд и на самом деле у них все работает совершенно по-другому и совершенно не так как они вам говорят давайте для начала немножко правда подробнее расскажу о себе как меня представил артем на всякий случай еще раз меня зовут павел селиванов я developer advocate в компании извиняюсь в как low solutions это называется если говорить про мою под кубинцем знаете я им пользуюсь правда давно года так эдак с 2016 я вот тут пытался вспомнить при подготовке к докладу и да наверное первый раз ко мне пришли разработчики сказали что мы вот там такую штуку развернули увидели вообще давайте на него посмотрим внимательней в конце 2012 года это был кубер найти с версия 14 вот собственно с тех пор я так или иначе плотно занимаюсь всякими инфраструктурами на базе каберне tissot в докеров облаков и так далее ну и вот последнее время я очень много занимаюсь так сказать просветительской деятельности помогаем инженером познавать облачные технологии познавать современные подходы devops и вот все что с этим связанно в управление инфраструктурой поэтому я думаю что про кубер нить из что-нибудь интересное рассказать смогу о чем я хочу сегодня с вами поговорить вообще давайте как бы в моем докладе вся суть до этого доклада абсолютно отражен будет состоять из двух частей но вначале давайте определим с вами проблему по поводу отказоустойчивости современных систем немного поговорим об этом немножко определимся в чем там проблема и даже немножко в историю как дальше с вами поговорим о том как кубер найти со объяснить что такое зоны доступности вообще зачем нужны эти самые зона доступности и рассмотрим в чем проблема собственно уку вернетесь и изначально была зонами доступности ну и вторая часть моего доклада собственно будет про то как нам уменьшать затраты на кластеры этом покажу немножко математика в конце даже реальный в общем поговорим об этом давайте переходить к основной сути вот этот мужчина который сейчас на вас на всех смотрят с экрана помимо меня его зовут джон фон нейман джон фон нейман это американский физик математик занимался теорией информатики ну и собственно наверно большинство но и известен потому что его перу принадлежит такая штука которая называется архитектура фон неймана на которой собственно сегодняшний день до сих пор все процессоры в компьютерах и устроены так вот помимо того о чем я сказал да джон фон нейман также занимался проблемами обеспечения ну так сказать отказоустойчивости и суть в том что еще в начале 20 века когда джон фон нейман этим занимался он сформулировал скажем так то что на сегодняшний день называется принципом избыточности тогда люди работали в основном над тем чтобы проектировать ну некие электронные схемы так вот джон фон нейман узнал гениальную вещь которая не выглядит гениальный но я сейчас объясню почему она гениальна том фон нейман сформулировал то что компонента электронных схем дают сбои и мы все еще в современном мире не смирились с этой мыслью и знаете что джон фон нейман даже придумал что дело не в производителя то есть какого бы производителя мы не выбрали а компоненты электронных схем от любого производителя все равно будут давать сбои а иногда даже гореть соответственно нам нужны какие-то принципы которые бы позволили ну фактически из ненадежных элементов создавать надёжные системы и собственно вот этот принцип да там даже было потом такой научная статья по этому поводу основа основанное на исследование там оформим она которая называлась надёжные машины из ненадежных элементов то о чем тогда собственно рассуждал джон фон нейман да это то что сегодня в нашем мире сегодня очень применимо для построения любых информационных систем мы до сих пор у нас фактически любые информационные системы на сегодняшний день они строятся по принципу кластером и всегда в них подразумеваем что нам нужно избыточности всегда мы подразумеваем что нам нужна какая-то отказоустойчивость то есть мы никогда не хотим чтобы была единая . отказы мы никогда не хотим чтобы какой-то из компонентов наших схем мог сгореть или выйти из строя или дать сбой и при этом вы вся наша схема поломалась мы разговариваем с вами ок убирайтесь и поверните собственно в этом естественно не исключение он более того одно из его основных задач решения ровной этой проблемы то есть как сделать нашу систему более отказоустойчивый ну и понятно что там есть много разных вещей которые применяются да по поводу того что сами но да они идентичны их несколько штук в кластере мастеровые компоненты они тоже более менее идентичны их тоже обычного отказоустойчивый кластер несколько штук там есть healing ну и вот это вот все прочее помимо этого есть естественно best practice о том что все что запускается в кубер найти все должно всегда как раз так и следовать принципу избыточности то есть всегда запускаться несколько инстинктов и the best practice обычно это прям нужно контролировать да ну мы и говорим о и мы говорим о том что то что не следует этому процессу скорее всего в кубер найтись как бы плюс отказоустойчивость 1 кг обернитесь они не получат окей знаете что так выходит что вот это-то как раз таки о чем я говорила о чем вам провайдеры о чем нам всем провайдеры вообще всегда врут о том что uber не tissue недостаточно быть отказа устойчивым просто у себя изнутри так выходит что запуская кубер нить из где-то в облаках да и вообще запуская любые виртуальные машины в облаках ваши приложения база данных и так далее и так далее облачная инфраструктура не обладает высоким уровнем надежности по крайней мере в классическом понимании этого слова и проблема тут как раз таки в том что обычно люди рассчитывают что ну в смысле мы же поедем облаком и же запустим там свою виртуальную машину и провайдер нам гарантирует что эта виртуальная машина будет всегда работать о и виртуальная машина перестала работать так это же значит виноват провайдер и как бы нужно провайдеру писать письма и требовать от провайдера компенсации провайдеры чаще всего на эту провокации дуб потому что провайдер очень хотят сохранить клиентов и очень хотят чтобы как бы вы у них оставили свою инфраструктуру так вот это не правда на самом деле выходит так что облачная инфраструктура правда менее надежно чем ваша собственная железная инфраструктура ну естественно при условии что мы считаем что изначальная железная инфраструктура была настроена по правильным принципам хорошими специалистами с прямыми руками и так далее почему так происходит ну вообще чем меньше абстракции тем меньше есть чему сломаться собственно железные сервера отчетом о чему там ломаться ну да у него что-то может иногда прям вот перегореть да он может иногда сломаться но в целом это происходит нечасто потому что над ним нет никаких слоев с ним работают инженеры скорее всего только вот непосредственно от тех людях компаний на которых там запущенной инфраструктура до той компании ну и все в отличие от провайдера у которых во первых этих самых физических серверов просто тонны соответственно вероятность того что один из этих серверов выйдет из строя она гораздо выше у провайдера потому что у него их просто тупо много вторая причина этого всего в том что у провайдера над физическими серверами данных физическим уровнем есть несколько уровней абстракции есть и а слой инфраструктурной есть паз слой слой приложения до того что предоставляется ну и так далее там счас и так далее и так далее и естественно мы знаем что чем сложнее система чем больше вне абстракции да тем выше вероятность того что в этой системе возникнет ошибка а плюсом к этому еще над всей инфраструктуры провайдер работает гораздо большее количество людей то есть если эксплуатирую свою собственную железную инфраструктуру у вас там есть отдел инженеров до грубо говоря сопровождении которые туда ходят ну вот как бы вот вот что может вызвать ошибку вашей инфраструктуре в отличие от этого у провайдера есть инженеры в дата-центрах есть инженеры сопровождения есть разработчики есть техническая поддержка так далее так далее огромное количество людей которые действия которых могут привести к на взаимоотношению человеческой ошибки и вот благодаря этому всему так выходит что чтобы вам не обещал провайдер облачная инфраструктура будет при прочих равных менее надежно чем ваша железная инфраструктура а у меня тогда вопрос зачем нам нужны облака почему бизнесу не хочет попасть собственно ответ на этот вопрос очень простой потому что железный сервер если упадет что маловероятно но если все же то он упадет так упадет то есть сгорел не знаю что то физическом сервере нам его нужно демонтировать нам нужно достать оттуда тиски нам нужно эти диски как ты с них восстановить информацию ставите в другой сервер отправить этот на ремонт ну и так далее и так далее то есть прям авария так авария которая вызывает простой того чтобы на этом сервер на какое-то длительное время в то время как облачного провайдера как раз таки вся инфраструктура подстроено под то чтобы такие сбои на физическом уровне они проходили для клиента максимально легко но упал один из гипервизоров ну ничего страшного виртуальной машины на этом гипервизор из мигрировали на другой гипервизор там где течет моргнула запустилась дальше но упал один из свечей но ничего страшного все как бы там с мигрировала перри подключилась и так далее и все продолжила работать и вот ровно то что у меня написано на слайде да есть такой параметр который называется минтай mtu recovery mtr то есть нам обычно важно нет не только количество сбоев которые у нас происходят но и время за которое мы можем после этого сбоя восстановить свою инфраструктуру и работу своих приложений так вот просто тупо mtr у провайдера облачного скорее всего будет ниже чем у железной инфраструктуры ну и собственно действительно один сбой железной инфраструктурами три часа явно хуже чем три сбоя облака по пять минут следующий момент облака нам дает всячески инструменты для управления инфраструктурой до это и там лайки и всякий пас инструмент и какие-то утилиты консольные ну и так далее и так далее да то есть это весь этот набор инструментов которые позволяют нашу инфраструктуру делать максимально гибкой применять все вот эти с временные клоуны эти подходы к то вот это есть штука есть баек тв райкин тв райден фраз структура ну и так далее сделать нашу инфраструктуру гораздо более гибкой и гораздо более быстро восстанавливаемый после боев и третий момент это собственно те самые инструменты для отказоустойчивой инфраструктуры и это как раз то о чем я к чему я вообще все это говорил да то вы наверное уже подумали что металлисты представителя облаков и начали давать рекламу облаков но я вам сказал что я сегодня их всех разоблачу так вот инструменты для создания отказоустойчивой инфраструктуры это то что предоставляет вам облака и чем вы в принципе можете не пользоваться но вот на мой взгляд тогда честно вопросы по поводу того почему что-то у меня упало задавать своим инженерам как вариант тем людям которые строили вашу инфраструктуру на базе облака а не самому облачного провайдера потому что вот на самом деле на чем я хочу сказать о простой вещи которое называется зоны доступности до что-то может упасть до у провайдера может даже целых дата-центра пасть но то что у провайдера 2 дата-центра одновременно упадет это как бы маловероятно при этом запускать или не запускать ваше приложение в зонах доступности до ваших мастером зонах доступности и так далее это исключительно выбор за вами тут вас никто к этому принудить не может и если вы такой молодец что запустили свою единственную базу данных нас единственной виртуалок единственной зоне доступности в единственном регионе то как бы ну вы сами себе виноват вот собственно вопрос в том окей мы хотим использовать зоны доступности нам это нужно мы хотим делать нормальную отказоустойчивой инфраструктуру а как бы обернитесь научить с этими зонами доступности жить раз уж мы его где-то там разворачивает ну и на ум сразу же приходят штук который называется под аните affinity это вот ровно то что у меня здесь сейчас приведена наслоить и и если вдруг вы не знакомы до с тем как работает патенте affinity такая настройка который можно сделать на подав кластера кубер нить из которая фактически говорит scheduler у о том что есть определенный лейбл нот в данном случае это пол аджику верните и о flash зон есть определенный лейбл кодов в данном случае а п п и в и льюс тор и потом the finite говорю что вот кода с одним и тем же лейблом а пор не должны запускаться на нотах в которых одинаковый стают лейбл назначение лейбла тополага хубер нить и o-zone то есть что получается что в принципе таким образом мы можем гарантировать что вот есть у нас какой-то deployment и по до этого deployment одновременно запустятся в разных зонах доступности потому что под affinity как раз таким запрещает запускаться в одной и той же с вами доступности и обычно по данте affinity работает на уровне серверов на уровне ног власти раку бернейс ну и типа с этим все окей мы просто говорим кластеру побирается что не надо пожалуйста все яйца как бы запихивать в одну корзину запускать их на одном сервере потому что он если что может упасть и мы бы не хотели чтобы все наше приложение в месяц этим сервером стала недоступна но в случае с зонами доступности это логика работает немножко по-другому потому что обычно зон доступности у нас ограниченное количество две-три там может быть больше данных но редко сильно больше если мы скажем с помощью подать affinity кубер не tissot запускай пожалуйста по одному году в каждой зоне доступности ноги а если у нас deployment с тремя репликами а при этом серверов естественно в каждой зон доступности у нас тоже не по одному а чё бы нам в каждой из зон доступности мне запустить столько падов сколько мы хотим но при этом как бы гарантировать что они запускаются и там и там вот собственно поэтому по данте affinity парит тут под эту задачу гарантированно не подходит и раньше ответ на этот вопрос собственная там много обучения my занимаюсь да и так далее этот вопрос неоднократно всплывало естественно раньше ответ на этот вопрос был очень простой губерн этот конструктор вы можете дописать переписать как бы в нем что угодно берете немножко голан вы берете немножко как бы документации и пишите свой шедевр и объясняйте ему что такое зона доступности как с ним работать но ура версии номер 1.19 кластера кубер нить из у нас появилась такая штука которая называется под топор jaspreet constraints я на всякий случай сразу же где я буду говорить про какие там штуки внутри купюрница да я буду на слайды прикладывать ссылочки на документацию я надеюсь что коллеги потом каким-то образом этими презентациями с вами поделятся ну и можно будет это просто тебя скопировать оттуда так вот вот у меня сейчас на слайде как раз таки представлен пример этой самой под анти affinity давайте посмотрим вообще для чего она нужно и что она умеет первое что нужно знать о под то полу jaspreet constraints это то что эта штука позволяет как раз таки научить обернитесь распределять воды по какому-то признаку равномерно между какими то там лейблами но зон доступности и так далее у нас есть первое что вот конфигурации этого да это параметр макс q который говорит о разнице по количестве кодов по зонам которые мы с помощью под apologize прет constraints выделяем то есть в данном случае 1 до говорит о том что у нас отличие одной зоны от другой зоны может быть максимум в один под если вдруг там что-то не влезает следующий момент который тут есть это собственно то паладжи кей это тот самый лейбл где-то на но тогда который по которому мы говорим о верните а по какому принципу вообще разделять наши ноты на те же самые зоны доступности то есть в какой зоне доступности это но находится прокаливаем доступности зонам но до находится соответственно для этого есть o'polo джи кей также давайте волен on so this file пока что пропустим рассмотрим очевидный параметр который или лайнелл селектор дома fables а п п п п это собственно какими под нами мы с помощью этого то положи sprite constraints хотели бы управлять то есть всеми подали на которых стоит метка а п п п п п их соответственно нужно будет с помощью этого то получу спред constraint а разделять по разным other ну и последний параметр который у меня тут есть это а что мы будем делать если нельзя удовлетворить настройки распределения и вот в данном случае здесь указан параметр по умолчанию donod shadow то есть не запускать соответственно если вдруг у нас там тепло и минти то есть десяток кодов и для зона доступности одна из зон доступности накрылась и туда больше нельзя запустить ничего значит оставшиеся пять кодов будут висеть и ничего не делать в кластер второй момент который тут может быть это shadow wave и этот параметр подразумевает что если вторая зона доступность 0 там 3 4 5 накрылась то значит года до нужно запустить в тех зонах доступности которые остались и как бы ничего страшного соответственно понятно да что вот такие штуки их выставления и как бы решения о том что должно быть то или иное это вопрос исключительно ваших личных необходимости и так далее соответственно а как вообще всю эту акулу jaspreet constraints можно использовать потому что ну очевидный вариант о том чтобы распределить как бы пробегать или там у провайдера посмотреть просто какой какая метка стоит на его нотах по поводу указания какой зоне доступности они относятся это очевидный вариант он скучный и как бы но но безусловно очень полезно еще один момент понятно что я тут про блокада про 21 век рассказываю и так далее и так далее естественно не у всех инфраструктур об облаках не всегда это хорошо запускать свою инфраструктуру в облаках не всегда это нужно да и не всегда это возможно что можно с помощью нее сделать а что если мы на каждую ноту грубо говоря в нашем кластере кубер нить повесим лейбл типа номер стойки например а потом с помощью то положи sprite constraint скажем пласти руку berlitz а распределяй пожалуйста наши коды по нодам на основанию а по стойкам а вот на основании этого лейбла и соответственно гарантирует пожалуйста что наши коды никогда не запущена в рамках одной стойке распределены всегда между несколькими всеми или каким-то количеством которые в кластере у нас есть и на мой взгляд наименее очевидный способ как это можно использовать это собственно для экономии денег почему при чем тут экономия денег вообще в облаках обычно есть такая штука которая называется spot это ноты которые сервера до виртуальной машины которые вы можете купить так сказать с аукциона потому что в данный момент право провайдера и there's a все просто являются они мне нужны и обычно он их готов вам продать по более низкой цене но взамен на то что как только провайдеру эти ресурсы станут нужны провайдеру вас их заберет и на них запустит что-нибудь другое да то есть ваши роды могут быть в любой момент выключены и не факт даже что вместо них вы как вы сможете запустить другие такое тоже бывает и вот эта вот штука со спотами это то что очень сильно позволяет экономить деньги то есть запускать свои например там продакшена на спотах это очень большая экономия но очень большие риски потому что понятно что в любой момент могут отобрать у вас ничего не останется а как гарантировать что у нас допустим есть часть статических инстансов обычных часть спотовых инстансов постерегу бернейс и как сделать так чтобы убирать запускал ну какую-то часть плодов на этих спотах а какую то часть кодов на обычном статических японцах а что если мы с помощью то паладжи спред constraints вообще посмотрим разобьем грубо говоря на шинода в кластере на те ноды на которые с потовые дай на них будет висеть лейбл что она типа спотовой эти ноты которые статические обычные них будут висеть лейбл что они статические и обычные таким образом мы с помощью того же 100 положи sprite constrain сможем всегда гарантировать что часть наших кодов запущена в носках а другая часть наших пода всегда запущен на статических инсультов и если вдруг спотовые мз и дз и перестанут работы да понятно что у нас станет в классе терем меньше инстансов нашего приложения меньше под каждого из дипломатов условных но при этом какая-то их часть которая работает всегда на статических родов она останется она скорее всего там как-нибудь переживет потерю всех остальных инстансов так как нибудь скорее всего по работает как минимум это даст вам время для того чтобы застрелить свой кластер девал статические ноды до большего количества и таким образом как бы от спотов отказаться дают за скилле свой просто обычный обычный пластик поэтому то поводу спред constraints я очень рад вообще что эта штука появилась крайне полезная вещь и всем рекомендую пользоваться и так сказать отстать в этом плане от провайдера использовать облака нормально в этом моменте мы переходим к следующему как раз пункту моей моего доклада это про экономию денег в принципе знаете еще один миф который существует пара блокада это о том что облака это очень дорого провайдер очень хочет чтобы вы все к нему пришли и как бы запустили там свою инфраструктуру и платили провайдеру как можно больше денег а ну естественно те дурачки которые на это ведутся до в итоге попадают на большого количества денег и начинают провайдеру очень дорого платить за инфраструктуру так вот рассказываю вам секрет еще один для того чтобы пользоваться облаком нормальные для того чтобы облака не было для вас безумно дорого к облаку нужны совершенно не те же самые подходы что и нужны к вашей старой железной инфраструктуре с точки зрения планирования ресурсов экономии денег это естественно этого то же касается и ответ на этот вопрос собственно как экономить деньги в облаке это авто скиллинг я так как мы с вами разговариваем про кубер не тесто буду рассказывать про губернаторский авторский link но мы с вами выйдем за пределы brings овса рассказов то скиллинга потому что на самом деле с помощью этого можно экономить очень сильная и очень на всем и я даже вам покажу реальные цифры так вот чего вообще как бы что у границы с авторским я думаю что это далеко не новой что он выверните то есть штук которая это делает называется горизонт алпатов то скейлер это встроенный объект власти раку вернитесь и он прекрасно работает хоть на облачную инфраструктуру хоть на свои собственные железный там он привез инфраструктуре все что он делает это следит за определенным параметрам в мониторинге и как только этот параметр мониторинга начинает зашкаливать он добавляет какое-то количество реплик вашем дипломе там которые у вас работают в кластер ну соответственно понятно что у него настраиваются пределы с кейлин когда нижняя граница верхняя граница понятно что он делает это в обе стороны то есть и увеличивает количество ресурсов которые коль количество точнее из приложения которые у вас в облаке запущены и уменьшает их но к сожалению с авто скиллингов кубер нить есть большие проблемы во первых из коробки работают только на основе нагрузки по циpкa то есть мне за какими другими параметрами кроме как нагрузка на циpкa вы с помощью горизонт был под авто скейлера прекрасной фичи встроили в интернете следить не сможете и это крайне грустно потому что на практике но нагрузка на циpкa и там нужно очень сильно постараться чтобы ваших приложениях была гарантия что нагрузками циpкa всегда carel коррелирует с тему что реально вашему приложению нужно шансов тогда она будет как бы справляться с этой нагрузкой потому что я видел такие приложения которые работают в полку всегда при одном рпц и как бы это их нормальное состояние если у них будет стоя поэтов а не будет а также в полка и продолжать выполнять свою работу а вторая проблема авто скейлера в класс терек оберните в том что он решает только задачу с кейлин гапонов и вот собственно тут наступает проблема потому что в чем экономия то ну хорошо но вот у нас есть кластер и купюрница опять под а вот к нам пришла нагрузкам и впасть в реку верните запустили 10 ботов а провайдеру какая принципиальная разница как он с вас как быть 5 у вас кодов постели 10 провайдер с вас будет брать деньги за то количество ресурсов которые вы ассоциируете в год то есть фактически за то количество нот сколько у вас есть пластик ударница и авто скейлер класть ираку берлинцами каким образом эту задачу решить не могут потому что вы в любом случае будете вынуждены создать себя от plaster кубер лица такой чтобы в него влазила то количество кодов которые у вас будет создавать максимов то скейлер все остальное просто не влезет и останется висеть и как бы какой смысл собственно давайте посмотрим как эти проблемы можно решить опять же практически нативную средствами купер найти со проблема номер 1 решается штукой которая называется горизонт алпатов то скейлер версии 2 вот про него я хочу рассказать немножечко поподробнее потому что там есть ряд интересных штук но для того чтобы понимать нужно сначала разобраться естественно с другими я мулами поверните давайте начнем с того как вот по классике работает в принципе и и первый горизонт алпатов скейлер на обычный значит проблема в том что обернитесь не имеет ни малейшего понятия ничего ни о каких метриках понятием метрики вообще в кластере кубер не tissot как таковой как бы и нет и api сервер кластера об этом ничего не знает и естественно у него нет никакого встроенного механизма мониторинга для того чтобы ходить опрашивать ваши коды и узнавать а сколько они там того же самого хотя бы несчастного циpкa используют единственно функционал которую кв-1 есть для того чтобы вернуть все объяснить что метрики все же есть и вообще объяснить кубинцу что есть что угодно и как угодно расширить его api потому что кайбер notice мажино им это конструктор до из которого можно и чайник собрать пожелание для этого купюрница есть такой специальный объект который называется кубер нить из api сервис и это то что вот у меня сейчас на слайде к с правой стороны изображенным небольшой кусочек yabla что из себя и сервис вообще в целом представляет ну вот всякие api вершины к in the metadata да и так далее понят что это то же самое что есть любого абсолютно объекта в постельку дарница нас как и обычно у всех я бликов куликовских первую очередь интересует speck ну и в общем то тут есть одно поле это сервис а дальше на именные space ii групп что это значит давайте будем разбираться с конца с группа вот эта конфигурация api сервиса и еще раз это не api сервер это именно api сервис как объект кластер об интернете эта конфигурация говорит что как только куда меньше и сервер на этот раз уже придут запросы в api для группы метр xk8 своем то есть ну буквально за пузо запрос вапжя который условно говоря там тем доходом будет выглядеть как flash api слэш метр xk8 с аленом слэш метров 100 условно этот запрос должен обрабатывать не обе сервер q берлинцы и возвращать 404 о том что он такого к сожалению не знает а он должен его прозрачно для за просившего переслать в сервис который называется метрика сервер который находится майспейсе куб систем и вот все это как бы исходит из того описания которое есть у нас api сервиса соответственно вот горизонта торцы скейлер он работает как ему нужны метрики для того чтобы понять вообще как скелет нужно потом он соответственно делает запрос куба peecher на руку bapi сервера есть вот эта конфигурация api сервиса он понимает что ему спросили про группу метр xk8 свое пересылает этот запрос на фактически уже нативный компонент купюрница данный метрик сервер который ходит к нодом кластера поверните со периодически собирает с них информацию о том а какие там у нас параметры у кодов да какая на них нагрузка и так далее и так далее ну и всю эту информацию собственно в правильном в том виде который каберне царский api сервер определяет до для этого api он его отдает api серверы api сервер от соответственно дает запросив шему в нашем случае горизонт алпатов то скейлер так вот благодаря всей этой системе у нас получается что в принципе вместо метрик сервера который не умеет ничего мы можем в общем то опять же с помощью api сервисов научить кубер нити совские сервер пересылать запросы на любой сервер мониторинга да и с любого сервера мониторинга любые данные вообще-то получать и использовать их в качестве параметров для авто скиллинга вот на ровной этой штуки основана работа горизонту удастся скейлера версии 2 он естественно все также имеет работу с метрик сервером а там немножко отличается формат я мало именно самого hop но не принципиально основная суть в том что горизонт год овцы скейлер как я уже сказал второй версии он позволяет использовать кастомные метрики метрики не только по циpкa но и по много чему еще собственный единственная проблема только для того чтобы вся эта штука работал до понять что нам нужно на пи сервис создать его в кабинет api сервере но нам также нужно чтобы ток чему будет пересылать наши запросы api сервер могло входить в нашу систему мониторинга доставать оттуда любые данные и передавать их в кубе рыцарский api сервер в нужном формате в том формате в котором api сервер как бы считают что вот его и это должно быть поэтому задача фактически прикручивания любой систему мониторинга к авто скиллинг убирается решается простым действием нам нужен кусочек кода который будет ходить в нашу систему мониторинга считывать оттуда нужные нам данные потом засовывать это фактически в джейсон чеки в правильном формате и выдавать эти серверу при его запросах все есть под это дело понятно вот у меня тут например в примере данной картинке есть как раз наверное самая распространенная система мониторинга для купюрница это prometheus и у меня тут как раз есть пример того как open-source нова такого провайдера до который называется prometheus адаптер который умеет делать вот ровным счетом то что я описал он может пойти в prometheus собрать оттуда все нужные нам метрики и выдать их для кубинцев ского api серверов нужно формате горизонт алпатов то скейлер во 2 может на эти метрики посмотреть и на их основании постелить коды в вашем пластыря вообще на основании чего это можно делать собственно чё горизонт алпатов the seller на предоставляет я сказал что-то по типу это не очень хороший вариант так вот прям на основании любой метрики на основании того что там через наш воды проходят такое-то количество рпс мы их смотрим допустим по всем потом нашего deployment а понимаем что там рпф при превысил определенный порог значит нужно дипломе ту добавить еще кодов добавляем еще кодов на основании рпс а которые проходят через ingresso то есть допустим наши коды не умеют свои собственные рпс и считать но в вашем кластере есть компонент который их точно умеет читать и точно умеет примет и усов тому же самому уже эту метрику отдавать именно с контроллер спрашиваем у него рпс на каждый ингрос но понятно даже ingress как объект понятно что за каждую энгр и сам как объектом стоит сервис как объект а за каждым сервисом как объект стоит как бы какой-то набор пазов который скорее всего обычно принадлежит одному дипломе соответственно на основании метрика рпс of на каждый раз отлично можем спилить более того мы даже можем стелить на основании метрик которые вообще никаким образом обернитесь не привязана а что если условно у нас никаких рпс of вообще в принципе ничего такого обозримого приложения нет это допустим просто worker и которые не знаю там сидят фоне до из очереди вычитывают сообщение и по два часа потом конвертирует видео в какие-нибудь формата но соответственно есть какая-то фишка которую в эту очередь добавляет просто задача отдам войти с келли кода в нашем кластере на основании метрики количество сообщений в очереди и соответственно если количество сообщений в очереди на каждый worker а там больше чем по два на варган давайте добавлять просто больше worker of и естественно вся эта штука она работает как вверх так и в из то есть есть у нас много сообщений в очереди добавили worker кончились у нас сообщение в очереди так клиентам наш сервис не нужен убрали все worker и они нам больше не нужны ничего в кластере не работает и вот благодаря такой настройки на основе горизонт алпатов скейлер версии 2 мы можем создавать в принципе системы которые могут изменять свои состояния на основании любых параметров как я уже сказал которые есть в наших системах мониторинга но собственно естественно как опять же я уже говорил у нас вся система авто скиллинга будет неполной ровно до тех пор пока у нас не будет уметь скейлится кластер и даже не только кластер соответственно для того чтобы все это дело работало прекрасно нам нужно ну во первых то что я уже упомянул горизонт алпатов то скейлер версии 2 во вторых это штука которая называется кластеров то скейлер это как раз таки то что обычно в облаках установлена покласть или кубинцы и позволяет ему понимать что если вдруг в вашем кластере кончились но да и у вас есть пода в состоянии pending да то есть те которые scheduler не знает куда она значит потому что для них нет нужных not to кластеров task killer автоматически добавит вам в кластере кубер месяц точнее сходят в пик провайдеру создаст вам надо для своего кластера и добавят их ваших власть и ваш plaster автоматически вырастет нужные буду там до запустятся и все будет работать более того естественно кластеров то скейлер точно так же как и горизонта lot of those киллер умеет работать в обе стороны то есть он умеет как увеличивать количество кодов которые работают в вашем кластере так и уменьшать это калико not извините как увеличивать количество нот которые есть в вашем кластере так и уменьшать то есть как только у вас под авто скейлер от работает в обратную сторону нагрузка спала допустим да сообщений в очереди осталось 0 worker и ваш с хлопнулись все кластеров то скейлер узнал что она у нас кажется в кластере есть свободные ноты которые ничем не занимаются прекрасно нам значит нужно эти ноты и сквозь 2 брать они нам теперь не нужны пусть пожалуйста владелец кластер они платят провайдеру затянуты который у него стоят и ничего не делают это вот то что касается кластеров uber месяца но давайте на самом деле посмотрим на это дело шире потому что помимо рабочих самих приложение дано продакшене которые у нас есть у нас есть еще обычно системы себя и сиди со своими вот этими тоже ранее раме и так далее так далее если вдруг вы живете еще в такой ситуации когда у вас есть там именно и количество jenkins или bamboo раннеров и на них установлен нужный вам софт и они как бы работают статические разработчики к вам приходят и ноют о том что года 5 мой убил два часа стоял набил сервере не мог собраться потому что там очередь или а поставьте нам пожалуйста пакетик вот такой-то потому что набил сервере а у нас его нету то пожалуйста прикрутите к своей seo и сиди какой-нибудь способ автоматического создания worker of это можно сделать не только в новых модных всяких там гид лапах которые имеют свои размеры прямо в кубер найти запускать ее запускать их по требованию быстро только тогда когда они нужны даже если вы не можете запустить раннеров кубер нить это тот же самый дженкинс можно отлично прекратить практически к любому облачную провайдеру и дженкинс отлично умеет создавать своих рамеров виде виртуалок по появлению задач в очереди то есть тут всегда есть вопроса сколько нам вообще этих самых рамира всей сиди нужно с какими конфигурациями ответ на него с теми конфигурациями которые сейчас нужно разработки и в тех количествах сколько они сейчас портной разработки поэтому кроме динамического их создания ответа тут никакого быть не может помимо того что это просто облегчение жизни на разработке это еще и конкретной экономия денег то есть статические райнеро существует всегда деньги они едят всегда бил дата не что-то не бил это не что то наверное разработчики с как у нас все еще к сожалению люди поэтому они наверное к сожалению вынуждены иногда спать по ночам поэтому наверное скорее всего ваш инфраструктура вот соседи вот это все ночью простаивает а статический build сервера как кушали деньги так и стоят а кушать и примерно то же самое касается и динамических стендов разработки да вот этот вопрос а сколько нам там тв ей стечь стендов нужно там мало 10-ти по двадцать тридцать сорок пятьдесят ну и так далее и так далее опять же если они у вас вдруг работают все еще статически очень хорошая практика превратить их в динамический стенда разработки запускать их ровно столько сколько их нужно для решения задач разработками текущий момент и помимо того что это просто удобство для разработки да это еще и экономия денег потому что опять же разработчик это так получается что по ночам иногда спят поэтому это все может по ночам схлопываться и не тратить деньги ну и собственно если у вас есть проблема то есть я мы хотим сделать вот эти все того пса вы и штуки авторский линги динамические стенда разработки и так далее но не знаем как обосновать бизнесу то что мы сейчас будем пилить такие штуки да и тратить время своем на это то расскажите бизнесу про то что на этом можно экономить бизнес вам paypass поймет и как бы будет очень рад если вы ему такое сделаете ну и собственно на этом месте раз уж мы заговорили про бизнес я бы хотел немножко поговорить про какие-то реальные цифры и их немножечко прикинул я не очень хороший математик но формула тут были на выходе достаточно просты и поэтому я думаю что мы с вами сможем разобраться значит я просто так из из головы представил гипотетическую инфраструктуру то есть условно у нас есть в компании 10 микро сервисов как вы понимаете 10 микро сервисов очень маленькая компания последние все проекты которые я видел на микро сервисах более-менее крупных компаниях они обычные стесняются уже сотнями мика-тян исав даже не десятками допустим каждый все это работает у нас купер лице понятно да допустим мы там провели нагрузочное тестирование что-то узнали о наших приложениях и так вот усредненно пришли к тому что каждый под из наших приложений есть по 500 миль и цыпа в кластере по 512 мегабайт оперативной памяти и в среднем выдерживает каждый под по 200р ps ok давайте еще представим что у нас есть опять же метрика о том сколько офисов на нашу систему сейчас приходится ну и будем считать что она сходящий рб с днем он в районе 1000 а входящие рпс ночью он в районе 50 опять же пользователи спят да там особая система никто не поняли да идти 50 которые у нас остались и есть от скорее всего принтом внешние систему мониторинга и googlebot и которые входят passat нашей системы ну и собственно за ночь раз уж мы с вами занялись математикой давайте примем временной интервал когда во первых темно во вторых который длится 8 часов вот исходя из этих данных которые на мой взгляд довольно приближены ну более-менее реальным условиям более-менее такой небольшой компании но которая успешно провела рекламу да потому что тысяч рпс для небольшой карп компания наверное там молила я пошел прям вот в нашего как клауд solutions и посмотрел а сколько мне будет стоить конфигурация там такой средненькой народа да там ну 6c пути по 6 гигабайт оперативной памяти в нашем облаке в час и у меня получилось значение ну что-то типа всем рублей в час да я округлил давайте считать что но да наша нашего кластер камберленд стоит 7 рублей в час вот теперь имея все эти данные мы можем посчитать по конкретным формулам значит давайте считать так что в нем мы считаем что договорились что рпс у нас 1000 прессов каждый под держит у нас по 200р псф соответственно у нас есть 10 микро сервисов в общей сложности до этого нам нужно запустить бьем 50 кодов с этим все понятно если считать это по ресурсам да то это получается мы там тоже говорили о том что у нас 500 милиции по 500 мегабайт оперативной памяти на каждый под у нас получается вот в конфигурация которой я сказал у нас получается что-то в районе пяти нот нам нужно запустить для того чтобы это все могло запустится в нашем пластику вернется теперь считаем все то же самое для но чем берём те же самые 50 берем точнее 50 рпс которые у нас ночи берём те же самые 200 рпс которые у нас вот каждый под мы знаем что может выдерживать умножаем на десять микро сервисов получаем 20 кодов я как говорил у меня с математикой не очень хорошо но на самом деле мы это мисс математикой у меня не очень хорошо я посчитал когда как бы у меня не сошлось опыт потом я понял почему потому что мы с вами все еще не забываем про избыточность избыточность хотим сохранять и даже ночью как мало ли что может случиться хэпу то рандомные пользователи ночью в нашей системе все-таки есть мы не хотим оставлять наши коды в единственном экземпляре и вообще хорошая практика даже когда у вас работает овца скейлер низкий лить все дотам единицы до и так далее потому что но хорошо сейчас не пользуется никто он начнёт пользоваться и сразу же что-то упадёт пока у вас все эти авто скейлеры и так далее сработают пройдет какое-то время поэтому минимальный запас ресурсов минимальный отказа включаться так он все-таки должен быть поэтому вот у нас ok получается что два по два года каждого из микро сервисов мы так и оставили получилось на выходе 20 кодов опять же если посчитать это по ресурсам это получается 10 циpкa 10 гигабайт оперативной памяти в общей сложности в сумме что полу даем нам две ноты ночью а теперь давайте посчитаем деньги то есть мы днем используем пять нот ночью используем 2 но до соответственно можем 3 но до свободно освободить за ночь мы приняли 8 часов количество нот которые мы можем ночью освободить три штуки но да у нас стоит 37 рублей в час ну и собственно умножим это все дела на 30 дней в месяц и у нас получится и того экономия в 5000 40 рублей кажется что сумма небольшая если не принять в расчет итоговую стоимость вообще всей нашей инфраструктуры и размер нашего проекта так получится да что ну относительно всеобще стоимость я ее в конце сейчас вам покажу да и то это будет довольно существенная часть более того мы же с вами говорили о том что авторский ли сможем не только продакшен приложения в постели купер не tissot давайте еще вспомнил что у нас есть вот те самые стенда df стенда стоит стенды который у нас там могут быть динамическими дай которые могут реально отключаться по ночам и не тратить деньги у нас есть всякие соседи райнер и которые тоже опять же могут запускаться динамические по требованию до при появлении задач на бил тепло и так далее которые тоже могут запускаться не запускаться по ночам даю не thrive наши ресурсы ну и в общей сложности если так посчитать то я думаю что вот эти 5000 вполне в такой реальной инфраструктуры даст вам дэвами чайками и прочим можно 1000 10 расширить смело это при том что вот если считать затраты на таки инфраструктуре то общее это трата она получается 30 тысяч то есть мы свободно с вами сейчас можем экономить треть и кажется что типа ну вот эти все авторские линги они нужны чтобы это удобно переживать черную пятницу то ее что я буду в течение суток у меня такие колебания будут инфраструктуры происходить я ничего на этом не выиграю вот когда на новый год там или в черную пятницу ко мне придут клиенты of the selling здорово мне поможет пережить нагрузку но нет это не только для этого это для того чтобы облака не казалось дорогим более того вот у нас получилось с 30-процентной экономия и вот я это все посчитал по формуле взяв эти все цифры до прикинув и тут я понял что мы вообще-то когда-то с нашими клиентами прямо в живую пробовали это все провернуть пробовали это все настроить и видели ровно те же самые цифры то есть я могу подтвердить что у реальных наших клиентов которые пользуются вот всеми рекомендациями касательно авто скиллинга дают грамотно подходят к построению своей инфраструктуры получается ровно это же экономия в 30 процентов а вот теперь возьмите стоимость условной ваш инфраструктуры железной возьмите стоимость облака вычтите из этого 30 процентов и ответьте себе еще раз на вопрос а дорогая ли облачная инфраструктура получается относительно вашей собственной ну и собственно теперь вот вывод из всего моего доклада который можно сделать и который я бы хотел сделать это то что облачная инфраструктура это не тоже самое что ежели разве инфраструктура и очень часто так как это вообще работает бизнес слышал про то что облака это хорошо и облака могут как бы сделать бизнес лучше удобнее гибче и так далее бизнес приходит в своему эти отделы к своему условном и сетевой симада и говорит о том что ребята нам нужно срочно бабло к сети оси я ем как отвар хватается за голову у него своих проблем хватает теперь ему еще нужно мигрировать в облака эти ребята прибегают к облачные провайдеры говорят отдавайте вот у нас есть инфраструктура давайте мы ее просто какие-то сунем в облака потом будем иммигрировать облачный провайдер очень хочет чтобы к нему пришли людям и говорит а давайте в ответ на это и что в итоге мы получаем мы в итоге получаем что бизнес не может понять а почему в облака переехали а стало хуже почему стала менее надежно наверно его плохого провайдеров и выбрали провайдер пытается понять а почему у него система такая ненадежная почему клиента к нему приезжает у них вечно что-то ломается ну как бы потому что клиента затащили инфраструктуру построена по совершенно не облачным принципам как бы технический отдел пытается понять отчете перед этим всем делать и почему она как бы плохо работает почему этим всем неудобно управлять и опять же почему у провайдера так все нестабильно так вот еще раз облачная инфраструктура требует облачных подходов и это совершенно не тоже самое что просто взять все ваши железные сервер и снять с них снапшоты и запустить их копии в облаке большое спасибо за внимание вот тут есть мои контакты если вдруг у вас нужно будут какая-то консультация возникнут после конференции вопроса захочется со мной пообщаться лично можно пожалуйста писать всегда рад со всеми пообщаться спасибо тим большое спасибо за доклад действительно обсудили столько секретов и особенностей использования облачных технологий сегодня здесь нашим зрителям это будет полезно очень много поговорили про облако про то как можно использовать губернатор как можно настроить там все еще использование гибридной особенно последнее время или он примется в структуры никуда не делась как принципы и какие-то подходы которые ты сегодня упомянул можно использовать в он примеси гибридная структуре прекрасно вот смотрите тот же самый например ну я упоминал до про тот же самый под спред constraints у вас тоже ваши вам принес инфраструктуре есть некие домен и доступности условная дать те же самые стойкие в рамках дата-центр прекрасному отзовите свои стойки авила били сезонами повесьте на но до в этих стойках какие-то лейблы и делайте то же самое что вы делали бозонами доступности поверьте все станет работать стабильнее там выход из строя одного свеча в одной стройки не будет выбивать вам как бы рандомный кусок ваших приложений просто подчистую есть наверное там у кого то несколько дата-центров между ними растянуты купюрницы ну и так далее и так далее да этим всем естественно можно пользоваться взять тот же самый авто скиллинг понятна проблема с тем что ну то есть горизонта ботов то скейлер внутри кластера купюрница может работать всегда прекрасна в он примесь инфраструктуре железный там облачно и так далее с кластеров тасс киллером всегда проблема потому что если инфраструктура ваша но вы теоретически можете накрутить собственную виртуализацию да я в тоске или ваш кластер убирается там в виртуалке в него добавлять с помощью кластеров то скейлера в чем экономия учитывая что ваши ресурсы там железные сервер они уже ваши поэтому в этом месяце то я наверное не сказал что тут есть какое-то преимущество относительно железный относительно смысл есть какой-то смысл в том же кластеров то скейлеры да на своей собственной железной инфраструктуре но вот например скиллинг кодов внутри кластера даже просто дадут чтобы кластер у вас не был непомерно огромным это полезно потому что обычно все таки микро сервисной архитектура это такая комплексная штука и нагрузки тута и задачи входят неравномерно не одновременно на все сервиса поэтому повозившись hop во 2 да и поймав от правильной метрики на основании которых можно стелить можно просто это сделать таким образом что вы будете овир селить свой кластер да то есть он у вас будет постоянно sauer комичен то есть условно если там все доске лица она туда не влезет но если это все будет скейлится по очереди на горы прошли задачи тут какая-то фишка застрелилась по ночам работают worker за skills потому что он в ночью читать что-то готовится так далее и так далее это все можно делать на гораздо более маленьком кластере чем боб если это все запустить на максимум как-то сока супер пасибо но мне кажется даже если используется он примется в структура но планируется переезд в облака использования скейлера в том числе даже кластеров то все лера позволит понять сколько ресурсов потребуется в облаке если использовать облачные подходы и заранее понять собственно какие будут плюсы какие будут бюджет и вот такого переезда и может быть получить хорошие обоснование для того что такой переезд совершить так что даже если вы еще не в облаке использования клауд найти в подходов может вам серьезно помочь большое спасибо у нас есть ещё несколько вопросов которые мы продолжим обсуждать в зум комнате после докладов которые мы сейчас все переместимся большое спасибо еще раз пауза доклад и давайте переместимся в комнату и продолжим обсуждение там вы сможете задавать вопросы лично или если у вас есть проблемы с аудио видео я помогу прочитаю вопросы и мы постараемся на них ответить супер спасибо пойдем давайте перемещаться замковым то пасибо"
}