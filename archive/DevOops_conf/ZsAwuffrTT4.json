{
  "video_id": "ZsAwuffrTT4",
  "channel": "DevOops_conf",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "всем привет меня зовут алёна проходчик я руковожу отделом разработки в компании раньше labs который находится в калифорнии и занимается внесу раз проектами организации контейнеров один из наших проектов это управление куда-нить с кластерами немножечко себе я сама из беларуси в 2007 году я выбрала green card внутри и переехала в соединённые штаты америки как-то так получилось что я сразу начала работать в сфере виртуале cs4 виртуальных машин на этом уроке не лот bouncy и вот сейчас работаю с контейнерами и сегодня мы будем говорить как мы строили систему управления кудрин эти с пасторами и пока и прячем перейти к презентации я хочу просто узнать у кого-нибудь есть в организации несколько кайбер натиск мастеров которыми вы управляете отличным а если кто-нибудь кто пишет приложение так uber notice так называемое на этих купюр норрис applications отлично первый вопрос на который мы хотим ответить это вообще зачем а зачем такая система и зачем в организации несколько кубер нить из кластеров почему нельзя развернуть один каберне this кластерном большое количество хвостов потому что кудрин эти в принципе легко масштабируемый как только мы ответим на этот вопрос мы перейдем к вопросу как как вообще построить систему управления кудрин эти с кластерами на что обратить внимание и какие вообще какие вообще аспекты должны быть сразу освещены на какие аспект обратить внимание что самое главное в этом и мое самое любимое естественно мы будем говорить про все косяки которые были сделаны по ходу разработки этой платформы чтобы ответить на вопрос зачем иногда полезно взглянуть на историю . кусочек история развития дата-центра я начала его с виртуальных машин виртуальной машины повлекли за собой такие системы как open стяг клаус так система регистрации виртуальных машин я была как у истоков клаус так и прошли весь путь это было очень интересно когда появились контейнеры история стало повторяться через какое-то время стали появляться системы для регистрации контейнеров такие как свой и вернитесь назад и на сегодняшний момент я считаю что можно сказать что регистратор для чего он вообще нужен чтобы масштабировать и запускать приложения на нескольких as так что было не рвать нагрузку этот минимум для чего он нужен оркестра то никто в принципе не рекомендует запускать контейнеры просто так без регистратора регистратор нужен и на сегодняшний момент кубер нить из это можно сказать что он самый популярный оркестра тары для контейнеров небольшое небольшое отступление в rancher вообще начиналась наша компания все начиналось как аналог купер нить из у нас тоже был свой оркестр атор вот и когда появился кабинете см и my зазором мы стали смотреть вообще что он наберет большую популярность и естественно в какой то момент мы увидели что все кубер найти с набирает популярность и мы переработали свою структуру структуру продукта и выпустили вторую версию с концентрации на кубер нить и вообще почему купер найти что же сделало его таким популярным ну и здесь на то что он вышел из гугла это добавило ему привлекательности потому что он использовался внутренние это создало ему своеобразную репутацию хорошие то что у него очень классные комьюнити очень большой очень большое внимание оказывается менеджмента комьюнити и 3 с точки зрения инженера это очень классная платформа для интеграции она расширяем а она очень она очень гибкая и как инженер я реально наслаждаюсь когда пишу компоненты для интеграции скутер найти но легкость не приходит без сложности то есть никто не сказал что будет легко сам кубер найти состоит из очень множество компонентов у них между собой свои отношения каждый разговаривая друг с другом по каким-то своим правилам и поначалу как только кубер найти с выложил ссылку 2n source запустить регистратор это было общей отделан очень нелегким сын все пытались как-то притвориться что все понимают вообще как его запустить вот этот человек на слайде келси хайтауэр давала про таки какой то момент он написал get хоп документ где он объяснил прошел через все шаги кубер найти с документа чтобы показать что ничего страшного признать что к вернитесь на самом деле он не легкий и установить его очень легко и апгрейт его тоже дело было нетривиальным то есть поначалу даже переход одной версии кубрина тициано другую вызывал собой перезапуск всех пользовательских приложений то есть какой-то момент он подчинился но все равно вы естественно существовало несколько видов инструкции для запуска оберните s-образных инфраструктурах в разных облачных провайдеров из какой но в какой-то момент установка поверните со оно стало своего рода комодики появились очень много очень много автоматизированных инструментов чтобы его запускает либо в каком-то специфическом облачном провайдере или или два в локальном кластере и в как вот и пару лиг по чуть чуть больше чем год назад появились вернитесь как сервис облаках сначала на губе а потом на амазоне и теперь по чтобы установить кубер на здесь тебе не надо быть великим специалиста можно просто пойти на google в двумя кликами мышки запустить культурное this кластера ну давайте пройдемся по вообще по всему то есть доктор запускает контейнер на хвосте обернитесь регистрирует регистрируют приложению жеское приложение на нескольких постах и кубер натиск вас торсом создается на клауд провайдера и что же теперь вот все можно ничего больше не делать на самом деле зачастую вам нужно несколько кудрина this кластеров и комбинация этих губерний кластеров может быть разные то есть что я больше всего встречала у наших пользователей это некоторые губер найти спа стран находится в локальном дата-центре некоторые на облачном провайдере такого чтобы кубер натиску стара были в разных облачных провайдеров это это бывает реже и зачастую это бывает когда просто люди пробуют где лучше их запускать пробуют разные модели прежде чем остановиться на какой-то одной ну зачем же вообще нужно несколько купюр на этих мастеров пойдем ся по нескольким примером то есть пример 1 он самый тривиальный это географическое разделение когда кубер нить из кластер создаётся на регион то есть если вы хотите ства ваше приложение было доступно в разных регионах вы создаете кластера один допустим в штатах другой в азии вот перед ним ставится какой-то глобальный балансировщик нагрузки которые в зависимости от клиентского а айпи адреса направляет запрос определенный кудри натиск vostro но опять же хочу вернуться к вопросу почему нельзя сделать один кудрин this кластер и развернуть его на хаст и между регионами оберните сам по себе его компонент очень любят находиться друг от друга далеко то есть в разных дата-центрах это еще ничего в разных регионах если вы попробуете оса запуск остановить то вы увидите разного сорта проблемы то есть первая проблема это будет естественно среди из базы данных и даже тоже и 5 сервер и кьюм лет и они тоже не любят находиться друг за друга далеко друг от друга вот поэтому если у вас есть если у вас использовать в разных регионах то рекомендация устанавливать кластеры на регион это географическое разделение второй разделение это логическое разделение по принципу безопасности это когда кластеры создается на проект то есть у вас может быть какой то кластер в локальном дата-центре используемый для давал обмен то и какой-то кластер может быть в в облаке и естественно каждому кластеры применяется разные степень защиты то есть кластер который production там вы можете поставить больше какого-то какого-то орит логин или фильтровать и пиа и запросы или защитить инфраструктуру лучшие не говорю что не занят защищаете до волокна на инфраструктуру но зачастую уровне уровни защиты бывают в этих call страх разные это логическое зрение по принципу безопасности 3 логическое разделение это по функциональному принципу это мое самое любимое разделение это когда создается кластер на команду у каждой команды есть свои лучшие практики и есть свои преференции кто-то любит кто-то любит какие-то стандартные решения такие как допустим использовать использовать инвест для ладан сами а кто-то хочет пробовать новые решения кто-то заинтересован в из тела допустим сервис меж они хотят использовать этот очень часто еще соединение команды и проекта для какого-то проекта нужны одни фичи для какого-то проекта нужны другие поэтому зачастую бывает кластер на команду то есть вы ответили на вопрос зачем несколько кластеров нужны как только мы ответили на вопрос зачем и мы поняли что нужда в этой системе есть мы стали отвечать на вопрос как что нужно на что нужно обратить внимание что нужно сделать первое это положить так называемый план б это сделает централизованный кубер нить из инсталлятор который будет работать везде который будет независимость от операционной системы или облачного провайдера устанавливать кубер найти s-class второе что нужно было сделать это нужно было организовать централизованный доступ кластерам когда у вас много кластеров вы не хотите идти на каждый из них и configure там какой-то один типичный друзей щен вы хотите чтобы это было все под одной крышей и сконфигурирована только и на временно чтобы делать это несколько раз и третье так как кубер нить из очень гибкий и сам по себе он не приходит опустимся лот балансе или даже тоже на и таким компоненты нужно устанавливать постфактум нужно было организовать какой-то централизованные администратора addon-ов где где администратор может выбрать что устанавливается на кубер нить из кластер и и чтобы это все было видно что установлен в кубе notice кластере вот мы строили строили и наконец построили в систему управления к оберните с кострами я признаю что эта диаграмма нашего маркетинга отдела и она выглядит что полночь то мы уже все мы уже закончили построение на самом деле естественно куча всего еще осталось сделать ну в общем и целом мы выпустили свой general оба любили ти релиз мы выпустили его весной и им уже начали пользоваться люди и начнем пройдемся по проблемам то есть первая проблема с которой мы столкнулись это инсталляция инфляция кобрина этих кластеров какие у нас требования были к инсталляции что чтобы в первую очередь чтобы можно было устанавливать кубер нити с кластер как сервис облаке для этого это было принципе очень легко нам нужно было просто от интегрироваться с из детей клауд провайдеров с такими как гул amazon тут ничего такого сложного не было второе требование было что некоторые пользователи хотят чтобы мы за них сделали инфраструктуру чтобы мы занесли развернули виртуальные машины в облаке и разворачивать класс там на этих виртуальных машинах для виртуальных машин а мы выбрали интеграцию с докер машин сейчас пересматриваем это решение - планируем использовать таро форму это еще пока находится в стадии обсуждения и третье это когда у пользователя уже есть какие-то хостеле и он просто хочет чтобы мы развернули кубер не тесто и получается что решение было принято по интеграции с облачным провайдер мисс дикий созданием виртуальных машин тоже в принципе сложности не было ну вот когда дело дошло как разворачивать кластер тут мы стали тут мы стали задумываться что же делать и муки выбора у нас были большие то есть ко мне подошел наш seo я сказал давайте ка посмотрим что вообще там есть на рынке с можем ли мы что-нибудь из этого использовать естественно такие такие вещи как cops мы сразу отмели потому что он работает только с амазона от любой зенки в спрей мы с ними поработали и мы нашли мы шли с камешком медленно имею были там еще свои какие-то проблемы сложность конфигурации было и в какой то момент мы просто решили что давайте-ка напишем свой вообще каберне this инсталлятор и естественно так как свой мы для него в сами требования можем диктовать естественно он должен работать везде на любой операционной системы в любом облаке он должен запускаться или с командной строки или с инфляцией брань шире то есть его в принципе можно использовать был независимо от rancher оправок продукта третье было мы захотели чтобы он запускал купер найти с докер контейнерах конфигурация должна быть минимальная и простая то есть мы начали с малого и и он должен быть написано и тут написано голый это потому что мы делаем интеграцию с докера мы делаем интеграцию скутер найти сам они все были написаны на голову поэтому это принципе была очень логично делать это вот и вот мы выпустили свой свой продукт который называется раньше кубер нить из 1 он сначала мы выпустили его как авт автономный инсталлятор это значит что вы просто можете его запустить как бинарник скормить ему конфигурационный файл и он развернет он развернет кластер на тех оставь которые вы запишите в этот конфигурационный файл весь код опять же open source у нас нет закрытых продуктов и люди стали сразу же им пользоваться и мы сразу же стали получать фидбек что принципе было очень классно потому что мы сразу могли начать фиксит баги и сразу дорабатывать этот продукт как автономный инсталлятор следующий шаг был наш интегрировать всего как драйвер вместе с другими драйверами для облачных провайдеров такие как дикие и kez эти с контейнер engine это имя этого драйвера он тоже open source интерфейс этого драйвера тоже очень простой что он создает кластер он обновляет кластер он его удаляет он его увеличивает или уменьшает это был второй шаг третий шаг это было сделать и пиа и management server надстройку над этим драйвером и как вообще это будет происходить все вот запрос как вот вообще создать кубер notice кластеров в таком решении использовать или просто приходит в нас говорит создать кластер и 5 говорит контейнер женщинам скармливать ему конфигурацию кластера контейнер engine зависимости от конфигурации выбирают какой драйвер запустить запускают кубер notice кластера и возвращает ключи доступа на ebay server server подсоединяется call стран используя ключ и и пользователь может означать создавать кубер нить из приложения нам кудрина this кластере непосредственны и тоже немного отступления то есть можно у нас была у нас была цель такая чтобы не завязывает с пользователями на наши и pr и пользователь может напрямую подсоединяется к uber notice кластеру которой мы для него раза разворачиваем и тут приходит наш самый первый большой косяк нож самая первая большая ошибка который нам стоило трех месяцев потерянного времени вот наша первая версия раньше две версии 2 продукта как она вообще была организована то есть мы первоначально написали ее на джаве с массе кулов как база данных это было сделано по следам нашей первой версии продукта у которого был свой оркестра ционный движок и пиа сервер написаны джаве он был классный то есть он работал здорово и я сама java developer а я люблю джаву реализовано представление кубер notice a и b объектов через rancher и я и это было 2 2 ключ третье мы пытались поддержать две модели создания объектов через ranger java и bio и и через нативно через key you control естественно репрезентацию объекта то что создано через keep control должно было также от своей первой и по-другому то что сразу на чейза и пия должно было отображаться как у control вот дизайн был ошибочен почему он был ошибочно на первых потому что все продукты с которыми им интегрировались они написаны на голову до актерах кубер найти тот же нас драйвера для создания кубер натиск мастеров второе то что у нас было два слоя коммуникация то есть один слой у нас это менеджмент слой который отвечает за создание кластеров а второй слой который отвечает за создание пользовательских приложений в купер notice кластеров и дизайн был просто очень отличалась между ними и как инженеры что переключиться с одного на другое это на самом деле было очень очень затратно и третье это что мы поддерживали две модели создания объектов через на темно через кубер нить из и через ранчо и pr это была куча усилия это была куча багов который нужно было фиксить и мы просто какой-то момент поняли что мы занимаемся только эти места того чтобы что-то новое создавать чтобы делать какие-то инновации мы просто optic сим эти баги поэтому мы просто решили что все даже вне зависимости от того что мы потратили столько усилий на все это три месяца ничего страшного давайте все перепишем и мы приступили к версии 2 проекта вторая версия проекта она не только управляет коберна эти с кластерами она еще и построена на базе кубер нить из что это значит это значит что мы используем кубер не тисках библиотеку база данных у нас не майся кол в базы данных у нас и cd написан ногах не сюрприз вместо того чтобы переписывать кубер нити с и пья rancher расширяет губер нити сыпь и каждый каждый и пия объект в раньше ри представлен как uber найти съели коснулись разлив внесен мы пройдемся еще что это такое все функциональные компоненты rancher а написаны как губер найти с контроля то есть мы строили себя в кудир нить из экосистему и мы и мы просто вот взяли это и или мы будем делать так как делаются на темно и кубер найти приложение uber нити само по себе это очень круто я давала перского платформа и она может быть расширена вообще начиная с контейнер ранда интерфейс и заканчивая какими-то клауд провайдеры вот но мы сконцентрируемся на и пиа сервер компоненте который может быть расширен через ipo extends через коснулись вожди финише это репрезентация и пи объекта а логика для этого каст иного объекта находится в кастомных angel рис и что же такое сервере что же такое кастомный кастомный ресурс каста на ресурс это как бы в способ строить свой ресурс кубер не тиц и pr чтобы его репрезентация не отличалась от нативного ресурса то есть либо я использую кубер нити сыпь я и либо использую клиент такой как ipsy тел если я говорю пепси тело get под это не должно отличаться от того как допустим я получаю alert который в данном случае это мой кастомный кастомный объект и пик станешь нато второй способ расширения кубер найтись и piano там есть свои сложности и есть свои проблемы кому то интересно им можно потом поговорить потому что мы попробовали и его тоже вот ему из поля решили использовать кастом ресурс дефиниций что же такое контроллер вот логика для этого кастом ресурс до финишной как она вообще работает то есть контроллер он запускается как нативная кубе найти в приложении что значит она запускается в контейнер в по девку вернитесь и контроллер подписывается в кубер нити с и пели на события какого-то определенного объекта то есть либо там но удалить папа от либо кастомного ресурсов и пользователь создаю как только пользователь создает объект через ее пьян контроллер получает уведомление о том что объект был или создан или модифицирован он выполняет какую-то свою логику и обновляет объект с результатом там есть свои лучшие практики как вообще работы с кастомными объектами и как вообще обновлять объектов кубер найтись чтобы не получилось так что бесконечные объекты будут происходить и ваш контроллер будет просто запускаться бесконечно он и нужно еще иметь в виду что над объектом зачастую работают несколько контроллеров и это значит что обновляют обновлять объект тоже могут быть несколько контроллеров и опять же там есть свои там есть свои лучшие практики самые популярные кастом контроллер из детей итак лангу и я могу с уверенностью сказать что она обновляется с той же частотой как и кубер не this yet я из такой же внимание ей уделяется как кубер не tissue и поэтому это как бы тоже опять же была главным фактором почему мы решили остановиться на гол я знаю что есть из дикие j'avais python им я не знаю насколько насколько они насколько они обновляются часто и сколько поддержки сколько поддержки они требуют и теперь если мы посмотрим на архитектуру rancher а с высоты птичьего полета мы видим что есть два слоя первый слой это management controller и которые отвечают за за разворот кластеров для сбора статистики для проверки статуса кластеров как они там живые или ниже умерли которые делают аутентификации и есть есть слой который user кластера контроллер с которой отвечает за коммуникацию с губернаторами непосредственно что они делают они устанавливают на трек полисе какие то она делают но делается не операции с кубер не this secret но в чем вообще прелесть тут мы поняли насколько это классно то что мы используем одну и ту же модель контроллеров и ко сну ресурсов потому что логика в менеджмент контроллерах и views are контроллерах друг от друга теперь они отличаются то есть те же принципы используется и если как инженера вам нужно перейти с разработки одного слоя на другой то это абсолютно никаких усилий не требует иногда зачастую даже по ошибке втыкаю контроля там менеджмент слоя только потом понимаю что я сделала что-то не то потому что все выглядят одинаково вот и так теперь вот можно пройтись работу над ошибками и делать это я всегда люблю чтобы что мы для себя вы если для из этой первой стадии разработки rancher а первых нужно быть осторожным когда выбираешь какой-то срок парень компонент для ключевой части продукта то есть если бы в самом начале мы решили не написать свой инсталлятор а мы решили бы использовать какой-то сторонний инсталлятор это было бы неправильным решением почему потому что мы поняли что тот то то количество доработок которые мы делаем то количество внимания которое мы уделяем именно инсталлятору это было бы очень сложно сделать каким-то сторонним продуктом и нам проще контролировать какие-то релизы нам проще где нам проще принимать решения если бы мы выбрали какой-то стороннику вернуть из мотеля то чтобы скорее всего мы сделали если вы остановили выбор на этом скорее создали свое свою ветку и скорее всего мы бы какой-то момент там не знаю три сорок процентов вы переписали под себя что в принципе не имеет смысла второй урок который мы для себя вынесли это если возможно выпускать часть проекта автономно то есть так как мы свой инсталлятор выпустили сразу как автономные команда intel который может запуститься независимо от rancher а но опять же я сделаю небольшое признание здесь это немножечко нас подвело в плане того что очень сложно делать дизайн проекта который может запускаться и в интеграции и автономно потому что концентрация всегда идет все равно на каком-то одном компоненте сейчас нам нужно его делать небольшую ри архитектуру потому что он изначально был написан как com онлайн тв и в интеграции там вылезают небольшие проблемы но это ничего то есть мы для себя сделали урок и мы продолжаем двигаться и третий урок это наш самый болезненный был это всегда нужно рассматривать возможность использования языка и технологий системы с которой происходит интеграция да это не просто я сама java инженеры мне пришлось перейти к 3 пришлось перейти на гол какие-то проекты я нога писала и раньше но это не была в таком объеме как сейчас ну ничего мы привыкли и в принципе в этом есть большие плюсы вот такие ошибки такие выводы мы для себя и сделали проблему вторую которую нам нужно было решить для себя это как делать аутентификацию и авторизацию в куприна this кого страх учитывая то что кубер нить из кластеров у вас может быть очень много что нам нужно было сделать то есть мы опять же начали с требований что нужно было сделать нам нужно было создать унифицированные аутентификатор и и задачу и здесь была такая чтобы чтобы пользователь мог конфигурировать все вещи опять же только один раз он не должен был идти на каждой кубер notice кластеры там запускать от мишин в пфу с какие-то интеграции делается с посторонним аутентификатором провайдером он должен был это делать один раз и чтобы это все чтобы это все делалось применялось ко всем кудрин одесского старом второе как только аутентификации сделано нужно было делать какую-то авторизацию лоб управление пользователями их правами то есть какие-то пользователи должны были администраторы должны были иметь естественно больше прав кто то они должны были уметь создавать кластер в какие-то пользователи только должны были делать запуск в пользовательских приложений и мне нужно было видеть все это административную часть 3 нам нужно было разработать систему защиты инфраструктурой от несанкционированного доступа что это значит чтобы даже если user аутентифицирован и он уже в системе чтобы защитить защитить систему от чего-то вот что вот using что пользователь мужа сделать по ошибке допустим открыть какие-то партийных а стен вот как мы реализовали централизованной аутентификацию rancher сидит как аутентификации прокси перед всеми кудрина this кластерами то есть весь доступ губернатор пастором осуществляется через нас как только запрос приходит на наш и пиа сервер мы мы осуществляем от инспекцию с одним из провайдеров ответишь на провайдером таким как active directory гид хоп pink израиле у нас их очень много и продолжают валяться по запросам по запросам пользователя вот как только запрос аутентифицирован мы его направляем в культурный кластер если кому-то как это сделано мы используем кубер нити сервис аккаунт и мы делаем и принц реализацию запроса мы добавляем туда пользователя и группу запрос чтобы дать кубинец понять кто выполняет это запрос как вообще осуществлена авторизация в раньше то есть мы решили взять за базу arbok авторизацию в кубер нити как arbok авторизация кубе найти сработает это прописывается так называемая роль в роли прописывается что конкретный роль может делать в данном случае конкретный роль может может делать git лист может читать информацию про кубер нить и спады только роль создана если вы хотите дать какому-то пользователю доступ к этой роли делать то что то роль прописывает создает так называемый ролл бай лин этот ролл банин создается для кубер нить из пользователя юзера или для группы или лесу с аккаунта для каждого для каждого пользователя создается 1 орал баней и как мы решили делать осуществить arbok авторизацию в раньше мы решили расширить а не заменять кубер найти с arbok что это значит расширить мы ввели концепцию проекта способа группировки namespaces вообще как используется на имс месяцев кубер найтись зачастую вас такое бывает что одни что что разные namespaces они принадлежат к одному и тому же к одному и тому же проекту а так как на тип навку вернитесь и arbok можно применить либо глобально либо какому-то конкретному на использую что получается что зачастую к одному как разным namespace он применяются одни и те же arbok правило это очень не удобно для системного администратора поэтому мы ввели концепцию проекта в как костюм рессор за финишем для группировки namespace с в кубе рн и сиси и arbok роли правила создаются а проект мы не лимитирует это то есть можно сделать в принципе и на уровне namespace с но барри фонд это все делается на проект и как только пользователь добавлен проект он получает автоматическое наследование прав доступа этого проекта да все делать автоматом и вот ради интереса как реализован один из кастомных контроллеров arbok контроллер раньше то есть контроллер подписывается на событие и удаление пользователя в проект сам проект имеет какие-то arbok правило arbok shoes и как только пользователь добавлен в проект контроллер копируют права доступа в каждый namespace который принадлежит к этому проекту автоматом на удалении то есть как только юзер удален из проекта его права удаляются очень простой тип знает но логику контроллеров брань через контроль это зачастую один файл он небольшой его логика его логика очень просто как только авторизация аутентификация у нас было решено мы стали смотреть на вторую проблему зачастую защиты инфраструктуры от своих защитное на не очень четко и слова но все же так же важно как защита на уровне икеа и от чужих то есть наверно у каждого что это бывало что по ошибке очень много записи на диск делались или все порты на кости открывали у меня такое бывало вот поэтому мы стали смотреть что же как оберните что же в кубер найтись и есть какие у него есть концепции которые мы можем использовать чтобы сделать эту вот защиту инфраструктуры теперь она что мы посмотрели это было кубер найти с после чего эти policy это это полисе предотвращает может предотвратить запуск контейнеров с рук привилегией может ограничить запуск доступ контейнера к определенным диском может запретить контейнеру открывать порты на костей и там много других много других правил что мы решили сделать то есть так как это в кабинете все уже было мы стали думать а как же сделать вообще rub это администратора легче что можно еще придумать от поверх этого подтягиваете полисе и мы придумали концепцию подцепили полисе template это как бы правило это это это темп может быть сконфигурирован дина временно администратором и он может быть применен либо к телу кластеру либо к индивидуальному проекту если допустим вашей организации вы решаете что какой-то определенный проект у него должны быть какие-то большие права или меньше и право вы можете создать специфичную потеки у эти полисы template и применить его проекту и что это значит это значит что все контейнеры в проекте или ибо все контейнере в кластере они унаследуют эти все правила которые прописаны в подсаке или template потом после то после этого мы стали смотреть на кубер не теснит в поясе и что ж это такое то есть по умолчанию вообще все контейнеры в кубе раньше они могут разговаривать друг с другом имеет доступ друг друга и конечно если у вас несколько проектов вашим кудри натиск love story это не то это не то что вы хотите видеть зачастую администратор ходит как то это контролировать очень хочет сказать что какие-то определенные контейнеры могут разговаривать не со всеми другими контейнерами а допустим с каким то определенным namespace и он хочет конца и администратор хочет контролировать эти права доступа на прополисе он позволяет прописать какие то правила и применить их либо на базе namespaces либо используя самую любимую кудр не this model at a level за интеллект раз что очень любят инженеры это дает эта достаточную гибкость ну как мы посмотрели наши пользователи это не очень любят пользователи хотят видеть что-то четкой они хотят видеть к чему что применяется и что мы сделали что мы расширили мы расширили и на прополисе из для поддержки молод и пенсий и мы применили на трек policy к проектам то есть тот проект тот сербии проект который мы создаем что можно сделать можно сделать так прописать что на прополисе говорит что какой-то определенный проект не может разговаривать с другим проектом в данном случае на примере ясно показала пилотный проект и стабильный проект и namespace с в одном проекте они могут разговаривать друг с другом но они не могут разговаривать с другими проектами и тоже опять же реализация была выполнена на базе на базе кастовых контроллеров которая конфигурирует network policy фак оберните скала страх базе проектов и опять же сделаем небольшую работу над ошибками что мы для себя из этого вынесли что всегда конечно нужно пытаться расширить а не заменить ключевую функциональная система которая происходит интеграция in grand и сделать не просто это решение принять не просто и если вы решаете использовать что-то из продукта интеграции следующий вопрос который приходит гула чем вообще можем сделать чтобы чтобы улучшить это ли как-то это дополнить и часто очень шаблоны часто используемых конфигурации она может выгляжу как что-то тривиальные но на самом деле наши пользователи очень благодарны нам за эти шаблоны и мы их стали использовать очень много для чего мы стали их использовать для того же оберните с кластером модели для для х стала для продвижения нгаха 100 то есть шаблону мы используем очень много и то есть мы прошлись по продвижению мы прошлись по 13 кечинов 3 женщин и сейчас придем к проблеме номер три это управление кубер нить из купер найти с аддонами такой kuben this addon то есть по себе кубер нити сон приходит только с ядром где есть по моему по умолчанию только один с а все остальное она ставится как нативный комбинате в приложении все нативных убираются приложение они запускаются обычно в контейнерах в падах и они расширяют функциональность кубер найти сам зачастую они запускаются как нативные к принцип приложений это значит что как остальные контроллеры это значит они создают все ардис это значит что их логика выполнена как выбирать из контроллер который слушает на какие-то события в каменной степи и программируют допустим тот же индекс контроллеры они программируют какой-то лот bouncer и вот и это дает большое преимущество потому что об греет а дона может выполняться независимо от апгрейда самого кластера то есть для того чтобы проблеме в dns вам не нужно там перезапускать кобрина this кластеры тоже относится к другим аддоном перейдем к к тому что addons в облачных провайдеров каберне tissot они не совсем гибкие и прозрачно во-первых они предустановлены во вторых их не рекомендуется модифицировать без модификации снова кластера без перехода на другую версию вернетесь то есть облачный провайдер он можно что-то делать за вас но вы этого не видите то есть это непрозрачные апреле то есть вы не видите какие у вас такие у вас и мы через используется для этих addon-ов да и самих addon-ов принципе не видно то есть она замаскировано wiha и пья и допустим в google и это написано си си ван гог bouncer и но вы не знаете как он вообще где он упаковано и спейси запущен какое и матч используется для этого для этого аддона что мы хотели сделать мы хотели установить важное addons по умолчанию для тех пользователей которым неважно какой кто у них там делаю гена сколь dns или sky dns неважно какая версия запущена и не нужны никакие дополнительные параметры запускать но мы хотели предоставить кастомизацию как альтернативы потому что многие пользователи хотели океане говорят вы запускаете вот engine x им раз контроллеры для лот баланса мы хотим передать ему такие параметры можно ли это сделать и мы предоставили такую возможность еще мы хотели четкой унифицирована показать какие системные сервисы запущены в кластере какие и мы час использованы для их запуска какие параметры какие параметры им переданы и addons в кадр не this кластерах который разворачивает rancher они конфигурируемые можете рассказать пример я мол конфигурации для нашего кластера где прописан провайдер индекс для ingrosso их можно поменять после установки если по какой-то причине вам не подошел инварс контроллер engine.exe игра с контроллера вы можете поменять его на что нибудь другое более того вы можете его отключить вообще запустить вне зависимости от инсталляции то есть уже можно постфактум сделать как мы это реализуем мы делаем а установку кастомных addon-ов через наш каталог что это у нас интеграция с холмом происходит на просто делаем для него для него гурий где просто можно выбрать по клику можно увидеть детали того что вы устанавливаете и установить приложение через каталог опять же работа над ошибками проделаем то есть для простых для простых кейсов установка addon-ов по умолчанию она она хороша то есть пользователь не задумывается о том что у него станет он просто получает dns он получает какой-то балансировщик он получает над freak но есть кейсы которая требует более более кастомной более костанай инсталляции для addon-ов и мы хотели каким-то образом предусмотреть две модели и получается так что у нас на самом деле это использована на пятьдесят процентов в одном 50 процентов другом то есть мы уделяем одинаковое внимание всему одну в прозрачных системных сервисов тоже очень тоже очень важно то есть гибкость она не должна заменяется прозрачность то есть то что вы можете остановить addons независимо от кубер нити sanyo и не должно приводить к тому что вообще не можете видеть как администратор не можете видеть что у вас там запущена какая dns провайдер какой какой ingress контроллер таким уроке мы для себя вынесли и что было самое главное чему мы старались следовать вообще самого начала мы не хотели привязываться к конкретному облачному провайдеру и мы не хотели а лимитировать юзера пользователя чтобы он привязывался к какому-то какой-то операционной системе то есть так как допустим окон shirt привязывает к мы не хотели привязывать пользователя к нашим и peas & white то есть вы можете использовать нас просто как инсталлятора кубер notice инсталлятора а подсоединяться к верните су используя их гипс и тел и мы хотели расширять они заменяются функциональность кабинете со и вот эти три пункта это те пункты которые учетом ошибок которые мы совершили это то что не изменилось в принципе и мы следовали этому это было очень важно то есть когда мы взяли что вот это вот мы в конце хотим сделать решение принимать было гораздо легче дома time попробуем всем видно хорошо или сделать побольше нормально окей так я сейчас нахожусь внутри и считаю на амазоне и здесь я буду запускать rancher а в конце арчер сам запускается в контейнере это самая базовая версия инсталляции сейчас наверно нужно убийца контейнер 1 я убью предыдущий контейнер я удалю все valens запустит что у нас там удалим все диски и запустим rancher по-новому так я буду запускать нашу еще пока неофициально поддерживаю версию так принципе занимает даже меньше 1 минуту запустить вот тут пятна сразу что раньше создает все сервис которые надо запускать какие-то драйвера вот для виртуальных машин amazon и езжу ради этого уж он и теперь вот после теперь можно начать смотреть на ивой видно отлично данный класс то и я не запустил этот кластер которая только что который только что развернула не запустил не тот раньше а другой то есть раньше в котором у меня уже есть парочка кластеров один из них это классно google а другой на даче the low sheen покажу пример как добавить новый кластеры то здесь мы представляем все свои драйвера которые можно выбрать то есть мой самый любимый это видит ложь и что самая простая у него конфигурация здесь я прописываю все свои arbok роли которые могут изменять этот кластер это место где я прописываю какую кубер notice версия хочу поставить у себя в кластере кокаин от рук правая хочу использовать для для турок изоляции и могу выбрать хочу ли я делать и над враг изоляцию между проектами или или в принципе нет опять же куча других параметров которые можно модифицировать а здесь это выбираем конкретно для виртуально для для виртуальных машин для проведения где я хочу устанавливаться виртуальной машины в данном случае это будет видеть лашин и я выбираю конфигурацию даче сложно для виртуальных машин я выбираю роли какие эти машины будут играть то есть в данном случае я хочу установить все поверните с компоненты на одном костя что в принципе неправильная модель но для давала пинта для тестинге а эта модель быстрая на в принципе годится нажимаю credit и она будет создавать все виртуальной машины разобрать что ним куда нить из это в принципе займёт какое-то время пару минут даже вот и в итоге получается вот такие вот кластера где можно просмотреть их всех в одном месте и теперь как же теперь нам нужно достучаться до этих кластеров если я пользователи хочу создать свою свое приложение на кубер нити у нас есть две модели то есть можно либо достучаться к ним нативно через fib сериал и это можно сделать либо через gui попробуем показывает либо можно взять конфиг файл и запустить его у себя на десктопе на моменте опять я хочу хочу сказать что себе есть доступ к верните с кластером вся аутентификация происходит через ranchers то есть этого rancher сервер он выступает в роли прокси и вот этот вот адрес это адрес не вашего кубинец костра это адрес кудрин это адрес раньше то есть либо клипсе тел через com онлайн либо можно пройти наш нашим двое тоже можно установить пользовательские приложения вот здесь вот под каждым к острому нас есть вид проектов то есть у нас есть дефолтный проект есть системный проект системный проект это где можно увидеть все все какие-то системные сервисы которые запущены в кабинете с кластере такие как наши сервисы системные какие-то логин dns и не раз контроллер а в дефолтном проекте это уже users к и приложения и тут у меня им запущен какой-то deployment engineer deployment и опять же это все это все переходит это не наша это наша и piano они переводят все в кубер нить из в купер на эти субъекты то есть когда я делаю до плеер apple id чтобы происходит мы запускаем купил найти тот момент что мы еще одну классную штуку делаем то есть мы делаем что-то extra af купились если вы знаете чтобы запустить ваше приложение и чтобы его предоставить для доступа должна создать два объекта нужно создать диплом это нужно создать сервис мы это сворачиваем в одной и перины нашей стране и за сцены мы создаем сервис то есть я могу здесь просто сказать что я хочу свою я хочу сделать дипломант я хочу чтобы он был опубликован на какой-то определенный порт nano vapor трендом то есть мы воссоздадим в таз и и этот порт на котором он опубликован его сюда поместим и что просто по то есть мы упрощаем работу для пользователя он не должен он не должен сам создавать сервис мы это сделаем для него и мы врать ему в точку доступа для его приложения а у меня все спасибо спасибо за то что пришли алена спасибо большое на самом деле у нас до завершения времени остается еще две минуты поэтому на один из вопросов мы успеем ответить здесь остальные же discussion здравствуй спасибо за ваш доклад хотелось спросить если раньше какие-нибудь интеграцию с continuous delivery инструменты есть я могу в пьяном перерывы показать у нас есть интеграция давайте еще один добрый день спасибо за доклад такой вопрос а есть ли примеры меньших классов мишка схема взаимодействия то есть норе запуска под в одном классе и хочу в январе на перемены пробросить entry point к сервису другого глостера это интересно что вы спросили а это потому что мой следующий проект на которым я буду работать это момент лот балансиры и только возможность возможность за deploy applications разных в страхе возможность представить точки доступа через через кластера и также у нас в планах это даже коммуникация в коммуникация по на творог между кластерами то есть сейчас такой прозрачная прозрачно проброс на двор к нет пока прозрачного проброс и нет но это будет вот в следующей версии которые мы планируем делать где это будет в начале следующего года и альфа-релиза у нас будет где-то в декабре"
}