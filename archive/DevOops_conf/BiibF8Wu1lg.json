{
  "video_id": "BiibF8Wu1lg",
  "channel": "DevOops_conf",
  "title": "",
  "views": 0,
  "duration": 0,
  "published": "",
  "text": "всем привет ещё раз так уже сказали я занимаюсь тем что строя платформу компания вид и работаю в unity у нас называется архитектура исторически вот и сегодня как раз расскажу именно про эту платформу почему мы называем пас платформа как сервис вот и в том числе еще перед началом хочу сказать что до печати в телеграме будут опросы с помощью которых мы в конце сможем посмотреть общее распределение по сегодняшней аудитории кто как строит платформу такие общего у кого подходы и вообще подведем итоги потому как какие есть варианты построения вот теперь давайте начинать начнем традиционность план а он на самом деле структурно небольшой во-первых посмотрим что такое платформа что он под не понимаем то есть какие вещи далее рассмотрим уже концепцию пас то есть почему платформу называем именно таким словом и затем уже будет такие более технические вещи то есть посмотрим какие проблемы у нас возникали при построении платформы какие технические решения мы использовали чтобы эти проблемы устранить ok давайте начинать зачем платформа какие вообще есть вещи на самом деле достаточно очевидные моменты но тем не менее давайте о нем пройдемся первое это естественно снижение оверхеды на интеграцию утра структурой то есть если мы вводим какие-то инфраструктурные вещи со всеми этими инструментами нужно уметь интегрируется и каждому из продуктовой команды нужно найти какие-то дополнительные вещи touring чтобы полноценность интегрироваться и использовать какой-то низкоуровневый инструмент в следующий момент это перри использование лучших практик когда мы внедряем какую-то новый подход новую технологию если у нас нет какого-то единого места единой платформы то внедрить на всю компанию какую-то практику достаточно сложно то есть нужно сходить там на все команды рассказать почему это так нужно делать или так хорошо делать и затем это все распространить полноценно и еще такой связанный со вторым пунктом момент это возможность вообще контроля централизованного различных технологий и решение которые не являются в компаниях потому что можно достаточно просто упустить момент когда в компании растет и в итоге в ней порождается большое количество разного рода решения которые решают на самом деле одну и ту же задачу получаем некоторого рода такой запах вот платформа призваны решить эти проблемы и именно такую платформу мы строим а давайте сначала прежде чем мы нырнем уже в такие подробности и поговорим про пас я расскажу еще немного про особенности нашей системы мы полностью все железо имею свое то есть не использую каких-то облачных провайдеров соответственно весь софт какие-то решения обвязки это все решения которые хочется у нас на нашем железе то есть здесь опять таки нет каких-то внешних вещей вот это могут быть окон ссор с решения но они полноценно работают у нас вот и облака которые построены лежит как бы в основе платформы она также наши не какое-то внешнее ok давайте рассмотрим какие вообще были так скажем виды изменения в нашей платформе то что было 4 года назад это к чему мы пришли примерно 2 года назад и продолжаем идти сейчас развиваю платформы вот если посмотреть на инфраструктуру инфраструктуры состоит из множества разных инструментов и каждый из этих инструментов имеет некоторых свой интерфейс соответственно с каждым из этих интерфейсов нужно разработчикам либо даже инженером там платформенных команд уметь интегрироваться и как-то собирать их в единое целое чтобы использовать их эффективно и соответственно там правильно использовать работе вот и мы когда начинали строить платформу централизованного vita пошли таким путем что взяли во-первых видик acquisto торгу вернитесь причем это был самый ванильный каберне this который был просто вот установлен как есть и соответственно в таком виде он и поставлялся в продуктовой команды самом камне трон там вот для того чтобы как-то ограничивать его использование мы использовали стандартные механизмы кубер надя сыр баг с помощью которых как раз контролировали каким разработчикам какие есть доступные фичи что можно использовать в этом каберне тисе вот далее все обрастала вокруг этого регистратора то есть какие-то вещи которые мы могли автоматизировать и видели ботаники мы их автоматизировали вида там например по и плагинов для тепло и сервисов континент играешь на системе и так далее какие то такие вещи которые лежат на поверхности вот и но в итоге разработчики использовали низкоуровневый интерфейс вида там hell для тепло и фитиль для того чтобы работать кубер нити сам и по сути являлись некоторого рода в том числе и администраторами этой системы потому что полноценно занимаюсь по сути всем циклом начинают разработки и заканчивая диплом в продакшн использование ска уровневой инструмент к чему мы пришли спустя два года после развития мы поняли что на самом деле все вот эти инструменты чтобы их использовать действительно правильно и эффективно разработчикам нужно потратить некоторое количество времени и с ростом компании когда у нас становился больше и больше инженеров мы увидели что это начало становиться проблемой действительно специалист которого мы находили снаружи не всегда знал весь от технологий которые у нас использовался и в итоге это все занимало достаточно большое количество времени чтобы решить проблему мы решили изменить немного курс развития направление нашей платформы и пришли к такому названию которым мы сегодня будем разговорить это платформа как сервис платформы как сервис по сути это такая эволюция того что мы делали до этого но она немного отличается от того что было раньше то есть эти вот низкоуровневые инструменты как это вообще все поддается пользователям первый момент все какие-то рутинные операции которые мы можем автоматизировать мы автоматизируем все что нужно делать там изо дня в день далее мы рассмотрим на платформу никак на какую-то инфраструктуру которая состоит из множества кубиков которые нужно складывать воедино а мы смотрим на платформу как на продукт который решает какие-то реальные потребности закрывает боли и ведем его по сути как такой продукт в продукте lavita бизнес внутри бизнеса и естественно обеспечиваем низкий порог входа него потому что одна из таких еще основных проблем платформы том что когда приходят новые разработчики в компанию им действительно нужно достаточно большое количество времени чтобы адаптироваться поэтому пас подразумевает низкий порог входа и взаимодействие со всеми компонентами простое вот и в идеале это 0 overhead вообще интеграции инфраструктурой то есть какие-то вещи которые нужно именно проводить со стороны разработки они все должны быть автоматизированы со стороны платформы вот не все это можно сделать автоматом но большинство вещей можно закрыть стороны платформы вот и как раз вот таком подходе мы и будем говорить будем сейчас говорить о том что у нас получилось и если смотреть со стороны пользователя стороны интерфейсов то пользователи есть два основных интерфейса первое это утилита авито командная через которую можно сделать на самом деле практически все действия которые у нас есть платформе то есть начиная от создания сервиса заканчивая и уже там его эксплуатации выкатка и так далее и второй инструмент уже такой инструмент визуализации этапа hd zboard который дублируют некоторые фичи авито утилиты но в том числе и позволяет посмотреть еще дополнительные различные вещи о которых мы посмотрим чуть позже вот такие два интерфейса торчат наружу все все остальные все инструменты которые были раньше там холмы пепси теле какие-то там настройки там энгр из контроллеров прочего все это инкапсулированы и скрыта за вот эти вот крышечкой красивый видео от этих двух интерфейс ok чуть-чуть затронуть тему как мы вообще к этому пришли то на самом деле если посмотреть на вопросы которые к нам приходили в саппорт и было большое количество и вот это вот некоторые из них на самом деле если просмотреть за годы количество вопросов которые нам задавали в саппорт по разного рода проблемы можно вот этот слайд закрасить там я думаю в десятых слоев и причем пять вопросов повторяются часть вопроса какие то просто и острые углы которые с которыми все время сталкиваются разработчики вот и что здесь интересно каждый из этих вопросов она отнимает не только время со стороны платформы то чтобы разрулить и причинить какую-то вещь сделать более удобный это в том числе и время которые тратят продуктовые разработчики для того чтобы решить свой бизнес задач вот и сейчас уже мы переходим к технической части я здесь предлагаю идти в таком стиле что мы будем смотреть сначала проблему то есть какая была проблема в определенной области и далее будем смотреть как мы ее решали но чтобы придать так структуру как я сейчас буду рассказывать мы будем идти по жизненному циклу разработки и в качестве жизненного цикла выберем вот такие этапы то есть это сознание сервиса разработка тестирования на этом о третьем этапе у нас заканчивается по сути такая основная работа разработчика здесь уже у нас есть какой-то готовый бизнес код который можно дели верить и далее вот четвертый пятый пункт это уже ближе к такой operational части это доставка сервисов и непосредственной эксплуатации вот на каждом из этих этапов у нас есть определенные боль и проблемы которые возникали и мы как-то решали давайте начнем с первого первое это создание сервисов вот здесь в принципе каких-то таких супер автоматизации придумать сложно поэтому давайте вообще сначала на посмотрим какие есть боль и первое это создание репозиторием казалось бы простая операция зашли там бит bucket гид лап что используется и создали репозиторий потом там пошли зарегистрировали системе в чем-то после этого создали еще редко 10 различных ресурсов и получили готовый сервис вот кажется что не проблема 20 минут потратили и все ок но на самом деле если посмотреть на вот эту вот операцию она происходит каждый раз и здесь с одной стороны можно каждый раз забыть какой нибудь создать ресурс сделать что-нибудь так и с другой стороны это такой моральный барьер который стоит каждый раз перед разработчиком я ему нужно создать новый сервис вот поэтому чтобы решить эту боль мы решили до автоматизировать процесс путем вообще сведение этого всего в одну команду под названием авито сервис клеит которая проводит через специальный wizard спрашивает какой нужен шаблон спрашивает еще там пару простых вопросов и после этого создает сервис со всеми нужными уже ресурсами автоматом в итоге пользователь сразу же получает там категорией получает полностью готовы и вся и pipeline и все обзор ability ресурсы которые ему нужны политики там настройки доступа извне всякий ингрид правила правила фаерволов это все сразу же автоматом прикидывается делается через одну команду в итоге experience создание сервисов тут конечно сильного улучшается ok следующий момент это разработка сервисов вот здесь уже больше различных моментов таких движимых частей давайте посмотрим какие есть основные первые это структура сервисов если дать всем разработчикам возможность пилить сервисы с нуля полностью то мы получим полностью разные сервис если они будут отличаться практически во всем несмотря на то что паттерны подходы будут использоваться примерно одинаковые выглядеть и они все равно будут все по разным какие-то там метрики лаги там интерфейсы и внутренние снаружи они все будут не похожи друг на друга и в итоге мы будем получать долгий вход то есть когда мы хотим пойти по комитет что-нибудь заплыли квесте в чужой сервис это настоящая проблема нужно разобраться потом сделать там pull request не там уже получить какие-то изменения реальной вот это все долго и достаточно больно вот поэтому здесь достаточно такое очевидное решение это унификация структуры мы сделали единую структуру для каждой технологии то есть такой некоторые шаблон в котором уже есть поддержка всех основных инструментов например там метрики логе всяких чеки поддержка авто генерации кода для офисе взаимодействие асинхронно взаимодействия все такие вещи они сразу же автоматизированы под каждую из технологий которые мы используем в компании плюс это еще и такой момент который позволяет контролировать вообще зоопарк и именно распространять конкретные технологии которые уже опробованы в компании и следующий момент уже такой более технически интересный эта конфигурация тут у нас был достаточно такой длинный эволюционной путь сначала мы начинали стандартных таких playing у бернадетт манифестов которые разработчики даже сами писали загружаю специальной репозитории далее с помощью администраторов раскатывали их там нужно и окружение нужные кластер а вот спустя там буквально полгода даже наверно меньше мы перешли на home чарте я думаю большинство используют для тепло и именно инструмент hell вот и мы так же использовали инструмент home долгое время и сейчас на самом деле для части еще сервисов его продолжаем использовать вот хан позволяет объединить множество манифестов такую единую коробочку достаточно удобные диплом вот и в итоге после того как мы где-то спустя пару лет посмотрели на то как выглядят эти hal манифесты мы увидели что на самом деле в каждом репозитории рядом с каждым сервисом у нас есть директория home которые вот занимает примерно 40 кило байтов такого чистого ямал текста всё как мы любим и вербальное описание для diplo & service а вот и самый интересный момент что когда мы подъехали эти все home директории между сервисами мы увидели что на самом деле div между ними это буквально несколько строк большинстве случаев так как мы распространяли их вместе с шаблонами то разработчики там мы меняли там несколько строк которые именно отличают настройку этого конкретного сервиса а все остальное она была примерно одинаковые вот в итоге мы посмотрев подумав что же с этим можно сделать когда у нас одинаковые манифесты хранятся в кучу репозиториев и нам при изменении какой-то опции во всех сервисах нужно пойти и сделать там 500 тур request of дождаться там их оправ мер живых ли выходки мы пришли к тому что хорошо было бы вынести эти движимые части какой-то один единый маленький манифест и таким манифестом у нас стало птому если посмотреть на talk он выглядит а это такое минималистичное описание в котором есть именно те самые движимые части то есть то что специфично этому сервису а вот эти вот все огромные манифесты они уже могут автоматически генерироваться на основе вот здесь есть окна специальная секция engine движок по сути технологии с помощью которой мы понимаем точно какие нужны манифеста этому сервису вот и уже соответственно можем конкретно сгенерить манифест под нужную технологий вот если говорить про оптом да как вообще из чего он состоит и его мать и мотивация создания туда как я уже сказал это только движущиеся части следующий момент почему-то молда мы привыкли работать с я малом у нас полная мула и тут вдруг мы вводим формата ну дело в том что формат там он позволил нам выйти во первых от такой вложенной структуры достаточно сильно который у нас был объявлен то есть такие действительно манифесты которые подразумевали использование линейки чтобы понять там насколько нужно табов сдвинуть нибудь очередную настройку чтобы она корректно заработала вот мы и вот эту вот вложенность убрали с помощью тома плюс решили еще проблему с окружениями случае с холмом мы использовали там были стейджинг вылез против здесь это уже не нужно было здесь прямо все в одном файле просто описывает а минус . окружение и пишем нужную опцию это все в пределах одного файла вот и всю конфигурацию мы начали дело с помощью переменных окружения теперь такой интересный момент как же с этим об том мы живем тут на самом деле все достаточно просто и очень гибко на мой взгляд это было такое одно из ключевых решений которое позволило нам очень сильно упростить вообще разработку платформенная в плане вот манифестов создание манифестов тут мы сделали выделенный сервис под названием хелген вот он исторический называется хауген потому что он всегда генерировал именно home манифест и вот сейчас как и мы видим на слайде здесь уже он отдает кубер нити с манифесты вот михаил манифеста но название у него такой же хауген вот в чем смысл он принимает на вход видим автомолл и пропускает через себя все настройки которые там есть и отдает уже готовые к применению в кластер кубер натиск манифесты которые выкатываются диплом диплом мы поговорим еще чуть позже если так смотреть высокому уровню что вообще коллаген делает первое это он является сервисом то есть у него есть теперь и он един для всей платформы соответственно все новые фичи которые мы делаем то есть к примеру захотели подкинуть всем сервисам там какую-нибудь новые переменные окружения или захотели там добавить новый сайт к заметь еще кого-нибудь штуку эта фича сразу же становится доступна для всех сервисов при следующем тепло и манифесты будут сгенерированы уже вот по новой схеме нужно вот он на вход принимает оптом и в итоге его процессе ты отдает готовы уже для применения в poster манифесты вот это очень удобная гибкая штука вот единственно 10 на такой неприятный момент что если вдруг у в нем что-то сделать не так то мы сразу же ломаем манифеста для всех сервисов вот поэтому мы его покрываем достаточно мощно тестами особенно snapshot тестами и смотрим как меняется манифесты после изменений его кода вот какая следующая у нас была боль еще это боль с управлением секретами мы исторически используем инструмент world и для того чтобы подключать интеграцию с ним всегда использовали специальный hal манифесты готовы которые подкидывали как раз вот это вот он директорию сервисом вот здесь есть несколько проблем первое это во первых нужно понимать как работает в лт далее нужно знать как именно в его древовидной системе нужно разложить ключи путям и вот это вот такой казалось бы не очень сложный пункт но правильно подключить интеграцию в hell на самом деле здесь очень много подводных камней достаточно что-нибудь неправильно скопипастить и все разваливается вот и и что и в этом плохого что на самом деле узнать корректность можно только в продакшене потому что даже если твои сестры денги манифесты все равно там немного отличаются между окружениями поэтому полноценно можно увидеть что все разнесло только уже в продакшене вот как мы с этим боремся мы приняли такой момент что секреты это по сути часть конфигурации соответственно с помощью пастер барда которая показала уже в начале разработчик заходят и просто вносит севилью значение по сути как переменное окружение вот авторизуюсь там через dex используем dex для авторизации кску бернадетт и далее сервис расклад раскатывается уже в свой namespace который соответствует его имени и это как раз есть такая security единица по которой мы автоматом понимаем какие секреты нужны именно этому сервису в итоге в ран тайме уже сервис полностью сам там атом получает все нужные ему секреты вот чтобы понять как это все проходит здесь лучше рассмотреть это на картинке как как здесь идет взаимодействие мы написали такую утилиту под названием world and по сути такой небольшой гол binary написано гол который стартует перед каждым сервис то есть вначале когда у нас поднимается сервис сначала запускается world and вот он поставляется у нас с помощью базового образа и запускается в самом начале как как работает получение секретов он с помощью дефолтного keep talking идет волк волк его авторизуют понимает кто это и далее отдает секреты этот болтун футере ту которая уже экспорте их для сервиса и спаунится ma-процесс сервисом и у сервиса уже соответственно есть переменное окружение которые ему нужны для работы секретные вот в итоге разработчик ничего не делает для интеграции секреты автоматом появляются какой есть еще момент это при разработке это взаимодействие сервисов вот здесь есть множество разных проблем вообще когда он переходит на микро сервисную архитектуру нас много получается стива взаимодействие нам нужно везде описывает новых клиентов для подключения к сервисам нужно не забывать обрабатывать там ошибки которые могут происходить в процессе работы там использовать circuit breaker библиотеки вообще следить за паттернами различными реализации при взаимодействии между сервисами прокидывать нужные fedora и там вообще космические вещи которые уж точно мало кто делает это там следить за тем что тайм-аут и которые мы выставляем на походные сервисных сервис действительно соответствует там не функциональным требованиям сервисов которые мы идем чтобы не было такого что мы там поставили time all 200 миллисекунд когда там этот сервис который мы идем на самом деле в базу данных ходит там только секунд какие вещи вот как мы решаем все эти вещи на уровне платформы первое это мы вводим контракты с двух сторон то есть и с клиентской и серверной стороны и на основе этих контрактов авто генерируем код в итоге этот авто генерируемый код поддерживает все эти паттерны сразу же из коробки вот мы для этого используем свой и такой небольшой формат описания под названием бриф который по сути является таким над множеством над или под множеством надо таким популярным форматами как прото 3 вт и уже может их использовать в качестве имплементации вот здесь он описывает такой минималистичный интерфейс помощью которого мы можем генерировать далее уже под любую реализацию необходимые нам библиотеки и клиенты вот выглядит таким образом если смотреть на контракты то контракты выглядят тоже очень похоже то есть у нас есть например сервис бриф это справа расположены это непосредственно описание нашего сервиса она имеет описи вузов сам который принимает на вход a и b для суммирования возвращает сумму а уже контракт со стороны клиента описывает по сути точно такой же манифест но за исключением того что он описывает только те методы и поля в структурах которые ему необходимы то есть пример вот здесь вот есть поля info право но его нет слева в описании контракта стороны клиента таким образом мы четко декларируем что мы не используем этот это поле и в авто гинрин и код она уже не попадает и соответственно со стороны сервера мы можем точно знать что это поле не используется например его даже просто удалять если не один из клиентов на него не завязался так вот даже нарушая обратную совместимость вот далее у нас помощью единой команды через завитую территории так уткин уже получается готовый клиент вот со всеми уже готовыми с брокерами установленным the time out a mi контекст пропадаешь на прочими штуками в огне и взаимодействие идет с помощью нашего внутреннего протокола разработчик на самом деле даже чтобы не задумываться о нем то есть он работает там под капотом и сразу же уже имеет в арсенале своего клиента все реализованный поттер вот синхронным взаимодействием подход точно такой же сервиса общаются ивентами которые явно декларируется с помощью манифестах тех же самых но только здесь немного другой синтаксис есть специальные ключевые слова схема с помощью которых мы декларируем какими ивентами общаются асинхронно сервисы итоги фарма тот же и сервисы автоматом получают интеграцию так называемым сервисом шины данных вот если смотреть как это выглядит архитектурно сверху у нас есть единый сервис с которым все взаимодействуют по специальному протоколу и написанному клиенту общему под все технологии и она тоже сервис database он под собой по сути инкапсулирует уже хранилище в качестве которого мы используем кафку и все сервисы работают через да тут высокоуровневые интерфейс да это бас который как раз позволяет делать все необходимые вещи цемент и между сервисный ok это что касается разработки следующий момент касается тестирования мы так потихоньку перетекаем из разработки больше в сторону уже доставки сервис на и перед тем как доставить сервис вы должны протестировать вот с тестами все не очень гладко когда мы начинаем использовать микро сервисную архитектуру и здесь вообще боли если мы начинаем мигрировать те же самые подходы какие мы использовали в монолите то это приводит на самом деле к плачевным результатам потому что это обычно большое количество сьют тестов которые там покрывают вообще весь проект в нашем случае там весь обито различными сценариями достаточно много различных intent тестов и все вот эти вот вещи они достаточно плохо перетекают на микро сервиса вот но тем не менее мы хотим все те же кейс и тестировать и получать к его прогнозируемое качество наших изменений не просто так деплоить без каких-либо тестов вот такой подход мы используем первое мы естественно стараемся как раньше это покрывать все иные дни тестами в первую очередь далее нтн тесты мы оставляем теперь только на критичные пути то есть это те пути которые бизнес критичны для авито и dmtn тестов применяем новый подход такой подход это подход в котором представим что у нас есть там цепочка взаимодействия не участвует 5 сервисов это на самом деле такое очень простой сценарий обычно там участвуют десятки ведь даже сотни сервисов которые участвуют в одном стенами и он допустим нам нужно протестировать изменения одного сервиса с 3 как мы это можем сделать мы он естественно разворачиваем в отдельном окружении тестовом и далее у нас есть 2 варианта на таких очевидных первое это поднять все эти сервисы рядом изолирована и протестировать вот этот вариант он чреват тем что во первых будет использовать достаточно много ресурсов но это ладно с этим можно допустим смириться вторая проблема то что стабильность таких тестов будет достаточно низкой вот и есть второй вариант это просто взять и пустые жинка окружении где уже все развернуто за тепло и там свой сервис и стриги проверить интеграцию тут как бы в целом севок поначалу но когда так начинает действовать все разработчики в итоге стрижек и окружения превращается в хаос и понять почему падает определенные тесты уже там практически невозможно потому что все разработчики там по сути в одной среде работает вот для того чтобы решить эту проблему мы используем сервис мышь и на уровне сети мы делаем так что когда мы хотим протестировать изменения services 3 мы now the youth and the end сценарий который полноценно хотим про достичь мы за все его запроса добавляем специальный feeder x-road который говорит о том что если сейчас мы хотим сходить в этот сервис из 3 там и на самом деле идем не настоящий сервис ис-3 стабилен и а в его тестовую копию которую мы сейчас тестируем и таким образом полноценно тестируем новую версию без изменения и слом мастеринга то есть все остальные запроса проходят так же ты раньше и мы не поднимаемся сервисы рядом тестом окружении то есть получаем достаточно стабильные и дешевые тестов вот таким образом тестируем нтн сценарий ok следующий момент доставка сервиса здесь есть тоже несколько таких важных изменений которым мы пришли со временем мы долгое время я уже сказал использовали инструмент help он действительно хороший но к сожалению у него достаточно плохой юзер экспириенс в плане работы с обратной связью то есть там почему что-то упало там и что происходит данный момент какие-то вещи если вдруг даже там что-то упало то как то гарантированно откатиться на нужную версию где все вещи они для разработчика достаточно неудобно вот но на самом деле мы жили с этим всем обложили сюда понятным инструментарием написали вокруг много всего и сделали это все достаточно удобным но однажды нас появилась задача которая нас обязала держать в пределах одного окружения на первых production несколько кластеров и тут все сломалось с холмом мы не могли достигать нужного нам это транзакционных тепло и то есть если сервис выкатывается на в определенное окружение то он должен полностью по всей системе во всех квартирах быть в одной версии вот поэтому мы решили взглянуть вообще на весь процесс деплоя как он нас происходит и увидели что на самом деле мы уже применяем готовые манифесты которые нам генерит инструментах и langen который мы уже рассмотрели то есть на не используем шаблонизация home и решили поменять подходы тепло и потому что на самом деле все что мы использовали от холма это сути keep сити али плай с ожиданием готовности сервиса вот то есть такая небольшая прослоечка вот к чему мы пришли вы пришли к написанию утилиты под названием джайв поздно к сожалению еще не зовут ансар шона но я думаю в таком на ближайшем будущем ее можно будет посмотреть на ноге т'хаб аккаунте авито тех вот но покажется давайте посмотрим в чем ее кардинальное отличие то есть что мы изменили в подходе к диплом допустим мы хотим разложить новую версию сервиса хотим это сделать сразу же для двух кластеров как мы сейчас поступаем мы в отличие от старого подхода мы не заменяем версию предыдущую а мы диплом рядом новую версию данном случае в 6 рядом с версии вы 5 дожидаемся когда во всех квартирах эта версия приходит в состояние ради после этого когда трафик у нас уже идет на полностью на версию вы 5 все еще на версия в шесть уже готова мы переключаем во всех кастерах трафик на новую версию дожидаемся когда все ок если все мог то после этого тушу предыдущую версию версию 5 данном случае таким образом достигаем действительно транзакционных дипломов единственно здесь есть узкий такой тонкий момент в том что если вы в момент переключение балансировки что-то пойдет не так то мы получим на какое-то время не консистентные состоянии но мы к этому готовы у нас есть еще дополнительный асинхронный компонент который просто и вынули восстановят справедливость и график будет посылать новую версию версию выше ность такой подход если смотреть на музыку уровне что этот инструмент у нас позволяет делать первое самое главное это молча state тепло и то есть это тепло и который происходит в несколько фаз то есть мы диплом как весь перечислено есть фазы камень этого сданных applications и фактом балансировки причем на каждый из фаз мы можем управлять даже вручную это дало нам возможность использовать такие подходы к канареечный deployment и blue green deployment и с холмом это достаточно проблематично сделать и естественно джайв итоге нам обеспечивает гарантию консистентной sti релизов между кластерами вот это очень хорошо нам помогло решить проблему с таким мыть кластер нам окружением окей мы подходим к такой уже заключительной части эксплуатации сервиса но тут есть достаточно много интересных моментов которые мы сделали со стороны платформы первое это управление ресурсами тема кажется небольшой на самом деле головной боли она у нас приносила очень много и здесь есть несколько инструментов которые мы внедрили давайте рассмотрим какие проблемы первое это вообще использование ресурсов как мы видим и здесь на левом графике у нас использования ресурсов где-то колеблется в пике 30-40 процентов вот тогда как request и то есть запрошенные ресурсы мастерах у нас там практически 100 процентов уходит вот это проблема потому что разработчика на самом деле сложно определить и запланировать нужное количество ресурсов даже если они это делают качественно начали то потом поддерживать со временем это достаточно проблематично вот в итоге мы получаем то что я показывал в предыдущем слайде если ты не эффективное использование ресурсов но еще и мы получаем деградации в продакшене из-за того что шедевр получается плохой стороны кубер notice a из шумные соседи но из-за таких тавер коммитов внезапных мы получаем деградации вот это все плохо с этим надо бороться как мы с этим боремся вот разработчик получает неприятные сообщения ему от этого больно чтобы это все побороть мы используем механизм детей это вертикально под авто с керлинг которые как выглядит мы уже на самом деле знаем статистику использования ресурсов у всех сервисов то есть сервисы они работают постоянно в продакшен и мы можем посмотреть что же было раньше какие какое было потребление вот мы берем поэтому эту статистику применяем к ней специальную функцию достаточно простая функция которая просто смотрит на тренд смотрят там 95 перед интере использования и затем генерирует для каждого контейнера необходимые request и по себе память для губернатор в итоге у нас они полностью все вычисляются на основе этой статистики есть конечно же корнер кейсы для новых сервисов для которых мы не знаем какие нужны ресурсы поэтому мы им достаточно большой запас и для таких вещей как резки гита всплески спайки мы водоем большие лимиты и народов поддерживаем всегда запас чтобы сервис если что могли из-за использовать ресурсы вот в итоге получаем действительно запросы ресурсов которые соответствуют потреблению scheduling роз предсказуемый я могу сказать что этот подход действительно работает и у нам он мешает им большое количество головной боли нас после его внедрения ушло большинство проблем с шумными соседями и с какими-то вещами что кто-то не может выглядеть вот но естественно проблема это не единственная в кубер на ti si мой рулем цепью и памятью есть еще другие параметры например сетевая утилизация как мы видим на графике вот здесь вот гигабитные интерфейсы 2 и на 1 ноги у нас утилизация в топе 400 гигабит на другой 800 мегабит ну естественно те сервисы которая находится на но здесь 800 мегабит начинают потихонечку деградировать вот эта проблема нативная поддержка в кубе rights есть только у топовой памяти есть там поддержка джипе you вот но с сетевой утилизации так вот напрямую нативно работать нельзя и к сожалению она напрямую не зависит от других ресурсов то есть ее нельзя как-то корректировать хорошо с помощью процессоры и память и вот в итоге мы получаем такую же проблему как и со стандартными ресурсами как мы это лечим но это легче на самом деле тем же самым практически подходом что мы дианы мы вводим такие же ресурсы которые называются extend ресурсы то есть расширенные ресурсы вызываем этот ресурс в случае с сетевой утилизации там моют и ruined util и его заполняем тем же самым подходом что и память и процессор на и время в итоге получаем решение этой проблемы и под-сеть но это тоже еще не конец когда у нас идет активная диплом а там в пике там при днем у нас там десятки сервисов выкатываются и кубер notice не всегда с первого раза при принимает верные решения в итоге мы получаем такое и консистентной распределение по сервисам получается так что одни ноды содержит большое количество кодов и утилизировано сильно другие ноды наоборот отдыхают вот чтобы решить эту проблему мы запускаем специальный инструмент и scheduler который позволяет автоматом переселять коды с одной на другую как он работает алгоритм достаточно простой то есть он проходит по всем физически многом в кластере смотрят на их системные метрики теперь secu память network и далее там где высока утилизированы вот эти ресурсы уже автоматом этот шедевр находят соответствующие коды и удаляет их то есть переселения на самом деле делается не дыша донором лишь адлер просто удаляет а уже кубер найти с помощью стандартного механизмов и доминга просто сам находит уже подходящую ноду 2 раза ok какие есть еще моменты runtime конфигурации когда мы хотим сходить из одного сервиса в другой сервис нам нужно во-первых знать как его найти и куда именно сходить здесь есть тоже ряд проблем первое это хардкот в конфигурации бти имею виду не хардкот там где-то в коде сервиса а даже вот в об том не конфигурации сервиса здесь какой-то разработчик например там ставил поход через ingress контроллер кто-то fooly cooly fight доменное имя туда за фигачил кто-то там пошел случайностей джинкс продакшена в общем такой некоторый хаос и особенно там если мы решили переезжать из одного кластера в другой все эти url и они там изменяются в итоге получается такая вообще жуткая боль вот к чему мы пришли вы пришли к тому что эту вещь можно тоже вынести на уровень платформы и мы в об том ли просто заставляем разработчиков явно декларировать все зависимости которые у них есть то есть они описывают именно сервисов далее из этих именно имен сервисов уже сервис холма ген автоматом на уровне платформы генерирует переменное окружение и заполняет их нужными значениями которые необходимы сервису там в проводишь ты джонги локально даже представляет их правильное лечение в итоге разработчики не указывать вообще неких your love система полностью это делает автоматом и более того здесь есть еще один слой магии авто генерируемые клиенты о которых я рассказывал они автоматом из этих переменных окружения так как они типа единого формата формируются они берут и сразу же подключаются к нужным сервисом вот в итоге вообще нигде не нужно даже знать о том что эти переменные окружения как-то под капотом регулируются вот такой подход следующая достаточно большая тема это сеть в целом на сетевое взаимодействие и обзору обелить наблюдаемой системы какие здесь есть моменты разработчики часто обращаются с такими вопросами как почему например происходит кида connection тайм-аут и почему вообще система к это нестабильное для живота уже почините сеть что постоянно происходит там почему вдруг у меня там запрос вдруг улетел в мертвый instance они там не заручилась автоматом здоровый почему происходят такие деградации и вообще как мне понять вот сейчас что произошло по каким сервисом таким потом сейчас прошло взаимодействия и что я в итоге получил наружу почему у меня такое ответа не другой вот эти вопросы они на самом деле были достаточно частотными но такая горячая проблема и мы ее решали инструментом котором я уже рассказывала в прошлом году на daewoo psy называется инструмент навигатор сервис меж с помощью которого мы собираем унифицированные метрики вообще по всем взаимодействием в системе то есть мы понимаем полностью что происходит как взаимодействуют сервисы между собой и второй момент мы внедряем на уровне сети такие подходы к к outlier datex коннектор троя прямо между всеми сервисами прозрачно то есть разработчик dash они не знает вот они просто делают ему более предсказуемо и взаимодействие между сервисами вот выглядит это таким образом все сервисы общаются не напрямую с помощью гелевой прокси который как раз контролирует навигатор в итоге мы получаем унифицированные метрики зависит атаки скриншот основных метрика там виды рпс африку стримов утилизации сети между сервисами но на самом деле там достаточно большой рейндж метры которые дополнительно можно еще просмотреть вот делаем мы это с помощью prometheus а мы с крейгом автоматом все endpoint янбаев которые находятся рядом с каждым сервисом и получаем модифицированном виде вообще по сути всю информацию о том как что с друг другом взаимодействовать и также мы еще и поддержали отправку tracing информации с помощью этого же инструмента с помощью двое вот мы также еще для трейдинга используем инструмент не тронешь его можно тоже найти на гитхабе вид отёк репозитории проекте вот но для того чтобы у нас все обзор ability вещи могли исполнять стороны январе мы также поддержали это и вангую вот в итоге мы получаем в том числе вот такие штуки то есть это трис и потому как проходил запрос вот здесь такая и сенситив информация поэтому она скрыта но здесь видно что можно вот таким образом быстро понимать как происходило взаимодействие где мы например тратимся полотенце какой сервис носит там наибольшую задержку какие есть еще момент и тут сразу же мы перейдем так скажем к решению тут есть такое некоторый набор вопросов которые приходят от разработчиков это например найти ответственного за сервис как вообще понять почему какой-то конкретный сервис деградирует и как вообще понятие как себя чувствовать система в целом вот тут решение какое мы внедрили специальный паз дашборд такая некоторые frontend утилита которая позволяет посмотреть различные вещи прямо в одном месте по всем сервисам то есть узнать и ответы на вопросы кому принадлежит сервис и посмотреть там метрики основные там как связана сейчас работы сервиса с инфраструктурой и деградируют ли физические ноды на которых находится сервис автоматом сразу же подсветить проблемы в центре какие-то бизнес ошибки которые происходят мы ж вы такие вещи сразу же автоматом сообщаем плюс коррелировать например дипломе то есть может быть в этот момент кто-то deep lounge вот такой инструмент и на самом деле по вот это вот об зирвак 9 это все давайте сейчас по вот этим вот всем вещам которые мы обсудили подведём некоторые итоги я сначала обсудим недостатки и преимущества какие мы увидели в момент того как когда разрабатывали эту всю систему первый момент это миграция существующих сервисов на самом деле когда мы начали делать паз у нас уже были сотни сервисов и перевести например тот же самый об том все сервисы эта операция достаточно дорогая и долгое вот поэтому это надо иметь в виду сигнал у нас заняло самом деле даже больше года чтобы полноценно так мигрировать второй момент это те разработчики которые работают уже долго в компании и привыкли использовать низкоуровневые инструменты и может быть достаточно так то чтобы сложно просто у них можно не быть мотивации чтобы перейти на вот эти вот более де высокоуровневые инструменты вот поэтому здесь нужно четко им показывает что это новый инструмент действительно решает их проблему а это не они просто какой-то инструмент который новый нужно изучить чтобы решать ту же самую проблему которая уже решено вот и здесь крайне важно находить правильный баланс между автоматизации гибкостью потому что можно настолько автоматизировать все в платформе что отдать разработчику там маленькую текст ирию которую сказать пиши вот сюда код и все весь это твое место для написания бизнес коды вот здесь важно найти дату от места в котором будет и удобно и гибко для разработчика и в то же время можно было бы все это контролировать стороны платформы в одном месте вот ну преимуществ пас на самом деле защиты больше мы получили много профит от его внедрения первое это естественно экономии времени и ресурсов страны продукта разработчики получили достаточно большой boost по таким вещам как например там такой рандомный пример интеграция с базой данных позже раз вот до автоматизации этого процесса разработчикам прямо замеряли это могло занимать неделю нужно было там интегрироваться с возрастом написать все манифесты сходить там к админам запросить базу данных нужно было затем там правильно прокинуть все секреты и эта истина такая трудоемкая операция в случае с автоматизацией с платформой это там занимает уже минуты на и даже секунд просто то есть по кнопке вот и по таким прокси метрикам мы это видим действительно скорость разработки здесь возрастает вот следующий момент это зоопарк которым мы говорили в начале действительно со стороны платформы когда мы предоставляем и такие высокоуровневые инструменты следить за всем значительно проще потому что какие-то кастомные штуки в итоге превращаются по сути в запросы команду платформы и все какие-то такие нестандартные кейс их можно разруливать в одном месте вот и такое самый наверное главный момент со стороны именно платформой команды действительно есть возможность носить любые какие-то платформенные изменения как например там обновление версии кубер notice а потом изменяется версию бернейса нужно там изменить манифесты по всем сервисам этого делать не нужно для всех мы просто берем и в одном месте это меняем в итоге получается очень все быстро не нужно договариваться вырезать там в рабочие процессы продуктовых команд все очень так получается хорошо вот и если резюмировать вообще в целом по платформе потому что я сегодня говорил вообще нужно понимать что на самом деле те вещи которые мы сегодня рассматривали это такие некоторые кусочек платформы которые я решил сегодня осветить есть такие самые яркие моменты которые мне показалось что можно переиспользовать и вещи которые можно прямо забрать себе вот и подводя итоги здесь хочется сказать что на самом деле такая платформа она необходима имя на когда компания растет и в компании приходят достаточно большое количество сотрудников либо в ней уже там большое количество сотрудников и интеграция с инфраструктурой уже явно прямо боль разработчики тратит много на нее времени вот случае если это там 34 разработчиков компании скорее всего такая платформа не нужно следующий момент это интеграция с инфраструктурой которые мы говорили по опыту могу сказать что действительно большинство процессов можно практически полностью 0 автоматизировать и сделать так чтобы по кнопке было все автоматом все работало сразу из коробки вот и еще такой важный момент что и говорил вначале что если рассматривать пас платформу именно как продукт то можно действительно найти и увидеть реальные проблемы пользователей или закрыть и порешать и дать им возможность расти бизнесу быстрее и эффективнее и тем самым как раз и показать бизнесу необходимость такого проекта потому что разработка подобной системы достаточно дорогостоящая вещь достаточно большая команда нужно чтобы поддерживать вот такого рода продукт ok на самом деле это все теперь давайте перейдем к вопросам и посмотрим какие вещи нужно сейчас еще до обсудить спасибо так на самом деле вопросов вопросов у нас в чате скопилось приличное количество в начале можем на просеке вообще в принципе посмотреть так сейчас я перейду да давай посмотрим на опросники какие у нас получились результаты так же правительство агрегат да судя по первым опроснику большая часть народа живет на внутренних облаках то есть это там 60 процентов против там 30 процентов те кто живут на публичных облаках вот это наверное скажем так показательна для россии потому что публично облака сейчас на самом деле только появляются не страдают а вы вот например сами кстати почему внутренние выбрали дату dota и интересный момент что на самом деле есть два момента первый момент таких вот публичных кулаков которые используют во всем мире в россии как минимум там года два назад еще чтобы не было и во вторых когда речь идет про большую систему большую компанию здесь обычно есть много таких корнер кейсов каких-то вещей особенностей которые есть компании их нужно учитывать при создании этой платформы вот и поэтому действительно иногда эффективнее и дешевле построить и развить свою систему именно на том уровне которая необходима в компании вот и мы выбрали именно такой подход развивались именно так внутренней платформе но с течением времени к сети не появлялась мысль и переехать на появившейся публично яблоко там над различных компаний просто чтобы снять себя какие-то часть поддержки должна сказать что мы research и лет момент смотрели различные уже решение которые есть и публичных облаках и надо сказать что во-первых та система которая уже отстроена компании какой достаточно большой продукт который уже решает потребности и он просто его лицо на дальше продолжает развиваться и закрывать какие-то новые боли которые возникает вот ну и плюс с точки зрения даже стоимости если рассматривать сколько нужно потратить ресурсов на внедрение таких технологий и развитие внутренней платформы после последних research и мы все равно пришли к тому что эффективнее строить именно свою внутреннюю платформу и развивать ее дальше лучей на самом деле 2 просим был связан с комментарием из чата в тот момент когда то рассказывал про то как у вас создаются сервисы и про то что есть проблема с тем что каждый раз когда ты даешь сервис тебе надо прописать в мониторинге и не подчеркнуть отметки сделать ну в общем какая-то ручная работа которая отнимает время вот в простом ну и вопрос ответ на чатике документ сразу родился а сколько в среднем человек у вас трудиться над одним сервисом и если какой-то сервиса умер и не наверно сразу пачкой туда же часто вообще создаются сервис у вас да это важный вопрос потому что первых разработчиков на один сервис и достаточно немного то есть обычно одним сервисом трудиться там буквально 2-3 разработчика вот в команде кросс функциональные обычно есть там несколько сервисов и там в один момент времени стараемся достигать чтобы не было таких толканий разработчик работает один сервис нет времени вот но если смотреть в целом то это соотношение наверное 23 вот по поводу насколько часто создаются сервисы самом деле какие-то новые фичи которые делаются у нас уже давно создается в микро сервисах поэтому если говорить о каком перейти и к сожалению часто не могу поделиться прямо точной цифрой но каждый месяц у нас там создаются ему наверное десятки новых сервисов которые полноценно в итоге там доезжая до продакшена вот этот рейд он поддерживается и он достаточно стабильной такой на мой взгляд высокий и когда разработчику ты говоришь что нужно там пройтись и создать все ресурсы вручную это такое каждый раз моральный барьер что нет все таки я пойду и пока ничего это входит монолитное приложение или в существующей сервисы не буду делать это там таким правильным подходом вот такое решение то есть это еще и на архитектуре при этом систем влияет там в конце речи был дополнение про то что а у этих сервисов получается какой-то сервис owner есть то есть те люди которые в считаю нанесением закрепляются и потом сопровождают их или как да здесь такой момент что после того как мы автоматизировали по сути весь pipeline от начала там от идеи до production а именно создать сервис на разработчик является ответственным за этот сервис он полностью менеджер все процессы и разработки и деплоя и в том числе эксплуатации то есть если вдруг происходит к эта деградация то именно разработчик отвечает за это им ему приходит allure ты там дежурные админы которые дежурят по всей системе если что звонят именно этому разработчику да у нас за каждым сервисом закреплен конкретный человек новый вот на эту тему я опросник в чат запускал как как народ вообще у себя все это делает как автоматизирует не автоматизирует и на самом деле получилось специфичный результат что автоматизирует только один с процентов то есть остальные либо по чек-листам идут либо надеются на людей которые периодически собирают информацию и где-то что-то прописывают и попросят на самом деле даже большинство проголосовало за то что они следуют за этим нет проблем но видимо может быть просто сервисов меньше чем у вас да тут на самом деле очень зависит от размеров то есть если это не проблема то действительно это автоматизирует не нужно но для нас это было достаточно таким большим бустом в ускорении вот мы видели в этом проблемы интересный результат вот потом следующий вопрос был кстати интересно действительно просил и утилиты ты примеры на сайтах показывал что у вас получается народ через этот человек практически полностью общается с подсолнухом и самим пасом вот и какая у вас вообще политика обновления этого силой это как-то автоматизировано принудительно или там и на новый с скажем так не обновишь и новые фичи не получишь как это происходит вообще да здесь естественно для того чтобы поддерживать такой софт который поставляется на ноутбуке разработчиков у нас есть механизм автоматического обновления вот делается это на самом деле достаточно просто у нас это вид и утилита она написана на питоне вот поэтому у нас перед каждым это выполнением команды там с определенным записи каширования идет проверка есть новая версия или нет если она есть то она автоматически прямо обновляется разработчик даже не следит за этим она под капотом всегда становится up to do it вот потому что до те новые фишки доступны именно там вот мы стараемся сделать так чтобы у разработчиков всегда были именно последней версии понятно так тут на самом деле к такому количеству вопросов что даже я думаю как какой вы поинтереснее выбрать или на самом деле вот следующий он тоже такой жизненный вопрос с точки зрения того что ты рассказал про то что хил у вас в некоторых случаях не устраивал и писали своих читателей ты ну и не не только там заменяющих я в другие как быть с экспертизой то есть как бы если это какие-то у концертные tool это пошел нас так airflow пошел в гитхаба обычно я вышлю довел real guitar пообщался и все выяснил у кого это как омичей есть и всегда есть поддержка вот собственно металлами не возникает никаких проблем с точки зрения того что не знаю там команда ушла в отпуск или там и никто в компании больше нет работать да тут есть такой момент что надо понимать что данном виде о том который месяц рассказывал нас целая команда выделенная занимается платформой вот это достаточно большая командой которая шарят между собой экспертизу в этих инструментах в том числе которой написано у нас внутри и такой ситуации что кто-то там один поддерживает определенный инструмент такого нет вот и то что команда там платформы вдруг вся ушла в отпуск как тоже естественно и получается потому что ее размер там это дань едва не 3 должны действенность человек большая достаточно команда вот что касается каких-то окон собственных инструментов это действительно такой у нас всегда вопрос который мы продумываем перед написанием каких-то новых утилит то есть тот же самый холмы использовали и используем сейчас достаточно активно для некоторых еще legacy сервисов вот инструменты вида джипы навигатор рождались нас именно из проблем который мы не могли решить с помощью собственных инструментов и у нас не получалось там полноценно протащить webstream какие-то наши изменение например с холмом там взять ну по сути целом достаточно сильно видоизменить многими пришли к таким утилитам надо сказать что джамп-навигатор это никита супер огромные проекты это достаточно такие небольшие штуки но которые полноценным и поддерживаем развиваем своей командой и за все время эксплуатации какой-то проблемы с вас фактором с этими инструментами ныне получали вот плюс мы стараемся их также делать open source нами и вокруг наших инструментов уже даже есть небольшой конвенте вовне вот и вы все можете посмотреть на вид отёк проекте у нас есть source выложены там навигатор например вот можете пробовать писать там есть какие-то проблемы заводите shews вы за этим всем активным мониторим и стараемся это развивать в том числе и в open source вот она там гали ответил на большую пачку вопросов и сейчас но некоторые цифры все же назвал вот тут вопросы были вот это все было написано ты примерно порядок говорил но вот какого размера команда примерно и за какие сроки если они секрет ну размер команды я уже примерно назвал то есть это там пара десятков человек который занимать именно от этого частью которые я сегодня рассказываю по поводу сроков тут нет такого чёткого ответа потому что надо понимать что все о чем я рассказываю это такой эволюционной подход то есть это не было такое что мы решили вот сейчас пас все выкинули из нуля написали что-то новое там крутое и большое это было именно поступает на развитие конкретно платформу вот в таком виде разливаем уже последние четыре года а в виде паса это где-то полтора тира два года у данного вида которой я сейчас рассказываю совершением от проблем с этими инструментами ну вот чем я попытаюсь объединить там тоже парочку вопросов они мне кажется про одно и тоже и связанное а не скажем так с гибкостью платформа ну понятно что как бы всех несчастий ведь прямо совсем на сто процентов но вопрос в том вы как-то подстраиваетесь под те сервисы которые не вписываются ваших платформу либо вы пытаетесь сервисы под вашу платформу адаптировать вот как как у с гибкостью поступаете общее не стыкуется или таких текстов не была моя ты очень интересный момент здесь есть две фазы то есть первая фаза вот когда мы начинали делать пример представим утоптал да просто представьте что вы в своей компании решили изменить подход к тепло и и решили внедрить там новый формат описания вот вы делаете там какой-то минималистичный формат и начинает его раскатывать на сервисы естественно в этот момент будет большое количество боли в плане того что какие-то кейсы подержанные этом что-то работает не стабильно и действительно в первое время когда мы внедряли у нас все эти проблемы были вот поэтому здесь из такого best practice наверное что можно выделить это когда вы начинаете это доставлять пользователям лучше всего сделать какой-то отдельный трек прямо не просто дежурство а прямо такой трек запиливания необходимых вещей общих которых не хватает в платформе и соответственно их быстро решать ли снимать боль и для продуктовых команд а уже далее с развитием платформы когда мы там наверное 95 процентов кейсов уже покрыли вот здесь уже действует такой подход что какие-то вещи которые общее также внедряются на всех а вещи которые совсем не ложатся в общую структуру для них находятся какие-то свои решения дополнительные то есть мы не делаем так что все натягиваем на один шаблон да действительно есть какие-то часть термин слов которые живут там может быть по другим pipeline нам что-то там немного иначе делается но тут важно понимать что именно там подавляющее большинство сервисов работают вот в едином той плане повод на этой платформе в таком виде ну понятно унификация решает в этом классе ну да она на самом деле быть часть жизни как ты упоминал с точки зрения зоопарка в том числе в виде поддержание какой-то целостной архитектуры у нас на самом деле времени не так много остается 0 думаю вот один вопрос еще достаточно интересный влезет тут спрашивают про то есть ли у вас какие-то требования или какие-то может быть фичи достаточно специфично пасху в платформе такие как проверки лицензии все патчи библиотекой или проверка наличия при фотографии security проверки сервисов ну или не знаю если этого ничего нету может быть есть какая-то специфичная странная фичу который бы ты мог сказать чем нестандартная так ну по поводу вот первого момента естественно это все есть то есть это по сути вещи которые на самом деле были всегда там какие-то винтер и проверки на security всякие асинхронные парсеры там пароль и прочего естественно это есть как часть платформы вот но тут я бы сказал что решение все стандартные поэтому у него носил их как какие-то особые вещи вот если говорить о каких то таких еще на особенностях вот из того что я не покрыл в сегодняшнем докладе достаточно большая тема это работа с базами данных естественно она тоже полностью автоматизирована и например такие кейсы как там создать базу данных во все окружения из интегрироваться с ней это вот вещи которые на мой взгляд крыма обязательно надо делать потому что каждый разработчик с этим сталкивается практически в каждом сервисе есть на базы и поэтому настроиться еще параллельно такая система как дтп сервис которая позволяет нам просто по запросу получать базу данных и далее через тот же самый автомолл легко с нее интегрироваться в плане там сервис discovery прочих штук вот такая вещь которую я не покрыл из интересных ну а вот про библиотеки то например вы вы пытаетесь контролировать как-то лицензии то есть понятно что там уязвимости это там первое что приходит в голову а с точки зрения лицензий тоже проверять да по поводу лицензии именно имеется ввиду там какой лицензии принадлежит игротека типа тома понимаете этого конкретно вот прямо на таком очень низкого уровня в виде мы не проверяем на у нас есть специальные механизмы которые позволяют найти сервисы которые использую библиотеку и в автоматическом режиме прямо из поздно же барда разослать нотификации вот в таком виде то есть это так можно сказать полу автоматизировано если говорить о лицензиях такой механизм используем понятно классно спасибо ну кажется он особенно подходит концу все также гуглил подключился да я пришел на четырьмя подходит концу спасибо вам ребята спасибо за вопросы макс и собственно спасибо за доклад мне кажется было очень интересно я бы лично верно спросил из всего этого зоопарка какая твоя самая больная технологию которую ты ненавидишь мечтаешь выпилить или она просто отнимает больше всего времени на поддержку было что то такое но к сожалению я уже как бы спросить то могу а вот ответ услышать я не смогу новые если хотите услышать ответ приходите в дискуссионную зону и я уверен что максим с александром на все ответят все расскажут поэтому пока пока"
}