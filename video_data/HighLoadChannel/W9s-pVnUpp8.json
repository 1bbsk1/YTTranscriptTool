{
  "video_id": "W9s-pVnUpp8",
  "channel": "HighLoadChannel",
  "title": "Дедупликация 5 миллионов событий в секунду на YDB в АппМетрике / Артем Исмагилов (Яндекс)",
  "views": 2695,
  "duration": 2238,
  "published": "2023-04-28T06:21:30-07:00",
  "text": "Всем привет Меня зовут Артем исмагилов я разработчик Яндекс метрики я вам расскажу про то как мы построили систему дедупликации которая сейчас способна обрабатывать 5 миллионов событий в секунду и масштабироваться при этом дальше построили мы ее на vdb для нашего сервиса отметрика о чем будет этот доклад как мы построили сервис деппликации на 5 миллионов ключей в секунду как мы уменьшили потребление CPU в базе данных в 10 раз с помощью нестандартного применения фильтра bluma и про то как мы организовали ротацию данных и возможно перешартирование нашего сервиса для чего мы все это делаем мы делаем систему аналитики мобильных приложений которые называется отметрика Она позволяет владельцу приложения посмотреть какую-то аналитику потому что происходило в приложении какие там происходили события Сколько было установок конкретный день как эти установки атрибутируются там кликом по рекламе и так далее на слайде можете видеть график из интерфейса от метрики примерно так пользователь отчетов видит свою аналитику как В целом работает наш сервис во-первых У нас есть отметрика sdk который запущен на мобильных устройствах где собственно установлено приложение оно отправляет событие которое происходит в приложениях на бэкент то есть движок от метрики это происходит по http и в движок от метрики таким образом поступает около 5 миллионов событий в секунду движок метрики принимает эти события с помощью http сервера и передает в конвейер который состоит из более чем 50 микросервисов для различной обработки этих событий и как раз дедулятор про который я буду рассказывать он является частью этого конвейера из более чем 50 микросервисов и позволяет события после обработки нашим конвейером события поступают влоги из которых мы строим какие-то отчеты и данные и также ему могут пользоваться 3 Яндекса туда поступает примерно 10 гигабайт данных в секунду и также поступают кластер Клик Хауса который Сейчас занимает 150 петабайт именно оттуда строится те отчеты про которые которые были показаны на предыдущем слайде Откуда в этой системе вообще могут появиться дубли схема достаточно стандартная так как мы работаем с мобильными устройствами на которых может быть любое качество соединения может произойти например такая ситуация что события успешно отправлены с ДК и полученные сервером для приема событий но при этом так как соединение например порвалось или что-то еще произошло ответ от сервера не получен мобильным sdk и в этом случае мы попробуем отправить событие снова чтобы их не потерять совсем в этом случае такие события поступят в наш конвейер два раза и соответственно превратятся в дубли Зачем такие дубли вообще удалять во-первых мы считаем что отчеты и данные влогах должны быть корректными поэтому дубли там не должно быть и также на наших объемах данных два-три процента дубли которые у нас есть приводят к хранению лишних 5 данных что очень много и дорого мы дубли хранить не хотим Давайте разберем как вообще взаимодействуют микросервисы нашего движка про тот конвейер из 50 микросервисов про которые я говорил они взаимодействуют с помощью закипера и Клик Хауса Каждый микросервис имеет две очереди это Входная очередь в закипери и выходная очередь в этих очередях лежат чанки которые микросервисы обрабатывают Чанг на самом деле является метаданными данных для обработки и каждый час указывает на какие-то данные которые лежат в виде таблицы в кластере хаоса Когда микросервисы необходимо обработать эти данные он считает первый Чанг из очереди читает данный ассоциированные с ним и выдает выходной Чанг Видимо это данных и выходные данные в кликхаусе после этого создается транзакция в киперина атомарное удаление очереди и добавление выходной очередь это транзакция комментится И после этого пропадает появляется выходной но также транзакция может быть не заканчивая или в процессе обработки может произойти произойти какая-то ошибка в этом случае мы попробуем обработать тот же самый Чанг снова стоит отметить что вообще почти ни один из наших микросервисов так как у нас очень большой поток данных не уменьшается на один шахт-обработки поэтому почти у каждого микросервиса предусмотрено шортирование которое делается просто созданием нескольких очередей в закиппере и собственно несколькими инстанциями сервиса дальше расскажу про то как было устроена предыдущая версия сервиса детпликации она представляла из себя классический микросервис движка и работала следующим образом она получала входной Чанг прогоняла его через хэш таблицу в оперативной памяти и соответственно выдавала Чанг на выход размеченный дубль это или нет такая схема имела несколько особенностей во-первых если произошла какая-то ошибка обработки то нельзя так просто взять и завершить этот работа этого сервиса потому что потеряется состояние которое находится в оперативной памяти поэтому если произошла какая-то ошибка необходимо откатить изменения которые были сделаны в таблице оперативной памяти и попробовать обработать снова также для того чтобы избежать потерь состояния например при старте перед завершением сервиса мы сохраняли состояние на жесткий диск и соответственно перед стартом загружали его обратно загрузить необходимо было примерно 200 Гб И вот схема рестарта занимала примерно 50 минут Что Достаточно долго но понятно что сервис на самом деле может завершиться по обстоятельствам которые от него не зависят и если не предусмотреть какие-то еще меры то состояние в таком случае потеряется поэтому у нас была предусмотрена репликация когда два и того же сервиса читали одни и те же данные то есть поддерживали одну и то же состояние и в случае ошибки обработки в одном из них можно было переключиться на второй какие были у этой версии плюсы и минусы во-первых вся обработка происходила в оперативной памяти поэтому сама дедупликация требовала достаточно мало ресурсов но при этом было много проблем во-первых такая схема все-таки легко могла потерять состояние и у нас были такие случаи во-вторых такая схема имела проблемы с масштабированием Как по размеру состояния так как невозможно увеличить просто так объем оперативной памяти одного сервиса так и по количеству шардов непосредственно сервисы потому что любое перешортирование требовало бы копирование данных с одной физической машины на другую какого-то их разделения это в любом случае такой ручной и долгий процесс что остановило бы нам обработку и создал бы отставание конвейера также он привязан к физическим машинам кластера А мы хотим перенести все наши решения в облако и также как я уже сказал перезапуск сервиса занимает примерно 50 минут из-за того что необходимо работать с жестким диском Мы сформулировали следующие требования к новой версии во-первых она должна потреблять не сильно больше ресурсов чем старая вторых она должна гарантировать сохранение состояния она должна легко масштабироваться как по окну дедупликации соответственно размеры состояния так и по количеству шортов самого сервиса и должно быть легко развернуть в Облаке чтобы нам не нужен был кластер физических машин Как такую задачу можно было бы решать Но можно было бы просто перенести эту схему которая у нас была с жестким диском их аж таблицы оперативной памяти на схему с где хэш таблица меняется на таблицу с хешами событиями в Киева Люба зиданных такая схема могла бы работать точно также то есть мы читаем входной Чанг делаем Селект в таблице с кошами событий где спрашивают дубль это или нет И водоем выходной частотой или нет в этом случае мы бы сделали схему такую таблицу следующий это девайсы и Хеш и в это два ключа по которым мы производимпликацию то есть события у которых совпадают девайсы эти хеши и венхеш мы считаем одинаковыми также мы бы добавили схему таблицы чтобы обеспечивать ротацию данных и настроили бы по нему той старые данные из таблицы бы удалялись постепенно в качестве Киева либо за данных мы используем мы выбрали в ADB в ADB вообще не ограничивается киевелью но при этом его можно достаточно эффективно для этого использовать мы выбрали его потому что она может линейно горизонтально масштабироваться она ГИА распределенная easyradon Time при этом это хранилище которое можно использовать как киевелью а в ней записи Уникальны по первичному ключу и поддерживается операция абсерд Это нам очень важно Я потом расскажу почему и также в деве есть эффективная вставка операции балко все которые не только вот заменяет данные если они уже есть таблицы Ну и делает это очень эффективно без привлечения транзакций тоже расскажу почему это важно как такая схема могла бы работать в верхнеуровнево во-первых у нас как я уже говорил есть микросервис движка которому вот нужно производить дедупликацию событий и мы решили сделать деппликатор отдельным сервисом с которыми микросервис бы общался по grpc А дедупликатор в свою очередь уже производил операции свой дебит в этом случае стоит отметить что как мигра-сервисы так дедупликатора есть свое собственное шортирование которое вообще может быть разным Сейчас расскажу про шортирование как я уже сказал С кем это блицы у нас следующее это девайс иди Хеш Иван хэши при этом девайсы мы сделали первичным ключом vdb про первичный ключ важно что данные в edb по нему Уникальны и также данный всегда отсортированы по этому первичному ключу как по тапу и единица хранения данных в этом случае партиция партиция представляет из себя такой отрезок данных первичного ключа для того чтобы согласовать шардирование сервиса дедупликации с шортированием vdb Мы решили сделать шорты сервиса детпликации также отрезками первичного ключа чтобы каждый шаг сервис идет публикации работал с ограниченным числом партиций то сделал такие локальные запросы и не работал со всеми птицами сразу следующий вызов с которым мы столкнулись это была транзакционная обработка данных Давайте представим такую ситуацию мы пытаемся обработать Чанг с событиями которые содержат два события с крышами один два три один два три четыре мы смотрим в таблицу в edb видим что там таких шее нет в этом случае мы решаем что это не дубли потом мы их вставляем в таблицу в ADB и теперь представим что произошла какая-то ошибка при обработке уже после того как мы вставили в ADB в этом случае если мы пробуем обработать тот же самый Чанг снова то мы посмотрим в таблицу Увидим что там эти хаши есть и решил что эти события дубли Но это на самом деле неверный ответ потому что мы этот чат просто пытаемся обработать второй раз события в нем дублями от этого не стали как вы решили такую проблему во-первых каждому chang мы добавили его ID и в базу данных мы дописали транзактный так называемый как это теперь работает представим Ту же самую ситуацию мы пытаемся обработать данные в таблице мы их не видим вставляем туда хеши событий и в колонку транзакшена ID вставляем ID чанка из которого они были взяты Теперь снова представим что произошла ошибка при обработке мы пытаемся обработать снова видим что хеши в таблице есть но при этом транзакшиной и в базе данных совпадается и девчонка который мы сейчас обрабатываем это значит что такие события на самом деле не дубли и не дубли в данном случае верный ответ Давайте теперь представим ситуацию что вот мы пытаемся обработать следующий Чанг с ID 11 и в нем содержится событие с кэшом 123 Мы видим что это событие есть базе данных видим что соответствующие строчки Найди не равен айтишник в этом случае мы решаем что такое событие дубль и это действительно верно что мы получили На данном этапе во-первых мы сделали полноценный Облачный сервис деппликации у него достаточно простая конструкция она хорошо работала для потока событий до 300 тысяч событий в секунду какие у него были минусы во-первых Мы заметили что ты на Большой таблицы тратят достаточно много ресурсов и при этом еще ухудшает производительность параллельных секретов которые мы делаем а такая схема при потоке в 300 тысяч событий в секунду потребляла примерно 250 ядер vdb User Pool и соответственно Если масштабировать эту схему на наши 5 миллионов событий в секунду то нам потребовалось бы три с половиной тысячи ядер vdb что очень много и дорого поэтому мы решили что необходимо работать над производительностью нашей системы в первую очередь мы думали в сторону отказа от отель и придумали такую схему Мы решили поделить все наши данные в таблице с крышами по инсультам на несколько таблиц в этом случае мы вставляем новые данные только в последнюю таблицу а делаем лукап во все сразу и при этом когда время очередной таблицы вставлять в нее закончилось мы просто сдаем новую начинаем вставлять в неё в этот момент нам также нужно начать делать в нее и лукапы тоже когда старая самая старая таблица стала слишком старой мы можем просто удалить ее полностью Что можно сделать очень эффективно так как не нужно ее полностью сканировать не нужно выбирать данные которые нам нужно удалить можно просто удалить все сразу В чем плюсы такого подхода во-первых отель перестал тратить ресурсы базы Потому что его просто нет во-вторых колонка insertime нам больше не нужна и перестала занимать у нас место на дисках в-третьих размер таблицы перестал быть ограничен производительностью ttl но такой схемы есть фатальные минусы во-первых число лукапов которые нам необходимо делать Просто умножается на число таблиц которые у нас есть параллельно в нашем случае это привело бы к делу четыре раза по пять миллионов лукапов каждую секунду что просто было бы еще дороже и такую схему ничего не получили Поэтому решили дальше уменьшать число лукав в нашей таблице делать Мы это решили с помощью фильтра Блума я Коротко напомню что это такое Давайте представим что мы хотим сделать структуру данных которая позволит вставлять в нее элементы и проверять Есть ли элемент в нашей структуре эта структура будет представлять из себя такой битвы массив Давайте представим что вот мы хотим добавить элемент X в нашу структуру посчитаем от элементы X несколько функций которые выдадут индексы в нашем битва массиве и выставим на соответствующих местах в битвы массиве единицы теперь добавляем элемент Y от него тоже считаем 3 функции этих функций указывают на какие-то свои элементы при этом как бы индексы могут пересекаться с элементом X и на этих местах тоже поставим единицы теперь как проверить наличие элементов такой структуре вот Давайте проверим наличие элементы X вычислим от него те же самые Хеш функции видим что на индексах которые не показали стоят единицы Это значит что такой элемент мог быть в нашей структуре А вот элемент Z от которого мы посчитали функции они указали на хотя бы одну позицию 10.0 вот красным помечено такого элемента у нас не могло быть потому что если бы он был то там бы стояли единицы понятно что такая структура может как бы давать ложный положительные ответы но при этом вероятность сложнопожительного ответа можно регулировать изменяя точнее увеличивая размер собственно битого массива и количество но в использовании фильтра bluma для нашей системы вообще была очень большая проблема которая заключалась в том что если работать с ним оперативной памяти то при старте сервиса он также потеряется поэтому необходимо было как-то его поддерживать в актуальном состоянии где-то помимо оперативной памяти Мы решили сделать таблицу в которая бы хранила наш фильтр Блума как мы это сделали мы поделили битвы массив в оперативной памяти на несколько бачей здесь они поделены по 8 битов на самом деле мы делили по 64 и сделать таблицу в edb которая на самом деле имела такой интерфейс разряженного массива где по индексу батча мы могли понять какие там находятся биты Как такую схему можно обновлять мы просто делаем в таблицу фильтра Bloom absort с новым значением битов нашего батча и его индексом в этом случае в vdb просто появляются актуальные данные здесь как раз важно что мы делаем именно операцию абсерд и появляется не новая строчка а заменяется строчка которая была до этого как такая схема теперь работает Давайте представим что вот дедупликатор получает ключи для деппликации что он делает с ними дальше он проверяет наличие этих ключей фильтры Bloom оперативной памяти Затем он делает лукап vdb по уже фильтрованным ключам здесь ключей значительно меньше чем поступило на вход Затем он вставляет данные в таблицу с кошами vdb и вставляет обновляет оперативной памяти Затем он заставляет измененные биты фильтр Блума в таблице фильмов после этого как фильтр Блума так и таблицы ключей находится уже в актуальном состоянии и можно отвечать на запрос при этом что мы делаем при старте нашего сервиса нам просто достаточно Вычитать таблицы фильтра Bloom из vdb Давайте вспомним что у нас на самом деле несколько таблиц параллельно и в этом случае для каждой таблицы мы поддерживаем свой фильтр Блума чтобы его можно было добавить вместе с новой таблицей и удалить вместе со старой и также вспомним что у нашего дедупликатора есть несколько шардов А если мы будем работать с одним и тем же фильтром на разных шардах у нас возникнет гонка поэтому на самом деле для каждого шарда необходимо также поддерживать свой фильтр Блума что у нас В итоге получилось в числах давайте договоримся что здесь все числа они представлены целиком на сервис сейчас у нас 50 шагов сервиса и окно дедупликации там примерно 16 часов мы получаем на вход 5 миллионов ключей в секунду это приводит к 20 миллионам лукапов в секунду фильтра Bloom оперативной памяти Так у нас 4 Таблицы с данными параллельно и дальше из этих 20 миллионов ключей Мы выбираем 600 тысяч по которым сделаем лукап vdb это примерно 3 процента потока и также делаем абсорт в таблице с ключами vdb это 5 миллионов строк в секунду которая к нам приходит на вход в таблице фильтра Bloom оперативной памяти мы делаем примерно 10 миллионов абсортов в секунду То есть 10 миллионов строк секунд вставляем здесь как раз очень важно что чтобы это происходило эффективно без транзакций и вот как раз той самой операции балко все вычитывание таблиц фильтр обломовой при старте занимает примерно 5 минут и суммарно на сервис мы вычитаем примерно 300 Гб при этом вся эта схема потребляет Примерно 200 CPU и 350 гигабайт оперативной памяти для обработки работы с фильтрами blooma и примерно 250 CPU в ADB 250 CPU vdb это экономия примерно в 14 раз по сравнению с подходом просто без фильтра Ну то есть тот который мы рассматривали в начале еще немного чисел у нас сейчас 4 таблицы в каждой из которых лежит 4 часа данных самая большая из них занимает 16 терабайта и таблицы Фильтрум для пары таблицы занимает 3 ГБ сварную таблицах E6 занимает 5 терабайт и таблицы фильтров примерно 600 гигабайт дальше бонусная часть моего доклада про то как перешардировать такой сервис и про то как еще можно оптимизировать его под конкретную задачу Давайте представим что у нас есть сервис который работает вот ну на картинке с тремя шардами и обрабатывает 5 миллионов ключей в секунду при этом фильтр Блума занимает 3 ГБ Давайте представим что мы хотим масштабировать его до 10 миллионов ключей в секунду в этом случае чтобы поддерживать количество можно положительных срабатывания фильтра bluma Нам необходимо также увеличить размер фильтра Bloom но при этом если бесконечно масштабировать этот сервис то фильтр был просто в какой-то момент перестанет помещаться в оперативную память одного инстанса что плохо и нам нужно увеличивать количество в какой-то момент для Что можно сделать чтобы вообще увеличить количество инстансов Давайте представим что у нас раньше было три шарда и хотим сделать 4 что мы Для этого можем сделать можем просто добавить новую таблицу с данными с новым шортированием в ней уже будет 4 фильтра так как старых будет три с новой таблицы мы можем работать так же как просто как будто бы ничего не менялось но нам нужно работать как-то со старыми данными что можем в этом случае сделать каждый шаг нового жердирования может загрузить свои фильтры Блума который ему необходимы то есть которыми он пересекается первый шаг нового шортирования пересекается только с первым шагом старого шортирования второму уже нужно будет загрузить первый и второй шарт соответственно логично третьему и четвертому только одну строчку вот этих фильтров нужно будет загрузить это приводит к верху на оперативную память и вычитывание при перешардировании Но на самом деле этот уйдет после того как мы отреагируем эти и новые таблицы нового жердирования вытеснят старые дальше расскажу про то как еще можно оптимизировать такой сервис Давайте представим что помимо хыша события по которым мы дедуплицируем у нас есть еще какой-то параметр eventime То есть у каждого события есть в Ин тайм и что важно у дубля Event Time точно такой же как и у оригинала как такая схема может работать у нас также может быть несколько таблиц которые поделены вместо insertime теперь even Time внизу подписаны в тайной и Давайте представим что 95 процентов новых ключей попадают в новые данные То есть фактически в самую последнюю таблицу это очень практичный кейс потому что на самом деле часто происходит какие-то события они происходят фактически сейчас и попадают в самые новые данные а старое это какой-то там отставание которое непонятное его достаточно мало Вот например в Новую таблицу попадает 95 процентов от старого попадает только 5 в этом случае фильтры Блума которые старые данные они нам по сути не сильно-то не нужны потому что туда приходит всего пять процентов данных поэтому можем просто их убрать и работать с теми таблицами старыми как будто там фильтр Блума никаких не было на этом У меня все спасибо большое по qr-коду можете оценить доклад и также вопросы для обсуждения Спасибо ваши вопросы у нас есть в центре Вот первый День добрый хотел спросить там была схема когда дубли разделяются между чанками и проверяется что по Чан к ID сейчас вот дальше Найди Если два дубля будут в одном чанке и как Ну как гарантировать чтобы они не были в одном чанке Да хороший вопрос В данном случае понятно что дубли может быть в одном чанке в данном случае деппликаторы их не опознает Но на самом деле у нас есть специальная клиентская библиотека для клиента дедупликатора которая Чанг Один который отправляет проверяет находит в нем дубли и сразу же на месте помечает то есть этот кейс Мы тоже обрабатываем но просто на клиенте потому что внутри одного чанка очень просто дублировать Привет Спасибо за доклад слушаем и немножко мотивации не хватило У меня такое ощущение что твоя первоначальная схема когда ты описывал что вас все в рам хранится из диска поднимается вот если все вот эта проблема фильтра и все остальное сделать она разгонялась намного больше чем 50 миллионов Зачем Яндекс Ну вот я тебе и всё остальное Да хорошо ну главное в чем была ее проблема она привязана к физическому кластеру на котором должен быть жесткий диск и если ее перенести на другие машины это просто нужно перекопировать данные Ну и я тебе не пользуется жестким диском когда хранить данные а в ID пользуется но мы хотим пользоваться жесткими дисками как сервисом а не как тем что нам нужно самим поддерживать сами просто не умеете этот сервис поднимать во влаги правильно понимаю я имеет в Облаке Да вот это схему мы не умеем поднимать в Облаке в ADB просто спокойно поднимается в Облаке предоставляется как сервис просто не умеете поднимать этого блоки Спасибо еще вопросы а вот тут прямо рядом и следующие вижу формируется с клиентом один Чанг это один блок отправки с клиента один шанк это вот условно одна отправка с клиента и клиент на самом деле отвечает за то чтобы гарантировать что в нем не будет точнее Что например Каждый раз он будет слать один и тот же Чанг чтобы как раз таки внутри Нет это неправда чанки из клиента У нас сейчас склеиваются на сервере но там есть схема с тем чтобы обрабатывать такие сложные кейсы Ну сейчас фактически Да у нас Ну да дубли которые внутри одного чанка они просматривают понимаете должны быть достаточно большие соответственно дубликации она тоже В каком смысле в кавычках нагруженная то есть там тоже большой набор данных которые нужно по несколько сотен тысяч событий которые вот одна единица обработки в детопликаторе источник который поклеивает он тоже получается 100.000 должна память дублировать Ну он отправляет на самом деле только хэши событий И это не так дорогом событий сейчас не очень хорошо и там и там до дублицируется по кожам событиям вот и получается Здесь в процессе и там там вы процессе целый блок то есть здесь делать дубликацию между блоками там внутри блок он тоже достаточно большой То есть это просто пьют Весь блок загружается оперативку проверяется дубликация правильно понимаю там Понятно несколько сотен тысяч ключей Это не проблема Спасибо у нас есть вопросы Пока туда идет микрофон Артём Я тебе задал вопрос воспользовавшись привилегиями Скажи мне Откуда появилась требование на масштабирование по окну дедупликации У нас есть помимо основного потока событий в 5 миллионов есть еще несколько побочных струй которые могут например измениться бизнес-требования и мы можем захотеть увеличить во-первых окно существующего дубликатора А во-вторых мы можем захотеть сделать еще одну там новую струю обработки событий которая в которой более серьезные требования на публикации и Спасибо И снова Да я правильно понимаю весь этот сервис нужен лишь из-за того что вы используете на входе протокол степи То есть если начать использовать какой-то другой протокол то все дубликаторы не будет необходимость да В целом скорее всего можно начать использовать какой-то другой протокол там на входе но у нас уже есть очень много устройств которые работают текущей системой они все точно никогда не обновятся вот до новой версии спасибо спасибо если у нас вопрос из Интернета нет поэтому продолжаем вот я помню там в глубине был вопрос да Артем Спасибо за доклад очень мне понравилось подача плотной такой хороший поток материала можешь пожалуйста вернуться примерно на 105 слайд там про цифры вот там 10 миллионов потоков 10 миллионов событий секунду 20 миллионов Расскажи пожалуйста откуда взялись эти цифры Почему именно такое соотношение вот очень интересно момент повторю Еще раз На мой вход поступает 5 миллионов ключей там сейчас 4 таблицы в каждую из которых мы делаем лука по параллельно это приводит к тому что фильтры Блума мы делаем 20 миллионов лука по суммарно просто количество выходных ключей на 4 600 тысяч ключей которые мы делаем лукапы в idb в большие таблицы это уже отфильтрованные фильтры bluma то есть Это вот три процента потока которые мы проверяем дополнительно помимо фильтра Блума То есть вы 600 тысяч выбрали как необходимое разрешение то что да то есть ну вот фильтр Bloom нам сказал что эти 600 тысяч могут быть дублями и мы их проверяем в эти дополнительно остальные ключи отфильтровываются потому что дублями не могут быть так как мы проверили фильтры Блума спасибо Вот тут у нас рядом вопрос раз еще раз спасибо за доклад Я может быть просто начало доклада пропустил но хотелось бы уточнить вот речь шла как бы про именно какое-то окно отложенный где дубликации Или это онлайн подход То есть это вот как она как данные приходят мои деду прицеп в нашем случае это онлайн подход то есть деппликация от момента вставки события до вот момента прихода следующего Спасибо еще вопросы в центре Вот он я вижу А вот там да Спасибо за доклад подскажите почему было выбрано 4 таблицы по 4 часа то есть у нас примеру есть проблема что при иногда данные приходят с задержкой семь дней у вас такого не наблюдается да как я уже сказал у нас есть несколько струй обработки событий вот на нашей главной струе которая 5 миллионов событий в секунду мы обрабатываем с окном где-то публикации в 16 часов потому что просто неделю данных хранить Было бы очень дорого и сейчас где это открою какой-нибудь Да у нас есть также струи где события лежат неделю в этом случае как бы тут скорее вопрос подбора вот этих коэффициентов сколько-то блин сделать сколько данных мы храним избыточно какие какого размера нам придется делать фильтры Блума И сколько вы загружать из bdb то есть вот эта схема например таймом она к этому более устойчива потому что в этом случае на каждое событие нужно делать лукап всего лишь в одну таблицу которая соответствует его тайму это гораздо проще например будет все подбирать то есть вопрос подбора спасибо Еще вопросы вот у нас здесь есть в центре не пускайте ваши руки чтобы у вас микрофон смог найти У меня вопрос А почему тогда вы дублицируете по отдельным событиям если так хорошо контролируйте клиенты не присваиваете блоку данных с клиента какой-нибудь айдишника просто не пробиваете его покажу который хранить распределенный Киеве Или те же самые 16 часов сейчас не совсем понял как смотри у тебя на клиенте есть блок данных который всегда одинаков и дублицированный уже на клиенте он пытается отправить на сервер соответственно логично ему присвоить некий хэш например как просто от содержимого отправить он пытается отправить на сервер и бургер сервер проверяет обработан блок или не обработал ну прям вот условно говоря в Киеве или просто хранить те самые 16 часов айчанка Ну ID клиента плюс it chang Ну ID блока Окей я понял хорошее замечание Почему мы так не сделали сейчас не может клиент это контролирует на клиенте она либо 1 час Либо в другой Мы же говорим про эту дупликацию событий с одного клиента клиентам полностью контролируем Он положит события либо в другой этого невозможно нет клиент внутри себя там Баско то есть мобильное приложение но в баску складывается быть и собирает блок данных для отправки на сервер все там никаких вариантов на самом деле думаю что мы сделали это то чтобы как можно меньше требований накладывать на клиента то есть на самом деле здесь требования не к тому чтобы каждый раз Чанг был одним и тем же требования к тому чтобы сейчас ваш поход насчет полезно скажем так если у вас чанки маленькие грубо говоря там по одному два события то ваш подход логичный если чанки большие сотни событий то получается что вас оверхед в сто раз Ну то есть вот этот мультипликатор как раз важен Я скорее по то что да ну то есть соотношение чанка и событий то есть работать на уровне отдельного события да Но можно было бы работать на уровне всего чанка присваиваем идентификатор сэкономив нагрузку Да я согласен что наверное можно избавиться от вот этого транзакшиной в базе наложив просто дополнительные требования клиента наверное так можно сделать вопрос закончится поэтому фильтр использовать примерно такие же нагрузках Это отличный разговор для дискуссионной зоны А тем временем У нас есть еще вопрос тоже в центре и Наверное это последний вопрос по времени Артём Спасибо за доклад такой вопрос Вы в самом начале говорили что на бэке то есть самом конце вы складываете все Клик Хаус верно я услышал А вы используете там replace 3 для того что уже окончательно схлопнуть все эти блоки и тогда проблема в том чтобы хранить лишние пять терапает Они же сами уйдут Да проблема в том что этот сервис изначально проектировался так чтобы не использовать replaying Med 4 я использовать просто обычный и у нас есть как бы уже данные собраны за всю историю которые лежат в таблице и движок переделать не так-то просто ну и соответственно изначально был деппликатор Ну да собственно А может быть есть еще какие-то проблемы у replace производительностью или чем-то еще достаточно плохо может быть у replace 3 есть еще какие-то проблемы производительности которые не вывозят например Это эти масштабы Нет вообще у нас есть еще помимо от метрики есть веб-метрика то есть для ну для сайтов то же самое система аналитики и там мы спокойно используем реплей Синг точнее коллапсинг 3 и там все вроде нормально работает но там нагрузка Ну по крайней мере в событиях в несколько раз меньше то есть там три может быть"
}