{
  "video_id": "Ot93PQELdcM",
  "channel": "HighLoadChannel",
  "title": "Автоматический подбор параметров для Spark-приложений / Валерия Дымбицкая (OneFactor)",
  "views": 1189,
  "duration": 2640,
  "published": "2023-04-28T06:19:52-07:00",
  "text": "Всем привет Меня зовут Лера и я расскажу сегодня о том как автоматически подбирать параметры конфигурации для Спарк приложений на основе логов и щепоткой машинного обучения я техлид команды data-инженеров в инфактор мы делаем платформу совместного конфиденциального машинного обучения я отвечаю за ту часть платформы которая непосредственно обрабатывает данные и это бач история бач-процессинг если вот Миша утром рассказывал про наш онлайн сервис то я буду рассказывать и также я сотрудничаю с Яндекс практикумом помогаю им делать курс по профессии инженер я расскажу про систему автоматического тюнинга которую мы разработали и внедрили Ван фактор расскажу как мы вообще дошли до такой жизни какую проблему решали что позволяет этой системе работать автономно без вмешательства человека Ну и в конце расскажу немножко про фейлы и проблемы с которыми мы столкнулись при внедрении и эксплуатации зачем же нам в принципе понадобилось тюнить приложение автоматически всю жизнь делали это руками просто заходишь в парк ЮА и смотришь на него подбираешь все что нужно и приложение работает Начнем с того что кластер у нас не резиновый это не какая-то история которая разворачивается В класс в Облаке по запросу это более консервативный вариант с ограниченным количеством узлов на тот момент когда мы начали сталкиваться с проблемами у нас было доступно около 30 терабайт памяти и где-то 5000 ядер звучит объемно Но даже в таком не резиновом классе У нас запускалась запускается очень большое количество приложений настолько большое что даже такого объема может не хватать и в том числе запускались и запускаются процессы продукта под названием Лид генерация и система тюнинга изначально предназначалась Именно для этого продукта Поэтому я сначала немножко расскажу как он устроен чтобы было лучше понять проблему Так как устроена лидогенерация У нас есть некоторая база то есть список абонентов мобильного оператора и есть какое-то Спарк pipline который по признакам некоторым отбирает из этой базы лидов Лиды это абоненты с которыми нужно совершить некая целевое действие например обзвонить база может меняться от запуска к запуску и вот такую пару из базы абонентов и пайплайна мы называем триггером триггеры делаются под каждого клиента ставятся на расписание работать регулярно стоит заметить что пайплайны все более-менее Уникальны в том числе потому что их делаем не мы их делают датцентисты мы им просто предоставляем библиотеки инструменты база делаются тоже не нами выводится это тоже без нас то есть с нашей точки зрения когда это инженеров это просто какая-то внешняя нагрузка которой мы не управляем и которую кластер должен выдерживать когда мы начали эту задачу решать нас запусков именно лидогенерации в день проходило около 600 и при этом каждый день приблизительно добавлялись новые триггеры создавались новые пайплайны регистрировались подключались и так далее И в такой ситуации мы не могли бы просто угнаться За каждым пайплайном брать каждое приложение отдельно тюнить его и так далее надо просто это было бы все чем бы мы занимались вот поэтому мы сначала поступали довольно просто всем выдавали одинаковые ресурсы Единственное что мы различали это два типа запусков те которые происходят каждый день те которые происходят Раз в час и если дело касается памяти то понятно что часовые мы выделяем Чуть поменьше потому что они должны быть полегче чтобы укладываться таким же проблемам это приводит Ну вот допустим у нас есть два триггера Пусть они даже на одном пайплайне но они обрабатывают разные базы один там 10 тысяч строк а другой например 250 миллионов строк понятно что для них нужно разное количество ресурсов но параметры которые мы задаем в спарке до ресурсов задаются с самого начала раз и навсегда Вот и поскольку мы выделяем всем одинаково то получается такая ситуация более Легкий расчет можно забрать себе например все тяжелые контейнеры и более тяжелому контейнеров не останется он останется висеть в листе ожидания но при этом слое его никуда не девается при этом мы понятно не до утилизируем ресурсы маленькому расчету хватило бы меньше явно И если бы мы выделили Ему меньше то скорее всего хватило бы ресурсов выделить большой контейнер большие контейнеры большому расчету и они бы выполнялись в параллель понятно что это начинает играть роль когда расчеты становится много и мы начали эту проблему решать первый наш подход был так сказать априорном Да мы хотим сразу перед запуском триггера определись такими же параметрами мы должны его запустить и мы подумали у нас все запускается через сервис пакетной обработки мы называем его эльпач мы могли бы все запуски разделить на какие-то классы классом подобрать параметры этот классификатор ставить мольбач и в общем-то с этим жить в теории звучит прекрасно на практике есть две проблемы во-первых Как выделить собственно эти классы учитывая то что наша прекрасная дата-центисты занимаются продуктовыми задачами нам это было довольно сложновато Но даже если бы мы эту задачу решили привели главная проблема остается в подборе параметров потому что для этого нужно провести довольно много экспериментов у него нет данных о том как эти триггеры повели бы себя при других ресурсах на другой конфигурации А эти эксперименты в свою очередь надо проводить на среде близкой плодовые лучше на продувай с продувой нагрузкой и всем таким но у нас прот Итак расхват Да и мы и так не хотим увеличивать нам докупать каких-то серверов для отдельных тестовых стендов Да мы Так начинаем немножко упираться в потолок вот поэтому выходит что этот самый подход делать долго и дорого и мы пошли другим путем второй подход Я по аналогии называю апостерионом Мы сначала даем приложению пройти смотрим на его метрики и по этим метрикам понимаем Какие же параметры нам нужно выставить этому приложению в следующий раз понятно что здесь играет тоже играет роль то что триггеры запускается регулярно то есть мы постепенно сможем оптимизировать все найти нужный нам минимум и так далее Потому что мы знаем что приложение будет еще запускаться и запускаться чтобы найти метрики по которым определить параметры Мы залезли в логиспарка Spark пишет очень много информации по которой собственно строится Спортивной это можно Включить запись этого Лога например на hdfs и тогда history Server сможет потом прочитать этилоги и поднять спаркиваю уже пробежавшего положение приложения вот эти логи мы и использовали в них есть две хорошие штуки первая штука это же сончик то есть в него можно залезть легко без особых как бы усилий даже с парка в общем-то не запуская не читаю все посмотреть все довольно подробно написано И это не просто джессон этот же сон Лайнс то есть по сути мы можем прочитать этот эти логики когда-то сет в формате json логис парк читают с парком Это приятно вторая Хорошая вещь заключается в том что там просто море информации например там есть информация о том сколько тоска шафлила сколько она спилила сколько она провела коллекторе когда началась Когда закончилась и в общем эта вся та же информация которую мы когда-то инженеры используем чтобы руками мы идем спать и смотрим ровно это же там сколько шахрилось сколько спилилось и так далее соответственно мы можем исходя из нашей экспертизы выделить какие-то правила которые позволили бы нам оценивать эти самые метрики я много говорила про память я буду говорить еще потому что наше основной задачей была оптимизация конкретного параметра Спартак Memory собственно Почему Потому что у нас была недоутилизация ядер в класс 3 был некоторые перекос эти памяти почти под потолок ядер бывают где-то иногда даже около половины Это значит что мы выделяем слишком мало слишком больших контейнеров мы бы хотели соответственно для смещения баланса выделять больше меньше контейнеров Поэтому соответственно мы и углубились в тюнинг этого параметра Но тот подход о котором я буду дальше рассказывать в принципе применим К любым параметрам просто так сложилось что у нас была такая задача Итак как же мы подходим к тюнингу Мы хотим найти минимальный порог с парком секьюри при котором приложение еще проходит но при этом не падает проходит не падает и укладывается в slay мы предполагаем изначально что мы хотим память снижать что мы выдаем слишком много поэтому мы ищем скорее не признаки того что снижать можно а признаки того что снижать больше нельзя и стопперы и мы смотрим с трех сторон со стороны Горбач коллектора со стороны сброса записи на диск И со стороны общего использования данных Первое правило по карточка лектору Мы хотим чтобы время сборки мусора превышало не превышала 10 процентов от времени работы приложения Почему мы так хотим потому что чем чаще спускается горка Вектор тем медленнее будет идти само приложение То есть это правило это такой супер по Sela не чтобы посчитать эту метрику нам Понятно нужно найти общее время пребывания коллекторе по всем таскакам общее время работы по всем тестам и посмотреть насколько их отношения отличается от 10 процентов мы получим некий Вектор Да некую Дельту куда крутить параметр и соответственно все это мы достаем все данные для этого мы достаем из структуры Task metrics там все это есть и названо достаточно очевидно и агрегируем суммируем получаем то что нам нужно и Вот пример того как это работает допустим у нас есть некоторые приложения которому выдано 16 Гб И у него в принципе все хорошо Мы видим что в целом проводят немного значит по идее мы можем дальше ему параметры снижать но мы допустим снизили слишком много и Поставили ему 8 гигабайт и в какой-то момент пребывание в горбаче коллекторе при превысила даже использование CPU это как раз та самая стоп-ситуация когда мы понимаем что мы дальше снижать уже не можем может быть нужно даже накинуть второе правило касается того факта что Спарк если записи не помещаются в память начинает сбрасывать их на диск мы хотим такую ситуацию в принципе избежать потому что сброс записи на диск Это долгая операция То есть это такой еще один не Явный супер признак того что приложение в общем-то не хватает памяти соответственно если мы такую видим нам нужно память ему накинуть Ну сколько например средняя между объемом памяти до сброса на диск здесь мемспил и объемом записи уже объема памяти уже на диске происходит сжатие немножко меньше и так далее Это принципе эмпирическое правило как в общем-то и 10 процентов на предыдущие слайдах Я думаю что в разных ситуациях можно делать здесь по-разному Но у нас просто сработало так все данные Мы точно также берем и структуры Task metrix и агрегируем в данном случае мы берем максимум чтобы посмотреть потребление на пике Вот пример того как это работает у нас есть приложение которого 7 Гигабайт Спарк executer Memory оно что-то читает оно что-то шафлят ему в принципе норм мы можем ему попробовать снизить память снизили но допустим снизили слишком много и видим что оно начинает спилить значит стоп все дальше снижать не можем дальше может быть накинем еще поговорю позже о том как мы принимаем решение о том как бы сколько накинуть сколько не накину с учетом разных правил 3 собственно объем данных которые используют приложение Мы хотим чтобы наши шафлы чтобы наши чтение формирование результатов и временный объекты которые во время джоина например генерируются укладывались в отведенное им место отведенное им место Согласно модели памяти Spark которая здесь есть регулируется параметром Спарк Memory frage И делится на такие две части настойчивыми между которыми могут перемещаться блоки данных смысле что одни могут вытеснить другие помимо того что я назвала шафлы чтения и так далее есть еще и другие статьи расходы памяти например кеширование до которые идет storage Моя Мария Да братка С переменный вот это вот все поэтому мы сознательно снижаем порог до половины то есть мы хотим чтобы наши шафлы чтение формирования результатов временные объекты помещались в половину от Spark Memory fraction вот соответствующие Соответственно по дефолту спорт-мене refraction это три четверти поэтому магическое число 0,375 это просто половина от Spark Mom refraction Не пугайтесь Почему здесь две компоненты здесь есть Макс байтс оу компонент и Макс пик мем Макс олл содержится чтение шафлы формирование результата Макс пик мен содержится пиковая память исполнения в которой содержится временный объект Почему здесь они в разных переменных потому что они из разных мест достаются формирования результатов также достаются из такс-метрикс из-подструктур я не стала их сюда выводить потому что было бы просто очень много текста Вот но там в тоск метрах есть отдельные под структуры под шафл отдельные там по чтение и так далее они называются очевидно Да если вы зайдете просто в этот же зонтик вы сможете это увидеть и просто понять что к чему Вот поэтому Мы это можем просуммировать и так далее но здесь нет Пика execution Memory Где же она она прячется в аккумуляторах аккумуляторы это такие переменные в которых спак накапливает значение некоторых статистик например пиковая память исполнения и влог содержится история изменения этих аккумуляторов соответственно Мы зайдя в соответствующее место да в task.info в данном случае структуру в аккумуляторы можем найти соответствующую статистику прореагировать посчитать и все получить что нужно Мы сначала этот параметр пропустили именно потому что он прятался не там где Казалось бы он должен быть в каком-то левом месте и без него наша формула давала сбои и ошибки когда мы нашли и добавили это собственно к Макс бойцов то все стало работать гораздо лучше так что вот это очень важный параметр и это можно также проиллюстрировать следующим образом вот у нас есть приложение и она очень много Джой нет поверьте может стоили больше Джона сделать серьезно и вот как бы сумма потаскам Пика execution Memory говорить сама за себя очень большое использование в таком в такой ситуации Мы скорее всего не будем снижать память это также показатель того что В некоторых случаях мы не можем никак скинуть приложение память что изначально цифры там 16 гигабайт которую мы выдавали в принципе была не случайно мы не то чтобы просто ошиблись с тем чтобы выдавать слишком много всем это привело к падению части приложений теперь собираем все вместе у нас есть три разных предсказания три от трех разных правил Мы хотим какой-то функции объединить интуитивно хотелось бы сделать это Например взвешенным средним каким-нибудь но у нас на нашем сетапе взвешенная средняя давала очень медленную сходимость нам все-таки хотелось бы чтобы это было побыстрее поэтому мы стали смотреть на другие варианты и неожиданно для нас работал минимум То есть это такая история получается что мы снижаем пока хотя бы один из параметров позволяет нам снижать только если все три сказали нет мы как бы смотрим дальше если все три сказали что что-то вы как-то что-то вы как-то немножко поторопились то мы еще и накинем а системы естественно теперь когда мы разобрались с тем как получить единое число мы можем Вспомнить о том что база разные для каждого запуска соответственно у нас есть как бы набор фактов база Fight line Вот и предсказанный для них память Да мы можем для каждого построить какую-то регрессию почему она такая страшненькая потому что мы взяли на самом деле достаточно тупую функцию мы взяли изотоническую регрессию которая представляет из себя просто неубывающие ломаную линию почему во-первых это просто берешь и делаешь во-вторых её можно легко обучить на двух-трех точках У нас есть случаи когда у пайплайн допустим всего две базы Ну и как бы что ради этого что-нибудь сложное выкатывать Вот а еще и очень удобно приводить строковый вид мы можем просто вывести ее опорные точки и тогда мы можем положить в базу данных в виде некоторого Лога и просто как разработ даже глазами просмотреть и понять быстро в какой ситуации да В каком состоянии у нас находится тюнинг и не накручивать каких-то дополнительных инструментов не прикручивать визуализацию не думать что такое понятно что сейчас у нас всего одна фича по сути да количество строк и это может измениться Это скорее всего изменится Да и сейчас у нас всего один параметр Мы хотим в перспективе там и больше предсказывать что мы делаем просто заменяем алгоритм если там сложности у него с визуализации и так далее ну там что-нибудь уже прикручиваем до дополнительная разработка но общий подход В общем тот же самый два изучается только алгоритм дальше эту самую регрессию Мы переобучаем после каждого раза после каждого запуска приложения здесь еще один в общем-то плюс изотонической регрессии очень легко переучить берешь опорные точки добавляешь новую информацию получаешь как бы новую ломаную линию все довольно очевидно какие-то более крутые Но более сложные алгоритмы могут например потребовать всей истории там быть капризными и так далее Так у нас все достаточно хорошо и мы получаем из такого сначала константного значения такую прикольную сложную линию как это все работает архитектурно как мы это сделали вот у нас есть наши мальбач сервис который все запускает и над ним мы сделали некоторые прокси назвали его эмаль бач-менеджер он умеет много чего на самом деле Но в данном случае нас интересует что он умеет предсказывать параметры запрос приходит боч-менеджер Почему менеджер определяет параметры посылает запрос дальше типа Запусти мне пожалуйста такой-то пайплайн на такой-то базе вот с такими параметрами хорошо совместить это все приложение выполняется метрики пишутся на HD FS и раз в день оффлайн жаба пробегается по всем этим метрикам агрегирует их и обучают новые регрессии все новые регрессии складываются в лог который у нас в данном случае хранится в игре и в следующий раз когда будет запускаться этот Триггер или батч-менеджер сходит в этот Лог посмотрит Ага вот у нас как бы новая свежая регрессия применит ее ко входным данным получит параметры и цикл продолжается Но что же делать если мы предсказали неправильно в конце концов это все только евристики Да все это может ошибаться тогда очень просто мы просто откатываемся назад У нас уже есть весь Лог мы можем откатиться назад и попробовать то что работало Ну значит где-то что-то пошло не так получается что вот допустим у нас есть итерация которая зафейлилась и мы откатываем откатываемся назад и смотрим А какая итерация даст нам больше здесь один параметр просто ради простоты Да понятно что там функция меняется и это итерация становится активной проходит день и оффлайн Джаба переобучение начинает переобучение с той итерации которая осталась последней активной Таким образом мы считаем что ну чет Мы намудрили в прошлые разы начнем с их точки это все дело мы Достаточно долго нудно выводили в прод потому что понятно что если это просто сразу бахнуть то есть шанс что класс так сломается нужно еще понять Вообще приносит ли это Профит и так далее но когда это все закончилось устаканилось где-то через пару месяцев мы поняли что получили в общем-то неплохие результаты мы суммарно сняли где-то где-то терабайт на контейнер имеется ввиду что вот эти все дельты было 16 стал там 15 или 10 или в некоторых случаях мы с 16 Гб тюнин или до 1 что вообще прекрасно вот сумма этих всех получалось в 1 терабайт главное что мы сделали то что мы хотели Мы сдвинули утилизацию с памяти на ядра примерно 15-16 процентов и это позволило нам существенно расширить непосредственно продукт если у нас было 600 запусков всего в день то сейчас у нас 300 дневных 5000 часовых отдельных запусков имеется в виду и еще добавились новые которые запускаются раз в 10 минут такие микрочи что особенно приятно мы сделали это не прямой тупой покупкой серверов да мы просто снизили Касты эту штуку можно использовать в любых процессах которые имеют регулярность Мы например еще один процесс перетащили на неё Это расчет кредитных коров Мы хотим эту штуку использовать в нашем теле там есть свои сложности связанные с тем что и цель может принимать например несколько входов Да там нужно немножко модифицировать это дело также другое поле для деятельности это расширение списка параметров потому что список параметров Спарта огромен Ну можно начать того же драйвера или ограничением Мы очень думали в сторону Но кажется что с тех пор вышел Третий парк и уже больше не надо потому что в третьем спарке есть динамическое определение количество партиций вот Хотя если оно не будет справляться то возможно этот тюнинг тоже поможет посмотрим и как отдельный бонус Мы в общем-то можем сделать тот классификатор который мы не смогли сделать когда пытались сделать первый подход потому что фактически мы вторым подходом полечили проблему первого мы собрали разнообразные данные напроде просто сделали это аккуратно не вмешиваясь не влияя особо на цикл эксплуатации теперь про то что мы делали не очень аккуратно это все касается слонов комнате слон в комнате это то как сама система тюнинга воздействует на кластер первое очевидная проблема что сама Джава слишком тяжелая мы запускаем все больше и больше нужна обрабатывать все больше и больше логов и к тому же Джаба была написана Так что она работала очень последовательно в том смысле что не параллельно поэтому там получался такой патруше Ты читаешь Файлик ждешь читаешь Файлик ждешь что очень эффективно потому что Спарк тратит много времени на переключение контекста в данном случае и все это время мы держим ресурсы получается что она работала типа 13 часов выжирала что-то несусветное и в общем-то уменьшало тот эффект который сама и должна производить не хорошо что же вы сделали Мы перенесли передвигацию логов непосредственно После выполнения приложений то есть после каждого приложения запускается маленький джойбик который при деградирует логи это все складывается в один dataset и предиктор Джаба уже берет это dataset и обрабатывает его причем мы естественно добавили туда параллельность потому что регрессии могут переобучаться Независимо друг от друга Почему мы не сделали так с самого начала потому что у нас не было менеджера потому что только менеджер мы стали полить джабы чтобы понять что она закончилась А до этого мы этого не делали и просто не могли повесить такой калбек и чтобы не делать все сразу сначала просто сделаю простое решение но потом когда оно стало не выдерживать переделывали на более правильную архитектуру вторая проблема немножко детективная Поэтому я не стала ее сразу вводить на слайд так Однажды утром проснувшись после беспокойного сна инженеры компании он фактор обнаружили что кластер стоит Причем он стоит не просто на нем очень мало что в принципе может запуститься потому что чтобы запуститься нужно сходить ресурс менеджер А ресурс менеджер лежит если его поднять Он снова ложится можете пока подумать что же могло произойти три два один в общем пока наши системные инженеры подбрасывали как горячую картошку два инстанса ресурс менеджера Чтобы хоть что-то могло запускаться другие не менее Отважные люди во всем этом мы собирались и выяснили вот что Помните я говорила что у нас появился новый тип запусков который проходит раз в 10 минут и также Недавно я говорила о том что после каждого запуска спускается маленький джойбик который короче по некоторым причинам триггеры сначала не запускались а потом как начали запускаться и наверстывать упущенное каждые 10 минут была увеличена нагрузка запускалась увеличенное количество приложений каждые 10 минут запускалась увеличенное количество маленьких джабиков перед агрегации эти джамки передвигации поскольку им не хватало ресурсов вставали в очередь получалась большая мы когда хотим запустить яркое приложение вы сначала спрашиваем они запущены ли уже такое же просто чтобы не дублировать вычисления чтобы ответить на этот вопрос нам нужно поднять список активных приложений в активных приложениях входит также приложение в списке ожидания он оказался настолько большой То есть даже типа там пять тысяч домов у нас было в списке ожидания какой-то момент мы его запрашивали настолько часто потому что мы утраились постоянно пытаясь запустить приложение что просто положили нафиг Ну первое что мы сделали Мы естественно выключили тюнинг потеряли эти жабы вот ой второе что мы сделали мы убрали из него 10 минутки потому что зачем их тюнить подумали мы они в общем-то и так маленькие мы поставили им вообще самый минимум самый плинтус и все они заработали мы выключились их из тюнинга все поехало Ну понятно что это такая неустойчивая ситуация всегда может произойти что-то еще из-за чего может увеличиться нагрузка поэтому дальше мы будем двигаться уже к стриминговый джабе которая будет одна висеть принимать себя поток там метрик или логов агрегировать их и дальше уже потом на основе этого будем все это переобучать выступление мое почти подходит концу и чтобы я хотела чтобы из него вынесли Ну во-первых любите логиз парка они помогут вам сэкономить на инфраструктуре и запихивать ваш кластер еще больше во-вторых автоматизируйте всякие задачи тюнинговые и так далее инфраструктурные сейчас это просто сэкономит вам время сэкономит время инженеров и позволит вам расшириться Ну делайте естественно это Осторожно чтобы ничего не сломать на этом точно все здесь есть мои контакты если вам интересно это обсудить приходите Так давайте вопросы у кого появились вопросы поднимать руки последний ряд поехали Привет Спасибо заклад У меня первый вопрос работает да Да продолжай просто Окей вы использовали динамические спатки или статические динамических то прости Спарки Да у нас динамик окна Окей А тогда второй параметр Почему мы не тянули Ну грубо говоря количество экзекьюторов и количество Core Ну то есть грубо говоря можно было увеличить в два раза количество идти кютеров и потом они бы использовали меньше память потому что тогда была бы большая конкуренция За ресурсы Она и так получалось Да у нас уже приложение в пензенку ходили они уже даже с текущими уходили в конкуренцию приложение Это столько бы осталось за компьютеров просто стало больше ну грубо говоря у тебя два компьютера было которые используют 8 памяти а может быть 4 который использует один в этом смысл Просто если мы уменьшаем количество экзекьютеров по динамической локация же умненькая она просто их автоматически увеличит если нужно И даже это позволит просто больше параллелизма сделать потому что ну грубо говоря да максимум до четырех А так она могла бы до 8 и на каждом нужно было бы меньше память у нас максимум редко достигается Максимум у нас 256 excuit of обычно в среднем максимум где-то 100 это такой прям потолок 256 редко кто использует и вряд ли кто будет выходить за него так все да все Спасибо Давайте вот слева молодой человек Валерия тебе нужно запоминать лучший вопрос Спасибо большое за доклад очень интересно я знаю чем я в понедельник займусь на работе такой вопрос сначала просто пояснить то есть мы знаем сколько у нас базе лежит да то есть ну то есть на вход Это количество подается или мы это узнаем во время выполнения задачи мы сейчас мы узнаем это во время выполнения задачи до таких метаданных не подается хорошо И второй вопрос рассматривалось ли это раньше либо это вот сейчас как на будущий задел смотреть на сам код задачи потому что операторы не все одинаково полезны как было сказано то есть дроины есть какие-то агрегации еще что-то иногда это очень любят что-то посчитать нас парке потом включить пандас и что-то посчитать на мастернодеток я просто стал я поняла вопрос смотрите это хорошо потому что он позволяет прояснить свои задачи смотрите У нас есть ресурсы которые требуют само приложение чтобы выполниться Да как бы не было написано код У нас есть величина контейнера которую мы задаем нашем случае величина контейнера больше вы говорите А что если вот это уменьшить Но вот это остается таким же даже этот зазор становится еще больше Наша задача была в том чтобы уменьшить Вот это соответственно если мы дальше оптимизируем сам код приложения да то тюнинг уменьшит еще и экзекьюторы и соответственно также добавлю что если там что-то поменяется и нужно будет наоборот больше выделять то естественно тюнинг тоже адаптируется потому что мы каждый раз все переобучаем хорошо спасибо Так давайте дальше много вопросов на первом ряду есть и в конце есть Так давайте на первом ряду так Напоминаем можно задавать вопросы в чате Кто смотрит трансляцию тоже пишите в чате также участвуйте Валерия очень приятный доклад честно говоря больная Тема тут я думаю вопрос ресурс менеджер явно показывает и управляет ресурсами которые мы выделены но есть часто происходит расхождение ресурс менеджер говорит что у меня ресурсов нет а хост говорит что меня еще много ресурсов потому что мы выделяем память для Java Java не всегда весь используют вот после вашего тюнинга вы не наблюдали как показывает пост загрузку своей памяти потому что расскажи между ресурс менеджером и хостом есть серьезное расхождение потребление памяти С одной стороны да с другой стороны у нас настолько высокая утилизация что практически 95 процентов и выше это общем-то норма ресурс менеджер или в кости потому что у нас была проблема что у нас висят все в пэндинге да то есть приложения но Хосты свободные то есть мониторинг кастов показывает что на костах памяти есть просто это оверхеды там дополнительные так далее неверное распределение памяти То есть вы не наблюдали вот такой не ведете статистику в графане потребление ресурс менеджера сколько ресурс менеджер показывает загрузку кластера и одновременно потребление хостов сколько они показывают потребление памяти приложения userman User пространство юзера Сколько потребляет памяти Мы честно говоря такой проблемой не сталкивались за кластером в основном сидят другая команда команда дивопсов Я не могу сейчас сказать если у них такие или нет так хорошо вопрос по центру поднимай руку тебя прям подойдут Спасибо за доклад очень интересно и полезно и Извините если я не услышал Это в начале доклада но есть мы рассматривали только оффлайн жопы Ну которая одноразово запускаются были у вас планы адаптировать алгоритм для streaming Job и если вообще такая возможность чтобы для стриминг от запуска к запуску менять таким образом ресурсы и лимиты а мы не рассматривали потому что у нас в том числе нет спак стриминга как такового то что у нас онлайн это по сути очень мелкие микробочи которые опираются на Киеве или storage так сначала просто есть поднимать руки Давайте третий ряд Да спасибо большое было очень интересно у меня пара совсем глупых вопросов то что я вообще не из мира bigdata Я даже не знаю что такое Spark ладно просто похлопал со всеми вот да вот почему если памяти мало Почему приложение начинает что-то сбрасывать на диск это что Java свопится так О нет это просто сейчас парка чтобы не падать чтобы не падать как бы просто Спарк делать Все вычисления в памяти Да у него фишка такая что он как бы все загружать данные в память как бы все вычисляет да потом там отправляет результат Вот Но если все не помещается в память то начинает сбрасывать на диск чтобы не упасть понятно И тогда второй вопрос почему такая странная регрессия То есть почему не накопить например много-много точек Ну вот я имею в виду зависимости памяти От количества рядов Я так понял да там внизу Роуз было написано вот почему Например это поле точек не проксимировать линейной регрессии какой-то да Вот зачем вот эти ступеньки нужны потому что как я говорила для может быть всего две точки А для какого-то их может быть много для чего-то Мы в принципе не можем накопить информацию и так будет много и изотоническая регрессия Да вот такая вот странненькая да оказалось довольно удобной для обоих случаев но вы же вот эти Джо б запускаете там каждую ночь значит А у вас по координате иксов Наверно нету разных координата X Это количество мы запустились на базе с на базе такого-то размера на базе такого-то размера если у нас нет других баз то у нас не будет других точек Понятно спасибо и последний вопрос Правильно я понимаю что это задача родилась из того что у вас дефицит оперативной памяти а центральных процессоров как раз много да И вам надо просто больше загрузить свои центральные процессоры поэтому выжимаетесь по памяти Ну более-мене так-то это довольно неординарная ситуация То есть у вас просто железо такое куплено нет это просто особенности того как как выделяются контейнеры да то есть Спарк Собственно как платформа для эксклюзионных отчислений к концу выступлений это сказала собственно и он запускает мастера и различных вокеров есть какие-то параметры Да это некий процесс у которого есть оперативная память и есть какие-то и у нас не хватало общего общего Пула оперативной памяти просто слишком большие выделяли контейнеры контейнер это не докер контейнер это какая-то сущность внутри Спарка внутри явно скорее Понял все Спасибо не только у Докера есть контейнер так второй ряд Большое спасибо за доклад у меня будет тоже наверное чуть глупый вопрос я чуть-чуть знаю спатки у нас была Недавно проблемы с тем что там не хватало оперативки не в executере а в драйверы и можно ли это же подход использовать для того чтобы определить сколько именно нужно устанавливать запуском конечно кстати пока еще не занимались этой задачей но технически Да конечно все это есть в логах нужно брать и использовать спасибо"
}