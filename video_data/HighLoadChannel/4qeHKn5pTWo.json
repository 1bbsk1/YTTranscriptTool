{
  "video_id": "4qeHKn5pTWo",
  "channel": "HighLoadChannel",
  "title": "ZFS как архитектурное решение для резервного копирования хостинга / Алексей Афошин (Timeweb)",
  "views": 1580,
  "duration": 2412,
  "published": "2023-01-19T05:55:16-08:00",
  "text": "всем привет меня зовут алессия фошон я работаю системным администратором в компании timeweb тем его докладывает резервное копирование виртуального хостинга он будет полезен тем кто сталкивается с резервное копирование горячих данных и собственно расскажу почему мы пришли гтс и что мы имели ранее в этом году компания той веб сполна вас уже 16 лет как мы работаем занимаем как мы занимаемся виртуальным хостингом той в больше чем хостинг и в прошлом году мы запустили собственные облака с качеством биллингом и возможности гибкого масштабирования инфраструктуры наши стойки разнесены по 3 дата центра и слова стойках собственно сыграл столько делится на два типа это севера виртуального хостинга на которых лежат клиентские данные и сиро для бэкапов то есть тораджа в каждом 3 каждой стойке мы производим обязательно резервное копирование то есть неважно клубе человек заказал то есть резервное копирование будет разводиться обязательно что если представляется виртуально хостинга это типичный лунном linux and unix apache mais quel php он служит для раздачи на статического и динамического контента на не нужно вертится со москве хочется заметить что данной на серверах довольно разрозненные то есть большие маленькие файлы и так как у нас было ранее настроена система резервное копирование был типичный типичный дтп в клиент является primary устройство сторону secondary циклизация на серверах допускается по некой очереди чтобы не забивать сторож и считается успешным если синхронизации 90 процентов собственно все просто далее нас тораджи делаются ставшего первым и таким образом можно иметь состояние backup а за несколько дней хочется сказать что на клиенте мы имеем быстрый ssd диски на сто раз же медленно излишки что мы мы поставили себе цель сделать 3030 бэкапов чтобы у нас было 30 ежедневный backup для клиентов также мы еще очень хотели проапгрейдить наше железо и получить 8 терабайт против двух хотелось бы лишь сократить время backup а то есть здесь она довольно сопоставимая то есть 30 минут до утра байт данных против 128 терабайт данных то есть практически одно и то же но мы хотели чтобы было меньше и все-таки мы смогли это сделать чуть позже расскажу как также мы хотели чтобы нас было три с резервной копией против трех и если ранее нас бэкапы делались через день потому что просто не успевали сделаться за ночь то мы хотели делать бэкапы каждый день вот так же мы хотели чтобы наша стороны же не сильно разрослись по объемам проблемы с которыми мы столкнулись собственно это мы попробовали толстый тонкий snapshot lvm то есть чем больше толстый слышат flv мы делаем тем меньше у нас тем медленнее у нас идет организация тонкие snapshot она тоже не подошли потому что мы не знаем сколько изменения пролетит и требовалось изменять подгонять размер тонкого stock photo при сайте его на лету что тоже занимала определенное количество времени также дбд чтобы избежать фризов файла системы на клиенте нам приходилось подгонять скорость синхронизации и в общем то это было очень и очень удобно также хотелось избежать избыточного потребления место нас тораджи дело в том что на ленте из другое устройство чтобы сканировать осторожно мы должны настороже создать такое же грибы и устройства такого же размера и не важно сколько там сколько данных находится найдет на этом устроить на устройстве клиента то есть будь то 500 мегабайт или два гигабайта мы должны сразу выделить место и это не нравилось также мы упирались в дбд гонку то есть идет синхронизация если клиент в этот момент впишется данные то мы должны написать их на сторож тем самым мы можем попасть в эту гонку и backup может сильно растянуться это вы тоже мы хотели хотели избежать мы хотели также уменьшить время бэкапы и у нас это получилось мы просто хороши для сеть до 10 гбит и мы копы стали делать быстрее ну по факту backup один backup стал делаться ровно столько же сколько делался но так мы могли нагружать сеть параллельно запуская быка по на разных серверах вот и это заработало коллега предложил использовать zfs эта идея показалась очень интересный потому что бы к при помощи шатов нас избавляет от гонки также на элементные снапшоты содержат себе ну портретики только данные то есть у нас нет у нас нет большой избыточности в итоге backup на странах стали занимать меньше места также на клиент не подстраивается не нужно подстраивать скорость бекапа под случае с дер беллена приходилось подстраивать скорость бекапа под сторож подверженный сторож вот случае ztf с этой проблемы не возникает мы просто грубо говоря через scp копируем став schott и здесь нет некие сложности до плюс ко всему ранее у нас пользовался и xtd rbd в.м. здесь же все но заменяет fs то есть это единая единая система также fs использовать копин в right at дает какую-то большую надежность используя файловой системы как мы побили сторож и себе виртуального хостинга собственно насер виртуальном хостинге мы создали 3d to set a 1 под москве один подход собственно это требовалось для того чтобы иметь разные настройки data set of например как размер блока или кэширование которым к поговорите попозже и это сет с резервированием о нем я расскажу позднее зачем он вам понадобился собственно сами параметры ctfs они взяты с советов от разработчиков с чуть позднее будет ссылка но при окончании зации москве лесам производит sing то есть нет смысла короче делать двойную синхронизацию поэтому используем стандарт размер был размер блока собственно он выбран эмпирическим путем это размер блока как раз для для данных пользователя где п п где картинки чем больше размер тем более высокую производительность мы получаем но стоит понимать что чем больше размер тем больше мы про пространство требуем да ну да больше конечно для бога кэша то есть если мы расширим статику то быстрее и отдаем если мы копируем даже php тег скрипт и помимо вот каша то тоже неплохо чем больше кэшировать обзоре работы да нет смысла в кэше и на т.п. мы искали сам обладает собственным собственность тем каширования и собственно параметра 16 килобайт взят с совет от разработчиков там очень много рекомендаций по москве по адресу по различным сервисам при настройке ну в общем то вот ссылка как раз по ней можно и ознакомиться как раз происходит backup ну все просто нас всегда есть единство счет мы создаем второй snapshot данные начинаю писать у него и 1 ст насчет на копирую на сто раз по окончании копирования мы удаляем первый snapshot нас остается один snapshot из проблем данные будет очищено с диска только тогда когда мы удалим слышат то есть здесь мы можем капнуть переполнение и следует следить за станет свободное пространство на диске это важно потому что он чуть позднее об этом скажут для быстрой очистки диска чтобы если предположим все вот у нас здесь заполнился мы на нашел все фризить мы можем создать второй snapshot и удалить первый без резервной копии это не нарушит косить этой связи резервной копии на сторону той следующий резервной копии окрас принесет нам инкрементальные данные на сторож в гости этом виде то есть как будто бы у нас насчет и удалился за трепал это отличная утилита разработчиков с она позволяет очень хорошо оптимизировать рутинные задачи по резервному копированию в ней можно строить много разных схем резервное копирование мы пробовали 3 до интереса первые две и в общем-то push по таймеру то есть мы можем задать периодичность выполнения наших задач будь то раз в секунду 1 минуту раз в час раз в день не важно то есть здесь именно таймер и количество копий которая нам требуется сделать но это нам не особо подошло потому что там такие удобнее было работать череда me push push по крону тоже может кому-то полезен здесь можно задать по крону выполнение , который запустит озадачу также задать количество копий которые нам требуется хранить нас тораджа но да да и за бы сказать что и здесь в во всех случаях является инициатором клиента да это самый скрипт который запуская задачу выполнения бэкапом за трепал пул здесь инициатором является сторож то есть что в принципе удобно когда у нас есть много соберу виртуального хостинга нам требуется построить очередь задач и управлять ими и нас тут же это сделать проще всего потому что от едино . где мы знаем чтобы кататься данный момент наши пользователи имеют возможность восстанавливать зарезервированные данные для восстановления используя панелях здесь полете может выбрать дату территорию который хочет восстановить или файл он может либо восстановить данные либо скачать архив с этими данными и как нас производится восстановление файлов на сторон же мы имеем город snapshot of эти снапшоты мы копируем и монтируем в директорию но после создания stock photo no photo сразу же монтируется директорию и это позволяет проверить что во-первых он создал что в рабочий что живут вот в 3 посмотреть что проверить что там есть этот файл и если что-то не сработало на систему это рынка zabbix реагирование и в общем то это уже реакция какое-то действие так вот мы примонтировали все свобод и в директории на сторону же а потом примонтировали эту территорию на клиент монтируем мы на старых играх мы монтируем через samba потому что fs есть неприятная блага если мы pfs примонтируем снапшоты с файлами имеющие одинаковые но ты-то отрывая один файл мы хотим и красоты бак открываю 2 2 files такой занудой у нас будет открыт первый файл и это неприятно поэтому мы там используем samba на да на старых играх новая драка уже используем fs и в общем то вам не нравится мы потом переделывать на пирсинг а потому что постоянное соединение это не очень удобно когда требуется провести работа на коммутаторе требуется всех отмонтировать все шары потом ремонтировать их заново и это это некомфортно backup маску и выставление бы к поиску или тут все просто если вы работали с левым то наверно понимаете что там есть изменяемость потшоты мы можем просто поднять mais quel на том снабжать и котором хотим и он нас ведется с давай сниз работает в фсс нам что-то неизменяемый но мы можем сделать клон из вас там что-то и поднять мой сколь и уже на нем а потом через москаль дам скопировать данные уже либо в архив клиенту либо уже на восстановление сер виртуального хостинга какая здесь есть проблема размер баз на одном северном составляет где-то порядка 700 гигабайт и это много таком случае москаль поднимается порядка 20 минут что можно было бы сделать в отличие в под расписку или есть типа int и от хорошо это удобно то есть мы можем вручную сделать чекпоинт и сделать и потом после этого сделать отчет в москве речи поинты в ручной мы сделать не можем лет типа int этом делаются по определенным схемам и их частое использование просаживать иной производительность мы делаем всевозможные слухи которые ему только можно придумать и после это делаешь snapshot собственно это помогает много экономика скорости но все-таки человек большого количества blitz это долгий процесс да для чего нам понадобился даст резерват перед начала внедрения я протестировал переполнение ctfs как он себя поведет с этим то есть я заполнил dataset полностью под завязку затем сделал snapshot он создал все хорошо сделал 2 3 4 5 10 snapshot и на доме snapshot of у меня файловая система вошла в ребенке он не позволил больше создавать snapshot он не позволил их удалить он не позволил больше ничего сделать с этой файловой системой и это было неприятно то есть проводя много тестов в 3 случаях из 5 где-то на пятом на шестом саша те короче я выводил с тем в ребенка что я придумал можно создать еще один сет зарезервировать в нем некое пространство и когда мы 2 таз от пули видел достояние неизменяемое то можно освободить в резервном то свете резервирование и пирата свет выйдет из этого состояния мы резервируем примерно 100 гигабайт сделано это для того чтобы дать какой-то дать время на освобождение дисковое пространство и до чтобы не хапнуть этот баг арк его нужно мониторить собственно придя на сдав назад fs на столе мониторить диска пространство в прям очень arc s суть в том что у нас пользуется при том что нас на севере очень много файлов примерно 135 миллионов на каждом и если мы забьем под завязку выше лимита арк мета либо arc de но ты-то у нас начнутся фрезы файловой системы ну для примера у нас скорость записи было 200 мегабайт секунду после забивки арк меты на скорость падает до двух мегабайт секунду и это очень плохо за этим нужно следить мы подобрали для себя какие-то определенные параметры которые нам подходит вряд ли они подойдут всем на 8 терабайт воспользуется примерно 70 мегабайт оперативной памяти выделяется для арка на 4 да работа у нас выделяется ну 30 40 гигабайт оперативной памяти вот арк до в качестве источника данных можно брать либо утилиту аркс у моря она очень удобно очень очень приятно показывает текущие данные либо через про ргб rock art стад для тебя скриптов например для zabbix а приготовилась очень хорошо собирает данные после tfs то есть можно его использовать там есть готовые квартиры года мы используем zabbix и собственно вот триггер формула триггера по которой мы рассчитываем то есть это что требуется реагировать надо информация об этом мешке а ты коротко ямой сам мы его не используем мы от него отказались полностью проблема связана с тем что мы не можем его нормальной кэшировать но если вы нормальный может нормально кэшировать и он сильно просаживает файлы системы по производительности мы написали костыль который отслеживает по тихой создание акита файлов он вычисляет что аха создался создалась таблицам и сам и он принудительные конвертирует мы не можем просто взять вот так и запретить не используйте мой сам нет наши клиенты могут устанавливать дам по нашим клиентам использовать разных cms которые требуют используемой сам но мы можем выхода конвертировать в и на тебе как вы заметили когда место падает ниже пяти процентов возникают не плане непонятные неполадки с файловой системы просаживается производительность возникают какие-то ошибки и стоит всегда держать свободное пространство не менее 5 процентов причем неважно это нас ректора байтах на 8 терабайтов то есть эти пять процентов нас на нас влияет везде так ну хочется подвести итог начали зрение участвовало два человека мы раскатали ctfs на 6-8 дилеров где-то на 2 месяца посмотреть что с ним будет то есть мониторить ситуацию и после того как смогли сделать это первично первичную интеграцию уже стали приносить туда массово пользователей потом подключилась еще три человека для внедрения это заняло еще три месяца для полной интеграции уже во всю нашу инструктору и дальше уже подключилась команда инженеров который занимается мониторингом реакцией вот были написаны инструкции здесь вот дела чего мы добились мы сейчас на снизили потребление ского пространстве на серверах но на клиентах она выросла и быстро скрылись уменьшилось количество костылей для резервного копирования мы забыли о проблемах с с отчётами с резервными копиями случае если вам приходилось постоянно чекать кастет из сортов зато с это предусмотрено но вне самой половой системы мы стали более бдительны к мониторингу более длительных завода пространство более длительной карту ну и хотелось сказать что нас есть стенд по таиланду и всем кто подает нему всем то там зарегистрируется мы дадим бесплатный тест на пару месяцев но если пользуетесь то сразу приходите будет интересно поделится своим мнением вот ну у меня все если у вас есть какие то вопросы пожалуйста задавайте так друзья а поднимайте руки будем передавать микрофончик так вот первый ряд собственно есть вопрос так я пока спрошу тоже первый ряд нормально вопросы я сейчас значит подискутируем просто обсудим а можете рассказать какие-нибудь самые такие жесткие факап и которые были в процессе переезда наверняка безболезненно все это не прошло самый жесткий fork-а был с орком мы его пропустили и какое-то время не понимали почему у нас происходит деградации производительности вот когда мы нашли в чем проблема сразу же по теме мониторинге сразу же написали костыли которые могут динамически изменять размер размер арка метров на ретро формулам и в общем то проблема была решена все понял так давайте говорят добрый день спасибо за доклад приятно было увидеть много технической метки вопрос такой 20 вопрос вопрос 1 экспериментировали вы с экономии места через дедов и компрессор и второй вопрос вот старая война за tfs реплика it против forcing были какие-то эксперименты натурные на вашем поле и почему выбрали 5z песенки кейт немножко не расслышал почему выбрали за tfsi пикет против forcing да спасибо собственно с начну с первого просо с компрессией мы нет не игрались мы начали использовать 084 став с потом мы обновились на 0 8 6 см являются на 214 и на 214 мы уже будем играться с компрессией дедупликации то есть это будет действительно интересно интересно пробовать почему выбрали стс публикация против расинга ну во-первых если я правильно понял вы про восстановление ведь виду или до а3 нас то что то но под капотом там вообще что-то типа саша scp в за трепал используется и но для передачи для уже накатки то есть на карте что штатная помазав с and вот и почему мы не стали использовать немного в каком моменте стоило бы использовать ctfs при стив так и запретив она получается да воскресив потом расинг потом fsn да ну нам показался удобным и стали использовать за трепал и там уже включены как бы все моменты проверки все моменты по оптимизации и нам даже не нужно думать что вот мы сначала копируем datasette потом короче это на котором сначала нам нужно не забыть порядок snapshot of нет arsenka за трепал об этом уже позаботиться за трепал довольно луной утилита также еще покажет статус резервной копируя сколько прошло сколько осталось вот так что мы просто не заморачивались так ответили на вопрос а так перегорят да добрый день георгий мельников контрибьютором зато просто не мог пройти мимо такой темы вопрос номер один вы упоминали что у вас при старой схеме был дагон данных при изменении после бэкапов хотя утащить а в итоге внимание со snapshot of disco файлов или чикаго быка по вот то есть что вы догоняли так с чего мы снимали снапшоты наверное аппарат номер один набор блочное ли у вас бабы капы или файловой backup на делается а выявить виду с по старой схеме ранее а ну так сейчас я пока немножко там просто была фраза про то что догоняем данные после изменения клиентам и это намекает на никогда стены на клиенте мы использовали в.м. то есть на клиенте устройство rbd блочные устройства рдд на котором я xt вот и методы больше устройство рдд синхронизировали с больше устройства были настороже то есть на клиенте срезали работаю соответственно бегать вдоль синхронности до выхода слышал извините средства менты работой синхронностью добивались да и не и консистентной и получается да ответственно dpd то есть когда сильный за достигло 100 процентов мы считаем что backup сделался полностью а потом уже на сто раз же мы делаем сам счет этого герб это устройство спасибо же вопрос следующее вы упоминали про двойную запись моей съели и приводили конфигурацию из офиса который является стандартным симка стандарт но это дефолту вначале хотел тащить а со стороны мыске литвы что-то меняли в итоге со стороны москве рядом и на насколько помню изменили w райт изменили воду наверное я сказать не смогу и я просто потому что приведенные вами параметры это дефолт зато fission на долю записи атмосфере не исключает сам себе я просто хотел тысяч ну стандарт дефолт до этого соответственно в маске или надо это да настраивать чтобы древние записи не было это мой съели отключать д.б. ну это это дефолт но можно конечно для большей надежности изменить его на конкретно принудительный sing вот и не стоит иногда не знаю иногда наверное стоит когда это данные не особо горячее и какие то не знает он может быть ключи хранятся но чем это может финансировать это вы собственно мы данные по моей школе взяли с performance tuning от разработчиков с там немного параметров буквально там штуки три или четыре и это позволяет выводить на и скрыли чтобы он работал понял спасибо еще вопрос о сыну и мешки с супер там центр вместо этого чем-либо еще для резервирования на трон записи то есть модель какие промышленные там на 0 по железу да по железу и мной и но и мешки как они переварить его продажи психоз превосходно первое время были как раз используется назад из нас пользуется raid 0 и этого позволяет выявить сбой на и семье диски практически сразу вывод то есть когда у нас синхронизация между двумя устройствами разваливается по уходит где great и мы по ошибке мы видим что какой-то устройство стает потому можем стать сказать что-то плохое устройством его меняем вот понял еще упоминали про размер ваших ног backup имеет угол сырое место или полезные приставки это было конечно второе пространство и полезный объем в рука зале сотни терабайт там по моему а но по фактам 90 с небольшим то есть это 100 трогает взята просто для округления но да это полезное пространство которое мы можем использовать после учета и последний вопрос про резервированию свободного места а перед этим вы забивали все на 100 процентов до вас было всё нормально других файловых систем на 100 процентов до были случаи когда нас резко возрастала запись и что-то например уходила мимо квоты или что-то уходила или что-то происходило странная то есть это когда много клиентов что-то странное может произойти вот и да у нас были случаи когда сбивалась если пространство сто процентов и северус принудительно порядке быстро сбежали и имела до использование то есть до этого вас была практика то что вы reader или места нет нет такой практики не было пока все почему-то cпасибо а первый ряд еще вопрос есть а извиняюсь ну да практика была штатно резервирование на 5 процентов то есть это файла системы а ну нет я имею то что по факту у вас резерва не нужно в любой файловой системе ну не совсем то есть текста 4 а ты переживала очень мягко не было такой проблемы как вот z fs потому что во первых переполнения плохо переживает то есть прям чувствуется что все шатает падает производительность файловой системы и также на неприятный баг когда мы можем ввести фару систему в редон или большие туда никогда не вывести очень не хотелось его словить вот поэтому назад с мы зарезервируем гораздо больше то есть там 100 ну то есть во первых мы держимся обязательно свободно пространство этим триггером по месту то есть это чтобы было там не менее 10 например короче + ваши резервирование вот которую мы можем использовать в какой-то критический момент вы по поводу свободного места на самом деле за tfs по умолчанию резервируют дополнительное свободное место как раз для этих целей чтобы не встать раком в ненужный момент и этот хотел посетить а вы то есть вы ловили на баги когда вы в редон и и заводили его придворных настройках это баги они заводили дело в том что даст возразили верует какое-то место но это место можно записать шутами то есть информация метаданных snapshot of но вот почему и как раз таки вот служат этим спасибо у меня вопрос такой есть чем организованная вообще регистрация всей системы бэкапов то есть если у вас допустим хотя бы в районе 100 стоек да то есть как бы то все это должно жить как то эта постановка на backup это среднюю link какой-то централизовано система управления нужно бы чем она реализована и сколько трудозатрат по времени то есть это у вас заняло то есть вы просто говорили что очень мало людей делало данная реализацию до его за короткий срок то есть как это выглядит вот в архитектурная глобального вся система бэкапов основанный на , да спасибо за вопрос у нас используется самописный самописная api с виртуального хостинга снова писано пик стражам странах есть написанные демоны которые выполняет резервы как который регистрирует резервное копирование все это накатывается при помощи salto то есть мы перешли с по питона salt и в общем то сколько времени это длилось шестнадцать лет сколько существует хостинг то есть мы это модернизируем вот эта система работала когда еще вы использовали трдд да вы просто перешли той реализация к этой перепили направления да да да просто из немножко изменили схему резервное копирование ингрид алгоритм как бы остался тот же то есть это потерял вот и нам не пришлось придумать заново спасибо так можем еще немножко пора создавать так еще один вопросик вспомнил вы упоминали использование скринь дамп для восстановления из резервной копии хотел тащить рассматривали ли вы другие инструменты например экстра backup берковский потому что там как раз есть механизмы ну во-первых вас есть вторая база до который в общем то изменение не надо можете как есть да переносите целом ты и запускать мой skillet рассматривали ли вы эту сторону посмотрели но я думал об этом то есть если например использовать кен томсон шо ты перка на эстраде by нас используется москве перк он а вот то чтобы избежать возможных блокировок базы данных нам нужно делать снапшоты тот же самый сервер вот на те же самые быстрые диски вот это моё предположение мой дома сил я еще не тестировал если можно мы можем подискутировать калары картина тематикой быть интересно вот и это мы это мои домыслы то есть нам бы потребовалось еще 700 гигабайт на сервере для того чтобы делать эти снапшоты вот если это возможно сделать на какой-то удаленный сервер на медленной удаленность который имеет медленно и жди диски и не особо богатую сеть не тут скорее со стороны восстановления распад его потому что snapshot вам уже достаточно то вы уже собрали передали зад офисе у вас все в этом плане гармонирует со стороны установления он теоретически может вам помочь на это в кулуар письмо окей хорошо спасибо причем можно короткий вопрос да по поводу там упала картинка сервера backup а они находится в этой же от стойки до то есть база верно как у вас вот происходит вот это вот если столько теряется но как это обыгрывается ой потеря стойки собственно говоря да и тот упал сервер где бэкапы на сервере мало стойка для нас есть несколько схемы разделанной копирования то есть то что мы сейчас рассматривали это обязательно резервные копии которые включены во все тарифы то есть пользователь за них дополнительно еще не платят у нас есть также резервное копирование по требованию которая делается да там в отдельный центр короче вот это в отдельную стойку вот кто там и собственно зайти быка только бы идет и понимается дополнительная плата и да они находятся далеко удаленно вот в случае если по 3 стойка на нас бывали проблемы с электроэнергией когда один вот отключался и в общем то у нас все продержалась на второй линии то есть нас подходит две линии к стойке то есть как и каждом и серверы подходит до 2 ввода вот ты позволяет избежать стойки и так чтобы нас подала прям целиком все полная стойка ну разве что только не знаю все эти труба лодзь какое то время то есть это но коммутатором выходил из строя и он быстро заменялся резервный встречали короче такой проблемы чтобы нас прямо запас освоить вся стойка то есть все платформы то есть очень редкая ситуация да и особо на она она может быть например по питанию то есть если нас это подпадали например очень сильно напряжение короче это то есть от нас все вышибло на все сгорело вот и но такого не было такого нам надо моей памяти не было так это первый ряд снова то есть фриске не включается полной потере стойки полная потеря стойка в риске не включена нет но она мешает перекрестно просто стойку маппить хотя бы в соседнем . копы перекрестно сделать только сеть да нам требуется сделать немножко по другому и чуть подороже желаю чтоб никогда не случилось за 16 лет видимо не случалось поэтому особо никто не приходило не говорил что срочно меняем систему backup ну да и на крайний случай у нас есть другой тип резервное копирование вот"
}