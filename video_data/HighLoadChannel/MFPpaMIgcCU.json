{
  "video_id": "MFPpaMIgcCU",
  "channel": "HighLoadChannel",
  "title": "Идеальный шторм: когда готовил систему к росту нагрузки, но не успел / Алексей Мерсон",
  "views": 2674,
  "duration": 2412,
  "published": "2023-01-19T07:03:35-08:00",
  "text": "всем привет я рад что вы пришли тем более так рано вообще на первый слог прийти это большой подвиг я лично только на свои доклады хожу утром меня зовут алексей emerson немножко о себе я двадцать лет работы войти больше времени я как разработчик последние склеп как архитектор и также параллельно как независимый эксперт консультируем время от времени может быть вы меня знаете как автор докладов продам и entry в indesign и интеграции и так же принимаю участие в организации конференции hds тут next и сообщества спб тут нет проекта который сегодня пойдет речь из сферы финн тех клиенты в разных регионах мира работает 24 на 7 нагружены 10000 транзакций в минуту в среднем до 30 тысяч максимуме и построен он был по архитектуре монолитно изначально после чего начали распиливать на микро сервисы считается что 1 одно из преимуществ в микро сервисной архитектура это возможность выбрать любой язык для написания конкретного сервиса команда проекта радостно воспользовалась этой возможностью поэтому на проекте используются четыре основных языка причем совершенно разных стеках о том что бывает каким последствиям приводит большое количество языков я буду рассказывать на тех лет конов в июне сегодня не буду на этом останавливаться про этот сервис хочу еще сказать что ну как любой как многие продукты фантики он под жестким индии поэтому я конечно сегодня постараюсь выдать максимум возможной информации но тем не менее многие детали придется подавать обтекаемо завуалированы может быть заменять и в первую очередь это касается того чем собственно занимался ну что делает продукт поэтому для целей сегодняшнего доклада давайте считать что это был сервис денежных переводов то есть клиенты могут переводить деньги другим клиентам архитектура в упрощенном виде начала описываемых событий выглядело примерно так и что здесь важно отметить что первое монолит и микро сервисы тепло велись в кубе и с клиентами с клиентскими устройством коммуницировали через ктп через engine но сервис процессинга которая собственно и занимался приём и обработка заявок на переводы дипломов на отдельную железку вне купера и связь была через веб сотен через поднятый сомнение websocket второй момент базы на поскрести были тот же за тепло и на отдельные железки и хотя некоторые сервисы имели свои базы как и должно быть по идее в микро сервисной архитектуре но тем не менее и многие сервисы и монолит и что печально сервис процессинга работали с жареной базы которая осталась со времен монолита и третий момент в этом проекте было достаточно большой отдел аналитика в котором нужно было много данных часть данных они получали асинхронной через кафку но тем ни менее большое количество данных вытаскивала съели скриптами по расписанию непосредственный мастер базы если говорить о технологиях то в инфраструктуре использовались докер куберы ansi был соседи был построен на гитхабе для метрик использовали прометею сыграв она для логов брелок из центре в общем-то выглядит все неплохо почему тогда что пошло не так и почему доклад называется идеальный шторм несколько слов буквально об этом об этой идиоме она пошла из собственно с названия фильма который был снят по книге себастьяна юнгера которая в свою очередь написано поверим событиям которые произошли осенью с первого года на побережье сша там в это время образовался ураган и что важно никто сначала не предполагал что у него будет та сила которая в итоге получилось но из такого редкого сочетания воздушных масс с разной температуры и влажности ураган стремительно набрал силу никто этого не предсказывал и привело это печальным последствиям но собственно так и получилось на этом проекте то есть в какой-то момент несколько месяцев его колбасит очень жестко колбасило и сегодня я расскажу как развивались события какие факторы привели к идеальному шторма на проекте что команда делала что-то с этими факторами справиться какие выводы можно сделать чтобы не попадать в такую ситуацию началось это все в январе какого-то года какого года не так важно когда топ-менеджера озвучили планы на год и один из ключевых показателей которые ожидалось кратно увеличить это да или active users то есть аудиторию активную аудиторию дневной очевидно было что увеличение аудитории ведет к увеличению нагрузки поэтому команда из тех рядов архитекторов взялась исследовать что в системе нужно улучшить для того чтобы подготовиться к росту нагрузки и спокойно его встретить и план сформировался такой два направления два узких мест а это жареная базы сервис процессинга но в принципе логично до самой нагруженные такие звенья и для жаренной базу предлагалось от масштабироваться вертикально поставить новое железо перевести гитель скрипты на реплику потому что до этого на из мастер божественного данные чем нагружала и и вынести нагруженный полис таблицы users тут я думаю что многие сталкивались такой же ситуации когда у вас начинается и невинно потом таблицы users они миллиона записи десятки полей и пишется за все что угодно но здесь так и было и одно поле было особенно нагруженным до него приходилось и львиная доля блокировок и проблемы с производительностью и нужно было его вынести в деле в отдельную базу но хотя бы из этой таблицы и также планировалось выносить по-прежнему были распиливать монолит то есть выносить сервисы в отдельной базы оптимизировать какие-то запросы но это уже по ситуации что касается сервиса процессинга опять же тоже представлял предлагалось поставить новое железо по мощнее а в идеале переехать в кубе потому что как я уже рассказал он был в никуда и также хотелось сделать горизонтальное масштабирование потому что ну конечно в идеале чтобы справляться с ростом нагрузки особенно внезапным нужно уметь скалится по горизонтали во втором квартале начали состав согласовывать этот план с бизнесом и было это не так просто потому что business hotel метрик вот diffusion of dune что какие метрики были какие стали как мы докажем что проведенные работы помогут справиться с нагрузкой но ребята не могли предоставить метрики почему потому что во первых не было нагрузочного тестирования а без нагрузочное тестирование невозможно проверить протестировать capacity по нагрузке без capacity собственно capacity по нагрузке есть то метрика которую нужно померить сдо оптимизации после оптимизации и понять какой есть запас по нагрузке когда эта нагрузка придет от реальных пользователей второе ну не то чтобы не было наблюдаемости но она была довольно разрозненная потому что ну то есть было графа нам туда вытаскивали в даже борды ребята ну кто какие метрики хотел такие вытаскивал грубо говоря то есть не было никакой гарантии что метриками покрыты все важные части а значит при нагрузочное тестирование мы могли бы не заметить некоторые узкие места потому что их просто не было метрик поэтому план по факту основывался на опыте и интуиции тех людей которые работали уже много лет на этом проекте и просто знали по опыту где стреляет под нагрузкой и поэтому могли собственный план предложить ну вот так же втором портале начали работу правда новое железо для базы не согласовали сказали что ну вот когда заболеть для приходите вот но с остальным в принципе пошло неплохо этель скрипит и даже успели вклада во втором квартале вынести чем снизили нагрузку нагруженный поле из users начали выносить работа там росли как снежный ком потому что куча зависимости образовывалось на разные другие моменты дальше ну вы нас сервисов и оптимизация запросов решили dios по мере необходимости в рамках как других задач для процессинга новое железо согласовали начали перенос скутер горизонтальное масштабирование отложили до лучших времен с этим были определенные сложности в том числе потому что сервис тей фу и наступает июле и первое что начинает расти нагрузка в июле в общем по операциям в день нагрузка выросла на 30 процентов за июль и стало понятно что ведь она именно здесь где-то и находилась максимальная та самая capacity по нагрузке системы потому что вместе с нагрузкой появились инцидент этот график на прямиком из жира это критически инциденты в процессинге которые без запланированных то есть только внезапные вот это график похож на созвездие назвал его созвездие большой писец и собственно картина инцидентов типичная была такая то есть по метрикам шла резкая просадка по обработанным заявкам грубо говоря про ботан операциям и потом постепенное восстановление конечно бизнес был крайне недоволен первый инцидент случился 2 июля и сначала выглядела середину инцидент на бывает а потом они начали происходить раз в два-три дня но опять первая половина июля все еще как-то относились к этому ну надо понятно надо заниматься проблема серьезная но по крайне мере стороны топов страну всех их не было такого пристального внимания но начали выстраивать взаимодействие потому что требовалось для исследований этих проблем но систему колбасина нужно очень много людей было скоординировать это в первую очередь все те же тех лет и архитекторы для того чтобы ну как-то исследовать в чем же проблема сделать регулярный sing по стабильности устраивали взаимодействиях появились какие-то гипотезы тут может быть проблема там может быть проблемы где-то для гипотез не хватало метриках закручивали по ходу дела это занимало время и в принципе в первой половине июля уже появилась гипотеза что есть проблема с idol in транзакций тут немножко поясню для тех кто не знает как и я не знал об этом на момент событий смотрите как было построено взаимодействия с базы на проекте есть паз крестовая базу у нее есть какое-то количество коннектор это количество довольно ограничены в пост дефицитный ресурс клиентов сервиса много и для них нужно среде не больше чем может предоставить базу поэтому сведение ставится пока баузер это puller который создает пул соединений соответственно клиентам получается ней не больше чем к базе за счет того что транзакции короткие и клиенты то что то делают они делают жонглирует этими входящими соединениями раскидывает их по меньшему количеству исходящих соединений в базу и все работает хорошо работает хорошо пока не появляется соединения некоторые не встают статусе один транзакций что это такое когда транзакция открыта но операция не выполняется соответственно транзакции открыто соединение занята только bouncer не может отсоединение отдать кому-то другому и она как бы получается выбивается из пула то есть пули доступности ей становится меньше если какой-то сервис начинает себя вести плохо и начинает много транзакции вешать статуса иван transaction то радикально мельчает и здесь оказалась бы в поле свободно половины соединения по факту до базы остается там только маленький процент из этих соединений в которые там пока bouncer пытается пихать все что нужно и система начнет колбасить на причем это начинает бить в разные совершенно во все что связано с конкретно с этой базы а это жареная база но опять же доклад назывался бы идеальный шторм если бы все так что спокойно и были бы проблемы только с инцидентами ну наверно их бы решал это самая группа довольно быстро но тут происходит еще два события увольняется сетей и начинается три локации тех рядов и архитекторов в другую страну и все это в середине июля и вот тут начинается идеальный шторм потому что эти процессы начинают самый поддерживать друг друга то есть это эти лиды архитектор это те же люди которые должны решать инциденты это те же люди которые должны принимать знание там передавать знания уходящего сити о то новому сити это те люди которым приходят топы к вопросам по которым они раньше приходили к техническому директору уход технического директора влияет на согласование бюджетов на им и так далее и при этом ли локация тоже грузит этих ребят по полной потому что нури локация в другую страну из-за не просто и вот тут начинается собственно идеальный шторм также параллельно продолжается работа и продолжаются но не заканчиваются то есть пытаются по-прежнему вынести поле из таблицы выносится трудом потому что опять же куча зависимости средств губерта же буксует позитивная новость переехали на новое железо причем ну вот у меня есть график для сервисов процессинга как ему полегчало когда переехали на новое железо то есть предсказание были абсолютно правильными в изначальном плане но также переехала и базы потому что как только собственно система заболел сразу же согласовали бюджета на новое железо для базы правда пришлось переехать временно на арендованные потому что поставка железо это не быстрая история наступает август и все продолжается инциденты продолжаются работы по всем пунктам которые намечены продолжается копание в дом transaction потому что надо было настроить на это мониторинге нас собрать какую-то аналитику по этому вопросу чем занимались детей и а у топов начинает подгорать потому что но инцидент это деньги которые терять бизнес правда они предусмотрительно не говорят сколько теряет денег бизнеса не как просто очень много денег очень много денег теряется сейчас когда происходит инцидент поэтому начались административные меры административные меры для того чтобы тоже как-то стабилизировать ситуацию первое выключили один из типов переводов операции которые выполнял сервис процессинга они были очень нагружены но денег приносили немного поэтому на это отключение бизнес пошел достаточно легко потом ввели фичи фриз потому что иначе непонятно вот выкатили очередной фичу а дальше инцидент инцидент произошел потому что выкатили фичу или потому что старые проблемы которая еще в июле были непонятная вводится через все фичи замораживаются тепло и дальше запретили тепло в часы пиковой нагрузки опять же да история такая же и так система нагружена система нагружена соответственно если происходит инцидент пользователи так страдают при этом тепло и сам по себе может нарушить стабильность даже без связи с инцидентами по разным причинам и влиятельный приоритетные epic у нас использовался там на проекте комбан соответственно была система приоритетов взгляде льды пик на все задачи которые связаны конкретно статы эпопеи с инцидентами и позитивность что во-первых консультировались все задачи разрозненные которые были где-то непонятно где существовали в облаке облаке образно вторых были четкие критерии появились четкие критерии выполнения то есть вот у нас есть проблема есть куча инцидентов что с этой проблемой как сформировать критерий когда мы скажем что проблема решена вот критерии предложили три дня без инцидентов выглядит смешно есть не было печально и в августе середине люди еще начинают массово выходить выпуска то есть три локации увольнению сетевое так далее добавляется отпуска причем они начали массово выходить в отпускании потому что ну как-то синхронизировались случайно да потому что накануне этого года в котором происходят события руководство предложила все масса выйти в августе потому что это же у людей есть связи друг на друга когда они все уйдут связи не будут рваться просто все уйдут а потом вернуться и начнут заниматься обратно работой но ушли все ну не все часть людей все-таки ушла как раз те кто были нужны для решения инцидентов из позитивчика наняли человека на нагрузочное тестирование в середине августа это начал заниматься нагрузки тестами прицельно и очень успешно я бы сказал и в самом конце августа таки закончили основные работы по выносу более столице users ну считайте сами сколько это заняло времени приходится сентябрь к этому времени либо подбивают статистику уже более-менее такую аналитику да по транзакциям и говорят что вообще в момент инцидентов до 80 транзакции статусе один транзакций то есть просто катастрофически выдается пул соединений в по губам siri и также сентябре допилили нагрузочные тесты до какого-то этапа когда смогли повторить ну в какой-то степени ситуацию с нагрузкой и стало понятно что сервис процессинга под нагрузкой действительно создаёт а это он транзакций то есть они не берутся там из воздуха в спокойном режиме именно под нагрузкой выстреливают окей значит processing выедает соединение из пула что делать давайте сделаем отдельный пул для этого процессинга тогда по крайней мере остальную систему он не будет задевать то есть был общий пул разделили на сторону сервиса 101 стал свой пул остальных сервисов свой пул естественно что делятся в какой то пропорция потому что день у нас общих больше не стала соответственно если этот сервис в него стреляют один transaction по крайней мере на эффекте сам себя и понятно где решать проблемы какие полы разделили выходит новый сити а вроде бы тоже позитив наконец есть руководитель и даже получилось три дня без инцидентов прожить на без включения тех отключенных операций которые включили в августе а дальше все начинается по новой опять инцидент и потому что ну когда включили эти операции инциденты полезны снова силы новый сетевой еще не совсем в курсе каких-то процессов тоже начинает ходить дергать людей ночью идеальный шторм и начинает закручиваться по новой наступает октябрь теперь начинают делить пока bouncer еще на более гранулярный пулы чтобы как-то локализовать проблемы silent развалины выясняются что когда был общий пул а этого на транзак становите были незаметны на общем фоне на фоне процессинга который стрелял очень мощно когда processing вынесли оставшиеся соединений в пуля для оставшихся сервисов стало гораздо меньше и те транзакции которые были в центра нашего монолите тоже начали стрелять поэтому ими прицельно занялись и даже пофиксили пофиксили рождение интересно там был сервис отложенных задач которые просто на star задача открывал транзакция на выполнение задачи закрываем транзакцию и задача могла выполняться минуты десятки минут вот вот собственно график в берлоге которые показывают как изменились один транзакций после фикса в монолите также занимались сексом процессинге но не смогли до конца пофиксить там довольно сложная ситуация пастырями забили размазали нагрузку так чтобы просто нагрузка была меньше в единицу времени более равномерно размазывалась но тем не менее тоже как-то затушили и в итоге стабильность в общем этого становилось то есть получили три дня без инцидентов и график общее по месяцам с инцидентами показывает как буквально с октября пошел спад на самом деле даже те которые в ноябре-декабре инцидента были они были не такие критичные на самом деле они просто тоже попали в этот срез инцидентов так про синхронизацию между людьми расскажу немножко значит смотрите я когда готовил этот доклад я про интервьюировал где то семь человек основных участников этой истории и я спрашивал один вопрос как вы считаете что привело к тому что стабильность восстановилась люди рассказывали абсолютно разные начиная от каких-то вещей типа железо для базы вот новые поставили кто-то говорил ну вот что-то в сервисе процессинга на оптимизировали а кто то говорил что мы ввели рые 3 мид и на входящие запросы хотя по факту эти рейд лимиты это было совершенно не про processing то есть это были совершенно другие ридли метана кто-то где-то что-то услышал и вот получалось что кто-то строит лимита на операции сделали причем хорошо бы чтобы оттереть лимиты на самом деле были изначально но позиция бизнеса была такая что все мы должны обрабатывать любую нагрузку деньги поэтому надо обрабатывать все на самом деле были и другие причины инцидентов помимо нагрузки помимо собственно один transaction частичные президенты были связаны с опытом людей частично с тем что они были все перегружены и горели и совершали просто какие-то странные вещи не создают но так или иначе это человеческий фактор приводил к некоторым инцидентом что-то за тепло или криво там было что что-то за тепло или откатили откатить не получилось backup стерся но всякие такие вещи потом внешней провайдеры временами просто отваливались провайдеры интернета где-то в регионах но так как возвращаемся так наблюдаемости не было достаточно наблюдаемости мы не могли понять сразу проблемы там или проблемы внутри то есть на метриках все так же падает количество обработанных операции но по факту это из того что творился провайдер в каком-то регионе кривая логика ретро значит классическая история когда у вас отваливается проблема в сервисе да на клиентах клиенты перри подключаются и таким образом перед подключением нагружают систему еще больше то есть у вас под нагрузкой просадка идет ретро имя система тоже доносится ну про не существую тепло и я уже рассказал и давайте перейдем к выводам выводы будут во первых технически айдан transaction воскрес это зло и кулер не спасет от 1 transaction то есть на самом деле это был как раз внезапный фактор который никто не учитывал в планах по оптимизации под нагрузку об этой проблеме не знали до того как это выстрелило жареная база это зло ну понятно что многие с этим сталкиваются вынуждены потому что запускаю ситуацию по когда уже распиливать базу становится число жены вовремя не вынесли что-то дальше чем дальше тем более сложно эта проблема становится по распиливания общей базы на кусочки нужен препарат и нагрузочный тест в этой системе на тот момент при prada не было был встречен который очень сильно отличался от prada и был пруд средстве на хочешь протестировать что-то в боевых условиях волкам на прот это плохо нужно иметь препарат который практически совпадает по инфраструктуре по мощностям по всему с продам но это же дорого да поэтому на этот довольно сложно выбить бюджет нагрузочные тесты нужны обязательно если у вас система которая работает под нагрузкой тем более если это какая-то распределенные сложная система потому что без нагрузочных тестов вы просто идете в слепую в будущее когда у вас trenet нагрузка не если а когда это приведёт к проблемам нужно наблюдаемость логин дрессировки здесь логе были налоги на момент начала истории хранились что-то типа там несколько дней неделю примерно вот так потому что но место подлоге кончалось и они просто перезаписывали то есть ты начинаешь я не завизировать инцидент недельной давности логов у тебя уже нет дрессировок не было и по крайне мере в этой истории их ввести так и не получилось то есть трассировки сложная история но они нужны в распределенной среде потому что иначе сложно понимать в каких местах локализуются проблемы и нужна возможность ограничивать нагрузку конечно по-хорошему нужно действительно делать рейд лимиты где-то на входе между внешним миром и системой чтобы можно было подкрутить какие-то гайки и нагрузку задавить чтобы пускай там какие-то пользователь отвалятся по крайне мере остановить постам остальные будут пользоваться системой спокойно если говорить про организованного то в первую очередь нужно нанимать и сергей если у вас какая-то распределенная система сложная наймите из ольге которые специалисты которые займется прицельно именно вопросами стабильности одна из проблем в этой ситуации была именно в том что стабильности занимались все но у всех были другие задачи кроме стабильности и поэтому это был такой коллективные сырья состоящие из кучи людей а людям нужно синхронизировать это второй момент синхронизироваться между людьми это очень тяжело я вот когда рассказываю про домой древний антон тоже очень много про это есть про синхронизацию синхронизация чем больше у вас людей занимается какой-то проблемой тем сложнее им прийти к общему знаменателю обменяться нужной информации тем более когда все в огне когда-никогда ввести пространные синьки ретро когда нужно здесь сейчас когда у тебя-то постоят над головой и вот что здесь важно и сделали в этой истории это завели журнал ну то есть фактически документ в google доки в которые записывали все пост мид и всю информацию существенная которая была связана с работать с этими инцидентами и на самом деле если бы этого журнала не было наверное не было бы этого доклада потому что восстановить хронологию восстановить то как все развивалось где какие были гипотеза проблемы было бы просто невозможно вторая важная вещь да это то что был epic в который были консолидированы задачи опять же попробуйте в жире найти какие-то задачи в каком-то произвольному критерию типа но я помню кажется вот было много задач связано с какой-то проблемой если они не помечены тегами не объединены в какие-то и такие вы просто никогда их не найдете задача требует времени иногда очень много времени смотрите железо запланировали весной внедрили летом поле решили выносить весной начали там где то в конце мая закончили на самом деле не закончили то есть закончили в конце августа существенную часть они существенны и отложили и в итоге там я не знаю даже закончили нет и другие нагрузочное тестирование еще быстрый пример человек вышел и всего за месяц уже какой-то результат выдал это я считаю очень быстро экстренные меры приготовьте просто нужно понимать что если заходит далеко то нужно принимать какие-то экстренные меры для того чтобы облегчить ситуацию как с точки зрения сервиса так с точки зрения команд которые в огне и не понимают куда смотреть то есть фича фриз ограничения на тепло и так далее все работающие меры которые можно в какой-то момент применить ну и увольнение релокации отпуска удалит самый неподходящий момент на самом деле смотрите ну вот такие перестановки кадровые скажем так это тоже нагрузка нагрузка на команду и и точно также желательно распределять равномерно они вносят команду какими-то организационными мероприятиями мне кажется что с таким набором инструментов по крайней мере ну как-то можно лучше справится и быстрее ситуации подобные той которая случилась на проекте лучше конечно в нее не попадать у меня на этом все спасибо спасибо ну что ж у нас есть возможность задать нашему докладчику вопросы я вижу что у нас много рук но перед этим если у нас вопрос из онлайна вопросов из онлайна нету поэтому здесь руки здесь спасибо большое за доклад а расскажите пожалуйста вот основная проблема случилась тогда когда сетевого первую шел 2 еще не было а кто брал на себя ответственность not быть пожарным и как он держал ответ перед сухой ударами которые говорили брэда все плохо деньги и так далее как он держал нагрузку свою досмотрите во-первых сити а на самом деле то есть объявили об увольнении середине июля но он на самом деле еще полтора месяца был там но в таком режиме как бы уже ну как знаете хромая утка штата с той политики до когда он как-бы вроде-бы есть но как бы и не совсем поэтому частично он занимался этим вопросом частично другие топ-менеджер там ситуация такая что другие некоторые ступ anger of были технически квалифицированы и они занимались какой-то координации частично архитекторы брали на себя роль координаторов то есть по факту было вот несколько людей которые ну были входящими входящими точками для запросов но фактически это было распределенная такая тоже структура то есть почему игры почему были сложности то есть лично никто не отвечал полностью то есть ни у кого не было полностью всех полномочий все были в какой-то степени какую-то выполняли роль здесь спасибо следующий вопрос алексей спасибо за очень интересный доклад у меня на самом деле два вопроса первый вопрос насколько я понимаю у вас а решение насел состав железе так рассматривали ли вы ему старым и вынесения какой-то части инфраструктуры в облачное решение чтобы по-быстрому залить проблема железом и его почковый benefit ну против облаков акционеры были и как выяснилось после уже не зря до после февраля поэтому в облачную структуру гипотетически шла речь о некоторых каких-то совершенно некритичных не содержащих никаких данных и так далее вещах но до этого не дошло дело насколько я знаю спасибо 2 вопроса правильно понимаю что большая стая составляющая этой проблемы она была административная то есть free но и процесс и руководство расфокусирован и так могли бы ли вы оценить степень проблем да вот технические проблемы в процентах и административной проблема это имеешь ввиду значит проблема в общем school идеального шторма я думаю что технического больше с одной стороны но на самом деле очень сложно сказать думаешь таки технического больше но ну типа там больше процентов 70 например да то есть то есть конечно административные истории ну как-то мы же все люди поэтому как бы здесь понятная среда то есть грубое топа на тебя давиться ты как-то понятно реагируя что нет фрустрации какой-то с технической история когда непонятно почему супер ты инциденты тебе нужно исследовать это сложнее спасибо и последний вопрос удалось ли пройти через эту ситуацию с когда блеск аьлча то есть когда никто никого не ругал или там приходилось прикрывать свою оба служит клеем ругаться я скажу так тех ряды основных монолита процессинга в итоге выгорели уволились после уже вот как после октября и во многом это было связано мне кажется из давлением тоже но тут такая история общая культура на проекте было как раз вроде бы им да я нашла ну исторически но некоторые на pajero эту культуру внутри нее не поддерживали но внешние поддерживали поэтому там было время вот такое вот балансировании на уровне всех уволим и нет мы никому не дней надо заниматься работать вот что то такое спасибо у нас есть еще вопросы может быть на той стороне вот здесь вот у нас тоже добрый день спасибо за доклад очень многие вещи которых вы рассказываете такие дают наверное знакомы большинству участников кто занимался хотелось бы вот просто спросить еще про какие-то измеримые показатели вот этого идеального шторма в росте нагрузки то есть мы увидели 30 процентов роста июле а вот рост за оставшееся время какой-то был дополнительный потому что вот если рассматривать что вот этот идеальный шторм с технической стороны всего 30 процентным увеличением нагрузки но это добавляет ему скажем так саспенса там на самом деле нагрузка росла где-то до августа очень резко потом немножко был спад потом еще рост то есть тут какая история как бы нагрузка с одной стороны постоянно росла а команда была в роли такого догоняющих она догонял уходящий полу уходящий поезд а есть внедрять какие-то меры они позволяли отыграть проблемы которые вызваны предыдущем уровнем нагрузки но уже подбегает новый уровень нагрузки которые перевешивают то что было сделано то есть все меры которые делались они имели эффект но запоздалое получается то есть нагрузка опережала а потом уже начали как бы сами изменения которые вносятся а это серьезное изменение когда начинает баузер там какие-то пулы двигать и так далее они уже сами начали тоже приводить к новым эффектом как я уже говорил да то есть сделали более гранулярный пулы с одной стороны изолировали проблемы с другой стороны у гранулярный полов коннекта стало меньше некоторым сервисом раньше в общем пули хватало перестало хватать в гранулярный пулика с коннектор даже без задал introduction и с этим тоже это вот как бы такой был процесс когда систему колбасило и постепенно подбирали опытным путем параметр которые приведут к стабильности все-таки общая за вот этот период есть какой-то показатель 50 процентов и того стали на 50 процентов больше так для понимания не знаю мне кажется что месяца за месяц-полтора где то наверное можно было бы решить эти проблемы если бы не вот ситуация идеального шторма в итоге да это заняло гораздо больше времени привело к гораздо более печальным последствиям что еще важно да то есть то что люди вот так выгорели уволились то что происходили такие вещи это сыграла роль спасибо спасибо большое простым спасибо за доклад очень интересно было скажите пожалуйста вот если назад отмотать вот до идеального шторма вот с технической точки зрения что можно было сделать по-другому если бы у вас были но бюджет полномочия 100 тыс чтобы избежать этого штурма выводы очень интересно хотите хочется понять что славная с технической точки зрения в какой последовательности вы бы по-другому скажем так запуск сервиса произвели это смотать в смысле прямо вот к истокам сервиса самого вообще продукта или или долг начала запуска может быть что-то забыли сделать либо забыли там с рейндже нет она не ты ли протестировал мне кажется в первую очередь нужно вовремя закрывать технический долг причем здесь не то чтобы вы не хотели закрывать над ним работали но всегда работа над техническим долгом отстают от отстают от того как о них нужно было бы чтоб шли и технический долг это в том числе как раз растаскивание базы на кусочки сделать скота продуктовый задачи которая начинает там еще больше завязываться на жареную базу можно было бы я сделать по-другому и тогда бы она на независимым источники основывалась работала быть независимыми базой но тогда это заняло бы дольше а продуктовой задачи нужно выкатывать быстро но такие вещи на самом деле здесь интересный момент такой про технический долг я прочитал какой-то момент разница менталитетов нашего и западного одно и то же выражение за технический долг воспринимается по-разному у нас и там потому что у нас технический долг а это типа мы взяли ну и отдадим когда сможем то же самое количество которое взяли а у них восприятие сразу что это процент то есть на технический долг капают проценты и работать сложный процент чем позже вы дадите технический долг тем дороже он вам встанет и тем сложнее будет закрывать ну вот здесь можно было бы это делать раньше ну а и сэр я да конечно если был человек который занимался этим было бы проще но нужно же обосновать как то что нужен этот человек пока не занятых нет но такая система стабильна значит стабильности все хорошо зачем нанимать человека"
}