{
  "video_id": "u58CcXYLFnM",
  "channel": "HighLoadChannel",
  "title": "Как менять инфраструктуру при взрывном темпе роста компании / Никита Маслянников (ЦИАН)",
  "views": 1191,
  "duration": 2250,
  "published": "2020-04-14T11:22:03-07:00",
  "text": "всем привет меня зовут никита я руковожу командой devops овцы они с целом администрирования уже больше десяти лет сань работы уже пятый год наверное и сам это один из крупнейших сервисов по недвижимости уже в россии ну и даже в мировом рейтинге тоже там в топе но для меня в первую очередь сам это команда крутых чуваков с которыми приятно решать разные интересные задачи на которых всегда можно рассчитывать на можно положиться на то что команда тебя поддержит это реально многого стоит немного цифр ну то есть сам начал на самом деле начал свое существование где-то в 2001 году но бурный рост начался с 2014 года и вот 2014 года мы значительно выросли мы выросла наша аудитория как следствие выросла и инфраструктура тоже и вот местами этот рост сопровождался какими-то правильными вещами правильными архитектурными решениями где-то они были неправильны и там мы много заливали железом просто брали там ну например тупит какой-то кластер мы взяли туда там еще 20 серверов там 30 raw ну да в многих случаях это помогает но как известно не всегда ну возьмем по 1 там кластер elastico у нас используется как для сбора логов так и для боевого поиска и вот не всегда можно просто взять и добавить там 50 серверов нужно еще менять там конфигурацию индексов добавлять реплики и вот надо как-то правильно его спроектировать так чтобы он нормально воспринял добавление этих серверов то есть изначально правильно построить вот сейчас тоже поговорим и в какой-то момент мы решили давайте-ка все наши компоненты проанализируем и подумаем что же можно такого архитектурном применять вот сейчас про такие самые интересные нюансы которые меняли на протяжении нашего рост я щас расскажу ну мониторинг куда же без него каждая инфраструктура должна обладать мониторингом из этого никак на оркестрацию конфигов тоже когда-то у нас уходило очень дофига времени сейчас уходит сильно меньше расскажу почему микро кластер а это уже скажем так и рельсы положены завернута в процесс расскажу парадигму наверняка это многим из вас пригодится очень классная штука хочется искать проблемы добродушен они на продакшен соответственно вот как это тоже в процессе роста во время роста нужно экономить время на каких-то рутинных задач ах это всем тренировать и при этом чтобы качество не страдала вот эта штука поможет здесь без комментариев это у всех должно быть не буду еще говорить и все же когда что-то доходит до продакшна как сделать так чтобы это было единожды или вообще ноль раз и никогда больше не повторялось поехали немного про сбор логов когда-то нас давно был такой классический как кластер elastic все было хорошо трафик рост количества документов в индексов тоже и мы начали исправляться были некоторые проблемы проблемы какие ну например перезагрузка сервера вела к потере логов там какой-либо сборе там по памяти власти купол и тоже логе потеряли ну в общем если например кто-то из разработки включил типок режим и в какой то момент времени там всплеск был несколько сотен гигабайт час и минуту то тоже теряли логе плохо переживали всплеск очень хотелось это как-то устранить что мы сделали а вот проблемы много маленьких индексов как раз последний что сказал что мы сделали мы внедрили кафку как бы про это решило большую часть наших проблем то есть все логе просто теперь начали сначала скапливаться в кафки потом уже переходить в ну как logs даже ходил и забирал их из кафки перекладывала перешли на полу модель соответственно уже круто отлично и еще интересная штука это орлова ротация индексов тоже эта штука помогает ротировать их не падает time а по количеству документов в яндексе ну или можно по количеству там по размеру индекса тоже ну мы мы решили по количеству документов это помогает избежать как раз накопления большого количества индексов потом например пару килобайт если они у вас растут эпизодически например там раз в неделю опытом приросло 10 гигабайт или там 100 гигабайт так вас будет яндексе фиксированного размера и кластер будет как с ним удобнее будет работать и на elastic тоже это влияет если много индексов лучше этого не допускать вот примерно так выглядит конфигуратора который кодирует индексы это может кто-нибудь скриншоте немножко слов про elastic тоже он был монолитный обычная классическая без разделения на бокс type и обычная система с какой-то конфигурации индексов там стандартно этом 1 шард одна реплика потом мы решили прийти на платформу тоже в принципе популярная тема у всех есть но мы решили ход еще больше сделать много хотов для разных типов индексов таким образом мы изолировали проблемы с каким-то одним потоком логов в рамках этого потока логов но то соответственно не надо чтоб страдали индекс который там в себе содержит логин джинса когда какой какие-то проблемы с лагами приложения например и к стало больше и лучше вот такие сущности разделять разделять методом бокс тайпа то есть вы просто с каждой ноги присваиваете какой-то бокс type там hot hot один ход 2-х тревор и потом перемещаете индексы между этими бог степень соответственно если какая-то но то выпадает то яндексе и relocker релаксируется в рамках одного бог стопа это изолирует как проблему такие нагрузку и стала лучше вот тоже куратором это все делается никакого в принципе рокет сайенс и здесь нет немного цифр то есть вот такой вот наш кластер текущий момент он содержит в себе около 13 миллиардов документов средний размер документа один килобайт но это средняя по больница бывает такое что прилетает же сон и у которых одно из полей это тоже джейсон и у которых еще огромные такие прям документы с ними бывает трудно то есть ну там нельзя просто так взять и считать рпс посту плену только рпс нужно еще учитывать размер документа и вот на таком железе это все работает то есть в принципе вот так и кластер позволяет нам хранить логе около четырех-пяти дней ну больше не надо то есть если надо мы по мере этого можем добавлять либо еще больше хотов либо еще больше дисков или нот в формы там норм собственно второй частью мониторинга является прометея графит тоже не буду вдаваться в какие-то детали когда-то был немножко экскурс когда-то был большой прометей с ним было ряд проблем долго стартуют сложно горизонтально масштабировать нитрит экшенов на какую-то часть метрик мы их начали решать разделили его по критериям один прометей одно приложение и вот таким образом на группе физических серверов у нас есть какое-то количество параметров там некоторые важные метрики мы получаем сразу несколькими прометея мину таким образом обеспечивается как отказоустойчивость что получили в итоге ну во-первых данные распределены если теряем только час теряем понятно в некоторых случаях мы использовали федерацию но это скорее сильного такого пользы для насильно не принесло это скорее просто минорное удобства в какой-то момент нам тоже понадобилось хранить метрики долго мы посмотрели на решение которые есть на рынке попробовали дефолтный ремонт старридж все круто но берешь на пяточки написать скрипт который просто ходит примет его фишка вы складываете эти метрики в графит именно те которые надо то есть мы сначала создали список именно тех метрик которые нам нужны и начали их писать графит там уже дальше они подчинялись стандартным ресепшеном которые описаны в в конфигах графита вот немного про наш график кластер график кластер у нас вполне go карбон карбон сериалы карбон api тоже классический кластер когда-то была одна но до стал таким и вот когда он стал таким тоже стало легче уже как бы есть там 3 сервера например можно сделать 4 5 у нас есть два сервера гетто и вот я сейчас расскажу зачем и to get the иногда бывает такое что в названии метрик приходит какая-то последовательность букв и цифр длинная и на каждую метрику возникает вот такая но то есть о чем рождаются просто пачка файлов в каждом название кэш и таких файлов может быть там сотни тысяч не знаю и у нас был кейс когда кластер размером 2 и 4 терабайта закончился минут за двадцать просто вот из за того что где-то в коде была ошибка и просто на каждый иди какой-то писалось отдельная метрика ну это плохо и так же как бы на пире у него есть такая проблемка а на самом деле где-то описано на гитхабе там как-то ее решают парни вроде но он не умеет показывать метрики из будущего поэтому мы не можем предсказывать мы не можем делать тренды методом под карбон api как мы это решили решить следующим образом мы просто делаем небольшой report по плохим метрикам часть из этих плохих метрик заворачиваем в я так как раз и там они он все время закончив сми дисками но туда можно зайти проанализировать эти названия и потом придти к разработчикам сказать парни у вас счет с метриками вот эти метрики скорее всего до вас не доходит надо бы поправить ноты правим таким образом ничего не теряем все остальное все что не нужно или заворачиваем black hole а тренды пишем музыку мой сквер как бы там все нормально можно рисовать метрики из будущего и в принципе все работает таким образом мы сейчас вот предсказываем когда у нас закончится диск место на жестких дисков жестких дисках очень помогает на самом деле потому что недавний кейс когда мы видели что через три месяца у нас там закончится серверов а тупо например об заказали его там за пару недель собрали нам дали просто alert и куда же без них больная тема наверняка для многих из вас тоже либо была когда-то то что у нас когда-то был такой непрерывный поток алё ртов у нас была там два человека в администрирование и эти alert и все шли им они все были сонные было плохо было много ложных срабатываний но это было когда-то давно сейчас уже не так в какой-то момент собрались разделили нашу группу админов на команды которые каждая команда из двух человек отвечает за свой компонент и просто настроили эскалации соответственно а лед приходит сначала одному человеку потом второму потом обоим потом на всю команду потом на всю группу админов и так далее тоже все сразу стали мудрыми и добрыми приходят ходят на работу с улыбкой тоже хорошо вот ну и ложное срабатывание в какой-то момент просто выделить несколько текстов на просто на то чтобы выпилить все ненужные alert и полностью перепилить вообще инфраструктуру мониторинга и это сильно облегчило жизнь такие вещи не надо оставлять на в долгий ящик лучше просто один раз есть отложить какие-то дела и сделать регистрации когда-то балансе был сейчас стал solstic и вот со стеком тоже первый подход был не очень удачный чем он был неудачный день что роли которыми раскатывали продакшен они были такие скажем так монолитные то есть в роли которая раскатывает фронты это было где-то 2015 году или около того в роли которое скатывает фронты там содержались вообще все конфиги которые нужны это роли ничего не находилась снаружи все было внутри в коробке соответственно такая структура порождало много копипасты вот нового сервера в эксплуатацию занимал достаточно долго времени этого хотелось как-то оптимизировать также по мере того как команда росла все хотели одновременно катитесь он опрос как известно это не всегда может так получится просто так мастер он один а людей много возникали конфликты и некоторые хорошие люди работали руками на против такого тоже делать не надо чем мы сделали с нашей структуры sales т.к. мы сделали просто картер и завернули в отдельную конфиг группу в отдельный конфликт такой сервис описали его отдельно и создали из них и конфет группы и просто можно их теперь через пилар in крутить и из них как из кирпичей строить свой сервер соответственно теперь все можно от мисс регистрация нового сервера сократилась 50 минут где-то до 10 может плюс минус опять же на железных серверов о благах все попроще вот и сильно удобнее стало когда мы сделали для каждого отдельно и окружение в отдельных папочках потому что теперь у нас push мастер запрещен теперь каждый все делает локально в своей папке потом делает но это все в своей ветке делает полу request эти полу request и уходят в ставят правильные лейблы в задаче в жире потом их проверяет робот когда видит что она лэйблы джинс поставлен до прогоню кая тест и прогоняет тесты затем его review это еще два живых человека который смотрят что вода все круто похоже что валидный конфиг валидный код все хорошо и после этого один релиз инженер который ретируется каждую неделю это все катит на бой то есть такой процесс действительно уменьшил количество ошибок на про теперь ошибки замечаем сильно раньше и они напрочь не попадают после вот эти все стадии review ну это как выкладка конфигов по типу какой-то в разработке у нас то есть только сильно облегчённая в планах конечно перейти на пол модель это позволит нам поддерживать весь пруд актуальным и плюс 5 спокойствию сна то знаешь точно что она бы никаких правок нет и все хорошо то есть как-то так микро квартира это интересная тема вот сейчас hyip по поводу микро сервисов же долгое время все переходят на микро сервиса почему деза инфраструктурные компоненты заворачивать на красивую ведь тоже не которые можно например мы начали с elastico вот была у нас только один большой боевой кластер поиска который вот когда почему свете жмешь он как раз elastic нагружается и выдает какую-то информацию но было с этим кластером и ряд проблем тоже игрались с количеством индекс с количеством шагов с количеством реплик это иногда помогало иногда не очень хотелось чего-то большего потому что ряд проблем все равно оставался это например stopsol pc то есть когда там народу приходило пачка запросов ответы которых были огромные какие-то супер огромные и там просто к это повышение нагрузки происходило ну например вот циpкa тоже был перегружен или всплеск трафика на какой-то одной конкретной ноги почему такой-то микро серую нагрузил то это влияло весь кластер это влияло на весь кластер это было плохо хотелось как-то это изолировать что сделали как бы взяли завернули в микро сервис почему нет за теплее лето все в docker и поставили hard для мид и и теперь у нас есть micro кластер ластика который содержит себе индексы которые нужны только определенному микро сервису в идеале один микро сервис один микро кластер это все крутится на одних физических тачках и фактически этот кластер соседям своим никак навредить не может то есть стала сильно лучше теперь стол pc происходит в рамках одного микро кластера и только тот микро сервис который его спровоцировал он как раз и страдает очень удобно ну и лимит выставили да теперь как бы нагрузить соседи наверно можно но очень сложно вот в планах еще внедрить микро кластеров по сгрыз также немножко про хейсман monkey это не тот классический calls monkey который от netflix а это скорее как я ранее говорил система обнаружения ошибок то продакшна что это такое ну то есть это просто dv стенд которой выложили каждую неделю выкладываем туда последнюю версию приложение каждую неделю заливаем кита тестовые данные но обновляем тестовые данные в разных хранилищах я для примера тут нарисовал просто несколько на самом деле их можно количество менять и те которым надо проверить ее направить взяли все и просто берем через api ты был там или какой-нибудь другой firewall ломаемся сетевой связанность симулируем как-то он time так и потерю пакетов потери пакетов это якобы за тут такой то есть как известно что полный отказ это лучше чем как бы медленно и сеятеле лили лучше чем например как то здесь извинения установлена но сервис ничего не отвечают в ответ лучше полностью сломаться чем вот тупить вот и таким образом мы берем и каждый компонент по очереди отрезаем от какого-то хранилища данных это помогает но уже нашли так хотят багов не на продакшене а вот на такой на таком стенде выходит так что если это автоматизировать если это завернуть мониторинг то в идеале вы можете видеть просто процент прохождения например смог тестов на какой-нибудь даже бороде на каком-нибудь графики и уже там например реагировать на alert который настроен этот дашборд то есть это можно автоматизировать у нас пока еще не полностью автоматизирована пока что ещё там есть просто как 2а фишки которые мы по очереди дергай и смотрим на результаты прохождения тестов которые потом передаем в отделке и они это анализируют ну и соответственно предпринимать какие-то меры если там есть что-то нехорошее тоже очень полезная штука что же про резервное копирование наверное уже очень много докладов было многие решают эти вещи наверняка все из вас делают резервные копии да кто из вас не делает выкопав есть такие ладно все делают хорошо кто из вас проверяет бэкапы есть такие кто проверяет бы как круто если несколько человек отлично а кто из вас автоматизировал проверку бэкапов такие есть люди которые делают это не руками отлично огонь есть кто-то кто мониторит эти автоматизированной проверки пока пав и получает ольмерта если этот пикап не вправо лидировал ся господи я рад вас видеть всех вас видеть на самом деле на вас особенно отлично ну вот мы тоже запустили этот процесс и сейчас как бы уже мониторим мониторим автоматизировали проверки и проверяем бэкапы но не все бэкапы можно так просто взять и например там развернуть есть у вас и бывать большие хранилища бывают такой backup который просто пока вы его развернете уже три следующих итераций резервной копии пройдет и уже когда вы узнаете что он валит на и это уже будет не актуально то есть нужны какие-то способы проверки backup а разное возможно не только развернуть и прогнать смог тесты хотя это идеальный способ вот просто взять построить тестовый стенд развернуть на него backup натравить на него какие-то последние версии приложений а если приложение отработала хорошо смог тесты прошли наш backup скорее все будет валютной ну то есть все хорошо но также можно например стрелять и сверять количество строк в исходной базе и восстановленном бэкапе то есть перед бэкапом просто отсчитываем все таблички тоже не везде возможно где если это возможно тоже так можно делать добавляют пару процентов вероятности того чтобы как будет валидным и восстановлена бэкапе и можем потом ты построить на ком-то графики либо куда-то загнать ее поставить на это alerts если там отличие сильно какое-то большое то значит что-то не так на больших базах можно смотреть встроенные инструменты например у нас был кейс кассандрой которую мы тоже бы копили ну там окей сделал снапшоты какого-то большого кластера на стон крайне 19 терабайт не так уж и много но и немало пока его восстанавливали точнее бы капель его три дня и восстанавливали его еще три дня и как бы за это время эти данные то что он актуален уже никому не интересно то есть нам нужно знать хотя бы на следующий день или там в какое-то такое время более близко поэтому можно просто сделать снапшоты вынести куда-то проверить их там тем же составил metadata посмотреть что там все хорошо по циферкам и все то есть это уже скорее всего backup годный разные опции есть но бэкап и проверять точно надо это точно надо делать не руками и не тратить на это время потому что рано или поздно человек забудет сделать рано или поздно придет начальник скажет дай backup а ты скажешь и все так плохо итак мы подошли к такой финальной части иногда что-то происходит все таки не так доходит до продакшена некоторые вещи и бывает такое что надо как-то это решать как это делать то есть что мы делаем вот проблем то есть происходит какой-то инцидент как понять насколько он важен как понять сразу все бросайте эти чинить или это как бы просто показалось или там мониторинг сломался или ну не знаю оценить надо как-то этот инцидент вот собственно что мы сделали для того чтобы актуально оценивать и не вставать со стула и бежать там грубо говоря каждый какой-то чьих обижать только тогда когда это надо сделать чтобы это не было ложных срабатываний вот что мы сделали определили во первых страниц и которым будем включить там самые важные если эти страницы ломаются то все точно плохо и мы точно встаем и точно идем все чинить и разделили на какие-то типы типы нам помогли сузить проблему то есть теперь например там главная поиск там подача еще и прочие компоненты мы сразу знаем какая команда ответственна за этот компонент куда идти и сразу приходим коллективно начинаем собираться сильно сокращает время на решение инцидента это реально офигенно пользователя не страдают соответственно теперь мы умеем быстрее решать эти проблемы решаем их по приоритетам и эти проблемы не доходят до пользователей как бы и не повторяются в принципе у меня все выводы можно сделать следующее вот подытожим все что мы делали надо делить elastic на бокс type и пользоваться rollover надо распределять ольмерт и по командам настраивать эскалации разбиваем структуру и сразу делаем правильный такой правильную структуру оркестра таро по блокам нужно искать проблемы до prada это важно и обязательно проверяем бэкап и те кто их проверяет и те кто мониторят молодцы все остальные тоже надо делать так же не допускаем естесно проблем на проводе проблемы на проводе пользователи не должны страдать сервис надо предоставлять 25 минут ваще жесть это я ускорился спасибо за доклад у меня два вопроса во-первых у вас на слайде с графитом был также упомянуть мускул вот интересно как вы его масштабируете масштабируете ли мосфильм и стандартный мастер слив на самом деле мой стиль мы пишем тренды потому что карбон api не умеет рисовать тренды а москаль умеет и только поэтому там был внедрен как бы kindom для этого может что угодно быть на самом деле и ну тут насколько важно это для вас тренды мы в принципе можем потерять и построить заново в любой момент поэтому его не надо как-то добрый день спросил спасибо за такого у меня вопрос честно говоря не знаю вот вы сказали что вы сансы было перешли на сайт стек да я понимаю что какие-то весомые причины были видимо какая то какие то проблемы с самим символом не сказал бы ну точнее как тогда нам нужна была какая-то а фишка через который можно было взаимодействовать с оркестра тарам на тот момент товар по моему уже был но он помощью платным был и отчётном короче в этом не понравился на мы решили какой-то оркестра тарса пешкой выбрать а так в целом наверное это упиралась также в экспертных знаниях у нас команде было много экспертов уже посылку и панси то есть а фишка это было не самым весомым аргументом чистого умели лучше наверно так пожалуйста спасибо за доклад вопрос никита никита я здесь никита правее правее вот вот так так да еще проверить второй стены про elastic на докладе там не насладился было написано шум уже ближе к что ты пользуешься куратором в последних версиях ластиком появилась такая штука называется индекса lifecycle management вот и там как раз таки вот с этими стадиями там ход cold так далее пробовали использовать седьмую пока еще только начинаем игра кластеров на бою как хранение логов и как ротация пока еще не использовали вот как раз только начинаем ну тестируем это еще на не на продакшене и я поэтому не рассказывал про это и спасибо наверное штука перспективная никита здравствуйте добрый день еще раз спасибо вам за докладную такой вопрос от коллега сама зона буквально вчера рассказывал тоже про то как у них устроена как они решают проблему высокой нагрузке и в том и в том числе я вот у вас увидел очень похоже решение как вот собственно ту калека с амазона также там рассказывались примеры что вот у них произошла чудовищная так проблему когда ты отрубился прям целый дата-центр надо вот были ли у вас подобного рода живет на к случае которые в конечном итоге повлияли на какие-то вот из-за решения которые вы описали сегодня что вот что то такое произошло и типа ребятам и после этого делаем вот так но у неё свои ошибки и делаем многое из этого было порождением таких вот ситуации когда например там но не знаю коммутатор стойки взорвался там условно или что-то с ним пошло не так и там половина серверов отвалилась то есть это нас в начале там о блин у нас сервера неравномерно распределены на даже равномерно по стойкам там везде бон тенге собрать весь детом стикеру и мы коммутаторы поставить и все такое то есть вот это было последствием то есть или там банально просто кластер elastico почему-то лег ну не знаю аномальная нагрузка пачка над выпало и тоже начали разделять на бокс type начали как-то изолировать и нагрузку потому что ну вот изоляции наверное это самый такой железобетонный способ как можно сделать вашу инфраструктуру надежнее просто на на ячейки разбивать изолировать пожалуйста здравствуйте и сервисом и там сервис 1 милости кластер да правильно понял что один сервис один кластер один кластер elastico один в которой задержит себе 1 ну на самом деле может быть и два но как правило 1 столько индексов сколько нужно для одного микро сервис а вот и вопрос здесь такой что скорее всего логе нужны в разрезе запрос а запрос проходит сквозняком там через несколько сервисов стоп я возможно некорректно выразился ну или как это нила кластер это именно боевой поиск он использует для микро сервиса и микро серво сходит в этот elastic и наверное не все индексы можно завернуть вот так как вы говорите да действительно то есть есть разные индексы которые снаряд между собой несколько микро сервисов но тут наверно принцип изолирует настолько насколько можешь то есть муки то есть и понятный вас есть рассказывать условный горит индикатор запроса и который вас ведется в один индекс что мы можем посмотреть поведение системы у нас в целом у каждого если говорить про такой термин 3 у нас в целом у каждого запроса есть свой идентификатор то есть это будет который генерируется в начале если он есть он пропитывается дальше и поэтому в том же елка можно все посмотреть как бы всю историю этого запроса ну все эти panther да ни один сервис один кастер все-таки могут быть несколько сервисов придти на один катер астика ну все же этого пока у нас такого нет пока что мы все сервиса стараемся все таки а персональный выделять plaster и пока это получается наверняка будут такие где это не будет получаться но пока что получается себя большое спасибо за доклад хотел спросить вы сказали что у вас много про митаев каким образом эти прометей ну грубо говоря разделяется данный какой-то там на проект на сервис и еще что то ну грубо говоря один экспортер один прометея если not export все там системные метрики в 1 pramy ты тут все там взгляд метрики например идут в другой прометей некоторые метрики сразу два прометея скрипят из экспортеров если нам нужно какое-то failover то есть спасибо и еще один маленький вопрос что вы используете для рассылки alert of еще раз а у нас есть один из популярных сервисов которые стал недавно частью дыры знаю реклама не реклама ночь с гений и все идет туда и там уже агрегируется мы там настраивали все эскалации на скриншоте вы тоже могли видеть картинку скриншот эскалации оттуда и в принципе это тоже как один раз присесть настройкам так много всевозможных правил все будет хорошо добрый день добрый день как часто вы делаете и что происходит когда ваш автоматическая проверка обнаружилось что backup некорректно создался но мы смотрим почему так то есть разбираемся в каждой причине как часто разные системы по разному бы кататься с разной частотой и опять же зависит от критичности данных от того на сколько они там быстро меняются то есть бывает такое что приходится ну например по сгрыз мы бы к пим как там полный дамп там раз в сутки потом володе каждые несколько часов выкопаем и ну то есть так чтобы минимизировать вот эту от потери данных как который мы который произойдет если пройдет файл что мы делаем когда этот backup когда validation failed ну это обычно бывает из-за того что либо где-то место закончилось либо просто база так разрослась что где-то что-то по времени перестала совпадать с добавляем ресурсы или как-то не знаю размазываем как правило добавляем ресурсов по крайней мере из последних то есть тут надо каждого индивидуальный случай да пожалуйста можно и кита спасибо за доклад у меня вопрос тоже про бэкап нот какого рода как или какими средствами вы проверяете консистентной бэкапов на разных уровнях то есть это история про то что допустим вас вышел тот из строя или вышел из строя сразу несколько систем у вас бэкапы на разные точки сделан да и вам нужно как-то выровнять там вот как вы добиваетесь ну или вы готовы что-то просто потерять есть несомненно вещи которые мы готовы потерять которые восполнимый который охраняется нескольких источниках да то есть их мы бы к пим скажем так реже и не всегда не все проверяется то что просто не надо есть важные для нас вещи и их мы бы к пим их мы храним естественный не в той же локации то есть у нас dc распределены и там например если что-то одно там бы куста чтобы капица в мск то что то другое то что куда мы бы хоть немного копыт например в спб вот и там уже проверка на в принципе там для нас там локалка между этими двумя локациями ну да там чуть подольше происходит нам тоже типа 10 г и все туда уносится и там проверяется точно так же то есть для нас так как то прозрачного 4 ну то есть никакого лога нет такого чтобы никита еще вопрос александр компании сбербанк спаси она так вот кстати вопрос по поводу house monkey вы получается housemarque не отменит анализ парни не я сказал я как говорил да хватит что это не классическая просто название от этого мы решили посмотреть на тот ну что они делают там они там в облаках это удобнее делать то можно допускать поднимать как бы да то есть на железных сверху тоже хочется чем делать ну как можно и 5 был сам просто блочить компоненты друг от друга ну перезагружать дорого то есть какой-то сервис может долго стартовать не факсу стартанет хочется проверить до этого там до того как вы вручную несет все делать просто завален как спанки добруше регистратором подружку зам пирса морем и запускаем тест и после того как прогонит запустил до 100 увидел процентам 99 условно за потом сломал сеть потом опять прогнал тест увидел о 83 процента что-то пошло не так от отнес теста кишки и они посмотрели пошли по ним вы еще сказали то что когда вы гасан там какие-то допустим ресурс создан данного ткнулась monkey у вас прогоняется смог тесты но в то же время вы иногда гаси там часть пакетов а это не всегда как бы то может какую-то поду ну кладет какую то он бы вполне вероятно то что у вас просто будет деградация клиентской пути там но время загрузки страницы там время обработки запроса ого смог смог тесты покрывают это или нет смог тест на самом деле покрывают достаточно на данный момент то что надо и вот вы играете про потери пакетов я правильно понял вопрос что это не всегда может быть на сто процентов как бы выявить проклянешь быть не ошибка да смог тест они получаются скорее выгоняем повторно плюс смог тестов есть ретро и и отправила ну то есть это уже как бы если есть подозрение что он какой-то не актуально или недостаточно точен то такому же просто собираемся вместе с передумаем где же эти места как правило это решается вот такими там три бам совещаниями барин штормами не знаю но про потери пакетов это ведь выпить и построим функционал там пишешь 83 бросилась например да вот и как правило это этого достаточно чтобы стимулировать именно вот торможение компонента можно конечно немножко в это закупаться еще использовать этот трафик шейперы как то есть именно который скорость замедляет замедлить скорость пару килобайт это вообще будет офигенно на самом деле потому что почти все системы нести граде рует она продолжит тупить когда туда обращается по кендо и это будет прямо вот сразу все всплывет что должно то есть вас тоже на покрывать ну это мы пока не автоматизировали то если есть какие-то подозрения вот мы там можем в каком-то одном месте руками грубая это сделать это не регулярно делается она всё я понял красиво да друзья еще вопросы так значит кому мы подарим за лучший вопрос почтительный призы как-то чашку книжку и решает можно озвучить вопрос человек махнет рукой это облегчит задачу мы сейчас принимает решения и переключимся в петербург орнамента то спросим как дела на самом деле меня от крайне вопрос понравился потери пакетов и про и эта достоверность этого метода потому что он действительно может быть недостоверным надо к нему отнестись супер никита у нас для тебя тоже есть памятные призы спасибо лишь за доклад сейчас ребята тебе вручат жму руку"
}