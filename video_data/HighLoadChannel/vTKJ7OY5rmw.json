{
  "video_id": "vTKJ7OY5rmw",
  "channel": "HighLoadChannel",
  "title": "Система сбора подробной статистики работы узлов CDN / Евгений Россинский (ivi)",
  "views": 924,
  "duration": 2634,
  "published": "2019-01-14T00:09:56-08:00",
  "text": "меня зовут женя я технический директор компании и и прежде чем перейти к всяким веселым штукам про обработку данных так не сказали вот за эту полосу не выходить я будут ходить вот отсюда до сюда про обработку данных про processing и проще всякие вещи поделюсь с вами маленьким опытом пока люди подходят как я провел вчерашний вечер и вчера я осознал что новосибирск на самом деле является столицей интерпретируемых языков вот по той причине что неспешно прогуливаясь по городу я увидел заведение которое должен посетить каждый уважающий себя разработчик это стрип-клуб с названием python работает он до 6 утра поэтому сегодня я планирую провести там пару собеседований талантливых разработчик возможно немножко утолю кадровый голод который как мы все знаем с вами сейчас очень велика во всех компаниях поэтому кто собрался в театр до 6 утра там можно получить обратную сторону и так друзья значит о чем я хочу вам сегодня рассказать поскольку иветта сервис легального контента наша главная задача это ну на самом деле качественно показать видео передо мной выступал саша из одноклассников он показывал свои метрики как они меряют качество видео сигнала как они имеют вообще качество сигнала у нас задача честно скажу несколько другая но от этого не менее геморройное и веселая поэтому как только мы появились первое что нас нахлобучило это то что звонят люди и города черт возьми не сегодня работает вот у всех работа это вас не работает и вот мы таки ну блин вот у нас точно такая же нога и не болит что за фигня вот и начали строить там лет 10 назад машинерии которые вокруг всего этого дела будет нам сообщать о том а что что у нас как бы что-то хорошо или что-то плохо и по хорошему есть две самые важные метрики за которыми мы следим их на самом деле больше 40 вот относительно качества видеопотока но вот такие царь метрики с флагом которым мы бежим это метрики это быстрый старт датой что вот человек нажал кнопку play дальше нужно максимально быстро показать картинку а потом нужно чтобы не было остановок достала в буферизацией чтобы человек потреблял контент и приносил нам как следствие какие-то деньги вот так эта штука сейчас ее включу вот значит но для того чтобы ответить на этот вопрос как улучшить обслуживание вот мы не дошли до ректальных крипто термометров которым меряем процессор на телефонов но я взял назад на заметку прям отличная штука теперь я тоже буду за куплю несколько датчиков и тоже буду измерять насколько мы загибаем камень того или иного устройства вот но поговорив сначала немножко тех самых метриках да я расскажу сначала как мы какие метрики мы собирали довольно длительное время а потом почему мы пришли к выводу что этого нам мало и так что забудь такая простыня текста я ее немножко расшифрую значит во первых здесь очень высоко уровень метрики которые могут позволить понять хорошо ли или плохо да метрики у нас делятся на 2 типа первое то что мы отправляем с стороны клиентов да у нас есть своя система событий пользователей до с пользовательской статистике там продуктовые аналитики вот который позволяет нам понимать что у человека что-то хорошо или нет и трекать его во время его жизни на нашем ресурсе собственно мыло героем факт получения ссылки на поток время с момента старта до того как человек увидел первый кадр и дальше нужно разделить два события первые события он начал смотреть видос сначала или он перематывает тут есть на самом деле определенная техническая разница в этих процессах потоки отдаются у нас чуть-чуть по-разному в этом случае вот нужно отловить факт остановки причем не той остановке когда он нажал на паузу это нормальная остановка в твой становки когда он ничего не делал а все упало вот надо узнать сколько времени он ждал этот самая каверзная метрика почему потому что он может сказать о черт вас ничего не работает и закрыть там вкладку браузер и выключить телевизор выкинуть приложение из памяти и мы никогда не узнаем о насколько плохо ему было вот соответственно естественно лаги руются ошибки этих ошибок там больше там 30 различных типов и каждый новый релиз каждые там операционной системы на которой мы работаем имеется ввиду клиентские приложения добавляют нам в кортеж ошибок еще энное количество веселых данных дальше все эти данные мы обогащаем контекстом да то есть тех теми условиями при которых смотрел пользователь самое важное то там битрейт контейнер да там под контейнером я понимаю там дашей чудес там mp4 вот ну и вообще все что до чего ты можешь дотянуться на клиенте в рамках того контекста player в котором ты работаешь потому что не всегда ты пишешь там свой собственный плеер чаще часто ты используешь какой-то стандартную штуку куда ты просто функцию play передаешь ссылку на видео поток и дальше она и какие-то настройки дальше она как то магическим образом работает вот значит это с одной стороны до были данные собираемые с клиентской стороны есть еще узлы седину на свой собственный сидим 30 городов присутствия вот довольно большой полосой в пике вот значит каждый узел сидена по сути бы отправляют в 100 т.д. у нас n-нное количество метрик да за которым следят как разработчики которые посматривают за своими релизами так и там дпс инженеры которые бьют разработчиков за то что выкатили опять какую-то фигню вот значит количество успешных запросов которые пришло на узел количество запросов которые были обслужены с ошибкой вот количество запросов которые этот узел отправил по той или иной причине на и вас на своего соседа на соседний кластер или на там самый главный и важный кластеры в москву нас в москве 3 дата центра который на самом деле такая буферная зона который принимает все то с чем-либо не справился либо по каким-то причинам трафик приземлился в москву и такой бред последний рубеж последний рубеж это еще файлы origin of да где который тоже на самом деле довольно работают под них могут работать под неплохой нагрузки но наша задача сделать так чтобы файлы origin of не принимали на себя существенную часть трафика и в общем то на по ним такие должны летать перекати-поле laughter лишь должен биться где-то в районе нуля и канал там должен быть там там совсем крошечный вот потому что если это не так тут вой сирен не очень хорошо справляется вот значит таким образом вот мы довольно долго следили за интегральными метриками каждого узла это был довольно индикативное и позволяло нам выявлять проблемы который связан атом пример со стыком с каким-нибудь оператором ну там какой-нибудь оператор устроил сетевой шторм положил всех сделал петлю вот и пользователь на самом деле все равно что это как бы не твоя проблема он пришел пользоваться своим сервисом он не обслуживался все дурак ты вот поэтому тебе приходится и надо отдать должное там практически всем операторам что нет никакого там снобизма высокомерие прав проблемами разбираемся вместе потому что иначе гетерогенные сети расползаются как старая шаль так вот на первый взгляд метрик достаточно дам из клиентов все трека им все хорошо мы трека им отдельно каждый у седина вроде классно там жили там несколько лет с таким счастьем вот казалось бы да но нет почему потому что главная проблема с которым мы столкнулись мы не всегда на каждой платформе разрабатываем свой собственный player почему этом кажется проблемой а проблема в том что вы отдали ссылку своему плееру и этот плеер не ваш и вы не знаете какой именно узел то есть плеер до знает он получил еще тебе ответы лечите ps ответ и только контекст player знает что внутри этого запроса происходит а вам только наверх прилетает какая-то ошибка что-то пошло не так если говорить про там платформы связанные танцуешь со smarttv там получить адекватную статистику крайне сложно вот в практически все ошибки там могут характеризоваться как что-то пошло не так а если сюда добавить еще и проблемы с системами защиты контента типа д р м и прочих всяких вещей которые в принципе тебе не рассказывают они маскируют ошибки неправильных ключей под ошибки хрен знает какие то да то есть и поэтому ты не можешь детектировать у тебя проблема там я не знаю ключи заек спарились как он грм системе нас 5 грн систем простых одновременно работает зависимость того на какой платформе мы отдаем поток вот и ты понимаешь что по-хорошему дат твой сервер возвращает в хедере какой-то хэшировать до идентификатор по которому ты можешь на клиенте понять аж с какой узел сидена его абсурдно от отдал ему этот ответ но находясь в рамках контекста с видео плеера который пишешь нет и ты это информации лишаешься и получается что у тебя есть не до конца обследованные проблема то есть да ты на больших чисел как на графиках становись например этот узел вышел из строя окей он сам вышел из эксплуатации те ошибки которые мы словили пока он был в эксплуатации они тут же ушли вновь всё хорошо но если ошибка плавающая если ошибка появляется не всегда если ошибка там на стороне оператора ты не можешь точно доказать не всегда может точно доказать что ты не дурак и у нас начались довольно серьезные проблемы связанные с тем у нас появились нелюбимые провайдеры очень интересный пример есть провайдеры которые обещают пользователям что мы дадим вам там 50 мегабит заплатите нам n-ную сумму денег вы получите 50 мегабит и в этот момент провайдеры немножко некоторые провайдеры немножко умолчу на самом деле вот та оборудование которое стоит в домовом хозяйстве она в принципе расчитана на 100 мегабит максимум и если ты сейчас потребляешь контент один ты получишь пятьдесят мега у меня не биговать мегабит прошу прощения ты получишь 50 мегабит но если твой сосед сейчас смотрят что-то или потребляет какой-то контент тяжеловесной и все люди начали смотреть то происходит деградация по общему каналу дома там коаксиальный кабель og который через себя пропускает ограниченное количество и все и все превращается в тыкву вот абонент приходит к тебе горит дорогой друг я заплатил тебе денег как вот почему твой сервис меня обслужил очень плохо и вот у нас появился список таких провайдеров на котором к сожалению начали немножко закрывать глаза было феерично история о том как естественно по генеральском у эффекту такой провайдер оказался у нашего генерального директора вот и на все наши наши попытки там скачку типу на год у нас все работает смотрите как хорошо графики там где нет ребята вот у меня не работает я приехал в гости как там привилегированный саппорт вот приехал в гости мы приехали в 5 часов вечера смотрим включаем здоровущий телевизор включаем все мило персик шоколад мармелад вот шеф мой расстроился говорит ну как же так ну блин вот зря приехал получается мы поужинали убитого ещё раз посмотрим включает и не работает вообще ничего и я начинаю мерить от как бы смотрю вайфая там смотрю пинге смотрю trace road и все это посмотрел выясняется что в момент когда я делала это исследование канал составлял пол мегабита у него там он хочет смотреть там full hd ну не бывает таких чудес вот и вот памятуя эту историю у нас в скриптах саппорта появились там истории что вот такие такие-то провайдеры если вот у них возникают проблемы на наших интегральных графиках все хорошо то ребята расслабляемся и говорим что вот обратитесь оттуда от это очень большая ошибка вот и к чему это привело это привело к тому что мы определенное количество раз об футболе или пользователя с тем что считали что все хорошо интегральные метрики в них может закопаться черт знает что да то есть неожиданная популярность твоего сервис и одновременно и падение его производительности дает тебе ровную линию на определенном узле и вот как бы это не было парадоксально но наименее вероятные события получается с наибольшей вероятности вот и мы стали хуже отлавливать плавающие баки а это все негативно может воздействовать на пользовательский опыт и увеличивать отток вот соответственно во время решения проблем в некоторых случаях с минут может увеличиться до дней потому что мы в принципе не признаем себе что проблема существует это вот самое печальное вот и мы действительно наступили на грабли когда мы полтора дня считали что проблемы с одним из провайдеров то есть проблема у нас не существует есть проблемы с одним из провайдеров вот этот провайдер оказался одним из самых крупных кто был прилеплен к той или иной части нашего кластера проблем было у нас мы за к метели паппет конфиг который для некоторые типы контента для некоторых контейнеру обслужил его плохо не не гарантировано плохо а иногда плохо и вот полтора полтора дня мы потратили на решение проблем из них день мы занимались информационным нигилизмом то есть мы садились и говорили как прекрасно у нас все хорошо это прям потом высекли себя посыпали голову пеплом вот и решили kia теперь мы сделаем там постановку задачи и которая должна нас от этого избавить есть у нас 30 главных городов присутствие 48 миллионов пользователей вот и по мнению нашего прекрасного партнеры мтс а в которой на конференции транс на 2017 года признал что мы являемся 4 по трафику вот собственно что нужно сделать нам нужно было научиться отслеживать каждый фрагмент видео для каждого пользователя с подробными лагами кто как обслужил с какими маршрутами и прочее прочее прочее плюс к этому мы захотели честно не на жонглированием больших чисел а честно померить воздействие обслуживания плохого обслуживают того или иного узла на пользовательский опыт то есть у нас есть собственная система статистики куда клиентские приложения бомбардируют события там пришел на такую страничку кликнул туда купил тото посмотрел та та та та вот и все это льется там довольно большим потоком я об этом по моему на каком-то из холодов рассказывал вот и вот нужно связать вот эти события которые происходят на серверном по с клиентскими событиями вот мы в общем первое что нам пришло в голову черт возьми у нас же есть система статистики давайте пульнем туда но это же просто тем более там ты написал статью на хабр из колодца у нас такая штука держит такую большую нагрузку и еще там x там 10 например там вперед она может еще выдержать вот решили попробовать ну и тут же откатили вот почему потому что это сейчас будет на следующем слайде чуть-чуть забегая вперед вот значит что мы решили отправлять решили отправлять факт успешного обслуживания факт начала обслуживания длительность обслуживание и факт перенаправлений плюс ошибки очень этого более чем достаточно то есть эту верхнюю история чтобы понимать что происходит вот и пришлете а как же нам связать клиентские событий сердце у нас же есть сессии просмотра view идентификатор зашит вор ли всё классно бежим и побежали проскочили код на часть узлов вот и приуныли есть века и высказывание пса шарика который говорит и маму в дом вот когда мы в нормально работающую систему который была построена ровно для своих целей добавили то что в общем не совсем была создана для этого вот значит первое что мы удивились мы благополучно забыли о том что один просмотр видео на самом деле это не один запрос и когда мы делали от оценку это все сейчас вам пересказываю там примерно там не недельные пертурбации круг черт возьми как так получился под ковер замел мусор и свет хорошо вот значит на самом деле pre-party почему данных оказалось много да потому что когда ты стримишь видос очень в редких случаях отдача одного потока это один из теперь запрос чаще это в случаях там тех устройств которые умеют им только mp4 вот и это в этом случае если например это старые какие нить от ios и которые там например сочи лесом по каким-то причинам не справились и у нас есть деградация 2 mp4 в этом случае все равно будут запросы рейнджерами то есть у тебя по сути на каждый просмотр видео идет большое количество из типичных запросов случае с чин коваными видосами и чин коваными форматами типы и чавеса или даже каждые две секунды 2 4 зависимости от длины чанг а вот у тебя идут запросы с условием что у тебя есть еще redirect и перенаправление запросов там различные типы событий получается что ты устроил такую не кислую ddos-атаку своей системы статистики сам если вы сказал себе в одном новое что-то не достаточно больно выстрелю в другую ну еще дереву уронил на одну из них вот после чего посмотрели отчет стал с табличками куда мы льём все эти события и построение отчетов по качеству просмотр видео превратилась в ад почему ну потому что надо же все это как-то джо и нить тому проще все вот эти конечно колоночный б д это прекрасно да там у нас есть ходу у нас есть и хаус это все хорошо вот но когда ты расскажешь об этом на конференции кто парус ласкал а вот данный мы льём туда на самом деле под этим кроется там чудовищный такой монстр конкуренции за ресурсов там мы приди устных задачек там для нормализации данных и прочих всяких вещей и выясняет что конечный человек который хочет выгрузить какую-то фичу на сервис видео стриминга и посмотреть а стало лучше или хуже он свой отчет там ждет там 15-20 минут для того чтобы ему получить что-то нестандартным должен взять в руки питон там или еще что-то и на коде каких-то агрегатов и вот это все становится неуправляемым в принципе вот увеличилось количество запросов вот еще важная фишка что мы построили архитектуру своего сидена таким образом что если пропадает связь а это часто бывает между центральным представительством то есть нашими московскими кластерами где где живет система сбора статистики и регионами то узлы сидел все равно продолжают обслужи все что они накачали они продолжают благополучно обслуживать пользователей данной на эту тему если интересно нас есть статейка как работает наш сидел там история про и не каст и прочие веселые штуки вот я хочу получается получается что мы имеем право совершенно спокойно ставить узлы своего сидена куда угодно даже там где канал до москвы там 10 мегабит 10 мегабит на весь там дата-центр который там не знаю подвал какого-нибудь студента очень сильно утрировать вот и вот когда ты отправляешь событий должен учесть что события должны de fer от отправляться за то есть отложено и тебе нужно это еще и реализовать и по хорошему нужно об этом думать вот и как результат вот это вот схема в лоб она оказалась на четыре минуты для нас болезненны после этого быстренько код откатили вот и решили что нам нужно искать компромис компромисс был достигнут довольно просто на самом деле для решения подобный бизнес задачи не обязательно прям супер-супер стремиться к получению данных в статистических отчетах в реальном времени то что интегральные характеристики никуда не делись но мы посмотрели все кейсы все разборы инцидентов и сколько времени ушло на то чтобы в принципе начать копаться в этом все это вопрос не секунд это вопрос все равно человеческого участия там где не не срабатывают стандартный триггер все равно нужно разбираться что как и почему потому что чаще всего жалоба для дядь я знаю что у тебя сломалась то что мальчик машина да вот чаще всего пишет пользователь у меня не играет видео все это может быть одно обращение там за полдня например из данного сегмента сети вот соответственно мы пришли к выводу что на самом деле если мы укажем потолок в пять-десять минут и просто не будем его превышать на самом деле скорее все будем укладываться в приближенные к реальному временно не будем его сильно превышать никто в общем-то не умрет вот так же раз у нас мы дали себе возможность сделать небольшую дельту значит можно ли нормализовать данные значит можно по ходу в потоке данных которые льются тебе нескончаемым потоком с твоих серверов можно одновременно смочить это уже по сессии просмотра видео смочить с пользователем с его окружением со всей этой информации который знает об этом пользователь наша уже построенная система статистики аналитики вот ну и а если мы вот пошли на такие уступки почему бы не работать пачками да то есть вот по моему в вконтакте рассказывали что они льют в кли house по 1 мегабайт по одному мегабайту да мы тоже лишь кликала спаду мегабайт потому что эмпирически оказалось нормально вот какой то мат модели я сейчас не могу вам привести но вот самая любимая мой предмет в институте был численные методы и самый любимый алгоритм метод половинного деления дихотомия вот собственно с помощью этого метода мы героически нашли размер пакета который можно удачно запихнуть в кликал поэтому вот родилась такая вот сверх технологичная схема вот над которой работали наши лучшие умы вот работает следующим образом клиенты прилипают к тому или иному узлу сидена дальше узел у себя агрегирует по сути это не совсем логе это наверно так лог событий да то есть это это это событие с контекстом вот и дальше либо клиент может сам запулить это все ravak либо какой-нить краулер может собрать зависимости от ситуации потребностей дальше вот собственно вот об этой самой магии я и хочу немножко рассказать как мы вообще дошли до жизни такой мы любим вот веселую белку под названием fling если кто то знает немецкий наверно переведет то ли быстрый то ли шустрый я не помню как это точно переводится это очень хороший инструмент для потоковой обработки данных у нас используется в нашей системы статистики вот и первое что нам пришло в голову было тоже довольно интересным то есть вот там где открывается портал по хорошему наверное классно по 2 поставить веб-сервис который бы принимал на вход вот эти вот пачки ну это как бы классическое так мы начнем умеем писать всякие мы мы описать веб-сервисе на пойти не умеем сети веб-сервисе на год давайте напишем на чем-нибудь это же просто очень простая задачка взял пачку логов вот и запихнул этого fling ну классно распланировали хорошо завтра напишем наутро приходит один из наших разработчиков горит слушать и мне приснилось то такая интересная штука вот по сути fling это инструмент который позволяет вам это инструмент в том числе и для дата-сайентистов и для людей которые занимаются сбором информации это инструмент который позволяет делать как параллельные вычисление так обрабатывать данные в потоке наш сотрудник сказал что тип друзья смотрите получение данных тоже является этапом обработки данных давайте мы возьмем и сделаем следующее давайте первым этапом в обработке данных в рамках контекстов ленка пусть у нас будет уже написанные людьми нормальный web-сервис джетте у таки и вроде как писать ничего не надо там чуть-чуть конфиге под настроил там и вперед музыки блин ну как ты во мне кажется что мы там как это сейчас сделаем какое-то чудовище 1 хрень написано на яве другая хрень написано яви у нас там всего как бы компетенция в компании больше склонна к python у и г вот папа яви всего там четыре человека вот мне кажется сейчас будет какой-то шантаж давайте попробуем ну как быть попробовать то в общем недолго и мы попробовали получилась вот такая вот штука да то есть да естественно поставили в engine кс перед всем этим делом исключительно для того чтобы поставить всего две машины вот на такой большой объем данных всего две машинки в 2 dc для там отказоустойчивости там преданности и проще всякие штуки и на стандартный наших прокси который просто принимают энное количество запросов вот сделали раунд роббеном раскидали на там две машинки по пятьдесят шесть портов получилось довольно просто быстро и весело а дальше все по классической схеме которые используются в африке да то есть вот джетте у нас получил какой-то кусок данных 1 мегабайт когда положил в бусинку вот откуда флешером эта штука вы цепляется вот из buffing и удаляется и уже преобразуется в в понятную флот мапу для того чтобы с этим потом можно было работать и обогащать это эти данные контекстом из других систем там да это может быть там какая-то с рынка это могут быть данные из рекомендательных систем или еще что то что то что нам позволит быстро ты нормализованной данные заливать в колон ночную базы данных мы используем на самом деле две колоночный базы данных мы используем вертик у используемые с тех пор пока еще не было клик хауса vertica была самой классной колоночный базы данных мы плачем кровавыми слезами платьем конские деньги за лицензии вот там они продают под терабайта мы используем вертик у только для данных которые мы предоставляем там право правообладателем рекламодателям где мы отчитываемся то что нельзя потерять там невозможно забыть и вот это вот все вот а для клиентской аналитики для продуктовой аналитики мы из поле для всего остального мы используем там либо hadoop либо crack house зависимости от тех задач которые мы перед собой ставим вот и вот так выглядит общий верфи уровневой схема того как мы вычтем данной то есть вот есть ровно такой же там нарисован групп потому что наша система сбора клиентских событий называется грут у нас все микро сервиса называются какими тьма русским героям уже так семь лет назад заведено или 8 до сих пор но каждый новый человек приходящий в компанию сначала пытается разобраться чем гамору отличается от гамбита вот что такое гидратом и просит руси профи вот бизнес на за это ненавидит особенно ненавидит финансисты потому что когда я не знаю как в других компаниях мы там отчитываемся там перед налоговой как бы что мы разработали показывай что люди делают что-то и вот есть там отчет в котором там написано там сделать там новый порт для гоморры и сделать интеграцию с гамбита муж блин вот это так написано в кити да вот в багтрекер разработчикам естественно чтобы перевести это на язык аудиторов финансистов и людей которые смотрят о чем мы собственно делаем вот тут мы делаем вот примерно вот так и делаем вид что вокруг никого нет вот собственно грут эта система которая построена примерно по такому же принципу да то есть есть fling который процессе по дороге много чего делает это для того чтобы почему нам по дороге нужен flink flink нужен потому что мы заранее предрассудкам различной воронки да то есть вот по сути нам нужно есть 20-30-40 стандартных отчетов который нужно уметь строить быстрым для какая конверсия с главной страницы в форму оплаты какая конверсия там скис промо-блок а вот в такое то пользовательское действие и поэтому с помощью клинка можно заранее при дочитать все эти штуки промаркировать от мариновать события чтобы они дождались относится ни к какому из какой из воронок какому из отчетов и потом бодренько все это до нормализована залить в табличку и отчеты строится вот так на самом деле также делают там ребята которые пишут и яндекс метрику если я правильно понимаю клик house был создан как раз для того чтобы быстро строить такие отчеты вот и а теперь нам нужно вот эти два клинка поженить поскольку мы очень любим редис и редис кластера собственно мы пришли к выводу что вероятность того что весь контекст который есть сейчас как как который мы знаем о пользователе он находится сейчас где-то в оперативной памяти соседней системы почему потому что вероятность того что человек начал смотреть видео а до этого он как-то по навигировать по нашей системе и как минимум нажал на кнопку play вот она близка к ста процентов а это значит что мы уже о нем очень много чего знаем и значит что вот последующие join и можно просто из них от них отказаться заранее свечи все эти данные и разложив в дополнительной колонки да это нисколько не экономичны но это те данные которые мы можем набирать через год стереть оставив только агрегационную представления о том а какой был процент у нас видео с буферизация me какое среднее время старта видео и прочее прочее прочее поэтому под такое расточительство можно себе позволить вот чем мы делаем мы в одном френки закидываем данные в freddy's а из другого fling а просто подключаемся к этому радису из обогащаем нужный нам контекст собственно и получаем у нас и всего лишь 6 просмотра дальше мы ее из него раскручиваемся что делал пусть что это за пользователь какие там у него покупки там еще еще что-то что-то что-то что-то для того чтобы быстренько построить нужный отчет и получить там уже практически в реальном времени отчет по качеству смотрения которые прекрасно можно соотнести с тем как человек вел себя на нашей платформе вот и что в итоге мы получили мы получили очень классные масштабируемое решение вот где от именно написания программного кода не так много ну то есть вот эта вся штука оперирует там тысячами событий в секунду при этом количество строк кода там ну очень небольшое вот мы нам удалось добиться того что задержки в от того как событие произошло на одном из наших нод до того как она попала в клика us виде отдельной записи составляет там не больше пяти минут вот и самое главное мы научились детектировать проблемы конкретного узла а это очень важно особенно когда ты делаешь эксперименты на узлах сидел к сожалению вот многие вещи протестировать для того чтобы построить тест в инфраструктуру как какой-нибудь гипотезы нас по улучшению алгоритмы кэширования там или еще чего-то у тебя синтетические тесты проходят все хорошо но как бы честный ответ ты получаешь так или иначе когда ты начинаешь какой-то процент пользователей отправлять на новые железки вот недавно мы решили что будем проводить эксперимент в уфе я приношу извинения перед всеми людьми кто из уфы вот там несколько несколько дней мы катали там новый код вот правда никто не умер все хорошо но там у них словно говоря там процент буферизации выросла на полтора процента это не так много но этот достаточно для того чтобы уйти на месяц и обратно переделывать все алгоритмы вот и поэтому точный анализ данных связанных с тем какой узел как обслужил когда ты можешь связать это с пользователь со всей его историю дает но и мощнейший инструмент и для предикатов последующих и для текущего мониторинга и реакции на инциденты я думаю что у меня все с удовольствием отвечу на ваши вопросы у кого вопросам поднимаем руку как обычно спасибо за доклад о такой вопрос обычно когда видео не показывает говорят нам откроется там speedtest.net еще что-нибудь и никогда спит с вот именно они думали вы или вы строить нечто подобное уж построили нет строите чтобы клиент мог сразу выдать пух-пух плеер explay ряд не очень хорошей ситуации потому что когда вы находитесь в плеере изменять контекст то есть в плеере самое полезное что мы делаем это там просим нажатием случае браузер на правую кнопочку и логе уходят к нам клиентские вот то что мы можем собрать на самом деле почему не надо верить speedtest у потому что любой уважающий себя провайдер поставил вот на последнем перед последним метром поставил себе сервер спидтеста и вы всегда будете мерить расстояние по дефолту да по сути своего провайдера вот поэтому у нас есть как и на самом деле ну практически у всех сервисов которые так или иначе занимаются массовым обслуживанием есть свой собственный свой собственный мерил к которая замеряет до твоего собственного сидена вот показывает тебе связанности вот мы на самом деле работает ровно по принципу speedtest просто рассказывает тебе там условно говоря посмотрит какому к узлу ты прилепился до смотрит вот твой о расстоянии до москвы до там смотрят твою расстояние до твоего локального кластера для твоего parent кластера от твоего локального кластера вот эти штуки собираются и ты уже знаешь вот с этим уже можно работать со спид с там можно не работать вообще то есть это уже штука которая себя тащите ты не выбираешь какой сервер ты хочешь протестировать можно этим не заниматься в плеер мы пробовали вставлять фишка в том что когда вы когда человек уже плохо как правило он закрывает контекст player ну то есть вот она а вы ничего не работает вот и если если он оставляет от если он оставляет плеер открытым нам более важны логе чем потому что по сути то что ты повторишь даже запустишь еще раз один тест это не значит того что ты получишь ровно тут проблему которая была сейчас да вот значит и пойдешь ровно тем же самым маршрут у нас была ситуация когда каждые там 2 или 3 запрос одного из операторов уходил через сингапур вот нормально сингапур нормально нормально сингапур вот и вот эту штуку ты там тесты никогда не отследишь вот у нас есть целая в каждом клиентском приложении зашита целые системы деградации то есть если мы понимаем что что-то идет не так то мы начинаем перебирать все вот как вот саша перебирает историю с тем какой у него над также мы перебираем из марокко на ближайшем club кластере фигово что мы можем еще сделать мы можем попробовать они проблем или это контейнера дат попробуем посудить контейнер отдавался в душе пробуем эй чел с можем уйти на патент кластер да то есть у нас есть мы по сути такое дерево иерархии тому каждого кластера есть какой-то его родитель на самый главный папа кластер в москве вот три папа кластера 3 папа да ну ладно вот значит вот и вот все эти штуки мы перебираем если там мы отдаёмся за пределами российской федерации мы смотрим там как там работаем как рам как отдается из россии вот эта вся цепочка если ничего не получилось там примерно три выстрела мы делаем потому что каждый выстрел ты должен все-таки выставить какой-то тайм-аут по которым должен дождаться какого-то результата и вот это более информативно и вот в этот момент и просто к себе пуляешь все результаты в свою к систему клиентской статистике вот а speedtest ну индикативная штука вот но не очень еще около ночь вопроса штат ни у кого ладно но стесняется об эфирные таблицы в crack house использую где то есть кроме буфера на клинки есть еще буферы ликом софт crack house экспериментируем по разному но то есть сейчас там разработка в клика у нас напоминает там полно разобранный самолет в котором мы летим мы продолжаем менять детали то есть проект развивается довольно быстро и для разных то есть вот мы сделали какое-то решение всего мы зафиксировали вот эту конфигурацию мы с ней работаем дальше все новые штуки то есть выходит что-то новое мы пытаемся это внедрить до пробуем с суровым продакшене нет но я думаю что там через месяц-два уже будет сейчас секунду спасибо за доклад скажите какой объем данных вас лежит тихо такой к серверов серверов штук восемь сейчас соответственно объем данных честно говоря поставили меня в тупик пытаюсь посчитать вот десятках примерно не рыбой в десятках тарабарит десятках терабайт а конфигурация оборудования сколько памяти пункт врум врум вот на этот вопрос и честно грязь с я боюсь наврать потому что там они вот случае с клик хаусом они все разные вам приходилось unity хаос и дописывать свои какие-то подселили нежатся кавычках у нас устраивает пасибо у нас есть самая большая проблема связана с клика усам который по хорошему и у нельзя отнести к клик хауса как к базе данных колоночный да и как к инструменту и можно отвести к эксплуатации и как части биой вот мы страдаем что мы не можем нормально подружить crack house табло то есть вот клик house прекрасен многим и основная проблема научите людей которые умеют писать реляционные запросы писать их немножко по-другому гоян я понимаю вот а вот собственно почему vertica хороша потому что ты берешь человека который там знает всего лишь реляционную алгебру и представление о том как можно вытянуть там-то он умеет select и join и вот там и есть никаких апдейтов просто на чтение вот и он получает в красивой формочки может быстренько по перетаскивая там что-то получить какие-то агрегатные таблицы и по сути избавить разработчиков и деби инженеров от того что нужно как-то помогать то есть это просто уже становится сервисом вот клика у сейчас сервисом для бизнеса не является я думаю что год полтора два этому ди би си джи би би си драйвера выйдут на уровень приемлемый вот сейчас пока это не так но это не мешает при хаусу выполнять свои функции колоночный базы данных высокоскоростной который решает свои задачи и еще один вопрос вот у вас все так прямо френки они раскиданы по всем дата-центром да это все в москве все в москве и какая суммарная пропускная способность секунд пиковая в чем в мегабитах через фильм проходит в мегабитах гигабит и я я думаю что наверное в гигабитных но она ничтожна по сравнению с видео ну то есть вот честно говоря об этом даже не знаю потому что нам приходится из объем видео гораздо больше превышают ну это это можно даже к гадалке не ходить я об этом даже не думаю это в ну как бы на фоне видосов нас фанов одессе гена там я даже отдельный не вычленяя это даже отдельные строчки в бюджет или нет но сегодня я поняла ещё раз спасибо закрывал спасибо за доклад мне очень понравилось очень живенько прям очень интересно интересная подача материала и такой необычный вопрос можно сфотографироваться футболка просто нравится жалко что вы не девушка ну то но я постараюсь как-то более-менее прилично выглядеть а может быть чубакка вопрос ни у кого вопросов нет больше в зале задайте это пока фотографировать напоминаю что визитки можете забирать на экран и настойки раздачи также надо голосовать за параши доклады и хотим поблагодарить евгения за его доклад прекрасный"
}