{
  "video_id": "8uLu6GWaLSY",
  "channel": "HighLoadChannel",
  "title": "Найди мне работу: как устроен поиск в hh.ru / Алексей Бичук (hh.ru)",
  "views": 7188,
  "duration": 3004,
  "published": "2019-05-15T04:46:19-07:00",
  "text": "всем привет меня зовут лёша я работ занимаюсь поиском в основном прага да на питоне и сегодня поговорим с вами о поиске чего я не буду делать на этом докладе это искать вам работу я немножко вас обманул буду рассказывать про то как устроен нас поиск но без кишок без жести как бы верхние уровни давайте познакомимся сначала кто занимается поиском классно а кто у кого поиск на эластики классно у кого поиск не на эластики у кого поиск на сфинкса анна люсия го хорошо она salary окей я понял а у кого самописный поиск хорошо clio давайте вообще немножко head hunters расскажу в приближении поиска то что нужно знать нашу базу резюме сейчас 36 миллионов это то почему нужно искать и активных вакансий 600 тысяч сто тысяч активных вакансий это то что создано за последнее время это почему ищет большинство пользователей когда приходит на сайт все это к нам оборачивается четырьмя тысячами запросы в секунду к поиску конкретно вот так технологии у нас в основном java есть немножко питона машинного обучения мастер данные хранятся в подписи чуть-чуть кое-что хранится в кассандре аналитика считается на ходу пи вот ну и разные штуки для машинного обучения есть если говорить о команде поиск мы уже разрослись до 18 человек то есть у нас есть жало разработчики которые делят programmer скую часть есть dots on this to которые обучают модельки чтоб поезд был лучшим вот есть аналитики есть продакт-менеджер который нами рулит есть даже свой фронт enter у нас кластер из 50 машин машины разные машины размазанным по двум дата-центром чтобы было отказоустойчивость вот это все вот ну у нас есть машины учение все покрыто метриками мы используем бы тестирование чтобы изменять поиск или контролировать изменения вот почему будем разговаривать вообще как сделать специализированный поиск что тут можно использовать с какими проблемами столкнулись в себя в хит хантере при построении так не называем artplay поиска как сделать хорошую поисковую выдачу что вообще хорошие поисковую выдачу потому что хорошие поисковой выдаче для всех это разные расскажу нашим кейси вот как померить качество поиска как понимать то что он улучшается ну и как можно применить вот всякие модные словечки машинного обучения в этом месте как сделать поиск для начала нужно вообще придется что нужно нам от поиском нужно нам чтобы пользователь находил информацию максимально быстро и максимально релевантны для него мы еще на самом деле нужно поддерживать различные запросы такие как энд кэрри буренка вы буллинг вере различные фразовые запросы запросы по расстоянию слов различные запросы по диапазону это полезно когда например еще в какой-то вилки зарплата или в каком-то диапазоне опыта вот нужно отфильтровать по различным параметрам то есть если человек приходит с запросом я хочу вакансии москве от то не нужно показывать муки другие города нужно это папа ранжировать потому что вакансии я говорю у нас больше 600 тысяч но даже после фильтрации может остаться например 10 тысяч и в итоге пользователь никогда не будет скрывались самый низ и смотреть все в кассе мне нужно показать все самое интересное для него в топ-10 в топ-30 максимум ну а желательно вообще на первых трех позициях вот нужно скрыть с поиска не нужная и полезная фича когда вы говорите что вот мое резюме had hunter но я не хочу чтобы в поиске мой работодатель еще вот пачка вот этих работодателей его видел когда-то тоже нужно сделать ну и здесь нужно сделать максимально быстро потому что мы живем в 2018 году в вот это все ответы меньше чем за 1 секунду какие бывают запросы то есть есть вот куча классических запросов когда есть какой-то запрос какие-то фильтрующие параметры и с этим с этой истории вроде все понятно как как тут это решать но 50 процентов запросов на сайтах от хантера не вот про вот это когда я пользуюсь приходят нажмет кнопку найти мне работу все найди мне что-нибудь и в этот момент случается такое что надо что-то показать и нужно показать что то это в топе запросу когда я приходил в как hunter мне как раз сказали вот три типа чувак у нас есть такая задача и тогда я призадумался что можно в этом месте сделать вот это как вы главнейший враг поиском но об этом я буду как раз рассказывать что еще есть за поиск на поиск на самом деле тот кто работает с поиском они знают что это не просто одна строчка сверху а это еще куча побочных систем которые есть рядом ну и плюс и в поиск ходит ходят люди не только сайта туда ходят еще различные рассылки которые приходят вам на email с рекомендациями различные push-уведомлений и подобные системы что есть готова лапки уже все поднимали кто чем использует все знакомы с так давайте кто незнаком общается ластиком есть такие ладно хорошо это такая штука которая полнотекстовый поиск умеет делать счетом по кластеру размазаться и 1 тон в наружи торчать вот и ластик и сара это как про одно и тоже плюс минус люси на это такая библиотечка которая написана на джая и она как раз в кишках ластика и solara то есть что с винкс про сфинкс я думаю на холоде должны все уже знать давайте кто не знает про сфинкс короче поищите доклад нам есть вам расскажет от андрея вот у нас мужчина и вот в этом моменте обычно случается вопроса почему почему не elastic причем на самом деле вот такие это водку классическая причина исторические причины когда я говорю исторические причины это не какой-то странный выбор был люси но он был вполне осознанно и потому что пластика просто еще не было в зачатке компания 18 лет когда понадобился поиск пластика него доллар был не очень стабильный но так как большинство из нас же вист это это был не сфинкс плюсе на люси на позволяет делать различные кастомизации залезать прямо в тишкин залезайте скоро рак так называемая та штука которая ранжирует можно подсовывать туда сразу работу с эмилем с моделями очень удобный плюс можно или всякий своей оптимизации кастомизации там ковыряться в кишках можно там есть формула ранжирования которые отдают score после запросов можно например одна из наших оптимизм как-то кастомизации было чтобы взять и вместо одного скоро по одной формуле считать сразу по 130 и верни там получается 7 формулы всем инварианты их параметров вот и чтобы использовать это просто в моей модели вот ну и у люси на еще хорошие комьюнити потому что даже если все остановится то есть прекрасные ласти который продается и которому нужно как-то отвечать перед клиентами которые придет и проталкивать люси ну в счастье в 11 java и когда куда угодно вот что люси на умеет из коробки для тех кто не знает она умеет многое все это связано с поиском то есть она умеет при процессе документы отфильтровывать ненужные слова так называемый стоп-слово которые например при поиске нам не важны такое слово как и например в поиске не очень важно потому что но будет встречаться скорее все в большинстве документов то есть мы не отфильтруем по нему ничего вот все это строится в инвертировано мы нe такси поддерживает нужные запросы умеет сегментацию индекса раскладывать индекс на кусочки вот плюс там есть базовая формула ранжирования которые можно сразу использовать и и побочные всякие штучки вот но и плюсы туда постоянно спиливают различные оптимизации андрей раз тоже рассказывал про то как постоянно идет война между сфинксом илюхиной за оптимизацией различные вот поэтому много всего сразу есть инвертировано индекс то кто знает как выглядит инвертированный индекс классно короче если рассказывать быстро есть коллекция всех слов которые встречаются во всех документов и есть mapping на листы документов на этой структуре основан основанный большинство полнотекстовых поисков в кирки вдаваться не выдох вот типа запросов которые поддерживать люси ну то есть есть просто запрос по слова иногда нам нужно такие запросы к которым мы привыкли в реляционных базах данных то есть сказать что я хочу работам на джаве накоплен без разницы ну или на джаве но только пожалуйста без android вот еще бывает фразовые запросы на важно когда мы хотим поискать два слова рядом или два слова на каком-то определенном расстоянии ну и rain сквере который как раз позволяет нам поискать по какому-то диапазону значений все это классно вроде понятно как там сделать поиск но вот есть проблемка есть похожие слова иногда когда нам хочется жить внутри чем мы просто вакансию на джаве ну вот и чары написали слово java по-русски java или пришел к это чувака просто написал хардкор над живем программисты не все себя называют программистами кто-то разработчик кто-то девелоперы но вот так давайте ребята есть тут же вас крип девелопера да вы не стесняйтесь ну вот java script мере там вообще от обычно творится там есть вы и девелоперы целях девелоперов вот и все это хотелось бы чтобы находилась тут мы сделали просто простенько очень кастомизацию когда собрали свой словарь синонимов для каждого сервера с сервиса он может быть специфичным у нас тут как бы ориентирован на поиск работы вот и что мы делаем при индексации когда к нам приходит документ мы рядом с оригинальным формам то есть рядом с оригинальным словом или фразой мы кладем еще маленьких фишек ссылку на этот словарь из которого потом мы разворачиваем такой запрос то есть мы говорим вот нам оригинальный запрос и давай еще если не найдется то или вот по вот этому фишку поискать с этим вроде все просто но вот документов очень многое надо как-то все вместить вместить все нужно обычного оперативную память индексов несколько на машине и тут приходит сортирование то есть классика распределенных систем но сортировать можно по-разному у нас изначально идет есть сортирование по типу документа потому что кажется это логично потому что документы различные по структуре и очень сильно отличаются по размеру ну и собственно по профилю нагрузки тоже то есть если вот есть яндекс вакансии активных которых занимает всего 600 тысяч 100 индекс резюме занимает 35 миллионов есть еще индекс откликов который занимает вообще там 100 миллионов по нему тоже нужно делать поиск еще можно сортировать по сроку давности когда у вас есть какие-то активные штуки по которым хочется искать всем пользователям есть еще штуки по которым нужно делать аналитику ну вот например у нас это архивный индекс вакансий по которым тоже иногда хочется искать но там пользователи аналитики готовы подождать какое-то время то есть если запрос займёт 10 секунд это не будет страшно если запрос плиту пятна основном яндексе то это будет прям катастрофой вот ну и также можно сортировать внутри типа тут все по классике просто берем делим поэтический кубок количество сортов с этим разобрались нагрузкой отказоустойчивость для начала про требованиям что мы хотим от поиском мы хотим на так мы хотим чтобы большинство запросов укладывались в рамкой 200 миллисекунд на поиск когда вот я говорю базовый поиск на самом деле это те поисковые машины которые работают конкретно с индексом еще вот есть у нас такой параметр как downtime это когда мы понимаем что все вот плохо налажали это когда на самом деле 25 соток в секунду на фронте нзинга front in depth к 4 тысяч запросов то есть 20 500 так это не страшно но у нас это уже инцидент и когда время ответа да франта вырастает больше четырех секунд то мы тоже понимаем что у нас вот мы лежим как выглядит хорошая картина на поиски вот хорошая картина в рамках дня выглядит как то так что мы никогда не вылазим за пределы 200 миллисекунд 95 процентов случаев есть вот этот пик который с утра выглядит странно это на самом деле пришла рассылка и начала активно искать вот и рассылать вот что можно сделать можно сделать во первых локально все улучшить можно можно закодировать фильтры то есть кажется что если у нас есть куча куча популярных фильтров таких как например поиск москве или удаленная работа или частичная занятость или опыт до года то можно каждый такой фильтр складывать в виде битовые мастики где каждый бит будет значить айди документа и проходит он виктор или нет когда к нам такой запрос приходит мы просто можем очень дешево сложить эти битвы и маски и уже отфильтровать конечный этой маской которая которая есть в крыше и уже искать по тем документам которые остались после фильтрации полнотекстовым запросам или другими фильтрами которые менее популярны можно еще делать параллельный поиск то есть сегменты у нас это отдельные независимые индексы индексы разбит по кусочкам можно по-разному крутить искать параллельно по нему и потом собирайте результат воедино вот как сделать еще на кластере отказоустойчивость у нас все разделено на роли в кластер причем раздельно в двух направлениях у нас есть мастер мастер а если его на мастерах идет индексация на слоях чтение поиск и у нас есть так называемый поисковые фронтэнда который мы называем метод поисками и поисковые backend и которые мы называем базовыми поисками отвечают они немножко заразные то есть мета не ранить никаких индексов она просто знает где какой индекс лежит так как и есть партии цианирования и индекса раскладывается на различные машины то нужно во-первых знать где что лежит вторых уметь балансировать нагрузку если какой-то базовый поиск то есть backend поиска сейчас тупит его нужно выкидывать из балансировки вот ну и плюс когда у нас есть партии цианирование то нужно собирать финальный ответ если мы там запросили страницу видит 30 результатов то нам придет ответ 2 реплик по 30 нам нужно сравнить их скоро и собрать отдать вот базовый поиск хранит определенный индекс или несколько индексов работает как раз с ним делает поиск это могут быть различные требования по железу это зависит от размера индекса чтобы все влезло в оперативную память вот ну и как я сказал есть разделение на индексацию индексацию мы делаем на одной машине в каждом дата-центре и уже раскатываем потом ее по репликам для репликации мы просто за использовать билл . а подселюсь она репликатор вот как мы справляемся с тайм-аута my есть за разные стратегии укладываться в тайм аут можно как бы просто ожидать задать и какой-то тайм-аут поиску и ходить последовательно если не укладываемся то есть идти дальше но кажется что мы коммерческая компания если мы перестанем вкладываться в time out and the прибегут недовольные пользователи довольны и пользователи это потеря денег вот можно сделать дорогой способ чтобы гарантированно отвечать просто ходить на все базовые разом но мы не яндекс мы не можем себе позволить кучу-кучу тачек так чтобы ходить одновременно на все сразу поэтому у нас используется стратегия так называемых спекулятивных retrieve это когда вот в общем понимаете допустим на секунду мы делаем еще небольшие тайм-аут и в рамках которых мы не отменяем запросы мы также продолжаем ждать ответ просто еще идем на следующий на следующую машину увеличивая вероятность ну и потом когда получаем ответ уже тогда отменяем остальные запросу вроде с нагрузкой отказоустойчивость разобрались ранжированием так кто знает что такое ранжирование все знают я просто смотрю на не засыпайте вы кажется что вот если есть запрос работа java программистом то не все слова одинаково полезны в нем потому что есть слово работам которая из 6 40 тысяч документов отфильтрует всего 50 и как 600000 почти еще останется вот есть слово программист которая встречается в 20 тысячах документов но есть слово которое очень низкочастотная и очень весомое слова java вот базовые формулы люсена говорят о том что есть такая концепция tf idef когда мы говорим что чем слова реже встречается в коллекции и чем чаще встречается в документе тем она более весомое вот и в люди не есть несколько реализации одна из реализации этого такая вот на первый взгляд страшная штука m25 я думаю люди в зале некоторые должны знать да давайте лавке кто знает классно да вот но pn25 чем можно использовать там на разные поля документы по разному делать и и взвешенный то есть если там приходится матч на заголовок документа то он более весомый чем на тело документа например и это все классно но поиск будет всегда одинаковым этом как-то хотим персонализировать если поиск человек еще java там джуниором давать одним вакансию нервам другие в общем мы хотим персонализацию что мы хотим когда мы хотим персонализацию во первых это вот про junior winner of опыт и вот это все во вторых мы хотим вот обрабатывать эти странные нажатия на кнопку когда человек игнорирует поля и ничего не будет первая версия была идея сделать тагирова ну то есть мы сделали им всё клёво никакого машинного обучения всех этих магических слов просто мы говорим что вакансии и резюме похоже если находится по похожему количеству под запросов вот выгребаем все из логов ты героем вакансию резюме и составляем еще различные кастомные правило кастомные теги вот но и в итоге считаем что если вот по какому то проценту тегов все совпало там и мы считаем что вакансии резюме похоже выглядит это как то так что если к нам приходит человек с опытом джая из опытом в питоне в машинном обучении у нас есть вакансия которая тоже там про djow там ты ластик есть еще немножко про java скрипта мы говорим вот как бы да они похожи вот отдай таким способом фильтруем по пустым это все классно это можно быстро напилить как-то обновлять из логов и до бесконечности придумывать разные различные вот эти кастомизации но есть нюансы нет понятного пути к улучшению как бы можно до кастомизация придумывать бесконечно но не ясно и это никак не зависит от поведения реального пользователей тут хочется вас есть из-за использовать то самое волшебное слово как которая решит все проблемы придумать какой-то идеальную формулу и в итоге хочется получить такое что если вот нас есть эти менеджер который приходит с каким-то своим режимом и если бы он пошел на старой поиск вакансий набрал маслом менеджер то мы бы просто отфильтровали по слову менеджер как-то отранжировали и все показали бы ему не совсем релевантные вакансии где-то внизу было бы возможно его релевантное вакансия которая как-то связано сайте то хотелось бы чтоб превратилась это в такое что сверху мы видим как раз те вакансии которые ему более релевантны ну и в итоге у нас выросло бы вы city or на первые страницы пользователь выросло бы пользовательское счастье и они по нему ходили там дальше 3 страниц в поиске что можно для этого использовать ну во первых у нас есть признаки которые находятся уже внутри резюме и внутри вакансия плюс у нас есть большой большой пользовательский лог предыдущем их поведение если смотреть на резюме то можно сразу заметить кучу как бы все то что есть то есть это из зарплатные ожидания человека его опыт это с чем он работал всякие вас кил им знание языков если смотреть на вакансию тоже как бы можно сразу все это увидеть и можно увидеть те вещи которые как бы мочиться на резюме на вакансию что можно использовать модели что мы используем у себя можно использовать различные совпадения по региону по вилки то есть если человек ищет себе работу с зарплатой от 80 например тысяч это не нужно показывать вам вакансию в которых до 60000 скорее всего вот можно мочить по различным регионам это можно делать даже на самом деле до машину обучения то есть если человек ищет в оканчиваю ярославль июне готов никуда переезжать то не нужно показывать ему иркутск вроде логично вот можно использовать различные расстояния так слышно да по-разному из-за разных полей документов вот ну и плюс можно использовать различные поведенческие признаки которым и выгребаем из логов какие могут быть поведенческие признаки самое простое это пользователь увидел вакансию значит он как-то поискал как-то она ему релевантно дальше он кликнула вакансию она его заинтересовал он ее открыл посмотрел более сильные действия этого добавления в избранное например когда человек не готов откликнуться сейчас или готов открыть ин сон или он просто и вот как бы за лайкал запомнил себя и совсем круто это когда человек откликнулся но такое бывает не всегда есть можно делать может просто пойти там позвонить поговорить о том вот есть еще история когда человек откликнулся и его позвали на собеседовании это прям вообще perfect match с этим точек круто работать но такого мало если как это выглядит жизни есть у нас какая-то история пологом как и мы можем понять то что пользователи похоже к если они совершают похожие действия называется call абортивной фильтрация вот и дальше происходит что если вот пользователь вася который похож на пользователю питью смотрит сегодня какие-то вакансии но и у него происходит отклики добавление в избранное то есть какие-то наши направленного действия то мы можем сказать что вот петя смотри тут кажется похожей ну тут вакансии которые тебе подойдут можем таким способом за использовать как это все выглядит и продакшне production отдельно немножко тема у нас есть индексы в яндексе по вакансиям сразу хранится миль виктора со значениями для модели который мы используем то есть когда приходит новая вакансия сначала она записывается в базу потом она идет на индексацию через сервис машинного обучения который подготавливает для нее виктора фич вот и мы уже с этим записываем в яндекс это накладывает некоторые ограничения то есть если мы хотим что-то поменять модель это нам нужно делать переиндексацию или хранить копии яндекса вот с индексом зима если мы говорим о поиске вакансий все немножко прощу нам нужно просто положить это в какой-то каш не нужно записывать в яндекс и в итоге когда пользователь приходит запросом на самом деле приходится какими-то фильтрующими параметрами с каким-то текстом но еще у него всегда есть резюме айтишник и мы идем с этим режима и хищником достаем его фичи и с этим уже делаем поиск делаем ранжирование все это классно но вот как бы сделали мы как-то там придумали схему как продакшене за использовали различные фичи а потом приходим в пруд и как бы вот такое запросы к ветке не выкладываемся time out боль народ на самом деле тоже решаемо потому что можно снизить нагрузку на тяжелую модель в итоге в фильтру в ранжиру ющий модели у нас около пятиста признаков получилось но вот есть у нас 620 тысяч вакансий допустим пользователь пришел с пустым запросам мы можем на самом деле сначала да еще машинного обучения по фильтровать его по различным листиком в листики это только как раз о чем я говорил про разницы зарплату по регион если человек например не готов переезжать из москвы куда-то то не нужно показывать ему всю россию можно показать ему только москву также можно сузить выдачу по прав области и по различным параметрам получить в итоге вот этот 100 тысяч вакансий на них можно прогнать и быструю линейную модельку из 15-ти сильных признаков дальше уже останется 30000 на которых можно по ранжировать тяжелой деревянной моделькой и в итоге у зоны десяти тысячах делать ранжирование то есть и избавиться от такой высокой нагрузке сразу за узел очередь того что что нужно ранжировать с этим разобрались теперь нужно как-то улучшается улучшать нужно нужно понимать что улучшать можно различить различно мерить показателя ну например можно мерить успешной сессии успешностью сессии для каждого это будет разная если это интернет магазин то это наверно покупки у нас это отклики добавления в избранное то есть то что пользователи нашел себе работу можно мерить средне абсолютные числа от того как это растет вот ну и плюс можно смотреть как меняются клики по выдаче если человек например раньше ходил до 30 вакансии ну раньше бог поиску большинство юзеров там 30 вакансии и стал до 10 то это типа клял вот эксперименты можно делать разные бывают эксперименты без изменение индекса когда мы просто можем поменять как туристический фильтр добавить что туда или сделать какие-то фичи которые будут зависеть от режима только но или бывает еще изменение индекса которые затрагивают как раз вакансию и тогда нам нужно уже перри индексировать менять индекс это накладывает определенные ограничения потому что в кластер у нас появляется еще различные экспериментальные индексы в которых хранятся фич и для других моделей или просто как-то по-другому по счету посчитанные фичи вот и все это мы делаем обычно через опыт эксперимента то есть чтобы понять то что мы что-то улучшили стала правда лучше мы сначала выкатываем это на пару недель на 5 процентов пользователей то есть есть у нас например гипотезу то что человек который хочет переехать на который не указал себе регион переездом живущий где-то в не в центральной россии ным при этом не в москве или в питере мы будем подвешивать оригинальный центр и москву с питером и тогда его счастье должно улучшить дал чтобы ради он хочет переехать на не знает куда как бы по понятным по возрастающей куда можно пережить подмешиваем это наша гипотеза сделаем реализацию выкатываем бы тестирование лучше внедряем хуже не внедряем вот как это выглядит на практике когда вы провели эксперимент у нас есть какие-то понятные метрики каждый столбик это определенная метрика как это улучшилась выдачи на сайте как-то улучшилась выдачи приложение клики отклики добавления в избранное и поэтому мы уже решаем выкатывать экспериментов production на всех юзеров или нет вот что в итоге вышло в итоге использовали готовое решение очень давно построили кластер под свои нужды расскать или в 2 dc так сказать на велосипеде свой такой почти elastic персонализировать и поиск с помощью и моля использовали там где это можно вот покрыли все метриками чтобы понимать то что мы правда делаем пользователем лучше вот но и все эксперименты аккуратненько выкатываем на на небольшое число и пользователь так сказать через система бы экспериментированием что хочу сказать не бойтесь строить поиск не бойтесь добавлять туда эмаль там где он действительно нужен и обязательно покрывайте все метриками чтобы понимать что ваше улучшение приводят к улучшением спрошу привет есть такой вопрос привет я максим если к вопрос по поводу вмешивания имели в люси ну на каком этапе это происходит то есть вот есть у меня документ для него есть скоро рот и скоро формирует там текстовый score и я для каждого документа получаю еще дополнительные скоры из имели потом как-то замешивал со скорым из текста или я беру от текста в top н его perego ranger у ип михайлов pipeline дальше у нас это берется прям то есть части печей записки записывается в яндекс часть вещей приходит это все используется вскоре то есть текстовая текстовая фильтрация используется либо на первом этапе либо не используется и то есть скоро он полностью построен в мыльный модель а утром он он ходит там есть вот несколько стадий который как раз я показывал быстренько вот-вот все стоп после эвристического фильтра происходит вскоре то есть здесь получается когда но скоро строят какие-то свои высота вот эту м25 один из дома там по пустили по тексту все дела получается что все равно на вход модели передается но информация о документе плюс тот score который посчитал ся или вы говорите реплей с этого скоро там в итоге риплейс этого скоро а.н. изначально мы используем эти скоро как в некоторых моделей как у некоторых модель в некоторых вещах как фильтры на то есть если ниже определенного то приходит но там как бы тоже все это очень зависит но по сути скоро как бы вообще не используют чаще всего скоро еще к ним наши вопросы после того как вы внедрили ну то есть особь было там версия поиска когда без емеля в этом ними люси ну всё классно а потом внедрили эмаль пришлось ли вам упростить запрос для того чтобы вы перестали использовать там фразовый matching перешли на term и вот было ли какое-то значимое упрощение заброс то нет были проблемы когда выкатывали моделей не влезали по нагрузке не проходили это вот вот этим вот решалась вот иногда и просто берется и запрос ему ему считается все вот эти показывают как то всё по формулам считается но в итоге этот запрос как фильтрация выкидывается используется вот уже как фичи и последний вопрос вот вся модель на точно на то что вы достаточно много знаете о пользователя который ищет какую-то вакансию как обрабатываться анонимные пользователи то есть как если там какое-то принципиальной мы работаем анонимные пользователи это для многих боль если человек пришел вообще еще ничего не сделано сайте то и у нас нет информации там ни от каких пикселей то не ясно чем показывать окей пассива сразу пока воспользуюсь микрофоном в этом углу привет привет короткий вопрос синонимы откуда берете вообще сейчас мы просто их дополняем и страшные просто непонятно самом деле как по-другому доесть его силе и какие-нибудь как это улучшать может быть наработки пока нет краудсорсинга нет на самом деле вот m-elle модель это как бы все решает то есть тебе даже зина нибудь становится не нужны к да ты не фильтровать по текстовому запросу модель плюс-минус начинает понимать чё там какие какой документ из какой области на что похож вот единственное это как бы сказать объяснить сложно уже становится если синоним все понятно какие документы пойдут то с морем уже все сложнее вот спасибо добрый день и спасибо за доклад меня зовут и news на хочу уточнить такой момент вот например есть слова которая созвучна звучат например web web можно написать там через е через как вы все на меня мах разруливаете вот такие поиски если какой-то подход или какие интересные решения вы находили в работе позволяющие решить эту проблему ну во первых у нас есть печатник та штука которая переформулировать твой вопрос если ты ошибся вот в синонимы иногда добавляем слова который не проявлен написано ну вот если мы смотрим и они по ним идет большое количество запросов то иногда вот такое то есть у вас идет аналитика того что часто неправильно набирается его же вручную это как-то со мной иногда есть но автоматических подход ароматические это вот как раз и печатник но это самописная какая-то да да да я понял спасибо раз тоси-боси больших за доклад привет зад андрей я такой более внутренний вопрос очень круто что получил внедрить моей в поиск а как при этом вы всю эту модель собирается то есть как именно собирается пулы размещаете сами и при помощи их то асессоров или вы пользуетесь какими-то сторонними услугами вот очень интересно как этот процесс устроен самое главное что то ты имеешь ввиду под размечен ими пламя ну не знаю мы вот хотел учить модель нам нужно там за такое количество запросов и как-то оценить какие в итоге документы оказались релевантно кинет и ну тебе есть у тебя должна быть обратная связь обратная связь нас есть в виде кликов иди откликов виде добавление в избранное то есть очевидно это уже когда мы дошли до оба теста до этого периода мы как это скорее это тоже есть у тебя есть человек это на что он откликался просто есть игровая т.к. это да это дом или у тебя есть эта разметка ну да там люди как бы страдали это не доходили таких-то вакансий но разметка эти остается физиологии просто если руками размечать это будет странный сложно и долго и не очень релевантно скорее всего потому что ты же не можешь себя вести как например водитель камаза думает немножко по-другому можно вот в догонку к предыдущему вопросу что вы делаете с новыми объявлениями на которых нету кликов еще если у вас по кликам обращаетесь за днем модель ну то есть это не всем на откликов там же есть какой-то контент какой-то опыт который хочется но а таргета модель лежнина контент или на что tarkett на клики же это к нему то но есть если модель без кликов и она если документ он без кликов и мало похожи на какие-то другие документы вот я так как его это как изымается печей то есть там есть контактное взаимодействие то есть насколько опыт требуемый похож на опыт человека вот есть различные штуки по зарплате по спец специализации ну то есть она похожесть надеетесь как бы что это тоже и вот на самом деле вопрос которых я спросить там вот был бла бла слайд правы эксперименты вот и там вот хороший вариант был показан а вообще там довольно часто бывает плохой вариант когда вот я так видел вас там несколько метров до на которые вы смотрите вот бывает что метрики ведут себя разнонаправленно то есть какая-то метрика значимо в хорошую сторону себя проявила какая-то плохую да и чего ты в этом случае делайте у вас какая то есть метода садимся думаем на самом деле ну как бы все зависит от того чего мы ожидали какая наша была гипотеза если это уже перестают укладываться рамки гипотеза тот садимся и пытаемся понять с полном показали метрики единого рецепта не каждый раз по-разному привет пример собственно вопрос был про метрики но про синонимы но поскольку уже ответил но ответил не полностью вот ты проезжал в докладе что у вас и на него создается там java там живем java там что-то еще джесс тоже понятно а вот недавно была вакансия что-то вроде стрелок на бронетранспортер вот на такие вакансии как-то в этом создаете синонимы или электромонтажника какой нибудь подобное если да то как в первых у нас вакансия такая вы скорее всего не прошло ни у вас я наш виджет или запретить на сложный вопрос зависит если она прямо вот пускай а то вряд ли она и так будет фильтроваться за счет того что если человек ищет брелок но нет я не ждите от вообще похвалит корректно говорить если человек ищет собиратель конструктора лего знаю предполагается что такие вакансии они более редкие все равно будет да ну на самом деле проблему синоним хорошо решает модель то есть потому что ты перестаешь фильтровать так сильно тебе фильтрует эмаль и он понимает где вот примерно как бы документ находится даже если там нет уже к конкретных слов или синоним они подмешан я не совсем привет спрашивая между что вот у вас есть окей программистские вакансии которых там 25 а есть еще вроде там типа 50 60 тысяч ну или там какое-то другое количество других вакансий ну и там он тоже есть но нему закачал как вы понимаете что нужны синонимы имени мужчиной это по количеству вакансии ли как то вы не знаю как то такие честно говоря не знаю потому что синонимы изначально были составлены до меня есть иногда приходят запросы от компании в саппорт они говорят почему вот наше имя не находится вот по синонимам другому окей спасибо вот но и такие запросы мы уже понимаем от добавлять не надо вот у меня есть вопрос так я тут привет меня зовут олег ты упомянула бы тестирования и высказать ты сказал что вы тестируете на 5 процентов пользователей будь как вы убеждаете что вы не делаете ошибку типа 1 балл смотрите то есть и есть такое понятие как вот как вы убеждаете что ваша гипотеза действительно верно его не делайте ошибку что у вас thanked другие факторы повлияли вот какая то реклама вайнер она захочет делаем 2 неделя да я знаю про сезонности провод от все но кажется что в рамках 2 недели все должно быть плюс-минус сок во вторых иногда мы перемешиваем вот эти сплит и то есть мы не просто берем делим поэтическом иногда мы добавляем соль чтобы пользователи как-то мешались вот иногда мы делаем эксперимент не на 5 на 10 процентах иногда делаем эксперт не две недели больше так как бы тоже зависит я просто спрашиваю почему потому что есть возможность делать с помощью танцевать и дал ей где чем ниже число тем более более уверены ты может быть более уверен в том что гипотеза верна и вот мне было интересно то есть две недели или в может быть как то мере эти количество пава количество трафика которые на обед раз пришло то есть вот какие-то такие моменты она плюс-минус но по 5 процента да я понимаю окей и второй вопрос вот уже спрашивали про пользователи анонимных вы используете какие-то и ты еще упомянул то что вы признаки трек эти всякие как например пользователь увидел вакансию да то есть вы трогаете когда он прострелил даниэля то что она в поиске у него отобразилось и то что раскрою используете ли вы и ты сразу же в поиске для персонализации поиска в смысле онлайн ну да вот для анонимного пользователя сейчас по момент понятно всем спасибо здравствуйте тема синонимов она всех беспокоит можно вернуть слайд где хэш вы подставляете в запрос испытал можете покровском скачали а вот этот хэш собственно он же из словаря синонимов у нас правильно и собственно оригинальное слова она в этот хэш в принципе тоже входит а но вас запросе она стоит через или но что соответственно равноценно требует от движка перелистать весь индекс ну вот это да это для чего делается для того чтобы вес документа когда в нем входят оригинальное слова был выше ну то есть если человек попался синоним то документ будет ниже чем с оригинальным слон даже если synonyms опечаткой ну то есть вот оригинальный слова у нас было с опечаткой если печатникова исправит то да если вакансии тоже печатка туда если нет то нет добрый день у меня вопрос вы говорили что когда объявление создается но изначально попадает в базу а затем происходит ну индексирование можете поподробнее рассказать вот этот вот процесс индексирования когда он проходит в какой момент у вас сразу ну то есть записали в базу отправили на индексацию то есть у вас никакие индексы больше но не используются то есть у вас типа rt индекса там до используется вот как свинг я например то есть к мы сразу можем писать типа да я понял спасибо а можно мне туда вопрос еще раз уж никто не хочет больше никто хочется есть хотят land the late здрасьте вы сказали про кэширование популярных фильтров до орбиты вы и маски работает наверно супер быстро как вы насколько вы пишете и как вы понимаете когда инвалиде ровать кэш особенно с учётом ваших оба экспериментов вводимых насколько нескажу скорее всего я знаю что по-моему 1000 популярных фильмов нас кэшируется вот если если произошла запись то крышка queen варьируется то есть ненадолго скорее всего там секунд спасибо еще вопрос забыл вот ты про оба теста вот рассказывал но вот apts ты довольно дорогая вообще методика приемки но нужно две недели ждать вот кто то есть у вас помимо в тестах чтобы проверить вообще типа стоит ли катить в pt100 оффлайн приемка так называемый какая то есть у вас и какая из команд можно сказать оффлайн прием как это как ну наверно как бы есть методика методики как бы принятие решения того что формула там ранжирование не знали индекс дает профит по качеству до того как выкатывать в.б. можно посчитать что-то на данных заранее понять это совсем плохо это может быть не очень плохо как бы дальше уже что-то типа с такой есть у меня есть но немного как бы когда мы смотрим на эвристические фильтр например то мы пробуем смотреть насколько это за узи тэга расширить выдачу вот то есть только такое а в терминах качества то есть ну там метрика которая не по кликам или там по кликам считается в историю не знаешь не так вот скорее всего он в процессе обучения модели происходит я просто не do to say that is so in this простите да и не до цените ст вот поэтому там то есть есть сам train выборки но я понял то есть сначала модели обучаете на каких-то отдельных это ракетах считаете проект потом уже вы бы сразу к покус ну прям до кусочком собирать все забыл спросить коррекция ошибок поисковых запросов пользователей как вы разогнулась хорошо просто я на самом деле не закопался в эту часть эта штука была реализована давно новой более стандартные люсьен функционал инженеры там что-то свое написано вот в этом интересах я не заказывал сейчас ocre"
}