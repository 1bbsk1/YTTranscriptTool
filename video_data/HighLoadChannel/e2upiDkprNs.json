{
  "video_id": "e2upiDkprNs",
  "channel": "HighLoadChannel",
  "title": "Tarantool: от коммита до прода за 20 минут / Роман Проскин",
  "views": 4646,
  "duration": 2237,
  "published": "2022-03-21T14:11:15-07:00",
  "text": "привет всем кто в зале спасибо что пришли привет всем в онлайне меня зовут броски норман я работаю в tarantul и четыре года успел поработать телекоме в банках занимаясь разработкой высоконагруженных высоконадежных приложений то чем тарантул славится вот и последние где-то полтора года я занимаюсь плотной и эксплуатации этих проектов как раз хочу рассказать о своем пути вот кем ошибки допускали и начну я с того что такое тарантул напомню немного или там кто-то может быть не знает не сталкивался на самой интересно я расскажу о том какие ошибки допускали и мы придем 5 правилам простым правилам как эти ошибки не допускать начнем с тарантула кто знаком или там давно знаком strontium поднимите руки есть зале отлично и наверное слышали что тарантулы это баз данных и сервер приложений в одном флаконе но я хочу немного переопределить это трандл это все-таки платформа для работы с вашими данными оперативы тарантул быстрый мы недавно завезли поддержку арма м 1 ноября мы намерили там 2 миллиона транзакций в секунду операции вот пруф у меня нет ну идем дальше надежность проект над которым я работал в телекоме вроде до полтора года в продакшене и без каких-то критических багов я уже тут давно ушел вот сейчас в другой области ну там мы относить небольшой кластер подняли он тарантул в том числе масштабируем сейчас вот у нас вроде там есть из 500 сенсов но мы сами у себя ты стиле 1000 2000 просто вот запускали в едином кластер и так мы определили что такое тарантул давайте немножко про его внутреннее устройство буквально там несколько слайдов схематично в игре следующим образом есть разные компоненты нас есть бинарных это колой про то которые для работы с пользователями запросами вот далее это у нас попадает в так называемый event луп там происходит основной бизнес только приложение и работа с данными а сами данные хранятся в арене это наше хранилище данных есть два механизма основной естественно работа в оперативен есть еще на диске но когда мы говорим что тарантул там работает с оперативной памятью все думают почему то что он не президент ный это не так у нас есть работа с диском тоже мы сохраним снапшоты наших данных и журнал изменений в лог так называемый вот и на этой схеме еще остался один квадратик точнее ромбик relay он связан с аппликацией данных когда мы объединяем наш стран тулы were приказ это делаем копию наших данных вот и что стоит понимать они все тарантулы все одинаковые вчера мой коллега рассказал про картридж кластер на решение tarantul и как раз вот предложение tarantul нам кластерном будет являться просто набор и таких инстансов они все одинаковые но могут разную бизнес-логику выполнять вот мы определили что такое тарантул и немножко смотрели на устройство что такое приложение на тротуаре теперь про то как его деплоить все как с ним работать и на самом деле это и не просто и просто одновременно нужно немножко отойти от подхода например 40 вам где мы устанавливаем его настраиваем он работает тонтон ножка не так подход этот циклический мы разрабатываем приложение затем его тестируем обязательно так и потом уже диплом в рот кажется довольно интуитивно все по годам если развернуть эту картинку она выглядит как то так у нас есть ты за нашу задачу мы разрабатываем тестируем у себя там локально на макбуке еще где-нибудь затем подключаем уже всей системы дженкинс beetle обсе и т'хаб экшен с неважно у нас есть статической анализа ция кода обязательно проверяемый когда мы вот протестировали проверили что наш наше приложение хорошее мы его собираем и диплом на тесты там вот на на на тестовых зонах проверяем например интеграцию с внешними сервисами как как наше приложение работает с реальными данными здесь мы заложили какую схему данных мы проверяем что на какой условный уроки он не пришлет ерунду какой не джейсон или общее бинарные данные дам запросим все это проверили подключаем мониторинг и делаем нагрузку здесь мы смотрим как наше приложение соответствует не функциональным требованиям отвечает ли она там нагрузки слой и только потом когда вот ну точно удостоверились что все функциональные не функциональные требования мы закрыли мы диплом все впрок вот идут схема это как мы делали вот deploy в телекоме в проект интерьера последний полтора года работал этот процесс цикличный как я уже сказал на финише он не заканчивается и вот как раз такую систему надо выстроить чтобы там за 20 минут в прод выкатить приложение после к метода но это еще не все все было бы просто давайте посмотрим какой у нас был схема деплоя на самом деле была простая это чисто пример чтобы быть в контексте у нас был небольшой проект ну как небольшой там полтора терабайта в оперативен нам сразу сказали что должно быть резервирование дали нам 21 мы назвали активным туда приходили все запросы пользовательские другой был в резерве но мы его использовали для диплом каждый год состоял из трех физических серверов там по вам были 24 и danesi он и гепид 148 на каждом сервере стоял стоял 18 инсов тарантула и как я уже говорил а не то все одинаковые но бизнес луком не было разные ты была роутера которые обрабатывали запросы и маршрутизировать их в кластере и 15 наших лошадок хранилищ данных которые были вот в шар дом распределены и хранились в общем такая примерно схема значит теперь наверно к самому интересному что пошло не так ай да что пошло не так в диплом поговорим про фейлами начнем с того что только как только мы пришли на этот проект мы deploy лись руками мы конечно не враги себе да мы там сразу решили не таскать архивы там руками на серве разархивировать там запускать мы сразу сделали плей букин diablo вы запускали мы все вручную не было никакого сей и тепло растягивался там на день потому что мы заняты занимаемся какими-то другими важными делами так никто не хочет попади плыть и коллеги которые нам помогали со стороны заказчика тоже были все время заняты каждый день превращался там в день вам нужен был искать людей выделять время и конечно нужно было делать вся и что мы в итоге и сделали но это еще не все у нас проект был сначала небольшой было не пятнадцати девять сервис серверов инстансов на сервере но потом данные стали расти естественно мы мы мы как бы закладывались но заранее не продумали это что у нас может настолько сильно вырасти наш сервис и мы перестали укладываться 20 минут и почему 20 минут вообще-то у нас было такое условие что там сервис не должен в течение обновления должен быть не должен быть доступен больше 20 минут соответственно как только это время превышала происходили автоаварии там приходили sms оповещение там почта звонки там высшему руководству и они там приходили ко мне почему вас все сломано почему сервисный работает вот на самом деле не было все сломано у нас просто были огромные инстанции там порядка полу there и которые должны были за это время прогрузится в оперативу тарантул использует все доступные ресурсы вот все эти сорок восемь ядер он просто не успевал за эти двадцать минут мне приходилось оправдываться что нет у нас все нормально пользователи не видят что наше приложение обновляется так скакнул немножко и мы заранее это предусмотрели чтобы в течение во время обновления пользователям всегда был доступен и для это у нас было 20 как раз дальше за все время как мы были на этом проекте мы писали эти отзывы playback и решили выложить их в open source когда сан-диего картридж по нему чуть попозже но суть там была такая в старых версиях это был один такой большой playbook у которого разные этапы деплоя были разделены тегами и предполагалось что там не знаю каким образом там руками мы по этим тегам запускаем различные этапы но я где-то потратила недельку принес это заказчику говорю вот вот мы сделали но мне сказали у нас там папки в других директориях создаются да нам нужно запускать не систем где а там супервизор использовать и мы не мы не смогли вот быстро строить другие инфраструктурные моменты в наш playbook вот это выделилась конечно в ticket который долго делали но в итоге они был картридж сейчас модульный довольно забавная ситуация произошла когда я уже ушел с проекта где-то спустя несколько месяцев вечером не пишет мой руководитель у вас сервис там 500 хочу не доступна запись не идет пошли разбираться оказалось что у нас логе то могут трактора байта выросли просто один файл вот он лежал рядом с данными на одном маунте и тарантул не мог создавать новые файлы не мог писать файл и изменений вот ну просто не работала с отдельно на на запись и мы конечно быстренько этапа провели там нашли коллег из инфраструктуры и которые помогли нам разделить это все спустя еще несколько месяцев очередная ситуация я смотрю ютубчик вечером опять пишут что у нас ничего не работает мы смотрим вроде логин все нормально место есть вот оранта салоне может открыть файлы и как оказалось агент мониторинга мы подключили мониторинг и он нам сыграла злую шутку он открыл кучу подключения пахать и т.п. и не закрывалась добавили в настройки там по моему подарок чтобы он закрывал collection и с этой проблемой мы тоже покончили можно было бы наверно про многое рассказать например что не надо в пятницу вечером деплоить да ну давайте перейдем к правилам мы поговорили что такое тарантул поговорили про файлы которые у нас были во время диплом а теперь 5 правил и первое это довольно очевидно телеметрии до нужно собирать метрики логе кажется все все это делают но по факту постоянно забывается и летом откладывается на потом мы потом сделаем не всегда это в итоге происходит ну суть том что пока мы рисовали наши даже борды в графа не в итоге у нас это вылилось в open source дашборд он доступен по ссылке и вот пример это полный старые-старые графике но здесь показано как вот у нас влоги заполнили весь наш мандата до 100 процентов то что в таранто вообще 100 лет следует мониторится да ну вот что есть нашим дашборде также покомпонентная по пользовательским запросам been applied колой про то нужно смотреть количество входящих подключений и объем трафика когда мы говорим про бизнес-логику это напрочь если тебе запрос и то же количество запросов и ошибки какие у нас вообще ошибки происходят и много ли их есть еще такое lua памяти то выделенная системой память под нашу интерпретатор например там не надо делать бесконечные циклы да или там создавать огромные таблички просто 2 2 гигабайта выделение закончится и тарантул пойдет далее по хранению нужно смотреть за тем что мы укладываемся выделенном память если мы дошли там до 80 процентов нужно уже задуматься тем чтобы добавлять новые инстанции или там железо увеличивать делать ришар ding данных то есть перекладывать их на другие хранилища и on следить за фрагментацией при работе с диском естественно следить за место чтобы не было как у нас когда логе съедают просто все все доступное пространство и если мы используем репликацию но это тоже нужно использовать нужно следить за тем чтобы данные были актуальны чтобы лак не рос сильно выглядит но дашборде это как-то так здесь снизу как раз графика логарифме кации там 100 миллисекунд на там среднем микросекунд по центру это кластерные метрики я про них ничего не говорил можно целом прочитать документации или в модуле metrics это все есть статус сервисов есть какие-то общие графики при этом количество памяти общее используемое ли количество запросов или операций в секунду просто примерно такая графика на самом деле ценится еще дальше там много метрик можете ознакомиться на сайте и так по телеметрии не забываем всегда подключаем все инстанции они у нас одинаковые но разные бизнес-логика они могут разные метки выдавать мы следим чтобы не превышали из критические показатели например 8 процентов памяти или вы можете добавить свои зависимости от ваших инфраструктуры ваших требований и до влоги кладем отдельно данных это просто такой вывод небольшой то еще правило 2 из 5 нужно делать маленькие инстанциям 30 гигабайт почему 39 чуть попозже расскажу вспомним какой у нас был как а нас был схема диплом у нас было полтора терабайта в оперативы и был до на несколько тонов чтобы затем игрались сразу возникает вопрос сколько копий данных делать резервирование и на сколько кусков нужно разбивать данные нашем проекте было вот 48 трейдов на серверы и нас была доступна 758 по моему гигабайт рама мы просто пирате экспериментальным путем пришли к тому что нужно по 30 гигабайт кусочки разбивать и делать столько копий кратно количеству соков это удобно при тепло и вот почему именно 30 есть две причины ограничения сверху это как я сказал у нас были очень огромные инстанции они не успевали подниматься с диска оперативу после рестарта после обновления и мы как раз искали ту золотую середину чтобы вот и вписываться в физическую инфраструктуру которую нас есть и по времени укладываться снизу ограничения в том что на 2 incense and ran to надо трейдера то есть полтора и драма instance и у нас была там x48 а нам нужно разместить еще агент сбора телеметрии который пал на джаве написан нам нужно еще роутеры разместить помимо вот хранилище данных и поэтому нам все но нельзя слишком низко уходить мы можем там по 10 гигабайт хранить слишком много будет тарантулов вот мы пришли к 30 ну для вашего инфраструктуры может быть своя золотая середина там плюс минус 10 гигабайт наверно так про это поговорили что у нас нужно помнить что мы разделяем всегда копии по нескольким сотням до настраивали аппликацию и делаем небольшие кусочки done взбиваем их на 30 гигабайт за еще правило серединка это обновление без простой это же как раз то чтобы к вам по ночам не приходили начальники не горели почему а сервис упал чтобы пользователи не видели ваши обновления неважно как она будет сделано руками или через дженкинс здесь скажу немножко другой кластер это вот как раз один из моих текущих проектов в банке там есть 3 терабайта данных общий объем 500 инстансов тарантула это с учетом резервирования и 400000 пользователь запросов ну мне кажется вчера сказал что даже давать места находят то есть ну такой небольшой хай-лоу задам у нас обязательно если своей не больше часа в год недоступность и возникает вопрос как это deploy да как-то обновлять чтобы наши клиенты наш пользователь не заметили есть два способа как раз вот за чем мы просили именно 2 отсюда или там неважно какой количестве на самом деле гана больше одного это антипод по плечам когда у нас один сот является активным принимает все пользователи запросы а два другие мы можем обновлять любом порядке и нам нужно механизм который это будет делать переключение вот среди к плюсам можно отнести что мы целиком обновляем весь все все доступно сервера я нам не нужно там гадать все ли мы обновили или нет нож какой-то instance остался на старая версия он относительно быстро этот способ вот в нашем в нашей инфраструктуры 45 инстансов за 6 минут обновлялись это там по 15 по 20 минут на 1 сот час и у нас к сожалению есть ограничение на общее количество incense и потому что мы ограничен по времени там 20 минутами мы не можем больше 15 за раз обновлять следующий способ в целом идей на такой же только мы обновляем не целиком а мы разделяем наши рипли кассеты по порядка му номеру то есть разделим их на группы и уже меняем вновь внутри групп мастеров все на самом деле они могут быть на разных судах есть у нас три на самом деле мы этот способ использовать для двух и здесь мы можем выбирать вот размер пальчиками можем выбирать сколько инстансов за раз обновить но это как когда приводит к тому что тепло немножко замедляется потому что меньше инстансов они конечно быстрее поднимутся до там не нет ни 20 минут а там 5 поднимутся за 10 минут у нас его 45 инстансов потому что все обновить нужно девять раз эту процедуру провести это выливается полтора часа вот но еще нужно уметь конечно выбираете то и в этом поможет они был коттедж про который как раз следующий правил будет но пока вспомни вспомним что нам нужно опять же несколько цодов обязательно больше одного чтобы использовать вот механизмы обновления без простоя и в общем-то обновляться таким образом как раз вот следующее правило предпоследнее это не обновляться вручную использовать всей дженкинс girl of actions что угодно использовать анти было картридж я про него говорил вот наконец мы к нему пришли что это такое это роль для анди бла которые содержат тридцать три отдельных клубу к 33 операции которые можно сделать кастерам наверное практически все можно завернуть он open source най ссылочка на экране и мы у себя тестировали просто так ради интереса до 1000 до 2000 инстансов за пять минут поднималась это правда без данных уже уже хорошо что он умеет до 33 playdough она немножко подробнее зависимости от вашей инфраструктуры он может работать с тем пакета месте redhat или центр сюда с где пакетами и универсальный tar.gz просто архивчик который подкладывается распаковывается в нужную директорию он умеет создавать ее нет файлы для систем де например и с их помощью останавливает запускать и перезапускать инстанция работать регистратором настраивать кластер тарантул картридж умеет настраивать в шар шарди рования данных и fila вверх это все из картриджа все штуки с его помощью можно настроить механизм его ли не по плечам или потом пачками как раз пример пример есть и он написан на гитхабе также можно вообще произвольный код внутри инстансов исполнять и сумели многое что еще и все 33 пункта я не могу сюда поместить вот ну здесь как раз пример если кто следит можете сканировать пример обновлений по плечам описан как его сделать анти был картридж ссылку на презентации будет поэтому сможете еще почитать и наконец последнее правило это но он довольно и на самые противоречивые обновляться почаще я вообще советую там хотя бы 1 квартал почему так этому на самом деле тоже 2 причина есть первое это релизный цикла тарантула да и вообще модулей тарантул в том числе он ложится вот в этот период в 3 месяца выходят новые мажорные версии новые баг фиксы новые фичи просто желательно хотя бы баг фиксы покрывать обновляться до новой версии это на самом деле screen реального случая я только пришел в тарантул одна из моих первых задач было написать сервер сбора телеметрии статистике я буду получи написал на ложке быстренько заде пойло где-то в докере на на одном знак серверов и он у меня два года там крутился а потом ко мне приходит коллега и говорит надо обновиться я уже и не помню на самом деле где я вообще-то поднята и когда залогиниться на этот сервер вот такое может происходить может быть и не часто но чаще всего не могут люди уйти и да за промежуток например в полгода поэтому я вообще советую не держать аптайм там больше полугода обновляться во-первых потому что баг фиксы приходят новые версии появляется во вторых потому что просто человеческий фактор поэтому обновляйтесь мы поговорили про эти 5 правил давайте немножко подытожим каждый из них вспомним первое это обязательно телеметрия собирать не только медики мной логе разбивать наши данные по маленьким кусочкам не надо пытаться полтора терабайта засунуть там в 10 стран то ничего хорошего из этого не выйдет можете попробовать обновлять приложение без простое использовать ангел картридж а и последнее обновляться почаще ставьте новых версиях этого 1 квартал вот эти 5 правил я конечно же не могу на все рассказать более подробно есть моей статье диплом тарантул без людей на хабре про мониторинг тоже мой коллега отлично написал я вообще ничего не упоминал про каберне this васи тебе рассказал в вебинаре личного на ю тубе но если вы хотите попользоваться картриджем составлять кластер есть локальная утилитка для этого тоже статья появись пользование есть на хабре вот мои контакты и ссылочку на презентацию если кто хочет там статьи открыть может отсканировать я оставлю этот сайт наверное у меня все есть ли у вас вопросы спасибо а я напомню что вопросы можно задавать как в онлайне такого плане и надо будет сделать сложную и ужасную штуку выбрать кого-то одного чтобы вручить за классный вопрос замечательный книгу до поаплодируем и ждем вопросов там книжечка интересно создавать образ так ну похоже что нет есть образа есть вопрос я следует проректор привет меня зовут тейк подскажи пожалуйста как я сейчас самый высоко нагруженный сервис используют таран вот это не напрямую относится к презентации но в целом а я его как раз вспоминала вот тот который на 500 инстансов ну на самом деле в более должны быть более сколько нагружены но с теми с которыми я работал это вот в банке на 800 крп с огромной incense и второй вопрос какая такой самый большой косяк вообще происходил то есть когда понимали чтобы lingo тут мы прям вот так это у нас были ситуации когда приходилось мы делали те 10 систему очень за кетчер для оракла и у нас была там две системы получения изменений и 40 и холодно загрузка как бы когда мы первоначальные данные выгружаем вот самые такие файлы наверно были когда мы приходим значит делаем вот эту холодно загрузку она потом там падать деньги середине и мы такие упс там ситуацию восстановление загрузки мы не продумали реки ups и она идет там долго где-то максимум у нас там где-то два дня 2 наверно это занимало месяцы свыше грузилась с десантом через день она там через полтора дня упала и номер и начинать заново вот это конечно файлы до давай-ка здесь вопрос первом ряду нет спасибо за доклад я вы конечно уже слышал несколько раз у неё появился вопрос так ты говорил что вы не пытаетесь в том числе тепло и тесь пачками invites и нет ли в этом случае проблем конфликтов внутри самого кода внутри самого плеча и как вы их разрешаете если они вдруг случились у нашего там могут быть половина не сэнсов сейчас в рестарте по инстансов работают но они все еще в одной половине кластера и понял вопрос смотри группы вот этих инстансов это один rip рикошет то есть это однако педан то есть мы например берем все вторые копии во всех реплика сетах и обновляем только их что касается если у нас какие-то проблемы внутри сода и даже не так и внутри общее приложение то мы стараемся сделать так чтобы у нас сами наши обновления были совместимы это конечно наверно смешно звучит но да это так мы проверяем и не делаем обновление которые меняют схему данных например тогда это бы нам сломала весь процесс потому что у нас были бы пользователи которые например только что работали со старой версией мы обновились им данных поменялось меняем мастеров и они сразу работают с новой версией а клиенты могут быть не готовы к этому например если мы там что-то сломали вот мы стараемся так не делать то есть вы деплоить и какой-то тот наполовину или нет да да мы мы не можем не все инстанции за деплоить только и только часть но при этом все эти incense они в одной группе вот именно как порядковый номер это уже как разделить на самом деле но они точно являются пока мере на момент деплоя они точно не используются вот так пони другие инстанции в соде могут тепло и ца могут работать а имя как разделим вот я кажется понял чему ты помнишь мы игра с их делим на маленькие пачки чтобы они быстрее успевали сделать рестарт и это не влияло на там других мастеров которые в этом же суде например вот именно поэтому не большие пачки вот еще один вопрос есть все высота cla а можно вопрос по поводу сильно ли нужно тюнить систему ios для того чтобы тарантул работал вот с тем как у вас работает в банке и что вы для этого использовали может быть какие-то бенчмарки под такую нагрузку у вас есть вы написали что вас есть там в конце а так называемый lodging тест вот что на себя представляет еще раз какой-то ст ну нагрузочного тестирования а later с вашей схема была написана привет перед продам да спасибо тут два вопроса до первые это те ним ли мы операционку дотянем мы например вот в том же банке мы выключали оон killer ну как заключали мы там делали тарантул наименьший или иной выше приоритетом смотря как посмотреть да чтобы короче тарантул только вот в последнюю очередь прибило и если вот вот так произошло там не знаю так сложились звезды чтоб тарантул последнего черепаново второй вопрос про latest мы делаем нагрузку на пользовательских данных и мы советуем всем так делать не все на слушаю сожалению то есть нам обязательно надо чтобы данные были реальны и была реальная нагрузка на сервер системы например держим 800 крп с мы на нагрузке даем там миллион рпс то есть мы проверяем что мы держим там плюс какой-то буфер от заявленной той который нужно сервису но обязательно на реальных пользовательских данных так так еще вопросы ну судя по всему тех кто готов задать вопрос прямо сейчас нет так что стоит выбрать кого-то чтобы вручить замечательную королем его это коллега да хорошо force touch замечательно да спасибо вопросу классный и я думаю что будут еще вопросы но по-крайней мере могут быть это будет уже в кулуарах то есть мы сейчас выйдем будет профилактика клары располагаются на выходе сразу справа в районе колонны да и еще одна маленькая но приятная штука подарок спикер о спасибо да спасибо"
}