{
  "video_id": "RuOlyao1epg",
  "channel": "HighLoadChannel",
  "title": "Кэширование пользовательских данных в СХД / Михаил Мотыленок (YADRO)",
  "views": 342,
  "duration": 2733,
  "published": "2025-01-17T02:28:27-08:00",
  "text": "Всем привет Меня зовут маленок Михаил сегодня будем говорить про протокол синхронизации кше Я работаю в R подразделении компании ядро и занимаюсь собственно уровнем кэширования пользовательских данных в схд семейство T unif про что мы сегодня будем говорить Какая у нас собственно говоря Анда В начале мы поговорим собственно говоря про са что это такое Как это работает посмотрим Зачем в ней нужен какой-то кэш Да зачем нам собственно говоря кэшировать пользовательские данные затем мы уже посмотрим Зачем этому кшу нужен какой-то протокол синхронизации что он у нас будет делать Какие перед ним есть задачи и дальше мы посмотрим какой был наш Трудный путь к текущему протоколу синхронизации кэше А что мы сделали в начале Как это не взлетело и что мы делали чтобы это исправить Ну коротко поговорим про саму схд основная задача системы хранения данных - это предоставлять на клиентское оборудование на клиентские серверы некоторый дисковый ресурс клиентские серверы подключаются к схд по протоколу Ази используется в качестве физического уровня Fiber Channel или eet и на клиентском оборудовании ресурсы системы хранения данных видны как обычные блочные устройства да то есть по сути как обычные диски в линуксе можно к ним подключить Любое приложение а какие вообще типовые кейсы использования схд Зачем заказчики вообще в принципе схд покупают Ну на ресурсах системы хранения данных можно разместить базы данных Да можно подключить любые БД pog mysql всё что угодно неважно А мы можем использовать ресурсы схд для того чтобы размещать там диски виртуальных машин то есть Можем подключить к schd гипервизор на схд можно настроить файловый доступ то есть поднимается файловая шара И большое количество клиентов имеют возможность получить доступ к файлам по протоколам NFS или smb и собственно говоря к схд можно подключить всё что угодно да то есть любое приложение любая бизнес-карте использовать систему хранения данных Ну теперь давайте посмотрим Почему вообще в схд нужен кэш такой Казалось бы довольно очевидный вопрос вот в нижна есть такая интересная табличка в ней показано какое будет время доступа к различным типам памяти если бы один цикл CPU занимал бы у нас од секунду ну целиком мы её смотреть сейчас конечно не будем нас вообще здесь интересуют только вот эти строчки здесь хорошо видно что в таком масштабе доступ к нашей основном на SSD диске заняло бы от 9 до 90 часов а на обычные шпиндельные жёсткие диски у нас могло бы занять от оного до 12 месяцев то есть ну разница здесь очевидна очевидно что если мы будем в нашей быстрой рам памяти кэшировать пользовательские данные Да мы можем сильно уменьшить latc I и радикально ускорить наше пользовательское приложение Ну давайте теперь так общим взглядом посмотрим на нашу систему хранения данных из чего Она состоит наша система хранения данных состоит из двух контроллеров хранения они соединены между собой высокоскоростным интерконнект и оба контроллера у нас имеют равный доступ до бэнда кэндо мы называем одну или несколько дисковых полок расширения это дж боды в которых собственно говоря стоят диски сверху к нам на этой схеме подключено Поль оборудование так как мы работаем в режиме symmetric Active Active пользовательское оборудование каждый инициатор подключен ик sp0 и к sp1 и может совершать а запросы и в любой контроллер в любом порядке Теперь давайте посмотрим на наш стек обработки данных на наш дапа когда к нам приходит пользователь ско IO первым делом оно попадает в сказ Target из кази таргета оно попадает уже к нам в кэш пользовательских данных выгрузка из кэша пользовательских данных осуществляется на уровень маппинга это такой компонент который позволяет реализовать такую интересную функциональность как снапшоты настоящие тонкие Тома и вот так далее ну а под уровнем маппинга уже располагается й это обычный наш софтвер Рей наша реализация под названием TL илий она нужна очевидно для того чтобы не допустить повреждения данных непосредственно на дисках и защитить нас от отказов поло в целом мы будем сегодня разговаривать только про компонент под названием кш данных Да другие компоненты мы сегодня рассматривать не будем и посмотрим собственно говоря как он работает ну кэш у нас оперирует страницами страницы имеют у нас размер 64кб Это обусловлено у нас нашей целевой платформой которая до недавнего времени у нас была Open м приходит запрос нари если это шми то он проваливается в эн читает из бэнда необходимые пользовательскому приложению данные возвращается обратно копируем после этого данные в нужную страницу ша если страницы ещё нет Мы её ларум после этого возвращаем эти данные наверх пользователю Ну соответственно если это шх то мы можем сразу почитать эти данные со с нашей страни если это запрос на сразу же копируем пользовательские данные в страницу кша и возвращаем наверх подтверждение записи через какое-то время страница будет вытеснена на бкн и записано уже в фоновом режиме А у страниц есть некоторые атрибуты вообще их на самом деле довольно много но мы сегодня будем говорить в основном про состояние страницы страница мы будем говорить что она находится в состоянии Clean если данные в ней полностью соответствуют тому что сейчас находится в кде и страница будет находиться в состоянии дте если в этой странице есть какие-то новые данные то есть пользователь туда что-то уже пописал помимо основных состояний страниц У нас есть служебное состояние это состояние empt для страниц в которые ещё ничего не попало и для тех страниц которые мы вытеснили из кша которые мы очистили и есть состояние страницы sinking это состояние в котором мы записываем страницу на когда мы вытесняет из кша она блокируется на запись а её запросы на запись будут вставать к ней в очередь И в этот момент Мы производим собственно говоря запись на н Теперь давайте поговорим зачем здесь вообще нужно что-то синхронизировать Ну чтобы это понять Давайте представим что у нас есть некоторая дамми реализация что мы вот вообще ничего делать не стали и кши у нас получились полностью независимые что в таком случае получится Ну вот будем рассматривать всё на таком примере вот у нас есть ш sp0 и ш sp1 SP - это вот storage процессор контроллер хранения допустим к нам на sp1 приходит запрос най мы его сразу же из кэша подтверждаем что у нас может получиться после этого опять же так как мы работаем в режиме symmetric Active Active ничего не мешает после этого клиенту взять и попробовать эти данные почитать из другого контроллера то есть R запрос придёт на sp0 что в таком случае получится ну довольно очевидно что ничего хорошего Ни о каких новых данных sp0 у нас ещё не знает это будет камис Мы проваливается в Кэн читаем оттуда старые данные возвращаем их наверх пользователю с точки зрения пользовательского приложения это будет выглядеть как дата капт да то есть он записал некоторые данные на диск а потом не смог прочитать их обратно ну естественно так жить нельзя поэтому мы добавим мирроринг То есть каждый новый пришедший к нам сверху й запрос мы первым делом берём и просто копируем через наш высокоскоростной interconnect чере используя технологию rdma на другой контроллер на sp0 После этого мы уже параллельно на обоих контроллерах складываем эту страницу в кэш там делаем все нужные для этого действия и э после этого кумулятивно подтверждаем запись наверх кумулятивно - Это значит что мы ждём что и sp0 и sp-1 запишет эту страницу в кэш Окей отлично теперь Данные есть на обоих контроллерах что же у нас будет с этим всем дальше но очевидно что вечно в кэше данные находиться не могут у нас в таком случае никакой памяти не хватит мы должны в какой-то момент эту страницу вытеснить и в случае если опять же мы рассматриваем нашу дамми реализацию в которой никакой синхронизации нет может произойти такая такая штука что политика которая управляет вытеснением из очередей кэша придёт на оба контроллера одновременно и оба контроллера начнут записывать эту страницу в энд Ну ничего хорошего из этого тоже не выйдет мы по сути с двух компьютеров пишем в одни диски чтобы это как-то разрешить придётся на уровне рейда делать блокировки распределённые блокировать регион всё это будет работать медленно и мы наверное тоже не очень этого всего хотим Ну наверное простым и эффективным решением чтобы эту проблему побороть будет статическое партиционирование то есть мы берём всё адресное пространство каждого Тома нарезаем на чередующейся партиции одинакового размера И говорим что все Например чётные партиции у нас будут принадлежать контроллеру сп0 А все нечётные партиции Ну соответственно оставшемуся контроллеру sp1 тогда для каждой страницы каждый контроллер может сразу сказать кто её будет записывать да кому она принадлежит Ну в данном случае страница находится у нас предположим в партиции сп0 и записывать её будет тоже sp0 А на оставшемся контроллере мы скажем что эта страница находится в пассивной партиции теперь у нас получается распределённая запись на эн работает быстро без блокировок всё очень круто но тут тоже остаётся некоторая проблема Проблема э остаётся на контроллере sp1 там где страница находится в пассивной партиции потому что она-то на не записывается она остатся в состоянии то есть нам теперь нужно добавить Управляй сигнал какое-то управляющее воздействие которое будет эту страницу на контроллере С1 также вытеснять Давайте теперь немножко Притормози и посмотрим что мы имеем на данный момент значит новые данные у нас всегда копируется на оба контроллера полная копия пользовательских данных лежит в обоих Шах и при этом запись на у нас между этими д контролерами чтобы каждый контроллер в каждый момент времени имел полную информацию о том Какое АО попало в энд А какой ещё нет Зачем же мы всё этого всего этого хотим Ну основная причина почему мы этого хотим мы хотим поддерживать когерентность мы не хотим допустить ситуации когда пользовательское приложение может обратиться к одному контроллеру хранения и почитать одни данные или обратиться к другому контроллеру хранения и почитать другие данные то есть Мы хотим чтобы пользовательское приложение работало с нашим с нашими по сути двумя отдельными Шами как с одним единым Шом мы не должны допустить ситуации когда один из контроллеров что-то вытеснил другой не вытеснил и страница находится в разном состоянии Ну и второе что на самом деле вытекает из первого это то что мы хотим проводить безопасный фловер после потери одного из контроллеров после того как один из контроллеров у нас допустим сшил на нём возник какая-то проблема И когда оставшийся контроллер понимает что он теперь один он просто Берт шип над всеми партиции говорит что он теперь здесь один-единственный дописывает все ти страницы в эн И после этого плавно выключает кэш и мы с выключенным кшм ждём Когда у нас восстановится избыточность Ну и небольшой дисклеймер Раз уж мы затронули темы и в деталях оста не будем не будем рассматривать как они работают там очень много специфики нашей конкретной платформы наверное Единственное что хочется запомнить Оке это то что чтобы нам не терять данные нам нужно поддерживать согласованность информации о состоянии страниц Ну и Давайте посмотрим как мы к этому какой план план у нас был следующий мы дадим каждому АО запросу на й какой-то идентификатор эти идентификаторы мы мы возьмём какой-то будем сохранять какой-то максимум этого идентификатора в страницах на активных партиции после чего после записи на backend отправляем максимальный идентификатор на другую сторону на соседнем контроллере просто сравниваем идентификатор с идентификатором в пассивной парти если он больше или равен значит всё круто данные уже в бэнде можно выходить из состояния переходить в Лин Ну Казалось бы что могло пойти не так план надёжный как швейцарские часы первое что нам здесь нужно сделать это понять как мы будем в принципе идентифицировать рай запроса но здесь ничего сложного нет мы ввели просто монотонно возрастающий счётчик в пределах каждого томат и наверное единственная особенность которая у нас здесь есть это то что мы на самом деле будем работать не с одним набором идентификаторов А с двумя у нас будет идентификатор для локального IO и для ремоут то есть для того которое пришло конкретно к нам на наш storage процессор и для того АО которое к нам было отмиранием эти идентификаторы Generation ID или просто Гены как они здесь называются и дальше здесь на слайде то что будет нарисовано слева мы будем считать для простоты что это активная сторона а ВС что нарисовано справа это у нас будет пассивная Рона давайте рассмотрим на примере Как это работает Значит у нас есть некоторая страница в активной партиции она обновляется й запросами локальными ремоут мы добавляем к ней два новых атрибута максимальный локальный Ген максимальный рейген после того как мы выс страницу Она записывается переходит состояние Син из энда приходит подтверждение о том что все данные были у нас записаны После этого мы эти максимумы упаковываем в синхронизацией транспорт на принимающей стороне на пассивной мы просто сравниваем максимальный локальный Агент с максимальным ремом максималь рены с максимальным локальным то есть крест накрест их сравниваем и если и тот иго равен переводим страницу в состояние кн ну здесь на самом деле пример на слайде довольно вырожденный тут несложно заметить что И те и другие Гены Просто крест накрест равны здесь мы можем просто сразу сказать что данные на сп0 на С1 и в бэнде у нас просто-напросто совпадают проблемы Ну довольно быстро когда мы начали всё это тестировать у себя в лаборатории сразу же стало понятно что на ничего не работает нас очень много проблем Давайте вообще немножко отвлечённым как вообще проблемы в схд в кэше могут проявляться как мы Тестируем собственно говоря схд Ну для тестирования схд мы э применяем в основном две утилиты это утилита fio Flexible IO Tester утилита действительно очень гибкая Она позволяет подавать IO с разной глубиной очереди с разным соотношением рядов и райтов с разным паттерном Ну и самое главное что позволяет делать fio оно позволяет проводить верификацию записанных данных Ну также разными способами и в разное время то есть F может проверить что то что оно записала тот паттерн который оказался на дисках это действительно тот паттерн который Ну и то что оно почиталось это ули которая на самом деле скорее больше бенчмарк чем утилита для тестирования схд Но на самом деле во время нагрузочных испытаний во время тестирования перформанса Мы выявили тоже довольно большое количество багов поэтому можно сказать что этой утилитой Мы тоже часто пользуемся Какие могут быть проблемы первая проблема которую мы можем встретить это если у нас Мы запустили fio на инициали или там на несколько томов и у нас всё зависло такое может быть Например если у нас есть какие-то проблемы в синхронизации состояний страниц допустим на пассивных партиции страницы перестали выходить из состояния ДТИ после чего очереди закончились потом закончились очереди в сказе потом закончилась глубина очереди на инициаторы и всё зависло вот здесь пример запустили 19% записали а потом ета в космос летает там апсы перестают показываться В общем всё здесь плохо вторая проблема которую мы можем встретить это если записанные данные у нас не проходят верификацию вот здесь тоже хороший пример fio записала некоторый паттерн ожидала затем прочитать F8 вместо F8 читает 72 да то есть не то что оно записывалось кше валидные а в другом ничего нет Да и кэши для пользователя выглядят по-разному в зависимости от того на какой он попадёт и третья проблема которая может быть это если мы пытаемся что-то записать в схд а она берёт и сразу отвечает ошибкой да На чтение или запись Ну такое может быть Например если у нас проблемы в rdma мирроринг Ну и Давайте поговорим о собственно говоря проблеме проблем как я уже сказал было много но наверное самая показательная проблема это проблема сденм допустим у нас в активную партию пришло два запроса сначала В1 затем V2 эти запросы успешно отмерили на пассивную сторону полетели в кэш соседнего контроллера но затем у нас на активной стороне запрос V2 у него Ген очевидно больше чем у В1 он пришёл запрос обт запрос ситуация у нас система с высоким как бы параллелизмом по два сокета в каждом процессоре да На каждом сокете куча ядер всё работает параллельно и с точки зрения пользовательского приложения это разные й запросы которые не пересекаются между собой лба о наших Шох страницах оно тоже естественно ничего не знает поэтому такое Вполне может быть и допустим нам здесь очень сильно не повезло например нам лей размер кша и как только запрос V2 у нас попал в страницу мы начали эту страницу сразу же записывает Да мы её сразу же вытеснили в таком случае запрос V1 до страницы не добирается он втыкается в очередь и после того как мы заканчиваем запись на backend мы отправляем синхронизацией Ген будет локальный ремоут просто-напросто равен Да потому что у нас самый старший Запрос который видели эти страницы это запрос V2 тогда на пассивной стороне мы просто берм и страницу состояние ДТИ переводим в состояние Лин после чего на активной стороне страница выходит из синхронизации и сразу же возвращается в состояние ДТИ потому что выгреба из очереди запрос В1 запрос о с обеих контроллеров подтверждается наверх клиенту и какая Уде выходит ситуация о запроса наверх подтверждены приложени жение считает что и тот и другой и та и другая запись закончена данные записаны но на пассивной стороне страница находится в состоянии Клин Несмотря на то что данные в ней отличаются от бэнда на данный момент ситуация уже очень плохая потому что страницу в состоянии Клин в пассивной партиции мы имеем право в любой момент вытеснить уже из рикша и очистить в таком случае у появятся проблемы с когерентность но е хуже ситуация в самый неподходящий момент этот контроллер Ну активный контроллер у нас ещ и крашится контроллер в котором страниц находится в активной партиции что в таком случае произойдёт вторая страна у нас возьмёт шип над всеми партиции запишет д страницы в эн но эту страниц находится в состоянии Клин её записывать никто не будет со временем скорее раньше чем позже вытеснена уже из Рид кша очищена и любой последующий Рид который сюда придёт прочтёт старые данные то есть в таком сценарии получилось что клиент записал в нас некоторые данные А мы эти данные просто совсем потеряли Да это дата loss Ну естественно с дата лом ни в какой продакшн идти нельзя это ну большая катастрофа на самом деле поэтому пришлось сидеть и думать как всё это исправлять Давайте посмотрим на наш новый план теперь Мы решили всё радикально упростить мы не будем использовать никакой скользящий максимум мы вместо этого будем сохранять в атрибуты страницы в некоторое множество все Гены всех запросов которыми страница обновлялась потом просто заталкивает в rdma и На другой стороне уже принимать какое-то решение Да на основании этого множества Можем ли мы сейчас переходить в Клин или не можем Ну давайте опять же на примере посмотрим как это работает Значит у нас есть опять же страница она находится допустим в активной партиции она обновляется некоторыми й запросами локальными ремоут мы теперь добавляем как бы новую такую структуру данных множество генов мы его называем просто Ген сеет Значит так как теперь Гены всех запросов лежат в одном месте в перемешку Мы решили просто переиспользовать старший битик да поставить туда генг Таким образом мы можем отличать тные запросы от локальных Как это работает дальше страница вытесняется записывается на backend приходит подтверждение мы берём и весь нсе отправляем на пассивную сторону на пассивной стороне мы теперь должны произвести какую-то обработку входящих сообщений мы должны теперь принять решение Можем ли мы выходить из дте сейчас или нам нужно ждать какого-то события в будущем на самом деде довольно для этого использовать обычное исключающее или по сути обычный ксор да Значит нетрудно заметить что то что находится в множестве которое к нам пришло с активной стороны Да то что находится в синхронизации это по сути Гены того АО которое уже было записано в энд но которое по какой-то причине не долетела до пассивной стороны Да это довольно редкая ситуация но такое Вполне может быть опять же А если Гены есть только на страни в стороне это значит что это просто какое-то новое АО которое до бэнда ещё не долетела но которое уже есть в кэше соответственно и те и другие ээ Гены мы не можем удалить из этого множества А вот те Гены которые находятся в пересечении то есть которые и попали в кнд и есть в пассивной странице это Гены того АО Чья обработка по сути закончена то есть мы взяли эту запись закро донесли до Кэн и мы можем больше её не трекать и удалить эти Гены из э генсека пассивной стороны как это имплементировать сообщение мы первым делом его инвертирующего стороны Да всё локальное становится ремом всё ремоут нае локальным После этого мы элемент за элементом просто добавляем в добавляем Каждый элемент в нсе на пассивной строне и если находим совпадающую пару удаляем здесь опять же всё просто видно что после такой операции Ген сеет на пассивной стороне станет пуст Да и это единственное как бы условие при котором мы можем перейти в Клин если у нас какое какие-то Гены в генсеки остались бы мы либо ждём следующую синхронизацию которая к нам принесёт новые Гены Аё которая записывается в кнд либо Мы ждём что возьмём эти Гены из входящего трафика да то есть какое-то АО к нам долетит которое почему-то опаздывает Ну и Давайте плавно переходить к итогам посмотрим Э что у нас получилось в результате значит сначала проговори что здесь осталось за кадром за кадром на самом деле осталось очень много всего разного Ну наверное из того что стоит здесь упомянуть то что помимо записи на энд существуют ещё промежуточные синхронизации то есть очень часто а под файловыми системами под базами данных есть какие-то горячие области Да например если это файловая система то под журналами под метаданными есть какие-то горячие области которые постоянно висят в кэше редко записываются постоянно обновляются и чтобы не накопить огромных генс тов и не вычерпать себе всю память Мы после какого-то разумного трешхолд вводим промежуточную синхронизацию медленной записи на диске там нет перформанс никак не аффектив Гены текущего а отправляем на пассивную сторону Ну и мы здесь совсем не говорили про приключение с транспорта уровнем Да у нас есть агрегация сообщений перед передачей через rdma чтобы эффективно использовать бевис и делать всё Опять же очень быстро ну есть бондинг Трай дупликация в общем очень много чего про что можно рассказывать отдельные доклады Ну и посмотрим как мы как нашу проблему исправляет такой подход теперь видно что при синхронизации мы отправим на пассивную сторону только Ген V2 о V1 у нас страница ещё ничего не знает соответственно после того как мы применим нашу операцию исключающего или у нас на пассивной стороне Ген V1 останется а на активной стороне когда мы вытащим эту страницу из состояния синн в состояние Ате Да сложим из очереди Запрос к себе У нас в генс также будет запрос V1 то есть мы видим здесь что страницы стали полностью зер Т состояние у них зеркальные генс когерентность здесь ни в какой момент не теряется и в общем ВС стало хорошо Ну и Давайте подводить уже итоги значит что хотелось бы проговорить в кше каждого контроллера у нас всё время лежит полная копия данных которые в end ещё не попали благодаря нашему протоколу клиентское оборудование может и записывать И читать данные в любой Stage процессор то есть мы работаем в полноценном режиме приказах процессора у нас также благодаря нашему протоколу данные не теряются и не повреждаются Да мы можем Дописать за погибший контроллер все данные без каких-либо потерь и наверное основной смысл который хотелось бы зашить в этот доклад - это то что при дизайне и при разработке таких систем хорошо бы следовать принципу Kiss keep it Simple Stupid изначально наша мотивация делать наш наивный подход было предположение что лнем для нас может стать rdma что мы можем вычерпывать место в буфера Но на самом деле это оказалось не так оказалось что бевис у нас огромный его хватает С запасом и мы по сути на ровном месте выдумали сами себе проблем Поэтому лучше следовать принципу Кис и такого не делать Ну а у меня на этом вроде бы как получается Всё вот сюда можно по Q коду оставлять свой фидбек голосовать А я готов отвечать на ваши вопросы пото что это именно то что мотивирует спикеров потом делать ещё более крутые доклады А нас а нам позволяет делать более крутые конференции и доклад напомнил мне злую шутку о том что в программировании есть две сложные штуки Как называть идентификаторы как инвалиди кэш и ошибки на единицу здесь в комплекте получается полный комплект да полный комплект Ну что ж кто начнёт борьбу за супрематический матрёшкой мы будем дарить ещё и крутую айтиш ную книгу какую не скажу пусть это будет приятным сюрпризом бумажная книжка которую можно почитать утром за кофе или вечером перед сном А пока вопросов из зала нет значит вон там есть а ой приступим Да Добрый день Михаил Вопрос такой вы мельком коснулись темы как раз вымывания кэша Да И вот эти вот ваши подобранные временные тайминги Когда происходит промежуточная синхронизация насколько они ну экспериментальным путём эффективно были выбраны чтобы не сильно снизить Как говорится производительность хд в моменты вот самой активного вымывания кэша да Это хороший вопрос на самом деле у нас есть целая отдельная команда которая этим занималась У нас есть перформанс инженеры которые когда мы вводим какие-то временные параметры какие-то трешхолд там в общем любые такие вещи которые можно тюнить эти перформанс инженеры запускают типовую нагрузку которую подают клиенты и Какие параметры афект собственно говоря выбирают оптимальное считают там точки насыщения смотрят как кэш вымывается в общем да мы это исследовали короткий ответ Спасибо за доклад скажи вот случае записи она идт для примера два разных контроллера и области Бут Юся как проб румом по насколько я помню по спецификациям сказ в таком случае мы просто должны вести себя как-то консистентность мы должны либо один результат либо другой предъявить и мы этому соответству Но вообще если вы в диски пишете по пересекающимся областям то есть Для клиента это как будто один ресурс Вот то вы соответственно можете получить пересечение либо как бы один сверху будет либо другой то есть но это работает Да привет Спасибо за доклад такой вопрос а вот допустим есть два сервиса верно которые Ну вот синхронизируются на первый запрос на запись поступило А на второй запрос на чтение в это время но синхронизации ещё между ними не произошло вот как в этом случае консистентность достигается А значит когда страница синхронизируется она блокируется на запись и значит в не записи который неё придут они не будут подтверждены То есть клиент будет считать что оно всё ещё записывается и соответственно странно читать то что ещё не записалось Вот Ну а то что не пересекается и пролетает на Рид оно пролетает и обслуживается да то есть почитать можно будет в этот момент я имею в виду вот оно записывается но оно записывается на одном сервисе а синхронизация ещё на второй не ушла это всё входит это это входит в как бы в состояние Син то есть оно пока оно пролетает туда-сюда Пома это будет синхрони подробности можно будет обсудить В кулуарах Да ты совершенно правильно делаешь Что оставляешь по метке потому что тяжело запомнить и выбрать лучший вопрос я помню ага в правой части зала был вопрос прошу Здравствуйте спасибо Я хотел уточнить Спасибо что были примеры но на самом деле не до конца понятно как бы сам кес я правильно понимаю что речь идт условно об облаке и у Вано совершенно разных юзеров у которых разная нагрузка и так далее на самом деле нет речь идёт о конкретных схд То есть это аппаратная как бы это аппаратная история есть система хранения данных это в шассе по сути вот два этих контроллера которые я здесь рисовал это по сути два компьютера которые стоят в одном шассе в него проводами через коммутаторы включаются серверы они с ним работают а основной кес - это собственно говоря поток А который оттуда идёт это чтение запись вот это уровень блочного устройства да то есть мы работаем с мы предоставляем клиенту на клиентском оборудовании блочное устройство и он может делать с ним всё что угодно он может отформатировать там файловую систему сман её и складывать туда файлы он может там например подключить туда гипервизор и и вот datastore туда затолкать И там будут храниться диски виртуальных машин Да vfs вот это всё в общем вот такой кейс понял спасибо А тогда Ну кажется тогда понятно почему их всего два потому что больше пока просто не требовалось да Ну да да это по сути позволяет реализовать это минимальный минимальное количество контроллеров которое позволяет реализовать Active и при этом иметь как бы избыточность Добрый день Спасибо хотел уточнить пенальти от использования синхронизации Шей на записи на стене отдельно да пенальти конечно определённое есть но оно на самом деле не очень большое тоже как отдельно наша команда Подсчитала вся эта синхронизация на трип обычно занимает вот сейчас боюсь в конкретных цифрах соврать потому что может быть по-разному можно в относительных это было Ну относительно это сотни микросекунд То есть это очень быстро и синхронизация происходит не каждый раз то есть есть просто некоторая вероятность что ваше iio оно попадёт на эту синхронизацию но в целом в обычном случае запрос просто от мирится и сложится в кэш это будет всё правильно понял что в относительных цифрах Получается примерно X2 если у вас новы мешки под вами и там они дают 100 микросекунд сами по себе свой дом да то это получается X2 от сырой произвольности да да но там с nvme отдельная история а на самом деле это может быть X2 но может быть и не быть зависит от паттерна котором как бы вы пишете тут затраты тут на самом деле основной Я про пенальти основное пенальти не Сколько от синхронизации синхронизация с нсе сколько пенальти у нас идёт от мирроринг потому что самая длинная операция вот в этом дапа это копирование целиком запроса особенно если к нам пришёл большой блок Например если включен page Cash на той стороне P выдав это пролазит через сзи И после этого копирования его по rma может занять некоторое время Спасибо за доклад вопрос может ли случиться спт между контроллерами и если может то какой план по борьбе со Сплит Брейном Да спт сптб конечно может случи знат инет между контроллерами это по сути опять же для redundancy два провода rdma там infin Band которые мо могут дать сбой Да их можно вытащить например мы так Сплит Brain Тестируем А значит в случае Сплит Брейна Ну мы в докладе я об этом не говорил в случае в случае Сплит Брейна мы переходим в специальный режим сначала стол замораживается всё АО затем специальные микросервисы которые у нас там есть э разруливает Кто станет мастером мастер выбирается пробиваем тот кто стал словом И после этого Работаем как бы как будто у нас обычный Следующий вопрос слева прошу Вадим Подольный аквариус руководитель разработки сходы аквариус не вижу откуда хочу вот отсюда а слева Хочу задать наверное не технический вопрос потому что ну смысла нет как бы мы все понимаем чтото внутри мы вот сейчас по сути есть несколько команд которые занимаются глубоко нормальным эм Да их не так много фактически можно по пальцам пересчитать если посмотреть и посчитать общее количество людей я думаю что найдёт человек 100 который в состоянии вообще что-нибудь подобное писать если мы посмотрим на какую-нибудь Дора это тысячи человек это 5 лет работы коллеги из Huawei это просто Фантастическая история вот у меня вопро мы вот когда из Як выйдем Да вот Как вы считаете когда мы сможем делать мультиконтроллер истории там на 16 на 32 контроллера то что-то такое И на самом деле это только первая часть вопроса есть ещё вторая часть вопроса Что на самом деле Вот то куда вообще идут современные СД низкоуровневые да то есть близко к железу вот э сэто чтобы можно было строить даже не 32 контроллера а 332 Вот первый вопрос Да и как и что нужно Как вы считаете Какие усилия для того чтобы перейти на новый фактически на новую механику я понял вопрос значит я к сожалению довольно ограничен различными соглашениями Чтобы на него Хорошо ответь У нас есть кулуары где ограничений будет чуть-чуть поменьше у нас довольно большое R подразделение Мы работаем над многими хорошими вещами которые будут в будущем и возможно через какое-то время в будущем будет и метрокредит и может быть оно будет а может нет я об этом всём говорить в общем-то не могу вот поэтому но интересных активностей много бы у на R идт В общем ВС хоро стороны я хотел бы добавить как человек который пишет код с девяностых и много видел что на первый взгляд действительно кажется что конкурировать с компанией у которой тысячи разработчиков довольно тяжело и есть например мнение что Сейчас никто не может создать ещё один браузер просто потому что объём работы который требуется для того чтобы имплементировать все спецификации поддержку всего Legacy исторически наложив отложений он чудовище Но это всё если нам нужно поддерживать leg си и в огромных компаниях все эти тысячи людей Они во многом не пилят новые фичи а обеспечивают поддержку совместимости с какими-то старыми фичами которые 30 лет назад сделали Для какого-то клиента и от которых нельзя отказаться Так что в этом плане если мы делаем что-то новое с учётом предыдущего опыта то есть нам не надо 10 лет делать r& Мы можем посмотреть уже как мы как кто-то другой сделал и нам не нужно поддерживать там какие-то пятидесятилетний с боингом то даже командой в несколько десятков человек можно сделать какую-то конкурентоспособную систему и для новых решений конкурировать с монстрами потому что к сожалению ещё каждый следующий человек добавляемые в команду он или она не всегда ускоряет процесс А часто замедляет мы не можем просто увеличить команду в 1.000 разработчиков до 2.000 и выкатывать в два раза больше фичей если нам не повезёт мы будем выкатывать меньше фичей как бы контри это не звучало Давайте завершающий вопрос с правой стороны за зала Да спасибо за доклад У меня вопрос простой вот звучали хорошие идеи Да вот есть ли какая-то пробация на практике этих идей Да может быч марки есть уже да что вот ваш контроллер Да там с кэширования там почтение даёт или может быть даже вы дальше пошли Да и уже про бенчи приклад который работает над вашим контролером Ну там постгрес например или ходу Ну да я я на самом деле частично отвечал на этот вопрос Вот когда на первый вопрос отвечал а опять же мы регулярно Ну перформанс в системе хранения данных он имеет как бы Краеугольный количество бенчмарком измерений в том числе Э бенчмарки не просто самого АО не просто самой схд а бенчмарке решений которые работают поверх её ресурсов да то есть мы берём там разворачиваем виртуальные машины в них делаем бенчмарки Там сверху наворачиваются Так что да мы всё про бенли цифры Обсудим в кулуарах аплодисменты спикеру и я попрошу э подготовить все наши фрезы А тебя попрошу выбрать того кто задал лучший вопрос Ну мне на самом деле понравился вопрос про Рид во время синхронизации довольно Да тоже много с этим работали под Поднимайся к нам на сцену супрематическая матрёшка гостю конференции который задал лучший вопрос и и книжка Так что это у нас открывая организации будущего это исследование и оно Довольно интересно спасибо тебе и нашему спикеру тоже спасибо специальный подарок и даже вот мы допечатной продолжаем печат"
}