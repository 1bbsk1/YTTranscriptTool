{
  "video_id": "XtfLrdfLOH0",
  "channel": "HighLoadChannel",
  "title": "Эволюция архитектуры транскодера / Дмитрий Лукшто (VK, Дзен)",
  "views": 228,
  "duration": 2210,
  "published": "2024-10-29T03:07:37-07:00",
  "text": "Привет Привет Меня зовут Дима лук Я старший разработчик в дзене и я занимаюсь видеохостингом Сегодня я вам расскажу как мы прокачивать свой транскодер как он эволюционировал как менялась его архитектура Ну давайте сначала разберёмся что такое транскодер что такое транскодирование транскодирование - это процесс преобразования данных из одного формата в другой В общем случае чаще всего это слово используют в контексте видеоданных вот и это подразумевает под собой изменения разрешения видео изменения битрейта изменение собственно размера видео изменение кодека и так далее А у нас транскодер выполняет э чуть больше функций аэ то есть нам приходит какой-то исходник от пользователя исходник видео А мы разные разрешения которые потом проигрываются в нашем плеере То есть это 360p 720p и так далее пере упаковываем видео меняем ему кодек запускаем всякие классификаторы чтобы подобрать наилучшие параметры транскодирование генерируем Сабы превьюшки тамбнейлы считаем аудио аудио и видео сигнатуры для дупликации как это вообще работает внутри на самом деле очень просто пользователь нам загружает исходник в каком-то формате вообще без разницы в каком мы запускаем над этим ffm вот получаем пачку разных разрешений показываем в плеер Профит вот буквально имея FP и исходник Вы можете поднять свою раздачу видео но на самом деле не всё так просто наш пайплайн транскодирование транскодирование очень упрощенно выглядит вот так вот то есть сначала Понка которая трансет видос качает исходник потом запускает над этим всем анализ то есть мы пытаемся узнать что вообще к нам пришло какой у этого Размер какой у этого битрейт какой у этого кодек потом мы запускаем над этим всем эльки Чтобы подобрать наилучшие параметры чтобы это потом всё классно смотрелось в плеере и занимало как можно меньше времени потом генерируем МБ собственно потом самый затратный по цпу процесс - это конвертация пом проце потом из того что получилось Мы считаем сигнатуры И после этого уже загружаем результат в хранил Вот почему всё что я перечислил до этого работает плохо здесь мы можем видеть утилизацию цпу в процессе транскодирование одного видоса то есть на старте у нас пу в полку потом где-то в ране там 20-4 и потом опять в полку так как это довольно длительный процесс то в разрезе транскодирование какого-то конкретного видео у нас отвратительная утилизация цпу была вот и соответственно наше железо утилизировать очень плохо и скорость обработки видосов могла быть X2 и X12 от длительности то есть ну для длинных видосов Они могли кодироваться действительно беконе что с этим делать Ну очевидный вариант это как-то попытаться распараллелить процесс чтобы ускорить весь пайплайн и как-то подобрать железо таким образом чтобы где надо его было мало для каких-то тяжёлых задач Когда у нас вот мы видели на графике что оно в полку было дофига Ну давайте посмотрим никак не параллели а четвёртый пятый шестой Мы можем как-то разнести по разным машинкам и потом снова строго последовательная обработка вот что с этим делать А собственно есть два варианта мы попробовали оба а Первый вариант - это мы для каждого конкретного разрешения запускаем под задачку на отдельной машинке Вот и потом загружаем результат быстрее и второй вариант - это мы бьём видео на чанки то есть ну там секунд по пять допустим и уже на каждой машинке транско все разрешения для какого-то небольшого кусочка видео потом мы это всё на опять же на какой-то одной машинке склеиваем и загружаем результат Ну давайте посмотрим на плюсы минусы обоих вариантов параллельно транскодирование разных разрешений Из плюсов нам даёт это предсказуемо число задачек Ну то есть для каждого видоса у нас будет там ну условно какой-то набор равный количеству разрешений которые мы поддерживаем в плеере предсказуемая загрузка кластера потому что ну мы знаем Какое количество задач у нас породит каждая задача вот и результат под задачек он независим то есть нам не нужно ничего Потом склеивать вот Ниго опыт процессинга нам не требуется Из минусов если у нас есть в кластере свободное железо мы его никак не утилизируем допустим нам прилетел какой-то супер длинный видос А мы сделали условно пять Под задачек и всё всё остальное цпу в кластере простаивает вот ну и соответственно для длинных видосов у нас скорость обработки будет также очень большой А Обработка по чанкам тут всё проще мы можем утилизировать примерно всю цпу которая у нас есть в кластере для одного видоса То есть если допустим у нас свободный кластер к нам влетает какой-то большой видос мы бьём его на чанки и чанки занимают весь кластер Но это также и минус потому что мы абсолютно не умеем предсказывать во-первых Сколько по времени займёт транскодирование каждого чанка И сколько их в итоге будет И как у нас будет загружен кластер вот ещ это нужно ВС будет в конце склеить каким-то образом вот итоговый пайплайн у нас вроди в что-то такое Сначала мы последовательно качаем исходник анализируем запускаем меку потом на отдельном кластере конвертируем остановились на втором варианте где каждая машинка качает свой кусочек исходника конвертирует его пока у нас бегут по задачки по конвертации чанков мы генерируем тумбы в процесс аудио в том когда чанки сконвертируйте бы пока ВС просто но на самом деле таких воркеров У нас очень много То есть это какие-то тысячи воркеров этим всем занимаются к нам каждый день льются сотни тысяч видео в суммарной длительностью просто в сся часов кластера у нас это примерно десятки тысяч цпу и вс это цпу нужно как-то эктив загружать для оптимальных параметров транскодирование у нас тоже жрёт довольно много цпу и тоже довольно требовательно ещё нам нужно генерировать ASR А и самое страшное У нас есть sla то есть мы не можем обрабатывать видео долго нам до появления МБО нужно уложиться в какие-то десятки секунд иначе у нас поломается для пользователей вот при этом При всём У нас есть как живые юзеры так и ба задачки которые нужно тоже считать давайте посмотрим вообще на то какие пользователи у нас есть то есть у нас условно есть три группы пользователей это новости их нужно обрабатывать просто как можно быстрее желательно в режиме реального времени в обычном порядке Мы обрабатываем обычных авторов То есть это все живые люди которые нам приходят фоновая работа Допустим мы подобрали новые классные параметры транскодирование и нам нужно перекодировать просто невероятно большое количество старых видосов вот и всё это должно работать на одном железе на одном кластере Вот и эффективно утилизировать пу при этом не нарушая до первого до появления МБО раньше мы использовали собственно квот для каждой руы была отдельная квота для новостей отдельная квота для живых юзеров отдельная квота для Реда Вот Но с этим есть проблема наверное Она довольно очевидная если у нас одна квота перегружена то железо простаивает то есть мы нези которая находится в другой квоте здесь на самом деле можно посмотреть на нашу суточную утилизацию нужно держать в голове что у нас примерно всегда огромная очередь задач перекодирование и в целом эта полосочка должна быть ровной Вот но здесь мы видим пики то есть днём мы утилизируем почти всё наше железо которое у нас есть потому что к нам приходит дофига пользователей ночью поток падает и э остаётся небольшой фон ре кода вот очень э неэффективно ну как с этим бороться собственно Нам нужен какой-то более интеллектуальный способ загружать кластер нужно добавить планировщик причём аэ этот планировщик аэ должен уметь загружать кластер ничего не зная Ни о длительности задач Ни о количестве задач а то есть э должен действовать как-то по ситуации мы использовали алгоритм он же используется в дупе вот это как раз алгоритм который умеет эффективно загружать кластер с учётом приоритетов задач ничего не зная ни их длительности не об их количестве как раз то что нам и нужно Давайте попробуем понять Вообще как эта штука работает собственно чтобы понять как работает дава берём пять терминов и посмотрим на небольшое количество картинок А до этого мы смотрели на пользователей то есть News Zen Rec Ну вот у каждого из у каждой из группы пользователей есть как раз пять параметров первый - это вес а определяет Как много тасо мы возьмём для пользователя Share работает итеративности зада которое равно весула слева Вы можете видеть как раз состояние очередей для пола пользователей справа загруженность кластера это наш как раз транскодер так называется ну вот первая итерация для первого первую очеред мы взяли три задачки для второ какое-то свободное место в кластере и задачки в очередях Ну собственно повторим А вот мы как раз загрузили весь кластер и достигли какого-то честного состояния которое мы как раз считаем что а каждому лу досталась честная доля вот это как раз Fair Share но интересно начинается дальше А у нас есть ещё такие понятия как Min Share и Fair Share Ну то есть мы для каждого пола можем определить какую-то минимальную долю меньше которую мы ему дать не можем и честную долю честную долю это вот как раз то что мы посчитали на предыдущем слайде Что происходит если у нас кластер не соответствует той честной доли которую мы хотим То есть у нас есть какое-то представление о том Каким должен быть кластер какой-то Вот Но сейчас у нас мы взяли ещё две задачки из и в нам приехало просто невероятное количество задач А мы хотим чтобы у нас картинка была как на предыдущем слайде вот что с этим делать здесь вступает в силу такой механизм как вытеснение то есть мым и ко прием две задачки которые не соответствуют нашей идеальной картинке мира возвращаем их в очередь и берём задачки из того пола из которого мы хотим Да мы теряем какое-то количество цпу работы для этого мы стараемся выбирать самые новые задачки чтобы как можно меньше пуста получить Ире какое-то врем повтор вот эти вот два параллельных процесса вытесняя задачки которые нам не нравятся и примерно накиды задачки которые нам нравятся у нас кластер сойдётся к тому состоянию которое мы хотим видеть да При этом мы потеряем какое-то количество пу работа Вот Но в среднем это даёт очень хорошие результаты мы сейчас видим что здесь есть какой-то сте и какая очеред Дава жим считать миллионером ту-ту-ту А на чём эта очередь сделано на редисе на ребит на кафке на покре на покре э чуть попозже я объясню почему А давайте сейчас посмотрим в принципе на то как это выглядит у нас архитектурно То есть у нас есть какая-то опиш куда нам присылают задачки есть какая-то очередь это не та же самая очередь про которую мы говорили до этого это просто такой амортизационный буфер перед планировщиком чтобы его не закидало Задачка есть как раз пог который реализует на очередь и есть два кластера кластер ри воркеров и суб таковых воркеров которые шлют как раз харби планировщик и через харби получает задачки и планировщик общатся после как задачки сделаны кидают и дальше этот кто-то ловит вот довольно простая схем Как выглядит у нас FL в квадратиках стрелочка Давайте разберёмся То есть первый процесс это у нас посчитать состояние которое мы вообще хотим видеть то есть как раз реализация поднимаем из неё состояние очереди нам обязательно нужно знать сколько задач в каждой очереди для какого пола дальше мы поднимаем Лера то есть Лера это то сколько тасо и где запущено из этого мы формируем какое-то отображение кластера то есть загруженность кластера и состояние очередей для каждого пола на основе всего этого мы считаем для каждого пола получаем какой-то des и находим задачки которые мы хотим прибить вот после этого у нас собственно есть два списка Ну стоит это задачи которые должны вернуться в очередь и задачи которые мы хотим отдать воркера второй процесс - это применить этот это уже собственно ответственность планировщика нам прилетает jpc Hard Bit он попадает в планировщик у планировщика есть des State который мы посчитали до этого у него есть дуле сто Ну то есть это состояние кластера которое сейчас актуальное вот после этого он идёт в очередь а и берёт тасо которую он хочет через этот хабит отдать керо Ну то есть два Фло всё очень просто а можно собственно Спросить зачем нам своя очередь но это на самом деле важный вопрос на который хочется ответить нам для реализации алгоритма обязательно знать какое количество задач для каждого ла пользователя У нас есть то есть Для этого нам нужно прочитать всю очеред Малое количество очередь без того чтобы как-то вытащить оттуда задачки Вот кроме этого нам нужно периодически возвращать задачи обратно в очередь при этом не теряя их порядок то есть отношение порядка у нас строится как раз не на том когда Задачка попала в эту самую очередь А на том когда она была в этой очереди шение порядка как раз на тайм смх А ещё для подзадачи нам нужно довольно сложные группировки а потому что если вдруг у нас упала основная задача нам нужно короче прибить все подзадачи которые у нас есть вот а а ещё мы сделали лидерство на на Айри лока Ну чтобы два раза не вставать не самая лучшая реализация но в целом довольно быстро и работа так как это всё на самом деле выглядит если посмотреть чуть поближе У нас есть какая-то очередь ну она у нас иха поэтому реализует интерфейс SQ мы ВС эту очередь вычитывает это всё Demon Share который подсчитывает как раз des также он смотрит на табличку Running tasks он считает какой-то Desire State передаёт его планировщик планировщик получает Desire смотрит в R tasks и пота передаёт всё это воркера на самом деле все задачки вре у нас лежат в одной табличке керо у нас как вы помните какое-то количество тысяч и каждый раз прилетая вызывает транзакцию как раз в нашей табличке здесь у нас показана схем Что у нас там есть там ещё есть щепотка индексов чтобы это всё быстро работало Вот но Каким образом это вообще работает Почему это такая простая штука Как а updates Skip Locked for updates Skip Locked которая позволяя для запроса пропустить а строчки которые уже кем-то залочен таким образом мы ходим довольно большим количеством воркеров в одну табличку и а не попадаем ни на какой Лог конвой э и всё Это довольно быстро и приятно работает у нас нет никакого разделения очередей по табличкам или ещё как-то логически у нас на каждой очереди есть просто признак ла собственно у каждой задачки есть и есть табличка с полами где описаны как раз параметры для каждого Кост в а квод на таски квод на суб таски в основном они нужны для того чтобы как-то тушить пожары Аа Если кто-то хорошо знаком с погрей то наверное логичный вопрос бы был Почему не использовать н ну то есть очевидно что у нас что-то похожее на теги вот э и Казалось бы можно было бы вместо B3 индексов использовать просто gin но к сожалению gin не умеет строить индексы по orderby вот при А ну и нам довольно сильно повезло потому что у нас комбинации тегов не очень много вот то есть у нас на самом деле пул пользователей это какая-то константная комбинация тегов которая Согласно бизнесу Ну не очень часто расширяется Вот Но если вдруг очень надо иметь теги и при этом похожую очередь то можно использовать бит Mask например так делает аба То есть у неё есть поле с каким-то бинарным представлением индексов и целая большая библиотека для работы с этим в поре Ну и собственно итоги у нас один инстанс планировщика тащит несколько тысяч воркеров вот при этом у него довольно большой запас по ка сказать по этому поводу мы написали его на Гош не особо запаривать за оптимизацию и хороший выбор ран тайма как раз позволяет нам а поддерживать одним инстам довольно большую инсталляцию вот очередь на постгрес держит на несколько порядок тасков больше чем нам надо на самом деле это можно говорить что каких-то там десятков миллионов но мы так и не дошли до того момента когда она ломается а утилизация кластера под цпу у нас выросла с 30 до 70% видео стали обрабатываться на 30-50 про быстрее и Fair Share у нас внезапно а кажется актуальным во многих частях бизнеса сейчас мы думаем о том чтобы выделить его как инфраструктурное решение и отдавать другим командам Спасибо за внимание Спасибо огромное Ну что А друзья Ели у нас вопросы Я вижу руки Вот давайте вот начнём отсюда пожалуйста Спасибо за доклад было очень интересно вот вы показывали график где зависимость загрузки была ну и вы объяснили что она зависит от того что вот днём большая нагрузка ночью небольшая и вот Не совсем понятно как вот этот ваш планировщик помогает именно для вот борьбы с вот этими вот временными эффектами Объясните пожалуйста здесь на самом деле очень просто у нас есть огромное количество задач которые мы генерируем сами то есть это задачи на перекодирование вот когда у нас были квоты все эти задачи они приставали то есть вот эта вот ночная нагрузка которая была Она была такой маленькой просто потому что у нас очередь на Ред разгребать очень медленно и не утилизировали кластер сейчас ночью у нас Fair Share просто отдаёт большую часть кластера рекорду Вот и когда туда прилетают какие-то живые пользователи он либо выбрасывает этот рекорд вот ну либо размещает свободные слоты если такие есть Спасибо огромное так Вот там ещё была рука я видела а отлично хорошо тогда можно пожалуйста вот сюда микрофон Спасибо за доклад а можно поинтересоваться речь про транскодер и при этом мы меряем цпу Почему в докладе ни разу не прозвучал ГПУ Ну на самом деле мы ВС считаем на цпу так вышло дорогое кластера мы только учимся строить Спасибо огромное рядом пожалуйста Спасибо за доклад Э по поводу чанков возник вопрос У важен наверняка стриминг через hls Dash и он тоже происходит чанка не было ли такой гипотезы что оставлять его чанка насколько я знаю например Cult and jins она в Map modde может даже типа сделать собрать из кучи чанков как будто это один плейлист Да это как раз на следующий Таргет То есть у нас для вода как раз Туран вот ну Собственно как и много где поэтому скорее всего в будущем Мы склеивать перестанем благодарю пожалуйста Дим Спасибо большое за доклад вопрос скорее на правах набросали есть инструменты чтобы управлять приоритетами процессов Можно ли было бы решить задачу просто низко приоритетными процессами рядом с высоко приоритетными процессами и таким образом гладить нагрузку на самом деле кажется что нет потому что не очень понятно как бы вела себя система при этом в принципе довольно работает с потоками как мне кажется вот поэтому типа предсказать как с пати штука будет себя вести как-то её ограничивать как-то Задавать ей приоритеты Ну короче непонятно вот возможно можно было бы но никто не тестил Добрый день спасибо за доклад У меня вопрос больше про обработку видео вы уже сказали о том что F по большему счёту однопоточный И вам нужно много профилей на выход сам предлагает решать эту проблему с помощью запуска нескольких процессов на каждый профиль один аутпут но в таком случае возникает другая проблема что у нас невалидна Ар Ну адаптив получается что они не выровнены между собой вот как вы решали эту проблему и вытекающие из этого Ели у вас чтото для мониторинга мы там надирова выв там БК скринов пикселизация вот что для этого может какие-то там в мав или п СНР у вас используется Ну а к сожалению я не могу хорошо ответить на этот вопрос потому что с самим фпм Я почти не работал То есть я пилил инфраструктуру которая вокруг него вот э у нас именно всё что связано с доменной областью пилят немножко другие ребята Вопрос хороший Спасибо огромное пожалуйста Следующий вопрос и вот потом у нас там точно есть два ещ Да привет Спасибо тебе большое за доклад вопрос следующего плана когда мы нарезаем видео на чанке то при раздаче на воркеры а потом обратно когда мы собираем всё в одно место у нас есть охд по передаче данных по сети решали вы както проблему может быть клиенты сразу нарезают на своей стороне отдают воркеры напрямую чтобы э переда какое-то p2p в этом месте но сейчас у нас довольно неплохо справляется просто S3 куда мы всё сваливаем и потом оттуда достаём то есть это одна из будущих оптимизаций которую мы когда-нибудь сделаем Я надеюсь супер Спасибо огромное прошу А добрый день спасибо за доклад вопрос Следующий Ну собственно В продолжение предыдущего вопроса каким образом вы подбирали оптимальные вот размер чанка то есть и варьируется ли он в зависимости от самого видео то есть очевидно что 8к видео там 120 кадров там 60 кадров это 5 секунд этого видео значительно там дольше будет раскодироваться чем там не знаю 360 пи или там любое другое насколько я помню Сейчас у нас размер чанка стандартный подбирали Мы точно опытным путём просто очень много тестов сделали А вот для разного формата у вас Ну хотя бы там допустим хардко какого-то нет для там допустим 8к там не знаю там 2 секунды условно а там для HD там 5 секунд вот в той часть кода я не заглядывал про к тому что очевидно что чанки они будут разного Ну в смысле по времени разного размера там и возможно по CPU тоже То есть тут как бы я далёк от пега но предполагаю что 8к как бы и CPU больше загружает наверно мы можем продолжить на самом деле делиться опытом каким-то уже в дискуссионной зоне потому что не на всё просто так ответить можно прямо здесь и сейчас я видела там ещё Руку по-моему вот сзади Да дада да вот пожалуйста прошу Спасибо за доклад хотел спросить именно про сам планировщик вот я видел что выбрали подс насколько вообще эффективно его использовать именно как очередь много раз слышал что в формате именно работы очереди он быстро деградирует и почему там не кавка или что-то ещё использовали под это А ну у нас довольно специфичные требования к очереди как я уже говорил то есть нам надо уметь читать всю очередь нам надо уметь возвращать задачки туда без каких-то без потери порядка вот при этом на постс мы это смогли довольно быстро сделать и в принципе у нас Она в очень слабой инсталляции держит очень много воркеров и очень много задачек то есть в принципе все наши потребности закрывает вот поэтому М не знаю мне кажется кавка в нашем случае это просто какой-то окил то есть мы там никакие ти никакой именно большой объём трафика не Прокачиваем никакого Большого Пса там нет Вот поэтому непонятно зачем там кавка которую ещё очень сложно поддерживать Спасибо огромное Ну что друзья есть ли у нас ещ какие-нибудь вопросы вижу руку пожалуйста Добрый день спасибо за доклад у меня такой вопрос а Наверное первые идеи были какие-то попроще например там динамическая квота что вот мы смотрим загруженность ну или там хотя бы по ночам устанавливаем побольше и там ВС Ну вот эти вот всплески они должны сгладить И более-менее вот вопрос Почему вы Вот выбрали не самое такое простое решение на первый взгляд на самом деле Сначала мы сделали квоты Вот и потом сразу искали что-то что будет плюс-минус закрывать все наши потребности вот посмотрели Как устроены другие планировщики То есть изучили экспертизу нашли хорошее решение у дупа которое идеально вписывается в наши потребности Ну и взяли как бы его так Ну что вижу вот здесь пожалуйста самое главное Сколько ресурсо потребляет кто именно Игрес и то приложени на гулаге Ну не уверен что я могу назвать точные фры но Давай скажу так ядрах живёт в трёх репликах А приложени на Ланке там что-то ну Меньше 20р меньше де на все три и меньше ДТИ на приложен А меньше де на одну на один инстанс поса и меньше 20 на один инстанс приложени сколько приложени так там же модель лидерства Так что каждый момент времени одна Ну и там фолловеров какое-то количество спасибо супер Спасибо огромное Ну что вроде больше рук нет в онлайне тоже пока Вопросов нет тогда у меня самый главный вопрос кому подарок будем дарить вот молодому человеку за отлич Спасибо огромное ребят Всем спасибо за классные вопросы на самом деле очень приятно когда второй есть у нас их три а у нас их три замечательно У нас ещё есть подарки от дзена вот а давайте э отметим отличный вопрос про Почему мы не используем jp не помню кто задал по по поднимите руку кто задал этот вопрос так можно пожалуйста подарок Да он с этой стороны мым больше подарков больше подарков Богу подарков так Окей подарок нашёл у героя супер мы справились следующий так были очень хорошие вопросы на которые я не смог ответить вот у молодого человека и вон там вон в конце супер ребят двойное Спасибо за вопро я такое просто обожаю так всё все подарки подарили ничего больше не осталось супер Замечательно спасибо Огромно Дим очень интересно подарок Дмитрию пожалуйста от онтика Ещё раз спасибо большое по аплодирую пожалуйста"
}