{
  "video_id": "sXfvJJ49cVY",
  "channel": "HighLoadChannel",
  "title": "Под капотом быстрого сплитования трафика для А/B-тестирования / Дмитрий Волков (Okko)",
  "views": 395,
  "duration": 2318,
  "published": "2024-10-29T02:38:32-07:00",
  "text": "Дмитрий Волков из компании ОККО встречаем Всем привет ребят Меня зовут Дима Волков я Работаю руководителем отдела мл разработки в компании ока ока это онлайн-кинотеатр крупнейший в России у нас более 15.000 единиц видео контента у нас несколько миллионов активных пользователей ежемесячно смотрят и создают нагрузку в сотни миллионов запросов в день на нашу инфраструктуру и для того чтобы побороть эту нагрузку У нас есть собственная сеть доставки контента А для того чтобы смотреть кино У нас есть собственная платформа для стриминга видео Но сегодня мы поговорим о том что такое а тесты и а платформы мы выкатываем множество фичей например новый дизайн но как понять что новый дизайн Ну как понять что новый дизайн действительно оказывает положительное влияние на наши ключевые метрики Ну например количество просмотров или посещение сайтов и первое что приходит в голову - это выкатить новый дизайн на всех пользователей и после этого посмотреть на наши целевые метрики но так делать не очень хорошо давайте ставим когда мы выкатываем новый дизайн и вместе с этим выходит новый тайтл Ну например Чебурашка и наши метрики резко взлетают вверх и как нам понять из-за чего это произошло из-за того что у нас новый классный дизайн или из-за того что вышел долгожданный тайтл для того чтобы не сталкиваться с подобными проблемами в в в индустрии принято проводить контролируемые онлайн эксперименты или проще говоря атест суть этого подхода заключается в том что мы делим весь наш трафик на две группы А и Б в группу А отправляем старый дизайн А в группу б новый и затем сравниваем эффект количество экспериментов вока со временем стало большим и мы задумались о том что нам нужно разрабатывать свою собственную а платформу а платформа может состоять из разного количества компонентов но самый высоко нагруженный из них сервис сптоваров об атрибутах пользователя и об атрибутах устройства с которой пользователь заходит на наш сайт эта информация позволяет нам запускать наши эксперименты исключительно на целевую аудиторию процесс выделения целевой аудитории называется сегментация сегменты строятся из атрибутов и типичный пример сегмента - это пользователи с подпиской или взрослые пользователи с подпиской или даже взрослые пользователи с подпиской которые заходят на сайт с устройств на платформе Android или IOS каждый эксперимент проводится строго над одним сегментом пользователей и таких экспериментов может быть одновременно запущено сотни и вот мы дошли до самого главного экспериментальной группы нам нужно разбить весь наш а сегмент весь трафик в каком-то сегменте разбить на группу А и на группу б например в пропорциях 30 на 70 как же нам это сделать Ну классический подход - это взять хэш от идентификатора пользователя по модулю 100 и всё что меньше тридцати отправить группу а а всё что свыше группу Б В итоге наш Новый сервис должен работать примерно по следующему алгоритму пользователь отправляет нам запрос мы Для этого пользователя вычисляем сегменты в которые он попал затем для этих сегментов получаем набор экспериментов и дальше определяем В какие группы в каждом эксперименте в который попал пользователь попадают хорошо с алгоритмом мы определили Давайте посмотрим Давайте посмотрим Из каких компонентов может состоять наш сервис Ну первое что нам кажется надо добавить это воркер который будет непосредственно считать наши сплиты но также нам хочется иметь сервер с пробами для того чтобы мы могли запускаться в кубернетес нам хочется иметь сервер с метриками который будет Отдавать метрики чтобы мы могли мониторить наш сервис и Нам нужен воркер который будет отправлять статистику в кафку по экспериментам Хорошо теперь Осталось решить как интегрировать наш сервис в ландшафт онлайн кинотеатра для того чтобы наш сервис мог считать сплиты ему необходимо знать информацию информацию об пользователе пользовательских атрибутов к счастью у нас уже есть такой сервис который этой информацией владеет этот сервис называется менеджер в нашем ландшафте и к нему мы и подключимся требования требования к нашему сервису предъявлены достаточно высокие мы должны выдерживать нагрузку свыше 000 запросов в секунду и требования ко времени ответа у нас 10 миллисекунд всего лишь в практически 100% запросов Ну хорошо с требованиями мы определились осталось выбрать на чём будем писать сервис мы попробовали посчитать примеры каких-то сегментов их ишей на коленке на языке Python и нам показалось что на Python это считать довольно-таки быстро польку мы считаем что мы умеем готовить сервис на языке Python быстро и качественно то решили писать на нём Ну хорошо алгоритм понятен требования согласованы стек технологий выбран осталось дело за малым нам нужно написать наш сервис и мы его написали и он даже завёлся но график времени ответа показал нам страшные нижи в начали искать узкие места нашего сервиса и хороший способ найти узкие места - Это профилирование для профилирования мы использовали инструмент под названием Flame это классический семплер профайлер который позволяет понять какие части кода занимают Больше всего времени работы сервиса и первое На что мы обратили внимание - это валидация входящих запросов Гра говорит что она занимает порядка 8% времени работы сервиса Но наш сервис внутренний в него ходит другой такой же внутренний сервис контракты между ними зафиксированы и практически никогда не меняются поэтому совместимость сервисов достаточно проверять на стей окружении и мы решили убрать валидацию и это дало нам небольшой прирост мы сократили время ответа сервиса на 1 миллисекунду но этого всё равно мало И начали Смотреть дальше погра самая тяжёлая часть запроса - это расчёт сегментов она занимает порядка 68% времени работы сервиса и на это место точно стоит посмотреть Чуть более внимательно как мы уже видели ранее наш сегмент - это некое алгеброй выражение для работы с такими выражениями существует библиотека под названием json Logic она есть на разных языках программирования в том числе и на Python с её помощью можно представить наш сегмент в виде дна а ещ она может подставить значение переменных в выражения записанные на джейсоне и рассчитать этот сегмент и это очень удобно мы замерили время расчёта различного количества сегментов и поняли что нам нужно проводить некоторые оптимизации рассмотрим два сегмента классических и с какой-то момент времени к нам приходит прокт и Хочет завести новый эксперимент над новым сегментом пользователей и заводит новый сегмент и мы смотрим внимательно на этот сегмент и замечаем что отчасти он состоит из других компонентов уже существующих сегментов поэтому когда мы обрабатываем запрос пользователя нам не нужно повторно просчитывать компоненты каждого э сегмента который уже рассчитывался и мы добавили мемоизация и это дало нам небольшой прирост к производительности Но на самом деле этого оказалось мало потому что прирост ощущается только на большом количестве сегментов А у нас их не так много и мы стали думать дальше ленивые вычисления вот сегменты в основном состоят из логических операций и и или мы выяснили что наш библиотека на самом деле рассчитывает каждый компонент этих логических выражений Но это делать необязательно чтобы им из е компонентов был положительным и мы на этом решили сыграть мы добавили ленивое вычисление и по нашим ожиданиям должны сократить время расчёта в среднем до двух раз ленивые вычисления тоже нам дали прирост но не такой как мы ожидали и тоже кажется что прирост хороший небольшой прирост только на большом количестве сегментов и все предыдущие оптимизации в целом давали приро скорее на большом количестве и кажется что нам нужно поменять подход к расчёту этих сегментов и мы стали копать дальше если внимательно посмотреть на наш сегмент то можем заметить что он выглядит Как некое дерево конечное направленное во внутренних узлах которого находятся операторы А в листьях операнды это есть что иное как определение абстрактного синтаксического дерева это некое представление программного кода которое Затем компилируется в питонов ский байт код и запускается интерпретатором проще говоря вычисление J выражения слева эквивалентно вычислению функции на Python справа в итоге мы мы решили транслировать наш выраже наше выражение В питонов А И затем откопировать его и в результате мы получили более чем двукратное ускорение производительности ра по сравнению с базовым подходом также Мы заметили что посе связаны между собой например телефоны Apple работают только на операционной системе iOS А бывают взаимоисключающие сегменты Например если пользователь мужчина то он точно не попадёт в эксперимент рассчи женскую лею аудиторию им прила иде единое выражение которое за нелинейное время скажет нам в какие сегменты попадает пользователь Ну Вот рассмотрим два простых сегмента которые мы уже видели ранее У нас они состоят из двух атрибутов платформа и версия приложения платформа - это категориальный атрибут а версия приложения числовой и перед нами встала задача придумать структуру такую которая сможет учесть особенности поиска по числовым и категориальные по числовым атрибутам в листьях которого находится хэш таблицы ключами которого является различная комбинация категориальных атрибутов а значения список сегментов и такая структура позволила находить нам сегменты за нелинейное время и вот мы посмотрели что нам это даёт это даёт тоже неплохой бусте сегментов Но поскольку мы смогли построить это дерево в виде J выражения то мы смогли его откомпание оно оказалось оптимизировать расчёт сегментов мы получили очень значительный прирост сегмента и даже стали укладываться в необходимые 10 миллисекунд но нас как инженеров интересовал ещё один вопрос в питоне есть два узких места это жил и сборщик мусора и мы решили исследовать как они влияют на время работы нашего сервиса Ну начнём со сборки мусора как питон собирает мусор в питоне есть два алгоритма сборки мусора Первое - это почёт ссылок а второе сборка мусора по поколениям почёт ссылок работает в риал тайме Он очень простой и эффективный и он суть его в том что мы удаляем объект питон удаляет объекты на которые больше никто не ссылается но его слабое место - это циклические ссылки и на этот счёт есть сборка мусора по поколениям сборщик мусора по поколениям определяет все неиспользуемые объекты и освобождает занимаемую ими память и каждый запуск сборщика мусора создаёт микропауза в работе нашего кода потому что делает stop the world и чтобы оценить влияние сборщика мусора на наш ранта мы мы выгрузили логи его работы с нашего сервиса под нагрузкой и увидели что за 30 секунд работы нашего сервиса было больше тысячи сборок мусора из них э нулевое поколение собиралось 34 раза в секунду и занимало до 0,3 миллисекунды это повлияло примерно на 20% запросов первое поколение собиралась три раза в секунду и с временем ответа чуть большим до 3 миллисекунд и это влияло на 1,5% запросов практически и третье второе поколение собралось всего один раз но заняло больше 100 миллисекунд всё это вносит свой вклад во время работы нашего сервиса и мы начали искать А где же создаются все эти объекты которые ригерт сборку мусора как оказалось эти объекты находятся в потоке С какой которая отправляется статистику запомним этот момент И через некоторое время мы к нему снова вернёмся А пока будем довольствоваться тем что мы знаем как сборщик мусора влияет на наш проект как на наш сервис и самое главное что мы знаем где узкое место затем мы решили проверить как влияет на нас наш сервис Гил все хорошо знают что Python - это однопоточный язык программирования Виной тому глобальный Лок интерпретатора в питоне два потока не могут одновременно операции то есть один поток Берт блокировку а остальные потоки ждут когда он её отпустит по умолчанию блокировка ВТО не берётся на 5 миллисекунд в нашем случае заместо под солнцем борется два потока это поток основной рабочий поток и поток С какой и нет ничего страшного в том если поток С какой подождёт пока свои дела сделает основной рабочий поток но есть пробле когда основной рабочий поток будет ждать поток С какой вет задержки в работе нашего сервиса вот так вот работал наш поток наши потоки изначально они были в одном процессе и мешали друг другу с помощью Гила затем мы решили вынести поток который отправляет в кафку А в отдельный процесс дочерний и стали передавать данные между процессами через unix Pipe вот так схема выглядело раньше два потока жили в одном процессе и влияли друг на друга и также ещё на них влиял сборщик мусора периодически а затем стал выглядеть Вот так мы вынесли каждый поток в свой процесс Теперь они живут порознь и это изменение дало Нам очень приятный прирост мы снизили наше время ответа ещё на одну миллисекунду и также мы заметно снизили дисперсию времени ответа это очень здорово помните про то что мы говорили что в потоке Каки с какой создаются много объектов которые ригерт сборку мусора так вот этот поток теперь живёт в отдельном процессе и по идее он не должен влиять на сборку мусора в основном процессе и мы ещё раз посмотрели сборки э на логи сборки мусора э нашего сервиса под нагрузкой и уже за за 2 минуты постреляли в него и мы увидели что сборка мусора делалась всего в восемь раз И честно говоря она делалась только в первые секунды времени работы нашего сервиса нулевое поколение собралось всего семь раз первое один раз а третья не собиралась ни разу в итоге сборщик мусора больше не должен оказывать влияние на работу нашего сервиса но мы решили Проверить всё равно вдруг что и мы отключили этот сборщик мусора в основном процессе и решили посмотреть как это повлияет на перформанс и это никак не отразилось на перформансе нашего сервиса быстрее не стало и Как и ожидалось вот поэтому можно его включать было обратно и кажется что наш сервис работает теперь очень быстро мы отвечаем за 5 миллисекунд в 999 Персен и пришло время интегрироваться в наш энд кинотеатра и мы с интегрировать с тег менеджером и нас ждал неприятный сюрприз доля запросов которые отвалились при тайма ауте 10 миллисекунд была в среднем 12% это крайне много и график тайм-аутов очень напоминал нам график суточной нагрузки на сервис так и есть доля запросов отвали по таймауту коррелирует с графиком нагрузки на сервис чем больше запросов тем больше ошибок таймаута и мы решили отключить тайм-ауты на тег менеджере и посмотреть За сколько мы действительно отвечаем тег менеджеру и выяснили что на самом деле мы теряем порядка 5 миллисекунд между двумя этими сервисами и кажется проблему надо искать посередине Мы пообщались с коллегами и узнали что между нашими сервисами есть и контролер и мы решили посмотреть какие задержки есть на балансера а также мы хотели проверить из-за чего у нас появляется корреляция между корреляция между графиком запросов и графиком таймаутов И для этого мы выгрузили логи с ингрессо и решили как-то визуализировать распределение запросов на подах мы разбили Тайлан наш Тайлан на отрезке по 5 миллисекунд и стали считать сколько запросов у нас попадает на каждый под в эти пятимиллионный что там происходило мы визуализировать запрос с по подом и случалось Так что на некоторые поды два ингресс балансировать одинаковый запрос отсюда и у нас и получается корреляция между графиками тайм-аутов и графиком суточной нагрузки на сервис в качестве решения Мы решили переключить работу ингресс контроллеров на режим Active standby когда только один из ингрессо балансирует трафик в один момент времени и это решило проблему но у нас появились ещё две другие проблемы это спайки и фон ошибок и мы подумали С чем может быть связан фон ошибок но возможно питон медленно считает и мы решили написать Эхо сервер на питоне мы выключили все расчёты которые мы делали и поставили заглушку кото отвечала только одним из вариантов ответов иниго не по подумали может быть действительно н просто медленный И мы поменяли эсер на питоне на эсер нае и тоже ничего не поменялось фон таймаутов остался прежним причину фона ошибок мы распознать так и не смогли и отдали это на проработку в отдел Dev а сами пошли дальше может быть нам повезёт больше со спайками таймаутов Давайте посмотрим на этот график повнимательней имет и случаются в одно и тоже время каждого часа Вот например спайки на 18Д пй минутах на пятой минуте на тридцать пятой На пятьдесят пятой и вот они все вместе Ну мы решили разобраться с чем связаны эти спайки и выяснили на пятой минуте у нас происходит Старт подов для обновления конфигов экспериментов и понима Мы думали что у нас рестарт в остальных случаях это были джобы обновления конфигурации внутреннего балансировщика и джобы которые сканировали внутренний балансировщик на безопасность Ну хорошо рестарт мы как можем починить Но что же делать с внутренним балансировщика по-хорошему нужно запускать д обновление конфигураций с последующим выводом балансировщика из из балансировки Но это делать долго и и это делать команде из другого отдела а нам надо уже сейчас и мы подумали что мы можем сделать балансировку прямо на тег менеджере балансировку по ингресс контроллера Вот и мы так и сделали мы стали балансировать трафик прямо с тек менеджера на ингресс контроллеры и спайки тайм-аутов ушли остался лишь небольшой фон и мы решили что пока проблемы изучаются мы можем снизить тайм-аут до нас до 7 миллисекунд и добавить ретрай это привело к тому что фон тайм-аутов ушёл получились какие-то единичные маленькие всплески таймаутов и в итоге мы стали укладываться в требуемую доступность сервиса Это победа наша задача наконец Была выполнена мы потратили полгода на то чтобы проделать эту работу и это нас кое-чему научило вот в нашем случае выбор ритма влиял на эффективность нашего сервиса больше даже Чем выбор языка программирования и хочется сказать чтобы что нужно проверять реалистичность требований прежде чем проектировать наш сервис мы поставили серьёзные технические требования к сервису не убедившись что сама инфраструктура к этим требованиям готова и из-за этого потратили кучу времени на доработку инф уже после выкатки нашего сервиса И последнее что хочется сказать - это коммуникация её нужно прокачивать вот мы проводили расследование проблем с инфраструктурой очень плотно общаясь с командой из devops они вот подозревали о многих потенциальных проблемах Но поскольку никто из компаний не приходил к ним с такими запросами проблемы продолжали существовать в течение долгого времени и если бы была бы налажена лучшая коммуникация то возможно мы сократили бы количество времени которое мы потратили на разработку нашего сервиса Спасибо Спасибо у нас это на самом деле было прямо Приключение Я смотрел Как вы Как в каске про Колобка двигались двигались и пытались преодолевать всё новые трудности у нас секция вопросов и ответов Пожалуйста поднимайте руки у кого есть вопрос так там есть вопрос один вижу Добрый день Меня зовут Александр архитектор У меня вопрос Когда вы рассматривали проблемы lency при общении с тек менеджером там общение было по листу или По grpc По рист А вы не рассматривали jpc как способ типа как сивер булт что он будет быстрее и даже не надо будет разбираться с чем-то ещё А у нас jpc в компании не практикуется пока вот мы только пока по рист общались поэтому чего не пробовали Спасибо вопрос В центре зала есть я пока напомню ребят голосуйте пожалуйста за доклад проходите опрос эти оценки очень важны чтобы делать конференцию ещё лучше спасибо Привет Спасибо за доклад Что будете делать когда один ингс перестанет справляться А у нас ингресс не один У нас их несколько просто они работают в режиме AC standby вот когда один отваливается включится другой это высокая доступность А масштабировать как масштабировать в плане количество запросов поднимается нужно бы хорошо бы разбросать их по нескольку инстан сов у нас пока нагрузка порядка там 3000 она вырастает в пиках там ну там до 6.000 пока вот об этом мы пока не задумывались вот вполне возможно отдел до восо подумает как эта штуку можно масштабировать вот Спасибо вопрос на тот же ряд чуть правее Владимир Спасибо за доклад очень интересный вопрос такой ты говорил что вам важна была Лан то есть Правильно я понимаю что вы считаете эти распределение по тесту в онлайне Да в онлайне сплиты считаются да да А почему в онлайне Почему бы не за при процеси их и всё и у вас 1 миллисекунда нет их не получается запри процеси потому что у нас на самом деле экспериментов может быть много вот и когда запросы летят вот тут расчёт хэшей он чуть Более сложный на самом деле у на есть там слои Вот и мы динамически выделяем какие группы Какие пользователи надо какие пользователи попадают в эти группы вот посчитать сразу на всю аудиторию это не получается А у нас есть слои которые мы используем для того чтобы выделять параллельные эксперименты Вот и и получится что нам нужно хранить по каждому эксперименту по всем пользователям эту информацию это было накладно ещё вопрос так вот в центре есть два но сначала вот мужчины в синий коф все потому что да спасибо Всем привет Меня зовут Дима Я из газпромнефти вопрос как от руководителя проекта вы добились потрясающих результатов хороший доклад который это всё показал и А в уроках отметили что а важно уточнять технические требования и исходя из них уже ставить цели но у вас получилось так что вы поставили себе амбициозную цель сами наверное не понимаю ещё амбициозность и вот поэтапно пошагово достигли её А вот если бы вы пошли сначала уточнять технические требования какая была бы цель Как вы считаете а Ну цель была такая же там 10 миску н но я думаю что мы бы подготовили бы инфраструктуру намного раньше чем мы подготовим наш сервис То есть у нас проблема в том что мы сделали сервис и всё-таки здорово кажется можно выходить в продакшн вот сроки не съехали классно на питоне быстро пишется а оказывается что нужно ещё проводить кучу времени на resch проблем вот он итеративный и достаточно тоже долгий был мы бы сократили вот просто Time to maret если бы мы заранее подготовили бы почву для того чтобы это делать Окей ну возможности были видны Спасибо большое спасибо за вопрос ещё вопросы справа тебя и на первом ряду да ещё много вопросов Спасибо там когда мы рисовали архитектуру сплите и менеджер когда на сплиттер прилетает трафик и ему надо собственно его зап зарез сегмент Да попал Ты в сегмент или не попал в сегмент вот чтобы понять что пользователь попал в сегмент нужно атрибуты пользова устано вот эти атрибуты пользователя Они все в запросе прилетают или они где-то ещё в какой-то базе хранятся там пол возраст Нет они хранятся в сервисе тек менеджер который нам отправляет эти запросы прямо пачкой с пачкой атрибутов пользователей и девайсов то есть на самом деле трафик идёт через тег менеджер в сплите да он прямо весь трафик прямо проходит насквозь через тег менеджер а потом к нам Спасибо Спасибо назад зад тебя Спасибо за доклад Меня зовут Александр вопрос по выводам конкретно по первому пункту там было написано что скорость алгоритмов она важнее чем быстрые языки программирования А у вас нет каких-то бенчмарком те же самые алгоритмы с чем-то кроме питона например сленгом спасибо спасибо за вопрос в нашем случае алгоритмы заша Да Берков которые те же самые алгоритмы лают на других языках У нас тоже нет тут как бы скорее больше подход про то как нам правильно рассчитывать эти сегменты То есть хэши например в питоне считаются очень быстро потому что они реализованы на си Вот и узкое место у нас было Вот именно в этой библиотеке J Logic если библиотека написана на гулаге то наверно она может быть побыстрее работает Вот но на самом деле в ней всё равно большой оверхед за счёт того что обходить надо ВС это это дерево подставлять туда переменные то есть всё равно А и компиляция какой-то нативный вид она должна помочь Ну мне кажется временная сложность алгоритмов независимо от того какой язык если это будет N квара это будет N кват в любом языке Ну не знаю но возможно Да какой-то вот алгоритм как сказал мог бы проще язык быстрее обработать тоже самый алгоритм тут конечно не поспоришь хорошо у нас есть вопрос первый ряд Спасибо большое за доклад Алексей Газпромбанк Вот вы говорили что у вас небольшое количество сегментов в принципе и вот было ли вот необходимость так сильно инвестировать в оптимизацию алгоритмов может быть просто где-то изначально было уже предопределили такие идеи от продуктов чтобы хранить сегменты для пользователя после того как эксперимент закончился для того чтобы можно было раскатывать эксперимент а ну прямо сразу на тот сегмент выключая как бы флаг что это эксперимент вот а И для этого пришлось бы хранить большее количество сегментов Вот это больше сотни потому что эксперимент вот Отработал и выключился Вот и а сегмент может остаться Спасибо ещё вопросы опять первый ряд да спасибо за доклад хотел несколько вопросов задать первый вопрос а у вас есть разделение на зарегистрированных и незарегистрированных пользователей и вот этот кейс вот а второй момент я вот не до конца систему понял то есть с одной стороны вы сначала их уже у вас есть какая-то система которая ну их как бы распределила по сегментам Да И вот после этого как бы происходит какой-то пометка пользователей о том что вотт он там в атест оди а и он же в атест 2 То есть я имею в виду что с одной стороны у вас тоже как бы Вроде это просчитано же получается или нет Вот а третий Вопрос вот не холивар ради Вот вы сказали что да алгоритмы важнее но Какие преимущества давал питон при всей моей любви к нему вам если Ну то есть не пробовали вы например не бороться с этим Garbage колектором и Вот этой штукой А попробовать вот реально что-то другой Да там например низкоуровневая то есть Вот как вот мысль по Больше интересно как мысль ваша двигалась в этом направлении Так ну я начну с последнего потому что я его ещё помню А значит питон мы просто пишем в отделе на Python у нас разные команды пишут на этом языке мы его умеем готовить Вот и нам просто как бы своя рубашка Ближе к телу вот поэтому мы как бы решили писать на нём Тем более что мы сначала посчитали как-то на коленке нам показалось что ну это достаточно приемлемая скорость расчёта вот поэтому мы как бы сначала смотрели Что ну нормально считается всё а потом проблемы они уже там по ходу дела стали появляться Ну не хочется бросать и писать всё заново плюс у нас нет экспертизы на гунг например популярном решении вот а можно пер а да зарегистрированные пользователи не зарегистрированные тоже есть вот как бы авторизованные не авторизованные мы их называем вот да быстрый ответ да Сейчас нет но впоследствии мы мы будем тоже катать Эксперименты на неавторизованный зону Там намного больше трафик вот я повторю Да подсчитаны ли они были заранее в вашей системе по пользователи я так авторизованные я так понимаю я могу перефразировать вопрос подсчитаны ли у вас пользователи которые участвуют в тех или иных тестах в рамках разных сегментов Да да да конечно мы отправляем статистику по всем вычислениям в кафку вот это вот ужасный поток который много проблем создал вот мы отправляем статистику в кафку вот плюс также мы ещё ширу если прилетает запрос какой-то от того же самого пользователя мы его заново не считаем Спасибо ещё один вопрос Я думаю наверное последний Да вот вот вот прям прям посередине Спасибо топ доклад просто это конференция Да я из ВБ хотел бы во-первых кто-то спрашивал про Go про то как Насколько быстро работает сплите нанг у меня есть один из примеров так уж получилось что пришлось за 3 недели написать и выкатить продакшн на Гонг сплите похожий Вот на этот вот причём силами Ну людей которые не разработчики вот сроки горели и так далее и соответственно не такие хорошие были алгоритмы но был гон Вот и В девяносто девятом нле получилось 80s Ну 99,9 вот ну для при Это я так понял Не вопрос это общи комментарий на дискуссию понял дадада вот собственно и хотелось ещё тоже прокомментировать про предрассветные мы пустили в систему а затем только его просили в тест Ну это Фигово потому что у него эффект наз он начинает получать опыт через КАТО времени А А сколько вы тестов уже провели на этом на этой системе продакшене Сколько сколько экспериментов мы запустили Да но сейчас у нас крутится порядка там тридцати экспериментов очень достойно спасибо спасибо всем за вопросы если у кого-то ещё остались вопросы Мы можем обсудить их потом в дискуссионной зоне вот а сейчас э самое интересное выбираем самый лучший вопрос который А тебе задали и вручаем подарки от нашего партнёра Так ну вопросов вопросов было много вот и мне кажется лучший вопрос был про пред просчёт Наверно кто с А всё вижу ребят У нас есть обладатель подарка за лучший вопрос Ну и конечно Мы очень благодарны тебе за за доклад Спасибо что выступил очень Мне кажется классный кейс и вот эта история как вы оптимизировали я если честно восторг и у нас есть подарок для тебя от конференции Спасибо ещё раз спасибо тебе за выступление у нас"
}