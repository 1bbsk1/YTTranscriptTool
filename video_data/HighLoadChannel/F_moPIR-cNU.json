{
  "video_id": "F_moPIR-cNU",
  "channel": "HighLoadChannel",
  "title": "Как мы обрабатываем миллиард событий в сутки без ClickHouse / Александр Харитонов (Pixonic)",
  "views": 3626,
  "duration": 2691,
  "published": "2020-04-27T12:06:30-07:00",
  "text": "всем привет меня водкой аналитической системы в компании пик sonic вот уже 18 сдаю высоконагруженные системы но однажды меня пригласили принять участие в разработке необычной аналитической системы это была не простая система на стыке hadoop это была собственная разработка компании которая принимала сотни миллионов игровых и технических событий в сутки и позволяла всем сотрудникам компании анализировать их через удобный веб-интерфейс за секунды я увидел как работает эта система и мне захотелось принять участие в ее разработке сегодня я расскажу вам о том как мы собираем события с наших мобильных устройств и серверов как моих храним почему мы не используем аналитические базы данных готовые почему мы до сих пор не перешли на клик house также я поделюсь тем как мы выполняем запросы по большому числу аналитических событий компания пик sonic делает мобильные игры и мы хотим знать что в этих играх происходит также мы хотим знать что происходит на наших игровых серверах поэтому мы собираем события но компания очень быстро росла и менялась поэтому нам было необходимо очень гибкая аналитическая система в 2011 году первая команда программистов не нашла подходящее решение и начала писать такую систему самостоятельно назвали ее об метр писанина java для хранения событий использовали кассандру просто у ребят был большой опыт работы именно с этими технологиями для распределенной обработки событий использовали шторм а для очередей кафку все это крутилась на железных серверах под linux в тот момент когда я пришёл в пик sonic наша мобильная игра war robots начала очень активно расти за полгода число принимаемых об метром событий выросла со 150 до 1 миллиарда событий в сутки наша прежняя архитектура перестала справляться и нам приходилось принимать быстрые решения от метр занимается пик сонике тремя вещами мы собираем события моих храним и позволяем выполнять сложные аналитические запросы по ним что собой представляет события которое отправляют игры в от метр события это простой джейсон объект у него есть имя и какой-то набор атрибутов мы заранее не знаем события с какими именами или атрибутами нам придут мы поддерживаем все стандартные типы данных которые разрешены в джейсон а набора событий объединяются в бочча именно в таком виде передаются нам по сети мы используем гибкую схему данных и это оказывается очень удобно разные команды разработки может могут придумывать новые события вносить изменения в старые например измените тип для какого-нибудь атрибута но вместе с тем гибкая схема данных привносит множество проблем на рынке не так много аналитических баз данных которые используют гибкую схему потому что написать такую систему достаточно сложно и если вы попробуйте написать и он самостоятельно то наверняка со временем ваш код будет становиться сложнее и сложнее в него тяжело будет вносить оптимизации все будет работать медленно кроме того сжать данные без схемы тоже непростая задача в нашем случае нам удалось частично обойти эти проблемы я сегодня об этом тоже расскажу вернемся к отправке событий ваттметр мы написали библиотеки для различных языков программирования платформ эти библиотеки копят в памяти события события формируют из них бочче сжимают их и отправляют по протоколу https на нашей веб-сервера какой у многих первым кто принимает уж тебе запросы является яндекс яндекс просто прокси rowan эти запросы на наши там коты где мы их партере фильтровали но дубликаты и обогащались события дополнительными атрибутами например в каждое событие игрока мы добавляли текущий уровень мы приняли события но из-за плохой мобильной связи клиенты часто делают несколько попыток отправить очередной матч клиента и мы получали дубликаты в тот момент мы для фильтрации дубликатов полагались на порядковый номер бача который хранился в настройках клиента инкрементироваться с каждой отправкой хранился в так называемых sharedpreferences но оказалось что sharedpreferences иногда откатывается назад и значением из него уже нельзя доверять теперь мы фильтруем дубликаты иначе теперь мы просто вычитываем контрольную сумму над телом запроса клиента сохраняемые в кассандру на полгода если клиент в течение полугода повторит запрос то мы его проигнорируем мы должны принять события прямо сейчас даже если часть серверов у нас перегружены или недоступна мы должны в любом случае принимать события если клиенты начали знать в несколько раз больше событий чем обычно например как во время фичеринга как же нам это удается мы написали простой микро сервис на vertex назвали его api прокси поставили между индексом и нашими там котами этот микро сервис целиком сохранял входящий http запрос в кафку а клиенту отвечал ok благодаря очереди обработки http запросов в кафки мы теперь без проблем переживаем пики нагрузки и различные сбои в дата-центре а все дело в том что второй микро сервис который вычитывает и степи запросы из кафки и пересылает на нашего пи сервера гарантированно может повторить запрос в отличие от клиента теперь обогащенные дополнительными атрибутами события поступают в сервис tracking который группирует их по имени события и дайте их возникновения формируя блоки событий каждые пять минут tracking записывает эти блоки каждый в кассандру каждый раз под новым ключом но раньше мы делая не так раньше как и многие мы формировали агрегаты из всех входящих событий у нас были агрегаты по дням и агрегаты по 5 минут com но клиенты очень часто присылали события из прошлого поэтому нам приходилось эти агрегаты генерировать заново и перезаписывать соответствующие ключи в кассандре а частая перезаписи данных в кассандре приводит катастрофическому замедлению их последующего чтения кроме того повторная формирование агрегатов сильно загружала сеть и росла нагрузка на нашу инфраструктуру у нас начинал тормозить квиринг в итоге мы отказались от создания агрегатов и теперь мы счастливы теперь tracking каждые пять минут сохраняет в кассандру ему табельный блок данных а если событие редкая то мы решили сохранять этот блок в кассандру на один узел а для частых событий мы сохраняем блоки событий на несколько узлов кассандры то есть мы измеряем скорость входящего потока событий за какое-то окно и динамически высчитываем на какое число узлов кассандры будут сохраняться блоки данного типа события кроме того мы хотим чтобы жесткие диски на наших серверах базы данных были загружены равномерно поэтому мы меняем ключ парте церовани и для сохраняемых блоков каждый день но в данном случае это легко потому что этот ключ содержит текущую дату что с собой представляет блок событий которые мы сохраняем в кассандру блок состоит из колонок для каждого атрибута имя то информация эта информация содержит параметры сжатия каждой колонки также мы храним минимальное и максимальное значение каждой колонке у нас есть возможность прочитать из этого блока какую нибудь колонку независимо другой допустим если мы хотим посчитать уровне игроков то мы прочитаем колонку level если мы захотим прочитать уровне игроков и их use ради там и одновременно прочитаем обе колонки и пар одинаковым порядковым номерам значение в обоих колонках будет соответствовать одно и то же событие как я говорил у нас нет фиксированной схема данных мы не знаем значение какого типа нам придет может оказаться что допустим юзера едином присылается в виде числа а уже через 2 секунды виде строки мы пытаемся на лету определить наиболее общий тип данных который будет использоваться для данной колонке в данном блоке а в другом блоки и та же самая колонка может иметь уже другой тип кроме того мы сжимаем колонки атрибутов независимо друг от друга мы анализируем поток значений анализируем кардинально sti другие параметры этого потока и выбираем наиболее оптимальный алгоритм сжатия мы поддерживаем различные методы сжатия колонок и когда формируем блок пробуем примерно оценить сколько будет занимать колонка после сжатия каждом из этих методов тот метод который выигрывает мы будем использовать дополнительно мы пробуем и сжать колонку с помощью вас за 4 но если размер колонки при этом не уменьшился то мы сохраним ее в кассандру как есть в таком виде мы храним в кассандре порядка 1 триллиона событий которые занимают около 40 терабайт данных многие считают что лучше использовать готовые решения вместо того что писать самостоятельно да я с вами согласен мы постоянно ищем готовые решения но то что мы находим оказывается значительно дороже чем стоимость нашей команды и наших серверов поэтому мы до сих пор сидим и своем решении мы ищем системы с функциональностью как у от метра мы могли бы использовать готовые аналитические базы данных да мы пробуем использовать клик house но у нас возникают с этим определенные проблемы основная проблема связана с тем что за годы работы от метра там накопились десятки тысяч различных событий с различными атрибутами и переложите встроенную схему данных crack house и задача достаточно непростая кроме того сказывается еще то что мы не можем в хаусе для каждого типа события заводить новую табличку потому что это будет являться анти поттер нам в использовании клик хауса мы написали свою систему описали ее на java считается что java но не очень быстро может работать я конечно сомневаюсь что когда-нибудь от метр достигни ск артикли хауса но я тут провел эксперимент запустил запрос за год по сотням миллионов событий с фильтрации по одному атрибуту и группировкой подать а так вот этот запрос у меня выполнялся на одном сервере со скоростью 72 миллиона событий в секунду нас это скорость вполне устраивает мы написали свою систему также мы написали к ней поддержку из quelle но эта поддержка очень ограниченная мы мой подержим только стандартные простые запросы мы не поддерживаем join и не поддерживаем под запросы но аналитики хотят выполнять сложные запросы мы не стали тратить свое время что-то придумывать мы просто взяли поставили на свои сервера facebook просто просто это полнофункциональный sql движок запросов который работает с вашими данными теперь аналитики через просто могут работать с данными из от метра и выполнять запросы произвольной сложности но не только просто используют аналитики также они используют zeppelin и python для работы с данными из от метра от метр очень популярен в компании благодаря своей гибкости любой сотрудник компании может создать новый проект раздать доступа к нему добавить даже борды с различными графиками поделиться ссылкой на график с коллегой кликать мышкой произвольные выражение фильтрации добавить группировки это оказывается очень удобно программисты и аналитики могут забирать данные зад метр с используем ним с quelle open а за рабочих и игр за две минуты могут интегрировать от метр из детей свой продукт придумать событию имя добавить какие-то атрибуты и уже через пять минут увидеть его на графиках перейдем к самой интересной части этого доклада как мы осуществляем квиринг по собранным событием на самом деле все просто у нас есть несколько квейрис серверов и два координатора для отказоустойчивости исходный и скулил запрос пользователя поступает на случайный координатор где мы его парсим и формируем схему выполнения запросов которые пересылаем на какой-нибудь кори сервера оказалось что написать свой sql диалект достаточно просто мы взяли готовую библиотеку анд lr4 написали для неё простую грамматику имплементировать и visitor и теперь вам без проблем партиям sql запрос говори сервера у нас партиции раваны по имени события поэтому говорите о поэтому координатор всегда знает на какой клэри сервер нужно переслать запрос пользователя а если этот квари сервер недоступен то запрос пересылается на следующий к вере сервер по кольцу quarry сервера кэширует на своих локальных жестких дисках блоки событий которые они скачивают из кассандры поэтому форестера могут использовать все преимущества пэйдж кэш операционной системы и работать с локальными данными достаточно эффективно нам легко кэшировать блоки события на каире серверах потому что они и мутабельные мы копируем блоки с использованием алгоритма и lru то есть наименее используемый блок удаляется в первую очередь для кэша дискового кэша мы используем обычные большие недорогие sata диски и этого нам вполне хватает по производительности на каждом каире сервере у нас простая схема папок у нас есть папка для каждого дня каждого события стоит отметить что первым компонентом в этой структуре папок является номер сервера он как раз нужен для того чтобы любой кори сервер мог взять на себя нагрузку предыдущего в кольце так как мы используем подход вертикальных баз данных то для каждого атрибута события в этой папке у нас отдельный файл также у нас есть два файла с мятой информации да и мета хранить информацию о том какие колонки какие блоки им ужас опрашивались кассандры и блок смета хранить информацию о параметры сжатия каждой колонки но раньше мы хранили данные в кэше не так для каждого типа события каждого дня мы писали данные в один большой файл и читали его сверху вниз пропуская не нужны для данного запроса колонки атрибутов а теперь мы храним разные колонки событий в разных файлах число объем данных считываем и нашим приложением диска не изменился но во втором случае скорость чтения выросло в разы как же нам удается ускорить выполнение запросов с используя дисковый кэш блоков во-первых мы используем только последовательные чтения и последовательную запись файлы если мы захотим записать в кэш новый блок данных то мы просто дописываем его колонки в конце соответствующих файлов колодок на диске когда мы начинаем выполнять запрос мы скачиваем из кассандры только те колонки атрибутов которые необходимы для выполнения данного запроса мы их записали на диск в кэш и теперь мы считываем из кэша опять же только те колонки которые необходимы для выполнения данного запроса но перед тем как считать какую-либо колонку мы ее mipim в память целиком что нам тоже позволяет ускориться стоит отметить что мы пить память можно файл и даже превышающей объем вашей оперативной памяти любой аналитический запрос который приходит в от метр выполняется в 4 простых этапа на первом этапе мы скачиваем из кассандры блоки данных данных в конечно к вере серверах затем мы их фильтруем далее мы группируем события из этих блоков с учетом функции агрегации который указана в запросе и на последнем этапе мы делаем пост-процессинг результатов выполнения запроса например получаем 10 самых популярных ключей рассмотрим некоторые из этих этапов подробнее стоит отметить что все эти этапы работают целиком с днями что нам позволяет использовать различные оптимизации ускорять наш код когда мы скачиваем блоки данных из cassandra clare сервер знает какие блоки уже есть у него в кэш а каких нет поэтому запрашивает из кассандры только новые блоки данных а если несколько к вере серверов одновременно захотят использовать одни и те же данные точнее несколько пользователи захотят использовать одни и те же данные для выполнения запроса так вере сервер скачает и хз кассандры только один раз когда мы парсим выражения фильтрации мы формируем в памяти дерево карты леонов каждый критерий он получает на вход какую-нибудь колонку блок атрибутов фильтрует эту колонку по какому-то выражению и возвращает бесед с результатом фильтрации каждому порядку мы номеру значения в исходной колонки в этом рецепте будет соответствовать бит 0 либо 11 значит данное значение удовлетворяет условию фильтрации criterion а dc ты полученные из разных карт и леонов объединяются с помощью логических выражений и формируется итоговый бесед с результатом фильтрации который и передается на следующий этап обработки приступаем группировки на вход группировки поступает исходный блок события и бесед с результатом фильтрация мы интересуемся по исходному блоку события с учетом bets эта и осуществляем группировку событий в памяти с использованием обычных hp по стоит отметить что мы всегда осуществляем группировку в памяти но если размер карты группировки превышает лимит и там ее и сортируем по ключу и записываем файл на диске создаем новую карту группировки в памяти когда она переполняется опять скидываем ее на диск по завершении группировки у нас оказывается несколько отсортированных файлов на диске и одна карта в памяти которую мы тоже сортируем у нас несколько отсортированных источников данных поэтому мы можем использовать мер сорт для их объединения мы создаем мир итератор над этими источниками данных и передаем на следующий этап обработки но не все с таким паттерном группировки было хорошо а именно с накоплением данных в памяти а потом резким ее освобождением и сбросом на диск тот момент мы использовали восьмую джаву garbage collector joan & hip 30 гигабайт на каждом сервере мы столкнулись с тем что garbage collector переставал собирать мусор встал старом поколения и у нас происходил длительный fuji sic мы посмотрели джесси логе и увидеть увидели там сообщение грести конкурент марк резерв overflow вроде бы hippo у нас хватало но при этом full джесси происходил оказалось что g1 не хватало внутреннего стыка для хранения графа наших временных объектов мы увеличили параметр г.м. марк стек says после чего мусор старом поколение начал собираться и длительный fujitsu перестал происходить мы посчитали результат выполнения запросов пришел клиент мы должны ему отправить что может пойти не так на самом деле многое дело в том что мы используем меж итератор то есть это некие отложенные вычисления которые мы не можем вычислить весь сразу иначе произойдет out of memory поэтому мы стремимся результат запроса клиенту с поддержкой бег пряжа параллельно кодируя ответ формат все swim если клиент начинает медленнее вычитывать результат там и медленнее и тире ruim ся помешивайте ротару медленнее читаем данные с диска streaming результаты нам помогает отдавать большие результаты запросов и так как же нам удается ускорить выполнение запросов в рамках одного сервера мы используем сервера с множеством ядер и от метр утилизирует их всем все данные в от метре протестирован и по дням поэтому разные дни мы можем обрабатывать параллельно на разных ядрах и эти разные потоки разных играх не используют никакого общего состояния мы можем сильно оптимизировать наш код не использовать конкурентной структурой данных что нам очень помогает мы можем не использовать блокировки кроме того мы стараемся использовать только нативные типы данных мы не используем стрингер если нам нужно отфильтровать раку там поклонниц под строке там и все операции производим над бортовыми массивами это нам тоже помогает ускориться ну и конечно мы делаем минимальное число аллокации новых временных объектов но если мы захотим ещё у скорость выполнения запроса в несколько раз то мы будем параллельно его выполнять на нескольких к вере серверах но квартира вара будут протестированы уже не по имени события а по группам пользователей то есть запрос для различных группах друг групп пользователей будет параллельно обрабатываться на разному на разных к вере серверах а результаты за под запросов будут нежиться на координатора но мы пока так не делаем потому что нас вполне устраивает скорость выполнения запроса на одном сервере напоследок расскажу как мы делим ресурсы каире сервера между параллельно выполняющимся запросами мы придумали решение которое очень хорошо себя показала на практике перед нами стояла задача максимально эффективно утилизировать ресурсы сервера но при этом позволить параллельное выполнение запросов сделали мы это так на каждом каире сервере запускается один instance нашего приложения этому приложению выделяется определенный объем оперативной памяти и определенное число ядер процессора для каждого ядра процессора в приложении стартует отдельный поток выполнения а память которая выделена к варе сервера делится поровну между этими потоками один поток не может использовать память выделенную другому потоку таким образом мы параллельно на одном квари сервере можем выполнять столько запросов сколько ядер в сервере а если мы не можем выделить новому запросу гарантированное ядро то это запрос встает в очередь на выполнение и так пришел новый запрос пользователя мы выделили ему гарантированно и ядро запрос начал выполняться и в этот момент срабатывает сервис перебалансировки сервис перебалансировки смотрят сколько у нас сейчас запущены запросов и сколько ядер в сервер и сервис перебалансировки может добавить любому активному запросу дополнительный поток выполнения или забрать его передав другому запросу сервис перебалансировки учитывает различные нюансы например некоторым запросам дополнительные потоки могут не помочь ускориться например если это запрос по одному дню я считаю что написать свое специализированное колонщина и хранилище не так сложно и она будет быстро работать даже на джаве при этом вам не обязательно агрегировать событие или заводить схему данных заранее это все что я успел на сегодня рассказать но когда мы делали эту систему у нас возникало множество проблем если вы хотите их обсудить то я буду ждать вас после доклада дискуссионной зоне или в нашем telegram канале от метр там же я выложу все слайды спасибо за внимание готов ответить на ваши вопросы здравствуйте я бы хотел задать вопрос какого размера у вас кластер кафки и какими ресурсами он собственно обрабатывает эти запросы в миллиард сообщений да с кафкой все просто у нас всего три узла кафки порядка двух терабайт диски на каждом узле и это вполне справляется с этой нагрузкой по памяти и процессором луковка практически не потребляет памятью поэтому даже не помню сколько там память и процессор тоже особо не загружены недорогие какие-то стоят добрый день и спасибо за доклад меня александр зовут меня из позвольте два вопроса первый вы рассказали о том что у вас существует поддержку функционал дедупликации событий но когда вы рассказывали про сервисы вот прикладыванием request это даль не очень понятно какой сервис на себя берёт собственно роль дедупликации и втором я вопрос про просто собственного говорить о том что просто вам позволяет реализовывать функционал join и и так далее а у вас для проезд написан диалект как вере серверу или вы используете join и то есть вытягивает из кассандра давайте отвечу на второй сначала вопрос по просто мы написали коннектор для просто коса как метру этот коннектор использует уже наша open от метра для получения сырых данных но дополнительно эти данные сырые данные могут фильтроваться на стороне от метра то есть мы можем пробросить условия фильтрации из бреста в наш движок но там упадет квари сервер правильно что будут использоваться поперек вари сервера готовы рассказывали для фильтрации но при этом группировку нельзя в просто пробросить наш движок поэтому это будет делать уже псам просто то есть мы не придумывали какой то еще один sql мы просто написали java код который работает с нашим api и работает внутри процесса просто так и первый вопрос какой блогер и по какой сервис по команде дупликация со быть даже репликация когда запрос поступает на веб сервера мы учитываем контрольную сумму из тела запроса и по этой контрольной сумме фильтруем весь запрос и целиком то есть не отдельные события а запрос целиком у нас гарантированно то что эти запросы всегда содержат одни и те же снимки событий то есть он просто не будет записан в кафку в этот момент как слоны распаковку мы сохраняем все запросы клиентов сразу без какой-либо обработки для переживания пиков нагрузки а уже потом фильтруем дубликаты спасибо александр спасибо за доклад подскажите у вас существует ли какое-то ограничение на сложность запросов и количество возвращаем их результатов поскольку составляют их аналитики независимо на вас вот и второй маленький вопрос что такое вы делаете в основных к нагруженного со своими процессорами что выделяете по ядру процессора на каждой спасибо значит первый вопрос еще раз какой ограничение на объем и сложность запросов а да у нас есть ограничение на длительность выполнения запроса любой запрос через сайт максимум может длиться пять минут если он длится больше пяти минут то мы отменяем выполняет запрос и останавливаем все процессы на кого резерве которые ты делают а если запрос за какой-то длительный период времени то мы подставим вот это вот ограничение пять минут увеличиваем и запрос укладывается в лимита плюс у нас есть api ключ и для продвинутых аналитиков которым можно заполнять за выполнять запросы очень большие но них выполняют так из вопрос про поток натка а так когда запрос один выполняется в нескольких потоках мы так делаем для того чтобы в каждом потоки использовать неконкурентной структур данных и писать очень простой код именно для этого мы вручную делим по потокам выполнению каждого запроса именно в этом цель чтобы упростить и ускорить можно еще вопрос простите одном ну там спасибо вот мне очень понравился ваш доклад потому что тема такая знакомая и у меня есть для вас персональный подарок очень спасибо от моей команды спасибо потому что самое главное что было сказано написать свою кайло ночную сюда ни слова снаружи на никогда так не делайте а сегодня алексей забыл дома прощевай слова специализированную колоночный база данных вот пожалуйста да еще вопрос приводили примеры быстрее класса метрики хауса а про скорость вы не сказали вот предположим у нас есть две недели данных и мы хотим по каким то параметрам посчитать несколько статистику по пользователям за какое время выполнится этот запрос ну запрос любой запрос с таким перевод часть пользователь попадет в эти агрегации может да за год даже вот запрос выполняется несколько секунд соответственно за меньший период быстрее спасибо большое есть данные по событиям которые генерятся и которые записываются данные по нагрузке на чтения какие значит наковырять серверах мы используем сервера с большим объёмом оперативки и очень большой шанс что все данные которые есть в кэше они уже есть в пэйдж кэш операционной системы и про читаются быстро сколько вообще tps of приходит изначально на фронтовую систему которая такая значит в сутки система выполняет порядка 15000 аналитических запросов спасибо здравствуйте я здесь смотрите нет прямо спасибо за такое было очень интересно от открыть и просто вот вы попадаете в новую компанию оценить прост по срокам сколько времени вам по трапу потребуется какой какая команда должна быть чтобы внедрить такое аналитическое решение в новой компании ну я бы не рекомендовал так делать потому взял житель на это очень много времени я пришел в пик sonic 3 года назад но эта система разрабатывалась 2011 года и команда программистов было около пяти человек тот момент и за это время не много чего придумали если бы я пришел с нуля мне бы я думаю тоже не менее 3 лет понадобилось чтобы дойти до этого уровня поэтому лучше все-таки использовать готовые решения да здравствуйте спасибо за доклад два вопроса первый можно все-таки более подробно рассказать про сервера на который хочется кассандра то есть сколько конкретно оперативы она забирает сколько ядер вот и второй вопрос вы рассказывали про некую файловую систему для исполнения ваших запросов на сколько я понял вот почему не посмотрели в сторону ходу по и мы при дьюса ну просто у меня такое впечатление сложилось что несколько похожая идеология вот так по первому запросу сервера кассандры для хранения непосредственно событий блоков события у нас 48 серверов кассандры на каждый из них 64 гигабайта оперативной памяти и 2 процессорами xeon и вот сервера в принципе недорогие по поводу мы придется ходу по я не могу точно ответить на этот вопрос потому что я пришел в компанию когда уже были приняты эти решения не захотелось развивать именно это решение потому что она очень интересно и очень гибкая и работы действительно быстро возможно бы на стыке hadoop тоже можно было сделать что-то подобное но я недостаточно компетентны в этом вопросе добрый день и спасибо большое за так вот скажите пожалуйста а по чс парк рассматривался ли в стеке технологии как если да да у нас ребята пытались использовать spark как-то его прикрутить сюда но что тот не получилось может невозможность разве за чего возможность связанных раз гибкости мы данных и недостаточно функционалом который был в тот момент в спарке нам чего-то не хватило то есть мы не смогли перевести на shoulders трудно spark саша здравствуй спасибо за доклад такой вопрос если у вас контроль целостности дискового кэша и бывали ли у вас битые файлы и как это отражалось да значит нам легко на к вере серверах у нас как файловый кеш блоков которые мы можем потерять это вообще не страшно мы можем выключить какую-нибудь говори сервер и просто запрос выполнить на другом кори сервера потому что источником целостности данных у нас является кассандра именно из неё говорит сервера за качают данные если что-то пошло не так кроме того мы храним данные в кашах на каире серверах вместе с контрольной суммой и при чтении проверяем эту контрольную сумму здравствуйте спасибо за доклад вопрос такое как вы во время выполнения запроса контролируете количество памяти которые потребуются для выполнения запросы как вы вообще выбираете сколько памяти задействовать вполне конкретно да как я уже говорил когда стартует к вере сервер он запускает фиксированное число потоков выполнения по количеству ядер процессора и память которая выделена всему каллари серверу делится поровну между всеми ядрами когда выполняется какой-то запрос мы считаем сколько в данном потоки по три билось памяти а как вы это делаете технически вручную то есть получается ну как бы у вас есть некий кода которая следит за ну да у нас есть скот который знает что вначале класса там 12 байт обязательно и потом там ещё ещё ещё в этом складывает все чаще силки и вычисляет примерный объем java объектов памяти хорошо спасибо добрый день сейчас начал пойду а вывод сказали про схему ивентов что их как бы не оттуда на несколько схему данных а с этим не возникает никаких проблем при говори потом там допустим где-нибудь стоимость спрашивать знаю где стоимость больше чем 1 1 декабря и еще не такой уж time stamp охранял симптом да значит если нам прислали число в виде строки то мы такие умные мы пытаемся это эту строку представить в виде числа если нам это удается там и типом данных для данной колонке в данный момент времени выбираем именно число они строку может оказаться что допустим за неделю в одной колонке хранились числа а потом строки и при этом строки нельзя от конвертировать в число и когда мы выполняем какой-нибудь условий фильтрации например на больше десяти то вот эти строки которые нельзя сконвертировать в число будут проигнорированы ну то есть данные которые не подходят под такую условно говоря низком уровне вас кем они игнорируются откидываются на аллен данные которые не подходят к условиям запроса игнорируются а как быть там с потерей точности до самого например вот что-нибудь такое не понял вот если у нас внешний винт приходит там со стоимостью до критической информация какая-то важная поточность этом знают и сломал бы что-нибудь ногами связаны скажу у нас есть зарезервированные два типа атрибутов это запуск игры и платеж и вот для вот этих событий список полей строго регламентирован но тем ни менее могут прийти и дополнительные поля случайная но есть стандартные которую надо именно так посылать спасибо так сейчас правая галёрка вы пока думайте кто получит миллион долларов за лучший вопрос вспоминаю спасибо за доклад вы говорили что у вас есть красивая вот мордочка в которой можно посидеть и быстренько натыкать сквере который мы хотим соответственно вопрос как вы собираете и в каких объемах информацию о частых значениях для событий ивентов частые значения для событий если у вас какая-то мордочка да то соответственно вы должны дать пользователю возможность рассказать что вот в этом ивенте есть такие the field и который или строка или таймс я понял да у нас есть отдельный сервис который формирует словаре имен событий которые нам пришли и словаре имен атрибутов и словари последних там 16 значение каждого атрибута именно содержимое этих словарей отображается в интерфейсе когда пользователю предлагается выбрать имя события предлагается выбрать имя атрибута и предлагается отфильтровать по какому-то значению и 16 достаточно достаточно потому что есть возможность ввести любое свое значение в аналитику поэтому так спасибо спасибо залог авто расскажите пожалуйста вот вы говорили о том что вы van een запросов пытайтесь понять строка это число или нет и если строке содержится число то вы ищете уже у себя число правильно понял сейчас немножко расшифрую когда мы принимаем события уже в этот момент времени мы при формировании блока выбираем наиболее общий тип данных для какого-то окна значение атрибутов то есть в рамках блока у нас к эта колонка имеет фиксированный тип данных а когда мы выполняем covering мы смотрим что в данном блоке хранится тип данных string для данного то есть этим данных и нтв допустим для данной колонке запрос хочет работать со стрингами там контент вызывают там и в момент выполнения запроса для данного блока данных генерируем в памяти байт-код конвертера которая преобразует из чистил байтовые массивы ну встре углу и уже потом выполняем запрос именно по тому критерию который прислал клиент но чаще всего запрос совпадает с типом данных которые уже лежит в блоке и вот это конвертация во время запроса делать не надо под его так мы сейчас уже потихоньку входим в режим кулуаров в принципе можете на сцену выйти может да давайте я поэтому дискуссионную зону и все что мы будем будет давайте последний вопрос уже зададим уже мужчина даже поднялся как бы калорий жег смотрите то есть если мы говорим про игровые события то явно у каждого события есть пару таких field of которые именно к событию относятся остальные относятся к непосредственно самому состоянию игрока то вон платящий тип некит дельфины так далее вопрос вы когда сохраняясь и event вы его насыщаете этим по каким-то способом и именно вы в каждые венты тавкр apple войти или вы как-то отдельно это складываете да у нас есть два типа проектов которые можно создавать ваттметры один проект для треккинга мобильных событий игр а второй проект для треккинга технических событий с различных серверов игры так вот если событие приходят с мобильного устройства то мы обогащаем это событие дополнительными атрибутами как я уже говорил мы добавляем текущий уровень игрока мы добавляем туда страну и вот то что вы говорите то есть кукша их заплатил там и так далее таким образом этой информации есть в каждом ивенте до в каждом в каждом ивенте от пользователя от мобильного устройства есть имена атрибутов которые начинаются с доллара это означает встроенные атрибуты который поддерживаться ваттметры спасибо александр спасибо большое это сейчас бурные аплодисменты супер остальные ручки сейчас это пихают сэндвичи себя во рту и сват оставшихся в зале кому отдадим и миллион долларов за лучший вопрос смотрю многие уже ушли а кто задавали вопрос поднимите руки пожалуйста чтобы вот александр давайте первый ряд да и на выходе со сцены вы тоже получаете тульский пряник и спасибо"
}