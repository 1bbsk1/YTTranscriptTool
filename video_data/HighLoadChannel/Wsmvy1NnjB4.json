{
  "video_id": "Wsmvy1NnjB4",
  "channel": "HighLoadChannel",
  "title": "Feature store / Анастасия Тушканова (Ozon)",
  "views": 1622,
  "duration": 2645,
  "published": "2024-04-17T01:10:27-07:00",
  "text": "Всем привет Меня зовут Настя И я разработчик в команде поиска озона сегодня мы с вами поговорим про нашу futurestore такое место он занимает в принципе в pypal не поиска чем они нравился нам год назад и какие хайки мы применили чтобы наш Future стал одним из самых высоконагруженных в России начнем с цифр у нас довольно много товаров у нас данный момент 150 миллионов товаров поисковом индексе каждый из них мы умеем искать каждый из них можно найти и заказать по количеству запросов у нас дневной трафик это порядка 10 тысяч за пару секунд и мы держим до 35 тысяч пики на распродажах также на жесткие тарге то по скорости то есть мы отвечаем за 50 и за 400 миллисекунд 099 рассмотрим как запрос проходит вообще в принципе поиска через какое-то количество сервисов фронтен и так далее запрос от клиента сайта приходит в Верхний поиск верхний поиск это сервис который является единой точкой входа для всех запросов не только для пользовательских но и для других сервисов верхний поиск преобразовывает запрос от клиента то есть нам приходит текст нам приходит фильтры нам локация пользователя и многое другое мы эти фильтры запрос так далее преобразовываем в специальный язык поиска Например если вы указываете какой срок доставки например Вы хотите чтобы товар вам привезли Сегодня мы преобразуем в список складов подходящих также верхний поиск ответственный за применение обтестов то есть какие-то разные запросы будут из верхнего поиска идти дальше верхний поиск отправляет преобразованные запрос в средний поиск средний поиск нас отвечает за ранжирование но перед этим он отправляет запрос базового поиск базовый поиск это как раз сервис который собственно ищет товара при него мы много говорить не будем все что нам нужно знать это что мы закидываем его через запроса фильтры И говорим что нам нужны какие-то дополнительные Поля из индекса а базовый поиск нам на выходе выдает какие-то подходящие товары под запрос а также какие-то поля которые мы запросили здесь важный момент что под запрос может подходить очень много разных товаров но нам не нужны они все мы получаем максимум 2000 при этом мы берем не просто какие-то две тысячи а мы берем лучше относительно релевантности запроса с точки зрения текста затем собственно начинается процесс ранжирования для того чтобы мы могли делать ранжирование нам нужно получить мл фичи займов отвечаем мы идем в Future Store рассмотрим что вообще такое мл фичи как они работают зачем они нам нужны это некоторые свойства товаров которые мы показываем эти фичи будут подаваться на вход модель и на выходе мы получаем некоторые порядок товаров этот процесс мы называем ранжирование Какие вообще фичи бывают рассмотрены на примере у нас бывают товарные фичи это фичи которые относятся к конкретному товару это может быть какой-то рейтинг его количество заказов конверсий и так далее также у нас есть парные фичи которые относятся к товару плюс конкретному запросу Например если вы покупаете товар по запросу платья то плюс единичка идет в эту парную фичу по запросу платья а не каким-то другим запросом также персонализированные например мы знаем покупали ранее пользователи это конкретный товар или можем использовать покупал товары такого же бренда ранее а на самом деле у нас еще много других вещей на них мы фокусироваться не будем мы будем фокусироваться только на этих трех видах Почему Потому что именно эти фичи У нас сейчас хранятся fitch Story рассмотрим на примере чему вообще могут быть равны фичи Например я как пользователю захожу на сайт и вбиваю запрос платья у меня выдается какая-то куча товаров но мы для примера возьмем первые четыре вот у нас первые четыре товара представим что у нас существует всего четыре фичи из них две товарные одна парная и одна персонализированная товарные фичи Мы возьмем это рейтинг товара и количество заказов за последнюю неделю рейтинг товара Мы можем с вами вывести накачки как любые пользователя А количество заказов от нас скрыто и мы только по специальному внутренним данным азону можно получать эти цифры также у нас есть парная фича Это количество заказов по запросу конкретного платья за последнюю неделю можем увидеть что вот четвёртый товар покупали всего один раз и только по одному запросу А все остальные покупались по каким-то дополнительным запросам также Как пример профессионализированные фичи это фича Был ли товар в избранном конкретном пользователя я вот например третий товар Уайт но у него горит красное сердечко это значит что товар у меня в избранном и скорее всего будет у меня ранжировать выше чем другие Окей мы сходили fitch Store получили мл фичи и можно начинать ранжирование то есть применять эмаль модель на вход модель будет подавать как раз маточка которую я показала а на выходе будет какой-то порядок товаров в итоге с этим порядком товаров мы делаем разную магию мы склеиваем разрезаем на страничке и добавляем рекламу на выходе клиент получает список ID товаров То есть это товар который будет потом показано там будут дорисовываться красивые картинки и так далее но работа поиска на этом закончена сфокусируемся на герое нашего доклада на фич-сторе что вообще у нас есть у нас средний поиск который Мы уже обсудили он является клиентом и фича история ходит в него также дата сентисты они раз сутки рассчитывают эти фичи по своим разным данным и рассудки складывают их на hdfs Наша задача перекачать эти данные чтобы они были доступны для среднего поиска с из самых в радиус костера и при помощи оболтера которые работают в ходупе прикладываем на заре времен у нас были только товарные парные фичи и в какой-то момент у нас начал быть не совсем стабильном ходу мы в то время еще Не умели его готовить и мы поняли что если в какой-то момент ходу пляжат то нам нужно иметь возможность получать в любом случае фичи даже если они будут вчерашние позавчерашние или вообще неделю назад Они были подсчитаны нам нужны фичи Хоть какие-то нам важно иметь данные для того чтобы был ранжирование потому что если данных не будет то мы начнем терять деньги компания будет грустить и всё такое поэтому мы решили что хотим немножко уйти от ходупа Мы решили что хотим сказать данные в кафку и потом в случае каких-то сбоев их оттуда вычитывать Таким образом мы сделали персонализированные фичи когда захотели их делать но здесь тоже есть проблема которую мы обсудим далее Вот такая схема она у нас была порядка двух лет она работала хорошо до момента пока у нас не начали расти и нагрузки То есть как у любого бизнеса у нас появляются РПС и появляются много новых товаров и вылезают разные проблемы рассмотрим что у нас здесь получилось первое самая большая проблема которая нас беспокоила это высокая нагрузка на редиса Откуда она выходит как я уже сказала у нас максимум 2000 товаров в запросе Сегодня мы это конечно 2000 а чуть меньше товаров запросе для каждого товара из-за того что нас равномерное шортирование в радиус кластеры по дефолту мы потенциально идем во все шарды и парных и товарных вещей с персонализированной в чем Такой проблемы например Нет потому что мы храним по пользователям в одном поисковом запросе ровно один пользуется и здесь проблем нет но при этом вот 1200 товаров это проблема при этом из-за того что мы используем радиус костра рейлинскую страсть более концов хэш Сватов И даже если на одной Наде родились кластеры оказалось несколько нужных слотов запрос все равно разбивается В итоге у нас получается что днем Когда у нас 10 тысяч РПС и в среднем у нас 1.200 товаров запросе у нас получается 12 млн РПС на рейсы Это довольно много учитывая то что пиковая нагрузка на редис это где-то 100.000 запросов А в распродаже эта цифра получается три раза выше мы начали думать что здесь можно сделать в какой-то момент мы даже упёрлись сетевую карто ссылку на сервере в 10 гигабитную мы попробовали увеличить количество шардов но на самом деле это была плохая идея потому что делаю только хуже потому что мы все равно ведем во все шорты мы увеличиваем нагрузку на сеть потому что стало больше а проблему исконную мы не решаем мы начали увеличивать количество реплик это помогает Но нам Пришлось сделать очень много реплик у нас было 18 реплик на одни данные и мы потребляли где-то 12 памяти и конечно эта цифра начала только расти с приходом новых товаров и с появлением новых нагрузок потому что мы начали добавлять еще больше реплик Какая здесь идея сразу приходит идея здесь следующее что мы хотим локализовать данные то есть мы не хотим ходить вообще мы хотим ходить в минимально возможное количество шортов с персонализированными вещами здесь все понятно мы и так все доходим с парнем и фичами но здесь появляется идея что можно ходить по запросам то есть парная фича она зависит от товара и зависит от поискового запроса в одном запросе поиск у нас ровно один текст соответственно если мы будем шортироваться по тексту мы всегда будем ходить ровно вольный шарт с товарами вещами здесь проблем больше потому что здесь такой удобной штуки как запрос нет но есть кое-что поинтереснее у нас есть категории товаров с категориями товаров Здесь тоже придется подумать Потому что это уже неравномерно какие-то категории больше какие-то меньше Но пока что сфокусируемся на других проблемах которые у нас были другая проблема Это что Данные постоянно устаревают какой-то товар продался его больше нет наличии его никто не найдет никто никогда его не купит но мы будем продолжать хранить его данные Почему Потому что мы рискуем потерять данные в случае какого-то сбоя если поставим То есть если например у нас будет болеть ходу мы не будем два или три дня заливать новые данные вы просто останемся вообще без всех данных Если вдруг пропустим эту проблему другая другая проблема которая у нас здесь есть это то что мы не можем здесь сделать хороший touring Как я уже говорила разные типы фичей появлялись разное время разные костыли применяли чтобы решить проблему но никакие эти к остальные не помогали но при этом мы хотим деважиться и тестироваться и иметь возможность получать произвольные данные и в данной схеме мы это делать не могли И если мы захотели протестировать наш торопись Store вы бы тестировали на самом деле не его а текст или которые мы сделаем для дебага последняя проблема которую я сейчас расскажу это то что к нам пришли да и сказали что мы хотим взять какую-то фичу э придумать к ней другую формулу и поставить например это могут быть какие-то конверсии можно поменять какой-то источник можно поменять формулу и сказать Вот у меня новая фича хочу посмотреть как это будет работать Но в нашем случае из-за того что у нас был высокий фактор репликации Мы просто не можем себе это позволить это понадобится продублировать все данные и это будет просто супер дорого Итак К какому решению мы здесь в итоге пришли единственное решение здесь правильно это просто всё снести и начать делать заново Ну что начнем с первого вопроса Первый вопрос Это какое хранилище взять Мы хотим хранилище которое мы будем быстро читать данные сможем перезаписывать раздеть фичи и при этом не развалиться формализуем эти требования первое требование от высокой скорочтения нужна высокий таргета второе требование это устойчивость к потери дата-центра как все сервисы в Озоне Мы обязаны не развалиться Если вдруг развалится наше железо и пользователь не должны этого заметить третий требования то что мы хотим без рисков для продакшна перезаписаны все данные раз в день некоторые базы данных имеют риск развалиться потому что мы хотим перезаписывать данные быстро но при этом не разрезать читающую нагрузку следующее требование наше то что нам скорочтения на самом деле важнее консистентности то есть если до 100 товаров из 150 миллионов нас будут данные не сегодняшняя вчерашние мы от этого не умрем Мы хотим скорее быстрее читать чем иметь какие-то вот такие вот консистентность и сильное требования последнее то что мы хотим иметь команду в азане отвечающую за эксплуатацию либо активное комьюнити То есть если мы напоримся на какой-то редкий баг то мы не хотим с этим Богом жить несколько месяцев и пинать ребят и в то же время мы хотим иметь людей которые можно прийти попросить что-то нам в итоге мы остановились на рейдесах то есть рейлисоном нравились но строительством есть проблемы с хашлатами которые я уже рассказала при этом радиусов есть второй вариант и к нему к стендолону ещё можно прикрыть рассмотрим чем они отличаются во-первых Он поддерживает шарнирование Это основное их различие соответственно если мы хотим использовать какой-то наш ориентирование по запросам по категориям нам для Sundown придёт всё писать Сами вот из-за этого у нас высекает следующее что если мы хотим добавлять удалять ноды как-то решардироваться то ради вас здесь понадобится ничего делать не надо будет а в редис надолон нужно думать самостоятельно также Вот минус radisk кластера это что у нас есть один запрос на один Хеш свод даже если на одной ноге несколько Штатов при этом в редисон из-за того что он не поддерживает шортирования можно делать один запрос на ноду важно здесь замечание Что radis conster также мы не имеем контрольно над тем Какие ключи будут лежать на одной ноге то есть мы не можем контролировать как свод будет лежать вместе с другими красотами при этом Так мы будем сами делать шардирование Мы можем контролировать это всё Дело это нам важно по той причине что как я сказала категории Они неравномерные и мы хотим какие-то супер большие категории на которые много трафика их отдельно от всех остальных чтобы они друг другу не мешали и важный пункт с точки зрения отказа устойчивости в рейдерского страха клиент сам должен следить за состоянием нот То есть если кто-то упал то ты должен сам это понять сам это как-то решить эту проблему в случае ради Standalone Можно прикрутить сенсонель несколько сенсонели будут сами следить за состоянием нот и в случаях проблем они собирают квору и сообщают клиентам на практике работают гораздо лучше чем клиентские библиотеки которые мы использовали и в итоге мы остановились на Standalone radissaх что мы можем сами придумать свои шардирование сами можем распределить категории большие и маленькие так чтобы распределить нагрузку максимально и мы можем не париться на счет разных цветов при том что на самом деле есть такое понятие как вайнинг то есть если несколько запросов идут в одну ноду они могут собираться в Бачи Но это все равно они нивелируют проблему того что нам надо распределять запросы и локализировать трафик Окей на графике появились рэдисы все остальное здесь уже знакомо дата сантиста который раз в день складывают данные на hdfs А плоддер который перекачивает данные из hdfs в radiss и средний поиск важно что мы здесь все-таки решили пошартироваться но с шортированием Конечно есть определенные проблемы с персонализированные парными отвечаями все уже вроде Понятно Но с продуктовыми вещами товарными есть сложности то есть категории мы уже придумали как раскидать мы просто возьмём список больших категорий на которые много трафика и будем складывать их отдельно от всех остальных А ещё есть проблемы Что категория на самом деле не статичная история у одного товара категория может меняться например завёл неправильный товар потом через неделю решил что надо это исправить и у товары меняется категория при этом мы не хотим эти данные потерять поэтому мы добавляем здесь кафку который мы вычитываем новую категории для товаров то есть в этой кавки нам приходится старая категория товара и новая категория товара и соответственно сервис некоторые высчитывает эту кавку и учитывает из одного ряда данные и перекладывает их в другой следующий проблема которая у нас была это что данные устаревают в случае как раз перекладывания данных по разным категориям это особенно острая проблема Ну что мы не можем удалять данный Старт категорий пока не убедимся что другие сервисы все потянули эти данные и что мы в синхронизированном состоянии находимся поэтому мы решили сделать второе холодное хранилище оно будет делать следующие задачи наши выполнять во-первых Мы хотим туда перенаправить максимум трафика который только можем чтобы их разгрузить это может быть трафик у других сервисов которым не важна скорость например к нам иногда ходят авто модерация ищут какие-то подозрительные товары им неважно ответим Мы за 100 миллисекунд или за 10 секунд поэтому можем увеличить здесь тайминги не потеряв не в точности ни в чём ещё также в холодном хранилище Мы хотим делать разные начистические запросы искать например выбросов данных э поэтому мы хотим какую-нибудь революционную баз данных либо просто SQL формализуем требования кроме сквеле Мы также хотим строгую или в конечном счете консистентность чтобы в случае чего мы были уверены что данные у нас правильные то есть здесь нам скорость тени на самом деле не важна и здесь мы как раз выбираем Эти стены также Нам нужен стандартное требование про устойчивость к потере дата-центра также мы хотим трупу чтобы он был достаточно для перезаписи данных раз в день но мы не хотим это делать быстро нам достаточно сделать за несколько часов и опять же важная строгая требование что нам нужна команда в зоне которая может отвечать за эксплуатацию И если что-то нам подтенеть В итоге у нас здесь стояла выбор на самом деле между кассандрой и погрессом Кассандра выигрывают сто четыре пункта А подгресс выиграли с точки зрения того что есть команда отвечаю за эксплуатацию и в итоге мы остановились и соответственно команда отвечающая запас Греции нам в итоге всё поняла здесь на схемы добавляются шарды постгресса мы их точно также раввидируем и точно также переключили весь возможный трафик и среднего поиска туда и переключили Читающий трафик и зря там апдейтора то есть на самом деле нам не нужно вычитывать данные из Реди шардов когда товар меняется категория нам достаточно прочитать ее из Холодного хранилища и таким образом какое-то Количество читающих нагрузки средусов Мы тоже убрали Окей проблема вторая уже третья которая у нас была это проблема туллинга то есть здесь на схеме Мы видим что у нас есть два сервиса которые читают два сервиса которые записывают И на самом деле мы хотим вот этот код держать в синхронизацию на суть каких-то изменений и поэтому мы ко всем этим сервисам приключили библиотеку а это библиотека занимается тем что одним и тем же кодом и читает и пишет В чём это сделано для разных вещей одинаково то еще нам нужно знать для добавления нового вида печей например Это какой у нее ключ как ее шортировать и как из данных хранить И на самом деле Вот это библиотеку из нее можно сделать микросервис и дать читать наши данные другим командам Окей последование требования которые у нас было это желание проводить об эксперименты над по разным посчитанными фичами что мы здесь Хотим мы хотим продублировать данные на самом деле когда мы пошали из-за того что нас уменьшилась нагрузка мы смогли уменьшить количество реплик с 18 до стандартных отказоустойчивых трех напарных вещах и с 18 до более как бы сказать Ну короче с 18 умножили до 5 потому что иначе все равно там трафика получается много Окей Мы решили что мы можем все-таки продублировать данные и завели механизм Бери визель что такое об ревизия bery Visa Это значит что у нас есть какие-то рейсы какие-то пазгрессы которые хранят определенный тип данных это мы объединяем в AB по ревизии то есть редис и пас Греции которых нет товарные фичи мы объединяем под ревизию товарных вещей то же самое с парными кристаллизирована под ревизии разных типов Мы объясняем в ревизию Что это значит для дата-сантистов это знаешь что в момент когда дата саннитист хочет поэкспериментировать на какими-то фичами там например изменить формулу конверсии приходит к нам Мы заводим базу данных либо берем свободные заводим его под ревизию с новыми экспериментальными свечами мы заводим под это новую обер ревизию и соответственно когда ты сидишь будет ставить эксперимент он указывает В какой какой процент трафика куда направлять соответственно с точки зрения надо сантиста будет выглядеть примерно так то есть он указывает какой трафик куда Первая страничка группы эксперимента это сколько трафика и группы разделяются на то какой трафик куда контрольная группа соответственно отправляет трафик в дефолдный ревизию А экспериментально уже в экспериментально при этом мы поддерживаем не только какие-то эксперименты надо вещами но также нам нужно эксперименты над моделями То есть если мы хотим добавить какие-то новые совершенно новые фичи Э мы можем завести точно также экспериментально б по древизию добавить там побольше потому что здесь например к нам пару раз приходили с желанием у нас было 50 Мы хотим теперь хранить то и в старой схеме мы бы Например так не смогли мы бы сказали что у нас просто не хватит места для вас а в данном случае мы можем завести redis побольше и соответственно у нас появляется новая возможность для экспериментов таким образом эксперимент будет выглядеть так что вот здесь добавляется кроме оберевизе еще и название модели Экспериментальный и будет идти трафик в экспериментальной ревизионами и будет дополнительно к этому поверх прикручивается моделька которая свечи новые используют Окей стоило ли эта игра свеч Давайте сравним все проблемы которые у нас были мы решили при этом мы улучшили тайминги мы уменьшили потребление памяти и мы на самом деле даже меньше потребление CPU то есть вот эта схема с уменьшением нагрузки наносить на самом деле помогла нашему среднему поиску потреблять меньше ресурсов О'кей А может быть стоило взять какой-то собственный решение Может быть там есть что-то получше но на самом деле с этим есть большие сложности собственное решение которое я нашла работает даже дольше чем наши старые решения при тех же потреблении памяти соответственно Я считаю что игра стоила свеч и надеюсь что вы не будете повторять наши ошибки у меня на этом всё спасибо за внимание и я готова отвечать на вопросы нанайсе Огромное тебе спасибо за замечательный доклад легкий презент от организаторов и у нас есть первые вопрос здесь Да привет Настя Спасибо за клад Классно мне вопрос по поводу немножко того как стороже располагается если перевернуть на слайд со сравнением это Изначально сказал что 12,5 ТБ памяти теперь у вас меньше Да занимает все это и коллеги Как вы умудрились снизить потребление памяти по сути говоря Вы же ну типа данные там и там Хранители в пазгрэ или нет да количество данных потребляемых меньше за то за счет того что мы уменьшили фактор репликации То есть если раньше у нас было 18 реплик на парные фичи то теперь мы храним три реплики то есть нас потребление памяти например для парных вещей меньше всего шесть раз с точки зрения персонализации там Практически не было изменений а продуктовых вещей они занимают гораздо меньше памяти чем продуктовые и соответственно Вот это разница немножко нивелировалась я еще тогда один вопрос задам быстрый А вот почему выбор такой постг для холодного хранилища если хочется быстро модельку получить разве какой-нибудь парк или еще какой-то из пост быстро прочитает вообще Вполне себе то есть мы как раз парком читаем и это получается где-то 3-4 часа она вычитывание Сколько гигабайт 3-4 часа на 100 ГБ для горячего хранилищ там получается 20-30 минут это вообще можно сказать ракета по сравнению с холодным хранилищем понял Спасибо И у нас следующий вопрос Добрый день спасибо за доклад хотелось бы спросить по поводу вот вы сказали сбоит ходу Вот с какими конкретными проблемами мы столкнулись как их решали То есть когда у вас ходу плёг это первый вопрос а второе сказали вот по потерю дата-центра вот у вас один hdfs образом то есть от ходу на несколько дата-центров То есть как или несколько скажем так кластеров ходуковских на Ну то есть друзья просьба задавать вопросы по одному просто достаточно сложно это все запомнить давайте начнем с первого по поводу неустойчивости ходу по на самом деле какое-то время ходовна жил вообще в одном центре То есть если у нас какие-то проблемы конкретным дата-центре то мы не можем не прочитать не какие-то вычисления там производить соответственно Бывало такое что вот там какая-то машинка развалилась и все у нас вся работа нарушена в конечном счете ходу протянули на все три центра лучше но как бы ошибки прошлого мы не хотим воспроизводить Спасибо Здравствуйте Можно доклад вопрос картинка с бульдозером выглядит так чтобы уволили предыдущую команду и набрали новую Сколько человек пострадало при этом на самом деле вот эта разработкой занималась два разработчика всего то есть я и другой разработчик и никакая команда никуда не ушла мы все на месте москва-сити он удобен или нет Я боюсь этот вопрос не относится к теме доклада коллеги мы сможем с радостью обсудить вне темы доклада зоникой А сейчас Мы возвращаемся к вопросам следующий вопрос Привет Спасибо за доклад У меня вопрос Как вы распределяете категории дам это ручной процесс автоматизированный изначально мы провели аналитику посмотрели Какие категории вообще большие Какие много трафика мы занесли в конфиг распределяем трафик категория большая например имеет одеждник один и мы отправляем ее в шар номер один Ага получается то что если запрос попадает например в одну категорию то он будет обрабатываться на ошарда если при этом он раскидывается по несколько категориям тогда все еще Также вы будете ходить несколько шардов да на самом деле человеческий запросы хоть мы и думаем что на самом деле это одна категория Но в среднем медиане это 4-5 разных категорий и из-за того что мы выделили большие шорты большие категории В отдельные шарды мы не выиграли с точки зрения того что нас утилизировался трафик То есть он так бы не очень сильно локально но выиграли с точки зрения того что нам не нужно париться насчет того что вдруг две большие категории попадут в одну в Один шар и начнут друг за другом друг с другом конкурировать за ресурса насколько равномерная нагрузка у вас получается по шагам получается там вплоть до того что на Один шар нагрузка в 10 раз меньше чем на другой не страдаете из-за этого то есть нам пришлось как раз факторы реплицирования делать 5 потому что есть нагруженные шарды и им нужно много реплик но при этом есть шарды менее нагруженный на самом деле Нужно меньше реплик Но чтобы не париться на Насчет того какой шарф первый какой второй и так далее мы сделали 5 реплик везде последний вопрос для того чтобы это работало Вы должны уметь по запросу понимать Каким категориям он относится как вас это реализовано категории товаров у нас поисковым индексе Видимо у меня снова не откатиться презентация Ну ладно в общем там когда есть поисковый индекс мы получаем список товаров мы получаем вместе с ними некоторые полезны одним полем является категория товара соответственно для каждого товара на транжира мы знаем его категорию и отправляем запросы было конкретный шар Спасибо вопрос как выбирать товары на базовом если их больше 1.200 как подбираете товары на базу если их больше 1200 и насколько я знаю там есть BM 25 коры и на основе них выбираются лучшие на основе этого скоро то есть мы считаем что товар лучше с точки зрения текста в релевантности если у него Скоро выше других все Спасибо за ответ и у нас следующий вопрос зала даже меня уже два микрофончик руках Привет Миша Одноклассники вот вопрос такой довольно много докладе делся акцент на то что данные заливается всего лишь раз сутки Однако кажется что ну для этих вещей такая что не очень подходит довольно скоро бизнес может захотеть обновлять данный раз 15 минут пять минут насколько ваши печастую готов к такому повороту событий готовитесь Когда будет на самом деле Да мы готовимся мы уже хотим сделать не раз сутки чаще обновлять все данные и в данном случае если на самом деле занимается другая команда которая пили свой собственный фьючерс но мы хотим их пересадить на наш и в этом случае можно будет отказаться от холодного хранилища потому что нам в случае короткой живущих данных не важна консистентность там неважно какие они там были вчера позавчера и так далее И на самом деле горячих хранилище и еще пока микрофон передают Да Кажется что вот не все те фичи которые лежат в рейдес одинаково горячие вот не думали Вы про какую-то гибридную схему например там часть хранится в редисе часть прям в кассандре не там сингаются как-то между собой что нет такое ну чтобы экономить то есть нас данные из источников подгружаются рассудки То есть я вижу у вас холодное хранилище есть но не как бы средится совсем то есть столько красотки когда данная прилетают и у нас следующий вопрос Спасибо за доклад у меня такой вопрос как фичи попадают futurestore с точки зрения этой информации То есть как вы понимаете как их считать Какой ключ будет у этой фичи вот как вы это сами делаете или это DS занимаются этим с точки зрения модели информации у нас есть отдельный сервис который я не показываю на схеме это сервис который хранит Мне эта информация То есть если вспомнить слайдется табличкой где я показываю какого товара Какие фичи на самом деле оттуда можно выразить три вектора первые векторы это будет продуктовые фичи и соответственно порядка статов мы храним и вот в этом массиве нам нужно знать только конкретный индекс конкретной фичи То есть например мы знаем что рейтинг товара лежит на позиции 10 и это значит что когда нам приходит запрос поезд Мы видим что модельки нужны такая такая фича Мы видим что ей нужно рейтинга вы учитываем данные из Реда уберем что на десятой позиции нет чуть другой вопрос был То есть например вы через в морду говорить Что например вот такой фичу А надо забирать вот из этой директории каждый день 8 утра например вот то есть это вы через морду делаете а сами значение считаются где-то слева от фича скоро вы их просто забираете к себе да значение считается Мы про это мало Что Знаем мы знаем только что их посчитали Вот они готовы и мы просто забираем готовые значения их выдаем поиск Понятно спасибо Спасибо за презентацию Меня зовут Василий компания DNS технологии два вопроса Первый вопрос На презентации было упомянуто несколько раз один из критериев это устойчивость к потере дата-центра как это обеспечивалось второй вопрос рассматривался ли Тарантул как альтернатива Radisson Спасибо первый вопрос с точки зрения отказа устойчивой потери до конца в случае их горячих и холодного хранилища нас поддерживают что допустим сестре реплики и три дата-центра каждая реплика лежит в своем дата-центре соответственно если мы теряем один дата центр мы теряем одну реплику из трех и мы можем читать данные из двух других это реализовывается в случае горячих конечностей то есть они говорят что она здесь треть реплик легла пожалуйста не считает это данные клиенты оттуда не читает данные читает из двух оставшихся в случае с холодным хранилищем спассями там точно такая же история только немцев мы на самом деле не рассматривали именно потому что в тот момент кажется еще не было активного комьюнити у тарантула Вот и остановились на этих двух получается Между центрами полная репликация идет с убд и какая-то мастер машина то есть выбирает куда запрос дойдет или как вот этот репликация на самом деле и в родительских возгресс из коробки идет то есть мы про это вообще не думаем то есть например если падает то в какой-то момент он поднимается вот эта новая надо учитывать данные из двух других если там что-то поменялось и мы об этом вообще не паримся То есть когда она уже готова к тому чтобы направлять трафик у него все уже есть спасибо спасибо за ответы на следующий вопрос не слышно Включите пожалуйста микрофон Здравствуйте У меня уточняющий вопрос получается у вас в начале вообще не было шародирования То есть у вас был было просто 18 копийцев правильно а потом вы сделали шарнирование на самом деле было но оно было можно сказать таким глупым То есть например товарных свечах что плохо влияет на локальность данных но с точки зрения трафика работает То есть у вас просто было не оптимальное шортирование получается вот и все Потом мы его оптимизировали за счет этого решился вопрос и можно сразу второй вопрос то есть вы говорите что у вас фичи объединяются в группы и для проведения об эксперимента вы делаете некую отдельную группу ее рассчитываете и ложите в отдельный шар А у вас нет такого что у вас есть некая формула ранжирования чуть выше которая выбирает раздельные фичи вы об этом не думали нет и уже делать об по формуле ранжирования формулы ранжирования мы считаем модель для каждой модели мы имеем Конфи Какие фич Ей нужны и в случае если например трафик пошёл неправильно ревизию То есть мне от этих нужнохвичей то эти фичи были меняться на какие-то дефолтные значения я правильно поняла вопрос а само формулу рассчитываете заранее получается Ну её подставляете или её в конце в самом берёте Ну вот когда abe эксперимент проводится для каждого про набор печей и вот такая матричка это подается на вход в отель и на выходе получается порядок сами модельки рассчитывают цитаты с аитистами то есть мы не берем модельки откуда-то в рам-тайме там не знаю условно раз в неделю приходит новая моделька новая тест просто вопрос был про то не думали ли вы про то чтобы именно делать об по модели то есть грубо говоря они не ложить там целый фичи То есть вы вас может быть там к примеру 12000 и 13 посчитали Ну чтобы целую группу не заводить кластер на самом деле есть такое что иногда модельки переобучаются без добавления новых печей то есть мы не хотим под каждую модель отдельно заводить базы данных При этом если появляется новые фичи то они нужны только одной модели то есть лишний раз первые данные мы не хотим без особого необходимости Ладно я понял спасибо спасибо за вопросы у нас еще один два ещё даже Спасибо большое за доклад Вопрос такой была ли у вас вообще проблема такая или может быть вы думали над ее решением Бывает так что DSM Бывает ли Так что DSM нужно иметь доступ к горячим данным но уже в историческом контексте То есть например Они понимают что там последнюю неделю напротив какая-то происходит деградация моделей и они хотят прокатить тот же самый провод но в дев среде соответственно данные в онлайн-фиторе они уже как бы обновились уже протухли там уже все поменялось они могут как-то в исторических данных забрать то что было когда-то напроде То есть если какое-то синхронизация между онлайн частью и оффлайн частью чтобы как раз можно было там диваживать проверять эту проблему сами для себя то есть когда они нам складывают данные в gtfs они на самом деле складывают их еще и бэкап то есть они сами для себя хранят данные там за последний месяц и на основе этих данных например обучают модели и соответственно если происходит какие-то проблемы на продакшене они смотрят на эти исторические данные которые сами себе уже сохранили и уже на основе них что-то решают наши онлайн-стор не подходит для этой задачи Спасибо за вопрос Анастасия здравствуйте Меня зовут Филипп У меня вопрос Следующий Озон очень быстро растет два раза год году Как вы думаете до архитектуру которую вы построили насколько ее вам хватит с точки зрения роста то есть нас может быть два вида роста либо растет количество товаров либо дата сантехниста хотят ранжировать больше товаров чем вот эти наши текущие 2000 с точки зрения роста товаров нам здесь нужно просто решартироваться То есть может быть такое что количество товаров стало больше мы перестали помещаться старые рейды старые погресса в этом случае мы завозим новые редисы новые погресса Возможно там ничего не какие-то факторы репликации и соответственно перекачиваем данные старые из старой боревизию побольше Но на самом деле для нас такая проблема Не стоит потому что на самом деле успешная тесты когда заводится новые radiss они на самом деле чаще чем нам нужно решать то есть на самом деле когда приходит новая тест мы сразу закладываемся что этот АБС может быть успешным и эти ради станции потом продал У меня и в какой-то момент могут произойти Просто рост товаров спасибо спасибо за вопрос вопросов а у нас еще один есть это последний вопрос Спасибо за доклад А скажите пожалуйста как вы очищаете ваш вич-стор от ненужных вещей потому что моделях машинного обучения используются сотни где-то 1000 фичей и может оказаться что вы фичи рассчитываете но никому никому не нужны и как вы рассказываете этот садистам что их надо удалять спасибо С точки зрения если обучили модели запустили в тест и он сам не успешным просто освобождаются которых лежали эти фичи и их можно использовать дальше для других обтестов в случае если какие-то фичи были проданы и они потом больше никому не нужны то в этом случае можно точно также как решалирование просто изменить конфликт то есть какие фичи были нужны какие фичи стали нужны и перестать их заливать либо в каком-то просто почистить нам уже не нужно Все спасибо за вопросы огромный огромный тебе благодарность поделилась с нами знаниями О сейчас надо выбрать два вопроса один лучших один подарок от онтика другой подарок от озона начнем Наверно с понтика я думаю был хороший вопрос про рост как мы готовимся к росту систему и второй у нас подарок от озона от вас второй вопрос я думаю хороший был там от девушки про то как можно чинить баги в исторических данных все рассказала Через 20 минут примерно у нас будет следующий"
}