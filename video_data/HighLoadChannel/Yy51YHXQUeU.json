{
  "video_id": "Yy51YHXQUeU",
  "channel": "HighLoadChannel",
  "title": "Краткая история NLP: от T9 до ChatGPT / Дани Эль Айясс,  Александр Абрамов (SberDevices)",
  "views": 426,
  "duration": 2959,
  "published": "2024-10-29T03:08:40-07:00",
  "text": "Давайте пригласим на сцену Дани он нам сейчас всё расскажет раз-раз коротенькая калибровка дальние ряды слышно видно нормально всё супер Да друзья рад всех сегодня приветствовать на таком замечательном мероприятии Вот в такой ещё замечательной ламповой аудитория сегодня хочется проговорить про NLP на самом деле про историю развития технологий Да и поговорим про то как мы пришли от базовой Да технологии которая начала развиваться в девяностых Т9 да до чат gpt что между ними общего Да и в целом С какими проблемами научное сообщество сталкивалась по мере развития технологий вот Давайте немножко расскажу почему у меня имеет смысл послушать Да в этом деле какой у меня опыт я сам являюсь магистром вмк мго вот как раз-таки изучал пнин поддержку вижу вот изучал пни и конкретно NLP вот там начал свой как-то первый путь Ну и на самом деле в NLP Я уже ну Уже больше 4 лет занимаюсь исключительно этой областью вот параллельно с этим работаю в Сбер девайсах занимаюсь разработкой гига чата нашего так скажем ответа да чат gpt нашей большой языковой модели Вот И вот сегодня тоже расскажу какие-то там внутренности подкорке и прочее план доклада следующий соответственно поговорим про саму задачу языкового моделирования что это зачем это почему Да поговорим про то какие модели бывают они бывают на самом деле разные статистические нейросетевые вот поговорим про бум Трансформеров Что такое трансформер Да и что за ним последовало Какое развитие вот ну и про какие-то последние самые такие актуальные вещи это что есть инструктивное до обучение и какие вызовы Да и там проблемы тренды На текущий момент стоят да Ну соответственно как я уже сказал Да есть там технология есть технология и можно зада вопросом А есть ли между ними что-то общее Ну и на самом деле тут есть прямой ответ что да между ними есть общее Это некоторая задача которую она решает так называемая задача языкового моделирования и разница глобальна лишь в том что Т9 - это как бы такая технология решения этой задачи Языково моделирования на минималках да В то время как чат gpt Это то же самое но с гораздо большим количеством Да новых там технологий ресурсов и прочего Вот и Давайте остановимся на том что такое языковое моделирование Да это вот некоторые такие исто и сама задача на самом деле не Нова то есть Несмотря на то что последнее время Она позволяет нам делать такой Прогресс Да в области обработки естественного языка некоторые её истоки Да можно датировать сорок восьм годом Вот и задача предельно простая Да это задача предсказание следующего слова то есть мы просто оцениваем вероятность следующего слова за какой-то последовательность предыдущих слов и одной из первых таких моделей Да была модель вот как раз-таки разработанная Клодом шенем это основоположник теории информации как я уже сказал в сорок восьмом году и этой моделью он показал что язык он в какой-то мере избыточно и избыточно он в том плане что как бы появление следующих слов Да оно не случайно если я вам вас попрошу Вот сказать следующее слово после фразы Привет как то с большой долей вероятности вы скажете тела Да а не какое-то случайное слово и очень интересно то как он эту модельку свою первую статистическую сделал он брал книгу открывал её В каком-то случайном месте брал оттуда слово выписывал е его в тетрадь дальше искал это же слово на какой-то другой странице и выписывал слово следующее за ним и так далее в цикле он это делал и у него получались вполне связанные тексты Да ну как минимум они были связаны с точки зрения грамматики Да и структуры языка Но там могли быть лишены смысла Вот это такие первые истоки Ну и понятно что модель такая практического применения особо не имеет Да но как бы зачатки она определённые дала Вот после чего у нас пошли Да статистические языковые модели которые использует аппарат теории вероятности и математической статистики тут такой маленький теоретический ликбез Да терминологический скорее по поводу того что вот эти модели Они часто называются энграмм нами Да грамма это просто какая-то последовательность слов в тексте То есть унигра - это просто одно какое-то слово грамма - это два последовательно идущих Ну и в общем случае грама Вот и такие модели они уже выучиваем объёме текстов Да и выглядели они следующим образом поскольку это да аппарат как я уже сказал теории вероятности математической статистики Мы хотим в целом оценить вероятность какого-то текста Да появления текста насколько он там может появиться насколько он вероятен вот Ну и понятно что просто оценить статистически Да как частота появления такой-то фразы Да делённое на количество вот слов такой фраз такой длины оно не совсем работает да и мы в целом можем придумать какую-то фразу которая ни в каких Да там документах текстах не фигурирует Но она более чем имеет место быть да ну с точки зрения там языка корректности и смысла Вот и идея этих подходов заключается в том что мы переходим от Вот раскладываем вот эту общую совместную вероятность на вероятность текущего слова при условии всех предыдущих Да ну и оцениваем понятное дело статистически как частота таких встречаний И опять-таки тут это вот часть которая before мы сталкиваемся с той же самой проблемой что самый последний да множитель мы тоже Ну не можем оценить потому что это вот вся та же самая фраза Ну минус одно слово да И нужно посчитать статистику в общем статистике может не хватать и идея статистических моделей заключается в том что давайте мы будем оценивать вероятность при условии какого-то конечного числа предыдущих слов вот тут на слайде представлена модель трёхгранные Да когда мы оцениваем вероятность текущего слова при условии двух предыдущих Вот и тут уже у нас обычно текстов хватает чтобы такую вероятность оценить вот и ну каким-то образом генерировать данные последовательности вот мы видим как мы зачёркивать всё предыдущее Да и учитываем только последние два слова Вот и как бы подход на самом деле рабочий здесь мы можем видеть пример генерации одной из таки таких моделей И если мы начнём Ну если мы не будем вчиться да смысл то мы увидим в целом что-то похожее на английский язык да где присутствует знаки припинания выглядят вроде более-менее хорошо но вот если мы как раз-таки начнём вчиться мы поймём что предложение полностью лишено смысла Ну и это связано как раз-таки с тем эффектом что мы как бы генерируем следующие слова да При условии только двух предыдущих и у нас как бы как будто смысл предложение которое мы хотим сгенерить он от нас утекает Вот и это некоторая ограниченность таких подходов то есть генерация выглядит там более-менее правдоподобно но они достаточно слабые А ну и понятно что параллельно с этим был было развитие там других технологий в частности нейросетевых Да и научное сообщество пришло к достаточно простой идее что а Давайте попробуем использовать Да нейронные сети как некоторые сильные апроксимальная модели Да которые могут выучиваем образом да то есть мы подаём в наш нейронную сеть пока Давайте представим как чёрную коробочку какой-то предыдущий Да кусок текста и будем оценивать вероятность там следующего слова и делаем Это по какому-то Ну относительно большому корпусу текстов Вот соответственно Какие модели здесь можно использовать немножко раскроем эту чёрную коробочку а и одними из первых таких нейросетевых моделей на удивление были на свёрточные нейронных сетях основано Да почему на удивление Ну тут Наверное некоторая привычно Да что свёрточные нейронные сети они были всё-таки больше про картинки Ну и на самом деле остаются про них Вот Но при этом они нашли применение в текста Вот например в этой задаче но важный момент что СВР здесь используются одномерное Не двумерные ну потому что у нас картинка двумерная Да мы как бы нашим патчи ком ходим по картинке здесь у нас как бы только одна такая сти Да Это Наша там последовательность и глобально Да поскольку мы эту модель уже выучиваем да то есть мы не просто считаем статистике А у нас есть параметры которые мы выучиваем да у нас выучиваем от ных моделей потому что вот это окошко с которым мы смотрим Оно обычно ограничено там три да слова либо пять Ну либо и так далее Ну больше обычно не берут потому что тоже уже сложно выучить Вот и дру отстранённая архитектура Да которая прямо нативно ложится под эту задачу это рекуррентные нейронные сети Ну почему они потому что рекуррентные нейронные сети они обрабатывают как бы последовательную структуру Да кое является там частым случаем текст вот Шаг за шагом и при этом у неё есть некоторый механизм памяти Да она как бы запоминает всё предыдущее что видела но запоминает настолько насколько позволяет ей вот внутреннее состояние то есть есть некоторые ограничения Ну и на слайде представлены некоторые архитектуры данной сети Да это вот обычная рекуррентная сеть которая обладает проблемами что у неё затухают либо там взрываются градиенты Да и Ну в целом она может не слишком долгие закономерности помнить Вот и более продвинутые - это СТМ дарованная нам шмидхубер и Гру там 2014 года как некоторое её улучшение Вот и к чему на самом деле вот обучение такой же модели может нас привести к чему оно привело Вот вот здесь представлены примеры генераций А вот здесь используется архитектура СТМ и обучается она посимвольно то есть мы как бы обучаем не на словах генерировать слово за слово а прямо символ за символом то есть вот прямо атом берём и Обычно она соответственно на левой части на латехе тут наверное не особо видно но как бы всё равно какие-то Да видно схожести а справа на сортах ядра линуса и мы видим что такая модель Да когда она начинает генерировать текст она начинает генерировать очень связанные Ну с точки зрения вида Да вещи на тех на который она обучалась То есть это явно похоже на код языка си Да вот это явно похоже на какой-то техов ский учебник Да но опять-таки глобальная проблема оставалась в том что если в это то оно тоже оставалось лишённым смысла потому что да Потому что контекста могло не хватать и модель тоже забывает вот какие-то предыдущие важные вещи Да но при этом Её задача продолжать дальше генерировать вот очень интересное отдельное направление которое было это визуализация нейронов то есть мы явно можем взять некоторое Вот это внутреннее состояние рекуррентной сети Да и посмотреть От каких вещей оно зависит и вот есть Прямо отдельные статьи которые это изучает вот верхняя часть слайда показывает есть отдельный нейрончик который активируется Когда у нас появляются условные выражения в языке программирования то есть синий цвет - это когда у нейрона малые значения он то есть не активен и красный когда он включается Да у него как бы состояние он понимает что я нахожусь вот в условном выражении нужно как бы генерировать что-то про это и наверное апогеем вот всего этого визуализировать от Open это по символьном она обучена на ревю из амазона то есть на товары вот ну и обычно Ну на ту же задачу Да предсказание следующее слово и что очень интересно что мы видим что модель начинает выучить Да какие-то очень сложные вещи Как сентимент как тональность То есть если вы начнёте вчиться зелёное Да это когда Нейрон активен там соответственно некоторые такие яркие Да эмоциональные Да фразы мы можем видеть как beauti developed да либо impressive и так далее То есть как бы положительная тональность Да и модель это понимает вот если мы пори смотрим на красные там мы можем видеть фразы типа absolut worst вот и что очень важно что мы обучали эту модель не решать задачу определения тональности Да а просто предсказывать следующее слово но как бы на текстах где может быть положительная и отрицательная тональность и вот здесь вот важную вещь Ну важное вещи нужно прийти Да и сообщество пришло что вот такая задача предсказания следующего слова она может быть очень полезна чтобы выучиваем Вот соответственно некоторым следующим венцом творения таких рекуррентных моделей была моделька эма Да это двунаправленная соответственно языковая модель Что значит двунаправленная что она одновременно Решала задачу предсказания следующего слова слева направо и справа налево да Ну это нужно было Для того чтобы как бы условно говоря мы не забывали что было в начале Когда идём слева направо да И вот модель которая идёт справо налево У неё как раз таки самые свежие знания идут с начала текста и она это хорошо помнит и модельку для следующей вещи то есть мы опять-таки пришли к тому что мы выучиваем какие-то зависимости которые могут помочь нам в решении наших задач Вот и Элма была такая моделька которая сделала в 2018 по-моему году да 2017 State of the Art На огромном перечне задач то есть выдала лучшее качество с помощью вот такого обучения То есть если раньше Да мы как решали задачи мы брали некоторые вектором слов либо символов да и подставляли её в нашу Task specific модельку то есть отдельная моделька для Ринга какая-то специфичная моделька для Там саммари зации и так далее то сейчас мы вместо этих векторов Да подставляем вот Элма да либо Кови - это достаточно близкая по архитектуре модель вот которая уже учитывают вот эти векторов Да контекстуального находятся Ну это помогает в решении там таких вещей как дезигуаль три да И как некоторый глагол в повелительном наклонении тереть и вот такая вещь Она помогает понять Да а вот что в данном случае это слово обозначает Вот и после того как мы эти вектором уже в нашу модель а удалось выбить СТ of the Art и соответственно эта модель Да эта статья получила награду based Paper award на naal 2017 года это одна из престижных NLP конференций Вот это про то какие были Вот первые такие важные шаги достигнуты с помощью задач языкового моделирования вот дальше 2017 год происходит появление статьи трансформер от Гугла Да это статья которая это архитектура который не использует в себе не конволюция Да не рекуррентные вот эти Да последовательны обработка текстов Да последовательности а использует так называемый механизм внимания который на самом деле появился впервые не в ней а ранее году вот 2014215 в задаче машинного перевода Да когда нам нужно было с одного языка перевести на другой И когда Мы начинали Уже переводить там где-то с середины Наш целевую текст Да мы могли уже забывать что происходило в начале там или в конце Да исходной последовательности и Механизм внимания позволял нам смотреть на весь текст одновременно также и тут но и моделька Да она изначально была для задачи переводов сделана Вот и Как вы уже знаете я думаю что сейчас Трансформеры они не только Natural Language Processing монополизировать А и компьютерное зрение и обработку звука и там перечень других областей вот а соответственно появляется некоторая новая архитектура Да которая выглядит очень хорошо то есть там каждое слово смотрит на каждое другое и мы можем ещё лучше улавливать контекст Да в отличие от рекуррентных сетей которые имеют свойства и в 2018 году достаточно вскоре после появления Трансформера выходит статья gpt пер опять-таки от ребят из Open и у них была предельно простая идея А давайте вот эту новую перспективную архитектуру возьмём и попробуем решить до более нам знакомую Да задачу языкового моделирования просто предсказание следующего текста следующего слова в тексте Вот и они сделали достаточно успешно Вот вот там показано как раз таки метрики да которые они получили если до этого были какие-то Ну там ряд разных зада Да вот у них получился прямо сильный скачок в качестве они Только две задачи не побили но в любом случае это большой Прогресс вот тут Они использовали 4 гиб текстов и в данном случае вот декодер этого Трансформера потому что он позволяет вот генерировать тексты Да и в целом тут тоже меняется некоторая парадигма что мы теперь имеем некоторую предо модель да и с помощью каких-то линейных слоёв просто дооб Буча её под нашу любую задачу целевую То есть у нас уже нету каких-то специфичных моделей Да у нас просто добавляется Линейная проекция под какую-то задачу которую мы хотим решить вот немножко другим путём пошли ребята из Google и на самом деле в том же году они выпустили модельку ber которые шифруется bal encer ой eding representation From Transformer и идея заключается в том что они взяли не декодер трансформер Да который смотрит только на предыдущие слова а взяли энкодер который смотрит каждое слово смотрит на каждое другое Почему это не сделали Open Ну потому что классическую задачу Языково моделирования так решить нельзя то есть мы можем смотреть в будущее Да и задача как бы лишается смысла то есть мы как бы смотря на всю последовательность мы не можем предсказывать следующее слово мы его прямо явно Витим и ребятам пришлось немножко подумать из Гугла Да и они дали некоторую альтернативную задачу так называемое маскированная языковое моделирование когда они берут текст э маскируют некоторые слова оттуда Да вот прямо специальным токеном Маск и задача модели восстановить эти слова которые там были Ну и задача Она тоже на самом деле не тривиальная она требует от модели понимать да как бы весь текст читать его по смыслу понять чтобы закодировать А что было скрыто Вот Но при этом парадигма остаётся той же самой то что после того как мы такую модельку предолимпийский слова Да они лучше соответственно кодируются потому что мы смотрим на все другие вот чуть-чуть поменяв задачу ну и внизу показано как вот выглядит В чём отличие как раз-таки gpt от Берта Да что Берт повторюсь каждое на каждое смотрит слово gpt только на предыдущее чтобы мы могли решать задачу предсказания следующего слова Вот и вот это второй сдвиг парадигмы причём на обеих этих моделях если до этого как мы обсудили были вот модель Элма Да и специфичная какая-то под нашу задачу другая модель то теперь мы приходим к тому что у нас есть некоторый такой мастер на все руки да универсальная модель которая просто берёт и там может спокойно до обучиться причём на малом количестве данных под какую-то другую задачу это опять некоторое подтверждение того что в ходе предобзор про какие-то факты о мире что позволяет ей как бы адаптироваться очень быстро под целевые наши задачи вот что происходит Дальше уже девятнадцатый год Open пошли в так называемые масштабирования да да то есть gpt в она архитектурно Ну минимально отличается от jpt первой То есть это также декодер Трансформера но как бы начали развиваться Ну ещё и до этого и ну в это же время соответственно вычислительные ресурсы Да конкретно видеокарты и они обучили модельку побольше Ну на самом деле они обучили серию моделей там четыре штук вот где gpt Первая это вот как того же размера что gpt2 Small - это 117 млн параметров и gpt втора Extra large самая большая на по млрд параметров то есть мы как бы прям сильно увеличиваем мер модели Ну и предполагаем что у неё как бы увеличивается ёмкость запомнить более какие-то нетривиальные сложные зависимости и также увеличиваем второй показатель предельно важный это данные То есть если мы обучали gpt пер на 4 Гб то тут как бы на порядок больше уже на 40 Гб И что происходит Дальше возникает новый эффект возникает новый эффект который Ну назвали Zot он заключается в том что мы теперь можем задачу решить не с помощью до обучения Да а до обучения предполагает соответственно вычисление градиента Да обновления параметров а просто тем что мы формулируем задачи на естественном языке как показано тут Да translate English To French то есть даём некоторый Task description и потом подаём ей тот пример который мы хотим чтобы она обработала Да там чиз стрелочка и просим как раз-таки в режиме предсказание следующего слова сказать А что идёт дальше Вот и на самом деле уже в такой парадигме да за счёт увеличения как размера модели соответственно её способности и данных а тут нету слайда про это Но там они показывают что наряд датасета они уже в такой парадигме без до обучения тоже выбивают самые там большие значения метрик gpt-3 она на самом деле продолжает иде gpt2 тут в какой-то мере можно Ну не знаю возразить что кажется тут нету там научной какой-то навесни да то есть мы просто идём в масштаб Но на самом деле это вот привело как раз-таки к тем эффектам которые вот я сейчас дальше покажу gpt 3 - это увеличенная gpt2 увеличена она опять-таки как по размеру данных там используется 570 ГБ Ну то есть уже тоже на порядок так больше И вот что показывает данная картинка а они увеличили сильно модель Вот но там сходу да не очевидно что увеличение моделе оно может прямо как-то очень сильно повлиять и ребята из Open начали обучать разные модели да то есть каждая кривая соответствует э разным модели Да разного размера Где самые верхние такие синенькие фиолетовые - это модели размера там 10 в пя количество параметров да то есть сотни тысяч и жёлтая моделька - Это вот их флагманская на 175 уже миллиардов параметров Ну если при в какой-то такой компьютерный эквивалент то это 650 гиб соответственно памяти То есть это прямо огромная модель Ну и чтобы обучить её очевидно нужно куча просто ресурсов Ну и по каким-то таким достаточно интересным не то что исследованиям да а там волонтёры начали смотреть А сколько примерно такая модель может стоить даже вот если брать из клауда самое дешёвое железо то стоимость обучения такой модели была 4,7 млн долларов ну по самым таким скромным Почём Вот Но при этом к чему мы приходим мы приходим ещё там к более новым эффектом это 2020 год приходим уже не КТУ который открылся в gpt2 а КТУ и к такому термину как in Learning заключается он в том что теперь мы да Ну тут Не очень видно конечно Но идея заключается в том что Вот вместо того чтобы подать описание задачи и то что мы хотим решить мы можем подать примеры того как эта задача уже решается То есть просто сделать примеры того как ну вот перево Да из одного слова в другое То есть когда мы один такой примерв описанием задачи и тем что мы хотим предсказать OT - это соответственно когда мы несколько таких примеров добавляем Ну и здесь показано что это альтернативно тому как мы делаем классический Файн тюнинг когда на каждом таком сплетаемся параметров то есть важный момент что никакого обучения модели с точки зрения изменения параметров не идёт но происходит обучение в так называемом контексте да то есть она видела очень много данных она видела какие-то учебники и когда мы вот даём такие примеры Да translate English какие-то пары там как одно в другой переводится это может позволить ей лучше попасть в контекст тех данных которые она видела то есть она видела какой-то учебник где есть Вот что-то похожее и это помогает ей улучшить качество предсказания вот ну и конкретно это выглядит следующим образом соответственно здесь по icx отложено вот количество этих примеров которые используются То есть как бы самая левая часть - это zer Shot потом идёт шт когда один пример да И ьшт когда мы там до дети условно говоря даём и разным цветом выделены разные модели 1,3 млрд 133 млрд и 175 млрд параметров и мы видим что на вот такой ьшт парадигме да у нас существенно увеличивается качество работы модели Ну и увеличивается оно не пропорционально там размеру модели Да ну то есть на Большой модели оно прямо сильно начинает стрелять ещё очень интересно тут же вот для каждого цвета два графика один пунктирный один целый вот целый - это тот который соответствует мпту Да текстовой затравки которая содержит описание задачи А пмт когда нету описания задачи то есть мы просто подаём примеры там условно говоря слово на английском стрелочка слова на английском но не просим перевести и модели этого уже достаточно чтобы понять Да вот что мы от неё просим то есть вот что нужно вот в таком же режиме продолжить генерировать Вот и но тут как бы есть ряд проблем с этой моделью что моделька как бы очень большая да и Ну там если говорить про какое-то промышленное железо это ну очень тяжело поднять То есть это для исключительно каких-то гигантов типа и там Microsoft Google и так далее Вот и на самом деле Кроме размеров модели есть ряд других проблем что даже такого юшта Для каких-то более сложных задач может не хватать Да но тут мы опять-таки можем остаться в парадигме что давайте мы её прямо градиентное ну и это имеет место быть но опять-таки до обучать модель на 175 млрд параметров это ну Очень дорогое удовольствие причём под одну какую-то задачу и вторая важная вещь которую тоже стоит осознать это Что продолжать генерировать следующее слово это не то же самое что следовать инструкции А если мы говорим про решение какой-то задаче Да Допустим даже про вот эту затравку Переведи с английского на французский Мы даём некоторую инструкцию которой хотим чтобы модель ей следовала вот и это не одно и тоже И здесь на выручку нам пришло инструктивное до обучение то есть в левой части слайда показано то как мы мыслили до этого Да но уже с учётом вот этих больших моделей то есть в случае там Сбер Т5 мы брали предо модель до обучали Градиент е под нашу какую-то задачу А и делали как бы решение этой задачи А в случае вот этот пром когда мы игрались с самой текстовой травкой да могли поменять разные примеры Ну чтобы добиться максимального качества но опять-таки говоря про одну задачу Вот и в статье Если не ошибаюсь 2022 года было предложено Вот такое инструктивное до обучение и датасет флан это вот датасет который содержит как раз-таки задачи с разными инструкциями и идея заключается в том что давайте мы попробуем сделать универсальную модель не заточенную под одну задачу и уйдём от предсказания просто следующего слова к следованию инструкции Вот то есть мы е доум на конструктивно на задачу там B C и D и она будет обобщать как бы и решать задачу А вот вопрос остаётся в том откуда брать данные такие инструктивные то есть явно мы можем пойти таким самым лобовым путём нанять там кучу асессоров разметчик целый штаб Да и попросить их генерировать сами задачи и правильные ответы на них но то как было Вот предложено это очень такой изящный способ он выглядит следующим образом то есть Давайте идейно ещё закину у нас явно за историю развития NLP да появилось очень много дасе вот Даже те которые были представлены там для gt1 Да на которых можно обучать модели и сравнивать их между собой вот и это некоторый труд да который уже ну был который уже был пройден да то есть эти датасеты собраны и идея заключается в том что давайте мы попробуем использовать эти датасеты где у нас есть некоторые да X на вход модели и некоторые Y которые мы хотим предсказать для порождения этих инструкций Вот вот здесь как раз таки представлен такой пример Это задача ко двум предложениям должны сказать является ли одно продолжением другого да Либо они никак не связаны Вот то есть прес - это как бы первое предложение это вторая и Target там Yes да как бы продолжение или нет И мы просто можем взять этот датасет и использовать для порождения там нескольких инструкции по разным шаблонам то есть Нам нужно прописать только шаблон допустим вот template One мы подставляем вот этот прес дальше даём инструкцию подставляем гипотез знак вопроса Да и соответственно дальше наша Ну ответ который мы знаем уже из датасета то есть мы никак не прибегаем к работе соответственно разметчик Да мы просто придумали какой-то шаблон и просто взяли датасет с помощью которого породили инструкции Ну и тут уже вопрос креативности что мы можем породить много таких разных темплейт которые представлены на слайде вот что ещё более Интересно что мы один датасет можем использовать для порождения инструкции для разных задач здесь уже другой датасет это если не ошибаюсь Amazon review То есть это Вот датасет который как раз таки отзывы по амазону и здесь мы опять-таки делаем некоторый текстовый шаблон Да где мы соответственно там для задач классификации given The following review подставляем наш sle да review Body predict The associated Rating Ну и так далее подставляем какой-то ответ вот мы можем пойти от обратного то есть теперь мы хотим предсказать не то сколько этот отзыв получит Да по шкале от одного до пяти а запрос Мы хотим сказать сгенерируйте пример того что надо генерировать это вот задача текстовой генерации когда мы просто меняем Таргет сксом вот ну и в данном датасете конкретно там есть два типа ревю один короткий да там на пару условно говоря слов там ну либо побольше и другой прямо полный большой и мы можем опять-таки эту историю использовать для задачи саммари зации когда мы опять-таки даём некоторую инструкцию Да Give Short sing following product RW даём полное ревю и дальше соответственно ревю которая является короткой Вот и некоторые таки самый такой передовой моделькой Да общего назначения на момент двадцать второго года стала модель Н5 Она до обч Ну тут Не очень видно на может быть Не очень видно на 1000800 задач то есть использовалось 500 датасет с помощью них были порождены Вот как раз таки различные задачи в данном случае 18800 и они до обучены причём в двух разных форматах были сделаны инструкции которые Ну просто инструктивные да условно говоря вот тея задача и вот ответ и так называемые CH ofs инструкции Да которые позволяют прийти к ответу не сразу модели да некоторая цепочка размышлений Шаг за шагом условно говоря как мы в школе решали задачи там первое действие второе действие третье вот с помощью Вот таких темпле были порождены ещё вот такие инструкции и моделька была обучена Ну и как бы следствием было что модель обобщается на другие задачи которых не было Вот в этом инструктивно обучении и она хорошо следует в целым инструкциям на там большой перечень задач просто сформулированных на естественном языке здесь приме Трик Да мы видим что тут сравнивается gt3 на 175 млр лты это когда просто описание задачи красненький - это и синенький - этон на3 Да на 137 млрд параметров то есть моделька поменьше и что важно в шоте то есть мы не добавляем никаких примерах и мы видим что такое инструктивное до обучение Да оно приводит к тому что у нас как бы метрики получаются лучше причём на разном перечне задач вот мы получили на самом деле уже на этом этапе такую модельку общего назначения Угу Да всё классно но на самом деле остаются проблемы и я думаю многие из вас слышали Особенно это хорошо подхватывают СМИ Да какие-то скандальная история когда модель начала оскорблять там кого-то да либо какую-то Ну токсичность генерировать Вот то есть это оставшиеся проблемы они не решаются инструктивно до обучением и даже такая доо обученная моделька она может ну как бы начать вести себя Ну не совсем так да повторюсь это вот токсичность некий агрессивный язык Да там и может выдавать ложную и опасную информацию это связано на самом деле с тем что на этапе предо первый Да вот те самые 570 ГБ gpt-3 они могут содержать и на самом деле содержат очень разные данные да то есть их на самом деле фильтруют вот вычищать оттуда плохие вещи Но поскольку это просто скра влей Интернет да А в интернете мы сами понимаем есть разные форумы в форумах могут ссориться там скандалить и прочее да и моделька всё-таки в каки какие-то эти закономерности может Ну тоже запоминать и в какой-то момент она может начать их выдавать и что мы хотим Да ну мы хотим эти проблемы решить то есть мы хотим чтобы ответы были полезные чтобы ответы были честны Ну и безобидный чтобы Да никого не пытались там оскорбить Вот и здесь как раз-таки приходит опять-таки Open с моделью instruct gpt и такой технологии как RF RF расшифровывается как reinforcement Learning From Human Feedback То есть это обучение с подкреплением с помощью фидбека человека и идейно Да идея достаточно простая что мы как бы хотим как бы приблизить модель то как она работает с ценностями человека Да что для нас важно Это опять-таки те самые пункты да это честность безобидно правдивость Вот и делают они следующим образом то есть п One первый шаг - это самое обычное инструктивное до обучение то есть они просто взяли gp3 и обучили её на каких-то инструкциях но как мы с вами обсудили Да проблемы всё равно остаются и вот шаг второй шаг третий - это вот уже непосредственно сам че идея заключается в следующем что как мы можем свести модель к тому чтобы её ценности соответствовали с ценностям человека Ну на самом деле с помощью включения человека в этот процесс то есть мы можем взять модель Да и сгенерировать несколько различ ри ответо на какую-то текстовую затравку на какую-то задачу попросить человека оценить Да и соответственно обучить её на каком-то самом лучшем ответе да то есть просто у нас ранжируются ответ мы понимаем Какой лучший и хотим почить модель взаимодействовать именно в таком формате но мы понимаем что если мы начнём таким образом добу модельку у нас как бы человек будет Вот включён в процесс и это будет некоторое бутылочное горлышко и что предлагается на шаге д после того как мы используем нашу модель Да берм некоторую текстовую затравку генерируем множество отве мы просим людей асессоров аранжировать Да какой ответ лучше какой хуже и переходим к тому что мы можем обучить некоторую другую модельку Да которая будет брать соответственно задачу ответ и по ним говорить Насколько этот ответ хороший то есть мы по факту просто дистиллируем в модель предпочтение человека и на шаге три мы уже как бы исключаем человека из цикла и просто берём нашу исходную модельку генерируем ответ просим вторую вот эту модель на шаге 2 до обученную оценить насколько он хорош и если он хорош мы по мо генерировать вот такие вещи Вот почему здесь нужно обучение с подкреплением Да откуда он тут берётся Ну это связано с тем что поскольку у нас есть вот этот шаг генерации ответов Да он не является непрерывным это некоторая дискретная операция поэтому нету градиента да у нас он не может обратно течь и идея заключается в том что да это некоторый способ оптимизировать Да не дифференцируемые функции потерь не дифференцируемые метрики вот здесь представле как ки заме человека по шкале от одного до семи насколько им ответ этот нравится ну по иксу показан размер модели мы видим самый нижни - это обычная gpt Да которая не промвала достаточно маленький скор дальше идёт gpt PROMT это когда мы добавляем уже вот этот ш Да и модель начинает как бы понимать что мы от неё хотим чуть лучше следующий шаг - это сун это то самое инструктивное Да обучение Мы видим что происходит ещ Да скачок того что ответ начинает нравиться человеку Ну и вот Последний пункт gpt модель получает там по семибалльная такие ответы особенно на МПТИ версии мы это видим Вот inst gpt - это прямо конкретная статья с которой можно ознакомиться со всеми деталями Как они это делали в то время как Chat gpt по ней статьи нету да но есть как бы сервис есть вот эта известная веб мордочка а глобально да В чём отличие между instr gpt и Chat gpt они очень близки Отличие в том включается что инструкции в Chat gpt используются диалоговые да то есть это не просто вопрос-ответ А некоторый Диалог да который может там ну затянуться на несколько реплик вот ну и Разумеется немалую роль сыграло появление вот этой веб мордочки Да которая позволила иметь очень простой доступ не через IP разработчику а любому пользователю просто зайти в браузер Да пообщаться с моделью Что дало некоторый такой бу и Форс Вот теперь уже такая новейшая история появляется gpt 4 по которой тоже нет статьи они так последнее время всё более и более закрывает технологию вот свою очень сильную Вот Но мы видим что произошёл очень Большой скачок по Метрика Вот но скорее всего опять-таки там вещи связаны с увеличением размера датасета Да с очисткой данных с увеличением размера модели но как бы точных цифр мы не знаем но при этом я остановлюсь на несколько таких важных моментах Первое - это что модель gp4 Она может работать с картинками и она работает на самом деле тоже в таком инструктивно формате То есть это некоторое отдельное направление Visual instruction Tuning Да когда мы используем то есть мы можем задать некоторый вопрос да или инструкцию по картинке и делается это следующим образом это некоторый такой общий взгляд да не конкретный имплементации что мы берём нашу картинку и с помощью какого-то Vis энкодера Да с помощью какой-то картиной модели мы можем просто представить её в виде вектором будет не особо понятен модели то есть условно говоря слово Чебурашка да И для него ти и для картинки Да с Чебурашкой они будут разные поэтому добавляются слой оранжевый который будет обучаться в ходе модели Да и он будет переводить один Вектор в тот Вектор который понятен ской модели Ну и дальше Мы также подаём инструкцию и вот эта вся история до обучается то есть мы обучили модель что она теперь может принимать на вход картинки понимать их и также обрабатывать вот другая достаточно интересная вещь Ну то что мы официально не знаем что есть ча gpt Да там gt4 есть некоторый тако доста авторитетный сли вре на самом деле не одна модель а восемь экспертов которые объединены в одну такую архитектуру причём там каждый эксперт по 220 млрд параметров то есть Самар там openi перевали и приблизились к 2 триллиона параметров на модель и вот здесь вот показана концепция того как этот mixture of experts может выглядеть А И на самом деле тоже идея Не новая эта идея уже лет 30 она ещё в девяностых годах датируется там статьями хинтон и здесь конкретно реализация Switch transform - это первая моделька которая перевалила как раз-таки за размер в триллион параметров то есть мы теперь в модели трансформер вместо того чтобы использовать один фва слой Да мы их инрум несколько разных вот которые будут вот этими самыми экспертами которые будут выучиваем подавать на все этих экспертов то мы придём к тому что время работы модели ну оно многократно увеличится И это не то что мы хотим то есть мы хотим попытаться увеличить возможности модели Да но при этом скорость её работы сохранить прежние и идея заключается в том что мы используем некоторый роутер который будет выбирать А вот текущее да слово текущую фразу Куда нужно соответственно маршрути Зро Так ну мы на самом деле уже по времени начинаем приближаться к концу давайте я Коротко Охарактеризуй Какие вызовы и тренды есть вот ну если говорить про вызовы Я думаю все вы знакомы с галлюцинациями вот эта история сейчас тоже пытается активно решаться объединением вот генеративной модели с какими-то экстрактивные Да модулями вот не интерпретируемые как модель работает но тут частично может помочь Chain of SS Да когда модель просим Шаг за шагом сгенерировать почему она к такому ответу пришла вот утрата актуальных данных что кажется модель нужно постоянно обучать но оно решается обычно тоже добавлением каких-то внешних модулей там с информацией куда модель может обращаться получать информацию Ну и ограниченная длина контекста что мы как бы ну не не любой текст для любого размера можем взять Ну и тренды Да это мультимодальной что сейчас все ну многие работают чтобы объединить не только даже картинки и текста А ещё и аудио и прочее вот мультиэкспозиция Спасибо Дани Я уверен у нас куча вопросов за два лучших вопроса предусмотрен приз от Сбер девайсов и приз от партнёра Газпромнефть Давайте задавать Спасибо за доклад Сергей нака Ну собственно два вопроса Первый вопрос - Это насколько я знаю чап обучали 3 года вот эту модельку 3 года вот а что будет в будущем Если она сама начнёт обучаться и сама переписывать себя то есть мы е уснём была четвёртая версия просыпаемся третья уже написала что с будущем будет с людьми так сказать пока мы её Учим и второй вопрос а с информационной безопасностью всё-таки мы пишем мты вот в частности это бывает там э код мы пишем ну всякие там кодекс кодми и так далее Вот и Они отправляются куда то вот как влияет собственно вот этот искуственный интеллект на информационную безопасность да Хорошие вопросы Ну что касается первого на самом деле Сама там Т gpt она обучалась не 3 года но условно говоря вся инфраструктура для подготовки её обучения и четвёртое Ну условно говоря это вот вся хронология Да начиная с jpt перво с восемнадцатого года это всё вот да итерационные подходы к тому чтобы вот появился это некоторый Венец творения на текущий момент вот а что касается того что она начнёт сама себя да обучать Ну тут вопрос наверное да из русла там захватит ли SkyNet фантастики Да SkyNet да-да Да SkyNet Вот это всё очень важный момент что вот эта часть с рели чеф Да мы как бы модель явно после того как мы сделали её прямо мощной Да когда она способна ответить на всё что угодно вплоть до того что рецепт какой-то там не знаю коктейля Молотова да либо ещё что похуже вот мы как бы Хотим её привести вот с выровнять с ценностями человека то есть это вот важный шаг который уводит модельку от того чтобы она там ушла в какой-то скажем так разнос да и мы поощряем её делать Ну как бы такие морально правильные ответы Да помогать человеку то есть мы делаем человека центри человека ориентированный вот поэтому На текущий момент Ну вот лично у меня вот эти вещи не вызывают каких-то Да опасений вот то есть мы явно её обучаем социальне скажем так Судного дня не будет пока нет пока нет вот и на второй вопрос да Отвечу а очень хороший на самом деле вопрос про то что ну у нас просто данные могут утека А многие компании Они обладают как чувствительными данными Да так и персонализированные которые не должны куда-то утека Ну и в какой-то мере Это ответ на то почему большие компании кто может себе позволить они идут в разработку собст которые будут жить в закрытом свом контуре и как бы мы явно понимаем что данные никуда не уходят они защищены вс хорошо вот и да то есть несмотря на популярность там таких моделей у обычных пользователей Да в формате просто пообщаться посмеяться Ну бизнес далеко не всегда готовы платить даже вот за такую технологию именно вот из-за этого фактора изза фактора утечки чувствительности данных что ушло в сервера скорее всего Оно там и остатся И это не совсем рооно рения gpt 4 - это священный граль Да скрытый там Тайный что если мы эту модельку будем раскатывать на других серверах её просто как бы ну там заберут и всё спасибо можно да Вопрос Спасибо большое за доклад Да я здесь вижу А у меня такой вопрос А есть ли какой-то класс языковых задач которые очень-очень плохо даются вот языковым современным моделям то есть самые худшие Ну на самом деле это вот самый дефолтная версия Да изы моделирования это фактологический вопросы да то есть Несмотря на то что модель обладает большой ёмкостью да то есть и в этой ёмкости она где-то хранит какие-то факты Да она всё равно продолжает ционис направление с которым борется то есть обычно Ну вот есть такой подход retrial аргумента Generation Да когда мы Ну условно говоря можем дать модели возможность пойти в тот же Google взять оттуда что-то потенциально релевантно добавить вот в эту текстовую затравку Да и уже с учётом этой информации сгенерировать что как бы позволит ей меньше генерировать Ну галлюцинации Да и неправда Вот наверное да вот такие фактологический вещи они достаточно трудно и вот задачи связанные с творчеством с креативностью на удивление они решаются Как раз-таки хорошо потому что они не требуют запоминания Да а требуют Ну какой-то да генерации там креативности и видя столько текстов модель к этому Ну выучиваем и приходит а это правда что gpt очень хреново шутит и Юмор у неё плохой ну юмор - это на самом деле сложный такой достаточно конструкт Да вот как его формализовать математически очень трудно но как бы она пытается что-то юмори Да но вот я лично когда пробовал каких-то прям Вау но это не только у gpt это других моделей с юмором как бы не очень у людей такая же проблема бывает Приветствую я справа Ага вижу а меня зовут Пётр компания русал и я хотел задать вопрос про будущее и обучение корпоративных как раз больших моделей вот как Вы сказали это доста стоящий процесс который требует не только денег Но и специалистов и времени и вот как мы видим развитие идёт тоже в сторону увеличения связи увеличения объёмов данных и усложнения а вот с точки зрения цены и окупаемости всех этих условий Как вы видите это тоже пойдёт в увеличение или как всё то что развивается оно всё-таки будет склоняться к удешевлению и более окупаемости там более доступ Хороший вопрос вот на самом деле один из последних трендо то есть изначально мы можем видеть что в 2020 году тренд - это просто увеличение масштабов модели Да но увеличение масштабов модели приводит их соответствующим Да финансовым там затратам Но вот последние тренды показывают то что сообщество идёт к более компактным моделям которые просто доо обучены Либо на данных почище на данных получше более качественных И на самом деле подольше Вот то есть есть ряд статей которые изучают А что влияет на финальное качество модели и это не только размер Да это такие факторы как количество вычислений которые мы тратим модели и количество данных вот поэтому понимаю что gp3 с точки зрения inference Да её работы не то что обучение могут позволить себе очень Малое количество компаний Да ну как бы практически мы идём к всё меньшим Ну не к меньшим и меньшим моделям но модель сильно меньше размерности которые сопоставимы Да по производительности вот так у меня мысль утекла немножко дадада то есть явно все эти вещи да да да да Ну и на самом деле Большое спасибо тем компаниям которые свои Трей да то есть вот первый шаг он на самом деле вычислитель самый дорогой то есть инструктивное до обучение и чев они ну по сумме дешевле вот есть модели которые просто лежат в том же гин фейсе в открытом доступе допустим модель там Лама от Фейсбука rp3 она тоже вот от Сбер наша лежит в открытом доступе то есть потенциально эти модели можно брать и дооб учить уже под свои задачи миновав самый дорогой шаг вот вот это важная история Спасибо Да ещё один вопрос в контексте недавней истории сном вопрос не вижу здесь в контекст недавно истории с Альма мы кото все слышали и утечек по кустар предположим они добились что было сереной пулей на самом дет Можно вопрос такой философский в том плане добились эффект искусственного интеллекта Да который заключается в том что вот в пятидесятых годах если спросить у человека что вот система которая победит шахматы да человека будет ли это прям H искусственный интеллект Ну и ответом на самом деле Ну был бы ответ да типа это прям сложная система Круто Ну и мы знаем исторически Да что Каспарова победили в шахматы Вот и после этого как бы пошла риторика другая что ну как бы это не то да это не это не совсем вот когда там усло модель победит в Да которая комбинатор более сложная задача да более сложная игра то тогда это уже будет какой-то сильный искусственный интеллект пятнадцатый год да De соответственно выпускает Альфа Go который выигрывает чемпиона мира и пошла риторика что Ну у вас там просто обучение с подкреплением Да но это не вот поэтому там agi он постоянно от нас утекает да то есть как бы Всё дальше и дальше то есть мы когда приходим к какому-то новому рубежу всё Мы как бы его отдаляемся это какая система которая будет некото мо задаю вопрос в контексте появления уровня обобщения нужной степени мы теряем этот эфект как бы в численном выражении для себя то есть мы знаем что если дать больше данных там да взять больше параметров Да он появится но ВТО если они сделали его то что на твой взгляд послужило этой вот как бы как я сказал Серебряной пулей вот точкой как бы вот просто на твои ощущение Сейчас возьму немножко пару секунд на подумать Ну то есть явно когда вот там появилось gp3 у меня были некоторые там внутренние возражения Да что это мне казалось Тупиковая ветвь развития она не научно Да это просто вот брутфорс масштабирование мне кажется какие-то вещи связанные с планированием Я знаю что Google сейчас своей модели Джени они как раз-таки идут в эту сторону чтобы научить модель вот такому какому-то планированию Да там краткосрочному среднесрочного и вот это уже позволит модели как-то более функционировать Да приближённо к человеку заниматься каким-то там горизонтом временным Вот наверное какие-то такие вещи когда они вложат в модель это уже будет но это тоже не про то что это будет agi Да но это будет некоторый хороший инкрементальный Шаг вперёд а Дани нам к сожалению надо по времени сейчас уже заканчивать с вопросами нет нам к сожалению нужно дальнейший вопрос переве в кулуары А сейчас Дане нужно выбрать Кто у нас задал лучший вопрос чтобы подарить подарок мне понравился вопрос про вот оптимизацию модели куда это помнить Итак это ещё не всё второй вопрос кто задал второй вопрос лучший Давайте про Сильвер булет хороо Спасибо болье вам тоже дам пое конференции Да ну и такой маленький моменти тоже хочу сказать буду рад там подискутировать пообщаться Вот я там же есть гдето дискуссионный коне дате продолжать дискуссию Дани здесь и пого буду ра пообщаться подискутировать подходить задавать вос Спасибо"
}