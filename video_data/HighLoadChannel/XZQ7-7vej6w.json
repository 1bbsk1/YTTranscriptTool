{
  "video_id": "XZQ7-7vej6w",
  "channel": "HighLoadChannel",
  "title": "Наш опыт с Kubernetes в небольших проектах / Дмитрий Столяров (Флант)",
  "views": 74990,
  "duration": 3564,
  "published": "2018-01-16T12:35:38-08:00",
  "text": "приветствуем дмитрий столяров спасибо большое окей значит меня зовут дмитрий столяров я технический директор и соучредитель компании флант чем занимается наша компания рассказывать не буду вы можете зайти на сайт почитайте вы можете прийти к нам на стенд мы расскажем но если вкратце мы занимаемся двop самые ну давайте по-простому внедряем губернатор если вам нужно помочь купер нас из интернета со мы вам поможем доклад называется наш опыт с небольшими проектами в cabernet что такое небольшие проекты смотрите современная версия актуальная последняя версия губернатор масштабируется до 5000 узлов и 150 тысяч контейнеров это очень много ну то есть я не думаю что здесь я сейчас спрошу у кого проект занимает 5 тысяч серваков тут кто-нибудь может кто-то поднимет руку на но это реально очень много я сделал логарифмическую шкалу такой же простую маленький средний большой 50500 5000 верхний предел ну и под небольшими мы будем понимать вот маленькие среднее но стоит понимать что у меня и у компании fanta опыт эксплуатации средних практически нет потому что 50 серваков у железно это но это тоже не мало про большие ну все понимаю но маленьких проектов она сейчас купер notes я не знаю сколько чтобы на десяток наверное ну то есть виду того что наша компания занимается аутсорсингом и внедряет дом разным клиентам у нас много инсталляции губернатор ok доклад будет в трех частях по ходу времени на вопросы не останется я сейчас буду говорить очень быстро потому что я сделал 350 слайдов опять и очень хочу вам все это рассказать первая часть будет немножко про предысторию зачем вообще этот губернатора чего было до губернатор вторая часть будет о том что такой губернатор я немножко расскажу вообще как он работает а третья часть будет как раз про наш опыт я расскажу как мы его используем ну какие у нас инсталляции как это выглядит какой у нас опыт получается и так часть 1 предыстория с чего все начиналось у нас был админ и был сервер на сервер был бы тем с базой ну и туда ходил трафик со временем лампа простого нам стало не хватать и нам понадобился крон но всем понятно да зачем это чуть позже у нас booking начал тормозить мы захотели что то запиши ровать еще чуть позже мы вспомнили что нас есть медленные клиенты что нам надо держать много коннектов и мы поставили джинкс хорошо еще чуть позже у нас появилась функциональное требование мы захотели хранить файлики мы сделали очень просто мы сделали папочку с каким-то туда складывали файлики с фронтам давали даже с фронта давали уже неплохо потом выяснил страшном периодически нужно делать какие-то сложными численные задачи и мы на них ответить сразу клиенту не можем ожидать составлять ленты ждать долго может быть тайм-аут могут быть проблемы нам понадобилось очередь нам понадобились бы гром / и потом наша база выросла и понадобился полнотекстовый поиск мы прикрутили индексацию с винкс и в целом вот такая инфраструктура проекта плюс-минус покрывает большую часть задач ну потому что все согласно то что это в целом там типовой проект напоминаю что мы до сих пор на одном сервере не нужна виртуальная машина железка все еще на одном сервере какие проблемы ну понятно что эта штука масштабируется только вертикально но соответственно нужно большой большой большой серый но не эта проблема проблема у такой инфраструктуры с моей точки зрения самое главное это то что сервисы не изолированы друг от друга если вам придет прилично трафика не знаю неважно что вас на букинге ну если у вас бэкенд на маленьком fpm но какой-то applications сервер который более-менее умеет управлять своими ресурсами еще все хорошо если это какой-то пачку можешь просто вырасти по памяти и не знаю прибить базу данных база данных перед опуститься ну как привить вырастить по памяти вам киллер пеллет место викенда базу данных база данных перезапустится с холодным кэшем у вас будет запросов очень медленно отвечать опять apache разрастется опять все тубе надписи так ну как бы такие инфраструктуры классически там очень много снежных комов ну и понятно что тут нет никакой отказоустойчивости окей что мы делаем с таким проектам правильно мы нанимаем 2 админов еще к тому к первому что они делают они вытаскивают нашего железного серого уже большого железного сервера отдельно базу потом делают своих потом мы им говорим ребята где вообще отказоустойчивость не горят сейчас сделаем отдельный applications сервер вытащим туда быть and want and макияж вытаскивают выясняется что фариг это у нас лежат на первом applications сервере они вообще не проблема часов а сам пожарим ну какой отказоустойчивость понимаете а еще выясняется что трафик нужно на оба фронта закидывать а там какие-то пью карпа надо делать verb сложно поэтому они делают вот так но человек трафик приходит на один фронтон рассказывается потому букин нам уже неплохо потом выясняется что один applications сервер загружен сильнее другого не начинают сервиса перекидывается одного на другой говорят что надо купить отдельную железку под сторож перекидывает редис на базу зачем он там потому что там были ресурсы перекидывают кроны нас laiv и обычно такая инфраструктура заканчивается тем что мы наверное стать сервера ставим новый проект ставим booking нового проекта почему вы знаете ответ да потому что там была чуть-чуть памяти чуть-чуть проца свободного но какие проблемы у такой инфраструктуры понятно что здесь никакой такая устойчивости нету хотя если вы нарисуете в серверах что вас два applications сервера мастер своих базы но это будет выглядеть как будто там есть отказоустойчивость но я здесь нет но в отличие от предыдущего варианта если у вас что-то сдохнет вы хотя бы не по бежать в магазин за новым за новым железом да вы уже что-то будете там несколько часов восстанавливать окей что вы делаете дальше вы нанимаете но админа который знает что такое amazon ну или или любое другое решение что делает этот админ он распихивать все сервисы по отдельным виртуальным машинам и даже настраивать в амазоне лот балансе и в целом такая инфраструктура уже вполне себе жизнеспособно даже если вам прям руками на каждой виртуальной машине поставил юникорн не знаю там ну в общем даже есть все это с поставлена руками это уже жизнеспособная история но почему во первых потому что здесь есть изоляция во-вторых потому что здесь есть отказоустойчивость ну тут еще мастерство iv понятно что мастер автоматом не стану в своих понятно что не станет автомата мастером понятно что файлы это все еще нфс сервер памяти там есть маунты понят что если виртуалок и сдохнет там прямо унты помогут бы сетевые проблемы но в целом это уже жизнеспособную инфраструктура что делать дальше наш классный новый админ который знает amazon еще знает что такое стри и убеждает наших разработчиков что он переписать все и ну как бы отказаться тонов с сервера разработчики переписывают и все становится неплохо хорошо что дальше он говорит час мы шифон там не знаю ансипа настроим горизонтальное масштабирование букетов давайте так ну то есть он описывает как будто rapid новая мода бэкенда даже настраивает в амазоне вот балансиры ли не важно делать дальше я буду горизонтальным масштабируемые штуки рисовать вот так просто . такое и потом наш админ совершать магию делает базу горизонтально масштабируемую на самом деле нет просто он полгода с разработчиками переписывает весь наш букет смазка елена манга для того чтобы это сделать но допустим у него это получилось дальше отказывается от родиться в пользу горизонтально масштабируем его масечка юнга но есть решение job сервера делает также сфинксы делает также и это клёвая а потому что в целом она уже везде горизонтально масштабируемые фронты пропускают новый jing с обычной пропускает обычно очень много трафика его спилить практически не надо там каши тоже много могут отрабатывать в кроне на в кроне у нас уже ничего не осталось мы просто в очередь ставим задание которое обрабатываются job серверами поэтому ну как потому что кроме падет ничего страшного у нас не будет можно пойти дальше можно сделать вообще все горизонтально масштабируем ну как бы круто идем дальше выясняется что очень все что мы делали это всего лишь один микро сервис и на самом деле картина выглядит вот так ну это прям видите йоды мастера секрет познать не каждый готов а в итоге вот если совсем по-простому можно сказать что вывод но если посмотреть эволюционно можно сказать что и вот есть шесть таких типов инфраструктуры кроме этого есть еще очень часто облачный цирк это когда в амазоне заказывают три огромных виртуалке и туда складывают ну вот понимаете есть еще микро сервисный цирка когда у людей цирк они говорят сейчас мы микро сервисами что исправим и ну примерно представляете что получается хорошо понятно что хочется хорошую или идеальную поступил потому что микро сервисное ничем не отличается там просто каждый микро сервис ну точнее те микро сервисы которые должны масштабироваться теме красиво которые должны быть отказу с очками их надо делать вот так давайте посмотрим что нужно сделать чтобы сделать вот эту вот идеальную инфраструктуру ну во-первых совсем базовая вещь нужно сделать логирование и метрики пока у нас была там три ящика или 5 виртуальных машин мы могли ходить с вашим смотреть топ и смотреть логи тейлом в такой инфраструктуре это невозможно настроить сборщик логов надо настроить сборку метрик для этого куча решения нам флюенс elastic ну че хотите кучу решений но это надо сделать и это много работы на при этом когда вы добавляете услуг нам это должно быть когда вы убили удаляете удаляйте узлы это надо сделать супер райзинг надзор тема на самом деле очень широкая но просто как только у вас система в которой много элементов каждый из которых имеет свойство там упасть или повиснуть если у вас их там три вы можете контролировать руками если у вас их 300 до вы этого уже делать не можете соответственно невероятно важно для того чтобы сделать такую инфраструктуру сделать чтобы каждый сервис как ты супер возился как минимум при падении процесса чтобы он поднимался как максимум были half чайки там я не знаю этот пышный запрос что он не повис потому что чаще всего таки процесс не падает объяснит мы видели зависание всего ну пожалуй кармен джинкс а ну только если у джинкс и ресурсы заканчиваются ну там как-нибудь memcache может легко повиснуть на старых версий там бывают баги ну в общем это очень важно acer вас discovering раз так шорты муж не понятно что как только мы хотим делать такую инфраструктуру она вся должна быть описано в некотором ансем лишив папетти чем-то там еще более того это танцы был шеф папе должны быть про интегрированность каким то ну механизма всероссийского если использовать шеф там можно все на шельфе сделать можно консул использовать но при этом вы должны сделать кучу кучу работы ну связанный с серва с discoveries infrastructure автомашин при этом смотрите как бы чтобы до конца все понимали когда мы добавляем новый бэкенд мы должны в каждом в на каждом фронте перетяните конфиг над всем понятно для того чтобы добавить новый бэкенд там когда добавляем но мы новый memcache то же самое при генерит конфиг на каждом букете от restarted наш booking возможно возможно нет зависит от от особенности вашего бэкенда с базой то же самое во все сервисы прописать базу хорошо скиллинг кроме того что мы должны сделать автоматизации инфраструктуру мы должны или хотя бы руками или выйди об этом в амазоне настроить авто скиллинг по реально реально ну то есть как бы каждый пункт сделать реально но это сложное это надо ну потратить много сил continuous integration continuous delivery это большая очень тема бесконечного размера если вкратце во-первых обычному почему-то думаем что код над диплоидный booking это не разный так потому что в вас к от кореи всего есть на друг серверах скорее всего есть в кроне скорее всего есть на сфинксе скорее есть даже на фронтах какой-то код не по не зная сет или ну есть у скорее всего это во первых во вторых там очень много подводных камней и в прошлом году я целый час рассказывал про то какие там есть подводные камни есть на хабре постам есть видео ну если интересно посмотрите если очень кратко то когда мы диплом есть главный паттерн выглядит он следующим образом у нас работает старая версия мы запускаем новую и ждем когда она прогреется когда она будет готова к на стартанет инициализируется и точно будет готова отвечать после этого мы переключаем трафик после этого останавливаем старую версию как только мы пытаемся сделать deploy так у нас возникает куча-куча интересных историй во первых у нас две версии должны работать параллельно это значит они должны подраться за базу они у них не должно быть конфликтов никаких ресурсов несетевых у них не должно быть конфликты в зависимости но понятно если мы используем докер то там большую часть вопрос можно решить ну собственно об этом и был мой доклад год назад а еще второй важный очень момент что нам нужно синхронизировать если мы делаем то есть нам нужно синхронизировать вот факт прогрева новой версии и переключение трафиком то есть нам нужно регистрировать работу фронтов викендов ну тогда у нас на буке не запустился упаси вас новая версия нужно франция перекинуть туда трафик такая непростая работа по координации окей допустим мы там запилили консул все сделали ну возможно почему нет дальше у нас есть история с виндой лаки нам смотрите если масса такая инфраструктура возьмем травма звание соскочить google и клауд за сзади невозможно но потому что там будет скорее всего завязка на amazon на функционал амазона на стороне амазона с кем много на что значит если вы не дай бог продаете ваше софт коробку причем вам в вашу коробку должны ставить клиенты у себя если не дай бог вам нужна вот такая инфраструктура вашей коробке то короче написать once i был роли чтобы вот это все разворачивалось около невозможно на потому что массово дни клиента будет amazon и других в амбаре у третьих железный серверов железных фигурок нет балансира и конец я не могу сказать что это восемь единственных проблему точнее единственных вещей которые нужно будет преодолеть для того чтобы сделать такую инфраструктуру но а это 8 главных вещей которые точно надо сделать и с остальным вопрос не столкнетесь пока это не сделаете но есть решение можно ничего этого не делать поставить мес или cabernet я правда ничего не знаю про месяц на говорят что он ничем не отличается значит история в чем реально выяснилось то что губернатор все эти вопросы решает из коробки и давайте я сейчас быстро расскажу часть номер два о том как же купер нас это делает окей архитектура cabernet у нас есть мастер узел есть ноды мастер может быть не один их может быть несколько там есть поддержка мысли multimaster на каждой ноги у нас стоит докер напоминаю что not может быть до пяти тысяч очень много на каждой ноги стоит куплет это управляющий софт который управляет докером и клип roxette управляющий софт который управляет и 5 был сам на каждой ноте мастер на мастере стоит и сервер которому собственно все эти компоненты узлов подключены плюс стоит эти сиди в котором open server хранит он его использует просто как сторож ну как 40 способом с ну как 40 с особенностями давайте так плюс есть чуду лир что dollar эта штука которая решает на какой на каком узле запустить наш контейнер очень просто то есть мы api хирургом запусти нам контейнер scheduler говорит запустить на вам там узле ну то есть он отвечает за планирование ресурсов и контроллер менеджер эта история которая отвечает за отказоустойчивость если вас одна надо упало-то контейнеры с нее будут их нотах и на за счет контроллер менеджера ok дальше у нас есть консольная утилита капец стояли в которую мы отправляем яму файлики при этом очень важно что dsl купюрница он абсолютно декларативно то есть мы не говорим создай нам там track три контейнера мы говорим нам нужно три контейнера потом меняем файлик файлики указываем меняем яму файлик файлики указываем 5 отправляем к вернется нам нужно пять губернатор видит раньше было три теперь нужно 5 создан кая 2 то есть это очень важно мы не это не не императивный dc лета декларативный dsl и в целом мы думаем об этой системе есть не нужно думать о том как убираться строя нужно думать что у нас есть яму файл консольная утилитка и облака контейнеров а вот то что там внутри но это некоторый уровень абстракции которым просто можно не думать вот с точки зрения использования архитектуру cabernet выглядит именно так окей что делает на борт что делает комбинацию бренд предоставляет но вот собственно чтобы запускать вот в этом облаке там запускается на самом деле не контейнера много разных штук вот комбинации предоставляет пачку примитивов пачку строительных элементов какие-то элементы ну во-первых контейнер понятно что контейнер это и матч плюс команда ну то есть в условиях губернатора в контейнер и запускается обычно только один процесс ну или если это там и джинкс или спички фэн потом запускается мастер процесс его потомки но главное что контейнер это и матч ваш публично неважно плюс команда что решая какие вопросы решаются на уровне контейнер на уровне контейнера решается вопрос логированием потому что есть соглашение о том что ну фактор 12 факторной методология heroku планируем was the gold standard собирает система вопросы 1 zynga на уровне контейнера мы указываем губернатор на уровне контейнера следи за нашим процессом перезапускает его выполняет half щеки и так далее дальше самые интересные в комбинации нельзя запускать контейнеры то есть несмотря на то что купюрница системы управления контейнером там нельзя создать просто контейнер то можно создать только под что такое под ну видите тут нарисован стручок собственно дословная дословный перевод слова под на русский язык это стручок в котором несколько контейнеров может быть и 1 но может быть несколько причем этих контейнеров общая сеть но общий локалхост они могут друг другу ходить по локальным портам и у них один айпишник на все контейнера то есть с точки зрения докера ни в одном системные space если проще говорить плюс можем подключать им некоторые общие папки плюс упада могут быть лейблы что такое лейблы оккупировать много чего кроме ну кроме как по кроме как у кодов еще у других объектов могут быть либо на главного подав значит лейбла любые кей вылью пара который вы определяете если вы деплоить интернет с несколько приложений вы можете ставить на плоды лейблы которые указывают название ваших приложений если вы если вас есть бэкенд frontend несколько слоев мы вы можете указывать свои если вы деплоить и несколько версий можете указывать раз окей лейбла нужны не сами по себе они нужны в связке select раме селектора это возможность искать по лейблам эти селекторы используются другими примитивами губернатор для серва здесь говорят же расскажу об этом окей под вот так выглядит обычно там один контейнер если там несколько контейнеров обычных называемой i'd care контейнеры но совсем простой пример у нас есть контейнер который генерит много лагов и второй сайт карт контейнер в котором стоит фрунзе который этиологии собирает может быть не очень удачный примерно вот что-то подобного рода есть основной функционал и какие-то дополнительные обслуживающих контейнера обычно так на уровне пода решается вопрос автоматизации инфраструктуру вместо того что писать шеф кубу кинсей был роли все что угодно мы пишем яму файлы на уровне под решается вопрос супервайзера под знает свой статус под готов к работе только если все контейнеры готовы на уровне подав решает вопросы рвать discovery ну за счет за счет лэйбл af и чуть подробнее про сиро discovery дальше ok возвращаясь к теме архитектуры cabernet у нас есть вот такая архитектура есть яблоки давайте быстро посмотрим что собой представляет ямал файл с описанием пода это под у него есть указание о том что это под ну то есть тип объекта под у него есть два поля metadata и спецификация ровно эквивалентным точно таком же формате описываются все другие элементы в кубер нации из них есть тип метаданные спецификация я сказал что под состоит из контейнеров собственно поэтому тут указаны контейнеры мы запускаем бунту вы будете запускаем баш все просто другой пример здесь есть лейбл который горит уж наш контейнер относится бы кенту наш под относится к бренду и здесь в этом примере используется наш какой-то локальный registry не не публичные мочу ok следующий примитив купер нация та реплика сад был под вместо того чтобы использовать один под мы хотим пачку кодов но мы хотим репликатор подав как это выглядит есть масса пика сет из двух кодов раз он из двух кодов значит мы указали что он должен быть размера 2 мы указываем зеленую колоночку desire q bernette следит за статусом он отслеживает сколько сейчас кодов есть и сколько из них готова если мы гришина надо 3 губерн это создает новый под под начинает стартовать у контейнеров под а могут быть описаны райден из пробы и когда под будет готов к brainz поймешь туда все хорошо если мы говорим что нам нужен один cabernet терменируют два года когда они остановятся он понимает что один окей на уровне реплика сайт решается два вопроса во-первых вопрос супервайзера казалось бы зачем нам реплика сад из одного пуда а вот зачем если проставку бренд создать под scheduler у вас планирует на какой-то узел если этот узел сдохнет да купер нас поймет что узел сдох спамят что под больше не доступен но он ничего с этим не сделаешь а вот если мы создадим реплика сад из одного кода и но какой-то из узлов на которые тот под попал умрет реплика сет уже увидит что от черт побери у меня ноль подав пересоздавая ну вот такой репликатор подав ну и плюс на уровне реплика сад решается вопрос масштабирования плюс крепко сайта можно прям прикрутить мозг который будет видеть загрузку наших кодов и если слишком много циpкa увеличивать ну говорить больше открутить этот показатель м как выглядит яму а очень просто опять есть тип опять есть метаданные со спецификацией указано сколько нужно реплик данном примере 3 есть поля тимплей в поле template ровно то же самое что мы писали в ямайке пода мы описывали под мы также описываем его в replicas этим но просто он тут называется шаблоном собственно по этому шаблону и будут создаваться под не надо указывать наивным и будет генерить или приказ от автоматом на основании своего и минуту сессии были приказ от называется baking под и будут называться пакет дефицитом случайные чисел ки важный момент что вот лейбл и которые указаны в шаблоне подан должны совпадать вот этим селектором чтобы реплика сайт находил свои коды ну как те которые относятся к нему и ну понимал сколько их важный момент в реплика с этому не можем поменять имидж то есть и почему понять можем но он при этом не не обновить наши коды соответственно мы не можем сделать где плоть помаши replicas эта для того чтобы сделать тепло и в кубе россией следующий примите в который называется deployment это реплика set + история старых реплика сетов плюс процесс обновления дипломат я буду рисовать вот такой корочкой такое облачко с подом значит как это работает у нас есть реплика сет со старой версии у которого внутри реплики соответственно съесть трипода если мы хотим выкатить новую версию мы создаем точнее мы создаем губернатор создает новый replicas от нулевого размера и дальше начинает последовательно старый даунс пилить новый обстреле увеличивает количество реплику новой версии при этом создается новый под когда этот под стартанет а он у старой версии уменьшает количество версий когда у старой версии под остановится он у новый увеличивает у старого уменьшает у новый увеличивает если в этом момент мы поймем что мы фигню выкатываем можно сказать стоп и он пойдет делать вот так ну понимаете да то есть в обратную сторону именно за счет этого производится откат накат изменений соответственно выкатили новую версию а старый в целом забыли ну то есть реплика сад остается но мы ним больше не думаем хотим выкатить следующую та же самая история осталось сколько хранится в истории настраиваться на 2050 можно задвинуть настраивается что ну зачем нужен диплом and понятно главное для чего нужен диплом into the continuous integration continues delivered очень важный примитив для ну для для тепло я дам потому что именно за счет него мы можем делать диплом как выглядит ямал тип deployment а больше он ничем от реплика сотни отличается ну то есть если взять реплика сайте поменять словно deployment мы получим диплом по которому уже можем я мне менять и мы можем поменять версию 015 70 15 9 у нас выкатиться отправить это в cabernet он увидит ой другая версия и все сделает сам это очень круто но дипломата можно настраивать много других параметров которых нельзя настраивать его реплика сад скорость как как с какой скоростью по одному полу он будет убивать или по 10 да ну просто если у вас размер выход большой если у вас 500 контейнеров и вы будете выкатывать по одному вы будете делать чертовски долго окей пока что мы разобрались только с одним примитивом губернатора который мы реально используем потому что не коды там не реплика сайты не мэйбл и в живую работая с кварцем и мы не используем используем только deployment и deployment это идеальная штука для вы к the state вас приложение да ну то есть бэкенд если он у вас нормально написан легко выкатывается дипломантом frontend ну и так далее побежали дальше серво es sehr вайс это dns имени плюс виртуальные пишут + слот балансер лот балансир разбрасывает запросы по кодам подходящим под selector и это сервис значит как это выглядит пример по которому вы все поймете у нас есть кучу кодов часть зеленая часть красные красные понятно там не готовы у которых какие то проблемы у части кодов есть лейбл что они относятся к букину из них часть относится к старой версии часть относится к новой версии и еще какие то остальные коды база данных там сайдкик не уж то создаем сервиз называемого бэкенд у него селектор о том что нужно найти воды относящиеся к бренду этот сервис найдет все коды относящиеся к бренду и будет разбрасывать трафик по тем из них которые активны соответственно если вы в кластере купер нас обратитесь к углом это т п : слышалось бэкенд вы попадете на виртуальные печник который слот балансе по живым потом это очень удобно ну то есть вы больше не думаете это главный примитив до сервис кого разумеется можно сделать сервис который указывает там на имеет более конкретный селектор в этом нет никакой проблемы ямал очень просто есть селектор который говорит какие коды нужны и есть указание как набить порты ну то есть нам пришел на виртуальные печник запросто 80-ый порт запросы будут разбрасываться порт по портам 8080 кодов вся эта штука работает на ipad и бился там на каждом узле губернатор заводит правило фактически destination от и и сделано очень просто очень надежно и работает просто шикарно 2 примитив побежали дальше job у нас есть под cabernet сумеет schedule и коды соответственно он может выбрать узел на который scheduled под если мы добавим логику которая будет отслеживать успешность выполнения кода жутко успешность выполнения как минимум что процесс завершился с нулевым кодом мы получим job зачем нужен job например вам нужно выкатить миграции ну обычно миграция то там ей my great или rake в играет или ну еще что-то ямал похоже на реплика сад тоже есть шаблон пода соответственно мы куприн отправляем джаббу губернатор поэтому чтобы создает под на каком-то узле если этот узел упал кудрин сам перезапускает плотно другому узле и так далее он добивается успешно выполнено у нас получается власти рисованный отказоустойчивый скриптик то есть мы хотим джобс сделали отказоустойчивые полнилась дача крон job берем job добавляем кранов к и расписание получаем кроме job яму выглядит следующим образом расписание формате крона ну по-моему очевидно и шаблон jobo шаблон jobo это только генерить пол то есть получается что король job это генератор job of job генератор кодов но мы получаем отказоустойчивый крону то есть мы просто пишем задание и у нас 1 минуту создается под нанана наименее загруженном узле если вдруг он не выполняется он перри задает и перезапускается другом очень круто побежали дальше воли мы потом можно подключать сторож ну потому что понятно что стоит сервис это классно но мы хотим встретил сервиса когда мы подключаем болью мы указываем три параметра says акция стайпа сторож класса что такой сайт понятно 10 гигабайт 50 гигабайт 500 гигабайт access time губернатор поддерживает три типа доступа ли драть vans там ну как мы обычно монтируем диск ну то есть пишем читаем только мы редон не менее это если мы ну какое-то сетевое блочное устройство например монтируем на нескольких серверах но нарядом ли с этим тоже все более-менее просто это поддерживает но все что угодно red white менее это ну понимаете да у нас же поду на разных физических серверах ну там физически сервов виртуальных машинах не важно у нас под разбросаны по по кластеру reader отмене это всегда или плюс рфс или cfs или ну то есть это какая-то сетевая какая-то классно файловая система сторож класс сторож класс это определенный админом губернатор есть когда вы за настраиваете как франция может определить сторож классы и когда определяете сторож класс вы выбираете они могут быть любые там слову нормам fast может быть там хдд с создаем может быть как как вы назовете david cook убираться с из коробки работает 19 возможных способов реализации этих сторож классов соответственно если вы плоти купер немцев google вы там можете использовать гугловый диск если в amazon использовать и б.с. если у вас на железе вы можете использовать cf элиас к исказили и еще угодно другое при этом очень важно что когда вы используете кубер нету когда вы описываете под вы не думаете о том сделано это гуглом ума зонам или еще чем то есть есть четкое разделение интерфейса от реализации волю можно подключить поду волю можно подключить крепле кассету но надо понимать что реплика создает воды и все эти коды будут работать с одним богам не у каждого своя с одним разумеется можно подключить deployment у надо понимать что если мы сделаем валиум секс с тайпом ридера it wants to replicate этот дипломантом страдать только один под все остальные просто не смогут получить доступ к это имеет смысл имеет смысл если вам нужно просто как бы отказоустойчивый под сделать вы делаете deployment сын равным одному и ну это заработает степ фулл сет был реплика сайт есть стейплз от смысл следующий большинство кластерных 100 раджей кассандра манга редис сентинель им все что угодно они общаются друг с другом узлы как только узнал можно общаться друг с другом они должны знать имя друг друга как только и нужно знать имя это имя должно быть стабильным вот именно для решения этой вопрос этого вопроса в конференции сделали стоит full site в чем отличие от deployment а или от реплика сета у deployment а если мы создаем дипломант который называется foo у него 3 реплики реплики будут называться вот так full рандомные буковки если мы создаем стоит вуз от который называем бар и у него 3 реплики под и будут называться вот так бар 0 bar 1 бар 2 и губернатор гарантирует что у них будут именно такие названия такие названия и такие же host name и но если совсем по-простому соответственно под и могут друг с другом общаться пафосно ему если у нас умрет под с названием бар 0 купер не найдёт новую тачку создаст нам под с таким же названием опять бар 0 про папу сейчас отдельно расскажу о кей в случае deployment а все коды работают с одной с одним в любом случае стоит вас это можно сделать так чтобы каждый под работал со своим волевым у нас есть стоит фасад из трех кодов и 3 волюма мы увеличиваем количество реплик до 5 по шаблончик у создаются метильных волюма это очень классная штука если мы хотим где плоть манга деби кассандру и так далее так далее так далее так далее и это 5 примитив который нам нужен для построения инфраструктуры ингрос у нас есть кубер над ним есть два сервиса full & бар если мы хотим пустить до пользователей мы должны дать им публичные печники или разделять по портам но это понимаете не очень рабочая история если у нас купер немного environment of мы хотим там production in development стояния нам нужна куча куча ip-адресов это неудобно поэтому выбирайте поступает следующим образом мы сервисы не вываливаем интернет а мы вываливаем интернет ingress и этому не государем публичные печник и пользователи ходят в ингрос что умеет ingress он умеет разбрасывать запросы или пафосно ему есть мы указываем что этому сервису такое has no им этому сервису такое has no им или по ул но соответственно там слэш фу это запроса пошли туда слышь бар пошли туда соответственно ingresso the host name плюс пас плюс энгра сможет терменировали цельное соединение и плюс у него указывается сервис в которой отправлять запросы яму выглядит следующим образом пачка правил хост такой то и несколько пассов если слыша сет отправили на frontend если просто слэш отправили на бэкенд очень просто и у ingresso точно также как уволим а точно так же как у практически у всего в купе нас есть очень четкое разделение реализации от интерфейс от реализации с точки зрения интерфейс у вас всегда ямайки одинаковы с точки зрения реализации это может быть или engine кс ну то есть убираться просто генерит конфиги для engine.exe следит ну restarted или это может быть но если вы деплоить и купер нес google cloud engine там есть свой ингрос и купина сумеет понастраивать и это последний шестой примитив который нам был нужен собственно губернатор эта штука в которой вот не просто контейнера а вот это штук а вот эти вот 6 понятий и с помощью этих шести понятий мы определяемся нашу инфраструктуру с помощью бликов которые генерят эти шесть примитивов возвращаясь к тем 8 вызовом но смотрите логирование с метриками я в том что для губернатора достаточно deploy да пару я бликов и там из коробки заработает барлога власти к из коробки заработает со метрика про металась вообще думать не надо этот продаж графа ней прям но это четыре команды как бы то есть мы тратим на этот 50 секунд реально ну вы представите да вы настраиваете блокирование с мониторингом за 50 секунд это этого у ну про супер возник я много говорил понимаете заточку перед следит за состоянием сервисов убираться перекидывает сервис между машинами про сервис discovery прежде всего есть примите в сервис который позволяет балансировать запросы это очень классно почему этот сервис будет работать на нас будет работать в амазоне будет работать угли одинаково 2 моменту комбинат есть классная пить если вам нужен навороченный серво discovery можете просто подключить с клиентом и ну делать то что вам нужно просто вчера тамошний они краше полностью зеленый почему потому что cooper and as не будет собирать вам докеры мы часами очень бы хотелось но вам нужно собрать докеры матч самим губернатор не будет заказывать железные серваке сам это очень большой минус и cabernet с не умеет как бы заказов новый железный сервак сам на него поставятся но реально мы обычно автоматизировали инфраструктуру шефом у нас от шеф рецептов осталось ну там слезы просто ну то есть от какой-то минимум который разворачивать но да и больше ничего не ну авто скиллинг тоже минус cabernet с не умеет сервера заказывать но cooper сумеет заказывать виртуалке в амазонии в гугле но соответственно он не только умеет спилить под но как только у него у кластер купаться закончились ресурсы он из коробка имеет заказывать новые виртуалке в амазоне ну и как бы как только не слиш не отдавать их обратно continues интересен continuous delivery но понятно что вам нужно поставить чайку понятно что вам нужно описать правила как вы собираете докер имидже как вы пашете яблоки в cabernet но после того как вы push 0 йамликуху берет с вам больше не о чем думать не надо vendor локи ну губернатор с коробки работает на всем и если вы например продаете ваш продукт как коробку со сложной инфраструктуры можете продукты прикладывать я блики и говорить о нем то ставьте губернатор клиент считает губернатор на vm ware я не знаю на железе на чем угодно и диплом туда вашей парики у него все работает все классно я закончил с баяном значит это был пересказ документации cabernet понятно что те кто его уже используется продакшена верно это было немножко скучно те кто в нем разбирались было скучно но я надеюсь что это вам будет полезно вы сможете эту презенташку потом использовать для того что мне знать своим коллегам объяснять как работает cabernet я думаю что всё равно это было всем полезно часть 3 наш опыт как мы делаем маленькие проекты на купер над с маленькие мы прям совсем маленькие мы заказываем 3 гипервизора железки ставим туда цех и ставим тут а ну софт для виртуализации мы много много много лет используем либерт с коим у кого им и маленьких проектах никаких окон стеков у вертов не надо мы используем просто вот как бы голову голая гипервизор и давайте так у нас получается возможность создавать виртуальные машины смотрите я сейчас рассказывающий про какую-то жесть какие контейнерами виртуальные машины ну я объясню почему собственно нас появляется возможность создавать виртуальные машины на каждом гипервизор который использовать локальный диск + у нас появляется возможность издавать виртуальной машины которые используют цехов ский диск и которая понятна они не сами запустится на другом гипервизор если у нас железка сдох мы хотя бы можем руками запустить или вживую мигрировать между гипервизора my достаточно быстро что за виртуалке мы создаем прежде всего три виртуалке падки мастер во вторых 3 виртуалке под моды куба если нам проект растет мы просто дальше добавляем и провизоры на них ставим только надо держать больше трех мастер вообще смысла нет ну или можно дальше масштабе масштабироваться уже чисто железными сервака мину зависит от деталей проекта мы делаем 3 виртуалке под фронт на которых настраиваем ungars контроллер как мы это делаем расскажу чуть позже и можем делать другие виртуалке зачем нам другие виртуалке сейчас тоже расскажу плюс мы можем делать отказоустойчивые рту алки например там ступень сервером сын офицером с теми вещами которые ну как мне они не то что отказоустойчивой ну так условно отказоустойчивая с теми вещами которые мы почему-то еще не складываем в кубер нации почему мы их не складываем а вот почему значит во первых что мы пока не размещаем в cabernet тот софт которому нужно очень много и опусов здесь у нас гигабитная сетка по гигабит на сетке работает цех там будет 300 iops of 500 iops а если у вас базе 500 типсов достаточно мы ее поставим в cabernet если вам базе нужно 50 тысяч abs of the никакой сетевой диск но никакой никакая и без к вам этого не даст нужно использовать локальную создать и только в кубер над этим можно потом подключать локальные папки но scheduler пока не умеет правильно делать стоит фулл сет с локальными папками об этом есть и кит как только этот секрет решат а скорее всего он будет в ближайшие недели ли кажется по моему версии 1 7 которая выходит в июне вопрос будет решен можно будет все приложения которые требуют много вепсов спокойно ставить в куб ну просто привязывая их жестко там которым у вас есть три узла со со знаками с быстрыми вы привязываете коды стоит фасеток этим узлам и там есть для этого в яблоке указания что привязать к потом с таким-то лейблом ушел в дебри мы не ставим анти облачные кластер и анти облачный постарше такой анти облачный кластер пейсмейкера то есть все что требует фен синга все что требует not какого-нибудь мультикаста для лидер или экшена и вот вот вот такие страшные штуки естественно нам нужно сделать высоко доступный кластер мой сквере лучше это сделать на 3 виртуалка там поставить pacemaker в один путь мастер остальные слои вы ну то есть вот пока нет готового решения о том в общем и пока не придумали что делать поэтому мы ставим пока на virtual как нам нормально а также ну есть всякие человеческие проблемы но вот страх ления инерция ну как бы мы над собой работаем и и по ходу придет к тому что он там через пару месяцев у нас останется только пункт 2 ну то есть мы будем выбирать ставить все кроме пункта 2 как мы делаем игры с контроллер я говорил что есть три виртуальные машины на эти три виртуальные машины мы диплом под ну в кубе нас есть еще другие примитивы кроме тех шести который рассказал то mesdames et но неважно мы тепло им под который называется н джинкс ingress контроллер это готовый и мышь от губернатора там прошка нога которая подписывается под сыр возник of discovery ok губернатор и когда мы добавляем новый явных с энгельсом там с новым красным столб османа перри генерит конфига дженкса и и рестартов джинкс и сама за этим следит приду получается так что он у нас вот есть три виртуалке там на восьмидесятом и на 443 портах слушает просто engine кс из вот этого года дальше нам остается закинуть до 3 почему ты еще объясню отказоустойчивых и печника причем закинуть так чтобы когда она виртуалка сдохнет айпишник с ней подход подхватывает любая другая когда две сдохнут все тряпичника на 1 когда они возвращаются api адреса перераспределяются обратно это можно делать как это сделать это история про конкретный сорт про конкретные возможности да ну то есть где то это вера и где ты такую карп где-то нужен приз maker для того чтобы api ну ты и скотина то там надо дергать описку для перекидывания но это делается и на эти три отказоустойчивых и печника мы отправляем уже до нас имена или но если проект за куратором и отправляем туда куратор если проект за сидена мы туда отправляем сидел и так далее как мы делаем continuous delivery год назад я рассказывал о том что continuous delivery о том что такое continuous delivery вообще в чем разница между континенты клещенко тимус delivery continues диплом он полон никто не знает я тоже уже запутался короче давайте лучше вообще с этим не разбираться каким из delivery значит эта цепочка от того как наш код сначала собирается потом тестируется потом релизиться и очень важно поток продакшене и эта цепочка заканчивается в тот момент когда наш код мы выкидываем с продакшена вот вот в этот момент по моему глубокому убеждению заканчивается эта цепочка во первых на чем мы делаем ну как какую себя систему мы используем мы используем vitlab ну это не система ну это чуть больше чем вся система но у нас есть опыт использование ким сети дженкинса и веса и и я не знаю еще чего он короче всего но вы остановились на том что нам подходит потому что я мол файлы там описание правил к ну описание pipeline а яму файлом есть у нас получается infrastructure и за кода волю мы стары часе кода сервисы там не знаю вот балансиры за кода continues on the gray scheme за кода ну то есть удобно все в тех же я бликах классным в git лобби живут проекты и один проект мы ну например так иногда мы делаем несколько инсталляций к вернется в устал в кубе рн офисе можно делать несколько инструментов ну это же не сколько много информантов например там продакшен стейджинг вообще сколько и каких информантов делать как построить pipeline это целая тоже большая тема кому-то достаточно prada и стайлинга кто-то хочет фичер branch и чтоб каждый разработчик мог из меньших квест нажать кнопочку него выкатилась превьюшка все это можно сделать ну то есть глубина этим наплодить только сколько надо интернатов никакой проблемы нету иногда у вас продавая инфраструктура дорогая develop министерство вы хотите дев инфраструктуру в каком-нибудь дешевом дешевом соде в этом случае у нас есть два кластеров убираться в одном патч конверта в другом пачкаем ферментов и мы избит лобо или по кнопочке или автоматом это тоже опять детали тепло им ну отправляем яблоки в cabernet по сути там то есть как бы все к этому сводится ну и также остальные проекты дальше вот эта цепочка гид build тест release a pirate у нас в детей лежит докер файл и лежит папочка там . кьюб схему файлами всем файлам бэкенда фронтэнда крона вместе с исходниками вашего приложения почему вместе я очень подробно рассказывал ровно год назад смотрите видео значит когда происходит commit у нас вызывается но условно докер build мы получаем и мач и этот и мы направляем в registry на стадии теста мы берем этот же и мышь и гоняем тесты ну как гонять тесты скорее всего тоже кубер над сам этот детали сильно выходит за-за доклад а при релизе мы берём яблоки кормим скармливаем их крепить целью shell скриптом и отправляем их в губернатор купер над скачает и мыть ну и и все и и работает да ну то есть 100 за стадия перед полностью отвечает cabernet на самом деле мы не используем никакие файлы не докер build a мы используем свою штуку которая называется доп и там доп файл и добил на самом деле вместо куб стиля мы сначала использовали фильм этапа как менеджер для губернатора ну то есть эта штука которая определяет структуру нескольких я бликов да ну пусть это пакет ну короче это управлять к куче ям накапливается так сейчас мы пришли к тому что нам функционала фильма не хватает и мы делаем обертку над фильмом в нашем даппи соответственно вас есть команда добил ты доп тепло и собственно про даб это upon собственный проект заходим звездочку мы собираем звездочки нам очень нужны звездочки там же есть документация то можно почитать но по сути не так важно мы используем доп можно использовать докер build это детали но процесс вот такой и продав я рассказывал на хайло один народ comfy на прошлом хай-лоу де есть видео в котором я подробно подробно подробно рассказываю о том зачем мы это понаделали почему как выглядит инфраструктура небольшого проекта до этого игру про маленький небольшой то есть это вот когда нет руки провизоров недостаточно чуть больше у нас есть облака кубер нету со контейнер на и облака неважно как оно сделано это ваша деталь если вам но если проект среднего размера у вас уже есть 100 ящиков и просто это делаете по железу очень важно чтобы в этом облаке кубинцы был настроен сторож я контролирую и очень важно чтобы в этом облаке купер натса был настроен ingress контроллер и скорее всего понадобится еще возможно сделать высоко доступные виртуалке ну вот ли лень страх инерция вот-вот всего этого и скорее всего вам понадобится делать virtual возможно сделать виртуалке с большим с большими окнами вот такая инфраструктура поверх такой инфраструктуры мы сейчас делаем любой проект ну вот в наших размерах как это выглядит была вот такая вот простая хорошая инфраструктура у нас есть платформа поверх который мы их делаем бэкенд это deployment база данных это virtual xiaomi кроны это крон джо бы отказоустойчивые frontend это ingress ммк что это стоит фулл сет из трех нам к шее и получилось так что вместо когда примитивная инфраструктура у нас очень просто очень быстро классная инфраструктура отказоустойчивая добавили файлики случае файлик подключаем cfs на букинге но вот если совсем попросту ну то есть если бы кинг и не переписать на s3 ну на обжиг 40 чтоб просто подключаем cfs редис это редис обычно satinelle а мы-то кластер там из трех узлов высоко доступно встретился эти земли к dance тепло или вкупе нотбук заработал бы grown джабба это дипломанты грамм джо бы обычно stay close сфинкс то же самое в целом ничто не мешает и базу перетащить внутрь просто сделав мощность и плоть москве эльф стоит в state фулл сет из трех узлов 1 будет мастером два других свои вам понятно что но тут история про и опции мы пока это не идеально но это можно у нас была вот эта история 6у инфраструктурами простая цирк облачное хорошие идеальное микро сервисная простая теперь у нас выглядит вот так и она стала хорошей ну что у нас даже там просто инфраструктура сразу с горизонтальный масштабируем масштабированием с полными отказоустойчивостью просто шик цирк ну да цирком и вообще не докатились нам он не нужен да у нас сразу хорошие облачная опять хорошая хорошая осталась хорошая идеальная осталось идеальной микро сервисная все микро сервисы в один губернатор можно в несколько неважно смысл следующий в том что куб губернатор с нашей точки зрения даст абсолютно дозрел до того чтобы его использовать ну в проектах любого размера я ничего не знаю про большие но я вижу по докладам ясно этом несколько крупных компаний которые используют кубинец я знаю что они давно уже это делают они без этого вообще никак не могли но сейчас получается так что купируется это возможность самого начала сделать проект с одной стороны очень просто страны невероятно надежно отказоустойчивость горизонтальным масштабированием понятно что есть подводные камни основном подводный камень это человеческий фактор ну то есть если у вас маленький проект с тремя разработчиками вы вряд ли найдете одного человека который будет знать весь этот стать потому что ну разбираться одновременно в цехе хорошо разбираться в сети потому что там есть некоторые тонкости сети в 5 bolsie в контейнеризации вот во всем этом просто вы не найдете так учила теле он будет стоить космических денег ему будет скучно у вас от вас уйдет и на самом деле вот единственная проблема почему моей точки зрения единственный подводный камень почему в совсем совсем маленьких проектах могут быть проблемы с губернатором но просто потому что багаж этих знаний тут разумеется мы можем помочь называется минутка рекламы на потому что ну как бы у нас это эти знания есть и мы по цене но условно там 1 админом и недорогого админа мы вам сделаем поддержку под ключ если проект среднего размера ну не знаю вас 2 3 команды разработчиков там 20 человек там например да вы уже можете легко себе это позволить и у вас не будет никакого цирк у вас не будет никаких вот этих вот ужасов блок на хабре у нас есть и мы все должны туда подписаться большое спасибо внимание друзья у нас немного времени буквально один два вопроса поднимаете рука у кого есть спасибо за доклад меня зовут тимур и у меня вопрос по поводу стоит full set of они сейчас находятся в альфа-версии не было ли у вас каких-либо проблем с этим типа проблем с данными с вольными они немножко недоделанные просто но вот через месяц они будут быть и не одно посмотрите стоит высоты появились месяцев 9 назад в общем нет проблем у нас нет история смотрите губернатор есть альфа-бета ну и и нормально а вот альфа это означает что этот функционал они могут удалить из последующих версий но если решат что он не подходит бета означает что этот функционал надо всем использовать но могут быть еще изменения яму форматов но уже гарантирует что он останется состоит из этом вряд ли кто-то удалит функционал ну то есть это скорее невозможно ну и альфа не означает что это не работает альфа это означает что это протестированный код который можно и нужно использовать но не факт что он останется в таком формате мы не боимся на нам не страшно поменять перейти состоит в усатому они раньше называть вообще пинцетами ну от кто знает спасибо за доклад вопрос будет спрос dc работали нет не знаете как там с этим вчера был очень интересный мем кто то сказал что мы раньше использовали мясо с марафоном а потом перешли на кубер datos мы никогда не использовали мы ничего не знаем об этом для того чтобы объективно сравнить губернаторы dieses нужно реально хорошо по пользоваться обоими системами у нас такого опыта нету единственное что я могу сказать это не технологический критерий а так ты житейский за губернатором стоит google с огромным камень стоит karos с огромным хай комьюнити стоят два самых крупных внедряться linux red hat и oracle и они сейчас говорят что к brands это их основная платформа я думаю что она останется как но основная через пару лет единственное вот это speak тивно и у меня нет доказательств последний вопрос вот пообещал спасибо за доклад меня зовут игорь компания провайдер и у меня такой вопрос чего он не хватило в hell из-за чего вы использовали именно там тепло и чего нам не хватило в holm из того что я помню ну во-первых смотреть там нужно дожидаться так фильм в последней версии дожидается того чтобы все поду выкатились у него не очень хорошо с логированием а ну сейчас вспомнил значит не хватает таких вещей как например у нас упал выход мы хотим на экран вместо дауд написать найти то место на котором упала и написать эту ошибку это фон самого фильме нет мы хотим если у нас упала откатить deployment обратно этого функционала фильме нет ну то есть мы используем фильм ну просто не ну как можно было наше ли там и фиков по написать но по сути мы надеемся то что мы будем писать вечера квесты фильмы ну по идее да хоть тоже развивается активно и мы надеемся что разовьется да это был последний вопрос у кого еще остались вопросы можете подойти на стенд ребятам с компанией флант и расспросить кто задавал прямую за про вопрос пожалуйста подойдите есть крошка франковский я столько мадам и я буду весь день отвечать на вопросы чем кубер над случае марафона и буквально на стенде привиделся пожалуйста буквально через несколько минут до и давайте поблагодарим через dmitry вам спасибо надеюсь было полезно"
}