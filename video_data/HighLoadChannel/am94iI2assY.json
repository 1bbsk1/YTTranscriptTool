{
  "video_id": "am94iI2assY",
  "channel": "HighLoadChannel",
  "title": "Сложности performance-тестирования / Андрей Акиньшин (JetBrains)",
  "views": 2428,
  "duration": 2993,
  "published": "2018-08-16T04:44:40-07:00",
  "text": "хорошо давайте немножко ещё пообщаемся я так понимаю раз вы пришли на доклад вам не безразлична тема перфоманса и скорее всего вы хотите чтобы ваши программы работали быстро а вот у кого хотя бы раз и была была такая ситуация когда вы выкатывали новую версию вашего отличного продукта и уже на стадии продакшена пользователи обнаруживали что есть какая-то performance деградация да не стесняемся не стесняемся неприятно правда хорошо вот сейчас на махнут и мы будем все можно начинать да здравствуйте еще раз меня зовут андреа kenshin и сегодня мы поговорим про сложности performance тестирования продолжим тему прошлого докладе к и начнём с общей методологии у нас сегодня будут несколько другие цели чем на первом докладе и первая наша цель основная цель которой мы хотим добиться это не допустить performance с деградацией особенно на продакшене чтобы пусть все будет плохо но не хуже чем в предыдущей версии вот это вот самое основное к чему мы идем кто хочет узнать как гарантированно не допускать performance деградации в продуктах кто-нибудь хочет на смотрите ответ никак в вас просто никогда этого сделать не получится если вас не знаю хэллоу волдо наверное если это хоть сколько-нибудь крупный проект не получится просто некогда смиритесь с этим здесь у нас появляется вторая цель что если мы допустили performance деградация мы рано или поздно ее допустим наша цель во время об этом узнать возможно через день возможно через неделю возможно через год но главное раньше чем об этом узнает кто нибудь другой дальше учитывая то что первая цель у нас недостижимо у нас появляется еще парочка целей цель номер три это снизить количество ошибок первого рода кто знает что такой ошибки первого рода ну и не надо это знать не забивайте себе голову простыми словами у нас все хорошо никто кодовую базу не попортил а наша умные system performance тестирования горит алярм все плохо нужно срочно разбираться главный performance инженер идет целый день разбирается объясняет что проблемы нет performance инженер расстроен вот это это плохой кисть потому что тратится много времени и нервов людей если ошибки первого рода происходят редко там один раз месяц два раза в месяц это нормально с этим нужно смириться если они происходят по пять раз в день просто вашей чудо системы анализа перфоманса никто никто не будет воспринимать ее всерьез потому что будет буду знать что оно учит постоянно честь ошибки второго рода это когда performance деградация есть кто-то что-то сильно испортил но мы не досмотрели это тоже неприятно мы тоже хотим как-нибудь с этим побороться но это уже не так критично но пропустили деградацию ну бывает потом починим и самая последняя пятая цель вот вот это вот все в принципе могут делать люди на может быть отдельная выделенная команда там из 10 людей которые приходят каждое утро на работу начинают разглядывать графики бри портить проблемы и так далее но мы этого не хотим мы программисты мы любим все автоматизировать чтоб но работала сама и это ну не так уж и просто к сожалению общей теории по тому как нужно организовывать процесс performance тестирования в индустрии нет ну серьезно сейчас 2017 год еще не сформировалась единое понимание того как нужно ловить разные performance проблемы специально перед этим докладом отлавливал за руки очень многих performance инженеров и совершенно разных компаний и спрашивал как они тестируют performance в половине случаев я услышал ответ никак просто смотрим что чтоб не тормозило или пользователи протестируют вот в половине случаев людей были определенные механизмы анализа но обычно это собственные велосипеды какие-нибудь собственные инфраструктуры заточены и чисто под проект но зачастую людей нет полного понимания методологии того как как что и должно работать поэтому я надеюсь что все мы вместе с вами эту методологию скоро сформулируем ну а пока я хотел было в рамках этого доклада поделиться немножко своим опытом кто знает такой продукт под названием raider на это нибудь если что это кроссплатформенное шар поддержка которая разрабатывается jetbrains кто знает такие продукты как ришар пир или intel и джей ди вот richard and intel и за идею нас используется чтоб raider работал то есть мы взяли два продукта же идею у нас это frontend наш мы с помощью него показываем юзер интерфейс ришар перед а backend он что-то там обсчитывает делает разные рефакторинга делает разные анализы и так далее я понимаю а если кто-нибудь кто десктопа разработка занимается здесь хоть один отличный я-я-я видимо и единственный вот но я буду частично рассказывать про performance тестирования десктопа но все концепции они работают в целом для индустрии еще такой вопросик кто считает что он работает над большим проектом как бы вы не определяли слова большой отлично вот я не знаю большой проект у нас или средний или маленький но вот немножко статистики у нас есть 200 тысяч файлов 20 миллионов строчек кода больше ста тысяч тестов в внутри мы тащим собой одну g выемку специальную версии за по чину и одну за по чину версию маны нам нужно что все это работало быстро на windows и linux и маки самая большая проблема наверно то что вот в эту кодовую базу постоянный кто-то делает изменения то есть чек 100 которые постоянно постоянно по она комитет мы хотим сейчас райда работает очень быстро и мы хотим чтоб он продолжал работать быстро и не деградировал по перфоманса но за этим очень сложно следить и как вообще наверно в любом продукте над которым работает он человек и в котором служит сложно отследить зависимости есть очень много так называемых специальных perfomance test рада что я называю слон специальным это теста который были написаны для того чтобы тестировать performance вот какие варианты есть например там нас есть тесты на холодный горячий старт да ну а не понятно что такое холодный горячий старт и надеюсь всем холодный старт мы запускаемся без каких-то к шее без с нуля возможно в некоторых тестах это происходит после ребута системы чтобы на системе не было ничего лишнего и смотрим за сколько мы про тупим чтобы начать делать что-то полезное горячий старт когда мы уже запущены у нас все хорошо все данные предпочитали за сколько мы обработаем какой-нибудь request есть разные micro mesh марки которые направлены на тестирование каких-то очень маленьких кусочков кода которые очень критично для нас по производительности и которые мы не хотим что прогрессировали есть разные perfomance test in a юзер интерфейс например у нас есть тесты на плавность скроллинга на у нас есть отдельные машины которым подключены реальные мониторы и мы там посмотрим что будет когда человек скроле чтобы она не тормозило есть но это не про raider но там соседних команд есть нагрузочное тестирование пример view треки через разные тоже инструменты есть измерение в этом сие пропускной способности понят понять чем еще различаются в этом все это у нас время за который мы обрабатываем один-единственный запрос пропускная способность это время за которое мы выполним величина обратной latency приучу с учетом того что в нас запихивает очень много запросов одновременно тоже некоторые не различают или меряют ровно одно дальше есть такая вещь как capacity planning или по поиск точек отказа то есть одно дело когда мы говорим вот нам интересно за какое время отработает там 10 запросов а другой вопрос который можем задать сколько запросов должно прийти чтоб мы развалились и вот можем так повышать повышать объем данных на котором которых мы процессе пока мы не развалимся и вот это число сколько мы можем всего выдержать тоже использовать как показатель какой-то performance то есть это не обязательно время еще есть разные алгоритмы на тестирование симпатики алгоритмов ну например когда выродка в этом в райдере ли в идее некоторые файлик у вас на этот файлик на пускаются всякие инспекции наш там такие штуки чтобы вам подсказать что у вас где то что то не так вот эти инспекции должны работать быстро если инспекции работают медленно то все плохо на больших файлах у вас сайт будет втыкать что означает медленно но вот мы для себя определились что инспекции не должны работать за квадрат если вы тратите от квадрат длины файла то все плохо как-то проверить есть теста который по очереди засовывают в инспекцию файлик больше больше больше больше больше и строит зависимость и там достаточно просто автоматом определить длине и на эта зависимость там ngn или же квадраты и все все все квадраты и больше мы сразу отлавливаем хотя походу ну не всегда понятно какая у нас асимптотика и специальных performance тестов можно придумать очень много это тесты которые разным образом тестируют что угодно но главная идея в том что они заточены на получение каких-то метрик производительности кто пишет у себя perfomance test и и все остальные почему не пишет хорошо ночам скажу по секрету что обычные тесты который длится хотя бы там больше 10 миллисекунд их уже можно использовать как performance если сразу просто вот оба хорошо кто пишет обычное тесто давайте так отлично у кого есть теста как обычные тесты там функциональные какие-то которые работают больше 10 миллисекунд есть такие отлично все с вами уже можно работать давайте сравним обычный тест и специальные значит обычные у нас запускаются на обычных машинах да не понять где для крутых perfomance test of обычно используется такое выделенная выделенные машины обычный тест у нас могут быть день быть там на виртуалке запущены в доке еще где-нибудь с волшебные специальные тесты запускаются обычно на реальном железе иначе очень сложно с ними работать но обычные тесты при этом вы их пишите если вы используете хорошей практики программирование вы их пишите просто по ходу разработки специальные тесты там думать надо там чтоб такое написать что performance нормально протестировать это не просто и обычных тестов их много и они достаются он бесплатно вот в ходе артефактов просто разработки специальные тесты их обычно мало их очень дорого с точки зрения разработки писать потому что это очень сложно и как правило они жрут много времени их дорого гонять на build агентах историю хочу вам рассказать как то раз мы в райдере жили продвинуть версию runtime и мы переезжали см она 4.9 нам 5.2 и часть наших тестов начала падать были тесты на от открытия определенных больших solution of раньше они проходили за три минуты а теперь начали падать с пятиминутным тайм-аутом вопрос что делать профилировать отличная идея кто хоть раз пробовал профилировать ману я пробовал если у вас маленькая программа которая длится несколько секунд с этим можно справиться когда у вас что-то идет дольше 5 минут и делает что-нибудь еще очень сложная там самый маленький snapshot чик который у меня получался 50 гигабайт который я просто не мог открыть нигде я честно ждал на винде просто все крашнулось на маке я сдал два часа честно потом не выдержал не работает профилирование что дальше делать куда продвигаться а вот проблема в том что есть только там ну грубо говоря небольшая группа тестов там их три четыре на которых все стало плохо остальные тесты нормально работают они никак не за эффекте леся плохо тем тестом с которым так сейчас я покажу сейчас вот у меня был был был слайд 20 миллионов строчек кода и ладно не там не все используются на миллионов 5 используется то есть это полное открытие solution а build анализа запуск тестов версию мода продвинули runtime там и нет старую версию тоже плохо там она у нас тоже пачино и потому что там все разваливается мы иногда к метим тоже в моду чтоб у нас попроще стала встроенным она профайлер а мы еще intel ведь он вам префаером еще первым и еще несколько не спас выйти он не спас знать что мы сделали мы взяли наши обычные тесты которых у нас 100 1000 и начали смотреть на них ночной трети теста не запускались на где попало virtual как в докер ах на разном железе просто собрали все тесты и по сравнивали performance на до перехода апгрейдом он и после апгрейдом он и и нашли более простой тест на котором ситуация воспроизводился за тест на комп лично то есть когда вы жмете control пробел да и вам всплывает такая штука которая предлагает заком плетить слова до апгрейда тест шел 4 секунды после стал идти 18 секунд и все вот наш анализ показал что нас две группы тестов on анализ вот этих больших solution of и вот тест на комп лишь на который с четырех секунд прыгнул на 18 + 18 секунд в том что их уже можно провалить нам правда call stack исхода ничего не показали там пришлось поприседать мы проседали два дня и в итоге сделали минимальная реп про я про выглядит так мы берем пустой массив кастинг его к аресту уже но я надеюсь в обще конце понятие и дальше в этого массива объектов берем количество элементов как вы думаете за сколько должен отрабатывать такой код неважно boxing там анбоксинг в в во временных юнитах микросекунды наносекунды сколько наносекунд хорошо вот небольшая табличка до апгрейда у нас было пять наносекунд после апгрейда на линуксе стала 1400 на маке 2 600 но на винде мы используем полный фармер поэтому там не репрезентативна мы нашли багу бага было в новом механизме диспетчеризации интерфейса в массивы мы там написали ребята нажму на эту багу уже пофиксили но в чем главный point этой в истории омоновцев есть много всяких тестов они заботятся о перфомансы не стараюсь не допускать агрессию они это просмотрели наши волшебные perfomance test это же этого не поймали но вот среди обычного с юта тестов на 100000 мы нашли там буквально несколько штучек на которых эта проблема выстрела может когда у вас много тестов и вы даже обычные тесты начинаете анализировать да там очень много проблем но из них также можно достать очень много пользы сейчас мы немножко еще по разговариваем про общие методологию про то чем вообще можно делать значит мы выгребаем как кометы для анализа и совершенно разных мест нас есть анализы по мастер ветки у нас есть анализы по всем врачам то есть мы просто все branch от всех людей смешиваем в кучу и запускаем анализ тоже находится очень много интересных у нас есть сеты заданных комментов по которым мы запускаем какие-то анализы можно собирать очень много вы в меток можно собирать время прохождения теста можно если тест большой им пить по ходу прохождения теста какие-то отдельные части тоже очень клевый метод можно снимать разные железные показатели там цепью memory disk можно вытаскивать разные характеристики runtime а например там используйте идут на это ли джаву очень здорово следить за тем сколько сборок мусора была потому что этот это число но во первых менее чувствительна к тому на чем вы запускаетесь то есть у вас условно говоря если все пойдет нормально memory трафик более-менее gaiter минирован и вне зависимости от железа и скорости циpкa во вторых количество сборок мусора она очень чувствительна ко всяким таким знатным продолжим когда у вас все стало плохо в этом на аллоцирование много объектов за этим намного проще следить все эти показатели можно снимать с любых тестов хоть специальных performance хоть обычных и скай там разные закономерности нужно быть конечно аккуратным потому что никто не отменял проблем множественных сравнений нас заключается в том что когда вы берете много много много много много показателей по многим брань самых запускаете то где-нибудь вы найдете что-нибудь что будет выглядеть как performance деградация просто неизбежно но на самом деле не деградация просто вам не повезло вот но с этим нужно аккуратно справляться какие способы можно использовать в качестве обороны против performance деградации мы уже послушали на прошлом докладе что и можно строить графики вот возникает вопрос что дальше делать с этими графиками у нас есть несколько несколько вариантов барон 1 штука у нас называется мертв робот в чем идея мы никогда не к метим в мастер ни один человек не может принципе закомитить в мастер но у нас есть мирт робот мы говорим merge роботу вот у нас есть branch eg мы хотим его влить в мастер мерч робот делает магию прогоняет все тесты убеждается что все тесты зеленые и только после этого совершает мертв соответственно мертв робот можно аккуратно навесить функциональный performance тестирование то есть чтобы мы проверяли что в мастер нельзя в мир жить не то как стабильно красные тесты но еще и стабильный медленные тесты в которых что-то сильно ухудшилось на кстати используя такие нехитрые стратегии биржа в мастер или все руками пушат интересно просто хотим сити мирза ну хорошо дальше кто использует деле тесты есть какое-нибудь деле тесты которая запускается не на каждый коммент а просто 1 день сделайте себе да или т строчку крутая штука бонус до или тестов у вас прогон может идти вплоть до 24 часов больше 24 не помрешь получается потому что тогда уже не будет не очень были но суть в том что за один раз в день вы можете позволить себе много времени потратить на то чтоб поанализировать performance вот на merge- в мастер вы хотите чтобы он прошел как можно быстрее вы не хотите долго ждать пока все тестом зеленые вот раз в день вы себе это может позволить такой длинный ран а еще есть варианты вариант который очень люблю называется нас ретроспектива мы берём большие большую историческую такую performance ную картину не знаю за месяц за полгода и начинаем анализировать очень клёво способ он хорош тем что можно очень делать все что угодно данные уже есть торопиться никуда не нужно берем анализируем мы сегодня с вами позанимаемся перспективой дальше есть такая штука еще которым называем чекпоинты когда кто-нибудь знает что он сейчас будет мер жить какие-то опасные изменения лучше не сначала в мир жить и потом посмотреть как оно пойдет а сначала посмотреть аккуратно оценить performance если знаешь там опасный код правее вот и только потом в мер жевать когда убедился что все хорошо и еще один тут то это же очень важно такой способ тестирования это предрелизной когда завтра релиз и сегодня мы начали тестировать performance проверить не сама слетаем чего у в разных способов и свои достоинства и недостатки допустим посмотрим на время обнаружения точность обнаружения деградации и как у нас построен процесс например в мир чоботи мы обнаружим деградацию во время да и процесс полностью автоматически то есть человек здесь не не участвует то что не надо мир жить мастер никогда не будет в мир жена в мастер это очень круто потому что просто мастер не попадает проблемы с которыми потом приходится героически сражаться до или тестов мы узнаем о проблеме поздно после того как она попала в мастер но зато точность получше мы можем больше всего всякого запустите процесс уже не не полностью автоматически полу ручной потому что если тесты разводились должен прийти человек и что-нибудь сделать в ретроспективе у нас уже слишком поздно обнаружим деградации но лучше поздно чем никогда там например через неделю у нас отличная точность и опять полу ручной процесс если мы говорим про чекпоинты тоже обнаружения во время точность отличная но делать нужно все руками каждый блинчик аккуратно смотреть это уходит очень много времени и перед релизом тоже совсем поздно просто совсем-совсем потому что релиз уже завтра уже никогда что то делать но зато нас есть дополнительная гарантия того что у нас все нормально есть еще очень много разных факторов например там время прогона здесь мы говорим о каких-нибудь микро бенчмарках которые мы внедряем куда ни в кого нибудь из этих методов то кто-нибудь микро benchmark просто один может идти не знают сутки чтобы добиться нужного уровня точности и на выделенной машины это только один тест для крутых performs тестов нам обычно нужно много bellagio build агентов желательно покруче желательно абсолютно одинаковой конфигурации иначе все будет плохо ну и разумеется в разных случаях мы будем там разные спектр проблем при тоже немножко побеседуем нужно очень четко понимать что вы считаете performance деградации когда нужно возбуждаться на то чтобы сигнализировать людям а все плохо во первых у нас есть такой ручной метод мы берем просто смотрим на перфоманс результата смотрим на графике пытаемся что-нибудь понять очень клево не тот очень очень клёвый но нужен человек который будет просто месяцами это делать и не сойдет с ума про проблемы иногда найти такого человека у нас есть автоматизированный способ когда мы в логику работы программы как-то пытаемся заложить что мы считаемся performance деградация например у нас можем сделать абсолютно и тайм-аут если тест идет больше пяти минут все всё плохо тест красный кто хоть раз зашивал какие-нибудь тайм-аут и для perfomance test of code у кого из-за этого потом были проблемы отлично сто процентов проблемы тест рано или поздно у вас начнет мигать либо вам поставят медленно агент в котором иногда будет филиса поймал либо тест станет работать чуть чуть подольше иногда чуть больше тайм-аут иногда чуть меньше тайм-аут потому что дисперсия большая и будет причинять очень много более вашим коллегам в конце концов его забьете ты никто уже никогда не разметет и и есть у кого-нибудь теста которого так мучаться и все на вот я вижу есть люди отличные дальше есть относительные тайм-аута например можем мерить относительный performance одного метода кому-нибудь талон относительно другого который мы тестируем но здесь тоже очень много разных эффектов есть зависящих от железа что на разном железе у вас значительная разница между двумя методами может быть разным также относительно этой мало сложно сравнивать между разными машинами здесь тоже можно разных разные средства использовать например в идее есть ряд perfomance test of который на каждой машине запускаются такие специальные чудо методы которые пытаются оценить нас насколько быстрая машина в некоторых родных попугаях затем нормируют просто попугаем какие метрики и пытаются что-то сравнить вот есть еще один вариант который особенно хорош для ретроспективы это смотреть на исторические данные некоторые стад показатели то есть у нас есть никакой the time limit какой который зашел зашил кто-то в программу за хардкоре а у нас есть история на сервере которые никто руками не за hard ходил и есть новые измерения и мы просто берем мы сравниваем и смотрим по каким-то волшебным статистическим формам который мы придумаем мы смотрим есть проблема или нет более того если у вас в эту штуку внедрить в процесс тестирования если у вас есть исторические данные на момент запуска теста то у вас есть уникальная способность вы можете делать много итераций и останавливаться в тот момент когда вы насчитали все что нам нужно то есть представьте что вы запустили одну итерацию теста и она оказалась на грани то есть где-нибудь вот с краюшку распределение не понятно то ли вам просто не повезло поле это какая-то именно стабильная деградация что можно делать если у вас есть исторические данные и если вы имеете их на момент начала теста вы можете понять ситуацию и продолжить делать итерации до тех пор пока вы не будете уверены что перед нами и есть полу автоматизированный способ это некоторые автоматической системы которая будет выдавать нам например топ некоторых performance аномалий или каких-то подозрительных моментов и дальше чем джек который разбирает список руками смотрят и заводит пишу и если он считает что действительно все пошел плохо в общем разные есть варианты давайте поиграемся немножко с performance аномалиями что вообще называют proformance аномалиями например деградация производительности до внезапно ты стал работать медленнее это может происходить медленно это может происходить быстро дальше у вас может один тест начать тормозить и может целая группа тестов возможно которые как-то связаны если вы делаете автоматический анализ это здорово отличать иногда производители из внезапно улучшается некоторые радуются этому и ничего по этому поводу не делают а некоторые начинают смотреть что у нас внезапно лучше стало в никто же не оптимизировал иногда оказывается что кто-то просто выключил какой-нибудь критично важный кусок кода он перестал запускаться и wrapper это его кого-нибудь так вот отличный в зависимости от параметров или окружения например там на разных операционных системах не знаю в разных браузерах у вас может быть разный performance еще один показатель которая люблю это дисперсии вот некоторые просто смотрят на дисперсии одного теста там большая она или маленькая а я люблю взять все тесты по ссортесь по дисперсии выбрать топ кто-нибудь делал так смотрели топ тестов по большой дисперсии вот сделайте посмотрите найдете много увлекательного серьезно обычно когда на то есть и большая дисперсия что-то там наверняка не так есть много распределения странной формы и есть чем много всего давайте посмотрим на пример как нам это уже нет времени но например примерчик реального теста из нашей кодовой базы здесь разными цветами помечены разные операционные системы синенькие квадратики это windows зелененький треугольнички linux оранжевые кружочки это маг вы можете это не запоминать просто запомните там разные штуки разные ос и вот здесь очень хорошо видно что в районе 5 сентября у нас произошла маленькая performance деградация и и никто толком не заметил потому что она была маленькая потому что даже там на этапе мирт робота такую деградацию не отследить потому что там условно находится в рамках ст от погрешности но через несколько дней на график очень хорошо видно что там linux стал работать медленнее я не 20 сентября нам стало еще хуже но никто опять не заметил вот и все продолжили дальше писать код а вот 7 октября проблемы накопились какой-то момент они наложились друг на друга и стало всё совсем совсем плохо performance начал плохо плохо общем устала и пришлось срочно бежать и чинить вот такие деградации их в момент коми то далеко не всегда удается поймать особенно если не маленький но если вы смотрите историк отдельно каждого взятого теста даже если это простой тест не performance просто вы можете найти очень много полезного и интересного по дисперсии вот один из реальных тестов вы с кодовой базы идеи время работы этого теста ходит от двух минут до 40 минут просто без каких-то chen j просто от раза к разу у нас то две минуты то 40 минут ну здесь конечно с точностью до операционной системы например там хуже всего могу по ряду особенностей тест является корректным он проходит с ним все нормально он тестит определенно такой стрессовый сценарий но вот этот график нам показывает что хорошо бы с ним что нибудь сделать хотя бы чтоб там бьются оптимизировать чтоб у нас перестал ходить так долго или вот еще график здесь очень хорошо видно следующий вот сенник это у нас windows на windows здесь работает медленно а linux и mac это вот треугольнички кружочки они работают быстро там если мы смотрим на какой-то один отдельно взятый график можем подумать но просто windows тупит да но если мы посмотрим на другой график то видим что там теперь ст windows стала быстрее стабильно а linux и mac медленнее и нужно в каждом таком случае разбираться в чем дело параметру вот для нас ливане всего операционной системы на параметры может быть другие например у команды ришар пера который встраивается в visual studio то же самое с версии visual studio то есть у них там в 13 visual studio может быть одной в 15 совсем другое параметров можно сколь угодно придумать и очень интересно посмотреть на теста даже на обычный тест у которых максимальная разница между этими параметрами опять-таки когда вы проглядывать итог можно найти очень много всякого веселого или вот еще один тест здесь linux mac отрабатывает за 100 миллисекунд и windows за тысячу с плюсом из-за просто из-за особенностей устройства сокетов на windows нам немного по-другому идет игра с задержками эта штука на том числе стреляла в продукте за того что на винде были небольшие небольшие такие задержки в не очень приятных местах и мы не могли понять почему а вот посмотрели просто топ тестов по разнице по параметру и нашли или вот еще веселенький тест тест проходит либо за примерно 0 секунд либо примерно за 10 когда мы заглянули в тест мы увидели что там стоит таймаут на 10 секунд причем что-то с тестом не так и когда ты молод выходит не кидается файл то есть тест идет как зеленый как проходящий но при этом когда мы смотрим на перфоманс картину ищем какие-нибудь и такие распределения мы можем также найти очень много проблем в нашей логике у нас ни один такой пример у нас наверное вот такой performance анализа выявил там добрых но как минимум штук 20 тестов именно с проблемами в логике в тестах в продукте счет где-нибудь которые не влияли на прохождение тестов то есть он все было зеленое но очень искажали performance ную картину или вот еще один пример он уже не из нашего продукта из runtime идут над корт я игрался с одним пестиком здесь по оси x отложены значения параметра да сколько мы запихиваем данных в тест а по оси y у нас идет время работы вот здесь очень много интересных феноменов есть во первых есть феномен в районе размера равного 500 когда у нас происходит что-то очень странный тасс начинает работать дико дольше а дальше нас идет муки модельное распределение у нас два локальных максимумов и непонятно в какой мы попадем это просто как повезет анализ вот таких графиков тоже по параметрам очень полезен вам нужно построить не только мы такую вот модель просто линию начертить как у нас в среднем мы зависим от кого-то параметры именно сам опять-таки смотреть на распределение 5 мультимодальные распределение можно автоматически искать распределение странной формы с какими-то подозрительными местами можно искать автоматически но они не все так гладко с обычными тест например вот на таком тесте наша чудо системы с детектива performance аномалию сказала о все плохо performance у плохо но на самом деле просто плохо стало несколько марковским агентом на которых выполнялись обычные тесты и performance стал на них неприятным просто из-за того что мы запустились разные фоновые процессы а у нас это произошла по сути ошибка первого рода такие ошибки встречаются но не очень часто на них что можно влиять изменения в самом тесте дамы что-то дописали в тест изменения в порядке тестов вагинки в окружающем мире и вообще любые изменения могут где угодно выстрелите как-то повлиять поэтому такие способы анализа они не лишены недостатков у вас будет очень много ложных срабатываний у нас будет очень много проблем но если продраться сквозь все эти проблемы можно просто абсолютно за бесплатно получить много полезных данных и найти очень много проблем и здесь я хотел бы немножко поговорить про performance на такую культуру если вы хотите чтобы у вас в продукте был хороший performance у вас вся команда должна заботиться о перфомансе всех это должно волновать у всех должны быть едины и performance цели у всех должно быть понимание performance чистоты что такое perform нас частота вот я называю к ситуацию когда есть например очень много тестов большой дисперсии не понятно почему она большая что это грязный с точки зрения перфоманса тесты и не нужно думать что вот у нас есть отдельный выделенный performance человек он все починит вот вы например когда пишете обычный кот вы не пишете как попало там вот именно по стилистике коды и говорят что у нас есть отдельный человек который отвечает за чистоту кода он мне потом отформатирует переименует сделает нормальные название переменных там сидела там рефакторинг вот нет такого человека ответственность за то чтобы ваш код был красивым читаемым лежит на вас или если вы пишете обычные функциональные тесты нет же такого что я сейчас закончу в мастер красный тест но я жить с написал ну ладно что он красный у нас есть отдельный человек который чинит с ты вот поезд пусть он идет течение ты тот с нету ответственность за то чтобы ваши тесты были зеленые тоже лежит на вас почему же с перфомансом должно быть по-другому если все члены команды будут не знаю смотреть на такие картинки волноваться о том сколько работает там какая-то их логика их под системы волноваться перфомансе в целом проекта иметь одинаковое представление о том как например какие должны быть трейдов например очень многие изменения они делают одну часть системы немножко побыстрее другую чуть помедленнее в таких случаях очень сложно сказать там это performance деградации ли performance улучшения система с таким никогда не справиться это решение должен принимать человек и вот только с каким-то общим пониманием того же к чему вы стремитесь именно по performance от всех членов команды будет прок еще расскажу про в заключении про один такой прием который позволяет улучшить ситуацию с производительностью и the performance driven development или пдд вот кто работает по т.д. поздравим devil там есть такие кто практикует ой смотрите новая методология сначала пишем новый perfomance test если вы там собрались чьи-нибудь рефакторинг дописывать новую функциональность напишите сначала новый perfomance test посмотрите на то как он работает посмотрите на распределение где-то минимум максимум там среднее медиана квартире перцентиле графики тренды что угодно все смотрите сформируйте полностью performance наука картины сделайте некоторый рефрен сет дальше спокойно можете приступать к рефакторинг ук оптимизация в конце проверяем что стало с перфомансом той системы которую вы переделывали носите здесь порядок очень важен главное не перепутать многие делают наоборот сначала пишут код а потом ну вроде стал немножко побыстрее посмотрел вроде раньше было секунды две сейчас стало полторы все хорошо не будет так хорошо может сегодня будет хорошо завтра будет хорошо через месяц и выселять такой подход в общем думайте о перфомансе performance тестирование это очень увлекательная деятельность а если вы занимаетесь с каких много разных есть инструментов новом они не помогут если вы не понимаете матчасти и как суть сути проблематике как вообще устроен и performance пространство на что стоит смотреть на что не стоит смотреть и так далее и куча performance данных на самом деле у вас уже есть даже по вашим обычным тестом зачастую не для всех проектов это тоже все очень индивидуально но зачастую можно очень многое сказать нужно просто понимать какие данные вам нужно собрать как и как их затем проанализировать спасибо за внимание какие есть вопросы если у нас есть именно вопросы с спасибо за доклад а можно все таки про инструментарий потому что очень много интересных тем была плотнее и интересует по всего это запуск тестов и проверка исторических данных да и соответственно второе to the state performance стоит да то есть это ваш какая-то собственно внутренней разработка нато открыто разорвут да это ну на текущий момент это наш собственный такой велосипедик на самом деле он пишется не сложно то есть вот графика которую он показывал там какой-то портатив который просто вы выпавшего и данной истин сити там сортирует их по стандартному отклонению или пораниться между операционными системами его написал за один там равно это очень простая тривиальная логика там нет никакого рокет сайенс а сейчас мы его гоняем как-то у себя если окажется что он подходит там не только под наши процессы но и в целом для разных людей и если мы там наведем красоту час там все очень специфически сделано то возможно мы его как нибудь сделаем доступным более широким массам еще просеки да спасибо за доклад вопросов земель 2 1 по поводу использованию не тестов для того что производительность вроде круто но у и тестов есть подготовкой не тестов как-то не цивилизационный код assert и они могут быть тяжелыми вот как можно доверять про длительности фомс тестов если есть накладные расходы как вы с этим справляетесь значит смотрите идея в следующем идея не проверить performance как таковой да это не методология для того чтобы проверить что сейчас все хорошо это методология для того чтобы проверить что не стала хуже чем вчера если у вас вот wasd с sawyer ходам с оси артами вчера работал десять секунд и сегодня стал работать 20 секунд то он ну наверное стоит понять почему так произошло возможно а сердца стали тупи бывает такое но чаще всего какие-то изменения в логике кодах даже на особенно на стыке ну в интеграционных тестах на стыке под систем они очень сильно эффекта те или иные тест это здесь за независимость не в сторону от тестов к коду что если ты что-то плохое показывать то что в коде не не так а что если у вас в коде поселилась серьезная performance деградация то она за эффекте тнт став если поискать засек чины и тесты то можно найти проблему до принципе логично тем менее сама логика может немного меньше по времени полне зачем визуализация дома просто не найдем эти проблемы может быть а я не горел что вы найдете все проблемы это просто один из способов который очень простой на него уходит очень мало не знаю времени сил что все это посмотреть но возможно в вашем проекте он не даст никакого толку но вот например у нас мы там смотрим raider ришар переде you он в каждом из проектов нашел очень много проблем про которые мы просто не подозревали это ещё один инструмент в арсенале performance инженера спасибо да вон еще вопросик я хотел спросить какой процент тестов и используется primergy и каким образом их выбирали . ограничили время и сколько занимает сколько покрывает деле тест но смотрите у нас там орут всего проходит за но не знаю там пару часов может быть может за полтора как как повезет в теле тесты мы заносим именно специальные perfomance test и про которые мы либо знаем что они дико долги там не знаю там то сможет идти просто там 10 минут 1 а тестов много либо это теста в которых нужно много как раз операции чтобы добиться более устойчивых показателей по performance у вот но функционально у нас так они еще у нас разбиты на много частей то есть мы не гоняем все тесты там друг за другом на одной машине и нас разбита на много много много кучек и каждая кучка запускается на своей выделенной машине за счет этого у нас field ну проходит быстро шапочки а сколько это все стоит в плане железо человека дней и кто-нибудь оценивал окупается ли все это что что я я много рассказывал ну в смысле все это форме тестирования она является ли профита был вообще она практиковать переносит мнению с кем-то другим затем что мы не зависит зависит от ваших бизнес-целей то есть насколько вам важен ваш ваши performance показатели вот например на там если говорить про raider в райдере performance это один из таких selling point вот не разрешают говорить там про называть конкретные другие бьешь кино вот если мы возьмем какую-нибудь другую популярную тут над арби яшку в ней могут в ней могут иногда обнаруживаться определенные проблемы с производительностью что она работает не так быстро как хотелось бы вот а в райдере многие вещи отрабатывают намного быстрее и поэтому многие люди переходят с других тут на этой дыре жек на raider не за то что у нас там много пищи не слышно все удобно просто потому что можно открыть там проект пить из solution с 500 ме проектами за какое-то вменяемое время и в этом плане мы просто не можем себе позволить деградировать по перфоманса то есть это одно из наших бизнес требований тут не перечь не про то окупится она или не окупится речь про то как найти способ чтобы performance не уходил в никуда просто нас но нет других способов так а сколько железок сколько едет используется всего в форме тестирования примерно у нас на самом деле для именно performance performance чудо тестов очень ограниченный пул у нас есть например там три абсолютно одинаковых да да да давайте дать в кулуарах уже всем спасибо за внимание еще раз спасибо что пришли если что меня можно поймать и задать то что вы не распяли спросить дальше"
}