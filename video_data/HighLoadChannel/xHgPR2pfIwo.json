{
  "video_id": "xHgPR2pfIwo",
  "channel": "HighLoadChannel",
  "title": "Service Mesh Big Survey / Максим Чудновский (СберТех)",
  "views": 445,
  "duration": 2674,
  "published": "2023-10-06T07:24:33-07:00",
  "text": "очень рад что еще Нашлись силы чтобы послушать интересные доклады в конце конференции Большое спасибо стараюсь быть интересным Ну давайте начинать пару слов обо мне как уже сказали Меня зовут Максим Я работаю с бертехе Я занимаюсь развитием интеграционной платформы синапс синапс это облачная интеграционная платформа Она имеет поддержку всех возможных стилей интеграции rpc файловый интеграции шаблоны even dreaming архитектор В общем Можно найти любую интеграцию по душе и так как там есть rpc естественно там есть сервис mash и как раз я занимаюсь разработкой развитием сервис и поэтому Как видно из названий из нашей платформы говорить мы сегодня будем Про Сервис в начале совсем немного теории ровно для того чтобы сбиться в определениях и говорить друг другом на одном языке и Лучше понимать что будет происходить дальше собственно Что такое сервис mash все же смеешь это достаточно простой паттерн интеграционный Для обеспечения механизмов сетевой упругости повышения безопасности и повышения обозреваемости приложений чаще всего сервис использует в облачных инфраструктурах это контейнеры и губернатос соответственно концептуальность состоит из двух частей первая часть это Control playing отвечает за то чтобы назначать распространять политики маршрутизации трафика распространять какие-то артефакты безопасности сертификаты такие на ключи отвечает за сбор телеметрии и интеграцию с внешней инфраструктурой то есть чаще всего это инфраструктура открыта ключей для инфраструктура журналирования мониторинга и так далее да и taplane размещается непосредственно рядышком с приложениями и занимается очень простым делом Он занимается тем что исполняет политики которые получит от Control plain соответственно Data Plane это чаще всего это сетевой прокси который отвечают за маршрутизацию балансировку трафика в нашей нагруженной среде механизма сетевой упругости которых я говорил Это тайм-аут и ретрайкеры и так далее вот да и такой непосредственно выполняет аутентификацию авторизацию всех вызовов и формирует три испаны формируют метрики для того чтобы а-а повысить уровень обермобилити и собственно всё вот сервис Он очень простой его все знают а много кто использует но э на самом деле я Достаточно давно уже занимаюсь разработкой сервис мыши за эти годы решение всего лишь появилось Ну очень много и на первый взгляд э они все делают одно и то же и различаются в каких-то нюансах и насколько важны эти нюансы не очень понятно и Давайте попробуем об этом поговорить и узнать о в каких нюансах эти сервисы реализации сервис mash могут различаться собственно первое что может различаться это компонентный состав ctrline как нетрудно догадаться он может быть микросервисным потому что с точки с функциональной точки зрения Control playing он очень хорошо бьётся на функциональный домены вот нам всё равно Discovery пожалуйста не карась-сервис А вот у нас здесь э работа с сертификатами пожалуйста нам сертификейшн или встречная ссорите А у нас detaplay на сайт карах нужны как-то инжектить пожалуйста вот у нас появляется фокус-сервис всё отлично разбивается а Однако в противовес этому есть и другой подход собственно монолитный когда все эти функции объединяются в рамках одного Димана бежит где-то у вас кластере и занимается полезными делами сервис mash собственно это демон простой его удобно сопровождать и все становится гораздо проще с этим гораздо легче жить с точки зрения API Control Plane может быть проприетарным то есть это вот Какая история разработчики сделали сервис mash Решили как вы будете управлять трафик по их мнению вот как им понравится и сделали такое Вот и вы будете жить с этими Пи это то как вы будете взаимодействовать с мышом как вы будете настраивать балансировку настраивать авторизацию и так далее следующий вариант это на самом деле пришёл из мира губернации и изначально отвечал за ингестрафик то есть есть большой вопрос Как заводить трафик кластеры Как сделать это понятным удобным образом например разделить зону ответственности между теми людьми инфраструктурными командами которые отвечают за ну там ингр из класса за тот индекс контроллер который в реальности будет бежать и пользователями которым нужно просто сделать какой-то http-road и завести трафик на свой World соответственно гейтвы Я родился там но получилось так что он очень хорошо подходит управление http трафиком и соответственно во многих реализациях Service mash Это нашло понимание и он потихонечку начинает туда перетекать и третий вариант который вы можете найти это сервис mash интерфейсфификейшн это Open sourn спецификация которая претендует на то чтобы стать неким стандартом по управлению сервис mash соответственно на текущий момент smi и Gateway двигаются в сторону объединения и скорее всего в будущем Мы увидим уже какую-то обобщённую спецификацию которая будет включать те или иные стандарты Однако вот эти три варианта вы встретите в любом сервисе В плане controlla с точки зрения Data Plane самый распространенный тип это Data Plane который построен на сайтах конструкция простая с рядом рядом с вашим Application контейнером размещается дополнительный контейнер в котором размещается сетевой прокси У какой бы то ни было это на самом деле по большому счету это даже не важно настраивается правило редиректа трафика таким образом что весь трафик на ваш плод и из вашего плода проходит через этот сайт контейнер который уже занимается L4 и L7 в процессингом соответственно э многодов много аплично много сайтов следующий вариант это not dimon идея здесь в том что у вашего плода больше нет сайдкара этот сайт Car спускается ниже на уровень оркеров губернатос То есть он э как бы шевелится между всеми ворклоудами между всеми подводами которые запущены сейчас на этом воркеере а-а понятно что это помогает это очень сильно сэкономить в плане ресурсов потому что гораздо меньше экземпляров сетевого прокси нам нужно Однако несет с собой ряд проблем о которых мы поговорим в финальной части доклада тем не менее такой подход имеет место быть это хороший подход много где встречается и еще один вариант это прокси лес А здесь идея совсем простая давайте откажемся от каких-либо сетевых прокси нам не нужны ни демоны ни сайдкары Давайте все это вернём на уровень приложения как мы делали в 2010 году подход хороший за исключением того что очень многие плюсы от servicemash его неинвазивного подключения этот подход Ну забирает собой и поэтому этот подход чаще всего применяется непосредственно таким образом что поддержка сервис встраивается сразу в тот фреймворк который вы используете например так Сделано в grpc вот если у вас свежий grpc вы на нем делаете сервисы в пару кликов Вы можете сделать так чтобы ваши gpc сервис поддерживал всю функциональность Data Plane и мог подключиться к какому-нибудь Control plain и ему уже не нужен никакой редирект соответственно это снижает latence и это снижает потребление ресурсов Однако лишает нас необходимой гибкости в плане там подключения обновления платформенных функций которые дает сервис mash с точки зрения API есть на самом деле такой протокол который называется xds xds вышел как протокол из-за проекта anwayproxy это opensort сетевой прокси который применяется во многом количестве сервис позволяет на самом деле настроить полностью все параметры нашего соединения он иерархический он не сложный То есть он отвечает и за сервис Discovery is a за правило маршрутизации какие-то экшен он отвечает полностью за демонстрацию бэкэнда и техинпоинтов которые будут обслуживать Этот бэкенд как они будут распределяться трафик по весам какие будут приоритеты Какая зональность будет поинтов как будет переключаться трафик в общем-то и здесь отвечает на все эти вопросы и позволяет динамически конфигурировать сетевой прокси и так уж сложилось что и где с получил широкий adoption вместе с вместе с самим проектом и стал де-факто наверное знаете некоторым стандартам в плане API для Data Plane в Service mash Но это именно что де-факто исходя из комьюнити из популярности собственно часть людей используют в построении сервис DS а часть людей его Не используют и тогда получается который тоже может имеет место быть там например есть сервис построенной инженексик там будет своя схема конфигурирования там по-моему называется до иными кап-стрим если прокси то будет другой API Ну в общем-то никакого стандарта не будет и в случае каких-то проблем вам придется погружаться и разбирать А что там за протокол вы никогда не найдете на это ответа С точки зрения перехвата трафика смотрите когда я рассказывал про сервис Я сказал Ключевая идея Да и там сетевой трафик забирает на себя и уже делает там L4 всем процессинг Как он может это сделать он может это сделать вот сразу запускаясь внутри повода то есть внутри нашего повода появляются еще один контейнер который на самом деле и нет контейнер под запускается и соответственно запускается нет контейнер и нет контейнер настраивает оверлейную сеть на воркере губернатос чтобы в рамках нашего повода трафик редиректился на сайтах соответственно всё хорошо никаких дополнительных компонентов а под по большому счёту настраивает правила или директа сам для себя это следующая схема она плоха только тем что нужно выдать дополнительные полномочия для того чтобы можно было настроить правила iptables это полномочия на трое на тадмин и вы их выдаем не только нет контейнером и выдаем их сразу на сервис аккаунт под которым будет бегать пот и таким образом наш Ворк лот Application контейнер Тоже полностью получает эти привилегии они ему в общем-то абсолютно не нужны и получается что мы нарушаем правила минимальных привилегий это может быть чувствительно в тех средах где очень высокие требования по информационной безопасности другой подход это нодлевел соответственно здесь идея проста что А в рамках нашего плода запускается только два контейнера никакого редиректа там не происходит Ну где-то в классе есть некоторые Диман который а-а делает это за нас и когда запускается пот это Диман это Диман видит Ага это под Он входит в сервис сейчас мы здесь настроим редиректора и соответственно с полномочиями работает сам Диман в губернатоса сеть так устроена Что чаще всего там используется не чаще всего почти всегда используется Тина это контейнер Нетворк интерфейс и соответственно вот этот сиена технология синяя Она позволяет использовать плагины основной плагин инициализирует и исполняет непосредственно оверлейную сеть губернатос а дополнительные могут сделать чего-нибудь полезного и чаще всего вот есть дополнительные сильные плагин в составе всё время снимаешь который в момент инициализации сетевого стека на нашем фоне э-э настраивает правила редиректора и вот этот Диман про который я говорил он делает ровно то что раскладывает нужные бинарь э-э в нужный потолок на всех воркерах Где будут запускаться под сервис потому что сеной а-а подразумевает что там всего четыре операции и вызов их это запуск конкретного бинарника на линуксовом Косте во всяком случае сейчас в будущем это поменяется на живописи Итого мы получили 5 пунктов по которым мы можем отранжировать любой сервис Маш вот мы можем посмотреть на него взять вот эти пять пунктов заполнить табличку и понять что это такое и в каком кейсе В каком случае это хорошо применять вот подходит он под текущие требования или нам нужно посмотреть что-нибудь другое очень удобно но я добавил шестой пункт extensions потому что он важен это не очень классификация А почему Он важен мы поговорим в следующей части доклада это классификацию получили Но на самом деле большой вопрос работает ли все это дело или это просто набор из пяти пунктов ну которые мы вывели умозрительно и они никак не стыкуются с реальным миром Давайте проверим как стыкуются с реальным миром на реальных решениях из того что вы можете сейчас найти в Open Source собственно первый сервис наверное самый популярный и много кто про него слышал это истину истио это Интересная история смеш тем что сейчас у него монолитный контр-плейн но так было не всегда когда начинался проект истинного Control plain был микросервисный Это был стандартный Ну весьма необычный транзишен от микросервисов коммуналиту в решении которые делаются для микросервисов а Однако на это были веские причины соответственно с точки зрения API поддерживаются все три варианта Единственное что спецификация smi поддерживается через специальный адаптеры А в пла Вы можете использовать сайт кары Вы можете использовать подход прокси леса подключать например gpc сервисы напрямую и также есть вариант Но Дима но он на самом деле еще в ранние в ранней быть может быть вы слышали провести ambient mash там разделяется лучше терял всем процессинг на разные вычислительные компоненты и соответственно таким образом появляется Дима для туннелирования В общем Достаточно знать что это просто есть и будет дальше Потому что построен на базе новой прокси перехват трафика Может быть как внутри плода так и внутри так и непосредственно на ноги поддерживается И то И то И что важно из экстеншинов поддерживается в бсмплели причём были поддерживается не просто потому что это умеет инвой а потому что есть API который позволяет вам нормально это использовать То есть вы можете сказать хочу такой вас модуль пожалуйста а сходи в доке рейджестре забери этот васан модуль закошируй и пускай он исполняется на прокси и это позволяет вам действительно вот эти экстеншины во-первых тиражировать во-вторых делают эту историю управляемой потому что в другом случае очень сложно это использовать второй сервис mash про который часто говорят это линкерди А линкерди э-э ну чем-то похож они все похожи соответственно контрольная панель имеет архитектуру микросервисов причём эти микросервисы как я и говорил чётко ограниченный своими функциональными доменами и даже незамысловато называются но у нас есть identity которая работает identity у нас есть Destination который работает С десятинами это всё время Discovery и у нас есть прокси инжектор который отвечает за инжектор сайдкаров с точки зрения API Вы можете использовать в линкерде smi либо пропитарный E5 который предлагается авторами этого сервис спецификация здесь отсутствует в плане dataplay на в линкерде вам доступны только сайт Car и это всегда будет сайт каровым мышца всеми его преимуществами и недостатками будет проприетарный о чем я говорю в линкерде в качестве сетевых прокси используется специальный прокси он так и называется linkerdi prox и он используется только в LinkedIn соответственно у него свой собственный протокол и в случае если что-то пойдёт не так но очень сложно понимать а что же там реально идёт по проводам Какая конфигурация текущего сайта Кара и ебашить Это не просто с точки зрения перехвата трафики стандарт перехвата трафика стандартно можно делать на уровне подачи нет контейнеры можно делать на уровне ноду через сильные плагин А здесь никаких нет дальше есть интересный сервис Smash который называется кума комната точно также на продукт он тоже построен на базе NV Proxy поэтому вы увидите те же самые сайдкары и тот же самый протокол xds для Data Plane он имеет монолитный контур клей То есть это один демон который бежит в нем специфику губернаторстве заправляет всем там же встроенный UI что интересно кума умеет хранить свой стейт не только в хранилищах губернации То есть это не только эти сиди и кубапе это может быть внешнее хранилище авторы называют посткель это может быть очень полезно Если вы строите мультикластерную систему И вам нужен мультипластерный сервис Discovery такая особенность Идем дальше Engine X ngx это известный бренд собственно инженец занимался Вот обработка сетевого трафика задолго до того как мы стали говорить про сервис-меши Было бы странно если бы инженекс не выпустил свой собственный сервис Smash и они выпустили Control plain Micro Service точно также все стандартно У нас есть сервис Discovery а части которые работают сертификатами часть которая работает в zerobility устроена это все на сайтах и только на сайт-карах и в качестве сайта Кара там понятно инженекс а соответственно Да И топлейный пиай проприетарный вот на этой конференции был доклад от разработчиков одного из форков инженерных свежего который назывался Engine Да Engine спасибо Вот соответственно они там тоже говорили про динамические обстримы внжи и вот когда это появится было бы круто если бы Энджи стал дроп энри плейсмента для сайтаров в разных сервисах Почему нет перехват трафика в инженекс сервис только на уровне кодов и расширение не поддерживаются То есть инженекс это такая знаете очень-очень ранее реализация Service mash вот в плане архитектуров и дизайна Идем дальше сириумэш сильно на самом деле это реализация в оверлильной сети изначально Это сильное плагин который построен на базе bpf но ребята пошли дальше и сделали свой собственный сервис Что здесь интересного интересно здесь то что как такового редиректа трафика больше не требуется потому что он спускается на уровень оверлейной сети и вот тот синий плагин который отвечает за реализацию этой сети он сразу же начинает отвечает за реализацию Service mash и забирает на себя или 4 процессинг это очень красивая идея Однако n7 Processing сделать сложнее Поэтому в дополнение к этому А в силе у меня живётся сервис прокси это известный нам инвой который размещается по модели Но вот Диман То есть это один вой который размещается на каждом горке или по интернетности отвечает за L7 Processing на этой ноге а с точки зрения API это пропитанные пи почему он проприетарный потому что он тоже делится на две части первая часть это Network policy в губернатоса нетворкполисе который экстендит силиум соответственно таким образом вы можете настраивать L4 А вот L7 здесь ситуация интересная потому что в силе уме не стали мудрить разработку своего API и сказали а давайте Вы будете использовать Экзист напрямую как это есть военный прокси Почему нет сделали crd Которая так и называется Это хорошо это дает максимальную гибкость и я хотел на самом деле на следующий слайд добавить Exist конфигурацию одного из сайдкаров из нашего продакшена Но это увеличивал презентацию на 50 слайдов поэтому я не стал это делать очень долго кликать придется в общем и где с конфиг он достаточно объемный и сложный писать его напрямую конфигурировать сайт каре может быть непросто таким образом скорее всего это решение будет улучшено в будущем но сейчас это нужно иметь в виду с точки зрения экстеншинов здесь есть веб-самбли модули потому что мы напрямую конфигурируем его и вот если мы Напрямую это делаем собственно Веба сэмбли мы можем загрузить там где в тех сервис-мешек которые э-э также построены на базе avo и функциональность в ассамбле есть но вот API для её конфигурирования отсутствует и поэтому пользоваться этим получается крайне сложно Идем дальше Консул Service mesh решения Консул на самом деле до эпохи сервиса mash также было всем известно как Киеве или сторы сервис Discovery но собственно если есть сервер Discovery то должен появиться сервис mash Он и появился Control Play проприетарный но используется сервис mash-интерфейс добавляя сюда стандартизацию построен на сайтах это снова nvid Proxy здесь без откровений Вот соответственно API xds перехват трафика только на уровне кодов через контейнеры и нет экстеншинов потому что прилитарное пиани не поддерживает этой самой тоже интересная особенность Несмотря на то что перехват трафика в консульмаш сделан на уровне пудов но на уровне оркера губернатор всё равно есть Диман А который отвечает уже не за конфигурирование сетевого стека А за работу с Консул сервис Discovery с Консул сервер и там размещаются Консул клиентов такая архитектурная особенность идём дальше Open Service mash Open Service mash он Ну вот вы знаете он бай-дизайн Open то есть в качестве Control plain а там используется Go Control plain это такой референсный проект для организации Control plain в servicemash и Service Discovery А в качестве dataplain используется опять NV Proxy с поддержкой HDS то есть тоже открытый стандарт и вот Open Service mashion потому и Open потому что во всех его частях используется а открытое решение которое получили широкий adoption в комьюнити но э скажем так погружаясь внутрь и мы видим что поддерживает тот поддерживает столько сервис интерфейс то есть с пекой переклад трафика только на уровне кодов через они контейнеры с выдачей промыш э-э повышенных сетевых привилегий пользовательским ворклоудом и не поддерживается extent следующий сервис mash у нас travic в обзоре Traffic интересен тем что это наверное первый Service mash который был построен на базе Data Plane который построен по принципу not dimon то есть изначально была конструкция что NV Proxy размещается один раз на Work или cubernets и обрабатывает весь сетевой трафик очень сильная экономия при этом накладные ресурсы которые требуются для запуска с сервисом Ешь С точки зрения API поддерживается только стандарты SMS соответственно экстеншины на наинвои будут недоступны а трафик capture Но для его потому что под левел в принципе нет входах кроме Application контейнеров ничего нет и контроль микросервисный и наверное последний в нашем сегодняшнем большом обзоре это синапс-сервис Мэш синапс всё же смешно но тем не менее это российский сервис Вы можете его получить также как и все остальные без проблем а-а когда начинался проект синапс-сервис он друзейно вырос из истила соответственно Он полностью совместим по E5 истио и имеет все функциональные особенности которые там есть за некоторыми исключениями в нашем сервисе mash Control plain micro-сервисный Это позволяет нам делать extension и поверхностью и Не отходить далеко от кодовой Базы и Стива для того чтобы не делать хардфорк и помимо стандартного xds протокола которым управляется Data Plane мы поддерживаем Lazy xds и вот как раз Решение той самой проблемы которую я говорил ранее xds он большой А если у вас большой сервис в нем десятки тысяч поводов то скажем сервис Discovery будет тратить очень много ресурсов на обслуживание всего этого понятно что можно настроить однако все равно это будет дорого и собственно лейзик здесь позволяет использовать не полный XS конфигурацию а только ту которая нужна в нашем На текущий момент если наш сервис работает только с двумя другими и остальную тысячу игнорирует то зачем ему про них в принципе знать соответственно мы загружаем ему только то что ему нужно и потом в ленивом режиме подгружаем что-то еще если ему это потребуется Окей мы прошли обзор Service mash теперь мы знаем какая есть классификация мы знаем какие есть решения они действительно классифицируются Понятно где там на что смотреть Но какой вывод из этого можно сделать Ну хорошо вот микросервисный контроль или монолитный Ну какая разница нужно как-то оценивать это и оценивать это можно Ну наверное нужно через призму какого-то опыта сервис и собственно Мы предлагаем это сделать соответственно наш опыт сервис Smash это платформе есть синапсервисмеш которую мы разрабатываем он используется в Сбере и первая инсталляции появились в 2018 году на сегодняшний день это Примерно 200 инсталляций в Промышленной среде мы не говорим про тестовые все стейджинговые полигоны под инсталляцией я понимаю один instance Control Plant который обслуживает один или несколько нам спейсов вкладов в одном или нескольких кластерах губернатос Ну или Red hotion соответственно это десятки тысяч обслуживаемых кодов и 17 команд сопровождения которые занимаются а-а Ну получается развитием этих инсталляций 17 команд Не потому что сложно сопровождать а потому что есть организационные деления и соответственно Ну инсталляции принадлежат там конкретному юниту и организуется своя служба сопровождения и Какие проблемы могут Можете вы встретить в Service mash в своем сервис majorney соответственно первый вопрос это масштабирование на которые нужно сразу ответить причем масштабирование именно в количестве кластеров сколько у вас будет кластеров И что вы хотите с этими кластерами делать хотите ли вы чтобы это были большие кластера с гигантским Сервис которые растянут на все эти кластера или вам больше подходит многих малых кластеров которые будут максимально независимы друг от друга и пересекаться только Ну необходимый минимум Когда нужно там прокинуть мостик между несколькими сервисами и собственно в рамках количества кластеров вам конечно же нужно смотреть но на архитектуру ctrlinetype и расширение Потому что когда вы будете строить микросервы Ну мультикластер но инсталляцию любой сервис скорее всего потребует от вас ответить на вопрос А как вы сделаете мультикластерную телем целью Как вы пробросите трафик А нужен ли вам фейловер между кластерами если эти кластера зональные и соответственно Когда будет начнете отвечать на все эти вопросы поймете что стандартного функционала все равно не хватает нужно как-то расширяться поэтому под конкретный кейс нужно смотреть нужный экстеншины дальше мы говорим уже про масштабирование каждого кластера в отдельности и на самом деле масштабирование в cubernets это ну прям целая история потому что у кластера есть много разных разрезов и он масштабирование его это прямо комплексная история сколько нот будет сколько рабочих аргрузок сколько сервисов и так далее и Соответственно в этом разрезе необходимо обращать внимание на тип dataplain а потому что Ну вот как я и говорил да это Play на сайт стоит 10x playing который по модели но Диман построен он будет строить там Ну один X это колоссальная экономия здесь только потому что если вы хотите чтобы ваши ходы были самодостаточными и запускали нет контей как и сами настраивали Директ трафикайте контейнеры тоже стоят ресурсов и на это стоит обращать внимание Идем дальше это взаимное влияние это на самом деле вот прям очень большой вопрос в коммунальных инсталляциях Потому что есть кластер Есть разные команды они начинают его использовать и потом что-то идет не так и соответственно система должна быть устойчива Вот к этому абстрактному что-то идет не так и соответственно област радиус у нас может быть разные в зависимости от того во-первых что произошло компенсация секретов или злоумышленник проник в applicational контейнер целиком и может что-то сделать или вот ваш сосед он вроде бы как живет в другом спейсе губернатоса Однако запустился на той же ноги используют тот же сетевой прокси что и Вы соответственно допустил какую-то утечку произошла компрометация и здесь важно смотреть именно на datape потому что подход Когда у нас сетевой прокси размещается демон на воркере cubernets он Да действительно Ну понятно почему экономит много ресурсов но это не серебряная пуля потому что он приносит много других вопросов особенно в плане безопасности потому что прям железного финансирования волосем-процессинге все равно Ну практически не добиться и таким образом ваш бластер радиус будет увеличиваться и модель сайтарами может быть безопаснее лучше попадать в требования без по безопасности Однако за это придётся заплатить Ну собственно trafficapture то же самое если вы делаете на уровне кодов выживём бластер радиус ваш пот если живёте на нодах бластра радиус вашу Да это тоже нужно учитывать сложности эксплуатации соответственно здесь основной момент это сложность конфигурирования когда Мы начинали работать сервисом что все говорили Смэш Это не просто И на самом деле какое-то время так и было когда посмотришь на конфигурации и тысячи ямов файлов которые нужно написать то невольно начинаешь задумываться А может быть я добавлю библиотеку как раньше и ну и вот понимаешь как-то сложно это все соответственно появляются API проприетарные которые пытаются решить эту проблему А не проще Вот например servicemash Интерфейс это очень простая спецификация там очень понятные правила например для редиректа трафика Однако настолько простая что какие-то хитрые кейсы маршрутизации она не покроет и тогда у вас возникнет проблема таким образом нужно вот искать именно этот баланс который нужен именно вам в том API Control plain который Вы будете использовать Вы можете покрыть любой кейс отдать пользователям сервис Smash который будет уметь всё но он уметь будет это через конфигурацию которая займёт 50 слайдов и Нужно ли это 99 процентам пользователей Ну большой вопрос может быть и нет и поэтому этот баланс важен и вам нужно смотреть на Контр Идем дальше это сила сообщества Ну силой сообщества на самом деле все просто это экстеншины чем больше всего сообщества тем больше И шью тем больше плагинов написано тем больше вероятность того что конкретно ваш кейс уже кем-то решен вы приходите в комьюнити Говорите проблема вам говорят пожалуйста вот если Всё решено или будет решено через год А вот пожалуйста Вам Warcraft Используйте это соответственно сила сообщества тоже важна и у всех решений сервис мышц которые мы посмотрели сообщество разные это нужно иметь в виду а Идем дальше это поддержка открытых стандартов вот здесь то о чем я говорил важно понимать что идет у нас по проводам Потому что когда что-то идет не так в эксплуатации понять что именно пошло не так если используется проприетарный очень сложно В некоторых случаях вообще невозможно сколько не настраиваем мониторинг вот сколько Метрика не собираюсь всё равно будет непонятно соответственно если стандарт открыт первое вы всегда можете посмотреть структуру и понять что происходит раз а второе это уже больше про Enterprise уровень Вы можете найти людей которые понимают что происходит если Ну самостоятельно не получается Нет ресурсов или времени если стандарт закрыт то есть с людьми будет вопрос и самому разобраться может быть непросто собственно Давайте подводить Итоги нашего доклада первое теперь мы знаем все о том Каким бывает все равно сможешь теории мы разобрали все архитектурные стили которые так или иначе применяются в Клауд на этих комьюнити мы понимаем что доступно на практике Есть множество решений Они сделаны по-разному они все хорошие и они просто попадают в разные требования и нужно понимать свои требования и собственно выбирать то решение которое необходимо мы знаем как принимать это решение Как выбирать На какие требования стоит обратить внимание то есть Как подойти к вопросу инсталляции Service mash в конкретном случае и с чем сталкивались мы Ну и последняя фраза золотого топора как не было так и нет нет сервис который решит все проблемы которые будет катастрофически экономичным максимально гибким супер производительным и при этом Супер простым что любой пользователь без документации с ним оберёд разберётся и все равно нужно внимательно подходить к этому оценивать свои требования оценивать решение и выбирать то что подходит именно Вам потому что сила в разнообразии но сегодня у меня все Большое спасибо за внимание Я готов ответить на вопросы Спасибо за доклад я наконец-то разобрался Что такое сервис mash из-за чего он нужен и надеюсь только я маленький президент чтобы ты запоминал про конференцию У нас сейчас наверное самое интересная часть это часть вопросов и ответов и те будет очень сложное задание в конце этой части все-таки выбрать лучше вопрос который мы подарим подарок а компании есть подскажи пожалуйста для себя какую архитектуру считаешь самой удачной и вот конкретно bpf на уровне ноты интересно Не золотая ли это пуля или серебряная с точки зрения безопасности Спасибо я понял вопрос но собственно Ладно давайте не буду щелкать так как я разрабатываю серый сможешь конечно его архитектура мне больше всего по душе на самом деле в плане архитектуры Control plain Несмотря на то что монолитный контроль гораздо проще в сопровождении все равно микросервисный но выигрышный в плане того что мы можем расширяться да и мы можем не в случае какого-то интерпрета решения или кастомизация решения под себя не отбегать от далеко от обстрима и сохранять свою связь с комьюнити с точки зрения архитектуры на самом деле и сайдкар в равной степени хороши Вот они просто попадают Ну как бы в разные Вот они вот расходятся в разные направления и это хорошо и это нужно смотреть именно на Бласт радиуса Требования по безопасности если строгие то стоит кары если что-то можно пропустить то спуститься на но Диман А вот BP акселерация она на самом деле Хороша и там и там потому что в случае с Каро вода тоже есть bpf и акселерации Например у нас она есть да И вот этот редирект трафика который я говорил он э осуществляется на уровне там BP программы да И это тоже работает максимально быстро самая красивая идея в этом это то что сейчас Service mash потихонечку спускается на уровень сиена и то есть вот он начинает объединяться и как это сделано в силе уме Вот и Да вот это на текущий момент вот такой самый трендовый подход и наверное один из самых перспективных но собственно Мы тоже в эту сторону идем и у нас тоже появляются свои механизмы для тонирования мы разделяем сознательно или 4 или 7 Processing и спускаемся туда Поэтому с точки зрения Вот именно сетевого стекла bpf акселерация и уровень мне нравится больше всего Спасибо за ответ Спасибо за вопрос Я прошу не забывать оценивать доклад чтобы максимум знал что улучшить следующие раз и на следующий вопрос а Какие сервисы могут работать не на губернатиках вообще не на контейнерных на отлично вопрос на самом деле скажу так почти всем но больше из них часть но здесь Но с оговоркой я сейчас поясню идея в том что Control plain для Service mash чаще всего это ну такой клаудный тип решения это что-то что бежит в губернатосе А вот в плане Data Plane можно подключать Work Load который не бегут в губернатосе а бегут на Ну где-то там на бэр-метал на виртуальных машинах В общем Там живут не контей не контейнеризованные Ворк клоуиды и соответственно вот этот вот этот мостик прокидывается и он прокидывается Ну на самом деле в большинстве решения о которых я сегодня рассказал нестандартный какой-то а ДТП будет какой-нибудь Ну мы выбираем просто контролл-лейн который поддерживает этот режим Вот и соответственно у нас появляется гибридные части World of контейнеризованные бегут в губернатос часть World of бегут на берлитов северо- серверах и виртуальных машинах Service позволяет вот убрать этот гэп и сделать так как будто это один и тот же сервис Вот например сено в сервис только умеет если будет интересно в дискуссионке я расскажу за счёт чего это достигается Спасибо и у нас следующий вопрос вопрос А вот есть два сразу прям Большое спасибо за доклад вопрос заключается в том что есть ли какие-то нюансы использования гибридной модели когда используется одновременно сайт Car и proxls Это хороший вопрос и нюансы есть нюансы заключаются в том что в зависимости от фреймворка который вы берете для прокси леса не все возможности их здесь а там еще реализованы если мы возьмем grpc то вы можете столкнуться с тем что политика балансировки заезжает на сайт кары а вот с этой частью прокси леса возникают вопросы она просто там в grpc ещё не реализована а вот с точки зрения но такой базовой эксплуатации да то есть как ворклоды будут видеть друг друга Как вы будете рулить трафиком как будет обеспечен ротация сертификатов всё то же самое потому что ну там прокси-лесье просто XS это такой же XS Как у вас на сайт kari Да никакой разницы А вот разница по ресурсам и логин есть Можем тоже в дискуссионке поговорить об этом спасибо Максим вечер Добрый Я бы хотел вас спросить про рейд ли митинг вот используют его Да как правило там на вафах и в используют в рамках аппетита Вы и внутри вот представим что у нас есть там ваф У нас есть кубик внутрь кубика стоит там да блок в и вот мы натягиваем технологию сервис mesh поскольку это все просеть вот насколько вообще разумно вот рейтинг настраивать именно на этом этапе от внешних запросов там внутрь системы Я понял хорошо Хороший вопрос но смотрите рейд лимитинг как часть функциональности это действительно Он входит почти во все сервисы не всегда нативно иногда через extension но он там есть вот как бы верно заметили но по большому счету в губернатор У нас есть два типа трафика Но это ринггресс трафик и внутри кластерный трафик Вот и внутри кластерный трафик это понятно что сервис вот здесь прям Ну как это вот поставили понимаешь и начали лимитить ворклоуда внутри кластера с точки с внешней точки зрения но я вам сейчас скажу свое личное мнение если у вас ингресс класс Ingress Gateway подключен к сервису является одновременно и границей кластера и границы мыша то почему нет вот если вы разделяете историю У вас своя собственная Ingress история и у вас там как это вы там выходите дальше за вафлит ну на вафли и так далее да то как бы здесь ну не факт что это будет супер полезно Благодарю А у нас есть вопрос чата насколько известны истину имеет только два протокола htp и grpc А в чем проблема упаковки другие протоколов и Умеет ли это твоё решение прекрасный вопрос но на самом деле смотрите поддерживает Больше чем два протокола То есть это i7 протокол http palanger PC понятно И на самом деле все что работает по tcp есть его тоже поддерживается единственное что для истева это ну Black Box он видит только статистику которая может собрать tcp вот tcp-шной сессии он может поуправлять в общем-то L4 Processing там есть он не знает деталей о конкретном протоколе Application протоколе да то есть он не понимает что это погрыз что он не понимает что это кавка Ну потому что он видит это как обычный tcp коннекты и больше ничего и его нужно учить вот чтобы он это понимал а учить это дорого потому что нужно реверсить сам протокол и мы это делаем мы действительно реверсим протоколы для тех решений которые есть у нас в платформе потому что платформе большая Там есть много там Понятно Есть Ну там стриминговые вещи там есть база данных и так далее Вот и в рамках своего решения учим наш сервис Smash понимать эти протоколы А вопрос а планируете ли Вы открыть то что в ущелье для сообщества чтобы нам всем снова это не реверсить слушайте Но это на самом деле сложный вопрос по многим факторам Давайте его на сегодня забрать и у нас есть время для финального вопроса на доклад Максима но вопросов нету всем все понятно нет У нас есть один вопрос к максимум выбери самый лучший вопрос но хорошо На самом деле все вопросы были замечательные мне очень понравились но так как я большой фанат ebpf а то вопрос про BF получает первое место Напомни какой участник проекта всё отлично вы поняли я понял Всем спасибо Кто пришёл на этот доклад у нас следующий доклад Через 20 минут и вы наверное видите кто вы спикером Спасибо большое"
}