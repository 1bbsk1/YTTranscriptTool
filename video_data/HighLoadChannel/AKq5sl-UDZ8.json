{
  "video_id": "AKq5sl-UDZ8",
  "channel": "HighLoadChannel",
  "title": "Высокодоступное приложение в кластере Kubernetes / Марсель Ибраев (Слёрм)",
  "views": 2392,
  "duration": 2465,
  "published": "2021-10-04T02:44:45-07:00",
  "text": "спасибо раз-раз-раз суетно видно видно точно привычкой заново видно поехали всем привет дорогие друзья уважаемые коллеги многие спикеры для меня уже говорили что рады видеть всех водворения что к вид этот под задолбал и так далее я искренне и их поддерживаю в этом спасибо организаторам хай вода что таки дали нам возможность встретиться и обмениваться не только знаниями но и микрофлоры тема моего докладываться к доступное приложение в play store cabernet с смотрите проблематика в следующем мы поставили кластер кубом а сзади поле туда свое приложение но это только пол дела потому что дальше нам нужно теперь все это дело поддерживать эксплуатировать и так далее и вот здесь за несколько лет работы с кудрин это сама у меня и у моих коллег возникали разные ситуации ну или как принято говорить войти кейсы вот про эти кейсы я бы хотел вам рассказать по сути это грабли на которой мы наступали и которые приводили к недоступности продакшен приложения учитесь на наших ошибках и не повторяйте то что мы вытворяли вот зовут меня марсель ибраев я сетевой компании flir являюсь инженером с восьмилетним стажем certify обернется администратор занимался внедрения мекку брюнет с в рамках компании south bridge и также являюсь разработчиком и спикером компании слез с прелюдия ми покончено давайте перейдем сразу кейсом единственное смотрите проведем небольшой эксперимент мне сейчас должен позвонить мой брат-близнец я короче его выведу на экран я не знаю как технически работает ну в общем он должен позвонить он должен появиться здесь он должен видеть вас должен видеть меня и мы вместе в общем этот доклад проведем единственно этот брат я вам честно скажу о не особо соски ли такой да ну в общем сейчас сами увидите сейчас он должен набрать сейчас мы посмотрим как как работает как бы вы видеть себя аватарка какая у него стоит не очень хочется брать трубку договаривались поэтому привет друзья пацаны и девчонки всех рад видеть всех убил ну что ты хотя бы это тачки с капюшоном снимите узел тебе не колоритным спасибо коллеги первый кейс который я сделать давайте я вам расскажу первый кейс первую историю подожди во рту смысле ты расскажешь первый кейс я стою на сцене давай я расскажу 1 кекс я буду вести мероприятие хорошо копируйте аргументация конечно типы и я на сцене но и квартире сизую чудово я буду вести искусных то есть ты хочешь решить вопрос показался вам правильно ты серьезно ну давай камень ножницы бумага раз два три олег ты переживает следующий раз повезет ну что ребята как обещал сейчас расскажу первый кейс давайте шарик ран секундочку вот так чтобы было видно и меня и презенташку итак первый кейс первая история которую я хочу рассказать связано с регламентными работами на экране вы видите картинку одно из моих любимейших которая описывает в принципе ситуация в принципе суть вопроса регламентных работ да вот но давайте ближе к делу собственно если у вас поверни таз не в облаке где-то там на хостинге на своих железках то вам периодически нужно делать будет какие-то ремонтные работы а именно там обновлять операционку заниматься каким-то обновлением там ядра тюнингом ядра обновлением сетевых компонентов устранением критических уязвимостей так далее так далее то есть такая знаете типовая админской а работа так вот суть ситуации состояла следующем разработчики выкатывают релиз приложения приложение работал в двух репликах использовал стратегий обновления rolling апдейт соответствовал нас одна реплика старая погасла 2 реплика нового приложения вот-вот должна сейчас стартануть но вот эти вот буквально несколько секунд приложение работает в одной реплики потому что повторюсь старое погасла новая сейчас вот-вот стартанет и админы именно в этот момент начинают проводить регламентные работы что именно они делают не суть важно но единственная рабочая реплика этого приложения было именно на том сервере на котором админы проводили свои работы ну я думаю тут даже не нужно говорить об итогах потому что тут написано профит но естественно никакого профита мы не получаем мы получаем буквально несколько секунд но факт что downtime нашего приложения и еще одна картинка которая уже описывать ситуацию для админов да к чему им готовиться так сказать потому что действительно в этой ситуации там все будут говорить что виноваты админы да то что тип вот зачем полезли там что-то делаете да мы тут собственный релизе им так далее так далее как из этой ситуации можно выйти и можно конечно заставить админов работать по ночам опять как это было раньше да там дипой за соответственно там ковыряться счет на своих серверах там где-то в субботу ночью когда трафика нет так далее так далее но у нас же infrastructure закон то есть что инженера зря получается учили всякие инси бутаро формы там так далее так далее тот же купер нетто с чтобы потом опять же сидеть и работать по ночам ну как то не очень прикольно потому что раз у нас infrastructure запад инженер хочет работать с инфраструктурой как с кодом прям в реал тайме в любое время также какие-то делают разработчики как из этой ситуации выйти у нас есть прекрасный инструмент от губернатора который называется под destruction badger этот инструмент ограничивать количество инстансов которые могут быть недоступны одновременно то есть вы можете в манифесте фонде ссср обшем budget или сокращенно pete by описать что у нас максимально недоступное количество реплик например может быть дверь или минимальное количество доступных реплик может быть 1 то есть вот этим самым ограничить вот это количество реплик который будут недоступны при этом обратите внимание никакой магии позе сообщим валют не делает питере работает с и fiction api и вспоминаем как вообще ведет себя администратор когда ему надо провести какие-то работы с сервером на котором работает cabernet он заходит на сервер делает куб ситель drain выгоняем секунды соответствии с этого сервера и уже на чистом на свободным сервере занимается своей работы по хорошему это должно происходить вот так и соответственно как раз таки купили drain она тоже работает selection и тем самым у нас поди сообщим budget по сути контролирует поведение коп стиль train то есть под destruction budget не дает куб ситель рэй неслучайно погасить какое-то критичное количество реплик подробнее почитать об этом можно документация также посылочки чисто технически там ничего сложного нет то есть это вот такого рода обычный манифест здесь у нас вот давайте я прям покажу спецификации мы описываем оксана вёльва или миновали был количество реплик который мы бы хотели оставить нашего приложения и через селектор через матчей был знакомый нам уже конструкции мы указываем какое именно приложение с каким именно приложением и этот поди сразу budget связь ну и потом этот манифест мы кладем в репозитории с кодом то есть у нас появляется еще один для блэк pdb и по моему мнению по destruction budget должны писать разработчики потому что только они знают насколько приложение которое они делают оно важно сколько реплик там важно сохранить так далее так далее и по destruction budget это своего рода ямал такой манифест яма соглашения между админами и разработчиками в плане того вот на сколько важно то или иное приложение закончил да давай слайды ok увидимся к сожалению второй кейс 2 истории жизни который я хочу рассказать прозошло лично со мной на проекте который я обслуживал ситуация в следующем приложении работать через deployment ничего критичного сверх естественного нет 3 реплики приложения стратегия обновления rolling апдейт а суть проблемы в том что когда мы выкатываем релиз новый релиз мы получаем кратковременной пятисотке причем их очень мало и они плавающие иногда получаем иногда не получаем и как следствие этого здесь написано в тэк от начальства на самом деле мы получали неудобный вопрос от клиента на тему того о чем вообще происходит мы ставили cabernet нам сказали все будет хорошо а чуть не очень хорошо ошибки какие то есть их мало но они есть пришлось погрузиться поразбираться и сейчас я с этим с вами тоже поделюсь смотрите независимо от того что мы делаем с подом мы его удаляем либо мы обновляем наше приложение либо мы эвакуируем под из какой-то надо на другую в любом случае у нас происходит удаление пода понятно когда мы делаем куб стиль далит у нас под удаляется напрямую это это окей когда мы обновляем версию приложения у нас под со старой версии удаляется появляется под с новой версии мы при эвакуации тоже никакой магии не происходит под на ноги удаляется на новый ноги запускается так вот что значит удаление пода под помечается вопи серверы как подвержены удалению а дальше несколько компонентов кубер не то со практически параллельно начинают свою работу in point контроллер начинает выкорчевывать in point из сервиса которые ведут на этот пот куб roxy начинает выкорчевывать айпи адреса из правил и пить и босса либо api веса которые ведут на этот пот убирать его из балансировки игры начинает там обновлять up stream и кардан с начинает обновлять конфигурацию и естественно параллельно со всем этим у нас блед как ответственной за воды на свои ноги тоже начинает свою работу а именно начинает останавливает процессы этой реплики этого пода ну для того чтобы действительно у нас удаления произошло но cabernet у нас умный и куплет соответственно тоже поэтому это делается не просто так сперва отправляется сигнал сектор затем по дефолту 30 секунд идет ожидании что приложение завершается до и после этого только отправляет сигнал siptu как можно догадаться проблема возникает именно на этом месте у нас приложение тухнет быстрее чем убирается из балансировки причин может быть много в частности очень много правило выпить и вас например они просто не успевают отработать и нам чтобы эту проблему решить нужно каким-то образом сделать так чтобы процесс гасился позднее того как из балансировки у нас под убирается нативных средств чтобы это решить нет но есть некоторые обходные пути которые я и свое время применил и про которые я сейчас тоже вам расскажу вообще каких собственно какой чек лист первое проверьте пожалуйста что ваше приложение адекватно реагирует на сигналы сектором то есть она не принимает новые соединения и завершает работу с теми что уже есть если ваше приложение не реагирует на сектором и исправить это невозможно подключаем приступ куку это несколько дополнительных строчек в манифесте который позволит передать какой-то специфичный сигнал вашему приложению если и это не работает как моем случае добавляем в приступах укус ли 5 либо slip 10 то есть мы вызываем slip то есть он поспит 5 либо 10 секунд конкретный пример ынджон x не знаю как сейчас честно но раньше он гасился нормально только через ключик и соответственно можно было через приступ хоку вот у нас появляется новый блок веб-сайта соответственно мы в приступ куку в команде прям пишем дженкс то минусы squid и соответственно ставим slip тогда в этом случае у нас под переходит статус термина и thing и параллельно с этим куплет вместо того чтобы катить под спит и вот пока он спит все остальные компоненты они свою работу выполняют и соответственно этот пот из балансировки убирают из балансировки под убирается затем происходит дальнейшее погашение этой реплики и под благополучно завершает свою работу это вот один из кейсов да когда мы прям передаем slip в прямом виде и я уже сказал такое что и приложение тоже не умеет отрабатывает сектор и вот здесь у нас тоже вариант это реальный пример из нашей инфраструктуры сайдкик не умеет на сектором реагировать мы можем отправить им сигнал сикти и степик и вот в таком случае соответствовал будет нормально гасится 2 небольшая под тема по доклада этот человеческий фактор людям свойственно ошибаться люди ошибаются в коде в яму манифестах настройках поэтому у нас куча всяких линкоров авто проверок целое дело делается тестирует тестировщики да но ошибки все равно происходит несмотря на то что у нас и куб сетей проверяют чем этом диплом и api сервер во лидирую что к нему приходит все равно ошибки случаются здесь нам опять потребуется вот этот товарищ мы конечно с человека можно и удалить уволить иди премировать и там публично выпороть на оперативке но это не выход нужно пользоваться инструментами вот про инструменты я сейчас расскажу эти инструменты позволят минимизировать человеческий фактор товарищ это сейчас должен выйти с нами словно связь все посмотрю а ну а но он решил пива попить замечательно все уже давно человеческий фактор до отличный этот товарищ вам там наплел уже по-любому про то что там увольнять там где премируют все это это моя заготовочка было он подрезал меня поэтому давайте сразу перейдем к делу сразу перейдем к рассмотрению кейсов и так истории жизни ситуация в следующем кейси следующие разработчики периодически забывают просто вид request они или лимит и сам не раз был свидетелем либо просто ставит их не корректно и как следствие у нас некорректный менеджмент ресурсов квестеры то есть мы не понимаем вот тот объем ресурсов который кластер потребляет он действительно реальный или там есть какой-то процент ошибок чтобы этот вопрос решить мы конечно можем сказать разработчикам работайте аккуратно и проверяйте за собой но мы уже с вами выяснили что человеческий фактор он такую контролю не поддается поэтому используем инструмент который называется лимит раньше это еще одна абстракция в купер нет все которая позволяет установить ресурсы для объектов квесте ра а именно дефолты ресурсы максимальные ресурсы и минимальные для таких объектов как контейнеры коды и висит давайте представим ситуацию теперь что мы можем сделать мы можем теперь сказать что вот кода у нас могут минимум запросить 64 милицы по ул максимум соответственно лимиту но у них может быть там один соплу если по какой-то причине там совершилось ошибка или место одного циpкa поставили 11 попытались за диполь это у нас lamie3 ругается и такое задевает не даст и очень прикольно очень удобно что у нас есть эта настройка и для певицы соответственно теперь место буковки д если появится буковка т.е. вместо ни одного гигабайта мы нечаянно запросим 1 терабайт из нашего исходя то этот момент тоже будет пресекаться потому что мы можем сказать что максимум висим можно запросить например от 1 до 100 гигабайт fever вот вот как то так но это еще не все почитать вы можете про этот инструмент в документация по подробнее и вообще если вы им не пользовались то я крайне рекомендую воспользоваться посмотрите применять у себя это еще не все потому что есть еще один кейс который произошел с коллегой уже с моим коллегой по чесноку если говорить это произошло несколько лет назад и это было нами зассе но ситуация настолько интересно что во-первых она может произойти и на кубе а во-вторых она такая показательная очень что действительно может сделать человек с квесте раме разработчик ошибся с количеством реплик приз кей линги он хотел застрелить до 1 но моргнул интернет клавиша за я вам такой один чуть не работает 1111 и вот у нас 111 миллионов реплик к чему это приведет в кластере я думаю говорить не нужно да сразу начнут а все эти реплики пытаться создаваться жрать соответственно ресурсы так далее так далее ничего хорошего не будет как из этой ситуации мы можем выйти поговорим но сперва рассмотрим еще один кейс этот кейс уже произошел лично со мной мы настроили край карты были разовые работы для клиента соответственно мы квасят настроили в настроили там сидит так далее и отдали все доступы клиенту и с пустякам года полтора где-то клиент возвращается на обслуживание и где захожу в квестеры лежит в квартире в принципе мусоркам потому что там какие-то сервисы которые ведут в никуда там работал под у которого аптайм больше года то есть типа либо разработчики трудиться ударным темпом да и часто выкатывают обновление либо действительно этот под уже нафиг никому не нужен он работает пять жеральд ресурсы что нужно сделать нужно придумать какой-то инструмент нужно его применить чтобы вот такую ситуацию такой ситуации избежать и есть такой инструмент этот инструмент называется ресурс кого-то он позволяет установить количество доступных ресурсов и количество объектов для нам space of кластер и соответственно вот спит не полный список того система может работать конкретный пример мы могу привести такой у нас есть например ну space development и мы можем сказать что вот теперь в этом нам спейси у нас максимум можно запросить 4 цифру 4 гигабайта озу а также в нем максимум может быть только 10 тонов и тогда если у нас космический скиллинг пойдет ему сразу и так бобру бит у нас может быть там максимум 5 сервисов три секрета один как фильмов и так далее то есть вот таким образом мы можем ограничить вот это неконтролируемый расход и ресурсов и соответственно количество объектов и тогда если разработчик там разрабатывает этому спейсера он упрется в эту квоту и тут у него два пути либо он посмотрит зайдет на space видишь что там что то есть лишь не это уберет либо если ничего лишнего не ставит тикет в эксплуатацию и ему эту ресурс квоту увеличивает почитать также можно подробнее документация тоже очень рекомендую использовать этот инструмент очень полезно вообще кстати рекомендую посматривать на фичи gate и и обвешен контроллеры которые появляются в cabernet все они там с разной периодичностью выходят гони которые можно юзать их уже в бете вот и они действительно добавляют полезности полезного инструментария в кластер кубинец следующий кейс твой ты чуть дальше рассказывать еще или не конечно окей тогда перевожу на слайды спасибо последний кейс последние истории жизни которая произошла это прям очень такая яркая история полезно для тех кто держит несколько окружений в одном кластере куба в кластере куба было два окружения production & development и в какой-то момент часть серверов квестора стала недоступна но куда нацию на сунну начал коды перевозить на рабочие ноды все хорошо все так как и планировалось но этого кейса не было бы здесь если все прошло действительно хорошо потому что угадаете под и какого окружения влезли целиком а какого не влезли муха тоскую пауза делать не буду собственно development у нас заехал целиком без проблем продакшен нет залез что с этим делать здесь вырисовывается такая необходимость что нам нужно каким-то образом приоритизировать под и production явно важнее чем development может полежать случай аварии ничего страшного есть такой инструмент который позволит нам это сделать инструмент называется priority class с помощью priority class и мы действительно можем сказать что вот у нас есть продакшн и вот у него приоритет такой высокий есть девелопмента него приоритет низкий и тогда в случае аварии первыми поедут рабочие ноды именно коды продакшен adv-1 соответственно если там что-то останется тоже поедут и более того если авария какая-то серьезная и у нас production не влезает то тогда not рабочих droppin' будет вы гоняться чтобы туда все-таки продакшен залез до конца и все у нас запустилась все стало хорошо инструмент очень хороший и вот еще раз повторюсь это маст хав вообще для тех кто юзает несколько окружений в одном кластере там также есть возможность в ресурс квотах по priority классом определять и опять же более подробно можно почитать про это в документации частенько в моем докладе это звучит почитайте документации почему да потому что в рамках одного доклада вот про все подробно нельзя рассказать но шесть часов вечера в шестом зале будет мастер-класс всех зову там вот вот эти все вещи которые я сейчас рассказал мы прям на живом кластере будем разворачивать ставить смотреть как они работают экспериментировать там я еще про некоторые кейс расскажу в общем 6 зал 6 вечера я понимаю это уже поздно первым 20 кто придет дам доступа на реальной кастера на этом техническая часть у меня все сейчас мы этого товарища на экран должна выведем он попрощается лечо я не знаю тебе просто по запчастям на вид и продадим и никто не спалит чё ты паришься блин ну что ж друзья на этом наш доход подошел концу слушаем мне кажется мы с тобой просто ли такой команду знаешь мы прям сработали с тобой тебя чип и дейл да вот этот шар ухом закрываться ну я шерлок 320 я обещал ты подход его норма ничего с неплохо наш я думаю может аввалин даже как-нибудь повторить ты чё там делаешь мне выпить что хочешь ты все наконец-то спасибо марселю его брату близнецу буквально пару парусов в общем дать кенийской части у меня закончится это был такой небольшой эксперимент честно говоря тяжело слушать просто слайды когда переключается так далее я решил немножко сымпровизировать надеюсь это было как минимум не скучно есть еще такая скромная надежда что это было даже полезно собственно те кто эти инструмента не использовал я надеюсь вы рассмотрите их использование у себя в квартирах в те кто использовал вы вспомнили для чего вы их собственно внедряли вот еще раз напоминаю про мастерко часов что то пропадает шесть часов вечера в шестом зале приходите 20 первых человек получит 20 доступа в кластер мы все вместе проходим будем делать я буду в онлайн все делается расскажу еще пару кейсов в общем должно быть интересно приходите и последнее от себя искренне каждому желаю побольше аптайма поменьше fa cup of вот теперь все спасибо аплодисменты давайте вопросики попробуем привет отличный доклад у меня есть вопрос я знаю ответ на и должного задать разработчик собственно тебя послушал побежал вкорячить под destruction budget естественно поставил там на единичку он же продуманный но у него одна реплика а что будет чтобы он поставил под destruction budget на 1 а у него приложение в одной реплики ну с ним ничего нельзя будет делать его даже нельзя будет обновить наверно потому что по сути они от обновить то можно будет под трап a jet у нас ограничивает работа с куб сиссель drain соответственно если одна реплика то соответственно если там что-то с этой ноты будет происходить то ничего не произойдет и как бы админ упрется в то что ему будет приходить там эти предупреждения я так понимаю я в одной реплику с одной реплика никогда не делал это такой лабораторном этот правильный ответ придет обмена даст по шапке потому что нормально работать но до и после этого словаре partida de и второй момент я про дефолтный ресурсы хотел пару слов сказать мы наоборот считаем что дефолтные ресурсы на контейнеры допустим лимиты это зло потому что стреляет много разработчики не понимают как это работает и классно когда они явно представляют риме и лимиты потому что они думают об этом а если у них есть какой-то дефолт они приходят мол у нас приложение падает полом и мы ничего не делали ничего не работали она сама ну вот ты спать и находят по приходят пару раз и потом собственно этот сталкивались то есть действительно дефолтный вот эти знать которые есть они защищают ваш квестеры от неправильного ресурс менеджмента потому что когда у вас квестор долго живет долго в него что-то делается вы смотрите на ресурсы у вас нет точного понимания это действительно важно то что там работает или есть некий процент ошибок то что там кто то ошиблись это не просто или и дефолты но я лично например ставил маленькие от ты на маленькие ставил типа 200-ми лицевую там 200 мегабайт озу но нормально приложение это не получалось ничего не взлетала до приходили разработчики даст ними проводились разъяснительной беседы что-то то надо ставить соответственно ставили и все было хорошо задаешь ты давай вы рассказывали про ресурс квоту вопрос такая какой смысл вообще в ресурс квоте на поду то есть если кто-то случайно бесконечное количество поднимет они все равно в ресурс квоту по ацп ул и раму проц и поэтому обычно в нам список юзает эти два инструмента вместе поэтому в кейсе аня тоже вместе и лимит раньше ресурс квоту вообще создание нам space если у нас прям полноценно infrastructure заход это прям грандиозная задача потому что помимо того что вам просто нужно сделать край tns на самом деле вам туда нужно добавить пользователей вам туда нужно накатить все вот эти ресурс кого-то лимит рейндже если есть какие-то сетевые политики туда же применить если есть под security policy туда же применить и так далее далее то есть это прямо задача такая это обычно на то что я видел это прям отдельная роль отдельный playbook который этим занимается ванцева и соответственно там все это прописано с вашей точки зрения при каком кистью вообще стоит применять лимит на количество запущенных подав если есть кого-то под спойлером что еще раз в конце я слышу от смысла делать квоту на количество запущенных кодов если уже есть вот мы с полу epam который ограничивает по сути количество запущенных кодов в конец эрону не слыша но смысл в чем смысл но вот как один из примеров да когда у нас killing происходит неконтролируемое это может быть действительно и за ошибки на самом деле у меня лично такого никогда не было это история от коллеги вот но технически это может быть особенно если есть привычка у инфраструктурной команда лезть в классе искусителем что-то там активно работать активно делать не использовать infrastructure код есть доля вероятности вот именно вот это человеческого фактора человеческой ошибки если речь про квоты в целом в кубрина это си которой есть нападает правильно понимаю типа пути какой смысл использовать или или нет но смотрите есть квота на цеху epam который назначается есть дефолт avaya для тепло ему то есть то которые можно вручную прописать а также вы сказали что есть лимиты на ресурс квоту на количество запущенных плодов там конфиг maps of секретов и так далее вот смысл запущу в квоте на количество запущенных полов но в принципе я рассказал илья я до конца видим вопрос не понимаю но мой телефон от добрый день марсель спасибо за интересный доклад борделе тоже респект вот у меня вопрос насчет кейса связанного с 500 ими ошибками при rolling облейте но у меня на самом деле кажется не очень хорошая идея ставить там слепить или slip 10 на завершения приложения потому что ну яга раньше положение завершится за пять секунд они за пять с половиной и не проскочишь 500 и что мешает данном кейсе использовать blue green и сначала переключать трафик на новую версию отключать старую и только после этого включать старый под спасибо начнем с blue green а вообще когда готовился к докладу я накидал достаточно много всего я хотел вам и про в том числе про нативные средства куба стратегии дипой рассказать про кинари популярен как это все можно сделать но не влезло поэтому до blue green это в принципе вариант кинерета в принципе вариант единственного сна если вы используете брыклин у вас должно быть x2 ресурсов кластера то здесь это тоже нужно понимать по поводу приступ хуки если пять у нас вот эти 5 секунд они не добавляются и это не время за которое приложение должна остановиться время она также остается 30 дефолтный секунд пять секунд него входят и просто это фора для остальных компонентов в стирку bernette с которой будут выводить этот пот из балансировки то есть но технически с погашением пода с погашением этого процесса ничего не меняется меняется ли мне некоторой очередность так скажем спасибо за доклад меня слышно спасибо за доклад не кажется ли вам что выставление ресурсов то есть request of лимита на namespace это порочная практика особенно для продакшн и в котором работать допустим кантал под авто скейлер и закладывание себе работы на будущее за затем чтобы следить за этими ресурсами тюнить их периодически и обязательно человеческий фактор когда им стрельнет и просто под текущую нагрузку допустим со встроенным hop ресурсов на на экспрессе просто не хватит вот и второй под вопрос как выделить тогда ресурсы кластер допустим у нас есть три давайте для простоты df и прут done in space он ничего больше нет мониторинг никакой там и уберите вообще не учитываем допустим как вы как вы делите ресурсы между на с пейсами с перри подпиской или нет так первое смотрите на что так немножко обобщать предвосхищают другие вопросы подобного плана вопрос ресурс менеджмента вк обернитесь и он очень важной и его зачастую действительно недооценивают и вот учитывает ресурсы действительно держать руку на пульсе понимать сколько у тебя сейчас потребленной сколько есть это надо ну это мое мнение и в том числе если у вас авто скейлер если у вас какой-нибудь поработать то тем более вы должны понимать что hop отработает целиком его все равно ресурсов останется то есть hop танцы в своей работе не загнет весь кластер да это кажется избыточным но как раз таки вот эта щепетильность и вот этот такой требовательный подход к ресурсам и к себе и к разработчикам и собственно ко всем кто пользуется кластер он как раз таки позволяет в будущем себе в ногу не выстрелить каким быть тем же самым hop так стать такой тоже вполне себе реально кейс который может случиться до почитали про то что х поможет скелетон по циpкa круто здорово давайте там пусть 10 реплик сделает нам еще анти affinity туда под affinity причем на каждой ноги он запускался что все это так размазывалась ровно вот и стрельнет по-любому стрельнет поэтому ресурс менеджмент квестеры я повторюсь это очень важно и пусть это кажется избыточным но нужно следить за этим то есть нужны мониторинге нужен сбор метрик который прям вот вам дашборде показывает сколько сейчас какие у вас там показатели и соответствующие alert и которые работают проактивно то есть не так что у вас уже сто процентов потолок и вот пришел олег ну и это это такие очень базовые вещи то есть понятно что там 80 процентов условно его солер приходит вы понимать что что-то идет не так вопрос то был немножко в другом я позвольте уточню никто не спорю что ресурс менеджмент это важные и нужные вещи это за этим нужно следить внимательно и контролировать я к тому что я бы не стал рекомендовать выставлять ресурсы на namespace вам по мне это очень порочная практика лучше уж пусть будет набор из привозите класс по данте affinity их по и лучше просто выдержит вас возник возросшую нагрузку и выдавит div весь пофиг чем мы упремся просто в ресурсный space ночью когда я там сплю например по моему вредный совет немножко по ресурсу на на in space от подхода зависит то есть в зависимости какой подход к ресурс менеджмента в компании это в принципе resource момент такой холивар на вопрос все делают немножко по-разному и но это та практика которую стараюсь придерживаться я особенно если это нам space и разработки а кейс продам можно что-то подумать хотя тоже такое но разработчики вот эти когда условно выкатываются какие-то например сыр и приложения они вдруг потекли по памяти началось у нас домино одна доза ноды начали складываться и за это вот вот эти моменты очень хочется как-то предвосхитить но ваше мнение тоже имеет место быть вопрос подходом давайте последний вопрос коротенько добрый день спасибо за очень интересный доклад я тоже присоединяюсь хотел спросить вот допустим небольшой интернет магазин да сделали для него конфигурацию кубов подняли все работает и тут магазин начал расти соответственно проект растет до сам по себе количество тонких но там и так далее все растет вот связи с возросшим масштабом да по практике по вашей что там только управление ресурсами какие то близкие или по мере роста проекта до нужно еще на что обратить внимание мы просто можем добавить ресурсов и все разом ресурс менеджмент или там есть какие-то вот такие вещи которые знаете вот нa лицa книжки не прочитаешь у вас есть на что посмотреть да ну да конечно были ребята которые условные стартапа переходили там более такие высокие нагрузки можно условно разделить нагрузки там на низкие средние действительно высокие нагрузки уровня там серьезно круг крупнейших магазинов да и так далее низкие там скорее всего на низких нагрузках это действительно уровень стартапа и там в принципе какая какой-то потребности в отказоустойчивость и так далее наверняка особо нет и там можно смело экспериментировать и эмпирически понимать сколько приложения может там выживать на тех или иных ресурсов на средних соответственно да там уже нужно будет смотреть здесь пока что на средних нам нужно да именно управлять ресурсами управлять количеством мощности наших количеством железо условно да и весь такой важный момент тоже опять же такое лично может быть даже предупреждение что дублицирование просто каких-то основных компонентов этого недостаточно это история которая в том числе перекликается там с хостингами если тут есть представители хостинга git наверняка подтвердят что условный там тир-2 по-моему это уровень отказоустойчивости хостинг это это не особо хорошо вот поэтому стоит смотреть на требований и до закладывать обязательно закладывать какой-то буфер когда у вас нагрузки стали слишком большими вот там помимо того что просто ресурс менеджмент и просто количество нот там скорее всего придется углубиться в тюнинг в тюнинг и компонентов кластера тюнинг ядра на то чтобы вот эти все connect и и бешеное количество трафика выдержать вот это уже более высокий может быть есть какие-то инструменты которые скажем так вот уже для масштабных проектов требуется и мы про них просто знаем по идее это просто грамотный мониторинг и ну да мониторинг я имел ввиду то есть какие-то может быть инструменты есть которые вот мелким проекта мне нужны округам без них никак не нет инструменты на самом деле плюс-минус одни и те же все потому что она типа prometheus графа на все уже достаточно давно обкатана а не масштабируются случае чего и там важен именно подход важно настроить его таким образом чтобы это было проактивно чтобы ну вот условно по графикам виде что вот мы начинаем подходить к роли я услышала любого использует там миллионы до инстансов вот у вас по практике тут можно как-то максимально с чем работали там а это бывает по практике по практике да не так много на самом деле в районе 20 нот вот из них там 05 май мастеровых полумиле трилли но до мастеров и worker of соответственно все остальное количество с не очень-то же большими нагрузками в среднем диапазоне так скажем поэтому там каких-то тонких тюнингов делать не пришлось но я почему спрашиваю если к нам придет devops и мы будем его собеседовать он нам скажет что миллион not поддерживал мы как бы понять радио псов как собеседовать мы уже поговорим в кулуар пасибо спасибо большое за доклад спасибо брату близнецу"
}