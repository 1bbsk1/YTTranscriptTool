{
  "video_id": "0GT_NGRp9fM",
  "channel": "HighLoadChannel",
  "title": "Мы охлаждаем воздухом. Дата-центр Яндекса. Какой он? / Егор Гордовский (Яндекс)",
  "views": 453,
  "duration": 3132,
  "published": "2024-04-17T01:10:21-07:00",
  "text": "Спасибо Егор Да вот у тебя сегодня очень интересная тема такая знаешь холодным это все про нагрузку про нагрузку но не про физическую да А ты нам расскажешь про то что нас просто прижмет совсем или поднимет наверх как это будет не знаю посмотрим наверное зависит от того Ну как я расскажу Да давай спасибо Спасибо большое что пришли послушать мой доклад и он сегодня Ну не знаю просто всю сетку докладов не видел но наверное немножко особенный потому что не будет ни строчки кода и наверное не будет Ну кроме может картинок привычных каких-то инструментов вот кликер Прошу прощения кликер Можно мне это я не вижу а не вижу Да сейчас поговорим как раз таки про физическую инфраструктуру Ну и Тема моего доклада мы охлаждаем воздухом это кусочек того о чем мы поговорим сегодня основном да это дата-центр Яндекса будем смотреть на примере нашего дата-центра в городе Владимир один из самых современных наверное в России и самый пока современный в Яндексе и Давайте поговорим о нем и посмотрим какой чуть-чуть познакомимся Меня зовут Егор как я уже сказал шесть лет работаю в Яндексе и все шесть лет занимаюсь тем что запускает от центра с нуля помимо этого чиню сервера последний год наверное приблизительно Занимаюсь тем что еще и рассказывает это центрах о наших И относительно недавно начал еще обучать новых инженеров мы растем инженеры новые приходят и надо им с ними делиться опытом Вот и наша скажем так подразделение до нашего части Яндекса Яндекс инфраструктура немножко про нее скажу мы строим дата-центры обеспечиваем связность по проводам без проводов а строим надежную платформу для наших разработчиков Яндекса и наши разработки Можно попробовать Яндекс клауди в Open Source uideb относительно недавно появился White saurus Да пожалуйста пробуйте смотрите Ну и собственно доклада если вот посмотреть на путь проектов Production такое упрощенный вид то можно выделить там условно несколько частей первая там для разработчиков думаю очень понятно мы тут ну я подошел просто к этому слайду так обобщённо есть разработчик он пишет код используют какие-то свои там любимые может быть нелюбимые инструменты пишет пишет отлично он его написал Дальше это надо куда-то за диплоить Вот как раз таки Ну грубо говоря вторая часть Да пути проектов Продакшен это как раз тепло и там может быть devops мониторинге для разработчика это всё равно так или иначе какая-то близкая знакомая Тема а третья часть собственно железа И тут если как бы вот ну на поверхности как правило там слышим про серверы про какую-то сеть про какие-то может быть стойки что-то там где-то чего-то Но как правило дальше уже в эту часть не углубляемся как раз таки сегодня хотелось бы поговорить именно вот это вот я его так еще называю глубинным бенди и как раз таки этот глубинный бэкенд это железо это то что объединяет сервисы между собой без железа сервисы не работают но это факт да код без железа нам как бы вот Но это всего-навсего код какие-то набор символов вот если брать Например Яндекс у нас много сервисов думаю многие пользовались там какими-то поиском может быть почтой ездит на такси заказывать на Маркете у нас много сервисов и как раз таки чтобы все эти сервисы работали нам нужно много железа Ну картинка для привлечения внимания Как выглядит стойка с серверами это наши стоечки наши дата-центре очень красиво я считаю и как раз таки вот этот вот глубинный бэкен железо дата-центры это такой Ну наверное все таки в какой-то мере черный ящик такой своеобразный в нем что-то там есть мы о нём как-то слышали Но обычно туда никто не заглядывает ну и реже всего кто-то туда вообще в принципе ездит Да и как раз таки Давайте попробуем этот черный ящик заглянуть и посмотреть что же там в нем на самом деле лежит и первый вопрос Что такое дата-центр из чего он состоит Давайте попробуем на него ответить дата-центр вот в моем таком понимании В упрощенном смысле это некое пространство может быть здание может быть комната может быть Вот этот зал Да мы в него устанавливаем наши серверы стойки с серверами объединяем в сеть просить скажу сразу мы сегодня практически не будем говорить это вообще отдельная тема очень обширная вот эти серверы хорошо мы их поставили но все знают что без электричества они не заработают прекрасно нам надо как-то организовать это систему электропитания на картинке трансформатор кто Ну там не знаком не видел этот трансформатор который преобразует напряжение мы подали питание на серверы они стали работать ни для кого не секрет что серверы во время работы греются если их вовремя не остудить не охладить они просто перестанут работать вот и как раз таки третья часть Да скажем так точнее даже нет наверно четвертая часть то есть пространство электричество серверы и системы охлаждения вот этот как раз упрощенная система упрощенная схема дата-центра и первое о чем вот в частности про дата-центр Хотелось бы поговорить это как раз таки организация пространства строить можно построить свой дата-центр а можно арендовать Но тут надо сказать что я в этом месте имею в виду Арендовать помещение не мощности Да какие-то не сервера а именно помещение и вот как раз таки первый подход про аренду он у нас называется бранфилд допустим есть некий старый завод хотя бы такой вот есть некий старый завод Brown Field обычно этот подход переводит как коричневое поле Я внизу там подписал нашел более емкий перевод заброшенко на мой взгляд это прям вот очень хорошо да согласитесь вот с этой картинкой очень хорошо коррелируется Кстати картинка сгенерирована шедеврами Вот вот есть некий старый завод и тут все по схеме Да мы делаем ремонт все там красиво Организуем организуем электропитание устанавливаем стойки охлаждаем и прекрасно у этого подхода есть свои плюсы и минусы очевидно Из плюсов что можно вот выделить Да это готовое здание или помещение это круто нам не надо строить в принципе да согласитесь вы пришли Просто стойки поставили может быть ремонт какой-то сделали и уже есть некая Инженерная инфраструктура Например у вас есть канализация это немаловажный факт мы об этом чуть попозже поговорим Возможно есть уже какой-то электропитание может быть даже вот если это помещение взять тот организовано уже какое-то система охлаждения будет здорово но есть и минусы если мы вспомним старый завод сгенерированный шедевром мы поймем что там нужен очень капитальный ремонт там Прям вообще капитальный ремонт и это правда на самом деле бывает ремонт очень дорогостоящие И трудоемкие это минус и необходимо переделать как-то под свои условия Ну потому что мы понимаем что там завод он может быть какой-то определенной формы надо может быть перегородки построить еще что-то то есть как-то под себя Под свою условия подстроить например вот эта площадка сегодняшняя А тут по моему был какой-то цех Машиностроительный он не предназначен для дата-центров поэтому Чтобы в нем организовать этот центр надо как-то это всё равно переделать и возможно наличие ограничивающих факторов Какие могут быть например вот эту помещение мы тут поставили какое-то количество стоит примерно запланировались на какое-то время нам его хватает Но вдруг наш сервис пошёл Вот мы там были стартапом и мы резко выстрелили у нас стало там расти количество пользователей нам нужно резко расти и мы просто площади уже все скушали тут нам некуда тут нет в этом месте расти надо как-то уже думать о будущем куда же нам масштабироваться либо пытаться найти соседнее помещение чтобы тут либо еще как-то могут быть трудности либо не рассчитаны под нагрузки Например у нас есть старая фабрика какая-нибудь кацкая пятый этаж мы там организовали но мы не можем там поставить не новые стойки хотя у нас есть площадь для этого есть электричество есть даже холод просто потому что перекрытие не выдержит и мы переедем вместе со всем заводом на первый этаж Это очень плохо Вот и Яндекс последнее время идет вторым путем он называется Greenfield или чистое поле это фотография не шедевром это реальная фотография нашей площадки во Владимире дата центра Владимир на ней можно увидеть три здания таких крупных это вот мы их тоже называем этот центрами кто-то там может их называть кластерами еще как-то Ну вот мы называем дата-центре У нас есть своя собственная даже подстанция даже своя собственная канализация потому что в чистом поле ей неоткуда взяться свои собственные системы водоподготовки то есть мы все организовали с нуля в чистом поле этот путь который выбирает Яндекс последнее время и почему мы Строим сами как раз таки Мы помним минусы до которые были у браунфилда мы не делаем дорогостоящие ремонты мы делаем сразу хорошо с нуля строим при этом мы можем спроектировать уже под свои нагрузки под свои мощности под свои нужды у нас может быть собственный какой-то дизайн нам легче масштабироваться в будущем мы понимаем к эту площадку Ну масштабировать реализовывать развивать в будущем есть прекрасная возможность применить какой-то Ну инновационный метод об этом как раз таки попозже и мы можем всецело контролировать запуск объекта на стадии строительства на стадии пуска наладочных работ то есть когда мы запускаем инженерную инфраструктуру мы за этим всем следим и когда мы уже получаем все это добро в эксплуатацию Мы знаем как с этим работать и как это все дело эксплуатировать Вот какие могут быть минусы я выделил один единственный необходим строить Да это наверное минус но опять же каждый будет выбирать для себя в настоящий момент на нашей площадке так как мы говорим про Да центр применительно нашей площадке во Владимире у нас сегодня 21 мегаватт это мощность которую мы потребляем Ну для большинства нар 21 мегаватт это просто цифры с чем ее можно сравнить я поискал Владимирской области нашел город гусь-хрустальный есть такой город может быть слышали о нем там живет 55 тысяч человек в нем есть различные предприятия в том числе стекольной промышленности вот этот город 55 тысяч человек в данный момент потребляется только сколько наш дата-центра 21 мегаватт но мы можем расти дальше для нас вот выделена мощность 48 мегаватт еще у нас есть запас 50 процентов даже чуть больше то есть нам есть куда расти и вот этот город 55 тысяч человек находится на площади 32 тысячи квадратных метров наша площадка А это сопоставимо с четырьмя с половиной футбольными полями Ну такими средними каким-то стадионом то есть четыре вот этих с половиной стадиончика там наш дата-центр с 55 тысячами человек да там под капотом со всеми предприятиями этого города мы установили уже больше полутора тысяч стоек в них установили более 41000 серверов 41000 серверов это много мало Это много да с чем это можно сравнить я провел мысленно эксперимент сложил серверы в стопку У меня получилось башня размером 5 эйфелевых башен есть одна на другую поставить или две башни бурдж-халифа сейчас мы уже даже чуть повыше их подсчитал количество хордов Ну вот такая вот цифра У меня вышло честно не помню на память сколько там тысяч дисков можете сами примерно представить какие там разные мысленные эксперименты сложить их в площадь или стопку там еще больше будет стопка там я не знаю сколько эфирах башен на десяток Точно Вот Давайте вспомним наш дата-центр вот этот мысленный эксперимент это у нас пространство мы о нём поговорили это один из факторов который надо учесть когда мы проектируем да и думаем где построить до центра нам нужна земля но нужно еще электропитание наши серверы без них не будет работать это вот как раз следующий этап и как выглядит классическая схема электропитания DC она в кавычках классическая схема потому что схема на самом деле классических Нет но я сейчас попробую просто общий подход подсказать кто-то помнит может быть слышал про всякие тиры это вот эта тема то есть Нам нужно зарезервироваться идея какая у нас есть площадка туда приходит какой-то городское питание Но основной вот питания но он может отключиться вот этот трансформатор если помните там на схем очки вот я рисовал Он нам нужен для того чтобы напряжение понизить как правило городское питание выше чем питание которое необходимо подать на стойки вот эта трансформатор преобразует напряжение Все проходит через источник бесперебойного питания Ну название само за себя говорит если кто помнит У нас под столами раньше были сейчас я уже почти не встречаю нашей системники вот это вот коробочка с аккумулятором с кнопочкой там пищащий или не пищащий у кого как вот это вот аналог и попал нужен на то что на какое-то время пока у нас город отвалился чтобы идти нагрузка у нас ну жила И в этот момент вот это ДГУ дизель-генераторная установочка которая внизу такая желтенькая нарисована она нужна чтобы преобразовывать топливо энергию топлива в электричество Ну я так уж просто пытаясь рассказать чтоб всем было понятно то есть мы ждем пока договоримся и бпп держит нагрузку дизель запустился всё прекрасно Итак у нас может быть энное количество модулей там первый второй третий там например 10 мудрый и они все зарезервированы мы пошли немножко другим путем это как раз таки вот второй Да скажем 2 второй фактор на который надо смотреть когда мы проектируем площадку наличие надежного поставщика электроэнергии у нас рядом есть PS Владимирская ПС это подстанция Владимирская 750 киловольт монет чуть попозже поговорим к нам приходят две линии независимых по 110 кВ эти две линии Аналогично друг другу каждый из линий может нашу всю площадку Вот эту вот 48 мвт под нас заложенных держать то есть мы считаем что у нас два города и благодаря этому у нас немножко другая схема электропитания Мы ушли от того что в каждом модуле стоит дизель у нас только один дополнительный вспомогательный модуль есть который нужен для проведения технического обслуживания То есть когда мы нам надо например трансформатор в модуле обслужить мы его отключим через этот красную линию на провод 4000 ампер мы подключим вспомогательный модуль зарезервируемся таким образом получим надежное питание наше трансформатор обслуживаем потом всё вернём назад Итак один из модулей то есть наш вот вспомогательный модуль может держать только один из модулей все пять сразу нельзя У нас вот в одном из зданий 5 модулей it и один вспомогательный то есть схема такая Мы ушли Вот как раз таки немножко на другой уровень зарезервировались на уровне городского электропитания и вот как раз таки почему мы там есть во Владимире Там есть такая подстанция Владимирская 750 киловольт это очень надежный поставщик Да это крупнейший энергетический объект Владимирской области один из крупных самых вот вообще в Центральной России и это Узловая подстанция которая объединяет в единую электросеть такие генерирующие мощности как жигулёвская гидроэлектростанция Костромская ГРЭС и даже Калининская там на электростанция которая под дверью то есть мы вот практически находимся под боком у генерации при этом мы не рядом с какой-то из этих подста- электростанции находимся всё это тоже очень здорово что нам это вообще в принципе даёт мы не ставим лишнее оборудование у нас в каждом модуле нет например дизелей этот вот дизеля который мы единственным Нам нужен именно только для технического обслуживания не для резервирования как-то как Вова в момент отключения города подключение практически генерации как я сказал и мы зарезервировались на уровне года И также у нас изменяется подход к выбору оборудования об этом сейчас как раз таки вот поговорим о чем О том что вот классические БП он почти как наш под столом Да только там не вот этот маленький аккумулятор а целая куча вот на картинке представлена причем только часть фотографии все все помещение не помещается просто то есть там еще несколько рядов Таких вот аккумуляторов их много они занимают много пространства с ними много Мороки их надо обслуживать надо менять и так далее поэтому мы выбрали вот такие вот и БП это такой металлический шкаф у него внутри есть маховик штука которая Весит примерно 900 кг и крутится в вакууме крутится и запасает вот за счет этой кинетической энергии энергию выглядит вот так Примерно вот наверное мне по поясу такая вот коробочка на фотографии Вот её видно на следующей фоточке как он уже установлен вбп в этот нас таких вот четыре ящичка на модуль 4 Вот таких шкафчика и это клёво на самом деле это здорово мы экономим пространство у нас нет вот этих вот химии нету металлов всяких там аккумуляторов и так далее мы ушли в другую сторону и заряда хватает аж на целых 18 секунд Как вы думаете Это много или мало подвох в опросе Да много Почему Потому что у нас есть дизель который должен успеть запуститься это тоже наш дизель кстати он Достаточно мощный и он запускается Аж за 6 секунд То есть у нас резервирование вот 18 секунд 6 То есть три раза каждые две недели мы его запускаем проверяем что с ним всё в порядке и всё здорово на самом деле он мощный как я сказал примерно 2,5 мвт это целый модуль может держать или вот много лошадок у него под капотом он очень сильно кричит Кстати когда запускаем Да здорово Мы зарезервировались по питанию но все равно вопрос может возникнуть на что если вдруг А что если вдруг то возможно для кого-то не секрет Яндекс резервируется как раз на уровне дата-центров у нас сейчас уже четыре но четыре площадки Да одна из них наш вот Владимир и если отключится один задаток центров Ну катастрофы не будет трафик просто перераспределиться между другими дата-центрами и Мы периодически даже проводим учение физически этот центр мы не Выключаем мы просто переводим трафик и смотрим Ну что всё в порядке как это выглядит вот у нас есть сетевая связанность это именно сетевая связанность разные площадки а объединяются через ядро сети и если вдруг Ну мало ли да думает всё будет хорошо да Но мысленно эксперимент отключился перевели трафик все в порядке нагрузка растёт не в разы А примерно там порядка десяти процентов то есть не катастрофически на самом деле Вот мы зарезервировались на уровне еще и дата-центры прекрасно мы поговорили про пространство про электричество надо как-то охладить теперь все это добро опять классическая схема но тут вот со снежинками это кондиционеры чем-то похоже на ваши кондиционеры может быть в офисе дома смысл В чем кондиционер берет теплый воздух охлаждает его под фальшполом это специальный такой полос отверстиями он очень прочный при этом стоят стойки стойки берут холодный воздух из Холодного коридора нагревает Ну то есть сами охлаждаются нагретый воздух удаляется в горящий коридор кондиционер опять берет охлаждает и пошло-поехало так по кругу мы пошли в другую сторону и применили инновационную технологию она называется прямой воздушной фрикулинг и она похожа в принципе просто на то что если мы вот поставили компьютер поставили перед ним вентилятор начали бы на него дуть открыли бы форточку и все вот то есть такой вот принцип практически Ну там чуть посложнее Конечно все равно у нас есть система холодных горячих коридоров У нас есть приточные вентиляторы которые берут воздух с улицы нагнетает его в холодный коридор стойкой берет воздух охлаждает сервер горячий воздух удаляем на улицу и это даже летом зимой чуть-чуть по-другому Зимой мы имеем на улице отрицательные температуры как правило Да ну или там просто холодные для железа не очень хорошо нам нужно этот воздух чуть-чуть подогреть Мы можем за счет собственного нагретого воздуха от серверов взяв его и добавив колонну подмешать получить плюс 20 и будет счастье что нам это дает в принципе Какие особенности фрикулинга мы не следим за температурой летом мы следим за ней только зимой когда нам надо вот эти плюс 20 обеспечить мы не следим за влажностью потому что конденсат нигде не выпадает и у нас просто 200 ступени фильтрации воздух надо с улицы все равно как-то очистить Вот почему Да и преимущество этого инновационного метода Это энергоэффективность самый первый то есть мы не используем фреоны и вот тут Я посчитал примерно что такое ПУЭ да то есть это вот энергетичность это показатель который говорит о том насколько эффективно мы используем электричество да то есть мощность всей площадки все что мы затратили на эту площадку на охлаждение на серверы там еще нас вспомогательные системы поделили на мощность оборудование именно эти оборудование которое только вот на вычислении используется и получили какой-то показатель чем ближе он к единице То есть он в принципе до выше чем один вот в нашем случае как раз одна целая одна десятая при кондиционировании 1,5 но я тут оговорюсь кондиционирование смешанный режим есть кондиционеры которые умеют зимой Во фрикулинге а летом все-таки охлаждают это вот как раз этот случай 1.5 Если только круглый год кондиционирования мы получим еще больше показатель там 175 где-то Бывает даже до двух и Ну экономичнее мы практически на 30 процентов это сто процентов эти 30 процентов можем использовать например на установку другого железа то есть мы экономим деньги и электричество помимо этого мы на схеме предыдущее посмотрели мы этот горячий воздух можем удалить на улицу А можем еще даже как-то по эксплуатировать то есть поставить радиатор с холодной водой на выходе То есть у нас есть сервер они нагрели воздух вентиляторами вместо чтобы на улицу все это выдать мы продули это через радиатор с холодной водой вода чуть-чуть нагрелась не прям вот до 100 градусов до чуть-чуть нагрелась Но это вот как раз таки подход такой Что лучше нагреть вот эту холодную воду чуть-чуть а потом передать ее как раз таки на станцию до нагрева где они уже будут нагревать не ледяную Да скажем так воду А уже чуть-чуть теплую этим самым сэкономим деньги на каких-то энергоносителях и получим вот этот вот такой ну бонус Это здорово Мы можем отапливать воздухом технологическая простота приточная Система состоит из вентиляторов систему управления и там на фотографии не видно но за ней за этой вот системой за вентиляторами стоят две ступени фильтрации все Ничего особенного вытяжная система еще проще просто вытяжные вентиляторы тоже Все очень просто и мы не используем ничего кроме воздуха для охлаждения никаких фреонов ничего И причем очень много воздуха мы вот этим воздухом в час которым обеспечиваем целый модуль можем надуть 105 воздушных шаров это вот эти вот на которых может быть кто-то летал я к сожалению нет у меня не было такого опыта настоящие вот эти воздушные шары пассажирские Да скажем так А если в шарики надувные Да вот эти маленькие там я по моему 20 лишним миллиона насчитал вообще этих шариков Мы можем наблюдать за один час хорошо мы поговорили уже про электричество по воздуху про воздух про пространство Ну и собственно теперь железо самое может быть интересное именно сервера и стойки как раз таки благодаря собственному железу собственным стоечным решением который в Яндексе разрабатываются у нас свои собственные серверы вот мы смогли перейти на фрикулинг Это здорово это как раз таки вот мы пошли в сторону того что у нас стойка это единое целое это вот Единая система из серверов и стойки и это вот стойка сама обеспечивает серверы питания и охлаждает их благодаря этому мы как раз таки вот еще можем понимать как нам строить собственный дата-центры мы знаем дизайн охлаждение дизайна дизайн электропитания у нас уже собственные серверы четвертого поколения то есть мы уже там несколько стадий прошли эволюции а внутри серверов оборудования выстроено по логике увеличения температуры допустимой хорды nvme диски а потом уже в конце радиаторы с процессорами и оперативная память сервер без собственных блоков питания и вентиляторов этим всем занимается стойка то есть сервер прям вот без этих вот дополнительных скажем так устройств горячая замена дисков обеспечена и вот этот ключевой скажем так момент почему мы можем не переживать летом за охлаждение прикурингом сервер работает от плюс 18 до плюс 40 диапазон температур то есть плюс 40 спокойно мы выдержим это при том что сервер нагружен на сто процентов этого теста проводились Как правило сервер нагружен процентов на 775 мы понимаем что нас еще даже небольшая Вот чем хорошо собственное железо тем что можно например на базе этого железа построить собственный суперкомпьютер например как вот на нашей площадке галушкин он есть у нас такой вот назван в честь Александр Галушкина одного из главных исследователей теории нейронных сетей на текущий момент 44 место Топ 500 это мировой рейтинг суперкомпьютеров и второе место среди суперкомпьютеров России Как вы думаете кто первое место занимает Сбер Яндекс И третье место кстати тоже Яндекс и суперкомпьютеры наш частности построен на базе четвёртого поколения наших серверов которые мы сами разработали он обучает Яндекс gpt Алису и вообще на самом деле Много чем там занимается хорошо мы поговорили про железо но Да у нас есть роботы У нас есть автоматизация но робота немножко в другом переносе пока нету этих вот до гуманоидных или как они там называется но у нас есть люди и без людей к сожалению никуда но может быть к счастью не знаю Смотря для кого и если посмотреть вот прям на дата-центр уже с точки зрения людей с точки зрения процессов Да эксплуатации у нас можно выделить Две таких вот скажем два направления это эксплуатацию инженерной инфраструктуры и эксплуатации именно it-оборудования вот служба it инфраструктуры TDC - это it дата центра диссиопс это дата центра Operation то есть Инженерная инфраструктура Что такое DC да То есть дата центра operations мы занимаемся вот я в частности тоже отношусь опсом мы занимаемся тем что обеспечиваем всю инженерку Да мы обеспечим системный электроснабжение система охлаждения и вентиляции механические системы какие-то вспомогательные системы их на самом деле очень много мониторинг мониторим именно нерпу контроль работ на площадке и вот на фотографии реально прям вот пример как человек работает и БП как он занимается настройкой именно маховика it DC у них ну чуть-чуть там другое направление это именно it инфраструктура серверы стойки и вот как раз таки есть первое направление их работы Changer квест с текущими серверами например что они делают у серверов может быть меняться конфигурация оборудования например надо добавить каких-то там объема дисков Апгрейд Может быть там сервис перестраховался заложил себе на железо чуть больше каких-то мощностей решили немножко понизиться сделать там даунгрейд можно какие-то изменения сети проводить всё-таки тоже там архитектуру какую-то новую может ещё что-то установка нового оборудования то есть приезжать стойка ее надо смонтировать поставить кинуть а плинки потом необходимо преднастройку провести то есть железо приезжает с завода оно там протестировалась но надо все равно убедиться все в порядке при настройке уже на месте происходит тоже надо как бы железо ввести в эксплуатацию налить свечи например запуск новых зданий тоже организовать например там юзерскую сеть apmy на самом деле еще много каких-то работ помимо этого у нас есть уже железо текущее и оно К сожалению Все равно выходит из строя какие-то проблемы бывают это вот как раз таки инциденты тут можно выделить скажем так два таких направления работ есть типизированные задачи нам как раз таки очень хорошо помогает роботы они автоматически сами проверяют железо смотрит там Пошли долларам еще как-то понимать что там что-то например не так с диском например Check Drive но при этом к нам прилетает Задачка в которой уже указано Какой диск в какой стойке в каком-то центре в каком именно с Лате вышел из строя инвентарник серийник мы приходим меняем и не задумываемся насчет того что ли мы делаем да то есть нам уже робот подсказал В каком месте можно все поменять это очень быстро решается задачи замена памяти Также может быть rmm это у нас задача связаны именно со стойками нашими потому что там кроме серверов и самой стойки есть различные системы мониторинг тот же самый там работа управление вентиляторами стойками псу то есть блоки питания в стойки то есть вот эти задачи Мы тоже решаем диагностика это уже чуть Более сложный момент например робот понял что там что-то с нотой не так но не очень понятно что конкретно вот тут начинается уже такой своеобразный детектив инженеры приходят на место и начинает диагностировать ноду пытаться выяснить что конкретно с ней тут могут быть замены матплат замены там например CPU какие-то работы с сетью там именно на этом сетевыми картами или ещё что-то системой там питание и для того чтобы нам обслуживать дата-центр с населением 55 тысяч человек до 21 мегаватт Нам необходимо 30 человек Сейчас у нас на площадке работает 30 человек и это все и инженеры и административный персонал и там инженер по охране труда даже есть у нас при этом мы 25 тысяч тикетов в год Найти оборудование выполняем 900 нарядов У нас вот Инженерная инфраструктура в год на техническое обслуживание это не считает там каких-то ремонтов потом мы постоянно вводим в эксплуатацию новые дата-центры вот у нас площадка растёт у нас было одно здание второе третье скоро там четвёртое появится то есть мы постоянно растём мы постоянно вводим новые модули новые дата-центры и все это силами от 30 человек при этом мы стабильно высокие selay свой держим мониторим всю инфраструктуру и оперативно реагируем на все события в дата-центре То есть это вот мы справляемся так если посмотреть вот такой небольшой лирическое отступление вот 97 год компьютер под столом а Яндекс начинался с этого сейчас в 23 год у нас уже до 5 собственных дата-центров до 5 собственных площадок одна из них вот во Владимире и скоро уже будем запускать в Калуге это будет самый мощный до центра Яндекса на данный момент вот я так клёво это здорово Ну и вообще да что нам дает как-то вот дата центра нашей собственной мы если в общих словах мы знаем про нашу инфраструктуру Все мы можем сами строить собственный центра они очень отказоустойчивые быстро запускать инфраструктуру например мы уверены что там облако наше Яндекс Клауд может спокойно жить на нашем железе на нашем вот этой а-а скажем так схеме питания с темами охлаждения и так далее мы копим знания мы можем решать задачи очень быстро и это здорово Надеюсь что вот мой доклад сегодня немножко Вот этот чёрный ящик приоткрыл показал что какая там жизнь да на Марсе есть какая-то определённая что там за стенами дата-центра есть помимо серверов стоит какие люди какие системы электропитания Какие бывают особенности и вот как раз что мы успели узнать что такое дата-центр что можно строить или арендовать например помещение Как организовать электроснабжение Как подойти к этому немножко по-другому Да охлаждать сервер можно окажется воздухом причем Даже летом и кто работает в центре а на этом спасибо да Поделитесь своим мнением о моем докладе Егор Спасибо тебе большое Очень очень круто я вот в свое время знаю как строится и эксплуатируется сервера до центра в Европе Мне кажется мы просто немножечко даже чуть повыше если конечно не брать дата-центра Яндекса наверное друзья поднимайте свои руки пока вы поднимаете один вопрос зачитаю из чата Филипп уйменов Какая категория электроснабжения так сейчас смотрите Да если вот брать правильно я понял вопрос или нет категории электроснабжения тут наверное надо понимать в два направления какой электробезопасности группы относится персонал то есть мы там четвертая группа Ты до тысячи и слышишь аттестуемся при этом у нас есть собственная подстанция на площадке именно PS Яндекс 110 20 там мы скажем так и так далее нашу подстанцию на эксплуатацию и на обслуживание подрядной организации именно там люди которые уже работали с подстанциями мы занимаемся немножко чуть другим напряжением Но вот все равно мы имеем эти группы 4 группа до и свыше 1000 вольт если брать классно скажем так вот именно саму подстанцию Владимирскую которая вот это вот мощная Да я сейчас точно не вспомню но у неё высокое отказоустойчивость То есть у неё там могут быть кратковременные какие-то там изменения в напряжении Но этому Вот как раз эбпс сглаживаем всё но у них отключение есть и бывает то там раз десятки лет может быть очень высокая степень надёжности этой подстанции Ну надеюсь так или иначе как-то вопрос ответил Спасибо я напомню что тебе надо будет в конце Выбрать самые лучшие вопросы в подарок так из зала спасибо очень интересно доклад У меня два вопроса первый Как у вас построен процесс утилизации старого оборудования то есть диски память возможно целые ноды Это первый вопрос и второй вопрос по каким критериям вы выбираете географию построения Но вот этот центров Ну и Например почему не строится до центра с востока натуральных от Уральских гор спасибо Так ну да спасибо за вопрос А первый вопрос смотрите Я из эксплуатации и правда честно точно прям не знаю У нас есть просто отдельная служба логистики они занимаются там как-то вот утилизации Ну вот я шесть лет работаю наша площадка как раз я начал буквально с первых дней как мы стали запускать дата-центр пока за 6 лет никакого оборудования мы не ну скажем так не утилизировали насчет дисков правда не скажу не знаю честно как там это вопрос реализован надо будет эту коллег уточнить нам постоянно новые диски приходят старые мы можем диагностировать смотреть как-то то есть насколько там есть проблемы что-то Даже Обратно брать в работу потому что мы понимаем что бывает сбой там какой-то может мониторинга может неправильно продиагностировался диск может еще что-то мы можем говорить обратно в работу что прям конкретно с железом старым бывает как его активизирует надо будет у коллег уточнить правда не могу вот прям точно ответить на этот вопрос а второй вопрос почему мы идем ближе к Москве Да почему не за Уралом А смотрите тут суть в чем вот если там схему помните где говорила про резервирование дата-центрами то есть именно там Если вдруг площадка выходит из строя то мы просто перераспределяем нагрузку вот Наша задача быть ближе к еду вот ближе к ядру сети чтобы как раз таки Ну иметь вот эти вот Ну тайминги да то есть задержки минимальные и благодаря тому что мы сейчас уже понимаем что дата-центр Ну во-первых не обязательно строить возле какой-то там генерации А можно рядом с Узловой подстанцией это будет как генерация практически вот как пример Владимир надёжный источник электропитания но при этом мы относительно далеко от электростанции находимся вот потом климат средняя полоса плюс 35 лет это вполне себе мы уже поняли что там плюс 40 мы можем эксплуатироваться даже То есть это вот по климату тоже То есть у нас нету необходимости идти за Урал то есть Наша задача быть ближе к ядру сети спасибо провокационные вопросы с чата а сможете сделать хостинг дешевле хеттснера во-первых не знаю кто такие правда не знаю кто такие во-вторых мы все-таки инфраструктура которая ну для Яндекса да то есть мы в первую очередь обеспечиваем свои сервисы которые дарят радость Уже вам пользователям в том числе Например Яндекс облако Вот это вопрос уже наверное к облаку надо задать непосредственно к ребятам Кто занимается У меня два вопроса вопрос первый про трупс то есть это всё-таки штука популярная Да она типами меньше места занимает и не надо батарея обслуживать и так далее но все-таки она ложится и примеры и в России они были вот вопрос первый насколько вы часто тестируете свои трусы и как Ну Насколько часто с ними были проблемы были а второй вопрос Это хорошо когда у тебя там свои стойки свои сервера и вот Спасибо диапазон температур А что из этого можно использовать в коммерческом дата-центре так по первому дропс немножко уточню тут надо понимать drops это система которая объединяет себя дизель и БП в одну коробочку Да у нас все-таки Упс то есть и BP отдельно дизель-генератор отдельно Да друг даже в принципе вот как Единая система он чуть подороже получается чем вот по отдельности вот эти вот схемы построить Как часто выходит из строя Ну мелкие трудности бывают Да мы все равно проводим постоянное то у нас есть график технического обслуживания мы стараемся как бы не запускать эти проблемы каких-то крупных факапов я Ну не знаю Да ну то есть какие-то мелкие вещи Бывают там иногда бывает там разбалансировка по подшипникам Ну это как бы текущий момент ну там можно пойти и ребята подкручивают там регулируют как раз на фотографии там был как раз этот момент регулировали подшипник для Вот как раз таки маховика То есть у нас есть график обслуживания мы по нему идем работаем и избегаем каких-то крупных факапов это наверное по первому вопросу А второй применимость вашей технологии смотрите вот у нас сейчас на площадке три дата центра мы их называем там кто-то может быть называет их там кластерами то есть у нас есть Альфа Бета Гамма Да например вот так вот три здания первым знаний У нас вот особенность наша модуля в дата-центре там стоит наше железо оно охлаждается прикурингом Но есть одна комната она называется НОК Network coperation Center там сетевое оборудование там мозг дата-центра Да скажем так вот сетевой в первом здании у нас там кондиционеры Вот именно в этом помещении и они умеют зимой Во фрикулинге а летом в кондиционировании потому что ног это все-таки вендоровское оборудование то есть сетевое вот тут перезаложились во втором здании решили Провести такой небольшой эксперимент поставили уже нашу ленту установку Ну как прикуринг только поставили ещё туда специальные радиатор чтобы охладить воздух Если надо будет посмотрели как себя ведет Здорово я вот в гамме у нас уже ног с вендоровским оборудованием уже наверное получается третье лето вот сейчас будет эксплуатироваться во фрикулинге То есть вендоровское оборудование Как показывает практика живет при фуфрикулинге весь вопрос о том как сам вот коммерческий цок к этой организации подойдет насколько они там условно готовы рисковать не готовы попробовать не готовы попробовать и насколько вот эта гарантия сохраняется да то есть тут весь вопрос потому что насколько я знаю вендер любит вот этот вот там плюс 23 + 27 по-моему максимум типа выше говорит пока мы за железа не отвечаем вот Ну вот эти вопросы надо скажем так коммерческом соду самим решить насколько они готовы применять вообще в принципе прикуринг Спасибо пожалуйста спасибо Вот здесь вопрос Привет Спасибо большое за доклад вопрос Вы сказали то что температура дата-центре может быть от 18 до 40° и всё будет работать стабильно но вопрос тогда Каково инженером при температуре 40 градусов И из этого следует Следующий вопрос Вы сказали о огромном количестве дисков и соответственно вопрос как часто допустим какой-либо оборудование выходит из рабочего состояния Как часто соответственно инженеры приходится приходить температура до плюс 40 градусов чтобы его обеспечивать смотрите Да ну летом тут какая есть температура на улице тут мы ничего с этим поделать не можем Но согласитесь плюс 35 И когда у вас есть вентилятор рядом Да их там много вот у нас в модуле все-таки более-менее комфортно если вот по ощущениям А вот когда Вы заходите в горячий коридор и вам там надо что-то поделать А там может температура достигать плюс 55 там да очень печально И желательно там находиться ну максимум 5-10 минут выходить как-то там себя приводить в порядок и если надо продолжать работать то есть долго работать в Горячем коридоре не стоит вот это что касается людей зимой плюс 20 вполне как бы комфортно но мы надеваем Например у нас есть просто толстовочка Рабочая можно ее надеть будет вполне хорошо Мы зимой плюс 20 все-таки обеспечиваем в модули поэтому и зимой летом но летом чуть может быть посложнее если прям жарко на улице но горячие коридор это вот и зимой летом там плохо очень тепло По поводу дисков Да ну ни для кого не секрет диски самая Слабое звено Да вот и вот цепи железа А если брать вообще в расчёте не помню там мы что-то читали у нас там выходит очень минимальное количество если относительно серверов и дисков Да диски выходят чаще из строя но всё равно это Малый процент У нас очень надёжный всё-таки инфраструктура у нас ребята которые проектируют серверы и еще заодно проверяют хорды разных вендоров разных модификаций Да скажем так Там же есть для дата-центров например харды там Зеленый на природышки по моему черные то есть они тестируют вот эти вот диски смотрят какие лучше всего себя ведут и как правило мы закупаем уже что-то надежное диски выходят чаще всех из строя Но если брать в общем проценте то там все равно маленькое количество день да приходится побегать если ты допустим вот на смене сколько порядков это дисков в день меняешь это один 10 100 или тысяч ну тысячи точно нет где-то наверное в диапазоне 50-70 штук вообще в принципе за смену я не один за смене до нас Несколько человек ну где-то вот примерно так понятно спасибо это вот совокупности по 41.000 серверов Давайте ещё два вопроса один из чатов поиска мест где Владимир Да разговариваем Сколько времени заняло от возведения до ввода в прот и какие-то запуска серверов у вас и с таким проблемам вы столкнулись при этом Ну да Если я правильно вопрос понял как много времени заходит ну э мы сейчас опустим вот этот вот этап Да когда площадку выбирали согласовывали потому что всё равно у нас бюрократия одного достаточно такое вот момент острый а трудный Но если вот всё в порядке всё согласованно И вот момента скажем так закладки Да там условного там первого там я не знаю камня да кирпичика до запуска Всего дата-центра имеется в виду вот стройки А примерно год ну у нас есть такой подход мы его называем модульность что это такое у нас до центра разбит на модуль вот так вот прямо они идентичны друг другу есть помогательные Опять эти модулей и вот мы когда строим здание новое как правило нам в эксплуатацию сразу продаются вспомогательные и первый модуль при этом стройка еще не закончена из-за того что модули изолированы друг от друга У них своя схема питания своя система охлаждения мы можем начинать уже запускать дата-центр Ну и модуль железо устанавливать и эксплуатировать даже еще в момент стройки то есть по факту первые модули мы получаем еще раньше там по моему спустя полгода месяца в 7 то есть вот и дата-центр продолжает строить саму же эксплуатируем железо То есть когда идет эксплуатация модуля то рядом-то за стеной и это нормально в принципе ничего такого страшного в этом нет это даже ещё быстрее вводим в эксплуатацию до центра а какие проблемы вообще возникли возникали и Ну вот возвели какие-то проблемы основные которые там не знаю электричество отвалилось пристройки там что он такое было вообще Нет смотрите У нас стройка вообще имеет свою схему питания она не заведена не как то есть она ограничена от эксплуатации скажем так то есть влияние стройки минимальное Спасибо Егор вопрос из чата разработка своего сервера вот то что ты рассказывал про собственные сервера собственной стойки Это имеется в виду что вы форм-факторе Юнита разработали собственный дизайн или ещё там собственные какие-то наработки По материнской плате а зуб блоком питания и так далее У нас э во-первых сервер - Это только часть у нас ещё есть стойка в ней у неё внутри организована система электропитания то есть есть так-то у нас шина вообще идёт медная нода садится своими клеммами питания на эту шину То есть у нас там организовано а-а и проектируется своё отдельное электропитание системы кулинга то есть охлаждения но до свой форм фактор в своём отплата в том числе понятно что процессоры память там хорды это ну как бы мы не разрабатываем Но вот именно вот в этой части мы создаём своё железо Ну то есть по факту вендорская - это процессработки давайте последний вопрос нет Ну давайте тогда остальные хотят спасибо спасибо за доклад во-первых во-вторых вы сказали что стройка своего до конца постройка своего дота-центра это способ попробовать новые технологии вот и есть ли какие-то технологии которые вы попробовали а затем решили от них отказаться при постройке следующих дота-центров и наоборот который настолько понравились что вы теперь ищите способы как внедрить их в старые Ну вот фрикулинг да это вот как раз таки тот случай когда мы попробовали новое это реально клёво и у нас есть дата-центр в Сасово он такой некий своеобразный гибрид да да браунхиллы то есть там есть Старый завод но мы в нём внутри строим модуль прям модуль и вот у него мы как раз внедряем в прикуринг и Это здорово а вот наверное насчёт технологии от которой стараемся отказываться это вот как раз система кондиционирования То есть у нас есть дата-центр Здорово круглый год кондиционирования и скорее всего вот этот подход мы не будем больше использовать спасибо большое Егор тебе надо выбрать лучшие вопросы зала так много что я что-то потерялся Да так сейчас я пытаюсь вспомнить Вот наверное вопрос про Почему мы не идем на Урал а строимся внутри есть кто-то Да прошу прощения я подарок оставил спикерской Подойдите ко мне я вам сейчас пойду еще один подарок значит организаторов А ещё от тебя будет Вот бомба Егор Спасибо тебе большое это Это кому интересно спасибо тебе большое за то что к нам пришёл классный доклад ребята не забывайте оценивать Егора Спасибо большое всем хорошего дня и отличного и отличная конференции"
}