{
  "video_id": "0DsW-614DPo",
  "channel": "HighLoadChannel",
  "title": "Масштабирование пропускной способности поиска Яндекса в 4 раза / Сергей Скородумов (Яндекс)",
  "views": 218,
  "duration": 2803,
  "published": "2024-10-29T02:58:10-07:00",
  "text": "Привет Сергей Ждём с нетерпением твой рассказ как вы выросли Для чего выросли Всем большое спасибо что пришли Как вы видите Меня зовут Сергей Скородумов Я работаю в Яндексе руковожу небой сбой это у нас так называют организацию из нескольких команд Вот и Сегодня я расскажу вам как мы отмашка в четыре раза то есть мы держали какое-то количество rps в поиск а теперь мы держим в четыре раза больше Итак изначально в какой-то момент времени у нас было три дата-центра и мы так как мы большая крупная уважающая себя компания всегда готовы к тому что один из этих дата-центров может отвалиться Может быть там трактор переедет провода А может быть мы выкати релиз плохой нам придётся этот дата-центр закрыть и поэтому если вот вы посчитаете получается нам надо жить на двух дата-центра и в каждом дата-центре держать 50% от пикового трафика регу что мы выдерживаем этот трафик проводили учение меняли веса на балансе и делали так что в нагружает приходило 50% от нашего пикового трафика наш Таргет проверяли что это так и переходили к следующему дата-центр если конечно это было не так то мы оперативно чинили и масштабировать но в какой-то момент всё изменилось один из наших трёх Датан дата-центров он был не в России и он был в Финском городе мля Скорее всего я неправильно это произношу И к сожалению у нас появились риски того что нам отключат электроэнергию Поэтому нам теперь надо было готовиться к тому что мы живём всего лишь на двух дата центрах а два дата-центра по сравнению с тремя значит что нам надо выдерживать весь трафик на одном дата-центре То есть 100% всего пикового трафика или в два раза больше чем столько сколько мы держали раньше при этом мы никак не могли просто взять и долить железо потому что просто у нас 2 и железа не было К сожалению или к счастью Зато у нас появилось Такая замечательная Задачка иже их грузить до отказа то есть проводили все те же самые учения но теперь мы не останавливались на нашем таргете предыдущем 50% одолевали до того момента когда наш дата-центр просто разлома ется и мы там начнём немножечко не отвечать пользователям Мы конечно же сразу же в этот момент убирали трафик обратно потому что мы очень ценим наших пользователей и не хотим не отвечать им Но таким образом Мы проверяли что Сколько сколько трафика мы держим в каждом дата-центре и э за счёт этого мы находили Какой из сервисов какое место сломалось и сразу же очень оперативно его чинили также Мы конечно же дошли до всех команд рассказали им какая у нас важная цель про масштабирование попросили их всех ещё до того как они сломаются пойти и отмашку топов ранжирования деградация э в данном контексте значит что мы немного меняем наши алгоритмы таким образом чтобы чуть-чуть уменьшилось качество но при этом мы выдержали значительно больший rps Сейчас я вам про это расскажу поподробней э итак вы видите э как очень упрощённую версию того как работает наше ранжирование слева по сути когда приходит запрос а сначала на Самой нижней стадии мы используем какие-то очень легковесные алгоритмы э в пересчёте на один Документ и ранжиру огромное количество документов потом мы сортируем их по релевантности по тому что на ранжированы и выбираем Top N отправляем их на стадию выше после чего повторяем тоже самое но уже с чуть-чуть более тяжёлыми алгоритмами выбираем какой-то топ K отправляем на стадию выше на более тяжёлые алгоритмы и так далее В итоге э там какое-то количество документов доходит до самой последней стадии на которой уже самые наши тяжёлые алгоритмы КАД бусты э Трансформеры ранжируются их и в итоге отдают пользователю а его топ-10 выдачи которую вы все Я очень надеюсь видели что же мы сделали А мы э придумали такую очень простую идею Почему бы нам эти топы количество документов которое мы ранжиру на каждой стадии не уменьшить то есть вот в данном примере мы там с Самой нижней стадии ранжиру 10 млн документов это не настоящая цифра я взял её из головы но а теперь будем ранжировать 5.000 а но но мы конечно же не хотим просто захард кодить эти цифры э и дальше с ними жить потому что э потому что всё меняется мы допустим какой-то какую-то стадию ускорим и соответственно она сможет выдерживать гораздо большие топы или наоборот замедлится уменьшать это слишком много работы нам этого не хочется делать поэтому мы сделали все наши алгоритмы адаптивные Ну и за счёт этого мы также имеем возможность в зависимости от текущей нагрузки какое сейчас время там днм у нас больше нагрузки ночью у нас меньше нагрузки когда мы теряем один дата-центр во время учений у нас ещё больше нагрузки мы можем размеры топов адаптивно менять И что мы и делаем то есть в обычное время пользователь видит самую лучшую выдачу с самым лучшим качеством А когда мы проводим учение качество немножечко уменьшается Это мы сделали за счёт того что каждый слой он следит за тем насколько он нагружен и отправляет верхнему слою информацию о своей загруженности после чего верхний слой решает какой размер топов должен быть у каждого из нижних слоёв и следующим запросом передаёт в Нижние слои информацию о том Какого размера слои должны быть на них также мы значительно изменили то Как проводятся наше учения раньше как я уже рассказывал мы меняли веса на балансе и какой-то процент трафика постепенно переносили из одного дата-центра в другой но у такого способа есть несколько проблем первая проблема - Это то что пользователи в таком случае могут получить не ответ то есть by дизайн когда мы проводим учение мы Дору Дант до такого состояния что польз начинают получать сразу же уменьшаем процент трафика чтобы никто не пострадал и из этого вытекает сразу же вторая проблема она заключается в том что мы не можем очень долго держать дата-центр в полонном состоянии что нам на самом деле хочется сделать потому что мы можем посмотреть налоги мы можем посмотреть на графики снять перф мы можем покрутить какие-то параметры и посмотреть что будет но к сожалению мы этого не можем сделать потому что мы очень Джим нашими пользователями и поэтому мы переделали учение и теперь трафик Мы не увеличиваем в том ДЦ в котором которое мы сейчас нагружая а полностью вводим пользовательский трафик из этого дата-центра а потом на балансе оставшегося дата-центра мы копируем запросы из оставшегося дата-центра в и таким образом из точнее оставшихся двух дата-центров пока что мы ещё живём на трёх дата-центра и из оставшихся двух дата-центров мы копируем трафик в нагружает и в этом случае уже пользователи не страдают как только мы в этом дата-центре появляются не ответы они происходят не настоящим живым людям а просто какому-то выдуманном нами трафику который при этом совпадает с настоящим и таким образом мы можем грузить до отказа и долго держать дата-центр в поломано состоянии после чего Ну мы можем там как я уже говорил снять перв например но остаётся Единственный минус у такой схемы что нельзя дата-центр нагрузить больше чем на 100% трафика потому что у нас в огромном количестве слоёв есть кэш и соответственно Если вы начнёте не просто слать запросы из друго дата-центра А ещё и копировать их то они попадут в кэш это будет уже нечестное учение но мы эту проблему тоже решили мы просто взяли наши логи за предыдущие дни и запросами из логов мы до нагрузили э нагружает сентр и таким образом мы по сути можем на учениях получать абсолютно любой rps какой захотим Итого за счёт вот этих вещей про которые я рассказал и ещё большого количество других оптимизаций более маленьких мы смогли удвоить пропускную способность наших дата-центров то есть теперь каждый дата-центр держит 100% трафика и мы полностью отказались от дата-центра в Финляндии то есть теперь мы полноценно живём на ддц Мы учимся зеркалирование и синтетикой оптимизировали кучу сервисов и внедрили крутые методы деградации но после этого возникла вторая проблема кто-то бы сказал что это не проблема но трафик на нас растёт постоянно это происходит во-первых из-за того что пользователи начинают чаще пользоваться Яндексом по сравнению с другими поисковиками то есть рост доли во-вторых пользователи делают больше запросов Ну а в-третьих появилось ряд сервисов и они постоянно растут это Алиса и сеч гпт которые Когда в них пользователи приходят с запросами они генерируют запросы в поиск Ну наверное септ это очевидно а Алисой многие тут Надеюсь пользователи пользовались и Ну вы понимаете что вы там что-то У Алисы спрашиваете и она не знает это просто с головы она делает запрос в поиск Итак Наш новый Таргет - это 150% трафика в каждом дата-центре Мы верим что трафик вырастет в полтора раза соответственно Нам нужно будет его выдержать то есть теперь новый наш основной Таргет - Это 150% от текущего трафика но но мы также верим что наши сервисы должны токо выживать нагрузку но и быть готовыми к тому что на нас придёт ещё больше нагрузки просто ну на всякий случай быть к этому готовыми и поэтому мы поставили себе ещё и второй Таргет выдержать 200% То есть если например трафика станет не в полтора раза больше а в два раза больше И одновременно с этим Мы потеряем один дата-центр то мы хотим к такому быть готовыми и поэтому у нас есть вторая цель про 200% но 200% мы конечно хотим держать не на самом нашем максимальном качестве это было бы слишком слишком дорого Мы хотим просто как-то отвечать пользователям решать проблему пользователей но при этом как при этом выдача поиска будет визуально отличаться от обычной А вот при 150% Мы хотим вот прям очень хорошую выдачу такую что человек в принципе не отличит какая выдача в обычное время какая выдача под нагрузкой и разница вот в 150% она только видна на а эксперименте А обычный человек её даже не заметит Итак как мы как мы этого достигли мы продолжили учение Теперь мы просто нагружая ещё больше мы улучшили наш туринг сделали ещё больше улучшений производительности и ещё больше методов деградации сейчас я вам про них расскажу вот тут на экране вы видите наш сервис мы назвали его перфоратор суть его на самом деле очень простая что перфоратор он раз в несколько минут обходит все продо вые сервисы и снимает с них перф Надеюсь все в курсе что такое перф если кто писал на c+ Plus то точно знают а потом Вот в такой юай которую вы видите на экране Вы можете выбрать свой сервис выбрать конкретное время когда был снят перф и посмотреть на Flame GRA и что важно Это тот самый Граф который был в проде э это это это очень важно Возможно кто-то не знает но когда вы просто у себя на деф тачке поднимаете сервис и как-то его нагружается случайными запросами то Flame GRA может очень сильно отличаться от того как он выглядит в проде под мы нашли в одном из наших сервисов spinlock который было совершенно не видно никогда Кроме того момента когда сервис уже полностью разваливался ещё несколько крутых оптимизаций это оптимизация балансировки тут я расскажу поподробней во-первых тайм-ауты Мы в какой-то момент э заметили что под нагрузкой наш сервис в наш сервис в какой-то момент начинало приходить гораздо больше запросов чем мы посылаем то есть там условно мы шлём X в него идёт X потом мы там 2x в него идёт 2 мы шм 3 А в него прилетает 5X запросов почему-то и после недолгого Дега мы поняли что дело в том по замедляется Вот это кстати правда скорее всего для большинства сервисов и вы себе тоже сможете внедрить такие вот оптимизации так вот под нагрузкой сервис замедляется чем больше нагрузки тем медленнее он отвечает и в какой-то момент сервис начинал отвечать настолько долго что уже не укладывался в свой таймаут из-за чего приходил пере запрос соответственно нагрузка увеличивалась сервис начинал отвечать ещё медленнее и приходило в него ещё больше запросов Таким образом он просто разваливался решили Мы конечно же эту проблему очень просто мы тайма подняли после чего все сервисы обошли которые у нас есть в поиске проверили что у них тоже адекватные тайм-ауты что они соответствуют тому времени сколько за какое сервис отвечает на учениях И что в таймы не уем вот вторая крутая оптимизация балансировки - это адаптивные хэдже запросы тут расскажу поподробнее Возможно кто-то не знает что такое хэдже запросы их придумали не в Яндексе это общая человеческая технология она существует для того чтобы ускорить ответ сервиса идея такая что в какой-то момент вам приходит запрос и ваш там балансер или сервис шлёт в сервис ниже запрос А через какое-то время может быть сразу же может быть через какое-то время там в q90 или в q70 он шлёт второй запрос но уже в другой под этого же самого сервиса а потом он ждёт э с самого первого ответа который он получит И уже использует его дальше то есть по сути таким образом вы посылаете два запроса возможно задержкой возможно без задержки и используете тот который придёт быстрее это на самом деле даже если слать запрос в каких-то высоких кванти то самые высокие квантили там условно q99 или q999 они значительно ускоряются за счёт хдт запросов но как вы наверное понимаете это добавляет значительную нагрузку на сервис что плохо в момент когда вы хотите выдержать как можно больше нагрузки что же мы сделали Мы просто заставили сервис сервис следить за тем насколько он нагружен и сообщать о своей нагрузке балансе или сервису выше который делает в него запросы и балансер если видел что сервис уже не справляется что в него больше не надо слать запросов он менял процент хэдже дов которые посылаются в сервис и соответственно мы получали замедление небольшое из-за этого но значительно расширяли то сколько ПС мы держим и что очень важно за счёт этого в обычное время когда у нас не потерян дата-центр когда мы не проводим учение мы всё ещё использовали хдт запросы и видели ускорение вот ещ одна оптимизация - это быстрые не ответы е ли сделать когда поняли что дата-центр - это очень жестокое место по сути Вот наверное все вы пользуетесь облаками если вот зайти в облако и накликать себе сервис из там тысяча подов в каждом поде там по 10 цпу 100 ГБ Ram там какое-то количество диска то вы как бы предполагаете что эти Э что эти выделенные поды в Облаке они абсолютно одинаковые они держат одинаковый rps но это не так в дата-центре может очень много чего происходить Может на разных подах быть разное цпу какие-то перегретый то плохие диски и из-за этого по нашим замерам какие-то поды одного и того же сервиса могут выдерживать на 20% больше нагрузки чем другие поды это очень плохо потому что вот эти 20% Мы в обычном случае теряем потому что сервис начинает не отвечать когда начинают не отвечать самые слабые его поды и в этот момент мы считаем что вот учения закончены мы взяли rps который могли взять что же мы сделали Мы заставили каждый под каждого сервиса Независимо друг от друга а считать насколько он загружен и отправлять это балансер после чего балансер когда он видел что какой-то пот уже не может отвечать Он это видел за счёт того что этот пот э на запрос из балансера послал быстры не ответ То есть он не начинал выполнять никакой код сервиса не начинал ничего ранжировать ничего считать он просто сразу же говорил у меня не хватает ресурсов чтобы Вам ответить и балансер это видел очень быстро и сразу же посылал запрос в другой под который менее нагружен и таким образом мы ещё намного расширили пропускную способность нашего сервиса и на самом деле такие вот технологии может себе внедрить кто угодно и вы тоже можете их себе внедрить и посмотреть на ваши тайм-ауты если они расставлены не супер правильно Если у вас есть хдд то сделать их адаптивными и в принципе в любой сервис который должен выдерживать большую нагрузку но не шардирование ответы также мы сделали куча архитектурных Изменений очень крутых например cash тут немного отклонён декс интернета все документы про которые мы знаем и которые можем показать пользователю на выдаче он просто самый самый большой по количеству ресурсов сервис в поиске Мета поиск - это тоже очень большой сервис на нём происходит самая последняя стадия ранжирования и он делает запросы в базовый поиск и раньше ВС работало Так что Мета поиск у себя на подах широва запросы к базовому ради того чтобы сэкономить железо Потому что если запросы не Каширова то соответственно там в два раза больше ПС приходит на базовый он требует два раза больше железа из этого изго запросы к базовому прямо на метапоисковые и положил кэш себе на диск а потом следующий запрос котики должен прийти уже в этот же под Мета поиска Иначе мы получим камис и из-за этого собственно мы получали недо утилизацию ресурсов Мета поиска потому что потому что в какой-то момент самый нагруженный под Мета поиска уже не мог отвечать И в этот момент нам нужно было либо принимать решение что всё уже дальше мы не можем либо слать запросы в другой под Мета поиска который тоже мог ответить Но он таким образом нагрузил бы базовый Мета поиск и соответственно Мы бы уже начали упираться не в метапоисковые поиск что же мы сделали Мы на самом деле выполнили очень простую идею ой Извините но не простую в реализации мы сделали отдельный сервис который у себя на ССД хранил кэш Мета поиска и когда в Мета поиск приходил запрос он сначала ходил вре ш проверял Есть ли кэш Если есть то использовал его и отвечал сразу из кша И теперь мы могли делать запросы в раун Робином соответственно полностью нагрузить все поды Мета поиска равномерно что важно Вот вы наверное все подумали что можно же просто было дис накрутить который бы из памяти отвечал но мы когда считали железо получилось что очень сильно много редис требует и гораздо проще было бы просто в Мета поиз долить железо чтобы он любую нагрузку выдерживал чем делать отдельный сервис на Реди вместо этого мы сделали реш который отвечает с SSD а не с памяти ещё одна оптимизация - Это умный картинки котиков но по каким-то другим запросам например YouTube картинку показывать необязательно Ну и вообще много таких запросов есть Ой а и поэтому у нас уже очень давно был пред классификатор по сути наши дорогие коллеги мелькин который брал запрос а на выход выдавал вероятность того что по этому запросу надо показать картинку и я вот сейчас рассказываю про картинки м Но вообще классификаторы они есть для большого количества сервисов для поиску по видео для поиску по товаров для рекламы и по сути Мы считали вероятность того что нужно показать картинки и если она больше чем какое-то захард кожанное число то мы запрос посылали в поиск по картинкам А если же оно меньше чем это число там например 5% то мы не посылали запрос мы так очень долго жили а потом решили что А давайте-ка мы будем эту вероятность менять адаптивно то есть мы например знаем что поис по картинкам выдерживает какой-то nrps и э на самом деле мы можем если в нас вообще в Обычный поиск льётся меньше чем nrps мы можем просто все запросы посылать и если вдруг наш классификатор ошибся Мы за счёт этого э выиграем получим аранжировать надо их посылать или нет Когда они у нас будут А если нагрузка очень большая то тогда нам наоборот надо посылать меньше запросов и мы как раз-таки за счёт умного классификатора выбираем самые лучшие запросы которые по которым наиболее вероятно нужно показать картинки и посылаем в поиск по картинкам только их вот тут на примере вы видите код Как это сделано просто для того чтобы понять Как это работает на самом деле Конечно мы не не складываем запросы в очередь и не сортируем их мы просто адаптивно меняем Вот эту вот константу э которая э константу вероятности того что нужно сходить в картинки того что нужно показать картинки по этому запросу и таким образом Гарантируем что в поис по картинкам всегда льётся не больше чем N rps где N - Это столько сколько они могут выдержать ещё одна крутая вещь - Это перебан сиров ресурсов по кнопке тут идея очень простая мы подключили ак ак - это infrastructure CDE идея такая что вместо того чтобы хранить спеки своих сервисов количество цпу Ram диска сети и так далее где-то в UI облако как это делают люди обычно вместо этого мы сохраняем эти спеки в наш репозиторий и благодаря этому мы во-первых можем из кода э видеть эти спеки кодом менять эти спеки и видеть историю коммитов того как мы Их поменяли После чего мы э после того как мы перешли большинством наших сервисов на infrastructure Code мы просто проанализировали э сколько каждый из наших сервисов потребляет цпу А у нас их очень много если там у вас три сервиса Вы можете просто вручную это сделать это вообще было бы не проблема но у нас микросервисов огромнейшее количество сотни поэтому вручную не получится их проанализировать мы проанализировали их кодом после чего те сервисы в которых потребление цпу было небольшим из них желе забрали и в те сервисы в которых потребление цпу было уже слишком большим и они не могли отвечать на учениях Вот ещё одна вещь - Это изоляция токсичных сервисов опять же вернёмся к облаку в Облаке вы обычно накли ете какое-то количество ресурсов там например 10 цпу 100 на один под И ожидаете что все эти ресурсы они ваши никто не может зари никто не может у вас их отобрать Но к сожалению это не так конечно же Дорогие наши коллеги из облака и дата-центра стараются очень сильно чтобы всегда все ресурсы которые вы себе забронировали были ваши но бывают Бывает такое что А на ваш под заселяются токсичные соседи которые выедания про который Кстати тут тоже были рассказы возможно кто-то из вас слушал в itus и из-за этого поиск не до получал железо которое было на самом деле его мы эту проблему решили так что с помощью цпу это такая линуксоида И за счёт этого значительно улучшили нашу жизнь в дата-центре если кто не знает нума ноды это такие кусочки цпу вот большой цпу например ам дш имеет они рекламируют его как 256 ядер Но на самом деле это в нём не 256 ядер в нём четыре нума ноды четыре Ну практически отдельные ЦП которые просто находятся на одной железке и если сервисы специально разделить по этим ну Мадам то они будут гораздо меньше мешать друг другу и вот так вот мы взяли наш Таргет в 150% трафика но это не всё дальше нам надо было взять 200% трафика расшириться ещё больше тут я очень хочу подчеркнуть что вот вот все те методы деградации которые про которые я вам расскажу сейчас они вам как пользователям Яндекса не видны То есть если вы пойдёте в Яндекс ни в какой момент времени вы не увидите то про что я вам сейчас расскажу Потому что эти оптимизации мы сделали специально для ситуации когда одновременно к нам придёт неадекватно большой трафик гораздо больше чем обычно и мы потеряем дата-центр такого ещё одновременно с этим такого пока что случалось Так что вы наши замечательные методы деградации никогда не видели Сейчас я вам про них расскажу Итак первый первая штука которую мы сделали - это платиновый тир он у нас уже раньше был А собственно идея такая что базовый поиск Э он очень большой и мы разделили его на несколько тиров например вот есть свежий тир в нём лежат новости и все самые свежие документы для которых важно на быстрое обновление и он оптимизирован под то чтобы документы обновлять быстро есть платиновый тир в нём лежат самые лучшие документы и большой тир в котором лежат Ну вообще все документы если смотреть по количеству железа то большой Тир на во много-много раз больше чем все остальные тиры но если смотреть по количеству документов которые показываются на выдаче пользователям то на самом деле самый популярный тир - это платиновый просто потому что там лежат все ваши любимые сайты ВК Ozone maret всё что пользователям нравится больше всего Оно лежит именно в платиновом тере И что же мы сделали тут идея очень простая мы просто чуть-чуть железа от большого тира откололи перелили его в платиновый тир И за счёт этого он может вообще без какой-либо деградации отвечать на все 200% трафика и за счёт этого потери качества на учениях они минимизируют ещё одна штука это по шардов деградация А тут важный момент Обычно люди когда говорят что у них сервис шардирование соответственно какие-то документы лежат в одном рде другие во втором треть в третьем и когда пользователь делает запрос в поиск этот запрос летит во все шарды базового поиска то есть на самом деле нам нам их все надо обойти чтобы отдать пользователю ответ тут-то мы и придумали такую идею что давайте-ка мы всё-таки не во все будем ходить а какие-то пропустим мы это сделали Так что шар каждый шарт смотрит на свою загруженность Вт эту информацию сервису выше примерно так же как это было с быстрыми не ответами и с скручиванием топов ранжирования после чего сервис выше он если видит что какой-то шарт уже слишком нагружен то Он меняет вероятность Ну уменьшает эту вероятность со 100% до какого-то меньшего количества процентов того что он пойдёт в этот шарт Ну и за счёт этого на самом деле выдача конечно же ухудшается э потому что теперь пользователь может видеть не все документы но э зато мы можем выдержать гораздо больше нагрузки Ну и э скорее всего по какому-то запросу Э релевантен не один конкретный документ а большое количество и пользователь просто увидит Другой документ Ну и конечно же как обычно мы а включаем по шардов деградацию адаптивно если у нас есть ресурсы Мы её не включаем если ресурсы закончили включаем И как я уже сказал мы настоящим живым пользователям такую деградацию не показываем никогда мы её включаем только на учениях только в какой-то критической ситуации ещё одна крутая вещь Это дистилляция моделей тут суть очень простая есть такой метод размер ML модели уменьшить её потребление ресурсов это дистилляция по сути берётся большая модель которая уже обучена и после этого маленькая модель обучается не не просто на том же датасете что и большая но она обучается на выходе большой модели То есть датасет он обычно размеченный вероятность тут в этом на примере 80% В другом 20 и маленькая модель обучается уже на более чистом датасете который ему сгенерировал большая модель И за счёт этого маленькая модель становится очень похожей на большую хоть и немножечко хуже а но очень похоже А потребляет при этом значительно меньше железа и что мы сделали Мы на все наши сервисы э кроме нашей большой модели положили ещё и маленькую и если ресурсы на этих сервисах заканчиваются мы адаптивно пере включаемся На Нашу маленькую модель за счёт чего выдерживаем ещё больше rps как мы очень любим Итак мы таким образом взяли 200% трафика Спасибо вам большое за то что послушали Меня Сергей Благодарю тебя за замечательный доклад буду знать как это всё работает я очень часто пользуюсь Яндексом коллеги Не забывайте голосовать за доклад давать обратную связь Сергею Это очень важный для спикеров и и для организаторов и у нас первый вопрос пока несут микрофон вопрос из чата Открой пожалуйста секрет вы при поиске показываете кап когда справляетесь с нагрузкой нет качу мы показываем в зависимости от того Какая вероятность того что вы робот если мы считаем что вы робот мы вам показываем кап либо если ты используешь мобильный Интернет это одно и тоже и вопрос Привет Миша Одноклассники понравился доклад вот такой вопрос Если я правильно понял методику Я здесь ели что всего этого подними пожалуста вот я здесь Алло привет Вот вот вижу ты про оранжевый так смотри А откуда мы понимаем что деградация по бизнес Метрика не очень большая если я так понимаю фактически а тесты не проводятся И пользователи реально это деградации не видели то есть в теории звучит так круто Как ты говоришь На самом деле Может быть так что и работает сильно быстрее но и по Метрика не так как хотелось а прям очень хороший вопрос Да это не очевидно по сути У нас есть не только онлайн метрики то есть онлайн метрики - это то что мы видим в а эксперименте когда мы показываем настоящим пользователям выдачу но у нас есть ещё и офлайн метрики и они работают Так что мы генерируем какую-то выдачу а потом мы отдаём эту выдачу асессором и они решают Хорошая эта выдача или нет И на самом деле наша выдача органики то есть документов просто с обычных сайтов не картинок ни видео ни товаров а просто вот обычные сайты она во многом построено именно на оффлайн Метрика которые нам размечают понял Спасибо И у нас следующий вопрос да Привет Саша Яндекс инфраструктура А кроме дистилляции модели квантизация ещё не рассматривали или не нужно были по качеству плохо возможно будет хорошо Спасибо за идею тут ребята на самом деле это делали из они нам как бы сказали что вот маленькие модели дистилляция Возможно они сделали кванти но я проверю обязательно спасибо объединят коллег с Яндекса они встречаются только здесь один раз лет нет маленько чаще и у нас следующий вопрос здесь справа от тебя Ещё правее тут просто яркий свет мы можем только слышать мы идм на звук без проблем Вопрос такой Я не совсем понял Вот пример про Бао поиск Мета поиск Почему кэш не был прикручен в бавом если он уже шардирование Да и Ну естественно любой запрос он тратит какое-то количество железа Даже если ты отвечаешь из кша и соответственно нам пришлось генерировать сотню запросов или там несколько сотен и потом с каждого из сервисов ответить из кэша вместо того чтобы Иша ответить один раз я понял Спасибо И у нас следующий вопрос Да ещё такой вопрос вот Большая работа была проделано а вс-таки что что больше дало эффекта То есть это конфигурация си это изменение инфраструктуры в каком-то виде или это на самом деле там кривые руки разработчиков отключили логирование всё взлетело и на этом всё закончилось Я могу сделал ремарку разработчиков руки всегда прямые Ну на самом-то деле сложно сказать Вот то что я рассказывал на слайдах после того как мы добились 120 150% это то что нам помогло с деградировать и выдержать 200% какие-то оптимизации да на самом деле всё по чуть-чуть дало то есть вот оптимизация балансировки довольно много нам дали скручивание топов очень сильная деградация которая позволила именно на базовом поиске увеличить количество rps которое мы держим Ну я думаю остальное я э тема для в дискуссии в кулуарах Да очень долго дискуссии Ответь мне пожалуйста на вопрос что для вас Значит 200% это в два раза больше нет это в четыре раза больше то есть от четыре раза больше чем наш изначальный Таргет то есть изначально у нас э каждый дата-центр выдерживал 50% от всего трафика То есть всего мы выдержи х было два Да и их было три то есть ну потому что мы мы всегда готовы к тому что один дата-центр будет потерян в какой-то момент а то есть мы выкати что-то не то дата-центр Придётся закрыть это происходит периодически А если у нас два дата-центра Значит мы должны быть готовы жить на одном дата-центре и выдерживать 100% всё понял Да я согласен с коллегой про руки разработчиков или у нас ложатся дата-центра и у нас последний вопрос Здравствуйте спасибо большое за доклад было очень круто Вот у меня есть вопрос про перфоратор очень интересный инструмент а а он как он всё время экспресс вопрос Да у меня короткий вопрос по архитектуре вот ну меня интересует кэш Мета поиска Я так понял что Мета поиск он сначала ходит в кэш а потом уже ходит в базовый Мета поиск не проще ли было бы занести код Мета поиска в кэш и буквально из кэша ходить в базовый поиск и потом уже оттуда же отвечаете код метапоисковые запросы в базовый да да такая идея была такая идея была когда мы только делали кш но мы поняли что ну это просто сложнее значительно усложнит нам архитектуру и усложнит нам распространение реу кша потому что это сервис который мы сначала внедрили там себе в Мета поиск потом посоветовали его другим коллегам которые тоже смогли его внедрить а если делать если делать запросы из него в базовый поиск то тогда это уже будет только наш сервис который только мы сможем использовать спасибо спасибо за вопросы Кто не успел задать есть зона луара есть чат его можно всегда поймать Сергей не убежит и к тебе последний вопрос в рамках доклада от ведущего Выбери лучше вопрос очень сложные очень сложные вопро А я простые вопросы не задаю наверное самый такой сложный неочевидный вопрос был про качество кто задавал вопрос про качество Выйди пожалуйста на сцену Спасибо за хороший вопрос подарок от наших партнёров от Газпром нефти Ура Спасибо вам большое Очень рад что вы пришли меня послушать вы были отличной аудиторией Я думаю ты с через год повторишь Да конечно что-нибудь Придумано уже что-нибудь другое через год А вы всё-таки сделаете мето поиск над мето поиском мето поиск над мето поиском подумаем над этим Спасибо за доклад Спасибо всем кто пришли Спасибо всем кто смотрит нас в прямом эфире"
}