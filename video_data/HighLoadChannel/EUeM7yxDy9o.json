{
  "video_id": "EUeM7yxDy9o",
  "channel": "HighLoadChannel",
  "title": "Эволюция службы эксплуатации «Spotify» / Лев Попов (Spotify)",
  "views": 364,
  "duration": 2962,
  "published": "2017-07-30T00:47:36-07:00",
  "text": "мы начинаем то еще не успел зайти и заходите слышите меня там в коридоре сегодня будет второй день он мне кажется даже будет понасыщеннее чем был вчерашний и начинаем мы рождения с музыкального гостя из стокгольма из компании спать и фай который вам расскажет о офигительный классный инженерной культуре в компании spotify и как собственно компании spotify делает эксплуатацию поприветствуем лет попов привет меня зовут лев как тут написано я вспотев относительно недавно с 2014 года работаю сервис и наберите инженер соответственно для тех кто не знает что такое спать и поспать ифа это сервис для стриминга музыки где вы можете слушать музыку бесплатно но стекла мая или за деньги без рекламы и с дополнительными всякими плюшками у нас достаточно большой каталог значит поподробнее к spotify у нас мы достаточно большие у нас более 60 миллионов активных пользователей ежемесячно и более 15 миллионов платящих пользователей ежемесячно они приходят потому что контент у нас достаточно богатый помимо того мы доступны на 50 8 рынках россия не один из них пока она надеюсь что какой-то момент мы идем и в россию при этом мы владеем достаточно большим количеством ресурсов у нас есть 4 дата центра 2 в европе и 2 в америке европе в стокгольме и в лондоне в америке в сан-хосе и ваш бур не совершенно разных побережьях у нас более 7000 железных серверов исторически так сложилось что мы фактически не используем виртуализацию почти все наши все наши приложения путиным железных серверах мы используем сервис на ориентированную архитектуру что значит что у нас есть сотни различных маленьких сервисов которые делают прослушивания музыки интересные приятные и помимо того мы еще толкаем очень большое количество трафика в интернет я думаю что 35 гигабит это устаревшее число сейчас это уже побольше но 35 гигабит было примерно равна 25 википедия в час так что это достаточно много помимо того само собой мы работаем 24 на 7 365 музыка не может останавливаться но давайте мы перейдем к разговорам о operations службе эксплуатации собственно того чтобы посмотреть как все это начиналось давайте перенесемся в прошлое и spotify был основан в 2006 году тогда еще devops и микро сервисы и все замечательные слова были не очень модными и как как обычно бывают организациях на момент старта фактически все ответственность за эксплуатацию лежал в одном человеке ну потом через какое то время количество инженеров эксплуатации росло и мы пришли вот такой схеме который ничего удивительного нет соответственно каждый сервис который мы владеем имел своего собственники разработки и собственника эксплуатации и попса умер он занимался всеми эксплуатационными задачами которые связаны с конкретным сервисом по именно того как правило об сауны одновременно владел несколькими сервисами не обязательно двумя некоторые владели пятью некоторые десятью и кроме того все инженер эксплуатации находились в команде эксплуатации которая отвечала за эксплутационные расходы в целом такие как быть на звонке в случае инцидентов построение систем мониторинга построение бил систему и так далее ну соответственно 2011 году мы уже были достаточно большими некоторые цифры из 2011 года и все вся эксплуатационный ответственность лежала на маленькой группе в пять человек сейчас время прошло и мы выросли продолжаем расти и я думаю для вас не будет странным сколько инженеров у нас сейчас у нас сейчас нет никакой команды в этом зале не она все знакомые devops и operations поэтому они не выглядят как этот чувак словах но перед тем как я продолжу рассказывать о том как мы пришли к но ups давайте поговорим немножко о инженерная культуре с поди фай в целом значит поскольку мы постоянно растем и развиваемся нам мы следуем некоторым принципам для того чтобы масштабировать наш сервис значит во первых у нас сервис-ориентированной архитектура для каждой для каждого функциональностью у нас есть маленький сервис эти сервисы взаимодействуют друг с другом через строго определенные пей и каждый сервис обслуживается разрабатывается и выкатывается отдельно помимо того мы следуем unix боев что значит что мы пытаемся сделать наш код модульным расширяемым и этот код должен быть легко этот код любой разработчик не обязательно его создатель должен мочь поддерживать или использовать в целях не связаны с прямой задачи разработка этого кода помимо этого мы еще следуем принципу kiss значит кипит вступит соответственно согласно этому принципу любая сложность которая не необходимо в продукте должно быть мы должны избегать и простота всегда должна быть ключом во всех архитектурных решениях мы пытаемся все делать просто и это приводит нас достаточно сложной схеме в целом как обычно и бывает с микро сервисами это цена на этой схеме в общем показано как работает спать и фай на самом деле она уже сейчас не репрезентативна но идею можно увидеть у нас есть огромное количество сервисов и для того чтобы поддерживать любой из них не обязательно знать как вся система весь по тихой работой в целом надо еще знать свой сервис и соседей с которым он общается большинство из таких сервисов автономные не зависят друг от друга мы пытаемся это сделать одной из основ архитектуры каждый каждая команда делает свой сервис так чтобы он работал сам себе даже в случаях апокалипсиса и количество этих сервисов постоянно растет сейчас у нас их несколько сотен не могу сказать точное число но давайте посмотрим кто же обслуживает и создает эти сервисы для того чтобы поддерживать такое большое количество сервисов нам нужно также каким-то образом масштабировать нашу организацию базовым юнитам разработки спать и появляется сущность так называемый сквот что означает в переводе с английского отряд это что-то вроде скрам команды это базовые нет разработки в spotify он за дизайнер так чтобы чувствовать себя как внутри стартапа то есть все члены команды сидят вместе они делают какой-то продукт у них есть все необходимые навыки инструменты для того чтобы создать этот продукт и соответственно они могут сами разработать протестировать и зарелизить вы хотите production помимо того воды они имеют автономность они самоорганизующиеся то есть каждый слот может выбирать сам свой путь работы некоторые из воды используют скрам техники не катере комбо мне которые может быть кому-то это нравится использует waterfall кто-то придумывает свои пути работы также они сами выбирают инструменты которые они пользуются у нас есть некоторые рекомендации best practices но если вы хотите использовать свои разработки какой то какой то экзотический инструмент и вам это действительно нужно почему бы нет этой иллюстрации показано что каждая функция в клиенте соответственно клиенте и функция которая ответственна за эту функцию клиенте в бэг-энде как правило поддерживается одним и одним входом на команды поскольку у нас skoda достаточно много и вообще работа с большим количеством команд всегда сложность кроме того у нас есть офисы в сша и в европе мы работаем разных часовых поясах это тоже добавляет комплекте работе для того чтобы с этим справляться у нас есть матричная структура значит выглядит это все скромно матричная структура состоит из следующих базовых сущности во первых сквоты которым я уже упомянул это базовый у нет разработки люди которые поскольку она создают фичи каждый сквот привязан входа кто умер у который является чем-то вроде предпринимателям он задает сходу направления в какой области они должны двигаться учитывая одновременно и технические аспекты деятельности и требование бизнеса помимо того каждый инженер каждый individual can компьютер состоит в сущности которая называется часто часто это группа людей со схожим набором умений и chapter лид который стоит во главе chapter а заботиться о том чтобы каждому из его чартера нравилось работать ему нравилось то чем он занимается он занимается карьерным ростом также зарплатами отпусками и всеми организационными задачами таким образом каждый каждый человек стоит 2 сущностях в складе chapter i в сумме это представляет собой матрицу поэтому это называется матричная структура что в ней хорошего что тебя есть как бы два так называемых босса и человек который ответственен за то чем ты занимаешься человек этот ответственна за твой карьерный рост это два разных человека нельзя сказать что это какая-то серебряная пуля это подойдет в любую организация но у нас этой работы достаточно хорошо помимо того в нашем от личности у тебя существует такое понятие как гильдия гильдия это неформальная организация в которую входят люди со схожими интересами и задачами которые хотят общаться и шарить свои знания тузы какой-то код собираться на тематические митинги например у нас есть гильдия quality assurance гильдии питон гильдия linux вы можете создать свою гильдию если хотите соответственно организационная составляющая состоящая на шаг текстурная оба и направлен на то чтобы масштабировать как продукт так и организацию быстро в условиях растущего бизнеса меняющихся требований но давайте вернемся к перешить можно ли смасштабировать данную схему ну во-первых их сколько operations инженеров нужно для того чтобы служить 100 200 300 тысяч и сервисов насколько сложно их найти сколько всего они должны знать ну мне кажется что эта схема не очень хорошо масштабируется ну даже если мы из масштабируем это будет работать не очень эффективно поэтому мы долго думали пришли к инициативе который называется absence квот operations in скотт значит почему мы к этому пришли во первых очень сложен и масштабировать централизованной у команду эксплуатации потому что во первых инженеров испугаться не так-то просто найти во вторых если мы хотим чтобы каждый инженер был ответственен за куст так сервис то он должен разбираться во всех технологиях которые лежат в этом сервисе начинают аппаратной платформе заканчиваю уровня приложения и не так просто найти людей которые одинаково хороши во всех уровнях гораздо проще найти узконаправленных специалистов помимо того мы верим что эксплуатация должна сидеть рядом с разработкой для того чтобы делать продукт более качественным уменьшать цикл или за и так далее вы все прекрасно знаете зачем это нужно помимо того мы хотели чтобы наши печати мы сейчас кг воды были автономны чтобы у них не было лишних dependence of чтобы они не не блокировались на выходки и помимо того мы хотели чтобы они были полностью ответственны за свой продукт от начала и до конца потому что никто лучше его создателей лучше создателей продукта не знают как его обслуживать соответственно здесь приведена схема эволюции нашего подразделения эксплуатации она не очень она не показывает отношение людей в этих командах само собой эта бочка была гораздо больше чем у perrie шанс но показывает тенденцию то есть в 2013 году мы соединили отделы сработки инфраструктуры либо когда я делал сетки для белить инженером как бы говоря системных администраторов некий один большой отдел который называется инфо страх чопперы и шанс эксплуатации инфраструктуры и примерно в это же время мы запустили центе вообще абсент вот так перейдем к тому что же такое как 4 шансы на абсент вот таким образом мы создали новый трек если вы помните из диаграммы трайб это глупо сводов которые работают над похожими функциями например над платформой для мобильного клиента и поставьте первично представляет из себя новый трайб который состоит из кодов каждый из который имеет какую-то продуктовую задачу продуктовую миссию то есть не просто поддерживать тот или иной сервис например создавать инструменты которые позволят фьючерс к водам создал быстро разворачивать повторяем ую конфигурацию или создать сетевую инфраструктуру которая позволит разработчикам не заботится топологии ну и так далее каждый каждый каждый из входов кнопки ряд рыба имеет некую миссию некий продукт который он поставляет своим пользователям внутри компания как правило собственно что поставляет ее трайб она поставляет некую платформу в целом поставляют инструменты и утилиты для того чтобы управлять этой платформой и делать задачи связанные с созданием и обслуживанием сервисов и помимо того рот ипс предоставляют документы и best practices для разработчиков о том как лучше пользоваться этой платформой таким образом у нас больше нет operations вместо перри шанс у нас теперь разработчики которые работают над какими-то продуктами и ответственность эксплуатация размазана на всю технологическую организацию что это caps and hats absence под это не значит что мы посадили перемещаться инженеров каждый ход значит что сквот команды разработчиков сама ответственна за все функции эксплуатации но при этом они всегда могут обратиться к специалистам зовет рэп за помощью за документацией за инструментами но одновременно они уже имеют доступ к платформе к инструментам best practices и могут самостоятельно найти путь как быстро и качественно выполнить те или иные эксплутационные обязанности мы ожидаем от собственных вот очень многих вещей мы хотим чтобы они сами делали свои сервисе разработчики сами делали свои сервисы высоко доступными сами планировали ресурсы которые потребуются в будущем сами настраивали мет какие самые строева лео яхты и само собой это не всегда просто дается но с каждым годом качество сервиса которые сами сетки доставляют разработчики растет ну и казалось бы все замечательно теперь стены вы бывший системный администратор вам теперь можно спать по ночам вести нормальную жизнь не нужно просыпаться на легке пускай разработчики сами просыпаются на свое лифты но это не всегда так вот если если подумать иногда случаются непредвиденные страшные вещи например пожар дата центра или реальная ситуация в прошлом году кабель между великобританией и сша перекусили акулы вот если такие вещи происходят кто же должен разбираться в случившемся хаосе что весь сервис недоступен все сломалось кто должен это обрабатывать помимо того чтобы быть частью организации с с продуктовыми миссиями соответственно часто разработчики часто сайт когда белить инженеры отвлекались на различные задачи которые требуют немедленной реакции поэтому видя этот паттерн мы сделали организация которая называется в называется core i3 core i3 состоит из инженеров которые помимо того состоят в не моя картинка состоят в складах с продуктовыми 7 и то есть они большую часть своего времени работают над какими-то продуктами сервисами но помимо того они периодически отвлекаются на различные срочные operations задание это стандартный карьерный рост любого из и реорганизации как только вы достигаете определенного уровня вы можете принять участие в квартире когда обладаете необходимыми скиллами чем занимаетесь кого серии мы обрабатываем основные важны инциденты такие как без против ой упал люди не могут залогиниться или музыка не играют мы помогаем с работником с проблемой масштабируемости помогаем им дизайне их систему более качественно и учим учим без как исчез в общем ну например случае если происходит какой-то серьезный инцидент например музыка перестает играть то один из представителей когда серии которые в данный момент находятся на дежурстве должен выяснить попер получить олег как а то можешь получить его к систему мониторинга так и от службы поддержки потом найти узкое место в системе что сломалось связаться с разработчиками которые владеют этими компонентами системы скоординировать восстановление и потом скоординировать посмотрим и принятие remedy и шанс бийске которые нужно выполнить после инцидента значит приведу несколько примеров о том как работает наша платформа в этой в целом например пару недель назад и месяц назад мы запускали спать и фай на playstation и ну для того чтобы запустить по тихой на playstation назначить дату выкатки сделали там центр управления полетами собрали представителей от каждой команды которая была вовлечена в создании продукта для playstation и стали следить за тем как пользователи приходят и количество пользователей которые пришло к нам неожиданно было сильно больше чем количество пользователей которые мы ожидали какой-то момент один из одна из команд ответственная за важный ключевой сервис выяснила что если мы не предпримем действие через какое-то время в одном в одном из это центров возникнет battle.net соответственно они пошли в в одной части нашей инфраструктуры система автоматического парижанин га и web-интерфейсе сказали что они хотят новости аппаратных серверов в их систему сказали тип сервера сказали что они хотят максимум два сервера настойку нажали кнопку и через полчаса у них было 100 дополнительных аппаратных серверов которые были автоматически установлены нашими нашей платформой сконфигурированный по пятам зарегистрированы в сервис discovery и стали в итоге частью компоненты и бутоны блузка нём то есть на базе таких подобных инструментов мы делаем наш сервис высоко доступным и перенося ответственны за эксплуатация разработчиков мы делаем мы делаем процессы происходящие в атаке сервиса более видимые потому что разработчики сами знают что мониторить и какие сервисы нужно делать так теперь я хотел бы немножко поговорить по поводу инцидента менеджмента в общем с по тифа и мы следуем такой культуре что в принципе ошибки это нормально если вы допустили ошибку и сервис свалился это не очень большая проблема ваза это не уволят вас за это не оштрафует до тех пор пока одна и та же ошибка не повторяется дважды повторять одну и ту же ошибку дважды плохо поэтому для того чтобы избегать повторения ошибок нас есть строгая процедур инцидент менеджментом соответственно в случае если происходит инцидент то если это важный инцидента его обрабатывает инцидент моно джеко который является частью организации карцере в случае если это инструмент не связаны с главным с главными функциональности его обрабатывает каско бочки самостоятельным у них есть свои ротации свои подверните свои рты свои графики это весна после каждого инцидента мы должны написать посмотрим посмотрим пишет со всеми людьми которые были вовлечены в инцидент владельцами сервиса заметная жермон call если нужно привлекаются stakeholder что кто там и паспорте мы должны быть зафиксированы все детали этого инцидента по итогам постам автома должны быть должен быть составлен план действий людей шанс которая должна быть предприняты для того чтобы предотвратить подобные инциденты в будущем каждый инцидент серьезный инцидент или несерьезные цикл проходит через эту процедуру потому что делать одну и ту же ошибку дважды глупов при этом сервисы они имеют различные требования к доступности например playback музыки он должен быть всегда доступен логин должен быть всегда доступен какая-нибудь синхронизация плейлистов может быть какое-то время недоступна потому что все клиента есть оффлайновой версии плейлистов и так далее и исходя из требований к работоспособностью инциденты могут быть обслужены и мгновенно ночью если это важно ницы или может быть отложено до следующего дня это все ответственность за то как менты лежит разработчиков они знают насколько важен их сервис они знают насколько он стабилен как может ли он выдержать какие-то неполадки так далее ответственно перед разработчиками мы ставим следующие условия если вы хотите просыпаться по ночам вы можете сделать ваш сервис михаэль и вы либо можете установить его на одну машину в один центр но тогда вас будет посреди ночи пейджер duty и вы будете сидеть и восстанавливать свой сервис либо вы можете сделать его халявы или было зарезервировать в разных центрах следовать нашему указаниям и best practices похолодало белики и тогда вам не придется просыпаться по ночам вы сможете чинить его в рабочее время как правильно писать посмотрим ну любой кто с мартом и знает этим костре правилам во-первых обязательно нужно после инцидента запланировать постановкам создать расписание event пригласить туда людей желательно чтобы этот ивент был вскоре после времени самого инцидента потому что память улетучивается вы забываете детали и в итоге вы можете пойти к тому что посмотрим пришел о том что случилось уже никто не помнит обязательно нужно зафиксировать все детали акта с которым случился инцидент зафиксировать какой версии был релиз какие действия были предприняты перед тем как инцидент случился какие действия были предприняты для того чтобы инцидент устранить кроме того решение инцидента должны участвовать все заинтересованные стороны и результат обязательно должен быть записан недостаточно просто придти поболтать и разойтись надо записать результат исходя из этого результата создать action plan определить действия которые нужно выполнить чтобы это не повторялось кроме того spotify и вообще в прогрессивных айти компаниях люди придерживаются так называем онлайн полисе это значит что на инцидент manger не на договор на инцидент митин инцидент просмотр митингах не нужно говорить это вася сломал нам сервис ты его вина это строго запрещено если что-то сломалось это не чья-то вина это вина организации процесса в целом и никогда blooming никогда не приводит никаким продуктивным результатом поэтому старайтесь держаться если вы всегда делаете посмотрим и стараетесь держаться подальше от обвинений конкретных личностей и работать как команда для того чтобы результат вашего митинга был продуктивный и полезный вот соответственно так после этого инцидента посмотрим action plan нужно зафиксировать создать тикет дыре или системе обороты документов которые вы используете для того чтобы в будущем можно было найти его детали и проанализировать помимо того сколько мы перенесли большую часть ответственности за он пал на разработчиков как организация не получает теперь так много инцидентов и как правило инциденты не идут по долгу частью и поскольку мы имеем также среди в команду в нью йорке мы решили буквально недавно реализовать он call который on-call дежурство расписанию которая следует за солнцем так сказать перед этим у нас была система что каждый каждый из представителей коры сирии по расписанию дежурить целую неделю включая день и ночь случается что-то падает его будет посреди ночи он занимается координацией решения этого инцидента на сейчас мы перенесли эту ответственность на 2 офисом и течение дня дежурит сайт или обелить инженера дежурит только в течение дня то есть мы нашли оптимальное расписание в час дня по нью-йорку всем часов по штатам и всем часов по что там сейчас дня частного типа в общем вот по этому времени мы меняем вижу гсту и передаем всю информацию о текущем состоянии текущих инцидентах в нашему напарнику которая сидит на другой стороне океана соответственно теперь можно ночью спокойно спать отдыхать и не не тратить свое личное время на решение проблем ну помимо того он коли у нас участвуют кодек турниры сервис оба любили this хватов и лид сервиса вы любили эти организации так это все на данный момент прецеденты то что то что мы сейчас имеем она не не решают все возможные проблемы до сих пор есть огромное огромное пространство для улучшения значит во первых то что мы ожидаем от всходов от команд к разработке не всегда ясно то есть вот там на предыдущем слайде было огромное облако тегов и она нет ничего не зафиксировано четко у нас нет четкого регламента того что должны делать разработчики и иногда это приводит к тому что некоторые склады относится халатно к эксплуатации мы пытаемся сделать четкий игла менты четкий аудит план для того чтобы сделать некую внутренне сертификацию наших команд разработки в том что они все делают правильно и таким образом полуют повысить качество в целом помимо того коммуникации между командами разработки и командами которые занимаются им к структурами не всегда хороши часто команды разработки задают вопросы которые не рассмотренные не отвечены в текущей документации командным к структуру или которая не не были приняты во внимание при проектировании платформы ну мы тоже пытаемся устранить эти пробелы в коммуникации за счет одновременно организационных средств и технических средств помимо того с развитием вас как 4 шанс и оптинского цены инициативы наша база знаний пополняется и все больше и больше больше и больше вопросов покрыта документации кроме того очень сложно оценить насколько инициатива усинска вас успешно в целом потому что я сложно измерить наши люди пытаются ввести какие-то метрики основанным может быть на down таймер сервис или на количестве заявок которые приходили в поддержку in phase 3 4 женским отпуска бочеков но пока это все очень расплывчато есть мы не можем сказать мы точно движемся в правильном направлении и не мы не можем точно определить что нам нужно поменять нашим процесс или того чтобы сделать все еще лучше кроме того организация спать и файл очень живое и склады команды разработки они периодически меняют свои миссии исчезают создаются новые skoda и какие-то сервисы остаются без хозяев и случаются ситуации когда сервис отказывает он выполняет какую-то важную функцию никто не знаю что с ним делать помимо того когда 11 вот исчезает мы хотим чтобы он передавал все сервисы которые владеют какому-то другому ск воду и так чтобы эти сервисы вписывались его миссию поскольку у нас нету четкого регламента о передаче прав на сервис иногда это происходит процедура происходит халатно без сопровождаю сопровождающей документации и в итоге несмотря на то что формально у сервис есть владелец владелец не очень то не очень хорошо понимаю что с ним делать это мы тоже пытаемся решить с помощью введения регламентов на передачу прав введения введения улучшения наших информационных систем которые в которых мы так им сервисы сигналом и так далее всем большое вам спасибо вопросы спасибо за доклад у меня такой вопрос вы сказали что нельзя повторять эту ошибку дважды до что тогда бывает если кто-то и повторит и что считается повтором если один тоже человек повторила шип или кто-то другой и если кто то другой то если человек новые не в курсе всех инцидентов что с не будет смотрите тут речь идет не только о том чтобы зафиксировать документально что вот была ошибка и эта ошибка все сломала том чтобы автоматизировать процесс так чтобы от ошибку было невозможно повторить то есть сделать модификацию вашей системе в вашем процессе вашим чек-листе коты делаете там при дипломе эти сервисы и обслуживание так чтобы следуя этому процессу или используя эти утилиты эта ошибка не повторялось монет а если она повторится ну это плохо значит вы действие которые отменили в прошлый раз неудовлетворительные нужно обдумать это придумать новое действие да ну просто повторить все сначала или все-таки до тех пор пока вы полностью не исключить возможность повторения этой ошибки в конце процесса это интеграция будет повторяться но они при этом все равно мы следуем нам двоим поясе то есть даже если один и тот же человек сломал одно и то же дважды это значит что просто ваша команда плохо поработала над устранением причин инцидента спасибо за доклад слушаю расскажи пожалуйста подробнее про тувы в морду через которую фичи тим заказывает себе серверы каких технологиях и не был сделан и и самое главное какая из команд а это дело значит у нас не кирилин вас как 46 так называемая продукт арья skoda объединенные в партере я нахожусь в одном из кодов сервис оба любили типа адаптере и вы имеем то же скотт сервис его любили тепло виктория в нью-йорке они занимаются платформой нижестоящими слоями различными платформа пробежал нас сама стоять самодельная она состоит из большого количества разных инструментов и скриптов но она развилась итерационно то есть качестве базовой технологиям используемой пикси мы используем только четыре модели серверов которые разработчик может получить соответственно у нас нет никакого хаоса с различными версиями и драка и пима и так далее соответственно мы используем ubuntu у нас есть свой пик синиц стайлер для ubuntu и есть просто сервер и вот морда и сектор который выполняет задачи попадающиеся в очень с этим перед тем как у нас появился сервис для моментального управления серверами часть из этого сервиса обслуживала крови же ним через же жива уже надо было создать провяжем ticket хотим указать свойства машины и раз какой интервал он выгребал все эти киты и выполнялось но тогда ожидание было там где-то полдня ты создал текке через полдня провяжем произошел сейчас это занимает полчаса и почесать занимают основном потому что первые по петром достаточно долго проходит спасибо спасибо за доклад у меня такой вопрос а чтоб в данном случае подразумевается под инцидентом то есть это не знаю отказ сервиса или там какой-то там начала течь память но все как-то плавает то есть что в данном случае лет ну во первых у нас есть а легче если сработала лифт это уже инцидент во вторых инцидента тогда не доступны к эта функциональность это может мы можем это знать из-за лифта или мы можем это узнать из reporto от другого пользователя внутри компании сайт касается внутреннего сервиса или от например службы поддержки позвонил клиент и говорит вот в лос-анджелесе не играет музыка мы начинаем создается инцидент ticket через службу поддержки это спецагент тикет управляют информацию в пейджер youtube ждите эту информацию с локкой на но дежурного инженера дежурный инженер начинает выяснить в чем проблема находит узкое место место связывается с командами которые владеют сервисом чтобы они помогли ему разобраться ну и по процедуре понятно и еще один небольшой вопрос вот у вас shift и получается но у смены по 12 часовом по 12 часов что ли или как просто подобрали так время чтобы более менее они на день попадали то есть имею ввиду вот скажем каждый инженером работать скажем два дня в неделю там по 12 а можно мне пожалуйста сливать назад или каждый день и по 12 все нельзя ну ладно каждый инженер работает 8-часовой рабочий день но помимо восьмичасового рабочего дня капитан говорит в офисе еще есть какой-то какой-то промежуток и мне когда он ответственен за инцидент сделали так чтобы это не приходилось на ночь потому что плохо когда люди работают по ночам они становятся менее активными они сбываются режимы много проблем как бы ну в офисе 8 часов отработал потом просто supported там как бы на телефоне сидит на попе джерри пока пока его смена у нас до сих пор недельные shift и то есть просто и два человека каждую неделю ответственны за инцидент magenta есть один человек яркий один стокгольме спасибо здравствуйте ну вот у вас очень большой парк машин вы можете рассказать подробнее о configuration менеджменте о continental евреи как вот суета организована я могу рассказать но мне кажется что это займет много времени у нас осталось с половиной минуты поэтому давайте вы потом подойдете и мы это обсудим лев спасибо за доклад вопрос на самом деле по инструментарию вы говорили что в ваших складах дается свобода выбора данную все так наверное есть какие то централизованные инструменты poobag трекингу вы упоминали gerl что вы для коммуникации в команде используете ну с вашим у нас есть best practices то что то что мы рекомендуем использовать для чего у нас есть наработанные всегда если вы хотите использовать экзотическую штука вам придется и делать с нуля и правило это неэффективно никто этим не занимается у нас есть m-city большой у нас есть дженкинс и дженкинса как сервис если вы делаете свой сервис вы ставите себе свой собственный дженкинс и дженкинс уже собран сам по 5 пакеты с десаль плагином то есть вы следуем она ловко свой репозитории добавляете dsl скрипт который описывает процесс затем через клик веб-интерфейс вы добавляете свой так дженкинс и весь pipeline вперед с этого до сельским то мы используем жир для тикетов мы используем папе для configuration менеджмента помимо того основной язык либо консервы сам сейчас это java и у нас есть свой фрейм бог который мы рекомендуем использовать разработчикам для создания сервисов поверх него и этот прибор реализует большую часть технологии которые мы рекомендуем использовать компания сервис discovery хорошенькое боится сбил pipe лайнами и его можно завернуть в контейнер там подобные вещи спасибо есть вопрос такой вас много вкладов кто отвечать за то чтобы все это вместе взлетела и кто дает задачи для core кто их ставить значит вот все разработчики непосредственно individual can компьютер находится в этой матричной структуре но помимо того есть мы нажмем который находится в иерархической структуре поверх матрицы этот мы начнем то никак бы не прямые руководителя но не работают над общей стратегии глобальный соответственно на наверху вырабатывается стратегия какие-то приоритеты потом через product owner of эти приоритеты доносится до команд после этого но команда не сидят само собой не в изолированных комнатах они друг с другом общаются ну например вот у нас был запуск playstation над этим работала там пять или шесть ввода в различных ну они постоянно собирались вместе работали над ними темами у них были после работы разные неформальные activity ли того чтобы наладить коммуникацию между ними по требованию когда возникает какой то продукт который требует большое количество команда команды собираются и делаю все обход для core кто дает задача для кого для корм для крыс рим вообще коры сирию сами по себе не должны ничего делать ну как конизация потому что корица и они помимо того состоят внутри из квода вк с продуктовой миссией а не нужно заниматься задачами которые которые их продуктов из под им поставляют красив и нужен только в случае чего то что требует немедленной реакции отвлекать людей от их основных обязанностей и они для них это более высокий приоритет на при этом у них есть основные задачи сходах а как тогда инфраструктура кем поддерживается единое общая глобальная ни гор нет смотрите в внутри трайб есть большое количество сходов и каждый ответственна за какую-то часть инфраструктуры точно также как разработки каждый сквот владеет какой-то часть инфраструктуры в случае если вся инфраструктура глобальная падает косарей они тоже размазанный по утрам они находятся на понятно есть отдельный склад который как раз третью и такой еще вопрос как вы добиваетесь того что команда разработчиков не делают абсолютно глупости из-за того что они не имеют опыта эксплуатации например полностью отказаться например полностью отказаться от мониторинга резервное копирование вообще на него как бы забить и и так далее значит мы пытаемся воспитать в людях эту культуру то есть вообще у нас у нас случаются такие проблемы что команда не придерживаются best practices и делают какие-то глупости ну во-первых пытаемся делать 0 sharing то есть мы устраиваем регулярно внутри офиса внутренние конференции где мы рассказываем о новых продуктах которые мы сделали как ими можно пользоваться помимо того мы ведем внутренние блоки то есть например вы сделали какой-то классный сервис и написали блок пост об этом ну я посылаем почту доносим это через лидеров помимо того мы делаем мы практикуем такую вещь как embedding то есть если у фьючерсах воды есть какая-то сложность у них есть кое-то задача и они не знаю как ее решить мы можем отдать одного из инженеров из трайб на несколько недель внутри этого склада он работал с ними интересными задачами но я так понимаю ваш вопрос если вот там сидят дураки они не знают ничего как делать как-то заставить это делать не хотят в мы пытаемся построить культуру так чтобы все хотели это вопрос доверия мы предоставляем всем всем кутовой доступны сектора которые они владеют и так далее мы пытаемся вкладывать как можно меньше ограничений и прививать в людях с желания сделать хорошо это вопросы больше культур и последний вопрос просто для понимания сколько инцидентов месяц всего и сразу 2 а как вы заливаете инцидента между продуктовыми командами если например проблема есть а где а команда начнут перекидывать между собой это у них нет этого них всё спасибо а мы привлечем обе команды и пускай они оба решают если вот не очевидно допустим у кого проблема вы смотрите я не могу сказать вам точно числа континент не помню просто ориентировочно это там газ в неделю или да и с этим magic magic инциденты ну в целом просто любопытно целого начинается следовать по моей дробь инцидентом их происходит знаю может быть раз в неделю что-то случается что требовать внимания к серии но это зависит бываешь в месяц ничего не происходит потому что то случается но как как правило они не в не очень важно не не влияют на действительно функциональный сервис а в целом я не могу вам четкие цифры сказать я просто не помню по поводу перекидывания от опять же вопросы культуры то есть когда вы решаете инцидент вы привлекаете сразу например у вас есть два сервиса и вы не понимаете какой из них сломался это произошло ночью в обоих сервисах есть человек сквот который вел нет их и каждом исходе swing of rotation и в каждом из кодов и свой дежурные вы вызываете обоих дежурных из этих склонов вместе с ними сидите и пытаетесь прошить инцидент как координатор и помогаете им как говорится в их данных они поскольку поскольку так заведено они не могут просто сказать и детей черт я пошел спать то есть они область сидят пока се не починят ну да потому что одна задача в том чтобы не то чтобы людей заставить это делать в том чтобы не понимали свою ответственность за это спасибо большое"
}