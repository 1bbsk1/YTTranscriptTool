{
  "video_id": "2HX_MF5Ic7o",
  "channel": "HighLoadChannel",
  "title": "Балансировка нагрузки в мульти-эксабайтном сторадже / Вадим Зотеев (Яндекс)",
  "views": 590,
  "duration": 3068,
  "published": "2023-10-06T07:19:07-07:00",
  "text": "Всем привет Я очень рад вас всех здесь сегодня видеть меня зовут Вадим Я последние три года занимаюсь развитием обжиг storage Яндекса и сегодня расскажу про него обжиг storage Яндекса если коротко это мультизабайтное хранилище которое обрабатывает миллион запросов в секунду и раздает тробит трафика в нем хранятся и раздаются данные практически всех продуктовых сервисов компании как правило это не структурированные данные различные фотографии видео аудиозаписи различные документы Например почта хранит у нас переписку пользователей диск хранит непосредственно данные которые заливает пользователи в КиноПоиске хранятся видеозаписи Когда вы слушаете какую-то музыку с яндекс.музыки либо смотрите фильм На самом деле данные качаются с нас и через специальные кэширующий слой раздается наружу под капотом сторож Яндекса представляет из себя сотни тысяч жестких дисков распределенных по 1000 серверам нескольких дата-центрах по моему кликер сломался Ладно я пока продолжим о все отлично заработало внутри для хранения данных используется как различные схемы репликации с разной коэффициентом репликации так и более сложные схемы с использованием рейндже кодинга вообще говоря построение такого хранилища Это нетривиальная задача которую в Яндексе занимается целая служба разработки и балансировка нагрузки является одной из актуальных под задач которые возникают собственно об этом сегодня мы поговорим начнем конечно с постановки задачи Допустим что мы разрабатываем некоторые распределенное отказывая устойчивое хранилище данных с очень простым API у нас по факту есть всего две операции есть операция записи данных по некоторому ключу и эти данные мы можем прочитать позднее по этому же ключу целимся в то что мы будем строить очень большое хранилище Поэтому будем скорее всего использовать HD диски и для них приемлемая латанцы на высоких перспектилях скажем 99 будет не больше нескольких сотен миллисекунд то что с одной стороны оказывается приемлемым для большинства приложений с другой стороны Вполне себе реализуема на хэ-дисках но при этом всё то о чём я буду говорить точно также можете остаться на другие типы дисков Наша задача заключается в том чтобы обрабатывать все чтобы оптимальным образом балансировать все запросы на запись и на чтение оптимальным значит то что никакие запросы мы не должны терять и укладываться в требуемое ограничение по латансе забегая вперед можно сказать то что в хранилище это будет значить что никакие диски и сеть не должны системно перегружаться потому что именно диски и сети являются дефицитами ресурсами В подобных подобном хранилище все это должно работать при различных моделях отказа начиная от диска поломки диска продолжая поломка сервера и заканчивая выходом из строя ТЦ или даже нескольких Вот и последнее но не по значению чем по возможности система должна получиться дешевле практически любое хранилище подобного типа можно представить в виде двух уровней первый уровень это уровень доступа который отвечает на запросы пользователям и предоставляет то самое API в нашем случае для чтения и записи ключей когда запрос приходит на одну из по всей в уровне доступа он проксирует запрос в уровень хранения данных целимся в большое количество данных поэтому конечно же будет использоваться шарнирование и Для обеспечения отказа устойчивости будем использовать схема репликации далее я буду говорить только про схему обычной репликации с клонированием данных но все на самом деле применимо и более сложным схемам с использованием кодирования собственно Задача в том когда запрос пришел на проксю Каким образом выбрать шарф который будет произведена запись и каким образом затем прочитать эти данные то каким образом распределяем шорты по железу это вообще говоря тоже постановка задачи Ну уровень доступа уровень хранения данных может выглядеть примерно так как изображен на этом фото это реальное фото из центра в Яндексе тут три стойки каждой стойке всего по четыре сервера и на каждый сервер идет более чем 100 дисков такая конфигурация Скорее всего будет использоваться в общих стороже который как правило используется для хранения каких-то холодных данных в современные сервера достаточно большой сетью в десятки Вполне себе работает такая конфигурация шарды мы каким-то образом распределяем по дискам дисков сервере много Например в стороже Яндекса мы используем достаточно большие шарды в 1 ТБ они ограничивают сверху размер ключа который мы можем записать собственно перейдем к решению задачи сначала поговорим про запись абстрагируясь от того каким именно образом распределяем шорты по железу поговорим о том как распределять данные по самим шардам здесь наверняка могут Многие могут вспомнить такой метод распределения данных хеширования в самом простом случае берем Хеш от ключа от него берем остаток на отделение на количество шаров и получаем тот шарт который мы записываем данные этого ключа при использовании достаточно равномерной функции хеширования данные будут распределяться плюс-минус равномерно для проблемы ришардинга есть алгоритмы консистентного хеширования которые позволяют не перевозить половину данных при изменении количества шардов Ну Казалось бы бери и пользуйся О чём вообще здесь игроки могут быть проблемы Ну давайте посмотрим может произойти перегрузка диска напоминаю то что на каждом диске у нас располагается Некоторое количество шардов хранилища может быть мультитаносе то есть хранить данные множество различных сервисов поэтому рядом попадают данные различные шарды с данными от разных сервисов если один из шардов испытывает достаточно большую нагрузку Ну например Здесь там популярный контент и его читают достаточно часто тогда диск будет уже достаточно сильно утилизирован дополнительная запись которая идёт на остальные шорты будет ещё сильнее повышать нагрузку на диск и он может упираться в полку таким образом часть вопросов мы будем терять м-м Ну собственно не то чего мы хотим то же самое может произойти при каком-то фоновом процессе в большом хранилище всегда есть большое количество фоновых процессов это может быть восстановление данных при потере какой-то реплики при отказе железа это может быть переезд данных или дефрагментация как чаще всего бывает в нашем случае когда данные ранее помечены удаленными окончательно удаляются с диском в этом случае результат будет точно такой же как предыдущем примере фоновый Процесс уже нагружает диск и соответственно дополнительная запись деградирует аналогичная ситуация может происходить и сетью когда сеть сервера уже утилизируются на достаточно большой процесс начинает упираться в полку и в результате мы теряем часть запросов что не айс каким-то образом нужно переживать поломки в железе Hardware проблемы например допустим у нас используется репликация X3 данные каким-то образом распределены по железу для того чтобы обеспечить отказа устойчивость Допустим мы используем форумную запись здесь это не принципиально если ломается одна реплика Все хорошо В этот шарт мы можем продолжать писать если вы хоть это из строя двери Африки то как бы конец потому что конкретно в этот шаг данные писать уже нельзя напоминает то что при детерминированном алгоритме хеширования часть ключей обязательно будет попадать в этот шаг поэтому мы теряем запись ну и наконец может произойти неравномерное распределение свободного места по различным шардам допустим сами ключи даже если распределяются равномерно они будут несколько отличаться в размерах соответственно какие-то шарды будут заполняться быстрее чем остальные для того чтобы система работала необходимо держать некоторые объем свободного места избыточного в системе для того чтобы никакой шарт не заполнялся полностью и в него можно было продолжать писать иногда этот объем дополнительного места может быть существенно составлять там десятки процентов и больше что конечно же отрицательно сказывается на финальной стоимости системы все эти системы все эти проблемы на самом деле можно выразить одной фразой то что у нас нет явного контроля надписью алгоритмы распределения ключей по шардам детерминированной и Когда возникает какая-то проблема с тем или иным шардом мы ничего с этим сделать не можем Ну если у нас нет контроля что нам нужен нам нужна власть Нужен контроль Клик Допустим мы будем применять не детерминированные алгоритм в каждый момент когда запрос приходит на еще не знаем заранее В какой именно шарт мы его запишем и учитывая все эти проблемы о которых я говорил раньше наличие фонового процесса утилизации диска утилизации сети записываем данные именно туда куда они могут быть записаны в данный момент это можно реализовать если построить у каждого шарда почитать для каждого шарда некоторый вес и выбирать шарф по алгоритму взвешенного рандома чем больше вес у шарда тем больше данных мы будем записывать в этот шар Чем меньше вес у шарда тем меньше данных собственно веса мы можем динамически менять в зависимости от текущее текущего состояния железа ну как считать веса это на самом деле самый главный вопрос работает метод когда они считаются с помощью произведения некоторых коэффициентов мы можем вести коэффициенты Для различных факторов например для утилизации дисков в этом дополнительный коэффициент когда утилизация соответствующих дисков Где живут шарды становится достаточно большая мы уменьшаем пропорционально обратно пропорционально вес на этот шар то же самое мы можем сделать для утилизации сети таким образом решая проблемы когда загружается период утилизируется сеть или диск для конкретных шардов мы можем ввести коэффициент для процента свободного места для того чтобы добиваться равномерного распределения данных по различным шардам можем ввести коэффициент для доступности всех реплик шардов чтобы писать данные только туда куда они могут быть записаны где доступны все реплики не словно Никакое железо А ну и в принципе для практически любого сценария мы можем вводить дополнительные коэффициенты которые будут учитывать эту проблему и позволяет бороться с ней плюс использование такого подхода с почётным весов дает дополнительные возможности не обязательно вовсе писать данные одновременно во все шарды можно выделить некоторые подмножество шардов и писать данные сначала в них вбиваясь их полной заполненности что это дает Когда шарт становится полностью записанным на него идет максимальная нагрузка как правило данные удаляются не очень активно но при этом с течением времени нагрузка на эти данные падать Ну фоточки которые мы залили на Яндекс диск Там С недавнего отпуска смотрят все фоточки которые мы залили год назад смотрится достаточно редко Поэтому со временем нагрузка на каждый конкретный шар будет падать и это дает возможность изменять способ хранения данных этого шарда например уменьшая количество реплик тем самым уменьшая стоимость хранения при условии если это не влияет существенно отказывая устойчивость и в дальнейшем переводить в более выгодные схемы хранения с использованием рейжа кодинга про это ещё ближе к концу доклада поговорю но вроде Казалось бы все хорошо но на самом деле нашей бочки меда есть ложка дёгтя раньше алгоритм был детерминированный и на каждое чтение для каждого ключа Мы точно знали Из каких шардов его нужно брать теперь это информация есть только на момент записи причём воспроизвести в другой момент мы её не сможем потому что А ну алгоритм выбора Рандомный и коэффициент состояние системы всё время меняется поэтому для того чтобы чтение работало необходимо хранить ID шарда в какой-то базе данных причём эта база данных будет достаточно на нежный потому что на все те сотни тысяч РПС которые приходят в наше хранилище запрос Придется делать в эту базу данных чтобы просто узнать где лежит ключ но тут есть небольшой лайфхак который заключается в том чтобы хранить ID шарда на стороне клиента Чтобы стало понятнее приведу такой Пример например мы строим приложение для прослушивания музыки будем хранить сами треки как значение в object storage и пользователи будут прослушивать их тоже из objectorred же делая запрос туда э-э стандартный пример работы с этим приложением пользователь получает список своих треков и дальше слушает их по одному друг за другом либо В каком-то случайном порядке соответственно на стороне продуктового сервиса то есть предложение будет уже некоторые база данных со списком треков для каждого пользователя собственно Когда мы можем складывать айдишник шарда для того чтобы избежать отдельной базы лишнего хопа сэкономив много железа и для крупных нагруженных потребителей такая схема собственно оказывается достаточно выгодной которую мы широко используем Кроме того возникает другой принципиальный момент как узнать состояние системы то есть я говорю о том что есть нагрузка на диске на сеть распределение свободного места какое-то железо ломается какое-то добавляется и так далее это всё большое количество информации причём с объемом данных с увеличением объёма данных оно только увеличивается а добавляется новое железо увеличивается этот объём информации и всё время динамически меняется нагрузка то растёт то падает а при этом подсчёт Весов и сама работа способность записи напрямую зависит от этого состояния от некоторого стейта хранилища а для как будто бы его собирать необходим некоторые компоненты которые словно схеме назван коллектором и он будет к сожалению вариться не мгновенно задержка от того как какое-то событие в системе происходит например ломается диск до того как мы соберем этот стейт посчитаем веса распространим эту информацию на такси и отразим в реальной записи это изменение может достигать минут и Ну даже больше его нужно оптимизировать это становится очередной точкой в которую упирается система при масштабировании причём это критическая Часть системы в зависимости от реализации возможно такие реализации и даже некоторых других системах не буду называть Какие возможности ситуации когда этот компонент сломается и в результате его встаёт вся в запись вообще систему Что является неприемлемым тут на самом деле есть некоторые компромисс между консистентностью и доступностью который правильно большой системой всегда решать полностью в пользу доступности например мы максимальным образом кэшируем состояние системы State на практике для того чтобы при возникновении каких-то софтварных ошибок Которые наиболее вероятны в коллекторе и собственно сборе стейта почёте весов это не влияло на работоспособность системы то есть в данном случае лучше деградировать чем умирать совсем это будет меньше из двух зол рассказал про то каким образом можно организовать выбор шарда для записи данных но ничего не сказал о том как распределять шорты по железу Это вопрос который тесным образом связан с чтением поэтому к нему и перейдём представим какую ситуацию У нас есть какие-то сервера и так получилось что на некоторые сервы X пришлось большое количество горячих шардов Вот таких которые много читают которые создают достаточно большую нагрузку при этом некоторые другой сервер Vibe пришло Малое количество горящих ордов пришли холодные шарды те на который читает мало испытывает низкую нагрузку соответственно в то время как сервер Wii простаивает сервер X будет э напрягаться там упираться в полку Всё может пройти плохо а-а такая проблема Возможно на совершенно разных уровнях на в рамках отдельных двух взятых дисков то же самое может происходить в рамках двух отдельных взятых стоек когда мы упираемся в сеть на одной стойке и даже в рамках отдельных дата центров когда мы упираемся в ёмкость конкретно вот этот центр Ну ёмкости в данном случае имеется в виду здесь для решения этой проблемы работает классический Облачный подход нам тут помогает то что хранилище которое мы строим это мультитаунсе и мы можем максимально равномерно распределять шарды разных сервисов по железу таким образом горячие шарды будут соединяться с холодными на одном железе и в целом нагрузка будет размазываться какие-то сервисы более может быть создают больший запросов но при этом маленьких другие наоборот создаёт мало запросов но при этом сильно утилизирует сеть при равномерном размазывании этого всего нагрузка тоже будет равно плюс-минус равномерно размазывается а такую вещь нужно делать на разных уровнях например мы это реализуем таким образом когда хотим создать новые шаг новую реплику шарда мы вначале помещаем его на уровне DC в тот DC таким образом что распределение это было плюс-минус равномерным далее спускаем это на уровне стойки да Или на уровне сервера далее на уровень диска такой прием одновременно помадкает лучше распределять запись потому что ну веса конечно могут помочь во многих ситуациях но если есть системная проблема с Когда у вас на части серверов много свободного места на другой части мало то запись вся пойдет в ту часть где много свободного места Ну ничего хорошего не будет Вот и такое равномерное распределение поэтому запись тоже помогает это конечно все хорошо Но рано или поздно придет поставка нового железа когда ты должна произойти И в этот момент собственно сложится как раз такая ситуация у нас есть много свободного места на новом железе запись преимущественно пойдет туда первое что произойдет в такой ситуации это новое железо будет убиваться по записи когда данные получится в конце концов записать так наперёд пришел когда данные получится в конце концов записать они начнут сервера начнут убиваться почтению потому что собственно свежезаписанные данные как правило самые горячие тут напрашивается решение в перевозе некотором данных для распределения свободного места по всему кластеру Ну для выравнивания врать нагрузки собственно такое решение мы применяем но для этого необходим некоторые чебулин Напоминаю что большом хранилище всегда есть большое количество фоновых процессов самое банальное дефрагментация или компактен где-то ломаются диски приходится поэтому восстанавливать данные до требовало уровня отказоустойчивости процессов много И поэтому нужно всех шадулить каким-то образом управлять одновременно переездом и другими процессами и этот же процесс распределения свободного места по железу нужно делать на всех уровнях например мы запускаем отдельный шеделлер который сначала при распределяет свободное место по разным центрам при условии что новое железо приехал какой-то конкретный центр потом в рамках каждого центра работает отдельный шедевр который распределяет свободное место уже между его серверами и дисками в целом такие подходы работают для равномерного плюс-минус равномерного распределения чтения по железу но остается вопрос как выбирать реплику Когда у нас реплик много как собственно читать первый подход который может напрашиваться давайте выберем рандомную какую-то реплику будем читать из неё и Уж если она сломается будем переходить к следующей А тут может возникнуть проблема из-за тупящих дисков которые будут Достаточно долго отвечать они продолжают работать но отвечает долго в результате там тайм-ауты и латанцы становятся неприемлемым э клиентами этого не радуется А поэтому латанцы высоких 30 лет здесь может быть большой с одной стороны а с другой стороны есть максимальные возможности для масштабирования по мере добавления количества реплик мы можем линейно увеличивать нагрузку которую держит этот шаг альтернативный подход отправлять запросы сразу во все реплики и отвечать клиенту с той реплики которая ответит первой А этот подход ожидаем даёт наилучший Но блин мы теперь вообще не можем никак масштабироваться по количеству реплик и есть на самом деле достаточно хороший подход который совмещает преимущество этих двух мы все также читаем с одной рандомно выбранной реплики но при достижении некоторого софт тайм-аута мы отправляем параллельный запрос следующую реплику и так далее по достижении софт тайм-аута можем третью и дальше когда ответ все-таки придет мы отправляем пользователю ответ стой реплики которая ответит первой выбирая размер софт тайм-аута можно балансировать между масштабируемостью этой схемой и максимальным латенсе которым мы хотим позволяет на высоких это достаточно хорошая схема которую мы используем во многих сервисах но на самом деле оптимальный подход тут будет зависеть от конкретного профиля нагрузки например на некоторых сервисов у которых суммарное количество запросов может быть оказывается приемлемым но при этом передаются большие объемы данных и ключи сами по себе читаемые достаточно большие мы используем параллельный подход когда читаем первый Чанг отправляемых данных со всех реплик и последующие чанке Берем с той реплики которая ответит первой Таким образом мы с одной стороны можем масштабироваться с другой стороны время отправки первого Чанга минимизируем уменьшаем максимальные То есть тут всё больше будет зависеть от конкретного паттерна в целом про основные способы решения вопросов балансировке поговорили но можно посмотреть на совсем высоко висящие фрукты Ну представим то что мы уже максимально равномерно распределяем данные различных сервисов по нашему железу но так получилось то что На одном сервере скажем даже на одном диске оказалось два горячих шарда разных сервисов и в результате какой-то конкретный диск не справляется с нагрузкой Возможно даже конкретный сервер не справляется с нагрузкой то есть общее пропускной способность сети этого сервера не хватает а-а и все данные соседей других сервисов начинались страдать В общем случае такую ситуацию в таком способе хранения решить сложно если нет оценки нагрузки То есть если мы не знаем какая нагрузка примерно приходится на каждый шаг и не можем понять там в ближайшее время какой она будет ну можно оставить это на откуп эксплуатации либо то что сделали мы сделали почет нагрузки мы построили pipeline со сбором исторических данных о нагрузке на каждый отдельный шарт в системе накапливая информацию за достаточно какой-то большой промежуток времени скажем несколько недель можно оценивать нагрузку на на следующий период некоторые время вперед на слайде представлен пример нагрузки на один из шардов в течение пяти дней вот так Примерно это выглядит сильная линия это чтение которое нас данном случае интересует Тут нужно сказать что когда строится подобное оценка нагрузки Не стоит относиться к ней очень строго Потому что есть большое количество различных неопределенности возникающих Ну как пример считать утилизацию дисков оценивать ее в будущем вообще оказывается достаточно сложно Потому что есть различные каши есть очереди к дискам есть разные алгоритмы шеделлинга его операции и так далее более того нагрузка все время скачет меняется перераспределяется поэтому этот подходит будет работать в том случае если мы не оцениваем тютельку в тютельку а делаем примерные прогнозы Ну с учётом того что нагрузка там на отдельные части Может взять и вырасти в два раза а и изначально закладываем такой запас А Вот это конечно хорошо но сущности мы делали оценку нагрузки немного для решения другой но связанных с балансировкой нагрузки с задачей это в целом тема отдельного доклада поэтому я пробежусь очень коротко помните в начале ближе к началу презентации я говорил про то что нагрузка на данные падает достаточно сильно с ее возрастом и соответственно конкретный шаг полностью заполненный можно менять способ хранения уменьшать количество реплик если отказывая устойчивость при этом будет оставаться приемлемой и в конечном счете приводить другие способы У нас например на некоторых сервисах реализуется способ когда изначально данные записываются в три реплики со временем с падением нагрузки оценивается нагрузка конкретно на этот шарф Мы видим что она упала переводим способ хранения в двери Африке и когда нагрузка совсем Dead переводим в схему rager kodio нас в случае эта схема с избыточностью хранения x1,5 То есть получается оказывается хранить данные в полтора раза дешевле чем то как мы записывали их изначально Ну вообще говоря такую штуку можно делать и без оценки нагрузок но можно ее оптимизировать например на некоторых наших сервисах подобная схема позволяет экономить десятки процентов места и явная оценка нагрузки позволяет улучшить этот процесс еще на дополнительные проценты на наших масштабах вливается в десятки заключение нужно сказать то что Для эффективного решения задачи балансировки в большом хранилище необходимо явным образом управлять записью учитывая при этом большое количество факторов это не бесплатно но оно того стоит необходимо равномерно распределять шарды по железу необходимо построить какой-то фоновый процесс для переезда данных который будет помогать справляться с добавлением нового железа и при этом не умирать Ну и с помощью таких приёмов как оценка нагрузки можно пытаться Дотянуться до совсем высоко висящей фруктов решать а некоторые трудности мы проблемы балансировки нагрузки и наконец экономить место а тем самым экономить деньги потому что стоимость объектов хранилища обычно вместо и упирается на этом У меня все Всем спасибо за внимание и я готов буду ответить на ваши вопросы если он будет Спасибо Да спасибо что поддерживаете Вадима Вадим Спасибо за классный доклад я со своей стороны хочу сказать что нас уже вопрос о сыпятся Это здорово поймался на мысли Мы 4 часа Всего на конференции уже послушали про то как устроены три объектных сториджа возможно крупнейших в нашей стране там в Яндексе в Одноклассниках ВКонтакте Это здорово конкретно Про Сервис которым говорил Вадим могу например сказать то что насколько мне известно только ваша метабаза настолько большая так много шардов имеет что даже для разделения аналитической нагрузки Вы пользовались нашим сервисом Да это трансфер для того чтобы аналитику нести в Клик Хаус вот поэтому можете делать выводы о том какие там объемы если даже про метабазу приходится решать такие задачи сразу могу сказать что вот поверх хранилище которое я рассказывал есть еще S3 построенная и собственно речи как раз об этом Давайте перейдем к вопросам Илья уже мне за это очень много рук Давай хотите задать вопрос поднимайте руку У нас есть вопросы Из зала из онлайна поэтому Кто в онлайне тоже задавайте и первый вопрос пожалуйста так вот Спасибо за интересно доклад интересно было действительно послушать У меня два вопроса небольших первый вопрос касательно того что ты сказал что айтишники шардов хранятся в некоторых метабазе Это означает что когда шарды переезжают между серверами или дата-центрами у них айтишники не меняются в них они всегда постоянно Да ну фиксируем магических да более того можно попадать в конечную ссылку которая попадает там фронтен и или в приложении клиента Поэтому если бы мы её меняли то ссылка переставала работать ну как бы перестраивает все И второй вопрос касательно предсказаний нагрузки это речь идет о каком-то фоновом процессе предсказателе или это сидят люди анализируют и думают как распределить или он сам можно было представить что здесь работать некоторые talker я сейчас импровизирую и на каждый шар смотрят Какая нагрузка Пришла Нет конечно это было бы слишком дорого вручную анализировать А поэтому есть Ну фоновый процесс который во-первых мы в фоне собираем а историческую нагрузку на все шорты и дальше в фоновом процессе раз некоторые период обычно в сутки делаем пересчёт и делаем переоценку нагрузка которая происходит а на самом деле нет тут мы больше ориентировались не в точность предсказаний А в то что должна быть достаточно большая в любом случае должен оставаться достаточно большой запас в плане алгоритма использования там используется простое скользящее среднее Ну и для наших задач этого оказалось достаточно То есть тут Возможно есть потенциал для роста Но вообще есть достаточно много точек для роста и других Спасибо за вопрос У нас в онлайне есть что-то в онлайне Пока пусто Пользуйтесь пожалуйста Вопрос Спасибо за доклад такой вопрос есть ли у вас какая-то явная система предсказания Нужно ли переналивать ушедшую терабайтную реплику или стоит чуть-чуть подождать и она сама вернётся И тем самым не надо забивать сеть Это зависит от того какая именно поломка происходит то есть в том случае если ломается диск У нас есть система которая отслеживает поломки дисков и Обычно предиктивно еще до того как диск сломается пытается понять что это как бы уже вот-вот наступит и в 90 процентов случаев действительно до того как диск сломается А мы уже понимаем что это произойдет и начинаем перевозить данные с этой реплики на другие реплики А в случае если речь идёт про конкретно отдельный сервер то здесь мы автоматику не делаем э ну прям поломка сервера Так что он ушёл и вернулся без возврата это достаточно редкое событие поэтому не делал я автоматической разводку но Да действительно в некоторых системах используется другой подход когда просто при выходе из строя данные начинает переезжать на другие сервера наших масштабах это представить сложно потому что Потому что при регулярно происходящем отказе в виде отключения до центра регулярным потому что проходят учения но как бы бывает и не учение специально проводятся учения чтобы мы знали что мы можем пережить отказы C в этом случае могла происходить как бы перевос данных всего DC восстановление репликации Но это было неприемлемо Именно поэтому используется подход когда мы не запускаем автоматику при поломке всего сервера Надеюсь ответил Так у нас есть где-то вдалеке там вопрос с дальних рубежей Здравствуй Спасибо за доклад У меня вопрос А как вы решаете проблему деду публикацию по ключам Если вы ее решаете но явно на своей стороне мы эту проблему не решаем эта проблема может решаться на уровне сервиса выше Ну потому что для кого-то это может быть актуально для кого-то может быть не актуально Например я знаю что Яндекс Диск явно решает эту проблему но тут на своем уровне и уже складывает дедацированные данные к нам но тут К сожалению технические детали я более подробно не могу сказать а вот как там Человек тянет уже несколько минут до Спасибо Вадим Спасибо за доклад на 29 слайде была сложная Сложная формула про то как вы балансируете нагрузку по шардам там зависит от нагрузки От количества свободного места вот наверняка эта формула меняется со временем и как его убеждаетесь что нету ошибки в новой формуле балансирования нагрузки которая служит все Угу хорошо вопрос Ну тут на самом деле никакой магии нету Когда происходит какие-то изменения весах они проходят через stagging то есть сначала выливается на трейсинг потом выливается в prestable на определенный процент нагрузки мы наблюдаем то что видим их проблем нет и потом можем выкатывать в далее Это в провод вопрос на самом деле очень интересный потому что любые изменения с нагрузкой они могут быть достаточно коварные Особенно это касается уменьшения изменения способы хранения в а для уменьшения стоимости когда у конкретного шарда уменьшается количество ретрик либо потом переводим мы выража кодинг потому что когда уменьшили количество реплик их с одной стороны можно поднять но это не так быстро будет Когда переводим в Реже кодинг это как бы процесс конвертации он долгий конвертировать обратно вообще долго и не хочется а нагрузку мы уже такую держать не сможем и как бы бомбануть оно может через какое-то время э-э Поэтому Когда делали эту систему относились к этому очень ответственное А И постепенно так сказать закручивали гайки коэффициенты которые говорили о том При каком уровне снижения нагрузки мы можем менять Там степень хранения вот я э-э надеюсь на изначальный вопрос ответил и вот про связанная тема решил тоже рассказать потому что А когда всё это делать было немного стрёмно так у кого еще есть вопросы вроде были желающие так вот у нас там вдалеке есть Человек тянет руку Да спасибо большое за доклад у меня следующий вопрос в случае дедоса не сталкивались ли вы с ним или у вас видос уже отсекается и он не доходит просто до базы он отсекается Спасибо Как сказать тут теоретически можно было бы deto сеть непосредственно нас то есть не знаю взять какой-то набор ссылок и пытаться достичь наше железо но при этом будет для конкретного целевого сервиса во многих случаях просто деградация А как бы не полный выход из строя поэтому выгоднее нас скажем так так в этой части зала У нас тоже вот есть вопрос то так и пока несут микрофон я напомню что у нас будет приз за лучший вопрос поэтому Уважаемые хелперы Пусть он где-то рядом с нами появится Давайте следующий вопрос из онлайн зададим а появился вопрос тогда сначала онлайн потом вы хорошо Итак прозвучало как раз вариант хранения id-шек шортов на стороне клиента Как решается ситуация при потере клиентам этой информации при потере клиентов нет информации честно говоря если Мета клиент потеряет свою метабазу то мне кажется у клиента будут большие проблемы чем просто Как получить свой ключ из нашего хранилища Ну то есть скажем вот на том примере котором я приводил Когда у нас есть музыка и на стороне продуктового сервиса музыки есть некоторые Мета базы в котором хранится список треков собственно сервисом музыки важно не потерять уже сам факт наличия какого-то трека у конкретного пользователя А ну тут должны использоваться стандартные методы репликации Для обеспечения отказа устойчивости Вот и в принципе не приводить к тому что такая ситуация возникнет если она всё-таки возникнет Конечно можно прибегаться по всему объему данных этого клиента но как бы если их много это будет достаточно быстро поэтому опция такая есть Ну это будет долгая фоновый процедура я не помню Честно говоря что-то такого происходило Окей давай Илья пойдем Я хочу уточнить данном случае клиент это не там мобильное приложение Яндекс музыки А это сервис Яндекс.Музыка клиент стороже да да то есть я тут использовал слово клиент для нас клиенты это как правило другие сервисы внутри Яндекса которые пользуются offix Origin мне просто такой же вопрос возник во время твоего выступления Так здесь у нас кто-то есть микрофоном уже да задавайте Скажите а количество шардов у вас постоянно или меняется Но количество сортов Ну естественно объем данных все время растет поэтому количество шардов приходится все время увеличивать то есть увеличивается как правило объем данных и по мере его увеличения добавляется дополнительные шарды причем мы когда приходят новые железо Мы не распределяем его заранее под клиентов А под конкретные нужды клиентов уже в рамках фото вводим в строй новые черты то есть функция распределения объектов не понял как вы распределяете объекты в таком случае то есть обычно это делается мы используем не детерминированный алгоритм то есть заранее неизвестно в какой шар попадёт ключ это определяется в инструмент записи и собственно дальше для того чтобы прочитать ключ нужно хранить ID шарда в которые записался ключ и когда добавляется новые шарды но просто как бы новые данные польются уже больше туда и второй вопрос Если у вас нет метабазы как вы решаете проблему с доступом То есть как вы делаете мульти теннет Если вы не понимаете какой тинант с каким объектом может взаимодействовать Ну в как правило в принципе всегда клиент он мы явно разделяем под множество шардов для конкретного сервиса то есть выделяем набор шардов скажем для почты набор чертов для музыки для КиноПоиск набор для диска и клиент работает уже с изначальным каким-то подмножеством то есть в рамках разных сервисов ключи не обязательно должны быть уникальными они должны быть Уникальны только в рамках конкретного а при этом Ну для это удобно для больших потребителей которых я назвал при этом есть там сотни более мелких потребителей внутри Яндекса которым Ну совершенно неудобно было бы держать свое объектное хранилище у себя там усложнять логику поэтому обжигсториджа о котором я рассказывал есть S3 objek storage который уже имеет место базу данных и в котором Ну можно явно по этой метр базе данных понять где лежит ключ клиента без необходимости хранить ID шарда на стороне клиента которая есть Ну сопоставляет да да то есть тут есть подход Когда у нас есть ну s-3 сервис построенный по верху Джек и клиент может пользоваться свой Стрим в котором есть метабаза либо если клиент крупный он может пользоваться сторожем напрямую и хранили Да на своей стране это будет ему более выгодно потому что в целом будет дешевле спасибо Так у нас Я видел еще вопрос тоже вот где-то там в дальней части зала Спасибо большое Вот вы упомянули что когда вы вставляете новые диски в свои сервера вы как бы записываете туда предварительно данные чтобы они грубо говоря меньше насиловались Да чтобы на них меньше нагрузка приходила Вы имеете ввиду 8 старые данные старые шарды на новые железо Ага то есть шардом является не диск получается а сервер да А нет На самом деле у нас шарды в системе размером порядка одного терабайта и в каждый диск но новые диски порядка 18 терабайт влезает большое количество шардов суммарно на сервер приходится там 1000 или несколько тысяч шардов так если еще вопрос Да есть вопрос первом ряду Спасибо за доклад Меня зовут Федор такой вопрос Вот вы говорите что мы Пишем пишем читаем читаем а данные когда-нибудь стираются вообще удаляются ли Да да допустим два года не было чтение вообще какой-то записи там никому Она не нужна ваша система когда-нибудь удаляет данные и второй вопрос под вопрос к этому я думаю ответ будет нет но если сам сервис говорит что вот эти данные надо Стереть Ну какая-нибудь индекс музыка не знаю фильм ну нужно удалить происходит ли это когда-нибудь И если у вас И для этого возможно Да конечно есть API предоставляет возможности для удаления данных я не касался этого вопроса как бы не говорил про то что вот есть ещё запросы потому что в целом это не сильно влияет на балансировку нагрузки только в некоторых нюансов Вот но Разумеется есть возможности у клиентов для удаления данных и Например если пользователь не знаю КиноПоиск скажем убирает часть старых трейлеров или другой какой-то клиент хочет удалить Свои данные то он явно приходит в апе делает запросы на удаление в этом случае после этого мы можем удалить данные на своей стране хочу сказать что вообще вопрос удаления данных он очень коварный и к нему нужно подходить Осторожно потому что гарантии хранения и то что мы точно будем хранить данные очень важны тут всегда Когда речь заходит удаление Есть риск совершения сафарные ошибки удалить что-нибудь не то поэтому это то место которое максимально вообще со всех сторон обкладывается различными подушечками например данные когда клиент явно их удаляет мы с физических дисков удаляем не сразу а только через некоторое время там не менее недели данные продолжает жить и в том случае если клиент поймет что он по своей ошибке удалил Данные есть какой-то запас для того чтобы их восстановить автоматическое В смысле А да есть это да есть короткий ответ такой Спасибо Спасибо так на этом У нас вопросы закончились и да аплодисменты спасибо спасибо за внимание так сложно подожди не расслабляйся Вадим теперь почетная обязанность спикера выбрать того счастливчика кто задал самый классный вопрос давай выберем два три вопроса У нас есть возможность Если хочешь какие-то подсказки записанные Михаил пирами Да я уже нашел мне очень понравилось Вопрос вот человека который сидит на третьем ряде про нагрузку и про то как вообще это мы убеждаемся что это корректно работает ну очень важный вопрос на самом деле во многих аспектах И мне очень понравилось вопрос про дедасы Отлично мы просим авторов этих вопросов подойти тогда ну хелпером либо к ним подойти либо автором вопросов подойти к сцене после завершения данной части и мы вручим подарочки и Вадима тоже Давайте поблагодарим Классно же было лица программного Комитета и небезызвестно для тебя компании Яндекс тебе подарок Всем спасибо очень рад что вы все сюда пришли было очень приятно вам рассказывать про такую тему Я рад что вам Интересно так же как мне класс Спасибо Приходи еще и Вы тоже приходите встретимся буквально через 15 минут в этом же зале Яндекс трек много еще чего про хранилищем Спасибо"
}