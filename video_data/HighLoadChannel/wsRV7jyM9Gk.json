{
  "video_id": "wsRV7jyM9Gk",
  "channel": "HighLoadChannel",
  "title": "Петабайт в YDB over HDD в процессингах Яндекс.Метрики / Антон Барабанов (Яндекс)",
  "views": 238,
  "duration": 2500,
  "published": "2024-10-29T03:07:39-07:00",
  "text": "здравствуйте да меня зовут Антон я в Яндексе руковожу службой ядра Яндекс метрики и довольно важной частью ядра Яндекс метрики являются процессинг собственно метрики метрик - это система веб-аналитики которая позволяет вам поставить на сайт JavaScript счётчик он будет собирать какие-то сигналы присылать их нам на бэнды и дальше Вы сможете видеть у себя там у нас точнее в интерфейсе все возможные красивые отчёты самый популярный в СНГ третье по популярности в мире по данным 2019 год сейчас я думаю что это будет четвёртое или пятое место но третье выглядит интереснее для слайда А мы собираем данные с сайта и это мы называем хитами также на сайте Мы собираем паблишер скую аналитику и мы собираем данные для вебра записываем ролики Дальше можно в интерфейсе посмотреть как пользователь двигал мышкой что он делал на сайте и как он себя вёл довольно фундаментальной структурой данных в метрике является понятие визита визит представляет из себя сессию как пользователь ВЛ себя на сайте от точки захода До завершения того самого визита в него мы записываем эти Самые верные ролики в него мы клам все хиты по нему мы определяем причину по которой пользователь оказался собственно на сайте то есть это может быть просто заход потому что пользователь помнит ваш сайт это может быть заход из поиска из рекламы ещё по каким-нибудь причинам по QR коду или ещё чему-нибудь помимо простой причины Да это там атрибуции Клик Мы также можем определять какие-то продвинутые атрибуции Почему пользователь попал на сайт впервые какой был предыдущий переход из поиска из рекламы и так далее ещ одной важной особенностью метрики являются цели они же конверсии в простом виде Это самый простой пример - это покупка То есть у вас магазин вы определяете цель это покупка на сайте соответственно человек когда платит вы в интерфейсе можете видеть что случилась такая вот конверсия Ну если говорить в общем то это выделение всего потока событий ключевых или самых важных для ва действий конверси в аналитике так и в рекламе Например можно затар рекламу на нажатие кнопки пожаловаться и таким образом собрать о себе какой-нибудь фидбек и переходя собственно к процессинга в очень простом виде процессинг метрики выглядит таким вот образом то есть у нас есть какой-то приём сигналов ТП сервер он отправляет данные через шину назовём её какой реальности этолог брокер какой-то поточный процессинг с другой стороны у нас стоит какой-то импорт кликов который забирает их тоже через какой-то какую-то тоже в нашем случае ке ВС это сливается в склейку визитов которая работает сом который у нас лежит на которая развернута поверх SSD зачем там стот потому что к нам пришёл хит нам нужно достать все предыдущие хиты за последние 5 1015 минут чтобы обновить этот самый визит чтобы их нам откуда-то достать нам очевидно их то соответственно общий рейт запросов в эту самую базу составляет под миллион запросов в секунду и е общая ёмкость довольно велика когда мы эти визиты все склеили нам их нужно куда-то сохранить чтобы потом можно было Из них построить отчёты мы их сохраняем в хаха это собственно замечательная база которая была когда-то для метрики и написана и чуть-чуть подробнее о том как мы используем обычно используем Иу для тогда он задубровка идея во-вторых в кафке могут случиться какие-нибудь дубли что-то где-то пошло не так дата центр закрылся открылся другой переотправить и так далее во-вторых для этой самой сборки визитов и у этой базы есть несколько ключевых свойств во-первых там довольно маленький ТТ нету смысла держать данные в этой самой супер горячей базе дольше 24 часов очевидно что за 24 часа любой Визит закончится люди имеют свойство уходить спать во-вторых у неё довольно небольшая ёмкость Ну как небольшая 100 ТБ но в терминах потока Яндекс метрики Это довольно небольшая ёмкость как мы с ней работаем если посмотреть на корень то в корне этой базы лежат какие-то папки зачем там папки чтобы просто не было огромной портянки разных таблиц это неудобно в папках лежат таблицы в таблицах партиции и Здесь начинается собственно особенности работы с idb idb - это распределённая база то есть она бесконечно скалия в теории и распределенность э самая достигается за счёт партиции партиции разбиваются по первичному ключу поэтому очень важно чтобы первичный ключ всегда был Шом от чего-нибудь потому что фактически Как происходит если у нас представить себе что первичный ключ первая сть первичного ключа это 1 байт то первая партия будет ну до 127 вторая от 128 до 255 хэш очевидно даёт более равномерное распределение второе важное особенностью является то что запрос в рамках одной партиции - это локальная транзакция это гораздо оптимальнее чем запрос в много партиции соответственно когда мы работаем с базой мы постоянно дёргаю des Ну как постоянно примерно раз в 10 20 минут условно узнаём текущее разбиение на партиции и лозу запросы когда мы их делаем то есть выполняем когда мы получаем бач для процессинга мы разбиваем этот бач по патиц Мы работаем с каждым чом По отдельности С этого получается Всё гораздо оптимальнее и мы используем ба АСР ба АСР - это замечательный способ ставить данные который работает мимо транзакционная и соответственно позволяет оперировать ставками не думая о транзакциях и не думая о том чтобы сделать всё это сильно оптимальнее Естественно что таблица партиции в таблице настраиваются при создании можно указать минимально максимально стартовое число и так далее можно уть максимальный размер партиции в байтах как правило максимально по-моему там 2 мегабайта или 2 гиба точно не помню и можно указать авсп по размеру по нагрузке Зачем важе все пользователи одинаковые например пользователю может быть краулер компании которая создаёт нагрузку мягко говоря гораздо более высокую чем обычные пользователи и в эту партию будет идти гораздо больше объём запросов если она будет равномерно по размеру В смысле интервала с другими партиями Соответственно в этом случае может автоматически такую таблицу партию рапти А другие партиции укрупнить для рабо этого самого очень важно чтобы проса и во-первых а во-вторых чтобы Макс был строго меньше больше мин чтобы всё это функционировало так работает лтава часть метрики А дальше собственно у нас возникла новая Задачка Задачка звучит примерно так у нас помимо конверсии на сайте есть конверсия в реальном мире например человек пришёл в магазин физически ногами и решил купить кроссовки купил кроссовки воспользовался картой лояльности сайт мачет по сло потому что миди по смотрел кроссовки выбрал Вот именно эти кроссовки А дальше решил дойти в магазин их померить походить и купить или нет ну кроссовки могут привести с примеркой а стиралки предположим с примеркой не приводят пока что по крайней мере а соответственно здесь получается что глубина поиска для проклейки она уже не единицы минут секунд или часов глубина проклейки должна быть несколько десятков дней это большая проблема потому что это сразу увеличивает наш стейт в поря по сути второй особенностью является то что у нас здесь нету строгих требований креал тайму Почему Потому что если нам прислали конверсию задержкой скажем в 10-15 часов Да есть вторая особенность не все магазины сразу же как только случилась покупка отправляют данные в Яндекс метрику они могут это сделать на следующий день с утра айтишник пришёл собрал и отправил в трику вполне нормальная история очевидно то что пришло С задержкой на 8-15 часов проклеивать за 15 секунд необязательно с другой стороны Конечно хочется не проклеивать это по 2030 часов то есть не уходить здесь какой-то глобальный мадс Потому что если человек ошибся то он хочет узнать об этом просто налив кофе вернувшись компьютеру а не на следующий день потом пошёл править потом снова в общем итерация Может затянуться на несколько дней для того чтобы это делать нам естественно Нужна база чтобы хранить вот этот самый увеличивший сто и мы пошли По стандартному пути выбора технологии сначала написали что нам собственно нужно с ней делать во-первых визитов метрики всего случается порядка 40 милардо в сутки размерность визита в среднем Это около 2 КБ во-вторых Естественно что мы не хотим использовать ССД потому что чтений здесь будет довольно мало мягко говоря не ко всем визитам приклеиваются конверсии на самом деле процент визитов к которым что-то прилепится потом меньше наверно проце соответственно простаивать сотни дорогих ССД То есть это будет простаивать сотни тысяч довольно дорогих эоп сов это бессмысленно А нам важно уметь доставать данные по ключу точнее по части ключа за какое-то Разумное время Ну потому что 10-15 минут - это всё-таки то во что мы хотим укладываться и нам конечно же важна стабильность потому что мы не хотим просыпаться по ночам мы не хотим бегать и чинить эти данные мы не хотим лопаты их разгребать классическими вариантами для метрики являются две базы это база ха которая бы разработана Изначально метрике и для метрики и мы изначально посмотрели на неё Мы отлично знаем что она умеет работать СД у нас развёрнуты наш собственные кластера Хауса наверное одни из крупнейших в мире на наших собственных серверах Нео облачных потому что в облако такое не совсем влезает п большая история взаимодействия есть налаженный личный контакт и так далее И у нас есть огромный опыт эксплуатации кликхаус мы знаем как его учинить как оно ломается То есть наверное сотни разных поломок Мы метрики у Хауса разных видели и способов починки и idb который мы тоже активно используем Ну на самом деле ещё мы используем My но класть My 40 млр визитов в день Это такая плохая идея мы поэтому не стали на это смотреть idb отлично работает с запросами она тоже активно развивается У нас тоже с idb налажены хороший личный контакт Мы с ними дружим и у нас тоже довольно большой опыт к эксплуатации idb у нас на наших серверах не работает оно работает у нас в Облаке но мы хорошо умеем ориентироваться в их графиках в их аналитических запросах в их приборах у нас просте с ними интеграция нашей Арт системы и так далее Из минусов кликхаус отвратительно работает как KV хранилище у нас был опыт у нас до недавнего времени буквально 2-3 месяца назад мы окончательно закопали всякий кликхаус как K value хранилище Не надо так делать во-вторых кликхаус плохо работает синхронная репликация всё-таки база писалась для другого база писалась для того чтобы гнать в неё огромные потоки запросов и оттуда вычитывать какими-то запросами какую-то аналитику опять же у нас был опыт работы синхронной репликации не очень idb мы не знали как это работает с в теории idb раньше на работала в теории ребята её оптимизировали это мы знали но мы у себя на наших нагрузках сво дела не имели Ну в общем так как минусы хаусов оказалось чуть больше Мы решили остановиться на и нарисовали первую схему первая СХ довольно простая есть онр какого-то тоже тока заго Ви Где мы пытаемся их приклеить к текущим визитам То есть за текущие сутки если вс-таки люди отправляют данные в реалтай то мы Это приклеим буквально в Реал тайме используя дорогую если не получилось то весь поток визитов которые скола сирова и уехали из мы отправляем в запись в медленное хранилище Ив оси для схема данных у нас получилась примерно такой во-первых естественно должен быть идентификатор во-вторых У нас должна быть версия в третих у нас должны быть данные зачем нам нужна версия у нас нету транзакции между входной очередью и idb то есть для входной очереди мы используем сейчас заус иногда мы используем zer и кликхаус для хранения Лобов иногда мы используем ке в общей инсталляции Иста соответственно транзакции между входным током и выходной таблицей у нас отсутствуют Таким образом у нас может ситуация когда Мы записали данные в таблицу и перед тем как мы котли удаление входной очереди мы упали Ну не знаю случился аборт или кто-нибудь пришёл сдел ми9 или просто машинка физически в Облаке отвалилась и наш про в которой мы оперируем входной очередью мы оперируем неким счётчиком Это счётчик итераций процессинг одной транзакции мы увеличиваем номер итерации и соответственно удаляем выходные данные и когда это актуально создаём выходные данные и этот самый номер итерации мы пишем во все строки idb за счёт этого мы достигаем того что когда мы обрабатываем очередную пачку мы знаем номер рации мы строчки которые нам интересны если обнаруживаем в них строчки с номером рации больше либо равным той с которой мы работаем сейчас Мы это можем просто удалить туже потому что это очевидно какой-то ретрай и чтобы мы всё-таки чистили базу обычно мы удаляем пост комит хуком все данные которые нам больше не нужны дальше мы пошли в сторону того чтобы подумать о том как будет работать чуть-чуть подробнее очевидно что нам нужен некий механизм очистки базы то есть мы храним все визиты в течение 20 дней а можно сделать единую таблицу это будет таблица размерностью в сотни терабайт и запустить на неё встроенный в idb ttl скорее всего это плохая идея потому что TL делает регулярное полное сканирование таблицы И для этого не очень предназначен по крайней мере мы его так стараемся не использовать Мы решили что лучшим решением будет решение за от единиц это просто дневные таблицы То есть каждый день когда он начинается мы заводим новую табличку и начинаем писать в неё данные соответственно у нас есть 20 табличек когда мы заводим 20 более старую таблицу мы просто дропаем целиком мы ВС это реализовали написали и попыта Есть некая интервальная зависимость размера хранилища От количества учих ресурсов количество Ух ресурсов зависит от оперативной памяти мы когда проектировали эту базу предполагали что будет огромное хранилище размерностью изначально в сотни терабайт порядка полота дальше мы это будем растить в пита данные лежат и лежат никому не мешают Но оказалось что во-первых даже просто начав запись Мы упёрлись в лимиты оперативной памяти то есть в idb просто фактически перестало функционировать закинули ресурсы дальше начали читать и упёрлись в то что активно кишит то есть руют буквально блоками и так блоки они довольно больши памяти дополнительно к этому возникла ещё такая интересная особенность что в нашей инсталляции внутри Яндекса У нас есть жёсткая привязка количество оперативной памяти к ядром процессора Ну я думаю что в публичном Яндекс облаке можно сделать разные ноты у нас она к сожалению только одна Поэтому в результате у нас получилась ситуация при которой долив вычислительные ресурсы мы получили довольно низкую утилизацию цпу порядка 8% ну с другой стороны наверное можно на этих 92 е что-нибудь полезное поделать но без доступа к дискам В общем дали мы учительные ресурсы по идее вс хорошо попытались включить снова и результат был примерно аналогичным на этот раз всё сломалось конечно писалось уже всё хорошо читалось плохо Почему всё плохо читалось потому что Как Мы работаем с этими самие во о конверсия некое полезное действие которое привязано к некоторому Сату ено онси зам собственно нуж бы та самая карта лояльно дальше нам нужно фактически найти все визиты по этим идентификатором пользователя причём их может быть несколько Потому что есть какие-то одни идентификаторы есть другие фика они связаны друг другом как всё ко всем многие ко многим точнее соответственно приходит о конверсия мы им в свои дневные таблички и инто Поль е визиты при этом скорее всего этого пользователя визитов не было вовсе то есть запрос Верт пустоту но idb имеет такую особенность что подняв блок с диска оно его положит в память и считает его целиком то есть читает не ключ первичный который нам собственно интересен а считает блок и это будет проблема потому что такая нагрузка укладывает хдд дис нам что ину способность пор на поря меньше чем бы просто иде будет вять использовать ноф встроены здесь не функционирует по двум причинам причина первая в нашем перечном ключе есть та самая итерация без которой мы не можем функционировать работает по полной версии первичного ключа нам сказали что может быть когда-нибудь они постараются с мы не зна идентифика собственно визи у него есть отдельный свой личный уникальный идентификатор мы знаем толь уникальный идентификатор пользователя поэтому мы решили воспользоваться замечательным свойством wbb умеет класть данные на HD А индексы класть на SSD То есть вы делаете запрос ожидаете что он сходит в индексы ничего не обнаружит и только подняв ИС подни данные иде пось него заработало потому что на самом деле если одним запросом сходить в такую таблицу со сложной структурой idb по-прежнему пойдёт и на SSD и на HDD диски и всё оттуда поднимет поэтому мы положили в итоге на SSD полный первичный ключ начали ходить туда вместо простого запроса сдвоенным запросом то есть первым запросом мы выгреба все существующие полные первичные ключи визитов в некий массив И дальше этим массивом идм уже в полную таблицу с хдд и запрашиваем данные в этой ситуации первый запрос работает без диска потому что он не просит поля которые лежат на диске над диске второй запрос ходит оптимально потому что ходит только за теми визитами которые там есть пройдя эти пути и помывшись мы снова решили это включить и как ни странно оно закро но есть ней момент у нас поток записи в эту базу итоговый оказался порядка 20 Гигабит в секунду соответственно нам критично Важно писать без отставания почему потому что наши очереди процессинга они построены для реалтайм обработки и естественно что они персистентные потому что Всё может падать и данные не должны Ни в коем случа терно э персистентность достигается за с того что уних под капотом Т же самы с диски которые дорогие вот эти самые 20 Гигабит в секунду откладывать отставанием на дорогие SSD диски Это очевидно очень дорого и делать этого не хочется а вот читание мы можем задержать задержать его на 5 10-15 минут почему Ну потому что мы помним о том что наши требования это проклейка за 10-15 минут Вполне себе нормально В итоге мы пришли к тому что схема первичная была такой стала вот такой очевидно что эта схема тоже не будет работать Почему Потому что олайн конверсия приехала в склейку супер быструю склейку в ID SSD А дальше если оно не проклеить там то конверсии поехали направо визиты поехали вниз и вот пока визиты едут вниз конверсия может проехать справа и уже считать из данные ничего там не найдет что произойдет В результате мы естественно не получим проклейку конверсии соответственно нам также надо добавить сюда какой-то буфер дальше это всё какое-то время про работу работу достаточно неплохо дальше мы получили некоторый опыт эксплуатации этой базы которым тоже хотелось бы поде об замечательны процесс под названием Что такое это дерево дерево это такая штука у которой при удалении данных данные физически не удаляются чтобы физически освободить место на диске запускается процесс компак который собственно также может инициировать собственно аспт или Арци здесь возникает момент Обычно когда мы работаем с SSD мы выставляем достаточно большие интервалы особенно когда мы работаем с SSD дисками в дневных таблицах такое У нас тоже есть Почему больший интервал потому что в текущий день мы пишем много и читаем Оттуда тоже много соответственно там нужно больше портиться чтобы держать входящую нагрузку в предыдущий день а тем более на пару дней назад мы читаем редко и вообще не пишем там можно промерзать партиции до максимального их размера В результате мы сэкономим количество партий А количество партий - это тоже достаточно дорогой ресурс То есть их число во всей базе тоже чем-то лимитировать также естественно Мы всегда стараемся поставлять юниформ но мы попадали на то что один раз мы юниформ не поставили поставили мин там 100 Макс 1000 а юниформ был не установлен в результате табличка создалась и не со ожидаемым числом партиции 100 а с нулём Точнее с одной партиции и естественно когда туда полилось если с хдд просто так взять и пойти по тому же пути то работать это будет плохо Почему Потому что с одной стороны мы данные очень редко удаляем фактически мы делаем дроп таблицы это не приводит к компак потому что там не надо перестраивать деревья дропается дерево целиком но если мы ставим большой интервал то есть условно скажем там пусть будет 25.000 в максимуме и выставляем авто Сплит по нагрузке то при заведении нового дня таблица начинает суперактиноид идущий день будет обратно мержи в 10.000 партиции что в результате в результате запустится очень агрессивный компак очень агрессивный компак который ваши диски положат и дальше примерно 2-3 часа ваша база будет работать Ну будет работать но не очень хорошо поэтому мы пришли к тому что да мы избыточно едим партиции Но что поделаешь минимально Что можно сделать это 20% точнее максимально 20% дифа между ми и Макс А лучше даже 10-15 на очень больших интервалах че в залида порядка 500 ТБ и мы в ближайшее время планируем Залить туда петабайт мы уже кладём туда не только визиты мы кладём туда ещё и хиты мы кладём туда ещё и рекламные клики и планируем Туда положить ещё много что интересного текущий поток порядка 20 Гбит этот поток прекрасно варится нашими текущими железными мощностями текущей инсталляции порядка 300 дисков справляется если умножить 300 дисков у нас это по-моему 15.000 и осов и соответственно мы вывозим туда всё что можно охладить охладить на самом деле можно многое то есть в любом большом крупном процессинг вы всегда можете найти данные которые вам скорее всего не нужны вы их храните просто потому что в теории Мали может быть например это может быть какой-то рекламный антифрод который работает с глубиной 2-3 месяца или это могут быть какие-нибудь мобильные профили в метрике которые нужно хранить в течение хотя бы года но естественно что если пользователь не возвращается в течение месяца то вряд ли он уже вернётся так далее и так далее примеров может быть много Ну и Мы надеемся что idb починит то что должно починить вот так Ну что спасибо за доклад вопросы Да здравствуйте Спасибо доклад У меня небольшой технический вопрос где-то Может в самом начале пода мы повторяли вы говорили что вы руками достаёт из базы информации партиции разбивайте ваш запрос чтобы типа обходить планировщик транзакции чтобы там не произошло ожидаемый вопрос А почему это не делает планировщик транзакции нельзя ему сказать что нам не нужны зависимости между ними это не должна быть чётка семо Пусть он сам разобьёт почему это должен делать Ну в данном случае пользователь вы отличный вопрос спасибо Мы не хотим чтобы наши запросы вообще уходили в планировку транзакций так как у нас тот самый миллион rps Мы стремимся к тому чтобы максимально оптимизировать запросы то есть теория на прино можно сказать что есть специфичный запрос Range по блоку Select по блоку то есть что мы делаем обычно мы кладём в запрос Блок из 1000 ключей и делаем селек поэто 1000 ключей должно будет понять что вот этот блок его надо разбить автоматически на эти самые партиции Нужно ли учить этому систему ну не очевидно мы не совсем обычно применяем всё-таки в idb кажется что разумнее это сделать самостоятельно при этом Если вы будете ходить по одному ключу то вам ничего не Придётся делать вы всегда будете ть в одну дици просто это похоже Может я неправильно понимаю в классическом примени условный там Н comit это ня вообще без разницы просто дай небудь Дайте Дай нибудь что дайте что-нибудь а а в общем типа я готов там два раза читать не готов и так далее аккуратно Составлю И вот я согласен но специфика в том что когда мы работаем с базой с нагрузкой скажем 1000 5000 РПС то мы можем рассчитывать на то что всё сделают за нас то есть Т будет работать за нас планировщик будет отрабатывать за нас дефолтный и так далее А когда мы стремимся к нагрузкам уже в миллион ПС в сот раба То есть эксп батри это поему сама п инсталляция idb в Яндексе а скорее всего имеет смысл специфично работать с базой Ну просто потому что это уже тонкий инструмент его нужно тонко настраивать хорошо спасибо спасибо за доклад А я вчера был у ваших коллег собственно по ydb докладу и у меня родился вопрос первый тормозит ли ydb и да и нет и второй за что вот так с Клик хаусом Почему Яндекс разлюбил своё детище неправда Мы обожаем кликхаус ещё раз у нас самая крупная инсталляция Хауса наверное в мире суммарная ёмкость под кликхаус у нас переваливает 25 это то что записано на диске Если что то есть кликхаус стоит у нас на железных серверах и прекрасно функционирует просто ха - это система которая была написана для того чтобы строить отты писа для того чтобы быстро туда обращаться за данными то есть не сделан для того чтобы быстро доставать строчку одну Ну просто разные системы мы их используем для разного вот тормозить ли idb Да если неправильно её готовить то есть специфика нашей работы сби в том что мы работаем нас афект Не деп процентили тайминга не дево а сотый потому что весь процессинг упирается в тот самый сотый процентиль работает по самому медленному Ну так как вся вот эта машинари с 20 гига битами в секунду и миллион РПС работает с хорошим слаем то Ну на самом деле в idb не тормозит Спасибо так Антон Спасибо большое Очень интересно Скажите пожалуйста считали ли вы экономическую выгоду То есть у вас был выбор разместить на ssdb вы выбрали другой вариант часть данных ну индексы там данные там это всё потребовало определённой работы вот эта работа она сколько-то стоит Она не нулевая вот рои вот этого дела или просто может быть проще было забить разместить на SSD они дешевеют SSD действительно дешевеют и по прогнозам по-моему в д седьмом году SSD будет производить в двадцать восьмом настолько же дорого насколько хдд но правда в том что в любой крупной компании в каждом сервере торчит 2Д диска то есть SSD дешевеют но они очень востребованы все едут на SSD а HDD вот лежат пары кбайт по серверам и никому не нужны на самом деле там лежат какие-нибудь логи которые никто никогда не прочитает поэтому HDD по сути бесплатно это момент первый момент второй Ну разница между по стоимости между SSD и хдд с точки зрения вычислительных мощностей в два порядка То есть хдд - это 50 тире 75 иов нормальный шный эшник даст больше 10.000 Ну вот они два порядка а соответственно HDD гораздо дешевле работа потребовалась не от нас на самом деле основная работа Была проведена ребятами в idb которую инсталляцию собственно собирали Для нас это просто база в том же самом облаке А ну и от нас потребовалось написать какой-то специфичный код работы с этим холодным хранилищем но всё равно пришлось бы писать то есть как показывает практика когда мы запускаем подобного размера процессинг мы всё равно собираем 5 10-15 граблей пару раз просыпаемся по ночам и как минимум один раз с лопатой всё это разгребаем потому что накопилось слишком много не научились мы пока что по-другому жить Спасибо Привет Спасибо классный был доклад очень интересно первый вопрос Как вы поступаете с задав событиями когда Ну поскольку Вы сессию склеен тайме какие-то события сессии обязательно запаздывают например с метрики с пикселя ещё не долетели до Ну до вас события Или например другой или другой пример когда конверсии приехали сильно позже чем сессия склеила Ну в этом нету никакой проблемы если событие задержалось событие отправляется из брауз то есть задер о вс-таки редко оно может задержаться на единице Т или даже часов Ну потому что человек ВЛ туннель Хотя браузер отправит но в теории это возможно потому что на сервер там прилёг и потом доопрацювати в сессию долетят мы вообще рекомендуем всё-таки 12 часов в некоторых кейсах Но обычно окно на прилёт реалтай мох событий гораздо меньше то есть с трудом себе могу представить как может из браузера на десятки часов задержаться отправка А в метрики где не браузер а мобилка там таких проблем нет там совсем другой мир это был про конвейер метри конвейер метрики У нас тоже есть он совсем другой и там нету визитов Спасибо И ещё совсем кратки вос если тут паттерн доступа ключ значения то Почему не почему не потому что мы для стей в основном используем так сложилось исторически вот мы используем для прокачки данных и так тоже сложилось исторически в тоста в щее мы тели получить быстро был предоставить не готов то есть помимо нам е необходима было кросно то есть мы всё-таки исходим из того что если дата-центр падает Ну потому что бывает экскаватор бывает авария на питание бывает там я не знаю всё что угодно Дант мы должны работать ври Яндек стать таки таких кластеров не было так спасибо Следующий вопрос слышно Спасибо вам большое за доклад А вот у нас такой вопрос заранее тоже хочу за него извиниться потому что мы только начинаем знакомство с идб А вот правильно мы поняли что у вас была проблема поиска по User ID правильно Да а а почему нельзя было построить индекс просто по нему не уникальный не праймери А обычный индекс или ADB - это локальный да секционированная Или яб это не поддерживает адб поддерживает поиск по части первичного ключа поэтому фактически у нас такой индекс был проблема в том что когда мы достаём данные по части первичного ключа мы не можем использовать встроенный в idv БМ фильтр потому что он работает только с полной версией первичного ключа А дальше если у нас вся строка лежит на хдд включая первичный ключ яб пытается считать оптимистично как можно больше и как можно больше прошивать соответственно блок данных большой ключик маленький вот всё это загружается в память и укладывает диски А какие поля у вас первичный ключ входят хэши от пользователя пользователя идентификатор счётчика идентификатор визита номер рации на скидку на первом месте шка от пользователя пользова то есть первичный ключ он сортированный х от пользова для того чтобы нормально работал позиционирование по пользователю условно правильно умеет патинировать пол по полному первичному ключу то если посмотреть на разбивку по партиям точно первая сть чно чат Сай остальные крупный пользователь то он разобьёт е и вглубь то есть пронино по пользователю Да локальные индексы поддер прон вторичные индексы есть Да есть idb вторичные индексы но вторичных индексов idb есть одна неприятная особенность по сути это вторая таблица сбоку соответственно Когда вы пользуетесь вторичным индексом вы автоматически налета на распределён транзакцию потому что при вставке вы вставляете локально здесь в партию А здесь она уходит в 10 партий поэтому мы вторичные индексы пишем руками то есть создаём сбоку сами таблицу сюда вставляем локально И сюда тоже вставляем локально сбалансировано просто разные индексы и Поэтому возникает разное позиционирование поэтому здесь вы локализовали до партиции А здесь у вас получился разброс Почему нет такой проблемы Извините почему ещ раз нет Тай про немет большой минус поэтому То есть можно работать очень оптимально если нужно очень много делать запросов очень быстрых если вам достаточно у вас другие сценарии то скорее всего вам просто это не нужно Добрый день спасибо за доклад изза специфики вашей области у вас Судя по складывается улы кластера то есть один и читает и пишет остальные редко читают там большая часть старых серверов ну которые давно была они простаивают Было ли Это для вас проблема И как вы с этим справились так как это то это не проблема то есть в обычном режиме Они бы просто стояли бы в серверах и всё равно их надо где-то хранить а вычислительные мощности в idb не строго привязаны к хранилищу поэтому они не простаивают они работают так ещё вопросы Ну что Судя по всему Вопросов нет Большое спасибо за доклад И просьба выбрать лучшего вопрошающий а сейчас вам хелпера принесут подарок спасибо от нашего партнёра Спасибо"
}