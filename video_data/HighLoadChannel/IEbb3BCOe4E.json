{
  "video_id": "IEbb3BCOe4E",
  "channel": "HighLoadChannel",
  "title": "Starship Enterprise Evolution: архитектура e-commerce-платформы / Алексей Скоробогатый (Lamoda)",
  "views": 1673,
  "duration": 2631,
  "published": "2020-04-14T11:07:21-07:00",
  "text": "ламода пожалуй классический коммерс в отличие только что не у каждого и коммерс есть такой развитая раз из этой автоматизация операционный отдел это автоматизация колл-центр автоматизация склада автоматизация фотостудии доставки но начнем пожалуй сайта все начинается сайта меня зовут алексей скоробогатый я системный архитектор и камеру с платформой lamoda и сегодня хочу поделиться с вами опытом того как мы в нашу архитектуру и камер с платформы добавили real-time data processing а как этап эволюции всей архитектуры поехали ламода составе отдела моды состоит в основном это три отдела которые очень плотно взаимодействуют друг с другом это и комикс платформа сайта real-time коммуникации с каста миром autoyes автоматизация бизнес-процессов и дата аналитика отдел дата аналитики собирает все данные которые продуцируют наши системы из двух предыдущих отделов и строит на на основе этого анализ и прогнозы отчетности для бизнеса и именно в плотных взаимодействие с бизнесом из аналитиками ставятся задачи для нас для найти и мы развиваем архитектуру делаем проекты исходя из тех проектов которые ставят для нас бизнес так родился вызов с которым мы столкнулись для того чтобы задуматься о эволюции нашей архитектуры в целом вопрос от бизнеса персонализированные политики что это это про мотивацию клиента быстрее обрабатываем накопленный даль данные больше вовлеченность такими фикации на сайте действие поощрения для примера клиент оставил комментарий на к позиции которые он выкупил мы дали ему бонусы эти бонусы повлияли на его лояльность следующий заказ можно делать больше процентов скидки удобно быстро классно и немножко подробнее это скидка на скидку действует проценты выкупа чем лучше costume выкупает с кем а это лучшего для нас то меньше операционные издержки ну и приятнее для костра он может больше заказать того что он купит и баллы баллы это как раз то через геймификации которая на карту 2 мы начисляем в итоге процент и две шкалы процент выкупа и баллы и они влияют как раз на тот процент скидки итоговые которые будет доступна каста миру и к 100 мир конкретный попадает в определенный сегмент он может двигаться вверх вниз вправо влево и мы даем ему прозрачно коммуницируем что он может сделать для того чтобы получить большую скидку также моему коммуницируем о том как баллы накапливаются за что за заказ за комментарии за любые действия это можно собирать все что угодно в конечном итоге все сводится на сайте к форме оба создания заказа и сюда именно мы применяем все наши собранные данные по пользователю и те тот процент со скидкой которую как пример который клиент может применить форма это можно разделить на блоке каждый блок как обратную связь получает информацию о том что это заказ на мир как пример выше это был про блок discount скидки лояльность влияет на то какую итоговую скидку по корзине конкретное получит костыль а также мы можем влиять на блок доставки предоставлять за всякие действия пользователя бесплатную доставку классно здорово какие-то особенные условия по оплате какие-то условия боксом и корзине подарок туда положить или ещё что-нибудь такое конечном итоге после после этого после создания заказа после валидации всех этих блоков заказ попадает вроде processing и дальше начинается оператор операционной процессов каждый блок это то что находится в рекламе с платформе как часть из формы заказа создание каждый блок это сервис которого есть своя база данных данные из этих база собираются в data warehouse а коллег тица в слове и деле провалиться аналитика и получаем то что называется data insight это то что какие-то новые знания о поведение пользователя и еще что то это очень важная информация и чем быстрее мы будем коллектив данные и применять их тем больше мы сможем сэкономить больше предоставить возможности больше кастомизацию для конкретного каста мира персонализацию что в действительности требуется еще раз скажу что учитывать историю клиента и его поведение коммуникация в реальном времени это очень важно чем быстрее мы будем идти данной коллекции в как обратную связь подавать для наших сервисов тем лучше определять группу кредит клиента чтобы показывать ему подсказывать как он может причинить перемещаться из сегментов сегмент и управлять настройками для группы давать бизнесу инструментарий для гибкого управления этими группами как это работает сейчас до того как мы задумались о внедрении слоя процессинга данных в реальном времени наши сервисы пвх лукатиель и они batch операциями какие-то операции там раз в день какие то раз в неделю какие-то инсайты получаются только раз в месяц и как обратная связь подается наши real-time сервисом и применяется долго мы не можем это в первую очередь масштабировать его вторую это давать обратную связь костюмеру мгновенно это принципиальная схема архитектуры многие-многие архитектуры и коммерс похоже на это это микро сервиса да все классно все любят микро сервиса а также мы полностью не отказались от монолитов микро сервисы это скорее другой слой поверх поверх существующего уже монолиты классически классно за дизайне на классическая су архитектура какие-то коран и которые крутятся репорты так хорошо мне пока что ставится да то есть если спасибо и такое тоже бывает но я про то что репликация разваливается все все слышно хорошо да вот да это наша архитектура монолиты микро сервисы какие-то функции следующий tom to the faction за сервис возможно все это стекается все это коллег тица д.х. аналитика и подаём обратной связи долго не отвечает нашим требований который бизнес нам поставили чем основная корень проблемы что нужно внести чтобы добиться коммуникацию в реального времени в первую очередь нам нужно получать доступ к данным быстро и чтобы это не влияло на производительность это должно масштабироваться каждый сервис является мастером по каким-то данным он замыкает на себе определенные потоки данных и чтобы получить эти данные нам нужно обратиться к этому сервису либо как-то их оттуда извлечь и уже обрабатывать так вот классическом подходе в старом не классическом при синхронном взаимодействии когда мы запрашиваем по по сети синхронно и дай мне информацию поэтому к 100 минут на заказом поэтому каста миру мы не можем от масштабироваться потребителей это информация о том какие заказы указ номера есть их много они растут со временем с развитием бизнеса в конечном итоге наш сервис нас монолит или можем его распилить в итоге это все равно ярко базе данных не выдержат нам нужно вынимать оттуда эти данные как в первую очередь это определить потоки данных и перейти от а сейчас немного назад наша цель запустить real-time data processing для этого перейти от batch обработки к событийной чтобы по каждому изменению по каждому событию мы пересчитывали давали коммуникацию от синхронного request a response за к дата с премом и обработки обработки потоков и слою трансформации которые обрабатывают поток событий который гибко настраивается в идеале без вмешательства разработки давать бизнесу какой-то инструментарий чтобы вносить корректировки изменений и наша картина изменится вот станет вот такой у нас есть real-time сервисы слой обработки данных в реальном времени подаем в течение секунд не real time а близ как real-time у несколько секунд это гораздо лучше чем в операции в течение дня недели подаём обратную связь к нашим сервисом напрямую коммуницируем клиенту все счастливы и общая архитектура изменится таким образом всем нашим сервисом который коллектор данные замыкает в себе данные мы подключаем data stream и то есть мы определяем где потоки данные которые замыкаются в этом сервисе и просим его поделиться этими данными пропускаем через все сервисы data stream и то есть кто-то производит эти данные кто-то потребляют мы напрямую теперь не обращаемся к мастер системе теперь мы говорим что мастер если нужны данные по конкретному домену по конкретному контексту моих коллег и мог так называемом source of trance то есть это централизованное хранилище где централизованное хранилище это важный момент и to describe the transaction log в котором мы коллег тим все data stream и и уже если кому-то нужен какой-то определенный стрим обращается к нему и производит processing data processing как ну вы-то как ему конкретно требуется также 2-х может забирать оттуда данные но для того чтобы запустить real time and processing нужно некоторый подготовительный этап и которых я немножко уже сказал определить контекст и и контракты это про то какие данные и каким образом мы будем передавать через data stream и то есть определить доменные контекст и и контракты передачи этих данных это проектирование событий собственно с forms формировать потоки научиться их собирать в едином месте и the source of trance тоже распределенный транзакционный лог у это hotlog кафка для примера и у нас кафка и также научиться потреблять эти данные и уже потом обрабатывать то как мы к этому пришли это та часть архитектуры которая стала подспорьем для нового революционного витка определять что за данные мы стараемся придерживаться видите определение доменных контекстов каждый контекст этот поток и описание спецификации используя open api соловьем коллег эти данные через шину событий это паттерн вас и само собой здесь есть чем a registry как раз тот мастер по что за событие потому что события должны быть едины как производителем данных в нба сам шине событий и потребителям тема registry это источник о том какие события нас в принципе есть сам коллектор данных source of trance в нашем случае это кафка мы выбрали кафку мы коллег там там данным и легковесные костюмеры по требованию бог скажет бизнес задачи быстро поднимать костюме и уже переходить к этапу обработки трансформации данных это должно быть из коробки уже у команды отлаженным способом работать как пример как работает у нас на примере нашего у нас есть монолит ордер processing который я старый-старый давно-давно самого начала моды знает все о заказах классно эти заказы нужны многим монолит выглядит как замыкает на себе многие контекст и один из них доменный контекст заказы на продажу то что мы называем с ордер так как нам нужны нужно передавать эту информацию многим потребителям мы не можем синхронно запрашивать как я уже говорил нагрузка растет нагрузка растет на этот монолит неравномерно подход не неудачный мы сделали так чтобы этот монолит его один из компонентов начал продюсер во-первых события о создании заказа и также каждое событие на изменение состояния заказа event-ы стоит трансфер каждое изменение состояния тоже формируется в поток событий и собирается в нашем едином коллекторе картины все проще и уже потребители забирают оттуда данные масштабируются отлично увеличили количество кантемиров на каждую бизнес задачу и отлично не нагружаем наш монолит после этого уже можно когда мы с как олег тим и научились предоставлять быстрый доступ тем данным которые нам нужны для сложных для сложного процессинга для сложной обработки можно построить data processing player если кратко что он умеет он умеет такие это x-trans ii может трансформаций лот то что обычно есть и в на базе data warehouse of но только нам нужно в реальном времени умеет хранить estate для сложных трансформации сам слой трансформации должен быть прозрачным мы должны его уметь хорошо мониторить и смотреть что в нем происходит в каждый момент времени и а легких случае если что-то пойдет не так так как этот слой в и конкурс платформе это реальное взаимодействие с пользователем это касается реальных критичных битных метрик мои быстрый bootstrap это про то что это должно быть каким-то стандартным компонентом который каждая группа разработки может быстренько переиспользовать и здесь вопрос про свое решение свой велосипед или готовое решение конечно же на рынке есть полно уже готовых решений все к этому пришли можно так сказать вышли на плато мы столкнулись такие вопросы на которые нам нужно было ответить в самом начале мы не до конца понимали как и что мы хотим получить много о том что я сказал мы поняли уже где-то в середине проработки бизнес-задачи если выбирать готовые решения взять с рынка то как безболезненно интегрировать в нашу архитектуру ресурсы на эксперименты и как научить команды разработки взаимодействует с этим новым компонентом и здесь немаловажная часть это про неизвестный неизвестный когда мы берем новое решение мы не знаем мы знаем функциональности которым предоставляет мы примерно по экспортировали как он ведется собой но мы не знаем о тех подводных камнях которые ждут нас в эксплуатации если нет экспертизы по эксплуатации этого нового компонента и в моде мы применяем подход такой использование boring технологий есть статья такая-то горенко технолоджи это максимальное использование скучных технологии скучные они потому что мы все о них знаем мы знаем как они работают но они не приносят тех самых неизвестных неизвестных меньше всего таким образом еще немножко расскажу про какие готовые решения были это конечно же кавказ 3 морковка есть три мы на базе этого можно построить processing apache fling сильно останавливаться не буду но больше всего как оказалось нам бы подошел apache по функциональности именно апачи пульсар сильно тоже не буду акцентировать а потому что все дальше буду рассказывать это вот пульсар это делает вот у него есть те тот transformation leer который для нашего контекста неплохо подошел бы но мы решили использовать свой велосипед построить свой велосипед при использовать максимально те компоненты которые у нас есть развить подстроить под пилить их под то чтобы построить data processing и так случае source of trance накопителем данных событий у нас apache кафка работает отлично иногда может разваливаться у нас есть в не камер с платформы по большей части мы строим свои микро сервис у нас есть бойлер plate собственной это g'lang и мы из api сервисов я чуть позже расскажу сделали data processing service то есть это именно тот transformation layer который обрабатывает но и для хранения сайта мы используем просто как и для своих api сервисов воскрес как мы изменили существующие инструменты то это даже не в не фреймворк это такой бойлер при для построения грешных микро сервисов которые мы это обрабатывать рисков expanse request response какой то стоит базе данных ходить в другие сервисы все просто там есть api слой слой процессор процессор это именно та бизнес логика которая замыкается в этом сервисе слой хранение сайта и слой сервис который умеет ходить общаться по сети с другими сервис тоже нужно мы его немного изменили добавили туда чтобы он умел обрабатывать ивента вместо api слое у нас консьюмер процессор также остается это так вот именно функция которая можно выделить из отбросить все транспортные слои это именно та бизнес логика которую работает с данными продюсер если нужно какие-то data insight обратно продюсер в наш блог коллектор блок транзакций коллектор историй для хранения сайта таким образом мы получили transformation слой который умеет в первую во первых получать event который умеет хранить стоит который умеет запускать процессоры цепочки процессоров это обработчики то есть элементарно то бизнес-логика простая сложным которую нужно с помощью которым получаем data insight и продюсер новые данные которые потом наши real-time сервисы могут переиспользовать потреблять их и как сохранять как локальный кэш и обновлять их по мере поступления новых событий про это уже немножко сказал события меняют состоянии инициируют задачи про задачи я не сказал если немножко вернуться то каждое новое событие не напрямую вызывает обработку запуск логике а отложено разделяем консьюмер и непосредственно обработку данных там может быть сложнее это задача которая должна быть выполнены отложено по времени рекурсивно рекуррентно простить задачи исполняет процессоры цепочки процессоров последовательно конку конкурентно worker и запускают задачи это тот это был вор киров такой осмотрщик на задачам смотрится когда в конкретный момент времени какие задачи должны быть выполнены мой source на процессоре трансформируют данные все просто получаем event изменяем состоит ставим задачу worker исполнять задачи задача исполняет цепочку процессоров процессор работает со скейтом получаем новые данные продюсер новый ивент теперь используем а второй вариант использования первый был событийной пришло какое-то событие быстро посчитали получили новые данные используем но это расчет того что уже произошло одна из частей бизнес задачи мы должны еще делать прогноз тем что еще не произошло это корзина пользователя корзина каста мира это еще не созданный заказ но мы знаем из чего он состоит мы должны рассчитать и проводится родич костюмеру а как изменится вот тот твой процент как в какой сегмент и можешь попасть может не попасть как язвить изменится твой о состоянии если ты создашь этот заказ и например полностью его выкупишь это мы перри используем синхронно те же процессоры и на чем они хороши это такие обособленные функции которые могут работать как событийной моделью так из асинхронной модели мы берем state добавляем к нему вот тут дельту что-то новое прогоняем ту же цепочку процессором получаем новые данные немножко расскажу про performance того компонента который мы построили то есть это один из будущих многих то есть он решает конкретно бизнес задачу с персонализацией но на данный момент вот он обрабатывает 700000 событий в день 150 job ставит в день и всего этих задач job но они обрабатываются не не сразу они там отложено всего около постоянно в районе двух миллионов мы проверили нагрузочным тестированием это вполне выдерживает скиллинг надо x10 нагрузки чего нам пока вполне достаточно и текущая нагрузка на для реально коммуникации в реальном времени когда мы получили новые данные мы складываем их употребляем из нового то пока складываем локальную базу предпочитающие описки и предоставляем это чтобы наш учитель сочетали сейчас там порядка 450 рпс и среднего время ответа мне средняя это на самом деле это перцентиль им 95 99 10 миллисекунд так как там все просто это плоские данные которые уже посчитаны просто отдать они на самом деле хранятся в кэше вами типа больше часть ещё раз про performance немножко с с картинкой и как данный подход то что мы внедрили в в архитектуру всей платформы они отдельно чтобы поставили это будет перри использоваться для последующих задач нам нужно задуматься о том как мы будем балансировать нагрузку при росте при масштабировании всего и то есть на примере персонализации да смотри политика policy processing коллег коллег тим обрабатываем данные складываем их обратно в кафку передаем этого пешку она отдает то что там 450р ps сейчас но у нас также есть например service delivery и его песка которая в принципе сама по себе очень загружена в наших реалиях до 2000р ps мы не можем себе добавка добавить туда позволит добавить синхронный поход в какой-то еще сервис в опишу синхронно посетим и мы это тоже немаловажный момент мы согласились на то для масштабирования на где нормализацию данных публикацию контекста в данном случае карта мира в каждый сервис где требуется персонализация обновление они получаются событий на также все это растекается и работают как мы это мониторим я упоминал что слой трансформацию должен быть прозрачной для нас он очень критично собираем метрики про метался также сервисов логе собираем elastic баночки банок grove on и строим даже борды смотрим них с помощью cengiz делаем алё рта и рассылаем когда что-нибудь сломалась вот так выглядит барда сервиса полисе здесь как технические метрики так и немаловажные бизнес метрики которые конечно затер название но они на самом деле обезличен безличные то есть мы смотреть сюда понимаем что происходит если картина меняется то мы примерно можем понять в каком месте что-то пошло не так бизнесом мог изменить правила и случайно что-то нарушит то есть какие-то границы допустимой как что-то может меняться ну и соберу итог по всему тому что чем я хотел поделиться тем выводам которую мы пришли мы поставили данные на первое место мы начали смотреть и точно понимать какой сервис с какими данными работает какие данные оно продуцирует то есть не что это за сервис как он работает со скейтом а какой поток данных он замыкается и как он и может поделиться с другими начали быстрее обрабатывать данные то что раньше на базе пвх об считывалась сейчас мы это можем в e-commerce платформе на стороне интернет-магазина в реальном времени обрабатывать и это пока что масштабируется пока что мы с теми бизнес-задачами которые в ближайшее время будем сталкиваться и покрывают но и максимально и перри использование собственных инструментов чем плюс в том что и команды разработки им не так стрессово переходить на новые технологии можно вот смотри у нас есть вот такой сервис ты строишь его так теперь мы здесь кое-что изменили он будет еще выполнять вот такую функцию если бы мы принесли разработчикам команды разработка вот смотрите есть апатит пульсар у него есть вот api построение функции построения функции вот разберитесь почитайте документацию как это работает ну и мы еще сами пока не понимаем как это работает но и европе всего этого мы построили за три месяца небольшой командой разработки там это 23 человека включая меня попутно разбираюсь и совместно с бизнесом и с аналитиками чего же мы хотим спасибо на этом все а теперь у вас есть время задать свои вопросы прошел по поднятой руке мы колпаком и с микрофоном и вы спрашиваете желающие так можно задавать вопрос виталий сбербанк вот вопрос такой а что если вот у вас цепочка преобразование данных происходит что если где-то посередине этой цепочке произойдет ошибка вот как вас система реагирует и что будет состоянием база данных соответственно какой-то вопрос если я правильно понял то это про когда мы вызываем уже запуск цепочки процессоров выполняется 1 2 3 и 4 сломался 4 сломался дата но стоит мы меняем после успешного выполнения всей цепочки и если нет то это горки обработчик задач задача интервью цепочку то это откатывается задача не выполняется помечена как невыполнение невыполнимая и она может ошибки могут быть временные вам элементарно не мы смогли обратиться из applications of state of там как connect из базы данных потерялись то есть можно за ретро и тито и запустить заново точно также мы откатываемся и про проматываем это заново либо ошибка которую нельзя перезапустить то есть оно не при трайбл но для этого в самих процессорах мы помечаем какие-то трайбл какие не ретро яблок то есть этот стоит который шел пошло совсем не так это просто помечается как файл и у нас на это тоже есть метрики мы смотрим разбираемся с конкретно что там пошло не так вот первое это разделение на перед трайбл и не ли травил был ошибки спасибо спасибо за доклад скажите при переходе вам стало проще до божиться почтить что понял вопрос вот что имеется ввиду под и божиться а можно микрофон была система когда вы принимали запрос это им куда-то складывали и вынимали данные да и соответственно если вы хотели разобраться в чем-то сути какого-то события какого-то ошибки там с фронта там пришедшие то было в принципе понятных более-менее flow сейчас он многократно усложнился это не принесло каких-то существенных последствий усложнился он наверное там вот именно в слой трансформации именно поэтому мы туда заранее закладываем кто что он должен быть прозрачным и должны понимать что в нем происходит во всем остальном это все так же остается все те синхронные запросы от фронта мы также трейсер смотрим где именно он упал и ошибка может произойти и я самая сложность что у нас слой трансформации начнет коробки данные как-то неправильно считать и уже предоставлять это для читающих опишет читающая песка ничего не можете сделать либо отдает то что имеется опять же мы можем если совсем все почиталось неправильно у нас был такой кейс кстати когда мы это от стабилизировали решение у нас были ошибки в расчетах мы так как это событийная модель мы перечитывали все данные из тех же топиков которые нам нужны и запускали пересчет всего окей но я был в том conti контексте я что все ли разработчики могут в этот цикл взять и поправить какой-то благу если что или тут нужно все-таки глубокого глубоко понимать вот этот бизнес процесс почему цепочка процессоров такая джаббы запускают в такой-то последовательности нет ли там какой-то вот сложности насколько вы усложните steam безус логически для разработчика это важный момент именно поэтому мы сделали из api сервиса то есть тот слой обработки бизнес-логика процессора перенесли это no data processing service чтобы для разработчика было понятно на самом деле вот этот процессор функция которая делает абстрактная любую обработку на данных она никак не поменялось у него поменялось только контекст вызвало добавились цепочки да и вот здесь конечно же накладывается уже но то что нужно понимать что зачем выполняется как данные преобразовываться отлично алексей спасибо за доклад вопрос про хранилища для стэйта можете какие-то привести бенчмарки поттер по мансу и например когда вы выбрали на выбрали поздра вы сравнивали его с чем-то ещё там я не знаю с кем илью каким-нибудь и но вам хочется понять по производительности по скорости отклика почему он вас устроил потому что так для более нагруженных и более конкурентных схему как бы но однозначно мы смотрим на какие-то более быстрое киева или хранилища спасибо хороший вопрос именно в том что сейчас поэтапно на него расскажу 1 почему мы взяли postgres потому что мы его всегда берем для многих задач он подходит для 90 процентов у нас нет там это не в интернет вещей где мы обрабатываем миллионы событий пока что у нас ведут показывал пример и там не так много там за день 150000 событий по заказам происходит на это будет расти конечно же и пока чтоб им пост грез полностью справляется там не даст не такие сложные операции там то там хранятся агрегата нигде нормализировать там сам небольшая нагрузка на на транзакционных на хранение этого стоит достаточно плоские данные для чтения полностью мы используем там можно икике верю в хранилище использовать так возвращаясь про про бенчмарки мы не уперлись вот the capacity ресурсов чтобы задуматься о том чтобы на чем-то это другому сделать да мы предполагаем что это мы дойдем до момента где нужно будет выбирать что-то другое и развивать возможно тогда мы как раз дойдем до того что использовать может быть какие-то готовые решения и не зря у кота рассмотрел то что мы смотрели на то как в пульсар работает потому что рано или поздно когда мы начнем плодить централизовано мы стоим по бизнес-задачи здесь мы построили обработку данных здесь обработку здесь 10 потом это станет повсеместно и много и уже нужно будет смотреть на что-то готовое либо мы дорастем разовьем свой велосипед для до какого-то такого enterprise решения либо уже возьмем тот же пульсар более глубже погрузимся в него ну пока что мы не тестировали на capacity наши цифры настолько больше чем сейчас мы обрабатываем события а понятно можно ещё дополнить у вас есть где-то в этой схеме когда в синхронном режиме во что-то вы обновляете какой то стоит в подгрести и собственно ждете этого процессинге или нет или только синхронно но чтобы понять ну если можете какой-то назовите по которым который бог возрасту предъявляйте разделяем там у меня был по примерчик с тем что по событийной модели то что рассчитываем и хранилища стоит а для событийной модели там ну время реакции и обработки job они настолько критично то есть это не синхронный запрос от фичи сервисов от от сайта но мы так же строим а те же посчитаны данные и когда нам нужно к этому существующему state у добавить синхронно request и и обработать и сразу же отдать там мы классический так как мы используем это api сервис тот же самый процессор используем api request добавляем к этому стоит у он у нас раздельный то есть и там мы только отдаем рекомендации возвращаюсь мы не меняем синхронно state для трансформации player алексей и здесь детей спасибо за доклад такой вопрос можете пояснить что все-таки из процессов бизнес-процессов вошло в периметр вот рилтайм и обработки там пошло резервирование заказу управлении товарными остатками магистральные логистику финансовый учет ну то есть все все вошло в рилтайм или там все таки есть какое-то разделение спасибо спасибо нет конечно это мы добавили то чего у нас не было все что лтп сервис они так и остались то есть транзакционные сервис которые управляют резервированием capacity чего-то там склада доставки еще что то они также остались как и были нам нужно было научиться в дополнение те данные которые мы имеем именно проводить на на этой базе аналитику и подавать вот как обратную связь к нашим сервисом чтобы они могли ее пользоваться чтобы предоставлять персонализацию в что мы сейчас коллекции мы обрабатываем вот то что точно уже есть это поток данных по заказу изменения заказы состояние заказа с поток событий и действий костнер это комментарии и подписки какие-то крики в приложении там перед переходы еще что то такая вот информация те что транзакционные подарки они так и остались которые должны менять где необходимо astron консистенции то что называется на интервальная доставка например она вошла в онлайн вот эту real time обработка нет там и сейчас пока что ничего не анализируем в реальном времени спасибо еще желающие но в таком случае у меня теперь для вас небольшое задание у нас есть традиции то что каждый раз в ведущий выбирает лучший вопрос по своему мнению и этот человек получает небольшой сюрприз прошу играете пожалуй вот от андрея вопрос"
}