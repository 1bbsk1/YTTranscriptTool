{
  "video_id": "0EngnFMwh_Q",
  "channel": "HighLoadChannel",
  "title": "Распределённая обработка платежей с помощью Apache Ignite / Николай Кувыркин (Райффайзен Банк)",
  "views": 246,
  "duration": 3106,
  "published": "2023-10-06T07:18:14-07:00",
  "text": "да видимо я чуть позже расскажу про нашу чудесную систему пока чуть-чуть расскажу про себя что было понятно кто я почему я здесь и в общем Стоит ли вообще меня слушать Я в разработке больше 20 лет основной Опыт мой связан с базами данных из интересных проектов с которыми за это время успел поработать это мы например делали первый билинг для первых wi-fi Hotspot of в Москве в отелях Marriott был интересный проект вот так же после Я работал в течение 10 лет лаборатории Касперского где занимался там два интересных проекта был один распределенная файловая система для хранения образцов вирусов и второй проект Да вот вроде завелось второй проект это система детектирования файлов от листинг так как локально на продукте все данные хранить невозможно облачная система Вот такая вот вот последние три года я работаю в райффайзенбанке в отделе комплаенс там из интересных проектов собственно первый был проект это обновление автоматическое обновление данных о пользователях и второй проект собственно система Star kg потоковой обработки платежей о которых хочу сегодня вам рассказать Когда у меня будет пультик вот дальше надо наверное как-то листануть мне пока что нечем А вот Да собственно история отделка Чем занимается комплаенс банки комплаенс это структура которая обеспечивает работу банка обеспечивает соответствие работы банка имеющимся законом таким как всякие требования Центробанка росфинмониторинга и так далее в частности Для отдела комплаенс в банке у нас была разработана такая система Legacy система которая называется Stark Вот это рабочее место офицера комплаенс в котором он может одобрять или запрещать платежи если они не соответствуют требованиям законодательства вот сечением времени количество платежей росло требование там всякие усложнялись но при этом было ощущение что большая часть платежей может обрабатываться автоматически и вот эта система Старт начала обрастать автоматической логикой обработки вот под капотом у системы был есть Microsoft SQL Server соответственно вся эта логика реализовывалась на транзакции SQL с этим собственно сложности потому что тестировать сложно модифицировать сложно но самое главное сложность в том что вот в позиции человека который занимается развитием модификации системы он это позиция должна совмещать для ортогональные совершенно компетенции одна это бизнес компетенция он должен хорошо знать как Как обрабатывать платежи Какие законодательные нормативы Есть на эту тему Где взять данные там и так далее первая вторая Техническая часть он должен хорошо знать как работает Microsoft сиквел сервер изнутри потому что например при модификации процедуры которая там обрабатывает эти платежи что-то сделано не так там не знаю статистики не обновились планы поехали обработка стала чтобы этого не происходило в общем человек должен быть такой своего рода уникальный хотелось от этого тоже отказаться потому что в определенный момент он может стать узким местом Во всей системе вот требования к сложности правила обработки возрастали требования к потоку платежей тоже возрастали платежей все больше клиентов больше сложность больше а как бы модификации системы доработки дают всё сложнее поэтому было принято решение вот эту часть системы автоматического отселить сделать её по-новому хорошо Вот вот так вот собственно было принято решение такая предыстория вот этого проекта вот можно следующий слайд И значит вот требования какие появились к этой автоматической системе она должна высоко обладать высокой степенью параллелизма это легко достигается потому что в целом обрабатываемым платежи они абсолютно независимы они друг с другом не связаны Это хорошо горизонтально масштабируется стабильное время ответа Понятно вне зависимости от какой-то нагрузки система должна отвечать примерно одинаково Потому что есть жесткие ограничения на время ответа максимально наблюдаемость системы Legacy система разрабатывалась те года когда промониторинг и сильно много думали вот мы бы хотели этого избежать Мы хотим видеть что происходит и максимально быстро реагировать Если что-то идет не так Поэтому нам важно чтобы система для нас была прозрачной легкая быстрая модификация правил имеется ввиду что сейчас ну мир стал сложнее всякие схемы мошенничества новые появляются которые мы в том числе должны выявлять с помощью Вот как раз наших вот этих вот правил обработки эти штуки они довольно быстро меняются то есть мы должны иметь возможность выкатывать новую логику быстро и легко возможность ретро тестирования о чем здесь речь здесь речь о том что хорошо бы иметь возможность перед выкладкой новой версии системы проверить работу вот этой новой версии на истории и сравнить с тем Как отработала уже существующая система чтобы понять эти правила наши лучше хуже насколько лучше и так далее какие-то метрики по ним можно вывести и посмотреть если все нормально то выкатываться в прот обновление без простоя Понятно бизнес не должен страдать из-за каких-то технических вопросов которые мы там решаем или не решаем Ну и масштабирование ясно потому что количество клиентов растет мы должны на это реагировать в частности Например у нас был такой пример что мы так получилось что в реальный провод Мы вышли 24 февраля и в общем вот в эту дату она ознаменовалась еще и тем что у нас резко увеличилось количество клиентов относительно нормального количества у нас при выходе в пруд у нас сразу резко получилось такое стресс-тестирование а вот можешь следующий слайд значит Как происходит обработка платежей Ну собственно платеж приходит мы получаем данные об этом платеже там клиента переводит клиенту б такую-то сумму цель платежа такая-то условно для принятия решения о том что делать с этим платежом эта информация недостаточно нужно обогатить эти данные например Входит ли этот клиент список каких-то террористов которые мы Извне получаем Там и так далее участвуют какие-то схемах вот обогащение данных также для принятия решения модели пропуски платежа или нет важно чтобы модель где-то хранила свое состояние что имею ввиду у нас например есть такие сущности и ограничения как то например нельзя снимать в банкомате больше чем там 100 тысяч рублей там за день или там за пять дней неважно такие ограничения они позволяют то что Для клиента должны вот эти вещи они где-то сохранятся это тоже хранится в левой части и как бы передаются в модель с помощью вот обогащения данных это все мы подтягиваем дальше идет обработка моделью и собственно модель выдает ответ который мы выдаем внешним системам можешь следующий слайд и мы вот таким образом разделили нашу систему на несколько частей на две Первая это платформа это то что занимается сбором агрегации данных и так далее то здесь вся Техническая часть сосредоточена вторая часть это модель где чисто бизнес логика там ничего нету другого И нам Еще хотелось бы вот в требованиях забыл про это написать Сейчас расскажу нам бы хотелось сделать так чтобы модель могли писать люди которые хорошо знают бизнес который хорошо знает данные для них главный язык это в основном поэтому То есть как бы то что нам представлено зелёненьком квадратике модель Это чисто просто функция на питоне у которой есть вход на котором на который подается все данные Какие нужны для обработки Ну и в их собственно где она выдает своё решение и обновлённое состоянии по данному клиенту все модель ничего не знает Откуда берутся данные как их сохранять что там быстро не быстро индексы Нины вообще ей это все неважно Это чистая функция которая что-то получила что-то посчитала и ответила все остальное делает платформа еще в качестве Как бы такого подкапотного двигателя всей этой штуки мы выбрали APA Check Night сравнивали разные системы Вот выбрали почему выбрали что это вообще такое собачьи гнает это проект стартовавший в 2007 году компании Great Game начинался все дело просто как распределенный кэш киви хранилище которое постепенно со временем обрастала кучей функционала там Сначала появилась возможность сохранять данные на диск то есть персистентные клише какие-то делать подключать внешние источники данных потом там появились различные структуры с этой очереди там всякие А томики в рамках класте и так далее можно там делать под кластеры которые будут делать вычисления там где лежат данные появился сиквел появилась машинное обучение в общем это такой как-то как бы швейцарский ножик в мире обработки данных Вот это очень понравилось я расскажу позже Почему нам нас это в какой-то момент спасло вот собственно выбрали почитать может следующий слайд архитектуры В общем довольно простая У нас есть быстрые источники информации в качестве них выступают очереди собственно по ним мы получаем данные о платежах сами платежи и туда же отвечаем есть медленные источники данных это datale X различными витринами которые не в реальном времени строятся обновляются там раз в сутки Раз в час мы по готовности к себе подтаскиваем есть имеющиеся уже система она здесь почему показана потому что не все можно обработать автоматически автоматика покрывает там 90 98 процентов 99 честно цифры не помню Ну много но не все есть спорные моменты где нужно участие человека поэтому когда модель отвечает что я не знаю что с этим платежом делать эта информация передается в легасе систему где человек комплаенс офицер может посмотреть всю историю По этому клиенту и принять решение Что с этим платежом делать Вот собственно вот так вот это все устроено можно следующий слайд А так это выглядит схема развертывания у нас как бы вся система разделена на три такие большие блока каждый из которых имеет свой релизный цикл свое назначение и так далее первое самое фундаментальное Это серверные ноды игнайт вот они наверху показаны Это самый такой фундамент нашей системы эта штука практически в релизный цикл не очень длинный то есть мы редко делаем диплои в основном это обновление версии либо там добавление какой-то ноды в кластер либо удаление Там и так далее вторая часть вторая и третья часть это клиентские ноды и ноды моделей это все работают в кластере кубера клиентские ноды это то Где реализована логика платформы собственно это то что общается с внешними системами обогащает данные и передает собственно все это интерфейс осуществляется с моделью вот а модель это собственно вот эти функции как вот аналог функции от амазона просто некая чистая функция которая диплоится в кластеры там как-то масштабируется в зависимости от нагрузки и выполнять задачи по применению этих правил к платежам а можно следующий слайд Что из себя представляет Ну вот клиентская нода Ну на самом деле много моделей то же самое Это приложение которое реализует логику обвешаны со всех сторон сайт караме видела файлбит менеджер клуб State Matrix и прометеос wildbit отгружаем логи Альт менеджер как бы сигнальным если что-то у нас пошло не так состояние пода Ну и про металлов как бы метрики все нам доступны в котором все это дело крутится там значит у нас куча технологий это И сейчас уже становящийся постепенно Legacy системными объемам куб Потому что сейчас в связи с ограничениями лицензионными многие компании от этого отказываются но как бы еще вот это используется на замену ему приходят Артемис Ну Кафка slack в котором мы собственно читаем все наши сообщения от Alert Manager LK Понятно посмотреть логин Ну и графа на через прометеос нам доступны метрики в которых мы понимаем что у нас как работает можно следующий слайд по производительности мы тестировали в банке Не ну как бы понятно что в крупных организациях есть Ну принятый в организации решения например там Каким образом разворачиваются кластер кубера вот у нас в банке кластеры решениям Варе мы его потестировали оно нам не подошло потому что среднее время ответа за счет этого их балансировщика nsx стандартным образом в банке настроенного нас не устроило мы развернули свой кубер на виртуалках там время нам показалось более приемлемо Поэтому в плане разработки такого рода систем важно посмотреть что есть и как бы подходит это не подходит тезис по максимуму использует то что есть это всегда Здорово потому что не нужно самим делать в данном случае у нас к сожалению не получилось для наших задач это не подошло но собственном классе Все нормально можно следующий слайд Теперь значит по конфигурации всей нашей системы серверные ноды платформы это те ноды на которых собственно хранятся данные работает опачек Night они в общем вот такие ну минимум Понятно два экземпляра чтобы хотя бы какая-то отказоустойчивость была обеспечена на данный момент у нас пять нот в каждой ноде 8 процессоров 32 ГБ памяти 250 Гб SSD в целом Мы на этом хорошо работаем отказоустойчивость всей вот этой шарманки обеспечивается средствами Апачи гнать то есть он сам следит за тем чтобы данные были доступны при недоступности каких-то нот можно следующий слайд кластера кубера для платформ и моделей собственно такое тоже видно что не Мега мощная система Вот это с учетом того что мы тестировали можно любую ноду мастер надо Понятно worker ноду можно прибить у нас как бы ресурсов хватает на то чтобы все работало следующую ноду можно ой Следующую страницу из клиентские под и платформы и модели Ну понятно чтоб хотя бы один экземпляр был запущен чтобы обработка как бы шла и для них характерно то что они не хранят состояние можно они все одинаковые можно их как угодно масштабировать В зависимости от нагрузки Ну и собственно отказоустойчивость в данном случае обеспечивает средствами губернатис важная вещь что в применении пачек на это посмотреть как он работает нужно собирать метрики чтобы было видно что происходит с кластером метрики были переписаны по-моему два или три релиза назад версии игнайта и там сейчас стало прям все очень хорошо то есть там можно посмотреть вот Ну не знаю как он левой рукой правую пятку чешет буквально То есть он вываливает очень много нам мы используем gmx Вот Но фильтруем что Он забирает все как бы все не отдаем потому что мы просто не успевали забирать все метрики которые мог предоставить Ну и собственно мы смотрим несколько метрик по кластеру вот они здесь показаны зачитывать не буду Просто кто Кому интересно можете потом посмотреть слайды в принципе вот этих метрик по кластеру по нодам и по кэшам на следующих слайдах будет их в целом достаточно чтобы вот прям вот просто ты посмотрел и тебе понятно работает что-то или нет Если нет то тоже понятно как чинить можно следующий слайд Вот это по нодам Метрика и еще следующий слайд там будут покашам метрики вот такие вот в принципе этого хватает чтобы понимать что системой творится можно следующий слайд технологии какие мы использовали в при разработке нашей Системы ну на платформе это патч игнайт Понятно начинали мы с восьмой Java потому что изначально и гнать написано 8 и как бы вот решили чтобы вот чтобы нас как можно меньше проблем смущало стартовать с того на чем он был написан потом перешли на 11 буквально недавно перешли на 17 кажется что вроде как переход в простой но на самом деле с точки зрения Вот именно инфраструктурных вопросов такие переходы не всегда получается нормально потому что например 17 javy там сильно усилили шифрование Если у вас есть какая-то кермера с авторизацией с какими-нибудь кавками Там и так далее с этим могут быть проблемы То есть как бы вроде кажется просто поменял и все помчали но не всегда это так Spring Q это технологии всем известные кайф для доступа к витринам а модели значит что про них можно сказать у нас изначально мы стартовали как бы с пониманием того что А мы хотим чтобы модели были написаны написаны на питоне модель обработки платежа и б мы ничего не хотим знать как это куда-то в какой-то кластер как это выкладывать как это масштабируется и так далее У нас в банке есть такой продукт который называется мрм Tool эта штука очень похожа на amazon лямбда функции просто это как бы ты в Гид кладешь условно Файлик который на вход на выход треснет и все дальше ты про это забываешь он сам уезжает кластер как-то там это масштабируется По какому По какому-то тебе доступно это функциональность Ну как бы вот эти можешь пользоваться стартовали именно потому что можно было запуститься сразу не тратя время потому что было куда еще время потратить но у них очень сложная модель диплои была поэтому мы сначала попробовали а потом перешли на флаг который нам показался наиболее меньше модификации нам пришлось делать И сейчас все вот оно работает и именно диплоиться Именно так как нам это нужно потому что у них очень сложная это общий банковский проект Вот это мрм-тул и там сложная какая-то система версионирования какие-то свои конвейеры которые под себя никак не подстроить В общем Мы перешли на свое собственное сейчас вот как бы вот так естественно как язык используется поэтому всякие правила машинного обучения они готовятся в дата лайки Ну и там целый зоопарк технологий питон Спарк Ну вот всё как бы вот это вот можно следующий слайд а можно следующий слайд способы доставки платформы и моделей на прод Ну везде максимально типовые варианты мы используем это Типовая себя У нас в банке как стандарт тренингит labsi вот там ничего такого нету естественно все должно быть автоматизировано потому что как только у вас где-то используется кубер без автоматизации Ну там повесишься примерно на второй минуте вот разворачивается все средствами платформы без простоя нажал на кнопочку все уехал на прод понятно что есть тесты версионирование там и так далее единственное отличие для эмэль всяких вещей которые работают Зато лайки в том что как бы эта зона ответственности смежных смежной команды поэтому обязательно ревью от команды Data Lake можно следующий слайд теперь с чем мы столкнулись когда мы у нас есть готовая система Как выходить в пруд вот как сделать так чтобы для бизнеса это было Ну прозрачно незаметно и все вот у нас как бы имеющая система работает таким образом есть Входная очередь по ней мы какие-то платежи получаем есть имеющиеся система и она что-то отвечает в их на выход всё хорошо все довольны но мы хотим новую систему подключить как это сделать можно следующий слайд Мы берем и как бы делим входную очередь на две и запускаем новую систему в параллель со старой при этом игнорируем полностью Результаты работы новой системы но мы знаем что у нас и в имеющейся системе и в новой системе мы естественно все сохраняем банки все сохраняют то есть какой платеж был какой платеж был от Старой системы ответ на платеж какой был старой системе ответ на платеж какой был в новой системе У нас есть возможность сравнить то Ну данные на вход одинаковые выход у нас есть и там и там мы можем посмотреть и Понять насколько новая система соответствует ожиданиям Все ли в ней так хорошо ли она работает или нет Если как бы нет мы имеем возможность подкрутить там эти правила там модели поправить еще что-то сделать чтобы они встали ну как бы наиболее близки по функционалу друг другу тоже понятно что правила там не видно не видно такого что там два плюс два равно 4 там какие-то ивристики где-то там такие лимиты где-то другие что-то по разному считаются это такой вот ответ скорее больше статистически посмотреть там похоже не похоже распределение ответов новой старой системе может следующий файл следующую картинку а когда мы убедились в том что новая система работает не хуже чем старая мы просто берем от Старой системы отключаем очереди А выходную очередь новые системы переключаем на Реальную боевую очередь этот процесс занимает Ну в реально впроде это занимает но может быть там блин несколько секунд Вот и таким образом мы не аффектим никого то есть работала старая система в какой-то момент стал работать новая система вот этот подход Мне кажется он вполне логичный и удобный Потому что это не аффекте бизнес можно следующую картинку теперь хочу рассказать о проблемах с которыми мы столкнулись в процессе работы разработки этой системы их основных две это как оказалось у нас могут случаться массовые платежи от одного клиента клиенты Это обычно юридические лица типа Озон Wildberries Яндекс ну и так далее здоровые компании в которых какая-то автоматика раз в сутки раз в неделю не знаю Там два раза в месяц выплат зарплата не знаю что они совершают массовые платежи там десятки тысяч платежей приходят одномоментно Ну и вторая проблема об этом тоже расскажу можно следующую картинку значит как обычно работает система платежи они друг от друга никак не зависит то есть разные клиенты что-то делают параллельно Никто никого не цепляет все все довольны у нас там ну сколько угодно потоков может быть мы запускаем столько чтобы нам хватало обрабатывать текущую нагрузку А можно следующую картинку что случается если у нас приходит один и тот же клиент как вот случай рассказывал есть там какой-нибудь там Яндекс или Озон делают платежи У нас есть сущность Для клиента которая хранит всяких состояния Ну например там сумма переводов там за 5 предыдущих рабочих дней там сумма не знаю там переводов насчет по маске вот такой-то за какой-то период там куча разных агрегатов которые считаться должны в рамках одного клиента строго последовательно иначе будут будет как бы неправильно читаться условно там у тебя есть 100 рублей ты берешь переводишь сначала 50 и потом еще 50 если все пришло в один момент если не блокировать то у тебя первый поток взял 100 -50 и второй стой минус 50 А по факту переведено 100 как бы это неправильно нужно очень такую информацию чтобы правильнее получать цифры вот случае если приходит массово куча платежей у нас получается такая картинка что вот сколько бы мы потоков не накидали хоть там тысячи 2000 Ну там реальный случай приходит 50 тысяч платежей То есть это что 50 тысяч потоков при том что они будут все работать последовательно потому что они ожидают вот этих вот данных которые должны последовательно обрабатываться подход плохой Сколько бы мы не сделали потоков это не поможет Это еще не самое плохое потому что для юридических лиц для них в целом неважно latency то есть понятно что здесь они так как они последовательно в одну секунду никто не ответит они там как-то обработаются им важно чтобы за сегодня это была обработано А вот для физических клиентов когда ты там в телефоне что-то отправляешь куда-то деньги тебе важно чтобы ты сразу получил ответ и вот если вот пришел такой юридический юридическое лицо которое ощущает такую кучу платежей то пока он не обработается по этой схеме физики вообще обрабатываться не будут все остальные вообще в очереди стоят и как бы у всех ничего не происходит а его тут как бы я вернусь к вопросу Как нам помог Апач игнайт в Апач игнайт так как это такой швейцарский ножик там есть прям структура данных которая называется распределенные очереди в рамках кластера и вот как раз этим он нам и помог А может следующую картинку а можно следующую картинку видимо нельзя А вот да мы вели Два счётчика клиент inflight Counter и Lock Time Lock Time считает время залочки вот этих чувствительных Для клиента данных если какой-то поток понимаешь что данный заложенность слишком много значит что где-то работают другие потоки которые которые как бы захватили эти данные если это время превышено либо текущее количество платежей Во всей системе превышает какой-то порог мы считаем что это пришел такой сложный клиент который массового платежи отправляет и мы просто берем не обрабатывая не ожидая ничего все следующие платежи мы отправляем эту вот отложенную очередь при этом мы не блокируем все остальное обработку клиенты BC спокойно на остальных потоках живут Вот когда это очередь полностью для этого клиента обрабатывается как бы все эти счетчики оказываются сброшенными как бы следующий платеж от клиента а он идет без всякой очереди что стандартным образом потом опять как только навалило раз в очередь пошло вышло то есть такая система как бы адаптируется такого рода нагрузки вот в этом нам как раз помог а патч игнать потому что там есть такие структуры данных которые можно для этого Использовать можно следующую картинку а теперь Значит про Split Brent чего хотелось бы рассказать это вообще сложная ситуация потому что выходить из него достаточно проблематично то есть обычно это ну либо повторно какая-то обработка данных которые были испорчены В общем восстанавливаться сложно что из себя представляет Brain splitbrain представляет то что класть разваливается на две части связь между которыми потеряна но внутри этих частей ноды нормально связь поддерживают вот здесь показано распределение э-э данных э-э в кластере как игнать их хранить данные он в конкретном кэше он бьёт содержимое на по умолчанию 1.000 партийцы и распределяет их по кластеру для каждой для кэша можно задать сколько быкапов дополнительно к основному набору данных хранится вот здесь они есть показательная ситуация когда одна мастер партиции две БК партии вот он как-то так вот их размазывает можно следующую картинку и при выходе вот этих вот НОД 102 они не видят ноды 3.4 аноды 3.4 соответственно не видит ноды 1.2 но при этом связь между 1.2 сохранена и между 3-4 сохранена эта ситуация бывает когда Например у вас сервер кластер размазан по нескольким датам просто связь между центрами пропала и локально живут две половинки кластера в данном случае мы видим что на каждой половинке у нас присутствует все данные которые нужны для работы и гнать не будет работать только в том случае если какой-то партиции для каша нету тогда он скажет Конечно недоступен всё в данном случае мастер партийцы есть исчезает если доступны бэкап она помечается как Мастер и продолжаем дальше работать вот в данном случае это сто процентов флирт Brain будет а можно следующую картинку Более сложный вариант Когда у нас пять нот а можно перелистнуть Да вот такая ситуация тоже данные размазаны по кластеру таким же образом и кажется вроде бы у нас одна мастер партиция две быка партиции кажется что если две ноды выходят то на них никак не Соберется всех данных потому что как бы есть сто процентов где-то три остальные ноды на которых будут лежать и мастер и вот два бэкапа то есть при любых раскладах У нас все хорошо На самом деле это не так можно следующую картинку вот если мы посмотрим выйдут ноды 1 и 2 Мы видим что партийцы 3 на этой половинке нету то есть в данном случае все хорошо слит Брейна нету левая часть 1 и 2 они просто как бы не работают та часть работает все нормально Ну а можно следующую картинку еще есть например выйдут из кластера ноды 1,3 Мы видим что здесь у них полный набор данных можем собрать и А это все нормально отличный сплит-брейн то есть ноды 2 4 5 один кластер но до 1.3 второй кластер работают параллельно вот такие штуки они зависят от чего вообще возможность предрейна она зависит от того как сколько ну от конфигурации кластера сколько нот в кластере раз и как настроены бэкапы для кошей два Ну и три соответственно распределение распределения партий по нодам А можно следующую картинку вот здесь вот приведены вероятности Сплит Брейна для наихудшего сценария наихудший сценарий это когда Ну понятно что при конфигурации Вот например пять нот и одна Мастер и два бэкапа это как бы говорит о том что количество вариантов распределения партиций по нодам оно равно количество Ну пока где это количество нот кайт количество копий данных в данном случае три вот количество и как бы понятно что для 4 нот это сто процентов при такой конфигурации для 550 процентов для шести ну 20 для семьи такой нету то есть такой конфигурацией Сплит Брейна не произойдет еще раз повторю что это для худшего варианта худший вариант когда количество Ну когда выбрано одно сочетание со всего возможного набора то есть вот допустим Ну все все да все все данные все партийцы распределены по одному паттерну Ну в случае например пяти Но для одной мастера двух бэкапов количество паттернов распределения партиций равно 10 Ну это собственно если один из них присутствует на всех партициях то вероятность будет 50 процентов если больше то она соответственно снижается если данные вообще распределены ну то есть все возможные размещения такие вот присутствуют в кластере то вероятность будет ровно Ну этого размещение сложно добиться потому что этим рулит сам игнайт и нельзя сказать как данные конкретно распределены в данный момент вот собственно вот так вот с этим бороться количество нот настройки размещения данных этим размещением данных можно рулить Ну так вот условно там одну ноду выдернуть дождаться ребаланса включить он опять что-то там перемешает ну такой периодически сломанный кластер чтобы он перераспределил данные но сто процентов гарантировать нельзя потому что этим заниматься иначе он там под капотом делает сложно сказать можно следующий слайд последнее значит О чем хотел рассказать так как мы как бы ну все-таки чтобы не отрываться от земли мы простые крестьяне пишем код хотелось бы показать пример которого собственно в документации по игнату нету ну-ка на которой наткнулись и успешно используем понимаете есть такая штука Как которая называется conquery эта штука используется для того чтобы из США выбирать данные используя какой-то фильтр выглядит фильтр Вот примерно Вот так вот Вот он там один функциональный интерфейс там значит метод apply он смотрит какое-то поле если там больше 100 то это значит что эти данные вернутся клиенту Значит что данные клиенту не вернутся вот эта вся штука вот этот код надо важная тут что он работает не на клиенте Он работает на каждый серверной ноте в этом плюс фильтрация осуществляется не так что все в начале себе и здесь что-то фильтруем у нас не памяти не хватит ничего не хватит вот это все работает локально и параллельно на вот этих вот нодах игнайта ну и собственно вот scanquery используя фильтр вот он вернет курсор по которым можно по данным по этим пройтись А следующий можно кадр вот ой предыдущий Вот вот так вот можно сделать удаление данных из кэша мы инжектим на каждой ноте будет подставлен соответствующий класс объект класса вот этого игнайт вот и выполнен вот этот вот фильтр но в фильтре при совпадении условия мы делаем не Return True типа возвращаем какие-то данные что флаг что мы возвращаем эти данные А мы просто удаляем как бы по данному ключу эту запись и всегда возвращаем false потому что нам на клиенте данные не нужны и тогда вот этот вот запрос он будет как бы удалять данные в кэше очень быстро потому что это всё как бы работает локально на нодах вот этот фильтр и он там отрабатывает мы так вот Чистим в кластере например ту информацию которая как бы нам больше не нужна вот собственно следующий слайд резюмирую Все я хочу сказать Вот что основные как бы что хотелось бы вот по результатам работы над этим проектом высказать что вот ну стандарты стандарты Вот это главная штука чем это хорошо тем что как бы грабли уже все Там собраны вероятность наткнуться на какие-то новые Она довольно мала и есть возможность использовать уже принятые стандарты в организации Ну например что касается различных мониторингов логирования Там метрики и так далее вы просто подключаете вы не пишите ничего Сами вы берете Готовое что уже есть у вас через пять минут начинает работать это очень удобно не надо свои велосипеды изобретать если что-то есть надо использовать то что есть правильный выбор технологии Ну как бы вот на этом мы не ошиблись потому что вот эта система когда вот массовые платежи мы обрабатывали Нам очень помогло Что изначально было ощущение что нам понадобится такие распределенные структуры данных которые есть вы знаете и Они нам реально понадобились то есть мы эту проблему решили не переписывая с нуля вот такой вот использование приняв в организации Практик разработки Ну понятно Это то что касается там не знаю того же самого диплоя принятых там средств мониторинга и так далее тестирование применимости в конкретном проекте идут как бы Как пример то что вот у нас есть кластеры в январь Ну вот нам он не подошел то есть решение нужно брать но проверять насколько они вам реально для вашего проекта хороши подходят Ну тестирование в режиме параллельной работы это тоже вполне понятная штука это очень удобно потому что При таком раскладе есть возможность исправить возможные проблемы системы до фактического аффекта неправильной работы на бизнес А это очень опасно потому что ну например в комплаенте если что-то идёт не так штрафы там реально чудовищные для банка как организации То есть тут грубо говоря какие-то назовём это косяки они не надо их допускать это плохо Вот поэтому параллельно Можно какое-то время поработать все сравнить добиться удовлетворительных результатов которые будут уже сразу видны Ну и потом переключиться незаметно для бизнеса и продолжить работу Вот наверное следующий слайд Можно мне кажется что это последний Да вот наверное все что я хотел рассказать Если есть какие-то вопросы буду рад ответить спасибо очень интересная тема и вопросов я думаю будет тоже очень много здравствуйте Дмитрий Удалось ли придумать какие-то мониторинги на возможность подсветили Что возможен В некоторых случаях да возможно то есть Например графане можно и найти есть такая штука он может текущий топологию показывать метриках это всё можно смотреть и если например Циферки поменялись Ну условно у вас пять нот стало четыре или там стало три и две то есть вот за этим можно следить Да это можно всё сделать настроить на это и получать их вот как бы можно я как понял с доклада данные могут лечь Так что какие-то ананады выйдут из кластеры произойдёт Да совершенно верно зависит от распределения так как распределением партиций в классе занимается именно сам игнайт это под капотом это может меняться от версии к версии у них нету Как сказать Ну это как чёрный ящик вот он работает как-то размазывает данные всё доступность обеспечена То есть я бы не полагался то что он вот именно так распределяю может как угодно количество распределений посчитать можно можно для каждого варианта присутствия в классе распределения Посчитайте эту вероятность Она легко считается Вот но надо понимать что как бы вот оно может быть любым Каким будет Черт его знает зависит от логики которая в конкретной версии Игната реализовано Вот и то есть закладываться я бы рассматривал Так что есть вероятность больше больше нуля то как бы это нам нужно что-то делать Я из доклада вынес что первое игнайт независимо от нас распределяет То есть если мы поставили ему 10 минут он будет это теоретически невозможно В принципе может на три заселить остальные не использовать у нас будет следы Если выйдет Две из них да Ну да вот то есть нам важно как данные уже легли Ну да важно в целом то есть мы конкретно как они там легли мы сказать не можем Но мы можем прикинуть для разных вариантов их размещения вероятность того будет бренда или нет Вот полагаться на это тоже потому что мы не знаем Можно наверное можно там порыться посмотреть где какие партиции куда легли Но это сегодня так завтра Он может переложить там например много где-нибудь сетка ребаланс случился он все перераспределил То есть тут как бы я бы вот смотрел с точки зрения вероятность не нулевая отчет с этим уже надо делать потому что он вполне реально зависит от того тут же еще смотрите Какая штука сам по себе это просто свойство распределенных систем они вот так устроены и тут надо Смотреть насколько вам вообще Ну например если вы можете потерять данные там за сегодняшний день они настолько важны Возможно у вас там какие-то метрики в обработке Ну там обрабатывали там 99,9 процента сегодня правильно обработанных ситуаций расстояние 99 Game потому что ну а не все клиенты в это время долбились когда этот Сплит Брайн случился в этом возможно платежи были которые как бы ну в плане у нас там платежи про платежи поэтому платежи которые вообще ничего не нарушит Там как бы они как Как прошли там так и там ну как бы то есть вот эти штуки как бы насколько вам важны ваши данные потому что понятно что данный типы важны Но не для всех систем важного стопроцентная гарантия того что данные как бы вот прям для всего правильной есть какие-то штуки которые Ну там как-то вот чуть-чуть оно не сработало но в целом всё хорошо то есть у вас такая система то может быть Сплит бренд ничего И такого просто там ну надо отключили завелись за заново часть данных чуть-чуть потерялась какая-то Ну и ладно может это ни на что и не повлияет потому что ну что касается платежей например во время Сплит Брейна пришёл там клиент сделал платеж а потом месяц не ходит Ну не делает платежи этих данные которые как бы даже неправильно посчитались они уже в историю Все скинуть они уже не в каких правилах Они уже не играют То есть как бы система не пострадала хотя мы данные потеряли А все работает дальше нормально Может наоборот начал массово платить и все стало как бы ну то есть системе complines допускается Вот такая вот нет как раз у нас не допускается Я просто про то что система разные очень тяжелые длинная тема Я вам предлагаю её обсудить чуть-чуть попозже уже в дискуссионной зоне А у нас ещё есть вопросы да Кирилл Данилов нас Пока Спасибо за доклад а там была отложена очередь я не совсем понял это создается Под каждый конкретный случай либо это все-таки статический набор отложенных очередей попробовали сначала создавать Под каждый случай К сожалению вы знаете Это не очень работает тоже динамически создавать удалять не очень хорошо Он какой-то момент если много таких становится А много можешь стать там например что-то сетью где-то в классе вот эти вот счётчики увеличиваются и все идут в эту очередь это редко но такое бывает вот э-э значит мы сделали просто ну как типа хэшмапы то есть у нас есть э набор этих очередей мы по хэшу от ключа в какую-то очередь попадает и там они просто набор вой- воркеров по этим очередям работают фиксированные наборы очередей Понятно Сейчас вот как бы так сделано А если бы задание знаете что у юридических лиц и физических лиц совершенно разные как бы заказанные SLV для обработки платежей зачем их в принципе пытаться в одну систему запихать то есть не получили сразу юридических лиц все в отложены Мы в серединке цепочки обработки платежей у вас нет признака Юрик это или есть да Так они точно сразу все юридических запихивать в эти очереди и пускания обрабатываются как смогут не все Таким образом мы просто лишние ресурсы будем на туда-сюда в очередь из очереди таких клиентов на самом деле не так много их там штук 10 может быть 15 которые вот прямо под них завести 10 статических очередей и всех туда же динамика нужна да плюс в том что она сама понимает когда надо в очередь а когда надо как бы не очень то есть идея была именно в том чтобы система сама училась вот этим вещам Понятно вы еще упомянули что у вас рыба пачек найтана растянут на разные цода видимо географические распределенные В чем смысл Я помню что даже у меня был простенький кластер он умудрялся разваливаться в одном цеде У нас капец у нас очень сложно сейчас играет прибить Мы специально прибивали ничего Зачем класть на несколько центов то есть нам надо чтобы Мы работали железно потому что ну не могу говорить Почему Вот Но нам нельзя падать в общем поэтому несколько цодов Ну в смысле сделать не на Ну в смысле переключиться с одного отсюда на другой но недостатки вот к власти до падче Night Ну к власти так на это наверное Очень чувствительный проблемам Нет он как раз он вот игнает это штука которую вот у нас сейчас убить очень сложно реально хорошо Видимо я отдельно вас помучаюсь много вопросов спасибо спасибо Так у нас еще есть там вопрос Здравствуйте спасибо за доклад У меня вопрос у вас был слайд на который который я не понял как вы его объяснили вы говорили про разделение кластера Но ведь вроде бы решается формуле формулы n пополам + 1 и вроде бы не должно быть те кто выпал они не могут собрать вором решается эта штука А можно вернуться на тот слайд и объясните пожалуйста Да можно отлистать Ребят можно полистать обратно там где семь узлов да еще и вот здесь Наверное нет следующий вот этот 7 узлов где мы написали что там ноль процентов да да это просто из распределения с как бы можно посчитать эту штуку при 7 узлах это все считается количество сочетаний из НБУ к порядок там не важен где N Это количество нот а к это количество ну Мастер плюс два бэкап в данном случае три вот при такой это При таком количестве сочетаний при любых распределениях Но это можно я вручную не считал я написал на питоне какую-то программку которая мне вот это все посчитала То есть она смотрит при выходе вот любых двух нот вот нету таких двух нот чтобы на них были все данные при любых распределениях по партиции Вот такая штука получилась но в целом как бы похоже на правду Спасибо вот точно последний вопрос Вот я вижу поднятую руку нет ну и там значит у меня коротенький вопрос Вот вы показывали на одного слайдов время отклика системы 80 секунд это вместе с обогащением данных это все вместе но в таком случае Насколько сложно у вас обогащение данных то что вас участвует у нас как источник данных мы все что чем обогащаем оно лежит локально вы знаете в этом как раз смысл потому что ну то есть вы заранее подкладываете вопрос это комплимент То есть просто я помню что это сложные проверки есть и нужно тащить практически всю базу и еще раз извини но это complines дата есть условно говоря проверки онлайн есть проверки там на подозрительные операции есть проверки которые мы должны отчитываться перед финмониторингом и последний пул он достаточно сложен для того чтобы весь память поднять Ну там же используются которые предрасчитывают какие-то Ну допустим у вас есть проверки из разряда проверить текст назначения платежа не является ли это получение акции Понятно А какой объем памяти это занимает все выгнать вот у нас пять нот сейчас по 250 гигабайт диски Ну и занято там наверное на данный момент мне кажется процентов 40 может быть Вот по всему этому кластеру вот более в оперативной памяти то есть мы Ну он как работает всё то что оперативная память когда включен персистенс она работает как некий кэш с наиболее поздней вытесняются с диска поднимается то что там сейчас нету и так далее То есть это вот как бы схема такая работа игнатия Спасибо пожалуйста это маленький вопрос Да друзья Спасибо вам за ваши вопросы Николая Огромное спасибо за такую интересную тему потому что действительно очень важный инструмент который действительно стал стандартом во многих программных решениях давай выберем самый понравившийся тебе вопрос и подарим подарок Давайте самому смелому какому Дмитрий Да я правильно помню Да вот Дмитрию подарок от Райффайзенбанка Николай тебе тоже ты не уйдешь отсюда без подарка маленький презент от конференции хайло супер пакетик Спасибо огромное Если есть какие-то вопросы Я готов ответить друзья Добро пожаловать в дискуссионную зону и там можно Николай задать может быть какие-то более серьезные более глубокие Вопросы Спасибо большое было очень приятно с вами пообщаться Да ничего главное"
}