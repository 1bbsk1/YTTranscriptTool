{
  "video_id": "5Rlbo309aOI",
  "channel": "HighLoadChannel",
  "title": "Технологии хранения для больших проектов / Сергей Платонов (RAIDIX)",
  "views": 2421,
  "duration": 2319,
  "published": "2018-08-16T04:57:04-07:00",
  "text": "меня зовут сергей платонов я работаю в компании редекс и сегодня я хочу поделиться с вами нашим опытом соучастия в каких-то больших проектах и в первую очередь рассказать о том как делать не надо так или иначе наши инженеры компании редекс нашими партнерами привлекаются к работе над теми или иными проектами и я расскажу как какие проекты выполнялись в каких-то больших инсталляциях в рамки каминах таки как нас таких как дисней что пытались туда предложить вендоры почему это не всегда работала и как поступали заказчики в конечном итоге ну давайте начнем начнем наверно и самого близкого пример одного из из последних компаниями назвать не смогу находится соединенных штатах америки это компания которая занимается биотехнологиями у них был классический такой речь писи кластер небольшой на 100 not но надо были хорошие все 4 socket ные что вычислителей множество традиционного программного обеспече такой классический классический high performance computing кластер объединенные высокопроизводительной сетью infiniband 56 гигабит и все приложения тоже как классические суперкомпьютерные хорошо скоординированы и обеспечивающих хорошо скоординированы отек общей системе хранения данных на этапе 0 uah файловая система хранения данных сцен fs и он начал понимать что файловая система хранения данных становится для него узким местом и он начал искать себе альтернативное решение объемы написаны пол петабайт а хранимых данных при работе вычислительных комплексов генерируется около 150 терабайт временных файлов которые практически полностью удаляются через какое то время но и обрабатывали объем данных среднем несколько десятков терабайт войди появилась какой-то момент денег он пошел по интегратором первое предложение которое мы выдали это было предложение от компании дмц это нарисован такой типичный пример этого приложения денег у него все-таки было недостаточно и анти решил построить свой собственный им все easy он используя открытое программное обеспечение и количество некое количество серверов которые подогнал им интегратор как раз таки и продав ему ответил из чего состояла его решение но в первую очередь этот набор серверов которые инди решил использовать одновременно как для вычислительных задач и так и для задачи хранения данных несколько коммутаторов 10 gigabit ethernet и распределенные хранилища это не только файловая система это такого не фиксированная они еще думаю многие вы про него слышали цех на тот момент это был cfd релизная версия с поддержкой blue star энди достаточно грамотно инженер он хорошо разбирается он читает вроде как разобрался со всеми подводными камнями цеха но не все у него пошло х в дальнейшем хорошо а почему он убил бы на cf в первую очередь решение условно-бесплатна и он хотел отделаться малой кровью цех обеспечивает высокую доступность данных на тот момент для файловой системы пустим совместимой поддерживал столько репликация данных и не поддерживалась рыжика thing но это вроде как казалось что достаточно система обещала быть масштабируемой узел узел и достаточно гибкому но ты могла бы добавляться удаляются из единой системы переводиться какой-то их статус ну и построил свой решение доделан это около 3 месяцев и столкнулся с некоторым набором проблем первую очередь от лице пришлось отказаться потому что на dc для файловой системы тогда работал нестабильно трехкратная репликация для полу петабайтов данных + запас под scratch сделал решение очень дорогим производительной записи в которую напейся составила 4 6 гигабайт секунду хотя изначально ожидал на стендах он получить около 20 гигабайт секунду а в режиме когда некоторые приложения работают режиме антуан он получил производительность около не несколько сотен мегабайт секунду тем самым даже не превысив и начальные показатели которые были у него на нфс шаре и будет игра ты предложил ему купить еще в каждую породу по флэш-накопитель у использовать еще флешки наши партнеры обратился к нам тоже совершенно безумной идеей с задачей а давайте мы продадите туда программный рейд по которым станет пацев и возможно и ускорит производительность наши ребята подключились начали задавать вопросы и одна из самых первых вопросов которые который такой типичный самых первых атлетов и зачем вы использовали именно распределенную систему хранения данных по типичного spicy нагрузку до нее отпечаток у меня очень много данных пол петабайт а данные быстро растут хотя вы видели растут они всего на 100 терабайт в год и это одно из общих рекомендаций которые я даю в последнее время это все таки очень внимательно относиться к тому как сейчас развиваются система хранения данных в том числе классической модульной архитектуры сейчас на обычной системе даже начального уровня петабайт данных на 2 контроллерах и как на каком-то наборе дисковых полок с плотной высокой нагрузке спокойно можно разместить петабайт полтора-два петабайт а и обеспечить производительность 67 гигабай секунду на системах среднего класса и медаль что high-end 15 байт секунду в общем стенде мы полностью переработали архитектурой расскажу что там будет дальше но перед тем как приходи дальше расскажу что первую очередь анти понимал неправильно есть три типа файловых систем которые создавались именно для решения задач больших кластерных системах хранения данных так называемой параллельные файловой системы хранения данных классные распределенные сейчас происходит некий сдвиг и разработчики параллельных и кластерных файловых систем хранения данных пытаются из своих решений сделать так или иначе распределенные происходит это потому что меняется в первую очередь именно паттерн приложений и приложение которые работают совершенно другой парадигме никогда данные доставляется к процессу а когда наоборот процесс опускается к данным появляются все чаще и чаще именно поэтому разработчики файловых систем пытаются реагировать на этот тренд и создавать какие-то свои решения все подобные сессии файловые системы они обладают общей набором функций реализованным в той или иной степени в первую очередь функция прозрачность все эти файлы системы должны работать для приложения точно также как локальная файловая система и никак от нее не отличаться раньше разработчики файловых систем боролись для того чтобы приложение не совершенно не знали где данные лежат конкретно на каком узле в какой части сети сейчас опять же связи с тем что они стараются сделать систему распределенную как-то бороться за локальность дали данных пошел обратный тренд и часто разработчики файловых систем предлагают приложение информацию о том где данные лежат конкретно данные лежащие на распределенных систем и должны быть естественно консистентной и консистентных данных распределенных системах достигается гораздо более с ложными способе чем локальной файловой системы потому что появляются дополнительные угрозы как классические угроза кэширования когда нам необходимо держать консистентные конечно множестве узлов и параллельно записывать данные в одни и те же области вопросы решили они решаются тремя методами это подход warm когда файловых вновь записаны файловые объектах становятся доступными только на чтение больше не обновляются такие подходы как распределенной блокировки и лизинг другой вид проблем которые с которыми встречаются разработчики распределенных классных параллельных файловых систем это вопрос когда приложение каком-то узлу открыла файл и забыла его закрыть либо крашнулось и у нас остается открытым файлом также есть проблема когда какой-то из узлов начинает пытаться получить доступ к данным напрямую не обращаясь к серверу метаданных села сервер блокировок и так далее и со всеми этими задачами необходимо бороться и разные производители бороться с ним разными образом масштабируемость ну конечно все подобные файловой системы обладают каким-то уровня масштабируемости системы должны быть так или иначе устойчивых к отказам обеспечивает балансировку нагрузки как для новых данных так и для старых ну и конечно предоставлять каким-то образом дон доступ к данным для приложений пока я буду в первую очередь именно про работу с данными по классическим пасека его интерфейсом так мы поехали дальше когда же плох такие решения которые использовал и иди а именно распределенная файловая система архитектуру shahid на pink heart нафиг обозначает архитектуру система распределенных когда нас нету никаких общих ресурсов так как есть общая сеть через которую различные сервера взаимодействуют между собой одесские памятью систем и независимо друг от друга но в первую очередь архитектура шея нафиг крайне плохо когда есть очень высокая истинность записи это более 80 процентов на самом деле нужно считать для каждого конкретного случая но так или иначе распределённая природа системы всегда несет за собой свою стоимость и всегда нам необходимо записать данные минимум на два узла а то и три и дополнительная всегда загружают interconnect и создается много дополнительных района запись и системы с распределю распределенной архитектурой при большом количестве записи всегда медленнее чем какие-то системы шер диск централизованным хранилищем нагрузка которого как раз эти характерно для классических писи когда есть приложение которые пишут параллельно множество узлов и а хорошего стань скоординированной также обычно плохо ложатся на фидерную системы хранения данных ну и конечно когда требуемая производительность одного приложения значительно превышает производительность системы хранения именно этого узла на которого расположена и нам не удается локализовать и и это тоже одна из очень частых ошибок с которыми я сталкиваюсь это попытка например создать систему хранения данных по да очень жирную базу данных соединив несколько систем хранения данных с помощью кластера небо параллельной файловая система не чем это хорошим обычно не заканчивается энди было предложено как а конкретная классической архитектуры с параллельной файловой системой чем же характерной параллельные файловой системы параллельные файловой системы они обеспечивают доступ к одному набору данных множество серверов при этом доступ каждому конкретному файлу на каждом конкретном сервере тоже пора лица клиентское по а клиентская по является обязательной частью проявления файловой системой эффективно распределять нагрузку кластеры обычно это большого размера до нескольких сотен тысяч узлов и зачастую метр асимметричные архитектора то есть всегда есть какой-то внешняя система хранения данных взаимодействия системы хранения данных идет не напрямую часть сервера файловой системы и с серверами файловой системы взаимодействия клиенты и также есть выделенные либо встроенный сервер метаданных которые содержат всю информацию о том где находится конкретно файлы каких узлах куда данные отправлять информацию о блокировках и так далее при этом взаимодействия от клиента с серверами файловая система передает напрямую то есть если вы видите на нижней картинке как происходит запись и чтение файлов сначала клиент обращается к серверу метаданных получать всю необходимую информацию куда данные записать не покуда их считать и потом уже напрямую на сервер файловой системой выполняет запись либо чтения ну и конкретный пример архитектуры на базе по полисам мгновенно популярнейший файловые системы параллельного типа это люстра на же правильно заяц аластер с максимальным элементами так внизу у нас есть клиенты и на клиенты всегда обязательно установленное клиентское программное обеспечение но может быть многослойным мастер не обязательно работает спаси исключительно способ стадиона поддерживает а другие протоколы доступа есть выделенная сеть так называемый интернет или кратко л нет это специализированная сеть которая выделяется для взаимодействия клиентов серверами хранение эту сеть 1 нужно отдельно настраивать отдельные тюните это тоже очень непростая задача есть сервера метаданных долгое время was the last поддерживала только один сервер метаданных находящийся в режиме отказоустойчивого пар из недавних времени мастер стала поддерживать множественные сервера метаданных несколько десятков для того чтобы обеспечить масштабирование нагрузки по именно по серверу метаданных вообще если вы увидите есть не конкретный предел на один сервер метаданных это не более десяти тысяч создаваемых файлов секунду на самом деле это не только создаваемые файлом секунду но количество открытий закрыть их конкретных файлов и кластерный пример плохо себя очень ведет если ваше приложение очень часто открывает закрывает файл с сервера метаданных обычно с этим не справляется дальше у нас есть сервера объект на его хранение так называемые сервера объектного храните называется именно серьгами объектного хранения потому что все файлы разделяются на кусочки на клиентской части эти кусочки записываются в объекты хранящиеся на серверах объектного хранения вот ну и сервера объектного хранения использовать какую-то файловую систему специализированную это либо open cfs либо л дисков с которой основано x3 и хранят уже в рамках локальных файловых систем свои фалик и подключаются обычно внешней энтерпрайз уровня системе хранения данных и вот эта архитектура очень часто пугают многих клиентов которые говорят что стоимость мастер и очень сильно зависит именно от стоимости это провести системы хранения данных отдельно стоит поговорить про распределение файла все файлы делится на так называемые чанки stripey вот эти stripe и записываются по множеству объектов находящихся на множестве объект не серверов хранения данных содержится несколько типов стриппинга если опять же говорить про мастер 22 основные политики стриппинга это round robin и коз как они действуют round robin действует работает на всех объектах которые доступны той или иной папки политики распределения данных по объектам можно настроить на уровне папки которые не которые на всех объекты на всех файловых системах локальных которые заполнены менее чем на 17 процентов это значение по умолчанию которая настраивается в данном случае файл делится на чанки размер которых задается при настройке папки и эти чайки отправляются на множество серверов по политике round robin как только мы превышаем 17 процентов то включается другая политика это политика косо политика коз занимается тем что распределяет чанки по порядку наименьший заполненности локальных систем локальных файловых систем такие параллельные файловой системе как диджеев с отдельно для приложений иметь свой api который позволяет приложение указатели как настроить политику на уровне конкретных файлов и более тонко настраивать распределение данных что же было предложено иди и какое решение предложил нас партнер ему помогали ему строить и так мы решили отказаться от не от распределенные файловые системы и построить систему именно параллельную на основе мастер часть серверов которые у него были изначально закуплено в мы стали использовать в качестве контроллера в хранении по тучек не внешние g воды за счет того что мы использовали программный рейд мы смогли отказаться от внешней системы хранения данных используя локальные диски которые отличаются по созыву собираем сервера работает режиме активные активные пары могут переключаться между собой в случае отказа опять же эта архитектура помад мама помогла нам защитить долгосрочность решение потому что иди еще очень боялся за ты за то что у него может измениться парадигма и может появиться приложение когда нужно процесс опускать до файлов и в данном случае за счет того что мы не используем какие-то внешние системы хранения данных а обычные сервера мы смогли использовать именно это решение а основной набор пол хранение данных мы хранили на той же cf только уже с использованием рыжак 1 га скорость там никакая не требовалось и используем специальное приложение которые занимались перекидывание данных нато классическая экспрессия архитектура между scratch файловой системой и архивной файловой системой в качестве сети мы использовали infiniband 100 гигабитный из ас для подключения дисков я про как я уже говорил вообще сэнди это было уже не первая попытка и года три назад мы уже начали использовать подобные архитектуры под из подобных же архитектуры разработали стоит система европейскому исследовательском центре в городе юля х5 же отказ от внешних систем хранения данных позволяет значительно сэкономить решение авто и в том числе размещать какие то дополнительные предложения непосредственно на серверах ранение вот отказаться можно тремя способами от внешней семьях и не дано мы использовали программный рейд который умеет мигрировать между двумя узлами благодаря intel у можно использовать рейды которые встроены внутрь файловой системе в open zfs либо есть программа аппаратная реализация контроллера а до ласса и centro каких результатов мы достигли мы в три раза увеличили производительность и снизили стоимость в in структура примерно 65 70 процентов а люстры не всегда можно использовать есть несколько типичных ошибок которые с которыми опять же начать первую очередь одна из типичных ошибок это попытка использовать люстру в качестве основного хранилища просто очень плохо себя в этом чувствует как кластеры невозможно рио балансировать только новые данные пишется на новые сервера надежные схемы n плюс 1 1 плюс 1 для чехлов кластеров вызывает сомнения ну и конечно также люстра очень плохо себя ведет для ника типичных задач например к остальным виртуальных машин либо кластер hadoop несмотря на то что люстра бодает hadoop коннектором использовать его как alter основное хранилище для mapreduce задача очень проста опять же за счет того что данная распределяется по какой-то своей политике и собрать собирать их нагружая сеть очень непросто и дорого с точки зрения сети другая задача опять же и писи кластер это задача лиг от компании наса уже была выбрана решение и решение файловой системе на обед fps он же обеим спектром скал основная задача была построить новую систему хранения данных у нас опять же есть проблематика того что у них два разных типа приложений должны работать с одним и тем же набором данном это как классические приложение суперкомпьютеры так и новые приложения так называемые новые типы аналитики когда необходимо опустить как при обработке больших данных процесс к данный максимально близко сам объем дпс поддерживает и варианты развертывания то система хранения данных как раз таки от найти которой относится ко всем трем типам она является и кластера она является параллельное она в том числе является распределенной системы хранения данных работающий только с локальными дисками нас от из тела все три возможных способа к сожалению классический вариант когда схд подключается ко всем узлам и мы им не подходит просто потому что так устроена сеть и кластеров очень много вариант с распределенной системы же не данных они тестировали опять же столкнулись той же проблемой что энди производительность было получено около одной трети от того чтобы возможно получить по оборудованию поэтому они используют классический прием когда есть часть серверов так называемая сервера нсд который экспортирует виртуальные диски на множество клиентов файловой системы и мы им с построением этой инфраструктуры помогали вообще обеим жив и gpf с версии 35 обладает локальными ритм системами так называемый не tiff rate it именно для того чтобы можно было опять же отказаться от каких-то внешних систем хранения данных и использовать контроллер хранения не только как не только для хранения но и для того чтобы на них выполнять какую-то работу но несмотря на то что эта функциональность появилось и было заявлено именно как возможность поддержки любого оборудования обеим очень жестко ограничен или то оборудование эти дисковые полки с сервера на которых найти фрейд работает и наса не получилось построить какую-то инфраструктуру именно с этой решение что было сделано по-умолчанию оборудования и тачки это является корпоративность стандартам наса сервера hp apollo 4520 пистолете еще и себя отказоустойчивые по пары были подключены к внешним независимыми j годом вот на каждом таком возле было сделано по 6 рейд схеме 28 плюс 3 и сервера нсд который экспортирует наверх виртуальные диски которые необходимы клиентам были установлены на виртуальные машины и также виртуальные машины были проданы адаптеры используя технологию 70 руб виртуализация вот но и производительность такой система которую мы достигли было где-то 17 сгибая секунду на один уже в таких узлов было 6 3 проект как раз таки которые относятся другой типа файловых систем этой очень интересных это японская национальная фра структуры для супер компьютер у них совершенно другая задача у них несколько раз системных площадок в которых необходимо хранить 50 петабайт данных эти данные должны обмен между площадками как-то шарится при этом приложении которое используется в национальный из писи кластеры как раз таки следует парадигме нахождения процесса максимально близко к данными но опять же необходимо было обеспечить максимальную пропускную способность и система хранения данных в японском кластере было и сейчас используется файловая система она очень интересно я впервые столкнулся именно там никогда раньше при не слышал так называемый вид фарм fs или в сокращении g-form fs основная идея этой файловой системы заключается в том что на множестве узлов имели на которых разберу да какая то локальная почем совместимой файловая система устанавливается дополнительные модули который позволяет объединить множество этих локальных file system и в единую консистентную систему ранее данных обладающий очень высокой производительностью доступа доступность данных обеспечивать двумя способами либо опять же используя какой-то внешняя система хранения данных которая обеспечивает отказоустойчивость либо используется репликация при этом это репликация в синхронном режиме работают в рамках одной площадке и в асинхронном режиме работает рамках между площадками не буду уже подробно рассказать про архитектуру времени очень немного это как времени нога расскажу ещё про несколько интересных и ней и смешных решений решение для видеоаналитики 4 петабайтов данных необходимо было для того чтобы хранить данные приходящие с камеры в пригороде сеула это решение которое использовалось власть полицейский кими полицейским управлением при города сеула во-первых требовалось гарантированно пропускная способность у них есть внутреннее регламенты которые требуют хранить данные не менее 30 дней и обеспечить от к защиту от потери потоков из-за того что система хранения данных просела по производительности ранее опять же не использовали распределённую файловую систему и распределенный нас который их не устоял именно по производителю как только камеры были заменены на full hd они клиентская часть которая была установлена на серверах виде аналитики перестала справляться с возможно с скоростью записи на необходимо на нас и они искали альтернативное решение также было два варианта альтернативное решение в первую очередь это используйтесь распределенную файловую систему которая могла бы быть встроена в microsoft так называемый стоишь прессы старик либо использовать класс новую файловую систему остановились они на классной файловой системы по одной простой причине причина заключалась в том что были слухи о том что систему необходимость масштабировать во дворах будет необходимо смасштабировать в два раза в течение следующего месяца и они испугались того что каждому конкретному серверу пришлось бы подключить по j буду вместо того чтобы единовременно увеличить какую-то из центральную систему хранения данных они выбрали именно использовать какую-то централизованная система хранения данных а для того чтобы обеспечить одновременный доступ к этой цифре зоной система в ней данных от множество серверов они используют платными файла систему костанай файловой системы отличается от параллельных и распределенных тем что кластерной афро вой система напрямую взаимодей забота это блочной системы хранения данных и позволяeт множество серверов синхронизируем между собой блокировки синхронизируя между собой статуса и о обращаться к какой-то единой блочная система хранения данных при этом блокировки возможны как на том так на файл так и на чанг и а с точки зрения парализация доступа то он практически отсутствует под хвостом и файловые системы они просто объединяют имеющиеся блочное стройся точно также как делает это ваш обычной локальной файловой системы например mtf с поверх динамических дисков и ничего более она не делает у меня заканчивается время к сожалению ним не могу рассказать про последний такой самый странный кейс но я если что могу в поворот про него рассказать время 5 вопросам вы можете задавать вопросы поднимаете руку я принесу вам микрофон спасибо меня зовут алексей по первому кейсу индии хотел бы уточнить в этом скалишься пять процентов экономии я-то понимаю что это стоимость железо плюс проекта внедрения по сравнению с обычным массивом а да это именно 100 это стоимость именно это точнее так это стоимость железо с проектом по внедрению по сравнению со стоимостью железной распределенной системе стоимость вот этой архитектуры по сравнению с той что была изначально вот этой спасибо здравствуйте меня зовут анда и все что вы сказали чрезвычайно интересно я примерно по центру зал и и вот хотя бы уточнить по такому кейсу как учет понимающего сейчас довольно распространенное использование файловой системы cf начнем систем хранения на базе цех да в которой это может сделать мощное устройство это загрузка сие тех же виртуальных машин а так делают лоб instek соответственно что вы думаете о подобных кейсах их перспективности о том насколько серьезный процесс на массаж можно сажать такие виртуальные машины и как все плохо это будет работать смотрите на самом деле но давайте этом если не касаться конкретно цеха и тех проблем с которыми сталкиваются люди цех в общем и целом системой неплохо если научиться ее готовить сам подход я в него верю если это говорить про какие-то там стандартные виртуальные машины которые не требуют серьезной производительности доступа к note если это виртуальной машины на которых не крутится приложения типа sopa и прочего это просто набор приложений которые используют обычный небольшое предприятие то вполне себе верю в эту архитектуру архитектура себя показывают неплохо последует по следующим параметрам первых стоимость внедрения и поддержки когда мы и имеем единую когерентную систему когда у нас одни и те же сервера и нам необходим нет необходимости иметь отдельно по администраторов которые администрировать системы хранения данных администратор в которой отдельно администрирует сын и отдельно по ул администраторов которые администрирует виртуальную структуру а есть просто набор администраторов которые вместе все вместе у нас один тип железо у нас постоянно мы получаем за счет того что у нас один тип железо облегченную поддержку все конечно прекрасно но есть несколько но 1 но если доступ на запись работы наших приложений которые на готова к крутится очень развит то мы опять же столкнулся с проблемой постоянные миграции данных по кластеру это раз два это то что стоит ожидать чем устроить быть готовым кластер через год перестанет быть симметричным сейчас оборудования развивается очень быстро если вы посмотрите например на то те же флэш-накопители который выпускает в этом digital intel они каждые полтора года становится два раза быстрее она становится кластер полностью асимметричного как вы понимаете при неправильной работе с кластером он будет работать по производить эти самого медленного узла поэтому нам нужно быть очень аккуратным именно статус продеваем от распределенных систем нам не очень хорошо понимать куда мы что кладем и как-то изолировать новые узлы с более значительно более производительными устройствами от старых то есть придется делать скорее всего несколько постеров да спасибо пожалуйста добрый вечер зовут артем я хотел задать следующий вопрос я как бы backend разработчик для меня of the file system министра этот мне далеко ваш доклад бы очень информативный потому не успел впитает но у меня возник вопрос мы работаем с докером вы знаете систему до заснять рисует какие то есть решение например для системы логирования которые построены в докер-контейнер потому что у нас ожидается довольно большой объем данных в плане логов их придется как-то распределять как бы на несколько возможно машин или даже кластеров какие файловой системы применяются с докер контейнерами при больших объемы данных смотрите но с точки зрения больших объемов данных опять же и файловых систем которые применяются можно опять же обратиться к тому что исходит от fx м здесь используется очень часто несмотря на все его недостатки по моему процентов 70 это точно когда речь идёт об obtain стеки речь идет о а докеров то используется именно цель файловая система вообще по до под до контейнерную виртуализацию есть несколько проектов которые создают систему хранения данных которые заточены исключительно под контент виртуализации к сожалению сейчас не вспомню именно вот так вот сходу ну вот прям можно поискать стоишь по докер и есть несколько стартапов есть которые там кто-то использует open source про диму распространения кто такой чисто корпоративная планом действительно можно найти несколько неплохих проектов которые создавались именно потому что появляется докеры по то что докер из своей этом запросы работе с данными спасибо еще вопрос будут тогда предлагаю поаплодировать спикеру как раз таки та же касается кластерные файловые системы так и нарисовано кто это вообще дисней дисней пришел к нашему партнеру это производитель аппаратного обеспечения и другому партнеру компания который производит кастовую систему задачи такой у них есть несколько распределенных географически площадок и есть 1 сот находящийся в центре так вы начнете распределенные площадки должны работать с единым набором данных который сад в центре хранит около трех петабайт эти три петабайт они распределены по трем уровням зависимости от от производительности и необходимо было обеспечить максимально быструю доставку данных на удаленные площадки и и обратно для того чтобы у множества людей работающих на удаленных площадках был единый вид данных размер а это несколько сотен килобайт но и требуется максимальная пропускная способность на удаленных периферийных площадках и начальное решение которая была и которая до сих пор остается основным центральном соде использовались раз таки им хейзел он 3 3 типов она удаленных площадках для и все а и зелон использовались на шлюзы который его буфере zero valio и и в дальнейшем каком-то каком-то об видео передавали их в центральные площадку решение в общем и целом было неплохое но почему-то как только на шлюз заполнялся более чем на 50 процентов все начинало тормозить привлекли нашего партнера наш партнер вместо того чтобы разбираться предложил очень интересное решение и пытался привлечь нас как поставщика как раз таки и а из kosis эстор уже до этого решения мы отказались это делать и вот почему что предложил наш партнер параллельно производителям классные файла системы в тот поставить несколько систем хранения данных которые экспортировали бы по протоколу а из case и через внешнюю сеть неважно какой пропускной способности но на расстоянии около 300 километров айс кайзи блочные дома там же в центральном судья разместить сервер метаданных для костанай файловой системы не буду называть и обеспечить возможность обращаться с удаленных площадок этим мастера метаданных ну и по протоколу а из case писаете данные ну понятно дело к чему привело во-первых постоянно были потеря доступа к данным потому что а из case через 70 километров это то ещё удовольствие постоянно разрушалась файловая система потому что не невозможно было гарантировать на таких расстояниях доставка общей информации лаках а лиза и так далее ну и конечно что больше всего нравилось поставщику классной файловой системы 2 2 файла система обычно не лицензируется по клиентам а так как клиентов было несколько тысяч вот эта стоимость решения по сравнению с изначальный по моему превысила процентов 800 вот и только производителю классные файла системы уходило около 20 миллионов долларов я не знаю что не курили ну дела проблема в том что это дошло до пилота это не решение на бумаге это местный был пилот совместно с этими компании понятно дело проект закончился не успехом вот и до сих пор у некстати решение нету мы в этот проект входили с просьбой давайте мы посмотрим на то что происходит сейчас почему где а почему происходит тормоза и давай и будем искать разбираться с конкретной причиной но как это промежуточное звено которая хотела продать всячески нас не допускала для проекта до заказчика насколько я знаю сейчас проблема и осталась возможно вот как раз таки g-form fs про который рассказывал могла бы одесную помочь потому что задача именно такая есть центральная площадка с большим объемом данных и необходимо обеспечить распределенные хранилище ну в общем все получилось несколько соборов потому что действительно подходов и информации очень много но так или иначе надеюсь у вас какое-то появилась информация в голове что читать дальше куда копать дальше потому что каждая файловая система каждый подход требует очень такого вдумчиво осмысления и только при работе с мастер требуется потратить наверное там не менее месяца просто на чтение лучшего опыта чтобы это все заработало"
}