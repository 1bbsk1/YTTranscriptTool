{
  "video_id": "JSLhU_LJ5VE",
  "channel": "HighLoadChannel",
  "title": "Паттерны хранения и обработки данных в ClickHouse / Александр Крашенинников (Badoo)",
  "views": 17742,
  "duration": 3107,
  "published": "2020-04-27T12:24:43-07:00",
  "text": "ну что ж давайте начнем вот и сегодня прикольные интересные штуки можно сделать с использование клик хауса меня зовут александр крашенинников я рукояти когда-то инженеров паду я люблю всякие крутые кластерной распределенной штуки оля клика узко зуб и иже с ним и я contributor пиклз о чем мы сегодня с вами поговорим мы рассмотрим конкретные кейсы который мне кажется потенциально вы можете с ними статуса и как самому принцу и просто музыка играет поэтому подтанцовывать так немножко например уварим по систему продуктов аналитик и поговорим про bts ты проговорим про time series и pro говорим про anomaly detection которым я уже не первый месяц просит рассказать поговорим про удаление данных и есть некий бонус трека секретный чтобы на миг ли хаусе мы знаем что он не тормозит при соблюдении ряда условий которые мы себя называем нормально делай нормально будет он обладает различными симптомами big d это такими как sharding репликации и прочее и он сделан в компании яндекс а это означает что это продукт достаточно высокого класса потому что там работают далеко не глупые люди когда нам хорошо подойдет когда у нас достаточно ощутимый большой поток данных когда мы хотим хранить их не только в чине этого дня но когда мы рассчитывать долгоиграющую историю мы знаем что crack house это провал об нагрузку что превысит письмо хотим использовать нам нужно где нормализовать структуру и мы знаем что у нас нету транзакционные в понятие бегин комет и прочего так рассмотрим один из моих любимых примеров я очень долго работаю над этим продуктом уже неоднократно рассказывал про него но тем не менее еще раз вкратце скажу и так у нас есть система продуктовой аналитики и которые в себе накапливает различный бизнес события происходящие в системе регистрации голоса загрузку контента пользовательского ну и разумеется ошибки потому что это важный показатель здоровье проектов чем хотим от нашей системы мы должны уметь принять событие обрабатывать их и предоставлять функциональность olap запросов мы знаем что у нас будет существенный поток данных и нам важно чтобы систем блогер распределенный потому что в наших реалиях у нас два до центра и мы хотим обрабатывать данные в двух местах что со представляет продуктовой аналитика в нашем случае это более 1000 различных бизнес событий и у них совершенно разная tribute вко вопрос задачи как все это добро мы могли бы сохранить крик хаусе ну само собой напрашивающийся решения которые возможно лизать на любой электронной базе данных это завести под каждый бизнес события свою табличку соответствующий атрибут севка и так далее и в принципе это правильный паттерн использование в клик хауса но он нам даст максимально производительность запросы будут по этим таблицам очень быстрые но есть одно ограничение что в условии когда вас будут сотни таких таблиц и в них действительно будут интенсивные вставки то это будет анти паттерном великороссы и у нас начнет деградация функционал selecta inserto и в общем все станет абсолютно плохо вариант который мы придумали мы называем его generic к таблице потому что мы на самом деле храним атрибуте фку событий в неком киева илью то есть в двух массивах в одном массиве нас хранятся ключи атрибутов в другом значении и когда мы не интенсивно льем в эту таблицу нас не деградирует insert потому что это одна таблица и на нее меньше накладных расходов чем на куча куча маленьких вот примерная структура то есть у нас есть имя события да так оно произошло и какие то ключи и значения то есть регистрация на платформе android в россии у мужчины случилось вот такой вот так событие кулак на можете crack house разумеется у этого варианта есть свои недостатки в частности в такой системе хранения будет хуже вид select потому что мы храним данный вне в предназначен для это столбцах там винты еще что-то булевые типы а в массиве строк это не очень хорошо и она существенно хуже сжимается что соответственно будет на диске у нас занимать больше места но принимая решение о том чтобы использовать такую схему мы знали об этих недостатках и понимали что для нас это будет допустимым следующий штуку которую важно иметь в виду когда мы говорим продуктов аналитику что объезда хотел бы знать настоящее положение дел то есть мы не можем несколько раз за дубликатом регистрации или какие то другие бизнес метрики нашего проекта поэтому на думать о гарантированной доставки где можно сломаться можно сломаться на сервере когда клиент посылает запрос на insert и а то люкс например по таймауту мы не очень понимаем прошла вставка или нет и другой вариант мы можем получить информацию о том что ставка прошла но почему-то клиент не сохранил эту информацию уже на своей стране клиентов много они все разные и бороться с ними кажется как пчелы против меда поэтому попробуем что-то сделать со стороны сервера и так у нас есть некий источник именованный это может быть топик исков к это может быть файл это может быть таблица в реляционной базе данных то есть источник и у событий в этом источнике есть некие порядковые номера то есть черных кафка например это будет офсет и вставляя события мы будем писать имя источника и офсет события в этом источнике вот примерно так будет у нас выглядеть то есть мы видим что у нас появились две дополнительные колонки совершенно не связанные с бизнес значениями the source найме source of set и мы туда засовываем порядковые номера событий а перед тем как мы будем обрабатывать источник заново мышц придав табличку сделаем получим информацию о том какой последние события туда вставили и пропустим просто часть данных из источника выглядит довольно круто и похоже что нам удалось сделать однократной доставки данных но тут пришел сибирский пушной зверек и выяснилось что вот эти вот а все ты у событий они занимают нас львиную долю место в базе относительно полезной нагрузки за счет того что последовательное значения монотонно возрастающей плохо сжимаются в лоб различные алгоритмы компрессии и эта колонка мы можем считать ржать и не в то время как для других полей струга типизированных и так далее клик house отлично справляется с задачами зажатия мы придумали очень элегантный трюк как мне кажется мы всегда пишем 0 для всех для всех ивентов а у последнего пишем его настоящий порядковый номер таким образом куча нулей у нас отлично сжимаются и у нас будет всего чуть-чуть записей отличающихся от 0 в частности вот вчера смотрел циферки такой подход дает нам более чем 1400 раз сжатие то есть сжатые данные существенно меньше занимают следующая проблема который здесь возникает это то что запрос на получение последней позиции вставленного в сводных данных он подтормаживает ну то есть не то чтобы нас это сильно бомбила но если на справа приложение которое поставляет данные затупляет на минуту например для того что перрин солировать свое состояние это становится неаккуратно немножко мы сделаем штуку в котором будем рядышком сохранять информацию о до последних вставленных строках она называется материала east view это функционал который нам позволяет подписаться на события вставки то есть мы произвели вставку тысячи строк в табличку у нас получилась некая пачечка на данными из этой пачки мы можем выполнить sql запрос и получить существенно меньше количество строк и сохраните в другую таблицу ну и невероятно круто то что эти материалы их можно более чем один навесить на вставляем ее данные но здесь легко попасть в ловушку когда у нас будет такое ратификация когда будет чуть-чуть строчек вставляться и последствия основную таблицу а потом каскады материальное сдвигу будут очень много паразитной нагрузки создавать поэтому с этим инструментом надо быть довольно аккуратным и думать вот такой зубодробительный запрос на первый взгляд но на самом деле все просто из того облачко данных которые мы вставили мы выбираем имя источника и значение последней позиции которая соответствует последним моменту мы будем хранить это эти значения в движке клик house который называется replacing марш 3 в хаусе забегая вперед скажу что невероятное количество различных крутейших движков и это дает богатые возможности по построение различных крупных систем и тогда так вот этот движок позволяет нам схлопываться хлопать значение дублей им иди дуплицировать будем о соус на ему то есть у нас для одного источника всегда будет одно значение соответствующей последним ставки но мы сделаем проецирование подати для того чтобы спустя там какой-то миг время например две недели удалить более неревар не релевантную информацию о старых вставках похоже что мы научились собирать и сохранять ивенты давайте подумаем чем будем делать с масштабированием тут достаточно все просто если у вас много данных вы не влезть в один сервер наверно вам придется well sharding есть вы хотите чтобы ваша система была достаточно надежной нам придется к шарлин гуще допилить репликацию и так удачно сошлись звезды что включаюсь это все есть из коробки поэтому сильно уделять этому внимание я не вижу большого смысла эти штуки там есть и они работают из коробки и работают круто будем считать что мы побороли масштабный эффект и остается штука которая не покрыты в документации потому что можно сказать что это частный случай например что мы будем делать данными которые проезжают в двух разных дата-центрах трюк состоит в том что у нас есть кое-то множество машин на которых нас инсталлирован клик house и документация говорит о том что ребята вы можете в топологию кластера указать что эти машины обменяются в кого-то кластер но не столь явно что на самом деле одна машина может принадлежать больше чем одному кластеру она может быть озеро мог быть вложенными вообще совершенно по-разному между собой машины переплетать и так далее для того чтобы работать с данными какого-то конкретного кластеры как логической сущности нам надо будет создать табличку с движком дистрибьютер в которой указать имя кластеров и тогда система поймет на какие машинки надо будет ходить за данными в конечном итоге и как говорится задача свелась к предыдущей у нас есть 20 центра мы создали два кластера там и там и один мета кластер над всеми этими машинами и таким образом мы можем выполнять запросы объединяющие данных двух до центров чем у нас научился пример с аналитикой мы умеем хранить данные в довольно странный формате киева илью но мы понимаем что его можно ограничено использовать мы научились делать века подобие вторичных индексов на базе материала из view и мы умеем с вами жонглировать машинами объединяет в различных кластеров сейчас я вам расскажу про систему разбивки и статистики по вариантам об и тестов apts ты значит у нас есть система аналитики она дает нам какие-то метрики и когда мы проводим эксперимент по улучшению значения метрик мы бы хотели видеть их в разбивке по обед с там например коллеги из а его с разработки занимались оптимизации загрузки картинок вот забегать вперед вот такую картинку они бы хотели получить что для такого варианта у нас столько времени занимает загрузка картинки а для такого столько как мы посчитаем средние время загрузки картинки мы сохраним факты технический метрики картинку меня загрузилась за столько-то и потом возьмем например за день средняя или перцентиль какой то неважно так оно будет выглядеть как мы будем пересекать это с тестами apts ты пользователь присылает хит я участвую в тесте я учился в тесте по оптимизации загрузки картинок у меня будет вариант control control чек или тот вариант который мы тестируем как новое улучшение вот они как-то идут идут хиты у нас а потом собственно возвращаемся к тому что нам надо посчитать метрику и в этот момент нам надо понять в это окошко за которым и селекции факты технически метрики сколько там было пересечение с абэ тестами но в терминах реляционных базы данных это будет jojen таблички с фактами табличку с привязкой и пользователь к бкб тестом ключевое слово здесь inner join в принципе просто join что мы знаем о джоне в клик house и он там есть всякими ограничениями в частности правой таблица должна целиком умещаться в память и следующий нюанс что нам нужно чтобы те данные которые между собой джо и те у них был до этого колите то есть они лежали на одном физическом сервере в нашем случае мы сначала сделали систему продуктовой аналитики потом стали к ней прикручивать разбивку по абэ тестом и до этого колите нам невозможно было обеспечить потому что не во всех аналитических событиях присутствует user айди как ни странно а janitza нам надо будет именно паузе ради ну задача решать как-то надо и кажется что хиты по тестам нам все равно надо будет склад сохранять мы будем сохранять в табличку клик хаусе тест вариант user и когда у нас случился хит по батисту собственно мы потом собираемся это joy нить но мы говорим о том что правой табличка должна вмещаться в память хитов у нас по тестам может быть много и join может разорвать как химикат капля никотина чё делать мы будем писать не абы куда в табличку с движком нам в отличие от на первый взгляд бесполезного виртуального устройства в линуксе д-р нал нал в откликаюсь и позволяет нам строить pipeline с используя материала из бью который я уже показывал ранее еще один зубодробительный запрос который сейчас разберем по пальчикам нам необходимо сделать агрегацию когда у пользователя был первый хит по тесту суть в том что получив один раз вариант в тесте человек для конкретно польза он никогда не поменяется то есть он может повторно присылать информацию о том что он прибит к этому тесту но вариант у него никогда не будет меняться поэтому можем его засунуть включи грига ции и опять же дуплицировать эти данные также нам важно понимать когда у пользователя были хиты первые и последние для того чтобы понимать включать его или не включать в окошко при расчете метрик мы презираем опять использованием движка под названием replacing the greeting мираж-3 простите будет улице ruim у юзерам эту инфу нового у нас никуда не ушел один тела к лисе от этого кольца мы достигаем за счет того что мы не используем функционал шарди рования данных вместо это у нас есть одна большая табличка которая я копья находится на каждом из узлов кластера соответственно можем с ней безопасно janitza используя данные аналитики к этой маленькой таблички с хитами мало ли табличка с хитами вот табличка с китами доросла до 150 тбайт и цифра на первый взгляд может показаться пугающей но в наших реалиях когда у нас bad is clear хаусом имеют жесткий диск порядка восьми терабайт для нас это не такая существенная цифра пока что но возможно рано или поздно нас будет думать о retain шине данных собственно у нас есть хиты у нас есть аналитические события можно их между собой по джойнер ну здесь все не так уже интересно в общем есть функционал join a freek хаосе который становится с каждым днем все лучше и лучше он становится все более sql compatible то есть вот здесь вот у меня достаточно древний чей вариант сейчас это можно уже писать более понятным для людей привычным и сквидом чему мы узнали из системы абэ тестов мы научились использовать движок нал для того чтобы на него цепляться хуками виде материализованный вьюшек и мы с вами узнали что cly хаусе можно делать некоторые локальные небольшие таблички которые равномерно размазаны по кластеру и это неплохо следующею space это time series наверняка не существует этих проектов которые не собирают метрики и не хранят их в каком-либо виде там series emin имя метрики время возникновения когда на измеренной и визг не какой-то какое-то значение по природе они у нас бывают исходными сырыми и прорезанными когда мы сворачиваем даны используем такой агрегат най функции как мы можем хранить time series данной можем их не хранить и всегда к нам надо визуализировать time series сходить в сырые данные посчитать метрики и показать их можем использовать движок иск ли house под названием графит merge 3 которой важным бонусом дает нам автоматически down сэмплинг данных то есть сворачивание их при устаревание данных но у него есть определенные условия использования то есть вашей таблице должна содержать ряд определенных полей и мы решили что нам это не совсем подойдет и мы сделали свое решение почему мы решили так делать потому что нас более почти полтора миллиарда различных метрик нам очень важно минимизировать занимаемые дисковое пространство и еще важное требование было что мы хотели бы low latency доступ к этим данным вообще treehouse он не совсем прост такие вот прям быстрые быстрые ответы то есть милисекундные но нам очень хотелось и мы подумали что возможно что-то и получится и важное дополнение что в отличие от большинства систем работы с time series данными мы свои метрики не стали себе целью обвешивать тегами лейбла годами демедж нами общем формулировки варьируется но суть одном этаже у нас каждый метрика независимо и мы не хотим ее не как размечать соответственно самым дешевым и самым производительным вариантом будет вместо имени метрики писать некий хочет не и потому что поиск по хорошо будет существенно быстрее собственно вот так вот мы будем искать метрику то есть мы не считаем хэш на стороне клиента мы этот даем наводку базе потому что при хаусе хэш-функции быстрые классные и не надо их изобретать на клиенте боль хэш хорошо сжимается потому что мы по нему сортируем метрики мы час говорим про не прорезанные time series то есть сырые точки хорошо у них сжимается хэш потому что он не по нему данное отсортированы и все круто становится поиск по хорошо разумеется с вами не чисел работает быстрее чем сравнение строк это дает нам наш желаемый low latency побочным эффектом то что при вставке нам надо брать хэш но в принципе нас это не так беспокоит и то что мы человеческие не сможем никогда поискать метрику то есть надо использовать вот эту конструкцию которая показала ими метрики заносить в хэш-функцию и в принципе мы подозревали что такое что то появится но во флот из хранящий значения метрика они довольно погано жмутся чё здесь можно придумать строительно недавно появилась крутейшая штука под названием кастомные кодеки для колонок то есть мы можем сказать что какая-то колонку нас совсем не сжимается 1 сжимается одним алгоритм другая другим нос вишенка на торте в том что и кодеки можно вязать между собой комбинировать и для значения time series мы сначала берем кодек дельта который рассчитывает разница между двумя точками и затем на его результат накладываем кодек компрессии собственно почему это работает это работает потому что соседние значения у метрики обычно не сильно различаются и дешевле посчитать дельту или дельта дельта чем хранить непосредственно сырое значения и в этом случае прирост при выигрыше в занимаемом месте у нас получился порядка 10 раз для колонки хранящие значения в чем мы узнали из этого кейса мы узнали что при хаусе можно делать low latency доступ к в час и здесь ней прорежем the series нового low latency доступ возможен при их запросах где используется максимально простые типы данных то есть мы отказались от строк за счет этого ловит на си и мы научились оптимизировать хранение данных с использованием дельта кодека штука которая не очень непосредственно про cliff house но про меня очень часто про не просто рассказать потому что тема горячий интересно и так далее но мы ли detection у нас есть множество эротических метрик например регистрации состоят из регистрации женщин регистрации мужчину это самый простой выраженный случай и разумеется там может быть гораздо большее число атрибутов там происходит какая-то ерунда иногда какие-то всплески и так далее мы бы хотели умная машина чтобы нам умела говорить в каких метриках у нас произошли существенные изменения для этого у нас есть die menschen метрики у нас есть актуальное значение и у нас есть функционал который нам предоставляет ожидаемое значение и доверительный интервал в который привел которого метрика может варьироваться выход за доверитель интервал означает что метрика ведет себя аномально и важное замечание что значение метрику нас выровнены по времени то есть у нас нет такого что метрика одна пришла в одну минуту другая в другом их выравниваем для того чтобы иметь некий слепок состоянием метрик в момент времени вот буквально финальный интерфейс выглядит так что у нас есть дата у нас есть например у нас аномалии вызовов пока контроллеров некоторых они разбиты по дата-центром и там происходит какая-то ерунда которую мы видим в юо и получаем модификации и так далее в таблице мы храним имя простить значение метрики время возникновения математические штуки оля медиана среднее квадратичное отклонение die menschen и мы храним там ожидаемые значения которое вы вычислили снаружи и ширину доверительного интервала и также у каждой метрики есть такая штука как вес зачем нам нужен вес потому что мы бы хотели видеть именно самые важные аномалии и для этого мы заносим вес и параметр аномалия в логарифм потому что логарифм это неистово круто он все выравнивает все стерпит и все нормируют нормально да здорово получилось так вот у нас есть два параметра которые влияют на значимость аномалии это ее вес относительно веса при метрики относительно всего множества метрик и геометрическая форма на сколько там у нас график расколбасило вес это значение метрики поделенная на сумму значений всех метрик когда это будет работать плохо когда значение метрики для нас должно быть чем ниже тем оно лучше то есть мы говорим про технический метрики время загрузки страницы то это будет не совсем корректно работать но в наших реалиях мы в основном оперируем аномальными над транзакциями регистрациями загрузкой фотографии и так далее где чем больше тем лучше и собственно геометрическую форму мы рассчитываем как разница медианы и актуальное значение определенное на отклонение здесь надо быть внимательным и не забывать что у делить на ноль не очень хорошо и довольно вредно можно быть поэтому надо учитывать что в знаменателе может быть 0 мелочь но при полезно держать в голове и в общем дальше задача достаточно тривиально выбрать нам выбрать метрики отсортированный повод этой важности который мы показали что здесь хорошо хорошо здесь то что в один запрос можем отранжировать значение и получить адекватный результат то что за счет вот это параметры уэйд мы можем учитывать иерархичность метрик чем мне не очень нравится это то что мы пока не перенесли непосредственно в cliff house функционал связанные с мат аппаратом этой штуки а именно вычисления ожидаемых окна назначение метрики и доверительных интервала но я думаю что мы и эту проблему решим все штуки про который рассказывал раньше и характерная особенность то что там данные не мутабельные то есть мы записали и забыли аномально точно такая штука что в нее данные иногда надо перезаписать ну то есть мы поняли что проблема случилась у нас часть данных например биллинга не доехала и мы зари портильо ребят у нас там транзакции упали в два раза но все получили смски и все получили по голове но потом выяснилось что данный доехали и надо в аномалии detection занести актуальные данные чтобы система не обучилась на некорректных данных для этого надо часть данных стереть и записать новыми как удалять данные в хаосе есть несколько способов мы вообще нет эскизы почему можем удалять потому что данные в принципе нам перестали быть актуальным надоели не нужны и вот кейс с заменой части данных по каким-то признакам и из каких-то своих соображений варианты удаления нас их два это удаление партиции в таблице то есть таблицы полезно какому-то признаку и поэтому же признаком будем удалять или выполнить запрос под названием дэвид и на getjar сейчас болтается по ул request который еще в стадии доработки который нам будет позволять не делать это руками позволит чтобы cliff house сам следил за данными в колонках портится их так далее и сам подрезал данные как удалить партицию достаточно тривиально понятно alter ты был дроп пропишем все данных за такую-то дату и такое-то время нету это пример с anomaly так что потому что горошин нас метрики выровнены по часу например и мы удаляем тоже данный за 1 час если мы проверяем эту операцию через оператор дэвид запрос выглядит похожим образом но под капотом все происходит совершенно по-другому чем они различаются оператор alto ты был гроб partition он синхронный если вам база отвечает что запрос прошел значит этой партиции больше нету в этом сервере если выполняйте до лет это асинхронной операция которая шину лица в background это же в бэкграунде отыгрывается в чине какого-то времени если мы сделали дроп партий шин все это произошло в целой таблицы и мы гарантированно больше не видим данных если мы делаем билет то вы будете постепенно видеть как данные где-то удаляются где-то еще не удалились и это процесс достаточно растянуты по времени с точки зрения операционной системы функционал дроп портишь он производит удаление данных nude эмму в папке оператор дает достаточно тяжеловесной потому что ему надо прочитать данные который нас уже лежат отфильтровать ненужные которые мы подвергаем когда попадает под критерии р и записать новые данные обратно в диск соответственно выглядеть так вот эта операция довольно дорогостоящее и честно говоря вот на мой взгляд функционал дэвид пока немножко мутноватый не хватает ручек для диагностики в каком состоянии сейчас дэвид находится но я думаю что со временем и эта проблема будет решена и бонусом который редко где упоминают почему-то это то что мы можем не удалять целую партицию например а в каких-то старых квартирах мы можем одну коля вколол затереть то есть заполнить дефолт нами значениями и освободить таким образом дисковое пространство себя замена данных вот в аномалию action как мы это делаем наши таблицы партиции раваны по часу соответственно мы хотим данный закаты час перезалить мы удаляем партицию и вставляем новую если мы улыбаемся где-то посерединке между пунктом 2 и 3 таблицу нас находится в не консистентной состоянии но эта штука отдано на откуп приложению который может перезапустится снова удалить несуществующую партицию и вставить новые данные чем у нас научила нам следует action мы научились ранжировать иерархический там series с учетом геометрической формы аномалии и веса метрик мы знаем о том что при проектировании схемы важно думать о том как быть модифицировать свои данные будете ли их модифицировать соответственно при проектировании схемы надо думать о портировании в частности для самых любознательных кто еще не ушел я приготовил для вас бонус-трек сейчас мы с вами поговорим про бутерброды из таблиц зачем нужна вредная пища например бутерброды с помощью бутербродов из таблиц можно решать специфичные задачи и спицы специфичными движками а именно движки по названию дистрибьютер баффер и мер для тела колите как мы говорили что есть мы хотим joy не данные они у нас должны лежать рядышком на одном и том же сервере и бывает так что у нас есть совершенно разные клиенты которые не очень знают про топологию cliff house of просто хотят куда-то записать данные и возможно потом их между собой над будь join its мы создаем дистрибьютор таблицу у которой в ее конструкторе в вскоре запросе сказано что мы будем шарли данный например по user айди соответственно если мы создадим две таких таблиц и но буду писать них совершенно разные по смыслу данные то данные с одним usa ради приедут на один и тот же сервер что выглядит как дешевое решение проблемы выравнивание клиентов между собой другой случай это когда мы хотим писать много но по чуть-чуть то есть много маленьких insert of по какой по каким-то причинам у нас нет возможности их объединять большие блоки что является хорошим паттерн им дали хауса мы хотим их понемножку писать часто-часто часто но при этом хотелось бы чтобы данные у нас равномерно размазались по кластеру потому что мне можем прогнозировать о них мало данных а завтра будет много давайте мы заранее их размажем по кластеру вот таком случае можно предложить вариант когда мы пишем в таблице слишком буфер которая спустя какое-то время по достижению числа строчек или по времени или по занимаемому вместо оперативной памяти произведет их проталкивание куда-то дальше например мы протолкнём в табличке дистрибьютер который of the magi и размажет нам данный по кластеру комбинация таких вот этих двух мешков позволяет такую провернуть есть очень интересный движок в хаосе под названием мерз я бы сказал что он чем-то похож на функционал union all то есть мы делаем select из двух источников и объединяем данные с помощью уделено примерно похожи но вам не надо будет писать unions руками вам надо будет создавать таблички попадающие под какой-то паттерн то есть мы создаем табличку с движком мер и говорим что все все таблицы которых название начинается с time series я буду в них делать запрос и в частности другие таблицы могут иметь совершенно другие движки например наш не прорезной там series бы храниться в нашем высоко оптимизирован am стороны хлопнут и данные roll-up могут храниться уже в движке графит мираж-3 и еще может быть у нас есть какая-нибудь очень быстро обновляемая статистика метрики которые будем хранить в движке memory и когда мы посылаем запрос в самую первую зеленую таблицу мы автоматически сходим во все остальные и соответственно используя эту штуку мы собстно построили хранилища метрик потому что при запросе с клиентом мы не знаем на каком сервере будет лежать метрика поэтому запрос присылается дистрибьютор таблицу которая его отправляет во все шарды кластера и там в каждом шар 2 идет запрос в 3 нижележащих таблицы вот такие бутерброды можно делать и вторая часть бонус трека это интеграция с внешними источниками кликаешь есть ряд штук позволяющих по внешней системы в первую очередь это словари когда вы в эвенский колбасу 0 единичек а потом хотите его отобразить обратно на бизнес значение имя стороны еще что-то и нам и и прочее у нас есть функция которая позволяет затянуть данный с используем ктп запроса появилась сравнительно недавно интеграция с hd весом который позволяет читать писать данные есть odbc коннекторы к другим драйвером и есть еще одна функция который мало кто знает она называется g dbc именно про нее я вам расскажу это минутка самопиара на самом деле потому что я эту штуку сделал и мне бы хотелось чтобы она нашла свое развитие и жила своей жизнью существует java прокси для crack house который зывается j gtc bridge at отделе запускаем а и приложение которое общается с как хаусом по одному из самых высоко оптимизированных протоколов а именно ромба мире более производительный только нативный протоколу базы но мне кажется это немножко некорректно влезать во внутреннюю реализацию и полагаться на на эти форматы поэтому мне кажется что лучше взять чуть чуть менее производительный но с понятным стабильный пий так вот есть отдельно standalone приложение на яве который умеет общаться клик хаусом ох т.п. в гбц bridge загружаются драйверы к вашим любимым газом сад базам соответственно нас есть jar of с mais que al драйвером адресом и неважно кем подсовываем в папочку гбц бриджу он их засасывает и когда наш пользователь приходит в cliff house и пишет пожалуйста сделай мне select из базы g dbc что-то там crack house отправить запрос в гбц прокси который с использованием коннектор которые мы подсунули уже сходит в настоящую базу и вернет ответ обратно почему это может быть нужно есть ощущение что есть организации где odbc не хотелось бы и где любит яву и выглядит так что если вы не любите воду бы цену любите я вот то эта штука может быть вам полезно ссылка на git хоп я думаю что когда-нибудь она переедет с этого урлов официальный репозиторий cliff house я с нетерпением жду этого дня о чем еще не с вами поговорили для решения различных задач клик хаусе очень богатый инструмент инструментарий можно лепить бутерброды можно менять движки можно настроить pipeline и обработки данных и так далее нужно выбирать нужное вам комбинировать и самое главное не бояться экспериментировать знаете однажды мы столкнулись ситуации что мы вставляем данный treehouse и если он не успевает в бэкграунде оптимизировать наши данные то есть их сливать между собой он выкидывает исключение у вас более 300 300 необработанных концертов мы не будем больше от воспринимать insert и мы подумали а что будет если мы например этот параметр выстоим не в 300 a800 мы выставили и знаете что произошло ничего но заработала то есть не надо бояться экспериментировать залог высокой производительности это выбор минимальных возможных структур данных как было в примере с time series мы заменили строку на хэш и все взлетела волшебным образом и при проектировании схемы хранилища при хаусе надо думать о том как мы будем модифицировать свои данные как мы будем их замечать будем ли вы замещать будем ли мы удалять что-то модифицировать и так далее на этом у меня все спасибо большое за ваше внимание готов ответить на вопросы добрый день спасибо за доклад вы рассказали правда тестирование но вы не упомянули одну важную вещь это применение ст от критериев вас просто есть какие-то числа непонятно как вы их сравнивать еще street после чего не упомянул стад критерий бы тестирование важное и часть это когда мы с вами это число нам нужно использовать эти чеки критерий чтобы понять действительно ли значимое отличие или нет да и ваня вашем примере а не просто отдельные и какой механизм и стал критериев использовать apple в данном случае этот сосед метрик которым показом они сугубо технические поэтому у них нету каких-то производных метрик которых зависят из мы вот строго работаем с временем загрузки картинок для более сложных бизнес кейсов и существует отдельная сложной аналитика которую не получается сделать на клих aussi и там вот эти все штуки че сквер я не очень сильна в непосредственно организация бы тестирования разбираюсь более сложная мать your system для всего что связано с бизнес метриками это про технические да просто даже технические метрики они могут изменяться случайно и если вы их поэтому может быть изменение просто случайно если вы принять этот критериев вы можете на графике нарисовать что-то на это на самом деле может быть случайной флуктуацией ну мне кажется что в данном случае предохранение с помощью control & control чек группы должно нас спасать если они совпадают то выглядит так как будто все идет о они могут случайно совпасть поэтому для этого применяется тот критерий его в тестировании даже для технических метрик хорошо я наверно учту это и попытаюсь как-то применить понять понять поговорить and юзерами действительно ли они сталкивались с такой проблемой то чтобы тестов проводится много и просто нам пришел заказ сделать нам разбивку тех метрик по абэ тестом учет актуальности это значение такой задачи у меня не было если можно еще один быстрый вопрос вас пользователи приходят эксперимент но никогда не уходят из него если пользователи прибило к какой-то группе теста дата он никогда из него не выпасть возвращается в какое-то время можете например использовать какой-то другой протокол ответа он попадет в эксперимент по этому протоколу ответ а потом и перестанете его использовать ну пользователю все еще будут метрики считаться что он в этом эксперименте если вы возьмете расчет например там не знаю за три дня и он побывает раз примите два дня на третий день эти числа еще будут портить значения в этом эксперименте получается потому как вы рассказали если мы изменяем условия теста да то есть у нас есть такое что мы растратили на какую-то страну на 5 процентов потом поменяли ещё штуки у нас есть помимо теста идею штук который решил не загромождать слайды с этим каиде то есть мы знаем какой и для текущие для теста который сейчас бежит как и у него идентификатор последних настроек для него и его мы используем в агрегации спасибо могли бы немножко числа звучит для ролл метрик то есть сколько там секунду падает и сколько сервировка это смотрите у нас инсталляция из 8 серверов для crack house 6 из них находится в центре в европе 2 в америке если говорить про аналитику то у нас систему влетает порядка полутора миллиона событий в секунду если там вспоминает еще абед стб тест этот там порядка 10 тысяч счетов в секунду что общем то копейки или три хаоса и по поводу вылетающих метрик к сожалению я сейчас не скажу по моему тоже сотнями тысяч в секунду и смеялся пока 300000 секунду простоять нас особенность такая что мы метрики как оскалами нас выравниваются по времени и мы говорим так сейчас мы считаем состоянии регистрации за последние сутки и делаем это каждый 5 минут поэтому там много нового скачкообразно возникает метрики но пишите при этом и по сути дела все полтора миллиона и в секунду и куда-то отображайте пойди итоге или это не по нет у нас нет нету граф анны про это у нас сторож в который мы переписываем метрики а сандра и прочие штуки но мы планируем вот после того как появился функционал belle танк 1 га мы планируем перевести все метрики в crack house потому что выглядит так как будто кассандра нам больше не нужно в наших реалиях когда у нас отсутствует тагила иглы на метрики когда не is all content спасибочко здравствуйте спасибо что рассказывайте как правильность различные подходы про использование кликал я не говорю что это правильно я говорю что работает то что рабочие подходы использовать и хаоса возможность такой два вопроса вот первый увлекался уже де как есть ограничение на количество вставок можно крамола увлекался же вроде как есть ограничение на количество ставок батчаев систему это вот то что я упомянул что есть нас например более 300 необработанных концертов такой хаос может отвергнуть вашу ставку такая есть защита да да но у вас при этом размер бачатой вас достаточно большой поток событий вы должны накапливать батчат у вас получается почти достаточно больших до baci измеряются по у него верхний порожек около полмиллиона элемент мегабайт сколько очень большие куски не могу щас к сожалению чего проблем от этого то что искать мер семена показателем вы знаете тут особенно в том что хорошо когда мы вставляем вот это предварительно отсортированные данные там умоляют помогает ли house и и мы вставляем событи который сейчас происходит они у нас сортируются по дате возникновения по времени и она близка и в общем а частота остатки какая а частота ставки какая приблизительно частота вставки и но мне кажется что с одного сервера на котором аккумулируются данные то есть у нас в системе происходят события потом не снижаются на меньшее количество серверов агрегат агрегируются и там собственно формируется эти бочки мне кажется что с одного сервера происходит порядка 1 вставки в не знаю секунд пять или десять зависит от пиков трафик и второй вопрос про дистрибьютер а то что вы вставляете чистить дистрибьютор бородавка через нее это же вроде антипа тарнов клик хаосе и они рекомендуют именно читать через нее да вы правы что это можно назвать анти поттер нам но в случае если для вас это применимо если вы не хотите сами думать о топологии кластера и самим непосредственно connectors и какому-то серверу записывает в настоящую физическую мир 3 таблицу этот бренд вполне можно использовать но а вот двери пред для высоконагруженные и наши штуки с аналитикой мы не пишем vadis три вида таблицу там учитывая топологию plaster а какие серверу на сейчас живы и кто умер и впишем по-настоящему по правильному напрямую отдельно масштаба в настоящие физические таблицы но я не могу сказать что это приносит мне большое удовольствие потому что вручную но то что пришлось это все писать функционал проверки живости и машин и так далее спасибо добрый день спасибо за доклад его сожаление не вижу вижу по мере эксплуатации клик хауса мы столкнулись с такой проблемой скажем так что конфигурация разрастается шарда то есть машин становится много и появляется такая мысль сделать виртуальные шарди рование ну то есть абстрагировать как бы от физической конфигурации игру говоря узлов шарди я не сделать некий виртуальный sharding были у вас какие-то мысли поют но вот эту деталь это история про которую я показывал что мы в один кластер объединили все ноды в системе не каштана про это ну нас виртуальный виртуальный кластер из двух дата-центров ну ну а конфигурацию виртуального кластер а где вы храните то есть это баксане y самой в xml джеральда а вот еще в момент когда говорили про бутерброды из таблиц такой стандартный паттерн вставка через буфер у вас а почему не использовали кафка engine какие-то предубеждения вы знаете у нас просто говорится исторически сложилось что в баду кафки особую не было ее завозят последнее время но вот для задач перегонки потоков данных у нас используется штука свои названием лсд ну да и то при войска клип streaming демон и в общем мне на самом деле не очень нравится штука которая не могу контролировать то есть вот интеграция с кафкой мне она не очень нравится потому что ну что-то там где-то засасывай данные из кафки у него есть какие-то ограничения да там rake лимитер но мне не очень-то все нравится в я хочу вот руками тудым сюдым данной гонять мне нравится спасибо а можно еще небольшой вопрос вот смотрите такой сценарий допустим usa ради по нему ивенты ну и и нужно допустим вычислить самый последний винт по данному юзеру вы это решаете с помощью коллапсе номер три то есть ну поскольку нас append он лет фактически события да то как вот или просто агрегатные функции у нас с этих данных с поток аналитических ивентов у нас строится агрегаты то есть у нас нет такой задачи по юзеру там вычислить последние его событие спасибо но я бы сказал что я знаю что таблицы семейства мираж-3 производят схлопывание строчек и так далее но я бы не стал на этом прям очень прочно строить бизнес-логику то есть например где мы и дуплицирование хиты по тестам да они могут хлопнуться а могут не схлопнуться а могут о принце попозже но в общем это такая штука гарантий нет нет гарантий есть что если произойдет слияние данных в пределах партиций то данный будут дуплицирование но нельзя слепо в это верить можно ли можно прогонять принудительный optima из когда вы хотите железобетон где дуплицировать ну можно не знаю спасибо здравствуй пасибо за доклад а вот и вначале сказали что создаете табличку на каждый тип события ты правильно понял мы рассматривали такой вариант хранить событий подобным образом но когда мы попытались это эксплуатировать продакшене эти рандомные сердце в кучу табличка не нам деградировали весь сервер от связано с тем что существуют накладные расходы на репликацию то есть новый кусок данных надо анонсировать в зуке перри когда много таблиц много табличек начинают ломиться в руки перед они очень весело его чему бы это все погано работает просто так и должно быть прекрасный тормозит понятно еще один вопрос про удаление данных я про нее что вы прям и запрете щенка кода удаляете партийцы то есть ну то есть штук который работается номер два решено первая ночь уделает меня запустили обработать аномалии связанные с часам дня сегодня первая часть делаю я приду в клик house и и сделаю дроп partition не страшно что на что-то не того что поудалять ну или не знаю не страшно нет наверное я я всегда знаю где взять эти данные снова дорогие пользователи пожалуйста загрузитесь но свои фотографии 91 года спасибо ещё вопросы есть где штаны и взял и скажу хорошо тогда увидимся через десять минут будем смотреть на яндекс такси через призму с вами петербурга message спасибо же она точно кому книжку подарим молодой человек который про б тест спрашивал я ничего не понял но очень интересно обязательно приду и дойду до наша команда аналитиков шипа ними все рассказали чтобы я так больше не позорился магните рукой унизительно в общем спасибо друзья у вас так и будем звать sage спасибо"
}