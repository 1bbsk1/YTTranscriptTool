{
  "video_id": "BOVuwr43ZTE",
  "channel": "HighLoadChannel",
  "title": "Есть ли жизнь без ELK? Как снизить стоимость Log Management / Денис Безкоровайный",
  "views": 8626,
  "duration": 2613,
  "published": "2022-03-21T14:14:02-07:00",
  "text": "меня зовут denis безкоровайная из компании про ты групп я хотел сегодня поговорить о том есть ли жизнь без ел cos t к как известно елка стек вообще ластик search уже сейчас является по сути таким определенным стандартам для хранения визуализации логов вообще как бы для работы с лагами вот есть у кого-то здесь ластик search для логов есть кто то кто не доволен тем количеством ресурсов которые отжирает elastic search отлично вот мы тоже были не очень довольны этим и соответственно мы решили свою конкретную задачу я расскажу как мы и решили с помощью в данном случае cliff house кафки вектора и других на о чем я с ним поговорю о том какую задачу мы решали как решали что мы пробовали и к чему пришли в итоге для начала описание задачи но собственно наш компании является сервис-провайдером у нас на обслуживание по разным сервисам находится много разных компаний и собственно эти компании они очень потребляют разные наши услуги и так или иначе нам нужны их лаги лаги этих очень-то заказчиков они все находятся не у нас надпись у каждого заказчика есть своя инфраструктура который где угодно может быть размещена и нам нужно было сделать систему для недорогого хранения и анализа этих логов почему недорогого потому что в принципе это не наш основной сервис то есть мы не продаем в данный момент сервис панель зло гав да это как бы сопутствующий сервис которым оказываем но заказчики не готовы как правило за это отдельно платить почему так получилось я тоже расскажу ну и собственно таких заказчиков много лагов среднем там от 100 до 400 50 мегабайт сутки у каждого и нам необходимо чтобы у нас был доступ и у них был доступ к этим логан технически это выглядит вот так то есть есть разные сайты есть разные локации и собственно есть наша инфраструктура на которой мы бы хотели хранить все эти лаги и вообще как-то их анализировать почему elastic всевышний подошел нам ну во-первых наверное все уже знаете да что чем больше данных тем медленнее это все начинает работаете тут есть два момента пир момента это индексация когда мы только загружаем данные они индексируются и второй момент это поиск когда мы делаем поисковый запрос если данных много если большой объем этих данных то соответственно сам поиск может происходить ответ очень может происходить так секунды или минуты вот ну и естественно все это дорого хранить обрабатывать с точки зрения накладных ресурсов именно по себе упором для хранения опять же используется много дисков и но естественно чем больше система тем больше избыточность в случае с кластером ластик search избыточность тоже довольно велика например один заказчик 120 гигабайт данных в сутки то ни сам большой заказчик ну и соответственно если мы храним эти данные 30 дней по рекомендациям самого ластик сердца нам нужно 120 умножить на 30 дней умножить на количество репликой плюс 1 и соответственно даже если у нас одна реплика мы получаем 7 терабайт данных которые нам нужны просто место под это хранение и если мы добавим еще рекомендации власти ксир чьи когда они говорят что нужно еще 25 процентов возложить на всякие непредвиденные расходы и на то чтобы в принципе он работал при чуть-чуть свободном месте мы уже получаем 9 терабайт это все если мы просто возьмем доживут одну зону без какой-либо отказоустойчивости если мы пойдем на ластик search на их сайт и просто забьем эти ресурсы в калькулятор мы получим что примерно 8 дисплей на терабайт данных нам будет стоить шесть тысяч долларов нет 6 долларов в час но естественно мы это все умножаем на 30 на 30 дней на минуту и так далее и получаем приблизительно 400 тысяч рублей в месяц это просто для одного из заказчиков есть ли варианты другие до возьмем российского провайдера конечно же опять же мы сейчас сравниваем не просто ресурсы не просто железки мы берем в данном случае сервис управляемый что от elastico что утро сийского провайдера примерно похоже ресурсы мы возьмем там столько же я der столько же памяти столько дисков возьмем даже просто два сервера и мы получаем приблизительно 200 тысяч рублей в месяц это как бы не очень много вот но опять же мы за это платим вот и это для одного из заказчиков не самого крупного как нам эту стоимость мы бы хотели снизить собственно нам не очень подошел графа налоги почему то уж на тот момент когда мы начинали этот проект он был еще довольно сырым продуктом вот и как бы графа налоги очень хорошо подходит для проектов которые хочется в каберне this когда у вас есть метрики в графа не в prometheus у вас есть логии собственно там одни и теже теги все очень удобно но нас не вся инфраструктура в кубер нить из для кого-то она вообще не в кабинете на тот момент это был не очень надежное решение и не все на тот момент поддерживалась то есть там не было даже поддержки multiline логов она потом уже добавилась в процессе есть конечно же промышленный enterprise решения которые созданы специально для от менеджмента и тоже мы просматривали лидером наверно является компании с планк но они не продаются в россии если вы их когда-то успели купить то наверное вы можете их продлить как текущий заказчик но новым заказчикам насколько знание не продают больше лицензии такие компании как и умер они соответственно когда мы с ними общались тоже были в процессе реорганизации их там купила другая компания и они тоже теперь не продает в россии компании greylock тоже как бы хорошее решение для управления лагами них есть энтерпрайз решения есть бесплатная версия под капотом тот же ластик search то есть все те же минусы по объему данных по накладным ресурсам и они тоже не продают свои решения в россии такое ощущение что система управления лагами это какая-то просто как от технологий двойного назначения и когда эти вендору знаю что вы из россии они говорят о истории мы не можем вам ничего продать но у нас есть очень бесплатной версии ночь пожалуйста используйте но мы не вступаем в коммерческие отношения с заказчиками из россии если у вас юрлицу в россии есть отдельное решение которое именно сансон ли они тоже очень хорошо подходит если у вас логов не так много то это прекрасное решение такие как new relic сумма лоджии clock золото the dog и их много на самом деле мы все смотрели на минусы все те же до для большого объема это довольно дорого это нет это точно не в россии и это ну как бы понятно дело есть риски потери связанности то есть все мы знаем эти риски вот пример одного из заказчиков в эта система new relic как мы видим здесь 400 гигабайт логов в день и есть какая-то неоднородность понятное дело что у всех по разному в среднем в месяц это 12000 гигабайт и это стоит приблизительно 265 тысяч рублей в месяц нажать сразу на 12 мы получаем приблизительно 3 там с чем-то миллиона рублей опять же вроде бы это небольшая сумма для компании но если у вас таких компаний много и это как бы побочные расходы хочется их минимизировать мы решили сделать такое решение сами и у нас были определенные требования такому решению мы хотели стримить логе в практически реальном времени мы хотели чтобы это было с минимальной нагрузкой на сервер и источники и с минимальным воздействием нам в принципе было достаточно визуализировать эти лаги и иметь возможность их анализа мы уже даже может быть не в реал тайме в постфактум сбор данных без потерь событий и относительно дешевого хранения это ключевая концепция была чтобы это сделать дешевле чем все те решения которые были до этого рассмотрены если мы уже эти требования разобьем как бы на части продукта на части решения то можно свести это к тому что нам нужно данные логе собирать серверов то есть некий коллектор этих логов нам нужны отправить куда-то и там принять не потеряв нам нужно их распарсить и нам нужно их где-то хранить ну и дальше нам нужно как-то визуализировать лучше конечно анализировать то есть не просто визуализировать но и анализировать ну и соответственно мы стали решать каждый из этих частей по отдельности для того чтобы собирать logis лидеров так называемой форвардер или коллектор есть уже много разных проверенных средств таких как флинн д который давно уже присутствует есть более современный флебит который классно работает с контейнерами подходит для кубер нить из есть файл битва то ластика вот ну есть вектор это относительно новое решение его уже в процессе как мы этим занимались купил дата dota сейчас эта часть do the dog но по сути делает отдельное решение которое очень бесплатно можно использовать под свободной лицензией если мы помочим если мы просто проанализируем все эти решения мы взяли один файлик и мы очень просто представить что он прихожу к заказчику устанавливаем агента на этой машине уже есть какой-то лог допустим а вот какой то он стандартный лот за сутки там сколько с одного сервера на стреляла там полтора гигабайта и и мы просто смотрим сколько данных сколько времени будет отсылаться этот лог и как это скажется на загрузке самой машины вроде бы по как бы загрузки самый нетребовательный это флинт н но на самом деле тут есть еще другие моменты что как может загружаться память может загружаться дисковая подсистема может загружаться сепию кто-то как флинн бит например все кэширует на диск там кто-то лаги рует сам себя и соответственно все это нужно тюниг тестить и в каждом случае конечно же можно улучшить показатели но что нам понравился два решения вектор то что она очень просто конфигурируется не требует какой-то отдельной настройки в данном случае мы файлики эти отсылали в кафку вектор тоже умеет это делать из ночь умеет это делать из коробки мы решили остановиться на решение вектор а вкратце что это такое то есть это система которая позволяет вам брать откуда угодно данные это могут быть логе метрики как-то их анализировать сделать с ними какие-то трансформации дальше куда-то их отправить и соответственно вот нам нужно было решить куда их отправить как их анализировать и как их парсить таким образом мы пришли к тому что просто на каждый сервер ставится агент и агент ну чисто теоретически может сразу отсылать данные куда-то в одну или несколько локаций понятно дело что нам такая схема не очень подходит потому что у нас много разных заказчиков они все в разных местах и как бы с каждого сервера слать данные куда-то в несколько мест не очень правильным как мы это решает потом дальше расскажу с другой стороны нам нужно данные как это распарсить и как бы вектор это умеет делать очень просто очень простой конфиг и по поводу парсинга важно понимать что данные должны положиться не на источнике событий потому что это влечет дополнительную нагрузку на сепию и соответственно мы должны иметь возможность как-то эти парсеры править как то добавлять их и так далее новый тип логов добавлять без воздействия на сам сервер с которого эти данные собираются потому что мы один раз поставили агента на сервера заказчиков и нам не нужно каждый раз куда-то ходить и что-то делать там чтобы добавить новый тип логов вот и но чем соответственно мы пришли к тому что логично вынести этот парсинг на отдельный сервер таким образом картинка стала примерно вот такой то есть есть заказчики у них есть агенты эти агенты шлют данные опять же в отдельный сервис который тоже на базе того же вектор сделан и уже этот вектор в себе содержит парсеры для нужных нам технологий ну дальше он отсылает куда-то собственно куда он их будет отсылать мы решили использовать плек house потому что это стильно модно молодежно это очень быстрая свд как раз подходит для логов есть в принципе очень много кейсов уже по использованию клик хауса для логов это быстро работает с точки зрения быстрой выборки то есть мы эти данные можем быстро запрашивать и здесь как бы ваш момент что но эти данные доступны практически сразу как только мы уже очень положили что-то мы можем это что-то сразу быстро достать и мы очень избавляемся от этого долгого процесса индексации и как бы второе как преимущество клика усы то что он очень оптимизирует хранение и для нас это дешево для примера возьмем у файлик на 1000 мегабайт 7 миллионов строк просто в zip-архиве это будет 66 мегабайт flickr us это будет ну в два раза больше но все равно это очень хороший сжатие и соответственно это не просто какой-то zip-архив который лежит мертвым грузом это данные которые можно уже вытаскивать можно их анализировать по ним строить какие-то dash горды и так далее таким образом схема стала примерно вот такой то есть данный инспектор отправляется в клик house и важно что даже сам вектор умеет это делать то есть это очень простой конфиг мы видим здесь один из примеров и соответственно важно только чтобы эти данные были уже раз паршин и то есть мы как бы парсим каждый тип logo мы заранее создаем табличку эту табличку наполняем данными которые ну собственно из вектора отсылаются второй момент но измены уже следующий момент это визуализация данных то есть как бы кликал съесть очень мощный движок но есть очень частая проблема что как бы как с этими данными работать как визуализировать есть index.dat лэнс но моего не хотели использовать потому что это привязка к конкретному вендору привязка к облаку наше решение в принципе может использоваться вне облачной среды вот мы хотели так и оставить есть графа на приносим привычная всем средства визуализации для него есть плагин для клик house есть даже порты для кабаны которые позволяют работать в японии но как бы в аналоге кибанова с данными из клик хауса к сожалению это все заброшенные проекты которые не получили развития и собственно но это не очень рабочая технология есть еще альтернативные варианты такие как капочи суперсеты прочие перед матчем системой визуализации их на самом деле тоже не мало вот но мы остановились на графа не которые есть уже готовый плагин для клика us вот таким образом схему выглядит примерно вот так на собственно вектор пишет клик house граф она забирает данные через плагин и мы можем наблюдать в привычной графа не теологии те данные которые мы собрали следующая задача это как бы обеспечить то что вот этот вот центральные сервера он как бы не станет . отказа и ну не будет вызывать каких-то проблем случае некорректных написанных парсеров который будет очень долго обрабатываться следующей задаче это собственно как-то сделать так чтобы эти данные не терялись при отправке ну и соответственно мы использовать для этого кафку для буфера как бы он в общем то является буфером для в ходе для входящих сообщений а если даже этот сервер парсер ну или там сервисы парсеры умерли то они эти данные не потеряются они в кафки останутся в принципе можно кафку использовать и как хранилище и вообще нет ночь не просто как шину данных но и как кратковременное хранилище для того чтобы по нему делать еще какой-то анализ мы к этому идем то есть мы сейчас используем просто как некий буфер для того чтобы эти данные потом вычитать вектором распарсить и уже положить откликался это конечно же дополнительные расходы но это увеличивает надежность сам вектор тоже умеет работать с кафкой как я как я говорил вот здесь пример того что уберем файлик engine x узкий но и собственно отправляем его в карту здесь можно даже указать сертификат и это будет под л.с. работать вот с другой стороны если нам нужно вектором забрать исков кито это тоже делается просто через конфиг вот здесь пример того как это выглядит тоже все по под л.с. в итоге схема выглядит примерно вот так да то есть это некая общая схема что есть заказчики своими какими-то отдельными локациями дата центрами есть агенты которые все дело очень просто собирают не используется парсинг на агентах для минимизации загрузки ресурсов ну вот и соответственно все данные в практически сыром виде отправляются в кафку там они тоже в принципе хранятся даже в сжатом виде из кафки потом отдельным сервисом данные вытаскиваются парсится то есть по содержимому топика мы понимаем нулей по топику мы понимаем что это за файл что это за тип источника расскажем так мы применяем нужный парсер раскладываем этапа ну то есть если это джейсон то его можно просто разложить на ключ значения и записать уже в подготовленную табличку в клик house ну из клика уса через плагин в графа не я мы все это визуализируем к чему мы пришли то есть значит нас есть даже борды парсеры для тех систем которые используются нашими заказчиками которые нам нужны которые нам нужны эти данные это apache и nginx это докер это кубер найти в принципе тот же вектор умеет из докер прям все забыть или все логе bright напрямую мы сделали естественно какие-то свои собственные у нас есть нужные нам приложения которые мы отслеживаем у наших заказчиков это в том числе агента и систем мониторинга производительности вот ну и например для баз данных графа не это выглядит примерно так как бы это привычный интерфейс тут есть опять же данные по столкнули и сами логе и их можно делать какие-то выборки нужные нам казалось бы что эти данные можно получить просто из метрик здесь мы смотрим на джинкс но если мы из метрик мы не получим как бы вот эти вот низлежащие события здесь мы можем посмотреть и какие-то агрегаты и конкретные события можем провалиться конечно же интерфейс графа но он не для всех данных подходят и вот здесь есть пример когда мы можем отфильтровать по каким-то уже раз паршин им полям это делается довольно просто в графа не grafana есть еще отдельные возможности для работы с лагами из того же локи но это должно быть поддержано на уровне datasource плагина вот в данный момент эта штука не поддерживается вот но это один из наших планов по улучшению в общем в итоге у нас есть данные у нас есть раз porsche иные данные которые хранятся в табличках мы эти данные можем вытаскивать все это работает довольно быстро как мы это планируем вообще использовать дальше то есть конечно же хотелось бы иметь здесь более простой поиск и даже наверное не просто поиск но и аналитику для этого есть у нас во первых данные в кафки с другой стороны мы можем данные запрашивать с того же клика усы уже распашные и здесь конечно же придется как-то написать какие-то свои интерфейсы какие-то свои сервисы и так далее для того чтобы делать например агрегацию вычисления уже это делать нет графа не я как бы в отдельном интерфейсе вот в принципе если мы поддержим в плагине граф анны для клик хауса интеграцию с влогами часть функционала grafana для работы с лагами будет пригодна для этого и но она снимет какие-то вопросы с этого есть еще альтернативные визуализации как мы говорили что мы рассматриваем в принципе варианты используем не только с граф анной но и с какими-то другими сервисами которые ну для нашего задачи будут тоже хорошо подходить ну вот тоже сейчас их из исследуем ну и автоматизация быстрого доступа для анализа архивных логов потому что но сейчас они просто хранятся определенное время но и в принципе если нужно мы их можем выгрузить как-то там заново проанализировать или заново загрузить но это такой пол ручной процесс конечно же хотелось бы иметь больше автоматизировано стена эту задачу вот начинка чему мы пришли в итоге да то есть мы экономим на ресурсах за счет компрессии данных oklick house sun сжимает данные в принципе мы немножко переплачиваем за кафку но в целом мы стали гораздо больше экономить пример те же 120 гигабайт логов который мы говорили в начале они в месяц занимает где-то 500 гигабайт в клика усе но и это гораздо эффективнее чем пластик search недолго индексации данные доступны сразу вот и собственно самое классное что это все можно попробовать самому сам вектор как бы он нужен бесплатный все конфиги основные а они довольно просты и даже часть из них я привел здесь плагин для графа на и он тоже как бы есть в ближайшее время возможно мы его как-то доработаем чтобы это был уже имена интеграции с графа налогами вот на этом наверно все и немножко не все время использовал готов ответить на вопрос и теперь 50 вопросов все твои отлично да спасибо что оставил время пожалуйста благодарю за доклад очень было интересно послушать елка это принципе наверно более для многих ну или для некоторых вопрос следующий клик house каком формате использовали кластерный режим же гипер может сталкивались какими-то проблемами в процессе спасибо но смотрите у нас как бы сейчас пока это некий это решение конкретной задачи да у нас допустим под конкретно нашу задачу мы сейчас это расскать или не на всех заказчиков у нас работает ну вот каждого заказчика по сути по отдельному серверу вот это не общий кластер там нет пока поддержки мультитональности скажем так но это все та же в планах например чтобы это был отдельно отдельно общая инсталляция в которой уже мульти теннесси будет сделано внутри то есть на основе такой вот идентификаторы клиента вот то есть это может быть и в одном большом кластере но опять же конечно там возникает уже свои проблемы по эксплуатации да добрый день большое спасибо за доклад кафка week of hell к действительно боль скажите пожалуйста у вас я так понимаю сейчас за счет специфики плагина или за счет специфики зале зации еще нет по сути дела возможности техподдержки в привычном режиме как с ки банной работать с лагами и вилкой я правильно понимаю да да то есть как бы у нас нету вот этой вот строке поиска грубо говоря в данный момент да но для нашего кейса этого достаточно то есть нам важно смотреть на общий графики нам важно это все использовать как но в режиме мониторинга при этом с возможностью провалиться до конкретного logo да пока этот workflow не такой как бы скажем так четкий как в случае с кем анны вот но в этом и особенность да то есть если вы хотите такой вот прям отработанный workflow вы конечно же переплачиваете за ресурсы за индексацию там излишне и так далее все это и второй вопрос короткий случай докер контейнеров или губер нету у вас вектор идет как сайт корр или прямо в контейнер вкладывается случае с кубер нити сам насколько я помню это это просто вот покаяния конкретно делал вот здесь есть мой коллега сергей вот он может ответить на эти вопросы но насколько я помню вектор сам живет как под в кубер найтись и и он умеет просто со всех остальных как данные собирать вот случае с джокером если у вас просто докер на хостел и запущены контейнеры то вектор он из докер демона просто этиологии вытаскивает женщин все собираем букет добрый день хочу спросить как планируется делать аналитику то есть по сути аналитика это какие-то сложные запросы несколько таблиц тут клик house как вы планируете то решать да смотрите есть определенные you space and то есть мы примерно представляем как мы используем эти лаги и мы под них построили определенные дешворда и так далее как я говорил что у нас в планах это развитие отдельного скажем так интерфейса как бы для именно запросов чтобы мы очень делали нужный нам запрос и уже сервис как бы транслировал это в нужный нам sql запрос самому клик хаосу и уже выдавал эти данные как-то 3 сол есть конечно же запросить эти данные у клика усы можно но вот просто это сейчас нужно делать практически вручную вот хотелось бы иметь ну что-то как в планке грубо говоря какие-то там агрегаторы среднее значение так далее чтобы это может был просто в строке поиска писать час пока это не реализовано конечно это в планах движение планирует просто набору статичных запросов которые заранее загружаются систему ну да то есть это некоторые типовые сценарии использования которые ну мы можем заранее предугадать эти запросы будут уже как бы транслироваться в правильный es que el который будет вытаскивать нужные данные спасибо да спасибо очень крутой доклад интересно услышать как бы альтернативные какие-то варианты как решать большую боль на самом деле меня куча вопросов но я постараюсь задать не все из них самый первый простое наверное alert инциденты что-то с ними есть как делаете да по поводу alert авто есть тут как бы можно это делать силами сама игра фаны то есть во первых вот во вторых опять же поскольку у нас данные хранятся в кафки да у нас как бы опять же в некоторые перспективе то есть мы будем запросы делать напрямую в кафку и уже оттуда это вытаскивать ну alert и запрос это по сути дела одно и то же то есть нам нужно знать когда случилось какое-то событие то есть это будет некий процесс который будет ходить допустим в кафку ну или даже в клика us и вытаскивать нужные нам параметры нужны нам данные по этим параметрам ну например там количество неуспешных входов на это дело будет это отдельный service data сейчас пока allure thing это опять же пока нет доработанная часть но это огромный скажем так пул работу по улучшению этой штуки я на самом деле с точки немножко защищу туда во всех search search есть ставим который позволяет себе сжимать и мир эти индексы ты можешь не тратить 8 терабайт ну вот это все и кушать он может при определенных вещах там меньше ладно мне на самом деле такой вопрос то что по сравнению на самом деле кабан игра фанатам очень они как бы разные никакого drill-down ничего как вы например конечному пользователю даете графа ну то есть вы для него стройте каждый раз или вы просто в киба не ты взял на настроить что захотел очень быстро а здесь как то есть как вы решаете да здесь это немножко сложнее делается то есть каждый дашборд настраивается заранее опять же год под конкретной технологии там есть у жены с него есть определенный пул дашбордов и все то есть соответственно поскольку это отдельный инсталляции это не мульти теннис режим да мы заканчиваем доступа он видит все эти дашборд вот но конечно же сам как бы неподготовленный пользователь и он не сможет так быстро накликать вот именно в такой системе потому что здесь запросы строятся более сложно чем в японии и соответственно не подготовлены пользователю лучше дать просто набор дашбордов который он сможет как-то там фильтровать или изменять просто промежуток времени но не сможет ну ведь не будет хотеть это сделать самостоятельно вот это мы делаем сейчас за него я прям вижу ваше будущее что в цифровых кулуарах тоже встречается вот меня продолжение прошлого вопрос или точнее комментарий прости пожалуйста можешь масочку снять вот на одну минуту до вот так лучше отлично продолжаю прошлый комментарий в плагине для графа на и crack house нет возможности делать аборты я никто не сделал вот мы в прошлом году с этим столкнулись все для этого года назад ноги чуваки ждет фичи request a его пока нет это плохая новость я думаю для вас вопрос такой а вот даже бардо и вся руками на кликается или в этом придумали какое-то wear санирование может быть авто генерацию steam plate of или просто каждый раз на проект кто-то ручками создает ну пока да пока для нас это не является большой проблемой в принципе есть методики как эти даже борды как бы клонировать и делать их просто генератором вот но это мы сейчас делаем просто вручную вот здравствуйте спасибо большое за доклад хотел спросить вот этот плагин и вот система она позволяет делать union по табличкам ну условно мне надо собрать из всех систем ну из разных местом все что связано там с request иди там один 2387 вот то есть я хочу сделать униан и просто ну прям использовать его как киба ну вот первый вопрос 2 еще кусочек использовали ли вы cliff house of ski штуки типа агрегате мираж-3 чтобы агрегировать и сэмплирование ли вы историю чтобы не хранить все просто что-то иметь ну по поводу первый вопрос не отвечу я предлагаю потом в коврах вот мой коллега сергей вот нашим расскажет вот по поводу специфики там использования возможности клика устно опять же мы пока это просто некий вот инструмент наш внутренний мы не используем все что возможно мы пока это не сделаем какой-то как внешний продукт вот но сейчас пока это работает вот в таком виде решают нашу конкретную задачу вот возможно конечно же тут можно много чего на оптимизировать много черного улучшать вот сейчас мы пока это не используем то что вы сказали спасибо за доклад у меня два вопроса поводу лог коллектора первый это вы пришли к текущему вектору как результат research а или вы двигались к нему итеративно пробовали одно второе третье это первый вопрос а второй как быть клик chausson когда у вас количество полей влоги разное то есть если таскать там докер-контейнер лику бернетт основе всякие лейблы аннотации были разные приложения пишут разные разные поля как вот с этим выдавались понял давайте с последнего отвечу то есть во первых у нас нет вот прям вообще полностью безлимитного скажем так без какой-то схема лес логирование да тут у нас есть логирование под конкретные типы источников под конкретный грубо говоря системы и мы под них пишем конкретный парсеры и раскладываем в конкретные как бы можем в в табличку да то есть это не то что это будет работать с любой системой мы это наверно можно сделать ну тогда просто весь лук будет просто лежать в одном как бы поле в кликал свет не то что нам нужно мы наоборот как бы расспрашиваем вот эти сет ключ-значение и и кладем их отдельно в каждую как бы ячейку вот поэтому если нам нужно добавить новую какую-то систему то мы это делаем явным образом мы добавляем нужный парсер мы создаем нужную табличку под это дело и в нее уже пишем ну наверно теоретически можно сделать так чтобы эта табличка на основе там содержимого logo сама создавалась вот но пока это не так у нас есть определенный пол систем которые мы поддерживаем вот это не универсальное решение на данный момент а первый вопрос был как вы пришли к ректору итеративной осерчал а ну я говорю мы сделали какой-то минимальный research на самом деле просто по загрузке secu памяти и так далее сколько какие есть возможности что нам понравилось векторе то что он ну относительно быстро работает относительно очень просто конфигурируется гораздо проще чем на все остальные вот это что он умеет нативно работать с кафкой то что он умеет даже работать с клика у сам он имеет принципе работать со всем чем угодно вот но для наших как бы из кейсов то есть первое это сбор данных с машины он оказался самым о чем-то простым и с минимальным воздействием на машину заказчика то есть мы один раз рассказать или агентов он вытаскивает просто целиком этот лог мэсседж и отсылает дальше вся обработка производится уже дальше вот нам это показалось очень удобно у вас не было шагах что вы взяли там коньки любит нашли проблему и отказались от него сразу же жизнь с вектором ну мы не делали скажем так проектов алленби там прямо таких масштабных мы взяли просто в лобби посмотрели все все все которые были решения вот какие то результаты этом привел но в целом векторном использовался самого начала потому что по результатам этого но лабораторного исследования он себя показал хорошо и был довольно прост конфигурация спасибо большое тем горячее так вот отличный вопрос такой от доводилось ли сталкиваться ситуации когда одно визуализируем ее события растянуто на несколько строк в логах иногда довольно сильно растянуты во времени и каким образом при помощи парсеров это решали есть решали то есть вы говорите про multiline логе да когда какой то ошибка какая-то вываливается на несколько строк то даже не так скорее это понятно да вот именно когда одно событие но там она вот было там минуту назад потом не знаю тысячи строк logo который к нему нерелевантные вот новая сосновая строчка но этот тогда какой то получается не консистентной лог ну то есть на самом деле почему начала обработки страничка и коней завершению страничке разные состояния такое тоже бывает то есть вообще но в принципе как это делается для понятно для этого есть механизмы дело если что-нибудь похожее векторе вот такой кейс я не уверен что мы вообще его рассматривали понял спасибо алексею могли это обратить час платную консультацию открыто отдаете микрофон так легко да друзья еще вам пожалуйста вот слева вопрос здрасте уж пожалуйста нашли клад вопрос по интеграции с up and racing елею подобными технологиями то есть это было бы круто не думали ли вы двери вать вектор с о по dressing или то начинала ними понял смотрите у нас просто для трейдинга мы используем другие технологии да мы используем коммерческие и пьем решения applications for ms менеджмент и в принципе у большинства наших заказчиков они уже как бы использоваться там есть tracing там есть ну всякие автоматические метрики автоматическое вычленение длительности транзакции так далее это то что делает любое обличье школы smash mouth решение вот наша задача была это сбор конкретных логов скажем так как бы мог с простых систем типа там engine x какой-то хай бриз до который там я коммерс вот и мы не внедряли tracing для того чтобы потом как-то это все дело сюда засунуть потому что tracing у этих заказчиков уже по большей части присутствует просто он в другой системе вот системе apple кишки фон с менеджмент но чисто теоретически можно в этот лог добавляете трой сойди и все что угодно то есть это уже зависит от настроек прикладной системы который пишет лог а вытаскивать вектором мы можем все что угодно вообще в принципе любым как бы парсером любым коллектором можно это вытаскивать класть тоже в определенное поле в табличке вот и потом по ним делать какие-то выборки покажи мне все логе связанные с этим trace айди то есть теоретически можно просто у нас эта кисть не используется в силу того что есть уже applications management system спасибо за доклад у меня вопрос по по кафке вы сказали что ее можно смотреть больше степени как хранилище и что вы к этому идете аморт в двух словах зачем это может быть нужно то есть если уже хранится что ты расспрашивать при хаусе так я не туда нажал сейчас секунду ну в принципе это может быть нужно как бы один из кейсов вот такой когда у нас есть например у нас есть данные в кафки они допустим хранятся там очень короткий период ну там 2 дня 7 дней и так далее что мы можем сделать в принципе сам клик house он умеет ходить в кафку и забирать оттуда данные ну их нужно предварительно распарсить поэтому возможен такой кейс когда данные хранятся уже то есть сырые данные хранятся в кафки а потом какой-то отдельный парсинг живут вот этот он отсюда их забрал и положил обратно сюда и уже потом клика us пошел напрямую ставку и забрал можно сделать так мы сделали по-другому мы сделали что сервер который парсит он же их и раскладывает по табличкам но в принципе вот одна из сфер парней про конечно можно одни и те же данные несколько раз использовать здесь мы взяли сырые данные потом оттуда вы читали их расспросили положили в эту же кафку потом их можно например тот же самый лифтинг и поиск на то есть когда мы говорим про быстрый поиск или allure think это можно сделать тоже обращаясь в кафку и уже оттуда эти данные вычитывать потому что это будет в реал тайме как бы как только это событие случилось мы об этом узнаем то есть вылету создавать le tue создавать новые события которые положите на него уже среагировать ну да то есть по сути дела в данном случае мы используем ее как буфер эту кафку но в теории когда эти данные там есть их можно несколько раз вычитывать можно по ним делать опять же какие-то запросы прямо на прямой вк авки вот привет алексей кокин компания нас пока вопрос такой мы храним очень много данных для нас это очень важно и как вы решаете проблему с архивными данными которые может могут год назад произойти потому что иногда у на запрос поступают именно такие данные и если у вас разделения на горячих холодные данные за неделю например и установим скидывается какой-то там понял да но у вас просто карточный как бы до дела у вас есть свои требования регуляторов вот и вам нужно наверно может и больше чем 12 месяцев хранить в нашем кейси мы храним данный 30 дней вот и дальше мы их как бы из клика усатому убираем из кафки еще раньше убираем но в принципе можно допустим тем же вектором отправлять не только в клик house но и в какое-то дешевое хранилище для просто складирования их уже для какого-то там холодного архива вот можно это делать из клика усы как бы куда-то выгружать но нам это не нужно у нас как 30 дней все отсечка прошла мы будем их больше не используем но конечно же для такого супер долгого архивного хранения это нужно как-то продумывать у нас просто нет таких регуляторных требований скажем так данный момент там справа сейчас стажер хочет вопрос задать пожалуйста привет у меня маленький комментарий на предыдущий вопрос в крик хаусе можно хранить в с3 поэтому можно делать холода хранилища быстрее приходи масти обратно вычитывать так мой вопрос об этом вообще про кафку я правильно понял что вас multisense нет даже при доставке логов в клик house и каждый клиент пишет в общей топик и нет ну как раз таки топике это и будет разделение ну вернее это и есть разделение по клиентам а то я скажу клиенту свой топик да а иначе были проблемы с food контролем всю я понял вот то есть как бы но тут можно по-разному делать multisense можно писать все в один топик но допустим писать как бы еще одно поле идентификатор клиента в это сообщение которое может добавлять тот же вектор то есть вектор может добавить любое количество полей татарстане да ну смотрели на вектор и он не умеет брать из метаданных клиента клайн пойди для коннектов кафку вот раньше так было высотной стало интересно появилась или нет я имеют у клиента де мы можем сами задать любой клиент иди и мы можем в конфиге да а как динамические понимать какой клиент какой клайн т.д. но мы же как бы эти конфиге раскатываем по клиентам вот здесь вода то есть у этого клиента 1 конфликту этого другой конфликт это не то что у них у всех один общий конфиг вот допустим файл пить можно и запеку бы доставить клайн to die подсунуть его как параметр int age- кафки но это можно даже некоторые не было всем спасибо но у нас просто беру каждого отдельный конфликт потому что у них разные наборы из ночи целевых систем то есть это не то что опять же у них у всех одинаковый ландшафт разные наборы целевых систем разные наборы вообще как бы данных они лежат в разных местах как у каждого свой конфиг и сейчас тот сладкий момент когда denis возьмет своего коллегу которого он пропиарил уже трижды и они пойдут цифровые кулуары чтобы ответить на кулуарные вопросы дни под нет сраного под запись но тем не менее там мы кое-что можно мимо микрофон и говорить кому мы книжку подарим за лучший вопрос молодой человек в первом ряду у которого был больше всех вопросов по моему он ну все даже успел задай но его приз в том что он в кулуары идет хорошо да да отлично пасибо за доклад спасибо серого что оставил время на дискуссии теперь тоже памятные призы и цифровые кулуары тебя ждут"
}