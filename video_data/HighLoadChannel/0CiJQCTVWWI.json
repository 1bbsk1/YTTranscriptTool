{
  "video_id": "0CiJQCTVWWI",
  "channel": "HighLoadChannel",
  "title": "10 мс на ответ с транзакциями, большими данными, гибкой логикой и OpenSource / Владимир Богдановский",
  "views": 1223,
  "duration": 2924,
  "published": "2023-04-28T06:10:46-07:00",
  "text": "Всем привет Меня зовут Владимир Я работаю в Хоум Кредит Банке мы сделали с командой интересные технически сложная производительное решение и некоторые вещи из него хотелось бы вам сказать немного о себе в бигдате высокопроизводительных приложениях работал в проектах связанных анальным поведением пользователей в телекоме в интернете различные интернет вещи мониторинга автомобилей и так далее теперь немного задачей какую задачу решаем что у нас за проект мы строим платформу для работы с данными Если говорить о данных какими данными интересует мы собираем данные об объекты которые нас интересует в банке это клиенты банка мобильные устройства наших клиентов различные договора заявки и прочее Какую информацию о них мы собираем различные характеристики признаки событий которые связаны с пользователями события связанные с подачей заявок на кредит и прочим Вот и взаимосвязи между объектами Откуда мы берем данные мы загружаем различных источников внешних федеральные там допустим спфр ФССП Бюро кредитных историй и прочие также банки есть много внутренних источников данных которые мы загружаем систему и часть данных мы вычисляем на основе имеющихся теперь немного про платформу Да что у нас за платформа наша платформа стоит из двух частей из двух платформ Первое это bigdata платформа в ней у нас хранятся сырые данные структурированном и структурированном виде в основном мы используем для различных оффлайн-запросов аналитики и прочем и другая платформа это сервисная платформа иногда мы назовём вот платформой её задачи - это обрабатывает запросы в режиме реального времени Вот они и будем разговаривать да значит что это за платформа она управляет сервисом для быстрого доступа какие мы требования к ним предъявляем первое она должна быть современная без всякого Legacy распределённая от каждого устойчивая А автоматизирована обязательно базируется на Open sockey Какие сервисы мы туда э-э диплом - это опер- операционный профиль различные справочная информация Мы в компании использовали мета-древен подход Когда у нас всё покрыто данными различные правила и прочее мы не харднем бизнес-логику один - это некоторые калькуляторы которые на основе имеющихся атрибутов и данных вычисляет различные предикторы а надо будет сегодня интересовать суперпрофиль опер-профиль его задача - это обеспечить работу с объектами режиме реального времени под объектами мы понимаем профили Как пример тип профиле это клиенты банка тип профилей это устройство тип профилей это контракт и прочее тут Важно отметить что мы воппер-профиль не храним сырые данные по клиенту во все события перемещение мышки по экрану и прочим мы храним только состояние например количество перемещений количество заявок у нас могут быть события но и должно быть большое количество например там все заявки за год клиента Да их не очень много таким мы можем хранить в виде состояния Вот и обязательно требование количество профилей может быть неограниченное то система должна масштабироваться обязательно Какие функции выполняет наши профиль Он позволяет нам искать профиле выполнять обычные крановские операции обновлять получать состояние есть операция объединение нескольких профилей в один дегтевация профиля и прочее наши мы к нему определяем высокие требования к строгой Согласно надежности качеству данных это банка сфере Нам очень важно система должна быть масштабируемая гибкая и прочее если говорить про слова Real Time понятно здесь на самом деле честный Real Time это такое не реалтаем если говорить со стороны бизнеса это мы просто должны обрабатывать за миллисекунды десятки миллисекунд Ну может быть до сотни миллисекунсов Теперь если взять требования Как построить профиль я выделил три основных шага которые Нужно рассмотреть понятно что шагов в принципе очень много но это такие ключевые важные Первое это API через которые работаем с нашими профилем мы проанализировали клиентов банки кто будет работать с нашим профилем И большинство из них работает через интерфейс и у нас система она рассчитана на быстрые отклики поэтому здесь синхронный режим подходит у нас основной интерфейс на 99% всех запросов это синхронные бывает асинхронные ответы когда допустим клиент запрашивает данные которых У нас сейчас нету его профиль контролирует загрузку этих данных расчеты прочее это может занимать долгое время Например если мы закрываем выпуску из ПФР она может прийти через три дня вот такой интерфейное Довольно простой понятный Несмотря на то что у нас интерфейс Довольно простой сама обработка внутри она синхронная синхронная сможет вам сервисов их взаимодействий отметить что важный компонент этот фасад все клиенты обращаются только в этот компонент фасад никто не ходит во внутренний сервис oper-профиля это необходимое условие для того чтобы дальнейшем могли обеспечить согласованность отсутствие дубликатов чистые данные предсказуемость безопасность авторизацией масштабильности прочее вот опера смотрели смотрим технический стек на котором у нас базируется профиль первое мы пишем код сервисов Нам нужен надежный язык программирования мы используем Java у нас большая экспертиза в качестве архитектуры мы используем микросервис архитектуру мы пишем код не на чисто Джаве Да мы используем в данном случае это популярный springboot Вот и нам очень важен эта скорость отклика это эффективная утилизация наших вычислительных ресурсов поэтому мы код пишем не в классическом сценарий там блокировками А мы используем реактивный подход используем по максимуму асинхрон асинхронность без блокировок Теперь если взять наши сервисы у нас довольно много в составе опер-профиля как они должны взаимодействовать друг с другом можно сделать напрямую синхронные взаимодействие но это совсем неудобно и это из-за того что все наши сервисы работают с разной нагрузкой Они у каждого свои ритм обработки А у каждого своё время обработки если используете электронные режимы то мы будем затыкаться поэтому мы выбрали простой архитектуру через очереди у каждого сервиса есть Входная очередь все запросы поступают ему в эту очередь Он обрабатывает своим режиме сервисы в этом плане не зависит от нагрузки друг друга условно есть в качестве реализации очередей мы используем кавку А теперь если технический секс смотреть с точки зрения хранилищ который мы будем использовать требования у нас Какие к ним мы к ним они должны работать очень быстро они должны предоставлять нам нужны нам операции поиска а изменение данных получения данных данные обязательно должны храниться надежно мы не можем ничего потерять Вот и также у нас обязательно требование частота данных отсутствует дубликации прочее поэтому Source он распространяется на опер-профиль тоже мы проанализировали посмотрели какие есть хранилища и такого одно хранилища мы не нашли но мы придумали вариант двумя хранилищами в общем-то этот вариант работает вот сейчас реализован для этого мы применили схему из двух хранились Первое это поисковое хранилище мать Stories второе это хранилище которых они все данные вот используя эти два хранилище мы смогли это сделать чуть дальше объясню как значит качество реализации матча мы взяли патч знает в качестве взяли кассандру Почему взяли игнайт игнайт это вообще он быстро обрабатывает данные потому что данные хранятся в памяти это распределённое решение самое такая основная фича из-за которой мы взяли это транзакции из коробки есть Нам без транзакции никуда Поэтому нам хотя бы одно и сохранились должно быть редакционное всё остальное мы пробуем обеспечить внутри э наших сервиса системы взаимодействиям и прочим Вот и на это есть такая важная штука Для нас это персистен свой слой то есть данные хранятся не только в памяти но можно их сохранять и в случае если мы при Стартуем кластер или ещё данные поднимается потом в каши с диска если говорить о кассандре то Кассандра мы взяли из-за крутой штуки из которой называется Atomic batch update у нас важная фишка чуть позже об этом скажу Ну Кассандра есть удобная схема данных очень важно что мы можем записывать и читать гарантированно строгой консистентностью в этом очень гибкая Она позволяет разные режимы записи чтения Вот и Кафка очень быстро на чтение и запись по ключу там десятки сотен тысяч 15 секунд и крутая фича которую мы используем это Partition они тоже позже расскажу значит технический осмотрели которые мы выбрали дальше про подходы приема алгоритм как мы все это склеили связали чтобы добиться наших целей рассмотрим я выделил Здесь тоже не все задачи основные три задачи первое мы должны обеспечить корректность данных второе надежность данных и третье высокую пропускную способность рассмотрим все эти пункты по отдельности Первое это корректность данных что под корректности понимаю отсутствие чистые данные отсутствие профилей дубликатов мы не можем взять какие-то дубли А если по одному профилю приходит конкурентное обновление то все должно разрулиться корректно данные не должны перемешаться и двух обновлений вот мы не должны терять данные у нас бывает так что по одному профиль приходит обновление ПО множеству данных и мы должны сохранить всё атамарно нельзя потерять часть какую-то Вот и коррекция определяет через строгое согласованность тоже об этом по этим пунктом сейчас пройдёмся начнём с первых двух значит отсутствие профили дублей изоляцию обновлений для этого мы используем наш матч посмотрим как у нас она организован Да мониторы что базируется на игнайте мы используем на это не всю функциональности во все фичи а только самые простые это каши мы для каждого типа профилей например клиенты банка или устройства мы строим 2 кэша первый кэш это mapping идентификатор профиля на его идентифицирующие поля Как пример полей фамилия отчество фамилия дата рождения этого поля вот также хранится текущий статус профиля и второй кэш это оптимизация поиска через поисковые комбинации у нас требуется искать профиль и не всегда нужно искать по конкретному какой-то отдельному полю то что нет смысла например искать по имени по имени Вася можно найти несколько миллионов профилей это а нам нужно релевантно один профиль поэтому логически бизнес формирует такие наборы минимальных комбинаций допустим это может быть фамилия имя отчество или фамилия отчество дата рождения есть конечно поля которые состоят из комбинации с одного поля например паспорта вот такие два каша мы строим для каждого типа профилей теперь как мы с этим работаем смотрим нам приходит запрос каким-то инициирующим полями на поиск Мы из них строим все возможные комбинации которые у нас отражены метаданных для данного типа профилей например устройства После этого мы все эти комбинации засылаем во второй кэш игнайта получаем список идентификаторов профилей и по этим индикаторам профиле Мы из первого каша вытаскиваем всю информацию идентифицирующими полями по этим профилям А дальше у нас работает сложный алгоритм тоже работающий на метаданных которые применяют различные правила исключения правила Весов и прочее для того чтобы определить релевантный профиль Значит тут стоит отметить что Match storage он помимо того что просто ищет он отвечает за создание процедура мачинга определяет создание профиль если мы не смогли найти профиль у нас создается профиль только в этом случае мы создаем профиль на текущий момент не требуется команда просто Извне создать профиль если мы нашли мы создаем также ситуация сложная когда мы обновляем какие-то инициирующие Поляны номер паспорта у профиля и он может совпасть с номером паспорта какого-то другого профиля по сути мы получается создаём профиль дубликат и в этом случае мы должны эти профили определять во время обновления и склеивать объединять эти вот за это всё у нас отвечает storage здесь понятно что без транзакций вообще никак значит Таким образом у нас весь Flow по поиску и созданию с обновлением он работает через транзакции мы внутри транзакции ищем профиль и дальше пытаемся допустим что-то у него обновить или его создать если его нету при этом мы используем не жесткий там блокирующий алгоритм Да мы используем оптимистический режим транзакции Когда мы пытаемся найти и создать и обновить Но может случиться конкурирующий по этому профилю она что-то Обновила Вот уже и мы не можем или может быть не по этому профиль Главное чтобы по ключу Да у нас может быть комбинациям описано несколько индикаторов вот если эти Ключи уже были задействованы в другой транзакции обновлены транзакция свалится и пойдёт собственно ретрай Да мы по новой проведём но суть в том что мы гарантируем что транзакция отработает может быть с несколькими главное что мы оставим данный корректно да Вот но и таким образом наш Match storage это ведущий storage В задачах создания профиля обновление интенсирующих полей слияние профилей управлением статусом профилей и прочим за все это мы делаем через Match Orange это нам обеспечивает отсутствие дубли изоляция обновления по одному профилю так рассмотрели эти два пункта теперь по корректной седана Следующий пункт это отсутствие потерь сохранения данных о чем это мы когда сохраняем данные по профилю мы можем передать бы довольно большой объем различные информации мы должны сохранить эту информацию нельзя сохранить часть потерять и потерять какую-то часть или сохранить Вот все наши данные мы сохраняем наш дата сторож Да который позиция базируется на кассандре то есть для каждого типа профилей У нас есть свой кейспейс в котором есть множество таблиц определённых методов данных для этого разреза допустим для разреза клиенты банка Да вот и может быть так что у нас в апдей запросе приходит обновление сразу по нескольким таблицам и нескольким допустим записям внутри одной таблице Вот как нам это корректно атомарно всё обновить для этого мы решили использовать фичу Кассандра который называется Partition Key любая таблица в кассандре она у нее есть у любой записи в таблице есть ключ плюс состоит из двух частей это Partition и Classic Key Partition Key нас интересует первая часть его Partition Сейчас объясню зачем она нужна и мы в своем решении сделали Так что у нас все таблицы для для разреза содержанки равные идентификатору профиля что это вам дает из-за того что все таблицы ID все данные одного профиля всегда попадают в одну партию это выглядит как бы хорошо но может возникнуть вопрос А если данных очень много Я думаю они не влезут партийцы здесь Мы возвращаемся к нашему требованию профили о том что мы храним состояния профили мы не храним всю историю там перемещение мыши клики или прочее и данных могут там на терабайты быть вот у нас состояние оно в разумных рамках для одного профиля или за этого партицию вообще если так говорить что Кассандра для одной партиции предполагает Максимум 2 миллиарда колонок Вот Но нам это с лихвой мы намного меньше конечно используем значит что нам это дает из того что у нас все данные находятся в одной партии для профиля это нам легально дает возможность Кассандра использовать атомарный бачок дейт его нужно использовать если у вас апдейт принадлежит именно одной партиции Ну и получается что мы все данные сохраняем атамарно все данные будут сохранены Значит нам обеспечить Кассандра мы рассмотрели отсутствие пати данных и дальше присмотри корректность хочется рассмотреть Последний пункт строго согласованность о чем это такое Что такое строго согласованность вот есть у нас какой-то клиент есть наша профиль клиент обновляет отправляет запрос на обновление профиля получает подтверждение профиля Что окей Я обновил после этого этот же клиент или какой-то другой может пойти запросом Дай мне эти изменения которые сейчас были произведены и опер профиль должен корректно отдать эти изменения вот если такая цепочка Она всегда истина то это строгая согласованность мы всегда можем получить измененные данные если мы получили подтверждение на то что от сервиса вот в нашей системе у нас довольно много сервисов много их взаимодействий коммуникаций и несколько хранишь как на всю склеить в рамках выполнения запроса Да и нам нужно обязательно упорядочить запросы в рамках одного профиля сейчас я позже расскажу зачем это нужно рассмотрим пример когда можем попасть не согласованное состояние вот есть у нас система мы получили в наш фасад апдейт запрос фасад передал этот запросов апдейтсервис внутренний апд сервис обновил наш отправил запрос на обновление в сторону дальше допустим 100 раз сервис еще команда не успел обработать при этом мы для эффективности производительности отклика можем уже этот момент отдать ответ клиенту о том что мы фасад в результате защищает ответ клиенту что все окей апдейт совершен клиент реально может пойти этот же или какой-то другой сделать дед запрос на Вот мои данные пошел запрос от фасада переходит в сторону сервис 40 сервис получив этот запрос идет в кассандр дата storage получает ответ возвращает Агент клиенту но после этого например может пройти наш апдейт запрос который мы совершили кассандру Это значит что мы получили дед запроса опередил апдейт запрос кассандре и мы получили то что вам давили хотя мы получили подтверждение то что всё намного Вино это вот Премьер не согласовано состояние Да в сложных распределённых системах ещё и с учетом того что мы хотим оптимизировать выдавать отклик быстрее чем допустим вся команда завершена вот мы можем получить такое состояние что здесь можно было бы сделать чтобы этого избежать значит для этого можно упорядочить команды по одному профилю в рамках вот этих очередей если нам пришла следом за ним Get не может выполняться до того как выполнится апдейт то есть после Сначала гарантированно вы полностью апдейт запрос потом бы Get Вот это является решением теперь посмотрим наши очереди они мы используем для реализации очередей кафку что нам дает кафку укавки есть понятие топик который хранит данные вот в рамках топика есть несколько партиций для масштабирования и Кафка гарантирует что все данные поступающей в одну партицию они будут упорядочены То есть все данные которые вы записались в том же порядке будут вычитаны Вот то есть Кафка обеспечивает фифов в рамках партии и выглядит как бы схема Так что хорошо бы все команды нашего профиля одного попадали в одну партицию раз они там порядочены мы получили то что мы хотели теперь как это можно добиться укавки есть такая фича что в сообщении Кафки можно отправлять ключ и Кафка говорить что если вы отправляете сообщение с одним и тем же ключом и гарантированно попадут в одну и ту же партию Ну в общем-то мы это и используем в качестве ключа нашего профиля и получается все команды относящиеся к этому профилю гарантированно попадают в одну партию из того что Кафка гарантирует что смартфон на уровне партиции мы получаем то что мы хотели все команды упорядочены потом профиль тут стоит отметить что в одной партии могут быть команды не только по одному профилю по разным нам важно чтобы для одного профиля всегда были в одной партии это нам Кафка позволяет сделать Таким образом мы получаем согласованность выполнения команд Мы сначала выполним апдейт вот в той схеме некорректной А после этого только будем выполнять деткам команду только в этом порядке Это нам дает упорядоченность согласованность вот ну и кавка ещё до этого дополнительная возможность для рекламы попозже об этом скажем здесь показано упрощенном виде схема реального апдейт команды который происходит здесь происходит несколько взаимодействий разных сервисов несколько циклов обработки от сервиса к сервису но тут Важно отметить что вот все очереди они всегда у нас процитированы по идентификатору профиля партицина половина каждого профиля корректность рассмотрели одну из задач 1 2 то что хотел сказать про надежность данных как мы добиваемся надежность я определил через надежность хранились которые мы используем через даже операции что это такое если у нас операция свалилась Мы обязательно должны довыполнить Иначе мы можем получить проблемы с надежностью и демоцентрации сейчас по пунктам пройдем по этим этим по этим пунктам сейчас пройдем Значит первое надежность хранилище у нас качество реджи используется игнайт и гнает нам дает райт и хотлок это нужно для того чтобы если вдруг произойдет сбой Мы не успели еще в кэш добавить эти данные мы не теряем при восстановлении гнается сначала читается данные за этой лого они накатываются поэтому там лежат собственно сами условно команды события до которые произведены были там это нам дает возможность что мы не теряем данные никакие мы их можем снова переиграть и накатить из коробки репликация вот для надежности очень важно вы знаете мы используем storage на SSD вот поэтому данные мы не потеряем мы их восстановим Если нужен и угнать есть транзакции для надежности нам это тоже очень нужно так Если говорить про кассандре у неё тоже есть райт Лог тоже есть репликация также мы используем режим чтение записи сказанного режиме localorm что это такое говорит о том что мы когда записыва здесь и скорее да И когда мы делаем апдейт апдейт запрос путь запрос с локал корм мы не получаем ответ до тех пор пока нам не подтвердили ответы больше чем половина реплик допустим у нас к примеру 10 реплик пока матчастья реплик не получим ответ апдейт команда не прошла Мы не получим макет Кассандры Вот и тоже самое отличительной мы используем пока мы не обработаем больше чем половину Наших реплик мы не получим ответ Это значит что нас точно есть какой-то пересечение между репликами в апдей команде и варить команде да и это нам гарантирует то что у нас данные всегда мы читаем именно те данные которые мы Обновили потому что какие-то реплика данные могут отставать по разным причинам Вот и сравнивать и берутся данные с последним таймс темпом соответственно из-за того что у нас есть пересечение реплик гарантированное между операцией обновления операции чтения мы получаем согласованное состояние Ну и важно важность надёжности Кассандра - это из коробки которую мы используем про хранилище поговорили дальше Вот про пункт зажимы операции что это такое вот у нас есть определенная система на любом этапе выполнения команды команда может упасть может даже возникли ситуации как я показывал мы апдейт команду выполняем и даем клиент ответ клиенту до того как еще Обновили например в кассандре То есть можно понимать что в любом месте может упасть и к этому нужно Быть готовым Как мы можем это решить Да в качестве некоторых механизмов таких упрощенных упрощенный перечень этих пунктов такие они ключевые Наверное это то что мы используем кафку для сервиса Если вдруг на что-то сервис упал и мы не успели заметить сообщение в кавку мы перестали снова прочитаем Это же сообщение То есть мы снова сможем передать эту команду мы не потеряем то есть мы если мы на каком-то этапе обработали эту операцию то мы комментием в кавку сообщений и больше мы его обрабатывать не будем и дальше по цепочке другие сервисы будут работать значит и Следующий пункт у нас может допустим весь класс стартануться или ещё что-то и в каких-то случаях Мы можем потерять некоторые контекст операции которые нам важен для выполнения поэтому там где это требуется мы сохраняем контекст операции вы знаете дополнительно чтобы мы эту команду гарантированно всегда могли дожать неважно какой ситуации возникнуть мы можем вообще весь кластер допустим дата-центр и свет пропал все перезапустили но все равно это как команду дожмем в конце концов вот и третий пункт очень важный это какая может возникнуть проблема Да у нас операция упала на каком-то этапе Но она до этого момента успела изменить состояние системы допустим она Обновила какие-то хранилище или еще что-то сделала если мы переигрываем то вот то что у нас сохранилось первый раз может повлиять на Конечный результат вот этого не должно быть и вот чтобы это решить такой понятие операции Если вы реализуете ваши логику Так что она патентна это значит что вы можете эту операцию прогонять сколько угодно раз или кто-то будет всегда гарантированной вообще это свойство достигается различными способами Вы можете просто допустим перетирать те же самые данные гарантинно То есть вы не порождаете больше меньше Даст или вы можете допустим удалять потом создавать главное что ваша операция должна была дать свойством у нас это важное требование все операции по обновлению должны быть про надежность осмотрели следующий нас задача осталась это про высокую скорость как у нас достигается Первое это за счет наших быстрых хранилище гнает это обработка данных в памяти это очень быстрая Кассандра это очень быстрое чтение запись по ключу то есть мы используем только именно тип паттерны которые дает нам хранилище мы не делаем какой-нибудь скан по Кассандра или еще чему-то мы используем самый быстрый возможность который нам дают между сервисами кафку Кафка довольно быстрый механизм основная основной пункт за счет чего это достигается это последовательное течение запись последовательно файл пишет данные а читаем мы последовательность этого файла последовательное чтение записи самое быстрое чтение запись к вот и еще много всяких трюковка используется кешиванием также допустим zerokep из-за того что Кафка в цепочке продюсер брокер не изменяет данные просто из каналов канал передавать эти данные то есть не обязательно брать эти данные в приложение что-то с ними делать и потом соки отправлять вот эта штука называется Zero Copy и Кафка этим это используют Когда у нас идёт прямая запись каналов в канал без подгрузки в приложения также еще у нас высокая скорость мы стараемся ответ клиенту отдавать тогда когда мы уже можем хоть мы не выполнили до конца операцию Мы точно знаем что в следующий раз когда клиент обратится за этими данными например то мы точно ему корректно всё отдадим но сейчас мы можем ответ на дверь команду княгине мы этим активно пользуемся и также мы используем активно реализации логики асинхронность без блокировок с помощью э реактивных стримов это нам тоже очень позволяет сильно поднять производительность реализации так начните все задачи мы рассмотрели если простек код мы пишем на 1 Джаве игнать у нас 2 12 кассандр 311 Кафка 28 теперь немного о инфраструктуре нашей сервисной платформы Да как у нас вообще выглядит у нас банки свой дата-центр свои железные сервера мы их объединяем сеть получается у нас кластер дальше мы платформу ставим Не на Железяки Да напрямую А мы используем виртуализацию виртуальной машины через терроформ пакет используется для операционных систем и поверх уже виртуальных машин мы накатываем наши сервисы это хранилище также для запуска наших сервисов один из которых мы сейчас рассматриваем профиль и всем этим у нас управляет дженкинс дженкинс это основная консоль управления через неё все теплоем апгрейдим Можем ли стартануть сервисы можем промасштабировать сервис довольно такой большой гибкий функционал дженки создаются под каждую стенд довольно много всяких Job которые можно вызывать с различными параметрами и управлять стендом значит для мониторинга мы используем прометеос и графану для логирования мы используем сервис Локи которых они данные в ис-3 меню и просмотр этих логов у нас происходит графа не если говорить про железо которое у нас сейчас в проде у нас это 8 серверов значит три из них используется собственно под сервисную платформу для выполнения наших Java сервисов их три машины 24 ядра 48 потоков 256 ГБ памяти дистан используется только для записи логов с приложений Вот таких у нас три машины и 5 машин у нас это для Хранителей для Кафки Кассандры для метаданных сейчас платит которому устанавливают на эти пять машин там машины отключается памятью памяти нам нужно больше потому что они данные в памяти поэтому у нас активно используется оперативная память и SSD у нас используется тоже для Игнатий в Персии далее если посмотреть на какие-то метрики то сейчас примерно загружена около 100 миллионов профилей если посмотреть во второй кэш игнайта то там комбинации примерно 1 миллиард на один профиль приходится примерно 1500 атрибутов Но это наш самый популярный разрез это клиенты Банка в каждом клиенту у нас там до полутора тысяч атрибутов различных собирается из разных источников если говорить про скорость выполнения задержки Да берем 90 процентов всех запросов которым мы Тестируем мы тестировали на продакшене создавали различные перформанс тесты и если взять 90 это значит 90 процентов запросов поезд у нас происходит за 8 миллисекунд обновление за 18 и чтение за 12 секунд и текущая наша инсталляция позволяет поддерживать примерно 5000 просто в секунду это не конкретного какого-то запроса это вот агрегированная Метрика мы примерно прикинули наш общий паттерн что допустим количество матчей операции 50% обновление Там допустим 30 где-то 20 Ну вот и вот это комбинированная метка тысяч запросов мы можем держать Ну и того У нас сейчас несколько месяцев уже в продакшене функционирует успешная Эта система данные у нас не портятся чистые дубликает мы постоянно это анализируем данные надежны ничего пока не теряли надеюсь так и будет и система у нас на протяжении нескольких месяцев не деградировал по отклику на стандартно мы видим стандартный отклик по времени все Всем спасибо Владимир Спасибо переходим к вопросу и вопросов тут уже много сейчас наши помощники будут разносить микрофоны Пожалуйста поднимайте руки по очереди Владимир Спасибо большое за доклад очень интересно Алексей меня зовут Смотри такой вопрос по паче игнайту вот говорить Что он райта хэтлок сразу пишет соответственно там вообще есть какой-то риск потери данных то есть этот Lock он прям вот вернулась операция что записали и оно значит уже где-то на диске или это все-таки больше про проблему холодного старта А если он таки туда точно пишет Зачем Кассандра потому что вроде как тут и транзакции тут и быстро да Алексей спасибо за вопрос значит он надёжный То есть это декларируется у него документации райт хэтлок это в принципе решение которое используется во многих определённых различных системах это и в Кафки и кассандре есть везде используется он может по-разному называться но эта штука она везде есть данные вы гарантированно не теряете если у вас ещё есть Да понятно почему мы не храним все данные только вы знаете потому что нет у нас данные в памяти Мы в нём храним только иденти поля до на текущий момент условно это там там два три терабайта может быть в кассандре у нас около 40 ТБ То есть если бы у нас был кластер условно такой бесконечный вот Ну и понятно что не требуется нам такой быстрый доступ именно каким-то атрибутом всегда а не фигня да потому что это in Memory если классика перезагрузился данные выгнать нужно поднять прежде чем их использовать Это довольно такая процедура поэтому у нас Игнат довольно небольшой только с идентифицирующими полями а Кассандра содержит все данные вот я говорил что у нас 1.500 полей но условно эстетически 500 полей там там 20-30 полей это инвестирующие которые иначе Да всё остальное это на самом деле лежит Спасибо за интересный доклад Мне интересно как вы гарантируете такой быстрый отклик на почти любые команды если существуют две вещи вы складируете команды в топик если какая-то фейлится то вы ее дожимаете до конца пока вы ее дожимаете все следующие команды не выполняются гарантированно и получается что у вас в любой момент может возникнуть из-за любого Бага команда Феликс они за там проблем На кластере за Бага и все остальные команды встают в очередь спасибо спасибо за вопрос первое что ответить мы нечестная система реального времени Да честно система она гарантирует Что Любой ответ будет за определенный за определенный период времени происходить у нас Примерно там вероятность этому мы говорили что 90 перцы у нас обычно так вот про те метрики которые допустим 8 миллисекунды проще Я говорю это при успешной работе основная работа несколько месяцев у нас может быть один вариант был когда что-то где-то там нас перезагружалось и не было доступно в основном это все работает 3 неправильно говорить о том что если мы у нас допустим в партиции лежит много команд по одному профилю там или несколько что мы не обрабатываем другие профиля в рамках этой партии потому что мы используем реактивщину реактивный клиент для Кафки То есть у нас происходит как мы из сказки можно вычислить много команд для разных профилей обрабатывать их параллель понятно что мы комики должны последнюю обработанную команду мы не можем допустим обрабатывать какую-то команду для профиля Вася Вот и закамителем мы так не можем сделать может перед этим могли быть команды которые обрабатывается для кого-нибудь профиля Петя они еще не до конца не выполнены Поэтому в этом случае у нас нет такого прямого что она допустим 10 партий значит 10 потоков обрабатывать они только последовательно Каждый каждый из своих партийцы обрабатывать Нет мы каждый из партийцы тоже параллельно обрабатывается за счёт от реактивного подхода но понятно что в рамках одного профиля мы не можем обрабатывать параллельно мы обязательно обрабатываем последовательно для этого у нас там в реализации очень тоже много всяких сложных механизмов для того чтобы мы храним вот эти очереди в рамках и блокировки в рамках одного профиля то есть чтобы не отправить на обработку профиль который сейчас обрабатывает на данный момент Ну вот за счёт вот этих механизмов у нас достигается Конечно если у нас скопится в каком-то случае допустим сервис недоступен какое-то время у нас может в очереди для него скопиться очень много команд они выполнятся когда сервис поднимется тут Конечно мы ничего гарантировать не можем до конца мы говорим о том что в нормальной работающей среде у нас отклик такой довольно Шустрый и в случае если у нас что-то перезагрузится или что-то произойдёт с боем мы данные не потеряем команду мы все дополнен Да они могут потом в какой-то момент какое-то время идти чуть дольше Владимир Владимир Спасибо за доклад у меня будет два вопроса Первый понятное дело Почему возникают такие требования к консистентности данных но не понятно Почему взялось требование в 10 миллисекунд Time Я возможно прослушал но хотелось бы узнать это именно было такое требование у бизнеса или ограничиваетесь скорость выполнения на мобилке или Почему Надеюсь Почему не 20 50 секунд данном случае 10 это просто такая маркетинговый ход название То есть у нас там Понятно условно окно я говорил matching 8 апдейтом до 20 может проходить вот требование бизнеса есть как можно быстрее потому что очень много процессов которые приходят на которые нужно очень быстро отвечать когда вы допустим подойти заявку на кредит вы хотите очень быстро получить ответ условно у нас система должна гарантировать например что мы за минуту должны дать ответ при этом для того чтобы кредит очень много всяких проверок должно произойти много всяких каких-то бизнес-логики выполнится в том числе обращение к нашим по сервисам должно пройти там алгоритмы по машинному учению и прочему то есть за всю эту минут приходит очень много всякой магии да и мы в рамках этого грубо говоря нам в процессе выдачи кредиты говорят вот вы должны все свое выполнить за секунду но при этом надо секунду там идет несколько обращений к нам то есть мы поэтому не можем просто так говорить что мы операцию за секунду выполним нам выполнить надо 10 секунд а у нас такого лица уже нет вот есть много таких процессов Или например там человек идет по торговому центру нужно быстро определить что он там чтобы говорить что вот зайти зал Мы те уже добрые кредит Может там купить что-нибудь То есть это нужно всё время принимать такие решения поэтому мы строим такую систему быстрого доступа понял Спасибо И еще вопрос возможно несколько наивный мой джунновский вопрос Но почему Если у вас есть такая такое критерий по скорости то почему вы выбрали как язык для написания своих сервисов Почему не подумали про про что-то компилируемые например сишарп или го но тут наверное несколько может быть это первая нам Эта система эволюционно достала до сначала делал банка это не интегратор там базировалась собственно решение использовать Java и прочее Я когда пришел там три года назад пересмотрели архитектуру начали делать но и что-то переиспользовать вот Сначала думали использовать тот самый стек потом все пересмотрели все просто выбросили по новой сделали второй у нас ребята которые над ним работает у них большая экспертизави у нас именно в команде большая экспертизы поэтому мы выбрали Java вот много северных приложений различных делали поэтому Ну понимали что потребности бизнеса мы сможем обеспечить если у нас были какие-то миллиарды запросов которые надо было секунду возможно нужно было что-то другое смотреть Ну по нашим требованиям мы смогли это сделать Спасибо Владимир Спасибо за доклад такое тоже короткий вопрос А вот почему Кассандра они сциллы например или там в кафке самой хранить там Рокс тебе Да первое решение было у нас надумали обойтись бой за то что нам от интегратора досталось ноч бейза есть атомарная запись только по одному плечу В рамках одной таблицы Да у нас важная штука которую мы искали Первое это товарный бачок дейт нам обязательно множество данных нужно сохранять единотомарно те решения которые посмотрели Мы стоим Даже не смотрели вот и плюс использовали тот опыт который у нас есть команде у нас был человек уже который работает до этого в реальном проекте С каким использованием Кассандры вот Кассандра нам дала то что нам нужно это быстрое чтение запись по ключу вообще не запись прямо там десятки тысяч не знаю кто может с этим конкурировать вот и второе это атамарный бачок дейт Но дальше довольно много всяких плюшек интеграции У нас есть Java есть по сути как это так ну прям Я не сказал что мы все решения рассматривали мы хотели на текущем сделать пересмотрели выбрали кассандру заявляется там до 40 раз быстрее чем вот Кассандра вот обычно ее как бы меняют плюс она совместимы в этом смысле Ну возможно я не спорю Ну понятно что у вас весь performance вы знаете поэтому вам как бы вас устраивает О'кей понял спасибо ну в кассандре тоже Кассандра на Мы же используем именно её паттерн основной это запись не по ключу это десятки сотни тысяч операций можно секунду произвести больше чем нам требуется вот я писал что у нас 5.000 PS для банка это сейчас вообще выше крыши пока Понятно спасибо Да еще один вопрос Кого ты импотентность как вы обеспечиваете вы берете просто айдишник вот этого бизнес entity какого-то да и с ним запрос вас формируется и по всей цепочке идёт или что-то приходится мудрить на клиенте там ретрайтить тем же ID запрос Вот как это у вас сделано я бы не сказал чтобы реализацией ему какой-то один механизм используют Нет мы используем Например если мы в кассандре по плечу обновим что-то да Мы точно и мы точно понимаем что мы не создадим каких-то новых данных Мы точно это перём И даже если туда что-то попало повторное пуд без дополнительного Гетта и вот очищения Мы точно можем обеспечить и мы на разных этапах вот как бы анализируем здесь мы допустим должны что-то удалить перед тем как обновить или нет Мы стараемся конечно чтобы ничего не надо было переигрывать там удалять очищать потому что это всё время мы стараемся если можно сделать путь гарантированно а-а не испортив данные мы так и делаем И вот так какого-то одного одного один какой-то механизм мы не используем то есть в разных цепочках там система мы разные трюки используем но важно просто логически У нас вот у нас появилась команда обновления она должна быть депотентно чтобы вы не делали она должна быть такая ну и на всех этапах мы это обеспечиваем спасибо и ещё Вопрос вот реактивная часть - это веб-флакс Это что такое у вас там реактор кто вот мы используем Project реактор Да прямо Вот его напрямую или что-то спринговая взяли типа разработчик что у нас ещё там реализован да и больше архитектор но да мы используем из финга флаг Флакс projector и я так услышал что реактивный драйверы для Кассандры и возможно ли что-то другого тоже А да он как бы там такой не совсем честный реактивный Да так же как и для Игната нету реактивного клиента то есть честно это Например для Кафки у нас для Кассандра игнайта он такой э для игр на это вообще просто обёртка на самом деле адаптер для того чтобы мы сделали в кассандре что-то есть если честно не помню До конца Вот концентрированный по-моему следующий версии там по-честному клиенту А вы сами В смысле дописывали вот эту реактивный у нас на самом деле очень много у нас клиент для кассандре там слой да для это свой для Кафки мы используем стандартные реактивный Спасибо большое Здравствуйте Владимир Спасибо большое за рассказ я вот стригировал словно такой утверждение как игнайт это надежно у меня в связи с этим два вопроса Первый вопрос вы используете персистан storage на SSD дисках устраивает ли вас надежность этого решения И второй вопрос соответственно у вас всего лишь 5 серверов выделенных для игнайта по моему опыту игнайт очень плохо реагирует на проблемы на отдельных узлах условно сбой на одном из узле часто приводит к деградации всего кластера у вас как вы это решаете Спасибо за вопрос значит про поводу надежности игнайта Да его мы используем эти персистент storage для игнайт из его коробки понятно что мы тоже читаем погнали форумы где-то пишет что там какие-то бывают проблемы возникают Мы решили сразу что мы загнать не будем все его использовать мы будем использовать только самые такие базы уже проверенные временем и то что мы используем опыте Допустим мы тоже почитали вроде пишет надежно что нам остается мы можем только проверить мы довольно много всяких экспериментов стрессов делали когда мы ломали очень по-разному сети Сеть ломаем ноду перезагружаем в классе у нас довольно большой период был когда мы активно тестировали игнать и ни разу не было чтобы у нас не восстановился Ну соответственно мы загружать очень много данных поэтому на наш экспериментов Мы никогда не ловили проблем вот что касается продакшена за несколько месяцев Нам пока еще ни разу не приходилось этого даже делать Да понятно что перед тем как указывать в пруд проверяли прямо напроде это все эти процедуры но пока у нас проблем не было ничего сказать у нас несколько месяцев если будут когда-то проблема в будущем потому что расскажу вот по поводу работы в кластере работать на одной ноге в локальном режиме или в кластере наверное в классе всегда потому что есть там нужно обеспечить синхронизацию изменений и прочего но какую-то прям большую деградацию нет И мы исходим из наших требований да то есть вот мы говорим нам она опять пишет PS Вот в этой инсталляции мы не до конца Даже утилизируем эти ресурсы на самом деле мы Под каждый выделили определенную виртуальные машины с небольшими ресурсами какими-то Ну то есть необходимыми для этого и мы добиваемся этого перформанса я говорю что если бы нам какие-то там не знаю совсем Заоблачные нужны были метрики возможно что-то изменилось у нас на пяти нодах на всех пяти но стоит игнать на всех пяти нотах стоит Кафка на всякий но стоит Кассандра ну каждого свои виртуальная машина пока каких-то проблем с перфомансом личинки деградации мы не видели я конечно может быть пока не очень релевант потому что мы только весной запустились продакшн Ну пока так спасибо Привет меня вопрос по поводу сказали что своего клиента написали это простой адаптер просто синхронный режим использования понятно что он работает не работает с реактивной Да но у нас внутри вся логика внутри используется реактивный API Поэтому нам чтобы работать Мы также если будущем будет реактивный Мы возьмем этот клиент конечно сразу Здравствуйте Большое спасибо за доклад мне такой маленький вопрос я не поняла хотела попросить вас пояснить вы говорили про согласованность выполнения операции обновление и чтения что сначала обновление потом чтение И что это обеспечивается последовательной запись в одну партицию в кафку и у меня картинка не сложилась потому что Getz запросы они обычно синхронные Кафка подразумевает асинхронно выполнение вот могли бы чуть подробнее объяснить как ты работает когда Get синхронно поступает вся цепочка как бы синхронно Работает спасибо спасибо за вопрос Значит первое я говорил что у нас есть интерфейс взаимодействия по сути это синхронный да то есть все клиенты наши работают на апдейт команды Get с нами синхронно но внутри Мы выполняем синхронные синхронную обработку для того чтобы добиться добиться нашего трупа распределённости прочим мы используем асинхронную обработку довольно сложная обработка с множество компонент у каждого компоненты есть в очереди тут идея В чём что все команды неважно апдейты или нет внутри они обрабатываются скромно и важно что если нам допустим нужны данные из Кассандры то мы обязательно должны и у нас есть операция update И после этого приходит Get Мы обязательно должны обдать выполнить перед геттом за счет чего это достигается у нас Кассандра напрямую с ней не работает какие-то сервисы Она всегда закрыта своим сервисом у нас данном случае называется дата сторож сервис дата старый сервис у него есть своя Входная очередь для запросов и в этой очереди данные по профилю они упорядочены да то есть если мы отправили в этот сервис сначала команду и потом пришла нам Get команда она точно гарантированно станет после апдейт Мы сначала выполнил апдейт команду по этому профилю потом дата сервис возьмет и будет ее выполнять и отправит ответ клиенту Если говорить о том как достигается Так что у нас вроде интерфейс синхронный с клиентом внутри синхронно для этого мы используем механизм request replay когда фасад отправляя запрос на выполнение в какой-нибудь сервис Он ещё в запросе указывает какой replay топик должен быть поставлен ответ и допустим история сервис генерируя ответ отправляет берет исходного запроса в контексте видит что ответ нужно предоставить в этот ритуал топе он отправить топик А этот ритуал топик читает тот же самый инфасада который отправил этот запрос после того как он получает свою рекламу топика Ответ он в этом же синхронном установленном соединении с клиентом отправляет ему ответ но это механизм называется Спасибо за доклад подскажите пожалуйста 10 или там 18 миллисекунд на запись Это что Это сохранить в кафку сохранить в гнать или сохранить в кавку выгнать в кавку в кассандру и опять в кафку вернуть ответ это время ответа клиенту То есть это не до конца выполненная операция Спасибо Ну а время у нас остается буквально на один вопрос и вот как раз в середине зала есть Владимир Спасибо интересная тема такая Вопрос такой по игнайту вот я так понял что он у вас реплицированном режиме работает все ноты они держат весь объем данных а не рассматривали вариант протекционированным режимом когда просили Чтобы меньше памяти занимал чтобы они вот делились как-то по нодам у нас общий подход то что Для нас это такое относительно новый инструмент и Судя Почитай на форуме люди кому-то Нравится кому-то не нравится мы выбрали Вот давайте мы берем железобетонные вещи которые мы на по крайней мере на форумах Увидим что они надёжные это взять кэш это использовать стандартный какой-то базофицирование Вот мы вот на этих там взять на этих персидствович Да мы думали допустим может быть нам в кассандре хранить эти изменения и прочее то есть есть возможность каких-то других тулах Ну мы взяли просто всё из коробки и вот минимальный набор чтобы решить нашу задачу то есть куда-то дальше мы на самом деле в оптимизацию памяти мы не Пошли потому что мы проанализировали есть на кластере вот ее достаточно для решения наших задач требования есть понимание Сколько сколько памяти плюс у нас система абсолютно горизонтально масштабируемая можем там взять через год загнать ноты вставить вот у нас все Плоское гибко добавляется то есть мы все режимы протестировали добавляем ноту всё действительно у нас масштабируется поэтому куда-то вам сложные вещи мы стараемся идти"
}