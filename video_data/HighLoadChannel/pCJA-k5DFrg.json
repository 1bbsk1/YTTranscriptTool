{
  "video_id": "pCJA-k5DFrg",
  "channel": "HighLoadChannel",
  "title": "TechTalk №4  Как устроен поиск ВКонтакте / Богдан Гаркушин (ВКонтакте)",
  "views": 592,
  "duration": 721,
  "published": "2023-04-28T06:09:45-07:00",
  "text": "приветствую вас на Saint highload 2022 студии онтика с вами Григорий Петров яврон и Богдан гаркушин VK Привет Богдан Привет Григорий Расскажи пожалуйста чем ты занимаешься в VK Во ВКонтакте я делаю поиск наверное это на этом нужно закончить или с этого можно начинать разговор поиск это и продукт и технологии и так далее и команда поиск это огромная просто Тема и наверное такая мечта всех людей которые 20 лет назад называли веб-мастерами сейчас называют seo-специалистами Как это работает Как это ранжируется вот это вот все скажи вот что такое поиск в большом веб-проекте как сейчас выглядит поиск большом проекте Ну разные поиски есть если наверное seo-специалисты больше интересуются про поиск который ищет по вебу но у нас поиск конечно не по выбоищет а по проектам ВКонтакте но тем не менее это реальный холод Это 250 миллионов запросов в сутки это 20 млн пользователей в сутки уникальных которые приходят поискать И это 54 кластера которые выполняют разные функциональные задачи начиная от продуктовых и заканчивая типа как базы данных Ну вот зона от трибутивный поиск Какие стеки технологий обычно сейчас используют для вот такого большого highload поиска технологии может быть разные и тут зависит от того что мы берем в ядре продукта то есть ядром продукта может быть Open Source решения и наверное я к этому буду склонять любых начинающих мастеров или стартаперов которые хотят использовать свой поиск или сделать свой поиск технологии могут начинаться от каких-то cms движков готовые решения внутри cms и это самое лучшее решение на старте проекта когда проект немножко подрастает и уже нужно решать какие-то задачи в поиске как правило вот эти cms-решения уже ну дают ограничения ну ещё можно базу данных использовать хоть как оператор like Вот И в этот момент мы приходим к тому что нужно взять что-то более функциональное и многие конечно хотят сделать что-то свое но лучше взять Open Souls решение на рынке они будут удовлетворять функциональным требованиям базовым и порешают какие-то минимальные проблемы с которыми столкнется Каждый кто решит делать свой поиск Ну например это люцена вес по Сфинкс на базе люцена есть ссор и эластик Search Ну и даже по-моему есть Open Search который тоже на базе люценой который амазон делает отдельно В общем решений на рынке и это не всё решение на рынке много я бы рекомендовал брать Вот такое решение которое уже готовое которое умеет делать которая умеет делать поиск А вот если вернуться к большому проекту к такому как ВК то конечно же движком это вообще не ограничивается и движок только начало Это только часть некой системы у нас с одной стороны функциональные требования с другой стороны Требования по ранжированию что и зачем то есть сначала Как должно искать Ты что должна находиться потом в каком порядке и Следующий вопрос А с какой нагрузкой Это должно уметь работать и все это требует разных архитектурных решений про нагрузку тоже говорил давай поговорим о том что выбрали вы как внутри устроен конкретно поиск ВКонтакте Да мы выбрали цену в качестве движка многие наверное пользуются эластик серчам например начинает иметь свои ограничения в таких проектах как наши потому что ну у нас особые Требования по индексации данных новые требования и по поиску поэтому всю эту обвязку вокруг люцена которую сделала сексе мы делаем самостоятельно собственно в ядре всей истории у нас есть люцена и она дает огромный Профит в плане функционала А над этим всем у нас строятся системы шарнирования репликации и некий бэкент который обеспечивает функциональные требования под каждый поиск поисков у нас во ВКонтакте 54 кластера но да но 6 основных таких крутых больших поиска которые продуктовые это поиск по постам поиск по людям в первую очередь поиск по сообществам поиск по музыке поиск по видео и глобальный поиск который объединяет все вот эти вот поиски вместе э ещё есть поиск по сообщениям чата и по чатам Вот и к этому всему Сегодня мы делаем движок и как-то там кластеризуем исходя из нагрузки исходя из необходимости разместить все шарды и так далее То есть репликацию шарнирование делая дальше над этим мы строим бэкент который функционально решает что это поиск по людям а для этого нужно подготовить данные в таком-то виде столько-то сходить движки и так далее но есть еще одна часть функциональная системы это ML кластер ML кластер ромель он состоит тоже из двух частей оффлайн и онлайн офлайн это когда мы подбираем форму зачем вообще имль нужен в поиске если у нас условно два фактора на которых мы можем построить ранжирование то наверное форму может придумать любой разработчик А если факторов становится 10 то уже предусмотреть все вариации практически невозможно и вот здесь вот есть оффлайн история которая нам подбирает форму онлайн История использует эту форму она по сути выполняет простую функцию Она собирает Все эти фичи которые мы в оффлайне загрузили при обучении она в онлайне их готовит отдает вот в этот кластер который ранжирует и выдает выдает это пользователя смотри использование maschine Learning для ранжирования это конечно хитрость и коварство Но к мэйшен Ленинг решением есть одна такая общая претензия это как у сеточки спросить почему она приняла такое решение И вообще понять что это решение правильно вот как вы понимаете что вы делаете хороший поиск что ваши мл решение находит действительно то что надо а не какой-то разумно выглядящий мусор но все очень просто здесь как раз мы утыкаемся или мы встречаемся с метриками поиска и поиска они ничем не отличаются от любых метрик любого продукта то есть по сути это воронка на входе есть пользователи которые пришли поискать на выходе есть пользователь который удовлетворен поиском или нет И как это измерить у нас между входом и выходом между началом поиска и финалом есть еще промежуточная История это результаты поиска здесь пользователь вёл запрос здесь он увидел результаты кликнул или не кликнул по ним И здесь он что-то совершил после результатов поиска и тут уже история на зависит от сервиса Мы в общем чтобы унифицировать эту историю называем себя это конверсионным действием но конверсионное действие в зависимости от сервиса разные например в поиске по музыке это просушка аудио в поиске по людям это добавление в друзья или диалог с этим пользователем в поиске по сообществам это вступление в сообщество либо это сообщество комментарий какой-то в поиске по видео просмотру естественно и вот мы таким вот образом выстраиваем все наши метрики и мы смотрим Что провели процесс изменения формулы запустили abe эксперимент и смотрим стало ли больше конверсионных действий или нет если их стало больше Значит мы сделали что-то хорошее если их стало меньше Значит мы такое не катим и пытаемся искать дальше неплохо и наверное финальный вопрос в нашем тех токе команда которая все это пилит насколько она большая Ну команда ресурсы это всегда история про ограничения даже если бы у вас был огромный коллектив там в Тысячу человек то мы всегда бы сталкивались с ограничениями но у нас команда сейчас очень сжатая очень небольшая очень эффективная пока же на каждый сейчас мы выделяем одного бэкента разработчика одного человека который обслуживает движки и он один вообще на все сервисы даже два Вот и два Вот и есть еще email специалист по факту Вот такая вот сжатая команда она обслуживает каждый Сервис Плюс еще какое-то ядро правильно Ну ядро в данном случае Вот как раз Back and разработчик он под каждый сервис выделяется Да это все равно десятки людей вот зачем нужны десятки людей если по сути вы используете готовый движок или цену они нужны Затем что движок Это технология а мы делаем продукт То есть это как бы критерий Почему мы вообще выбрали Готовое решение они стали его пилить сами у нас изначально было свое решение но Три года назад мы технологическую стратегию обновили мы пошли в сторону Вот как раз Open Source чтобы не тратить время то есть нам в первую очередь нужно быстрее донести решение до пользователя и не технологию в виде базы данных которая умеет делать это это нам нужно донести до пользователя именно продуктовую историю которая помогает пользователю быстрее находить результат и вот эта команда она как раз занимается обучением моделей подготовка технологии на уровне бкнда оптимизации скорости потому что Ну представьте себе движков мы достаем тысячу результатов а если этих движков если они расшардированы для одного сервиса Ну допустим на 16 шардов это 16.000 результатов из этих 16.000 результатов Реал тайм нужно выбрать всего 1.000 которая мы отдадим пользователя А эту 1.000 надо ещё от ранжировать и вот этим занимается команда Вот это есть её работа подробности Я предлагаю обсудить на вашем стенде с вами были Богдан гаркушин из VK Григорий Петров из еврон Встречаемся на конференции вживую и между докладами в записи"
}