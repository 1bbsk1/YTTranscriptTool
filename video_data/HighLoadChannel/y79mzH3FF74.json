{
  "video_id": "y79mzH3FF74",
  "channel": "HighLoadChannel",
  "title": "Распределенный отказоустойчивый сервис финансовых транзакций / Алексей Бурылов (Qiwi)",
  "views": 2397,
  "duration": 3228,
  "published": "2018-08-16T03:49:30-07:00",
  "text": "сегодня я вам расскажу как написать отказоустойчивый сервис мечту любого распределять chicco он гарантирует выполнение любой операция один и только один раз может пережить падение целого дата центра и самое главное я расскажу как проверить что это все работает q так как мы ожидаем потому что бывают серьезные проблемы соответственно и самое главное я расскажу как сделать так чтоб этот сервер смог пережить злобного админа на самом деле админ не злобно это просто воплощение закона мерфи как говорит что если что-то может пойти не так оно неизбежно когда это пойдет не так соответственно рассмотрим стандартный откуда ты че вы сервис который наверняка есть у вас то есть у вас есть несколько нот которые выполняют обрабатывают запросы клиентов это из облака и если одна из этих нот падает все вроде хорошо но даст и стоит без и все у вас будет отлично работать но поскольку этот сервис должен где-то хранить свои данные реальной архитектуры выглядит вот так с точки зрения злобного админа она выглядит вот так потому что у нас есть один и . отказа на самом деле это не очень страшно поскольку база данных современных очень высокая доступность и соответственно три девятки для такого сервиса совершенно нормально но проблемы начинаются если у нас нам надо стреляться соответственно мы ставим больше баз данных и если бы клиенты работали только с одной из этих баз данных все бы было хорошо но поскольку у нас баз данных несколько штук и клиенты как правило работают со всеми ними очень редко удается всех клиентов запихнуть на одну базу данных то у нас при отказоустойчивости 1 шарды то есть одной базы данных на уровне трех девяток если мы масштабируем сервис до 12 босс у нас даже 1 девятки уже не выходит садится на 1 девятка мало для каких сервисов приемлемо если для однако мы можем пережить падение любой из наших баз то есть мы там настроили как-нибудь хитро так репликацию то у нас получается замечательная совершенно картина при 12 базах у нас доступность четыре девятки я напомню у одной базы данных доступность по-прежнему три девятки то есть у нас все хорошо соответственно такой сервис можно назвать откуда устойчивым это вообще очень удобно поскольку мы можем процессе эксплуатации прямо делать rolling апдейт то есть обновлять базы по очереди обновлять наши сервера по очереди это очень удобно для эксплуатации для всего основной проблемой для того чтобы делать такие сервисы является cup теорема пока в тереме такой сервис сделать невозможных сожалению это очень печально для любых реальных применений cup теоремы состоит из трех параметров это согласованность доступность и устойчивость к разделению согласованность это если все сервисы отвечают одинаково то есть когда мы делаем запрос все сервисы отвечают одинаково при том если мы что-то обновляем она одновременно изменится на всех сервисах для того чтобы обеспечить согласованность нам надо тратить много вычислительных ресурсов это атомарные операции на процессоре это репликация дорогая в общем штука доступность это когда сервис отвечает 500 3p при попытке чего нибудь там сохранить или что-нибудь получить то есть если у нас нет доступности мы будем падать доступность на требует большой репликация по нодам сервисов соответственно повышение доступности увеличивает загрузку наших баз данных то есть у нас больше данных будет лишь устойчивость к разделению устойчивость к разделению это если а связь между нашими нодами может прерываться мы можем терять сообщение соответственно если у нас нет устойчивости к разделению то сеть должна работать не только не терять сообщение но при этом должна отрабатывать гарантированно за определенное время поскольку реальные сети этим свойством не обладают ну какое мы эксплуатируем мы обычно не можем от устойчивостью к разделению полностью отказаться но например в аппаратных решениях таких как нас или там мэйнфреймы мы можем на ходу менять жесткие диски там процессорные модули как раз благодаря тому что там нет устойчивости к разделению там шина гарантирует время ответа соответственно cup теорема звучит достаточно просто согласованность доступность просто разделения устойчивость к разделению они меньше 2 то есть могут быть равны двум но поскольку в реальной системе эту достается очень дорогой ценой она всегда меньше 2 соответственно как вы думаете коптером работает до к сожалению она действительно работает у нее есть математическое достаточно строгое доказательства его не смогли опровергнуть его несколько раз пытались опровергать в реальности как я уже говорил даже два из трех получить очень очень сложно никто это не делает по факту ну давайте рассмотрим такой интересный сервис то есть у нас есть три шарды с каждый шар дай работает свой собственный клиент соответственно если у нас две шарды упадет то для двух клиентов все станет плохо но для третьего клиента у нас будет одновременно доступность и согласованность хотя казалось бы сообщение до двух short не проходят но поскольку они лежат соответственно у нас получается что коптер рима нарушена для третьего клиента но это не совсем так если мы посмотрим эту формулу коптильня с математической точки зрения все параметры дискретные то есть они либо единицы либо ноль но в реальной жизни они могут быть ней дискретные ну и судя по нашему примеру по крайней мере для одного клиента они не совсем дискретные в такой ситуации cup теорема приобретает случайный характер то есть у нас сервис для некоторых клиентов может работать от других не будет работать ну вот такая схема нам как я уже говорил мне очень интересно поскольку часть клиентов все равно отвалилась рассмотрим более такую распространенную вещь биткоины cup теорема соответственно bitcoin кажется доступным согласованным и все с ним хорошо потому что в bitcoin проводит хоть за очень большое но и известное время биткойн проводит финансовые транзакции финансовых транзакций ахмад согласованности отказаться не большим поскольку если мы откажемся то могут быть откат атаки виды двойная проводка или может утеряться зачислении денег на счет то есть мы перевели деньги они куда-то там исчезли просто в процессе ну в реальной системе bitcoin не работает потому что если например злобный админ сломает великий китайский firewall то bitcoin разделиться на две ветки соответственно до ветка будет там вот всем мире другая в китае в китае очень много майнеров поэтому две ветки будут почти одинаковой длины и соответственно в каждой из них можно будет провести списание с одного счета с одного выхода и когда эти ветки объединяться firewall починит то внезапно одна из проводок исчезнет соответственно поскольку два разных сервера ответили по-разному согласованности нет также bitcoin проводит транзакций очень медленно штатное время составляет час а если злобный админ сломает скажем глобальную таблицу маршрутизации трафик какой-то новой зеландии уйдет в канаду то в канаде вообще не будут транзакций проводиться время подтверждение будет несколько месяцев я не могу это назвать доступностью то есть bitcoin в целом cup теореме не противоречить но среднему пользователю это нормально его все устраивает то есть bitcoin для него нормально это доступен и нормально согласован соответственно встает вопрос а можем мы сделать такую систему которая будет обеспечивать нам строгую согласованность но при этом не очень высокую доступность не очень устают высокую стучались разделению но такую что всех пользователю стать устраивала ответ можем соответственно дальше я расскажу как ее сделать соответственно краткий итог отказоустойчивости доступность это практически одно и тоже отказоустойчивые можно считать систему которым может тереть ноды и тогда у него будет высокая доступность к параметры в реальной жизни могут быть дискретно это уже не является коптером и как таковой но тем не менее мы можем с этим как-то жить что они не дискретные если они не дискретная система приобретает вероятностный характер то есть может работать может нет может быть согласованность может нет но чем больше мы на этот не дискретный параметр вешаем тем больше у нас вероятно что все будет хорошо соответственно проектируем такой процессе сначала но любое проектирование начинается с требований то есть мы начинаем с 5000 проводок в секунду но было просто такой бизнес требование чтобы было 5000 проводок в секунду чтоб от системы были куда расти соответственно надо было еще чтоб 1000 проводок на один объект то есть на один счет один агрегат мы могли тысячу проводок провести требовалось строгая согласованность поскольку киви это все-таки финансовый сервис катя у нас очень много всяких стартап-проектов но все они в финансовой области поэтому от строк и согласованности мы отойти не можем и требовалось отказоустойчивость ну просто что приятно было работать соответственно диком позируем нашу задачу в 5000 проводок в секунду делаются шарден гам но это все знают наверно highload все таки вот с тысячей проводок на объект хуже то есть нам надо эти данные как то будет реплицировать по сети проблемой репликации посеете в том что она медленно и банально ну потому что есть эти высокие задержки соответственно нам надо будет тег туда какую-то пакетную обработку чтобы мы могли эти данные обрабатывать по сети ну соответственно на строгую согласованности отказоустойчивость обеспечит нам алгоритм консенсуса рассмотрим стек соответственно и алгоритм шарден га то есть непосредственно то что позволит параллели допустим нам надо перевести деньги со счета 1 насчет 2 можно конечно это сделать и тамара на мой алгоритм это предусматривает но к счастью в текущей задачи нам не потребовалось атомарных проводок они как более согласованы будут требовать больше вычислительных ресурсов соответственно клиент кидает сценарий до исполнитель какой-то сценария этот исполнитель сначала банально связывается шар d1 там несколько серверов делает списание денежных средств потом после того как списание выполнена связываться шар d2 делает зачисление денежных средств после того как все готово он отвечает клиенту что мы все провели или не провели но соответственно исполнитель сценарий это банальная единая точка отказа и злобный админ неизбежное и когда им сломает поэтому проблема решается просто мы делаем 2 исполнителя сценария соответственно поскольку согласованность обеспечивать наш алгоритм консенсуса где-то внизу то никаких проблем в том чтобы добавить исполнители сценарий нет соответственно дальше пакетная обработка суть в чем мы объединяем в транзакции в так называемые группы блоки то есть это список транзакций передаваемые одним пакетом тогда нам придется синхронизовать спасите не каждую транзакцию а целый пакет это будет быстрее поскольку пакет большой в нем может быть очень много транзакций и время синхронизации не будет зависеть от числа трансакция который синхронизируется за раз мог уровень там сетевой передачи соответственно рассмотрим как это работает мы исполнить или сценария кидают какие-то транзакции поскольку исполнители сценарий может быть много то они назначаются просто случайно выбираются клиентам из кластера и это все будет хорошо работать они посылают транзакции на ноты ну злобный админ тут опять постарался сломал связь соответственно ноды формируют из этих транзакций два пакета каких-то и начинают согласовывать после согласования соответственно пока не согласуют исполнители сценарии какие-то уже другие могут накидать еще транзакцию но они не будут приняты но годами зависнут входящим буфере потому что и так работает как на алгоритм нагло потеряете кто в курсе соответственно после согласования на всех нотах оказывается одинаковый блок соответственно часть транзакций в него может не войти но как я говорил система имеет случайную природу мы с этим не должны бороться смириться просто ну да часть не вошла в следующий блок войдет соответственно после того как блок согласован исполнители принимает следующий блок его опять согласует и это все будет хорошо работать и третий алгоритм это непосредственно то что обеспечит нам борьбу skype теоремой то есть требования к этому алгоритму он должен идеальную согласованность он должен обеспечивать этом одну операцию место между несколькими шар даме поскольку изначально такой требование было потом от него мне удалось уйти но тем не менее он должен работать при потере одного дата-центра соответственно в терминах cup теоремы мы можем сказать что у него консистентной один доступность 1 3 устойчивость к разделению 1 3 это в реальных терминах означает что мы можем потерять треть not соответственно и остаться согласованными доступными но соответственно нам надо 3 дата центра если мы теряем один data center все будет прекрасно работать поскольку мы собственно я ношу не дискретную формулу не нарушаем коап теоремы я ее уже назвать не могу ну можно не изобретать велосипед есть готовые алгоритмы paxus сожалению хорошее описание пока это 300 страниц и к тому же он не поддерживает пакетную обработку транзакций ровд неустойчив и не может там арно переводить между нодами соответственно чтобы решите главную проблему у них есть лидер то есть эта единая точка отказа если с ним что-то случается все падает соответственно решается эта проблема очень проста мы разрешаем много лидеров соответственно раз у нас много лидеров мы их можем раскидать то есть по всем новом нашей шарады то есть на каждый шарди просто запустить лидер и любая надо кластер и будет знать куда эти транзакции слать поскольку не надо никаких странных алгоритмов выбора недостаток такого подхода это лишний трафик поскольку это все придется гонять по сети из у нас три лидера ту настроя больше трафика носите сейчас работают быстро это на самом деле не очень большая проблема соответственно алгоритм консенсусу клин ну клиент в данном случае согласовать исполнитель сценария он кидает транзакции на все ноды они там как-то согласуются алгоритм достаточно простой получился то есть и см и весом приходит нам пакет этот в своем мира ван и уже предыдущему алгоритму если большинство нот сформировали одинаковый пакет то мы просто переходим на следующую стадию если часть но большинство не достигнуто мы выбираем блок с наименьшим крышам и повторяем согласование в надежде что вы просто большинством над согласовал один пакет когда согласовали мы соответственно сохраняем блок в persistent на их хранили еще соответственно этот блок уже хранится в отель системном крайние лещина еще не подтвержден мы снова смотрим что большинство нот согласовали блок сохранили в persistent хранилище одинаковый блок если это это произошло то мы снова переходим на следующую стадию а если нет то ждем пока большинство все-таки согласует соответственно когда какая то надо видит что часть но ты уже сохранила данные в persistent на экране члена сразу переходит к следующей стадии поскольку известно что она была когда-то согласовано простите но что-то с кликером соответственно если большинство на цикламен на грани ли блок пирсе снят на и хранилище то алгоритм завершён кто читал описание paxus могут сравнить разницу здесь алгоритм полностью кто захочет может разобраться потом посла видео просто его очень долго наговаривать и того sharding наше все только sharding обеспечивает реально высокую нагрузку чтобы обеспечить большую скорость проведения по счету нам нужно пакетная репликация для большинства задач алгоритмы ровд и paxus вполне хорошее но создать свой алгоритм консенсуса пуску заточенный очень просто и ничего там страшного нету и специализированное решение под конкретную задачу она гораздо проще и лучше работает чем к общее решение но это всегда так соответственно я расскажу теперь вот мы написали это алгоритм как убедиться что он работает так как мы ожидаем то есть test driven development распределенной системы это мой собственный супер крутой мега разом метод к курьер очень долго разрабатывал его краткая схема в общем нестандартный подход на самом деле все части уже где-то были но никто не догадался все это объединили тестирование распределенной системы ну зачем это нам нужно нам нужно проверить работает алгоритм даже очень простой алгоритм такой как мой имеет много очень странных зависимостей когда работает в распределенной системе поскольку там все работает по случайному характеру нам потребуется оценка вероятности это очень важно например тот же ровд неустойчив потому что у него плохая плохое матожидание вероятностная и банально это очень много времени сэкономит для разработки поскольку мы можем его просто методом научного тыка писать я например свой алгоритм сократил почти вдвое после т.д. то есть для того чтобы тестировать алгоритм представим его в виде конечного автомата есть то есть у конечного автомата кто не помнит я скажу напомню это есть какое-то состояние и конечный автомат принимает на вход символ то есть сообщение и меняет свое от конечное состояние в соответствии с этим сообщением однозначным образом то есть на одно сообщение он может перейти только в одно новое состояние соответственно у нас вот есть такой конечный автомат у него он в начальном состоянии инициализирован у него все ноль соответственно первой фазой является инициализации он подгружает наши данные но те которые группировать иль блоков собрал соответственно у него нет сохраненного состояния в президенты хранилище и соответственно мы наш алгоритм изначально представили в виде конечного автомата если мы можем подать ему на вход символ падения моды то есть в нашей программе такого конечно нет ну мы же эмулируем работу системы в тесте соответственно нам нет никаких проблем сбросить его состояние просто удалив объект создав новый соответственно у нас есть такой символ после этого он снова получает на вход с ним в конце лизации следующем цикле но он уже инициализируется естественно в другое состояние тут уже у нас тест остается на мы должны как-то имитировать перезагрузку можно считать это символ инициализации 2 поскольку он в другое состояние переходит то есть надо именно чтоб возможность тестировать соответственно конечный автомат может получить сообщение от других но там от но до 1 1 2 и перейти в согласованное состоянии но для нашего алгоритма а может не получить символ от какой-то ноды то есть его в к 1 символ состоит на самом деле из трех сообщения других not но если мы для трёх нот тестируем соответственно из этого состояния он снова может упасть но при этом сохраненное в пирсе сохранились и состояние должно у нас банально остаться поскольку хранилище persistent на и соответственно при перезапуске он опять окажется в том же самом состоянии уже не нулевом соответственно сообщение от других минут может не дойти и тогда он не перейдет ни в какое состояние это замыкание на самос на самого себя фактически соответственно но у нас власть 33 ноды мы можем объединить три конечных автоматов 1 и это будет тоже конечный автомат просто он уже будет иметь более сложное состояние в не раз больше то есть степенную функцию больше 1 и соответственно он на вход будет принимать уже большой такой символ из трех символов состоящий но это все равно будет конечный автомат и мы все равно можем его тестировать соответственно мы можем подать ему на вход символ что одна надо упал две другие рапунцель и zero вались можем подать символ что 2 надо упал на инициализировал и теперь у нас есть конечный автомат которым мы можем подавать разные символы и прикол в чем мы точно знаем какой результат должен выдать наш алгоритм наш алгоритм должен выдать согласованный блок когда все три конечных автоматов придут к одному состоянию и соответственно мы точно знаем недопустимый результат когда разные части нашего конечного автомата согласовали разное состояние то есть фактически в обычном тесте мы не знаем что у нас должно точно получится на входе для всех параметров тут независимо от параметров мы мы точно знаем что должно получится на выходе соответственно мы можем банально простым перебором поиском ширину перебрать все возможные данные которым можно поступить на вход конечного автомата их не так уж много там падения ноды перезапуск ноды потеря сообщения и корректный проход сообщения фактически все а еще более старые сообщение может прийти соответственно поскольку этих состояний на самом деле очень много то есть очень много вариантов но конечный автомат он тем хорош что так он-то куда он перейдет зависит только от его состояние и от символа какой rip придет на вход соответственно мы достаточно просто можем сохранить все состояние в которые мы уже посетили и наш алгоритм превратиться в банальный поиск в ширину по графу одновременно то есть мы автоматически построим граф всех возможных переходов что куда и банально поиском ширину найдем что у нас получится для всех состояний соответственно алгоритм достаточно простой получается ну к сожалению недостатком нашего конечного автомата является в том что он на самом деле бесконечен потому что у нас бесконечное число блоков может согласоваться при каждой принц или зации у нас согласуется ну появляется новый блок но на самом деле нам вообще неважно какой там блок нам важно либо они отличаются на разных конечных автоматах либо одинаковый то есть мы можем упростить это бесконечно состоянии долг конечного то есть просто отбросив все лишнее блоки самый главный проблемы как у я столкнулся вот в этим поиском ширину я пытался конечно автоматов сглаживать пологом и это вообще бесполезно просто я потратил неделю времени на отладку одного единственного бога и ничего не понял так и не смог понять но решение на самом деле есть у нас же конечный автомат это по сути граф и соответственно нам известны все переходы в этом графе есть такая прекрасная утилита называется grandis она позволяет визуализировать наш граф вывести в его в графическое представление ой а где вообще видео о в графическое представление соответственно получается такая очень простая схема нам потому что не надо все ребра выводить этого конечно графа поскольку в этих ребер там 300 тысяч будет даже для моего алгоритма для боксу со там ваще наверно под 10 миллионов нам надо только самые кратчайшие пути как мы достигли такой какое-то нехорошее условного состояния на самом деле эта схема уже достаточно простая и с ней можно работать я по ней уже отлаживал ну на самом деле достаточно ну это увеличение этой схемы я соответственно для трёх нот текущее состояние сохраненная последний раз состоянии что он получил на вход этот автомат gravis понимает html и надо и сразу можно сформировать такую табличку в html и все будет отлично работать соответственно для упрощения надо просто для отдельного узла где что-то пошло не так уметь выводить самый кратчайший путь до него и все шаги на какой он может перейти в следующий раз то следующим шагом вот для этого узла например выведена это уже очень понятная схема с не реально очень просто был работать надо было сразу так сделать но всем мы задним умом как вы думаете вот мы проверили наш конечный автомат с перебором она вообще будет работать до действительно она к сожалению не будет работать я напомню что cup теореме значения не дискретные мы придумали новый теорем значение дискретной мы придумали новую теорему где значение не дискретные но поскольку в капче римляне дискретные если они не дискретные то значит а у нас всегда может доступность быть равна нулю и циклический переход соответственно это выражается тем что в нашем конечном автомате возникают циклические переходы то есть либо мы переходим на свое собственное состояние либо где-то там поэтому как конечном автомату гуляем но не можем достичь конечного автомата притом поскольку к это коап теорем от adler автобакс осо тоже есть такие циклическое состояния ну соответственно нам надо когда мы будем тестировать проверить что такие циклические перехода когда мы не можем выйти они возникают только если у нас нарушается кворум это первое что мы должны проверить второе мы должны оценить как то вероятность того что когда у нас что-то пойдет не так то есть систему уже разваливается падают но ты там так далее то у нас все будет не очень страшно мы сможем из этого во-первых выйти кайдо все ноты поднимутся это вообще обязательная проверка поскольку не все алгоритм этому условию удовлетворяют во-вторых мы должны убедиться что вероятность того что из у нас там часть not попадала потом все кроме одной поднялись там и с достаточно малой вероятностью засыплемся поскольку мы в такой ситуации уже в любом случае можем зациклиться но должна быть маленькая соответственно для оценки этого существует такая вещь как цепи маркова кто не помнит теорию вероятности это цепи позволяющая оценить вероятность того что мы придем в определенное состояние то есть мы назначаем для каждого ребра какой-то вес то есть например что инициализацию нас пройдет корректно это сто процентов что при попытке получить сообщение от ноты у нас на 30 процентов что надо перезагрузиться но это уже у нас уже авария так что вероятность достаточно высоко и что сообщение там дойдет других not у нас вероятность 50 процентов потому что 30 процентов что надо перри заботиться еще 10 процентов что сообщение где-то там сети по пути потеряется соответственно мы просто для всех переходов нашим конечным автомате а я как их уже говорил не очень много получения сообщений не получения перезапуск ноты получения старого сообщения всего ничего призываем практически от балды какую то вероятность но так на основе эмпирических оценок как у нас кластер будет разваливаться и можем посчитать соответственно какая вероятность что мы придем в каждое состояние и что будем ходить по каждому пути но для этого случая например что у нас фаза достигнет состоянии но нового блока у нас вероятность сто процентов 130 процентов что мы перейдем в начальное состояние это не должно пугать поскольку мы на в него можем перед попасть более 1 раза то есть соответственно у нас 30 процентов что надо уже пойдет инициализируется в другое состояние не такое как была в начале но это соответственно важно мы соответственно можем оценить вероятность того что у нас для одной ноты примется состоянии 50 процентов для трех но соответственно вероятность будет вдвое втрое ниже 12 процентов всего в такой системе но это не очень страшно и соответственно в после того как состояние принялась у нас опять есть 30-процентной вероятность что все упадет таким образом мы с пятнадцати процентной вероятностью оказываемся состоянии согласовано но не подтверждено соответственно из нового состояния мы же тоже можем согласоваться и у нас опять есть 30 центов вероятность в 50 процентов перебить и согласованное состоянии но для того состояния вероятность уже всего 30 процентов соответственно при переходе у нас получается еще дополнительно 15 процентов соответственно опять-таки что все упадет у нас вероятность 15 тридцать процентов на 15 получается всего там 4 процента что ли и у нас вот такая вероятность задается навесом и для всех состояний вот так посчитаем вероятность сначала что мы в них окажемся потом банальным алгоритмом дейкстры хотя по графу мы можем определить все вероятности что мы зациклился если состояние системы не будет изменяться то есть у нас много упала потом поднялась и мы соответственно можно посчитать алгоритмом дейкстры вероятность что мы не придем в результате согласованное состоянии поскольку как я уже говорил одно одно и она хорошо известно к чему мы должны прийти это достаточно просто соответственно цепи маркова тут идеально у нас есть гров у нас есть конечный автомат прямо для которого как будто все и создавалась все переходы нам уже известны просто методом перебора но мы объединяем одинаковые переходы то есть у нас в системе будет очень много одинаковых похожих переходов потом чё-то глюкнуло но не сильно присыпаем вероятности суммируем все возвратные состояния то есть когда наша система не согласовала и с этим вот блин такого алгоритма который я приводил мы считали вышло что у нас один платьев жгут будет застревать из таких неприятностей у нас гораздо больше будет из банального падения путин от одновременно застревать и ничего страшного нету изначального алгоритмы было была ошибка и было почти 20 процент на вероятность что алгоритм зациклиться соответственно это просто ужас 20 процентов платежей буду вставать в процессе ну вот мы все проверили все померили посчитали в вероятности как думаете теперь то пофиг заработала она действительно теперь не заработала то сколько вы не учли один важный фактор это злобный atween потому что как бы наша система была не хорошо написана она все равно упадет поскольку мы не можем менять конфигурацию системе уже когда началась авария это будет противоречить целями синхронизации сообщений в распределенной системе она является частным случаем карте ремонт таким образом она будет противоречить ей то есть если мы не можем обойти коап терему не можем соответственно менять конфигурацию системы когда все упало иначе у нас нарушится согласованность но проблема в том что она будет чаще всего падать когда мы как раз что-то меняем поскольку это просто фактор риска и система все равно может прийти в несогласованное состоянии с некой вероятностью это возможно на урок ли возможно на всем что угодно потому что таков мир такова жизнь и с этим надо смириться не чем мы сделать не можем соответственно надо что то делать чтоб мы могли из этого несогласованного состояния снова прийти в согласованное состояние для этого банально существует проблема решение это блокчейн то есть мы в каждый блок который я уже описывал просто добавляем хэш предыдущего благ и поскольку криптографической функцией он не несет мы нам достаточно добавить ни один два байта просто чтобы различить блоки в разных цепочках и соответственно при следующем начали в следующего согласования алгоритм консенсусу видит что блоки ты у нас на самом деле разные из может это определить и согласовать их уже корректно и соответственно на самом деле решение простое но нам надо не потерять историю после того как это все случилось потому что может позвонить клиент сказать я там зачислил себе 100 тысяч рублей на счет они куда-то пропали и что делать непонятно соответственно нам от достаточно этот хэш в первичный ключ блока блок страница в какой базе в первичный ключ добавляем и у нас все хорошо поскольку мы сможем по истории потом найти чего на случилось и все починить итог короче алгоритм мы сразу пишем в виде конечного автомата лучше даже сначала написать тест какой обходит нашу систему по всем возможным статусом потом к ней уже написать этот алгоритм прима потом его копипастим в прод этот конечный автомат он работает быстрее мы можем это сделать поскольку результат работы известен хорошо мы точно знаем что у нас должно получиться мы это все делаем методом поиска в ширину то есть банально перебирая посылая на конечный автомат все возможные сообщения какое у нас могут быть и мы должны проверить возвратное состоянии то есть циклы в нашем графе потому что мы можем банально зацикливаться и даже если мы реализуем сами рафты ли подсос нам все равно надо будет это сделать это реально просто делается фактически я неделю потратил на написание алгоритма еще детей усмотрел ноги соответственно все но алгоритм изначально все это создавалось как прототип криптовалюты по этому алгоритму может быть использован как решение bgp проблемы но это по сути дела все алгоритма типа raw подсос но алгоритм в отличие от ровд и paxus одноранговой то есть на каждой ноги крипта валюты мы можем его запустить и все будет хорошо работать алгоритм полностью детализован он не требует какого-то центрального узла как он решит что же наконец правильно в системе уже есть блокчейн мы можем заменить коротких и шина криптографически чтобы генерировать центральных блокчейн нам достаточно с помощью древо merkle объединились все каши хороши в один блок и для этого блока тоже выполнить согласование нашем же алгоритму он уже у нас есть готовый и соответственно мы можем получить центральный блок чей таким образом processing лёгким движением руки превращается в блокчейн processing не знаю если кому-то это надо соответственно и итог у нас такой что с коптера мы можем жить конечно да у нее параметры в реальных артериями дискретно но в реально в реальности если мы уйдем от математики можем считать их не дискретными но при этом работа нашей системы станет недетерминированные то есть мы можем внезапно прийти в заблокирована и состоянии мы можем потерять внезапно консистентной то есть согласованность можем мы теряем устойчивость к разделению а устойчивость к разделению это устойчивый с сетевым задержкам то есть у нас в системе вырастут задержки мы можем получить большую скорость если дело сделаем этой системе пакетную репликацию сразу если мы напишем алгоритм в виде конечного автомата его будет очень просто протестировать и главное не забыть проверить на вот эти циклы поскольку про них я ваще нигде не видел упоминания это и мне в самому пришлось додумываться просто глядя на алгоритм и думаешь что же там может пойти не так всем спасибо опрос спасибо за доклад у меня вот вопрос связи со всеми этими вот повторами согласованиями зацикливания my не могли бы прояснить вот пакетная обработка это как но мы объединяем транзакции все в один пакет и весь пакет целиком отправляем наш орду соответственно нам не надо каждую транзакцию пересылать по сети по отдельности у нас есть уже целый пакет который здесь реализуется целиком это происходит очень быстро поскольку но стриминговый детализация вообще работает быстро соответственно нашу чем это детализованным целиком пакет у нас просто инструкции леди действия для сервера что там надо сделать в процессе его обработки они находятся по порядку их можно одним потоком очень быстро выполнить у них не будет никаких конкурентного доступа ничего и это банально очень быстро работает поскольку этот пакет может быть большой туда могут сотни тысячи транзакций даже войти соответственно у нас практически не очень мало ограниченная производительность на на один счет получается потому что вывод говорили что у вас 1000 изменений к одной сущностью до 1000 изменений к сущности это просто минимум на самом деле наверное выйдет больше но больше просто не требовал спадет заданию я должна и тестировал 1000 правильно понимаю что это вот один пакет с 1000 изменений за дарованные переданы по сети будет одновременно выполнен да и потом изменение результата будет за один раз записано вас да именно так она и будет работать и это соответственно по именно поэтому очень быстро поскольку она одна операция идет с это вот если часть из этих изменений из тысячи изменений до потребует дополнительных согласований вот а суть именно в том что мы все дополнительные согласования которые нужны делать между шагами мы вносим их в отдельные пакеты и они согласуются отдельно то есть за тысячу уже готов изменить да просто применить к сущность до соответственно а раз за разделение у нас отвечает алгоритм шарден к который делит изменениями на на пакеты я вас понял спасибо день новый завод надо удачной тестирование здесь вы проводили нагрузочное тестирование да конечно но у нас получилось 250 транзакций на 1 ядро процессора потом ноды виртуальные то есть там ядро даже до реального ядра сильно недотягивало то что очень хороший результат соответственно у нас был кластеры 6 нот развёрнут который фигачил 2013 секунду что ли или где-то так ну упоминали там было 5 тысяч ну да мы сервис может масштабироваться неограниченно поскольку он масштабируется пожар дам мы даже можем на отдельный счет выделить свою собственную шараду в алгоритме поэтому services горизонтально масштабируется и фактически единственный мой его ограничением что при большом числе not он перестанет быть отказоустойчивых поскольку у нас упадёт доступность то есть как правильно я понимаю то что масштабирование реально горизонтальная а не с каким-то коэффициентом да но на самом деле там конечно присутствует логарифм поскольку он у нас скрыт в системе сетевой балансировки но балансировка и сетевых запросов она же как любая сортировка должна иметь логарифмическую сложность и там будет логарифм но он очень маленький и достанут его точно можно при им можно пренебречь a priest анодах у нас уже будет 100 тысячи транзакций в секунду в стока в мире просто нет финансовых транзакций это на одной ноги там вы держите тысячу операций да на одной ноте держим тысячу операций как раз спасибо за доклад вопрос такой если значит когда клиент получает подтверждение транзакции при этом значит от транзакции не согласовано с другими дата-центр ими с другими народами нет клиент подтверждает получает подтверждение именно после того как транзакция согласуется со всеми тремя дата центрами точнее согласуется по кворум этой с большинством дата-центров в нашем случае с двумя из 3 ну то есть а если один отвалился это некритично до 2 то уже для всех сервис будет недоступен к сожалению да поскольку cup теорема нам запрещает оставаться согласованными если у нас свалилось больше половины not я это вроде достаточно хорошо показал типа добрый день здесь спасибо за доклад можете вернуть на вероятности где вас цепь маркова все ошибка какая ну могло очередь и перчатка а пожалуйста поручитель назад слайды где то с 6 10 слайдов простите может и словами объяснить и тут как эта проблема там у вас было изначально определение вероятности событий и на основании этого определения вы прогнозировали вероятность перехода состояния потом на основании этого уже прогнозировали вероятность образования циклов в графе я да правильно я понимаю что вы первичную вероятность оценивали эмпирически при ее какой-то неправильной оценке дальнейшее просчеты в том числе попадания вот этих транзакций в цикл графе она уже даст неверный результат да мы эмпирически оцениваю ну я ценю окна чата можно избежать чтобы не прождать неправильно из при эмпирические по тому же ну можно собрать нереальную стать вероятности вождение цикл можно собрать реальную статистику с уже работающего кластера это будет достаточно точно но по факту можно просто сделать трехкратный запас по эмпирической оценки чтобы сигма 3 туда точно входила и алгоритм просто подобрать так чтобы даже при трехкратном запасе вероятность возникновение циклов было очень мало это ничего сложного на самом деле если мы остаемся в рамках кворума то есть не превышаем наш устойчивых к разделению никаких проблем с обеспечением хороший вероятности нет когда мы уже есть мы при оценке просто можем заложить там в 2 3 раза больше вероятность негативных событий и мы не получим проблем на выходе до входа в цикл да эта оценка выполняется только когда у нас кластер уже развалился то есть у нас все уже плохо соответственно там на самом деле вероятности достаточно большие получаются единственная проверка курю мы должны сделать что если кластер у нас не развалился мы однозначно прямым путем без всяких циклов и без ничего дойдем до согласованного состоянии на самом деле когда все все уже плохо одно но даже я со станет еще хуже ну и фиг с ним спасибо здравствуйте свопе здесь с другой стороны вот во первых спасибо большое за доклад и вот тут уже спрашивали про нагрузочное тестирование вот а что с отказоустойчивостью вы тестировали там это как-то может быть там данном там пытались убить там или scales monkey может там пожить на каком тестом кластере ну отказоустойчивость мы тестировали путем банального путила это не совсем корректно метрологически ну оценка тут же математическая оценка на достоверные любого что мы можем добиться в реальной ситуации я еще сейчас пытаюсь протестировать индекс танкан но к сожалению из-за подготовке конференции не успел бы индекс танком это все прогнать ну так как математически проверенная достоверная система она должна по идее в реальности работать и я вас понял почти большая здравствуйте спасибо за зажигательный доклад отличный как вы восстанавливаете ноды в случае чего ты смотрите у вас вдох ладно но до вдох диск на двух новых вы пока летите до пришел злой админ воткнул новый диск вы восстанавливаете стоит но в момент когда новая но до запустится она войдет в кольцо и соответственно при первом же согласование она увидит что у нее данные отстают сильно от старых not фактически и она начнет реплицировать себе актуальные данные из базы запросит просто у других not автоматически это вот в подробном описании алгоритма есть а еще такой вопрос вы рассматривали перекодирование то есть пусть вас появляется какой-то упор ты клиент который прямо в 1 шард долбится эда харди рование у нас осуществлена в банальным консистенции hishe там кольцо любой кто работал со мной но успели знает что такое кольцо к шее и соответственно мы можем диапазона длиной в один хэш выделить одной ноты директивно что она только с этим hашем работает соответственно а добавление удаления новых нот обычному consist интерес то есть будут изменены шахты только для тех людей которые соответствуют моды ну это просто спасибо спасибо за доклад вот вы упомянули алгоритм txt как поиск кратчайшего а вы рассматривали другие алгоритмы а смысл но алгоритм дейкстры прекрасно работает в этой штуке это как раз поиск в ширину у нас есть все ребра в графе робертом всего 300 тысяч not 370 просто в grid мам декстер пробежать по ним по всем это микросекунды но у неё есть свои недостатки можно же увеличить число not число вебер тогда у нас увеличивается время затачиваем на решение данной задачи да но конкретно в этой реализации такой проблемы не возникло ни где не делать лишнюю работу и все места спасибо настя хотел спросить почему вы отказались от существующих алгоритмов paxum может это сказали ну просто мы хоган разновидность количество есть есть такие которые в принципе не только с одним людям да есть с несколькими да зачем нужна но он фактически я не отказывался от алгоритма pax из одессы вы посмотрите алгоритм внимательно вы поймете что это просто процесс со множеством лидеров я фактически его модифицировал отпилил все лишнее что нам не нужна и получил вот такой простенький алгоритм это реально paxus никто не изобретал велосипед спасибо и еще вопрос а где вы пакетирование транзакции какой момент транзакция приходят от клиента они грант вам просто размазываются по всему кластеру кластер уже знает какие шарды отвечают стать тот или иной счет и он присылает на эти шаги транзакции вот момент присылке транзакций уж ардов есть входящий буфер транзакции в нем накапливаются и в момент когда согласование завершается он смотрит есть мы входящим буфере что-нибудь и все что вы входящим буфере начинает согласовать то есть уже за клиентом на shuh-shuh-гах фактически до уже даже не за клиентом за алгоритмом исполнителя сценария какое объединяет транзакции по шар дам скажите вот тоже получается такой ну как бы не блокчейн а сколько весит этот блок чень то есть какие размеры или вот вас предположим упала одна из нот не нужно синхронизировать то есть заново скачать какие-то блоки сколько это по времени занимает то есть какие-нибудь такие но примерные затраты по размерам и по времени норе синхронизацию спасибо так я честно говоря не помню он пример но что-то там поскольку у меня никаких к шейка ключей так далее не хранится у меня там мы все очень компактно получилось основным затратами были состояния балансов счетов но сейчас я придумал как от всего этого уйти и соответственно того это на 100 миллионов в транзакцией 100 гигабайт бездну часть получится по оценке но я точно центра к сожалению не помню сейчас мне обязательно точно примерно до но на самом деле достаточно компактный к нам уже у нас не настоящий блок ощущений сейчас нам он всю историю хранить не нужно а для хранения текущего среза но то есть за последний месяц чтоб мы могли там агрегата считать вперед отсчитывать их назад нам нужен средств за месяц где-то там реально оценки 300 400 гигабит на весь кластер то есть на сотни миллионов транзакций спасибо в день всем спасибо а если что можете связаться со мной хлор"
}