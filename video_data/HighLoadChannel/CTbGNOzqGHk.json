{
  "video_id": "CTbGNOzqGHk",
  "channel": "HighLoadChannel",
  "title": "Cluster and Resource Management at Facebook / Артемий Колесников (Facebook)",
  "views": 401,
  "duration": 2814,
  "published": "2020-04-14T11:23:48-07:00",
  "text": "всем привет меня зовут артем я работаю в фейсбуке в калифорнии три года и все эти три год я занимаюсь кластер менеджментом и сегодня я хочу рассказать про то как мы построили нашу систему кластер менеджмента для управления сервисами facebook facebook это социальная платформа которая состоит из множества различных сервисов таких как facebook сама социальная сеть workplace это звук для организаций instagram messenger но и так далее этими приложениями пользуются миллиарды людей по всей планете и что свою очередь создает большую нагрузку для наших серверов и для того чтобы вы выдержать эту нагрузку мы строим дата центры по всему миру что в свою очередь создает следующую проблему как обеспечить надежность и управляемость этих процентов например если какой-то из регионов по какой-то причине мы потеряем надо обеспечить выживаемость нашего сервиса и убедиться что остальные оставшиеся регионы могут выдержать увеличенную нагрузку или например другая задача как убедиться что все сервера которые находятся или дацан дыханий имеют последнюю версию программного обеспечения и обновляются быстро и в срок до имея несколько миллионов серверов и сложные иерархии взаимодействия между ними решить эту задачу вручную в принципе невозможно поэтому мы разработали нашу систему артист рации контейнеров или систему управления кластером который позволяет нам справляться со всеми этими проблемами я расскажу моем докладе об этой системе и сделал общего where you и расскажу более подробно про компонент ресурс брокер который один из ключевых компонентов которые выполняют управление ресурсами серверов для представляет этот функционал для систем управления сервисами а мы обсудим несколько тем в начале я расскажу общее описание проблемы то есть какая проблема стоит перед перед компанией которые мы пытаемся решить дальше расскажу как устроен в целом кластер менеджмент общее представление как работать один из базовых компонентов ресурс брокер и в исключении расскажу какие мы чему научились создавать такую систему с нуля какие мы сделали выводы дам ссылки на дополнительные материалы если хоть если будет интересно узнать про про систему прониклась тем больше если посмотреть на фейсбук изнутри то он состоит из множества сервисов внутренних сервисов которые не видны наружу этими сервисами нужно каким-то образом управлять решать много задач и таких как обновление себя с discovery мониторинг правильное распределение по регионам и если вспомнить что регионов у нас много и соответственно серверов этих регионов то тоже достаточно много задач становится достаточно непростой и несмотря на эту сложность facebook как сервис должен быть запущен 24 на 7 и круглый год все по которое и скат которая запущена на наше всегда должна регулярно обновляться и естественно как в любой распределенной системе у нас оборудование уходит вас из строя в случайные промежутки времени но сервисы при этом должны быть к этому устойчиво смотря на то что мы теряем регионы или мы у нас разное оборудование улетает у питание мы должны как бы быть к этому устойчивы все это вместе делает требование к кластер менеджмент весьма специфические то есть мы не можем взять готовый какое-то решение и у нас чтобы удовлетворить чтобы справляться стой стой с тем размерам инфрастуктуры которые у нас есть то есть система управления регистрации контейнеров должна уметь управлять всем этим разнообразием сервисов то есть у нас есть много belkin сервисов они имеют разную архитектуру они имеют разный workload и одни из них это просто readonly стоит лот-сервис и другие это какие-то системы хранения данных у них у всех свои не стандартные требования этих сервисов очень много не разнородны нам нужно каким-то образом не всеми управлять нам нужно управлять всеми серверами во всех регионах иначе как бы смысл такой системы не будет если она может работать только там какую-то часть оборудования сама по себе система должна быть надежной и доступной 20 или на 7 дождь и носить круглый год и также она должна быть устойчива к выходам оборудования из строя все это делает задачу сама по себе не простой но зато интересный значит для решения задачи мы построили нашу систему кластер менеджмента который называется ти w для того чтобы поднять какой-либо контейнер какой-либо сервис внутри фейсбука пользователи обращаются к сервису cw к системе ти до боли указываю свое спецификации сервисы виде так называемой джобс пики и тотже успеха содержит всю информацию необходимого для того чтобы запустить данный сервис например сколько нужно контейнеров какие требования к этим контейнером как эти контейнеры должны быть распределены по по регионам и дата-центром самки w состоит из множества различных сервисов то есть мы используем микро сервисную архитектуру в этом случае и а сны и компоненты это фронт-энд он представляет клиентские pr и например когда нужно обновить новые контейнеры ли получить какую-то информацию о текущем состоянии а клиенты либо силой они обращаются к этому фронтенда дальше у нас есть scheduler ный прокси то есть он занимается балансировкой нагрузки распределяет все запросы которые приходят на обновление контейнеров по различным по различным шутером дальше нас есть непосредственно scheduler scheduler выполняет координирует работу множество контейнеров то есть он если у нас пользователь говорит что мне нужно поднять мой сервис который состоит из ста контейнеров и каждый контейнер в единицу времени должен должна была длиться не более ти процентов scheduler соблюдает эту эту полисе и обновляет контейнер используя например rolling апдейт дальше у нас есть ресурс о локатор он отвечает на вопрос выполняет контроллер ресурсы отвечает на вопрос где запустить контейнер и который попросил пользователь в этом случае у нас а вокодер говорит что а scheduler говорю говорит как защиту именно выполняет то что ему скажут авокадо с лакота возвращает ему список машин исходя из требований сервиса например через говорит у меня есть то контейнер в каждому контейнеру нужно столько лидер столько-то памяти локатор находит свободные ресурсы на доступных машинках и возвращает список машинок scheduler у а scheduler уже исходя из стола каким образом пользователь простил запустить эти сервисы запустите контейнеры простите поднимает запускаете контейнер непосредственно машинках следующий компонент который у нас есть это ресурс брокер он как раз таки предоставляет этот унифицированный вид унифицированную картину серверов и ресурсов которые на этих серверах доступным данный момент то есть он знает печать на вопрос сколько у нас есть машин в каждом регионе сколько есть ресурсов на это на этих машинках сколько доступно в данный момент также занимается координацией между локаторами помогает локатором координировать работу между собой и по сути является охране с хранилищем для данных его котором также у нас есть своя собственная ли зация агента то есть это компонент который управляет непосредственно контейнерами на хвосте cw стык работает в каждом регионе независимо и это позволяет нам быть более менее нечувствительным к ситуациям когда у нас происходит подключение кого-либо дата-центра мы начали разработку наша система достаточно давно в тот момент не было еще некие готовых решений но мы стараемся смотреть что предлагает нам индустрия какие есть открытые либо закрыты решение пытаемся перенимать некоторый опыт на данный момент близкий по функционалу открыто аналог и так обернитесь вот если сравнить плюсы и минусы кубер не this а эти w то получается времени такой список в чем кубер нить из лучше или чем куда лучше после нести до воли-то поверните гораздо проще расширять потому что изначально он был создан для того чтобы можно было использовать в различных небольших сервисах и в этих сервисов разные какие разные требования по функционалу и поэтому разработчики кубер найти изначально заложили дизайн чтобы можно было настроить ее дописать какие-то компоненты самому дальше кубер notice он более юзер френдли опять таки чтобы снизить порог входа для новых пользователей скупитесь лучше посторонись довольно поддерживает persistent forums так называемый глостер или не только тотчас тардис лучше поддерживать различные типы ресурсов то есть из очевидных это джипу и secu сеть и кроме очевидных 10 память он также хорошо подержит же пью и дисковые ресурсы и w с этим не так хорошо например что касается ле пью и так как кабинете расширяем то можно подключать различные старание решения для централизованного сбора логов в cw с этим чуть чуть менее удобно если сравнивать какие есть преимущества у тебя луи пастера низкую берете сам самая большая самое большое преимущество это масштабируемость так у нас большая инфраструктура нам нужно управлять на самом деле миллионами серверов рядовой справляется такой нагрузкой в то время как оберните с упирайся начинаются проблемы когда кластер состоит из более чем 5000 нот или 300 тысяч контейнеров есть различные есть система federation которая позволяет создать как бы набор кластеров каберне this но эта система сейчас в активной разработке настолько мы знаем нет пока готового пока рабочего премьера работы использования в эту систему в продакшене также мы общались с с следами кубинец и и с лидерами разработки кабинете они сказали что сейчас проявить эту кубрина this именно маленький либо средние сервисы это сионист но они пытаются сделать повернуть из более масштабируемым ног убирать называет развивается очень активно и поэтому посмотрим что будет дальше также у тебя будет лучшая поддержка стекл сервисов то есть у нас есть возможность подключить свое расширение для стрел сервиса которые когда контейнеры обновляются он может сказать он обладать информацией где находится данный исходя из этого может ускорить как бы обновление таким образом у нас стоит сервис они обновляются гораздо быстрее также ти w гораздо лучше распределяет сервисы по full доменом так у нас очень важно чтобы сервис был устойчив да не сервиса были устойчивы к различным проблемам мы очень много времени потратили на то как вы выяснить как правильно располагать сервисы по регионам под это центром и попал домена на 3 дата-центров имея глобальный вид всех доступных ресурсов гораздо в кидалове гораздо проще устроен capacity management мы знаем сколько ресурсов у нас сейчас доступна сколько из них использовать каким сервисом то есть мы можем считать утилизацию можно смотреть я насколько хорошо у нас насколько каждому сервису нужно добавить или убрать какие-то машинки также у нас лучшая поддержка made in nc и обработка ошибок то есть когда опять-таки благодаря тому что нас унифицированное представление всех ресурсов мы знаем как выводить машинки с продакшена в случае если случается какой-то какая то ошибка или случается или нам нужно делать и плановое обслуживание оборудования дата-центра также у нас единый runtime а контейнерный runtime для всей компании что позволяет нам когда мы работаем над управлением контейнеров на хостах нам позволяет это стоит учиться на эффективности в ущерб гибкости то есть мы используем например больше плотно работаем с командой ядра разработке ядра что позволяет нам развивать те фичи в ядре которые нам помогают которые нам нужны например мы используем btrfs для дисковой изоляции и игру плиту либо dpf на данный момент тебя будет запущена на 80 процентах серверах 20 процентов это не стандартный сервисы которые по тем или иным причинам ти w не подходит но тренд идет к тому что мы все больше сервисов мигрируем найти w в какой-то момент мы поняли что для того чтобы справиться с управлением растущей инфраструктуры мы решили выделить нам необходимо выделить управление ресурсами серверов в отдельную систему которую назвали ресурс брокер то есть задача эта система стала унификация представление серверов для двух сторон 1 сторона это сервиса непосредственно которые лоцируется на этих машинках вторая сторона эта инфраструктура дата-центр в котором нужно идти машинками управлять например уводить x продакшна случается made in stare дать при этом имея такую систему посреди у нас инфраструктура дата-центров она может не знать ничего о сервисах которые работают на этих на этих машинках таким образом координируя работу внутри дата-центра когда нужно потушить несколько машин для того чтобы сделать мейтенс при этом не думаю что запущен на этих машинах значит ресурс брокер по сути это сервис которая с одной стороны предоставляет унифицирована картину для сервисов с другой стороны предоставляет инфекционным картину для 2 центров также он через него через risus байкер у нас выполняется владение машины с продакшена когда у нас не доступно несколько машин по питанию или по тем другим причинам через брокер нас выполняется автоматическая балансировка нагрузки например если есть ли балансировка сервиса допустим если есть носить специальный компонент который называется или балансировщик он если он видит что какие-то два контейнера стойки использую слишком много сети он перри балансировать и контейнеры на разные стойки чтобы они друг другу не мешали также через ресурс брокер мы выполняем автологическое масштабирование допустим есть какой-то сервис который сейчас находится of pig него не приходит достаточно много нагрузки он может одолжить какое-то количество машин для других оффлайн сервисов которые могут в бэкграунде сделать какие-то вычисления так только нагрузка приходит обратно эти сервисы отдают машинки обратно тому сервиса которые изначально их использовал основным клиентом для ресурс брокер является сервисный локатор он использует данные ресурс брокера как как в source of truth источник информации о серверах а доступных ресурсов на этих серверах так как ресурс брокер является базовым компонентом к нему предоставляются особенные требования самое важное самое интересное требование то что он не должен ничего зависеть то есть например этому сервису необходимо хранить это данные мы не можем использовать такой стандартный сторож которые у нас есть в компании потому что стоит что тоже сервис это сервис запущен найти w5w использовать ресурс брокер для того чтобы эти сервисы управлять поэтому нам нужен какой-то свой столь плеер который ничего ни от чего не зависит также ресурс блоки должен быть доступным высоко-высоко доступным и надежным и данные которые хранятся должны быть стройной системы потому что мы не хотим получить ситуации когда 2 локатора или два сервиса думаю что они владеют одна и та же машинка тайны что буду другом интерферировать поэтому особое требование это целый континент из данных внутри ресурс брокер это тоже набор микро сервисов которые между собой взаимодействуют используя описи вызовы с самым базовым компонентом от центральной является elle decor по сути это эта база данных который содержит данные о серверах и с некоторым и пья для доступа к этим данным этапе вам определять семантику доступа тыс как мы можем как можем читать данные и также пристально паленой гарантии например can карасин control когда у нас два локаторы пытаются идеализировать одну и ту же машинку он традиционно разрешает кто в конце получит эту машинку чтобы на ней уже можно будет можно было бы запускать контейнера один из самых важных принципов ир бикор это то что он не имеет никаких запущенных фоне процессов то есть мы называем его экшн с то есть по сути это просто request response сервис который позволяет читать и записывать данные но фоновые процессы как бы нужно выполнять так или иначе потому что есть какое-то взаимодействие с каким-то внешним клиентами или внешними сервисами для этого нас это вынесены в отдельный процесс который мы называем сайдкик он как бы решает задачу фоновых взаимодействий это сильно повышает надежность системы потому что если у нас в худшем случае ничего не работает все сервис у нас у поля стал столь бикор он все равно может отвечать и сервисного кадры все равно могут работать и читать эти данные в качестве хранилища данных мы используем делос это нами разработанная база данных которая фактически in process любуются вместе с руби core хранить данные машинках запущенных сервисов декор и позволяет быть нам независимыми от внешних сторожей также у нас есть кирби frontend он предоставляет и пей для внешних клиентов и набор силой утилит клиентская библиотека она позволяет работать с внутренними сервисами если нужен фон доступ полной функциональности р декора переживут брокера то есть клиент использует клиентская библиотека таким клиентом является ресурса локатор расскажу подробнее про судьба киркор edios ресурс брокер core запущен в каждом дата-центре и управляет серверами в этом дата-центре как я сказал это просто набор и 5 это набор функций для чтения и записи данных о серверах и он хранит всю информацию о ресурсов сколько машина к доступно какие на них какие есть ресурсы сколько ресурсов заняты и так далее для хранения данных мы используем дело с который ленку это непосредственно в процесс и все данные мы храним на локальном диске одним приложением делась та система которая позволяет реплицировать state через share нaйти шерлок то есть все данные хранятся на локальном диске данные бывают двух типов это непосредственно шаре блок который мы используем для того чтобы проигрывать как бы для того чтобы реплицировать данные ассистент на используя алгоритм вроде подсоса и материализованный state когда мы хотим добавить какую-то запись в ресурс брокер мы записываем команду на записи у шейный лак и дальше мы проигрываем эту запись и как результат мы получаем утилизированы матери зова ный стоит нашем случае это в качестве мотивированы стоят а это рокс baby таблица который потом можно использовать для того чтобы читать данные делась имеет слоеную архитектуру то есть каждый слой шире длог applications where или runtime они независимы и они разделены и пианино можно прозрачно менять как бы реализацию то есть например можно мы использовали шерлоку самом начале для хранения шельфа и налогам использую теперь впоследствии мы переписали сейчас система полностью независимо ни от чего и работает автономно и используя просто локальный диск для того чтобы хранить непосредственно из жареный лук и данные клиентская библиотека представляет собой расширен доступ функциональности ресурс брокера она используется внутренними компонентами кластер менеджмента когда информация о сервисе о сервере каким-то образом меняется допустим сервер стал недоступным он синхронить это информация через синха через цикл синхронизации поступает в клиентскую библиотеку и дальше эти данные попадают в ресурс engine все это такой специальный компонент который знает что смотрит если данные кита поменялись то исходя из этих данных исходя из того что данный это меняется он уведомляет клиентов используя камбеки через очередь сообщений клиенты подписываются на эти кубики например если они хотят знать что когда сервер стал доступного никогда серии стала недоступна нашем случае в случае ресурса локаторы если сервер стала недоступным ресурс локатор может это сервер заблокируют для последующих локаций то есть вывести из продакшна сказал что я больше не буду его использовать на того чтобы пускать новые контейнеры если ресурс локатор хочет освободить резервацию допустимы для того чтобы больше не использовать заблокированный сервер он использует клиентский и 5 для этого дальше эти данные синхронизируются через тот же самый synth loops ресурс брокером имея отдельно такой цикл синхронизации позволяет нам очень гибы контролировать нагрузку например если ого катар начинает много писать мы просто уменьшаем частоту синхронизации и таким образом мы не перегружаем лесу с брокер также благодаря модели call back off нам достаточно просто интегрировать новых клиентов так библиотека используется очень интенсивно нам пришлось интегрировать достаточно новый клиентов эти кубики нам сохранили очень много времени фронт-энд представлять 35 для внешних по отношению к кластер менеджменту клиентам так он является внутренней сервиса монтажа используется использует sqlite лайбрери то есть он синхронизирует данные со всех дата-центров и строить на основе этих данных инвертированный индекс серверов alt инвертированный индекс потом может использоваться клиентами например для поиска для быстрого поиска серверов то есть допустим мы хотим узнать сколько у нас серверов находится пленка центре или сколько из них недоступна по тем или иным причинам или сколько celeron зарезервировано разными разными сервисом для разными локаторами да один из важных принципов на котором основан ресурс брокера то что он не имеет никаких запущенный фоне процессов и для таких десерт запусков багрон процесс мы используем отдельный сервисы attack который отвечает за обработку этих вот этих фоновых процессов например общение с внешними клиентами лишь внешними компонентами у некоторых случаях сайдкик у нас просят например tw агент убить какие-то контейнеры если scheduler который запустил по какой-то причине больше этими контейнерами не владеет также сайтик обрабатывает запросы на вы ласиль урок из продакшена в качестве конкретного примера использования рисуем руки расскажу как мы вводим сервера собственность продакшена в случае метана цели случай когда у нас какой-то причине сиро становится недоступным под нам нужно вводить стираешь продакшна по низким причинам есть у нас есть плановые обслужили например дата-центре нужно сделать обслуживание компонентов питания для этого нам нужно питание отключите сервера больше не могут некоторое время используется production нам нужно видами кластер менеджмент об этом и это один из кейсов которому используем лесу с брокер дальше нас бывает системное обновление то есть обычно нужно было обновить ядро или нужно обновить firmware на свечах которые к которым подключены сервера это тоже приводит потому что сера становятся недоступными также у нас есть незапланированные выхода из строя оборудования которые власти менеджмента же должна об этом знать и не использовать некоторое время эти машины допустим у нас есть кластер серверов объединенных 100 объедены в стойку связанных между собой свечами если switch выходит из строя то все сервера которые на этом свече в свои-то в этой стойке они становятся недоступными у нас есть специальный компонент которые периодически pingui сервера и проверяя доступна ли они посетили нет если они недоступны какое-то время toyota cami компонент уведомляет ресурс брокер для этого он съедает специальный эвент который говорит о том что данный список серверов недоступен за сети этот винт обрабатывается лесу с брокером устанавливать специальный статус для серверов дальше-то статус используя клиентскую библиотеку получает а локатор и он блокирует эту машинку для новых локаций ты смываешь что я больше не буду использовать машинку потому что недоступно локатор в свою очередь предоставляешь еду и ru некий и пи ай который позволяет получить список сестру и ров который давно это недоступно исходя из настроек сервисы например серию sounder может сказать если сервер недоступен 10 минут переместим я на другой copper нечестивым перевести мой контейнер на другой а другую машинку или если это какой-то небольшой над рублёв то ничего не делай и сходя со строек scheduler уже каким-то образом обрабатывает эти события недоступности машины так как серверов много ивентов может быть возникать тоже очень много я не возникают не пасть непредсказуемо например выход из строя оборудования питания или какой-то какие-то события мейтан сильному потери дата центра у нас в каждом от времени происходит каждое от времени мы обрабатывать несколько тысяч и diventa лес для десятков тысяч серверов . r бикор экшн лес для надежности у нас обработка ментов вынесены в отдельный компонент который называется сайт тик-так это работает то есть у нас некие компоненты говорят ресурс брокеру что машинка не доступно дальше то выпадает все в очередь обработки request of и сайте катере песто обрабатываю устанавливать специальный статус в таблице серверов дальше этот этот статус получает ресурсный локатор и говорит scheduler у что машинка не доступна с и долю свою очередь каким-то образом зависит от того от настроек сервиса то 40 scheduler и каким-то образом это это событие обрабатывает и например машинка больше не используется а в текущий момент и лесу сборки работать в продакшене в каждом дата-центре и является источником данных ресурсов для всего всех компонентов plaster менеджмента и управляет ресурсами каждого сервиса и обрабатывают десятки миллионов запросов в день заключение низких предложений что такое ресурс брокер это базовый тир зиру компонент который не зависит от внешних сервисов координирует управление разделения серверов между различными владельцами сервисными локаторами либо сервисе непосредственно координирует управляет доступностью управляет доступностью capacity в в наших центрах и также работает в одном серверов из production есть они не доступны в какой-то причине значит когда мы приступили к разработке ресурс брокера мы постарались решить с чего всего текущего опытом и постараюсь решить те проблемы которые у нас стояли на тот момент некоторые выводы из этого то что мы те уроки которые мы выучили создание отдельного компонента который предоставляет картину мира серверов это очень позволяет сильно упростить координацию между инфраструктура до центров и сервисов также создание in process база данных в принципе возможно строй консистентной процесс база данных и она решает проблему zero db насти которой она стояла при разработке ресурс брокера держать компоненты простыми решающим мало задач ну хорошо это жизненно необходимо и очень и и важно то есть ресурс блоке находится сейчас в очень как бы архитектур находится в очень удобном месте с точки зрения создания быстрых но необязательно эффективных решений и фактически такому лицу коммуникационная шины который объединяет все компоненты кластер менеджмента и очень часто возникает соблазн добавить некоторые от хёка решения которые бы решило проблему в данный момент но в будущем могла бы нам сильно навредить и поэтому очень много усилий мы тратим на то чтобы понять как решить ту или иную задачу при этом оставляя ресурс брокер или другие компоненты простым простыми это достаточно сложно само по себе также создание богаты библиотеки клиентской сильно упрощает интеграцию остальных компонентов потому что нам приходилось бы решать одну и ту же задачу снова и снова если мы подключали бы новые и новые компоненты ресурс брокеру благодаря этой библиотеке мы это сильно все упростили для более полной картины как устроен кластер менеджмент и сервис вокруг него я собрал некоторые ссылки которые можно почитать из общее общая общую view кластер менеджмента описание дело со система хранит данных которые мы используем для рисовых брокеры и описание нашего текущего проекта компьютер за сервисы overview на этом все спасибо за внимание российский вопросы а еще одна маленькая заметка у нас в 12 раз начнется хедж-фондов лег и будет первая подсказка опубликовано во всех соц сетях хай-лоу да и в тот то первые несколько людей которые пройдут этот квест получат аппу lisbor спасибо огромное а сейчас будет сессию вопросов-ответов и задача выбрать самый понравившийся тебе ответ дорогие вопросы как много добрый день спасибо за доклад интересно было я тут вот этот компонент который стоит в дата-центре это ресурс брокер да это называется что происходит когда он ложится а если уложиться то как бы каждый компонент каждый ресурс брокер он управляет непосредственно дата-центром которым он находится здесь он полностью ложится это значит что конкретный это дата центр он не сможет не сможет управляться кластер менеджментом но остальные до центра они как будут независимо работать да то есть как бы он он да как бы единстве что теряется это возможность обновлять контейнеры и собрались хочется новый контейнер поднять или остальные системы работа второму независимо если теряется как бы он труп лейн спасибо что за доклад скажите вот вас миллионы серверов и наверняка не все одинаковые соответственно впр разные secu это как-то обрабатывать учитываете да у нас есть мы разделяем машинки на разные типы вот сейчас есть несколько типов машин есть мы пытаемся унифицировать и в пределах одного типа и дальше как бы каратэ когда мы хотим запустить в этот контейнер мы указываем какой тип машины мы хотим зависимости от задачи например 3 сервис который просто греет циpкa у него он скорее всего убьет один тип машины если это сторож выбери другой тип машины а дальше уже как бы каждой машинка выделяется сервису в практически автономно и пользование и сервис может делать с не как установить что угодно настраивать каким-то образом там и пытаться и утилизировать на сто процентов привет спасибо за доклад такой вопрос ты говорил что если надо для maintenance а вывести машину то это делать сайдкик а как например проходит апдейт ядра критического какое обновление безопасности когда надо обновить есть дата центр как много времени это занимает я честно я не знаю точно как это много времени занимается какая конкретно цифра но в принципе как бы с точки зрения механики и как это устроено нет разницы мы хотим сделать мейтенс быстро или медленно то есть мы просто мы проходим проводим фетиш сама процедура мы также создаём monavie белить ивенты мы также их обрабатываем просто вопрос о том что как мы их как мы их schedule им то есть мы хотим например делать медленные апдейт есть мы вводим помине сколько машинок либо секунд и критический обед мы вводим больше машины по единицу времени вот если мы разделяем наши дата центры по full доменом и мы сервисы знает где находится файл домены и распределяют себя по ул домену то в теории мы можем один фундамент просто такой потушить и сделать сюжета нам нужно потом поднять и другой фундамент критических ситуациях мы принципе можем использовать эту технику делать обновление с очень быстро спасибо за доклад сергей старицын do the line не следующий вопрос если я верно понял вас один фраг коммутатор на стойку и сервера к нему подключены и в случае выхода из строя вы их детектив и отвал и почему не делаете мульти шоссе лак например и не цепляете на 2 top of rack коммутатора я пример пламя слов понял я как бы не говоря два коммутатора которым сервера подключены двумя сетевуха меня понял деда я не знаю почему так как бы сейчас такое состояние я могу только спекулировать на тему наверное как бы экономии денег или сюртук так он дай там уже 3 правда много и в принципе наверно если положить на одну чашу весов что киев нас будет всегда доступна или мытья и надо теряя машинки и versus сколько денег нужно то чтобы они всегда были доступны она решение принялась в пользу там того или другого но я как бы точно я не знаю почему так но все что выше нас есть хорошая статья про network фабрик на нашем блоге инженерным и там как бы описано то есть как он столь на сеть и выше свеча как будто мриданге там все что угодно может производить произойти на подойди на взрыва и как бы никто машинки вот так же доступны вот но после таракан есть нюансы у меня вопрос тоже про сайдкик поскольку мейтенс делается садиком через очередь она может забиваться и это увеличивает blast radius судя потому что мы слышали вчера на конференции как вы с этим боретесь является реальной проблемой как бы в теории такая проблема возможно но в принципе так это способы как бы у нас есть очередь можем и обрабатывать много способов каким образом обрабатывать эту очередь правильно другой есть некие классические подходы то есть у нас фактически есть через сообщения и мы можем в теории сделать бесконечное число экзекуторов этих сантиков которые будут вычитывать зато очереди и делать обработка этих как бы сообщений если сообщение зависимо между собой если как бы порядок обработки этой очереди не важен домой принципе можем сделать несколько процессоров и они как бы ну они конечно могут забиться но это как бы стелется горизонтально вот есть нюанс в том что идет некоторые сообщения не меж собой зависим например мы отправим сообщение машинка ушла машинка пришла если мы сделаем rior doing машинка у нас будет как бы уйдет навсегда поэтому нужны вот это учитывать может быть да говоря если у нас опять таки нас разбито по full доменом если мы видим что 2 или 3 full домена недоступна мы имейте нас не делаем потому что иначе нас совсем будет сапог и это как бы все мониторится опять-таки спасибо за доклад константин шинкаренко вконтакте такой вопрос вот вы сказали что если несколько сервисов запущена в контейнерах на серверах одной стойке то вошли балансировщик их разнесет на разные стойте если они считают ну хорошо если они выживают много сети вопрос в том как он их перенесет прямо фронтами если у них например их природа не подразумевает консенсус или если именно в данный момент просто происходит но какая-то какое-то очень жирное действие имеется ввиду а в чем проблема то есть мы у нас другим выбран тайме увидите контейнеры для что произойдет до нас как будет лив мая гришин а то есть принципе у нас есть некоторые требования к сервисам когда мы чтобы ли того чтобы сервис за был запущен нашим ран тайме нашим облаке другая внутри к нему есть некие некий набор как бы требований на точно должен удовлетворять например мы не поддерживали в мой гришин поэтому как бы сервис должен уметь если он моего потушим и поднимем на другой машинки то для него это не должно быть неожиданность нашей стороны мы предоставим тоже некоторые гарантии например если мы хотим тебя перенести из того что уважаешь сеть мы сделаем уведомления там за какое то время то есть мы не сразу же убиваем машинку не в контейнером и сначала отправляем ники и 12 но dice который говорит что скоро мы тебя убьем по какой-то причине и дальше даем это какое то время чтобы он сделал все что он хочет дальше мы убиваем поднимаемого машинки как так это работает спасибо за доклад александр плотников gps вот вы говорили что преимущество тебе покажется перед губером тем что купер плохо сканируется какие-то а какой-то анализ проводили то есть там 5 тысяч note 3 стака контейнеров что в кубе и начинает первую очередь разваливаться хранили что мы не знали там что мастера какие компоненты не терпят первую очередь отказывать начинают при сильной нагрузке хороший вопрос я честно говоря не знаю мы проводили на лети куда мы общались то есть мы собирались встречались с разработчиком каберне tissot задавались этим вопросом из какой решительные проблемы но конкретно мы то есть круговая для того чтобы попробовать начать пробовать к обернитесь нужно просто ответить накида база вопросов имеет лет смысла винит правильно то есть мы не дошли до момент когда у ким и поняли что наверное имеет смысл попробовать создать какие-то базовые вопросы как бы поддержит ли он хотя бы вот такую is the reason искали что нет у нас другой приоритет мы работаем над функциональность условно говоря и пойду к вам его кем мы не будем даже даже пробовать поэтому не могу каком компетентно ответить что там именно ломается я служу что эти де начинают как бы немножко затыкаться если да то есть как бы это это возможно да да можно я продолжаю иван глушков посмеет я сперва хотел бы ответить на вопрос да за такой цене найтись идеи и я как раз хотел спросить про архитектуру я не очень понял scheduler ресурс брокер они работают на одной машине и это в одном и том же в один и тот же стареть они пишут или нет бы они были разнесены как компоненты но непонятно было после того как приз информация о проблеме на одном из инстансов приходит в ротик было сказано что она потом должна попасть scheduler соответственно это получается разные базу данных как бы scheduler независимо это независимый сервисов другая в каждом регионе и обеде кисть это несколько реплик да то есть как бы в одном регионе может быть несколько реплик шутеров они мир собой это полз независимой сервис сервис а вот и у них свой сторож они как бы хранят данные где-то сами но у нас как бы шериф флит то есть каждый локатор знают про все машинки и как бы он может не вводить машинку данный момент но может захотеть эта машинка получить в использовании и поэтому он должен знать что такие машинка доступны или нет это это понятно я имею виду что как раз это очень сильно похожи на но кто-то как делается в cabernet с единственное отличие что у вас получается возницын ее компоненты которые следят за сервисами те которые schedule от их соответственно это совершенно разные базы данных и у них может быть другая другой скилл соответственно 5000 это из-за того что затыкается идти sedef cabernet если он не может писать информацию большие сеги нужно для того чтобы делать аппликацию и файлов можно было меня спросить у вас получается делается на другом уровне и вас какая-то древовидной структуры есть или как то есть вот сколько записи может делать массаж брокер лишь адуляр в свою базу данных как бы мы ведь хранить нужно лики данностью у нас надо хранить информацию обо локациях образе рваться которые у нас фактически хранить свои со сборки там gaga нагрузка нас она не такая большая получается все это так как у нас они шарди раваны повода центром вот в одном да центре может быть там несколько тысяч машин в принципе как бы события машинка появилась машинка ушла хочу зацеловать хочу дела тировать они возникают что часто но это не на мне миллиард рпс как там в того приходит например там как таковой проблемы затыкания баз данных нет значит сами scheduler они скоро они достаточно интенсивных данный пишут но scheduler устроен таким образом что у него есть так он не распределён торин компонент при suzuki распределенным у нужной консистенции там другие проблемы scheduler он как бы единый компонент он хранит данные в виде да это чанков эссе реализует их и читает чан коми поэтому это же очень быстро внутри он содержит в памяти индекс огромный индекса несколько десятков гигабайт поэтому как бы опять таки там тоже нет проблем с performance пусть он как бы синхронно грубо говоря асинхронность синхронизирует данные в этот оля была в стоишь поэтому в принципе у нас нет места где можно заткнуться именно вот на на чтение из базы данных я честно я не знаю почему это не затыкается как бы как он устроен но как бы у нас такой проблемы нет ну мы тоже вроде как на выбор собачьим там не услышу до последний вопрос дорогие здравствуйте александр интерфейс у меня два вопроса есть первый вопрос что под капотом находится машина которых развернутой компоненты тебя будет это виртуалке этой железкой и как они deep ловятся ну то есть когда дата-центр напилась как они deep ловится это первый опрос и второй вопрос изначальной концепции вот эта вся структур разработана для devops а для инженеров кто тепло и через эту систему может ли разработчик bi будет если разработчик может де площадь то как разграничено и права ну то есть допустим я познаю системы где разработчик виде только то куда он может появиться инженер видят только так ударным продакшен пример теперь вопрос мы использовали этих контейнер то есть это неважно специалист по контейнер на самом деле мы спим мы запускаем используете синди создании виртуалки это вот контейнер и детализация то есть мы для изоляции мы используем оси групп свиту вот соответственно даст не виртуальная машина второй вопрос этих компонента и компоненты и дабы они из поля не запускаются в принципе доволен то есть это вы использовали того чтобы когда вы тоже как бы там есть некие некой проблемой вы курицы курица яйца до что запустить 1 то есть мы у нас есть за метр scheduler и которые запускаются другого руками то есть мы непосредственно вызываем агент в звании теперь запустим дальше агентом автономного будет как бы следить за тем чтобы процесс не попадалось и он будет поэтому буду переставать или стартовать ему не нужно никакой какой внешний pink как только он подняли этот super set me the scheduler of можно них поднимаем нормальный scheduler который имеет way который как бы уже используются обычными пользователями точно так же мы понимаем как бы базовые ресурсы брокер то есть близко сборки тоже запускается на метр scheduler ах вот это это удобно тем что мы также получаем некий урай вот а дальше как бы за деплоить сервис может любой разработчик то есть когда у нас есть у нас есть boot camp когда приходят новые сотрудники они как бы практически в первый день тепло от некий сервис используйте да будет взят однозначно просто это было просто 3 года назад три года назад когда пришел company чистый еще проще то есть нас есть дополнительные компоненты сервис-менеджмент а там практически можно сервис на кликать мышкой то есть это очень удобно по поводу по поводу авторизации естественно как бы мы разграничиваем доступ то есть нас например я не смогу на вино тогда я смогу потому что я занимаюсь не может никак был другой одного отдела люди рождаются рекламы они скорее всего не смогут там обновлять джаббы из из поиска и наоборот то есть нас есть разделение прав доступа спасибо огромное мы хотим вот в таком вот виде сказать спасибо на память о наступлении удобную сумочку соответственно до аплодисменты от тебя нужно тот вопрос который больше всего понравился мне понравился заполнить вопрос про эти diy про ваня соответственно подарки от компании wargaming спасибо большое спасибо вам дорогие"
}