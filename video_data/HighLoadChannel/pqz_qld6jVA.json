{
  "video_id": "pqz_qld6jVA",
  "channel": "HighLoadChannel",
  "title": "Платформа 4К-стриминга на миллион онлайнов / Александр Тоболь (Одноклассники)",
  "views": 4082,
  "duration": 2868,
  "published": "2019-05-15T05:11:09-07:00",
  "text": "итак привет здравствуйте меня зовут александр и я отвечаю на платформу видео в одноклассниках а сегодня мы с вами поговорим про 4к streaming проговорим про миллиону онлайн-опрос тримминг натрите рабита начнем с того что такое видео wog это более 600 миллионов просмотров в сутки это три терабит а трафика на топовые пиковый онлайн трансляции и это еще порядка отдельно полутора терабит трафика на обычные видео ролики сегодняшняя наша часть доклада будет посвящена именно онлайн трансляция в 3 терабит а в качестве 4k и где-то порядка до двух миллионов онлайн of и так господа кто из вас знает как раздать терабит поднимите руки отлично конференция highload многие знают и действительно этим никого сейчас не удивишь и мы тоже на самом деле последние лет пять думали что хорошо знаем как раздать терабит пока к нам не пришли и криштиану смеси не шить не решили у нас погонять мячик вот погоняли они у нас его на два и два миллиона онлайн of и нам прямо было очень плохо в этот момент вот после этого мы сделали работу над ошибками вот и уже через несколько месяцев прекрасно показали на 12 ти рабита хоккей финал олимпиады при этом у нас задержки были даже меньше чем у первого канала мы показывали нам пару секунд раньше вот и вот совсем недавно смогли раздать из своих серверов 3 терабит а на обоих нур гамидова против макгрегора вот соответственно вот всем этим я с вами поделюсь тем какие мы выводы сделали как мы это исправили какую систему построили ну и как обычно всегда все сервисы начинаются с архитектурой и т.д. мы работаем соцсети 10 сети и технических заданий у нас нет архитектура вмещается на один слайд архитектура любого видео сервиса выглядит как у вас есть видео которое к вам входит на вход она входит у вас с мобильных телефонов с десктопов или от партнеров как-то преобразуется и выходит на вашу аудиторию на клиентов здесь все просто как мы уже поняли действительно раздать терабит когда есть терабит это легко поэтому мы будем сегодня разговаривать про то что действительно сложно а сложно раздать например 3 терабит а когда к вам пришли за шестью а почему так на самом деле происходит почему нельзя заранее спрогнозировать на самом деле угадать сколько у вас будет аудитории на трансляции крайне трудно это зависит от дня от времени суток это зависит от времени года это зависит от погоды в принципе предсказать аудитории на ваши трансляции то же самое что предсказывать погоду такое же неблагодарное занятие и даже если вы все правильно предсказали всегда есть варианты когда у вас будут ложиться ваши коллеги конкуренты партнеры и вы будете прямо видеть как пользователи приходят с их сервисы на ваш время смены пользователями сервиса на меньше минуты то есть если где-то трансляция залипла и пользовались за 10 20 секунд не у и продолжил смотрение он очень быстро перемещается в поисковик и там находится окна говорят те сервисы которые работают вы даже не успеете с адаптироваться следующий проблема котором будем с вами решать что в любой видеосервис это pipeline видео входят как то там транс кодируется преобразуется раздается и мы хотим на всех этапах иметь возможность масштабировать отдельно все эти параметры системы под наши нагрузки ну и следующая наша самое главно задача сделать пользователи счастливыми а для счастья пользователей это собственно высокое качество и мы себе поставили планочку в 4к это быстро первый кадр когда вы начинаете смотреть видео что вы понимаете что сервис откликается он живой и низкие задержки наверное да многие зла знаю такую ситуацию сидишь смотришь в онлайне у вас уже все соседи радуются забитому голу или шайбе а у вас еще там две три секунды задержки даже такая небольшая задержка она неприятна вы ещё смотрите матч а там уже все все празднуют и так что сегодня ждет сегодня нас ждет 4k streaming натрите рабита мы порешаем с вами задачу от сделать пользователи счастливыми сделать себе систему удобно масштабируемый и подготовиться к непредсказуемым нагрузкам план на сегодня мы рассмотрим видео pipeline оптимизируем ffmpeg посмотрим как там у нас используется engine кстати у ним tcp а и разберемся как с это масштабировать дальше мы на все это нашу систему подадим нагрузки и и по перегружаем потом поговорим про лол intense streaming и поговорим про будущие видео стриминга ну что самое простое первая часть видео входит на вход она входит например в 4к и для того чтобы дальше наша задача для каждого зрителя подобрать ему наиболее оптимальный битрейт и нарезать его ну начнем со входа и ей что же он так перескочил с отлично с обхода ну у нас наши польстили одновременно проводит там до 2000 live трансляции мы себе поставили планку максимально трансляция 4k мы себе ставим битрейт на нее порядка 50 мегабит можем и больше вот понятно дело что есть это все перемножить еще посмотреть средний трафик это порядка шести гигабит на вход это никакой не хайло здесь очень-очень просто форматы мы принимаем вот там написано артем пиве партесь и на вход еще свой формат принимаем как выглядит загрузку этих стримов пользователей или профессиональные правообладатели приходится своим контентом в определенных форматах узнают айпи нашего сервера начинают его отправлять свой стрим вот мы это отправляем на трансформацию на наш pipeline дальше и все пришедшие данные сохраняем в распределенных хранилище для того чтобы вы могли получить потом запись и ничего не пропало если у нас пропадет сервер клиент пойдет по-другому айпи адресу и собственный попадет на тот же transform все ровненько склеится никаких проблем здесь не будет под крупные трансляция конечно для того чтобы не произошло diedas или еще что-нибудь мы закрываем главной серы трансляции и собстна говорят через него идёт только одна трансляция особых проблем здесь нету общий трафик на выходе этих серверов это 18 гигабит в хранилище еще шесть гигабит на transform и формат он дальше поддерживать уже внутри системы пробрасываем все порты м.п. балансируем понятно dns раунд роббеном нет никаких сложностей трансформация трансформация на вход у нас приходят разные котики чуть чуть дальше скажу что это вообще как это работает вот битрейт порядка 50 мегабит 2000 стримов все просто для того чтобы понять вообще чем занимается транскодер нам нужно понять что такое видео видео это 24 кадра в секунду или 60 кадров в секунду это просто к деке вот задача собственно говоря вот это видео задача кодека его сжать понятно что неэффективно просто хранить 60 картинок поэтому есть такая абстракция как опорный кадр считать что просто g пешка и есть некоторые div и к ней это отличие этого это некоторые изменения этой картинке от предыдущей или от предыдущих вот saudi оклады кому еще проще чем просто режет на куски кодируют сейчас самый популярный кодек h264 300 нас останется время мы об этом тоже поговорим так часто многие спрашивают про битрейт resolution вот что эти цифры в плеере означают вот наши соотношение битрейтов и resolution of которые мы отдаем пользователем снизу есть ссылка на статью в которой указаны рекомендованные диапазон и понятно что 4k resolution можно отдать и в одном megabyte но там будут только одни артефакты кубиков поэтому делайте как то примерно как там написано ff и mpeg ну изначально мы его использовали в лоб брали 4k stream и начинали транс кодировать значка у нас было с одной стороны low latency потому что мы хотим шайбу во время смотреть вот другой стороны просто стримов у нас много вот и мы хотим экономить оборудование и тут мы померили сколько у нас занимает транскодирование из 4 кого все нижние качества на различном оборудовании смотрим это оборудование и видим что на 60fps входного потока у нас справиться только и nvidia по 4 вот и в принципе из доступных у нас процессоров которые мы используем для транс корень да ничего нам помочь не может с одной стороны не хватало производительности сепию с другой стороны мы решили все таки деревне транс ходит все на джипе you a посмотреть вообще какие там есть проблемы как работает f em back у вас есть де кодировщик заходит stream он начинает вам эти кадры видео отдавать доложим они у вас в 4к дальше вам нужно получить все качество ниже вас запускается рис келлер который с большой картинке кодирует воля маленькую после этого у вас запускается еще один рaз келлер который еще в более маленькую так у вас будет там порядка 7 раз происходить одна и та же операция который с большого скейла делает маленький дальше это все отправляются в кодировщик и кодировщики они классные они там vfm пеги с отдельными квартирами работают но если посмотреть на вот такой подход то с одной стороны по performance узкий стоимость операции скейла она пропорциональна именно входному разрешению 5 в минус скелетина вот соответственно это она со мной стран дорогая с другой стороны если вы почитаете увидите что на самом деле на всяких фото форумах что нельзя скелете с 4к 240 это плохая операция лучше использовать промежуточный скиллинг и у вас даже картинка будет лучше собственно поэтому мы pipeline ав ав им пега в этом месте подправили подправили и стали хотя бы из 4k с келли в 2к и потом уже эту застеленную картинку отдавать дальше вот сэкономить еще порядка тридцати процентов ресурсов скиллинга получили картинку получше следующая штука в темпике для каждого входного какого-то потока существует некоторый source straight который его обслуживает вот так его выглядит архитектура у вас есть departure в котором вы указываете сколько worker of там работает 510 неважно вот он вас отдает кадры эти кадры последовательно этим source при этом начинаются скейлится все сен хроника происходит и потом отправляются в энкодера который тоже вы настраиваете сколько вам потоков на это будет требоваться классно ну картина выглядит примерно так у вас есть source thread который занимается все время скалирования и пытаются все кодеки накормить активно вот остальные в это время простаивают как пофиксить ну отказаться от ссор страда или вот один из вариантов которые рекомендуют самых фанфик это запустить несколько декодеров но вот и кодеров 4k они бы довольно дорогие их еще придется как-то там синхронизировать между тем для переключения качество вот и есть второй вариант опилить транскодер как это делает вич здесь есть статья не как раз этой проблемы серьезно озаботились вот мы тоже запилили свой транскодер полностью отказались от 43 до используем одну блокирующую очередь на один кадр для этого делает для каждого рис келлера запускаем отдельные worker а вот подцепили в фунтах как библиотеку в результате после этого тюнинга даже на сепию получилось резать 4k еще особенность если вы будете резать 260 четверку то у вас будет такая ситуация что чем больше вы тредов даете энкодеру или декодеру тем выше лэйкин seat входящего от входного кадра до выходного то есть вы ему подали картинку там чем больше трейдов тем дольше он что-то с ней делает прежде чем вы на выходе получите тот же самый фрейм который был на входе ну и да так как мы занимаемся ловлей пенси трансляциями обязательно если вам нужно будет почитайте про тенге ролэнси для h 264 это обязательно нужно включить теперь что у нас именно вышла полотенце между кадрами для от над рядовых и видите задержка там чуть больше 100 миллисекунд по времени от входа и выхода а вот если посмотреть для 720 в этом месте вот видно что если мы 720 качество дадим 4 3d на той же машине то ли там все у нас вырастит поэтому если у вас справляется secu с нагрузкой лучше используйте минимум тредов дальше начиная с 1080 мы начинаем использовать же бью вот там такая же история с на транс кодируем мы либо на сыпью на облачных транскодер ах вот здесь есть статья про то как у нас устроено облако либо мы гоняем это на gp4 уже пил понятно фпс выше на выходе больше может больше скоростью транс кодировать сепию сейчас по крайней мере по стоимости получаются подешевле аднан ведет же пью и 4 который мы используем на pascal она может резать транс кодировать во все нижние 24 к вот одна у одно утилизирует энкодеры декодер на 40 процентов все обратить внимание никуда ядра а именно энкодер декодер собственно говоря что такое как вообще nvidia работает с кодированием что каин wink отдельно на карте установлено еще процессоры которые занимаются выполняют команды именно связанные с транскодирование мвидео в принципе самое главное что то мне делать это не копировать ничего за пределы карты работать с кадрами внутри риски тоже прекрасно работает на джипе you и сам интересно что у вас куда ядра в это время будут вообще простаивать то есть она будет работать только на этих 100 процессорах вот если кто не знает как эти куда ядра можно еще утилизировать там не знаю эфир по майнить пишите я пока не разобрался как это можно параллельно сделать транс к водяным и так если вам нужно затратить видео у вас стримы меньше 4k у вас расчетное количество просто возьмите ffm пик возьмите сепию и ничего не делаете если у вас 4 com раскошелюсь на джипе юшки паскаль возьмете вольт я думаю там вообще космос будет вот если у вас все таки стримов стало больше порядка тысяч то уже стоит оптимизировать риски л и собственно говоря если у вас еще и 4k и всего много ну не поленитесь перепишите pipeline не так это трудно что мы получили в итоге patrons форму выходной у него кладок 264 битрейт на одно видео уже до 100 мегабит то есть 50 входной плюс все которые ниже вот максимум там 2 тысячи стримах мы держим и выходной трафик это 12 geoby c тех этого всего кластер получился балансируется он таким образом что находит машина на которой больше всего свободных ресурсов которые подходят для выполнения задачи туда отправляет и собственно дальше дальше раздачам раздаём раздавать мы умеем со своих железных серверов там которых у нас порядка сотни умеем раздавать еще серверов облака еще ссылка на облака вот железки нас раздают порядка 40 гигабит в секунду можно и больше вроде бы задача очень простая вот у нас пришло две тысячи стримов мы их прогнали через за плоды там 200 transform of для них 400 докладов и вроде бы там три терабит из этого железа довольно легко раздать вот вопрос зал сможем ли мы при такой схеме раздать кто думает что сможем все думают что не сможем хорошо действительно войти посмотрим где здесь есть у нас подвох входной stream 5 мегабит дальше по 50 мегабит дальше он превращать это в соточку этот вот у нас один транскодер обслуживает 400 download of таким образом получается что этому транскодер а нужно еще и по 40 гигабит трафика вставить так к транскодер у нас облачные и использую мы различные железо которые у нас есть на это что везде снабдить его такой сетью для случится популярного стрима мы не можем вот поэтому мы добавили себе промежуточный промежуточную стадию она выглядит таким образом это сегмента один transform обслужит может стремиться свой строим порядка десяти сегмента ну ему сеть это позволяет 100 мегабит на 10 там как раз где-то гигабит получается можно гигабитная сеть на трансформа использовать вот и дальше это сегмент уже со своими 10 гигабитными внешними сетями могут обслуживать до сотни town родов таким образом при такой схеме в наши математика получается такое что мы можем обслужить до тысячи серверов по 40 гигабитные в пределе это 40 терабит перескакивать вот это в пределе 40 терабит что в принципе больше чем достаточно нас эта архитектура устраивать собственно почему все называется сегментом ну потому что сам ли кроме того что он их мультипликаций фактор нам делает для раздачи он еще все потоки которые есть упаковывать в различные контейнер и об этом мы поговорим упокоит vd6 вот и поэтому количество разнообразного видео с него выходит еще побольше в плане трафика сегментация для того чтобы ударить по сегментации нужно понять как вообще выставляется видео для oppo льда пользователя у вас есть аудио и видеопотоки закодированные кодеками вам нужно доставлять это видео оно разбивается на некоторые небольшие сегменты эти сегменты снабжены видео + аудио для этого времени вот эти транспорта может данном случае быть impact с этим фрагментом mp4 вот и соответственно есть манифест который клиент периодически обновляют в котором есть все ссылки на эти фрагменты одним из таких форматов является им пердишь вы можете часто встречать манифестом pdm внутри этот манифест содержат ссылки на все качества ваш плеер получает манифест и согласно вашему вашей пропускной способности сети выбирают качество которое нужно играть и в процессе изменения может переключаться переключаться он может на границе фрагментов и каждый фрагмент начинается с того самого опорного кадра которые мы говорили то есть фактически с фотографии ну очевидно для чего это нужно для того чтобы высота моменту могли начать играть есть еще следующий формат streaming это each lesson чуть более старой у него из особенностей является то что у него два уровня манифеста общей манифест который ссылается на манифесты каждого качества которые ссылаются уже носами в сегменты что не очень хорошо в плане старта у вас есть как минимум лишний round trip при обновлении но это не от не самая главная проблема кроме этого исторически у него транспортом является мтс мтс транспорте пакеты размером по сто восемьдесят восемь байт это ведет к лишнему вверх и доносить и это ведет к тому что ваше оборудование должно генерировать собственно говоря эти пакеты там на 40 гигабит в секунду получается надо двигать 6 миллионов пакетов в секунду таких формировать ну и пусть это тоже не проблема но играющий player в качества в 4к и через он прямо на него смотреть больно вот поэтому это довольно плохой формат для именно воспроизведения высокого resolution on плане даже сети доставки в плане player вот apple на в 2015 году анонсировал фрагмент от mp4 внутри и чавеса в принципе это сейчас доступно на ios и но я не мы это не поддерживаем и уже ушли от этого формата вот может быть вы кто не встречали сервера для фрагмента темп тех 4 внутри через собственно форматы которые поддерживаются apple исторически всегда продавливает лечил с google продавливает душ поэтому дэш в экзо плеере на андроиде доступен youtube всегда все стримит играет в душ и у него и чела ситов по удачный формат но и мы тоже у себя кроме каких-то low light and интерактивных трансляций тоже стараемся использовать даже что нужно чтоб сегментировать видео используя ffmpeg выберите контейнера даже чрез если много будет раскодировать защититесь вот таким вот способом защитить и свой транскодер от трафика раздача ну сегментов в итоге выходной трафик сегментов может быть их по 10 гигабитными карточками порядка 20 штук может быть 200 гигабит формата он уже отдает rtmp дыши через все что мы от него захотим и на каждый стрима нас минимум два сегмента на случай если один выйдет из строя но и в случае популярных трансляции понятно это все масштабируется следующая штука осталось все нарезали упаковали доставить до пользователя доставка допустили выглядит примерно так мы на одном сервере хостинг как бы server service наш на джаве и каширу и мы джинсам вот он ходит к нам пока ттп забирают к нему приходят пользователи все предельно просто задачка pro 500 миллисекунд смотрим на наши фрагменты которые качаются все качаются от с одинаковой скоростью 100 миллисекунд один иногда проскакивает 600 если все будут качаться по 300 миллисекунд он будет проскакивать 800 вот как вы думаете кто в этом случае тормозит and james или java давайте кто думает что проблема в джинсе один человек 234 кто думает что java тормозит гораздо больше вот на самом деле в этом случае посмотрим что у нас есть клиент он приходит за джинкс и заданными он проектирует их до нашего сервера наш сервер ему отвечает все здорово если приходит другой клиент в одно такой неудачный момент в тот момент когда и джинс выполняет запрос у него взята блокировка он говорит что этот фрагмент у него качается все здорово то он заводит таймер и таймер на 500 миллисекунд черт его знает но это захардкожены вот соответственно ответит случае все пользователи пришедшие на момент загрузки этого сегмента джинсам сервера получит вот такую задержку виде этого таймера соответственно если вы хотите раздавать вам это лучше пофиксить ну почему потому что очевидно что для нашего estima таро внутри плеера который считает какое качество нужно качать появление таких фрагментов которые начинают жёстко там в 6 раз больше качаться средства там просят косатки сети и вы для пользователя это видите как периодическое желание этого плеера переключиться по ниже в зависимости от настроек дейсвительно будут player которые будут переключаться возвращаться обратно в такие моменты жизни мы заварили благу обязательно конечно мы заварили багу мы выложили патч пожалуйста тоже можно посмотреть вот пока она нас не обращают внимание в мастере до сих пор эта проблема сохраняется надеюсь что когда нибудь поправит дальше-то как мы с вами задрали планку и все время хотим раздавать 4k то надо будет немножко и тисе пи поправить для этого надо вспомнить наверняка тут много в главных залогов докладов против типе но немножко расскажу у нас есть какой-то сервер отправляет данные у нас есть пачка данных которые нужно доставить тисе пи он с гарантированной доставкой он вам скажет что он либо доставила либо нет поэтому он держит пакетики отправляет они уходят на клиента клиентов право отвечая таком вот за некоторые round trip вы получаете это так вроде бы все здорово вторая задачка задачка такая программист вася нахо есть какой-то классно идти компании высокой башни с прекрасным вайфаем на 400 мегабит хочет посмотреть конференцию apple которая находится в калифорнии pink у него в 250 миллисекунд вася хочет посмотреть и всего лишь full hd кто думает что ваша сможет посмотреть действительно почему же вася не сможет посмотреть на самом деле вася можете сможет посмотреть но все зависит от того какой пол настроенность сервера и какого размера у них стенд буферной отправки дело в том что у тисе пи есть такая особенность что так как у вас есть тот самый сын буфер то а то данный уходит ровно тогда когда у вас придет окна эти данные поэтому отправлять больше чем размер цент буфера за время round trip а он не может поэтому все мы аккуратненько посчитаем то получаем что пропускная способность у нас ограничено размером буфера за оборот round trip а если аккуратненько все пересчитать то для данной сети получится всего 4 мегабита получается от нашей полосы в 400 осталось только один процент и в таком случае если у нас and буфера у apple 128 kb това се не сможет посмотреть вот я прямо сидя в офисе на небольшом пинге правил send буфер и смотрю как у меня прекрасно растет скорость поэтому если вы собираетесь там delivery 30 50 мегабит 4k то обязательно поставьте хотя бы 256 килобайт концерн control надеюсь тоже главных залах многие из вас знают что это но быстренько про гонимся у нас есть сеть вы отправляйте пакеты часть из них пропадает все хорошо вы отправляйте больше пакетов они все пропадают происходит перегрузка задача кондрашин control защитить как раз сеть от таких перегрузок работает он за счет некоторого алгоритма окна кондрашин винду суть его в том что нельзя отправить данных больше чем размер окна если размер окна один то вся ваша передача будет выглядеть таким образом если окно например увеличивать стал 4 вы можете отправить сразу четыре пакета и дальше по мере приходов сможете отправлять дальше как это работает сейчас дефолтный кондрашин windows стартовой везде 10 он разгоняется и после каждого успешной доставки получения окно уже мента начинает увеличивать его в два раза после этого происходит перегрузка сети пакет дропа еца он хлопает к это окно вроде бы все классные работает с точки зрения маршрутизаторов этого бит так начало у вас все ровненько ровненько приходит потом нам шатрах появляются очереди вы отправляете много пакетов а доходят они уже так чуть-чуть разрежена очередь копятся и дальше он начинает дропать пакеты понятно что мы шатер довольно умный он перед тем как у него сеть на выходе переполнится начинает рандомном различных подключениях дропать пакеты для чего для того чтобы все клиенты с алгоритмом окна поняли что у них пакет пропал вот хлопнули окно и таким образом backup реже до реализован в этом марше за три вот ваш клиент соответственно с окном 80 потерял пакет с хлопнул и нашего разгоняться дальше вроде бы все выглядит логично но если у вас высокий пакет lose the от вашей пропускной способности сети ничего не остается а у нас уже давно мобильных клиентов больше чем десктопных и все сидят на беспроводных интернетах и этот прикид лосс он действительно очень плохо влияет понятно что кондишен control они стоят на месте они развиваются сейчас дефолтный кондрашин control в основном везде кубик он дефолтный для linux для android и для ios а он в принципе хорош но в качестве bk использует лосс то есть наличие потери пакетов google 2 шестнадцатом предложил би би ор мы сейчас про немножко поговорим который несколько лучше решает проблему перегрузки сети и так вот наш мир театр которому хорошо вот кубик который среагировал и стал уменьшать окно при потере пакетов обе bear начинают уменьшать окно опираясь не на потерю пакетов она время их доставки то есть как только у вас начинает появляться задержки отправки бибер понимать что надо как-то уменьшить передаваемый трафик в принципе это здорово мы включили у себя пибиар это не трудно это нам очень сильно увеличило время смотрения поэтому если вы занимаетесь именно трансляциями и вам необходимо уменьшить буферизации у клиентов и доставлять именно стрима вы и потоки то бибер это решения для вас какая есть проблема бибер когда смотрит он смотрит на время пинга то есть если у вас появляется очередь на мушке затари воспитан начинает расти для viber это как раз признак того что надо уменьшать окно вот я по пинговала тут highload на местном вай фай чеки вот получилась такая интересная штука что пинг-то у меня очень неравномерной а я сети не перегружал ни разу вот на соберем сетях существует джиттер это на отклонение между соседними пинкамину и вот средний джиттер получился сорок шесть миллисекунд нас в сетях он тоже порядка 50 миллисекунд и для если померить например speedtest им он тоже сейчас говорит очень часто пинки говорит еще джиттер но если у вас сеть вот с пингом 173 миллисекунды плюс джиттер еще 173 миллисекунды то это уже очень плохая сети бибер на ней будет работать крайне плохо поэтому это не серебряная пуля когда вы включаете ведь на проверьте графики могу рекомендовать или исключительно для видео стриминга в остальных случаях контролируйте и так раздачам как я сказал порядка 40 гигабит сервером для там и 10 миллионов онлайн мы готовы в плане того что это нетрудно в количестве подключений форматы раздаём различные мвд 6s в партесь и и все-все-все балансируем мы трафик насер балансируем нагрузку на серверах контролируя входной выходной трафик и сепию на этих серверах что вам нужно если чтобы раздать видео возьмите nginx возьмите патч ссылка была если 4к поставьте сын буфер побольше поменяйте к нашим control это уменьшит количество буферизацией на ваших клиентах но в принципе с одной хорошей железки можно наверное 80-ти габит раздать ничего не делая специального для это для этого мы так какой у нас в итоге финальный pipeline вакха для этого получился у нас внешний сервера раздачи они ходят к внутренним сегментом все transform и стримит на определенное количество сегментов в зависимости от популярности стрима а плоды которые принимают все пишется в сторону для того чтобы вам потом предоставит запись ну и все мы это делаем на звуки перри которой оркестре рует и говорит для каждого стрима на каких серверах сколько серверов и где это находится для того чтобы это все балансировать видео плеер в принципе конференциях и лот поэтому про play будет крайне мало различные стандартные плееры с дэш вас все устроит и будут играть единственность 4k будут небольшие проблемы в андроиде на определенных устройствах они будут заявлять что они играют 4к на это не происходит нам пришлось для них даже некоторые blacklist завести ну и в смарте видеодекодера 14 к другой обычный вам нужно выбирать правильный зависимости от того какой у вас три заранее в каждом плеере существует из team at our я уже пора раз про него говорю что он делает у вас есть какие-то качества есть какая-то скорость осетинка на каждом шаге принимать решение качать качество ниже выше или или остаться на текущем вот номер ситуации когда у вас уже сегмент предыдущий скачался пусть у вас сегменты знаю 6 секунд три секунды он уже проиграл у вас осталось 4 секунды в этом сегменте может стать следующей на что он не будет рисковать перекричать качество выше потому что даже если на вроде бы успевает если он не успеет то предыдущий сегмента играть следующем еще не скачал поэтому обычно не действует консервативна и продолжает качать в этом качестве делая небольшую буферизацию задел вперед когда буферизации хватает steam атор принимать решение что ему нужно переключиться так ну что весь pipeline мы обсудили от загрузки до player давайте попробуем это все на по нагружать как я уже сказал там порядка 100 серверов раздачи у нас в одноклассниках 4 дата центра вот раздавали мм и порядка трех терабит как мы до балансировка раздачи все вроде бы просто у вас есть secu входной выходной трафик вы оцениваете сервера какими-то весами и взвешенным рандомом разбрасывайте пользователь ну понятно почему нельзя валить на самый не загруженный потому что у вас есть late in sea по обновлению метрик сервера вы можете очень быстро прикончить ну вот бывают такие ситуации когда у вас наступает перегрузка и вы понимаете что на самом деле у вас уже все сервера загружены и пользователи в принципе отправлять никуда причем почему нельзя перегружать сервера почему нужно доходить до какого-то лимита и дальше этого не делать потому что если у вас пропадают пакет у вас начнется ретранслятор если у вас затупил плеер то он начнет перри запрашивать данные с этого сервера потом пойдет по другим серверам у вас будет ловить на отключение вот как мы называем это ретро hell с точки зрения video player это выглядит так плеер у вас равномерно качает сегмента с вареньем стрима скачал один сегмент спокойно скачал 2 3 вот после этого склеим некоторое время затупил затупил затопил вот он немножко отстал он игра в это время мог буфер даже играть у вас могло даже не быть крутилки ну подотстал от трансляции что происходит он догоняет и догоняет он так он постоянно начинаете все сегменты выкачивать то есть если стремишься которое у вас есть стрим входной от партнеров или от пользователя под затупил и много зрителей то получите удар трафиком вот поэтому перегружать оборудование крайне не стоит если ваш три раза дупе он тоже получит удар трафиком которые так без тому уставшему сервера без того уставший сервер добьет кто знает что делать в таких случаях есть киньте варианты давай ну сидел это хорошо если вообще все сервисы кончились все что можно было уже все да действительно на самом деле ваши сервис они должны ложиться под нагрузкой ваша сервисы должны деградировать не пишите сервисы которые подают ну как деградировать ну первое на маленьком уровне сервер должен обязательно защищать себя открыт от пользователей для этого он должен включать трота когда он понимает что у него сеть заканчивается он начинает всем подтормаживать соответственные steam отары пойму что скорость сети просела переключиться ниже будет не очень страшно вот но клетки плеер может не успеть адаптироваться к этому делу могут появляться какие-то проблемы буферизации поэтому действительно того чтобы это было мягче мы еще умеем убирать и качество и из player но для этого вам нужно поправить свои player по возможности где вы и где вы на них можете влиять чтобы они пели запрашивали тот самый манифест который содержит файлы что вы увидели что в этом у них есть и пропала качество вы его убрали вот и таким образом снизили себе нагрузку по балансировку трафик по балансировки трафика в момент начала трансляции мы оцениваем трафик и делаем всякую там утреннюю монтировку перебалансировку для этой трансляции и если поняла что носки это стыке заканчиваются подходит концу ли сильно скидываем сторон не сидел те или иные подсети в итоге балансировкой выглядит так а поводу map он серым round robin он понятное дело что трансформа умеем запускать где-то в облаках и так далее дальше балансируем сегменты по количеству популярности стрима download и по входному выходному трафику дальше это сбрасываем сторон не сидим и дальше начинаем отключать качество что еще хочется сказать не забудьте защитить свой трафик всегда могут прийти люди которые захотят ваш трафик показать своем плеере на своем сайте еще где-то поэтому сгенерируйте большие ссылки которые включаете там user-agent айди пользователя эти все остальное контролируете погиб что там где ссылку брали ее играют именно в той же примерно локации вот для защиты вэба используйте кросс origin что еще можно делать можно еще использовать поэтому и раздачу в рамках за рамками этого доклада можно экономить трафик трафиком и порядка тридцати процентов на не экономим при этом можно кормить больше можно экономить вплоть до 80 но тут проблема что вам мешает всех-всех перепилить одна из проблем это установка пир toupper соединение пробивка на то мы здесь есть ссылочка на мой доклад про звонки и как раз звонки тоже пир toupper и 80 процентов звонков это именно установка пир toupper соединения вот супер-дупер раздаче есть проблема что нам растит лайта в этом си вам нужно кэшировать больше сегмента вперед чтобы этими сегментами друзей дружит раздавать их друзьям или тем кто вашей соседней подсвети находится вот это как у нас все клиенты получают различный тип трафика мы отдаем на ios дыши так далее у нас очень часто эти сегменты разные они не пересекаются какие еще и способы борьбы с задержки как ее померить очень просто вы включаете таймер можете таймер за стримить вот видно что время от снятия с камеры до отображения дисплея у нас там буквально 50 15 миллисекунд и потом вы включаете трансляцию на выбив смотрите задержка там двенадцать секунд 20 секунд например это стандартный zircon и чавеса можно так все сфоткать и померить вот как с этим бороться но есть существуют лалетин сидевший через нужно уменьшать размеры сегментов вот порядка трех секунд можно нас удается получить задержку для всяких хоккее всего остального для того чтобы вы смотрели раньше чем телевизор какие проблемы для 4k low latency человек это вообще плохо по нагрузкам на клиентские плееры у вас растет битрейт и оверхед чем меньше у вас сегментов и собственно варя каждый сегмент еще должен общаться с keyframe поэтому получается деградация с одной стороны у вас битрейт потока высокий с другой стороны качества у вас падает вот и когда вы затягиваете под low latency у вас падает эффективность раздатчик перту пера какие есть вообще проблем эти цепи вот это стандартный интернет наших пользователей на котором они видео смотрят мобильные порядка 17 мегабита 01 pocket лосс вот если вы в такой сети используете для доставки стрима и для стриминга с клиента используйте die happy у вас возникают такие стандартные проблемы как хэдов line blocking на многие знают вы запихнули аудио-видео доставили отправили это в сеть на клей это все на клиенте уже собралось но потерялся один пакет для видео на самом деле вот эти пришедшие данные которые никак не может забрать вообще не проблема потеряли какие-то кусочек данных кодов с ним прекрасно справится но если happy вы никак не можете вы читать следующие данные вы вынуждены просится вторая проблема что у вас если перед такой называем буфер баллов на 10 что если вы уже перегрузили буфер данными понимаете что у вас скорость просела вы никак нет буфер уже повлиять не можете соответственно пользователь может убить такую крутилку минус это крутилки в том что в простом случае в плеере и трудно догнать то есть там уже отстал на пару секунд он так и будет смотреть трансляцию с отставанием какое интересное решение наверно протокол quick все воссе наверно слышали про него все здесь уже много докладов его основная существа он реализован по верху и т.п. он имеет набор стримов они там приватизируется google заявляет что у них на 18 процентов меньше 100 love на крики что в принципе похоже на правду уже порядка 12 процентов трафика в мире это quik для тех можно сделать это открытие но когда выходите с хромов google это крик на youtube вы тоже ходите the quick поиск в андроиде quick google drive следующий релиз это тоже клик и даже последний раз конференции сказала что у них тоже будет поддержка крика вскоре в скором времени вот поэтому мы тоже уже очень давно используем у себя египет для уменьшения задержки и для менее популярных стримов которых нам не требуется доставка пир тапиров использование сторонних сидел of мы на этих стримах используем у себя это протоколы типа в партии для доставки или свои по самописные протоколы light in se порядка 300 миллисекунд можно гарантировать единственные проблемам является что раздавать в дипе несколько дорого потому что он дольше можете зиру и цапа таблицы при отправке и есть и щелочь сегментов лода тисе пи который позволяет вам не формировать все пакеты насыпью а вот если вы делаете египет а у вас работает сепию для каждого пакета это проблема не только на линуксе она и на виндоусе присутствует поэтому перформанс у диппера сдачи несколько ниже вот в одноклассниках у нас есть свой протокол стриминга поверх и т.п. и вот собственно говоря ссылочка на хабр там можно про все это почитать и последняя часть очень небольшая собственного ряд тренда и что нас ждет в будущем в раздаче видео кажется что фпг должно решить проблему транскодирования g пью очень классные но кажется что мы платим за куду которая стоит то есть принципе всего два с кодировщика решают нашу проблему мы тестировали x links вот соответственно пока нас не уст не устроила его скорость но кажется что это правильное направление кодеки сейчас все работает 264 есть 265 vp9 их основная проблема что они классные лучше на 265 играет только в microsoft edge браузере и на smart tv лепи 9 он хоть и поддерживается хромом но при этом в него не существует софтверных кодировках hard верных кодировщик of за ответный кодировщик это во-первых дорого во вторых на клиенте он моментально сожрет вам всю батарейку поэтому google сейчас продвигает коды к эви ван он уже вышел он доступны в софтверной реализации и где-то 2020 году я думаю что появится в хардкорной реализации мы сможем туда переехать в общем ждем верим пока основной 264 будущее доставки вот интересный тренд на сроках смотрители видео у них за последний год соотношения в file type это все стороны алтея будет все больше больше люди пользоваться мобильным интернетом против подключения конкретного вайфай в конкретных там кафе гостиницах и так далее вот скорость растет за последний год у нас мобильный интернет тоже наших пользователей с 1 и 2 мегабита подрос до 17 что нас ждет дальше дальше надеюсь что к нам придет айпи вы сэкс его multicast им мы полностью уйдем от тисе пи раздачи у нас везде будет у диппера сдача тогда кроме того что вы сможете гарантировать late in se использовать все эти преимущества кодеков для потери пакетов вы еще сможете один пакет и вставлять сразу куча пользователей за счет multicast а вот поэтому думаю что будущее будет айпи в сексе dp и multicast и так заканчиваем что мы получили мы получили 4k streaming натрите рабита который даже не ложится под когда от него хотят 6 он просто мягко деградирует мы получили непрерывный streaming при потере любого оборудования и получили возможность горизонтально масштабироваться под нашим нагрузки что вы узнали что ваши сервисы должны вложить не ложится деградировать вы узнали как работает видео pipeline как его можно оптимизировать и по тюнить и вы узнали что на самом деле все матчи ла лиги можно смотреть 4к на одноклассниках и еще еще одна последняя вещь если вы хотите сделать классный сервис который там показывает стримит даже 1080 поднимитесь все планку до 4k вы сразу пофиксите проблемы в кодировщик ах в раздачах в больших сегментах и в прочих вещах мы столько всего интересного вычистили что потом ваш 1080 будет работать просто прекрасно спасибо вопросы давайте подождите микрофон пожалуйста скажите пожалуйста вы не пробовали строить системы распределенного транс кодинга для того чтобы избежать использованием вот аппаратных средств таких как от nvidia и так далее что что распределён то есть фактически бить входной сигнал в том виде в каком он есть на сегменты это раскодировать их можно один сегментов пройти в одно место другой друга да да да у нас сильно light on se вырастет делать тонн это условный вырвать 3 как как бы как работает кодек у вас есть опорный кадр есть к нему div и поэтому если кодек кодирует что-то то он должен иметь keyframe а до следующего keyframe а все данные на выхода требуется с него 1 поэтому соответственно зависит от частоты keyframe of если у вас часто такие фреймов там весь раз в 2 секунды что довольно часто уже то в принципе вы можете брать кусок 2 секунды отправлять на один серый кусок 2 секунды на другой и потом склеивать но мы здесь бились залейте nsi мы не хотели ждать допустим 300 миллисекунд леденцы кодекс бились для того меняли pipeline ffm пега чтобы у вас было сто то есть мы как бы работали вот насадках мире секунд поэтому позволить себе прирост 2 секунды только на транс к денги мы никак не могли но это произойдет только случаев вы будете заранее готовить сегменты они отправляют сразу же в выбранный worker скажем так можно на восток на себя будет 5 возникает тот самый случай с хоккеем когда у вас вдруг смотрят на а р т и у него нужен шайбу забила вы у вас ее нет еще я думаю можно судить хорошо много вопросов выбирайте а вот мне такого вопроса вы сказали что на пиковых событиях вас 343 биты отдача да это все с внутренних мощностей или с используем там внешних силен of прочее для топовой раздачи мы используем свое железо которая у нас есть железная постоянно стоит мы используем в серверов нашим claude свободны прямо там сотни михала церу им там до 5 гигабит мы с 1 s1 но до использую и все что не взять мочу скидываем сидена все порядка там более трех торопиться сумела то есть до 3 3 бита вы спокойно выдерживать нагрузку без использования сервисов то есть это для вас экономически выгодно что как бы тупике именно да лучше конечно пике дальше уже в некотором случае экономически становится выгоднее скидывать в сидел сторонний до чем держать еще дополнительное оборудование но в принципе наличие клауда когда у вас есть там ресурсы вы можете что-то в облаке отмасштабировать поднять кучу ресурсов необходимость сидения она становится все меньше и меньше и меньше спасибо кладу а можно вопрос давайте сдвиньте вот до такой большой объем данных вы даете вы сами верно подметили что можно увеличить о с раздачи до 800 гигабит снова you into да сейчас уже в легкой можно историй болезни гибель снова юниты отдавать почему и работа пока не идете понял спасибо ну как было собрано когда-то так как от масштабируемся все расскажу можно на там были вопросы а вы хотите сказать я могу повторить если спасибо хороший доклад про инженера при методе если вам будет появиться его новыми с накладными расходами конечно же он увидит по если он будет смотреть на компе у него эти накладные расходы они будут соответственно уничтожен есть на мобилки соответственно даже большим пингом путь будет завтра вербально да но и по уэк у вполне возможно у многих надежды то что по уик-энд конечно увидит полутьмы про 2000 лет ситуацию они сюда по крику он даже порой партесь и это увидит нет ну по тисе пи town увидит из того что ему буфер маленький она может поставить гослинга без тюнинга без тюнинга вот у нас просто качается с никто он видит трансляцию в этот тестер и да спасибо большой хороший доклад в общем lexus давайте кодирование вы знаете мы сравнивали наше оборудование по кодированию с серверами элементов наверно это что-то схожее с тем что вы имеете виду отлично тогда я знаю что это дело в том что элементов да это такая классная очень дорогая железка набита цепью картами по производительности стоимости наша комодики железо особенно говоря вот с такой штукой она прямо в разы дешевле если вы попробуйте 2000 лайвов порезать или ментолом вам придётся очень много денег потратить на оборудование да да"
}