{
  "video_id": "vTF1uSWsIL0",
  "channel": "HighLoadChannel",
  "title": "Зачем и как мы написали MapReduce-движок над Clickhouse / Андрей Кузнецов (ВКонтакте, VK)",
  "views": 219,
  "duration": 2028,
  "published": "2024-04-17T00:59:44-07:00",
  "text": "Здравствуйте меня зовут Андрей Кузнецов и сегодня я вам расскажу зачем и как мы написали mys движок кликхаус Прежде чем идти далее расскажу пару слов о себе Яру команд инфраструктуру аналитики ВКонтакте на протяжении уже более чем двух с половиной лет команда делает сервисы для работы с данными и поддерживает большой кластер кликала сейчас у нас 96 хостов до этого я работал Яндекс и 404 Давайте начинать Дайте представим себе что у нас есть какой-то страшный запрос на Клик Хаусе например Выглядишь так и попробуем его запустить Здесь через запрос не обязательно можно обратить внимание лишь на две вещи первая вещь Это то что здесь есть Joint а второе то что здесь есть конструкция Global in то есть сделан какой-то блинчики она еще какую-то табличку и пытаемся что-то посчитать что у нас получается Если мы запустим просто кликхаусе то мы получим Memory limit ошибку потому что наш запрос использует почти 227 ГБ оперативной памяти у нас ВКонтакте есть некоторые синтаксический сахар который мы добавляем этот запрос и запускаем и Он отрабатывает без каких-либо проблем и вот тому как мы сделали такую обертку и будет посвящен мой доклад а какой у нас план на сегодня поговорим об исторических предпосылках аналитики ВКонтакте поговорим о сборке агрегатов витрин настоящее время как у нас устроено сейчас и посмотрим как сборка агрегатов привела к появлению нашего посмотрим что он умеет и в конце посмотрим как мы подводим еще эксперименты с помощью нашего Итак кликалась продуктовой аналитики ВКонтакте появился у нас в 2018 году и тогда типичная таблица у нас была устроена вот таким образом данные хранились в базе Local и это были собственно хостовые таблицы семейства мирштрий вот каждая такая таблица имела префикс Local чтобы было проще опознавать и Каждая таблица в обязательном порядке имела следующие дополнительные вещи Это был Partition Buy обычно это какое-то поле по временной метке допустим день или месяц и в обязательном порядке обычно это User ID но могут быть и другие варианты Например комьюнити если это таблица например по сообществу далее на каждом Кости для этой таблицы есть буфер который сбрасывает данные в эту таблицу расположены на базе буфер и имеет соответственно префикс и этот слой используется для вставки данных мы используем буферные таблицы для того чтобы формировать более крупные Бачи И тем самым менее нагружать файловую систему Хоста и экономить ресурсы и поверх всего этого существует кластерная таблица дистрибутив из которой собственно происходит чтение данных она у нас лежит в базе дефолт Вот соответственно это очень важный сайт здесь нужно запомнить вот три вещи есть база Local которые собственно хранить данные из база буфер который данные пишется есть база дефолт которая есть база состоящая из таблицы с которой мы читаем данные вот это было в 2018 году и действительности это устройство живо до сих пор с небольшими изменениями иногда но в целом оно соответствует тому что есть сейчас какие у нас есть первый тип таблиц и самое важное это логи логи они пишутся спрода прямо с событий происходящего против это большие таблицы как правило от 100 миллионов до 10 15 миллиардов и обычно мы используем движки репрессия номер 3 для этих таблиц второй тип таблицы это агрегаты они наполняются путем запуска обработки логов и как правило гораздо меньше потому что не по факту является сжатыми логами это 10-100 миллионов строк и здесь разнообразие движков чуть побольше мы иногда используем лишь 3 для специфических кейсов Вот теперь что касается как аналитика была устроена в прошлом 2019 году она приобрела вот такой вот вид данные напрямую писались из в кпхп через kitton House clic House то есть Прямо вставляется Стрим в среднем задержка в те времена и сейчас была приблизительно пять минут и до сих пор на такой остается основными клиентами были супер сет дата грипп и клехаус клиент суперсети обитали в основном аналитики тогда немногочисленные они также любили делать забросики из дата гриппа и крихаус клиент Это был скорее админский интерфейс для тех у кого был доступ логи писались напрямую с провода а чтение и сжатие данных в агрегаты происходило с помощью самописанных агрегатора на Java который буквально делал следующее типичный сборка агрегатов это запросы Select и с каким-то грубаем Ну вот здесь на стадии представлена грубаи в дату и соответственно теперь как это происходило Как устроена сборка агрегатов было тогда давайте посмотрим на правую часть слайда значит запрос разбивался на подмножеством подмножество определяются сэмплом который как вы помните мы задавали при создании таблицы то есть сэмпл действительность это часть ключа таблицы по которому можно читать данные соответственно сэмпл разбивает таблицу на какое-то количество не пересекающихся множество которые можно прочитать отдельно обработать и Сложить это результаты обработки этого множества данных уже в таблицу агрегат Вот почему схема называется диагональной потому что мы считаем из дистрибутика то есть мы запрашиваем дистрибью таблицы Первый кусок данных если будет таблица говорит Окей посылает запросы каждый номер три получает ответы дагрегирует их и потом в конкретный уже хост то есть чтение у нас происходит со всего кластера то есть диагонально а вставка происходит у нас в конкретный хост в чем минус такой схемы Ну очевидный минус это большое количество сетевого взаимодействия потому что каждый кусок данных берется со всех серверов до агрегируется в дистрибутив на конкретном Хосте а потом вставляется уже тоже в конкретный хвост то есть здесь основная проблема Это именно сетевое взаимодействие вот мы до сих пор используем эту схему но мы не стоим на месте и достаточно сильно развиваем наш агрегатор прежде чем переходить к новой схеме Греции Давайте пройдемся потому как был алгоритм девятнадцатом году Итак разбиваем весь набор не пересекающиеся множество sample.by или через модулы вариант модуль он гораздо хуже потому что он требует скана а Давай это все-таки чтение по индексу но тем не менее тоже возможный вариант и он иногда используется далее на каждом из N кусков Мы выполняем агрегацию складываем результат в один из хостов Ну или шардов далее если мы вставляем буферную таблицу то Мы обязаны проверить сброс буфера и если мы вставляем реплицированную таблицу То есть если мы должны дожидаться окончания репликации последние два пункта очень важны Потому что если у нас одна сборка зависит от другой и мы не дождались просто буфера или мы не рождались репликации мы можем прочитать некорректное под множество данных что чего нужно любыми силами поэтому уже были в 2019 году встроены такие проверки в 2020 году мы решили переписать агрегатор на питон это нам сильно упростило поддержку Мы очень сильно поработали над отказоустойчивость то есть Мы научились переживать отказ одной реплики в шарде и даже кратковременные выпадение шарда в рантами то есть лишь по какой-то причине полностью ушел там например 120 секунд то мы можем просто его пропустить и работать с другими шардами если схема сборки нам это позволяет делать также Мы научились восстанавливать упавшие агрегации с последней успешной вставки То есть если у нас какая-то длинная сборка бежала например в течение часа и на середине упала а то раньше мы как делали мы зачищали таблицу и заливали заново сейчас этого делать не нужно можно просто передать соответствующий флаг агрегатор он поймет какую последнюю часть данных он ставил и начнет собирать дальше с неё и самое главное что она позволяет делать новый агрегатор это локальные сборки кроме диагональных теперь вкратце посмотрим что такое локальная сборка локальная сборка отличается от диагональной тем что мы у каждого Хоста запрашиваем все под множество данных и обрабатываем на каждом Хосте эти Мне кажется данных независимо от других остов то есть мы запросили энный кусок у допустим первого Хоста обработали его и положили вот сразу же в тот же самый Host но в другую таблицу Итого у нас Хосты обрабатывают данные полностью независимым и никакого сетевого взаимодействия передача данных по сети не происходит важный момент в том что при этом чтение происходит не из таблицы дистрибутив до кластерная происходит именно из самого мир штрихи мы это видим секции From где у нас запрос идет базу Local и соответственно в таблицу с префиксом То есть это локальный мир 3 и без него они всего кластера это нам сильно экономит ресурсы и когда это возможно мы всегда стараемся использовать локальную сборку и это вот основная фича нового агрегатора которая позволяет делать всё гораздо быстрее и более аккуратно по отношению к нашему железу типичная среда обитания агрегатора это вообще etlyne итель pip-line это консольная утилита которая дергается каким-то шеделером например азкабаном или airflow и она имеет следующий аргументы первый аргумент это название агрегата второй аргумент Это количество не пересекающихся кусков Тут нужно пояснить что здесь количество кусков один в действительности мы домножаем это фоне на 32 потому что на 32 шарда далее количество потоков То есть если мы указываем 32 потока У нас 32 шарда и при этом собираем локально то мы параллель грузим все 32 шарда нашего кластера и они работают Независимо и вот последний флаг это флаг как раз изучающий локальный сборка то здесь мы распиливаем все множество куска и собираем его локально в 32 потока мы пошли дальше и задумались А что если дать http интерфейс агрегатору вот у нас были следующие соображения какие будут плюсы первый плюс очевидный аналитики смогут самостоятельно готовить себе агрегаты без лишней бюрократии джира долгого кадре вьюги Клаб и так далее То есть без нашей помощи станет проще тестировать сложные написали запрос котором 1000 строк Вы можете его запустить и посмотреть то ли он собирает то что нужно вообще работает ли он и так далее и наконец-то появится потребованию у нас был сильно встроен основные Pay планы и его было невозможно запустить на своих основных метриках не добавив Поэтому если дать возможность людям самим собирать агрегаты то появится возможность читать обтесты минусы минусы были такого характера Мы подумали что аналитики Будут еще сильнее упороваться и хаос аналитики будут вынуждены разбираться в http то есть чтобы ходить по можете пить дёргать агрегатор и придется писать много кода но действительности получилось гораздо лучше чем мы себе представляли получилось у нас следующее у нас получилось удобное защита парня у нас появилось видео то есть теперь между аналитиками и агрегаторами Клин хаосом есть который защищает перегрузки сетевой слой мы спрятали в удобный клиент который выглядит очень активно и забавный случай один из аналитиков Я работал с клиентом как будто бы он работает локально То есть он не понимал что действительности его запрос его данные обрабатываются на бэкенде а не у него в юпитер-тиратке аналитики используют API как сервис для даты инженерного самообслуживания они знают escale могут использовать его для сборки своих витрин самостоятельно не ставим задачи И тем самым достаточно быстро падает значит Кроме того важные интересные моменты то что наш инструмент он конкурируется с парком для запуска отходов запросов на больших данных часто аналитики выбирают где будем запустить запросы чаще они предпочитают на самом деле наш инструмент они Спарк естественно мы изначально планировали у нас это получилось сделать интерфейс для обмена больших данных и дополнительно мы автоматизировали рутину по добавлению колонок и удалению всяких кривых данных ошибочно аналитики нами вот теперь прежде чем переходить к внутреннему устройству Давайте остановимся на основных терминах которые нам будут важны для дальнейшего понимания первый термин это операция это набор логически связанных действий которые требуются сделать для получения Конечный результат Какой может быть Конечный результат это может быть сборка витрины то есть операция greation это может быть Селект который отдает данные аналитику в тетрадку это может быть подсчета б или Альтер конечно это добавление или удаление дальше джоба это составная часть операции по факту это запрос над ним куском данных из множества от одного до n кусков pipeline это последовательно выполняемый набор операции которым как может быть связан по данным так Нет вы можете независимой операции винить paypline для какой-то своей цели выполнить даже если они связаны по данным такая возможность имеется теперь вкратце поговорим о внутреннем устройстве внутреннее устройство сервиса очень простое архитектура максимального упрощенная для того чтобы можно было легче разрабатывать и быстрее внедрять новые фичи Значит мы используем в качестве внутреннего хранилища статусов и логов операций пост в качестве очередей Мы берем рейтис и дальше у нас стоят norker они все общего назначения То есть может как выполнять агрегации так и выполнять все эффекты так и считать и эти воркеры уже непосредственно работают с данными на Клик Хаусе в качестве входной точки перед ними стоит который обрабатывают входящие запросы из бетон клиента потом клиент это обычный обертка на до 6 Без запросами для того чтобы аналитикам было удобней обычно питон клиент вызывается либо из Юпитера либо например с airflow Если кто-то что-то хочет автоматизировать поставить на расписание и так далее важный момент в устройстве всего нашего бэкенда это устройство шеделлинга это наверное самое интересная часть о которой можно остановиться чуть подробнее У нас сейчас есть предопределенный классы очереди есть класс коротких Чародей куда операция попадает если секунд средняя то есть от 121 до 900 секунд и длинные это 91 секунды до 6 часов Это сейчас определяется аналитиком когда он описывается операцию питоновским кодом он задает тайм-аут для своей операции будущем Мы хотим отказаться от этого решения и перейти на eml в определении таймингов для каждой операции но сейчас мы используем схему с мультиваркерами А до этого мы использовали схему одного очередь один worker как это работа приходила операция классы очереди определялся по проставленному тайм-ауту А дальше внутри класса выбиралась самая короткая очередь вот сейчас у нас для одного класса очередей существует только одна очередь которую уже читает несколько оргеров Как легко понять это сильно снижает Время ожидания в очереди и в принципе работает более прозрачно и Понятно чем схема с одного очередь один worker и мы с конца августа полностью пришли на схему скажу еще пару слов очень простой в качестве веб-сервера мы используем фастафе потому что на синхронные современные принципе довольно удобно обрабатывать и уже был опыт работы с ласком поэтому поставьте выглядит хорошим вариантом в качестве бэкенда воркеров мы используем workers наверное самая простая библиотека для синхронных людей Нам пока хватает невозможности мы довольно неплохо научились ее готовить поэтому используем ее То есть она у нас используется не только в проекте API Ну например в проекте Аномалия То есть у нас такой довольно стандартный стек который нам позволяет создавать сервисы на работоданами довольно быстро логика воркеров это кастомная библиотеке То есть каждый из операций действительности это кастомная библиотека например для агрегации и для секретов используется greation Tool То есть это тот агрегатор который описывал на первых слайдах для подведения об тестов нас используется библиотечка достаточно короткие порядка полторы тысячи строк всего поэтому объем кода на самом деле очень-очень небольшой вычислительная часть у нас довольно таки стандартная это прежде всего нам пай пандус и для подведения результатов мы используем еще с Models линейной моделью об этом чуть позже Теперь давайте посмотрим на устройство клиента наверное самое интересная часть и будет более понятным Какие возможности у нас сейчас есть И что появится в дальнейшем для начала схема классов клиенте выглядит вот так есть две основные ветки наследования от базовой операции это ветка админских операций типа alter of и пользовательские операции которые наследуется вот потомка Operation plain Operation plation операция может быть использована и вот здесь мы видим что есть операция bregation есть Наследник Селект агрешена есть еще Наследники бакинскому кастом в баке с кастом символ это шаблон для а б дальше тоже подробно об этом расскажу Ну и в принципе система построена таким образом чтобы аналитики и те кому требуется что-то писать могли легко отслеживается базовый классы реализовать себе нужны Итак первая операция самая основная tagrecation по факту это сборка витрины То есть это выполнение на сердце Давайте посмотрим более подробно слева мы видим код запроса код витрины который собирает у нас что-то из таблицы слогами видим прокинутый placeholder в sample of Set Они будут заменяться агрегатором на соответствующие куски также видим поиск холдер для даты дальше это все добавляет свою оберты прокидывается клиенту соединение и указывается тайм-аут в данном случае Мы оказались на 20 секунд это значит мы попадем быстро очередь и указывается гранулярность гранулярность Как вы помните Это количество кусков на которые надо разбивать в нашем случае здесь это будет равно 32 куском дальше операции стандартный интерфейс И в нем вот интерфейсе есть метод который согните операцию Back and и дальше Back and GS этой операцией работает какие есть еще аргументы в Операции самое главное аргумент как я говорю это локальная сборка и называется Документ и включен то соответственно сборка происходит локально можно указывать конкретный шар на котором вы хотите собрать можно указывать даты можно указывать даже когда запустить То есть вы можете создать операцию ее гибко зашитулить и она бы кендом запустится в то время в котором вы сказали вот Ну понятно Я уже понимал что есть количество оркеров которые надо передать в целом это очень конфигурируемая операция по факту больше 20 флагов и можно довольно гибко настраивать то что вы хотите следующая операция Select Select это самая популярная операция наверное самое важное Давайте посмотрим подробно представим что у нас есть запрос вот мы что-то агрегируем в некоторых существенно ради по таблице контент тут Важно отметить что таблица контент сэмплирована по User ID то есть мы вычитываем подножить ради агрегируем вовне то есть каждом из N кусков на котором может быть представлен ключом ради эн раз вот поэтому мы действительности делаем в этом селекте предогрегацию а потом уселикты есть специальный метод который можно передать свой кастомный запрос который уже выполнит до агрегации что мы делаем Сначала мы на кусках собираем агрегацию а потом эти N кусков поверх них запускаем агрегацию вида селектор сначала разби эскиз их агрегировали а потом до агрегировали эти куски если посмотреть на Pay планки преобразовались данные то на стадии предогрегации Мы из таблицы контент Юз получили временную генерируемую таблицу в таблице condonalds при этом было 11 миллиардов строк а на выходе мы получили 163 миллиона строк а дальше настать до агрегации Мы из временной таблицы генерируемой 163 миллиона строк взяли и на выходе получили 29 на каждой стадии мы последовательно уменьшали объем нашей таблицы и наверное логично предположить что операция Select это некоторые pipeline что происходит по запросу происходит inference схема промежуточной таблицы то есть название колонок из вашего и типы дальше происходит запуск промежуточной таблицы потом происходит запросы с промежуточной таблицы и до агрегации если требуется дальше ваши данные попадают в Юпитер значит сборщик У нас есть сборщик мусора временные таблицы удаляются по какому-то заданности в действительности если мы посмотрим на это чуть по другим углом то запуск операции это стадия стадия запроса из промежуточной таблицы навигации это стадии редис Итого Селект агрегации у нас реализует стратегии то есть мы развили набор данных на куски их Независимо отработали а потом по какому-то набору ключей нам нужно сделали и собственно Select это и есть некоторые мапредил с интерфейс данном Клик Хауса который нам позволяет обрабатывать очень большие объемы данных которые не влазят в память одной машины обрабатывать дикие селекции с джойнами и всякие такие вещи которые часто пишут аналитики Ни в чем их не ограничи ваем Давайте теперь перейдем к paypaline который реализуют функциональность которую мы писали выше Давайте представим что мы хотим использовать нашему Стратегию и результат до агрегации потом мы хотим еще куда-то вставить Мы считаем что это большое догрегируем и поэтому допустим рисуем какую-то картинку в суперсети любой системе для решения такой задачи можно использовать что для этого нужно сделать первое самое важное это зададим название временной таблице заранее То есть вы видите мы написали придумали какое-то название указали номер реплики в которой нужно уложить дальше мы это название используем в аргументе был то есть таблицы будут сгенерированы именно с этим названием а следующее задание операция агрегации У нас считает из этой таблицы подставляем знания таблицы создаем объект добавляем туда две операции через метод и запуска и в действительности вот эта вот схема реализует mapress стратегию в виде такого вот пайплайна чем это может быть полезно но как я уже говорил Допустим мы хотим нарисовать график по очень большому набору данных Сначала мы должны Map сделать потом уже после этого рисовать вот ну параллельно где-то если это требуется но это уже все должно происходить на уровне биосистемы то есть здесь самое важное часть это последовательность которую мы реализуем вот таким вот достаточно простым и я бы сказал шаблонным кодом отмечу здесь что в таблице временные которые создает операция Селект мы еще указали что мы хотим иметь ключ сэмплирование еще мы это сделали для того чтобы наша операция агрегации в случае если у нас очень много данных запишется промежуточную таблицу могла тоже отработать по частям и это нам позволит даже случае большого набора и промежуточных данных не упасть и обработать Мы в первой таблице мы читаем а потом промежуточная в которой будем делать финальную агрегацию это все достаточно просто и код шаблоны можно импортировать и второй важный пример использования пайплайна это подсчета B Для начала я расскажу как вообще у нас устроена AB Значит у нас а б считается не на юзерном распределении она по бакетам Почему Потому что у нас очень большое количество экспериментов если я все правильно помню то это порядка 130 экспериментов в день действительности можем обрабатывать и значительно большее количество экспериментов каждого эксперименте порядка там 500 может быть 600 метрик и поэтому мы переходим по бакетному распределению распределение получается следующим образом это берется какой-то Хеш от юзреть И делится на количество пакетов и этом бакету применяется какая-то грядирующая функция например сумма соответственно нас выборка уменьшается достаточно сильно и мы работаем с маленькими распределениями которые можно быстро легко обрабатывать Давайте себе представим что у нас есть аддитивная Метрика То есть это Метрика которую можно за весь период эксперимента сложить допустим просмотр постов в умной ленте чтобы каждый раз не запрашивать весь диапазон данных давайте сделаем следующую штуку Давайте будем хранить кумулятивную сумму по этой метрике в этом конкретном эксперименте и в этом пакете и каждый день будем подливать небольшой кусочек данных И тем самым у нас диапазон чтения будет всегда один и тот же то есть мы будем всегда читать набор ключей и скорость чтения не будет зависеть от количества дней которые идет эксперимента будет зависеть только от количества ключей в этом эксперименте Мы сначала получаем по бакетное распределение а потом распределение для аддитивных метрик уже агрегируем в кумулятивную таблицу которую мы храним данные за весь период и после того как мы все необходимые агрегации сделали мы вызываем уже собственно подводилку AB на полученных данных как это выглядит подсчет А Б как я говорю это уже paypaline он состоит из следующих частей первая часть это получение по банкетного распределения для этого есть уже упоминавшийся мной шаблоном баке с кастом это шаблон по факту генерирующие запрос вида Join блога на Join Экспериментальный размер экспериментальной разметка это просто логирование вызовов где-то Брукс которым из которого мы понимаем в какую группу попал конкретный юзер конкретном эксперименте мы видим в шаблоне перечисления метрик и потом перечисление формул того как они считаются и указание вот ну и какие-то вы A Condition дальше это добавляется в Starting Jobs класса by плана бпайплайнда класс Наследник базового класса paypeline специфический который внутри себя генерит необходимую нам последовательность сборки куму сборки багетного распределения дальше сборки кумулятивной суммы и вызовов уже Starting Jobs мы можем распечатать планы посмотреть что и вот код который указан на данном сайте он генерирует следующий это получение по бакену распределения вот тут для 27 октября дальше идет тоже агрегация но это агрегация в кумулятивную таблицу то есть дальше идет два вызова операция B первая операция б Это для неадитивных метрик а вторая для детей то есть одна из них ходит в бакетное распределение а другая в кумулятивный байке на распределение Вот то есть бyплайн это шаблон для B который уже под капотом генерирует все нужные операции в последовательности корректной и соответственно аналитиком обычным людям об этом думать не нужно И это все уже делается на них автоматически Вот теперь перейдем самому интересно это план на будущее не всегда легко приятно говорить как я говорю уже распределение по очередям было бы на самом деле сделать очень хорошо потому что люди часто ошибаются с тем тайм-аутом который не указывают и попадают в некорректную очередь мы уже достаточно хорошо научились предсказывать время выполнения операции и сейчас эта задача находится уже в активной разработки дальше Мы хотим автоматически развивать на куски выиграешь то есть не указывать placehold сэмплов система должна сама понять что ставить Как нужно сыграть а кроме того Мы целимся в очень амбициозную задачу эта интеграция с биосистемы что мы хотим делать Мы хотим тяжелые запросы исполнять через кодогенерацию Pay планов в нашей связке selectation если аналитик написал очень тяжелый запрос например подсчет не знаю просмотров постов умной ленте за год по каждому юзеру то такой код запускать на кликхаусе в прямую нельзя он будет сгенерирован соответствующий питон код для pipeline и он будет запущен уже и человек получит свой график несколько попозже но и самое наверное амбициозная Цель этого консоль сейчас у нас кот относительно небольшой маленькой команды и наверное будущем Мы бы хотели чтобы поделиться сообществом нашими наработками И наверное Возможно кому-то это будет интересно Вы уже собственных компаниях и теперь несколько забавных цифр сейчас у нас примерно 9 воркеров в кликаосапе выполняя задачи два человека выполнять задачи по разработке то есть команда действительно очень маленькая еженедельно запускается примерно 10000 агрегации Select и agregation активная аудитория сервиса это 70 человек в неделю это большинство из них аналитика но есть разработчики мэрии инженеры и иногда даже продукты за прошлый год больше всего запустил человек операции это 39 тысяч операций А по рекорд это две с половиной тысячи операций за год спасибо за внимание Буду рад ответить на ваши вопросы Спасибо"
}