{
  "video_id": "ObMpl5gFVgw",
  "channel": "HighLoadChannel",
  "title": "Использование Hadoop в Badoo / Валерий Старынин (Badoo)",
  "views": 301,
  "duration": 3111,
  "published": "2017-04-22T14:47:42-07:00",
  "text": "те кто интересуется использованием ходу своей практике те кто уже используют те кто хочет его использовать но пока не знает как или просто может быть боится так же как и мы боялись год назад но в своем докладе я расскажу как фактически перестать бояться его использовать и начать его использовать как сегодня пошутил один мой бывший коллега это говорит доклад про тот кластер который вас год простояла вы на нем ничего не делали да это доклад про тот кластер который сначала мы никак не могли использовать но сейчас он работает на полную катушку и со временем мы планируем друль но вы все новые и новые использования этого нашего кластера для наших задач вычисления статистики в своем докладе я расскажу какую мы собираем статистику зачем как зачем нам потребовалось вообще ходу почему так много всего стала объясню какие простые шаги мы сделали это того чтобы перестать бояться ходу по и расскажу что мы уже сделали и что мы планируем делать в ближайшее время вкратце ободу подует социальная сеть новых знакомств на данный момент это 200 226 миллионов активных пользователей это сеть знакомств работает по всему миру работает на любых устройствах то есть это мобильные телефоны это компьютеры в общем практически везде у нас два с половиной дата-центра в европе в америке в азии в вазе у нас хостинг фотографий поэтому там нет никакого полне моего кода поэтому половинка и всего это более трех тысяч серверов и все эти сервера производят какие то события которые мы потом считаем что же такое событие событие это действие пользователей в первую очередь основной поток событий это действия пользователя пользователи кликают в интерфейсе пользователь отправляет сообщение пользователя читают сообщение в общем любое действие пользователя это событие действие модераторов это событие скрипты работают тоже порождают какие-то события или серии событий ошибки происходят это тоже событие отчеты о выполнении каких-либо задач это тоже события все эти события мы собираем с помощью инфраструктуры построены нас край по на каждом сервере у нас есть skype сервер который принимает события от выполняемого кода на этом сервере очень быстро в течение там 500 микросекунд по моему и дальше занимается доставкой он передает его на главный сервер на в это в этом дата-центре и оттуда событий уже попадают туда куда необходимо skype умеет доставлять файлы только файлы он записывает файлы все дальше с этим надо что-то делать у нас есть разные варианты что с этим делать в том числе и помещать в ходу значит до этого до того как мы начали использовать hadoop мы использовали систему stats коллектора который рассказывал на d в конфе на хабре можно найти его мое выступление ссылки на слайды в конечном итоге события агрегировать и по каждому пользователю нас получалась вот такая вот статистика значит здесь агрегация идет по дате вот первое число второе число и по идентификатору пользователя то есть получается например вот это один пользователя за одну дату это и тот же пользователь за другую дату и по каждому пользователю мы считаем различные показатели здесь я представил показатели голосов и время проведенное на сайте то есть понятное дело время проведенное на сайте это просто сумма сумма всех отрезков через который пользователь давалось какие-то запросы вот она суммируется задней получается такой то чет но у этого были проблемы проблемы были ну во-первых достаточно большой объем данных база данных только по допустим актив активность из мобильных приложений занимала в месяц 350 гигабайт соответственно чтобы добавить новую метрику добавить новую колонку приходил сделать сальто который не всегда укладывался в рабочий день а при этом статистика еще не пишется таблица заблокировано были разные хитрые трюки как это обойти но это очень неприятно долго и муторно не хватает места на серверах понятное дело что просто так там никакой раздельщик не добавишь мой стиль так не поймет надо переносить на другой сервер с большим количеством дисков и так далее вот но и самое главное что нет детальной информации информация за день вся с агрегированном то есть пользователь сделал например 5 открытие профиля кого-то все мы знаем только цифру 5 мы даже не знаем сколько раз он повторно открывал профиль одного и того же пользователя мы не можем посчитать уникальные значения нашим аналитикам хотелось это знать не каждый день но такая информация им нужно было и в конечном итоге мы решили воспользоваться в общем решили что она как чуть придумать и без ситуацию исправлять значит чего нам захотелось чтоб наша новая система могла хранить все в не агрегированном виде то есть каждое событие в отдельности все хранить и хранить как можно дольше нам выдвигали сроки два года мы хотим хранить еще дольше мы хотим расширяться без проблем то есть купить новый сервер не проблема главное чтоб его как-то можно было включить в общую структуру использовать его диски и вперед решить этот вопрос с колонками но очень у нас напрягал как-то надо было что-то с этим делать им данные должны быть доступны для анализа то есть вариант скинуть всё на ленточку убрать куда-нибудь на полку потом достать развернуть бэкапы в общем через неделю аналитику предоставить данные потому что админы тоже занимаются какими-то своими другими делами это не вариант ну и хочется также использовать что-то искали подобное илийский в самом хорошем варианте потому что привыкли к нему удобно значит тут один и наших коллег предложила давайте мы hadoop используем я говорит на предыдущем месте вы использовал нормально работает ну давайте попробуем продукт известны используются большими компаниями вроде удачно используется на конференциях также про него рассказывают взяли поставили работает наверно подойдет но мы тоже взяли поставили чуть дальше делать боимся не знаем как туда данный помещать как их туда извлекать почитали книжки танцу как-то сложно не понятно мы приди устам генерация пары ключ-значение потом там как то все все очень непонятно ну в общем начале у дальше смотреть ковырять и что оказалось hadoop это отнюдь не черный ящик в которой там как-то что-то помещается где-то что-то куда-то там можно найти знакомые вещи например файлы хранятся в виде файликов на диске да именно немножко изменены можно докопаться и понять что-то тот этот файл это приятно хотя бы что-то знакомое дальше данные сами реплицируются то есть файлик один раз залив ходу полного сам раскидал с нужным коэффициентом репликации ну по умолчанию используется три мы довольно в принципе этим фактом мы тоже используем этот фактор репликации получается что мы можем потерять два сервера и при этом у нас данные останутся живыми дальше посмотрели есть hotel haifa и он очень похож по синтаксису на mais quel порой даже ссылки на какие-то функции отсылают на d в москве ком то есть там строковые функции к примеру какие то они брались прямо туда синтаксис такой же здорово можно учетом попытаться и оказывается hive понимает tab separate файлы и джейсон файла в результате если взять и создать вот такую табличку то есть залить туда файлики просто файлики текстовые разделенные табами значит здесь поля 5 полей перечисленным файлики делятся по каталогам в каталоге содержатся поля dt дата то есть мы все делим по датам и вот эту дату мы прям в имени каталога храним их а его это удобно когда мы запрашиваем за одну дату он берет просто файл из одного каталога не лезет вообще другие дальше говорим что ряды у нас значит в разделе по поля разделяются табом а ряды разделяются переносом строки все данные лежат вот здесь но это переменная нас заменяется на реальный путь и он раз уже есть табличка потом из нее делаешь select он берет сам читает файл выводит уже как будто прям select сделал азбука точно так же как в моей сквере то есть казалось бы вроде положили файлик написали запрос на создание таблицы поверх этого файла и hive уже умеет читать уже умеет обрабатывать то здесь же можно и группировки применяйте все все он уже готов к работе здорово уже уже не так стало страшно уже стало понятно что не надо ковыряться в глубине не надо разбираться там к см и приду сам оно само как-то работает то есть до hadoop это набор к столе и подпорок но он такой удачный что его можно можно разбирать на составляющая часть а можно не разбирать можно просто взять и использовать ну и дальше мы начали применять и с помощью hadoop мы сейчас сделали четыре вещи во-первых это вот активность по которой я уже говорил дальше мы используем его для длительного хранения данных то есть мы данный из моей сквер перегружаем hadoop в миску или долго не храним сделали проект под названием hot пену и пишем полностью клик стрим со всего сайта тоже храним его в ходу пин на случай всяких подробных анализов может быть который нам пригодятся значит активность мы начали собирать вот в таком виде действие действие пользователей у нас сайт посвящен взаимодействия пользователей поэтому у нас есть понятное дело время в которое произошло событие пользователь который его сделал есть второй пользователь по отношению к которому это было сделано например голосование за фотографию кто-то нажимает на кнопку но при этом увидеть фотографию другого человека вот pc фьюза рейде это вот чья фотография дальше пользователь зашел к кому-то в профиль это пользователя один совершил действия а зашел в профиль к другому это соответственно другой это пассивный пользователь по отношению к которому совершено действия дальше пишется действие пишется еще поле каунт как бы для просмотров для голосов понятное дело она будет единичкой а вот например для времени проведенного на сайте оно уже будет не единичкой поэтому поле каунт нужно значит как мы в ходу кладем события вроде кажется hadoop это система для работы с большими объемами данных а события у нас одно маленькое ну там не знаю 50 байт в ходу пи говорят там надо данным там мегабайтами заливать что файлики мегабайт не были мы его с помощью того же самого скрабы собираем собираем а skype умеет сам нарезать фрагментами с установленным размером наберем time limit 300 мегабайт получаем файлы по 300 мегабайт содержащий просто строки с этими событиями дальше мы разделяем таким образом чтобы в одном файле была только одна дата то есть на границе дней возможно попадание в один файл событий с разными датами мы это делим для удобства для того чтобы в разные каталоге положить в файлике ссыпаются и заливаются в ходу все мы из маленьких единичных событий создали большой файл удобно удобной для анализа естественно за день таких файлов получается много но для ходу поэта не проблема дальше мы строим агрегаты сначала таблицу просто с хлопковым по дням нам надо построить всю ту же самую активность сырые данные у нас уже есть это хорошо мы можем с помощью хайло там делать запросы ну надо построить то что мы уже имели до этого значит строится в два этапа первый этап суммируется за день одинаковые одинаковые события для одинаковых пользователя то есть например вот первая строка и последнее это действие голосования нет но тоном она у нас так называется значит пользователь один тот же действие суммируется синичка и ничка получаем вот эту двоечку сумма а вторая колонка это количество зная суммы количество мы можем посчитать среднее значение дальше вторая строка и предпоследнее время проведенное пользователем на сайте тоже суммируется получается такое такое значение cal и количество строк просуммированы устройте уникальные переходят в этот лог один к одному и здесь самый главный факт если мы собираем новое событие то нам не надо на этом этапе знать как какие значения может принимать вот эта колонка мы просто группируем по ней все группируем по дайте пользователю и этой колонке неважно какие там значение добавили новое отлично она группируется и попадет во вторую табличку все все будет хорошо то есть на этом этапе добавление нового счетчика не требует никаких действий вообще а на следующем этапе мы данные помещаем в нашу базу данных экзо sol экзо solutions это такая быстро колоночный обазом с которой работают уже наши аналитики вот и вот на этом этапе это единственное место где требуется указать новую колонку мы просто группируем по пользователю и по дате а дальше каждая строка в этом запросе это и есть ну это строка предназначена для создания новой колонки все мы получили возможность собирать ту же наша активность которая у нас была в том же самом виде она работает мы уже перешли на использование новой системы полностью отказались от старой это что касается активности второй шаг у нас были другие отчеты которые тоже собирались stats коллектором тоже хранились мои сквере да не меньше но все равно их много их периодически очищали старые данные нам хотелось бы сохранить хранить долго и иногда еще кое какое удобство в обработке получить дело в том что stats коллектор при добавлении новых колонок он старый эта таблица не трогает начали передавать больше данных он новую табличку создал с большим количеством колонок а старый не изменил что мы сделали просто выгружаем все в виде текстового файла с заголовком какие колонки у нас есть вызывается обычный майский с передачей запроса select он то все пишет файл перенаправляем файл zip у им кладем в ходу при узи второй скрипт скриптик написали который получает на вход список колонок которые нужны и в нужном порядке уже при вычитке файла возвращает значение если колонки нет подставлять значения по умолчанию там кроме имен колонок передается еще типа ну по типу можно понять если это число то но если эта дата то там 4 0 тире 2 0 тире 2 0 если строка то пустая строка удобно добавили это все в конфиг то есть система дальше сама берет просто конфиг смотрится нам надо выгружать смотрит если у насти данные если нет то просто выгружает если есть ничего не делает ждёт нового дня данное опять появились опять выгружаем все она сама работает сама крутится само выгружает опять же получается достаточно большие файлы потому что если мы выгружаем за день там как раз получается уже с файлами габаритного размера опять же удобно для ходу по 3 вещи что мы сделали этот проект под названием hot pnl он предполагался как замена google analytics то есть для сайтов понятно берем google analytics ставим работаем для мобильных приложений не все так гладко вроде что-то было под android но она тоже не совсем хорошо работала а и ос win на бал никак подумали подумали давайте попробуем сделать начали начали собирать сначала собирать просто события которые происходят в интерфейсе который не не посылается на сервер они не приводят к запросу серверному то есть мы бы их так до этого не могли собирать теперь что сделали собираемых на клиенте набралось там сколько-то отправляем на сервер прям пачкой событие решили отправлять в формате джейсон потому что можно его расширять потом будет и уже проводились какие-то изменения то есть разные форматы для разных приложений а оказалось что что можно это все дело прям джейсоне положить в hadoop и потом hadoop и hive прекрасно работают с этим джейсоном анализирует можно по разным параметрам и очень получается сумасшедшая статистика с большим количеством всяких разных там вариантов я здесь по моему да здесь операционная система можно выбирать здесь события которое происходит здесь версия мобильного приложения но версию входят также там вот тут android тут где-то был от ios плохо наверно видно но в общем статистике море можно из этого выжить и все это делается запросами в запив запросами на песнями на hive quelle то есть он читает джейсон ему все равно делает это тоже достаточно быстро я затрудняюсь привести время как за которая строится вот эта статистика но агрегирование активности производится 15 минут нашим кластером то есть за день берется не агрегируется за 15 минут дальше клик стрим мы пишем просто все действия которые происходят на сайте мы пишем урл реферера и притом user айди пишем все события stats коллектора которые производятся в процессе этого запроса и в общем это позволяет нам при необходимости подробно взглянуть на все все все что происходило с точностью до секунды до вызова и проанализировать что-то позволяет иногда статистику которую мы не собирали уже начать собирать если мы знаем адрес по которому было вызов был вызов знаем мы не стефи катар пользователь мы можем понять что там пользователя этим запросам сделал в общем это все что мы сделали именно для аналитиков но для того чтобы все наш кластер работал надо еще немножечко усилий приложить мониторинг значит мы тут свой велосипед построили на прошлом хай ладим мы послушали прокла удеру но там бесплатный вариант по моему до 50 серверов это по-хорошему и над лицензировать надо платить там за каждую ноту туда-сюда чем мы поставили чистый hadoop который взяли на бочонком сайте мониторинг сделали своими силами мониторим на общее состояние серверов пингуется не пингуется мониторим количество дата нодов и task трекеров ну это количество храни серверов хранилищ серверов обработки вот такое что сервер выпадает бывает наоборот что админ с там что-то ставили запустили взяли og новый сервер в кластере которого не ждали тут на него репликация полилась вот количество блоков ago reply кредит это излишних и андре прикатят наоборот если надо отвалилась та часть данных не недостаточно реплицироваться все это дело мониторим просто при нем при необходимости принимаем какие-то действия предпринимаем бы капец собака пиццы надо ну естественно данный бэкапить уже сложно их многом то есть мы их не быка пима не так по три копии имеют а вот единую точку отказа на им ноду мы вынуждены бэкапить во-первых исходные файлы хранятся два дня на серверах с которых мы загружаем то есть даже если все пропадет мы имеем ноду из старого как он backup а развернем ее файлики эти дольем туда соответственно делается бэкап них не всего содержимого найм ноды а только лишь описания которое описывает имена и блоки составляющие эти файлики и делается такой еще backup который позволяет вообще вручную даже все восстановить я уже говорил что все лежит файликах так вот если дать команду ходу puffy стеком с таким набором параметров то выданной информации достаточно чтобы определить из каких блоков состоит этот файл и его восстановить блоки просто склеиваются обычным катом то есть в один файл это все сливаем и все мод можно даже вручную восстановить вот так мы бы copy может быть несколько кривовато но это имеет под собой определенную основу сейчас вот потому что проблемы hadoop ну не всегда так прям вот сам решает все внутренние проблемы как хотелось бы то есть он может не очень очевидно них предупреждать может быть мы конечно не умеем его готовить еще так сильно но факт остается фактом например если появляется сбойные блоки то при чтении файла периодически выдается ошибка типа мы попытались прочитать на него не сошлась контрольная сумма печаль там с третьего раза вы все-таки выбирается с другого сервера файл читается приходится это не вручную просто удалять файл заливать его за но то есть зачитывать его к себе в конечном итоге удачно и потом заливать обратно она один раз нас был была история мы долго искали битый диск появлялись не до реплицировали файл и потом сообщение то пропадал появлялась пропадала оказалось просто на одной ноги был битый диск и все вот никакой меняем информации нет от версии к версии меняется api меняется название параметров в конфигах в общем меняется практически все то есть обновить просто взять обновить нельзя надо поставить рядышком посмотреть как настроить тоже что мы настраивали посмотреть дефолтные настройки которые могли вполне поменяться сравнить все это дело только потом уже можем обновляться вот ну в общем вот такой вот он забавный но мы его любим и применяем вот и значит что мы хотим в будущем сделать хотим мы обновиться с ходу по на которого мы начинали мы он не трогали вот по этой причине что сложно обновляться мы хотим нравиться dot свеженькой версии 25 хотим попробовать вообще использовать spark потому что обещают очень серьезный прирост обещает что если вы будете все данные необходимы хранить в памяти то вообще в 100 раз быстрее это интересно это уже ближе ближе ближе к тому что мы хотим хотим начинайте какую не замену скраб потому что проект практически не поддерживается до он сейчас работает в той конфигурации в которой нас есть но хотим попробовать что-нибудь поискать на ему на замену какую-нибудь другую систему которая будет доставлять нам наше сообщение от распределенного ну совсем со всей с со всего дата-центра на центральный сервер там уже обрабатывать от ну и напоследок немного гиг порно итак встречайте его силён быстро вынослив наш hadoop кластер итак наш hadoop кластер состоит из 16 серверов один из них это на им надо она несколько похожи по конфигурации четырехъядерный процессор с гипертрейдинг am то есть получается 8 64 гигабайта оперативной памяти один жесткий диск 1 1 терабайт ну видимого обмена в так такие были потому что он не используется там используя процентов на десять от силы значит и 15 data not any помощнее потому что на них же не только данные хранятся на вещей обработка производится это 16 ядерный процессор с гипер трейдингом и того 32 сто девяносто два гигабайта оперативной памяти и дисковый массив из 10 дисков по 1 1 1 терабайт диски не объединены не как они просто смонтированы в 10 разных каталогов и ходу пускай она вот у тебя 10 каталогов куда класть файлики окей хорошо буду буду размазывать их сам вот и теперь к объемам данных которые мы собираем значит активности у нас 100 гигабайт в день собирается экстрима 1 и 2 терабайта все это дело сжимается получаем 270 гигабайт в день в сжатом виде газе пам обыкновенным газетам ну и естественно фактор облигации 3 то есть в 800 10 гигабайт из всех дисков мы ежедневно забираем вот ну и объемы полоса бы по количеству событий примерно одинаковая у активности у клик стрима по миллиону то есть где то 2 миллиона в день у нас набегает вот так мы и живем спасибо за внимание ваши вопросы здравствуйте спасибо за доклад у меня вопросы комментарии одновременно я не очень понял почему вы выбрали хранение данных в ходу пи виде текста xii кованого при том что как бы есть нативной структуры типа rc файлов и optimizer торце файлов которые вам сразу на порядок увеличить скорость выборки через hive вот мне интересно почему именно это и снова комментарии как бы если мы при этом еще возьмем импалу мы получим еще на порядок выигрыш вот ну я больше ответ на вопрос интересует спасибо значит если я ничего не путаю трц это не текстовый формат ада это не текстовые форматы не тем не менее он поддерживает блочные сжатие оптимизирован для кого ночных структур и всегда можно selecta монсир там получить текст обратно да значит мы думали об этом формате но он жестко привязана к дубу когда мы начинали все это делать мы не были уверены в том что мы останемся на ходу пи почему у нас есть и всевозможные варианты вот пропадает java с ходу пам у нас есть все чтобы восстановить все наши данные мы мы же боялись его использовать и мы не были уверены мы рассматривали этот вариант но он требует дополнительные дополнительного преобразования здесь все преобразования выполняются очень очень простыми средствами ну вот этот вот миллиард событий в день он обсчитывается у тебя уже сказал за 15 минут даже меньше меньше по моему щас минут за десять уже кстати ускориться нам удалось после того как мы исключили старые медленные ноды удивительно но факт задание поступали на старые ноды они там пытались общаться и за это время новые заканчивали свою часть работы делали еще и часть за старые в общем лишние перегонка данных по сети она была просто лишним когда мы избавились от этого мы ускорились вот ну в общем формат просто потому что он не текстовые мы не можем просчитать глазами + тексты в очень удобен для отладки по поводу импалы ну мы в общем будем пробовать что-нибудь другое мы сейчас начало отстроили это а вот как раз сейчас мы подходим к тому моменту когда мы будем пробовать что-нибудь новенькое еще добрый день ложку тут максим у меня такой вопрос вы сказали что у вас есть некий буфер компоненты которая собирает мелкие логе да файлы по 300 мегабайт после этого кладет на gdfs плюс есть проблема с мастернодой на delphi секретную себе по копированием почему скажи мне взять кассандру которая специально предназначена для быстрой записи мелких сообщений логов при этом интегрируется с ходу пам фактически print по интерфейсу dfs а не имеет мастерноды поскольку является централизованным хранилищем тоже реплицируется данные вот собственно в этом и вопрос hadoop мы выбрали потому что у нас был человек который имел опыт работы с ходу пам нет вы не поняли сейчас сандра она заменяет в ходу пьеха dfs то есть является хранилищем причем специально нацеленным под мелкий ну запись большого количества небольшой информации ходу непосредственно библиотеки mapreduce они также по тем же интерфейсом что и к dfs умеют подключаться к кассандре соответственно мы решаем проблему с мастерноды либо копированием потому что кассандра до централизованное хранилище это раз во вторых решаем проблему с вот этой буферным звеном которая собирает данные мелкие в большие файлы но данный это собрать нам все равно надо будет но их надо можно просто напрямую лить в кассандру а если мы начинаем напрямую лить в кассандру то мы упираемся в сеть как каждый скрип должен подключиться к кассандре отправить туда сообщения это долго но кассандры до централизованно хранится можно тоже поставить парковаться ходули 16 нот вот столько же нот кассандры нет у нас скрип запущен на каждом сервере локальный скрип запущены на каждом сервере приложение отдавая сообщение подключается локально и до этого очень очень быстро а дальше локальный skype уже занимается доставкой в центральной stripe то есть вот этот вот как раз та утилита по которой вы говорили скраб значит доставка производится отправка для со для приложения производится практически мгновенно если мы включаем туда работу посетит а мы сразу увеличиваем ну на порядок я думаю из-за сетевые задержки понятно пасибо добрый день у меня такой вопрос а вы не подскажите насколько изменилось количество но чтобы пришли там с решение на мой скрин с майским на ходу количество нот ну как сказать статистику по статистику по активности мы собирали на 2 на 2 серверах майский но у нас не было детально данных поэтому ну как сказать да она увеличилась на увеличивается в восемь раз но мы собираем намного больше и подробнее данные поэтому я считаю что это оправданно они оценивали если бы оставались при той же технологию мы бы просто не смогли собирать эти данные вообще у нас агрегированные данные занимает 350 гигабайт в месяц а теперь представьте какой объем занимали бы сырые данные там примерно в восемь раз они уменьшаются после агрегации спасибо понят здравствуйте скажите а судя потому что я видел слова that those tracker вы все еще используете mapreduce первая версия да мы планируем переход на вторую версию наверно возьмите если могу найти да вот мне а как вас такой повтор когда вы запускали доставщик данных повязали именно скраб он ну довольно давно вы не поддерживается почему сразу не сыграет у нас используются по моему 2011 может быть даже 2010 года то есть у нас есть сеть край по которым он вас испарился ранчо да да у нас алиса мон стас коллектор поверх него плюс там разнообразные логе собираются ну просто данные передаются еще тоже другими командами у нас есть она что вы больше смотрите на блюм ирина комку вы знаете я пока просто смотрю в общем потому что я и не не проводил еще тестов и у нас есть план по которому мы будем тестировать у нас то есть есть критерии по которым мы будем выбирать возможно какой-нибудь следующей конференции если мы сделаем этот выбор я подготовлю доклад о том как мы выбирали что мы выбрали ладно спасибо добрый день меня зовут владимир спасибо за доклад скажите пожалуйста вот вы сказали что ходу страшный постоянные апдейты меняют все тем не менее вы его любите сколько времени занимают оплетаю восход с учетом всех этих невнятных непонятных изменений и насколько вы отстаете от последней версии из-за невозможности нормального объекта то есть насколько вообще реально его использовать в большом продакшене сколько времени занимают among many things ну после того как все настроено мой день и тела уже не надо то есть если в прощением если у них нет никаких поломок то он просто работает ну а переход на новую версию обрит и вот мы будем делать переход тогда после этого я смогу вам сказать наверное начале следующего года то есть вы живете на той же версии на которой начались упрощений да понял спасибо мне так вольер привет спасибо за так вот вопрос про activity сами события по идее ниже разнородны по структуре да то есть тоже не однородная структура строго однородно но сейчас у вас до а по идее например там клик по фотке да там ли лайк фотки он помимо tom dice не к фотке да этот пассив обжиг он же по идее можете держать например там айди альбома два пользователя например там и так далее то есть какой-то контексте дополнительный я прошу причине одну секундочку значит по поводу удар уже дальше анализом весь дальнейший анализ производит сам ну в нашей аналитической базе данных где есть данный а пользователя полностью там страна пол возраст мы не храним it off как в каждом сообщении те же самые данные про фотоальбомы ну фактически да мы не анализируем на данный момент по какой именно фотография щелкнул пользователь на как он поставил лайк вот но во первых то можно выдернуть из клик стрима но просто наши аналитики пока не просили этого делать вот а так для других всех событий мы собираем дам данные в другом формате это бывает реже то бывает реже поэтому ну в общем этом потоке активности мы не собираем такую информацию она вот сугубо однородно понял спасибо вопрос такой я про я понимаю что вы book 5 свои данные так несколько по-своему обычный backup просто потому что было бы у вас прецендент потому что вы его останавливали файлы из этого быка по прецедентов на боевых серверах не было но мы проводили тестирование а что будет если мы сломаем вот так то есть мы взяли отдельно кластер налили туда данных начали его ломать и все удалось по времени то есть по времени естественно что это будет очень долго надо бы знаете файлики надо будет это все делать но это просто возможно еще и я и понял что вы не используете ганглий у для собственно сбора вообще данных с вашего кластера то есть нет данных именно по вашему кластера то есть с ходу knot с data not например 100 doors ганглий на для мониторинга то есть очень такая полезная штука именно для хотела нет спасибо мы пользуемся стандартными средствами ходу по он выдает отчеты о статусе not мы запускаем fsck на нотах периодически и дальше просто парсим текстовый вывод ну все наверно знает меня просто такой может советую поставить в сторону ганглий очень полезная штука именно для сбора статистики в саму кластеру и для визуализации там америка ции всех данных именно по вашему кластеров еще еще раз как называется англия ганглии ганглия очень очень полезная штука спасибо спасибо спасибо добрый день спасибо за доклад есть вопрос я здесь есть вопрос по поводу тот кто опять же отказоустойчивости да вот клаудин рекомендует использовать не больше 30 процентов и занятости дисков на dat анодах как у с этим боретесь что делаете вот когда бы у вас вот такой crash test сколько как бы not вы убивали ну естественно 2 при факторы липли к репликации три тридцать процентов то есть это получается что ставим диск там на терабайт используем 300 гигабайт и я не слышу к сожалению не могли бы вернуть микрофон doc дали рекомендуют использовать не больше 30 процентов да то есть но занимаем в случае вот если делаем три копии да ну как бы блока если две умирают остается косыми на видимо одна вот как вы с этим варить нет я я не могу понять суть вопроса ну мы используем допустим нет 30 процентов жесткого диска не вижу я не понимаю почему была дана эта рекомендация потому что какие-то внутренние перераспределения что ли внутри жесткого диска но нет я не понимаю почему надо использовать 30 мая утяжелен тоже как бы не готов сказать такую рекомендацию именно тот колдер нет нет мы по крайней мере я такой рекомендации не в курсе и у нас нет такой цели использовать не более тридцати процентов можно добавить 30 диска процентов тут видимо речь идет о том что на самом деле входа эфесе нужно оставлять какое-то место но там не 30 процентов желательно там 10-15 но про 30 вот ну как бы 20 . хорошо на 15 принципе по практике в поле хватает вот а про то что использует 30 процентов диск такого как у них это очень накладно будет ну то есть на случай если сервер выходит из строя чтобы было место для репликации ну для чтобы он пребыл а не могу продолжить да там бардак при 20 процентах critical pre десяти процентах место на hdd fs для нормального распределения возможности дополнительный репликации при выпадении дата надо у меня вопрос от принципов разумно вполне требования вопрос мой у вас достаточно такая сложная кастомная штуку севска сохранением текстового вывода есть около удaры из yarn они взяли на им надо failover решение ну хай лола бились и когда двиной моды и там это верно версия 2 до этого мерсин во но они это сделали даунгрейд до по моему вот не помню 12 или есть но отсюда первой версии ходу по без ярно они тоже сделали просто это можно они бы лыжи лет назад в open source можно это взять используется значительным более проще чем я про таких разъемные решения уже достаточно прошила 192 giga откуда такая она одна но до вышла решение уже достаточно много вроде бы никакого ebay за и прочего impale не используете зачем только оперативки как было решено кто решал опять же я не в курсе ну не которого на некоторые просто к сожалению не могу ответить я не принимал участие а какой у су что ну то есть насколько они все утилизируются не знают не готовы у она пассивно мы планируем переход на более современной версии как я уже говорил причем буквально вот ближайшее время и там я думаю что будет память использована наполнились еще такое тоже дополнение принципе shark считается уже устаревшим сейчас с парковки верят создатели же спарка продвигается хорошо вообще на самом деле да я когда посмотрел на сайт с парка я не нашел упоминания sharky ну возможно человек который мне это говорил о наших планах он несколько информация несколько устарело о том что мы будем использовать будем использовать самые современные по возможности добрый день у меня два вопроса первый уже вскользь упомянули это если у нас скажем вопрос начинается если у нас на и многие выходят из строя вот то есть эта информация вся погибает как вот нам быть то есть ну это как бы слабое звено ходу по то есть если кассандра у нас врага восстанавливаем если у нас там три тысячи серверов скажу но очень много как это все решать вот эти вот секан дориным ноды они тоже как бы ну ну скажем у него by погибли или как там как вот с этим быть это первый вопрос второй вопрос такой больше как бы программист почему выбрали именно в качестве ключа дату за сегодняшний день то есть если бы скажем размазали этот ключ как-то по большему количеству серверов то есть и подати а ниже локализовались данные на каком-то сервере и мы все запросы за сегодня у нас они скорее ну там на 1 или на реплики пойдут ну все-таки не по кластеру размажутся как бы нагрузка я понял спасибо моей закрыл уже время заканчивается значит за один день получается гораздо больше файлов чем у нас not то есть один файл минут за десять пишется соответственно количество файлов можете посчитать и на одной ноги будут обработаны больше одного файла на каждой ноги то есть данные не пойдут на один сервер за 1 день ну если мы возьмем сколько получается несколько гигабайт и у нас файлики там пор 70 мегабайт в сжатом виде то есть сами понимаете что данные будут обрабатываться на нескольких серверах по поводу отказоустойчивости дело в том что hadoop кластер на пять машин hadoop кластер на 50 на 100 на 600 там на 1000 на 3000 это разные подходы это все по разному поэтому а трех тысячах мы пока не мечтаем у нас пока не столько денежков добрый день когда вы говорите что выкачиваете данные есть ходу политической баз данных для анализа вы не отмотали варианта гоняли точки допросе потрясно по классу выхода по и допустить и поставить весь парк ходу будет работать сто раз быстрее тогда я понял значит у нас для отображения данных применяется такая система как майк растраты g она позволяет визуально создавать отчеты и к этом отчет она генерирует и сквер эскель запускается им побежал ходу в том виде в котором есть сейчас он дает ответ на сложный вопрос но медленно он может дать ответ на простой вопрос ну в принципе тоже не быстро поэтому использовать его в связке microsd и также на данный момент не получится с аспаркам возможно да возможно мы перейдем к использованию ходу по напрямую но пока у нас его нет мы не можем даже потестировать когда потестируем если это будет возможно мы конечно же перейдем потому что это полнитель ный шаг использование место в этой базе которой лицензируется по месту и так далее да мы откажемся от такого перекладывания лишнего ещё вопросы есть еще пару минут есть вопрос в приложении в нашем от хай-лоу д но возможно немного невнимательный вопрос такой сколько серверов у вас сквозь тире и какова их примерная конфигурация и вторая часть вопроса как часто случаются down тайну вас hadoop кластер еще ни разу не было дантэ не ну а вопроса конфигурация вот он ответ на вопрос еще из опроса спасибо за доклад спасибо за вопросе вижу-вижу есть да да да вопрос дайте микрофон добрый день спасибо за доклад вот по моим подсчетам у вас место закончится где-то через 60 дней как мы удалим к extreme мы удалим старые тексты удалять extreme достаю дома и activity вы храните надо найти виде мы храним пожизненно то есть у нас просили два года но мы будем и хранить пожизненно думаю что нам запрос от увеличения место в кластере мы будем удалять quick stream а сброса скажите пожалуйста а вы используете hadoop потому что один из ваших коллег имел опыт работы с неправильно да другие колоночки ориентированные базы вообще рассматривались как то наверняка даже hadoop не колоночка ориентированная база ну для ахахаха сейчас я я поясню я понял у нас используется экзо solutions распределенная колон ночная база с вертикаль у нас дружба не сложилось а по каким причинам давайте я отвечу на вопросы о ренегат я не готов просто хорошо и не знаю официальную политику партии по этому поводу данные агрегируются рассудке a real-time данных речи не идет на данный момент мы только маленькую часть данных представляем не real-time но около того где то задержка в час почему мы планируем сейчас изменить структуру за загрузки данных таким образом чтобы предоставлять все данные по возможности там небольшой задержкой то есть мы оперативно просто дорабатываем нашу систему и вот приходим к тому что мы хотим прийти к этому"
}