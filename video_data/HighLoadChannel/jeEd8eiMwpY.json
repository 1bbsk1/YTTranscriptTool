{
  "video_id": "jeEd8eiMwpY",
  "channel": "HighLoadChannel",
  "title": "Тернии контейнеризованных приложений и микросервисов / Иван Круглов (Booking.com)",
  "views": 24021,
  "duration": 3179,
  "published": "2018-11-19T02:42:29-08:00",
  "text": "я ваня есть компании о ком там последнее время занимаюсь разработки нашей платформы про компанию вы наверное все знаете поэтому такого ничего интересного вам рассказывать про не не буду расскажу только покажу вам какие-то такие интересные факты интересные цифры booking.com есть такой один главный сайт называется вывода буквы booking.com это вы все его знаете все бронирую там гостиницы потом когда вы выбрали свою гостиницу есть такая вторая часть которая менее известно тем не менее каждый из них не взаимодействует и так он называется secure . booking.com это там где выводе свои персональные данные там в эти где вбивайте данные своей кредитки и собственно нажимаете на кнопку забронировать есть еще один такой вот сайт называется он админ . booking.com это если у вас есть там не знаю комната вела замок что угодно и вы решили его за заместить на сайте вот заходите туда этого вбивайте свои данные давайте что называется вела билете и так далее есть еще один такой большой компонент называется но офис . букин . мы такая внутренняя штука доступна только сотрудникам если у вас есть у вас если вы когда-либо будете работать в букинге мы постоянно харим вот вы познакомитесь что это такое для чего это все показывают показатель для того чтобы рассказать вам то что вот это все вещи они поддерживаются я поддерживаются обслуживают разрабатывается папа порядка полутора тысяч человек на данный момент и все это запущена на порядка 30 тысячах серверов а все эти сервера мы обслуживаем сами это железное сервера они расположены там в нескольких на центр которые распределены по миру и вся эта вещь все эти все эти 30 тысяч серверов на них работают следующие вещи в первую очередь у нас есть такая штука которая называется сервер д.б. это по сути утилита которая managed весь этот флот серверов это базе данных ничего по большому счету не имеет никакого отношения но тут некоторые то какая то за который управляется все эти сервера они управляются сконфигурированы управляется по пятам на них запущен код бизнес-кот написаны на перлина яви все данные этот код хранить на мускуле который оркестре руется такую штуку которая называется orchestrator этого концерта разработка в которой мы тоже вкладываемся это такая вещь который умеет там магическим образом переключать делать феллоу мастеров из лаймов есть какой-то сторож а если у нас свой сервис discovery что интересно он условно звуки перед что интересно вся система наша букинге система сер discovery выросла из баз данных из из из из-за зоны база данных то есть нам нужно было решить проблему как находить пул слоев на и вот откуда это все появилось есть развита система тета стриминга потому что booking сильно основываться на данных он на данный момент нам в переходном состоянии частично основан на кафки частично нашем внутреннем решение поверх этого есть какой-то достаточно сложный да да да это гергей шин на который позволяет нам там агрегировать данные там почасово поминутно посекундно то мы вплоть до меня месяцев и годов есть у нас практически полностью с нуля переписанный graphite овский встык то есть изначально graphite написано питанием его практически полностью переписали на гол а для того чтобы поднять производительность и это все дело сверху прикрыта внутренняя разработка для полетов да есть такая очень интересная вещь система которая позволяет нам на основании некоторых метрик делать prediction и и на основании prediction of какие то делать оповещение зачем это вам все будем показывают я все это вам показываю объяснить для того чтобы объяснить что в букинге будто существует нас ждет седьмого года есть какая-то устоявшийся система как это можно назвать платформа до которая работает которая приносит компании вилью из которому собственно все там сидим работы мы разрабатываем но в один момент к нам пришел бизнес и сказал что чуваки а этого мало нам этого мало нам нужно нам нужно больше мы хотим быстрее выходить на рынок мы хотим быстрее наши идеи внедрять имплементировать ну и в итоге вы выходить на рынок вот и собственно говоря весь мой и и вот весь мой доклад будет проходить в этом контексте на то есть нам нам перед нами будет бизнес поставил задачу то что нам нужно построить какую-то платформу для того чтобы быстрее быстрее делать операции и одно из этих 1 компонентов этого этой задачи было переход на серию сомнительную архитектуру в том числе номерка сервисы но мой доклад будет про то как мы строили нашу платформу вот и собственно учитывать то что у нас все уже было готово я хочу рассказать вам какая там была проблема проблема была в следующем вот эти вот все четыре квадрате который он показал они у нас называется серию роли на сервер это по сути там группа машины несколько сотен машин которые которые на которых зовут запущен код если он нужен какой-то вывести там новый продукт или какой-то но в новый сервис круто новую фичу нужно сделать новую то что называем сервер роль для того чтобы сделать нужно написать какой там папе 3 потом с помощью серию тебе как а там за за продвижение сервера я ни в коем случае не забыть про то что нам нужен встречен сервера нам уже какие такие винты на которых делается разработка но нужно открыть доступ в базе данных для новой роли каким-то образом проковырять дырки фаерволов нужно сконфигурировать сервис discovery сделать так чтобы этот сайт был доступен извне и в конце концов просто взять и скопировать deployment как бы все есть у работает а в чем проблема кроме болт в том что это все все эти операции выполнялись вручную и более того все эти операции выполнялись такой достаточно узкой группы людей которые которые обладали этим магическим знанием как создать новую серию роль и понятно то что а вся эта процедура была достаточно продолжительная и занимала нам по факту тогда и занимательно сейчас дни часто в плотном до недели и недели а что нам хотелось но хотелось чтобы это все дело занимала минуты и вот именно вот этот переход от того чтобы создать некоторую площадку на котором можно было развернуть запустить код предоставить доступ к базе данных центрирует лот банкир это-то простому будет мой доклад и для того чтобы это сделать мы начали строить некоторые платформу платформа за сервис может назвать его облака сразу хочу сказать думает уже понятно из за доклада из название доклада то что путь туда у нас был не всегда красивыми облаками он часто был были у нас и шторма и молнии и не всегда у нас было все хорошо на этом пути мы прошли через три основных этапа на нас было три эти рации нашей платформе и сама первая операция была основана на марафоне вторая операция была основана на обшивке и последняя операция была основана на кубер найтись и вот про них я вам и буду рассказывать тут такой интересный момент с мой доклад называется тернии к карте рисованных приложений и микро сервисов на самом деле самом деле то прошли вам буду рассказывать это про то как мы пропили 2 с половиной наших платформ и вот почему 2 с половиной потому что первые две мы профи капель это точно я объясню почему последний купер найтись мы запустили буквально летом вроде бы как все работает вот но как бы до конца уверенности пока еще нет то есть вполне есть такой шанс что на следующем хайло дениза когда он там будет тонна в питере либо в сибири либо в москве через год я буду вам рассказывать про то как мы прав и копили три платформы вот и спасибо такой момент то что я в проекте не самого начала я пришел в проект где-то в середине его пути там в один из самых тяжелых у периодов это что буду рассказывать такой как бы мой взгляд мой взгляд человека который пришел в проект в середине видел его середине окей поехали значит марафон это такая штука регистратор контейнеров на начал с он как хакатон там середине 2016 года еще 100 до того как нам пришел бизнес с этим вот фасады market месседжем вот начался котону прикол в том что он до сих пор как бы в том состоянии и остается до сих пор не закончился по сути ты был такой набор там скриптов питон баш это все склеена изолентой перекручена но тем не менее она работала у нас была там какое-то количество жареных кластеров дано который был там запущенной какой-то там какое-то количество проектов все хорошо работала проработал это буквально там у нас там полгода после этого мы переключились на мы поняли то что нам не них не хватает сич конкретно нужен был там разделение прав доступа из-за того что в марафоне рандомные партии там очень сложно было очень сложно было проговаривать фривольные дырки поэтому мы от него решили отказаться но здесь хочется такую как бы не то чтобы умную мысль но такое знаете мое жизненное наблюдение вы сказать то что ну вот я всегда удивляюсь способностью кода выживать для меня это такой как бы такой организм который постоянно борется за свое существование то там написал какой код за диплом думаешь ну вот блин сейчас один раз написал никто не увидит нет обязательно кто не скопирует для trainz копирует куда-нибудь там в десятках мест и и потом ты будешь с этим бороться еще там до конца жизни такое наблюдение да и этот эта система марафону мы наконец-то я надеюсь мы сможем убить мы сможем не убить вот до конца этого года окей ну а нас марафона все было как бы кратко и понятно давайте перейдем на у польши в топ от шифта такая вещь от радха то это такой как бы кубер найти с большим количеством настроек и тут у нас случилась такая случилось нас и фария у нас все были счастливы по случае перехода на опыт шеф почему потому что и все это случилось начали то 14-го года почему но во-первых у польши в shows купер не tissot купер найтись у нас разработал google это же круто это просто офигенно второе у польши вт идет с большим количеством это как бы коробочный продукт и enterprise решения он идет там каким-то количеством скриптов рис и которые написаны на нан себе который позволяет вам там развернуть класть и разогрейте классе задал гредин кластеры ну и так далее вот ведь который нужен металлистом все это есть и для наших админов которые там всю жизнь гелий ковыряли попить это было просто какая-то там манна небесная круто анти болт и последняя вещь который называется планшет это такой виртуальный раутер который позволяет вам на логическом уровне разбить вашу физическую сетка там на несколько частей каким-то образом там настроить правила доступа эта вся вещь на котором мы смотрели нас это было просто супер кукол фичино которую мы смотрели в розовых очках потому что мы думали что нам сейчас возьмёт и порешать все наши проблемы сможет на там и заизолировать на нашу сетку нас может там динамические открывать и закрывать на фривольные формальные дырки да и вообще она может решить на месте не все проблемы с комплексом но большинство из них вот как объяснить нашу сеть ситуации тогда вот есть меня такая картинка я очень счастлив что я нашел потому что мне кажется она очень точно отражает ментальное состояние команды в тот момент это вот это вот как вам рассказать это вот команда которая получила очень клевые инструменты и и очень хотелось вот прямо чесались руки что-нибудь с ними сделать использовать эти фичи по максимуму возможности но потому что блин кубер не tissue on seba up in which круто вот и но что мы не понимали не понимали мы в тот момент такую одну проблему потому что по сути мы запустили такую бомбу с часовым механизмом потому что а кейт все развернули и настроили она даже все работало там первые первые там сколько-то месяцев до но потом у нас учили шторма начались штурманов почему потому что потому что мы плохо понимали как все работает почему потому что кубер найти сам по себе это сложная система инси балки может быть не настолько сложного тем не менее это абсолютно новое новые как бы сфера для букинга да и up in which кто работал со 100 switch мало один человек вижу как у вас ним опыт что in english короче гра у нас супн-8 у нас с мечом у нас не срослось мягко говоря да почему потому что ну скажем так опытом настраивать такие сложные политики маршрутизации вот это все как бы ну там есть как документации в которую да там можно разобраться но как бы мы не понимали что происходит внутри да и в итоге когда у нас в определенным моментом по определенной нагрузке начали тупо пропадать все правила даю нас просто встала колом все ну как бы разобраться было очень сложно очень очень сложно в убираете есть такая вещь который называется сервисе который там формирует и в том числе и об инверсии они там все вместе оперирует опять ленты был сам в итоге там на каждой ноги у нас появлялась там посмотрим правилам по сотням правилу на каждой ноги с этим все было тоже очень сложно разобраться тут тупо там например от tracid от того как запрос от одного подойдет на запрос другому поводу были нас таки прикольный случай самим кудри найти сумму же когда он там на каждой ноги тупо все контейнеры все коды все конторе и начинали раз стартовать ему плохо понимали почему это происходит вот и была такая сегодня мы прикольный случай когда у нас одна но до обе объявила себя супергероем она короче гра решила что оно самое главное и я нас может вынести все поэтому на взяла за скетчу лего на себя 356 кодов и к сведению в среднем у нас пост состоит из 5 контейнеров на давайте посчитаем то есть получается то что одна но до попыталась asked на себя полторы тысячи контейнеров на ну как вы понимаете после этого у нас все опять прилягу вот какая из этого какой мы себя вынесли урок даты наши проблемы на мой взгляд были вот из за того что вот этот пул versus боренька куклева боренька грустно они не интересно она как бы вот уровень крутости до в проекте он просто зашкаливал зашкаливал до того уровня что же мы просто просто вот эти три крупные сложные систем мы вместе еще и вместе на они они они сформировали такое поведение которая нам было очень сложно объяснить на мой взгляд нам нужно было сделать как-то ну вот ближе вот вот к такому да то есть как бы но новые технологии круто все стоит делать да но как бы есть вот тот как бы стек до которые как бы ну да он может быть не настолько круто не настолько интересным он просто он понятен и он работает это как бы мысль номер два вот и такое тоже интересный момент как вы думаете у нас вот учитывает все то что я сказал до этого как вы думаете что у нас было с техподдержкой обслуживанием этого всего кластера вы угадали ии не было у нас оскар так у нас были какие-то alert и и мониторы да но нашим самым главным монитором угадайте кто был все правильно вы знаете это то о чем я говорю к нам приходили пользователи говорим чуваки я вас здесь работает его там у вас не работает и вот тут-то вас гады тоже не работает у нас не благ вот в наших кластерах почему им почему потому что ну вот мы сконфигурировали да и были нас таки прикольно случай когда люди не специально по ошибке был на случай когда нас один человек опечатался один нашу пользователя печатал своем по моему вместо того чтобы набить не помню сколько на карраш в итоге он у нас доске jewel по моему несколько сотен или несколько тысяч а контейнеров просто отпечатавшийся в в нашем кластере естественно просто выживу все доступные ресурсы и опять же все прерву и с capacity planning мы тоже всего достаточно грустно вот знаете мне к такое ощущение что вот вот это вот все утверждение верно знаю да мне казалось что ну то есть мне кажется что что мы мы тогда надеялись то что нас есть облака которые магическим образом решить все наши проблемы да но к сожалению но вот это вот вот это вот оно все по-прежнему правда да и более того вот тот компьютер на котором все это было запущено это был наш компьютер и более того за за поддержкой то компьютер нам еще платили деньги проблем когда вот и тут я себя чувство в таком не очень неудобно положение поэтому потому что вы наверняка думаете вань вы чё вообще там делали вы вам за чьи-то вообще платили деньги да вы какой-то хернёй страдали вот тут у меня есть как бы два на два таких момента все что я сказал предыдущий это все правда и это все было частью проблемы но на самом деле была было 2 еще более крупные проблемы и сейчас я в моем хочу про них рассказать проблема номер один ее можно изобразить вот такой вот большой большой красной кнопкой с названием диплом я вам хочу попытаться рассказать что я здесь имею ввиду когда мы строили нашу платформу что мы хотели сделать мы хотели сделать для наших пользователей из пользователей нашей девелоперы внутри компании до простой и удобный интерфейс вот очень часто вадима нисона рассказывал про то как это все работает в обитаемых мы делали тоже самое вот очень похожи да если вы работали с heroku вы знаете то что там насколько там все просто там одной двумя командами все запускается конфигурируется развертывается инсталлируется все круто мы потому мы хотели сделать примерно так же у нас получилось у нас даже получилось это даже работала какое-то время там 1 пару месяцев до нас был и наш юзер было отличный опыт первые разы но проблема была в том что вот наших юзеров мы научили нажимать кнопку мы показали одну кнопку именно мучились вот чтобы за деплоить нажми на вот эту кнопку и все еще происходит внутри маятник скрыли мы решили построить абстракцию наш юзеры не знали то что не взаимодействует с кластером open shift то что там на самом деле когда не нажимать на эту кнопку у них там формируется может формироваться новый проект он там билде c пушиться image потом это все дело deploy савка вернитесь там создаются под и создают deployment и создаются сервисы конфигурируется a daunting подключается к база данных регистрируется до пенсии и так далее на то есть такой как бы целая цепочка и проблема здесь в том что когда они там 1 разу не все работало они нажимали на кнопку всю работу ног потом когда они нажимали эту кнопку них чет там ломалась они понятия не имели что происходит и поэтому они приходили к нам и вот ну представителям команду там 20 человек пытается обеспечить поддержку там полутора тысячам человек в компании не вся полтора точно какой-то значительной части нас просто-напросто утопили в количестве вопросов количестве той поддержке которым нужно было обеспечивать соответственно у нас время которые мы могли бы инвестировать в развитие инфраструктуры с ли снизилась практически к нулю это первое второе такой интересный момент то что построив эту абстракцию мы абстракции получилось смотря сейчас назад не не очень хороший почему потому что она как бы работала в стандартной конфигурациях но у нас был какое-то количество достаточно немало количество опытных пользователей которые очень быстро эта абстракции перерастали да и они опять же приходили к нам потому что периодически нужно было сделать что-то что мы не могли нам постоянно приходилось распиливать нашего абстракцию добавляется новый новые фичи вот какая здесь такая мысль да у меня есть в чем была наша фундаментально проблем нашему фундаментально была проблема в том что мы не выстреле наши взаимоотношения с пользователем то есть мы не только не определили что пользователям стоит ожидать от нас как от провайдеров этой инфраструктуры но также мы сами для себя не определились что мы ожидаем от пользователей а то что наверное стоит сделать так чтобы наши пользователи имели какие-то хотя бы базовые знания о том что мы о том в какую платформу они тепло и c это была проблема номер один и проблема номер два это ведет под этим словом интеграции что здесь я имею ввиду смотрите у польши цик uber нить из это такая самодостаточная система в нем есть наверное все на в нем есть там свой оркестр свойски джоли рудникам есть свой dns есть 8 discovery есть свои секреты есть если нет ни не все то много что но проблема в том что когда вы начинаете дату подшивки кубер нити с деплоить в компании в которой уже есть развитая инфраструктура букинге это именно такая компания люди пытаются диплом внутрь туда приложение эти приложения хотят работать со старой системой соответственно вам нужно сделать так чтобы ваша система умело сконфигурировать штука который мы называем дистрибьютор банкир внутри букинга можно сделать так чтобы эти приложения могли интегрироваться с той старой системой сервис discovery есть сервис меж которая ну скажем так она старая условно напросто как бы но на современный но она как бы не не интегрирована с осквернитесь может был интегрироваться там есть такое у нас pipeline и ливенцев нужно каким-то образом там слать это graphite овский метрики прошу букинге там есть сеть relative даетесь или эти рилай нужно было сделать доступный внутри кодов нужно был каким-то образом тигрицы со сторожем там всевозможные инстанции с мускульным кассандр редисом кафкой со всем этим нужно было делать интеграцию это занимала огромное количество времени такой интересный момент вот букинге как каждый раз когда поднимается все сервер паппет для него конфигурирует с сертификаты 13 текке а вот когда стартует контейнер как это сделать нам пришлось решать вплоть до таких проблем потому что как бы фундаментальная перестройка того как работает такие глубинные процессы в компании вот 13 ти менеджмент для серверов да вот как вот когда стартует под как вы его будете делать такой вот как бы самый самый такой наглядный пример насколько как бы у нас проходил сложный процесс интеграции с суб лишь в том миска верните сам этот проблема секретами вот смотрите в повернитесь есть секреты как бы клево вроде бы все работает на тот момент была проблема в том что и она кстати до сих пор остается в том что кубе в секреты в кабинете не шифруется по дефолту на и и цитируйте нам сказал чуваки новый да потому что ну как бы вот пароль от той самой базы данных корнев которой хранится все транзакции букинга ну вот мы не разрешим вам хранить не в зашифрованном виде поддержка шифрования появилась в версии 1 7 на тот момент это было версии 13 14 она до сих пор остается версии alpha ее для того чтобы ее за активирует нам нужно когда вы стартуете 5 сервер там запустить его эксперимент all чего-то там они хотели зарелизить ее в джей в 19 до сих пор то они не сделали по сейчас полно сколько понимаем планы сдвинулись на 113 но вот баланс такой проблемы как мы решали до нас компания был волк хорошие хранилище для секретов нужно было с ним интегрироваться для этого в open source и мы нашли такую штуку которая завоевала контроллер на тот момент там полтора года назад да это было на там была на уровне пруфов concept поэтому нам пришлось его форк нуца и дописывать его в срочном режиме до состояния production reddit я хочу спросить вы вкратце рассказать как этот вот контроллер работа для что вы понимали как бы нас насколько сложнее стал процесс а вот у нас есть под для того чтобы внутри кода получить секрет в нем есть такой и нет контейнер и нет контейнера он идёт с руку которая называется волк контроллер которая идет в копирайте себя для того чтобы проверить что действительно ли такой поту на меня в кластер старта ну он проверяет допустим это ответ положительный дальше вот контроллер идет волк он вытягивает такую штуку называется wraps talkin' обернутый talking который на обратно возвращает вынет контейнер инь-ян контейнер этот токен разворачивает для этого он идет волк развёрнут только но сохраняет на диск после и только после этого стартует основной контейнер который берет этот токен и идет волк для того чтобы вытащить сам пароль вот workflow который как бы секундный сложный носик юный вот нам пришлось этим всем заниматься отсюда следует мысль номер четыре вы вернетесь это клево но вот его интеграции в существующую систему на мой взгляд она гораздо важнее чем наличие сама упираетесь это мы познали на таком очень пройдя этот путь этот к такой был там звездочка она она она здесь для того чтобы вот если вы начинаете с нуля и вас все отвыкла отметив с момента создания компании то во все будет здорово в квадрате и все а вот если вы уже пытаетесь что-то сделать в существующую экосистему вот тогда вам стоит про это подумать короче круто где все наши проблемы привели нас к состоянии то что она во-первых мы были утоплены в поддержке по самые по самое не балуй с юзерами которыми у нас ожидали магию которую в которой мы не понимали инфраструктуру у нас была бита с тех пор с техобслуживанием всего этого и полным бык логом того что нам нужно сделать потому что ну вот нам нужно все делать интеграции распиливать наши абстракции и так далее все это была такая печальная ситуация до когда мы что привело к полной практически полной потере доверия со стороны наши пользователи да это было скажем так больно не только для самок команды но для компании нам пришлось выйти на большую сцену три компании сказать что node40 чуваки у нас тут у нас блок все работает поэтому нам нужно какое-то время мы взяли паузу вот но у нас был план у нас был план план состоял из таких нескольких ключевых идей основная ключевая идея номер один это минимум клёвых штук вот из тех трех вещей с которым шел польши втк обернитесь анти был его по новой свечи мы отказались от двух мы оставили только тот минимум который нам действительно нужен от всего как от крутых вещей мы просто взяли и от них отказали в поле более простых и понятных решений каких я расскажу позже мы существенно решили вложиться в мониторинг инфраструктуры и основной здесь цели было то что не пользователи сообщали нам что у нас проблем а мы мы как провайдеры платформы сообщали вот люди сейчас у нас проблемы про понятна проблема всегда у нас будут наносное то что это мы сообщаем вам то что у нас проблемы и там в течение часа это проблемы решатся такой-то список кластеров которые за секции такой-то список приложений которые замечены такой the timeline мы выстроили очень четкие ожидания нам нужно было выстроить очень четкие ожидании между в первую очередь нами самими и 2 x во вторую очередь пользователями на то есть мы сформировали такие два основных постулата о том что если вы хотите использовать нашу платформу вы в первую очередь должны знать о купер найти сильно должны иметь базовые представления о том что есть и второе это вы должны обеспечивать поддержку от того что вы за тепло или да то есть четкое разделение то что у нас есть какой-то стать приложение мы поддержим инфраструктуру вы сами поддерживайте все что связано с бизнес логикой и еще один момент а к интересны то что нам нужно было мы поняли то что нам нужно больше выстроить взаимодействия с наш имеет провайдерами интеграции да тут но потом стой стой как команды который поддержит bouncer стой команды который поддерживает waltz той команды который поддерживает сторож а то чтобы пользователи которые раньше то есть они что-то связано с базы данных ломалась или там со стороны чтобы они не к нам шли а мы уже перед рисовали вопроса чтобы они взаимодействовали напрямую таким образом мы рассчитали то что нас существенно сократится количество вот этого взаимодействия количество вот этого вопросов поддержки которые которое мы испытывали на тот момент и конечно нужно было серьезно инвестировать в здании знание и и как со стороны инфраструктурой так со стороны нашей пользователя том что такое кубер найтись как все работа подножка нравится целая эко-система на в которой нужно хорошо разбираться вот мы планировали выстроить тренинги там создатели закупить онлайн-курсы закупить книжки и и вложиться в документацию вот этими всеми вот с этими со всеми идеями мы перешли к третьему шагу 3 нашей операции которые слова снова вернитесь и почему на купер найтись потому что в польше все red hat у него политика такого отставания от версии то есть мы хотели чтобы у нас была последней версии ножка убирайтесь этом если посмотреть в 0 пешки там половина его пешки в не альфа-бета нам хотелось как бы чтобы ну вот альфа-бета ее к ним поставим уходит фиксы на мы хотели чтобы все все фиксы были у нас и последний сколько там не осталось минут доклада 15 минут доклада да я хочу вам рассказать о том как мы построили нашу новую инфраструктуру учитывают весь наш предыдущий опыт положительные но и негативный и все это началось примерно год назад начале 2018 года окей с архитектурой если вы почитайте документацию кубер найти самку вернитесь рекомендует вам создать один большой кластер ну или там короче относительно большие кластера который будет покрывать там несколько дата-центров мы решили так не делать мы решили так не делать в этом мы решили сделать несколько относительно небольших кластеров там по несколько сотен нос а почему потому что но даже если в самом купер найтись у вас нас есть какие то там механизмы которые позволяют нам выживать на в случае отказа целого дата центра до то мы просто хотели защититься от самих себя да то есть если вы по какой-то причине тупо сконфигурируем или внесем изменения в конфигурации кластера которые у нас положит весь кластер на то мы по крайней мере не положим а все на то есть у нас приложение за dipline это в несколько кластеров одновременно да то есть мы по сути положим как бы одну реплику чего у нас было внутри кластера на здесь опять же у нас такая немножко не стандартная конфигурация опять же основа я основываясь на основываясь на нашем опыте а то есть у нас кластер состоит ну такой косички компоненты ноты которые называются мастерами на них запущенные все стандартные компоненты купер нити сайте сидибе а серверской делю контрольно менеджер кур dns и нас есть там вот для того чтобы сделать эти все интеграции нам пришлось написать какой то такое там порядка 5 или 10 контроллеров которые обеспечивали нам интеграции со всеми нашими этими внешними системами это все запустилось запуска вас там она она запускалась в печи и режиме то есть практически практически все эти компоненты there'sa лидеры election в виде сиди работали работать в парах 0 даже там тройках понятно есть ноты до на которых собственно запускается сам workload на котором запущен docker укрепляет все драйвер и там другие вспомогательные компоненты здесь есть такой интересный момент то что если почитайте документацию вернетесь и говорит вам скажет если у вас есть сервер запущен в режиме вам нужен вот bouncing а что конкретно advancing он умалчивает конкретно лондонцы у тебя мы используем янв ай да который который проектирует взаимодействие между кубой там и теперь 550 сервером прокси румына уровне т.п. почему потому что для авторизации мы используем сертификаты и вот это вот tls мы чуть ls между гибли там и и по серверам на lcm уровне не не срастается только на уровне l3 такой как бы самый спорный для нас для нас или для вас наверно все скорее моменты и если вы знакомы с архитектурой кубер найти цитата что мы создали такие отдельные ноты которые мы назвали gateway и на них мы вынесли кипр обсе по умолчанию почитать в коммутации и прокси допускается на тех же самых новых на который запускается гибрид почему мы это сделали потому что основывать на нашем опыте как бы с опыт шифтом и вот с тем количеством и пить и брумс который сформировался у нас нам сильно и и которые просто ломали наших взаимодействий между нодами нам сильно хотелось то чтобы вот это вот все эти 5 босс они были где-то в стороне от самих нот на то что когда мы заходим на надуем и когда у нас но дам 10 между собой это было чистое взаимодействие без какого-либо там количество без огромного количества правил в и 5 balls это как бы такой спорный момент но у нас есть одно решение которое его существенным образом компенсирует об этом я расскажу буквально через одну минуту до и последний компонент внутри нашего кластер и это то что мы называем мониторы и та-та-та-та площадка на которой запущен нас прометей который собирает конфигурацию который скрипит все все наши ноты то есть все что можно внутри кластером скрипит почему вы несли его наружу потому что там сильно не хотелось чтобы у нас мониторинг умер он умер вместе кластером а то сиськи ты проблема внутри кластера мы очень сильно хотели чтобы у нас мониторинг умер последним да и эти все мумонитор это все эти два прометея которые были запущены они все работали в активном режиме . актив актив на них были в конце игры на них сконфигурированные alert и которые о которые идут за наш внутренний штукатура называется alertpay и которая у них и у нее есть прикольное свойства что она умеет где дуплицировать да то есть если даже достучаться до 22 промедления оба скрипят если чается проблем они обнаруживают то что у нас есть какой-то какой-то alert высылает его валер . но в итоге alert опять только высылает одну там в пейджер duty или jabber хочу вкратце рассказать вам про наш сетап сетки и почему потому что ну вот вот эту проблемы с open виде switch-ем это была наша основная проблема да и решение у нас и вас потому что мы мы разработали как бы мы нашли такой вариант конфигурации который мы назвали директор и роналду был под то есть целью slide в том чтобы сформировать плоскую самое главное понятную сетку на то есть мы хотели чтобы наши коды получали те же самые api адреса те же самые ip-адреса что и а наши обычные машины с этой целью каждой ноги мы выдавали там с 20-му сетку это там порядка 30 айпи адресов мы использовать такой плагин об этом можете почитать в суп тигра я не хочу здесь вдаваться в подробности а вот но повторюсь то что каждый под у нас получал и пятка каждый под нос получает айпи адрес ровно такой же который получает любой физический сервер и между ними взаимодействие идёт напрямую и соответственно на мы приводим к такой ситуации то что мы могли полностью отказаться от такой штуки в ковырять который называется энгр снг рассвета . убирайтесь класса обычно сформировано такая внутренняя сеть обычно 107 2 чего нибудь там дело чтобы зама действует внешнего мира вам нужен ingress на в нашем случае так как у нас все наши коды внутри кластера они имели те же не было прям прямой доступ к любому к любому api адреса в рамках сети букинга точно про я сволота помог взять и и по пинговать любой под ему всячески поощряем это использования эти айпи адреса подав мы напрямую экспортируем сервис discovery то есть все идет не через сервиса объект оберните со а через сердце discovery почти экспортируем в дерби почему потому что сирийская у нас построен на основании января которыми это динамический и конфигурируется да то есть проблем действует проблем в том что под и постоянно стартовать умирает нужно динамическим на ли конфигурацию adele бью нас построен на иначе прокси который до последней версии это не не умел сейчас он это умеет и вот мы и в скором времени ты перейдем туда этого вообще как бы сама идея того чтобы у вас вошло bouncing общался не через сервис объект купер нить из и потом через пот и а напрямую из ладно онтеро в воды она получает она получаете мне кажется это мое личное мнение она получит дальнейшее развитие например google буквально пару недель назад опубликовав статью в котором они делать ровно то же самое это такая интересная фича на мой взгляд какое-то такое мне кажется за этим будущее еще один такой интересный момент потому что нас наши мастера имеют плоскую структуру у нас получилось сделать следующее то есть ли в обернитесь и у вас есть сервисы для него в к россии есть войны на серве он формирует вам некоторое имя внутри кластера мы это все дело скрестили нашим букинга с нашим букинга мем денисов и нас получилось так что любой сервис который стартует у нас внутри кластера он получает уникальное имя в рамках всей букинга вс к сети да то есть вот вот этот кластер . local по сути меняем на название кластер это и en el prado в это в этом случае и booking.com это это фичи который нас очень любят наши разработчики потому что ну вот она она как бы доступно от идут и имя анна доступное отовсюду внутри кластер а вот ну да и соответственно если мы используем hitler сервиса о комплект опять взаимодействие идет не через сервис обеих поверните сам напрямую в кодах это то как мы пришли проблему сеткой да и вот проблему с мониторингом естественно мы построили prometheus там все скрепим много полетов но основная наша фишка здесь заключается в том что мы написали такой компонент который называется куб проб а тут как бы мы выбрали имя которое конфликтует с open source а то есть как бы это здесь не путайте кому-то не разработки надеюсь что в конечном итоге мы ее зовут enforcer видимо под другим именем но смысл в том что это как бы функциональное тестирование для купер найтись и такая кран жаба который стартует внутри кластера внутри всех наших кластеров там каждую минуту из и проверять там базовые функции кубер notice это в первую очередь там могу ли создать новый дипломе могу ли создать новый сервис там могу ли я там сходить в во внешнее сетку во внутреннюю сетку но также направляет все наши интеграции например там каждую там каждые 5 минут он проверяет а могу ли я подключиться к базе данных могу ли я за праве женить новый persistent волю и использовать его и если я сам сервисе из экспортирую его всерьез directory а могу ли я после этого использовать его на этот вот это вот это такой она наша самая ключевая фича который позволяет нам в компании первыми определять то что у нас есть проблема с нас было несколько случаев когда если даже вот на просторах же есть проблемы мы 1 узнаем о том что у нас есть мы как бы мы является клиентами это вас тораджи мы знаем 1 о том что в столь же есть проблемы за счет такого функционального тестирования скобы это вот такая хелен свечи для нас мне почти я почти закончил я буквально осталось два слайда и пять минут времени отлично хочется а как бы просуммировать рассказать вам про наше достижение построили мы нашим платформы путь туда у нас был такой очень долгий и тернистый взобрались мы этот пик что мы имеем на данный момент на данный момент у нас есть стабильная система которая работает вне за deploy на порядка 200 уникальных сервисов со стабильным ростом у нас появилось четкое понимание происходящего как с стороны инфраструктуры так со стороны пользователей они то есть мы их обучили что такое купер найтись как с ним взаимодействовать нас отлажена и взаимодействие между этими тремя компонентами команда пользователей и провайдер интеграции то чем я не упоминал ранее но хочется в карцер сейчас то есть все это система мы строили по принципу самообслуживания нас пользователь сами заходит и праве же нет namespace сами оттуда deployed сами конфигурирует лобан сердцами подключается к база да ну то есть полный клубы полный сервис и мы выставили отлаженной системы обучения тренинги онлайн-курсы книги документация из всего этого какие для себя я делал выводы . бы те такие мысли которые я сам бы хотел бы знать там год назад и я думаю команда тоже бы очень сильно хотел бы знать в тот момент когда вы это только все начиналось в первую очередь это управление ожиданиями это то что по сути нас похоронила на управление ожиданиями то ключ успеха то есть ожидание может быть высокий тогда вам нужно придется догонять или ожидания может быть низкими тогда вам по сути как бы сильно можно не париться но вам важно то что эти ожидания они всегда должны совпадать то что ожидает пользователь как бы высоко и как бы низко это ни было оно должно быть должен будет матч второе это то что я уже говорил хочется еще раз повторить это есть копировать и сам по себе хорош но мое личное мнение о том что его интеграции в компанию это такая сфера которая сейчас мне кажется многих как бы и упускает из виду все что вы деплоить и внутрь каким-то образом будет взаимодействие с нашей с вашей существующей инфраструктуры на опять же если вы не начинаете с нуля у вас вот только кубер найти съесть все только внутри тогда вас и клёво а если у вас есть что-то существующего пред с этим интегрироваться если это советы то что как бы к вернитесь но такая сложная система там очень много компонентов и наверно много раз видели и диаграмму это сложная система она требует понимания требует понимания и как со стороны пользователей так и как со страны инфраструктуры и так со стороны и пользователи это целый новый мир который нужно понимать ну и конечно если вы хотите сделать вложитесь в обучении обучайте ваших сотрудников это целый новый мир предпоследний слайд значит у меня есть митап и который завтра будет в час в калининграде там я как бы такое жизненная разговор о том как жить после инсталляции настройки купер найтись на хочу я начать его там с таким технических подробностей о том как мы это все дело полируем прикольный момент в том что например а у себя мы хитрым образом считаем с его силой вот не на основании аки response рейд и и a pirate и пять серверов это как бы на мой взгляд он не показываем реальный юзер экспириенс нас там весьма прикольный вещь которым хочет поделиться которым хочется поделиться приходите я думаю будет интересно поэтому меня все большое спасибо мне осталось лиланд спасибо большое спасибо друзья если есть вопросы поднимите ручки смотрите вот раз а тем временем я дарю тебе подарок от оргкомитета и грамотного благодарность за участие спасибо большое вы у нас как взрослых теперь вся включите нам свет взяли пожалуйста чтобы было видно вот там задача у молодого человека бы вопрос вот девушка просто прорывается с микрофоном сквозь толпы меняющих зал но это ничего две тысячи потоков все таки серьезная вот можно человека микрофон я первый взял так микрофон есть где вы махнуть рукой я здесь где а вот не вижу справа все три голдап но встаньте пожара красивым ужин должно быть видно да пожалуйста добрый день владимир козлов год сру ближе-ближе даже у меня в принципе обряд вопросов и по ходу вашего рассказа я меня возникли первые темы мы тоже проходили через миграцию краски контейнеров всего вот такого вы нас тоже стоял выбор там выбирать какой регистраторы и как управлять этой инфраструктурой и мне вот первое что мы наверное подумали это о том как команда разработки будет собственно подключаться к этому всему делал как и нас может работать вопрос не знаю насколько он риторический почему вы об этом не подумали мы подумали то есть мы мы знатно наше первое решение было мы не будем людей парить вот это все фигней мы построим им абстракцию дадим им большую красную кнопку под названием deploy и все будет работать делать свои хорошо сделать все хорошо но как бы может у вас это сработает у нас это не сработало с работала по-другому мы сдались на что готовы наши разработчики вот вот это вот это как раз вот тот момент которую мы пустили и и вида это скорее всего да это единственные вопросы не были давайте может если вас многом вопросов я всегда буду там спасибо пообщаемся здравствуйте спасибо за доклад где вы вот вот вот всё вижу у меня собственно такой опрос какой вид в конце выдали пользователям для тепло и своих сервисов кнопка или там вот ssh заходите на сервер руб смотрите то есть мы им дали им следующее мы им дали тузу который позволяет им забудь 105 их не сервис надо сделать какой-то скелет для летнего сервиса после этого мы им даем у них доступ к ним space у и они туда тепло и c то есть есть какой-то темплейт они куда-то и вода смысл в том что мы как бы людям даем приму по сути прямой доступ к вернитесь на но при этом при этом доступе есть как это такая вытоптанная тропинка по которой если вы будете следовать во все будет круто потому что там все как бы все при конфигурировании есть какой-то набор интим plate of если вы будете использовать все будет круто но при этом как бы если вы хотите уйти там сделать следующий шаг вправо вы опытный пользователь пк уберетесь пожалуйста вас вас namespace вашим нам спейси вы админ делайте что хотите все понял спасибо все еще была просто спасибо большое вот вопрос мордочка несколько вопросов зовут константин давайте один наверное быстрый хелм использовать сюда не да и вопрос по поводу когда вы даете кубер нить из как полную платформу для разработчик они могут использовать любой функционал как потом обновляться обновлять что кулер не this как вы узнаете что не засек тити кого-то кто вдруг стал использовать какую-то функциональность которые никто до этого не использовал но там имейте виду обратная совместимость ук убирайся есть обратно совместим скажем так мы уже обгадились 19 на 111 сейчас идет об говорит нади 12 пока проблем как бы не было не было а вопрос и по поводу как вы определяете что человек достиг каких-то базовых знаний в купер не this ну тут это конечно такой открытый вопрос да то есть мы как бы по большому счету даем kb на утку сами пользователям да здесь как бы что самое главное то что нас любые есть четкой как бы внутрь утки выставленные границы даже если человек начал использовать купер найтись не имеем базовых знаний но как бы мы в итоге беру чуваки чувак ну извини вот у нас как бы наш контракт так и бы ну вот ты вот сходи на тренинг то есть и тебя проблемы твой поясок мы поддерживать не будем и вот сходи на тренинг и потом ты к нам придешь и насквозь мы на сбой части спасибо вот у меня доклад по центру здравствуй меня зовут арсений компаниям банк x насколько я понял все микро сердце завернутый в докер правильно же я понимаю в кооператив мы по-другому не боишься ну мало ли собственно два вопроса подключали или какие-то системы сборки типы дженкинса и второй момент к тому вопрос когда вы настроили складные и питта и печники для драко драко дав да как вы их друг от друга защищали от нервных сервис то смотрите получается как бы отвечая на первый вопрос у меня был ответ я забыл вопрос системы сборки систему сборки socket лап связь и деленки кислоты соседи отвечай на 2 просто чтобы эти фото не доступны к бы внутри сети внутри букинга свете изменение естественного извне букинга не недоступны по большому счету там кабаном внутри к посетим мы не защищаем свой сервис там есть как бы отдельный огороженный такое забором зона который называется песня там где все что связано с комплексом да все что в ней нет и будут а простая плоская соседка а в перспективе у нас сервис мышей и мы чуть не делась между сервисами ответ спасибо спасибо будьте добры здравствуйте иван назаров азарт и club такой вопрос если вы первые узнаете о проблеме и даже прогнозируете через сколько на починиться почему у вас нет статус пейдж приходится узнавать надо undetected почему у нас у вас не ацетат успей статус плечи у нас есть у нас есть такая большая даже не нашел то есть у нас как бы у нас есть два варианта первый у нас есть большая даже пардо который отражает состояние все кластеров но очень более круто в том что короче говоря любой сервис который на союза за тепло и мы знаем его owner of да и в прометею нас configuring alert и которые ты короче если у нас в классе что-то сломалось прометея у нас знает какие приложения запущены внутри и он сам генерирует alert и для всех тех пользователей и скручиваем генерирует alert и для приложений если наши пользователи подписались на эти alert и они получат это об этом у и давление короче если на случается проблема в кластере наши пользователи я об этом узнаю я имел ввиду статус space наружу чтобы внешние пользователя вашим и пи например узнавали когда что я понял когда почему скажем так я думал мы к этому придем спасибо еще один вопрос разрезал можно да правда конечно у меня зовут илья компаниям биндекс по поводу функциональных тестов вы говорили что есть какая-то проба тест проба которая в которой заряженную какой-то набор функциональных тестов правильно понимаю это тест некоторые бизнес-логики с точки с точки зрения конечного пользователя нет это этот как бы этот тест бизнес-логики на нос со стороны функционала платформы вот вы говорили о том то что либо вы узнаете о проблеме о том что что кладочная пользователь стучаться потому что ни черта не работает либо ну то есть вот а про активности с точки зрения мониторинга вот если понимание что сейчас бизнес-логика начинает не работать потому что в контексте какого-то бизнес-процесса ну скажем так не понимаем есть бизнес функция не есть какие-то ключевые показатели вы сейчас смотрите например performance кипиа и просел или теперь вал ability просто не понимаете что цены за начинает отваливаться это как-то делается ну вот на есть вот реальный трафик я понимаю из реального трафика вы какие-то метрики снимайте к по которым можете спрогнозировать падение сервиса то есть первое на данный момент нет отвечай кратко но я думаю мы к этому придем то есть идея была в том что у нас есть как бы мы выставили четкое разделение ответственности мы отвечаем за инфраструктуру наши девелоперы и их не там сср и devops и они отвечают за приложения вот эти как бы бизнес метрики они относятся как бы вот-вот верхних категории до но я думаю как как провайдеры платформы и там через какое-то время мы придем к какому-то не то что там по дефолту если у вас там ребята вы использовать наши дефолтные тип темплейты да вы экспортируете метрики в стандартном варианте мы автоматически вам настраиваем в том числе и alert и но на момент этого нету да еще есть такой момент вот синтетич друзья простите меня пожалуйста дальше консультация будет платный уже я думаю да нужно ребята у нас пять минут технического перерыва до следующего доклада и за эти пять минут можно накинуться на его на он крепкий парень и он быстро кулуарно вам ответит поэтому спуститесь пожалуйста задайте все друзья пять минут потом немцы kavita и больше обнимемся а ты хотел красавчик"
}