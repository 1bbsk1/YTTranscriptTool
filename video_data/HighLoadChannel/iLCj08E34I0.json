{
  "video_id": "iLCj08E34I0",
  "channel": "HighLoadChannel",
  "title": "Тестируем производительность распределенных систем хранения данных / Александр Киров (Parallels)",
  "views": 115,
  "duration": 2741,
  "published": "2017-04-22T13:15:54-07:00",
  "text": "меня зовут граф александр а сегодня вам расскажу доклад про тестирование производительность распределенных систем для начала поднимите руки те кто используют какие-нибудь софтверные распределенные системы своих проектах хорошо пасибо кто использует железные решения типа сын нас коробки хорошо меньше таких тренд на яйцо а кто использует у себя рейд или директа touches to reach спасибо сегодня в принципе всем из вас будет интересно независимо от того используете вы распределенную систему хранения данных или нет потому что общие принципы тестирования о которых я расскажу они в принципе применимы ко многим системам хранения данных я в компании parallels работаю больше пяти лет я занимался тестированию производительности parallels plesk пену визирования серверами предательски сервера не реализации и производительности parallels club сторону сейчас я пробовала 40 о чем должен быть интерес я расскажу о том какие принципы необходимо применять при фиксировании системе на меня на первую очередь обращаешь внимание чтобы не наступать на грабли и не собираем подводные камни встречаются и тестирование система хранения надо также мы поделимся с вами утилитами и с критами для тестирования система хранения почему мы имеем экспертизу в строении поранился соответственно своей виртуализации автоматизации виртуализации хостинговых услуг мы кроме всего прочего разрабатываемые распределенные системы хранения данных прочувствовал сторож она меняет вокалистки вы распределенная система лидеры общей сторож для доения файл система автоматически балансируют нагрузку занимается устойчивость его поддерживает необходимый уровень избыточности в системе чтобы обеспечивать высокую доступность для да как это работает индивидуально виски меняются и единый кластер и каждый записанный в файловую систему ордер пво разделяется на куски определенного размера чанки в данном случае поют был поделен на три кусочка и система автоматически создает копии чанков и раскладывает пригороды сервера для того чтобы случае если произошло падение серверов даме были по-прежнему доступны система автоматически контролирую что некоторые чаки имеют меньшее количество рей причер необходимо для обеспечения отказоустойчивости и автоматически регулирует эти данные на другие себя вопрос корр тестирование систем распределенные системы немножечко сложнее чем они летающие те виды различных локальный диск в первую очередь нужно определиться с тем что хочется получить для конкретного проекта будет использоваться тайная система хранения да потому что вы это накладывает некоторые обязательства на доставления качество сервиса вот системы хранения данных лучше заранее определиться лучше численно какие именно параметры системы банк необходимо это могут быть качественное определение торговых у кадра вы хотите получить того количества операций в секунду или в томске и обязательно придеться раз такого количества постов вы хотите получить эту производительность потому что сценарий очень сильно отличается от того нагружает систему с одного единственного сервера и тогда паровая система должна обеспечивать произволению диспетчер производительности тут только один сами а случае если у вас к системе подачи материала то не так важно как система хранения будет хорошо обслуживать один единственные серы намного важнее как в целом будет обслуживать систем и именно сценарий дальнейшего использования сходы определяет то как ее нужно тестировать перекупили внедрение может быть еще третий сценарий который наиболее используются нашими клиентами тогда пользовательские приложения работают на тех же самых практически хвостах которые отдают сам самого пространства для хранения файл первый же которые обычно применяют при тестировании системы хранения данных это записать какие-то данных с помощью беды гильзе рава но мало кто об этом знает но в принципе после плат с и сразу становится понятно что система как ее собственное решение верное решение имеют специальные оптимизации на различные виды покерных нагрузки и вы можете с легкостью получить на ssd очень хорошую производительность на записи млеют дан новые дома начинаете зале свои случайные данные ваши производительные совершенно другое поэтому смысла тестировать копательные в данных нет улучшение такого теста может быть читать из рэндома но это делать хорошо потому что party создается именно во время нагрузки и может оказывать влияние на семью в то время когда идет нагрузка на саму систему лучше создавать попер заранее и создавать данные случайные чтобы не натыкаться на возможные проблемы производительности необходимо также выбирать большой workers and work in set это тот объем данных которая работает тест если он достаточно маленький вы можете легко попадать в каше и получать производительность в десятки раз больше чем фактически вы можете получить при увеличении объема нагрузки лучше выбирать такой объем нагрузки который применим к вашей системе и выбирали немаленький working on the частое использование очень популярных программ для нагрузки от типа и озон производится нагрузка в основном памяти потому что отец очень маленькие мы получаем совершенно нет у производительность которая кстати говоря очень важно как именно нагружать и диски как именно нагружать систему потому что если мы продолжаем всего лишь один диск то из рейда то он может давать какую то производительность примерно хозяин из диску и отсюда может последовать логическое заключение что если мы оставим это the red ни один диска данном случае 6 дисков то раз производительность будет шесть раз больше но на самом деле это не так если вы фактически ставите braid все диски и одновременно начнете грузить все диски то суммарный производительность у вас может совсем отличаться вот той которую вы ожидали поэтому если вы планируете действительно задевать использовать все диски брейди для системы необходимо тестировать именно тот сценарий когда все диски будут загружены на эти цифры это уровень и raid 0 и то есть каждый диск отдельно я немножко дальше подробнее расскажу про объединение диска а пока что любой замер при любом замерим не нужно пренебрегать статистикой нужно делать задержки сколько рация и лучше продолжительное время чем дольше тем лучше как вам позволяет маша ваша система тестирования насколько вы можете потратить времени чтобы получить те результаты которые дает система при долгой нагрузки также не стоит избегать статистика вычислить среднее между замерами и необходимо обязательно устраивать честное сравнение если вы тестируете разные системы что необходимо сравнивать яблоки с яблока нельзя странными системы основная фраза устойчивостью и система с неработающей на одинаковом железе не галина кого условиях условия должны быть одинаковая же из должен быть одинаковый уровень отказоустойчивости должен тени на кого то есть не стоит сравнить производительность не strike нас раньше стэн который один обеспечить отказоустойчивость а другой нет а большинство людей при анализе производительности ставишь что нужно мерить рэндом секвенсор что можно запись и чтение но довольно мало людей задумываться о том что нужно еще и мерить операцию как sing данная операция довольно важно для баз данных и виртуальных машин мы как разработчики стороны который оптимизирован для бар-коды виртуальных машин об этом очень хорошо знаем из-за чего это происходит почему эта операция так важно пользовательское приложение записывает данные говорит команды райт операционной системе грациозно систем отвечаешь туда на данный записал ему фактически данные еще пока не лежат на жестком диске они лежат в ранг и шеи в оперативной памяти операционной системе и для того чтобы их сбросить на диск нужно еще нужно либо какое-то время когда пройдет и операционной система сама и сбросит либо пользовательское приложение когда завершает транзакцию база данных завершает а загсу и требуется сбросить данные на диск на день необходимо позвать операцию сим которая о которой операционная система от правильно жалеют получение железа от молишь о том что данное действительно за медали и отправляет предложение пользу и предложения завершает назвать обратить внимание что на самом деле здесь не на самом диске и 1 or 2 персоны система может не заботится о том упал и ирина файл конце концов это не скинет людей железо 3 о том что файлы записан операционной системы этим деле то что он действительно дошел до диска и уже вопрос филина того насколько вы доверяете что доверяет кренделей что меня поговорив концерта а теперь немножко о масштабируемости операции sing если мы задраны потом просим эти данные записать на диск залепила подвале операций сами данные должны быть записаны на жесткий диск если не сказать это у нас все эти работы со скоростью не поисками производительность с нового диска на простом если мы собираем из диск не зря дисков в nero то краю презрительные син кара да будут производить не больше чем на дрова не шкас английский если мы собирается да то если мы собираем mirar и страйпа то тоже получаем производительности рейс b6 то же самое сегодня то есть для того чтобы наса решили пиратский sim мы должны сбросить на все дисками которые перри полагаются сбросьте только 1 соответственно вывод о том что на самом деле операция sing довольно плохо масштабируются при росте компонент системы толимана и еще сложнее игра состоит еще сложнее дело когда в определенных системы хранения когда у вас есть несколько разделенных состав и какой-то кусок да вы сначала модифицируется где стоп ордер дальше вниз несколько копий абсолютно разные системы он будет искать приложение говоришь что гола team 7 должен быть совершенно нескольких удаленных машины после этого мы можем громить от можешь потому что дорогая и так стоит еще помни что если мы медитируем 17 x1 системы то это ситуация но не всегда настю каблуки рождения работает своя система питания может быть в соседнем приложений которые не g71 на записала какие-то данные но на на зеленый осени данный послали sing и наверное все что у нас есть в бедре включая это сторонние да потому что мы не можем сказать о радикальных пар не плачь вот в этом смысле ситуации сим в бедности нам еще сложнее и поэтому 7 передач можно тестировать и при этом смотреть на результат и иногда системы решают эту операцию есть если посмотреть данные и графики история показала в этой ситуации больше количестве 5 7 чем фактически возможно на системе из семи note 4 на стилинг а вот дальше там вопрос к тому чтобы тестировать дальше систему на к чести и знаю как мы у себя тестируем parallels клаус сторож какие конкретно сценарии мере мы измеряем трендом рейд райт и обязательно меряем rising операцию с кучей любые несколько вопросов по рядам и идущие возрождение нам позволяет или испуга симка по очереди заправились и крепкой зритель будет каждая система на и миль масштабируемости с увеличением количества физических авто в кластере то есть кластер из одной машины и ресторан больше будет больше больше и мере один мир окружающий поток на 24 16 когда таким образом мы получаем 1 марта неровности ладно узко ваше и потому размер кластера которые у нас есть используемые для этого утилиту которая разработана творят parallels tools она позволяет нам мерить большинство сценариев в том числе и сим мы знаем или на работает лидер копро фигурировать правильный working нагрузить несколько дисков одновременно семей от разных сценарии кэшированные не кашированная его директор асинхронного его и также тем кто хочет посмотреть что действительно делает эта программа можно сказать поднять работники и будет написано конкретно все цари все все все команды которые выполняют утилита отеле волос результаты которые получают с помощью этой утилиты как я говорю мы имеем размер кластера и осуществляем здесь по-разному сумму суммарная производительность со всех ног кластера то есть мы у нас царь что каждая грузит кластер и придает скорость и на каждом каждый столбец это кластер из но 10 21 ему дали мы сравнили производительность лаками придав кумир по всем и наши распределенные системы но сады крашером либо бизнес уровне это то что касается записи да конечно а вот на следующем слайде я вам покажу на которые вы сможете скачать методологию также можно скачать отель его обступили то вы можете ее использовать в своих проектах вот все славе будут выложены после конференции но если вы хотите прямо сейчас использовать вполне можно скачать игру прямо сейчас также стоит еще поговорить о разных говорилось инкина что сим сложные операции но она обеспечивает нам жить на кнопочку вот раз она обеспечивает консистентной данных то необходимо поговорить о тестировании к чести от насти данных распределенных систем что такое к системности какая она бывает ассистент нас есть строгое в терминах такой сервис который обычно используется для питания надо строго пинсер здесь и consisting of конечном счете строго консистенции то когда вы в систему хранения записали данные и после ног вы ее всегда и на самые последние данные последнее изменение если вы записали в систему хранения данных с и веничком консистенции то вы можете какое-то время читать еще старые данные только потом уже увидите новые данные в системе как проверять на systemd из принципе довольно простой подход записали данные прочитали и посмотрели у нас старый данные прыжка ледниковый данный прочитаю роликов довольно просто но с этим распределенную не есть кашиик решением не только на турнир моде году на которой мы запускаем тест они еще есть на других серверах которого собрано распределенные системы хранения данных также ассистент начищу сильно зависит от того какие компоненты сейчас присутствуют или отсутствуют в системе из заказа устойчивость этой случай если что-то сломалось в системе для тестирования к системности у нас заработано утилита хай флеш чек это утилита работает по следующему принципу есть какой-то клиент сервис который мы хотим протестировать то как он обеспечит ли он строгую консистентной и действительно ли он записывает данные систему при операции sing клиент записывает данные на диск говорит sing система вас возвращаешься на данное действительно записала после этого клиента отдает аудитору информацию о том что запись с таким-то номером действительно упала на нет после чего мы отключаем какой-то компонент системы или клиенты общем у нас есть доказал устойчивые системы хранения данных которая в принципе может любой компонент выключать вот нужно вытащить ролика занят и после этого клиента проверяет те данные которые он видит на диске после того как случился в городе соавторы система снова стала доступна и сверяет сам где там действительно нет те данные которые находятся на диске они в актуальном состоянии лени система сказала что на и записала но в итоге мы читаем какие-то старые данные для этого аудитор с клиента на сверяет весит она также эту утилиту тоже можно скачать вот она довольно простая и сейчас я немножко расскажу о том как она влияет на производительность распределенной системы уже видели что и сильных тогда жили сказал он не записаны но на самом деле там есть еще много деревьев момент в частности есть кашель года не железо или контроллера которые родились 20 данные и ездите на положил и плакали такую операция завершена есть вы когда советские принимают свои данные самом деле единицах это мини-компьютер которым стоит процессор выразительные пареньки и какие бла бла котором ssd-диск тот контент который пришел и создание скачает операцию сим которая может и официальные комментаторы которую полина план создали цска и позволяет какое то время после включения кидая упражнений и сбросил своей души с своими номерами памяти на банке с соответственно в случае по пути есть то есть которые учить матерей детали с помощью конденсатора по-прежнему питали ssd диск и сам рейд контроллер должен быть батарейки чтобы ему тоже интерес данные которые ходят с тобой крыльев и шеи и еще хотелось бы сказать про надёжность ssd дисков предводители обычными больший срок службы это параметр не уродились акации на него тоже в теле тут опять внимание потому что обычно я создаю стильная духа они не хранятся но еще и могут выйти из строя быстрее отдых нагрузками которые дают сервера следующий интересный момент наработка на отказ то насколько система надежных calendo это не только из разных клинических старей вы можете нанять это наработка на отказ назовите квадратично вот нет времени на свое и протестировать на состояние все это меньше берем что у нас есть какая система вы что устроенных и до легендарной и перелью данные и все рады быть количество времени чтобы можно было потерять и вот то время который травить на нем она влияет то система надежна паралич намечаем быстрее реплицирует дары чем быстрее является более надежным как это проверить тестирование скорее просто можете загрузить и какой-то системы мониторе картин состояние и измеряете его время сколько система восстанавливается после сбоя если мы говорим о локальных рынках когда она смесь несколько дисков каждый доллар независимо от того raid1 нас время восстановления скорость 1 диск чему-то штаны она сама цель не рискнул выдачи не поменяют на новый и наверх ritmo dynamic пропали на слово низкий их поставить на бис новый которых регион соответственно мы можем буду писать со скоростью не больше чем скорость работы рано а диск если система распределенная имеют несколько групп и и схожую систему break the будут такие же проблемы с производительностью восстановления надо 10 приведен пример скорости восстановления нагонит к то есть уборе и 2 слова сторож которые имеют возможность разбить данные на несколько чанков разлейте по всему таким образом случае восстановление данных мы можем читать файлы читать ангес на север и в паре другие сервера и тарифицировать таким образом с ростом величины глостера повышается надежность кластеров потому что скорость восстановления она от этого растет вот и хотелось бы еще дайте ссылочки на продукты по район слова стоящие вы можете также попробовать о борьбе за из февраля месяца мы выпускаем новый для supra я устал сторож которая будет работать на archer цен на компасе 14 и на различных дистрибутивов и теперь сердце я хочу заранее сказать что автор самого интересного вопроса получит компании parallels лицензию на аральское том и parallels access для виртуализации на марке доступа смотрели построить здравствуйте спасибо большое за доклад меня зовут андрей вопрос он не совсем имеет отношение тестированию производительности у скорее относится к пролез клаус . подскажите пожалуйста на parallels клаус торт жирный мото марина фонд российской федерации которые происходят с метаданными они нас есть и сервисы которые обслуживают выпрыгиваете операции и пососу они определяют borgore если есть мальчики приняла изменения то но во первых применяется идет и response а там что время прошел и распределяется на все стали вопрос собственно первый вопрос по производительностью рассказали пущего приказ капитана а как с конкурентным там всякие церкви проще краснокамске закрытом азовский например цифры цифра к херам токио запас славик на мнение уже нового нашего продукта проел слова сторож я как раз анархия на одном и том же железе на канале мной тоже системе был собран церковь с кэшированием на журналирования не воздействие лагерь пока маленькие из пяти нот и одном gigabyte но будущем обязательно вести когда будем ближе к релизу после поскольку барон вообще тут один пилот решение перестать сама зона мы сравнивали я как бы не могу сейчас показать закрепления нет нет не запасено было для амазона бана за ниже там еще такая вещь и чтобы покупая ели с выставки платите за какое-то количество опций вы можете купить и волне с обычно количество эмоций большим количеством волос а вы не можете вырасти больше чем где-то ещё и усах и любое слово не дает если вы используете к вам сторож то низкий plaster и они направлены на то чтобы обеспечить работу в том числе и 1 1 на грузчика случае если у вас есть один для виртуальной машины и она поделена на кусочки а затем еще и amazon гораздо дороже amazon гораздо дороже привычность 3 выше немножко сравнивайте решение как бы повлекла да и решение правят клауда который области постройте себя внутренне то есть паблик клауд вам дает кабели сервис без лиза понятно все таки круче уильям если вы конечно больше потому что озон вы не можете у себя дома построить мне не нужны может быть решением и поэтому ну понятно как хорошо он свалился пролезу в обмен на любительскую цифры железный то есть вот я хорошо решение вы наденьте получите больший обзор с меньшими затратами на железо умеем работать поверх коммунити hard way а то есть вы можете купить обычные десктопы и на обычных десктопах собрать себе кластер это будет очень дешево по сравнению с тем что спасибо и второй вопрос по другую сторону вы искали что 8 то данной покайся но процесс имеет вполне понятно ограничения на масштабируемость ну словно вот мы берем мы читаем потолок скажем на 100 мирно файлов и пытаемся это читать это может быть этого медленно если мы читаем параллельно очень много вообще какие ограничения здесь спасибо правильный вопрос потому что вы клаус тораджи мы ориентированы на хранение больших файлов такие как образы виртуальных машин backup а базы данных и целая виртуальная машина она разделяется на чанки фиксированного размера чанг у нас 64 мегабайта соответственно файлы меньше чем 64 мегабайта вы можете хранить они уже не будут разделяться на более мелкие кусочки и ориентирована хранения больших файлов для хранения больших файлов у нас производительность метода сервер достаточно для того чтобы целиться до восьми петабайт соответственно мелкие файлы будут работать немножко менее активно будет меньше потенциалов scalability но на больших файлов таких как виртуальной машины и база данных поразительность я хочу задать такой вопрос случае сбоя одного из узлов каков механизм восстановления то есть допустим кластер работал один из узлов вышел допустим на шесть часов из строя как он будет догонять остальные узлы то есть это полная репликация частичной дефекации или что то еще представим что один узел вышел из строя из класть кластер детектирует эту ситуацию что один из узлов не доступен с теста и смотрит на то что данные там данные вышли из кластер а после чего система детектирует где есть еще купи вот этих же самых данных то есть мы рекомендуем хранить 3 реплики соответственно когда у нас вышла одна но до из строя то остальные реплики у нас по-прежнему доступны и мы можем с других хостов читать и сделать копию данных на других состав то есть грубо говоря система за излечивает себя за счет тайри данте которая накопилась на других оставь случае если мы бы начали уже репликацию но хост вернулся обратно мы видим что вас вернулся обратно на нем есть какие то данные о данных есть своя версия случай если у нас какой-то чанг не было еще вотрите лиц ирован данные обратно вернулись в той же самой версии соответственно мы те же самые данные на на вновь минувшем сухость и оставляем если данные уже изменились то мы уже где-то создали копию данных в оставшемся рабочем классе нам придется удалить эту аудит копию с тобой сервера который выходил понятно то есть если допустим количество оставшихся в живых узлов позволяет сохранить заданную редан dance это он фактически вернется чистым пустым да и включиться в работу если он если прошло там очень много времени до допустим данные бог полностью успели рассосаться да если данные полностью успели от lead реплицироваться то он вернется но необязательно чистым потому что когда если все данные изменились то он вернется чистому мне придется дрогнуть все эти версии если он вернется ним не полностью чистым обернется с каким-то количеством старых данных не количество новых данных дальше системе у система получится допустим 4chan к одинаковые версии нужно какое-то удалить она удалит не обязательно на этом сервере может ударить на каком-то другом чтобы сбалансировать данное понятно и второй вопрос у вас где-то может быть не собой на сайте есть более полное сравнение допустим с той же кластеров с или люстры что-нибудь ну то есть вот то сравнение которое мы видели на слайдах она как бы до 50 процентов наверное даже не дотягивает то есть она сильно не полная где то есть более адекватное представление в паблике сейчас нету работаем давайте добрый день меня зовут ты не прав у меня следующего против вот данные caught это объектно хранилище или блочный ты лучше это файл властями файловая система то есть мы храним файлы и далее файл и разбиваем на блоке то есть модель близко по монтированию как ветер в суде дамы через фьюз монтируем ся как файловая система и дальше исключительно в каком состоянии realidad поддержки локов на повысить ими потому что вы упомянули готовый для open вот эта виртуалка этот вопрос важен вопрос я имею ввиду когда процесс делают лукина файлах как она в куб корректно обрабатывает но лоб глобальный на файле но как бы вас вы используете виртуальную машину и виртуальной машины 1 именно по ним момент времени может отклоняться на одном посте поэтому где машина исполняется там лук и взят на имидж контейнер и соответственно контейнеру пэн в здешние сейчас имеет пилу это один низкая и мешанине горстка файлов вот на этот имидж и берется лака берет лог тот тот сервер который спать как обрабатывать ту ситуации когда на 2 но дух одновременно идет запись в файл с одним и тем же именем мы такого не позволяем запись на файл она может быть только с одного клиента с одной машины на второй машине как какая директ и будь то у систему как когда видно пишет вам они дадут его от крика записи вы упоминали что в принципе это можно разворачивать подобен вот это решение есть ли у вас продакшен deployment этой системы и если есть то сколько нот участвует в кантилим production deployment ну вот параллель склад стороже у нас несколько сотен клиентов конкретная вам цифру сказать не могу потому что меня не разглашаем но у нас несколько сотен клиентов и несколько петабайт данных хранится в клаус сторожи спасибо еще один вопрос связанный вот как раз с ситуациями отказов восстановление понятиями кворума но у я вот чтобы понятен был вопрос как бы пример приведу значит у нас есть клауд старец состоящих предположим и 6 но да вот есть какое-то там количеству клиентов там активно читающих там дай там что-то там пишущих туда вот у нас происходит вот такая вот нехорошая вещь которую там vm ware называет пар течение то есть у нас разваливается сетка таким образом что у нас как бы остаются три три ноты вот в одном и как бы сегменте сети положим коммутатор какой-то корневой отказал по 3 ноты в одном сегменте вместе своими клиентами три ноты в другом сегменте месяц своими клиентами значит что произойдет в данном случае либо значит обе обе части развалившегося кластеру идут в отказ потому что недостаточен кворум либо у нас используется алгоритм paxus соответственно там где останется наибольшее количество metadata серверов они проголосуют и па па па ра родных у вас быть не может потому что у вас должно быть нечетное количество сервисов если вы делите пополам у вас она какой-то какое-то количество будет будет больше"
}