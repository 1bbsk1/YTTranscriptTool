{
  "video_id": "VqCz_KOtzlw",
  "channel": "HighLoadChannel",
  "title": "Как мы храним 60000 событий в секунду / Арсен Мукучян (AdRiver)",
  "views": 1074,
  "duration": 2973,
  "published": "2017-04-22T14:46:48-07:00",
  "text": "здравствуйте еще раз меня зовут арсен я занимаюсь и плюс плюс разработкой в компании одри wear расскажу о том как мы храним обрабатываем свои данные о чем будем разговаривать сначала пару слов о предметной области затем я покажу несколько примеров задач которые требуют хранение обработки событий которые генерируют система дальше мы поговорим о нашем старом решение я расскажу о проблемах с которыми мы столкнулись и которые начали решать затем постараюсь обосновать почему новое хранилище данных мы решили разработать свое не стали пользоваться не одним из готовых решений последняя часть доклада будет наиболее интересно для тех кто сам не прочь по велосипеде я расскажу о ключевых подходах которые обеспечивают работоспособность нашего решения в двух словах о бадре very на рынке одри город система управления интернет-рекламой на рынке мы 16 лет объем запросов к системе увеличился за это время примерно в 2000 раз сейчас к нам приходит до 60 тысяч запросов в секунду среднесуточное значение примерно 40 за двенадцатый год например мы обработали чуть более одного от триллионы запросов также в качестве факта для дальнейших или важных вопросов по данным comscore например в мае 13 года штатах было обработано 20 миллиардов поисковых запросов одри wear за это время ответил на 60 миллиардов рекламных наш рабочий объем годовых данных сейчас составляет 200 терабайт дальше мы будем разговаривать преимущественно о них открывая практически любую страницу интернета ваш браузер делает один или несколько запросов поставщику рекламного контента система должна ответить на каждый запрос не более чем за 2 десятых секунды выбор рекламного материала производится на основе той информации о пользователе которую нам передаёт площадка на основе этой информации о пользователе которую мы уже не узнаем также на основе всевозможных условий критериев которые настраивают наши клиенты в админке системы результатам обработки запросы помимо выдачи рекламы пользователю является регистрация этого факта в системе виде события событие это по сути набор параметров которые в дальнейшем позволят однозначно идентифицировать сам в банях которые мы выбрали почему его выбрали и пользователя которому его показали размер наших событий колеблется от 100 байт до двух килобайт при средней нагрузке на систему мы получаем их порядка 30 гигабайт в час или 700 в сутки закладываешь же от пиковые но реальные нагрузки на систему мы можем сгенерировать до 400 терабайт данных за год зачем же они нам нужны анализ сохраненных произошедших событий позволяет строить отчетность для клиентов по факту оказания услуг также мы производим оценка эффективности проведения рекламных кампаний мы рассказываем своим клиентам о том как они могут воспользоваться настройками системы для того чтобы более эффективно охватывать свою целевую аудиторию 3 класс задачи я условно назвал прочее интересная аналитика это действительно интересные задачи сейчас их больше всего и они самые востребованные так или иначе любая аналитика над событиями приводит к нужно и приводит к изменению настроек выбора рекламных материалов это может производиться либо вручную клиент получил отчет посмотрел цифры что-то поменял в настройках своей компании или же это производится автоматически система анализирует действия пользователей происходящие события и подстраивается сама себя подстраивают для того чтобы более эффективно охватывать целевые аудитории наших клиентов первые два класса задач они имеют четкие требования и с ними как правило не возникает никаких проблем дьявол кроется в задачах той самой интересной аналитики каждая такая задача это это сюрприз их невозможно систематизировать нам удается для них предложить общего решения я обещал дать несколько примеров таких задач базовая метрика которую снимает рекламная система это например эффективная частота по горизонтали отложена количество показов баннера пользователям по вертикали число пользователей которые кликнули на банях после определенного числа показов эта метрика позволяет определить максимальное количество раз которая имеет смысл показывать конкретный банер конкретному юзера из графика видно что больше четырех показав одному пользователю делать не стоит потому что мы будем мы впустую полить показа следующая задача это пост клик анализ или так называемые лиды мы анализируем свои события для того чтобы определить цепочке перехода в пользователей и выделить в них ключевые действия это позволяет нашим клиентам покупать именно целевые действия пользователей такие как регистрация на звонок покупка так как пользователи совершают свои действия переходы напротив не только в рамках одного часа но например в течение суток или даже месяца решать такую задачу в реальном времени не представляется возможным нам нужно хранить и анализировать события следующая подзадача это ноу-хау компании мне разрешили о нем немножко рассказать сегодня с недавних пор мы начали считать наведение пользователей на баню на ведение это когда пользователь елозят курсором мыши по рекламному материалу на практике доказано что такой способ взаимодействия говорит об интересе к целевому продукту мы определяем категории пользователей которые таким образом проявили свой интерес передаем эту информацию клиентам они подстраивают свои компании и ctr их рекламных кампаний увеличивается до двух раз также по итогу проведения всевозможных рекламных кампаний мы определяем ядро аудитории это группы пользователей которые оказываются наиболее лояльными к площадкам где видели рекламу к самим целевым продуктам этим пользователям мы например в дальнейшем показываем банер сергей которые они заполняют не уходя со страницы где его увидели и оставляют некий фидбэк как о самом продукте так и о проведенной кампании это тоже очень ценная информация анализ сохраненных событий позволяет нам строить формулы которые вычисляют вероятность взаимодействия конкретного пользователя с конкретным рекламным материалам эти формулы в дальнейшем используются в runtime и при обработке поступающих запросов последняя задача который получится сегодня рассказать это поисковая строка когда пользователь переходит на один из сайтов нашей сети из поисковой выдачи он может увидеть динамически сгенерированный банях который будет содержать текст недавно введенного им поискового запроса и некую альтернативную выдачу результатов связанную с целевым опять-таки продуктом таким образом мы например рекламировали один новый поисковик несколько лет назад я надеюсь мне удалось вас убедить что рекламной системе действительно нужно хранить и анализировать генерируемые события аналитика нам нужно естественно для оказания основных услуг взаиморасчетов с клиентами также мы поняли и в этом тоже убедились на практике что аналитика нужно что аналитику можно продавать как отдельный продукт и что на ее базе можно строить новые продукты но это все требует обеспечить должное качество раньше генерируемые события рассылались напрямую в инструменты подсчета различные статистике эта подсистема была разработана где-то в начале двухтысячных годов и почти десять лет справлялась с поставленными требованиями также событие сохранялись и слогом за это время нагрузки увеличились примерно на два порядка это приводило к неприятным конфузом время от времени одно и то же событие доходило до одного инструмента но не попадала в другой syslog в принципе известен своей склонностью сливать данные при перегрузках и это в общем является его фичей как раз таки в конечном итоге такое поведение приводила к тому что одни и те же показатели подсчитанные различными инструментами они совпадали это вызывало неудобные вопросы случаях когда часть информации все-таки сохранялась где-либо мы восстанавливали целостность данных вручную исправляю бинарные файлы hex editor у меня до сих пор передергивает от от таких от такого рода деятельности перестала работать примерно в 2006 2007 годах когда модерам работал свой первый миллиард запросов сутки мы сначала почти год мы занимались разработки всяких заплаток пытались оживить старое решение но потом решили разработать новое хранилище данных что от него требовалось в первую очередь это успеть все записывать сливов никаких не нужно во вторую очередь мы больше не хотим больше не хотели получать расхождение счетчиков все во всевозможных показателей нужно было обеспечить консистентной данных все клиенты хранилище должны работать с единым набором данных в дальнейшем это требование эволюционировала в необходимость разработки механизмов которые позволяли бы сравнить уже прочитанные события с теми что сейчас находятся в базе мы должны были естественно обеспечить произвольный доступ ко всем данным рассчитывали примерно на тысячу клиентов каждый из них должен был бы получать данные со скоростью 500 тысяч записей в секунду надежность как я уже упомянул нужно стопроцентная для нас выражается в том что любая порция данных которая помечена как успешно сохраненная она может быть гарантирована прочитано и обработано целевыми показателями для масштабируемости являлись объем данных их размер и скорость записи и чтения отказоустойчивость для нас заключается в том что после аварийных остановов достаточно лишь леночку мельник чтобы система включилась и восстановила свое состояние до аварии самое последнее но в то же время самое главное требование это стоимость итогового хранилище под эту задачу было выделить было выделено a1 42 минут avaya стойка и 32 ампера ток а это с учетом всех возможных перспектив после того как мы определили с требованиями предстояло ответить на самый главный вопрос на чем же мы поедем купим в магазине готовый блестящий велосипед или закроемся в гараже и разработаем свой начали естественно с магазина попробовали собрать мини хранилище на базе mais quel hadoop x-bass и также использовали open-source проекта диска project случае mais quel мы получали большое в перестроение индексов они постоянно перестраивались также нас не устроила скорость чтения с такого кластера x-bass всем понравился кроме стоимости сходу было понятно что такие объемы можно поднять на пластике которым должно быть более сотни серверов и это явно не вписывалось наши бюджеты похожая ситуация была из диска единственно нас не устроил также стандартный питоновский стерилизатор пиклз который используется в диска и также помимо большого количества железа было нужно разрабатывать свои механизмы сериализации детализации так мы пришли к тому что решили разрабатывать свое хранилище данных забегая вперед я расскажу в двух словах о том что мы имеем сейчас работает кластер из десяти not построенных на базе абсолютно типового серверного железа там хранится 400 терабайт годовых данных которые после поковки превратились 200 параллельно с этим кластером работает до двух тысяч клиентов они записывают события со скоростью примерно 100 тысяч записей в секунду и читают со скоростью также приблизительно около 400 тысяч записей в секунду все это порождает сетевой трафик до 20 грабит секунду время от начала разработки до внедрения первой версии составила чуть менее одного года все выполнено на языке си плюс плюс программные интерфейсы мы реализовали в си плюс плюс питон так же есть набор команд line утилит которые дают доступ базой функциональности теперь мы поговорим о том как это все работает сначала в общем посмотрим на систему поговорим о записи о механизмах которые обеспечивают консистентных данных и о механизмах которые обеспечивают чтение данных дальше пройдемся по ключевым подходом которые заставляют все это работать и подведем итог наш plaster состоит из абсолютно равноправных not которые отличаются только настройками и самими хранимыми данными они объединены мультиплексоров запросов в качестве него сейчас используется обычный dns алиас которые это всех устраивает наше хранилище умеет выполнять три основных функции записывать события отдавать их клиентам по запросу и синхронизировать данные внутри кластера как я уже говорил единицы информации подсистема хранения является события это набор параметров сейчас она состоит из 94 полей с каждым новым релизом добавляются новые мы условно разделили событие на 2 логических части мета информация и сами и блок самих данных и эта информация это по сути индекс базы данных он состоит из времени генерации события источника идентификатора идентификатор инкремент аллен в рамках каждого источника подсистема выбора рекламных материалов которые генерируют события отправляют их в хранилище history это кстати говоря забыл сказать history это название нашего хранилище каждая нота получая порцию событий подтверждает их запись и ведет учет индексов всех своих событий всех событий которыми владеет благодаря тому что идентификатор инкремент аллен в рамках каждого источника последовательности индексов сворачиваются во множество и все операции анализа индексов они сводятся к простейшими тематике над множествами это происходит очень быстро и нам очень на руку потому что анализ индексов это базовая операция при любых взаимодействиях с кластером время от времени каждая нота синхронизируется со своими соседями она отправляет запрос остальным участникам plaster а в запросе передает индекс своих событий ответ получает от каждой удаленные моды ответ содержит индекс событий которых у ноды нету также в ответе содержится состоянии данных на удаленной стороне в каком насколько они оптимизированы для чтения также каждой удаленная но до сообщает о своих доступных мощностях на базе этой информацией выбирается одна или несколько нот в которых производится чтение и синхронизация данных в реальности это выглядит так на верхнем уровне у нас работают 3 сервера которые получают события от подсистемы выбора рекламных материалов эти сервера изолированы от основных программных клиентов для усиления надежности потому что мы испытывали некоторые проблемы связанные с их перегрузом к нулевому уровню мы допускаем только тех программных клиентов которые генерируют прогнозируемую нагрузку и требуют получения данных с задержкой в несколько секунд с отставанием же в час полтора данные спускаются ниже на также 3 сервера первого уровня которые дублируют друг друга и хранят события примерно месяц два чем старше становится данные тем меньшее количество копий мы обеспечиваем и тем ниже они спускаются я хочу особо отметить это нам пригодится дальше что сейчас каждая но до владеет всем набором событий для любого своего хранимого периода пока такая техническая возможность есть для получения событий клиент обращается на мультиплексор запросов то есть как а всем участникам plaster а за содержит целевой период данных может опционально содержать индекс данных которые клиент уже обработал и не хочет получать повторно также запрос может содержать опциональную уточняющую информацию вроде всевозможных фильтров на этот запрос отвечает каждый сервер кластера сообщает индекс данных которые у него есть по запросу в каком они находятся состояний и есть доступные мощности у ну да клиент проанализировав ответы выбирают один или несколько серверов с которых производит чтение данных любую все ключевые подходы содержатся именно в реализации каждой моды любую новую разработку в компании мы строим на базе аксиомы проектирования которое гласит что любой компонент должен выполнять хотя бы одну функцию хорошо вместо того чтобы предоставлять множество но плохо в случае подсистема хранения такими функциями являются запись и чтение синхронизация событий так мы получили три функциональных компоненты ноды которые оригинально назвали кистей в райтер сервера менеджер в риттер получает сгенерированные события пишет их на диск сервер читает их с диска и отдает по запросу клиентам менеджер как я уже сказал отвечает за синхронизацию с остальными участниками кластер а также по итогу эксплуатации первых прототипов мы поняли то что в райтера удобнее писать событие в том порядке в котором они приходят по сети а серверу удобнее их раздавать в том порядке в котором вы хотят получать клиенты поэтому менеджер мы нагрузили дополнительные логикой он занимается перепаковка и данных читает файлы записанные в райтерам и пишет их в том формате который более удобен для клиентов сервера также менеджер применяет да нам различные схемы оптимизации и алгоритмы сжатия в паре созвучен и аксиома идет вторая мысль лучшая оптимизация которая может предложить разработчик это алгоритмической оптимизация в случае подсистема хранения такой оптимизации является уменьшение объема данных которые нужно реально обработать на процессоре этого мы добились полностью отрезав все компоненты от системы от обработки содержательной части событий они оперируют только индексами событий не влезая в остальные 90 с небольшим полей это позволяет данным путешествовать по маршруту сеть память дисковый кэш диска и обратно при обработке больших объемов данных рано или поздно приходится столкнуться с нехваткой памяти точки мониторинга невозможно поставить везде поэтому время от времени мы получаем отказу в обслуживании по out of memory подобные проблемы мы решили обойти на корму и применили потоковую схема хранения и последовательного логику обработки события пишутся на диск последовательно читаются также в памяти всегда находится только то порция данных которая подлежит обработке этот подход мы используем и в инструментах более высокого уровня которые занимаются именно подсчетам статистике это позволяет им обрабатывать терабайт данных не задумываясь о памяти естественно событие обрабатываются не по одному мы применяем страничную файловую организацию и жмем своей странице в райтер получая события заполняет имя страницы данных пакуют их одним из алгоритмов и сбрасывает на диск сервер же читает эти по кованые страницы и раздает их клиентам клиент получив очередную страницу данных вынужден распаковать и и здесь и реализовать события и обработать их за это время сервер успевает прочитать очередную страницу данных для этого клиента и обслужить остальных на уровне представления данных мы используем собственный протокол он отдаленно напоминает google про табу отличается тем что перед каждой записью мы пишем ее размер это позволяет в дальнейшем при чтении пропускать записи не прогоняя их через процессор также у нас реализован чуть более богатый выбор типов данных и мы считаем то что собственная реализация ключевых технологий таких как силе зация позволяет их более тонко оптимизировать под свои задачи например в отличие от google про табов мы отказались сравнении с google про the buff мы отказались от возможности генерировать протокол сериализации на основе метаописания но зато весь от уровня представления данных у нас введен в онлайн и это сказывается хорошо на производительности кстати говоря на низком уровне мы используем асинхронное чтение синхронную работу с диском сейчас используется по sexo его следующий шаг в планах перейти на черного его по итогу использования различных алгоритмов сжатия для себя мы выбрали следующую схему сетевой обмен и данные кратковременного хранения мы пакуем с помощью лживо он не менее требователен к процессу но хуже жмет данные которые подлежат долговременному хранению мы пакуем с помощью zip он сходу жмет в два-три раза лучше чем за но процессора требует больше раз в десять я говорил то что каждая нота сейчас владеет всем набором событий для периода потому что этого возможно технически пока на тот случай когда это окно возможности закроется мы реализовали схему горизонтального масштабирования данных подсистема выбора рекламных материалов умеет разделять генерируемые события о настраиваемом условия и полученные множество событий рассылает разным группам серверов в дальнейшем мигрируя вниз эти подмножество событий могут либо объединяться если это будет возможно технически либо могут продолжать жить своей распределенной жизнью клиенты все равно обращаются на мультиплексора запросов то есть ко всему множеству событий аппетит приходит во время еды после того как мы избавились от проблем с целостностью данных клиенты захотели получать большие отчеты за длительные периоды еще быстрее увеличить скорость чтение данных мы можем используя упомянутой схемы горизонтального масштабирования но в этом случае потребуется вводить все больше больше единиц железо при нашем требование по стоимости это не тот вариант который хочется использовать первым делом мы решили ускорять получение данных алгоритмически проанализировали наиболее массовые запросы хранилищу и поняли что многие программные клиенты считают статистику не по всей системе в целом за период а по какому то конкретному ее объекту например по сайту баннеру или рекламной кампании сейчас каждый такой клиент вынужден запросив данные хранилище получить весь набор событий за период из него выбрать интересующие его события и обработать их мы выделили ключевые параметры события по которым может потребоваться фильтрация по которым клиенты могут захотеть получать данные это сущности вроде с сайта рекламной кампании баннера в общем довольно очевидные вещи мы выделили эти сущности дополнительный ключ данным и менеджер стал объединять события по таким дополнительным ключам теперь клиент который использует фильтрацию передает на сервер дополнительный ключ с указанием объектов по которым хочет получить данные и обрабатывает только тот поток событий которые его реально интересует это увеличило скорость чтения на 2 5 порядков но естественно только в тех случаях когда клиент использует фильтрацию следующая задача это уменьшение latency все больше и больше задач требуют реакции моментальной реакции системы на какие то действия пользователей например то же самое поисковая строка который я говорил задержки в часы или в секунды не устраивают многих программных клиентов мы реализовали следующий механизм клиент может подписаться на получение самых свежих событий и в райтер ему их отправят до того как они попадут на диск задержки в этом случае милисекундные все описанные подходы обеспечивают следующие показатели производительности с каждой отдельной ноды она обрабатывает до 500 клиентов читает из трех тысяч файлов запись производится со скоростью сто тысяч записей в секунду приблизительно чтение как я уже говорил 400 это порождает трафик с но до v2 гигабита в секунду при этом экземплярах каждый компонент и занимает не более 2 4 гигабайт в памяти тот же сервер который обслуживает упомянутые 500 клиентов нагрузка на процессор составляет порядка 20 и 40 процентов подводя итог можно сказать что поставленная задача была решена и наши заказчики остались довольны на этом пожалуй все спасибо добрый день ир сен меня зовут костям я программист очень много вопросов на самом деле очень интересно список больше чем из десяти штук выделю наверное самый основной из озвученных цифр то там полу килобайта до двух килобайт по-моему на событие до примерно получается в районе 30 терабайт в месяц данных по моему озвучено для history двадцать пять терабайт на сервер 10 серверов всего и при этом как-то звучит что каждый сервер хранит данные за период ну то есть и все вроде как обладают одним и тем же набором данных у меня математика взрывается главе то есть используется агрегация тем или иным способом если правильно понимаю то есть сколько короче вопрос сколько времени хранятся роу event да и сколько агрегированные как в этот процесс выстроен уже у сжатии какой период времени роу какой период с real-time и так далее я немножко поясню схемку уже презентацию шла с схему где был расписан наш plaster как он есть сейчас на нулевом уровне 3 сервера ниже еще три чем дальше тем ниже говоря о том что каждая но до обладает всем набором событий для периода я имел ввиду что например на ноги хранится час и это значит что все события системы за этот час есть на этой ноте ну или по-крайней мере хранилища стремится прийти к этому состоянию но между собой разные ноды обладают разным набором данных то есть разным они хранят разные периоды естественно хранимые периоды пересекаются где-то на первых двух у них серверов данные полностью дублируют друг друга так как эта информация самая актуальная и в общем мы стараемся понадежнее там все делать вот поэтому умножить размер событий на количество серверов не совсем корректно потому что они хранят все таки каждый свой период данных как я говорил на нулевом событий поступает на нулевой уровень там сейчас данные хранятся примерно за месяц за два но с отставанием в час полтора от реального времени данные сразу же поступают ниже в основной кластер с которым общаются программные клиенты на первом уровне тоже события сохраняться в зависимости от сервера если мне не изменяет память от 2 месяц вводим новые единицы в смысле меняем старые железки на новые последняя на первом уровне хранит до полугода события нет-нет-нет смысл тоже я видимо не очень точно этот момент пояснил нас интересует хранение именно сырых событий потому что если мы уже их как-то отсекли из агрегирование то этот фарш невозможно провернуть назад мы как бы потеряли даю сжатие с потерями вот поэтому события которые хранятся в хистори они именно сырые мы реализовали также я не стал об этом в докладе упоминать мы реализовали схемы которые позволяют обрезать события выкидывать из них толстые и не всегда нужные поля например строковые вещи всевозможные греф user-agent и они занимают много места и в случае когда данных становится уже много там когда не становится совсем старыми эти поля могут отбрасываться но в реальности эта схема еще не используется в кластере то есть есть несколько тестовых машин с которых клиенты читают данные с минимальной задержкой и мы вот там как раз режем события до минимального минимально необходимого набора полей чисто для увеличения скорости получения спасибо за интересный доклад а совсем скажите пожалуйста если не секрет сколько примерно человеко-лет занял разработка до выхода на первоначально запланированных показателей ну а всем велосипеда строителям будет интересно да наш отдел си плюс плюс разработки состоит из пяти человек мы занимались разработкой этой системы коллективно отвлекаясь естественно на многие другие задачи как я сказал внедрили первую версию с того момента как начали разработку примерно за год следующий год ушел на разработку первая версия в общем это естественно первая версия сразу же еще в сразу после внедрения было понятно какие места нужно оптимизировать где переписать на этого ушел еще один год то есть сухого времени наверное два года при полной занятости ну скорее всего 2 программистов если так усреднять после этого мы занимаемся в основном поддержкой этой системы и делаем новые фичи вроде увеличение скорости уменьшения леденцы вот я ответил с растертым здесь спасибо вам за ваш доклад давай смотрите у вас огромное количество статистических данных и вы считаете то есть делайте какую-то аналитику по этим данным в том числе что эти вероятности и вот если бы у вас например был бы ходу ты лишь быстро тут все понятно вы бы использовали мог радиус для того чтобы распараллелить эти вычисления но получается что свое решение и мне интересно каким образом вы раз параллели выйти ваши задачи чтобы обработать эти огромные массивы данных мы многое отдали на откуп как как это стало понятно из доклада мы отдали многое на откуп разработчикам клиентов хранилищу то есть мы исходно разделили задачу хранения данных и их обработки потому что это были как раз грабли нашего старого решения некоторые инструменты использовавшиеся делали все они хранили и обрабатывали потом чуть-чуть что-то поменялось ну или же уже существенно поменялась в нагрузках и в общем начала трещать по швам поэтому мы исходно нацеливались на разделение хранения и обработки для распараллеливания вычислений уже именно вычислений статистике аналитики со стороны хранилищем мы предоставлены предоставляем возможности параллели каждая но да я уже сказал сколько параллельно может обслужить клиентов ключевой подход это станичная организация потоковое хранение в принципе клиент не может обработать данные с такой скоростью с которой он с которой ему их отдает сервер это позволяет во многое количество потоков процессов клиентам запрашивать данные из хранилища вот собственно так происходит расправление вич вычислений то есть вот вы показывали слайд с вероятностями где скажем какой-то баннер будет будет пока будет показан сколько жирностью или вернее на него клиент обратить внимание сколько вероятностью т.е. получается что вот эту аналитику обсчитывает сам клиент все-таки под клиентам здесь я имею ввиду естественно не заказчика компании который хочет эффективно показывать рекламу от клиентам я имею ввиду программного клиента который разработан в нашей компании которым занимаются в общем мои коллеги понятный то есть этого сделается в оффлайне все-таки или варил то не очень условная граница real-time для нас это именно генерация события выбор банер а после того как событие сгенерированное ушло в подсистема хранения это может быть ну в лучшем случае псевдо real-time задержкой миллисекунды в секунды или же в недели статистика обновляется с каждым новым пакетом данных или она как-то вас пакет на обновляется то есть восемь раз если вы формируете именно по сому статистику есть разные виды статистике я как как сумел обрисовал в общем какие они бывают часть статистике обновляется с минимальной задержкой например вот я рассказывал про ту же самую поисковую строку до пользователь переходит на сайт из поисковой выдачи система реагируют на его поисковый запрос примерно задержкой в несколько секунд до 10 секунд и после этого показывает пользователю динамический сгенерированный баннер который содержит вот текст недавно введенного запроса наверное это псевдо real time аналитика много по статистике считается ежесуточно еженедельно вот примерно так спасибо скажите пожалуйста у вас был слайд по поводу памятью говорили что вас проблемы с памятью то есть если я правильно понял вы храните на диске данные блоками и как бы подсасывать и в памяти потом обрабатываете если проблема с дисками то есть я так понимаю что проблема в том что они медленно если кэширование муж какой вспоминая слова предыдущего докладчика боюсь что меня заподозрит подготовленном вопросе есть два подхода один мы читаем данные максимально большими кусками и забиваем столько памяти сколько есть в этом случае мы можем получить проблемы с памятью потому что данных внезапно может стать больше или что-то где-то в балансе рах переклинит сам характер данных изменится и все у нас out of memory мы как я сказал используем потоковые схема хранения в сравнении с альтернативным подходом мы конечно же теряя в производительности погружая данные маленькими кусочками мы читаем диск медленнее чем если вы читали его как бы в меньшее количество файловых операций большими кусками но эта плата которую мы осознанно заплатили то есть это не проблема для вас если у вас будет задержка там с тем что диск работает не так быстро как хочется штатном мы не получаем проблем с этим вот кстати говоря та же схема с кластером где я говорил про то что сервера верхнего уровня немного огорожены от остальных от основных клиентов это было сделано именно по причине перегрузок когда очень все больше и больше клиентов разрабатываются и хотят получать самые свежие данные они обращаются соответственно на сегодня которые ими обладают и в общем от таких запросов да у нас сервера немного уходили в диск поэтому мы их огородили скажем так вот но проблемы как таковой нету случае перегрузок мы действительно каждый компонент при каком-то критическом поведение он входит либо в диск либо в процессор то есть это связано с теми с тем какие данные обрабатываются там как я говорил например то что пажа то зипом требует процессора до как бы надо перегружается именно по процессу если на чтение то она как правило перегружается в диске вот диски sata или нет sata время от времени мы экспериментируем с sd но пока еще продаж это не попала спасибо арсен пожалуйста вопрос право расскажите про то каким образом статистика и аналитика воздействует на выбор банеры себя как я сказал они могут воздействовать двумя способами либо автоматически система анализирует события что-то вычисляет и сама меняет настройки показа либо же вручную наш технический наш саппорт получают аналитические данные меняют что-то в настройках показа или же сами клиенты они пользуются админкой где могут тоже управлять условиями выбора рекламных материалов они получают отчеты которые мы для них считаем что то решают как им получится более эффективно охватить своих пользователей вот и менять настройки показа я вы спрашивали то есть так или иначе через вот те девяносто четыре параметра как-то модифицируются достройки показы те девяносто четыре параметра это результат выбора рекламного материала мы можем проинтерпретировать эту информацию различными способами и в общем да интерпретируя эти множества событий меняются настройки показа вопрос в том что для real-time выбора доступны только эти девяносто четыре параметра real-time подсистема она не оперирует этими событиями она их как раз таки генерирует происходит выбор рекламного материала есть runtime базы которые используются при естественно они без диска вы все в памяти они используются при выборе рекламных материалов выбирается банер генерируются события происходит клик по баннеру опять генерируются события дальше мы можем эту информацию про найдут как раз таки задача стоит именно в том чтобы те события хранить и сделать механизмы для их анализа последующего тогда вопросы о том как устроена эта база в памяти ну на самом деле эта тема я наверно в коридоре я сказал бы на эта тема действительно для большого доклада примерно объем условий критериев на основе которых выбираются рекламные материалы в runtime и сейчас составляет около одного терабайта они размазаны по множество серверов эта информация должна быть частично проанализировано при ответе на любой запрос как я говорил на каждый из своих 60 тысяч запросов мы должны ответить не более чем за 2 десятых секунды конкретных технологиях ну просто долго рассказывать я не думаю что спасибо и тогда спросим после доклада спасибо арсен а можно вопрос я правильно понимаю что клиенты запрашивают информацию из хранилищ в основном основываясь на интервале времени это основной ключ доступа к данным какой минимальный интервал которые не могут спросить так как яндекс эта информация события его индекс он относится к каждому отдельному события в принципе клиент может запросить конкретное одиночное событие он укажет период укажет индекс событий которые у него уже как бы есть которой он не хочет получать и легко может получить в ответ всего одно событие минимальный интервал времени именно в запросе который используется это секунды то есть в вашем случае в секунду я так понимаю 60000 событий в пике до этого то есть если клиент не подготовлен он минимум может выгрузить 60000 событий если клиент но если он если это его первый запрос если он начинает какой-то отчет строить него выгружают 60000 событий если это пика и 2 тогда вопрос немножко связаны вот у вас был слайд где была оценка производительности 100 тысяч четыреста тысяч это в событиях 100000 событий назад с 400 тысяч событий на чтение всему давайте последний вопрос вопрос про сетевую структуру вот у вас много нот гигабайтами отдает еще клиенты гигабайтами забирают как устроена коммутация между всеми этими нотами централизованной централизовано это какой-то трафик на этом это не централизовано как я говорил ноды объединены банально dns оля сам сейчас теоретически его можно понять на что угодно запросы централизовано отправляются именно запрос на получение этой информации то есть клиент говорит я хочу что-то получать или надо например говорит остальным я хочу что-то синхронизировать вот выбираю что на этот запрос отвечают все участники кластера всегда а дальше выбирается конкретные сервера и уже основной сетевой обмен происходит между клиентом и выбранной но дай ну вот обмен между клиент выбранный ноты он сетевого как они закомментированы напрямую тоже вас в каждом сервере там 10 сетевых карт условно говоря и каждые там на другой разные могут быть топологию сети или вас стоит один большой маршрутизатор у нас стоит один большой маршрутизатор я подсматриваю ответ у нашего системного администратора которая лучше меня этому ориентируется в общем чуть позже я готов буду его привлечь к ответу арсен спасибо спасибо"
}