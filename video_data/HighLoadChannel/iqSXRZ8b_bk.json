{
  "video_id": "iqSXRZ8b_bk",
  "channel": "HighLoadChannel",
  "title": "Как выбрать SDN для высоких нагрузок / Александр Попов (Mail.ru Cloud Solutions)",
  "views": 1950,
  "duration": 2914,
  "published": "2020-04-14T11:04:32-07:00",
  "text": "сегодня мы поговорим о с д м с а программно определяемых сетях software and networks начнем пожалуй уже почти 3 года mail.ru клоун solutions представляет облачную платформу своим клиентам в эту платформу помимо обычных из сервисов входит также ра значит многочисленный pas que byrney эти с базы данных big data все это в одиночных как кластерных конфигурациях также с почти самого начала доступные сервисы балансировки нагрузки или pen но работа этого всего было бы невозможно без масштабируемой отказоустойчивой программно-определяемой сети или sd в которой который является фактически фундаментом стабильного облака в мчс я являюсь разработчиком который отвечает за функционирование и быстродействие этой сети сейчас у нас десятки тысяч виртуалок сотни гипервизоров и в своем докладе я хочу рассказать о том какой путь мы прошли с какими проблемами встретились какие выводы сделали для того чтобы удерживать такие нагрузки в качестве основы и ас в мчс используется upon стек в качестве из d соответственно один из его компонентов нейтрон нейтрон это такой конструктор от upon стека со всеми плюсами и минусами open source он очень функционален очень вариативен у настройках отличная документация а если чего-то мне не хватает всегда можно спросить у сообщества вот и начнем мы свой разговор с его архитектуры устроен он достаточно просто есть нейтрон api это сервис который принимает все входящие запросы от пользователь всю базовую логику работы нейтрона реализует плагин называемый mr2 именно он работает с агентами через рыбе темпе и именно он записывает все что нужно в базу данных в данном случае mais quel наиболее часто используемая агенты занимаются непосредственно настройкой do the plain и есть много разных видов агентов я здесь привел не все только самые основные вот в качестве do the plain используется опыта вы свечи различные средства представляемые ядром linux такие как на виртуальные сетевые интерфейсы сетевые пространство имен различные демоны такие как dns маска прокси keep-alive ты там подобное как же все это работает на примере например пользователь хочет создать виртуальную сеть он делает запрос нейтрон api нейтрон api передает его плагину эмаль 2 плагин рассылает уведомления всем агентам через разбитым кью рассылает события о том что такая сеть создается агенты его получают и в зависимости от того на их вычислительных узлах создается эта сеть или нет либо игнорирует это сообщение либо начинают настраивать do the plain казалось бы простая и логичная схема ну чего тут может сломаться но не все так просто давайте поговорим об архитектурных особенностях с которыми мы встретились процессе эксплуатации и первое что хочется сказать это то что агенты не помнят что они настраивали они получают события настраивают как to do the plain и забывают об этом никакой обратной связи о том как именно настроился do the plain и настроился ли вообще у них нету но в реальном мире может произойти всякое например агент был выключен какое-то время и пропустил часть событий агент был перезагружен перезагружено была вся но до на которой он располагался для того чтобы обойти как-то эту проблему внутри агентов предусмотрен такой механизм называемый full sing как он работает когда агент чувствует что что-то пошло не так то есть произошла какая-то ошибка в обращении к рыбе тымкив или сервер в ответ на запрос саги ты прислал ошибку или агент был только что запущен он запрашивает всю информацию о своем состоянии у сервера вроде бы ничего страшного но когда таких когда сущностей с которым он работает на ноги немного это проходит быстро и незаметно а когда сущности количество сущностей начинает измеряться сотнями the full swing начинает идти часами и все это время этими сущностями невозможно нормально управлять и самое главное агент нигде не пишет что у него начался этот fusing и прочее то есть мониторить это тоже достаточно сложно поэтому самое плохое в этом механизме это его непредсказуемость это конечно же обязательно нужно мониторить и но для того чтобы сделать нормальный мониторинг придется слегка папа чит-код агента для того чтобы встроиться в то место где вызывает фурсенко а также выводить размер очереди который у него там остался еще насинг следующая особенность заключается в том что центра центром всего do the plain как правило вот в ней троне в в оверлей най конфигурации является опыт вы switch то есть для он и отвечает и за связь виртуальных машин между собой и за подключение всяких сетевых функциях типа там где шкип и dns metadata прокси и там подобное и за настройку панова свеча отвечает один агент опытный switch агент в результате вся логика фактически по работе всей всей сети находятся внутри у панова свеча это приводит к тому что и внутри одного агента это приводит к тому что даже для нескольких виртуалок прямых несколько запустили на но give up in вы свече будут тысячи правил если не десятки тысяч понять что там идет не так довольно сложно даже трассировка о понго свеча встроенный механизм не всегда помогает еще сложнее написать мониторинг того что же куда ж надо смотреть чтобы понять что там нет что там идет не так вот но мониторинг нужно писать обязательно следующая особенность это то что агенты сервером общаются событиями то есть событийная модель работы надо понимать что в коде всех сервисов и самого нейтрона и агентов и выбитым какие есть ошибки например агенты получают какие-то события но часть агентов ошибочно проигнорировали эти события может быть убитым киу потерял события при доставке из-за разных там райс кондишен of в общем трудно найти rudkus для всех случаев мы не со всеми разбирались но неоспоримый факт агенты теряют события приводит это конечно же дырка в настройки do the plain трафик не ходит-ходит не так и мониторить это все достаточно сложно следующая особенность то что все общение между агентами и контроллером или сервером идет через рыбе think you в общем и целом это достаточно нормальная схема но надо понимать что при отказе этого компонента весь сдм неработоспособен то есть им нельзя больше управлять все что мы в а пешку не отправили до агентов никогда не доедет кроме того рыбе тымкив хороший сервис но он не обладает бесконечным бесконечной масштабируемостью и со временем с увеличением поток событий через него он начинает терять эти события еще часто обрывать соединения что приводит к sing агентов причем массовым к перенастроить конина настроенным кускам do to play на ко всем выше перечисленным проблемам поэтому можно смело сказать что самый главный мониторинг в ней троне это мониторинг орбиты мкс ну и последняя на сегодняшний день особенности нейтрон очень вариативен настройках можно по всякому настроить можно вклю ведь много всякой функциональности но это имеет свою цену и цена это резкое почти лавинообразное увеличение количества событий от каждого включенного функционала то есть например возьмем нейтрон в простой конфигурации плейнс и плейн сеть или валом сеть и подключены там какие-то самые базовые сервисы например деж цепи и metadata прокси такой агент такой нейтрон способен функционировать на 1000 гипервизоров наверно бессильно заметных проблем они трон со включенными power line ими сетями где включен механизм распределенной маршрутизации отказывал стоит чего signaling джипе и прочие плюшки он начинает загибаться от количества событий этот от этих всех проблем уже где-то на сотни вычислительных узлов самым правильным подходом конечно же при включении вы дополнительного функционала и при его выборе вообще принять решение о том что мы хотим вот это вот использовать является нагрузочное тестирование то есть сделали с темп поставили туда нейтрон включили нужный функционал по тестировали посмотрели как это будет на наших нагрузка также это хорошо так же это хорошо поможет написание мониторинга потому что нагруженным тестирование все эти проблемы сразу станут видны вот мы но мы все эти проблемы так или иначе как-то закрывали костылями там чинили прикрывали но после нескольких чувствительных fa cup of поняли что надо что-то менять и дерево выбора выглядело так чаще всего не у нас единственных проблемы с ней троном и чаще всего в интернете предлагают переехать на какой-то другой sdn больше всего советует танк стенд фабрик он же up in control чуть менее часто это о в н ы up in daylight я в своем обзоре только про танк фабрика расскажу на остальные наверное времени к сожалению хватит ну и конечно же можно переработать у нас же есть отдел разработки можно переработать трон в то что нам нужно починить все его проблемы почему нет перед тем как что-то делать мы с руководством договорились о некотором техническом задании которым должно соответствовать вот выбранное решение да я его сейчас приведу ну прежде всего мы подключили достаточно много функционала и клиента им уже пользуются мы не можем его отключить поэтому нам нужно решение которое имеет такой функционал дальше в наших датацентрах сети построены особым образом называемом или три настойку или el tren a top top of rack switch это делает неработоспособным некоторые протоколы отказоустойчивости типы в ррп и которые опираются на единый l2 широковещательный домен между всеми серверами тут его нет он разделен на стойке в каждой стойке свой широко вещать на домен кроме того весь внешний трафик который нам нужно вывести с этих стоек надо sign надо сигналить в сторону то рапой бюджет и также хочется выбранном решении иметь возможность масштабироваться ну или хотя бы сортироваться до чтобы в случае обнаружения узких мест залить это железом и дать время команде разработки как тает починить опять же все что мы сделаем не должно никак повлиять на работу клиентов то есть мы должны переехать на новое сдам плавно и незаметно так чтобы никто ничего не понял ну и конечно мы понимали что вряд ли мы найдем такой сдм который будет удовлетворять всем нашим требованиям поэтому всякие van doorn ее решения типа вымывали nsx джунипер control и прочие они отпадали только open source только то что мы можем до работать самостоятельно ну и давайте пройдемся по всем вариантам первый кандидат это танк стан фабрик достаточно распространенный sdn который часто советуют замену базовым нейтрон у вот много про него информации в интернете много документации хотя честно сказать она больше всего пользовательской и админской разработчики документации очень мало можно найти там айпи reference но вообще понять как него устраиваться очень сложно но этот sdn очень любят все кисти вы инженер и потому что он хорошо интегрируется сетевым оборудованием и в нашей сети он тоже очень хорошо зашел бы вот но есть и минусы прежде всего для нас он не имеет всего необходимого функционала нам сразу придется его допиливать допиливание упирается в во вторую проблему это просто огромная кодовая база в ней нужно разобраться понять что это ну какую-то документацию иметь но всего этого очень мало кроме того внутри этого sd на используется достаточно широкий набор технологий этой кассандра и rabbit им круизу кипер и менее распространенных map в районе аналитики там используются кафка редис в общем очень много это все тяжким грузом упадет на команду эксплуатацией затруднит внедрение честно говоря осложнит поддержку ну и в этом издании все сделано по своему сложно вот так вот взять нейтрон и перетащить перед взять примете у нейтроны перетащить вот на этот sdn нужно что-то допиливать писать какие-то скрипты интеграции опять же все упирается в огромную кодовую базу самого control а вот а что если переписать нейтрон в дом нашей мечты ну плюсы очевидны кажется что мы уже имеем весь необходимый функционал можно обойтись вообще без миграции немножко мигрировать мы так уже на ней троне можно пошагово улучшать как-то развязывать вот эти узкие места все делать это постепенно типы по мере нужды ну и опять же за годы работы с ним мы уже как-то в нем разобрались поняли что он из себя представляет но есть и минусы во первых не подходящая архитектура сменить архитектуру такой вещи особенно как sdn и на ходу непростая задача кроме того у нейтрона достаточно большая кодовая база внутри него много всякого функционала который мы не используем но тем не менее он плотный перемешан с тем функционалом который мы используем и выкинуть его оттуда не так-то просто и надо понимать что переделка архитектуре приведет к тому что мы будем по шагам эту архитектуру переделывать сначала сделали одну архитектуру перешли на нее потом вторую и третью и так далее сколько этих шагов будет неизвестно и переход на каждый из них достаточно долгий поэтому суммарно будет очень большой тайм ту марки то есть время когда мы получим действительно там законченный устраивающий нас решение что же делать все варианты выглядят какими-то компромиссным и не гарантирующими успеха неужели нет какого-то другого выхода что если разработать свой собственный из д.н. разработчики мы в конце концов или погулять вышли идея как минимум достойное рассмотрение давайте рассмотрим но прежде всего конечно написать sdr не так-то легко не так уж и много их есть и для того чтобы это сделать нужны команду обладающие широкой экспертизы и там контра online i do the plain и знаний сетей вот кроме того до того как его можно будет потрогать руками пройдет достаточно много времени но и плюсы хорошим можно сделать любую архитектуру убрать все не устраивающее нас моменты можно поддержать только нужный нам функционал он не нужны будет тащить за собой совместимость там с чем-то ещё какой-то legacy функционал который нужен там в каких других конфигурациях можно сделать опять же любую миграцию если разрабатывать самого начала то и возможность эмиграции можно сразу предусмотреть и опять же это конечно же интересная задача я думаю разработчики сидящие в зале меня хорошо поймут гораздо интереснее разрабатывать что-то новое чем копаться в тоне legacy кода после некоторой торговли с руководством мы решили пойти именно этим путём позвольте представить спрут sdn разработанный нами менее чем за год сейчас он работает параллельно вместе с нейтронов в продакшене нам конечно есть еще не весь функционал да и баги есть но мы получили самое главное мы избавились от всех проблем нейтрона правда для этого пришлось полностью переработать его архитектуру давайте с вами вместе по шагам пройдем и посмотрим как же это произошло возвращаемся к архитектуре нейтрона если кто ее успел подзабыть и первое что хочется сделать поработав с нейтроном это сделать так чтобы агенты знали о том как же там настроен to the plain пусть они собирают постоянной информацию о том как настроен do the plain следующее что хочется сделать это и уйти от событийной модели общения между компонентами пусть вместо этого агенты получают всегда целевое состояние в котором они должны быть от сервера непостоянные перри запрашивают этакий постоянный full sing вот после этого агента перестают быть ничего не знающими исполнителями они получают целевое состояние от сервера получают текущее состояние до to play на сравнивают их накладывают необходимый div и надо the plain и приводит его к актуальному состоянию в теории автоматического управления такой подход называется замкнутый контур управления ну и после таких переделок кажется что выбитым кью в этом месте становится излишним через него ли начинают летать не всякие маленькие события большие такие было бы с данными о таргет нам состояние всех агентов вот им проще перейти на обычный ешьте теперь и степень он гораздо проще мониторит проще его в разработке что опять же упростит слегка разработку и честно говоря за годы работы уайти сообщества наработалась очень много способов чтобы его ускорить вот но все эти переделки пока не решают самой главной проблемы сложновато to play на для того чтобы сказать что именно сделать за to play нам не нужно сделать небольшое теоретическое отступление вообще в мире виртуальных сетей против мира сетей железных все сделано по своему и вот давайте возьмем за основу примитивы с которыми оперирует нейтрон это виртуальная сеть виртуальной подсети виртуальный порт но для того чтобы разработать sdn нужно понимать что это такое можно посмотреть код нейтрона но тут начинаются проблемы до класс виртуального порта определён действительно в одном месте но и еще в 10 местах переопределены а весь дополнительный функционал который подключается к ней трону еще десять раз его переопределяет сложно становится действительно сложно неужели облачные сети настолько сложнее обычных железных сетей а как будет обычная виртуальная сеть вот выглядеть в выглядело бы как она в мире железных сетей да очень просто на самом деле виртуальная сеть превращается в switch и в metadata прокси виртуальная подсеть превращается в der sieg и сервер роутер виртуальный становится роутер железный роутерам железным ну а порты это такие представления клиентов сети то есть обычных машин или машин виртуальных до в случае виртуальности вот но в реальном мире все это все эти компоненты распределены распределены по огромному кластеру из железных узлов и нужно их как-то между собой соединять в железном мире это решается вот так я думаю большинство сетевых инженеров или системных администраторов эту схему сразу узнает схема обычной сети в дата-центре все компоненты соединены между собой набором коммутаторов разделение на сегменты среди этих коммутаторов происходит с помощью технологий мылом или там более современный mpls в данном случае все эти сегменты представляют собой различные виртуальные сети и при проектировании спрутом мы точно также разделили зону ответственности один уровень у нас отвечает за предоставление всяких сетевых сущностей а второй за связь их между собой мы назвали их соответственно and even network function виртуализацию и sdn что брал создала некоторую тавтологию вот ну и поскольку нам нужно оставить совместимость по api это нам понадобился еще один уровень applications который в которой будет приходить нейтрон или объект но запросы на логичный нейтрона а он будет конвертировать их в примитивы уже наших уровней и так возвращаемся к оставленной нами схеме и наносим все это туда получаем фактически два независимых уровней каждый со своими агентами контроллерами базами данных и а фишками каждый работает со своими примитивами и никак не зависит от соседа это соединяет их вместе уровень applications все это очень сильно упростило на самом деле разработку control play на и убрала все основные проблемы нейтрона агенты больше не забывают о том как настроен do the plain они всегда эту информацию получают нету потери событий за неимением событийной модели ну и настройка do the plain стало проще нам не можно думать обо всем сразу мы можем с одной стороны настраивать связаны с другой стороны реализовывать сетевые функции давайте подробно посмотрим как эти уровни устроены и пройдемся по каждому из них начнем с уровня и стен вот уровень sdn состоит из агентов работающих на вычислительных узлах и контроллера и опишите работающих на узлах которые реализуют свой управление ну здесь немножко упрощенный пример на самом деле к йоге каждый из квадратиков это некоторый набор микро сервисов я просто их логически немножко объединил вот и первые что делают агенты когда запускаются это они создают каждый на своем костей виртуальный коммутатор и соединяет их между собой туннелями конфигурация тоннелей определяет командой эксплуатации и она может быть в принципе любой там полная связанность частичное разделенный региона неважно главное чтобы она выполняла свою задачу после того как агенты все это создали они сообщают обо всем этом контроллер вот у меня тут такой типа коммутатор он соединён такими панелями но она бы на этом работа их не заканчивается следующее что они делают они начинают мониторить живость туннелей посылаю по ним специальные пакетики потом вот и информацию о пакетиках они информацию о том жив туннель или нет они тоже быстро сообщают в контроллер контроллер на основании полученной информации строит граффити где вершинами графа являются коммутаторы ребрами графа являются построенный туннель и на этот граф постоянно наносится актуальная информация о состоянии туннелей после этого уровень sdn готов к работе он готов создавать примитивы работает этот уровень с двумя примитивами это link in point их картинки как раз лучше всего характеризует что ж это такое они себя представляют and point этот . подключения к системе из коммутатора falling это сущность соединяющая два in point между собой и ограничения картинки тоже очень хорошо вот показывают то есть линк мы малинку мы можем соединить только два intent-а и соответственно к одному импланту можем подключить только один линк предположим нам нужно соединить между собой но дай 1 и 4 первое что мы делаем это мы идем в а пешку и создаем на этих новых 2 inpaint 1 and 2 пока ничего интересного не происходит только запись его опишу только добавляются и в базу соответственно дальше мы строим между ними link и вот тут начинается самое интересное контроллер используя grove city и алгоритм дейкстры алгоритм поиска кратчайшего пути в графе находит маршрут от первого узла ко второму дальше для всех коммутаторов по пути исследования этого маршрута генерятся правила того как передать трафик с какого порта на какой потом все эти правила загружаются в агентов и они это строят уже на реальных свечах эту схему и трафик начинает двигаться больше всего это напоминает работу протокола маршрутизации ospf хотя конечно здесь контроллер центральной и сущности немного другие там не префиксы коммутаторы вот но очень похожи но надо понимать что как бы одного этого недостаточно для того чтобы сделать действительно виртуальную сеть мы можем с помощью link of in point of соединить 2 2 виртуалке между собой но этого недостаточно нам нужно еще всякие сетевые сущности поэтому перейдём к следующему уровню уровень in heavy именно этот уровень занимается предоставлением сетевых сущностей устроен он очень похожи на уровень изъян только есть некоторое исключение его агенты могут располагаться не на всех узлах а только на тех которые там решит команды эксплуатации например чтобы сетевыми сущностями не отнимать вычислительные ресурсы у виртуальных машин вот оперирует этот уровень уже известными примитивами это всякие сетевые функции типа там der себе switch роутер так далее функции на самом деле больше чем я здесь привел и кроме того они постоянно мы создаем новые или имеем несколько версий уже текущих чтобы там как-то экспериментировать и первое что делают агенты когда запускаются это создают несколько нисколько не которая заранее определенное количество этих сущностей ну просто пустых никем не используемых где сколько какое количество все это решает команды эксплуатации ну и какие сущности тоже при этом запускаются я вот так вот для примера показал после этого агенты сообщает всю информацию контроллеру и этот уровень тоже готов за обслуживанию запросов пользователей например пользователь хочет создать виртуальную сеть он делает запрос в уровень applications этот уровень перенаправляет его и превращает в набор сущностей уровней sdn и in heaven он из уровня in heavy берутся некоторые неиспользуемой сущности и соединяются между собой линками and go into my так чтобы это получилось схема сети подобное то что показывал ранее в этой в этой сети пока не хватает только виртуальных машин дальше пользователь обращается к сервису pants т.к. ного которая занимается именно виртуальными машинами и запрос пользователя доходит до агента но вы новый компьютер и она уже запускает непосредственно процесс q ему брат в момент запуска этого процесса новый компьютер обращается опять же в апликэйшен уровень за тем чтобы подключить виртуальную машину к сети и applications уровень превращает это в набор links and point of для того чтобы соединить наш нашу виртуальную машину со switch-ем вот именно так получается виртуальная сеть вот на самом деле схема получивших получившегося sdn сильно больше того чем я рассказываю сложно в рамках доклада рассказать обо всем этом потому что тут очень много остается не затронутых вопросов вопросы scheduling до того где как располагается там сходимости и прочее все это очень интересно новым времени недостаточно значит я немного еще расскажу о уровне блики он у нас представляет собой кастомный плагин для нейтрона это замена имел два плагина и именно он делает всю работу по преобразованию наши сущности нейтрона в нашей сущности ровно такой же подход использует остальные sdn и который я вот привел в обзоре мы тут не были какими-то первопроходцами также для того чтобы новый раб с нами нормально работала мы написали свой новый драйвер и он тоже используется и подключается вот давайте резюмируем получилась такая микро сервисная архитектура это достаточно большой набор микро сервисов в качестве do to play но мы используем привычную жена мог бы новый switch сделали интеграцию снова и нейтронов стараемся повсеместно использовать замкнутый контур управления чтобы в отдельных местах гарантировать себе а именно вот сходимость вообще про за мкад и контур управления завтра будет доклад у моего коллеги дмитрия в этом же зале в час насколько я помню вот можете приходить послушать он там расскажет нам более понятных примерах что это такое получилась небольшая достаточно кодовая база это достигнуто во многом благодаря тому что мы делали только то что нам нужно и не пытались поддержать все все все все все как поддерживает нейтрон у нас там вариант только умерла иной сети мы не поддерживали все возможные сети ну и меньше чем за год мы дотащили эту разработку до продакшеном причем достаточно небольшими силами и в данный момент она находится в закрытом бета-тесте то есть мы работаем там с пользователями и как ты и фиксит баги дополняем функционал вот давайте резюмируем уже все что я рассказал прежде всего хочу сказать что выбор sdn закрытыми глазами и вообще на сдачу опасное занятие лучше всего начать выбор sd нс нагрузочного тестирования берёте какой-нить сдм который вам понравился включаете нем нужный вам функционал и начинайте сеть нагрузочном тестировать это сразу покажет очень много также с первых дней работы выбранного вами sdn продакшене надо развивать мониторинг в некоторых из д-р есть встроенный мониторинг но его как правило недостаточно он слишком не точно показывает проблему нужно гораздо точнее гранулярный и как правило показывает не то что нужно не то что нужно увидеть если вдруг все пошло не так возможно написание своего езды она это не такая уж и плохая затея по крайней мере по нашему опыту можно сказать что казалось нам это сложнее чем было в итоге ну и основная заслуга того что это была просто это правильная архитектура вообще архитектуру надо заранее согласовывать со всеми членами команды разработки нужно чтобы каждый понимал как именно работает какой компонент какая связь между компонентами какими данными обмениваются всё это сделать надо заранее до того как начать писать код ну и на этом все всем спасибо за внимание жду ваших вопросов с адой а теперь у нас есть возможность задать вопрос схемам просто я подхожу даю микрофон и вы задаете вопрос и только сейчас подождите а свет включу да значит смотрите если вдруг у кого-то не хватит времени задать вопросы там или вы не сможете их сейчас придумать то меня можно найти на нашем стенде мы стенд mail.ru клал solutions приходите поговорим с d или по-крайней мере если меня там нет подскажут где меня можно найти или свяжусь со мной сергей россельхозбанк большое спасибо за интересный доклад за такую разработку вопрос вы разработали продукт разработали вы инструменты чтобы за это дыба жить trouble шутить чтобы в этом нужно было понимать что там происходит внутри если это да есть некоторый набор инструментов мало того мы продолжаем его сейчас разрабатывать потому что именно сейчас он как раз нужнее всего поставок этот продукт выехал в продакшен и в нем стали появляться проблемы которые мы не заметили в момент разработки вот сейчас как раз мы сосредоточены очень сильно на инструментах уже некоторые инструменты есть но не все пока еще конечно же мне кажется вообще инструменты чтобы все были такой nor не бывает развить их инструмента конечно есть мало того и на развитие самого езды она тоже когда будет открытое бета-тестирование и будет ли смотрите вообще наша цель это незаметный переезд скорее вы сможете его уже использовать но не знаете но не будете знать об этом будет ли открытое бета-тестирование пока не знаю не думали такой вопрос не продумывали еще собирать не продавать не выпускать нет мы вопросы этот в смысле вы имеете виду на в пределах нашего публичного облака или выложить саму разработку приватной облако уже в приватный облака пока не знаю надо договариваться если приходите к нашим коммерческому департаменту если у вас есть кейсы которые покрывают только новые сдм мы конечно их рассмотрим то есть до договариваться с нами об этом можно спасибо поднимайте руки вы будете следующим сейчас подает добрый день меня зовут алексей показывает me up спасибо за доклад до было интересно особенно интересно привилегий пишите свои велосипеды вопрос собственно такой ну когда пишешь собственное решение да то есть но классика что нужно понимать зачем как это будет работать и так далее как насколько вы хорошо во-первых умеете обрабатывать случайные не случайные ошибки а рядом breach на виртуалке то есть надо так классический там братка шторма и так далее вообще возможно ли это в вашем расстояние и вот вы про нагрузочное тестирование сказали хотелось бы если какие-то цифры сравнение эффективности нейтрона на примере вашего решения но смотрите значит начну с конца вот основная цифра которую мы получили эта цифра 0 0 всяких проблем не замеченных в издании 0 пропущенных событий не донастроена вода to play на 0 неожиданного фурсенко это были основные цифры на самом деле с точки зрения производительности вот если выкинуть из нейтронов все эти проблемы до которых я говорил там что-то вдруг внезапно пошло не так то а нейтрон еще справляется с нашей текущей нагрузкой по поводу конкретных цифр мы проводили свои синтетические тесты но поскольку спрут пока в продакшене занимает не очень большой процент как бы хвостов дата их явно сравнить пока сложно тем более у них принципиально разные архитектура вот и пока но ответ нет нету нигде готов я сейчас предоставить какие-то цифры вот и поговорить о них так первая часть вопроса про то хорошо ли мы понимаем что там происходит да хорошо понимаем что именно в каком месте происходит благодаря замкнутому контуру правления и мы конечно же самого начала с момента разработки этого езды на абв звали его кучей графиков кучей мониторингов он обвешен гораздо лучше нейтрона как ни странно несмотря на то что меньше но мы хорошо очень знали мы очень ждали некоторых мест то есть понимали что да здесь могут быть проблемы и быстренько их сразу обложили мониторинга my и графиками поэтому ну если сравнивать вот в эксплуатации как бы то ну нам как основным разработчиком краткий взгляд на дашборд с графиками прям позволяет сказать есть там проблема сейчас или нету вот так наверное еще спасибо за доклад интересно было вопрос 1 я увидел на вашем последнем слайде со схемой аж целых три баз данных юзера pivot на пи пи ле api я правильно понимаю что это 3-ри лицом ки стоят разные смотрите это дори все это реляционные базы данных хотя в общем и целом это не обязательно но вы же понимаете что микро сервисы и мы можем любой кусок переписать когда хотим вот на самом деле это понятие базы данных в терминах субд и то есть мы идем как бы говорим клеить дтп с мыска или да поэтому с точки зрения инстансов самой базы это может быть одна база сейчас на небольших нагрузках это все одна база внутренние просто три базы с данными находится в момент возрастании нагрузки мы их скорее всего разнесем и вопрос 2 2 так как это три разные базы у вас не бывает конфликтов сущностей что что ну рассинхрон а так как это вообще разные вещи если бывают как вы к чему это приводит и как вы с этим боретесь но наверное нет потому что опять же замкнутый контур управления мы постоянно перепроверяем себя до постоянно получаем нужное состояние и перепроверяем ну и сущности достаточно разные то есть они прям ну а не друг от друга явно не зависит вот конечно как бы для функционирования сети там пользователя нужны и те и другие сущности но с какими-то серьезными проблемами рассинхрон а мы не сталкивались как правило там закрытой транзакции везде и если пользователь сумел выполнить запрос вопи на создание виртуальной сети значит все сущности как правило создались с какими-то серьезными проблемами пока не сталкивались спасибо за доклад сергея зенков red hat у меня вопрос а почему все-таки выбраны реляционной базы данных скажи мне включить ночники на самом деле как ни странно для уменьшения темпту market у нас было уже много всяких фреймворков разработок на эту тему и мы просто использовали готовые наработки чтобы ускорить свою разработку то есть ну пошли пока более простым путем там не везде используется реляционная база данных есть места где используется тарантул вот но как бы ну так вот в овервью сложно об этом сказать пока что ну возможно это первый блин раз спасибо за доклад вот я здесь а а все спасибо орлов григорий компания brand phone and скажите а вот можно слайде где вот сеть была я вела этом были вот это оставить и допросила скажите а вот не очень просто понятно было у вас поддерживаться какая-то discovering вот этой топологии автоматически с определением как бы изменения вот это по логину представим там не знать до link упал вот и то есть как контроллер там и остальная систему знаю что link упал и пересчитывает вот эти вот перри прокладывают маршруты доведены до да да но вы какого относительно уровня sdn если говорить вот например здесь дата я уже сказал что мы действительно поддерживаем постоянный мониторинг этих туннелей там of the discovery нга нет потому что конфигурацию туннеле определяет команды эксплуатации пока нет никакого of the discovery нга и вроде бы ну не было в них нужды туннеле постоянно мониторится и если вдруг что-то упало это отражается на графе сети и все что связанно с упавшим линком там пока это сделано через отношения в базе данных вот все что связано с упавшим линком все пересчитывается ну пока сходимость мы это специально грустно тестировали сходимость на каких-то таких разумных топология хотя бы нескольких тысяч коммутаторов было все равно в секунды максимум секунды то есть при этом все равно к примеру вот если такая вот конфигурация да у нас mk1 упадет то у нас по сути ленка смотрите link это сущность соединяющий два in point он просто логическая сущность и то есть физическое под ней нету там нет тут под ней физической сущности никакой подними эта физическая сущность ленка это набор правил в коммутаторах как только упадет один из каналов набор край правил в коммутаторах перестроиться этот линк ляжет по-другому алинка на самом деле просто логическая сущность внутри база данных которая соединяет два and one to any point у вас соответственно не на виртуалке континента не ну point этот если там дальше пойти это вообще каждая точка под ключ денег системе из коммутаторов нам нужно что-то подключить к системе из коммутаторов мы создаем intent и intent и на самом деле тоже логические сущности и они явно не привязаны к конкретному коммутатору они могут переехать поэтому если например виртуалка мигрировала the end point переедет вслед за ней а внутрь in point и есть информации о том как омон свечу относится какой это порт какой у этого порта тек или кое какой там тип порта соответственно это там плейн в икстлан wlan может быть вот и соответственно все это тоже перестроиться если виртуалка переедет the end point ну с точки зрения иди в базы данных останется тот же самый указывать он уже будет на точку соединения в другом месте то есть на самом деле она все очень гибко переезжает и сущности уровня in heavy к ним это относится то же самое например упала какая-то но да где находится например виртуальный switch scheduler это очень быстро понимает и выбирает другой где-то свободный виртуальный switch и все этаж отеля туда и то есть явно and point и не перри создаются у них просто обновляется информация теперь ты находишься на таком-то хвосте и соединяешься с такой-то сущностью то есть в с точки зрения пользователя которые но вот м поинты создались в user api они остаются теми же самыми информации внутренних обновляется чтобы отразить текущей сцене сети я ответил на ваш вопрос нет это ну контроллер это на самом деле какой-то процесс их несколько штук они там сортируются в зависимости от вот то есть это не железный сервера на самом деле они могут располагаться где угодно на тех же вычислительных нотах если вам нужно так они просто будут жрать ресурсы у этих нот а так все хорошо нет он уже сортируется не хочу показаться невежливым но во время сражения потому что давайте теперь у меня для вас небольшое здание выбрать того кто задал наиболее понравившийся вам вопрос более понравившийся вопрос у нас есть традиция нагружать самого внимательного не внимательного слушателя который задал самый интересный интригующий вопрос который больше понял со спикером вот наверное предпоследний вопрос как раз он позволил очень хорошо раскрыть некоторые моменты которые были плохо освещен и спасибо за вопрос очень понравился и сейчас собственно вы сами не уйдете с пустыми руками мы выражаем вам благодарность за то что вы пришли на нашу конференцию и прочитали свой доклад сейчас ваши награду ибо это вам прошу спасибо вам большое"
}