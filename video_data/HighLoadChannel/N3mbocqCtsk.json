{
  "video_id": "N3mbocqCtsk",
  "channel": "HighLoadChannel",
  "title": "OK S3: Строим Систему Сами / Вадим Цесько (Одноклассники)",
  "views": 2199,
  "duration": 3230,
  "published": "2023-04-28T06:19:52-07:00",
  "text": "так Всем привет Меня зовут Вадим Я ведущий разработчик платформе Одноклассников и сегодня расскажу вам как мы написали свою собственную реализацию сервиса S3 но для начала Пару слов про Одноклассники с одной стороны это популярная соцсеть которая пользуется десятки миллионов людей Но кроме того это технологическая платформа VK на нашей инфраструктуре и технологиях работают такие видеозвонки и недавно запущенные русторм у нас куча железа всеми дата-центрах мы генерируем большой трафик выжимаем из железа максимум 100 тысяч РПС это одна нода на 4 ядра наши хранилище перевалили за их забайт и растут дальше и 99 процентов кода написано а Джоу мы знаем и любим Зачем нам вообще S3 помимо куча систем которые мы разрабатываем сами внутри мы используем готовые продукты но только разворачиваем их на нашей инфраструктуре это вот всякие хранилища образов артефактов логов и так далее да и вообще задача хранения надежного хранения каких-то файлов или объектов возникает повсеместно это могут быть бэкапы дампы да что угодно поняли что пора все это унифицировать Потому что везде были какие-то свои бэконды хранения и начали смотреть К чему все это можно свести Ну и в разных продуктах действительно вот эти плагины являются благо был ну в докере их там много прямо где-то поменьше где-то совсем мало есть плагины которые позволяют например с интегрироваться с три вообще если все это свести в одну кучу Получается примерно такой список но единственный общий знаменатель это амазон С3 Причём здесь важно понимать это может быть как и сервис vs в котором вы храните вот ваши артефакты так и что-то развернутое локальное на совместимое с vs потому что вот эти все плагины они позволяют указать кастомный Поинт Куда нужно ходить вот где живет реализация S3 Ну отчасти это понятно Чем объясняется S3 был почти первым исторически у него огромная развитая экосистема для всех популярных языков есть sdk протестировано всё куча людей используют баги все равно есть Так что же такое S3 расшифровывается это как сервис и это с одной стороны сервис ABS с другой стороны этот документация на API дальше мы будем иметь в виду всегда документация на epa мы не используем S3 есть бакыты в бакетах лежат объекты они неизменяемые опциональные версии они количество объектов в бакете неограничено никак у объектов есть ключ до 1000 символов объекты могут быть составными и максимальный размер составного объекта 5Т Вот это одна сущность у S3 очень развитые Пиа есть там можно бакеты посмотреть с объектами разные действия делать заливать удалять их копировать теги менять Ну и есть пара интересных методов которые позволяют листить объекты про них мы поговорим позднее они важны будут для доклада объектов в бакете может быть сколько угодно Поэтому чтобы ими управлять можно бакету настроить правила жизненного цикла вы говорите каким объектом применять правила и что собственно делать например удалять через какое-то время или удалять старые версии или портить незавершенные загрузки составных объектов но ко всем системам которые мы привносим в нашу инфраструктуру или разрабатываем внутри мы применяем определенные требования во-первых это масштабируемость в случае хранилища это масштабируемость по объему по пропускной способности и по возможности локальные доступ к данным у нас много дата-центров мы предпочитаем читать данные из локального DC второе требование откозоустойчивость Мы хотим выдерживать потерю дата-центра целиком такое случалось и случится случится снова были пожары и сетевые проблемы и еще менее очевидное это удобство эксплуатации по возможности Мы хотим чтобы система чинились самостоятельно как все наши другие системы и все должно быть максимально автоматизировано в частности масштабирование в принципе любую реализацию Stream можно представить из двух компонентов первые компонент это хранилище метаданных но относительно небольшое может быть на файловой системе в виде скрытых файликов каких-то или иным мемори или реляционной или нереляционные storage или все что угодно и хранилище самих данных это опять же может быть файловая система Но там понятно ограничение ай ноды там все дела Вот сколько вы можете положить на один диск файлов может быть что-то совмещенное извращенное с метаданные Например можно было бы реляционную базу класть либо это может быть отдельный блок storage Мы конечно перед тем как что-то разрабатывать посмотрели на все что есть наверное один из самых одна из самых известных реализаций Street это такой космический корабль с кучей компонент и по отзывам наших коллег которые его эксплуатируют там все не просто особенно в случае масштабирования и отказов дата-центров еще есть популярного есть меню вот много звездочек там хайв вокруг Ну Раньше он был ограничен одной файловой системой на одной машине потом они добавили Федерации но никаких больших инсталляций нигде не описано есть конечно проекты которые разрабатывают один девелопер там вообще звездочки зашкаливают Всё круто но опять же неизвестно насколько крупная инсталляции Бывает Почему я на этом делаю акцент потому что мы не можем быть уверены что на нашем масштабе всё это будет работать нормально Там нет граблей Но на самом деле мы даже в обратную уверенность Это точно есть вот сколько мы не разворачивали всякие штуки типа эластик сёрч или ещё чего-то всё это приходилось фиксить Но это конечно не полный список Есть еще разные экзотические штуки и кроме вопросов эксплуатации здесь еще есть важный Аспект помните говорил что он почти все пишем на Джаве мы Java узнаем любим контрибьютим знаем все особенности умеем готовить все вот это написано на чудом стеке технологии на практике всё что мы привносим внутрь рано или поздно Мы фоткаем Ну скорее рано Поэтому будем разрабатывать что-то свое для этого для начала поймем А как мы храним данные в Одноклассниках у нас есть свое хранилище горячих и холодных данных У нас есть New SQL на базе Кассандры для транзакционной нагрузки хранения метаданных и все это крутится поверх нашего собственного облака Давайте чуть пробежимся по всем этим компонентам про хранилище glob of есть классный доклад Саша христофорова все эти сноски кликабельны Можно потом приедут скачать там у нас в неволе в горячее хранилище льются все новые данные там объем больше сотни петабайт Ну и стандартную репликация 3 холодное хранилище оно вкачивает в себя холодные данные СБС охладевшие данные огромными сегментами хранит их гораздо эффективнее и при этом еще гораздо надежнее оно уже перевалило за их забайт а броню скойл есть классный статья Олега Ну и множество докладов у нас сотни кластеров распределенных под нагрузкой не успел основан напачкасандре который мы добавили свои координаторы транзакций но не надо думать что это транзакция общего назначения это позиционированные транзакции то есть каждая транзакция внутри себя может трогать данные только одной партии для соцсети Как вы понимаете это норма потому что у нас все вращается вокруг пользователя группы альбомы чего-то еще ленты каналы и так далее мы активно используем fatcline паттерн у нас нет координаторов кассандре у нас каждый клиент является координатором и Он участвует в гости знает всю топологию кластеры обращается сразу к нужном и мы активно используем спекулятивное исполнение что позволяет нам добиться очень быстрого времени отклика Кассандры то есть запросы уходят сразу всем репликам Мы даже даром никаких дайджестов ничего такого как здесь важно чуть-чуть Напомнить моделирование в кассандре потому что это будет иметь значение в дальнейшем вот если у нас есть такая табличка в кассандре простая Вот видите у нас прямо реки состоит из трех компонентов то первое пурпурная Пи это во вложенных скобках это ключ позиционирования по нему все строки в этой таблице будут объединяться в партии остальные компоненты праймерики это ключ кластеризации по нему строки сортируются внутри партии Ну чтобы понятнее было Вот пример предложим есть такая таблица у нас в ней две партии строки будут сгруппированы заметьте между партициями порядка нет по ключу кластеризации все данные отсорсированы внутри каждой партии вот оливкового цвета физические данные так и будут лежать эта схема конечно упрощенная но мы видим что у нас партит или лежат непрерывным непрерывными сегментами на диске конечно там есть SSD было там все немножко хитрее Но каждый был внутри устроен так и это позволяет нам делать очень эффективное время запроса внутри партиции Если мы фиксируем партицию можем любой кусок вынуть внутри все отсортировано партиция это единица шардирования партиции размазываются по нодам кластеры и единицы репликации и наконец Пару слов про облако поверх которого все работает есть куча докладов в том числе на хайлоуиде про разные аспекты нашего облака Ну из интересного оно поддерживает приоритеты потоков что позволяет нам например отдать все свободные ресурсы жидкомпилятору Java Java и ускорить прогрев процессов облако тесно интегрируется со сторожами они рапортуют ему свои доступности облако знает когда что можно перезапускать а когда ничего нельзя перезапускать Это позволяет нам безопасно диплоить катить от автоапдейты и автоматически мигрировать ноды без боях и наконец все операции в Облаке максимально автоматизированы можно одной командой там увеличить кластер Кассандры или заменить ноду еще фоном облака дефрагментирует ресурсы и пакует их по железо чтобы эффективнее всего утилизировать То есть у нас есть вообще все билдинг блоки У нас есть New SQL У нас есть ABS осталось сделать вот State laservice stria Ну схема это немножко упрощенная потому что вот все гораздо сложнее Здесь тоже не все отображено как Вот в такой схеме может обрабатываться запрос пользователя пользователь через jusb DNS попадает на живой дата-центр там через балансировщик попадает на один из инстансов вапе апи через Wold У нас есть распределенный устойчивый болт вот эти цитирует запрос проверяет роли пользователя Можно ли ему в этом бакете вообще что-то делать затем обращается за методами к нордом кассандре через координатор транзакций если мы пишем данные то пишет на реплике lbs если читаем то читается локально и отдаёт клиенту но мы будем концентрироваться именно на апе вот остальное у нас готовое либо за кадром мы можем судить потом какие фичи мы решили сделать сразу еще на этапе дизайна во-первых реализовать дедупликацию прозрачность Для клиента зачем чтобы бесплатно копировать объекты если они уже лежат нашим стороже то вот копирование мгновенно будет мой потенциально возможно сэкономить место Хотя это вторично де дублицировать будем на уровне части составного объекта То есть он не составной то объекта целиком и нам придется трекать ссылки на блоки из разных частей объектов Но самое главное что блоки никогда не перемещаются если мы единожды загрузили уникальный блок в сторож все он будет там лежать его никто никогда не будет копировать пока не пропадет последняя ссылка и блок не удалят и вторая часть она опционально в в сестре написано в документации все методы специфицированы это версионирование мы решили его делать сразу она включено по дефолту оно позволяет нам отказаться бэкапов от бэкапов потому что мы можем восстановиться на любую старую версию но чтобы версии не копились Нам нужно будет реализовать поддержку Ну давайте начнем смотреть как все устроено начнем с управления блоками блок мы используем контент БСТ и то есть мы берем содержимое блока считаем от него шаг 256 прибавляем длину Вот это ID блока Если кто-то придет И снова попытается блок ID будет такой же над блоками У нас есть всего три типа операции добавить ссылку на блог удалить ссылку на блог и проверить если хоть еще одна ссылка на блог чтобы понять можно его удалять после удаления объекта или нет положим ссылки на блог такую табличку простую в кассандре у нас ключ позиционирования это блок кто ссылается на этот блок этот ключ кластеризации таким образом все ссылки на каждый блок лежат в одной партии Ну например это может быть выглядит так вот на блок блок есть две ссылки из объектов разных бакетов вот два блока внизу они Уникальны на них только одна ссылка что это нам дает у нас блоки лежат помним в крайней блобов ссылки на блоки в нее ускорили вот может выглядеть примерно так если мы копируем последний объект мы просто добавляем новые ссылки на блоки исходного объекта никакого обращения КБС никакого ввода вывода только манипуляции с метаданными если мы удаляем какой-то объект мы удаляем ссылку на этот блок если мы удаляем последнюю ссылку на блог то мы удаляем и блок в том числе а но здесь есть определенная проблема у нас есть транзакционный сторож есть нет транзакционных сторож и все эти операции которые мы делаем мы делаем двумя сторожами они должны быть атомарные но сломаться может что первое что и второй сеть всегда может флопнуть несмотря на всю репликацию на все остальное То есть если мы например при заливке нового объекта сначала создадим блок ВБС а потом попытаемся создать на него ссылку и не сможем её создать то мы потратили место мы потеряли блок Ну у нас потекла память если мы наоборот сначала создадим ссылку на будущий блок а потом не сможем его записать в БС то у нас будет жить в сестре Объект который нельзя получить не существует блока на который он ссылается Здесь нам на помощь приходит конечный автомат и Давайте трекать состояние блоков при этом нужно понимать что клиенты они не глупые если они увидели ошибку они попадаются ретрит запрос и хорошо бы это поддержать а кроме того вся система по построению параллельны с одними и теми же объектами блоками может куча клиентов работать и все должно работать корректно Ну давайте заведем будем поддерживать три состояния блоков для каждого блока у него статус либо он в процессе загрузки оплодин либо он уже онлайн его можно читать либо он в процессе удаления мы не знаем удалили Еще или нет на конечном автомате Happy Pub может выглядеть как-то так когда мы грузим уникальный Блок Мы его помечаем как оплодинг после этого пытаемся записать в ABS если удалось помечаем его как онлайн Он доступен он живет какое-то время когда удаляется последняя ссылка мы помечаем его как в процессе удаления если смогли удалить из ABS то удаляем вообще информацию об этом блоке ну его не существует в случае ошибок каких-то блоков остается промежуточном состоянии вот мы например пытались записать в АБС может клиент даже Real retrial но не смогли все это означает что блок живет в состоянии оплодинга мы не знаем что произошло дальше можно сам либо Bass смогли записать надо онлайн не дотащили это но чтобы блоки такие не висели вечно на самом деле клиента может сдался потом придет другой клиент и попытается удалить этот Блок Мы допускаем промежуточные переходы Между этим президентом состояниями но здесь используем фишку Кассандры мы читаем время вставки вот этого состояния блоков если оно было Достаточно давно это значит клиент оставил свои попытки и мы можем перевести в противоречивое состояние блока а то есть вот запись уникального блока выглядит как-то Так у нас есть работа с хранилищем glob of она обрамлена двумя очень быстрыми транзакциями при удалении последней ссылки на блок происходит то же самое только мы с другим статусом оперируем Ну и видите здесь есть всякие проверки чек статуса ремонт может быть кто-то уже доделал работу за нас Вот кто-то пытался тот же самый объект заливать Ну и в принципе работала норм пока Однажды не загорелись мониторы мы влогах не увидели тайм-ауты от Кассандры которые сказала что я вот не могу выполнить такой простой запрос Дайте мне хотя бы один живую ссылку на этот блок это собственно запрос проверяет Если же вы ссылки Вот одна реплика не ответила ну мы пошли разбираться и смотреть статистику по таблицам и выяснилось вот та самая табличка блока референсов что у неё есть партиция размером 7 ГБ то есть такой блок на который есть 7 ГБ ссылок сжатых ссылок там еще метаданные через std нормально сжимаются Ну если так визуально представить что происходило У нас есть блок на который куча ссылок ссылки приходят но удаление в кассандре это не удаление ячейки это вставка могилы тумстон документации зачем это нужно потому что если какая-то реплика пропустила удаление чтобы она не возродила данные поэтому могилы они хранятся ими обмениваются реплики они живут какое-то время постепенно эти могилы копятся пока не образуются Ну так называемое кладбище могил и когда Кассандра выполняет тот самый запрос Дай мне хоть одну живую ссылку на я их лопатит в попытке их найти но в кассандре есть некоторые лимиты чтобы она не жрала вечно и возникают тайм-ауты когда она просканировала слишком много могил В общем здесь самое время поговорить о широких партициях в кассандре вот Помимо того эффекта который мы писали по дефолту Кассандра фэйлит запрос если 100.000 могил подряд отсканировала надо помнить что могилы живут довольно долго дефолту 10 дней у нас в Одноклассниках 7 дней помимо проблем с могилами в кассандре есть механизм read Repair который сравнивает ответ от реплик даже если они запросили диапазончики партиций небольшие но там есть расхождения дальше врубается ридрипэр просит партиции целиком чтобы починить реплики которые отстали он будет все это гонять Ну и последнее компактен для широких партиций работает чуть медленнее потому что Кассандра не хочет тащить это все в хип и она будет это все компактить медленно через тип Как вы решили исходную проблему мы завели такой праймордил блоков они как бы существуют всегда мы знаем их контент мы знаем Хайди мы не трекаем ссылки на них они просто есть Ну как вы думаете что это было за блок там который было семь гигабайт Какие идеи у кого-нибудь блок это просто массив байт Так ну я взялся кто-нибудь догадается пустой блок В общем мы даже на стадии дизайна прита думали что вообще наверное не стоит их хранить но как-то забылось это А вот пустые объекты многие системы используют как маркеры окончания чего-то например ну как может в ходу Пили там Дан точка Дан или еще что-то ну Это не единственный претендент еще Привет Апач флинк он в адском количестве генерирует объекты содержимым 10 ну и соответственно 10 это блок на него все три ссылки трек с и вот там гигантские партии образуются так Окей с блоками разобрались как мы храним метаданные Ну про функциональные требования Все понятно У нас есть документация PS3 она описывает все операции над загрузками объектами версиями причем там везде есть две альтернативы версионируемые операции не версия нервируемые видимо версионирование в баке ты потом добавили нам нужно обеспечивать транзакционность конкурентных модификаций нам нужно эффективно реализовать лист методы лист методы они довольно хитрые там можно фильтровать объекты по префиксу группировать поделиметру либо вообще рекурсивно все обходить при этом всё в результате должно быть сортировано и ещё погибаться есть там максимум размер страницы 1.000 элементов Ну вот это всё должно работать принципе можно взять такое решение берём одну партицию то есть у нас Бакет - это одна партиция и мы туда кладем все объекты Вы не поверите я такое видел на гитхабе реализации в таком роде Ну и Лок один делаем Ну и в принципе даже будет работать у нас внутри объекта от сортированы этот лист будет работать всё что угодно но помним про широкие партийцы у нас после сотни тысяч объектов в бакити Вот в такой реализации начнутся определённые Проблемы то в принципе нормально будет работать Давайте поговорим тогда при функциональные требования Мы хотим уметь позиционировать данные для масшта стоимости Мы хотим уметь стропить транзакции То есть их распределять по большему числу координаторов это нам нужно для пропускной способности Мы хотим эффективно публиковать составные загрузки где скопирования данных Клиент уже потратил время он загрузил все эти части 5 ТБ объекта он конфликт загрузку хочет чтобы объект появился и мы хотим быстро раздавать объекты вот уже опубликованные Привет Чем объекты могут быть супер популярными поэтому давайте рассмотрим такую схему ведем логическую иерархию объектов Ну как файловой системе поделим их послышу у нас будет иерархия папок быстрее нет никаких папок если дальше буду говорить папкой там ничего такого нет будем позиционировать объекты по родительскому каталогу и внутри каталога будем кластеризовывать объекты по имени и версии здесь важно понять это я непосредственно объекты которые лежат в родительском каталоге нам придется поддерживать индекс для иерархии этих каталогов и индекс для последних версий объектов потому что пользователи могут использовать неверсионируемые методы атомарность будем обеспечивать на уровне каждого объекта То есть все операции с определенным ключом Они стерилизованы в отличие от предыдущего подхода где мы реализовали все на баките а выглядит это может примерно так вот у нас как бы Ну чем-то похоже на файловую систему Да вот в корне лежат два объекта в до отеля под даты лежат два объекта у первого просто Две версии Ну и так далее Давайте поговорим про схему разберемся сразу с локами нам нужно обеспечить атомарность операции на уровне каждого ключа заведем такую-то резидентную табличку у неё в ней ничего не хранится она используется в виде паттерна селектора апдейт у нас все поля являются ключом собственно ключ Это плюс name это ключ объекта Ну из интересного здесь есть еще одно поле это ю токен это на самом деле Хеш отключая имени Именно его мы используем для выбора координатора транзакций который будет обслуживать наши запросы потому что как вы понимаете Последний символ имени объекта не очень хороший прецедент для позиционирования транзакций Окей загрузка составных объектов начинается создание загрузки загрузки частей может быть Вот три загрузки объектов Вот видите первые объекты репорт док его два раза грузят это разные версии и у них есть части некоторые части в процессе онлайн значит они уже залиты в ABS некоторые части в процессе оплодинг То есть у нас У каждой части есть блок который есть соответствует есть статус Это тот же подход который мы применяли к ссылкам на блоке и применяем тот же паттерс конечным автоматом вот конечно автомат загрузки блоков он становится вложенным теперь снаружи появляется состояние загрузки частей когда мы грузим какую-то новую часть мы говорим мы это часть в процессе загрузки потом пытаемся ее контент загрузить если сработало дедупликация оранжевая ветка то мы сразу отмечаем часть как онлайн уже есть такой блок в обсе Если нет то идем по тому описанному пути загрузки комплитятся и превращаются в версии объектов Вот были такие загрузки и вот моих комплитием пищи загрузки исчезают появляются версии объектов их уже видно во всяких методах и стрел с что здесь интересно здесь каждая строка соответствует версии объектов она содержит всю Мета информацию чтобы получить этот объект и эту строку можно читать Нет транзакционно Кассандра обеспечивает нам атомарность изменений на уровне каждой строки каждая строка содержит набор блоков То есть если вы прочитали все можете отдавать объект клиенту Кроме того не все могли заметить в качестве версии мы используем идентификатор загрузки Это позволяет нам при публикации загрузки ничего не делать со ссылками то есть ссылки на какие-то блоки из частей загрузки это ссылки на те же ссылки на те же блоки из частей опубликованного объекта погрузили грузили и сама публикация загрузки это очень быстро Ну например это может выглядеть так вот мы грузили объект трек флаг из трех частей и транзакционно у нас не SQL мы опубликовали версию объекта трек флаг и вот список из трех частей Окей у нас осталось пара индексных таблиц для неверсионируемых запросов нам нужно нужен индексные объекты вот если у нас есть куча версий разных объектов может быть табличка объектов но не может она будет содержать такие записи вот смотрите наверху есть объект 2.doc но у него самая свежая версия 678 это Delete Marker Ну в терминах S3 Ну соответственно такого объекта не должно быть видно если вы делаете неверсионером внизу его нет и осталось поговорить про индекс для иерархии каталогов это простая табличка она под каждым родителям хранит сортированный список детей и когда мы вставляем S3 объекты меня масло HD мы создаем три записи в эту таблицу всегда просто ничего не проверяем всегда пишем что в корне есть подкаталог а поднимемся под каталог б в самой глубине под каталог цен D это уже имя имя объекта и все запросы к этой таблице нет транзакционные Ну тут может показаться что-то некорректным но используется тот же хаксрай таймом когда мы встречаем пустой каталог мы удаляем информацию о нем Ну в казаре есть возможность указать тайм записи и мы можем указать времени немножко в прошлом если конкурентно кто-то пытается ставить в первом пункте информацию о том что существует такой каталог наше удаление проиграет оно будет более старым мы подошли к тем лист методам Каким образом выглядит лист с делимером мы делаем запрос за каталогами мы делаем запрос за объектами оба запроса нет транзакционные и получаем результат здесь из интересного Мы запрашиваем один элемент больше чтобы понять у нас транкейт этот результат или нет листинг бездельметра он похож на рекурсивный лес файловой системе он гораздо хитрее мы пытаемся извлечь Ну не пытаемся мы вычисляем брендс префикс если он указан понимаем Откуда мы начинаем спуск вниз мы позиционируемся по маркеру если он указан при использовании пагинации или если префикс оканчивается не на слэш это суффикс Откуда мы начнем искать дальше мы запускаем обход дерева это дерево живет в кассандре в виде индексов каталогов и Информация об объектах мы извлекаем под каталогии извлекаем объект под объекты или версия или загрузки что хотелось сортируем уже отсортированные потоки Ну и Идем дальше Итак пока не столько результатов Сколько нужно таким образом вот каждый шаг спуска это два не транзакционных запроса и по пути если встречаем пустые каталоги то мы их Чистим здесь есть Тяжелый случай ну тяжелый В каком смысле все равно работает но работает медленнее это путешествие по глубокой иерархии почти пустых каталогов потому так ходим по дереву поднимаемся опускаемся пока не наберем Сколько нужно результатов здесь хочется опять же вернуться к широким партийцам каждый содержимое каждого каталога непосредственно лежит в одной партии в кассандре но не включая под каталоге и у нас прописан во внутренних Дока гайдлайн ребята Старайтесь держать не больше 100 тысяч объектов в каждом каталоге дальше работать может но будет работать все медленнее медленнее потом могут начаться тайм-аут особенно при удалениях и в принципе этого легко добиться вот во всех кейсах которые у нас были потому что можно позиционировать там по дате платформе группе Еще каким-то признаком Ну мы начали запускать переключать наши готовые системы на эту штуку и одна из проблем с которой мы столкнулись Это был Нексус который как публикует артефакты это ну что на файловой системе чтобы Стрим У него одна и та же схема Сначала он кладет блог ABC в контент темпы потом он перекладывает его позиционированный каталог уже можете догадаться ребята прошли по граблям файловых систем Уже и удаляет темпы потом они берут следующий блок кладут перекладывают И так дальше И так дальше Ну и в конце концов начинает тайм-аутинс на эти кладбищах могилу Ну фикс был простой мы как обычно и взяли их тот же подход который они используют для редактирующих файлов то есть выглядит так нет Я не уверен что не это примут вам Стрим но не знаю попробуем Окей мой рассмотрели как мы храним блоки как мы храним метаданные у нас остался лайфстайкл шеделлер это такая штука которая елозит по всем объектам и применяет правила но у нас все должно быть Ну что он делает он грузит Что нужно сделать потом стримова сканирует все версии втягивает в себя группирует по объектам проверяет Применяется ли правило если нужно делать действия Все просто так у нас всё должно быть распределенное отказоустойчиво могут жить разных дата центрах как между ними разделить работу здесь нам помогает фишка Кассандры мы можем просканировать всё в диапазоне токенов мы знаем всё пространство токенов партии Кассандры -264 до плюс 264 можем разделить его на части работы они будут пересекаться никак Ну так и работает Вот Но я упоминал там всякие гайды но у нас после запуска шадулера нашелся один клиент который все-таки положил в одну папку миллион объектов потом узнал про шедоллер пришел настроил правила экспирации Сказал через месяц все стирать мне через месяц объекты не нужны ну прошел месяц пришел шедоллер и все его объекты из папки стёр Ну и опять поразил кладбище в общем это все проходили уже что мы сделали Мы затролили в шадулях создания могил То есть дуллер может удалять объект немного позже но никогда сам сама лично кладбище не пройдёт последователям которые будут эмблеметить ас-3 хочется поделиться болью в S3 есть свой способ аутентификации у нас снова на расчёте хэш и от всяких признаков запроса Ну тут упрощенная схема Все выглядит так там вот все сортируется приходит приводится канонически каноническому виду ну по сути клиент считает Хеш и сервер знает секретный ключ стоит хэш потом сравнивает Вот вы это имплементите их шине сходятся Ну сидишь и думаешь Ну окей Что может быть не так вот есть пекарь имеет В общем отлаживать это боль просто правило правило вот приведения канонического виду они его довольно хитрые можно каждую букву читать внимательно API местами очень консистентный есть например такая штука В S3 подписать URL с экспирацией то есть клиенту который есть ключ он может подписать URL на доступ к какому-то объекту например на месяц и раздать Ему Кому угодно и люди смогут качать поэтому все в руле в параметрах его Вот он немножко иначе выглядит чем все другие правила для составных объектов расчет etego довольно трудно найти это гуглится официальных доках ничего нет используется кастомный urine код я вначале не понял почему они приводят пример кодирования разных символов документации В общем недаром стандартные не работает пример сломанные то есть там вообще другой формат выдачи имеет смысл его дергать реальные всс-3 или яндекс облако например есть такая штука Как http expect 100 continue Кто знает про неё о есть такие люди Классно Здорово я узнал Вот пока это им приметил В общем это такая возможность клиент Может отправить заголовки Если тебе серверу и попросить прислать ему 100 continue если сервер все устраивает он получит 100 continue то Он отправит тело ну быстрее это используется придется реализовать не знаю что за веб-сервер будет еще например есть возможность отправлять стримова данные вот там стриминг стриминг ладно что там с ним там аутентификация есть который идет чанга-чанком цепочки такие вот это всё надо аккуратно реализовать тоже используется общем пора поговорить про перформанс его можно мерить по-разному там чтение запись летсенсе еще как-то но перед тем как что-то мерить Давайте поговорим про очевидной оптимизации вот предположим У нас есть класть РБС есть кластер API и клиент и клиент хочет получить объект состоящий из одной части Ну как можно сделать можно взять блок этот затянуть его внутрь типа API и затолкать в сокет клиенту работать будет что если этот блок Очень популярный вот каждый блок хранится на трех репликах хбс все эти запросы абсолютно идентичные и такими запросами можно просто поставить диск в полку на этой реплике мы ближайшей реплики либо если блок page утилизировать сетку на этом Ну что Кроме того есть неочевидные сайд эффект есть лишнее копирование данных если мы эти данные Прокачиваем через Java Hip про это я рассказывал в докладе про раздатчик музыки Что делаем давайте заведем такой кэширующий пул блоков он будет жить вовсеппе то есть он такой локальный для каждого инстанция и в нем живут блоки блоки бывает двух состояниях они могут быть Реди то есть они скачаны из АБС их можно раздавать клиентам вот как красный зеленый и или loading он в процессе закачки из АБС на него уже могут быть какие-то ссылки какие-то клиенты Его ждут Ну и у каждого блока есть счетчик ссылок вот примеры оранжевый блок по центру ждут два клиента и там Лев 2 а вот у верхнего ссылок Нет в принципе это кандидат на вытеснение если у вас места в пуле кончается по сути мы кэшируем недавние блоки сильно снижаем нагрузку на BS потому что например какие-то имиджи качают там из докер registry то куча нот начинает их вытягивать сейчас у нас конечно там кошерочный слой стоит но тем не менее Ну и сильно снижаем letency у нас уже есть данные в памяти нам не нужно ничего ждать Сейчас мы используем ларью каждый Блок Мы грузим Ровно раз если куча клиентов пришли за одним и тем же блоком будет грузиться единожды потом отдастся всем одновременно и все это живет в кипе мы не копируем память но должны очень аккуратно тратить ссылки на блоки это уже от сейф любой любая ошибка это либо утечка памяти либо Есть еще одна ситуация предположим У нас есть блок объект большой размазанный кучу блоков блоки распределены по всему кластеру обс здесь их три для примера приходит за ним клиент с толстым каналом 10 Гигабит Ну на самом деле внутренние потребитель какой-то какая-нибудь штука и она хочет вытянуть это Большой объект себе ну как она может вытягивать Ну например со скоростью 2 гигабита то что мы идем за первым блоком 10 секунд сикаемся по диску находим откуда его читать но диск выдаст нам в лучшем случае 200 мегабайт Ну может 300 ну 2 ГБ отдаём клиенту клиент конечно счастлив Но ждем ещё 10 секунд выкачиваем следующий блок отдаём клиенту В общем работает так себе ну полоса клиенты не утилизирована если Ну вот все так будет если последнее синхронно сервереть блоки Что делаем Давайте запилим реактивный префеджер блоков он работает Просто он знает да тут еще актер есть но это отдельная история он знает какие блоки нужно отдать клиенту в каком порядке блоки размазаны по всему кластеру без поэтому он параллельно в кучу потоков начинает втягивать в себя клиенту через реактивный Стрим отправляет их в нужном порядке здесь у нас под капотом Ну да все на Ванги Таким образом у нас все не очень быстрые мы одним не транзакционным запросом вынимаем 7 метаданные список блоков затем мы параллельно тянем с кластера АБС в принципе ожидание возникает но только на Первом блоке 10 миллисекунд И то если он не в кэше и У нас под капотом реактивные стримы которые позволяют нам реализовать если клиент медленно и мне очень интенсивно затягиваем блоки збс если он быстрый то там работает на полную катушку а модификации вы медленнее В общем случае это 4 последовательных транзакций но по нашим графикам в сумме это 10-30 миллисекунд все четыре дальше доминирует работа с ABS либо запись либо удаление здесь уже время зависит от размера блока он реально пишется на диске в лучшем случае это если сработала дедупликация это три последовательные транзакции Но что важно в случае составных объектов больших все это время амортизируется потому что часть объектов можно грузить параллельно это официальная рекомендация во все гайдлайнах S3 Ну у нас все происходит ровно также нужно залить Большой объект создаешь загрузку и вот как можешь фигачишь всё это расходится по кучно а давайте про реальные цифры поговорим мы взяли такой стенд Ну среднюю тачку и чтобы не упираться в диск сдевшим работаем то есть в оперативной памяти все у нас используем абсел и стандартный параллелизм выставили 64 генерируем Рандомный Файлик на 200 Гб больше нас не влезет там дальше но Рандомный чтобы дедупликация не сработала и берем АВС 3cp заливаем его в наш багет что получилось получилось так что вот один процесс он отправляет Гигабит в S3 каждый нагрузка размазалась между двумя инстанциями каждый получает пол гигабита и отправляет в обс полтора гигабита он пишет на три реплики сразу на самом деле что интересно здесь мы уперлись не в нашу реализацию уперлись запустить два процесса то все удваивается со скоростью 1 гигабит/с потреблялась там на каждом инстасия 4 ядра мы взяли в руки наш любимый профайзеры посмотрели на что тратится и в общем там контрольные суммы Хорошо давайте мы этот файл скачаем к себе локально С какой скоростью все будет работать получились Такие цифры вот один процессовая сила и в себя закачивается скорость 1 Гигабит ну и соответственно мы с ближайшие реплики качаем Ну мы опять уперлись возможности то 2 Гб но здесь интересного работает хитро он берет метаданные объекты режет его на части ренджами начинает параллельно к себе все выкачивает в принципе все норм Просто такая интересная особенность здесь мы потребляли два ядра опять же у нас всё уходит на чек-сами Кстати опциональный мы просто его форсируем чтобы детективные и хорошо Ну много соединений это как-то прикольно но сколько можем выжить в одно соединение Давайте используем эту фичу с представим трудом подпишем URL на доступ к объекту нам я его вернули и курлом запустим скачивание у нас Все Попало на один контейнер вообще себя прокачивает 2 гигабита здесь мы не в кур уже уперлись у нас У этого контейнера квота на входящий трафик 2 гигабита Окей А что если делать еще один эксперимент давайте мы 2 гигабита да и Давайте такой эксперимент Мы возьмем и запустим 3 курлас тем же урлом на одной машине чтобы сымитировать как будто объект Очень популярный Ну получилось такое мы качаем из БС 1,2 гигабита и раздаем в три раза больше клиентов здесь вот видите стрелки клиентам они красненькие потому что мы уперлись в исходящую кого-то контейнера там 4 гигабита Ну почему мы там просто квоты не подняли и контейнер не сделали больше потому что в нашем облаке мы стараемся делать понятные протестированные нагрузочные небольшие контейнеры из кели все числом контейнеров так можно гораздо эффективнее утилизировать ресурсы железа потому что повышается вероятность найти место при размещении этого контейнера В общем Ну цифры такие и мы посмотрели на кэш хит нашего полублоков Ну 60 процентов Все сходится То есть он реально часто ему удается использовать блоки потому что все эти курлы качают примерно а здесь мы потребляли два ядра мер для синтрафайлером Ну из интересного Джесси вообще курил Потому что при скачивании все прокачивается через полблоков хиппи что у нас продакшене у нас уже десятки бакетов появляется их все больше бакеты сейчас текущие размером до 50 терабайт и до сотни миллионов объектов Ну средняя нагрузка нагруженные баки то это кластера Правда Ну то есть это кластерапи за ними кластера абсов до 6000 rps доходит но пики Адские бывают время ответа на получение метаданных Три девятки за 15 секунд мы перевели уже все эти продукты на наши Стри и у нас есть куча внутренних потребителей У нас есть свой репозиторий рпм-пакетов он хранит все гастрит статика раскладывается через острие вот ну статики в Одноклассниках много мы храним артефакты автотеста в S3 Это один из самых крупных бакетов там вот всякие картинки при запуске куча автотестов все это сверяется там куча логов всего такого артефакты профилирование Android приложений лежат в S3 про это есть все доклад Кирилла Попова наконец профили непрерывного профилирования в продакшене лежат в S3 это недавний доклад Андрей пагина с коллегами Мы в нашем облаке профилируем все джавы приложения строим хитмапы они хранятся месяц Чтобы разбираться с каким-то старыми инцидентами когда мы пропустили какую-то проблему Ну из недавно запущенного русского тоже работает поверхностно Спасибо за внимание Я готов ответить на ваши вопросы Добрый день Меня зовут Александр Огромное спасибо за доклад очень интересно у меня два вопроса Первый простой смотрите Я так понял что решение у вас завязаны на ландшафт вашей инфраструктурные да А если какие-то планы по предоставлению коробочного решения например в рамках импортозамещения и сразу можно тогда второй вопрос чуть посложнее это вот вы говорили про папки про то что у вас путь разделен слышами но с точки зрения S3 протокола это обычный дилеметр и я в прамедилиметр выбрать вообще любой что если я захочу выбрать делимптором точку или там не знаю другой любой символ придется ли вам перестраивать всю систему или это как-то можно сконфигурировать спасибо спасибо за вопрос Давайте со второго начнём здесь есть два две вещи во-первых мы пока не видели кучу систем переключили кучу сами делали Кому бы это потребовалось А во-вторых вот в этой схеме эта схема э позиционирования вот когда Мы путешествуем по этой иерархии нам ничто не мешает на самом деле путешествовать до другого делимметра мы можем не останавливаться представляете мы построили Вот вы делаете LS Record LS файловой системе рекурсивно Говорите А я хочу не послышал это же не мешает ЛС у путешествовать по иерархии папок и строить файлы до первого делимитора кастомного Да да мы будем клеить и копить их Да но мы будем идти Останавливаться не надо лимитом будем продолжать идти пока не встретим кастомное время указаны То есть это такое траверсал общего назначения Но зафиксировано именно способ ветвления сейчас Ну и не будут работать оптимизацией типа там вот когда мы префикс там определяем из него перенес вырезаем Вот это не будет работать просто будем запускать как бы рекурсивный обход а коробочное решение коробочное решение у нас есть VK Cloud там сейчас свойств 3 Но что-то может поменяться наверное просто рано об этом говорить пожалуйста вот там Простите это все-таки не в Open Source будет это если будет только как сервис у нас обс него pansource и New Skyline они очень сильно на инфраструктуру завязано да Меня зовут Иван Спасибо за доклад мой вопрос немножко не про ис-3 Вот вы сказали что познакомились со Сток континьо не приходилось ли использовать какой-то бэкент который используется continuo закрытый Engine и соответственно решали вы как-то проблему Что Джинкс не поддерживает 100 continue такая идея была Мы думали если бы мы всерьез рассуждали ставить ли там фронтен какой-то еще дополнительный но у нас все уже реализовано кэширование внутри у нас ваннево поддерживает ssl то есть ради только ради этого не хотелось а на самом деле 100 continue немного времени заняло здесь скорее был прикол я первый раз увидел и подумал Да это никому не надо Что это за странная штука Нет первый же клиенты куда 100 continue прислали мне Ну просто если использовать прокси-пасом отдаёт 100 continuo сразу Спасибо я бы раньше Да дальше раньше смотри там два вопроса один от темного чувака из зоны второй от светлого сейчас вот посмотрим как второй вопрос У меня всё-таки Да огромное спасибо вообще прям на все почти вопросы мои ответил Светлая вещь но есть нюансы вопрос первый Ты рассказывал про версионирование как замену бэкапов вот хочется понять это наверное какая-то вообще история про обс не про ис-3 Но первый вопрос мерили ли какое верхад если мы допустим отказываемся от поддержки версии имеем нормальные там бэкапы то есть вот можно или как-то эти истории сравнить а второй очень понравилось что там Nexus и прочие вы получается приходили прямо вот к продуктам и говорили надо пачить чтобы Мы работали это прямо вот правда в жизни работает ну у нас очень много запечатано правда то есть мы мы там тот же волтфоркнут как происходит вы развернули в продакшене тому уже проблемы там уже реальная нагрузка вы ну завели еще там никто не реагирует вы сами диагностируете фиксите Ну релизите свою форму тую версию и дальше отправлять это в обстримы через полтора года кто-то видос Ну вот приходится так И здесь тоже Проблема была бы что разработка как бы заметила эффекты уже что-то там не публикуется назад откатывать уже не хотелось стало понятно что фикс это простой довольно Вот про первый вопрос так что там было-то Ну да вот значит первый мерили какой будет если мы все-таки версии не поддерживаем Ну вот альтернативный сценарий Вопрос такой ответ такой у нас ABS реплицированный отказа устойчивый есть S3 которые хранит данные в обсе дальше альтернатива Либо мы делаем еще одну систему хранения адского размера который в котором будем переливать данные S3 Либо мы в S3 включаем версионирование версия нас уже спасало несколько раз когда люди по ошибке что-то удаляли но они-то не знали что версионирование включены это может достать всю историю версии Удалить это Delete Marker и объекты воскресает нет так как оно в баке по умолчанию не включено но у нас заинфорсина его нельзя выключить даже вот такого сравнения сколько там лишних версий хранится у меня цифр Нет на самом деле перезапись она не такая частая скорее создается когда люди что-то удаляют Ну и под ним лежит исходная версия нет такого что куча версии с контентом именно Копится таких кейсов очень мало и вот еще вот да небольшой опрос про взаимодействие с продуктами а не было вот проблемы с тем что Копится кусочки мультипартов не за компличенные потому что кто-то не умеет работать там сестре не знаю но мы везде в гайдлайнах пишем чтобы люди включали Бакет lifecial policy вот помните который помимо аборт мало типа плод и ну это Ну пока вручную начекаем в принципе ну это политика она приходит через месяц например ну дропает сегодня завершённое Ну вот такой проблемы нет Там если внимательно смотреть раскраску этих колонок видно что мол типа арта плодах не может быть больших партий там ключи по-другому устроены Ну времени не было про это поговорить спасибо Я уже хотел сказать со следующей минуты общения будет платным пожалуйста первый ряд Добрый день спасибо за вопрос который есть Ну контейнер на который лимитируется там все такие контейнеры между собой Кошевого обмениваются или нет это ну Хороший вопрос но пока нет он такой очень кратко живущий десятки гигабайт но через него может ну все меняться в нем за несколько секунд и второй вопрос один дубликации при совпадении если разные блоки но хэш совпал все разные блоки Но это вероятно совпадение ша-256 и они должны быть еще одного размера эти блоки мы на это пошли Да но мы еще будем считать что первый блок который мы залили мы его будем раздавать Но на самом деле у вас много в криптографии сломается Когда коллизия ша-256 начнутся но и не нулевая вероятность Может на 512 перейдем Здравствуйте извини надо ближе 256 это криптографическая функция и коллизи в лоббах это очень маловероятно в конце февраля вы стали поддерживать еще четырех функции в том числе сервисы 32 и какая-то реализация от Гугла которая заточена ну под какие-то конструкции Судя по морду хорошей функции по вашему слайду пробовали ли вы других функций чтобы менять его формат Спасибо друзья Спасибо Это был великолепно Спасибо Вадим кого мы поощрим за лучший вопрос какой был самый ценный самое сложное тогда пока думаешь Прими приз от конференции спасибо Так мне кажется Давайте вот мне понравилось вопрос товарищи Светлом светлый Да светлый парень получает сумочку с призами"
}