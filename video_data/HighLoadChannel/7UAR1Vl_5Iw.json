{
  "video_id": "7UAR1Vl_5Iw",
  "channel": "HighLoadChannel",
  "title": "Поисковая система Одноклассники.ру / Алексей Шевчук (Одноклассники)",
  "views": 610,
  "duration": 2603,
  "published": "2017-04-22T14:45:51-07:00",
  "text": "о боже паника как разобраться в этом море фотографий как найти своих одноклассников как выбрать лучшие фотки об этом в докладе алексей шевчука и миллионов музыки и другого контента на пике в нас пять с половиной миллионов пользователей онлайн в секунду происходит очень много всего в том числе более трех тысяч поисковых запросов от пользователей что мы индексируем на данный момент мы индексируем конечно пользователи это ваш главный самый большой яндекс музыка видео группы подарки ну и так далее все это все что ну или менее легко индексировать мы пока не яндексе рамки этого примеру форумы групп новости то есть где контента очень-очень-очень много потихоньку к этому приближаемся и я расскажу как раз это сделала я не буду рассказывать про тонкости как мы там ранжируем как мы обрабатываем текст то есть все это у нас сделано достаточно примитивном уровне как правильно сказал андрей то есть ну там чем глубже в лес тем больше дров больше сложностей тяжелее все это как настраивать так и мониторить в три года назад когда встал вопрос о том как быть нам сделать поисковую систему у нас уже работал поиск по пользователям работаем like my skill в тот момент было 30-40 миллионов пользователей и они были разделены на 16 партиции то есть рассортированы на 16 бас соответственно каждый пояс как пользователи которых уже в тот момент было достаточно много вредитель со на 16 баз и базам от этого было не очень хорошо они не были рады такому full screen так как за индексировать такое количество полей не поломав запись достаточно сложно вот начали искать наш проект это в принципе теса 9 и 7 java поэтому мы смотрели только java skype source проект опять же чтобы у нас была возможность вмешаться в работу проекта и было проще вас интегрировать наша система как у любого большого проекта нас достаточно много им систем мониторинга статистике конфигурирования и интегрировать для которых есть утилита на джами из integration просто подключить библиотеку гораздо проще чем сделать интеграцию там 100 ужасе просмотреть два проекта втб джавида сейчас есть два таких менеджер open-source проекта там же хочешь ее люсин джифорс откинули сразу так как это такой нацеленный на исследовательскую деятельность проект и сосредоточились на люси начинали тестирование ссор это люси навский в разврат вокруг песен которые можно за конфигурировать под индексацию поиск с кучей всяких потерь и плагинов значит мы протестировали у нас очень не устроил то есть по крайней мере в тот момент нам не удалось без каких-то хаков я нагрузить больше чем на полтора ядра процессора все из за того что там внутри стоял синхронной слов фактически на яндекс вот плюс там исследования кода и тоже показали что написано достаточно посредственно были вопросы крепили кации которая сделала на у них на баш скриптах то есть тоже как бы это как это мониторить и как это расстраивать потенциально в будущем там на десяток серверов было много вопросов мы на так как у нас был салаг то есть возможность запустить погонять индекса посмотреть мы взяли и на нем очень внимательно изучили как работает люсин то есть что мы тестировали мы тестировали как меняется ее производительность размер индекса в зависимости от количества документов количество апдейтов смотрели как меняется производительность то есть изучили очень очень внимательно все аспекты которые мы с которыми мы можем столкнуться принципе тестировали заняло около месяца то есть не месяц это достаточно много прогнали там тестов но принципе по максим который мы получили документы пытался найти это было 300 запросов в секунду на полтора гигабайта нам яндексе 40 миллионов при загружено 8 ядерной машине притом это не пиковая загрузка то есть вчера ребята с прогресс рассказывали про того что они на сто процентов выкрутили 12 коровы машину это не тестировали мы смотрели где то есть нагрузок повышение нагрузки заканчивая где мы видели что начинается значительно деградация вот потом мы сделали прототип уже на своих разборках и там мы сняли порядка 800 запросов в секунду применив исправив те проблемы которые мы ранее нашли в соло сейчас я вкратце расскажу о том как устроен алисе принципе мне опять же очень много рассказал документа приходят в на индексацию и разбивается на термины в рамках одной транзакции мы пишем один сегмент да то есть вот верхний приветствие чертой разделена и в этом сегменте у нас идет своя нумерация документов то есть вот видно что термином на разбили на блуждает и у нас для каждого термина указан номер документа в котором этот термин встречается также есть отдельные файлы там с постинг листами с хранимыми значениями это важный кстати момент потому что я думу во всех inverted индексов хранимой значения хранятся отдельно и соответственно вы не можете это создает проблема если вам нужно что-то перепроверить например айди пользователя в поиске онлайн авто есть онлайн сейчас или нет вот такие группы файлов по ним идет поиск соответственно следующий коммент у нас новая группа файлов если лисин видит что у нас накопилось слишком много мелких файлов она их сливает вместе у нас получается новый сегмент старый удаляются новый соответственно используется для поиска на следующем этапе работы с индексом значит за 3 года использования лисин of protection мы сделали собственную репликацию то есть это одна из первых вещей которые мы делали которое позволяет нам во первых удобно конфигурировать как куда расходится яндекс имеет достаточно строгий мониторинг того как он меняется с как меняется снова структура индекса статистика там его размер количество файлов мы контроле видим на на графиков сколько у нас перекачивается файлов сколько это занимает на какие сервера то есть это все очень очень подробно лаги руется для такого проекта это очень важно потом мы сделали хранения индексов памяти когда мы тестировали сначала solar потом уже свой прототип мы перебрали много вариантов во-первых это конечно чтением индексы с диска там там все совсем плохо совсем медленно мы попробовали перетащить индекс нарам drive вполне логично и простое решение уже лучше мы попробовали осиновский способ хранить индекс happy он на таких индексов и количество запросов работал очень плохо мы также попробовали решение которое мы используем себе активно в кэширование то есть когда информацию которая нам долго надо иметь в памяти мы помещаем в архип то есть у него есть возможность переместить файлы в честь памяти которые управляются вручную ну примерно как это делается все про это сегодня будет рассказывать мой коллега пан ген ближе к вечеру если вам это интересно обязательно сходите значит на победил самый простой вариант просто закачивать файлы в память в виде buy тариф ну то есть если у нас есть яндекс запустил частом самая большая портится это 7 гигабайт состоит из топалов ну значит вот мы эти сто файлов вот батареями затягиваем в память что получается так как объектов на все равно остается всего лишь что это никак не создает проблем двигаться более того то чего можно было бы опасаться более того они сразу сдаются выученный решен то есть мы никак не бьем по тому как рабочий работает garbage collection и у нас полностью отсутствует любой overheat который хочешь не хочешь создает библиотека работы с файловой системой ввода-вывода ну то есть там есть сколько таком случае поисковых запросов мы очень делаем много видов маленьких тестом этот как рассказал андрей мы читаем по бантиком там по по кусочкам но это все убираем и над ближайшим конкурентом этот вариант выиграл 30 процентов что на таком проекте очень очень существенно то что именно время поиска сочи с записью мы ничего не колдовали она нас идет на диск стандартным способом там более чем есть запас у люси ничего придумывать не надо позже уже делаю поиск музыки мы столкнулись связи с резким изменением требований с проблемами производительности из-за времени загрузки хранимых полей люсин 24 который мы до сих пор используем это достаточно тяжелый процесс который к тому же генерирует много ненужных нам объектов поэтому мы и переделали это и сейчас мы здесь и реализуем ну допустим если нам надо стать long мы специально при варке передаем pointer вот где этот лед начинает хранится вот в этих байдары огромных и сразу учитываем в лонг без каких-то промежуточных объектов с меньшим количеством всевозможных переходов и так далее то есть это очень это вниз когда была применена на просто вот индексы тот же юзер для такой проблемы вроде не стояло это снизило нам время gc в два раза то есть это очень очень большую нагрузку создавала золото пока мы реализовали наши разные поиски создали не знаю десяток разных свои какие-то запросов и модификации существующих она лазер и понятное дело были сделаны это менее я думаю вам интересно значит как был переход как я говорил уже поиск шел бой массу спелом это 16 баз и работал он уже на момент запуска где-то 10 16 секунд обычно запрос отрабатывал что для кнут совершенно неприемлемо плюс создавал большую нагрузку когда мы перенесли индекс на новую систему то нагрузка на базу и потом более чем в два раза что несомненно хорошо значит принципиальной архитектура у нас есть presentation player то есть который занимается только приемом запросов и рендерингом есть также специализированные сервисы как там общий lair бизнес-логики так и всевозможных спец сервис и как тот же самый поиск до сервис меняет какую-то из entity который мы индексируем он также отсылает модификацию на индексируемый сервис сера notification она отсылается асинхронно через очередь то есть если яндексе какой то момент будет недоступен то там теоретически заложены что в чине несколько часов его может не быть все скопится в очереди как только он поднимется на него все это до гонится в яндексе сервисе отдельный процесс на отдельной машине который занимается исключительно подготовка индексов ничего больше то есть его задача сделать яндекс у него внутри есть еще несколько череде этой чтобы тоже если вот он только-только поднялся значит на него прилетает там запросто несколько сотен тысяч запросов на то что у меня потом вася поменял фамилию маша переехала в другой город значит то чтоб он не смог там стоит очередь 1 до которая идет в локальную базу данных с поисковыми профилями ну этом случае группу нас были тоже профиль игру берет сохраненную там версию и применяет обновление и потом ставит следующую очередь которая запишет в яндекс яндекс во время комментов а немножко замедляется чтоб это не блокировала процесс применение обновления то есть там тоже воткнули отдельную очередь зачем нам понадобилось вот это локальная база данные для поиска пользователи мы собирались трех видов is clear bass то есть ввиду того что объем данных нас очень большой у нас есть балы с пользователями есть база с историей изменения профиля есть базы не хранятся девичьей фамилии то есть чтобы это все вместе в одно место заняла достаточно могли бы я вижу простой муки в одну базу это не запихнуть вот чтобы если нам первый понадобится индексация то есть полностью пересоздать индекс например мы меняем там какой-то на лазер чтобы они не повторять это тяжелый процесс который достаточно сна может создать стресс для систем портала мы сделали базу дано где это все сложно мы один раз закачали дальше это там все собирается поэтому мы можем взять и в любой момент открыть новый дескриптор индекса туда все закачать разумеется падает и которые приходят они применяются в новые в старой только он будет готов мы на него переключаемся рассылаем по серверам репликация индексе ртс сервер как только у него есть новая версия сер индекса он начинает проходить к вере сервисы который занимается исключительно выполнением запросов на яндексе и смотреть кому чего не хватает у нас поменялась структура файлов мы увидим такса этому не хватает этого это этого закачиваем потом переключить так эти уголь и то есть полностью контролируем процесс и сервиса как я уже говорил они держат индексов памяти + копия на диске чтобы мы могли подняться после крыша то есть у нас там в случае что торнадо налетал и сносило крышу и дата-центра то есть вполне реалистично в такой ситуации если яндексе расчет не будет жив куэрри сервисы подниматься у них есть версия индексов от понеслись обрабатывать запросы часто но на таком объеме имеется немножко другая схема есть которая кажется может может показаться более надежной это когда вместо эликсира у нас есть при процессов то есть машина которая миллиона путин получает обновит составляет новый документ для индексации рассылает иванов секторе сервиса то есть каждый камере сервис тогда будет сам писал этот индекс окажется более сложным во первых он не решают проблему того что у вас все завязано на какую-то одну или две машины которые готовят индексы на во вторых гораздо сложнее становится контролировать консистентной потому что один кларе сервис упала как вы потом разберетесь какие документа туда достались какие нет случае сотни файлов которых есть четкие имена это все очень-очень легко реализуется горе сервисы значит они выполняют запросы и никуда не обращаются запросах не приходят от search processing system и который получает от презентаций остаётся задача все же processing получить запрос понять что его результат для этого пользователя вас еще не запиши rowan сходить паре сервис выполнить его значит запиши ровать сходить потом в entity кэш достать все данные необходимые для рендеринга дать результат на презентацию это основа на ней мы запускали в этом группы и сообщества и сразу скажу ваша была ошибка мы для преемственности взяли это же там создали локальные базы данных лучше кум этого не делает вот ты сам это необходимости не было и мы больше получили проблем нежели решили их за счет преемственности дальше эксплуатация тестировали мы первую версию тоже очень внимательно the senate сначала прошло даст прошла достаточно строгие синтетические тесты потом насколько тупо работала в параллель с старым поиском поисковый запрос от уходил туда и туда и мы смотрели как на реально вопросах это все работает потому что хорошо придумали нагрузочное тестирование вы все равно не придумывайте то что придумывает пользователя то есть выдержать там трехкратное она отлично нагрузочное синтетическое тестирование потом тестирование в фоне это не гарантирует что первые несколько часов пользователя вам не положит систему потому что они придумают такие патроны увидев новый уай особенно а если это еще и изменение и который вам все испортят значит при тестировании у нас были вопросы к тому как работал garbage collector было видно что что то не так но хотелось быстрее запустить из-за той же нагрузке намазом и это запустили через полгода нам это почти чтобы вышло боком потому что в результате каких-то изменений на портале dos активность подскочила и это проблема начала очень стремительно прогрессировать найти ее быстро production системе оказалось гораздо сложнее там была ошибка общем-то sperry открытием индекс достаточно то есть выловить на продакшене это было мы могли гораздо быстрее это и без с меньшим стрессом дожать нагрузочное тестирование потом следующее это если вам вы хотите чтобы у вас была возможность например при создать индекс делать это регулярно у нас это происходит 70 размера индекса раз в неделю до от 1 день до раз в неделю то есть но если маленький индекс можно каждый день и переливать работает с трассы это не создает чего мы не сделали сразу мы подумали что у нас все очень надежно у нас там классную очереди мы не будем делать синхронизацию никакую со стрелами чтобы нагнать потерянные данные потому что в принципе то что мы сделали хорошо какие-то изменю потеряли все равно пользователь производит какую-то активность придет отдает мы увидим что он не в яндексе он догонит на самом деле со временем у нас накопилось достаточно число пользователей которые все же не дошли до индекса и нам пришлось сделать эту синхронизацию сейчас у нас есть возможность ее про графикой на нам мы видим что это это нужно например был какой-то большой сбой значит следующим этапом развитие система это был мгновенный поиск и применение социального графа мы достаточно много знаем о вас если у нас пользователь то есть мы знаем всех ваших друзей всех с кем бы вы могли быть знакомы то есть друзья друзей вашей группы группы которых вы не состоите на состоят ваши друзья это то что можно так чему скорее всего вы будете очень часто обращаться для этого мы усложнили на всю нашу search processing system теперь она при получении от вас первого запроса шла соответствующей системы портала и собирала ваш по для вас временный индекс то есть мы выбрали инвариант с временным индексом и потому что во-первых это позволяет ваши первые запросы пока вы не набрали много полностью отдавать с него не создавая нагрузку для больших индексов особенно префикс ный поиск на больших индексах это очень очень очень тяжело и плохо вот второе такая эта фича потом много где начала использоваться то есть возможный подсказки и друзей поиски друзей в тагирова ней я даже не знаю все места где это сейчас используется вот эти вот под индексы и старый вариант да то есть было просто кэшировать все интересные фишки и передавать их как hand и большие индекс м но как я сказала это бы создало дополнительную ненужную нагрузку по мере того как росла нагрузка раз объемно на поставку с тем что вот эта инициализация поискового вашего персонального маленького индекса она занимает все больше времени поэтому мы еще другие проблемы это немножко переделали важная вещь все больше стало появляться каких-то маленьких запросов на подгруппу данных до из вашего из вашего полного персонального индекса и каждый раз загружать его полностью становилось все больше все дороже и дороже поэтому придумали понятия схемы то есть когда приходит запросам первым делом говорит я вот мне нужно от это схема френдс не френдс ладно там не знаю группы это значит что в параллели мыши дублируем сбор для ваших групп групп ваших друзей и дальше переходим на следующую интересную фразу где мы говорим значит нам нужно собрать вот сделать такой поиск по вашим группам по группам друзей по группам на портале это опять же уходит на исполнении у и за право на данный запрос дальше имеет выбор но может ли бы сказать white owl я хочу дождаться белый редис получить все результаты сказать что мне нужно мне хватит ответа на два запроса 3 хорошо не придет . вот или дождаться вообще вот это очень актуально в лайве поэтому товарным поиски что не нужно семь результатов за то есть если у нас есть результатов скорее всего мы можем уже их показать пользователь может быть они подойдут не подойдут будет убирать дальше получать следующие результаты когда условие выполнено мы соответственно делаем редис результатов выбираем из них только нужные молотом исходя из скоро из факторов то есть достаточно простая простой алгоритм как мы это выбираем вот делаем соответственно поход я нарезал то есть на entity кыш и чтобы загрузить всю необходимую информацию для рендеринга и возвращаем в принципе в этот момент еще может не закончиться инициализация вашей схемы да если не было условия white он еще на слайде написано то есть то что отчасти что вас побудило то что статистика показывала чтобы быстрее всего собираем ваших друзей но при этом дольше всего ждем запрос из юзер индекса при этом больше всего у нас собираются сообщества а из индекса мы получаем итак на этом одна две миллисекунды максимум то что кстати ласкать на следующем слайде ваша сессия живет где то 10 но по списку минут где-то 10 минут из моего создали вы им пользуетесь 10 минут не пользуетесь выкидываем поддерживать для сотен миллионов пользователей консистентными тот маленький индекс это было бы нереально как я говорил уже что даже большие центральные индексы консистентными поддержать достаточно трудоемкая задача эффективность кеширование хочешь нужно кэшировать не нужно вы начинали разработку мы думали что каширование очень нужно очень-очень важно все-таки запросы будут повторяться реально сейчас мы пишем только 5 процентов запросов от пользователей это запроса в которой который мы точно знаем что и очень часто повторяются да то есть но может понадобиться следующий чанг и при этом нам их очень дорого получить ну то есть дорого это больше там 20 миллисекунд вот это уже дорого то по нашим меркам что большинство запросов на срабатываем там до 10 миллисекунд вот и при таком подходе у нас в cash cash хит получается где-то для 6 60 процентов результатов по тому же поиск музыки там есть статистика только запросов топ 1000 за день до даже не за какой-то интервал чик вот пока есть пока не обновилась версия индекса то менее двух процентов из кеширование там это более создание проблем они решения их там уже в принципе индекса вас память это уже фактически кэш поэтому вот если бы сейчас я делал первую версию я бы и сделано вот так я бы выбрал этот промежуточный уровень и ничего не кашира вал то есть эти со сном основная бизнес-логика просто бы обращалась к варе серов сам отец загружала винтить кэширование нагрузка первой версии как вы видели у нас были каши которые кэшированные результаты по пользователям но так как для тех же пользователей кэшировать generic запрос нет смысла да то есть вы ищете иванова вы петрова то есть разброс очень большой у нас-то работала все примерно так у нас были ремоут каши в которых сохранилось сохранился как запросов позже эти поисковые персональные яндексе соответственно в отличие от других проектах у нас очень строго следит за резервированием и запись каждого каша шла на две машины чтения разумеется с 1 по мере роста проекта роста объема данных вот эти к 16 становится бутылок да и можно было добавить еще парочку и таким образом это решить но это было видно что это очень неэффективно плюс очень очень значительная часть трафика для сервисов она была именно каша значит что вы сделали каждая пользователей на портале есть домен это вычисляется из юзера иди и благодаря этому мы можем распределить пользователей по сервисам то есть за каждым пользователем фактически закреплен сервис конкретный сервер до который он обращает соответственно когда начинается пользоваться поиском он идет сразу на него этот сервис он создает поисковый индекс и делает его сохраняет его копи но здесь показана на еще одного сына своего заместителя который четко прописано в конфигурации реально мы делаем копии две копии до чтобы выдерживать более серьезные сбои зачем вообще нужны перенос если произойдет беда пользователи начнут пользователи перебросит на соответственно заместителей и пришедшая к ним пачка людей которые уже ищут и соответственно необходимость срочно создать для них индексы может повалить и следующий сервис на раз там уже это опять же создаст стресс для систем которые обеспечивают создание этого индекса таким образом в случае сбоя пользователи обычно ничего не замечает сейчас тогда количество систем у нас выросло более чем за десяток мы столкнулись с большой проблемой это управление по папкам этим значит они запускались мы весит абсолютно ворона решили что новых лучше изолировать друг от друга то есть каждый индекс совершенно независимо не в них не было общих сидоров это позволяло на первый запуск при каких то изменениях и заканчиваться на тестирование как платного сервиса вторых сбой и под проблемы перегруза 1 никак не влияли на 2 но когда от вас уже сейчас дюжина систем который все обеспечивают поддерживать их консистентными во первых надо фармить и на тестовом формате уже большая проблема потому что лично забываешь где-нибудь что-нибудь обновить вечно где-нибудь сидит какая-то настройка или кто-то постучится опять поиск не работает плюс на продакшене тоже получается не очень интересная ситуация когда какие-то сервера нагружены достаточно по нашим меркам сильно какие то вообще простаивают мы пошли сейчас находимся в процессе объявили объединение всех систем в унифицированы то есть пока у нас есть один яндексе для которого есть запасная да значит рассказал про яндекс я забыл упомянуть что вот эта поисковая база у нее есть дублер над она тоже с резервированием и на этом дублере мы можем быстро поднять замену прилетевшим у yandex.ru в случае сбоя то есть это тоже было продумано честно аналогично сейчас у нас есть центральный яндексе на него слетаются все необходимые изменения он готовит все индексы это уже создала проблема потому что какие-то изменения в нагрузке на индексацию одних индексов начали влиять на другие вот но зато очень сильно упростилась управление всем кластерам потому что все мы все клэри сервера они абсолютно унифицированы и зависит допустим перри балансировать систему что уже приходилось делать занимает там надолго дождаться пока зальются индексы на на ту машину которая предполагается завести заходим в конфигурацию если она еще не в реплике очень пластырь и соответственно заводим ее туда меняем рипли kenshin mapping влогах видно что индексы загрузились вводим в продакшен все очень быстро просто и удобно но вот проблемы которые вызывают такое смещение они тоже имеют место быть и точно не надо делать так когда вы только начинаете работать с каким-то классовым систем с новый библиотек мы принципе принципе уже рассказал основную часть сейчас немножко закуски на таком большом портале как наш пользователи часто пишут и просят сделать не простую вещь таких вещей не просит очень много в частности вот пользователи поиск пользователей групп то есть ну вот как бы выжег можете посмотреть из пользователей групп все просто но сделаете по ним поиск проблема в том что показать вам несколько страниц это просто мы идем в базу которая хранит связь группы пользователей потом достаем пользователь ну реально не из базы is easier кэша его мы показываем и сделать опять же для группы то на тысячу даже человек это можно но когда у нас значительное число групп на сотни тысяч и даже на миллионы пользователей уже попытка это сделаете она в принципе сразу это будет сразу нему поэтому мы модифицировали поисковой сессии то есть мы создали новую разновидность которых первую очередь живет дольше во вторых тогда из какой-то группы приходит запрос на поиск мы начинаем собирать для них индекс мы его собираем пунктом 500 миллисекунд после этого если мы его не собрали еще полностью мы делаем в доступном для поиска чтобы отпустить запрос возможно пользователь уже найдет того кого ищет до при этом параллельно продолжается сборка по что сборка для больших то многомиллионных групп она может занимать этот пункт 10 20 секунд так по памяти может уже меньше может уже больше вот соответственно у нас появляется вот такая схема и тут у вас возникли по связи с этим проблема то есть для больших групп индекс получался достаточно большим и попытка его хранить во внешней памяти привела дело к тому что когда мы получали к ним модификации да и просто запросы вот это вот туда-сюда tyga tyga не я создавала правда нагрузку нога цену и вообще сама вот это вот копирование он тоже создавала само по себе уже не малую нагрузку поэтому для больших групп и групп которые активно очень меняется мы сделали исключение а не какой-то момент большие группы сразу зависают если идет много апдейтов они меняют свой тип и зависают в основной памяти таких групп у нас десятки тысяч на то есть это не там не 10-20 при этом нахождение в основной памяти не создает проблем когда же мы попробовали все группы поместить в основной памяти то уже было тяжело это очень много памяти и много мелких объектов чего не очень любит же а следующие так следующий от реки вещь это поиск пользователи онлайн у нас пять с половиной миллионов может быть пользователи онлайн и пользователя хотели их искать по возрасту из по городу по полу это сделать в первом варианте нам просто было гораздо проще реализовать мы это сделали как запрос в яндекс пользователей и дальше мы результаты из результатов пытаемся выудить тех кто онлайн это достаточно тяжелый запрос потому что нам вот то что сформулировал пользователь может попасть там много миллионов пользователей да и из них лишь небольшой процент white alive соответственно нужно сделать умный загрузок идентификаторов что занимает время и создает опять же нагрузку вот из них учились надо выдать ран ран дома да то есть мы не можем их выдать релевантным это все не имеет смысла для этой фич сейчас попробовали перейти на вариант что мы делаем отдельный индекс тока с теми кто онлайн работает быстро логика простая никаких трюков не опять же на пике в него прилетает этот 200000 изменений минут это как в их потери статуса онлайн так и приобретение статуса я онлайн в случае если удаление то это наделит который кстати тоже имеет индекс они очень любят случае если это онлайн то нам надо еще успеть быстренько его достать и заметите каша и за индексировать плюс значит такая схема очень сильно зависит индексируются сервер то есть если в первом варианте случае падения яндексе ра нас все продолжают работать как работала у нас есть вполне еще консистентной индекс пользователей по которому мы еще у нас ничего не волны здесь у нас в яндексе только 5 5 миллионов мы их под фильтром то есть мы перепроверяем конечно что они онлайн или не онлайн это уже не стоит нам дорого на если мы быстро не починим индексируются сервер то может получиться так что на многие запрос уже некого выдаваться не будет просто те пользователи которые были онлайн они ушли они вы фильтра вы ваются новый не попадает вот это все сейчас пожалуйста хорошо уложились сейчас пожалуйста ваши вопросы там да девушки даже повторим до тут да вопрос про репликацию срыв сравни если сил значит в ссоре человек немножко если у вас если индекс памяти дата репликация на самом деле будет работать в схеме мастер slave потому что реплика нужно от реплицировать на другую физическую машину если вас один из нас понятное дело все все просто на в память никаких репликаций нет мы почему мы отказались от салона да еще была вторая часть как я уже говорил нас не устроил ни перформанс не качество кулада на тот момент от три с половиной года назад я следом они уже сильно подтянулись любые вопросы так стабильность потому что не скорости работы ремонте нг интерфейса для нас достаточно критично тогда мы использовали джонс ремонте сейчас мы используем очень эффективное свое решение которой тоже все не будет еще рассказываться вот и они позволяют значительно повысить производительность server server это мы были вот дополнительный фактор а если вы маленький проект вам определённо не стоит андре правильно сказала заморачиваться вы берете золото или там его конкурента прямого elastic и поднимаете на их кластеризацией полностью в вам их должно хватить когда вырастите ну сделайте как сейчас еще вопрос проблем базы размеру миксов пользовательский index spot 15 гигабайт если он не был ни был бы порционирования льна он разбит на индексы где-то там по три четыре гигабайта на орла спасибо алексею за доклад вопрос такой скажите у вас получается на яндекс устроиться на каждую какую-то характеристику свой ну предположим пол это свой индекс там какие то там личные данные там помаду по городам это свои индексы и вот вопрос такой если нужно найти людей по сразу по нескольким индексом то есть вы делаете это параллельно асинхронным получается потом склеиваете это все вместе и вы даете уже результирующий набор ну то есть нужно из москвы в сектам девушек там до 20 лет хороший чем вопрос а значит они действительно грамотный вопрос как мы это делаем на самом деле вот случае ну это актуально в первую очередь мы на каждую группу данных у нас есть своя свой индекс на то есть группы люди музыка ну и так далее дальше люди самый большой самый сложный и действительно имеющий такие неприятные характеристики как да там страна вот возраст поиск пользователей сейчас разбит на 4 партиции это по полям да это нам позволяет случае если указывал полу искать только по половине индекса второе вот разбиение это разбиение по регионам у нас мы поделили постарались поделить это более-менее равномерно то есть если вы указываете страну у нас сразу с уже еще больше сужается количество и миксах которые нам нужно запросить соответственно да то есть вот в этой системе дисковой системе она смотрит какой запрос и вы разбивает и он уходит на параллельное исполнение на разные сервера собрали отранжировали выбрали чего показываем так далее вот если не участвует индекс пользователь это да это 10 миллисекунд если участвует индекс пользователь сейчас это в районе где-то со средней apple 40 миллисекунд вот здравствуйте спасибо за доклад интересный и я хотел спросить через какой период времени попадают обновленные данные в яндекс ну то есть грубера создать много пользователь когда он себя увидит в поиске для пользователю это пять минут нас выбранным трава для некоторых index of the 30 секунд то есть мы опять же смотрим по нагрузке интенсивности и влиянию это большой индекс период кровать достаточно дорого реплицировать часто дорого поэтому ну и опять же задержка 5 минут оно вряд ли что-то решит мы не новостной сайт автоматически да у нас происходит программу спасибо если еще менялись вопрос это можно подходить я постараюсь ответить все всем спасибо"
}