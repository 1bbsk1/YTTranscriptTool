{
  "video_id": "m0FUAsAzxz0",
  "channel": "HighLoadChannel",
  "title": "Что мы знаем о микросервисах? / Вадим Мадисон (Avito)",
  "views": 17861,
  "duration": 2188,
  "published": "2018-11-19T02:39:43-08:00",
  "text": "всем привет давайте что все не выспались потому что рано утра и всё такое и просто скажу что не авито на одной из лучших компаний для технарей где топовые технологии где пик всего самого модного современного где по-настоящему ценят то что мы делаем и перейду непосредственно к теме доклада смотрите я достаточно много рассказываю про то как разрабатываются микро сервисы и сейчас немножко решил неком таком формате гайдара сказать как вот с этими микро сервисами на каждом этапе нужно какую информацию о них знать собирает каком таком структурированном виде для того чтобы дальше это можно было поддерживать и последнее время я очень часто слышу ну так и проблем это сейчас вот мы распилим это монолитно много маленьких странных кусочков это за тягой чем всю эту в губернаторы будет у нас счастье здорово только вот почему то в реальной жизни так не работает вот эта картинка это микро сервиса варит а то есть это именно разные сервисы и связи между ними то здесь чуть несколько сотен этих сервисов и в общем-то уже сейчас как бы поддерживать всю эту штуку это не самая тривиальная задача вот если там чуть покрупнее то здесь можно увидеть что там у каждого сервиса есть куча куча взаимосвязей и не всегда они нато в явном виде я здесь как бы надо понимать что основные проблемы микро сервисов они в том что их просто тупо много то есть это не единицы то есть это там то даже не десятки это сотни и на вот это количество она накладывает там свои какие-то ограничения вот во первых это много репозиториях то есть если раньше у вас был кот в одном битому репозитории выгнута скачали загрузили ваши уделено там полчаса потупила потом то вы там можете этот код три фактора ведь вы можете сделать там fly and use что-то еще попробуйте тут выключить 500 репозиториев и по ним сделать то же самое облейте опять же если раньше у вас был монолит то так или иначе со всеми кусками этого кода работала вся команда где-то там больше где то меньше ну так или иначе каждый разработчик примерно представлял где что и как когда-то вы переходите в microserver ночью рута все резко меняется каждая команда капсуле руется в своем контексте каждый разработчик думает о своем конкретном сервисе и может быть там немножко соседних то есть единой картины ее просто нет нет нет того человека который бы стал . знаний который будет теперь можете прийти и спросить а чё где вот мне вот это узнать вот и наконец то мне последняя проблема всей этой истории вы фрагментации данных если раньше у вас там была какая-то большая огромная база вы написали там какой-то ключ запрос и там увидели где у вас там с данными что-то потерялось где у вас там они стали не консистентной что-то еще относительно просто попробуйте это в 1000 бат теперь сделать и следующий момент это то что касается инструментария то есть и и дело даже не в том что он его в принципе нужно знать о в том что его ну тупо много то есть это там и егерь из n 3 и там his tricks and с там граф связанности сервисов там вместе у это наконец там мониторинг водка все это еще истории там the notification которая там с лайки в имели у вас документация там расползлась по каким-нибудь вики там вот мы например там в качестве вики используем стаж вот это вся эта штука приводит к тому что да у вас вроде как все работает да у вас там есть мониторинг и над и поверьте мне на то мониторинг здесь не самое интересное поэтому когда вот я тезисы к докладу писал то я на то сразу написал что я расскажу про метрики и про информацию а какую мы собираем а сервиса на мой взгляд это даже самая важная часть то есть вы смотрите вот концептуально схема выглядит вот так то есть нижний слой это биомед он то есть мы прям на реальном железе дни мне в облаках выше этого у нас поверни таз и верхний слой это сервис мышь то вот на эту пару значков это собственно то мости у январь то есть это та самая штука которая нам очень сильно в этом помогает но к сожалению не решает все вот следующий квадратик это мониторинг вот если вы хотите там чуть поподробнее про это узнать то завтра володя колобаев расскажет очень очень подробно про эту штуку вот и наконец последний квадратик и to pass it on the go to машинерия про которую я в основном сейчас я хочу рассказать то как мы справляемся в итоге со всей этой штукой что мы автоматизируем и какую информацию мы собираем то есть вот общий концепт он примерно такой то есть не то неких это три кита то есть это отличная утилита которая берет на себя основные трудности по началу разработки микро сервиса которая помогает разработчику сделать на входе все правильно с и минимальными усилия следующее это некий агрегированный дашборд я потом покажу пару скриншотов то есть это та штука которая как раз решает проблему а кучу-кучу даже бордов разных тузов которые вы все должны знать иметь там 50 логинов и понимать как вам ты из 20 местом в голове скомпилировать все это и наконец asked wars это та система которую мы реализуем в виде неких триггеров то есть на каждые там обязательные действия мы не надеемся что там кто-то там себе тасс джерри поставит а кто-то там не забудет мы все это автоматизированный про это я тоже немножко расскажу смотрите вот это достаточно стандартный pipeline то как сервис начинается от этапа разработки из и заканчивая тем что он там успешно или там не очень живет в продакшене и давайте прям вот по вот этим вот шажочком пройдемся первое это вот noclip уж почему именно так потому что это некий такой это инструмент который во многом заменяет вот тот самый первый push эта история про то что мы очень долго бились тем что пытались наших разработчиков научить делать правильные микро service in the объяснять учить как писать там сначала мы не тесту bernette as a потом там manifesto home и прочее прочее все это приводило к тому что мы каждый раз должны были в грузить очень много достаточно сложные постоянно меняющий информации в наших разработчиков это не всегда просто и не всегда быстро и мы в какой-то момент поняли что это очень сильно тормозит внедрение микро сервисов просто когда у тебя там под тобой земля горит тебе нужен это пилить продукт то он очень сложно найти там хоть об этом две-три недели на то чтобы вот полноценно скрупулезного все это въехать поэтому что мы это здесь сделали первым делом мы сделали очень простую утилиту которая автоматизирует на ту многие стартовые вещи во-первых это создание шаблона то есть одной команды ты говоришь создайте мне новый сервис мы поддерживаем сейчас шаблоны для печки голлинга и по моему python а вот эта дата в режиме никого такого визарда соответственно второй этап который вот это лично утилита помогает это собственно развернуть я в среду под локальную на машине разработчика то есть мы автоматом поднимаем одной командой mini cube мента генерим манифеста home chart мы загружаем их это в этот mini cube и все это делается вот автоматом без участия разработчика то есть мы теперь не требуем от каждого знать вот ну так всю эту механику полностью когда-то мы начинали с того что пытались там в контру men sene это все это описывать и это не работает точно так же история с подключения к базе данных то есть ну там и в этой кличкой utility над одной команды говорим добавь мне поддержку на топаз gres вот и разработчику даже не нужно знать там не и печник не логин и пароль для того чтобы достучаться к этой базе что локально что встречу что в породе пас это полностью берет на себя сейчас мы полноценно поддерживаем позарез на подходит мангой редис может быть там в дальней перспективе ищу будет тарантул причем как бы это развертка уже гнутов отказоустойчивой виде там с пиджи балансирами своей лазером и прочее нта канителью который тоже теперь мне нужно разработчикам следующий момент это лаял сборка то есть вынут из своей доешьте что поправили мы поймали на файловой системе что-то поменялось тут же надо перри собрали контейнеры перегенерируем чарты идут обновили то есть не нужно теперь все это каждый раз делает руками вот ну и наконец последний вот этот момент это генерация авто тестов то есть мы тоже эти болванки генерим там буквально одной командой вот вы очень то кому то и изобильном виде смотрите на как некая эта иллюстрация как вот выглядит типичный диплом сервиса то есть есть там докер файл есть конфиг есть home чарт это хорошо если еще есть стеклом чарт вот этот хлам чарт выглядит как вот такая вот над портянка из кучи файлов там с тимплей томи с чертами с конкретными значениями которые там под разные среды еще конфигурация ну вот и плюс это завязано на манифеста губернатора который периодически меняется формат минут а вот у нас там была например проблемы переехать там с одной версии купер нас на другой том что тупо надо манифеста перекрашивать в итоге мы от этой истории тоже отказались вот велено то собственные файлик на точный простой автомолл и выглядит это вот так то есть обрезало сь но ok сути это очень просто это вот практически все что теперь заполняют разработчики для того чтобы скрыть configure свой сервис что здесь интересно это вот венджи не это смол то есть это тот самый брик который определяет лимиты которые мы выставляем в кубе ри для конкретного сервиса дальше этого смотрите на то переменные среды то есть мы это здесь выставляем дефолтную что мы запускаем там одну копию в stay gentoo мы запускаем 3 вроде 5 все вот из этой штуки ну там и дальше с генерим уже то все необходимые хэлом чарты мы сами создадим необходимые подключения к базе данных и прочие штукой вот это та вещь которая позволила нам именно с точки зрения платформы достаточно прозрачно саппортить тот pipeline разработки и те правильные вещи которые должны быть на входе то есть мы теперь уже можем вот отталкиваясь от этой истории видеть и так и где и как у нас разработчики что делают правильно и что неправильно в чем мы можем им помочь автоматом где что мы можем проверить и вот эти вот нато проверки анин ты у нас постоянно накачиваются именно об автоматическом виде то есть ментов там понятно что на тупо там базовые файлы проверяем вот но точно так же минуту проверяем здесь на при этом наличие документации зависимости и что самое важное вот на этом этапе мы проверяем то что вот в этом вот нато файлики разработчик указал какие метрики нам нужно добавить в мониторинг и валентинка для того чтобы действительно понимать что сервис работает корректно потому что смотрите там проверить там памяти пьют а что про процесс станут они повернуты все остальное это вообще не вопрос а вот понять что си разительно корректно работает это нужно отталкиваться от продуктовых метрик то есть нет и вы что-то для себя в конкретном сервисе это знаете не то чтобы четко понимает что ну тогда вот этот конкретный сервис ведет себя правильно и это не история про одну то там тайм ту респаун сетанта не про количество там-то рпс of и прочие штуки то есть вот эту это вещь мы отслеживаем автоматом на вот этом этапе вот через вот эту машинерию следующий in the moment сбора информации о micro series of эта документация вот наверно этот вопрос сейчас не то плохо видно ваше лицо но наверное вы сейчас улыбаетесь вот очень уж очевидная штука типа ну чё ты всех знаешь что документацию то делать надо вот и кто-нибудь вот здесь может мы-то поднять руку и сказать что ну тогда мы надо постоянно пишем на каждый сервис полную документацию и у нас к ней нет претензий вот почти никто не поднял да и это вот та история которую ну тут опять же на этом этапе достаточно легко можно проверить если у вас есть определенная конвенция вот например ну там вот в какой-то момент за там достаточно большое количество итераций выявили какие-то шаги здесь должны быть там необходимые достаточно комната описание понятно но что здесь важно это не должно быть вот такая портянка текста и и не будут читать это должно быть два три емких понятных предложения о том что делает материальный сервис о чем это вообще важный момент когда вы формируете документацию то должна быть это схема архитектуры то есть должно быть понятно вот в этом месте вы reddy's используйте к кэш или это persistent на и хранилище нам надо мониторить что у этого radisson этот действительно там поесть они валидны или там то нам это вообще неважно есть вот эта штука на прямо на основе вики это confluence соответственно для нас это ссылка на confluence на вот эту вот над архитектурную часть точно так же тут вот в этой документации должен быть кусок на которые там этот ваш не то сосед который review это ваш сервис посмотрит и поймет как его запустить где какие тонкости там например нужно выставить обязательно вот эту переменную среды а вот здесь нужно там на файл уху там файлик подложить что-то еще вот и один пожалуй не самых главных моментов это раздел faq потому что чаще всего в этой документацию лазит не потому что все хорошо и потому что людям это скучно и они решили там повысить свой уровень образования потому что то где то упало и нужно на ты побежать это попытаться найти это может быть кто-то уже про это подумал и рассказал как с этим бороться вот это как раз faq вот про то что он должно быть описанием поинтов этого как раз та самая очевидная часть которая скорее всего есть у всех а вот например про лейблы это вот это очень интересная история из нет и здесь минимум должно быть три лейбла это лейбл ну тут право привязки к продукту например этот там-то биллинг или это там это подача объявления то есть это нато про функциональность какой-то там это узкий скоб и это привязка к структурному подразделению чтобы вы это могли ну ты четко понять какие там то в вашем там unity над вашим там отделе разработки реализуются сервисы чтобы видеть где и как у вас та или иная функциональность уже реализовано может быть это что-то уже есть потому что она там год я лично наблюдал ситуацию когда три разных там группы разработки пытались реализовывать один и тот же сервис который на самом деле 4 уже сделала им для этой истории в том числе дата важно знать владельца то есть его можно там автоматом определяет как это от пытаемся сделать мы это либо просто прописывать то же самое документации и опять же это важный момент документация должна точно так же надо иметь pipeline review то есть как вот вы review & tina то ваш код там призывая ваших коллег точно так же на там и review им и документацию вот и все вот эту информацию мы разбиваем на блоки и отправляем в наш род единое хранилище то есть у нас есть сервис который знает о наших сервисах он еще маленький но постоянно растет вот он называется атлас я дальше буду там нет а ссылаться вот соответственно вот эти вот куски мента разбиваем на значимые блоки на лэйбле там вот на вот это все идут отправляем вот над в единое хранилище дальше это часть которая просит здесь в общем то на там мы подготавливаем pipeline этим сити мы готовим репозитории мы там то решаем вопросы с правами и в том числе как раз мы смотрим вот эти самые пуше для того чтобы найти кто он является там-то omnirom как мы это делаем во-первых мы стараемся для каждого сервиса знать а минимум 2 ордеров если по коду этого нет значит сервис через триггер поставится в карантин что нато ребята у вас снова на вот этом сервисе бас фактор никто кроме вот там петь и про него не знает эта проблема соответственно так либо кто-то должен включиться в ревью на талибана то каким то еще образом найтись как минимум 2 человек вот и соответственно то мы анализируем количество кода в pushed то есть ты можешь там queen тайфик сделать запятую где-то поставить а муж пол сервиса переписать ну там и это различаем и от этого надо me им соответственно авнер ства он соответственно мы эту всю эту информацию точно также мы передаем в единое хранилище и здесь живет интересный момент здесь мы анализируем иммиграции принесли мы видим что в какой-то миграции есть что-то опасное то есть этом alter the apple что-то еще что может нарушить совместимость схемы данных между разными версиями сервисом мы не запускаем автоматом в эту миграцию но и не выкидываем и и мы ставим ее в карантин мы наш единый сервис говорим слушай вот это сервис версии 11 пытается запустить вот такую миграцию стать это v-триггер соответственно срабатывает вот эту вот носки дрын да он подхватывает что когда в роде не останется версии ниже 11 то он с лак напрямую обмерам этого сервиса еще для этого мы собираем кто авнер мы напишем что нато чувак мы можем запустить для себя вот эту миграцию это еще актуальна ничего там не поменялось вот он нам grid да мы запускаем эту миграцию соответственно вот портянка старых метаданных старый с кем она вот таким образом с помощью надо системы сама себя поддерживает следующий этап это как мы вот эту штуку теперь он это запаковываем то есть понятно что он тает это классический докер вот но что здесь интересно это что вот я говорил что вот потому маленькому об том лу мы генерируем автоматом вот эти вот home чарты то есть мы полностью нато сами подкладываем все что необходимо для того чтобы встать же там то сконектиться с базой с протянутой это там какой-то момент как создать например тикет на админов чтобы там в положить что это хранилище нтам neo секретов логин пароль доступа там так базе и так далее на этом женаты этапе мы проверяем там покрытием тентом тестамент окна считаем ходкевич и вот эту информацию мы точно также отправляем в единое хранилище соответственно смотрим если кот кавирадж там ниже определенного порога мы скорее всего сервиса не пустим вообще если он там какой-то грани там и нато поставим сервис некую такую писи мизер ую щую in the циферку которая это будет показывать что если там долгое время это не меняется разработчику на ту нужно об этом напомнить и сказать слушай там твой сервис он не прогрессирует по тестам это плохо вот и это точно также идет вот это через единое хранилище через тот самый скейтборд вот и здесь же на то мы учитываем ограничение по памяти сыпью и здесь достаточно интересный момент я просто несколько раз вот на эту историю наталкиваться решил рассказать это вот прям отдельным пунктом смотрите на там и свои сервисы пишем в основном нога лэнге запускаемых на кубе рн от оси и здесь вот ну то выстреливает интересная штука с гамак props давайте вот здесь посмотрим все это вот график здесь наверное не очень хорошо видно но если у нас вообще без ограничений запущена гамак strokes по дефолту это все ядра на машине и мы запускаем у нас получается что за сорок две миллисекунды выполняется там какая-то операция если же мы оставляем вот гамак строк по дефолту вот те самые 16 и нато при этом выставляем ограничение там в губернаторе что это у нас типа там 1 ядро то он то вот посмотрите разницу сирот две миллисекунды и 643 вот и ну там здесь как бы еще интереснее эта ситуация становится если у нас есть какие-то нато блокировки там например набьют иксах то есть здесь вот это лучшее значение 151 ность наносекунда in the худшее это почти там 15 1000 условно вот ну то-то это вот ссылочка как раз вот ребята из юбера тоже это заметили графике это тоже оттуда из их там то и ешь и номер один поэтому обратите на это внимание вот мы подобные вещи делаем как раз вот на вот и этапе bake то есть мы учитываем вот это вот это размеру тот самый small and учитываем какие-то характеристики нужно поставить причем вот эта библиотечка она умеет это делать достаточно удобным видим на следующий этап это собранную всю эту штуку нужно начать доставлять по средам и вот на этом этапе мы запустили уже этот сервис у нас задача теперь проверить конвенция конвенции у нас нет а там сейчас вот во это не очень много но он-то их список ну то он расширяется и дорастет нам до достаточно большого потому что чем больше вот этих вот соглашений правильном удобном виде который всем понятен тем проще поддерживать вот весь этот зоопарк из нескольких сотен причем если как бы про in point and 10 см это все более менее понятно то например определенные заголовки в запросах это важно при чем вот обратите внимание мы эти самые заголовки отправляем не только одну то в запросе к сервису там напрямую там неважно там все синхронно и асинхронно и не принципиально в этот момент нам там может помочь часть его увидеть grub взаимосвязи но если у нас взаимодействие через шину тот самый event was the мы точно также готов пилот добавляем этот xor сойди какой сервис отправил вот эту информацию и точно так же каждый сервис который принял использовал данные из shinee в этот самый event on the в наш единое хранилище метрика отправят я поймал и винт вот такой то у него ну ты источник вот такой потому что иначе полную схему взаимодействия всех сервисов вы не соберете доната по вызовам а вы это увидите с точки зрения отвод всего pipeline а обработки данных нет вот это вот это очень важный момент который очень часто пропускается ну ты наконец вот этап тестов здесь тоже в общем то над все просто за исключением одной простой штуки я достаточно регулярно слышу вопросы а как воткнута тестировать и внутрь всегда как бы такой капитанский ответ да а тестируйте закрытом контуре но долгое время было ответить ответить нечего на вопрос а как мы это делаем и в общем то сейчас там у каждого ну то есть свои тузы у всех нет больших парней этот вопрос так или иначе решен в одном и том же виде то есть на то есть это некий прокси он записывает реальную нагрузку потом ты ее эмулирует вот например на ковер fly to open source на я штука которая вот с этим как раз помогает а теперь нато интересный момент это нагрузочное тестирование мы его делаем по ряду причин то есть во первых потому что мы ищем некую дельту в производительности то есть каждую эту версию сервиса мы прогоняем через эти тесты и смотрим насколько она поменялась причем мы смотрим от версии к версии это чтобы увидеть не то там слишком большой youtube прасад например сказать нет ребята у вас performance сервиса над упал в два раза давайте вот эти вещи и над и исправляйте такой сервис не пойдет вот точно так же ну ты история когда у вас там за пять версий тоже это случилось там про сад на при на 20 процентов это тоже очень много эту историю мы точно также отслеживать то есть все вот эти вот результаты они уходят на тоф в единый сервис который вот эти метрики собирает и на то она анализируется вот в нескольких вариациях потому что от этой информации мы в том числе отталкиваемся чтобы правильно сделать авто скиллинг чтобы правильно понять что ну ты у нас сервис вообще в принципе масштабируемым и следующий момент это кинари тесты то есть это та история которую в микро сервисной архитектуре так или иначе должны иметь в виду все вот и сын теми самыми генри тестами зато когда мы запускаем там на маленьком-маленьком проценте пользователей есть занятная история есть те кто не могут там все позволить на там за 10 процентов своих пользователей запустить что-то и считает не тонул отвалятся и фиг с ними вернуться есть те кто не может вот авито например не может поэтому это для нас как бы вот это такая история это про то что мы можем себе позволить запустите на там супер маленьком количестве пользователей и это означает что мы должны очень тщательно проанализировать то что мы увидели там на эти 10 пользователях то есть это вот ну некий такой не то ли нет который мы там условно можем себе позволить соответственно мы там одним когда кому-то заходим смотрим на там определенные вещи и дальше мы там то условно через 2 часа мы поговорим о кейм теперь можно то повышать процент то есть это все что мы могли вот этих вот это супер малого количества пользователю видеть мы уже сняли что мы здесь в первую очередь смотрим это те вещи которые завязаны на именно продуктовые ошибки то есть вот то что я говорил что для именно каждого микро сервиса там узнать единой проблемы соотнести это не вопрос но очень важно увидеть именно историю про продуктовые метрики то есть это вот та история когда мы по каждому сервису помимо вот каких-то стандартных вещей помимо того что мы там посмотрим не знаю что у нас там 500 их нету мы посмотрим именно продуктовые метрики и отслеживаем их и делаем это на наших реальных пользователей которые создают реальную правильную нагрузку потому что он и так как мы все знаем там какие вы прекрасные нагрузочные тесты мы не писали синтетическая нагрузка и то есть синтетическая нагрузка без вариантов вот и здесь еще важный момент то есть мы считаем вот на этом этапе количество всех ошибок не важно обработали мы внутри сервиса это либо сервис на таких там то куда то на то вывалил там какому-то на ты видишь вон их не смог переварить и те и другие мы считаем их количество и отправляю в нашем-то а сервис который собирает вот эти метрики то есть на этапе canary тестов мы должны четко понять вообще какое количество ошибок у нас случилось и сравнить это с некой нормы то есть по всем версиям вот этого сервиса мы знаем какое количество ошибок на этапе canary тестов у нас было и неважно обработали вы их там через 35 или нет важно что они случились что нато вам начали лететь какие-то данные которые что-то ломают это что-то поменялось в логике что-то привело вот такому boost und вот этих вот ошибок да вы их можете обработать но это вот это один из важных показателей того что что то где то поменялось и вот это вот надо метрика то что у вас эта цифра скакнула она значима вот причем как бы это может быть определенная норма ну просто на примере с авторизации в него там без там логина пароля долбится соответственно то вы там скажите штата и извини там-то 403 тебе и это будет ок вот а вот если нато у вас вдруг ни один хэш не подошел то это явно какая-то надо проблему ваш сервис вдруг перестал работать следующая интересная штука я впервые там достаточно давно увидел о netflix он но вот именно на микро сервисной 1 тик турин на оказалось что но это вот одна из таких вот это очень важных вещей это тестирование через выдавливание если tanto tanto штука когда мы даем на один instance реальную нагрузку до тех пор пока он прям в полку не нагрузиться при этом мы должны очень точно надо смотреть за его метриками над а затем что он ты с ним происходит и отслеживать что да вот это его реальная полка после этого мы добавляем + 1 instance смотрим дельту смотрим где его полка и над и находим вот это ту самую закономерность понятно что могут быть ситуации когда у нас там не хватило нагрузки там на вот этом вот testing или там еще что-то но в чем здесь нет а плюс том что он то вам не нужно это вот в этом случае писать реальные нагрузочные тесты которые индекс равно всегда синтетические это реальное нагрузка это реальное поведение пользователей которую вы ну тут но когда вы пишете эти ваши тесты все равно где то что то внутри не учтёте реальные пользователи вам а с большей вероятностью это покажут и соответственно вот эти метрики мы точно также собираем кладем это в единое наши хранилища знаний и сравниваем их они-то с метриками нагрузки на сколько вот эти данные отличаются от того что мы получили на нагрузочном тесте сколько они могут повлиять на те или иные вещи ну и наконец production здесь нет и история на то право масштабированием я пару лет назад рассказывал про авто скиллинг и рассказывал про то что он вот мы четко смотрим просто на трп сын ты исходя из вот этой циферке мы делаем скиллинг это отлично работает если у вас какой-то сервис там на уровне каких-то нато потоковых обработок там обработки видео на то что то еще но это не очень хорошо работает на продуктовых сервисах поэтому здесь нет а подключаются и продуктовый метрика то есть я всегда обращаю на них внимание то есть эти метрики вы точно так же в этом едином сервисе должны знать и учитывать это а те данные которые вы должны учитывать при стерлинге вашего сервиса соответственно то сейчас вот итоговая схема для нас выглядит вот так то есть это не так гибрид вот некая суперпозиция которая в том числе учитывает исторические данные и здесь возникает вопрос ну окей вывод этот сервис на за масштабировать а под ним еще десяток да они над упадут под нагрузками то что с этим делать что мы сделаем вот сейчас мы опять же берем как бы смотрим какие зависимости от уровня там tab + 11 сервис кто под ним напрямую мы смотрим исторические данные этого сервиса мы смотрим исторические данные каждого сервиса из тех кто нато лежит ниже и вот в этом едином хранилище мы дополнительно учитываем а кто-то уже отправил запрос на скиллинг вот этого + 1 или нет чтобы у нас не случилось этой конкуренции you и вот из этой общей информации мы делаем вывод этот инстанций еще увеличить или нет то есть это некая такая получается сборная солянка но именно вот на прогнозах вот этот подход достаточно неплохо себя показывает и наконец вот мы это все все все вот это замечательно собрали в нашей системе вот и единого знания атома сервисе in the тогда на полу нас включается историю что мы можем вот эти вот самые trigger in подключать то есть ног про миграции int и я уже говорил но помимо этого есть еще сервисные штуки то есть например это там security апдейт то есть мы по каждому сервису знаем от какого сервиса например на ту базовым слоем мы создавали нато докер-образ соответственно если на этот образ выходят security апдейты если там вот какие-то патче вышли танту минуты через нашу платформу каждому сервису пытаемся надо сначала сделать автоматом перри сборку через нас по plugin если это не возможно то мы станем сервис на карантин отправляем разработчику извещение там флаг почту что чувак это надо сделать int и запускаем обратный отчет то же самое на то сервисом который давно не обновлялись есть сервисы там два года их написали потом никто не трогал вроде как хорошо потому то что-то где-то упало вдруг проводили там на кластер другой надо перенести а он вдруг не собирается вот мы не то запускаем тут регулярную пересборка и с этой перед сборкой мы смотрим на сервис живой не живой он собирается не собирается устарели какие-то библиотеки не устарели вот точно так же история про какие-то требования платформы то есть выходят новые требования новые чеки новые нато возможности соответственно мы смотрим вот под этой нашей базе знаний о каждом а сервисе какой сервис не реализует те или иные вещи по уровню критичности мента в итоге точно также извещаем вот и тот самый дашборд про который ты я обещал немножко рассказать вот выглядит это примерно так здесь нет а вот кусочки то есть мы в одном месте смотрим информацию о том как там это сервис написан и на то что с ним происходит вот например на то вот это состоит же это здесь как band мы зажали наш сервис ответственно то вы даже бардем и в том числе над и видим то что он посмотри чувак вот этот твой сервис нато постоянно не влезает а вот она протянута он не просто не влезает это вот как раз на ту кусочек сп рода это здесь нет и его вот там-то завидной регулярностью прибывает он киллер это значит что она там и проанализируем вот эти вот и винты мы это посмотрим то в том числе что нато год в этот самый брик сервис не вылезает и на то нужно либо менять манифесту это либо что то еще с этим делать точно так же на ты здесь мы показываем какие коды на каких машинах запущены там в каком количестве вся эта информация она в едином агрегированном виде ну собственно все спасибо за внимание владим спасибо тебе большое ты прошел нагрузочное тестирование с сжатым временем тебе подарок и благодарность грамота грамоту можешь отдать дантисту они любят на стенке вешать в талише видно и еще посмотри в таллинне в коем случае еще посмотри пожалуйста на тех кто не уходит и выберет двух самых внимательных которым мы подарим книжки так вот молодой человек да да да вот он явно не уходит на то сидит в проходе с бородой готра и так и вовсе обходят а он стоически сидит стоический ситец до психически стоит ну или так да и пусть он и здесь так много народу не уходит из тех кто внимательно слушал вот самовыдвиженец внимательны и вот отлично вот смотрите вот юноша бежит в сером и мужчина никуда не уходит с бородой запись в очень спасибки огромное друзья вопросы задавайте вадиму пожалуйста кулуарно спасибо ребят аплодисменты можно со свистом громкие громкие"
}