{
  "video_id": "AFJMOOpMjQM",
  "channel": "HighLoadChannel",
  "title": "Считаем число просмотров видео для десятков миллионов пользователей в день / Иван Соколов (Яндекс)",
  "views": 982,
  "duration": 2289,
  "published": "2023-01-19T05:55:19-08:00",
  "text": "собственно всем привет для начала я бы хотел рассказать вообще что такое видео хвостик видеохостинг это некоторый базовый инфраструктурный компонент индекса который предоставляет другим площадкам возможность заливать и показывать у себя видео среди наших потребителей ну самый крупный наш текущий потребитель это цен соответственно вся команда разработки сейчас находится там также это индекс т.в. станция интеграции внутренне хостинг видео поиски новости market но и другие сервисы суммарная дневная аудитория этих сервисов более 10 миллионов пользователей в сутки о так как видео показывается разными площадками то есть абсолютно разные пользовательский сценарий то и виды контента на этих площадках также разные это как блогерской ролики как спортивные матчи фильмы и сериалы новостные сюжеты телепрограммы и к записи то есть на текущий момент суммарное количество контента в нашей базе около больше 10 миллионов вот раздачи это информации по видео эта информация это все что не связано непосредственно с раздачей самого видео потока то есть например название видео его описании ссылка на превьюшку раздачи хитами этой информации занимается команда runtime а видеохостинга ты понимание наших технических водных наша обычная нагрузка в районе 10000 рпс многие из этих запросов на самом деле являются баль запросами то есть мы отдаем не какой-то один элемент несколько на самом деле как правило несколько сотен элемент зависимости от конкретных запросов у нас разные сыры село естественно но к в качестве ориентира можно считать что мы должны отвечать в районе сотен миллисекунд для матч запросов но помимо стандартной нагрузки у нас так же случаются случается такая вещь как спайки это супер популярные ролики и трансляции например супер известный блогер выкладывает новый ролик всячески пиарит его в своих каналах пользователи приходят смотрят его еще больше нагрузки может давать live-трансляция естественно так как она начинается в определённый момент и в качестве примера самого крупного спайку на нашу историю можно привести матч чемпионата мира по футболу 2018 года когда сборная россии вышла в финал играла схавать и тогда нашу нагрузку в 10 раз превысило стандартной от начала этого матча среди мета информации которую мы отдаем про видео есть две важные метрики о популярности видео это число просмотров и счетчик зрителей это похоже и метрики то есть они так или иначе учитывают сколько человек смотрит посмотрел или смотрит ролик но считаются немного по-разному просмотры это в целом монотонно растущая величина то есть пользователь приходит смотрят видео мы засчитывать просмотр и дальше счетчик этот растет за редким исключением когда да какая то накрутка и anti-fraud его уменьшает счетчик зрителей это real-time а величина соответственно отличие от просмотров она но она так же как и просмотре учитывает то что пользователь пришел на трансляцию но когда пользователь уходит мы дискриминируем этот счетчик соответственно это некоторое количество пользователей которые смотрят видео в моменте но как правило оно применимо и больше всего клайв трансляциям хотя в теории может использоваться для какой-нибудь премьеры и для обычных видео и так как же мы это читаем для начала вообще стоит определиться что такое просмотр видео случае обычных статьи например в интернете когда пользователь заходит на хабр читает статью то если он ее открыл с большой вероятностью он заинтересовался читает ее и мы можно проксиме ровать это как просмотр страницы но в случае с видео такая логика не подходит во первых видео может быть встроена без autoplay то есть пользователю нужно еще дополнительное действие после того как страницу с видео открылось но и до этого учитывать просмотр будет неправильным во вторых есть сценарий когда пользователь отдается и одно опочка видео то есть например лента рекомендаций когда пользователь просто ли стоит и останавливается на интересных видео чтобы посмотреть их дальше соответственно таком случае мы даже не знаем такие этой пачке видео будут воспроизведены какими пользователь заинтересуется окей наверное мы можем взять информацию от инициализации player за просмотр но как мы будем ее считать случае веб-страницах первых мысль может быть взять обращение для вас крип баблу который встраивается player через который страниц плеер но на самом деле это еще более плохая идея член использовать непосредственно обращения к сервису потому что банда кэшируется браузером между перезахода my на страницу для оптимизации трафика случай нативных приложений любых устройств это ещё более сложно задача то есть код плеера непосредственно на диплом вместе с приложением окей тогда у нас нам может прийти в голову идея а давайте брать первый запрос непосредственно зачатком видео физически мы начали качать поток наверно пользователь его начал смотреть смогу неверным потому что для ускорения воспроизведения для того чтобы пользователи например блистающий ленту рекомендации смог быстрее перейти к новому ролику и начать его смотреть плеер может использовать технику которая называется приладим то есть пользователь всмотревшись недосмотрел предыдущий ролик и плеер автоматически начинает скачивать чанки следующего для того что переключение между ними произошло бесшовно быки что же нам тогда делать на помощь приходит в телеметрия которую посылает player телеметрия это некоторый поток событий например эта информация о начале воспроизведения это разбиты это регулярные или не очень пинге о том что пользователь посмотрел еще какую-то часть видео то есть прошло ещё сколько-то секунд смотрения также в телеметрии есть ошибки информации о смене потока но для просмотров нам в первую очередь интересные первые два типа событий на основе которых сервис может сформулировать свою логику того что же считать просмотр помимо этого телеметрия посылается player со всех наших поверхностей на которых возможна в произведении видео с десктопов с мобильных устройств с умных устройств телевизоров таким образом здесь мы прямо значительно можем сэкономить себе разработку если у нас есть такой поток событий и он уже приходит итак у нас были была информация по которой мы могли посчитать количество просмотров видео и на самом деле она активно использовалась то есть рекламные выплаты какая-то аналитика все считалось по блогам и из потока событий player но в доставка логов не моментально и дело то есть соответственно приходилось ждать часы или даже сутки до того как эти значения посчитаются и отобразятся автору в личном кабинете он соответственно автор который хотели понять насколько ролик популярен просто сидели и ждали все это время и перед нами стала задача научиться считать число просмотров максимально близко real-time вот и так у нас были десятки миллионов пользователей десятки миллионов видео и желание посчитать просмотр как можно быстрее собственно как я уже упоминал у нас был контур который занимался сбором логов из потока событий player но на него приходила порядка сотен тысяч сообщение в секунду и пускает такую нагрузку напрямую в базу не хотелось была она потребовалось бы весьма и весьма много железа плюс у нас вы помните были спайки соответственно железо вы нужно было еще больше про запас который будет просто так стоять вот но наша задача не посчитать и счетчик реал тайме а печатать его наиболее близко к real-time и соответственно мы можем добавить некоторый промежуточный в их рисующий контур который предварительно посчитает сколько посчитает из агрегирует просмотр по одному видео за несколько секунд за одно мы можем здесь у меньший поток событий то есть из всего потока событий от плеер и все телеметрии мы можем оставить только интересующие нас события таким образом еще больше еще больше снизив нагрузку для того чтобы наша железо нашей одра утилизировали довольно эффективно контур сбора логов и буферный слой общаются друг другом через очередь сообщение в яндексе очередь сообщение реализация очереди сообщений называется брокер это внутренняя наша реализация но на самом деле интерфейс на максимально похоже на apache кафку собственно когда-то это и было настройка надо пачка вк ой но с тех пор переехали на свои технологии очередь сообщение было брокере называется топиком топик состоит из партиций каждая partition это некоторым образом порядочный набор сообщений ну и соответственно гарантия порядка скупятся только внутри одной партиции горизонтальное масштабирование путем добавления новых партиций так как каждый partition ограничена в пропускной способности и ну как правило не может пропускать больше чем несколько мегабайт данных в секунду вот стандартный паттерн при работе с такими очередями состоит в том чтобы писать взаимосвязанные данные в одну партицию то есть в нашем случае это был означало что мы пишем все события которые пришли от плеера по одному видео в одну партицию но просмотры вещь довольно неравномерно и то есть большую часть смотрения приходится на относительно небольшое количество роликов таким образом мы бы себя не сильно ограничивали и у нас были бы сильные перекосы нагрузки между партициями вот но можно заметить что счетчик просмотров это довольно простой счетчик он просто и комментируется и нам случае ничего не произойдет если мы начнем писать данные без прибытия к партиции то есть любое событие по любому видео мы сможем записать любую партицию и не поломать таким образом счетчик ok наконец финально все данные агрегируются описательным которые непосредственно взаимодействует с базой в данный момент на этом этапе мы уже сократили изначально поток событий на несколько порядков больше чем в тысячу раз и соответственно писатель которые пишут базу может быть 1 а все остальные просто выполняют роль горячий хлеб реплик в случае выпадения текущего писателя окей мы посчитали просмотр но нам нужно также как-то его отдать пользователю то есть мало посчитать нужно еще и понять это поразительно пользователю в интерфейсе в качестве хранилищ мы используем body by это изначально внутренняя разработка яндекса но буквально недавно вышла open source приходите пользуйтесь минутка рекламы вот соответственно у и деби и представляет собой распределенную базу данных соску и like запросами данные ну как и обычное лицо нашу где хранятся в таблицах каждая таблица представляет собой набор шар дав соответственно при росте данных или количестве нагрузки можно от масштабироваться увеличив количество городов для выполнение runtime запросов для описания графа запроса мы используем во frost еще одну нашу внутреннюю разработку непосредственных аналогов наверное яблоком source и даже не могу привести но в целом можно смотреть на нее как на и 5 твой навороченный с внутренними интеграциями в нашем случае описывает направленный граф выполнения запроса описывает микро сервисы где они расположены как между как них ходить и поток данных между микро сервисами как правило мы получаем информацию о конкретных идей видео которые необходимы для которых необходимо отдать просмотры уже ближе к концу выполнения запроса то есть стандартный сценарий мы сначала сходили в рекомендатель мы ничего не знаем о видео которое он отдаст рекомендателя тяжелая вещь выполняется долго и у нас остается не так много времени обычно в районе 15 20 миллисекунд для того чтобы получить всю остальную эту информацию про видео ok на мы храним просмотры в кирилл истории gera что может пойти не так мы запустились и увидели сильно большие тайминги чтобы понять почему вкратце расскажу как устроена runtime видеохостинга но и практически на самом деле любой любой runtime яндексе и не только runtime живет в нескольких дата-центрах соответственно при аварии попадение сетевыми связанности и так далее 1 если один из дата-центрах закрывается та трафик из него переливается в оставшиеся два таким автоматически естественно и таким образом пользователь ничего не замечают то есть стабильный сервиса и страдает пользователь продолжает им пользоваться руки и первое что мы обнаружили наши бренды и классовой devil живут разных дата-центрах почему это плохо как правило round trip time time внутри одного дата-центра то есть время на поход по сети в районе миллисекунды или даже меньше но если мы говорим о двух дата-центрах сильно разделена географическим то есть если между ними десятки там тысячи километров таран 3 type может составлять десятки миллисекунд а мы целимся полностью получить ответ за это время окей мы переди появились в этом мире все еще были высоки почему почему так могло случится здесь нужно немножко залезть под капот войди by и рассказать что у них есть два принципиальных слоя это компьютер и 40 слой соответственно 100 раз слой от отвечает за надёжное хранение данных хранит их дела распределена в нескольких дата-центров компьют слой это можно можно назвать слоем микро сервисов которые в терминологии войди by называются таблетками и таблетки отвечают за согласованное обращение и изменения данных соответственно каждая таблетка обслуживает один шар твой но данные хранятся гил распределена соответственно таблетка может быть поднята в любом из дата-центров и таким образом мы видим следующую картину наши бренды ходили в intent его и тебе в том же дата-центре in point и это на самом деле д-р писи практик которые под капотом для обращения и получения данных из нужного шарда могли взять между дата-центра вый поход в таблетку которая поднята над этим шар дом вот и таким образом мы снова увидели просто все походы просто замаскированные и на уровне выше ниже для решения этой проблемы существует технология которые называются fall away a replica mi то есть опять же данные хранятся гиа распределена и мы можем поднять таблетку на на данными во всех дата-центрах естественно не за просто так данные в этом дата-центре могут отставать на некоторое время потому что таблетка отвечающая за записи согласованные изменения данных она может быть только одна и пока данные до едут из одного дата-центров другой пройдет может пойти немного времени но как правило репликации осуществляется в районе секунду нам в целом не страшно получить данные отстающие на секунду вот таким образом мы полностью избавились от между центровых походов но тайминги стали сильно лучше на на высоких перцентиль ах все еще бывали прострелы и мы погружаемся все дальше в тебе под капотом используются lsm дерево лакс трактат 03 я вкратце расскажу как она работает соответственно когда приходит некоторые по когда приходят некоторые данные на запись изначально они сохраняются в некотором дереве в памяти приходит новая запись добавляется в это дерево и так далее пока не пройдет достаточно много времени или дерево не достигнет определенного размера после чего дерево в сортированы виде сохраняется на диск и начинает наполняться заново поставок а наполнилась мы также сохраняем еще один сортированный чанг и так далее но так как процесс условно бесконечный то нужен некоторый процесс объединяющий и мир училище чанки который называется комп актив акций соответственно раз какое-то время этот процесс запускается берет все чем кино одном уровне и складывают их в другой более большой чанг на уровень ниже здесь довольно много нюансов и деталей но в целом схема работает примерно так вот соответственно происходит во время чтения мы приходим начинаем искать данные в памяти если не нашли их то идем в 1 из чанков который был сохранен на диск если мы не нашли их там-то мы ещё не можем гарантировать что их нет у остальных чарах соответственно мы идем хронологическом порядке по всем танкам по всем уровням и в худшем случае проходим все это дерево часть данных уже может лежать в холодном виде на диске обращения к ним соответственно будет происходить достаточно долгое количество времени то есть первую очередь это влияет на ключи которых нет в базе если ключ есть базе то мы найдем довольно быстро если он был записан совсем недавно там и прямо мгновенно получил практически ответ из памяти вот и для довольно стандартные решения для того чтобы избежать всего этого обхода дерева это будем фильтр вот был фильтр это вероятность инструктору данных которая достаточно большой уверенностью позволяют но на самом деле зависит от размера был фильтр который мы видели позволяет сказать если сейчас ключ в множестве конкретно в таблице или нет работает она примерно следующим образом мы выделяем и мбайт мы задаем несколько хэш-функций которые мопед наш ключ в число от 0 до -1 после чего вычисляем эти функции каждый раз для записи при записи ключа и выставляем те биты которые кашу некоторые посчитал там например который отобразилось число кв выставляем к -1 порядковый бит и так далее при записи каждого ключа соответственно при чтении мы можем также провернуть эту операцию мы можем также посчитать эти хэши и дальше возможно две ситуации первое если один избит ключа который мы хотим прочитать будет равен нулю то это гарантированно с гарантиями дает нам информацию о том что этого ключа нет в базе потому что иначе бы этот бит бит был выставлен в единицу если же все биты равны единице то с большой вероятностью если хеш-функции и размер был фильтра достаточно большие можно утверждать что ключ из базе хотя здесь возможны фолз вот соответственно то есть эти биты могли быть выставлены какими-то другими включая вот это капрал просмотры здесь мы обеспечиваем горизонтальное масштабирование как на запись так и на чтение на запись через дочери сообщение так как мы не пребываем данные к партиции то мы можем увеличивать количество партиций и также инстансов которые обрабатывают и и вот и на чтение путем формирования vdb соответственно суммарное время на все буферизацию все обновление счетчика данных в районе 10 секунд что в целом нас устраивает и итоговое время на чтение составляет 10 12 секунд по 99 он перцентиль у для патчей сотен элементов а для единичных ключей единицы миллисекунд окей у нас и счетчик просмотров он в принципе подходит для обычных видео но для live трансляции пользовать автору интересна не только завлечь пользователя но и удержать его от счетчик просмотров не дает такой информации соответственно мы нам нужен некоторый некоторые аккумулятор который будет отображать сколько пользователей смотрят эту трансляцию прямо сейчас 1 наши мысли у может быть а давайте просто сохраняем в таком формате кивал и как и для просмотров то есть по айди трансляции мы посчитаем сколько пользователей смотрят и будем раздавать эту информацию но здесь есть подводный камень пользователь то есть когда мы когда пользователь приходит на трансляцию мы увеличиваем этот счетчик когда пользователь уходит мы должны его вычесть но что если у пользователя пропадет питания упадет браузер ну или еще случится что-то что не позор или просто te voi ошибка которая не позволит нам уменьшить это счетчик таким образом тогда счетчик будет увеличен то есть там будут призрачные пользователи которые останутся там навсегда для того чтобы избежать этого нам нужно регулярным образом получать некоторые пинге от пользователя что он все еще жив он все еще смотрит трансляцию на помощь нам снова приходится быть от плеера или более конкретно 30-секундные их орбиты которые посылаются регулярно пока пользователь смотрит некоторые видео здесь на рисунке соответственно разными цветами отмечены разные пользователи и точки это события важно заметить что старт и видео тоже можно считать как 30-секундных орбиты просто на нулевой секунде и мы можем заметить что интересующий на счетчик можно получить просто просуммировав пользователей запах орбиты пользователя и количество кораблей тоф пользователей за последние 30 секунд вроде бы все хорошо но как мы будем хранить эти события если мы сохраним и в сыром виде то есть это некоторые эти трансляции timestream hard by the и индикатор пользователя то наша таблица будет расти вместе по данным она будет расти в зависимости от количества пользователей у нас бывает спайки если у нас внезапно прибежит могу пользователи тумма резко распух не подано ни очень хочется так делать хочется как-то суметь комплекте фиксировать таблицу так чтобы где хранилась не так много данных вот плюс такой формат хранение приведет к довольно сложным запросом ран тайме которые могут пойти много сортов сразу и таким образом опять же себя ограничен масштабе в дальнейшем масштабирование но мы сразу можем заметить что на мне так интересно какой пользователь в какую секунду смотрел трансляцию нам достаточно получить счетчик пользователей за конкретное в секунду таким образом мы сильно упростим формат хранения можем пойти ещё дальше из агрегировать пользователи не в одну секунду по 5-ти секундным интервалом и уменьшить себе нагрузку еще шесть раз вот плюс нам не так интересны данные которые были сохранены исторически то есть мы будем накапливать информацию в этой таблице постоянно и но мы можем удалять все что старше примерно полуминуты для этого в виде by также есть авто удаление данных под отелю то есть можно выставить некоторые значения после которого данные должны быть удалены важно заметить что данные удаляются не сразу то есть здесь уменьшил консистенции но в конечном счете даны в таблице не станет что нам сильно облегчит поддержку нам не нужно будет чистить данные раз какое-то время сами вот это кого мы можем модифицировать на предыдущий алгоритм скользящего окна нам нужно взять и просуммировать зрителей в 6 5 секундных байках важно что мы не должны учитывать текущей интервал так как на который указывает текущей streams темп так как он еще может быть неполным вот этого счетчик зрителей является real-time величиной но читаем о его дискретные моменты времени раз в 5 секунд что у нас в целом устраивает никто не ждет что счетчик будет прямо обновляться супер мгновенно вот ну и для живости пользователей для того чтобы выпавший пользователи по каким-то причинам не а пока не отправившее событий об уходе с трансляции также учитывались мы используем карбид и player которые посылаются раз в 30 секунд подготовка данных максимально похожа на ту что было в просмотрах то есть мы также агрегирует по фильтрованный поток данных на самом деле это тоже очередь событий за за несколько секунд что еще важно заметить что мы свели задачу инкрементирование счетчиков и соответственно также не важно чтобы данные были прибиты партиции то есть мы также можем писать данные независимо и увеличивать партиции за дешево но внедрением буфером и если дополнительную задержку то здесь нужно при чтении будет учитывать два условия первое что мы не попадаем в текущей незаполненный интервал и второе что данные который еще находится на этапе буферизации что должны пройти контур буферизации вот это кого мы читаем данные в runtime примерно по такой формуле берем текущий там стоим по округляем вниз до 5 отступаем на два интервала или суммируем их здесь у нас уже был опыт crosse bc взаимодействием соответственно мы запустились изначально увидели устраивающий нас тайминги но с каждым днем время ответ начинала все увеличится увеличиваться сначала мы подумали что что то не так софта удалением но все оказалось сильно проезжаешь не пользователь смотрит live трансляции не только в момент когда они идут но также иногда пересматривать записи изначально мы считали количество зрителей и для них но продуктового это не было не нужно соответственно мы добавили фильтрацию и улучшили себе время на чтение по счетчику по алгоритму почет и счетчика он стабилен если пользователи не уходят и не приходится трансляции ну да исключения естественно сетевых проблем когда по каким-то причинам карбид нам не пришел новый пользователь учитываются со временем буферизации через пять десять секунд ушедшие трансляции когда пройдет последних 30 секундных орбит через 35 40 секунд ну и насовсем напоследок забором случае про просмотра один раз мы увидели 18 триллионов просмотров и видео здесь довольно очевидно что это был переполнение вниз не знаю может быть у кого-то есть идеи почему так могло случится это было anti-fraud он решил вы чуть чуть больше просмотров чем было увидим спасибо жду ваших вопросов да иван спасибо большое тут еще обязательно нужно тебя выручить смотри какая красивая штука спасибо тебе за то что ты выступил да давайте вот по центру вопрос здрасьте спасибо за доклад я хотел уточнить я правильно понимаю что если я хочу увидеть конкретное число просмотров который прямо сейчас это будет в реальном времени вот это вот выборка там по формуле сумма чего-то и вы не храните уже предпочитаю цифру заранее смотри тут важно отличать счетчик просмотров и счетчик зрителей я про зрителей в для живых трансляций да то есть это будет реально выборка но нам нужно со береги ровать последние с учетом отступа шесть пакетов то есть да это мы получаем с агрегированные данные за пять секунд по 6 баки там мы предварительно реагируем но не полностью да то есть в конечном счете мы суммируем их в реал тайме 0 довольно дешевый операции и 2 короткий вопрос я брали понимаю что если я прикручу какой-то свой кастомный плеер который через api будет показывать ваш этом видео ли просто трансляции то никакие счетчики работать не будут ну потому что логов нет естественно но мы очень хотим чтобы вы показывали через наш плеер моего оптимизируем чтобы пользователи могли смотреть видео спасибо вот слева этот вопрос привет скажи счетчик просмотров никак не уникализируете то есть один пользователь может несколько осмотров накрутить да смотри мы не ощущаем не канализацию во первых это довольно дорогая операция в реал тайме то есть чтобы она потребовала довольно большой и сложной логики поверх во вторых это в принципе принятая практика в индустрию то есть пользователь может посмотреть видео два раза и эту кей когда я помню увидел новость чтобы и бишь арка 10 миллиардов просмотров я потому что столько людей нет на земле но тем не менее они как-то посчитали так а есть какая-то минимальная задержка между если я буду обновлять просто видео и смотреть его заново сколько она примерно слушать здесь наверное больше вопрос к нашему ativ роду и как правило то есть здесь любые накрутки детектив родом и считываются здесь правильно узнать по центру видела просто скажите а вот вашим аналоги кафки эти события не навсегда остаются даже из потом уже не нужны если я правильно понял вопрос то речь про то насколько долго события остаются в очереди после того как сообщение помечаются за комичен им данные могут удалиться то есть они живут не вечно очередь это в первую очередь средство доставки сообщения но не и хранения слева добрый день меня тоже зовут андрей смирнов отряд вашу статистику можно пахать путем посылки фейковый вот и статистической информации от несуществующих плееров если оно больше ни на чем не основаны на сего смотри если опять же правильно понимаю вопрос то есть ты имеешь ввиду что один пользователь начнет посылать события как будто это много пользователей и так далее вот но в данном случае это все довольно тривиально для птицы это все с одной машины мы увидим что это естественно уже оффлайн процесс то есть сам начала счетчик вырастет но через некоторое время через там десятки минут может быть часы будет видно по флагам то что было много запросов с одного ip и и пользователь притворялся разными другими пользователями что скручивается является скруткой и соответственно после этого читается привет спасибо за доклад не нужен а там опять же здесь наверное больше вопрос команде эти фродо это в принципе известная задачу то есть это это это задача эти фродо который не непосредственно связана как с счетчик просмотров это также дтп adidas и так далее что примерно по тем же технологиям работает привет и спасибо за доклад а у меня как раз продолжение этого вопроса которого предыдущий у нас есть телеметрия с player эта аналитика а условно предположим что мы начинаем платить пользователям за просмотр их видео мы такие собираем с player аналитику пошел фрод а вопрос о аналитик усилены мы берем чтобы посчитать настоящих зрителей то есть это медленно дорого но настоящие данные смотреть да вот этот счетчик про который рассказывал он информационный то есть нам нужен для того чтобы понять автору насколько вообще ролик смотрят он не участвует в рекламных выплатах он не участвует в ранжировании там соответственно собираются логе далеко не только события от player logis сидена какие то просто проклейки с другими с другой информации которые у нас есть и пользователя то есть ответ да там учитывается ну сильно боль факторов спасибо а второй вопрос такой вот начали все говорить про виде beat a practical с как будто забыли зрители это аналитика все события от player это в том числе на литические данные почему я тебя не crack house ok на самом деле здесь довольно много удобств которые вы деби предоставляет нам из коробки без необходимости поддерживать в рамках мультитран от кластера нам не нужно заморачиваться практически вообще не нужно заморачиваться над его devops он то есть просто взял и пользу он сам бы тебе сами отвечают за сохранность данных у них совсем ну относительно недавно появилась возможность авто формирования то есть когда я тебе сам понимает что нагрузка или данные увеличились и нужно увеличивать количество шар дав то есть о маме делать это не только по стандартные шарды они ровно дают отсечку по какой-то функции то есть мы задаем хэш-функций and ключом и говорим что вот по ней хочу сделать 16 хардов выделить более хитрый алгоритм эвристики которые позволят автоматически зависьте от нагрузки наш арт с оптимизировать количество сортов и их размеры а я же правильно понял что это мы считаем зрителям но аналитические данные идут дальше куда-то не только в ютюбе то есть уже какие-то сыр и аналитические данные по которым не только зрители можем посчитать общее состояние плееров какой-то момент и все остальное там но на самом деле тип данных очень много то есть даже cliff house лопнет с поэтому здесь используется мапри deus как как наверно можно так ответить спасибо так все вроде больше вопросов нет я напомню зрителям онлайн трансляции что вы можете задавать все вопросы в онлайне мы вас можем вывести на экран давайте еще раз поблагодарим его на за отличный доклад спасибо большое зубов"
}