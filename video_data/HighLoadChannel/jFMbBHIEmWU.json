{
  "video_id": "jFMbBHIEmWU",
  "channel": "HighLoadChannel",
  "title": "Стенд метрик: как построить архитектуру для расчета тысячи метрик и попасть в SLA / Евгений Пак",
  "views": 320,
  "duration": 3055,
  "published": "2025-01-17T02:24:59-08:00",
  "text": "Всем привет Рад вас всех видеть и то с чего я бы хотел начать свой доклад с того что я просто влюбился в мерик и в Спарк и хочу в этом докладе передать мою любовь и к команде И к продукту который мы сделали за Эти прекрасные 4 года и начнём с того с чего ВС начиналось у нас начиналось всё с cto то есть Антон Степаненко сказал о том что к нему приходят люди и каждый рапортует о том что он увеличивает Джим вина 0,5% и Он утверждает что 50% этих экспериментов каннибализме это очень легко если мы вот всё то что ему относится сделаем сумму то это никак не бьёт сумам Джим озона Значит есть где-то ложь и основная суть в том что ему не интересно какую сделали фичу в дизайне в новом тупо запустили фичу и тупо вот такой эффект и так чтобы каждая фича изменялась в одних и тех же координатах то есть чтобы мы яблока сравнивали с яблоками это наш основной девиз Ну и конечно очевидно Какая жизнь вообще у продуктов наров да у них жизнь в том что они сделали некую фичу идут к аналитику и описывает ему Ну давай парень на майни мне чтобы метрики шли наверх и дальше аналитик там значит ищет нужный разрез в котором метрик идёт наверх может найти день да и или убрать выброс о которых говорили в докладе до этого что на самом деле не выброс Вот и мето наверх все идут к топам вот и всё отлично и поэтому наша цель сделать единую эталонную метрику которая обеспечит прозрачность методологии единые формулы вычисления метрик и своевременность расчёта то есть наш сла и Суть в том что до этого у каждой команды была типа своя линейка которая не измеряли эффект с погрешностью разной кривые косы и так далее и Суть в том что ваша линейка может быть компания непростой А вы из неё можете сделать золотую линейку А я как менеджер говорю что любая вещь становится золотой если вокруг неё есть регламент и процессы и о регламенте и процессах можно будет узнать в статье которая выйдет в июле Я старался там статья для всех для менеджеров аналитиков и бэнде Итак идём дальше и Суть в том что Stand Met Это не просто инструмент который сделали Бендеры не просто линейка с определённым уровнем погрешности а это инструмент который определяет жизнь или смерть фич который уходит в прод Ну очевидно что последнее слово остаётся за продуктов Но если ваша фича увеличи ваши целевые метрики уехала наверх но уронила основную го метрику озона ей смерть и вариантов нету Итак давайте поймём вообще В чём отличие метрик бизнеса Да там и метрик бэнда который мы все знаем любим и смотрим в эти графики в фане Отличие в том что если у нас есть эффект в бэнде как в графике в первом где условно мы включили кэш rps упал в полку и крутая оптимизация либо же как на втором графике снизу мы улучшили девяносто девятую квантиль она стала меньше и мы в общем на взгляд увидели да типа стало меньше вроде бы нормально значит В чём отличие бизнес мерик в том что Мери которые мы считаем в фане они идут за интервал в последние 5 минут и агрегирующие заключается в том что мы определяем тот сигнал который видели в тесте и делим его на шум и дальше пова определяем Был ли значимый прокрас или нет так что же конкретно должен считать мерик Итак чтобы нам в целом измерить какой-то эффект на юзера Да Нам нужно измерить некий его признак и вот атомарный изменением признака юзера у нас называется га То есть это тот gmv который был сделан у юзера к примеру в течение дня делал ли он а заказ была ли конверсия вту C и так далее Э что же есть Метрика а Метрика очевидно оперирует этими агрегатами все метрики которые у нас есть они относительные то есть у нас есть агрегат некий признак юзеров числителе в знаменателе и дальше стат критерии которые мы используем Ну то есть к примеру как у нас есть а средне gmv Да в которой мы суммарный gmv делим на количество юзеров Ну и считаем его всё Easy и получается так что основная задача сри сводится к двум Первое - это расчёт агрегатов второе - это расчёт метрик Ну и для того чтобы мы в целом могли определиться А как же мы можем считать данные в целом в вазоне Давайте узнаем который у нас есть а в том что У нас есть юзер с мобилочке Ну или с Веба неважно дальше все его ивенты уходят в трекер в трекере его ивенты обогащаются информацией о пользователе его экспериментах дальше информация обогащённое экспериментом уходит в кафку из Кафки у нас данные уходят в ходу hdfs и в кликхаус одна из особенностей озона что в ходу и в клике данные будут одинаковые Ну как бы но прикол в том что Клик намного быстрее ходу О чем я буду описывать дальше Ну что в принципе очевидно Да и из особенности dat Flow что у нас также есть данные которые не идут из каки в рил тайме не обогащённый информацию про эксперименты это данные которые идут на поста аналитике это всякие базки из которых по airflow данные утекают в Верку и мы её тоже используем в качестве исходных данных Итак как же мы можем построить Ну и по идее у нас идёт базовая гипотеза Easy Win э о том что любой аналитик может построить стенд метрик типа сделай 1 2 3 4 5 вот и всё будет отлично и мы так и сделали на самом деле То есть первый комит мерик был сделан 11 оди а и это же не обычная дата А в Китае это праздник еком дение ластика совпадение не думаю двадцатого года 4 года назад Ну и получается так что Давайте с вами сформируем дизайн нашего эксперимента значит наша гипоте в том что любой аналитика может построить метрик метрики по которым оцениваем что метрики посчитано к нужному времени что нету багов И я длительность в 2 недели Итак проверим её Значит первое расчёт агрегатов как мы описывали Давайте поймём в м его суть на приме уб узе она у нас одна которая у нас уходила в ходу помните и в ней 27 ТБ данных и Суть в том что нам нужно взять за весь день и посчитать на этих данных для каждого юзера его агрегаты то есть там gmv конверсии и так далее ну Задачка не очень простая Итак посмотрим что же может сделать наш аналитик раз он задаёт базовую структуру агрегатов которую он указывает сами агрегаты и разрезы в которых мы будем считать это дата сессия платформ Ну информацию про эксперименты два наш аналитик указывает сами запросы агрегатов которые работают на трёх источниках данных на верке крике и дупе и и дальше эти агрегаты ещё внутри у нас бьются на А доменный датафрейме как видно в примере это каталог и поиск три после того как мы значит эти агрегаты описали мы их объединяем и делаем группировку по полям в разрезе которых мы считаем агрегаты и четыре сохраняем в таблицу в ходу в прекраснейший на спарке помните что вторая задача у нас была расчёт метрик То есть первый вроде всё Изи Итак в ЧМ суть этой задачи в том что у нас есть эксперимент мы узна какую время шёл этот эксперимент за всё это время набираем агрегаты Да которые у нас будут у метрики в числители в знаменателе после этого считаем тот шум и эффект который был метрики сравниваем данные в тесте и в контроле по ста критерию вычисляем pv Ну и определяем Был ли значим эффект или нет всё Изи Итак первое что делает наш аналитик он идт в наш сервис берёт информацию про эксперимент узна длительность и варианты эксперимента которые есть два дальше нам агрегаты которые есть мы всё время эксперимент нас по gmv по юзеру интересует сумма если у нас история с конверсии то нас интересует чтобы Что ели в один из дней она была Она была во все дни то есть функция Макс четыре после этого мы указываем наш справочник мерик где мы указываем агрегат в числителе в знаменатель ста критерий который мы используем То есть это mone затест тест и Boot который конечно очень сильно бьёт по производительности итак по идее метрики посчитали агрегаты почитали а для всего основа есть Спарк и ходу они сами всё сделают то есть есть можно не париться Вот и по и по идее по считаные агрегаты метрики вычисленные парампампам отда но однако же что было дальше А дальше было то что через несколько недель прекрасный код аналитика взорвался Вот и расчёт агрегатов шёл во часов и это пря описание и таски чтобы вы поняли и упало по памяти что-то непонятно что и вот с этого момента велая жизнь аналитика Закончилась И началась настоящая и драма чего драма настоящего Хайда с которого у нас и начинается полномасштабный стенд метрик Потому что есть пару нюансов которых мы не учли это что у нас есть Тесла на расчёт данных это что вообще это у нас хадуп не резиновый и он ограничен том что у нас есть стабильность расчёта и постоянно что-то в источниках отлива и конечно же что в начале агрегатов и метрик было немного а сейчас га тов более чем 900 метрик более чем 700 и количество экспериментов в месяц более 100 и также нужно ещё учесть что ваш код метрик нужно чтобы добавляли Его другие аналитики у нас более 200 метрик добавляется в год и тоже ну как бы это сделать не Изи бизи и конечно же одна из любимых вещей что очень важно чтобы ваши данные были достоверны о чём я пишу далее ну и важно не забывать о развитии продукта и получается так что отл это мы отвергаем нашу нулевую гипотезу и по идее в том дизайне эксперимента который был но это я сделал для флёра Да что а с дизайном экспериментом но мы на самом деле в нём ошиблись мы ошиблись во времени и ошиблись в метках которые мы использовали и для того чтобы у вас таких не было ошибок в проводе У нас есть специальная Команда валидации А тестов которые не пропустит такой тест продакшн о том как построить процессы с валидации и дизайном а тестов будет в статье ждите в июле Итак переходим к лоду оптимизации Итак для того чтобы нам понять что мы можем оптимизировать Давайте узнаем как устроил наш дворец СН metric и у нас есть четыре уровня на которых мы работаем с данными самый низкий уровень - Это работа с файлами Где мы работаем с м hdfs дальше нам удобнее работать в сколе с таблицами для этого мы используем metastore Ну использовали дальше выполнение запросов осуществляет уже спар где он оптимальным образом пытается исполнить все те запросы которые нужны в расчёте и сами расчёт уровня Application находится на самом высоком уровне где мы их описываем и поэтому начнём с самого основного участка который нам необходимо оптимизировать с тем чтобы запросы выполнялись быстро Итак что в целом можно оптимизировать в спарке На что мы можем влиять и что мы подкручивать Ну и конечно же это шал partitions То есть в принципе в спарке есть одна Самая дорогостоящая операцию и в основном все оптимизации идут на то чтобы мы её сделали меньше или вообще убрали давайте быстро опишу в чём её смысл Смысл в том что у вас есть исходные данные которые находятся в датафрейме и дальше вам нужно над этими данными сделать некую операцию но то как эти данные сгруппированы в патиц вам не подходит и получается так что вам эти данные Нужно пере группировать по новому полю которое у нас будет в Операции Red и дальше мы что делаем мы эти данные бьём на бакеты условно на партиции и дальше самое интересное что мы указываем это значение в переменной спарка то есть вот это N после этого как мы побили исходные данные на партиции они уже уходят таски к следующему экзекьют который будет делать операцию ю и как бы В чём основная проблема здесь в том что это Константа и получается так что если мы указываем большое значение у константы этих этой то у нас будет очень много мелких таск и позиций которые необходимо будет исполнить это ведёт к тому что у нас с вами остаётся свободная память в экзекьют плохая утилизация идёт и также будет много таск много таск много накладных расходов Ну и другая крайность что мы шал partitions значение указа маленькое и тогда у нас исходные данные будут биться на большие блоки дальше эти блоки мы попробуем обработать в тас дальше мы в экю можем выйти за память могут начаться с пилы больно и неприятно также мы плохо используем штуку с параллельностью расчёта Да потому что просто мало та что же делать и получается так что нам остаётся одна вещь что эти ша нужно вычислять Итак как В целом можно это сделать что мы и пытались сделать в целом мым в ровка каталист с которым работает Спарк по совершенным запросам оттуда понять примерно объём данных с которым мы работаем И после этого Наша задача определить А какой для нас в целом идеальный размер партиции то есть объём таски с которой нам было бы клёво работать на последующих шагах и тут ну как бы мы определили это о том что это у нас 256 Мб потому что в целом мы агрегаты и медианный размер каждого Ну агрегата 251 МБ дальше Значит мы определили исходный объём данных поделили их на 256 Мб и получили количество партиции то есть в целом так делать можно если у вас постоянный профиль нагрузки но у нас всё ломается Почему Потому что каждый день У нас стартует разное количество экспериментов которые идут на разную длительность каждый день мы используем разное количество метрик и каждый день к нам приходит разное количество юзеров и получается так что в в понедельник у нас примерно такой профиль нагрузки во вторник он у нас уже стал меньше в среду больше и получается так что мы не можем а планировать объём таск Да там и Shuffle partitions отталкиваясь от того что у нас было на истории что же делать Вот и тут как об этом говорят се Бет один ответ ае adaptive quy execution то есть СР сделал обалденную штуку о том что он пота статистике который есть может определить то оптимальное количество партий которые вам нужно сделать и вы можете в настройки указать тот обм который Вам необходим это штука убирает все ручные расчёты которые есть И что же у нас вышло у нас вышло Так что спилы ушли и вы можете прямо увидеть что ипу сайз у нас ну просто просто филигранный идёт близким к тому значению которое мы указывали 256 Мб считаю что это просто успех Итак что мы можем делать е с подкрутили Давайте дальше на же Ну что запрос и первое что мы оптимизируем в запросах это мы оптимизируем работу с источниками данных как Мы помним У нас есть Верки ходу и первое что мы делали мы добавляли промежуточные источники данных То есть к примеру У нас вышло Так что ветика работает не очень быстро мы данные из Верки перекладывать с ним также мы использовали промежуточные таблицы для Преда агрегатов Ну и также как я писал до этого кликхаус у нас быстрее он дублирует даны входу PD фейсе и поэтому мы использовали его для вычисления наших агрегатов Ну и дальше интересная вещь Это после того как мы поработали с источниками то нам что остаётся нам остаётся идти в самое мясо и делать эксплей запросов наших и получается так как вы помните это реальный пример который я нашёл в таска что вот у нас была ошибка что User agregat работает в среднем 8 часов что-то упало по памяти вы можете увидеть что здесь у нас сколько шесть жей Это говорит о том что у нас пять операций шал это ну типа прямо очень больно очень больно Итак что же сделали ребята анализировать план запроса они увидели что пять часов времени уходит на агрегаты с использованием User defined function и первое что сделали его убрали потом в агрегата Как вы помните я указывал что для каждого агрегата мы указываем функцию Да где мы вычисляем на периоде получается так что аккаунт изменили на МАКС стало быстрее и также Вот до этого был докладчику написал про Стак ма утни его до этого использу локальной машинке в панс перекинули на Спарк И это всё привело к тому что у нас стало четыре жа три операции шал И прирост скорости в пять раз вот Итого мы подкрутили Спарк оптимизировали запросы Да ну и опять-таки может бы Окан доклад Но мы его не оканчивает идёт долго Вот и мы подкрутили всё то что было внизу и получается так что нам остаётся одно одно - это пришло время менять что А архитектурой с вами на самый высокий уровень апликейшн где у нас живут сами расчёты и посмотрим что мы здесь с вами сможем оптимизировать это испо в дупе количество рассчитанных данных и стабильность Итак вспомним В чём у нас была задача с агрегатами в том что была исходная таблица которые были все 27 ТБ и дальше мы в разрезе для экспериментов считали агрегаты так какие были особенности что у нас уникальный агрегат это уникальный колоно в таблице да Ну и как бы пока их было 19 было Норм во время было 156 колонок уже не очень удобно да потом особенность в том что Как Мы помним бы аналитика был шаг три да где у нас одна общая операция группировки агрегатов то есть вот эти все 156 агрегатов Мы вычисляем в одной операции вот ну это там можно как-то оптимизировать да И третье о том что каждый агрегат идёт в разрезе по эксперименту который пересчитай Итак Какие из этого вышли боли значит ну первый боль что вычисление всех агрегатов идут в одной жабе видите update User aggregates и получается так что ни один из 100 пся шести агрегатов не может быть посчитано пока не будут данные для всех очень больно Ну это операция дорогая ресурсна и самое интересное что вот эта штука - это Страх и ненависть стент Потому что если вам нужно добавить новый агрегат в эти вы очень осторожно это делайте в коде потому что не дай Бог вы ошибётесь вас ждёт инцидент метрики не посчитано и это в общем боль и ужас и проблема в дублировании данных который идёт дальше Да что мы считаем данные в разрезе по экспериментом но вообще-то у нас эксперимент может идти не один за весь день юзер может участвовать в нескольких экспериментах и получается так что у нас агрегаты для данного юзера везде будут одинаковы и у нас есть дублирование данных Итак Какое же наше решение наше решение - это используя низкий принцип связанности данных в агрегатах отсечь всё лишнее и что сделать перевернуть таблицу то есть уйти от того что у нас очень много колонок и что мы сделали Мы как вы можете видеть в схеме убрали эксперимент ID из таблице условно и дальше все колоночки переметнулся но и дало конечно накладные расходы минусы минусы что хоть мы и убрали лишние данные для каждого эксперимента то всё равно из-за того что мы колоночки перенесли в срочки данных все равно стало больше и как бы это больно и теперь поскольку мы убрали информацию про эксперименты то нам отдельно нужно в жабе считать эксперименты в которые каждый день попал юзер и это Джаба прекрасна а прекрасна она тем что после того как мы её сделали здесь Видно плохо да Да плохо видно она её время исполнения было 6 с по часов то есть Нам нужно было 6 по часов просто для того чтобы мы могли посчитать сколько у нас какий эксперимент попал юзер вижу времени остаётся мало Итак мы оптимизировали эту штуку с эвристика в которой мы просто стали брать первый и последний в течение дня который был у юзера и этом позволило увеличить время исполнения запроса в два раза Ну и вот все эти минусы перевешивают один огромный плюс о том что тепер вы можете видеть все агрегаты считаются по мере готовности То есть это всё идёт как некий да и у нас отлично утилизируется цпу потому что мы не ждём условно какого-то в течение дня как то гото дальше Какая штука мы живём так и помните одна из вещей которые я описывал что очень важна достоверность потому что вообще базовый жизнь у нас что к нам приходят аналитики и описывают что вот ваш энмек посчитал какое-то Ну в общем то что не очень приятно пахнет Вот и Суть в том что у нас Постоянно ошибки и Однажды Ну ну это не так но вот в этот раз это было именно так то есть Суть в том что из-за той архитектуры который сделали мы где мы убрали подсчёт агрегата по эксперименту сделали по дням то у нас может быть так что в один день может стартовать несколько экспериментов если наш эксперимент стартует в 2 дня то у нас сейчас идёт ошибка в агрегате мы учитываем все ивенты за день м лишние ивенты которые были до дву дня и с этим надо что-то делать Вот и ничего не остаётся как ну можно подумать что мы бы вернули расчёт агрегатов по экспериментам но мы сделали умнее мы сделали так что мы день стали бить на Бачи некие отрезки которые мы назвали шарда и теперь в рамках каждого из этих отрезков мы считаем наши агрегаты и получается так что у нас есть типа есть три типа в шада с который мы работаем это шад префикс это тот набор ивентов который нужно учесть в начале эксперимента суффикс в конце и - это за весь день и эта вся штука усложняется тем что Шады нужно учитывать в калькуляции метрик в агрегатах необходимо а вычислять их в тот для того чтобы одеть разрезы условно в которых нужно считать агрегаты Итак и эта штука оказалась непростой вы можете увидеть прямо тас которая у нас была и она длилась со шада полгода мы их делали То есть за эти полгода было измене в 142 файлах и 56 коммитов Ильда Спасибо что ты это сделал и Суть в том что мы это делали полгода Но это привело нас к огромной боль ещё в расчётах потому что нагрузка на расчёт стала увеличиваться кратно и получается так что нагрузка стала увеличиваться на то количество экспериментов которое стало в этот день у нас либо стартовать либо оканчиваться и за верность нужно платить и это наша плата но наша плата этим не окончилась она пошла дальше спустя некоторое время у нас загорелся Мета стор который мы используем загорелся он так что у нас в хаве стало больше чем миллион порций вот всё стало плохо это был фактический инцидент Это был из-за того что каждый день мы поладили от пяти до 10.000 партиции Вы ещё можете увидеть объём файлика он 7 м вот а под него вы блок S 256 я сделано это потому что то позиционирование которое мы использовали было сделан по AV Date SH и agate name потому что в хаве есть ограничение что в нём нельзя сделать запись файл конкурентную и чтобы делать это асинхронно нужно было делать на каждой агрегат партиции Какое же решение решение в том что мы на уровне работы с метаданными стали использовать AP isberg У него особенность в том что он под капотом использует Мета даты файлов в которых можно видеть эволюции и он обеспечивает нам работу с конкурентной записи с транзакциями далее Что ещё мы оптимизируем то есть Что вышло что мы убрали оставили одну позицию в течение дня но боль в чём в том что файлов стало много вот а селективность по файлам плохая Ну то есть типа нам нужно обработать много файлов чтобы взять нужные агрегаты для обработки и как здесь указано приме что условно У нас есть файлы у нас есть пакет где для каждой колонки фай есть Ma статистика Вот но файл обходить неудобно что же придумал наш техлит который в зале а придумал он то что А давайте мы данные отсортируйте селективно сильно выросла что мы можем увидеть на следующем слайде вот в котором У нас есть информации количестве партиции которые мы скинули Да вот их довольно много время подходит к концу Я постараюсь всё-таки общаться медленно оптимизация и стабильность Какая же у нас есть буд не описано в книжках А с которыми он сталкивается а буд его в том что мы обще таки всего озона и у нас больше 100 источников получается так что в течение дня обязательно один из этих источников отблизо рас как оптимизации кото можно сделать это атинг мониторинг культура аналитико О чём будет описано в статье и получается так что мы подкрутили всё сделали и всё равно долго то есть всё равно мы долго ждём И что же делать мы ещё ждём не просто так а ждёт ещё это всё дело в огне и нужно опять чтото оптимизировать ипп архите вспомним до этого мы её оптимизировали в чём Что мы уменьшали связанность в агрегатах А теперь мы Уменьши связанность в Метрика то есть в чём идея идея в том что как у нас сейчас устроен рассчёт он устроен так был что у нас источники данных считается синхронно агрегаты синхронно но Метрика может зависеть от разных агрегатов и поэтому мы ждали расчёта всех агрегатов Ну то есть и дальше и считали все метрики которые у нас есть сейчас у нас больше чем 700 вот что так себе потом мы сделали первую итерацию мы увидели Аха типа есть источники данных длинные долгие А есть быстрые и мы стали бить все метрики на дф быстрых и долгих источников данных и получается так что это ускорило Но это не идеальное решение а идеальное решение к которому мы пришли - это ПГП то есть метрики по мере готовности в которых как только у нас рассчитан агрегат мы сразу считаем метрики как это реализовано у нас это сделано вообще довольно просто гениальное решение о том что в день стата расчёта мы заполняем некую матрицу в таблице в которой есть набор всех экспериментов и метрики которые нужно посчитать потом раз в 30 минут стартует Джаба которой мы смотрим Какие метрики не насчитать А есть ли агрегаты если они есть то мы их считаем и получается так что к чему пришли мы мы э пришли к архитектуре которая обеспечивает то что 700 м к не ждут пока доедут данные для них теперь Значит наш э дежурный как настоящий Хипстер пё просто пьёт смузи Э во время работы то есть у нас теперь лёгкая деградация чего мы очень долго ждали потом у нас есть шикарный мониторинг в котором есть данные в разрезе каждого агрегата метрики эксперимента и самое основное что это настоящий AV Dream который даёт не только попадание в SL но также максимально утилизирует цеп в течение всего дня и мы не ставим и не ждём насчёт метрик и Ключевая вещь в том что мы с этим релизом уехали в прошлом месяце и хотелось бы сказать отдельное спасибо ребятам из ндо метри которые это сделали потому что я просто менеджер который ват ту прекрасную работу которую они сделали здесь ещё стоит сказать о том что внизу это два человека из команды поиска которые были теми самыми аналитиками с гипотезы с нашей дальше уже идёт команда метрик Ребят вы лучши и получается так что из этого доклада вы узнали про эволюцию расчёта метрик про то как он устроен под капотом оптимизации которую мы сделали уров и архитектуру расчёта агрегатов Ирик и хотелось бы сказать ещ раз что у нас все фичи которые в азоне измеряются метрик о том что мерик у нас надёжен чёткий и достоверный и это моя гордость и я влюбился в метрик и в Спарк за которым идёт Ну просто бешено инженерии Буду рад ответить на ваш вопрос Надеюсь что доклад для вас был полезный Оцените пожалуйста его буду рад обратной связи Да И вот Ваня вот ещё на вопрос может отвечать тех Лид которые в зале потому что я тупой менеджер Да спасибо а Итак время вопросов поднимаем руки а получаем микрофоны никого не забудем Сколько времени у нас а есть до 12 да 10 минут хватит на сколько-то вопросов Да можно задавать Да спасибо за доклад У меня вопрос про оптимизацию запросов э выглядело как будто был конкретный кейс где работало долго а вы сделали чтобы работало быстрее А как вы ну на дистанции эту проблему как бы обрабатывайте то есть что завтра придёт аналитик и снова напишет неоптимальный запрос Это отличный вопрос это то о чём я не успел описать в докладе это про то будущее к которому мы идём оно идёт к тому что новые агрегаторы которые будут добавлять аналитики будут идти через некую машинку которая будет анализировать их Запрос который исполняют и алеть если он выходит за некие халды которые мы исполняем то есть это то что мы будем делать в будущем вопрос прям супер Да Жень привет классный доклад Спасибо на самом деле примерно пробил человек мой вопрос Ну то есть вы получается зашли сервиса какого-то когда сам ВС делает А пришли к тому что его запросы какой-то орган Ну ты сам про это сказал что кто-то будет его анализировать то есть тут получается очень большая зависимость от какой-то инфраструктурной команды то есть вот я понял что будет какой-то там сервис который будет анализировать запрос но мне кажется что это там не серебреное пуля оно там сразу Не сработает я е добавлю том что у нас же есть некий процес отдельный как мы добавляем метрики в метри кстати не описал это как у нас сделано у нас идёт Так что прежде чем Метрика подаёт в СН метрик она описывается идёт тех ревю потом аналитик делает код потом этот код идёт в в ревью аналитикам потом у нас в есть отдельный который прогоняет этот код определяет результат расчёта который будет потом чтобы это уронило наш провод идёт на ревю к бэнде Вот и только после этого как ребята ставят лайк они эту метрику выкатывают в продакшн то есть там всё очень аккуратно мы уже обжигали Хороший вопрос Хороший вопрос был какой-то тату Маркет у этого процесс Да хороший прмо больной Ну типа аналитики вообще жалуются вот ну то есть есть те кто описывает что месяц есть те кто неделя есть те кто две В общем у нас здесь большая шумность Вот но очевидно что эту штуку мы будем оптимизировать у нас для этого уже готовится интерфейс чтобы это всё было удобнее накликать и ну боль есть но зато в приводе всё стабильно Да Евгений Спасибо большое за доклад очень интересно вопрос Следующий у меня вы говорили что несколько фич тестируется в рамках одного дня Да их в принципе может тестироваться Одновременно несколько Как у вас определялась выборка для А теста по сессии по юзера а разделялись ли собственно юзеры которые тестируют разные фичи чтобы они не накладывались и не было ли такого что на малом количестве вы могли не попасть вца и убить потенциально эффективную фичу какую-то Да большо спасибо за вопрос Вот и тут отвечу последовательно на него А первое о том что если аудитория у нас небольшая То есть у нас есть в Диме один инструмент который называется э калькулятор mde который вы указываете свою метрику тот эффект который Вам необходимо измерить он может показать то количество дней Сколько необходимо чтобы был АТС Ну то есть атака из той аудитории которая была то есть это решается вторая вещь с дизайном эксперимента СПТУ по сессия либо по устройствам у нас можно и так и сяк Ну то есть делать очевидно что это о той гипотезой которая у вас есть в эксперименте и опять-таки все гипотезы с тестов проходит через этап валидации в которой аналитики смотрит Верно ли всё описано То есть можно делать расчёт в разрезах в любых Здравствуйте спасибо за доклад А в самом начале Вы упомянули про sla про проблему Что там запросы выполнялись 8 часов это неприемлемо и вот интересно А вопервых к чему пришли к каким цифрам и как договорились До конкретного SL и то ли это чего ожидали аналитики и Это хороший вопрос на самом деле ждут не аналитики ждёт бизнес Вот и мы вот в общем то к чему пришли мы в нашей архитектуре Ключевая идея этого доклада в том что у нас получается так что не может быть на все метрики потому что все метрики отличаются очень сильно у одной метрики источник готов условно в ночи то есть мы его уже обработать можем Но в 9 вечера и поэтому сейчас шли в Метрика помере готовности общего SL у нас условно есть SL на каждую метрику если этот SL у нас вается то инцидент уже будет не на нас А на тех людей которые отвечают за источник данных и как только они его починят в это время будет стартовать расчёт метрики вот ну то есть он у нас уникален для каждого набора метрик теперь будет а до этого был в полтора дня и то к чему пришли мы Ну в общем не к тому что у нас ну то есть с У нас остался Мы попадаем в него но теперь Суть в том что как только есть данные для вашей метрики она уже есть Ну типа условно 8 утра одна в 10 утра вторая и это просто вообще Другой мир метрик и это ок для бизнеса Да ну то есть в чём плюс бизнеса это же был запрос от бизнеса у нас Одним из основных заказчиков является реклама рекламе нужно видеть метрики по своим экспериментам как только это возможно и получается так что если у нас passable мы им отдаём и они благодарны нам Спасибо Привет Спасибо большое за доклад я тут тут тут тут на самом деле актуальна для нас проблема тоже Ты в самом начале сказал что вы дублирует трекер в кха и ну в hfs получатся Почему вы сразу не пошли в сторону клика а так Это же очень прикольная штука В чём минус с кликом что если ты пытаешься пойти в Клик взять данные агрегатов Ну условно зна там за 10 дней то наш Клик тебя отблизо для того чтобы мы в фане могли смотреть условно йм метрики для бизнеса А хадуп - это поста аналитика Где мы можем долго обрабатывать и где у нас не отливает и поэтому в этом отношении на посто обработки работа с хопом сильно с м То есть у них отличатся це у на Спасибо аналогично Да привет У меня такой вопрос приходилось ли вам сталкиваться с перекоса данных в спарке во время оптимизации Если да то как вы это разруливает я ему передаю слова раз приди как раз-таки вот Женя описывал сем Бет один ответ ае э это самое наверное лучшее что недавно добавили спарке это как раз-таки ну адаптивное исполнение запросов по пост шал аналитики Ну то есть происходит какой-то шал и появляется статистика для того чтобы мы могли там что-то посчитать и как раз-таки есть одна из настроек как раз-таки которые оптимизируют ск в партиции можно там задать тоже там некие трешхолд что вы считаете как раз таки что для вас что нет Там есть всякие параметры типа Можно мультипликатор делать Ну как некий мультипликатор этих всех параметров Ну и там очень такая довольно гибкая настройка ну и соответственно мы её там полностью обу ЗМ как только можем и она очень хорошо у нас работает Ну и самое что главное нам помогает делать это не заботиться у на вообще нагрузка какой у на запрос вобще что там внутри проходит на статистике по шафла Ну мы можем там всё вот это оптимизировать Добрый день спасибо за доклад появился вопрос Вот вы проводите аналитику там по многим Метрика и всегда есть шумы в данных то есть приходят какие-то мусорные данные где-то кто-то там парсит и так далее вот отсюда вопрос как с этим боретесь Либо это на предварительном этапе как-то отсекает чтобы точную аналитику получить Спасибо за вопрос здесь у нас сделано так что мы на раннем этапе У нас есть команда Анти фрода которая готовит таблицу с информации о юзера из сессии которые роде получается так что мы на этапе расчёта агре их выбрасываем дальше с выбросами на самом деле вопрос не простой то есть одно Делон а другое дело что пришёл один юзер сделал огромный заказ или не о на 20 Да там и они типа повлияли на нашей метрике Вот это уже Вопрос с выбросами то что мы сейчас исследуем То есть как описывал докладчик до этого из Яндекса Что не использу можно использовать для того чтобы опять-таки их можно ить с лями на эту ерику люди не идут и это у нас то есть сейчас на этапе исследований которые идут Евгений Спасибо за доклад можешь пожалуйста подсказать как вы используете Data Quality на входных данных то есть как-то реагируете на них предварительных запускаете и только потом начинаете расчёт агрегатов либо это уже узнаётся по факту расчёта агрегата самого И как вы с этим работаете Есть ли какой-то механизм перезапуска доступный аналитиков или это всё на висит и он должен это постоянно тегать перезапускать например за месяц за год вот собственно такие два небольших вопроса Я отвечу Вам если что Исправь меня Значит у нас сделано как отдельная штука с дата о том что мы смотрим на значение в агрегате которые было за историю И отталкиваясь от тех значения которые были на истори поправит сиг определяем есть условны Интер кото необходи значение в текущий день если мы в этот интервал не попадаем у нас есть ну как бы графана в которые описаны вот эти все штуки Да там если не попали у нас идёт автоматом атна аналитика дальше дюти спрат что у аналитика всё ли Ок не всё ли ОК но если у нас агрегат Равин нулю то у нас он вычисляться не будет И в чём плюс что если какая-то ошибка то там у нас будет автоматом ти перезапуск расчёта и Вань наверное добавить хочешь Да Вань ну да То есть у нас глобально есть алерты на Data кте они как раз-таки приходят сразу после расчёта агрегатов но до этого источники валидировать мы не можем потому что у нас довольно Там могут быть там условно широкие таблицы и какая-то одна колонка не доехала и соответственно мы не делаем какой-то дополнительную историю мы делаем вот расчёт агрегатов и на истории смотрим по некоторым там трём Сигма пытались ещё квантили делать Ну вот пока на трёх Сигма остановились как самое лучшее У нас есть такой большой сервис Где вся хранится Мета информация о метках и агрегатах там есть ответственные за источники за данные они автоматически гаю гается автоматически дежурный Ну и он в таком довольно Ну полуавтоматическом режиме уже начинается разбор ну и соответственно если есть какие-то проблемы Она там искали инцидент и всё такое и потом Да ну у дежурного есть механизмы лёгкого перезапуска Но это на дежурном Да спасибо это был последний наш вопрос который мы успели сейчас разобрать я добавлю две вещи одну из вещей о ценности доклада что я не видел ещё ни одного доклада что там было описано как поэтому в которой будет описано особенности как мы работаем с с парком Ну и третье что я бы хотел сказать ещё что вот Ваня которая окол мене помогает мне и очень плотно помогал с докладом ему отдельное спасибо и Ваня стал тимлидом буквально в прошлом месяце из тех Лида Вот и пожелаем Ване Удачного быть крутым менеджером и руководителем А если не секрет где планируется выход статьи на Хабары статья будет на хабре отлично в блоге Озон бло Озон отлично Подписывайтесь чтобы не пропустить её и сейчас сложный вопрос кому хочется за отличные вопросы вручить подарки подарка два у нас так значит один вопрос был первый Да там интересный по поводу чего он был сечас с запросами Да да да с оптимизацией а с тем как мы Решаем проблему плохих запросов аналитиков Вот это было Вот молодому человеку в очках Угу Вот и второй вопрос был очень больной который спросил как раз участник конференции это по поводу сла как мы попадаем в него как мы в него вписываем из-за это у нас инциденты были это прямо жизне боль меня такие помет готовности которых решает Поэтому вот молодому человеку по второй подарок Ну и спасибо вам большое на самом деле я волновался переживал готовился кПа 4:00 утра Вы лучшая публика Да вот которая есть на самом деле да Вот и спасибо вам за то что за ваш вайб вот и что я получил удовольствие доклада"
}