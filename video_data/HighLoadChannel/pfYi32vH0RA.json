{
  "video_id": "pfYi32vH0RA",
  "channel": "HighLoadChannel",
  "title": "MySQL Replication vs Galera: что лучше для твоей нагрузки? / Николай Ихалайнен (Percona)",
  "views": 3148,
  "duration": 2792,
  "published": "2020-04-14T10:49:32-07:00",
  "text": "меня зовут николай холодит я работаю в компании пир кора эта компания которая занимается mais quel мозга де пье воскресил выпуская свои дистрибутив майской чем достаточно известна и сегодня я буду рассказывать про виды и репликации поиску и буду рассказывать как они работают и из этого будет следовать почему одни лучше для конкретной задачи другие хуже каждое приложение начинается с одной строчки и чаще всего сразу после этого мы задумываемся о том где данные хранитель и хранимых в одном из носимой заданных получаем у какого-нибудь системного администратора логин пароль и к на типсах у такого подхода огромный недостаток как только мы выросли за пределы самого большого сервера нам приходится искать новую работу и пусть другие люди занимаются переписыванием вот работал с большим количеством серверов почему этот переписывание требуется потому что если приложение отправляет запрос и случае репликации эти запросы уходят на какой-то сервер и уже этот сервер сможет распространить запроса на остальные реплики и приложение должно знать что кроме того как на главном сервере есть данные данные можно получить и на дополнительных репликах самый простой способ репликации который приходит голову берем тот же самый запрос который только что выполнился на мастере и как есть отправляем на злых этот подход достаточно долго был в майской какие-то люди до сих пор пытаются использовать стоит нас баязид репликацию к сожалению такая репликация легко расползается из-за различных видов запросов а также из-за изоляции рядками ты так вот решение этого вопроса это после того как и транзакция выполнилась собрать все строчки которые поменяла транзакции и от на свой свой в получил строчку и после этого он должен эту строчку вы помните если в табличке нет никаких адекватных индексов то морской ничего не остается кроме как сделать полный скан таблички такой метод репликации может оказаться сесиль на медленнее на плохо созданных базах поэтому обязательно делайте прайма реки или ключи которые хотя бы будет иметь большую кардинально sti что же будет передаваться может передаваться как полностью вся строка может передаваться доставка которая получилась после обновления а также может в случае апдейтов передаваться и как то что мы поменяли так это что у нас изначально была событие репликации будь то запросы или строчки они куда-то должен записываться войска ль записывает эти события репликации в бинарный лог когда slave пытается этот бинарный локу получить с мастера он записывает его себе локального систему в белый лак для того чтобы понять на какой именно позиции бинарного logo мы находились когда мы начали репликацию и на какой мы сейчас находимся мы еще используем дополнительные файлы с метаданными которых прочим могут сохраняться и в транзакционному движке сохранение на дебит если у нас мастер выполнил запрос сразу после этого умер и не успел передать данные то клиент на результат commit получается окей мы сохранили данные приложение больше не будет повторять попыток сохранить эти конкретные данные но мастер умер и больше мы не увидим этих событий потому что своих не успел читать чтобы избегать этой ситуации есть 70 к репликации в ней можете задать от одного можно больше серверов слоев которые должны призвать подтверждение о том что они получили это событие получил это не значит что он обязательно исполнит на своём может произойти ошибка при применении данного события репликации но это хотя бы лучше чем ничего другой вариант если у нас умирает life казалось бы это не такая большая проблема потому что у нас все данные есть на мастере и даже если is life откатит последние несколько транзакций то он сможет продолжить с нужной позиции и получите консистентную с мастер базу однако какая позиция нужно если у нас используются только транзакционные движки хранения например и на гибель то мы можем включить дополнительные возможности майской например опять же сохранения метаданных табличках и на гибель и тогда своих когда будет стартовать после сбоя будет откатывать те транзакций и в конечном итоге он найдет нужную позицию если мы на мастере будем одновременно пытаться закомитить несколько транзакций то эти транзакции можно записать на диск одним махом это технология группками то мы можем также заставить мастер подождать какое-то количество микросекунд до тех пор пока у нас не накопится нужное количество транзакций например хотим мы количество сбросов на диск из за транзакции уменьшить в 5 раз и при этом у нас постоянно 1020 одновременных запросов крутится пожалуйста выставляем нужный тайм-аут для нашего количество запросов в секунду и получаем что к количество записей надеюсь к пропорционально уменьшается следующая важная особенность которая нужна для репликации работать с оксидами в файлах бинарных логов очень тяжело потому что каждый раз когда майскую серверы стартует он начинает новый бинарный лака и таким образом бинарные логе название файла это что в них содержится они разные на разных серверах классной репликации когда мы захотим переключиться с одного сервера на другой у нас возникнут проблемы чтобы с этой проблемой бороться мы на каждую транзакцию которая комитет мастер создаем идентификатор это индикатор серого мастера и номер транзакции начиная с начала времен когда сервер только был установлен если кластер живет достаточно долго то общее состояние нашего костра можно описать будет уже не одним идентификаторам и числом большим количеством таких пар и это неудобно и в какой-то момент приходится останавливать запись весь кластер и производить затем этих глобальных закатов транзакции следующий момент раз мы уже выделили транзакция как что-то особенное что у нас одинаковая по всему кластеру почему бы нам и все строчки которые меняют транзакция не выделить в отдельный рецепт то есть рецепт это просто строчки который поменяла транзакция и в этих строчках у нас будут название база данных таблички и значения прайма реки если для каждой строчке мы посчитаем хэш то это нам поможет в дальнейшем потому что мы можем уже сравнивать транзакции меняет ли они одни и те же строчки или не меняют sleeve исполняет запросы как раньше было в один поток поучили искали запросов мастера исполнен использовать просто от мастер мазь запрос на мастер исполнялся два часа значит на свой веет запрос закончится работать если свой в такой же быстр как мастер через 4 часа если мы используем базу данных для совершенно разных активностей то для одной активностью у нас будет например форум какой-нибудь одна база для наших заказов у нас будет другая база да практически все запросы которые будут работать с этими двумя базам данных не будут пересекающихся по базам поэтому их можно исполнять на своего параллельно это не очень эффективно потому что по большей части даже в таких ситуациях возникает связь между одной базы данных и другой базу данных или нам гораздо проще разделить эти базы данных разные сервера чтобы решить эту проблему команда москаль добавила другой тип параллельно репликации если транзакции входили в один и тот же группками ты-то эти транзакции исполнились одновременно с точки зрения пользователя значит мы их можем параллельно исполнить на слове я за акции которая более длительную работу имеем мы должны четкую последовательность выдерживать если у нас транзакция закончилась раньше-то пользователь может ожидать что и на свой век он сначала увидеть одни данные потом другие данные так вот даже если у нас не было параллельного исполнения на мастере мы можем с помощью расчетов и ходжей на строчками выяснить является ли транзакции конфликтующими или нет конфликта транзакции возникает в тот момент когда мы пытаемся поменять две строчки как база данных обрабатывают конфликта они используют блокировки но случае репликации у нас блокировки только запутают потому что нужная транзакция может не получить такую блокировку которой она получала у уже на мастере и это не сработает но с помощью рецептов мы можем получить параллельное исполнение даже если транзакция изначально не исполнялись параллельно мастере таким образом мастер исполнял много-много транзакций последовательно в один поток свой вы могут ту же самую работу сделать гораздо быстрее потому что они могут исполнять это 8 или в 32 потока когда у нас есть очень быстрое исполнение когда у нас есть возможность для каждой транзакции понять как она меняет состояние всего к старой репликации мы можем задуматься о том чтобы уже начать менять все строчки во всех серверах одновременно опять же за счет репликации ракетами мы можем легко разруливать конфликты на разных серверах и так как с помощью глобальных индикаторов транзакций вы точно знаем что данный набор детей не соответствуют определенному состоянию кластера если у нас в этом состоянии узел не может выполнить запрос конфликт строк например не существует такой строке значит с узлом что-то не так надо его просто пересоздать на всех узлах одинаковой дана команда mais quel сделала новый протокол репликации это новая сетевая часть которая опирается на старые механизмы выполнение запросов протокол называется extend communications и по умолчанию он очень похож на обычные сильфоны мастер sleeve у вас есть сервер в которую вы записываете и сервер и много серверов из которых вы можете читать данные и если сервер который праймари в которой вы пишете пропадает то кластера автоматически переключит на другой сервер другой сервер станет проект в режиме мульти праймари вы можете производить запись одновременно во все узлы протокол связи и в групповой репликации основан на алгоритме концепции сип access и он обеспечивает единую последовательность фиксация транзакций вообщем классная вы исполняете транзакции параллельно на узле один на узле 2 на узле 3 для вас может показаться что порядок исполнения транзакции совершенно случайный для того чтобы базы данных работала и для того чтобы она была консистентные на всех узлах мы должны гарантировать что порядок применения транзакций будет в кластеры все время определен после того как они испортились так вот как же работает алгоритма консенсуса по кс у него есть две части сначала происходит выбор лидера выбор того кто будет предлагать изменения после того как мы осознали что этот например будет узел один мы можем позвать само изменение на все сервера и как только все сервера даже не все сервера как только кворум как только большинство из этих сигналов вернуться туда эту транзакцию можно применить то мы принимаем эту транзакцию а почему транзакцию нельзя можно применить и потому что на другом сервере может быть начато какая-то транзакция раньше который меняет те же самые строчки одни и те же самые строчки в кластере транзакции не могут менять одновременно значит одна из транзакции будет отменена рубок алгоритм maxus он явно содержит много дублирующий работы зачем мы каждый раз выбираем лидером почему бы не взять и не выбрать лидера один раз а потом производить именно посылку транзакций и применения этот вариант он не подходит для мульти мастера для мульти мастера нам нужен какой-нибудь фокус - это разновидность алгоритма paxus делает этот фокус мы все время которое у нас есть разбиваем на слоты и в первом салату будет лидером первый узел условно 1 потому что несоответствия между номерами узлов в котором вы назовете и действительно что-то будет по номерам сватов во втором после того как первый узел отправили транзакцию и и приняли будет работать второй узел предлагать третий и так далее 1 если у нас три узла если какому-то узлу нечего сказать например если вы пишите все изменения только в один узел то узлы котором нечего сказать просто отправляется сообщение по пропускам и диски алгоритм paxus требует чтобы мы посылали полное сообщение со всеми транзакции между серверами в икс-ком есть оптимизация что нам нужно посылать большую транзакцию с данным тоже только один раз если сервер пропадает ну например потому что произошел сбой the web access не описано что нам это делать разработчики майской xcom они решили что сервер с наименьшим номером свата будет обрабатывать этот пропавший слот из того что у нас есть какое-то определенное количество слотов следует то что у нас должно быть ограничение а сколько у нас вообще должно быть максимально узлов на которой происходит запись команда моей сколь в выбрала цифру 9 в живую я видел сервера с синхронная репликация максимум 7 узлов достаточно часто встречается пять узлов наиболее часто встречаются три узла если узел будет слишком долго не отвечать на сообщения например если ему надо передать очень большую транзакцию то остальная часть кластера посчитает что этот узел пропал и выкиньте его из костра чтобы с этим бороться есть тайм-аут и а также большие транзакции мы можем разбить на несколько пакетов и потом эти пакеты соберутся в одну большую транзакцию у которой опять же есть и какой-то ограниченный максимальный размер даже если вы используете групповую репликацию в режиме сингл праймари это гораздо лучше и проще чем обычная синхронная репликация потому что у вас есть автоматическое управление всеми узлами и так как эта синхронная репликация то либо запрос выполнится на всех узлах либо он вызовет ошибку и вы увидите эту ошибку сразу и сможете есть брать режим мульти праймари он более продвинутой потому что он налагает ограничения например если вы работаете изменяете базу только на одном узле то у вас будут работать блокировки строк на промежутке между строками такие блокировки нельзя дешевым способом реализовать на всем кластере поэтому эти блокировки не будут работать в режиме multiply мире также если мы делаем на одном узле азер ты бы она другом возле мы делаем изард в эту самую табличку to insert может провалиться потому что у нас одновременно меня вас таблица поэтому в режиме multipro мире не крым дуется под высокой нагрузкой менять таблички если мы вспомним про рецепт это у него ограничение что внешние ключи для расчетов не работает внешние ключи в московии реализованные на уровне на 100 рассчитан на уровне движка хранения и поэтому они не могут работать до уровня репликации мы изменили какую-то строчку и часто приложение которое привыкло к этому серверу она ожидает что эта строчка следующем запросам куда бы мы его не отправили на обязательно будет на другие сервера это не работает для виртуальной синхронных репликаций поэтому нужны и специальные режимы работы транзакции в которых мы можем сказать что перед выполнении моей транзакции пожалуйста пусть под запрос подождет пока все текущие транзакции не завершатся или после выполнения моей транзакции не возвращайте мне сразу окей что оно принято подождите пока все изменения точно появится на всех узлах для того чтобы начать репликацию у нас должно быть одинаковое состоянии все одинаковые строчки таблицы на всех узлах если узел недавно был в репликации то у него такой же набор g тейде просто на какие-то из ювелиров и секундомера эти sequence number и немножко меньше если мы запустим репликацию групповую с таким сервером то он воспользуется обычной асинхронной репликации догонят до текущего состояния костнера а потом включат этот узел в костер для того чтобы с нуля начать дальше репликацию мы должны использовать backup мы делаем backup обычным способом который позволяет использовать глобальный в которой транзакций и после этого у нас вот эта разница она наконец а с помощью вот этого до автоматического использования асинхронной репликации это не очень удобно поэтому это не позволяет сделать автоматическое управление узлами поэтому команда майской реализовала погиб которая делает то же самое что мой сколь этапа из бука бы то же самое что делает их стрельбе кластер но делает это когда оба сервер майской находится во включенном состоянии сервер который подсоединяется запрашивает новые данные а сам удаляет все данные на диске которого были на сервере которые являются донором происходит копирование всех файлов из директории world морской в такую же директорию на последнем ся после того как мы скопировали все файлы мы не можем стартовать базу потому что mais quel и и no debe действует как кэш на запись конечно запись имеет какие-то данные в памяти и более того для того чтобы это все было консистентная когда мы уберем питание от сервера или когда мы убьем процесс по иску людей есть еще в лог транзакций который позволяет повторить те же самые действия и после того как все действия повторили с момента как мы начали копировать данные до момента как мы закончили у нас будет база в consistent нам правильном состоянии и там не будет ошибок что вот у нас есть гитаре а вот у нас есть 2 гитаре потому что здесь мы сделали разделение двух страниц но мы это разделение сделали в памяти и влоги транзакций в файл и еще не успели записать так вот поэтому следующая стадия после копирования файлов это копирования модифицированных страниц из работающего сервера в другой работающий сервер в памяти последняя стадия это как раз копирование logo транзакций решение просто репликация недостаточно для того чтобы удобно работать сквозь драм нам еще нужен способ чтобы мы могли из приложения назначить какой-то сервер какой-то порт и мы знаем что это порт используется для записи все запросы на at&t ли ты сердце они будут идти туда другой порт используется для чтения и тогда если мы находимся в режиме сигнал праймеры там айс-кофе роутер автоматически все запросы на запись будет направлять на живой праймари а все запросы на чтения будет по-вашему алгоритму как вы зададите распределять между остальными серверами следующий важный компонент для и no debe кластера кроме групповой репликации майское роутер это мой сколько мои сколь шоу это утилита командной строки и и перед для бетона и java скрипта это итог позволяет простым способом буквально за одну команду сказать что на этом узле находится кластер на этом узле находится новая нота которая должна присоединиться к костров и если мы добавим узел который полностью чистый томас корешь задаст вопрос в текстовом виде а хотите ли вы использовать клон для того чтобы его установить также мы можем видеть статус текущего состояния сервера проект групповой репликации существует уже три года и вы можете сказать что достаточно стабильной к сожалению кроме самой групповой репликация есть улучшение которое в ней происходили есть дополнительные возможности как как коллагена которые появились буквально несколько месяцев назад поэтому несмотря на то что решения нагиба классно рано очень хорошая я рекомендую вам как ним попробовать его но для продакшена вы должны предварительно провести хорошие тестирование для того чтобы выяснить что у вас нет самого падения поисково сервера и для того что у вас нет ситуации с вашим приложением когда все узлы покидают их кластеры например еще недавно одной огромной транзакций которые мы отослали из клиента было достаточно для того чтобы все узлы покинули кластер перка на x-terra деби кластер это другое решение которое также позволяет делать синхронную репликацию это решение основано на галера галера это протокола репликации который был добавлен очень давно еще в мае сколь 55 и поэтому это достаточно стабильна и решение и сейчас все улучшения которые делаются и в gallery избиркоме кстати by кластер направлены для того чтобы как можно меньше действий производить вы хотите добавить новый узел вы просто прописываете в конфигурационный файл где им этому новому слову на каких городах и на хлебе адресах найти новый костер и он автоматически скачает все данные с других узлов и стартанет также как и случае сын египет кластер на каждом узле находится полная копия датах поэтому даже если мы хотим писать сразу много узлов мы не получим с этого большого прибытка потому что все равно все изменения должны быть записан на все узлы и в том числе вас самый медленный узел и самый медленный узел замедлит у нас весь класс звучи карьера если узел теряет связь с кластером то у него могут быть отстающие данные чтобы приложение не читала отстающих данных выключается возможность как записывать в этот узел так и считывать из него да так как галера была реализована очень давно еще даже до появления g5 в моей школе то она опирается на внутренности самого мои сколь сервера там есть руки на определенные события в моей школе и не используется напрямую бинарный ловко для создания репликации а если мы бинарный лоб не используем то куда же мы должны складировать события репликации события репликации складируются в циклический буфер чаще всего это циклический буфер находится в оперативной памяти но иногда при очень больших транзакциях он может и вылезти на дисках но чаще всего это оперативная память поэтому однозначно быстро под ракетами в gallery понимается просто выделенные строчки из транзакции которые были изменены какие-то дополнительных и шеи не читается как же галера обрабатывает типичную транзакцию после того как полностью транзакция было исполнено транзакция повисает на комедию приложение видит что commit из буквально несколько миллисекунд или микросекунд мог стать долгим и за счет чего он тонкий быстро операция выделить ракета быстрая операция поучить их кадры транзакции от них который транзакций внутри галера потом этот раз эту сеть и должен быть передана остальными сервера и мы ждем того момента когда нам станет понятно что другие сервера исполнили нашу транзакцию напрямую другие сервера не посылают подтверждение что не исполним нашу транзакцию вместо этого используются номера последовательностей как только номер последовательности другого узла для большого зала увеличился до нужного значения значит наши транзак собой исполнен в этот момент мы отправляем аки клиенту этот протокол консенсуса это темп сегава рекордере и выше протокол он достаточно сложный я не буду рассказывать детали но смысл следующий что у нас каждый узел обязан сделать какое-то действие для того чтобы транзакций за комитет при этом много параллельных транзакций можно работать одновременно то есть транзакция относительно друг друга совершенно синхронно и то как применена транзакции или нет используется растущей номер последовательность что за последовательность такие у каждого сервера есть за циферка и он хранит циферки такие же для других серверов эти циферки при перезапуске всего костра они не сбрасываются на 0 они остаются при исполнении каждой транзакции циферка увеличивается на единичку если мы получаем другую транзакцию то там будет прописано что видео эта транзакция какой последует последний sequence видео эта транзакция и какое у нее был sequence на момент исполнения это все нам позволяет понять тот диапазон кадров транзакции в котором могут быть и конфликты ведь если транзакция спиртного до новой транзакции и у нас есть конфликта мы значит новую трансляцию должны отменить в случае асинхронной работы может возникать такая ситуация когда наши транзакции накопились на каком-то медленном серверами ну например диски у себя в гараже зуба или и он перестал флашей транзакции на диск с нужной скоростью у него накопилось очередь это в случае перк оникс среди big кластера 100 транзакций больше и в этом случае это configure параметры это в этом случае тормозящий узел отошлёт всем сообщение flow control сос я не могу исполнять на 1 акция с вашей скоростью все узлы как только получает сообщение flow control тормозят свое исполнение таким образом есть возможность не выбрасывать узел из костра а просто квартир становится чуть чуть медленнее так же как и в случае с энди by костер когда мы применяем транзакцию она не мгновенно может быть времена на остальных узлов чтобы с этим жить и работать есть специальные переменные и функции в четвертой версии галера 8 версии для комикса риббек лазера который позволяет подождать определенном событии в качестве балансировщика нагрузки в пикси используется прокси сковиля это на мой взгляд более продвинутое решение по балансировке так как он работает не только с портами но и может на основе регулярных выражений работать а также он полностью понимает протокол майской и может понять разницу это просто запрос или это запрос транзакции если у нас есть какой-то буксировщик нагрузки он может быть узким местом во первых во вторых единой точки отказа случае с прокси sky вы можете запустить много прокси-серверов совершенно разной конфигурацией если вы будете менять конфигурации на одном сервере и из кластерах то автоматически конфигурация будет и на всех остальных узлов конфигурация происходит с помощью названия запросов по порту 6032 по протоколу майской эти запросы могут менять табличку с серверами состоянии серверов отслеживаться автоматически если у нас сервер пропал из репликации то из прокси и вскоре он тоже будет помечен как недоступной так как у нас есть двойная конфигурация пользователей чтобы пользователи должны создать и в пропуска или и в майской если мы создаем пользователю в мае сколь мы можем воспользоваться скриптом который делать синхронизацию между этими двумя разными местами хранения вот самое важное на мой взгляд особенность это то что мы какие-то запросы можем отправить на одну группу серверов какие-то запросы на другую группам сервера или даже переписать запросы в режиме реального времени чтобы их заставить использовать другую яндекс или чтобы поменять по вопросам например программисты очень не любят добавлять епихин ты в запросе и в целом их можно понять потому что киты могут работать хорошо на одной версии а потом этот хит использую яндекс перестанет работать просто потому что этого индекса не стало в новой версии базы данных а сами запросам и забыли поправить если мы перезаписываем hand и на уровне праксис quelle то это список того чтобы перезаписываем гораздо проще видеть галера поддерживает работу через интернет с как она может это сделать потому что все охраны репликации мы же ведь отправили запрос и пока этот наше транзакции не будет применена вот это центре дата-центре бы мы не можем клиенту вернуть окей а значит как минимум время передачи данных из одного конца в другой она будет очень сильно влиять на скорость работы кластера тут такой момент что не всем приложениям нужна максимальная скорость есть приложение которое нужно надежности и при этом при параллельной работе в разных подключениях будет возможность создавать много параллельных транзакций которые будут создавать приемлемую скорость работы потому что вот эта задержка на передачу она влияет на одну транзакцию но для двух транзакций параллельных она будет совершенно такая же кроме этого мы можем задавать создать арбитра это специальный узел который не требует непосредственного применения данных на нем но которое будет стоять в третьем дата-центре и если дата центра а умер а orbiter видит дата-центра б то у нас все еще сохраняется к вору если произошло наоборот the data center а будет работать если вы просто поставили до 2 дата центра и обычно синхронную репликацию то это нормально работать не будет автоматически потому что может будет специальный человек который будет проверять а х а наши пользователи они видят дата завтра а больше не дата-центр b или дата-центр б полностью пропал также галера позволяет передавать трафик репликации через какой-то узел если у нас связь через интернет мы можем задать сегмент один в дата-центре а сегмент 2 dtc при бы тогда все изменения которые сделаны в первом дата-центре они будут локальное передаваться на каждый сервер а все изменения которые выйдут в 2 дата-центр они будут скопированы с первого узла который получил эти изменения и того чтобы работать с а тертый был в галерее есть два способа первый способ по умолчанию мы блокируем все запросы нам модификацию данных которые есть в кластере одновременно и одновременно начинаем исполнять альберт apple это гарантирует то что у нас не возникает проблема того что мы делали актер ты был на одном узле и в этот момент мы попытались поменять запросам табличку старом формате и что нам делать когда этот запрос будет реплицироваться но у нас появляется возможность подвесить весь кластер за счет 1 артикул чтобы этого подвешивания не произошло вы можете использовать утилиту 5 онлайн таким очень это утилита создает теневую табличку с новой структурой после этого в нее начинается копирования с помощью insert игнор селектор из оригинальной таблицы сюда плюсом все изменения которые произошли за это время они будут переданы из оригинальной таблицы в теневую с помощью триггеров когда все скопировалась произойдет своп этих таблиц таким образом блокировка всего классного происходят на быстрые события на создание триггера и на создание новые пустой таблицы и нас в лоб таблицы все эти операции не требуют копирование большого количества данных в этом они быстры для того чтобы получить на новом сервере те же самые данные используются полной боковую только это полный backup полностью автоматический вам достаточно просто настроить подключение узлов кластера в конфигурационном файле после этого стартануть этот узел и рано или поздно полностью произойдет быковать на одном из долларов узлов и подключающийся узел подключиться полностью коста если нам по каким-то причинам надо выключить узел например чтобы измените конфигурацию которая не динамическая или для того чтобы еще что-то полезное сделать то мы можем использовать инкрементальные перенос состояния которое из буфера аджика ешь существующего сервера передаст на разница на подключившись в ярком декстер baby class на восьмой версии появилась возможность плюсом к вот этому автоматическое восстановление узла автоматически выставлять узел совершенно другой мажорной версии майское ли при этом будет автоматически выполнит моей сколь апгрейды и сделаны важные действия например для того чтобы а с 5 майской обновиться то с 5 7 мая скоро обновится до 8 майской вам нужно правильным образом остановить мой сколь сервер и удалить логе транзакции это будет автоматически сделано в отличилась чтобы 7 транзакциями в 4 gallery и и все восемь есть технологии streaming реплики который делает делит большую транзакцию на фрагменты и как только первый фрагмент будет сертифицирован другими серверами эта транзакция она уже не будет отменена конфликтующим и ту же возможность мы можем использовать в ручном режиме для того чтобы изменять горячие строки когда в нашем кластере одна строка меняется сразу во многих местах на многих узлах кластера то постоянно возникают откаты транзакций чтобы этого избежать мы объявляем эту транзакцию как котлин даже если она меняет одну строчку и у нас этот запрос получит боевики то я рекомендую вам попробовать и и на тебе кластер и перка на x-tribe костнер это очень просто сделать с помощью убираетесь а и с помощью утилиты написаны на гол который называется 9 пор ее очень легко установить через get хоп после этого вы скачиваете дистрибутив либо софи циального сайта либо используйте команду для скачивания и за просим кука мода на вашем локальном ноутбуке или каком-то серверы для тестов которого вы используете поднимается нужное количество узлов вот эта команда поднимает на одном сервере на разных портах кластер из трех узлов и на типе костра в режиме секу примеры можно ей в августе пропустить то же самое для пикси практически все возможности которые есть и в том решение в другом я уже рассказал вот итоговая таблица который просто подводит итог поэтому если у вас есть вопросы обязательно задавайте я повторю вопрос на настоящий момент готов ли до готов ли перка на x ребекка астра 8 версии он существует он готов для теста но не готов для продакшна для того чтобы какая-то технология по мнению бирками стала готово для продакшена она должна уже поработать на каком-то количестве серверов плюс проходить все наши тесты плюс должна быть реализованы все фичи потому что геркона старается не добавлять фичи когда уже есть запись поэтому сейчас вы можете скачать можете попробовать но в про также не рекомендуется использовать в риге за тоже не был есть только бета-версии спасибо большое вопрос такой если пикси есть скажем пять узлов там просто про тотем было сказано это что он будет ждать подтверждение от каждого узла если приходит большие сетевые ножницы у нас слева остается два узла справа три узла он учитывает какое количество узлов было пластыри чтобы был вором чтобы эти два а отдельно работает механизм сертификацией и отдельно работает механизм apple я не какие узлы жила на момент когда остальные два узла после тайм-аута это по-моему что-то вроде то ли 15 секунд или 30 секунд до момента когда будет ясно что других узлов нет мы отпустим транзакцию которая ждет подтверждение от двух мертвых узлов и образовавший спать а если часть клиентов видит два сервера часть 3 то будет работать только произойдет пропадании сети на том узле который вызвал проблем у нас допустим мы посылали запрос на один из двух мертвых узлов то клиент увидит ошибку узлы которой не имеет корма в пикси они получают ошибку на любой запрос есть специальная опция которая позволяет включить чтение но она выключена по умолчанию потому что это неправильно еще вопрос есть у нас используется 56 пикси и останавливали одну машину удаляли данные делали полное восстановление и после этого через какое-то время обнаружили то едите один крючок на них были разные на них были разные наборы джеки день дело в том что толстым бар сильно отставал это был не лак они прям вот очень да дело в том что жить и аде и которые используются индикаторы транзакции пикси это друге идентификатора не те которые московские жить эти мои скобелевский джи ти аиде их надо прописать правильным способом были версии и экстра backup который использовался используется как раз для из асти которые не прописывали джек иди на момент быкова и там оказывался житья иди на момент отката транзакций по моему чуть ли не сбрасывания жить ради там на единичку где это происходило и в этом случае у нас сервер который вы становился он содержит все правильные данные он совершенно корректно работает в костер и но он не может использоваться для создания репликации джеки иди потому что сама позиция которая восстановил экстра пока она не корректно чтобы с этим бороться это по-моему не было проблемой в 5 и чтобы с этим бороться можно использовать для хранения вот как раз метаданных репликации этих мастер игровых файлов можно использовать таблички тебе для того чтобы корректно recovery происходил но другие вопросы вы можете задать диффузионной зоне спасибо вам большое за доклад"
}