{
  "video_id": "Hga5OPvPh-Y",
  "channel": "HighLoadChannel",
  "title": "Как мы варим данные Gigachat Pretrain / Иван Басков",
  "views": 794,
  "duration": 2873,
  "published": "2024-12-25T05:39:24-08:00",
  "text": "Всем привет А для начала познакомимся немного обо мне а я сейчас руковожу подготовкой данных для гига Chat при Train а закончил шат с отличием а поработал в Яндексе делал голос Алисы перевёл синтез с конци на полностью нейронный Ну и сейчас делаю крутые данные для ЛМ Ну и первый вопрос Что за собственно такое Трей да это у нас самая тяжёлая самая значительная по ресурсам стадия обучения ЛМ а тысячи видеокарт месяцы обучения а огромное количество данных триллионы токенов десятки теб текста а и от этой стадии наибольшее э влияние на то что ваша сеть будет знать или уметь Да а потом её уже сильно дооб учить не получится Соответственно что может быть потом ещё рассматривают в основном пару стадий sft - это сува fing Она учит сеть инструктивными способностям то есть сеть учится выделять что же хочет пользователи вмте находит инструкцию и пытается исполнить как может ну и RL - это уже шлифовка самый простейший случай ли - это Direct Preference optimization мы показываем в сети два примера один из них хороший другой плохой и она учится смещаться в сторону хорошего например так можно до обучать модели математики показывать ей решение правильное и неправильное сегодня мы поговорим из чего вообще состоят данные трейна как их можно обрабатывать рассмотрим дупликации фильтрацию и кластеризации также Как сравнивать различные данные и немножко поговорим о том как генерировать новые данные синтетически данные тре в основном состоят из того что у нас есть в Интернете то есть есть скачанный интернет отдельно выделяется в нём достаточно большая подгруппа это кодовые данные которые лежат на гитхабе и ещё различные данные из более таких мелких доменных источников Ну например действующее законодательство и прежде всего мы поговорим про то что у нас есть вебе соответственно мы используем открытый он вет всего у нас там данных для обучения около там 20 пиб и обкакаться отдельная группа гата для обработки Именно таких текстовых данных такого количества нам требуется M инфраструктура мы используем CL Spark на 3.000 ядер обработка такого количества занимает несколько месяцев И что я имею в виду под обработкой да то есть вообще говоря в ко есть не только HTML там есть уже пропар с этих HTML текст который лежит в отдельных файлах которые называются файлы Ну вот в статье DM на которую я сегодня ещ буду сила замерили эти файлы и также другие Методы обработки данных да то есть как именно из HTML получить текст мы используем Фира в статье использовалась библиотека и вот сравнение показывает что файлы ведут себя Хуже всего да то есть это на тесте данных которые у них получается и чтобы проиллюстрировать этот пример я вам покажу сейчас Вы наверно Тут немного увидите Но что будет если мы возьмём для какой-то произвольной статьи текст из файла я тут даже разбил на два Потому что тут всё начало слайда это полностью какой-то бессмысленный текст он содержит в себе навигационный элементы назва фо но содержит какие-то подписи картинкам которые показываются когда картинки в браузере отключены и много всякой такой фигни на самом деле до текста мы тут даже ещ не дошли А вот что будет ЕС мы обработаем это фиро здесь весь текст осмысленный я бы отсюда убрал один элемент который вот в самом верху написано фото пресслужба промобот но в лом всё остальное я бы оставил также видим что размер текста уменьшился с 33 КБ До Д то есть более чем в 15 раз то есть из страницы мы достали самую мякотка Что можно делать с данными Web да то есть мы сегодня поговорим про три обработки основных Первое - это дедупликация далее о фильтрации и о кластеризации собственно начнём с дупликации Зачем её делать да то есть вот есть такая Достаточно уже старая статья ей уже наверно года два про то что будет если вы будете учиться на данных которых содержит много дублей соответственно Провели два эксперимента в одном дубле исключили и мы видим что в эксперименте где дубле исключены Он учится в два раза быстрее достигает того же значения метрики за в два раза меньшее количество итераций и дупликация она бывает разная соответственно самая простая - это просто убрать одинаковые элементы которые абсолютно совпадают но если чуть пойти сложнее то есть нечёткие методы для дупликации и мы чуть-чуть поговорим про нш Здесь вы видите так называемые сигнатуры ш обычно там значение функции которые немножко сложнее у нас там используется fp64 Но для наглядности я просто сделал бинарные вектора и по этим двум бинарным векторам мы можем посчитать похожесть как получаются вектора то есть генерируются какие-то случайные хэш-функции которые потом шит документы документы можем вать по-разному ну самое простое разбивание документа - это Например Через пробел по словам и далее в соответствующий Слот записывается минимальное значение хэш-функции которое было получено при проходе по этому документу дальше для того чтобы сравнить два документа мы просто сравниваем совпадает ли у нас значение хэш-функции и мы делим количество таких совпадений на количество слотов получаем похожие ard дальше Мы можем отсекать по какому-либо выбираем исходя из каких-то тестов или наших представлений о количестве данных которые нам нужно получить Ну если мы будем делать только это то мы получим сразу же квадратичный алгоритм И это оче не очень хорошо поэтому здесь всё ещё усложняется и у нас там считается индекс вот эти вот значения хэш-функции они ещё заново ширу более простыми функциями создаются корзины в которые кладутся документы козине то есть допустим у нас Т 15 значений мы разбиваем их натри корзины по пять если хотя бы в одной корзине совпало значение функции то имеет смысл посмотреть эти документы подробнее применить к ним полное полный подсчёт схожесть карда и дальше уже если оно превышает какой-то порог то данные документы будут СБ Мы работаем с миллиардами документов да то чтобы индекс создать нам нужно много оперативной памяти и не всегда у нас всё помещается в эту оперативную память Поэтому приходится создавать несколько индексов документы шарди случайно создаётся индекс и через этот индекс все документы которые есть прогоняю и создаются группы дубликатов которые мы дальше ОТС матри Причём здесь Какие могут быть проблемы если вы захотите такое делать на питоне то вам будет немножко сложно потому что питону достаточно сложно обращаться к памяти которая большая да То есть вы можете разделить всё на много-много процессов но у них будет собственная память там можно пытаться это обходить Но это очень-очень больно поэтому мы Конкретно этот алгоритм реализовывали на раз там таких проблем нет ну Собственно как и C п+ и так далее да то есть у вас есть Трейдинг который позволяет обращаться из труда ко всей памяти процесса и вот допустим группу дубликатов обнаружили как же выбрать в общем-то элемент да То есть можно это делать случайно можно это делать По каким-то естественным штуковина Например если вы фильтрует веб странички то у них часто есть тайм СМП когда данная версия страниц была получена датасет соответственно можно брать самую свежую При таком подходе Если у вас есть несколько версий одной страницы При этом они отличаются то останется самая свежая которая наиболее актуальная также мы сейчас начнём говорить о фильтрации Если у вас есть какие-то скоры классификатора Вы можете всю группу поско и выбрать наибольшим скором Ну собственно вот переходим к фильтрации а фильтрация больше всего влияет на качество данных а оно как известно больше всего влияет на силу модели соответственно мы с вами сегодня рассмотрим пару методов фильтрации это им соответствует статьи которые были написаны первая hugin Face они выпустили открытый датасет Web и идея там была очень простая мы берём LM в промт запихивает и вопрос являет Ну обладает ли данный текст какой-либо обучающей ценностью То есть может ли он чему-то научить Ну вот сразу приведу пример это некий пост на Пикабу и здесь просто история какая-то из жизни котора мало вообще чему может научить Ну просто прикольно и в противовес пример там с Википедии статья о формуле баса она может многому научить поэтому это как бы положительный пример соответственно если вы захотите данный подход использовать для всех тех миллиардов документов которые содержится вре это достаточно много компью поэтому обычно это ВС дело упрощается того чается какое-то количество элементов например 10000000 с помощью и дальше уже учится какой-то более легковесный классификатор который конечно не такое качество обеспечивает Зато его можно без проблем запустить на миллиардах текстах соответственно другой подход который не строится на м а просто людьми был выбран положительный класс текстов было ихва но я расскажу про один соответственно положительный класс это Sub explain me like Да Sub это посвящён такой тематике что люди приходят что-то спрашивают и дальше в ответах им пытаются максимально подробно и понятно объяснить почему так происходит либо что это за веление либо какой-то факт и так далее соответственно у нас есть положительный класс отрицательны Мы объявляем просто случайные текст из интернета строим классификатор получаем очень крутой датасет который на котором метрики растут как грибы после дождя соответственно Вот пример положительного класса вопрос состоит в том Почему мы можем пройти много миль и всё будет О'кей А если мы стоим на месте не двигаемся то через 15 минут начинаем испытывать дискомфорт и ответ заключается в том что вот у нас в ногах венах есть обратные клапаны Но эти обратные клапаны они работают только когда мыш соответствующие рядом с ними и когда мы стоим и не двигаемся мышцы не двигаются и кровь застаивается мы начинаем испытывать дискомфорт немножко поговорили с вами про фильтрацию фильтрация позволяет нам отобрать наиболее качественные данные и соответственно Есть множество фильтрации но так или иначе почти все связа с построением который даёт нам скоры и мы по порогу можем отсечь наиболее лучшие данные сейчас поговорим о кластеризации соответственно в Интернете у нас есть некоторое натуральное распределение данных Да и вопрос заключается в том Почему мы учим сеть именно на этих данных соответственно если там будет каких-то данных не хватать вот я привёл пару простых примеров Да если мы датасет сделаем без математики то ваша сеть ничего не сможет решить Ну либо если мы решили учить только на русском да то английского модель не будет знать и например не сможет решать задачу перевода с английского на русский и обратно простейший алгоритм кластеризации выглядит так мы берём случайные данные из них получаем динги векторов с помощью какого-нибудь Эдера типа E5 Мулин далее мы снижаем размерность чтобы заработала следующая стадия алгоритма и заем ческую кластеризации после чего получаем метки кластеров для текстов дальше если мы размерность снизим ещё до двух то мы можем эти метки отобразить на какую-то такую карту и что мы можем с помощью такой карты делать да самое простое Мы можем с помощью неё оценивать Как работает например фильтрация Вот вы сейчас видите как работает фильтрация по обучающей ценности а соответственно видим как у нас уменьшаются какие-то кластера Ну и например вот фиолетовый если посмотреть да то он практически не уменьшается А вот есть справа зелёный и вот Он исчезает просто да этот зелёный он называется здесь откровенный контент Но на самом деле он назывался по-другому просто вот девушка которая вот эти два слайда готовила она его немножко переименовала но я вам расскажу как он назывался эния вились метки для кластеров мы можем просто взять случайные тексты в определённом количестве сколько у нас там поместится опять-таки передать их в промке ВМ и попросить как-то назвать эту группу текстов Да с каким-нибудь ограничением что использую не более пяти слов и мы получим метки соответственно вот здесь ещё одна диаграмма Да только другая опять-таки как фильтрация по обучающей ценности может повлиять на ваши кластера и если у нас есть какие-то достаточно обучающие скажем так данные Ну например первые - это юридические и финансовые аспекты жизни то они в большом количестве сохраняются Как видите около 50% остаётся можно поговорить о более таких крутых данных Научные исследования и образования там вот остаётся около 75% если и вот тре с конца я вам обещал название кластера он называется эротические порнографические материалы он и есть только синий столбик оранжевого нету потому что он зафирова вдоль соответственно Какие ещё тут стоят вызовы алгоритм кластеризации э штука тяжёлая и обычно можно кластеризованный это делать но 10 млн - это очень маленькая часть нашего датасета соответственно мы тут опять-таки приходим к классификатором после кластеризации это у нас обучающая выборка мы на ней обучаемся и дальше классификатором остальные тексты классифицируй После чего мы можем произвольно поменять распределение обучения и протестировать это соответственно мы можем менять распределение и здесь мы как раз переходим что на изменённом распределени нам нужно как-то оценить то что у нас получается и соответственно переходим к секции теру данные от качества данных как я говорил у нас ВС зависит Но как же нам это самое качество данных оценить если мы просто будем это оценивать с помощью обучения нового м то это очень много денег в первую очередь ну и времени вот здесь вы видите типичное обучение М одна из метрик кото мыу это входе всего обучения здесь видим что в начале она колеблется около 025 Это соответствует случайному гаданию потому что в метрике один вопрос и четыре варианта ответа и вероятность угадать правильный - это 25% но далее с обучением начинает Метрика подниматься и где-то в конце выходит на какое-то такое плато соответственно базовый сетап эксперимента он мы берём какой-либо чекпоинт Да вот мы видели сейчас на графике точки Да в каждой из этих точек сохранён чекпоинт мы берём один из этих чекпоинтов и пытаемся на коротком промежутке оценить как наши данные влияют на метрику либо метрики соответственно мы можем разные чекпоинты брать если мы берём с нуля да то мы видим что первые несколько чекпоинтов вообще ничего не меняет поэтому тут наши вычисления просто будут уходить какую-то пустоту каждый раз если мы возьмём чекпоинт который ближе к концу то поскольку мки Учатся с каким-то расписанием то в конце обычно оно очень низкая И если мы возьмём данный чекпоинт то к сожалению уже не особо будет виден сигнал от разных данных потому что там Метрика вышла на плата поэтому мы берём там где модель Е не обу поно у не Метрика только-только начинает поднима но при это надо что азы начнёт выучиваем линия - это эксперимент синяя - это базовое обучение и мы видим что данные на эксперименте по крайней мере относительно вот этой метрики они лучше эксперимент обычно бежит около где-то 5% от реального обучения но при этом позволяет с достаточно большой уверенностью оценить данные лучше или хуже и данный эксперимент можно ещё модифицировать да то есть провести его в режиме отжига Да это значит что во время самого эксперимента мы резко снижаем LR и у нас сигнал от этого растёт Что значит резко снижаем вот у нас есть обучение оранжевый график - это как бы соответствует полному обучению сети он здесь обрывается потому что я весь решил не показывать Ну и в начале У нас есть War да разогрев оптимизатора а синий это как раз таки во время эксперимента мы заем р достаточно резко как будто бы время эксперимента это и есть полное обучение сети и к чему это приводит да то есть если у нас данные одни и те же то если мы отжигаем у нас опять-таки Метрика растёт намного быстрее чем при стандартном обучени Ино в экспериментах это приводит к то да то есть какие-то кусочки данных заменив там или отфильтровать сигнал потому что у нас сила сигнала увеличилась и вот здесь небольшие изменения были внесены дас в пропорциональном количестве соответственно при этом было получено достаточно неплохое качество на эксперименте Ну мы с вами обсудили эксперименты это Одно из самых сильных э из того что может влиять на модель потому что это опять-таки позволяет определить качество данных а от качества данных зависит от того то какая модель у вас в итоге получится ну и перейдём к генерации синтетических данных а соответственно у нас два направления генерации есть это код и математика А на данный момент почему именно такие потому что во-первых ну код много кому интересен и ещё он не достиг того качества которое бы хотелось бы а во-вторых в моделях есть такое свойство когда они переходят в режим рассуждений и и математические и кодовые данные очень сильно влияют на способности сети рассуждать Ну и там есть ещё много причин например что а очень хорошо мерить генерализации да то есть показывать сети задачки которые она не видела и смотреть может ли она их решать то есть генерализованном классе задач а также в отличие от каких-то ограниченных версий да типа где у нас есть четыре варианта ответа в математики особенно в коде У нас есть огромное количество вариантов ответа поэтому угадать Случайный ответ не получится одно из первых таких работ в была статья Microsoft у них было три стадии обучения Первое - это просто код такой хороший код по фильтрованный классификатора вторая они добавляли учебники на натуральном языке которые описывали вообще как там кодить на питоне И третье ещё упражнение к этим учебникам и от каждой риста модельки на которых они экспериментировали они были довольно маленькие гораздо меньше чем текущие мки Но вот когда мы синтетический код генерируем у нас начинают возникать проблемы да то есть вот у нас есть какой-то учебник допустим по питону и в нём в содержании там Ну 100-200 тем если мы просто начнём генерировать по этим темам то у нас очень быстро выя примеры которые получаются если мы их пройдём где дупликации то всё что лишнее оно удалится Поэтому возникает проблема разнообразия данных которая у нас получилась почему она возникает потому что данных в трене очень много если вы в него маленький кусочек добавляете то они утопают в этих данных Вы можете этот маленький кусочек заосередные поэтому здесь Надо что-то придумывать и вот мы придумывали во-первых пересекать темы Да я называю вот эти вот придумки множителями Да потому что они в какое-то количество раз увеличивают конечное количество данных которое получилось и липогранулема с пересечён темами например как можно использовать Лин задаче определения детерминанта матрицы и также к этому ещё один множитель добавляется Это липомы это специальные инструкции для мки которые меняют вывод например при генерации каждое шестое слово должно начинаться на букву Т это немножко такая кринжовая техника но на качество текста на самом делена сильно не влияет при этом раст в итоге количество данных вырастает десятки тысяч раз чем в сравнени с простым подходом и их становится довольно много и заметно заметное количество для трейна соответственно мы что делаем мы сразу делаем такой как бы кусочек учебника который состоит из текста на натуральном языке которому далее Как пример добавляется условие задачи эта задача решается и далее после ещё решения есть некоторые тесты этого решения всё это дело мы запускаем мы вот сгенерировать 160 млн сэмплов из них отобрали около 10 млн которые были запущены и прошли все тесты и что из этого может получиться Да вот на тесте данных Здесь замеряют три последних чекпоинта потому что Метрика довольно дисперсная вал это по сути возможность модельки решать какие-то простенькие задачки на питоне и тест данный го он бежит где-то п там 57% от полного обучения сети и на тесте данных Мы видим что если без синтетики у нас Метрика 021 то добавляя всего лишь 1% пропорции синтетики мы получаем метрику 34 Это довольно крутой результат но задачи простые Давайте поговорим немножко про сложные задачи это где ещё пока L не так сильны Да вот недавно на работе У меня Возникала задача которая связана с такой таким алгоритмом Long и сейчас из всех м которые были мне там доступны задачку Вот именно рабочую смогла решить только о1 Да которая недавно достаточно вышла для таких зада сложнее решать Да И вот представим что вы олимпиадный программист и составляете задачу для какого-то раунда соревнований А вы придумали условия написали для него референсное решение сгенерировать какие-то маленькие тесты и также сгенерировать большие тесты которые для которых ответ получается референсный решением и тут возникает вопрос а как вот понять что вот эти тесты правильные что референсное решение правильно Ну и короткий ответ на него это никак А длинный ответ А можно использовать какие-то системы формальной верификации но они очень сложные и требуют больших трудозатрат и находятся за пределами темы нашего доклада соответственно А на этой позитивной ноте мы перейдём к математике а и я сделаю сильное заявление что современные м они умеют решать практически любую задачу по математике Но к нему идёт такое небольшое дополнение это сво дит да то есть когда мы как раз таки генерируем решения у нас могут просто случайным образом потому что характер генерации он вероятности сгенерировать какие-то плохие токены для этого решения и потом смотря на эти токены сеть уже не сможет уйти в правильную ветку решения она решит неправильно для того чтобы это обходить механи реть на самый популярный ответ это вот буквально определение majority voe а но а оно тоже не достаточно сильно повышает Сейчас я расскажу как примерно мы генерируем да то есть мы делаем какое-то количество генераций при этом это не простое текстовое решение задачи это мы просим сеть использовать код содержащий библиотеку питонов вскую сипай это библиотека для символьной математики и с помощью неё можно решать математические задачки далее мы смотрим на ответы которые получились и если эти ответы достаточно уверенно совпадают то есть не просто марити да а пересекают какой-то порог совпадения и иногда у нас порог может быть 100% для особо сложных задач то мы ещё делаем текстовую генерацию если ещё и у неё совпадает ответ с ними то мы считаем эту текстовую генерацию правильной кладём е в трей Таким образом мы получили положили уже больше 20 млн задач и как это выглядит На тестах данных Здесь вы видите сравнение с достаточно сильным орным дасе нумина код называется и вот наша синтетика она вот продолжает в ходе теста данных подниматься А нумина код стагнировать и вот вторая с первой она круче Ну и дальше мы продолжаем развиваться в этом вопросе мы не единственные генерируем синтетику есть открытые датасеты с похожими подходами и вот например NVIDIA выпустила статью буквально недавно называется Open 2 соответственно Здесь вы видите пример задачи из открытого датасета задача очень простая она про то там сколько каких блокнотов было продано есть блокнотов их там три типа один тип там больше другого продан был в общем простая Задачка Что интересно в не это решение да то есть решение которое в том же датасете есть соответственно оно было получено тоже с помощью И что в НМ Интересно что ответ получается 7826 И это не очень корректный Ответ когда вы ожидалось целое число поэтому я отвечу 78 и в конце о добавляет Но конечно же этот Ответ неправильный это достаточно интересный факт про который я хотел рассказать соответственно если мы перейдём к задачам посложнее например вот есть задача есть у нас координаты обычные трёхмерные и нам нужно перевести их в полярные то ответы в задачах начинают выглядеть сложнее и при это возникает вопрос сния уть ответы А первый ответ который вы видите - это ответ текстового решения Второй ответ - Это ответ сипай да то есть это был какой-то код он был запущен ответ был выведен И вот так выглядит вывод этого ответа мы тут видим что а первая и третья компонента совпадают А вот вторая она немножко отличается соответственно мы опять-таки можем подать эти два ответа в ЛМ и попросить её сравнить и нам отвечает что AR 2 у по 3 это Пи на 4 поэтому пи ми пи на 4 - это 3пи на 4 и ответы совпадают Вот такими возможностями интересными сейчас обладают мки Ну и скажем так в математике человечество пытается дать вот последний бой этому скату и буквально недавно пару недель назад выпущен содержит очень сложные задачки которые современными м решаются на 2% примерно ну лучшими из там вот такого типа задачки Да уже само условие выглядит достаточно сложно а решение её выглядит ещё на несколько листов там и сложнее а фишка этого бенчмарка в том что самим людям для того чтобы его решить им нужно потратить несколько дней то есть профессиональный математи такую задачу решает За несколько дней да то есть которые ещё специализируется в данной области Ну и мы подходим к концу доклада мы с вами сегодня поговорили Как обрабатывать существующие данные Как тестировать данные как генерировать новые данные и также данные подходы Вы можете применять не только для задачи генерации трейна длям можно их использовать в текстовых задачах Если у вас есть какие-то это текстовые задачи Вы можете либо генерировать данные для неё с помощью м либо попытаться их как-то пофигу вас голосовать за мой доклад Спасибо что пришли Спасибо Вань коллеги есть небольшая игра интерактивная Да если мы поднимаем руку то когда вы задаёте вопрос в целях конференции пожалуйста поднимайтесь И представляете А я буду выбирать примерно поровну у нас достаточно времени пока А мне кажется коллега на первом ряду был первым а ня дал свой микрофон Да Привет Меня зовут Марк Спасибо за доклад слушай Я знаю что вы активно работаете над мультимодальной семейства моделей можешь чуть подробнее рассказать какие модальности рассматриваете только ли это картинки голос или будет что-то ещё собираете ли вы для них данные обучайте или отдельные модели общие модели Да обучайте как вообще вот с этим с этой частью задач работаете так Ну я не знаю насколько я могу распространяться Ну давай так у нас уже есть как минимум модальность картинки да то есть чат умеет распознавать картинки Что находится в ней я тут частично работаю соответственно я собираю картинки которые там есть в интернете и пытаюсь связать их с текстами которые есть в интернете по планам соответственно Я думаю что в итоге мы будем добавлять Все существующие модальности когда-нибудь из таких очевидных осталось ещё аудио и видео а Но вот по этим направлениям я конкретно не работаю не могу подсказать 1 2Т Ваня ты помнишь что нужно запоминать у нас много вопросов да и два приза Коллеги с этой вот на первой парте вайте попробую встать Добрый день зовут Семён Спасибо за доклад и несколько вопросов первое планируете ли обучение или же там перевод кода старых языков программирования то есть не секрет там в Европе очень много языков вра и в том числе банковские системы да На что На чём-то там капитально старом есть ли такой в планах Вообще и вообще проводились ли тесты Это первый вопрос так ну планов насколько я знаю особо нет да то есть мы сейчас пока развиваемся в сторону хороших так сказать результатов на коде И вообще говоря именно что касается перевода систем с этим есть сейчас большие сложности которые пытаются в мире решить заключаются в том что у модели ограниченный контекст Ну сейчас там модели которые в широком использовании контекст около 100-200 сся токенов и это очень маленький кодовый проект Если вы хотите его полностью в засунуть соответственно есть модели с миллионом токенов но они послабее к этот у вас уже существующий контекст тем меньше каждый следующий токен генерируется и соответственно Если вы хотите миллион токенов перевести на другой язык и получится примерно миллион токенов то это будет работать очень долго то есть если решать вло получается это крайне сложно а если разбивать на части то фактически уже половина задачи сделано да да разбивать на части это нужно отдельно до обучать пытаться понять Как именно модифицировать кусочек который не имеет представления о других кусочках и это наверное отдельная задача сейчас не поясню как это делается второй момент новой версии гига Chat га Chat Макс который это полностью новая версия новое обучение с нуля или это до обучение всех предыдущих это полностью новое обучение с нуля а тоже и третий вопрос последний там как отбираются данные для первичного обучения вы сказали что есть некий там массив слов который был выбран там и на него опирается Но это массив если правильно понял вс-таки иностранный да потом в своих слайдах вы указали что если не будет русского языка не будет кодинга и так далее То есть кто-то этот массив И это получается Один массив или их множество кто их собирает и как вы проверяете то что в самом этом базовом массиве не содержится каких-либо злонамеренный искажений если один разго выбрали кажы раз и поздно что-нибудь накопится Да соответственно данные для первичного обучения мы формируем как я говорил на основании кокл Это буквально слепок интернета а и мы дальше своими силами отбираем из него Данные могу сказать что английского там 50% на втором месте в интернете находится русский язык его 5% все остальные языки представлены в меньшем количестве а соответственно Мы в основном работаем с английским и русским ещё некоторые европейские языки некоторые языки СНГ такие как казахский и узбекский всё это дело мы обрабатываем и фильтруем соответственно возможности к фильтрации у нас не такие большие да то есть невозможно весь этот массив данных прогнать через людей это раз а во-вторых во многих статьях показывается и на наших экспериментах тоже что люди отбирают хуже чем сами модели но в целом как-то мы фильтруем какие-то злонамеренный штуки мы убираем Спасибо коллеги Есть Да можно пожалуйста вот вторая парта И сколько у нас четыре вопроса у нас 10 минут есть Поэтому успеваем Здравствуйте спасибо за доклад Меня зовут Александр компания warm Soft а такой вопрос даже немножко в стор Мы очень много разговаривали А про то как происходит обучение А у меня будет несколько вопросов один из другого будет вытекать а как происходит цензури промто то есть ну он же не всегда ответит Не знаю сделай самодельную бомбу мне расскажи Ну явно гига чат не ответит про это а я к сожалению достаточно далеко от этого отстаю То есть у нас э то что сеть отвечает Не отвечает оно В отдельной группе находится соответственно Как именно оно происходит я не знаю но мы стараемся соответствовать действующему законодательству Да чтобы там не нарушать его Я понял просто че вопрос сложился была такая небольшая Боль у нас мы пишем платформу небольшую там есть конференц звонки он записывается транскрибируемая не проходит Ничего в ответ не приходит а там суть такая что должно разбиваться на черновик задач и исходя из созвона Ну там по сотрудникам соответственно и всё ничего не работает мы начинаем думать на свою сторону копать копать копать копать оказывается что там у них за разговор на 30 минут транскрибация ушла а в разговоре был просто небольшой кусочек такой что отправьте сотруднику письмо в Крым и это очень сильно затримку который нужно указать в запросе описать а сценарий который мы хотим исключить в цензи Ну если честно это было прямо чуть-чуть неудобно Я просто хотел Ну и думал вы знаете исходя из чего сходит цензура в этом плане да вот насколько я знаю что сейчас Если вы юрлицо то вы можете написать поддержку и отменить цензуру полностью насколько я знаю да да вот ну там такая достаточно длительная процедура вс-таки Всё спасибо большое коллеги вот молодой человек на второй парте вопрос прол может поть насколько он вообще покрывает Какой процент текущих рунных в НМ есть уже говорил на предыдущий вопрос где-то 50% - Это тексты на английском языке и 5% - это тексты на русском языке Это имеется в виду именно Сколько всего Ну есть есть интернет он там всё есть или там какой-то процент не присутствует Ну там в целом скажем так не весь интернет присутствует но значительная его часть соответственно большая часть рунета там есть А есть какие-то Прат источники вы не покупаете у них данные чтобы тамр и так далее занимаемся покупкой некоторых источников данных К сожалению именно разглашать Какие источники мы покупаем я не могу понял спасибо Передайте пожалуйста сзади и у нас здесь один вопрос остался Да Иван Большое спасибо за доклад Меня зовут Дима Я хотел спросить про то в итоге гигачад нацелен именно на математику код или на общую помощь человека это вопрос которые дальше я хочу узнать Ну в целом сети двигаются к тому что вот у вас есть какая-то боль или проблема И мы хотим её решить соответственно вот да И тогда такой вопрос что ты рассказывал что есть кластеризация потом можно получить человекочитаемый там описание кластера а потом ещё пофиг человека и из-за этого там всякие не знаю бытовые темы по типу футбола или ещё чего-то отсеивать пользоваться дети которые задают базовые вопросы на которые они ответы не знают или просто люди которые хотят тоже чтото базовое узнать и в этом случае такой фильтр отсе вот эти варианты или там хотят не зна с юмором что-то придумать а юмор тоже что-то такое базовое поэтому тоже выкинет Да есть такая проблема но в целом Вот как я говорил второй подход он эту проблему В некоторой степени учитывает потому что на там сате который описывал там буквально очень большой раз вопро во-первых оно сдвигает немножко ещё в эту сторону Хотя вот лично для меня пока ещё непонятно как мы переходя От обучающих данных к скажем так более развлекательным Да мы получаем огромный рост метрик Ещё ещё за счёт этого А во-вторых насколько мне известно Вот именно доля запросов которые такие развлекательные она на самом деле достаточно низкая ну здесь может быть Ловушка какой-то обратной связи Ну да если он показывает что мы умеем код или математику делать то вряд ли будут что-то простое спрашивать Да как у Алисы например коллеги У нас вот да молодой человек Здравствуйте меня зовут Евгений Спасибо за информативный доклад И вопрос такой отличаются ли ваши подходы вот в предобработки данных там дупликации фильтрации для э текстов на человеческих языках и на языках программирования и Если да то в какую категорию попали бы учебники по программированию где есть примеры кода которые Ну вроде как по определённым вашим же Метрика должны иметь высокую обучающую способность скажем так так они совершенно точно отличаются потому что для текстах на естественном языке мы пробуем разные подходы соответственно их сложно применять к текстам которые содержат код потому что для кода мы вырабатываем уже даже не просто какую-то обучающую ценность а какую-то серию критериев Ну например для кода обычные критерии которые могут применяться это там что он хорошо читается там что он хорош к изменениям да то есть что какое-то изменение кода не повлечёт у вас там 1000 часов трудозатрат соответственно для них фильтры немножко другие сам подход не отличается да то есть мы можем использовать либо м либо какого-то рода другие классификаторы для того чтобы отделять хороший код от плохого так а вторая часть вопроса Напомните Ну вторая часть вопроса с этой связана То есть если если у вас есть учебник по программированию там где есть и человеческий текст и программный код и причём Ну допустим это хорошие примеры и они вот типа учат Как хорошо писать код Можете ли вы каким-то образом извлечь пользу из того что они рядом И вот это вот хороший пример Ну если у нас есть учебник да то скорее всего если мы его классифицируют то у него будет очень высокая обучающая ценность Э да и с то с этой точки зрения Это хороший пример текста соответственно то что там и код и текст рядом находятся это очень круто про это как раз рассказывал генерации синтетического кода то есть мы когда пишем запрос кмки мы его пишем на обычно естественном языке и соответственно алм нужно произвести код а поэтому для неё очень важна связь естественного языка и кода И поэтому это является хорошим примером но как-то дополнительно использовать мы это не не используем то есть вот как есть кладём и и парсить вы его будете Ну вот предобработки обычный человекочитаемый текст Да да спасибо Да коллеги мы вынуждены закончить с q&a сессией А Ваня твой буфер не переполниться вопрос вспоминай теперь два вопроса которые выиграют так вот мужчине приз от онтико за серию вопросов Ага И вот здесь просили про русский язык в comm это приз от коллега поднимите руку кто спросил про кон крол Ага понятно а Вань а теперь мы хотим тебя поблагодарить как докладчика Спасибо тебе большое за доклад Спасибо вам всё Ваня мы тебя отпускаем"
}