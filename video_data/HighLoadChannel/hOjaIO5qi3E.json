{
  "video_id": "hOjaIO5qi3E",
  "channel": "HighLoadChannel",
  "title": "Адаптивная оптимизация запросов в реляционных СУБД / Олег Иванов (Postgres Professional)",
  "views": 3086,
  "duration": 2605,
  "published": "2017-04-22T14:48:18-07:00",
  "text": "у нас с вами олег иванов еще один эксперты из компании под professional ребята уже второй день дают классные доклады править вернулся с gresso и рассказывать интересные штуки и сейчас у нас речь пойдет об оптимизации запросов острым основе оптимизации которые сейчас внутри на в базах данных и адаптивной оптимизации это разработка которой который ребята трудятся олег расскажет чем она принципиально отличается от tm основе оптимизации и почему вероятно за ней может быть будущее давайте поприветствуем олега 1 раз как уже говорили расскажу про адаптивной оптимизация запросов пан martabak закончил сначала расскажу просто про то что такой оптимизации запросов зачем она нужна и как оно происходит современных sbd затем скажу о том что я понимаю под адаптивной оптимизации запросов потом мне нужно будет сказать пару слов про машины обучение метод к ближайших соседей мы его активно используем и потом протоках удалось применить машины обучения для адаптивной оптимизации запросов и что из этого получилось катю этого есть перспектива начать надо с того что секрет это декларативный язык поэтому в запросе указано какие операции нужно сделать с данными конкретный способ выполнения этих операций должна выбрать свобода этот способ вполне не операция называется планом выполнения запроса пример приведен на слайде узлам дерево в этом плане соответствует физически операции над данными а ребрам потоки информация на проблема в том что для одного и того же запроса существует множество различных планов его выполнения и время выполнения этих планов может отличаться на порядке поэтому от того как хорошо зубы до выбирает план выполнения запросов очень сильно зависит ее производительность собственно та часть sabado который отвечает за выбор плана выполнения запроса называется оптимизаторам запросов но сам процесс называется оптимизации запросов я чуть попозже расскажу как он происходит сейчас на стоять и отображён вывод команды explain a в суде по возрасту и стоили помады expand заставляет планировщику есть этот план для запроса который будет выполнено что интересного ну во-первых следует отметить что этот план это тот же самым плохим был изображен на предыдущем слайде только несколько в другом видео во вторых что нам будет интересно это два часа для каждого для каждой вершины плана это ее стоимость ее мощность она же кардинально что такое мощность вершина плана это количество кортежей которые она вернет на более высокий уровень и стоимость вершина плана и это оцениваемая оптимизаторам количество ресурсов для выполнения этой вершины и ее под дерево тогда понятно что стоимость всего плана то есть количество ресурсов которые требуется чтоб выполнить весь план это будет просто стоимость корневой вершины основная идея стоимостной оптимизации запросов это среди всех планов выбирать план с наименьшей стоимостью она была предложена в и system bar 1 реляционных субд и в 1907 четвертом году и осталась практически неизменной до наших дней ну что свидетельство о том что это достаточно хорошая идея давайте посмотрим как эта стоимость от количества ресурсов вычисляется например в поскольку и далее следует сделать замечание буду много говорить про возраст и сказать потому что я работаю с этой свобода но то же самое может быть временное к другим и свобода в том числе стоимостная в модель возраст и скрыли подрывают что каждый вершин умеет выполнять пойти различных типов операций последовательный доступ к диску параллель доступ к диску обработка кортежи на процессоре и так далее все эти ли каждый этих операций имеет свою стоимость они представлены в конструкцию конфигурационном файле рассказ их можно менять даже наверное нужно менять если вы хотите достигнуть хорошей производительности но об этом уже рассказал мой коллега иванов раков день назад давайте разберем на примере у нас есть простой запрос выбрать всех пользователей младше 25 лет даже этот запрос можно выполнить двумя способами это последовать на просканировать все записи которые есть в таблице и и использовать яндекс по возрасту в первом случае как мы оценим стоимость мы должны во первых прочесть все записи из таблицы и для каждой для каждой из них проверить выполнение условия то есть первое слагаемое то мы перемножаем количество страниц таблица на стоимость чтение 1 страница а второе слагаемое это мы перемножаем количество записей в таблице насту стоимость проверки условия для одной записи и другой вариант это произвести сканирование по индекса в данном случае нам не обязательно проверять условия потому что в яндексе записи уже держаться отсортированы по возрасту нам достаточно просто взять все записи оттуда но при этом но при этом из того что индекс от сложной структуры данных мы тратим больше времени на работу с ним мы и читаем диск непоследовательно как-то случайно по нему мотаемся поэтому мы домножаем количество записи которые мы должны вернуть на стоимость случайного доступа гадес кого ну и именно в этот момент нам нужно мощность вершинного то количество записи который мы вернем это и есть мощность как мы ее можем вычислить делать для ее вычисления мы используем гистограмм допустим нас есть инстаграма по возрасту в ней и вот в итоге стать всего 30 столбцов и и поэтому мы можем вычислить мы просто складываем первые пять столбцов и их сумма краски будет равна тому количеству записи которая вернет наш запрос на мне нужно выполнять чтобы понять кого будет мощность этой вершины дальше меня было много слайдов с форумами для других типов совершенно но я и все выкинул потому что над не так интересно основе что следует отметить это то что во всех формулах учитывается учитывается мощность вершина то есть если мы хотим оценить стоимость плана там и можно сказать сначала оценивая мощность каждой вершины плана вот это это выражено красными цифрами а потом опираясь на эти мощности вычисляем стоимость каждой вершины плана отлично мы умеем считать стоимость плавитель казалось бы нам нужно просто перед перебрать все и выбрать самый самый дешевый ну на самом деле так тоже никто не делать потому что различных планов очень много поэтому такой метка как полный перебор и здесь не подходит на самом деле заметим что мы здесь ставим задачу оптимизации мы должны минимизировать функцию стоимости плана и поэтому можем решать ее известным методами дискретной оптимизации ну известный место декрет на оптимизации это динамическое программирование или генетические алгоритмы я опять не буду подробно описывать что происходит в каждом из методов и я просто пишу их свойства динамическое программирование было предложено опять его первые реляционные свобода систем она имеет временную сложность 3 в степени n и сложность по памяти 2 в степени казалось бы экспоненциальная сложность это очень плохо но даже это намного лучше чем полный перебор всех возможных планов потому что их еще больше ну и никакой плюс этого метода что он всегда находит план с наименьшей стоимостью то есть в этого места есть четкие теоретически гарантии что вот он закончил работать после этого мы нашли самый лучший план но с точки зрения нашей функции стоимости минус конечно и во время работы если мы будем менять 30 ношения то это займет уже несколько дней это никуда не годится по-моему если говорить про если говорить про пост бреста какому-то германскую института потребовалось обвинять так много отношений больше чем несколько десятков и и они предложили использовать единицы генетический алгоритм для оптимизации но для оптимизации пространства такой большой размерности чем генетический алгоритм хорош ну это вообще общие средства если у вас есть какая то инфа трудная задача которую ну вы естественно не можете решить точно но решать ее очень надо то это ваш выбор у него есть достаточно много различных настроек параметров которые могут кардинально менять его поведение также у него есть потому что его можно остановить после любой итерации то есть если прерывать на середине динамическое программирование то вы ничего не получите здесь его можно прервать на любой итерации все равно получится хоть и плохое но какой-то план на выполнение запроса но минус как и у любого решением патруну на гадать что нет нет нет не дает никаких гарантий если мы его останавливаем через конечно чувствует и рацией для данной задачи приходится с этим жить и собственно возвращаясь к общей схеме из томатной оптимизация узким местом здесь является вовсе не мид не метод оптимизация того же чаще всего но мы не объединяем там 30 отношений я думаю в повседневной жизни анатомическом программировании мы всегда решаем задачи оптимизации точно а узким местом является оценка стоимости плана как мы помним оно состоит из двух подзадач которые решаются последовательно это оценка мощности всех вершин оценка стоимости поскольку задача решается последовательно то в общем задачи решаются настолько плох насколько решается худший из под задач мы попытались определить какая из этих подзадач лишается хуже сейчас будет сложный слайд на левом графике мы видим качество решения оценки мощности а на правом качество решения оценки стоимости на ям графики по x' и по оси x у нас отложено предсказанная оптимизаторам запросов мощность каждой и совершенно а по оси y это истинно так который мы получили после выполнения этого запроса и прошу также отметить что на левом графике масштаб осей логарифмический на правом графике а и каждая точка это вершина какого-то плана на правом графике по оси x отложено стоимость которая вызвала оптимизатор запросов а по оси y время сколько-то вершин исполнялось самом деле ну у нас в течение эксперимента там не было никакой ники конкурирующих запросов в лабораторных условиях можно сказать надо понять какая задача решена хуже на самом деле левая почему потому что там масштаб силы к ритмические смакует . которая нам кажется достаточно плохой то слева это будет ошибка в 300 раз а справа ошибка в четыре раза мне кажется что и здесь уже очевидно что стрим остальная модель более менее адекватно в то время как оценка мощность можно серьезно улучшать ну с нами согласно некоторые коллеги которые параллельно провели тоже исследований написали статью на эту тему поэтому вернёмся к тому как мы вычисляем мощность на самом деле когда вы числе мощность мы несли не складываем не остался гистограммой и говорим что эта мощность мы складываем первые пять столбцов гистограмм делим их на сумму всех столбцов инстаграм и говорим что это будет доля записи которые удовлетворяют нашему условию эта доля также называется выборочно стью и вычисляем мощность как произведение выбор честно количество записей в таблице зачем так сложно а затем что у нас может быть несколько условий но допустим мы хотим найти всех людей младше 25 которые живут в москве но поскольку instagram у нас строится по одному столбцу то у нас есть выбрать только этих двух условий в отдельности вот допустим у нас одна треть и людей можешь 25 1 7 живёт в москве как найти сколько какая доля всех удовлетворяет и тому и другому условия ну казалось бы картинки очевидно 1 21 мы перемножаем одно на другое на самом деле почему нарисовали эту картинку не так мы считаем что эти условия независимо то есть доля тех кто младше 25 лет среди москвичей не москвича я на 1а мы считаем так и поэтому получаем вот вот такую форму который сейчас используется в современных в симуляторах запросов единственное исключение это условия которые обращаются к одному это муж стал столбцу они могут быть эффективно почтительно по одной и той же гистограмме например если эти условия виды но вида там а меньше бы уменьшиться как правильно слайде ну тут есть большой подвох потому что на самом деле есть все для моложе 25 лет нет всем скачи были бы моложе 25 лет у нас вы брачность обоих этих условий была бы равна 1 7 есть наоборот никто из тех то моложе 25 лет не жил бы в москве то их выбросить была бы равна 0 то есть что брать но зарплата еще будет на самом деле этот пример может к с надувным но есть много примеров когда когда мы вполне естественно получаем то что у нас совместной выборочно из двух условий не равна произведению их выборочно они могут быть взаимоисключающими ся взаимоисключающими и наоборот полагающимися друг на друга как как например ну показаны на слайдах эти и наиболее часты случаи когда условия последовательно дополняют друг друга то есть мы редко сдаем сдаем запросто получить пустое множество мы обычно описан наши множество как то какими свойствами которые друг друга усиливают но но тем ни менее пузырь считают их независим поэтому часто он недооценивает истинно мощность вершина ну понятно что это боль существует достаточно давно в суп в сообществе и и все пытаются как-то решить наиболее популярным решением является многомерной гистограмма казалось бы есть мы строим гистограмму по двум столбцам то мы можем находить совместной выбор часть уже 2 условия если по трем стал санта можем находить совместная облачность трех условий и так далее но тут мы сталкиваемся проклятием размерности что если мы хотим сохранить ту же самую точность то одна мера гистограмма в ней сто значений в двумерный 10000 трехмерный миллиона с 4 метра и мы уже не сможем работать по этом возникает вопрос на каких столбцах и встроить это нужен на пятки к из администратор который будет изучать запросы смотреть корреляции всем этим заниматься это это один из подходов к решению задачи на хочу поговорить с другом это адаптивной оптимизация ну да адаптивная оптимизация заключается в том что как выглядит процесс оптимизации запросов мы используем гистограмм который подсчитано по данным при подсчитаны по данным мы используем какие-то статические стоимостные модели после это мы выполняем запрос мы с совершенно бесплатно получаем статистику его выполнения сколько наполнялся сколько кортежи у нас было получено в каждой из его вершины и мы забываем это статистика мы начинаем снова оптимизирует запрос как будто бы мы ее ни когда не видели может ли статистика выполнения одного запрос помочь при оптимизации другого если два совершенно разных запрос которые не имеют друг друга отношения то нет не может поэтому есть на запросы все абсолютно разные то ничего с этим не сделаешь но обычно у нас нагрузка на базу примерно одинаково нас идут запрос одно и то же структура в них меняется константа или запрос даже разной структуры но все равно в них есть киты кита общее паттерна и тогда зачем нам повторять одну и ту же ошибку при оптимизации несколько раз можем собрать статистику чести и и все еще раз утилизировать помни ну именно именно эта людей я понимаю под адаптивной оптимизации запросов то есть она адаптивно ананас настраиваться с учетом предыдущих запросов она настраивается с учетом того какие данные у нас лежат в базе но здесь не нужно будет сказать пару слов про машины обучение потому что без этого никуда вообще четкую машинное обучение машинное обучение это некоторый набор методов которые ищут закономерности в данных зачем искать закономерности в данных ну например у нас вот есть некоторый набор объектов каждому из них соответствует синий или красный шарик нужно поискать в шарик будет для нового объекта я думаю что глазами это сделать не так-то просто но есть если применить клонить частички тест например то можно заметить шестеренки никак не влияет на свет шарика а после этого загримировать объекта и тогда представление станет уже более менее очевидно в общем можно сказать что скорее всего шарик будет синим это очень базовый пример привел не чтобы объяснить какие-то нет машин обычай нашего повести терминология машины обучи нечно закономерности в данных так вот данные представлены в виде набора объектов у каждого объекта есть некоторые признаки и есть скрытое значение которое мы хотим предсказывать вот эти объекты у которых скрытое значение есть чтобы мы могли как-то обучаться они называются обучающей выборкой а те для которых мы должны предстать открытый значение это тестовое выборка ну вот разберём на примере метода к ближайших соседей допустим у нас есть набор людей прокажу из которых нам известно какие то анкетные данные там если у него высшее образование семья сколько у него детей сколько ему лет и хотим предсказать самая нижняя строка коня зарплата там допустим 1000 рублей мы считаем что давайте что наверное у похожих людей зарплата тоже примерно похоже не буду говорить как как вводить функция сколько защелок похожи друг на друга это можешь делать многими разными способами давайте найдем двух ближайших двух наиболее похожих на того человека для которой мы хотим что-то предсказать людей и как-то среднем и зарплат скажем что вот наверное у него зарплата примерно такая же это место это нет достаточно базу он был предложен очень давно у него есть существенный недостаток мы должны хранить всех всех людей должны хранить все обучающую выборку потом я покажу что это достаточно обременительно в нашем случае в нашем применение так система управления базами данных поэтому пришлось разработать метод который в котором мы сможем жестко задавать ограничение на то сколько объектов мы храним основная идея метода в том что у нас появляются какие-то виртуальные объекты то есть например место 6 людей мы храним двух людей но у одного человека там есть 1 3 ребенка 1 3 жены и он на две трети образован в принципе дается product and такой человек сложно найти что он чтобы он где-то существовал но при этом оказывает считать их виртуальных объектов вполне достаточно для того чтобы достаточно точно делать предсказание которое нам нужно и опять и я не буду породы подробно рассказывать как именно такие виртуальный объект получается и почему метод называется методом градиентного ближайших соседей в принципе если здесь есть тот кто очень любит кровавый матан то я могу его рассказать после доклада с большим удовольствием вот а пока что я думаю пора переходить к тому как применить машины обучения для адаптивной оптимизация запросов как мы помним и хотим предсказывать мощность потому что это наша наибольшая проблема с помощью метов машины обучения первую идея что нам нужно предсказывать наш объект это вершина и все ее под дерево потому что если мы будем делать в предсказании независимо для каждой вершины если мы не будем учитывать те условия которые есть под дерево то это будет таким же предположением о независимости этих условий мы с этой проблемой столкнулись со столкнулись некоторое время с этим работали потом оказалось что иначе нельзя вот это конечно дает некоторое замедление потому что в пост раз обычно работает с вершинами по отдельности независимо но приходится как как мы формулируем задачу вот нас есть инстаграм и у нас есть некоторые условия и как часть оптимизатора в даешься мощность вот эта вершина будет такая мы и делаем несколько иначе мы заменяем во всех условиях константы но мы их просто туда удаляем мы говорим что действует как константа но чтобы не терять информацию о них мы вместо этого к условию дописываем его выборочно есть которая подсчитано по инстаграм с помощью стандартных методов по сброса и это у нас будет объектами машину обучения то есть если есть и ставить задачу форманта у нас есть условие с удаленными константами это признаки есть их выборочно стен значения признаков и по ним мы должны предсказать мощность вершина в по дереву которые эти признаки есть ну вот примерно так выглядит работа этого мента если убрать всю нижнюю часть то получится просто как свободой обычно оптимизирует запрос портят оптимизирует выполняет но здесь мы добавляем некоторую часть машинного обучения которые который отвечает на запросы про мощность вершин и после после того как это запрос выполнится новый он обучается на новой статистике ну да тут и можно сказать про теоретически гарантия то есть будет ли это нет работать со дессалин куда неделе будет ассоциировать как быстро он сойдется есть сойдется и куда садятся ну при некоторых условиях мы доказали что да он сойдется за конечное число шагов но нет мы не можем дать некую разумную теоретическую оценку этого числа шагов но на практике это происходит достаточно быстро за несколько итераций и за несколько десятков итерации то есть там несколько десятков раз выполнить запросы после этого метод работает стабильно куда сойдется ну во-первых предсказания будут не хуже чем те которые были до сходимости и во вторых есть у нас есть хорошая стоимостная модель которая конвертирует эти мощности в стоимости то время выполнение запроса не время полнения запроса не увеличится то есть мы не можем сделать хуже то есть если мы применим наш нет время за время выполнения увеличилась то значит что то не так с томатной модель на самом деле вот но понятно что все это некоторые сферический конь в вакууме и таких предположений в реальной жизни обычно встречается но они бы но это полезно для тоже понимать как нет может работать как нет работать не должен на данных ну и собственно переходим к практическим результатом что есть в этого получилось во первых должен выбрать конкретный метод машинного обучения на самом деле мы пробовали много разных методов и я опять не буду подробно объяснять про каждый из них что это за метод и зачем он ну ну во первых что видно стандартная модель а где предполагается независимость что все методы работает лучше чем она ну это понятно что стандартная модель она ваще никак не обучается каждый метод там все-таки какие-то параметры оптимизируется но видно что по желтым цветом отображено линейной регрессии она бы очень быстро выходит на к это константный уровень миссия дальше не обучается это недостаточно не достаточно ёмкая модель она нас не устраивает остальные trimmed да каждый из них имеет свои плюсы и свои минусы но не более важным плюсом для нас является поведение на ранних этапах в этом видим например что до первых по оси y отложено ошибка при тканей поэтому чем меньше тем лучше об этом следовало сказать ну мы там вели меру как когда квадратичная ошибка но надо логарифм у меня то есть конкретно форма писать не стал но в общем то она есть соответственно например нас есть метод который вы вроде глобальный работы лучше всего методы кредит на будете надо решающим деревьями вот он голубеньким отображен он начинает работать лучше всего только потом после до того как и две тысячи раз или там полторы выполним запрос у нас бы вопрос который выполняется полдня день мы не хотим выполнять его полторы тысячи раз чтобы чтобы получить какое-то преимущество по результатам ну по результатам предсказания поэтому нас очень интересует те методы которые работают хорошо самого начала и видно что вы все с большим отрывом сначала работает нитка ближайших соседей но как я уже говорил у нас есть проблема что и если мы будем хранить все обучающую выборку то он очень скоро займет всю память будет работать очень долго положит систему именно поэтому нам потребовалось ввести ограничения на количество объектов ну собственно здесь показана trimmed до 1 это метка оближешь соседи где мы просто храним последние 500 объектов самая очевидная идея но видно что он также выходит на некоторые константы что он скаут момент пристает обучаться есть метка ближе к сети который без ограничении мы храним все обучающую выборку он достаточно точно предсказывает лучше всех остальных но при этом у него есть вот проблема со временем с памятью и тут нет которой мы предложили он отмечен чертом видно во первых он использует те же 500 объектов что и фиолетовая метод то есть по памяти по времени он работает ровно столько же но при этом намного лучше он работает практически как нитка ближайших соседей ну немножко похожего поэтому и выбрали именно его ну ну да видно что он даже работает лучше даже не расти вот на этом обозримом этапе ну собственно здесь мы измеряли ошибка предсказания метод а ты точно насколько мощность соответствует тем как которые должны быть но вопрос насколько это ускоряет производительность системы вот например есть известный benchmark тепихе аж на нем результате не очень впечатляющие на запрос который выполняется быстро ну суммарный прирост производительности там доли процента на долгом здесь даже есть небольшой минус но это не из-за того же план изменился просто погрешность измерений вот более интересные результаты на бенчмарки тебе cds в нем даже на самых быстрых запрос которые выполняются меньше секунды на них уже есть заметный прирост производительности этот 12 процентов ну и в принципе я не буду твоим дублировать всецело которые есть на слайдах или буду для следующей группы прирост 24 процента потом 41 потом двести восемьдесят пять процентов видеть что эта группа запросто выполняется четыре раза быстрее после применения оптимизации почти ну и для последней тоже там достаточно значительно улучшилась какой из этого можно сделать вывод ну во-первых четко видная тенденция что чем более долгая группы запросов тем больше прирост производительности и его вто и и во-вторых чем более сложный запрос потому что в типе cds запроса более сложно чем в типе тяж то есть чем более сложные запросы чем дольше они работать тем в большее количество раз они могут быть ускоренными то есть на самом деле некоторые сложные ресурсоемкие запросы на самом деле несложные ресурсоемкие а сложно и не до оптимизированы можем их до оптимизировать получить существенный выигрыш в производительности вот еще один график и здесь в каждой группе я показываю максимальное ускорение то есть в какое максимальное количество раз курица запрос видим то тот самый эффект что в группу который выполняется быстро там максимальное ускорение где-то раз в пять в 10 и 17 а в очень в группах долгих запросов ускорения может быть ну вот в сто раз как мы видим по графику то есть там вместо двух часов они стали выполняться полторы минуты ну вы спросите какие накладные расходы на все это то есть мы раньше просто перемножали здесь какой-то машину обучения какие-то ближайшие соседи но я экспериментально измерил накладные расходы которые не получились конечно для метнули генетического алгоритма не больше потому что он чаще вызывает оценку мощности для динамического программирования они меньше для если есть запросы идут достаточно плотным потоком то может быть эти накладные расходы и существенно но если запрос выполняется и секунды минуты часы и дни то такое увлечение времени планирования но не очень заметно ну и также следует отметить что эти накладные расходы которые отображены я успел посмотреть основная часть там не математика математика краски спроектировано так чтобы быть очень легкой основная часть это расходы на сведении задачи оценки мощности к задач машину обучения вот там преобразование условий все это то есть эту часть еще можно оптимизировать и накладные расходы которые здесь отображены это не окончательный вердикт они могут быть еще лучше ну и какой вывод из этого можно сделать что не понятно как там будет запросами которые вполне достаточно быстро но для каких-то сложных аналитических запросов которые которые однотипно там не знаю построить отчет в конце месяца или что-то в этом роде билет этих запросов этот метод мы предполагаем что он будет работать хорошего ну естественно если у вас есть запросто mckinsey и этики но очень и очень простые да просто там оптимизировать особенно нечего там достаточно хорошо справляется и стандартный оптимизатора то есть и нет применим больше к сложным запросом ну еще о чем я должен сказать это динамика обучения что в теоремах в этих теоретических гарантиях сказано что поставкам обучаемся все будет хорошо но как долго но в процессе того как мы обучаемся может быть чуть не понятно пример время полнее запрос может начала меньше с потом увеличится потом снова уменьшится но потом она уже выйдет на q & the constant которая была ну которое стало меньше чем исходное время исходное время выполнения может случиться вообще страшно как отображено на стенде я опять обращаю внимание что шкала y логарифмическая то и запрос начала выполнялось 4 секунды потом полчаса после этого стал наполняться 1 секунда в общем-то да это проблема ну все хорошо что хорошо заканчивается но это поведение метод которого пока не удалось уйти ну а на свидетельство о том что видимо перед тем как что лучше запрос оптимизировать деньги на отдельной машине может быть а потом загружать уже это на какую-то более важную машина блин где останется при мне только результаты этой оптимизация вот но наиболее часто случай коленками конечно когда запрос оптимизируется за одна итерация достаточно быстро ну или оптимизируется там уж ускоряется далеко не все запрос на самом деле некоторые запросы ускоряются некоторые запросы остаются такими же и их наверное больше всего ну хотя зависит от бенчмарка в любви в некоторых ускоряются больше чем остаются неизменными ну и даже бываешь некоторые запросы замедляются при том что мощность но это было предсказано лучшая ну как я уже говорил это не достать тесто им основе модели наверное никогда не будет исправлено но таких случаев намного меньше чем предыдущих чем остальных то есть часть запрос все-таки ускоряется чем замедляется ну и я обещал рассказать про то что будет происходить дальше с этой темой какие у этого есть перспектива ну во-первых понятно что этот проект еще нельзя сказать что вот мы здесь все исследовали что это последнее слово там и есть ещё много чего исследовать есть много способов это улучшать можно бороться с этой проблемой который указал два слайда назад то что запрос выполняется очень нестабильно в процессе оптимизации можно использовать метод под нестабильность сгладить можно улучшать стоимостные модели тоже с поздним адаптивных методов например можно адаптивного вставляйте конца не константа с тремя стая модель анод эти пять которые в подписи были можно может очень очень много что делать но общий предпосылка такая что видимо система но вообще всей системы с которым работаем они постепенно становятся более более умными адаптивными и поэтому я я думаю что дальше будет значит будет только лучше вот ну и ну да собственно где можно попробовать найти код который применяет скопас курсу ссылка на слайде мои контакты тоже на слайде всегда можно если вы решите это используется вас нет какие то вопросы то не стесняйся обращаться всегда рад об этом поговорить понятно что версия кода из совсем не итоговая совсем не production но тем ни менее ее уже можно используется оно не падает она ускоряет запросы для жидких на гитхабе вот не совсем понятно с устареванием вот если статистику устаивает там до када а тут должно быть вообще минут каком-то в большом я понял этот хороший вопрос ну естественно если а статья статистика если резко изменилась нагрузка если резко изменились данные в базе или если резко изменился характер запросов то нет лучше перезапустится он не может сразу понять что все кардинально изменилось но еще одно хорошее свойство градиентного меток оближешь соседи который мы разработали заключается в том что даже к таким ситуациям он может адаптироваться то есть то есть он постепенно забывает старую статистику поэтому если все резко изменилось некоторое время он будет работать плохо естественно но но потом он начнет вести себя так как будто его перезапустили спасибо за доклад скажите скажите вот вот есть достаточно мощная с мощное средство для работы со статистикой из теми же адаптивными планами вот то что вы не стали этот опыт перенимать это связано с патентами или просто вы принципиально решили идти таким со своим путем ну во первых должен сказать точно что происходит на три урока потому что они об этом говорят не очень подробно в во вторых что использован сергей интерфейса на самом деле здесь можно замечать некоторые общие черты с тем что происходит в рохля то есть у нас не было цели скопировать то что work ли мы здесь сделать то что нам казалось лучше в эти механизмы рассматривать как некую отправную точку нет тут получилось по-другому что мы начали делать шаг сами в итоге пришли к чему-то может быть похожим может быть нет нет спасибо большое за доклад очень интересно пожалуйста в текущей реализации есть способ сделать так чтобы какой-то незначительный процент запросов использовать для обучения остальные шли напрямую но таким образом избегая груды висящих запросов но постепенно собираю статистическим текущей реализации сделал не так там можно ну может вручную вручную поставить а допрос используют для обучения и используют ли в нем для предсказания машинном обучении то есть можно отключать любую из частей можно плеча часть которой который отвечает за сбор статистики можно отвечать отключать часть которой отвечает за предсказание такого чтобы отключать чтобы обучаться на каком то по оценить запросы такого пока нет но это понятно не сложно делается спасибо за доклад скажите вот в методе к ближайших соседей например есть всякие настройки которые нужно задать задать заранее например сколько ближайшие соседи мы будем использовать каким образом и подбираются эти настройки ну ну во первых можно менять конкретно мы использовали трех ближайших соседей потому что наш эксперимент доказал наилучший результат ноги если кто-то уверен что не знаю 4 или 2 подойдет лучше то это меняется что касается расстояний а то опять там тоже есть расстояние конкретно на выглядит как евклидова расстояния в в графе логарифмируют пространстве выборочно стена то есть и такие экспериментальные данные которые у нас работают в принципе мы вполне можно применять ту часть которая связана с машинным обучением там например изменить функции расстояния и получить какие то может быть другие результата спасибо коллеги ну я думаю давайте мы еще раундов аплодисментов благодарим олега сознательный доклад"
}