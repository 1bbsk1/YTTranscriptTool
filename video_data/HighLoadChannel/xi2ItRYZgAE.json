{
  "video_id": "xi2ItRYZgAE",
  "channel": "HighLoadChannel",
  "title": "Изображая продакшн  / Михаил Бородин (Код Безопасности)",
  "views": 439,
  "duration": 2602,
  "published": "2020-04-14T11:14:46-07:00",
  "text": "всем привет меня зовут михаил бородин и четыре года назад я сменил род деятельности с системного администрирования на нагрузочное тестирование и за это время я прокачал скиллы в написании авто тестов в обрел свое видение нагрузок в продакшен частных облаках а также стал хранителем знаний в области репликации нашего продукта кроме нагрузочного тестирования я также занимаюсь исследованиями в области безопасности и меня вдохновляет создании сложных систем которыми легко пользоваться сегодня я хотел бы вам рассказать о построении нагрузочных стендов для симуляции сложных больших продакшен сред виртуализации пару слов о компании которую я представляю код безопасности это достаточно крупный отечественный вендор давно на рынке большой объем собственных средств защиты информации куча сертификатов от силовых ведомств и много клиентов клиенты хотят от нас получить наших продуктах безопасность для своего бизнеса они хотят получить соответствие комплаенс у и они хотят чтобы наши продукты были максимально стабильными и безболезненные интегрировались в их крупные инфраструктуры основными заказчиками на текущий момент являются госкорпорации финансовые учреждения органы власти и крупные облачные провайдеры сегодняшняя презентация пройдет в контексте тестирование средства защиты платформу виртуализации под названием wingate на слайде вы можете видеть общую схему минимального деплоя этого продукта обратите внимание что сервер встает как бы шлюзом между админами и виртуализации и при этом у нас наше агентское пола ставится как у админов на хостах так и на сервер и управления и на гипервизорах везде по сути кроме виртуалок немного фичах основные фичи для безопасности это соответственно тонкое разграничение доступа админов к виртуализации и собственно выполнения наборов требований все и dice с гост так далее также есть и такие фичи которые вызывают иногда очень большой отклик от инфраструктуры которые являются по-настоящему высоконагруженные в крупных инфраструктурах это как firewall виртуальных машин это просчет контроля и контроля целостности виртуалок и мониторинг всех событий безопасности в виртуализации когда я пришел в команду разработки у меня было две основные цели это непосредственно тестирование производительности нашего продукта и поддержка в максимальном пригодном рабочем состоянии нагрузочной лабы и спустя всего пару месяцев появилась очевидная проблема наши стенды оказывается отставали сильно от инфраструктур наших передовых костюмеров немного продажники перестарались на тот момент давайте рассмотрим в 3-х кейсах как мы дошли до этой проблемы первый релиз десятом году был прав а в конце по сути надо было реализовать минимум необходимый для того чтобы продукт вышел и аха роде даже наперёд сильно никто не думал и ничего такого не закладывал потом мы быстро реактивно нарастили фичи добавили поддержку новых платформ а вот тестирования хайло да она немножко отставала и была по сути ну на коленке то есть куча старых ростов которые объедены в какую-то сеть там что-то гоняется совершенно какой-то минимум пока хватало но потом как раз наступил тот момент когда фичи стала настолько много и продукт был настолько с функциональной точки зрения уже готов что нас начали покупать и пытаться интегрировать действительно крупные кости мир с большими инфраструктурами на которых действительно против свой лад и еще на этапе интеграции на этапе развертывания выяснилось что у нас проблема какие грабли мы с этого собрали мы понесли маленькие репутационные риски ничего значительного но тем не менее у нас появились небольшие на тот момент сложности с тем чтобы продавать крупным костюмером продукт потому что мы не могли предоставить им данных о том что мы у них абсолютно точно заведемся и будем хорошо стабильно работать а также из-за специфики продукта из-за того что продукт сертифицируется в стеком и сертификация это не быстрое выпуск новых релизов он медленный он действительно самое часто что можно сделать это раз полгода по факту год поэтому даже выпуск патчей может затянуть разработку и стоить больших денег что же нам было делать нам надо было думать наперёд строить какую-то инфраструктуру развертывать лабы которые опережают наших самых передовых заказчиков поэтому как milestone для самой первой версии нагрузочного стенда мы для себя приняли 250 состав виртуализации и от пяти тысяч и больше винтовок при этом нам требовалось еще и скиллинг потому что продукт роз касты люка 100 миров хотелки тоже очень быстро росли при этом стоит еще учитывать специфику нашего холода на этой конференции как правило принимается что highload это когда сотни тысяч и десятки тысяч клиентов лезут на какой-нибудь условный front-end и поэтому нужно держать эту нагрузку у нас немного специфика другая у нас всего один администратор в одной сессии может сделать один запрос который в процессе обработки нашей бизнес логикой логикой защиты может обернуться в такой шторм в инфраструктуре что там будут лететь между нашим сервером и гипервизора my сотни тысяч запросов какие у нас были варианты для того чтобы построить этот нагрузочный стенд мы могли действительно развернуть вот такой вот локальный образец частного облака при этом это было бы чудовищная заморозка средств вместо того чтобы тратить средства на разработку мы бы потратили их на закупку железо которая нам не нужна по сути она бы служило лишь только для тестирования это было бы полнейшим overkill еще было варианта вариант аренды но он нам не подошел потому что у нас есть стандарты безопасной разработки плюс имеется некоторые проблемы с оперативностью и удобством администрирования в данном кейсе а вот симуляция хайло да на виртуализации локально она как раз нам подойти могла поэтому давайте прикинем примерный чек-лист построения стенда на именно основе нейстат виртуализации нам нужно выбрать платформу виртуализации нам нужно определиться с требованиями которые мы предъявляем к нашим железным ломом чтобы не стать виртуализацию крутилась нужно подобрать в соответствии с этими требованиями сервер и обеспечить сети которые тоже соответствует требованиям и не забыть про хранилище на которых будут у нас лежать наши сотни гипервизоров тысячи виртуалок вот это вот перед нами стоял выбор между в сферой и диппер we мы посмотрели что в основном крутится у наших заказчиков продакшене ну это было не сюрприз это было гипер we это было вас сфера извините мы посмотрели на лимиты которые есть у сервера управления и это те лимиты вы можете видеть их на слайде который мы в построении стенда стремимся какие есть ограничения с точки зрения именно холстов виртуализации в гипервизор и для вас веры в эссексе максимальные пределы довольно высоки и на текущий момент тяжело найти сервер и с такими характерными ресурсами в продаже ну и тем более в рамках бюджета то и вовсе нереально а вот насчет минимальных ограничений не все так просто сама dvr говорит что минимальные ресурсы для запуска если косая это два ядра это четыре гигабайта памяти и гигов 510 на диске при этом более старые версии 5551 могли запускаться и на меньшем количестве оперативной памяти вплоть до двух гигабайт при этом стоит заметить что от этой оперативной памяти сильно варьируются ресурсы которые нам предоставляет гипервизор в плане количество запускаемых виртуальных машин и в плане хранящихся виртуальных машин так на 4 гигах которые являются абсолютным фактически минимумом для если совет часть 5 6 7 запускается всего лишь 8 виртуалок это даже бизнес и хранится могут 30 на 1 найдет при этом мы проводили наше тестирование и выяснили что даже в рамках очень ограниченных ресурсов когда у нас всего два виртуальных процессором у которых зарезаны частота и когда у нас на том же и секса и 55 еще и уменьшено количество памяти до трех гигов у нас агент продолжает стабильно нормально работать выполнять свои функции из этого мы делаем вывод что так как у костюмеров на продакшене как правило у гипервизоров ресурсов значительно больше мы будем работать на любом количестве ресурсов которые нам может предоставить из эксайт немного поговорим про особенности менеджмента ресурсов в гипервизорах в современных гипервизор и допускают так называемые power commitment ресурсов это когда мы можем занять больше ядер для виртуализации больше памяти чем у нас есть фактически на холстах ресурсы при этом тонко распределяются между всеми виртуальными машинами по результатам наших тестов мы для себя определили что память мы можем занимать примерно в два раза больше чем у нас есть на хостах и в плане процессоров мы можем на одном физическом ядре с гепард рейтингом или сайте запускать до 14 виртуальных процессоров мы определили требования для первой очереди нашего стенда это 500 виртуальных ядер терабайт оперативной памяти ресурсы с учетом overcoming менты вы видите перед собой при этом дополнительными требованиями было чтобы у нас без проблем устанавливались и запускались гипервизор и последнее например если касающийся а также чтобы у нас была нормальная вычислительная плотность чтобы место в стойке была занята ну не все какие у нас были варианты у нас были варианты закупки новых серверов они не прошли по бюджету просто не соответствовали требованиям мы рассматривали как вариант хэнда вакансию мерзкое железо 3d ripper и экстрима и 9 и так далее получилось очень низкая плотность как по размещению так и по памяти были варианты с легаси вами серверами как вот например альбомом 3 или ген носимые бриллианты но там уже венгер завязывая поддержку этих процессоров в этих платформах и наши поиски более менее остановились на gen8 на сэнди ivy bridge и мы закупили в первую очередь 4 сервера proliant dl 30 п gen8 с 10 ядерными все регионами и в каждом из них было по 256 памяти гигов при этом то что в этих гипервизорах были всего лишь гигабитной сети стало для нас очередная проблемой у наших к стримеров в инфраструктуре зачастую бывает от канал от 10 мегабит да и 10 гигабит и больше 10 гигабит абсолютный минимум для такого стенда и соответственно также нам нужны было нужно было симулировать плавающие задержки в сети а также дроп и кадров последние две проблемы были решены тем что мы развернули на наших серверах дополнительные виртуальной машины с на основе centos включенным бриджег и модулем на там для эмуляции задержек и дропов кадров но осталась проблема с скоростями поэтому как первый вариант было гонять 10 гигабитные нагрузки только внутри отдельных br металл серверов между не статье сексами внутреннего тогда у нас получалось что между остальными has to me все равно гигабит получалось не очень равномерная скорость на самом деле потому что иногда можно дать у них до того что у нас 10 гигабитная сеть становится и 14 15 16 гигабит а когда ресурсов мало и циpкa нагружен можно получить наоборот задержки неожиданны и важно заметить что пристрастности рование всегда нужно новым vr подтюнить некоторые параметры citylog увеличить рекс буферы подключить всего поэтому мы пришли к тому что нужно закупить коммутатор вести гигабитный были варианты насчет cisco и интер ских карт это нет метимся тоже не прошло по бюджету был вариант с небольшим коммутатором modeling новым совпа плюс всего 10 портов не прошло именно потому что скай линк очень оказывался ограничен в дальнейшем остановились на нескольких вариантов бы уж на железо от aristo или hp и бюджетные карты совпа плюс выиграл этот поставщик который давал наибольшую garantie и мы взяли вот такой коммутатор по сути ядро небольшого отсюда вполне себе хорошие железка анапские карты одна портовые и так как всего было упаковано плотно в одной стойки не очень длинные до кабели медные схема сети получилось следующее между сетью администраторов и людей там с основными серверами инфраструктуры cisco и гигабитный коммутатор здесь располагаются наши гипервизор и бермята на них крутятся мест есть секса и и они объединены в 10 гигабит ную сеть коммутатором . что же делается хранилищами в первую очередь надо подумать как можно сэкономить место каждому он с тут есть сексуально который мы разворачиваем требуется место под слаб место под системные файлы подраздел scratch для дампа йогов и место под файлики вложенных винтовок сэкономить на scratch и и на sloppy мы особо не можем на scratch и мы не экономим потому что у нас иначе он бы висел во-первых памяти во-вторых он нужен нам для сбора дампов swap если зарезать уйдет как раз преимущество laver кометном то памяти с возможностью развернуть больше нас такие сексов чем у нас есть ресурсов соответственно можно поступить двумя путями можно выделять отдельные диски для тяжелых виртуалок чтобы не занимать на основном диски место для этого а также развертывать все вот это вот через в сферах перепой грузится по сети тем самым мы не тащим для каждого если косая загрузочные файлы схема технологий авто дипой представлена на слайде по сути это секс с подгрузкой образа и конфигов с в центр по требованиям объема и по требованию быстродействия в результате нам для каждого нас тут есть секса я нужно было 12 гигов памяти на дисках суммарно не менее трех спаренный терабайт лучше больше и для данного профиля нагрузки в результате тестирования мы поняли что сатошей счас классические жесткие диски нам не подходят они являются battle никому и не выдаёт достаточного количества ooops i live in se у них получается неприемлемо и поэтому мы пришли изначально к решению что поставим raid 0 из 2 samsung тела получится терабайт и доставим еще выдашь ней диск обычный жесткий для тяжелых виртуалок которых будет немного но получилось так что вы дошли диск оказался не очень хорошо работал с нашими гена 8-ми про лентами и постоянно выдавал высокие значения на датчик температуры из-за этого несколько раз сервера перезагружались и мы просто подменили эти лодыжки на торги ssd накручиваем x500 по про байта в итоге конфигурация получилось на первых четырех серверах это 8 трейлер 160 потоков терабайт памяти 8 терабайт стороны создашь на на этих ресурсах мы развернули сначала 250 виртуальных гипервизоров после этого немножко по тень или развернули еще 150 получилось 400 и в финальном итоге у нас в инвентаре от центра было 12 чертог при этом между всеми на с лицами была стабильная 10 гигабитная сеть с какими трудностями при реализации этого этой 1 стенда мы столкнулись поставки ушной железо ну очень нестабильны и могут быть нехилые задержки потому что поставщикам железо приезжает они вот если она оказывается не то или она плохо работает они снова вижу поэтому задержки были плюс на всем периоде наши эксплуатации с данным проектом авто типа с ним были траблы причем начались они еще когда мы только начали это реализовывать на ранних версиях 65 и они продолжаются до сих пор даже на относительно свежих версиях 67 периодически приходится и стартовать сервисов то тепла так же мы столкнулись с тем что старт серверов старт села бы довольно таки долгие как железок которым который не очень спешат проверять свою память так и у нас ты сексов которая из авто диплом грузится сами по себе не очень быстро и приходится размазывать их запуск на времени чтобы они друг другу не мешали и aver commitment сработала правильно плюс не знаю насколько это может быть проблемой для нагрузочного стенда но примерно такой свои получилось 90 50 процентов в год всего лишь большая часть простое было связано с тем что я не очень прилежный администратор что же по цифрам если бы мы на момент закупки сложили сумму всех очевидных решений от наших поставщиков скупили бы домашний сервер и cisco вский коммутатор карточки им то получилось бы хорошо так за три миллиона а закупив мощное железо закупив то железо которое мы нашли мы сэкономили почти в четыре раза диски обошлись примерно еще в 80 тысяч насчет скиллинга и выводов которые я сделалась построение этого стенда мы процессе эксплуатации вывели очень много хай лот багов которые связаны с количеством нот с количеством трафика с тем как у нас параллели c многие процесс продукте и все эти проблемы мы потихоньку либо уже устранили либо стране при этом возможности нашего продукта и запросы наших костюмеров они все еще растут и поэтому мы с келли нашу нагрузочную инфраструктуру мы закупили над текущий момент еще четыре сервера того 8 на которых сейчас кроется 650 на статье сексов и около 20000 виртуальных машин и мы получили возможно стимулировать распределенные дата-центры наших костюмеров довольно тонко как вывод как напутствие для закуп очников для админов для нагрузочных тестировщиков для devops of которые работают своем в я хотел бы пожелать не расстраиваться если вам не выделяют всю эту сумму под ваши хотелки если хорошенько подумать то можно найти немного нестандартное решение и развернуть достойный нагрузочный стенд даже подхватив мало денег спасибо спасибо тебе огромное соответственно у нас есть для тебя подарок в ответ за твой подарок который ты сделал этим утром нам и сейчас будет секция вопросов-ответов и у тебя будет задача выбрать из них самые интересные и более всего понравившийся тебе да отлично я вижу минимум четыре руки спасибо за доклад у меня вопрос ты упоминал что аренда серверов не прошла по требования безопасности а если не секрет какие-то были требований почему ни один вариант не подошел там так как продукт сертифицируются непосредственно и нагрузки могут быть связаны с защитой в том числе и гостайны мы не можем разрабатывать продукты отлаживать его где-то в не доверенной среди поэтому мы были ограничены тем что мы можем развернуть он сайт сути михаил спасибо за доклад у меня в принципе первый вопрос было аналогичным почему вы не рассматривали вариант аренда смотрите вы бы решили кучука стали этим я работаю в компании do the line вас огромный install.wim варяжского облака в россии у нас можно взять приватный стенд нам пример со сферой и не иметь никаких проблем с безопасностью не согласованием всевозможных комплексов потому что все это есть во вторых у нас есть более мощное железо чем вы вот тут представили но это как бы я не знаю это калькулятор просто у вас вопрос в том а смысл вы бы вы пытались сэкономить денег или вы пытались работать для нас это было решением которое доступно для нас в любой момент которая для нас можно очень оперативно отлаживать в том числе и быстро добавлять на эти бермята серверы наши продукты физической защиты как собой например это отдельная писей на плато поэтому нам было желательно чтобы мы могли в любой момент получить доступ к железу в любой момент серверами сделать то что мы хотим и очень оперативно переключать все эти кроме того у нас все-таки часть стенда было уже развернута в железе локально до того как мы начали закупать эти серверы соответственно развернув лабу локально мне не пришлось сильно перенастраивать остальную часть моей моего стенда я мог используют все тот же а.д. я мог использовать все тот же основной и резервный сервер людей то и мог оставлять тех железных клиентов которые у меня локально уже были развернуты также в офисе можно я прямо сейчас вот вдогонку предыдущему оратору еще один детский вопрос задам меня зовут тимур вы смотрите вы собирали довольно много больших кораблей по поводу того что там этот сервер взять тот все взять скажите сколько времени это заняло у вас папа вот от начала сбора этого стенда и до того как у вас начали продукци он ее тесты гоняться там постоянно ну от начала процесса закупок до развертывания в первом видео на авто дипой вот первых 250 саксов порядка что-то трех месяцев не особо быстро но часть из этих задержка была связана именно с тем что поставки были не особо быстрыми я к чему в тот момент когда вы рассказали об общих требованиях я такой сижу пике до 10-15 штук баксов tops на это пойдет почти попал вы привели сиз цифры в три с чем-то миллиона тоже понимаю что так можно до смотрите если от трех миллионов отнять те сам этом 800 около миллиона то то что получилось да останется 2 белье на на потраченных на как это по-русски то чтоб без мата на возню с всем этим стендом чтобы он наконец заработал из палок и веток и задержки тестов который они позволили нам во время сделать продажу еще что такое репутационные риски бабла блаблабла стоило лечится так 7 2 миллионов экономии эти два миллиона экономии на самом деле стоили нам не трех месяцев задержки а примерно месяц а потому что вот на все вот эти вот подставки приемочные тесты и так далее вот ушел месяц остальное время было потрачено на то чтобы разобраться с грёбаным of the deep oem настроить правильно собственно бан давай и прочее как то так ну то есть я не считаю что именно поставка бы уж него железа нас сильная содержала настолько сильно что нам не стоило бы учитывать два с половиной миллиона economy окей хорошо понятно пасибо вопрос уж нет спасибо за доклад я здесь с другой стороны у меня такой вопрос вы говорили что ваши каста мир и хотели знать крупные скажу то есть справится ли он с нагрузкой ваш продукт при этом вы нам рассказываете сейчас что на тестовом стенде у вас постоянно этот а верка нет то есть у вас меньше памяти по факту меньше процессоров то есть вы у себя внутри не можете сказать что вот такой объем он потерян ну потому что вас железо более собой то есть вы по количеству not можете сказать в 256 например not у нас тянет потому что ну там нет никаких проблем с взаимодействие но по времени вы и ничего у себя а нагнано нагруженности ранен таком стан дениса сказать не можете какая шуринс у вас идет по нагрузке для покупателей я понял спасибо за вопрос мы можем дать полный assurance когда мы aver commitment используем очень мало или не используем это вот тоже один из вариантов использования этого стенда когда у нас для всех нас это сексов абсолютно полностью хватает в памяти и ярко мид нам по сути не используется вся память настраиваться так чтобы она резервировать под носатые секса но и в случае с тестами когда у нас aver commitment включен когда он широко используется когда у нас по максимуму развернуто на статье сексов мы не особо в наших тестах находим разницу over commitment конкретно на функционирование наших агентов не особо влияет то есть ресурсов ресурсы который потребляет наш агент они не так значительны и и там всего хватается ровно даже несмотря на верхами и по памяти и по процессам из вопрос у меня тогда давайте пока это просто не совсем популяции структуры но про нагрузку мало того что нам надо нагрузить из тысячи виртуалка нам надо понять как они работают вот с этим вопрос мониторили как и поскольку их очень много это тысячи виртуалок наверное да как-то было учитывать что сам мониторинг будет какие-то ресурс кушать с этим как стало проблемой какие как решали мониторили с помощью вембо на и он был развёрнут отдельно при этом мониторе лась это им не всегда часть времени когда нам была полная нужна была полная независимость от вот этих вот всяких дополнительных нагрузок отсчитывания контров мониторинга мы просто мониторили через встроенные метрики самой с малого центра через у сфер вот тут есть вопрос еще один вы сказали что вы создали нечто которое позволяет эмулировать распределенный дата-центр там где-то есть какие-то требования гост но к сожалению я видимо отношусь к тем кто не понял ничего поэтому основной вопрос а что собственно тестирует администраторы в чем заключается эта highload задача что они там делают такого особенного что хайло задачи появляется пожалуйста на ну например кейс у нас есть micro сегментация в этой микро сегментации мы можем создавать правила которые выглядят очень простыми первоначально но когда они раскатываются на гипервизор и они становятся сложными их становится очень много например мы можем создать правило что по вот этому вот списку портов запретить связь между вот этим сегментом в котором 2.000 винтовок и вот этим сегментом где 5000 виртуалок и мы создав одно это правило берем две тысячи умножаем на 5 тысяч умножаем на 2 и умножаем на список портов и вот этот вот весь список правил летит на каждый из эссекса рф чтобы ну на которых включена сетевое межсетевого экранирования поэтому один администратор может создать действительно своим запросам вполне себе валидным большую нагрузку дорогие последний вопрос видимо зовут иван вопрос на самом деле такое смотрите секса я до 10 тысяч 500 штук виртуальных которые место они вас наверняка постоянно падали ну потому что ваш компонент это ставится внутри секса какое-то правило да как проанализировали logis этих секса рак был какой-то ваш сервер который все это анализировал потому что надо понимать это его либо ваш продукт либо эту инфраструктуру либо эта сеть да мы изолируем в продукт драйвер и соответственно бывали такие случаи когда особенно на ранних этапах реализации разных вещей есть секса и падали в соды разумеется при этом для этого нам нужен как раз с кароч раздел на диске в котором у нас откладывает дам и откладывают салон в основном анализ был пологом при этом в некоторых случаях мы просто подключались к консолям и отлаживали 100 спасибо спасибо большое соответственно подарки вот эта вот хайло да тебе на память о том что ты здесь выступал сумка и сертификат и соответственно теперь нужно выбрать пассив дорогие а теперь нужно выбрать самый понравишься вопрос из зала но на самом деле последний вопрос действительно было актуальным на счет входов потому что да это будет кружкой азии книга от компании wargaming спасибо большое"
}