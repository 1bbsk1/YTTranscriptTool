{
  "video_id": "MKRTv_fMmOE",
  "channel": "HighLoadChannel",
  "title": "Распил монолита в Леруа Мерлен / Павел Юркин (Леруа Мерлен)",
  "views": 18202,
  "duration": 3010,
  "published": "2019-07-18T07:48:19-07:00",
  "text": "вот меня зовут юркин павел я java разработчик и я один из первых java разработчиков которые были приняты в эту компанию и сегодня я бы хотел вам рассказать каким образом мы рассматривали наш монолит в леруа мерлен я надеюсь эта тема будет актуально для тех кто только начинает приходить на микро сервисную архитектуру и вы возьмете некоторое количество идей и кораблей как их избежать поэтому давайте начинается мой доклад будет где-то 35 минут после этого будет сессия ответов на вопросы если вы не успеете задать свой вопрос давайте общаться в дискуссионных зонах или пишите и так прежде чем я приступлю непосредственно к технической части я бы хотел немножечко вас погрузить в контекст я бы хотел рассказать вообще откуда у нас монолит почему у нас он не устраивал и вообще каким образом leroy merlin там школа ремонта до пришла на эти конференцию потому что это один из самых частых вопросов задаваемых мне и ребятам которые работают на стенде ребята вы же вроде там ламинат продаете чем вы вообще тут делаете это российский интернет технологии и на самом деле очень грамотные правильный вопрос потому что еще два с половиной года назад никакой разработки в леруа не было и поэтому садитесь поудобнее сейчас я вам расскажу немножечко истории пойти в леруа небольшая вопрос кто из вас знаком с лором варкрафта поднимите просто руки оно понятно спасибо видимо следующий раз я сделаю слайды сумерками ok хорошо итак давным-давно в во франции семья лирой семья мерлин объединились для того чтобы делать то что у них получается лучше всего помимо воспроизведения потомства они занимались тем что продаются стройматериалы и у них так хорошо получалось довольно скоро компания заняла лидирующие позиции на французском рынке открывались и филиалы в других городах и через некоторое время рынок просто стала перенасыщенным грудных валился золото стало не хватать и руководство компании приняло решение о развитии в других странах я мерлин открыл представительство в испании италии португалии ряда других стран и в конце концов в россии начально в россии строился по западным лекалам что я имею ввиду здесь все эти и все производство эти было во франции и в россии глина не это всего лишь продавцы и менеджеры которым нужно было инкрементировать процесс который спустился из материнской компании но мы все прекрасно с вами понимаем что тут вам не там да и спроси его немножко отличается писаться во франции ну простой пример в во франции неболшой инсайд леруа мерлен это магазин для премиум класса а в россии это для обычного обывателя но тем не менее россия использовала французский софт и соответственно те правила игры которые он задавал ей приходилось исполнять и так все жило и развивалась пока не появился бизнес который захотел большего первоначально это бизнес попытался договориться с французами на самом деле леруа мерлен россии это unit 2 о оборотом ну и бизнес россии сказал ребят ну вот мы приносим довольно большую часть пирога давайте вы нам наши доработчики поднимите в приоритете на что наши коллеги сказали конечно ребята естественно сейчас мы все сделаем ну и по объективным или субъективным причинам этого не было сделано и ну наверное основная причина заключалась в том что франция писала сов не только для россии но и для всех остальных business unit of плюс к этому так как разработчика всегда не хватало и capacity всегда по всегда была маленькая софт он не касался как это должно было быть то есть есть и из впишется под заказ обычно софт закупался закупался и просто напивался напильником а вы прекрасно знаете что бывает купить поезд и потом с помощью напильника сделать из него самолет особенно если этот поезд монолитный и нашему бизнесу необходимо было что-то что удовлетворит его амбиции в быстрых изменениях ему необходимо было собственно идти и именно в это время два с половиной года назад было решение было принято решение о создании разработки в леруа мерлен и полина нет и первые четыре java разработчика среди них один из ваших привет среди них ваш покорный слуга и наша основная задача была наша основная задача это отказаться от старого проприетарного софта прошу прощения за старого софта и в общем выкинули все монолитные решения и напилить на новых технологиях сот который позволял бы бизнесу реализовал реализовывать свои желания и потребности со вступительной части все давайте теперь рассмотрим непосредственно архитектуру всего леруа итак у нас было энное количество стареньких master system в этих мастер системах заводились продукты они занимались кладами они считали количество товаров на полке и т.д. и т.п. и при любом изменении в них в формате xml один из лучших форматов который придумывать человечество они послали информацию через старенькую лесби шин у нас она так и называется enterprise service вас есть b и эта информация переходила в нижележащие системы одно из этих нижележащих систем был опус и обувь это фактически система которая является мастер данными для интернет-магазина то есть все данные которые вы увидите если вы зайдете на сайт mengen.ru будь то товары будь то опытом характеристики еще что то это все было в опусе зачем это нужно было делать и почему нельзя было брать из старых мастеров со всем потому что все старые мастер системы помимо того что они были монолитные обладали одним замечательным прекрасным свойством которые нам всем очень нравится они очень плохо давали попей то есть они просто тупо не держали нагрузку и не было возможности перекрасить интернет-магазина эти старые системы поэтому был создан некий танкер перепало прослойка это опус который являлась мастер данными для интернет магазина ну так как а по создавался в тоже времена в те же времена что и старенький master system он тоже естественно был монолитным и это именно та система которую мы с вами сегодня будем распиливать и это именно та система которые я буду сегодня рассказывать и так давайте рассмотрим архитектуру старой системы и поймем почему она нас не устраивало и почему мы решили от нее отказаться ну во-первых старая система она состояла из двух частей первая часть это интеграционная она довольно маленькая она всего лишь занималась приёмом информации от master system и она клала это все в базу данных мы использовали кучу bass их есть маленькая вернее уже большая подсистема core которая занималась обработкой запросов с фронта а также пересчетом данных производных от данных получаемых из мастер системы также использовался ластик для поиска новых запросов и вы наверное думаете каким же образом он наполнялся а вот таким образом он наполнялся на самом деле просто был один вкус были написаны на java который при любом insert или обрить этот документ просто брал и тупо кого ластик то есть фактически получалось что и вкус базе власти key 1 ему одни и те же документы с точки с точностью до каких-то там форматов и косметических изменений причем если мы посмотрим на то каким образом информация хранилась в базе данных то было принято довольно оригинальное решение на api разработки этой системы всего лишь был один то есть во всей базе данных была лишь одна коллекция она звалась дата и там лежало абсолютно все каждый документ лежащие в этой коллекции имел свой тип например были документы с типом продукт и были документы с типом атрибут и все эти документы они ссылались друг на друга огонь они ссылались друг на друга и то есть в документе stepan продукт было верное количество ссылок на атрибуты это просто был массив которые потом доставались из той же самой коллекции по ключу и вы прекрасно понимаете что с одной стороны это конечно же все хорошо и и это хорошо потому что можно было все что угодно взять и запихать в базу просто декларируем новый там тип практически ничего не надо менять в базу можно запихать вообще абсолютно все что угодно но с другой стороны чтобы вытащить один полный продукт 11 полный документ необходимо было сделать несколько запросов базу и это дело работала до того момента пока лера мерлин была маленькая и не разрослась у нас сейчас 90 магазинов и мы считали что чтобы вытащить один документ полностью необходимо сделать до 40 запросов в базу естественно это никого не устраивало или по performance у это не проходило и когда я говорю о том что в этой базе лежала все в этой базе лежала действительно абсолютно все включая некоторые перми шины юзеров и собственно говоря вышесказанное она рождала некоторые проблемы как вы понимаете первая проблема это проблема с производительностью потому что необходимо было несколько запросов в базу несколько много запросов базовой это просто непир формула 2 самос монолит у вас проблемы с производительностью вас автоматически есть проблемы с устойчивостью потому что вы можете подать если у вас большая нагрузка и в том числе понятное дело что невозможно было что-то быстро и хорошо менять потому что в монолите естественно все со всем связано и изменяете в одном месте ломается в другом вы прекрасно понимаете и вот с этими проблемами к нам пришел наш бизнес и сказал ребята вы как бы вас падает время и мы не можем там свои хотелки реализовать давайте вот чем делать думать что разработчики на это дело на третью разработчики посмотрели на эту монолитную систему разработчики посмотрели на это способ хранения поржали и сказали ребята вот мы сейчас вот это все выкинем и на первом новую систему есть монолитно на спасти не удастся надо его поставил от него отказаться ногу не будет новой системы через n месяцев у вас будет ребят конфетка прям вот будет просто огонь что на это сказал классический бизнес нет нет ребята мы не готовы так работать вы через м месяца нам напились и какого-то абстрактного коня в вакууме будете пилить ещё два года интегрироваться и в общем нас это не устраивает давайте казаться давайте все сразу и мы поняли что у нас получится просто взять выкину системы напилить новую мы поняли что нам необходимо сделать каким-то образом плавный желательно бесшовный переход от старой системы к новой и как бы есть слона по частям а монолит пилить по кусочкам и собственно говоря первый же вопрос который у нас стало каким образом мы будем пилить нашу нашу монолитную систему на кусочки или на микро сервисы ну вот товарищ самим который написал много книг по микро сервисом но со своей книге билде майка services он говорит о том что необходимо делитесь микро сервисы по бизнес функционал и я с ним в принципе абсолютно согласен но только вот есть вопросы а вот продукт это бизнес функционал вообще что такое бизнес-функций но вот продукт а атрибут продукт это бизнес функционал непонятно и я бы хотел дополнить некоторыми нашими своими соображениями по поводу того каким образом вот мы разделили наш монолит во-первых если у вас есть сложная какая-то бизнес-логика то это очень неплохой кандидат на внесение в микро сервис приведу простой пример но допустим весь продукт и у него есть атрибут сопутствующие товары там каждого продукта есть сопутствующие товары если вы зайдете на мой сайт ритейла и расколите вместо на этим пэйдж на корочку продукта и пасквале эти вниз там будет некая табличка с этим товаром еще покупают вот вот это и на самом деле вот чтобы понять что там еще с ним покупают необходимо проанализировать историю продаж вообще это довольно не тривиальная вещь существует целый компании которая у которых один данный продукт это именно построение вот это вот этих вот рекомендовать рекомендательных систем в россии the retail ракет в сша и торричелли man's я кстати работал у них работал с кодом так вот я вам скажу что пока они не почистили историю гита их репозиторий весь занимал 10 гигабайт я не знаю как но реально это занимало вот 10 гигабайт то есть понятное дело довольно сложное бизнес-логика поэтому имеет смысл эти стены ну и также мы выносили все атрибуты продукта которые имели контекст что это означает под контекстом я подразумеваю изменение значения в зависимости от магазина или региона ну то есть если допустим имя продукта название продукта это не контексту ализе равный атрибут то есть он везде плюс-минус по россии одинаковый а цена продукта может влиять различаться в зависимости от региона и магазина и поэтому это контексту ализе равана атрибут мы решили у себя контексту ализе раны и атрибута тоже выносить в отдельный микро сервиса чтобы не раздувать продукта репозиториев где мы хранили контексту ализе раваны данные и давайте теперь рассмотрим а типовой пример того каким и ну что он как мы выносили бизнес функционал из нашей системы вот у нас есть наша старая монолитная системы и там есть такой business france она как видимость эта характеристика товара которая говорит о том надо вступим ли этот товар по прямой ссылке на сайте в том или ином магазине или регионы некоторые магазины хотят продавать хотят продавать этот продукт некоторые не хотят они могут вскрывать это или смотреть и или у диктовать свое бизнес-логику основная проблема еще раз которые мы решаем эта проблема то что безу суде чем заняться почему потому что он не может изменять это бизнес-логику этого этой характеристики и первая проблема это это как раз то что мы решили мы просто то повынесли этого бизнес-логику в отдельный микро сервис и поначалу этот микро сервис был очень простым тире тупым все что он делал это раз в сутки он считывал из с парой же системы всю необходимую информацию для того чтобы рассчитать характеристику видимость рассчитывал ее и записывал обратно то есть ничего практически не изменилась просто мы вынесли бизнес-логику в микро сервис и сразу же мы столкнулись с проблемой что нашим монолитная система она была очень чувствительна приему информации больших объемах вы представляете если вы рассудке считаете там 300 тысяч товаров а у нас 90 магазинов то вот это все идет несется в вашу систему она монолитно она может прилечь всем будет не очень хорошо поэтому появилась база данных которая помогла размазать нагрузку самое элементарное что она сделала это мы сохраняли значения уже рассчитанная вчера для видимости всех продуктов расчитывали их сегодня по расписанию и сравнивали если значения изменилось мы записывали в систему если оно не изменилось то ничего не изменилось ничего писать не надо таким образом размазав за данные которые мы посылали в нашу систему мы научили ее кушать что мы как бы все что мы захотели бы скормить а новым фронтом и новым командам и вообще компании мы сказали что ребята вот наш наш вот это микро сервис видимость это теперь до факта master system по характеристике видимость и вам необходимо ходить в нашу систему если вам необходима эта характеристика они в старые старую систему но при этом старый фронт все еще ходил старую систему потом объясню почему и таким образом все стало хорошо за исключением того что мы делали это все рассудке чтобы получить обновление real-time необходимо было просто взять и переключить поток из master system в наш микро сервис и таким образом мы смогли отказаться от таймера мы смогли получать информацию в реальном и и обсчитывать в реальном времени и мы смогли отказаться от любых чтения из старой системы теперь мы туда только записывали записывали мы почему потому что фронт все еще сидел на этой старой системе вы прекрасно понимаете что таким же образом можно вынести все остальные бизнес характеристики бизнес функциональности что собственно говоря мы и сделали у нас у каждой у каждого мика сердца который отвечал за ту или иную бизнес функциональность было свое опять очень простое которая давала ну то что надо и когда это все сделали мы не могли сказать front ую ребята вот у нас есть наши микро сервиса пожалуйста идите все информации есть пожалуйста вернее мы могли мы потом попытались но ничего не получилось потому что у нас фронт немножко ленивый хотя на самом деле или него задача реально много разных и фронт хотел шовного перехода вплоть до сохранения контракта а так ему бы пришлось самому собирать из наших игр сервисов тот же сон которую который которому он привык поэтому у нас появился оркестра tor который делает эту работу за фронт этот регистратор имел тоже контракт с фронтам как не старая master system после этого фронт смог подключиться просто поменяв ссылку со старой системы на оркестра то старую систему можно было опускать теперь давайте рассмотрим производительность потому что это довольно важные вещи и наверное одна из самых важных при проектировании такого типа архитектуры по сравнению с прошлым слайдам здесь добавилась еще одна сущность этой битвой для тех кто не знаком с микро сервисными паттернами где твой это просто точка входа которая ограничивает вашу доменную модель от внешнего мира какой бы он ни был и вы можете там прокидывать с получить url и определенные какие-то технические url и вы наоборот закрываете чтобы внешний пользователь не смог вашу систему сломать или каким-то образом ей воспользоваться как вы не хотите и у нас была задача и мы поставили перед собой своей успеть за 60 миллисекунд при этом мы знали что каждый микро сервис вследствие работы внутренней а ну и плюс сеть он съедает где-то 15 это до 20 миллисекунд где-то 10 15 20 а это значит что за 60 миллисекунд мы не могли себе позволить количество последовать их соединений больше трех у нас их уже три что это означает это означает что лежащие микро сервисы которые вот который на базу данных смотрят они не имеют возможности не имеют времени ходить еще куда то портите пи все что они могут это просто взять информацию из базы данных и отдать ее наверх то есть таким образом мы говорим о том что вся информация уже должна быть пред рассчитана они считаться на уровне на момент запроса мы используем джоуи стек spring good и мунга и мы знаем что у нас при такой при такой схеме данных в manga деби если мы сделаем если мы сделаем яндекс по региону и продукту то мы сможем успевать по запросу отдавать данные булевских максима лишь за 15-20 секунд конечно зависит от того какой объем данных в этом перегоняется там и т.д. и т.п. в запросе можете несколько указать но это не суть в общем где-то где-то 15-20 я на самом деле если по нескольким продуктом еще меньше таким образом не лежащий микро сервисе отвечали за 20 еще два раза по 20 накидывали другие микро сервиса и мы в наш если попадали вот здесь есть график мы используем не указ так и здесь есть график из рибаны по производительности поднимите руки кто знает что такое персен тилль спасибо для тех кто не знает смотрите я допустим 75 пирсинг или это 40 процентов 75 посетили это 40 миллисекунд это означает что 75 процентов request of было отвечено за 40 миллисекунд по вот этому графику видно что наш наша система 70 95 пирсинг держит в районе где-то 60 миллисекунд и это именно то что мы чего мы обещали о чем мы хотели к чему мы стремились а 99 он не выходит за сотню то есть нас это устраивало теперь давайте рассмотрим потоки данных потому что зажигает вопроса каким образом нас вообще эти при гальку ленты наших микро сервисах образовывались вот допустим рассмотрим на примере все тоже видимости предположим что видимо считается последующем характеристикам последующие бизнес-логики прошу прощения предположим что продукт считается видимым если у него есть название ну контент щеки завели название и если у него есть цена то есть установили цену на самом деле гораздо сложнее бизнес-логика но этой презентации допустим просто вот так будет у нас используется и в andre van подход и каждый наш микро сервис при изменении своего состояния отправляет измененное состояние той характеристики за которые он отвечает в шину мы используем ребенку при этом другие микро сервисы которым которые заинтересованы в информации они могут подписаться на эту эти очереди на это так и получать информацию в реальном времени таким образом наш микро сервис видимости должен подписаться на продукты и на цены но вы прекрасно понимаете что все эти два два потока данных они приходят не одновременно что это означает это означает что когда пришла цена вы уже должны знать а у вас есть у продукта имя или нет иначе вы не рассчитаете или если пришел изменение по продукту вы должны знать о вообще цена она как бы есть и таким образом так как мы апологеты микро сервисной архитектуры и мы не можем ходить в другие базы нам необходимо было сохранять информацию вот эти ивенты от сторонних сервисов в наш микросферы с данном случае видимости идеи скажите вы батенька да у вас тут некая дубля ция данных вы будете абсолютно права одобряться данных она чему приводит ну вот она примерно вот этому приводит и вы абсолютно правы и в данном случае при данной архитектуре дубля ца данных это неизбежное зло для того чтобы просто иметь возможность быстро ответить у нас это л бизнес и для нас гораздо важнее ответить быстро но не совсем точными данными нежели чем ответить абсолютно точными данными но через 30 секунд нас то есть наш клиент просто столько ждать не будет но есть пара моментов которые боль с некой системностью могут облегчить первый момент тупятся данных не обязательно должна быть полный то есть если что-то изменяется в продукте микро сервис продукт допустим изменяется какой-то атрибут микро сервис отсылает ивент в машину и отсылает полный список микро сервиса учтивости ему которому интересно . имя ему нету необходимости хранить все событие более того ему даже не нужно хранить само имя я вот например спуск бы не копировал в по микро сервисом в системе можете случиться и таким образом весь большой гигантский jison он в микро сервисе видимость его рождался только водку близкое значение которое использовалось бы при расчете которая показывала вообще имя есть или нет то есть дубляже данных не совсем полная второй момент заключается в том что он еще раз одну бля цыганок полностью отказаться не получится так давайте сделаем так чтобы смогли река улица очень быстро если есть какие то проблемы наш каждый микро сервис оснащен механизмом а ринита и этот механизм он позволяет просто запустить еще раз бизнес сущность за который отвечает микро сервис в очередь таким образом если какой-то микро сервис изменил логику или какой-то метро в каком-то микро сервисе был один бак и нужно что-то пересчитать мы просто запускаем нет и у нас на шаг резистентность восстанавливается более того это ниц мы еще раз только обнаглели что отдали на поддержку понятное дело что разработчики они не 24 на 7 поддерживают пруд ну вот у нас в леруа политика you build your own но понятное дело что там ночь вы сидеть не будете это правильно это нормально и существует поддержка это на тоже нормально так вот если есть какой-то инцидент на продакшене который требует оперативного вмешательства то наша поддержка умеет вот эти орлы и дергать и таким образом опять же восстанавливать справедливость в нашей системы останавливая системность вот таким образом мы не совсем не не очень сильно страдаем от дубля ц'ада на который рассказал итак давайте рассмотрим итоге что у нас получилось ну вот у нас получилось с архитектурой которые вы видите на слайде распределенная система которая держит 30 fps на самом деле побочным на самом деле больше но у нас просто на проди такую большую нету тристар пьес с временем ответа 95 персен тилль 60 секунд и это без тюнинга что значит без тюнинга это означает что вы просто вот сделали оля что-то похожее использовали при calculate и поставили несколько issues of микро сервисов и у вас автоматически вот такая производительность будет вам не нужно настраивать и какие гарбич коллекторы там еще что то образом играться с количеством этих микро сервисов инстансов хотя это тоже возможно если вы хотите больше большей продуктивности это тоже реально но вот это вот как бы фактически результаты архитектуры из коробки дальше мы получили устойчивую систему потому что мы используем микро сервиса и один контейнер упадет не страшно из другие все замечательно и самое главное что мы получили для бизнеса это возможность изменений потому что разные релизные циклы релизом только один миг росси раз все замечательно все хорошо ну и плюс все остальное что позволяет все остальное все остальные преимущества которое дает микро сервисной архитектура я перечислять не стал потому что вы их прекрасно знаете лучше чем я на что следует обратить внимание и чтобы я хотел чтобы вы вынесли вообще из сегодняшнего доклада сегодняшнего разговора это самое главное с моей точки зрения вещи для проектировке данного типа тем что архитектуры про которую мы сегодня с вами поговорили необходимо сделать первое минимизировать количество последовательных вызовов почему потому что последовательные вызовы это не только плюс вашем времени ответа вашего самого верхнего сервиса но еще это . отказа и вероятно ну и повышение вероятности того что вам придется request повторять я не знаю как у вас но у нас есть бывает ведет себя очень очень нестабильно и очень очень хорошо то есть лишний узел это возможность еще раз сделать запрос и с соответственно будет гораздо хуже второе чтобы минимизировать количество последовательных вызовов на каком-то этапе необходимо будет использовать при calculate и то есть micro сервисы на каком-то этапе будет вынужден ходить только в базу отдавать что-то по ключу и никуда к другим микро сервис для того чтобы эти при calculate и просчитать мы используем andre van подход там где все подписчики грейся микро сервиса посылают в шину сообщение сообщение о своих статусах изменениях и икра сервисы которым это интересно они подписываются и таким образом внутри себя или рассчитывают данные за которые они отвечают при этом нужно понимать что при таком подходе будет все равно какая-то дубль отца данных и и хорошо бы минимизировать как минимизировать не обязательно храните весь ивент в исходном виде необходимо хранить только ту характеристику которые на которую за завязано бизнес-логика и наверное если нужно понимать специфику вашего бизнеса если для вас частота данных это прямо сам очень важно то наверно вот это все дело не подходит просто для нас важнее отдать быстрее и если какие-то проблемы есть в системе и консистенции была нарушена та связку справедливость помогут восстановить ринит и которые я рекомендую сделать в каждом сервисе и юзать когда есть какие-то проблемы на этом у меня все большое спасибо за внимание я готов ответить на ваши вопросы вот уже руки-руки-руки сейчас посмотрим дикого быстро скажите пожалуйста либо николаем тесты у меня вопрос к архитектуре вашего решения рассматривали ли вы кафку и почему выбрали рэпе темпе спасибо очень хороший вопрос на самом деле все еще просто наши ивенты которые мы получаем от наших master system вот этих старинных я не знаю почему на наша master system она может писать только в розыгрыш и потом из этого подвеса уже наши ресиверы считывают информацию и отправляет в наш ребят таким образом зачем бы для нас нужно было кавказа тем что это лог-сообщение можно ли процессить это классно но вследствие того что я сказал выше у нас просто все эти события лежат в плоскости я понимаю что с архитектурной точки зрения это не совсем грамотно и мы просто там можем проставить статус new с процесс и наши ресивер снова это все дело за процессе ты таким образом мы сможем еще раз прогнать венты то есть нам не нужно не нужно сохранять ивенты потому что они уже сохранены a rabbit с моей точки зрения дает большую гибкость в плане роутинга сообщение по exin чем чем кафка поэтому ребят я надеюсь это этим ножку добрый день дмитрий вопрос по поводу вашей съемок делаем и хотите рукой встать и потом представители россии дмитрий вопрос по поводу обработки собственно событий которые шины поступают на вашей схеме у вас получается что сами веб-сервисы отвечает за обработку событие конечно вопрос тогда с мышц масштабируемостью как вы решаете этот вопрос разворачиваем на и количество сервисов которые подписываются на события в примите это решается очень просто у вас есть легче и вы понимаете какое то количество сервисов с одинаковым прекращены вас создается автоматически очередь через которую который из которой читают количество военное количество ваших соединим ваших микро сервисов и они распределяют приписан раздает все его нтр из этой очереди по разному иисусом разные с и могут соответственно в многопоточном режиме работать параллельно столько проблем нет как они определяют кто именно пазу вашу правят в которой хранится при калькулировании информацию кто именно до из инстансов тут тут есть два момента первое если использовать манга мы используем он году мы используем только одну коллекцию там есть такая история как оптимистичная блокировка таким образом если два микро сервиса пытаются что то сделать с одним и тем же документом они его сначала считывают там есть версия лонгов и они оба обновляют после этого кто первый записал версия поднялась 2 упадет после того как он упадет сообщение фактически 0 вернется выбит на самом деле немножко другой технологии но не суть врач он сообщения это еще раз считает и он еще раз попытается за процессить и тогда уже в базе она будет нужна новая обновленная версия то есть учитывая вот этот вот механизм этих проблем ну проблем с точностью нет второй случай когда что можно сделать это использовать подрос и блокировки на не было бы удобнее вынести обработка событий в отдельную службу обработку события в отдельную службу мне кажется это архитектурный холивар давай вынесен за рамки давай пообщаемся дискуссии на зоне мне очень интересно как ты это видишь и мы можем пообщаться пространство это это большой разговор я я чувствую добрый день спасибо за доклад вы говорите что нужно экономить последовательных вызовов и стремя с теми hope my уже достигает границы которые для себя познать 60 миллисекунд общем вы нет ни какие твои и регистратор ваш который игра говорят просто прямой хаптор неких если не насколько и видно на схеме пожалуйста я я услышала и росту почему вы ты и твой регистратор не объединили эркера свои я понял эту убрать тоже очень хороший вопрос по следующим причинам моей точки зрения с архитектурной точки вот я и так вижу регистратор это сущность которая создается для определенного фронта придет другой фронт ним он ему нужно формат другой формат данных и по хорошему для него нужен другой регистратор который будет ходить в другие микро сервиса и реализовывать другие углы и почему потому что если это все будет делать один регистратор то будут релизы друг друг на друга аффекте и возможно будут проблемы поэтому оркестра тары с моей точки зрения это разные сущности один под потребителем а gateway это просто тупой тупая прокси а реверс которая ну просто запрещает ходить по определенным углом я понимаю отдельные orchestrator чтобы держать текущей фронт да то существует проблемы когда кто-то будет писать новые фронта и вам придется думать о проблемах фронта на самом деле не совсем так на самом деле у нас компании есть еще такой проект называется api менеджер и это такой проект который вот мы все наши микро сервисы которые мы сделали мы вспомним в этот api менеджер а дальше сама команда фронта с помощью квадратиков и там графический интерфейс у нас сама может она тыкать того что ей надо и вообще наша идеальная ситуация это тогда когда мы делаем только данные а уже любая команда фронта берет то что ей необходимо это каким образом она берет анохин лицам а потому что в таком случае в по вере в противном случае мы просто не успеем для всех и будет очень долго спасибо за доклад следующим вопросом часто до какого-то представления нужно сделать выборку связанных данных из разных микро сервисов вот на каком уровне вы реагируете связанные данные или выбрать именно из разных микро сервиса или это уровень жить вы там правку или ли это отдельным coursers которые хранить там минимальный набор данных это делается в их этом какое-то утконос то замечательный вопрос потому что на самом деле вот данная архитектура она действительно очень страдает условно говоря если нужно делать какие-то сложные join и сложные запросы потому что мы как бы по микро сервисом распределили а если нужно это же нескольно там где все в одной базе мы решаем эту проблему так у нас допустим есть необходимо сделать какой-то отчет в котором есть какая-то агрегация для нас каждый отчет это отдельный микро сервис и он подписывается на все те же очереди которые необходимы для то чтобы его построить и там тоже есть соответственно дубля ца данных ну а для нас отчеты это не низкий приоритет мы стараемся подписывать их на очередь чтобы они не ходили по по его майк и не грузили систему таким образом от все работает спасибо сначала 1 потом здравствуйте спасибо за доклад на соответственно у меня три вопроса один наверное бульварный то делаете кажется что ваша архитектура все-таки не микро сервисное просто сервис-ориентированной архитектуры мы об этом обсудим может быть чуть чуть попозже второй вопрос может быть вы чуть-чуть раскроете внутренности вашей микро сервисной инфраструктуры то есть это мониторинг это логирование это вот api управления это может быть какие то стоит full state vs разделения вот внук внутрянку то есть понятно что вы сейчас такие логические единицы рассказывали но обязательным они на что-то опираются вот если есть что сказать я бы с удовольствием услышал вот и третий вопрос если я правильно понял у вас на каждый микро сервис экземпляр манги 1 и соответственно рано или поздно ну то есть пиковая производительность вашей всей системы будет упираться в пиковую производительность каждый манги по отдельности ну то есть это будет суммарный эффект и соответственно как вы собираетесь это преодолевать то есть как вы будете реагировать мангу первый вопрос это холивар добро пожаловать в дискуссионную за ну давай обсудим второй вопрос насчет внутрянки мы для лакирования используем но если как это все сейчас работает у нас пока нет кубе раз мы туда переходим сейчас мы просто диплом ручками не ручками через дженкинс используя такер canpol и просто поднимаем контейнеры и там они живут и работают пишут мог в консоль докер это все файлы собирает сан используется емкости для того чтобы собирать логе делать alert и т.д. и т.п. мониторинг у нас мы используем запись и что еще до управления и соответственно от сами функции api guide а вы мне кажется чуть-чуть проблем бывает двух видов 1 она у нас просто есть и мы просто даем один общий swagger на гитаре то есть то что мы позволяем дергать это первое второе мы регистрируем в нашем наши api платформе и дальше как это живет пока ещё я не могу сказать в деталях просто там приходят пользователи и юзают это насчет манги у нас первоначально как было вот еще раз 4 разработчика мы сделали все сами мы подняли мозгу в контейнере свою мам смотри нам на диск все работало замечательно был только один инцидент когда docker и встал напротив за два года я считают очень хороший результат и но сейчас мы переходим с нашим ангел руб ручной на мангу которую поднимая lady bee уже которые в этом шарят и мы от них пытаемся получать и вскоре временем будем мангой за сервис грубо говоря таким образом они внутри это сделают я думаю что это будет один кластер манги там будут штампованный существует база данных до есть основание то есть это 1 от 1 г при кассет для каждого микро сервиса там будет своя база данных это сущность манги и один из микро сервисов это продукты так как их действительно очень много абсолютно точно будет шарди rowan гонка позволяет сортировать пока реакция и вот у нас вот одна коллекция будет шарди равана она сейчас родированное просто сейчас в разных это манга х ребята планируют собрать эту одну на счет производительности которые мы упремся мы сейчас тестировали и у нас мелкие микро сервисы они держат 2005 секунду нам пока хватает вероятности что мы будем каким-то образом расти гораздо быстрее у нас нет и две минуты последний вопрос остально в кулуарах простите пожалуйста спасибо за такое хотел спросить среди от с ориентацией на события вообще построения армен данных по всей системе несколько вопросов первое это обеспечиваете ли вы как-то и задумывались о гарантированной доставки сообщения да ребята дашиной т.е. понятно если она в шину попало то она так или иначе шина гарантируем don't откуда обработчика обработчика там по своему вот гарантированно доставляются сообщение дашей но вас я что имею ввиду например что-то изменился быть и произошло мы опубликовали сообщение ну например сеть магнолии дары бетону не да не добралось вот этот вопрос как то вы отдельно я вас понял на самом деле подавляющем большинстве случаев в подавляющем большинстве случаев за исключением только наверное ручного взаимодействия любые события которые могут не дайте они поражаются другими событиями что это означает это означает что вот у вас пришло события там допустим а изменение цены в там магазине дацана перестала быть актуальной там деактивирована цена микро сервису видимости нужно пересчитать он пересчитывает ставит фолз отправляет данного то отправляет ваши и падает не дошло что означает это с точки зрения обработки сообщения о изменении цены которые столь же шины пришло это означает что мика сервис он пока не получит и не будет уверен то что он обработал все правильно то есть не получат вот ребята ответ о том что я твои сообщения принял вот это по видимости он не скажет что сообщение по прайсу было обработано и только тогда когда микро сервис который обрабатывает событие говорит что все замечательно он говорит ребят у так и это все в работал можно удалять таким образом механизм retrieve позволяет обеспечить то что вся цепочка будет иметь место быть проблема в том когда если у тебя по http пользоваться что-то меняет руками ну тогда ему 500 ошибка вернется и мы и мы надеемся что он будет пробовать еще раз ну то есть маркетологи думали что вы подшучивают друг над другом да у нас время закончилось вы даже сейчас продолжительной поблагодарим и спросим кому мы дарим книжку за лучший вопрос тут был бы не пожалуйста три вопроса зубодробительных важных хороших по архитектуре я думаю что стоит отлично говорить если вы до спикера не дойдете вы можете как раз у коллеги уточнить"
}