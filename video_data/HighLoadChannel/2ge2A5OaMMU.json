{
  "video_id": "2ge2A5OaMMU",
  "channel": "HighLoadChannel",
  "title": "Docker & Puppet: как их скрестить и надо ли вам это / Антон Турецкий (Badoo)",
  "views": 228,
  "duration": 2638,
  "published": "2017-04-22T13:38:18-07:00",
  "text": "там работая в компании баду системным инженером то есть это исключительно отдела эксплуатации к программированию отношения постольку поскольку вот и сегодня я смотрю что зал достаточно пол не то есть технологии я думаю что для всех кто пришел на ну во первых это не новая технология но это такой модный новый тренд я думаю что большинство из вас права продукт уже слышали раз вы пришли послушать и мне хотелось бы после вот там нет а по про docker и всего остального увидеть руки тех кто пробовал от продукт просто запускать а на него смотреть есть такие общем зале отлично те кто запустил докер в продакшен работать с ним сейчас я могу даже поднять вторую руку они от вот чуть больше чуть больше вот хорошо ладно значит тем кто еще не запустил это вполне возможно будет интересно итак значит начнем содержание о чем я попытаюсь рассказать вот в рамках этой сессии про докер я попытаюсь рассказать вам предысторию того вообще почему правильно этот продукт посмотрел я посмотрел наш отдел вообще в целом наша компания из сиама выбирали почему мы к этому пришли потом расскажу вам про подготовку железной составляющей то есть как было подготовить вообще нашу текущую инфраструктуру до того как внедрить туда этот докер как начать все таки с ним работать и использовать его в буду расскажу про ключевые узлы данной системы той что я необходимы и достаточны для того чтобы начать не работать что у вас должно быть дальше мы коснемся темы моего любимого папито и я расскажу о том как получить какой-то рабочий прототип на отдельно взятой машине как это дело раскатать на множество серверов потому что у нас их на данный момент порядка трех тысяч единиц чуть меньше дальше мы коснемся вопроса системы сборки образов для запуска контейнеров которые мы собрали с помощью папито и существующих наших ресурсов расскажу вам про диплом сервиса с точки зрения отдела эксплуатации потому что тепло и сервис разработчикам и тепло и сервиса отделом эксплуатации это такие немножко разные сущности и поэтому я считаю что не стоит обратить внимание я обязательно коснусь вопроса того что не бывает никакого внедрения по крайней мере на моей практике не было еще такого ни разу что ты выдумаешь про продукт в к таком идеальном вакууме ты его внедряешь точно так же у тебя нет ни одних там кораблей проблем с которыми приходится справляться грабли с докером были я думаю что они еще продолжу быть мы будем их исправлять я коснусь нескольких моментов которые мы нашли поправили и пока живем с этим счастлива я коснусь момента того чего хотелось бы видеть в неком рудные пи данного продукта и некоторые мои соображения про который я расскажу после общения с жеромом который приезжал москвы с компанией docker некоторые видения совпадают и то есть некоторые штуки про которые я скажу о том что хотелось бы их видеть они действительно могут сделано в каких-то следующих релизах может быть в течение одного года может быть чуть больше и в конечном итоге я постараюсь подвести заключение в котором расскажу чего мы в году хотели от докера и чего мы имеем на данный момент итак начнем начнем с того что же такое все-таки докер все вы я думаю это уже знаете это не система виртуализации это в принципе это даже не какой-то ноу-хау все было придумано до этого как в свое время сказал джерома сказал о том что если бы не придут не не придумали и не выпустили бы этот первый там бета или альфой релиз докер а то кто-нибудь пришел бы на месяц на 2 на 3 чуть позже придумал бы что-то вот такое же и назвал бы это иначе потому что сама суть и главные идеи докер на данный момент это в нужное время в нужном месте то есть тем кто знаком с джейлом free bsd то те кто когда-то работал с linux контейнерами с системой в это должно быть достаточно знакомо al-exe хорошая штука но мне очень не нравится интерфейс то есть в дочери и то есть это добавили это достаточно хорошо вторая проблема перед тем как вот начать внедрять я смотрел порядка двух или трех месяцев на развитие продукта я пытался смотреть на чем она переключается на я пытался смотреть на нашу инфраструктуру смотреть на самом docker и пытаться вот понять какой же стороны подойти вот в какую часть инфраструктуры попробуете применить для начала это был достаточно такой трудный выбор и вот самое главное что было сделано это вот все-таки нашли какую-то первую точку составили не некую карту наших хотелок и пожеланий из всего того что в принципе можно достичь с помощью докеры о чем написано на их сайте на всех без practices мы решили выработать свои там несколько пунктов которые как только мы их достигнем мы пойдем дальше итак начнем с того что мы хотели получить изначально докеры мы хотели получить гарантированный некий гарантированный state любого из наших внутри написанных сервисов то есть это значит следующее что если мы когда-то сервис собрали в образ запустили ее в какой то момент времени мы хотим иметь гарантию того что если нам придется откатываться по каким-то там причинам недостаток кода на стороне фронтэнда или какие-то там еще прочие вещи там не работает новая версия не до конца тестировано мы хотели хотели и хотим быть уверенным в том что если сервис мы запускали хотя бы раз и он работал нас устраивал к этой версии мы можем откатиться всегда это один из основных бенефитов перед началом внедрение докера дальше мы хотели получить и упростить немножко систему выкладки сервисов такого деплоя сервисов на оконечное оборудование на котором он работает с точки зрения отдела эксплуатации то есть у нас на текущий момент существует такая текущая схема диплом с нашей стороны и сбоку к ней не нарушая того что было мы пытаемся добавить и точнее отчасти уже добавили какой-то процент сервисов видим новую диплом мы подумали что вот этого достичь с помощью докер тоже сможем также не маловажный был момент того что мы хотим иметь для например такого классического понимания сервиса то есть это сервис без там никого пирсе стас тораджа своего какой-то самописная татуировать для которого достаточно там оперативной памяти какого-то набора данных где-то там никого загрузчика в которой с помощью этого загрузчиком этом из условные базы можем закинуть нужные нам данных дальше работать там в памяти или по какой-то логики данного сервиса и мы хотели получить резервирование которых заключается в том что мы скажем так должны быть уверены в том что если у нас выходит физически из строя там сервер или кластер серверов мы гарантированно там там железе который стоит у нас на складе просто в очереди мы можем взять развернуться и быстро запустится с минимум каких-то потерь и такая тоже достаточно интересная штука которая нам показалось что мы хотим и не попробовать и посмотреть как это будет сделано с помощью докер они с кучей разных машин которые другой реплицируют дублируют и так далее все это собрать в рамках там например одной машинки мы смотрели на задачу такой некой балансировки внутри одного сервиса и пытались и пытаемся достичь того чтобы в случае каких-то плановых обновлений или изменение наших сервисов downtime у нас был минимальным то есть мы его хотим сократить такой не тот самый последний момент который мы тоже хотели получить в силу того что у нас конторе присут присутствует нас ли такое наследие как совсем старого оборудования но она относительно старый присутствует оборудование там средних лет может быть годичной давности присутствуют какие-то тупые железки все это может быть в рамках одного кластера и на какой-то момент времени за счет того что некоторым из наших сервисов нужна была такая полная изоляция другого сервиса там по портам еще почему-то мы не всегда могли взять и запустить на одной машине три разных там по типу например сервисов которая служит такие совершенно разные данные и очень часто мы имели такую картину что например у нас работает какой-то один сервис на относительно там старом саней какой-нибудь 4170 ему кроме оперативной памяти скорость вообще это даже от памяти не сильно важно но от кроме потребление там порядка 12 гигабайт может быть в пиках своей работы оперативной памяти сервера больше ничего не нужно то есть у нас мы занимаем тем самым какое-то электричество место в стойке сетевые порты все остальное при этом наша железы простаивать процентов наверное на 70 такие случаи были и мы хотели сделать некое уплотнения то взять там либо топовый синюю неважно какой сервер и подсадить на него ни один сервис а подсадить на него там 510 или там сколько влезет зависимости от анализа и ожиданий эту цель мы достаточно удачно достигли итак начнем всяко более копнем чуть глубже и так исторически сложилось так что в компании баду основной единственный дистрибутив на данный момент времени и пока мы упомянем не планирую менять остается сюзи linux enterprise соответственно плюсы минусы данного дистрибутива я не думаю что есть очень много людей кто использует его может быть там у себя на своих машинах дистрибутив достаточно консервативный на текущий день порядка по моему трех или четырех дней назад с компания зарелизили 12 след в котором наконец то есть там свежие кадры все остальное там наконец-то появился систем да и прочее с чем столкнулись мы на момент нашей текущей актуальной стабильной версии которые мы используем на всех наших машинах первое когда вы открываете сайт там докеры хотите его поставить на какое-то свое железо вы понимаете что одно из первых требований это версии ядра версии там 3 8 и выше соответственно в дефолтной поставки и слез 11 смотри мы имеем вот последнее security патч версии ядра у них 30 101 то что было чуть раньше не сильно различается то есть здесь мы принимаем решение о том что гидра нам нужно в любом случае выкатывать новые так как мы в стандартном дистрибутивно мне дали используем определенные свои почти которые нужны для работы каких специфических сервисов соответственно чтобы выкатить новые ядро нужны эти почти переписать линейка и изменение достаточно большая его нужно несколько раз собрать как-то тестировать посмотреть плюсы минусы и после того как мы принимаем решения то что если вы хотели б ушла без причины ушло потому что приличное количество времени на данный момент мы остановились на версии регистрации зимой а он даже соответственно в консервативном смеси в зависимости от там разных требований самого докера нам пришлось обновлять кучу там внутрянки всякий linux учился и 5 билл сайфер out и так далее следующим интересным моментом был обзор и тестирования тех стороны бэкенда где вот как бы крутится ваш контейнер лежат какие-то данные него неважно постоянное хранилище временное хранилище для логов или счет вот этот вопрос выбора между 100 раджим соответственно мы рассматривали два варианта это стандартный девайс maker плюс x 4 и рассматривали очень горячо любимый сообществом сюзи linux enterprise и вообще upon сьюзи в том числе файловых систем и btrfs игру вся эта штука там принципе никому не новое то там живет я не знаю сколько наверное живет linux практически только это все дело живет на момент времени до докера скажем так мы его настолько тесно не использовали соответствует было еще эта вещь которую пришлось немножко и последний чем у нас изменения таблицы раздел эти договоренности о том как должна выглядеть как может выглядеть в изменении там больше поднимайся на те боли и страх здесь у нас появился еще один такой тимплей создание машина под игр пусть это прямо придет мысль мнению вот это все было подготовлено про про такой большой тесто было братвы но соответственно должно было рассказать поделился нескольку нам нужно было рассказать хозяйства на так как мы получили конфигурации логично мы цепляем соответственно чтобы сделать для автоматика того чтобы хотим использовать вместо одной машины хотим использовать 10-15 50 машину которую будет тема вот у нас закончилось тем что касательно добавки вот и были добавлены создания отдельного как в репозитории вас пакет создать в автоматическом режиме из за исключением того если мы например машин отправляем неевреем стало ли мы не ставим новую машину машина была под каким-то другим проектам и нам нужно на ней изменить там таблицу разделов то есть здесь возникает необходимость участия инженера как бы тут все зависимости от то есть иногда проще и там скину все сервисы стоит машина отправите were set up и смотреть на нее как на новую машину из чего состоит докер что вообще ему нужна для того чтобы работать самой основная единица это docker host это может быть там вашей локальной машиной в нашем случае это некий сервер на котором мы собираемся крутить эти образы на текущее состояние для нас docker host это обычный linux сервер на котором установим сьюзи linux enterprise 11 смотри используем на данный момент времени мы докер 120130 который вышел не так давно мы посмотрим изучаем и так далее и выбор такого стороны бэкенда у нас полна btrfs есть еще такая штука то есть сам docker host может хранить ваши образы удалять собирать делать с ними все что как бы заблагорассудится но так как нужна какая-то централизация то есть мы же не можем там с 1 до краха стать делает там tor бал какой-то кидать его на другой хочет запустить другой контейнер потому что это отменяет всю идею того что все это быстро централизовано круто и так далее нужен докер ли jetstream докер лет жесты это такой набор скажем так архивчик of и метаданных который хранит информацию там оля очень похожи на структуру комментов детей а он хранит в неком централизовано месте и также он хранит вот определенные лаэртом веер или набор лееров которые нужны для поднятия вашего контейнера каком-то своем месте что по умолчанию предлагает докер не предлагаю docker hub docker hub штука клевая вопросов нет они даже предлагают приватной репозитории за деньги где ваши какие-то там ценные данные вашей компании никто не увидит не узнает но все это находится где-то там в облаках где-то далеко и так далее а нам нужно быстро и сейчас и нужно этот желательно локально то есть если там у нас две площадки 20 дата-центр по в каждом дата-центре но нужно иметь свой registry для того чтобы быстро тягать эти образы между центральным местом и между конечно сервером на данный момент у нас докер digest а это по большому счету точно такой-же обычный docker host на котором запущен контейнер с докер ridgid официальной дальше мы используем для авторизации потому что из коробки авторизации не то эта штука важны мы используем обычную вы ящиков авторизацию в яндексе и там просто для chic верности и любви работы докера между registry и конечной точкой к с целью не вопрос мы используем видимо я перепутал слайды это соответственно слайд к придут про предыдущий момент о чем я рассказывал зачем нам нужен паппет на данный момент времени как мы раскатываем от одного там какого-то эталонного hasta мы собираем данные которые мы меняли и уже делаем это централизовано в на любом количестве на дальше дальше такая довольно интересная история мы рассказывали я рассказывал на конференции код fest в этом году есть видеозапись кому будет интересно рассказывал о том какую швы придумали классную в отделе эксплуатации как уж мы придумали классную карту сервисов в которой мы хотим видеть и понимать на какой группе хостов или на каком посте работает какой сервис какой тип у этого сервиса какая версия там сборки у этого сервиса то есть этот как бы такое эталонное состоянии там площадке вот с точки зрения наших сервисов как должно быть когда мы начали внедрять boker мы увидели там всякие клёвые штуки типа экспо за в докер файле и тогда летом подобные поняли что в нашей базе знаний не хватает одного немаловажного момента мы не храним информацию по тем портам который обслуживает данный конкретный сервис соответственно эту информацию мы достаточно легко внесли и получили уже есть такой карты там году сервис есть мы добавили партс мы получили некий но лишь бы из страны эксплуатации этот но лишь бы соответственно представляете себя там пару табличек в базе данных из которых мы можем генерить там зависимости наших пожеланий что угодно мы можем формировать из этого в джейсоне какой-то там список для мониторинга наших сервисов на можем там выбирать порты занятые свободные определять что мы можем отдать какие порты можем отдать под новые сервисы и прочее с точки зрения такой автоматизации сборки и выкатки этих образов и в том числе тестирования работает от таким образом наша старая текущая система тепло и использует очень сильный плотно pumped мы не можем взять сломать и построить что-то новое поэтому в задачу в нашем входило просто взять и как-то с минимумом из изменений построить систему такую дублирующую рядышком соответственно вся конфигурации описания нашу инфраструктуру содержится изначально в этой базе знаний потом из нее формируется некий ямал с помощью обычных скрипников и мы решили что все данные у нас уже есть как мы как как и не рулить на удаленных системах мы знаем почему бы нам не создать систему сборки на каком-то выделенном сервере соответственно система сборки себя представляет при первом приближении обычную директорию в этой директорию существует под директории с именем сервиса и в директории с именем сервиса уже присутствует динамические формируемый докер файл существует количество файликов которые необходимы для сборки данного контейнера и существует избыточная достаточно информация с теми файлами которые необходимо потом при создании образа на следующем этапе это работает следующим образом нам нужно не обходить сервер необходимо на и сервис нарастить версию что-то там поменять изменить конфиге мы это меняем либо в базе знаниях либо мы правим манифесты помята после чего в автоматическом режиме попить приходит на машину для сборки проходит по каждому сервису если он видит какие-то изменения в этом сервисе запускается автоматическая процедура peterbilt который соответственно и формирует уже имидж для запуска контейнера здесь есть как бы два состояния первое состояние контейнер у нас собрался соответственно все хорошо и второе состояние контейнер мне собрался на каком-то там шаги отвалился что-то не там файлика не хватает или инструкцию dockerfile указан неправильно и в зависимости от мы поступаем уже по разному если на все пошло хорошо соответственно мы представляем так нашей сборки представляем там дату сборки версию которую мы собрали делаем параллельно пушкам наш registry формируем какую-то задачу на отдел тестирование том что вот собрался новый пакет его где-то там нет запустить его как-то чтобы погонять и если там все хорошо вы там ставите галочку о том что все хорошо с этой сборке и отдел эксплуатации понимает что это то сборка которую там девелопер собрал тестирование тестировала по первым тестом ничего плохого не выявлено мы можем это дело запускать продакшене если что-то не собралось соответственно приходит отбивка отдела эксплуатации о том что вот что-то там пошло не так и здесь уже в ручном режиме разбирается админ инженер так как докер в случае ошибки сохраняет там тот последний свой сайт на котором он там не сможешь что-то сделать админ смотрит руками начинает заходит вот стоит проект какие-то инструкции делают какие-то действия и процедура вся проходит по новой в автоматическом режиме после уверенности в том что все уже собралось все хорошо как бы чем нам здесь помогает пока в данном случае он поддерживает полностью в нужном нам состояние вот это вот окружение для сборки он выполняет их затем на основании своих либо изменений либо не изменений соответственно оповещает как-то в случае если это необходимо и сборка прошла удачно мы делаем push в наш репозиторий для тестирования мы можем автоматическом режиме на выделенных машинах делать пул из этого репозитория мы можем синхронизировать всегда состоянии нужные нам на конкретных машинах состоянии репозитории мы можем ненужные нам старые образы и контейнеры просто с машины выкидывать то есть вот их просто раз и нет там на файлы системе чисто нет какого-то такого загрязнения виде того что разработчикам закинул новый бин аль старый не удалил и потом вот все это движется на протяжении полугода то вдоль в итоге структура директорий вырастает во что-то неимоверное и еще у нас работает как такой помогал так скажем так он делает не совсем rc скрипты для запуска нужных контейнеров он просто выдает инженером в случае необходимости что-то запустить он выдает ту команду который нужно запустить на основе соответственно тех данных которым используют при сборке это иногда полезная штука потому что там ключей и приза для запуска контейнера может быть масса и не не всегда их удобно запоминать для тем более того или иного сервиса что вот как выглядит downtime и вообще перезапуск сервиса штука перезапуск сервиса там условно классического в отделе эксплуатации для того чтобы нам выкатить новый бинарную нужно пойти на машину где работает этот сервис взять новый бин аль удостоверится в том что все конфиги нему подходит но хотя бы запускается там не сиквел птица соответственно что нам нужно сделать вот как бы здесь при пример такой удачной работы сервисы и там клиента который что-то спрашивает соответственно что у нас происходит если мы придем делаем обновлением первое что мы должны сделать это остановить демон соответственно момент остановки демон все запросы которые там прилетают на этот сервис выдают все что угодно там тайм-аут can connect то есть здесь все уже дальше зависит от изобретательности не знаю верстальщиков или там дизайнеров которые красиво от рисует а почему не работает чем нам здесь может помочь докер в данном случае все на самом деле просто и прозрачно ничего нового здесь нет то есть что такое балансировщик всем известно всем понятно понятно что это можно размазывать не в рамках одной физической машины можно просто сделать балансировщик сделать несколько об стримов сделать какую-то умный логику который будет вычислять жив ли up stream посылаете туда пакет или нет но случай с джокером и из только системы которая уже на данный момент построена вокруг него есть определенные продукты которые вот ты просто взял поставил они ну то есть они работают все это сделать там сильно проще что мы попробовали что мы сделали в данном случае мы взяли подняли сервис эти сидим это очень такое легковесные простенькое киева или хранилища она очень здорово реплицируются там выстраиваются различные цепочки с помощью него можно сделать что такое что там можно надо совершенно спокойно убрать потушить все данные переедут на оставшиеся участники этих нот и тогда ли он достаточно легко песен настраивается достаточно тоже просто то есть вот оно как бы было поставили заработай дополнительно то есть зачем он нам нужен он нам нужен для того чтобы например у нас работает старый демон мы хотим запустить новую версию нашего софта порт который обслуживает данный сервис слушает у нас в данном случае не сам сервис а слушает его nickelodeon сыр у которого есть информация в его обсудим а когда мы запускаем новую версию новый контейнер мы соответственно сообщаем по определенному пути в эти сети о том что я вот новый контейнер с таким-то сервисом я запустился у меня все хорошо я работаю на таком-то адресе слушаю такой-то приватный порт на такой прям внутри приватной сети которую мы там формируют помощью бриджа данные эти приезжают есть такая еще класная штука как он где он соответственно тоже цепляется к этому эти сиддик этой базе кирью слушает смотрит на определенные ключи и смотрит изменений если изменение пришло там зависимости от поднялся и контейнер или наоборот опустился старый он вносит изменения по заранее заданным шаблоном х прокси и делает соответственно рилот из плюсов здесь получается что здесь получается что мы все это делаем локально мы не трогаем никаким образом внешнюю сеть внешней сети я считаю то что выходит за рамки вот там это х интерфейса конкретного сервера на котором мы это делаем мы не трогать никакие dns и dhcp и так далее то есть мы принимаем как бы причиняем в любом случае какой-то минимум изменений к внешней системе то есть мы работаем на одном деле взятом хостить на самом деле тема еще такая гораздо больше я попытался вот вкратце объяснить как можно к этому подойти с применением докер как бы плавно подошли к тому как бы вот все это красиво так звучала что-то там сделали тепло и поменяли там сервера настроили ядра каких протестировали много всяких разных слов но как бы не получилось сделать так чтобы не устроить нам какой-то может быть небольшой downtime что-то пошло не так первое на что мы наступили это соответственно на нагруженном с точки зрения сети и количество запросов там запросов ответов сервиса при попытке запустить докер просто в базовой конфигурации то есть мы там сделали вот этот стандартный bridge мы сделали над запустили в приватной сетки наш сервис соответственно про кинули один к одному на внешний интерфейс порт первое что мы получили это мы получили проблемой на в контракт я думаю все там вели в донецке я думаю что неоднократно там прибыл из full варианты исправлений это когда мы берём и поднимаем значение таблицы контроле к мы его поднимаем все равно там остаются промажет поднять его до такого состояния чтобы этой таблицей хватило но как бы ни о чем это не очень нужно соответственно как мы вот данную проблему на части сервисов мы и решили решение достаточно простой мы на данный момент можем запустить докер с мэппингом на физический существующей интерфейс и для нужного нам сервиса или портам и v2 то скажем так в два правила таблица raw можем дописать что вот мы не хотим например для соединений которые идут вот с определенного порта куда-то и для тех соединений который приходит на этот порт мы не хотим какого то конечно треккинга вот просто говорим на у трек в данном случае нас этого избавил от переполнения таблицы и там от выпадания просто сервера и состояние своей работы следующий момент который мы тоже поймали девайс метро x4 все просто прозрачно мы знаем как этим пользоваться мы знаем что кайтом л.м. это на первый взгляд здорово работает здесь мы поймали какую-то просто не то что кучу ну какие довольно неприятные моменты когда у нас слои отваливались при попытке запустить новый контейнер или работы старого контейнера у нас разваливалась именно сама fs вот в этих вот снам шортиках нужно было делать ей двоих . соответственно потом бились какие-то метод данные очень происходило что-то не очень понятно где то не где-то она еще с гитхабе докера как выяснилось что мы с такой проблемой не одни одни я бы тут выбор пал такой что давайте попробуем тогда btrfs почему нет интересная штука почему нет соответственно btrfs версия далеко не последнее которое сейчас есть на сайте разработчика файла система интересная но я думаю что в том числе для меня в ней на данный момент достаточно большое количество каких-то загадок of поведения во всем остальном как надо смотреть но с момента переключения на сторону виде btrfs проблемы такого характера который было не было больше ни разу и самое последнее такая интересная штука на которой мы должны вступили это волею мэппинг как выяснилось что при попытке зама писем link внутрь контейнера мы заметим просто фактически местоположение той директории ли того файла который мы этом случае попытки завопить какое то какой какую-то директорию и в эту директорию если подмонтировать какие-то устройства в нашем случае это там некий лук девайс и мы это можем сделать на момент старта ну дальше мы получаем интересную проблему докер-контейнер при старте читает табличка прок mount и соответственно при запуске весят про клаус переносится в контейнер дальше происходит следующее что мы хотим от монтировать лоб девайс на ход системе и сделать этого не можем потому что у нас держит процесс мы хотим ремонтировать новый лук девайсы чтобы он появился в запущенном контейнере без привилегированного запуска это сделать тоже невозможно то есть но в данном случае получается что не использовать те директории которые вы собираетесь динамически именно включать и выключать подойдем к тому что хотелось бы вот чего хотелось бы видеть в дочери на данный момент и этого нет без каких-то дополнительных телодвижений 1 этот централизованный мониторинг то есть хотелось бы видеть состояние тех холстов на которых рыться контейнер хотелось быть из коробки мы выбрали на данный момент к advisor как слушать собирать статистику отправлять куда-то дальше мы используем там графа noindex дбм это все храним то есть мы это получили но это как бы определенный труд определенные там заморочки так далее из коробки чего-то такого рабочего мы не нашли очень хотелось бы видеть управление инфраструктурой как вот кластером то есть хотелось бы зайти в одну точку и понять где у нас там сколько ресурсов свободно сколько контейнеров запущена где мы можно запустить что-то еще опять же такого прям что поставил работает нету несколько продуктов посмотрели не все соответствовали ожиданиям на на данный момент так по большей части пользуемся шик ярдом но больше пока присматриваемся то есть пока управление на самом деле из серии набор скриптов через api и то есть shipyard пока не сказать что прям сильно интегрирован очень интересный проект написанные тоже на голову для организации таких внутренних приватных каких-то сетей это вот есть вив которого он очень клёвый ну вот его также нет из коробки какой-то такой серьезной интеграции у докера пока с ним нету его попробовать стоит в том случае если вы пытаетесь что-то настроить и очень сильно не хватает возможности управления о пинга свечам именно с помощью каких-то там тонких настроек докер у то есть понятно что мы можем сказать только вы не использую никакой бритья bridge за сделаю сам тебе и сам тебе его сделаю выдам и дальше ковыряться уже вручную с окон в с мячом то есть на данный момент она работает просто со стандартными там breed шутил с linux вами и пока больше ничего не даёт хотелось бы это видеть и жиром сказал что у них на данный момент 5 человек в отделе разработки занимаются процессом интеграции доке пропали свеча это я считаю такие достаточно хороший приятный новости в заключение чего вот к чему от чего мы шли чему пришли в году с докером мы смогли уплотнить за счет и валяться контейнеров и сервисов между собой мы смогли построить схему при которой мы можем запускать более одном из сервиса на одном сервере причем разных типов как таким дополнительным плюсом мы толчком к токам pink он даже сказал наверное создание карты сервисов докер на что же привело получили клевую штуку очень нужно и уже теперь ну возможно мы бы получили конечно и без докер у но просто здесь вот так в параллель сделали классно мы построили систему сборки и тестирования для новых сервисов не сломав старую то есть мы построили рядом при этом не перестраиваю особо структуру ли сильно не трогай структуру того что было на данный момент есть гарантии того что если мы сервис запустили когда-то положили в registry мы его запустим уже в любой момент времени на там любой другой железки ну и как бы как иногда наверное важно участвовать каких-то новых там тенденциях смотреть что-то изучать в данном случае от докер итак судя по всему мы не сильно отстали особенно после общение ных этапах и наяки вот собственно все спасибо вам за внимание спасибо что пришли если есть вопрос добрый день базе мощный за доклад меня зовут алексей и у нас добро я честно говоря не понял большую часть из того что вы говорили вот слишком специфичная очень слабы с этим знаком вот но у нас компания сейчас только развивается и идет движение в сторону со вот и проблемы которая у нас сейчас возникло даже не столько на серверах сколько в отделе разработки что у нас много разработчиков и каждому нужно держать рабочую версию с большим количеством спину там небольшим пока там три сервиса да вот и они должны как-то обновляться вот была идея решить эту проблему через докер или через пакет собственно и ну если у вас такая проблема доводили разработки и как вот подает ли докер для решения и и ну вот интересные проблемы которые обычно встречаются случаи тестирования запускаются какой-то тест тест оспорить какой-то набор данных базе данных и каждый следующий тест который запускается он должен использовать исходный набор тестов в базе данных и самое невыгодное что получается чтобы нам прогнать тесто там последовательно или как то еще нам нужно базу запустить данными наполнить дан тест прошел данные удалить данные налить и запустить еще раз в случае с джокером из вот этой возможностью copyright во первых вы можете получить на основе одного имиджа там хоть сто контейнеров там за раз с одним набором данных и гонять изолированы эти тесты то есть в этом случае вам это может подойти и в том числе там запускать один контейнер но при этом опять директорию с вашими новыми by ногами у вас каждый раз будет там свою ногу то окружение которое нужно для тестирования папин в данном случае не поможет ни как паппет это на самом деле такая подготовка железа подготовка платформы для запуска чего-либо т.е. в данном случае по питон нет то есть сама использовали контейнеров для того чтобы вам поднимать такие эталонные тачки в большом количестве и быстро без использования особо ресурсов вам это может подойти просто тут идет именно рабочие места разработчика да вот как бы интересует такая вещь чтобы не прыгнул чтобы каждый человек имел актуальную версию проекта до который состоит из нескольких то модулей там до сервисов есть отличный проект который такой опыт проект где ребята написали такой проброс сессии как раз таки это сделано для разработчиков они поднимают некий талоны контейнер мопед honda на каком-то сервере разработчика то есть есть некие того что то там поднимается и когда разработчик случится там по 2 сторон порту заходит на машину падает не на физический хост он попадает уже на запущенный контейнер к делать содержится актуальной версии его там binary конфигов всего остального то есть возможно я сейчас название честно не вспомню если вам интересно вы можете не вот либо написать по контактам либо я там в коридоре меня можно будет эти я просто вспомню название этого проекта и скажу то есть это интересная штука который как раз таки сможет вам реализовать вот эти вот такие отдельные окружения для домашних директории пользователя для от сборки для работы разработчика спасибо здрасте anton спасибо за доклад хотела у вас спросить и здесь до далеко-далеко до а хотел вас спросить вот вы упомянули о том что проблемы с монтированием болью ну и как вы живете то с этим вы то есть их не монтируете получила проблема с монтированием уволена заключается только в том случае если вот в той директории которой вы пытаетесь монтировать внутренние это в какой-то инфраструктуре подмонтировать на некие внешние девайсы которые динамически обновляются в данном случае для таких хвостов мы избавились от такого поведения то есть мы ушли от лук девайса к обычной директории наполнены уже видимся в том то есть там всякие мув линки и так далее то есть ну то есть по факту мы взяли вообще ушли от лук девайсов но давно понял без баб девайса спасибо спасибо за доклад пара вопросов до 1 этапа питерский модуль до по петровский модуль который для сборки докер-контейнер of залпа царстве нет понятно какой то сделать я могу поделиться этими знаниями но у меня пока я такая проблема за open source ведь она заключается не из-за того что жалко да из-за того что мне кажется что там я что-то пишу не оптимально это понятно зачастую только мне там интересно единицу поэтому я очень редко это делаю если вам это действительно телесная вам могу там прислать координату на почту или там выложить к себе ноги тропы вы посмотрите ну ради интереса было бы здорово спасибо а еще вопрос нет ли у вас проблем с регистрацией контейнерами в условиях много- сегментный сети вас вообще сеть в основном плоское или присутствует рамках одного дата-центра множество различных сегментов с точки зрения критичности безопасности данных которые там протекают и очки зрения безопасности наверное на более горизонтально робот 1d c то есть есть ограничение на данный момент есть какие-то подсети в которых есть оси от цели на физических железках но преимущественно вот разграничение трафика между машинами лекала стиральную машину этапе ты этот последний вопрос продолжение собственно с точки зрения оркестрация slide не использовали прав ли ты то за последние три дня 2 на 0 3 вопрос не пробовал просто не смотрел то есть как бы уточни как бы я почитал доки но пока не запускал то есть я в курсе что это но попробовать жил у него ладно спасибо пожалуйста так вопросы из приложения почему именно докер они open базе есть какие-то специфические причины вопрос этот был нами то пию два дня назад все-таки о том ваза эту карту ре зация докер это не виртуализации в докере идея докера первоначально один контейнер один процесс на самом деле я с этим утверждением не совсем согласен для все-таки больше согласен с тем что один контейнер это одна задача и процессов может быть более одного но при этом у вас в контейнере нет никакого и не то нет вообще ничего у вас есть 1 ядро системные у вас есть там lip контейнер которые как раз таки как то вот так делает не гибрид между основной системой и контейнером openweather все-таки это больше системы виртуализации и то есть играх это там побольше будет чем именно в контейнере еще один вопрос из приложения как вы отслеживаете зависимости например программист стал использовать свежую либо в сусе еще старая какая роль разработчика в состоянии создание контейнера ну как бы скомпилировать binary наверно все таки статически можно считали бы тогда то есть если встает вопрос зависимости присутствие либо там новостей или еще что то мне кажется более правильным решением будет собрать такой binary который не будет требовать этих зависимости на конечной системе то есть собрать именно образ который будет работать если ты собираешь образ используешь какую-то новую библиотеку тебе никто не мешает в этот образ запаковать эту библиотеку и не использовать системных библиотек на хвосте надо рассматривать контейнер и образ как такую независимую единицу которую ты можешь запустить где угодно если нужны какие-то либо положили в себе внутрь антон спасибо это были вопросы зала поэтому мой следующий а ну а верх молодцы что докер и вообще новой технологии осваиваете вот было спич про shipyard такой меньше но еще до этого что-то говорили у меня в голове побои мать вопросом было месяц вы пробовали морель на минус марафон чтобы ищите ни разу не смотрел я смотрел внимательно от гугла кибернет которые они предлагают мне в принципе все понравилось за исключением того что какая основная цель чего хотелось бы а тут и турки регистрации хотелось бы самому указывать на каких машинах что запускать подобных вот видите вернется то есть они решают эту задачу как нам нужен некий вычислить на узел у нас есть вот набор кластеров мы хотим что-то посчитать где-то будет запускаться как бы это дело твою то есть мы отдали тебе ног количество машин считай здесь хочется сейчас пока на этом переходном этапе хочется просто видеть общую картину но при этом указание системе где что запускать выдавать в руку вручную я его просто не смотрел то есть спасибо за совет я обязательно посмотрю то есть громче громче говорите продолжить хороши коллеги подождите у нас если индекс нас подошёл индекс докладчика проявись отлично хорошо нас есть еще один вопрос пока он на готовимся защита клатч как-то вопросов было море тогда из приложения например production сервисы запускается по пятам а если сверх моргнет или в торе стойка кто поднимет сервис перезапуск сервиса осуществляется в автоматическом режиме но при наблюдении в консоль инженерным у нас есть специальный окна так называемого в пик трафика когда мы можем заниматься перезапуском сервиса если это не какой-то там fatal или что нибудь такое у нас есть автоматика который готовит весь набор команд для инженера но перезапуск сервиса осуществляется только с помощью инженер то есть под его контролем пока он не скажу что я готов я смотрю никакого перезапуска не будет вот здесь поднимали рук да конечно я рассказывал на прошлом я кипра систему риск от которые мы используем для разворачивания серверов но это по факту как бы по если там diash степи все это там советский перловый наборов таят профилей у нас получается как у нас получается первоначальный сетапу совсем с уровня железо это там x карт который как раз таки по excel куча всего вокруг как только машина заступилась автоматически вам либо подписывается либо нет в попить и определяется ее роль и дальше папе ту же настраивать такую по 15 лет операционной системой экспорт настраивает у нас железку и готовить операционную систему вопит пробегается и делает уже такую готовую для запуска приложения системы то есть работает это вот такой связке - по определенному сценарию до автомате то есть у факты есть возможность использовать внешний и там некий скрипт который принимает решение подписать не подписать да мы подписываем прибегая к помощи этого скрипта то есть мы не подписываю все ключи но мы подписываем то что мы хотим подписывать вот так это автоматическом режиме"
}