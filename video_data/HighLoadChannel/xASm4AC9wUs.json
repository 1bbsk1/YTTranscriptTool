{
  "video_id": "xASm4AC9wUs",
  "channel": "HighLoadChannel",
  "title": "Сегментируем 600 млн. пользователей / Артем Маринов (Data-Centric Alliance)",
  "views": 1450,
  "duration": 2565,
  "published": "2017-04-22T14:48:23-07:00",
  "text": "ещё раз всем привет меня зовут артем маринов являясь лидером разработки деньги faced в компании дата-центре кальян мне бы хотелось рассказать вам о том какую пусть мы прошли для того чтобы сегментировать всю нашу базу пользователей в режиме реального времени как мы используем для этого и чубайса кафку поехали мы делаем дмп что так вообще дмп дмп the data management and from платформа по сбору агрегации и обработки различных пользовательских данных наши дмп насчитывает порядка 600 миллионов пользователей мы видим их по всему интернету собираем различные ивенты о них это могут быть визиты на какие-либо странице какие то поисковые запросы которые сбивают пользователя пайпе user-agent и и 3 данные в офлайн мере нам надо все это ивенте видеть агрегировать сохранять и размечать пользователей в группы аудиторных сегментов в целом над они выглядят примерно вот так вот в день мы видим около пяти с половиной миллиардов ивентов что в пике достигает сотни тысяч в секунду на мать и ванда нужно регистрировать и размечать пользуется ли его некоторые группы аудиторных сегментов примером сегмента могут быть женщины любят котиков вегетарианцы и шут автосервис и так далее зачем вообще нужна сегментировать людей применение ту множество например может быть какой-то рекламодатель который хочет показал свою рекламу не всем подряд а только тем кто мне действительно заинтересован а вот как раз пример автосервис мощный актуальный рекламодатель может хотеть показывать рекламу только людям которые metapure подержанный автомобиль либо это может быть какое-то динамическое изменение контента сайта на основе сегментов либо какой-то внутренний вскоре нг людей данные мы видим из множества разных мест это могут быть прямые установки нашего пикселя которые мы даем клиентам для того чтобы собирать анализировать их не аудиторию это может быть какой то от онлайн поток данных от наших партнеров которые готовы нас поставить и делиться своими данными как-то их монетизировать и так далее либо это могут быть оффлайн выгрузки от наших партнеров они выгружаю там лог-файла мы их кладем к себе и также обогащаем нашу базу знаний о пользователях нужно понимать что одним что когда вы делаете какой-то интернет-магазин либо приложение вы можете прикинуть примерное количество пользовали которых будет у вас и прикинуть как вы вообще будете расти масштабироваться увеличиваться в размерах и так далее в случае же с дмп нас в любой момент может подставить какой-то очень крупный партнер например ночью и нам нужно не прилечь при этом не потерять какие-то свои характеристики качества систем и так далее поэтому одним из ключевых мест которые должны быть у себя мета горизонтальной масштабируемость также нам самим очень важно понимать и оценивать объемы различных сегментов но то есть приходит какой-то человек и он скажет ребята вот сколько у вас женщин из екатеринбурга которые являются молодыми мамами и хотят там купить квартиру например вот нам надо уметь быстро давать ответы на такие вопросы чтобы потенциальные рекламодатели мог как-то строить кайт медиаплана на основе этих данных со стороны разработки нам хочется чтобы все это можно было удобно мониторить удобный де боже как разрабатывать и так далее смотреть влоги метрики мониторинг это все ну и также одно из основных характеристик системы это так как он быстро реагируем на тот event который мы увидели пользователям например если вы ищете билет в театр и мы начнем вам показать kites перед предложение точнее ним и она же клиента который использует наши данные только спустя несколько суток вам это будет уже не интересно потому что может быть вы уже сходили в театр посмотрели это спектакль и так далее хотелось бы рассказать что у нас было от одной точкой когда мы начали переходить на real-time все логик все ивента которые мы видели на заворачивались в лог файлы и складывались на pdf с также данные от партнеров миди файлов а также складывались на вич dfs далее был некий класс написали на джай виновники сервис который знает как преобразовать строчку logo в event на обогащение данных из бойцов путь запросу чубайс кто вообще как ты работал судьба и сможете вот руку поднять обозначить себя представляет что это такое и отлично давайте немного рассмотрим например как мы храним данному любой за и если кто то не знает что сказать быть на примере в них нет либо из это не реляционная кукла багаряков ориентированная база данных там есть таблицы и в 3 таблицы есть такая сущность какой руки это тот ключ по которому вы хранить какие-то ивенты нашем случае ключом является некоторые сгенерированные нами пользователю юзер айди какая-то случайная буква буква на символьная последовательность в каждом руке и может быть несколько колумб земле на уровне команд family мы можете управлять этими политиками хранения тем как долгого данный хранить в рамках это колумб своим или задавать различные metadata внутри колонн family есть собственно колонны нашем случае для колонов мы используем как раз те характеристики которые мы видим по пользователю это могут быть как раз отдельные комнаты миль с визитами сайте адресами с user-agent им с поисковыми запросами каким-то данными из тайн мира и так далее и самое важное в рамках каждой колонке мире данные хранятся в виде версия и собственно значения и версия может по умолчанию быть просто некоторым платы инкрементом же . либо вы можете сами указывать какую-то версию мы в качестве версии использованным стемп ивента в текущем примере видно что пользователь в различные там стенд посетил три различных url и и видимо хайло утру за да и видимо на хайло утру он пришел как раз с поискового запроса конференция потому что там стенку них одинаков как вы можете быстро получить всю информацию по конкретному пользователю можете получить только url можете получить последние пять его визитов либо можете задавать get запрос или вы можете просто сканировать в таблицу и получать там всю информацию либо ее часть окей таким образом loader знает как сделать путь запросу space запрос на обогащение данных и преобразовать какой-то ивент из лога партнера в некий болот уже конкретного пользователя далее рассудке запускалась задача которая называлась аналитик engine она сканировала и gps во много потоков и размечала пользу или как раз в аудиторные сегменты что такое все разметка сегмент на вход это некий набор скриптов которые на вход имеет все данные о пользователе она выход дает не которые результат сегментации если быть грубым то это просто набор с тех самых сегментов допустим к идентификатору окей это задача запускалась работала она там несколько часов укладывалось в сутки делала выгрузки различным партнерам результаты сегментации и писала статистику в этом случае так как задача запускалась рассудке для того чтобы понять сколько у нас пользователь в сегменте мы могли просто не применить какой-то каунтер конкретного идентификатора сегмента за определенный день мы это делали в манге рассудке избранному приходилось жаба incremental счетчики мы видели сколько в каком сегменте у нас пользователи эта схема была довольно хорошо в свое время она позволяла нам горизонтально масштабируется обрабатывать растущие нагрузки мы могли оценивать объем аудитории смотреть как а все меняются различные объемы аудитории при изменении параметров сегментации но также она была и не лишена рядом недостатков например вы не могли удобно подключиться какому-то куску продакшн и драйв к что-то поди баррель не всегда было просто посмотреть логи не всегда было легко сделать q & the monitoring и главным минусом этой системы по сути было то что время реакции на события пользователям было очень велико то есть так как задача запускается раз в сутки есть некий кэп между тем когда мы увидели вон та пользователь и тем когда мы на него среагировали был на такой интересный кейс есть у нас клиент который занимается тем что в регионах она выдает микрокредита населению за партнер лишь мы с ними и было у нас есть некоторая а фишка в которой они отдают индикатор свое поле цель и возвращают некоторые там скоро балу на основе которого решают давать пользователю кредит или не давать все начали с ними работать все идет хорошо потом они решили взять чистую систему ну то есть защита 5 то есть это будет гарантированно тот пользы которую мы ещё не видели походить по интернету и потом зайти на свой сайт и оставить заявку на кредит стоит заметишь них был такой силой что они обещали дать пользователю ответ еще не 15 минут а если все таки может быть до сразу деньги должны перед вскинули начали ходить посетили много страниц в интернете зашли к себе на сайт оставили заявку спрашивать у нас по api-данные мы говорим говорим ребята извините данных пока что нет а не напишите по ну ребята ну почему мы же вот походили где начали объяснять что вот еда есть ли key-lock после того как мы там увидим пользователя и так далее ну и собственно казалось бы в случае это бы рекламой ну среагировали вы чуть позже на ln ну увидит пальцы рекламу на 5 на 10 на 20 на час позже ну не умрет он от этого а тут может быть у пользовали реально рушилась жизни мы там кредит не дали так далее либо наоборот это была одна из тех причин почему нам сильно был нужен real-time таких сигналов было много и пошли мы по пути реализации полного перехода на всем in a real-time собственных чего мы хотим как только мы видим иван то пользователи мы сразу хотим стать bass как только данные по полису любишь безе изменились мы сразу хотим и запускается сегментацию этого пользовались процессы отметки сегменты если у нас в принципе получится как-то подцепиться какой-то часть a production трафика с developing sky машины при этом вообще никак не аффекта продакшн это будет очень удобно ну и в целом этим собирать различные метрики смотря как система себя ведет как она влияет как она работает под этими изменениями в не так далее начали мы с того что приступили к решению сначала 2 задача сегментировать пользователи сразу после того как он изменился в любезен ранее ешь bass у нас находился на одних и тех же серверах что и наша компьют надо ярно мы придется это было хорошо случай быть процессинга когда она запускала задача она сканировать это данные там была максимальная дата лака лиц и все было хорошо но в случае же с тем когда мы переходим на real-time вы понимали что у нас очень сильно возрастет объем случайных чтений и поэтому нельзя чтобы какая-то тяжелая задача запущенные на кластере как to affect сила наш production она дралась за ресурсы железом так далее поэтому первое что мы сделали это мы вышли из bass на отдельный сервер а далее покрутили такие вещи как блок кошевым фильтр сделали все хорошо был далее мы неплохо поработали над плотностью храним данный патч быть нам без потери качества удалось пожрать hbs примерно в два раза то есть он там было около десяти терабайт стал раза в два меньше сделали мы это ну применив варят и очевидных оптимизации так например хранить описание квеста ринго я лангом ну и мячу винных там все что может блокировать классифицировали и так далее далее у вич бэйза есть механизм похожий на триггер и в обычных базах данных там вы можете реализовать какую-то костюмная логику то есть что делает если полить или изменился вас очень беден мы красота заюзали это называется как о процессоре реализовали там такую логику что как только пользу изменился мы его идентификатор отправляем в кафку а если кто-то ставку не работал то считайте пока что что это просто некий место же брокер чуть позже я поподробнее расскажу итак отправляется идентификатор пользователя в очередь на сегментацию есть некий сервис который просто но является демоном он считается какую-то машину и начинает потреблять список видов на сегментацию получает фанимся данные из ebay за и запускает карта самом конце сегментации этот набор скриптов которая находит получается данное поле сцена выхода это набору сегмент и пишет результаты своей сегментации обратно в коробку но уже в другой топик не понти постигнешь нарезал запустили на каком-то проценте графика всё было довольно неплохо стали увеличивать нагрузку то есть пытаться обрабатывать не там есть процент от краёв к тому же постепенно увеличивать увидели проблему что наши сервера сигмент отаров начинают по одному вылетать из употребления данных то есть вот такая лесенка сначала один вылетел все остальные перес при делении нагрузка потом второй вылетел все перераспределить нагрузку и так далее не шли-шли шли в итоге очередь толком не разгребали было это связано с тем что у кафки есть интересный механизм работы хит гитов если кому-то будет интересно я попозже расскажу это here by dave ну отправки информации о том что кончины что живой из останется время я поподробнее расскажу как мы это запороли начали сегментировать сто процентов нашего трафика и примечательно то что сегментации это чистая сепию bound жаба и мы можем полностью утилизировать все нашу сервером и посмотрели прекрасно ну и также чтобы понимать примерные нагрузки сейчас sign that are в секунду обрабатывает на бортик имитаторов в секунду обрабатывает порядка пяти гигабит даных далее есть несколько потребителей результатов сегментации это просто отдельные сервисы которые можно поставить и независимо читать те самые результаты которые кто-то уже написал есть какой-то потребитель который выгружать данные партнером есть потребитель который выгружает данные в google api в наши des petits дебит папе и есть канчи мир который пишет статистику тут стоит отметить что мы уже не можем просто взять интернете счетчик в рельсе интернете . манги потому что пользователь может быть несколько раз про сегментирован на дню и соответственно если мы будем и приметить каунтер сегмента за дату мы получим просто какой-то бред то есть по сути нам надо решать задачу подсчета количества уников в потоке мы для решение этой задачи взяли вероятностью фактуру данных айкарли клок и использовали ее реализацию в одессе таким образом мы можем хранить ключ каждый раз к этому ключу отправлять новые виды на добавление что не нужно быть в этом сегменте и если мы уже добавляли то этот каунтер просто не будет меняться есть там некоторые ограничения связаны с тем что структура все-таки вероятностная там есть порядка одного процента погрешности в целом нам для оценки объемов это отлично подходят также благодаря также в одессе можно его пирло клубе сделать так что вы можете посмотреть кардинале ти между двумя сэтами и покинуть сколько у вас вообще людей любят кошек либо любит собак примерно понять сколько людей любят домашних животных принципе вот подробнее про хай куклы клок будет рассказ константин игнатов насколько я помню в этом зале после меня если кому то интересно послушать и должно быть очень интересным как и таким образом немножко поподробнее о том как работает кафка кафка это месяц брокер у него есть различные топики и тут о куда данными за ссылаются топике состоят из спортивных пар тишин если быть грубым то это просто файлы на жестком диске вы сами выбираете число практичных на топик нужно понимать что выбираешь лоб артистов вы выбираете максимальный уровень параллелизма с которым вы сможете потреблять заплутать и как например если вы сделаете 200 партиций и поставить 15 кантемиров то реально у вас будет работать здесь контейнером остальные пять будут курить партиции могут реплицироваться между кафка брокерами если какой-то брокер выходит из строя то автоматически происходит reasoning и начинают сердца против другого брокера есть продюсеры это те кто пишет данные они по сути пишу данные просто в конец файла это последовательное записано вышли быстро работает хорошо в нашем случае продюсер ряда как раз тот самый и же без триггер который отправляет ее данных сегментацию либо сегмента то который отрывает результата сегментации в кафку и есть так называемые conti мир группы это некоторые потребители данных которые читают данные из и кафки то есть мы опять у нас это sign that are который читает поток видов канчи мир группы оперируют таким понятием как овцы этот оппозиция на которой сейчас находится курсор чтения из конкретной партиции то есть сейчас хочу мир группа а потребляет из партиции один на на седьмом сообщение а испортится 2 на пятом сообщение примечательно то что все это все эти данные хранятся непосредственно на канчи мир группу то есть вы можете добавить другую канчи мир группу и она будет потреблять независимо получается вот данные пишите в кадку один раз и они у вас разным контейнерам доступны и независимо друг от друга это даёт вам то что вы можете прозрачно подключить нового кончина каким-то уже написанным данным например у вас включается вам новый партнер и он хочет получить данные за последний день не проблема вы его добавлять новую кончиной группу и говорите сдвинуться в сет на один день и троллей последний день либо есть какой-то консьюмер он говорит типа ребят у меня было бакая вот один день не смог от вас принять то мы могли бы переслать токио и сдвигаем на 1 день назад и переодеваемся ну и вы можете горизонтально стрелять карту конце мая кругу по количества thread'ов либо по количеству серверов вот это выглядит как то вот так вот все автоматически перья баланса и продолжит работать также если какая-то какой-то кончине рваных кончиной группа выходит из строя этого автомате что же происходит ребаланс event как раз вот основанные на этих самых механизмов here битов и они перри подключаются все остальные таким образом мы начали сегментировать пользуется ли как только они меняются вич bass мы можем независимо подключать новых кантемиров мы можем сами подключаться к кому-то проценту продакшен трафика как-то его проверяет локально при этом вообще не отёк the production можем двигаться в сайта как хотим теперь уже на моду нужно добиться того что как только пользователь изменился мы пишем мы пишем его сразу же в x-bass итак мы понимали что если раньше пользователь совершал ивента и он находить попадал в лог-файл то если у совершил многое want of the в какой попадал лишь один ивент от этого пользователя на сегментацию теперь же когда мы хотим писать сразу в кафку ивенты сильно возрастет погрузка на сегмент отары сильно возрастет число случайно чтение любой из первое что мы сделали это мы передвинули и кейс на ssd-диски стоит заметить что и любой со сковородки не умеет переехать на ssd делает это средствами gdfs а вы можете выжить офиса конкретную папочку соперница на конкретную группу дисков вот сделали мы это потом спустя какое-то время увидели что вместо нас нас с действительно кончается при этом как бы и чубайс не так сильно растет оказалось то что мы заменили его на ssd и собственно запенили даже и снапшоты как только мы начали делать что-то не клади кларенс на ssd и и создана ли начали забиваться эта проблема также решается тем что вы можете взять и создавая snapshot экспортировать его потом целиком находи fs удалив при этом в себе эту информацию и сам 700 из ebay за если необходимо зари ставится вы просто делаете обратно импорт и историк так как процедура не часто это все это работает довольно неплохо и так раньше лог-файлы поступали к нам на gdfs а еще мы подкрутили иной на ssd вынесли в рай да хоть лоб потерь или мем стар и включили такую опцию как с black on white это штука которая позволяет горячим данном той сферы записанном оставаться горячими чтобы они каши рвались в памяти ok далее все наши потоки логов которые у нас были мы вместо того что указательных и dfs и завернули в кафу сделали там на каждого поставщика данных отдельную очередь и сделали несколько дней мир групп одна из них sing it обратно данные в ходе fs чтобы у нас в принципе эти данные не терялись они у нас были если нужно нашим аналитикам сделать это запрос они могли по ним пройти из проанализировать их и сделали ещё одну кончиной группу которая в данных назвать стрим факт слайдер стрелок году она работает с тем же самым кодом что и работал старладдер но делает это в потоке то есть приходит event она знает как от конкретного поставщика преобразовать этот ивент у запрос на обогащение данных убийству запросов bass и делает это мы за спойлером запустили эту штуку запустили на проценты трафика все было хорошо потом начали увеличить трафик и дейсвительно увидели что мы не успеваем сегментировать семьи татары начинает захлебываться что тут видно мы собираем такую метрику как сколько сообщение пролежала в очереди краски у каждого сообщения в кафки есть timestream когда она была создана мы на наших потребителях сравним этот темп стенд с текущим находим разницу и понимаем сколько сообщения пролежала в очереди но это вот максимальное попортится время сколько пролежала в карты соответственно тут видно что после запуска это время начинает расти и мы просто не успеваем разгребать наша очередь почему это происходило вообще во первых который рассказал носитель на очень сильно выросло количество эвентов которые приходят на сегмент ad и во вторых у нас параллельно работал старый loader и новый мы благо могли себе это позволить потому что данные будь выезде по сути деду прицениваются по той самой версии а так как вот одинаковый мы имеем одинаковый путь запрос как и на оффлайн loader их так и на стрим в их водах поэтому запускали мы с ним и параллельно как бы старую мне выключаем и так как они работали параллельно запросов также значительно больше а старая система имела некоторые нюансы что создавалось много столь файлов ich bin ich бы с очень часто компакта есть там у него с этим нелогичное поведение если хотите потом подходите я расскажу окей побороли нелогичное поведение с компактном и начали думать что вообще делать с тем что мы не успеваем сегментировать как бы один возможных решений просто было горизонтально стелется по серверам и сказать что все задача завершена но не наш подход мы как бы хотим честно максимально утилизировать железо не делать лишних действий на это смотреть глазами что вообще происходит увидели следующее что пуль типичная сессия пользователя в интернете это зайти на какой-то сайт и быстро дайте нам нужны ему страницы либо зайти на какой-то сайт допустим новостной тематики и в фоновые вкладочки открыть много побегов получается что получается мы на наших loader их видим эти ивенты к возьмём лишь борис и либо из молодец подсылает юля на сегментацию и получается на какой то момент времени вот уже текущие у нас в очереди очень много ивентов на сегментацию одного и того же пользователя получается вот по пользователю а например данный матч быть уже есть они самые актуальные sign that are видит этот цепляет его получается данный язык борис сегментировать все-таки потом следующий want пользу ли он делает ровно тоже самое по тем же самым данным потом снова снова и снова благодаря тому что в кафки вы можете сделать определить свой партий shiner вы можете добиться того чтобы пользователь пара всегда попадал на sign that are a team а пользователь dns арендаторы 2 например это дает классные свойства что вы можете кэшировать прямо in-memory какую-то информацию по каким-то пользу что мы стали делать мы ввели такое понятие как timestream последний сегментация конкретного пользователя начали хранит это просто в память то есть как только пользователь от сегментирован мы пишем какую-то мапу что вот такой экстрим сегментация этого пользы и если приходящий event имеет таймс темп раньше даты последней сегментации то мы просто скипом этот ивент потому что мы на самом деле может сегментировали по самым актуальным да ну очень здорово то что благодаря этап темизация система получила такое свойство как сама масштабированием если ночью нас стоит какой-то крупный партнер мы видим там знаю в два раза больше ивентов мы благодаря этой схеме у нас к письму какой-то лак естественно какое-то отставание то есть мы сегментируем уже не за секунда там не знаю за минуту за две минуты за три минуты лак мэтлок статичный и просто мыски пыль какая-то часть ивентов которые просто не надо сегментировать если же у нас ресурсы есть там и честно сегментируем каждый event и так как это вообще выглядело когда мы запустились вот это ноготь и снимание по оси y это процесс распределения как вообще относительно другой распределены сегментация и пропуске то есть видно что ночью у нас ресурсы видимо более свободные меньше ивентов мы видим а пользователя потому что ночью люди спят либо просто ходят более вяло по интернету и мы сегментируем больше ивентов днем же нагрузка видимо выше мы сегментировать больше skip им но при этом как бы характеристик системы не теряем ну вон есть некоторые скачок в конце видим какой-то партнер подлил на какой-то здоровый лог-файл там было много информации о разных пользователей ну и пришлось честно сегментировать эта схема довольно часто нас вручала когда мы видели что у нас растет трафик и случается к эту проблему мы видели что мы balancing и все равно не отстаём также внимательный зритель не раза меньше потому что снег то и синяя полоска синяя полоска это наша боль это боты боты это самые тяжелые пользователи роботы с точки зрения сегментации у бота огромное количество визитов он делает их частом и так как некоторые скрипты и плюс-минус линейны по времени сегментации то сегментировать ботов очень тяжело ну опять же благодаря тому что мы можем ботов видеть на одном и том же сервере мы можем сделать так что ввести некоторые метрику как часто пользуюсь совершает ивента и что мы сделаем сделали некоторую скрипт в сегментации который смотрят насколько активно пользуется сделать вы понять как супер активный если он совершает например там по одному ивентов в секунду в течение часа то мы просто добавляем его в некоторые временный blacklist и скипа им все ивента от него считая что это бот вот ну вот тут вот это как раз та синяя часть казалось бы синей части немного она маленькая но из-за того что по бокам огромное количество визитов это дало нам очень хороший boost по производительности там не знаю пахнет и наверное 30 пью мы сэкономили и так вся крутость этой схемы получилось в том что у нас по сути весь pipeline находит теперь перед глазами мы полностью понимаем как она работает полностью понимают что происходит в случае с мог радиусом это было не всегда так и потому что там и логе посмотреть не так легко и и там каунтер это же посмотреть не дают они такого палец так много полезной информации но вот например видно что когда-то что-то пошло немного не так и то самое максимальное время ивентов в кафки там секунды выросла до 4 минуты риторика бой за несколько минут это вот та самая самому штабелями если мы что-то меняем мы видим реакцию на эти изменения мы можем мониторить вот пример скрипта им мы релизом новый скрипт какую-то новую логику которая на основе данных и отмечает в новые сегменты мы видим как долго она работает как она влияет на не знаю там другие скрипты возможны мы можем легко привести кого-то после анализ что вообще было что по случаю послужило причиной какой-то проблемы либо вот мы зарелизили как это зафиксирует и части системы например можно сделать в это альтинг например был нас интересный случай что когда-то мы там по-разному крутили потоки данных как мы куда пишем и где-то не докрутили и забыли и сказки логе завернуть в х dfs из почвы с не попадали по прежнему пользу сегментировали новых и dfs они не попадали замедлим и это опять же благодаря а лифтингу то есть работала ли типа ребята смотрите у вас там очередь уже прям переполнена там никто ему из нее тот кончина который есть не увидели включили обратно и вот тот гэп там знает два дня кому было в этом блоге с филина хдс ничего не потеряли так авто на здоров подстраховал а ну и стоит отметить что в кадке вы можете сами настроить различные политики для различных топиков например вы можете сказать что я хочу чтобы в этом топике хранилось не более 7 дней данных но при этом размер партиции не превышал 10 гигабайт вы это настраиваете и сами управляете сколько ваши данных у вас будет в кафки какие у вас там fault tolerance и будут требоваться так далее немного наших планах на будущее у нас есть продукт кластер который в общем то ночью сильно занят он делает различные выгрузки партнером там различные расчеты бегут а днем приснился свободен иногда там аналитики запускаю запроса но в целом он свободен мы можем что сделать кантине лидировать наше приложение наши сегмента ты и запускать его найар не указав при этом такую политику что когда кластер свободен ты пожалуйста поднимаю в этот контейнер и помогай днем вот сегментацию а если вечером турине вечером если кто-то запускает аналитический запрос сказать что вот этот запрос на самом деле важнее чем этот сегмент атор чем эта помощь ты пожалуйста выйти из него общее при туши и дай ресурс вот к тому пользователю также мы хотим перейти уже точным средством хранения статистики в потоке вероятность структура данных она конечно хороша она экономит память а то есть сейчас у нас в ней данные по моему месяцев за 9 по куча сегментов за каждый день и это занимает по всей базе 600 миллионов пользователей порядка в 8 гигабайт оперативки это очень здорово мы можем смотреть динамику как у нас там менялась разных сегментов определенный день как там любителей кошек менялись в там летом зимой не так далее но довольно сложно строить объемы пересечений то есть сложно посчитать сколько там женщина из новосибирска которой смыслу говоря не знаю потому что так как структура данных вероятностная у нее ответы тоже ну как бы имеют вероятность как сама погрешность один процент и вы можете в общем-то посчитать пересечении зная основ зная размеры объединений то есть вот вас есть сет а ssb вы знаете размер абэ и размерах объединения вы можете взять просуммировать вычесть объединение получить размер пересечения но если брать просто в тупую high black lac это вам даст такую ситуацию что у вас ответ может быть какое-то отрицательное число это происходит потому что погрешность что если размеры сайтов очень отличается у вас погрешность определения размера большего будет в итоге больше чем итоговый размер пересечения что-то задача решается всякими но ты один из основных путей решения это использование колонн oriented баз данных мы попробовали клика усы куда-то наушников очень понравились плюс там есть понятный способ как стриме туда данные вот но у нас также есть большой опыт калдари impala и мы хотели попробовать эту вместе с куда-то ударим пола и хранить как то результат вот это на очереди ну и в целом оптимизируя по оси пью сегмента тори так это чисто сию баллон жаба сэкономив 10 процентов сепию мы экономишь это 10 процентов косцов железо сегмента таро ну подводя итоги кафка хороша но как и hbs но это не серебряная пуля надо приставать к но работает помнить об этих нюансах помнить что у вас различные гарантии место гибели юрия о том очерёдности сообщение работа всегда в рамках партиции конкретный и если вам нужна какая-то гарантия в рамках всего брокера то возможно стоит подумать подходит ли вам кафка он всегда важны реальные данные если бы вы ни апк от если бы мы ни об катались на реальных данных мы бы и не увидели этой проблемы с ботами не увидели бы проблемы с типичной сессии пользователя делали бы непонятно что там течение какого-то периода времени запустили бы просто сразу были глины production ну и опять же представитель на монитор все что можно мониторить собираете различные метрики стараются играть метрики которые действительно важны allure здесь на них и так далее вот в целом это плюс минус все что я хотел вам рассказать буду рад ответить на ваши вопросы если имеются собственно к вопросу вас есть комментатору он каким-то образом even тоннель нарезает пользователь нарезает по сегментам каким образом вы как раз понимаю пока мы сложным правило у нас к новым сайтом искать логическом by нации более простых там у всех союзнику это по сроку либо чему-то равен какой-то полилога каким образом вы собственно большое количество правил на трафике быстро сегментируйте смотрите что так вообще правила правила бывают очень разные это может быть визит пользователя на конкретную тематику странице есть у нас там синтаксический анализатор страниц либо это может быть визит пользователя на прям конкретно страницу где продаются лекарства соответственно есть набор правил и разные ступе работают по-разному то есть может быть кто-то кадр означает по визитам кто-то который означает по поисковым словам кто-то кто смотрит если пользовалась сегмент любит кошек или любит собак то он любит в домашних животных такой скрипт он у нас там их порядка более 30 штук разных то есть у вас набор эвристик из которого все прогоняется да и ну да и подгоняемый через них всегда все данные пользуются там если различный look-alike модели они также прогоняются но вот по всем данным полицию а такой вопрос если у вас есть имели то вы в realtime выплатили нет периодически модели обновляются то есть они составляются не в реал тайме но периодически я про apple ой ну применение модели как страх применяем в realtime 8 и время сегментации полюс или она там занимает миллисекунды этом 95 см и в общем то мы можем себе это позволить вполне спасибо спасибо за интересный доход а вопрос вот просто рассказали про сегментация в этом pipeline кластеризация пользователя на как то возникает ну допустим хожу с разных браузеров у меня к разная да под которой вы меня видите да кластеризация пользовали конечно же есть но у нас есть механизм интегрального профиля благодаря которым мы можем понять что в общем то вот вы с этого браузера и с этого браузера на самом деле один пользователь либо там высота во браузера и там с мобилки а вот эти вот штуки на самом деле одна кука и мы стареем с таких пользователей и объединять в некоторые интегральный профиль и сегментировать уже по максимальному охвату всех известных данных это происходит до того как вы в кафку положили это сегмент атор где-то делает это нет это делать не sign that are это отдельная джабба которая тоже смотрит на потоки данных о митингах различных вообще том как мы пользовали видим и как-то но уже в другом процессе может пользователей безе и распайки механи митинги и понимаю что вот это вот там пользователь пять лет на самом деле лишь кусочек ткут и большего пользоваться летом 10 тогда мы запрашиваем там по десятому и уже его значительно больше данных а тогда вопрос такой давно версию пастеризатора закатываете вы как бы только looking forward делать этой задним числом перемен же не запускаем сейчас не перемен спасибо можно было видеть вообще ну как она меняется как развивается спасибо а артем привет здесь здесь вот тут тут тут тут в другую сторону а а хочешь большое спасибо за доклад два вопроса вопрос первый вас есть аналитики они восходят напрямую ваш боится ли вы им какие-то не знаю там базы с агрегатами отдаете мы верим выпускаемую бы читать понятно окей и второй вопрос есть такая реализация вот по сути с жильем то архитектуры есть такая реализация ее как патч орикс не тестировали не пробовали нет не пробовали спасибо артем привет спасибо за доклад и у меня такой вопрос ты говорил вы используете хай первый блок для подсчета уникальных пользователей сегменте скажите как вы решаете и решаете ли вообще проблема opt out of то есть когда когда приходит допустим какой-то этот провайдер прислал свои данные потом говорит половина из этих данных некорректной просто в чем проблема то что хоть лопни поддерживая своего удаление посмотрите на задача решается на уровне с-1 татара тем что мы просто перестаем размечать эти данные в новый сегмент соответственно мы у себя внутри для сакральных сегмента также хранимого и распайки пользу поменялся мы записали уже актуальны данный вырос паек партнером отлили уже актуальные данные они у тебя могут сделать iv и вытеснить пользу из сегмента не отключать рекламу у него в статистике да это не поддерживается ну как бы она завтра будет завтра будут актуальной цифры спасибо артем привет сюда налево налево спасибо за доклад это первое что хотел сказать меня два вопроса первый вопрос как вы обеспечиваете отказоустойчивость вашего решения в общем и в частности как вы обеспечиваете устойчивость вашего кафка plaster а смотрите на сейчас кафка кластере пять серверов ну используя марио факторы там то ли два то ли три не помню если какой-то сервер вылетает то автоматически происходит рио-сан контейнеров и они начинают все вот эта птица уже с другого сервера то есть ответственно 101 1 попал то ничего страшного по поводу секунд а ты raw если падает конкретно сегмента то остальные понимают что кафка понимая что это то что мир отвалился и те партиции что он обрабатывал больше некому обрабатывать она всем другим потребителям кидает ребаланс event они заново стучаться в нее говорят доме новые silent получают новый a silent и начинают сами обрабатываю партиции вот то есть кафку на уровне кластера сегментации на уровне того что вы по конкретным окончания не влечет какой-то там потери данных и производительности кроме там нападение и но вот этот процент что улыбался кантаты окей это блокировку со второй вопрос насчет собственно говоря но были ли какие-то проблемы которые вам удалось решить из которой мы могли поделиться с нами вот и минсков к например пропадали о сообщении тоже бывает я понял сообщение преподали чем могу поделиться тем что мы начинали с 08 кафки сейчас актуальна 09 08 было прям не очень потому что 09 она была уже так хороша стабильно но здесь вообще хороша хайям огонь то есть но если брать брать 010 естественно из проблем понимать как работают heart beat а потому что там прилично нервов поела что мы там смотрим чтоб тачка выпало как капс это не заметила сообщения я обработала как бы вроде бы все работают все пишут что они сегментируют на самом деле очередь не разгибается вот это было было что добавлению партиции она не всегда происходит прозрачно надо там сделать несколько там плясок с бубном с ну то есть у вас не так вот тут есть 3 сервера вы добавляете 4 сервис когда вы добавите profits у вас старые данные не уеду туда только новый до начал подписаться как бы тоже надо принимать в принципе у кафки довольно хороший гайд на но у них на сайте его кмм стоит целиком прочитать и спаси большие еще вопросы вот рукава человек добрый день нет прямо сайт сайдингом не поделитесь к ним соображениями сколько на как это работает ну сайзинг кафки я имею туда то есть вашим объемом грубо говоря от не понял вот все те ивента с плане миллиардов на час обрабатывает кафки пять серверов единственного что мы упираемся упираемся это диски в его дисков соответственно там сейчас довольно простой железо типа там 220 d 4 ядра с hyperthreading этого 832 оперативки но реально под кафку меньше отведено и ну диски по 2 терабайта столько сколько позволяет собственно сама . вот смотрим zabbix если его вы это там растут то понимаем что надо либо размер сообщения уменьшать сейчас мы к ним очень просто джейсон и можно с этим тоже поиграть либо добавлять тачку так как тачки довольно дешевые пока что нам проще добавить тачку вот единственная с того что я помню к этому вряд ли процентов на 30 мы там с трех до пяти тачку шли нормально пасиба вопрос слева причем расскажите пожалуйста на каком гендере ходу по вы живете то есть это ваниль куда почему почему вы выбрали и дальше властные на там есть какие то проблемы то есть вы например хотите десятую кафку оживить еще накала дыры в которой там нет 10 кафки как вы решаете вопросы накатки новой кафки на поле старк ладан всего собственно у нас каутер колдер мы выбрали во-первых потому что у нас есть там внутренний инструмент построенные по верхом полы ну а им палатку д-р вот но также в целом у наших админов хороший экспертиза именно складе рик удобно там добавить новый так туда новые роли скатать и так далее как боремся с тем что там кавказца ретвитом нам кавказ таро мы просто не используем кафку от этой колоды у нас есть отдельно сервера bare metal где стоит просто кафка то есть в калдари у нас кафка не заведено так раз потому что там ставим вот тот еще что-то хочет если вопросов больше нет благодарим до кончика"
}