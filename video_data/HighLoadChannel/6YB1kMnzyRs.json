{
  "video_id": "6YB1kMnzyRs",
  "channel": "HighLoadChannel",
  "title": "Что делать, когда минута простоя стоит 100000$ / Евгений Кузовлев (EcommPay IT)",
  "views": 10300,
  "duration": 3077,
  "published": "2019-05-15T04:23:15-07:00",
  "text": "друзья привет меня зовут кузовлев евгений я из компании и компаний конкретное подразделение компании эти подразделения ком групп компаний и сегодня мы с вами поговорим поговорим в своем вообще a down time of a того как их сбежать от того как минимизировать их последствия если избежать не удастся ну и тематика она заявлена как вот что делать когда минута простой стоит 100000 долларов у нас забегая вперед цифры сравнимая кто мы такие и почему я вот тут вот перед вами стою почему я имею правом что-то рассказывать и о чем вообще сегодня вот поговорим чуть более подробно групп компаний и компаний это международный аквариумы процессом платежи процессе платежи по всему миру в россии в европе в юго-восточной азии он раунда вал у нас 9 офисов 500 сотрудников всего или примерно чуть меньше половины из них это эти специалисты все что мы делаем все на чем мы зарабатываем деньги мы сделали сами у нас все наши продукты их у нас достаточно много у нас в линейке наших от больших айти продуктов у нас их порядка там 16 различных компонент мы написали сами мы сами пишем массами развиваем и на данный момент мы проводим около миллиона транзакций в день миллионный наверное это будет правильно сказано мы молодая компания достаточно нам всего около 6 лет и 6 лет назад это был такой стартап когда пришли ребята вместе с бизнесом они обе были объединены идея была идея больше ничего не было кроме идеи и мы побежали ну как любой стартап мы бежали быстрее чем ну тут у нас была важна скорость они качество какой-то момент мы остановились осознали что мы уже не можем стой с тем качествам из той скоростью как тут жить и нам с этим нужно что-то делать в первую очередь качеством в этот момент мы там приняли решение написать там новую платформу которая будет правильный масштабируемой надежной эту платформу начали писать но в какой-то момент начали вкладывать развивать разработку тестирование в какой-то момент поняли что вот только разработкой только тестирования она не позволяет выйти на абсолютно новый уровень качественности сервиса то есть вы делаете новый продукт вы его доставляете на продакшен но все равно что то где то идет не так и сегодня мы поговорим о том как выйти на новый качественный уровень у нас как это получилось про наш опыт вынося за скобки разработку тестирования мы поговорим про то что доступной эксплуатации что эксплуатация может сделать сама что она может предложить разработки мы что она может предложить тестирование чтобы влиять на качество всегда основной краеугольный камень о чем мы сегодня будем собственно говорить это вот это downtime страшное слово да если у нас случился downtime у нас все плохо мы бежим поднимать админы сервер держит даст бог не упадет до как в песне поется вот об этом мы с вами сегодня поговорим когда мы начали менять наши подходы мы сформировали для себя четыре заповеди они вот у меня представлен на слайдах эти заповеди они достаточно просты быстро выявить проблему еще быстрее от нее избавиться помочь понять причину потом для разработчиков и стандартизировать подходы обращу ваше внимание на пункт номер 2 мы избавляемся от проблемы они решаемы и решить это вторично для нас первично это то что пользователь будет ограждён от этой проблема она будет существовать в неком изолированном окружении но это окружение никак с ним не будет контактировать собственно мы с вами пройдемся по этим четырем но четырем группам проблем по каким-то подробнее по каким-то менее подробнее я расскажу про то что мы используем про то какой у нас опыт соответствующих решениях но начнем мы немножко по поляне по порядку начнем мы с пункта номер два как быстро избавиться от проблем у нас есть проблема нам не надо устранить что нам с этим сделать до основной вопрос и когда мы начали думать про то что нам надо устранить проблему мы для себя выработали некоторые требования которым устранение проблем должно должно следовать и чтобы эти требования сформулировать мы решили себе задать вопрос а когда у нас случаются проблемы и проблемы как уяснила случается 4 случаях аппаратная неисправность сбой внешних сервисов сменой версии по тут самый диплом и взрывной рост нагрузки про первые две подробно мы с вами говорить не будем аппаратная неисправность она решается достаточно просто вас должно быть все дублирован если это дискета диски должны быть собраны в рейд если это сервер с приложением он у вас должен быть дублирован если у вас есть сетевая инфраструктура вы должны построй поставить вторую копию сетевой инфраструктуры то есть просто вы берете дублируйте если вас что-то отказывает вы переключаетесь на резервной на резервную мощность здесь больше чего-то сказать сложно второе это сбой внешних сервис для большинства систем это вообще не проблема но не для нас так как мы процессе платежи то по факту мы такой агрегатор который стоит между пользователем который вводит карту карточные свои данные да и банка банками платежными системами типа visa mastercard там мира того же нашего российского да и внешним платил внешним нашим сервисом платежным системам банком им свойственно сбоить повлиять не мы не вы если у вас есть такие же сервисы на это не можете что тогда делать ну здесь варианта два во первых если вы можете вы должны за дублировать этот сервис каким-то образом например мы если можем мы перекидываем трафик с одного сервисом другого к не знаю процессе sillerton карты через сбербанк у сбербанка проблемами приводим трафик там не знаю на raw айзен условно второе что мы можем сделать это мы можем очень быстро заметить сбой внешних сервисов и про это мы поговорим скорость реакции в него в следующей части доклада по факту из этих четырех мы можем конкретно повлиять на смену версии делать сделать действие которые приведут к улучшению ситуации в контексте тепло iv и в контексте взрывного роста нагрузки собственному это и сделай здесь опять же маленькая ремарка вот из этих четырех проблем несколько решается сразу если у вас есть облака если вы находитесь в облаках там microsoft ажур amazon.com используйте наши облака там от яндекса там лет mail а то как минимум аппаратная неисправность становится их проблемой и у вас сразу все становится хорошо в контексте аппаратной исправности мы немножечко нестандартная компания здесь вот все много говорят нам про cabernet про облака у нас нет ни купюрница у нас нет облаков у нас за то есть стойки железом во множестве дата-центров и на этом железе собственно мы вынуждены жить им еду вынуждена отвечать за это за все вот поэтому вот в этом контексте как мы будем разговаривать и так про проблемы первые две вынесли за скобки смена версии по базису наши базисы с которыми мы живем у нас разработчики не имеют доступа production ну почему так а просто мы сертифицированы по пища и dss у нас разработчики не имеют права лазить прот вот все . совсем поэтому ответственность разработки заканчивается ровно в тот момент когда разработка передала билд на релиз второй наш базис который у нас есть который нам тоже сильно помогает это отсутствует уникальны уникальны и недокументированные знания я надеюсь что у вас также потому что если это не так то тогда у вас проблема проблема возникает тогда когда эти уникальные и документированные знания не будут присутствовать нужное время в нужном месте допустим у вас один человек знает как тепло тикают компонент это человека нет он вот заболел все у вас проблемы и 3 базис которому мы пришли мы пришли к нему через боль и кровь боль и кровь и слёзы это мы пришли к тому что любой наш бил содержит ошибки даже если он без ошибок мы для себя так решили что когда мы что-то диплом когда мы что-то хотим в рот у нас билд с ошибками из этих базисов мы сформировали требования которым наши системы который нашим подходит должны удовлетворять а этих требований 3 мы должны быстро откатить deploy уметь мы должны минимизировать влияние неуспешного деплоя и мы должны иметь возможность быстро параллельно за тепло its именно в таком порядке почему в таком порядке потому что в первую очередь при тепло и новой версии вам не важна скорость но вам важно если что-то пошло не так быстро откатиться и оказать минимальное влияние но если у вас есть набор версии на продакшене которые в которых выяснилось что содержится ошибка это выяснилось как снег на голову тепло и не была просто ошибка содержится вам важно здесь уже скорость тепло и последующую чем мы сделали для того чтобы пришить эти требования мы прибегли к такой методологии она достаточно известным и не изобрели ни разу это blue green deployment что это такое у вас для каждого абсолютно для каждого для каждой группы серверов на которых стоят ваши приложения ваше аппликации должна быть копия копия которая такая теплая на ней нету трафика но в любой момент этот трафик на эту копию можно пустить это копия содержит предыдущую версию и в момент диплом вы выкапываете код на неактивную копию и потом переключаете часть трафика или весь на новую версию тем самым для того чтобы изменить поток трафика со старой версии на новую вам нужно сделать только одно действие нужно в об стриме поменять балансировщик и поменять направление с одного об стрима на другой это очень удобно это решает проблему быстрого переключения быстрого отката здесь же решение 2 вопроса минимизации вы можете пустить на новую линию на линию с новым годом только часть вашего трафика мы пускаем например 2 процента и эти два процента они не сто процентов если у вас потерялся сто процентов трафика при неудачном деплоя это страшно если у вас потерялась 2 процента с трафика это неприятно но это не страшно мало того пользователи даже скорее всего это не заметит потому что в некоторых случаях не во всех один и тот же пользователь нажав 2-ой и 5-ой едва он попадет на допустим другую работающую версию при этом не все так просто с blue green диплом у нас три группы наверно компонент вот все наши компоненты можно разделить на 3 группы это фронт-энд это там платежные страницы которые видят наши клиенты это ядро процессинга и это адаптер для работы с платежными системами платежными системами но я подразумеваю в банке я подразумеваю mastercard везут вот этих вот ребят и здесь есть нюанс и он заключается в роу тенге между линиями если вы просто переключаете сто процентов трафика у вас этих проблем нету но если вы хотите переключить 2 процента у вас начинается вопроса как это сделать ну самое простое в лоб вы можете просто по случайному выбору да там round robin в яндексе настроить и вас там два процента налево 98 процентов на право но это не всегда подходит у нас например пользователь взаимодействует с нашей системой не одним запросом ну это нормально это там два три четыре пять запросов у вас системы могут быть такие же если вам важно чтобы все запросы пользователя пришли на ту же самую линию на который пришел первый запрос или второй момент все запросы пользователя пришли на новую линию после переключения работать он мог начать раньше системой дом переключения тогда случайно и вот это распределение вам не подходит тогда я следующие варианты но первый вариант самый простой на основе базовых параметров клиента и пихаешь у вас есть тайге и вы пайпер пайпер нику разделяете направо-налево тогда у вас работает в случае второго моим описанного случая когда вот произошел диплом пользователь уже мог начать работать вашей системой и с момента тепло и все запросы пойдут на новую новую линию на ту же самую скажем если это по каким-то причинам вам не подходит и вам надо обязательно отправлять запросы на ту линию куда пришел первичный нет ни запрос пользователя когда у вас есть два варианта первый вариант вы можете взять платный джонс плюс там есть механизм стики sessions который при первичном запросе пользователь выставляет пользователю сессию и привязывает его к тому или иному up stream все пальцы последующие запросы пользователя в рамках срока жизни сессии отправиться на ту же up stream куда была выставлена сессия но нам это не подошло нам это не подошло просто потому что у нас был уже индекса обычный переходить на engine s plus не то чтобы это дорого просто это было для нас несколько больно не очень правильным стики совершенством для нас например не сработали по той простой причине что стеки с шанс они не дают возможности рауте по признаку или или там можно задать что вот мистики station сделаем например по и печник у или по apes нику и покупки или по там пост параметру а вот или или или там будут уже сложнее поэтому мы пришли к четвертому варианту мы взяли индекс на стероидах это понравится это такой же джинкс который дополнительно поддерживает включение в себя ла ла скриптов вы можете написать любой lua скрипт подсунуть его этому понравится это скрипт будет выполняться в тот момент когда пришел запрос пользователя и мы написали собственно такой скриптик поставили себе upon расти и в этом скрипте мы перебираем пятом шесть различных параметров по конкатенации или и в зависимости от наличия того или иного параметра мы знаем что пользователь там пришел на одну страницу или на другую одной линии на другую конечно можно было сделать нам немножко проще там тоже те же стихий stations использовать но у нас есть еще такой нюанс что у нас с нами не только пользователь взаимодействует в рамках вот 1 процессинга процессинга одной транзакции но с нами еще взаимодействуют платежные системы мы после того как processing транзакцию отправили запрос платежную систему получаем call back и допустим если вы внутри нашего контур мы можем нам прокинуть айпишник пользователя во всех запросах и на основе печника пользователь разделять его там уже не скажем той же визе чуваки ну вот мы тут такая компания да такая мы выводим международным на сайте в россии опрокинете нам пожалуйста и печник пользователь дополнительное поле ваш протокол стандартизированный ну понятное дело и не согласятся поэтому для нас от не подошло вот мы сделали окон есть соответственно с роутинга у нас вот получилось вот так вот у blumarine тепло есть соответственно достоинство которой я сказал и недостатки недостатков 2 1 вам надо заморачиваться с рейтингом и второй основной недостаток это расходы вам надо в два раза больше серверов вам надо в два раза больше операционных ресурсов о надо тратить два раза больше сил на поддержание всего этого зоопарк ну кстати в достоинствах еще одна вещь про который я до этого не упоминал у вас есть резерв цру чей рост нагрузки если на вас взрывной рост нагрузки повалили большое количество пользователей то вы просто включаете вторую линию в распределении 50 на 50 его сразу x2 серверов вашем кластере пока вы не решите там проблему еще налить еще серверов окей мы поговорили там как решить проблему минимизации и быстрого отката но вопрос остается как быстро тепло и ца здесь краткое все просто первое у вас должно быть среди система continue to deliver без нее никуда если у вас один сервер вы можете с диплом за ручками у нас серверов примерно полторы тысяч и ручками на полторы тысячи понятное дело что вот мы можем посадить отдел там размером с этот зал чтобы только диплом дипой должен быть параллельным если у вас deploy последовательный то все плохо один сервер нормально полторы тысячи серверов вы деплоить весь день ну и третье опять же для ускорения это уже необязательно наверное при тепло и зачастую выполняются сборка проекта но если у вас веб-проект есть на фронт он часть вы там делаете например випок npm собираете но что такое и это все процесс он в принципе не долги то минут пять но эти пять минут могут быть критичными поэтому мы например так не делаем отмыть и пять минут убрали мы диплом артефакт что такое артефакт артефакты the pride собранный билд в котором уже выполнено вся сборочная части этот артефакт мы храним в хранилище артефактов таких хранилищ артефактов мы свои время использовать дуа это был nexus и сейчас джефри карте factory nexus мы изначально использовали потому что мы начали как бы этот подход практиковать в java приложениях он там хорошо под это подходил потом туда же засунули часть приложение которое написан на php и nexus уже не подходила вы для этого выбрали джефри карты factory штука которая умеет артефакта рить абсолютно все мы прошли даже потому что мы туда храним арт мы в этом хранилище артефактов храним пакеты собственные бинарные которые мы для серверов собираем поговорили про смену версий по следующее что у нас есть это взрывной рост нагрузки здесь наверное я понимаю под словом взрывной рост нагрузки не совсем правильную вещь мы написали новую систему она сервис-ориентированной а такая модная до красиво и везде worker и везде очереди везде асинхронность и в таких системах тронул данные могут идти по разным flow в первому там для первой транзакции там это может быть там за действом 1 3 10 worker для второй транзакций там 2 4 5 и сегодня допустим с утра у вас идет поток данных которые задействуют первые три маркера а вечером у вас резко меняется и все задействует 2 3 ордера и здесь получается так что вам нужно как-то смасштабировать worker и вам нужно как-то сможет собирать ваши сервисы но при этом не допустить раздувание ресурс мы подошли к этой проблемы над моим опять же определили для себя требования эти требования не достаточно просты что там был сервис discovery параметризации ну все стандартно для построения вот таких от масштабируемых систем кроме одного пункта это амортизация ресурс мы сказали что мы не готовы ресурсы амортизировать ну вот на ветер что прям сервера воздух реле мы взяли консулу мы взяли нам от которые управляют нашими вор кроме почему-то не нас проблема давать немножко назад откачусь у нас за нами стоит сейчас в районе 70 платежных систем с утра трафик идет через сбербанк потом сбербанк упал к примеру и моего переключаем на другую платежную систему у нас работал с по worker of сбербанка а после этого нам надо резко поднять 100 worker of для там другой платежной системы это все желательно что происходило без человеческого участия ну потому что если человеческую части tube там 24 на 7 должен слить инженер который должен это вот только этим заниматься потому что такие сборе когда у тебя 70 систем за тобой не происходит регулярно поэтому мы посмотрели на нам от у которого есть открытый api и написали свою штуку сказал нам от skyla которая делает примерно следующее она следит за ростом очередь и и уменьшает или увеличивает количество worker в зависимости от динамики изменения очень когда мы сделали мы подумали потому что мы в open source сеть потом они посмотрели в принципе она простая как две копейки до сих пор моя open sourced не стали но если вдруг у вас нам после доклада после осознания того что вам нужна такая штука то появится необходимость в ней на последнем слайде там есть мои контакты напишите мне пожалуйста из там наберется хотя бы там не знаю 35 человек наверное в конкурсе как это работает давайте посмотрим у меня забегая вперед с правой стороны для вас с левой стороны есть кусочек нашего мониторинга это одна линия сверху время обработки событий посередине это количество транзакции снизу это количество worker of если посмотреть вот на этой картинке есть бой на верхнем графике там один из графиков вылетел за 45 секунд одна из платежная система легла тут же был приведён трафик там за 2 минуты и пошел рост очереди на другой платежной системе где не было worker of но мы не утилизировали ресурсы наоборот утилизировали сурс корректно мы не хотели греть там был там минимальное количество уровень питьевой без тимур киров но они не справляются иначе и на последнем графике виден горб который как раз говорит о том что с колена поднял это количество в два раза мы потом когда график немножко опустился он немножко под уменьшено количество ордеров было изменено автоматическом режиме вот так вот штука работает поговорили про пункт номер два как избавляться быстро от причин теперь про первый пункт как быстро выявить проблему что такое быстро навить проблем мониторинг мы должны быстро понимать определённые вещи какие вещи мы должны быстро понимать 3 вещь мы должны быстро понимать и быстрому и мониторить соответственно работоспособность наших собственных ресурсов мы должны быстро понимать вы хождение строем мониторить работоспособность систем который для нас являются внешними и третий пункт это выявление логических ошибок и тогда система работает у вас по всем показателям все норм но происходит что-то не так здесь я наверное мал что расскажу такого прям крутого по буду капитаном очевидность мы искали что есть на рынке у нас сложился веселый зоопарк вот такой зоопарк у нас сейчас сложился у нас zabbix используется для мониторинга железа для мониторинга основных показателей серверов ок метр мы используем для баз данных графа ну с прометеем мы используем для всех остальных показателей которые не подошли под первые два но причем часть графин с прометеем часть графа нас рефлюксом и телеграфу вытер га вот и мы год назад хотели использовать new relic и время хочу вам сказать классная штука она умеет все но насколько она все умеет настолько же она дорогая когда мы выросли вот собственно до объема в полторы тысячи серверов нам пришел vendor сказал давайте заключать договор на следующий год мы посмотрели на цену и решили что нет мы так делать не будем сейчас мы отмыли камер или к отказываемся у нас там уровень 15 сироп осталось под мониторингом не ролика но вот цена оказалась нашим дикой и есть один инструмент который мы реализовали сами этот блогер сначала мы назвали bagger но потом прошел нас учитель английского дико засмеялся и перри назвали на дебаггер что это такое это инструмент который по факту раз 15-30 секунд на каждом компоненте как черный ящик системы на каждом черным ящиком над каждым компонентом запускает тесты на общую работоспособность компонента например если внешняя страница ну платежная страница он просто открывает смотрит что она выглядит так как оно должно выглядеть если это processing он пуляет тестовый транзак усмотрят что это транзак а дошла если эта связь с платежными системами там соответственно пуляем тестовый запрос где мужем и смотрим что у нас все хорошо что мы мониторим в основном какие показатели для нас важны response time rp с на фронтах очень важным показателем сразу отключает что у вас что-то не так количество необработанных сообщений во всех очередях количество worker of и основные метрики корректности последний пункт это бизнесовый это бизнесовая метрика основная вам вот если вы хотите тоже самый мониторить вам надо определить одну-две метрики которые вот для вас является самим показателями у нас такая метрика это проходимость это отношение количества успешных транзакций к общему потоку транзакции если с ней чего-то меняется вот на интервале там 5 10 15 минут значит у вас есть проблемы ну кардинально меняется как это у нас выглядит вот пример одного из наших душ бардов с левой стороны 6 графиков это соответственно по линиям время обработки количества маркеров и количество сообщений в очередях с правой стороны там рпс ртс а снизу как раз тот самый вот та самая метрика которая бизнесовая и на бизнесовые метрики у нас сразу видно вот что что-то пошло не так на 2 средних графиках это как раз опять же у нас упала очередная система которая стоит за нами второе что нам надо было сделать это отследить падение внешних платежных систем здесь мы взяли up and racing такой механизм который позволяет трассировать ну стандарт да и парадигму который позволяет трассировать распределенные системы и его немножко изменили стандартный пульт racing парадигма она говорит о том что вот ну мы строим трассировку каждого отдельного запроса нам это было не надо им и этой звезды вернули в трассировку суммарную агрегационную сделали инструмент который нам позволяет отслеживать о скорости ответов система стоящих за нами опять же вот график который нам показывает что там одна из платежных систем начала отвечать за 3 секунды у нас появились проблемы при этом этот эта штука среагирует когда проблем начались этом интервале 20 30 секунд и третий класс ошибок мониторинга который есть это мониторинг логический я честно говоря не знаю черная стоит нарисовать потому что мы искали долго что что-то на рынке что нам подойдет ничего не нашли поэтому пришлось сделать своим что я подразумеваю под мониторингом логическим но представьте себе вы делаете систему ну например колонке дыра вы и сделали запустили успешный менеджер значит вася пупкин поставила на телефон ведь нам девушку лайкает и и alike уходят не девушки alike уходят охраннику михалычу из это уже бизнес-центра менеджер значит пускается вниз и потом не умела это почему этот охранник михалыч собственно ему так приятно улыбается вот в таких вот ситуациях для нас эта ситуация на низу звучит немножко по-разному то что я писала такая репутационной а потеря которая косвенно ведет к потерям финансовому нас ситуация обратная мы прямые финансовые потери можем понести например если мы провели транзакцию как успешной она была неуспешной ну или наоборот нам пришлось написать собственный инструмент который отслеживает по бизнес показателям количество успешных транзакций в динамике на временном интервале не нашли ничего на рынке я вот именно эту мысль хотел донести что для решение такого про как бы такого типа задач на рынке ничего нет это к вопросу о том как быстро вы ведь проблем четвертая группа 3 группа задач которую мы решаем это после того как мы проблему выявили после того как мы от неё избавились хорошо бы помочь понять причину для разработки для тестирования и что с этим сделать соответственно нам надо исследовать нам надо поднять что логе если мы говорим про логину основная причина как бы патологию основная часть логов у нас елка стек умы всех так практически да ну у кого то может быть не видел к но если вы большой там вы пишете логином гигабайтами то вы рано или поздно придете к телкам и пишем их терабайтами здесь есть проблема мы пофиксили про мы исправили для пользователя ошибку начали раскапывать что же там было такое залезли в киба ну вели там айдишник транзакции получили вот такую картинку в этой портянки не понятно ничего ровным счетом почему да потому что непонятно какая часть относится к какому квартиру какая часть относится к этому компоненту и в этот момент мы поняли что нам нужно трассировка то самый up and racing про который я говорил подумали мы это год назад посмотрели в свой взор в сторону рынка и там оказалось два инструмента это zip кен это егерь zip кину егеря то по факту такой диалогический наследника идеологически продолжать визитки на визитке не все хорошо кроме того что он не умеет агрегировать ну не умеет включать в трассировку логе это только трассировка времени а егерь это поддержу смотрели на егеря можно инструмен тировать приложение можно писать вопи стандарта петров для печки на тот момент был нет не утвержден это год назад сейчас уже утвержден но абсолютно не было клиента муки подумали мы и написали собственных клиент что у нас получилось вот примерно так это в виде выглядит в игре создаются на каждое сообщение с планы когда пользователь наш открывает систему он видит один или два блока она каждый на каждый входящий запрос там 12345 насколько входящих запросов от пользователю было столько блог для того чтобы пользователям было легче мы добавили к тегам и к тренировке временной мы добавили теги блогом и дрессировки временной мы добавили теги соответственно наше приложение в случае ошибки она про маркирует лог соответствующим тегом р если чуть-чуть увеличить понимаю что плохо видно наверное но видно что можно отфильтровать например по тегу р и выведется только те span и которая содержит этот блок с ошибкой вот так вот это выглядит если мы развернем span внутри spano есть на борт рейсов в данном случае это три тестовых от рейса и 3 trace говорит нам о том что произошла ошибка при этом здесь же мы видим временную трассировку у нас сверху временная шкала и мы видим на каком временном интервале у нас тот или иной лоб был записан соответственно у нас это зашло отлично мы написали собственно и расширение и моего за о консорциуме если вы захотите работать с дрессировка если вы захотите работать с егерем на языке php есть наше расширение vl continues как говорится у нас это расширение клиент для работы с up and racing apk сделано как печка extension то есть вам надо будет собрать и подложить систему год назад не было ничего другого сейчас появились еще другие клиенты которые как компоненты здесь дела ваши до либо вы composer ого к тете компонент либо вы используете extension опцию про три заповеди поговорили четвертая заповедь от стандартизировать подходу про что это но это примерно вот про это почему корпоративное здесь слова не потому что мы там большая люблю критическая компания нет корпоративное слова я хотел здесь употребить в контексте что у каждого у каждой компании у каждой у каждого продукта эти стандарты должны быть свои у вас том числе какие стандарты есть у нас у нас эти стандарты это у нас есть регламент диплом мы без него никуда не движемся не можем диплом сям и при этом порядка 60 раз в неделю то есть у нас тепло и происходит практически постоянно но при этом у нас есть например в регламенте тепло и вто будет тепло и в пятницу вот в принципе мы не цепляемся у нас обязательно документация ни один новый компонент у нас не попадает в рот если меня нет документации даже если это рождена под пером наших ран дачников мы требуем от них инструкцию по диплому карту мониторинга и примерное описание ну как программисты могут написать до того как это компонент работает и как с ним как его trouble суть мы решаем не причину проблемы а проблема то про что я уже говорил для нас важно оградить пользователя от проблемы у нас есть допуски например мы не считаем downtime а если мы на протяжении 2 минут потеряли 2 процент трафик ну просто в принципе это не падает нашу статистику если больше либо в процентном соотношении если во временном мы тоже считаем и мы всегда пишем паспортом чтобы у нас не случилось любой любая ситуация которая повела себя нештатно на продакшене она будет отражено вкус мордами пост мартом этот документ в котором вы пишите что у вас произошло подробный тайминг что вы сделали для исправления и обязателен блок что вы сделаете чтобы такого не допустить в будущем обязательно очень необходимо для последующего анализа к чему это все привело это все привело к тому что у нас были проблемы определенной со стабильностью она не устраивал не клиентов не нас за последние шесть месяцев наш показатель стабильности 999 в принципе можно сказать что наверное это не очень много данном есть к чему стремится из этого показателя примерно половина этапа стабильность как бы не наша нашего веб пульки что firewall который перед нами стоит используется как сервис но клиентам на это все равно мы научились спать по ночам наконец-то полгода назад мы не умели и вот здесь вот на этой вот ноте с итогами мне хочется сделать одну ремарочку вчера вечером был замечательный доклад про система управления ядерным реактором вот если меня слушают люди которые писали эту систему пожалуйста забудьте про то что я говорил про 2 процента это downtime для вас 2 процента это downtime даже если на 2 минуты ну на этом все ваши вопросы нам вот там там там вот мужчина сходит рукой добрый вечер спасибо большое за такой админский доклад вопрос буквально коротенький на тему ваших балансировщик of вы обмолвились о что у вас есть вов то есть как я понимаю в качестве балансировщика вы испортить как какой-то внешней нет катер балансировщика мы используем свои сервисы в данном случае вов для нас является исключительно инструментом защита anti ddos чтобы русле фан а балансировщика как я уже сказал это группа серверов опен-рейз те их у нас сейчас 5 групп резервированных которые отвечают но исключительно просто сервис сервер на котором стоит исключить только окон расти он только проектирует трафик спасибо соответственно для понимания сколько мы держим мы сейчас у нас сейчас штатный поток трафика это несколько сотен мегабит они справляются им хорошо даже не напрягаются нужно еще простой вопрос от blue green deployment а что вы делаете например с миграциями из базами данных хороший вопрос смотрите мы blue green для памяти для очередей у нас есть отдельные очереди на каждую линию то есть если мы говорим про очередей сама череда событий который передаются от worker и квартиру там отдельной очереди на блюли new in a green линию если мы говорим про саму базу данных то у нас мы нарочно ее сузили как только могли переложили все практически в очереди в базе данных у нас столько стоит транзакции хранится и стоит транзакции у нас един для всех линий с базой данных в данном контексте мы ее на blue green не разделяем потому что оба варианта кода должны знать что происходит транзакции еще друзья у меня тут еще такой есть маленький приступ вас постигнуть у меня тут книжка и мне и надо вручить за самый лучший вопрос вот тут руку еще поднимает здравствуйте и спасибо за доклад и вопрос такой вы мониторите платежи вы мониторите сервисы которыми вы общаетесь но как вы мониторите так что вот допустим человек каким-то образом пришел на вашу платежную страницу и например сделать сделала плату а то что проект ему зачислил деньги то есть как вы мониторить то что мерчант доступен и принял ваш call back мир чем для нас в данном случае является точно такие же внешне по внешним сервисом как и платежная система мы мониторим скорость ответа мерчант вот там вот молодой человек попрошу прощения чуть чуть раньше руку поднимал если можно ему микрофон прошу прошу 1 раз здравствуйте у меня немножко чуть-чуть рядом вопрос собственно у вас в чувствительный данный подпись один сайт хотел узнать как вы в очередях храните планы в которую ним вам необходимо портировать использовать его шифрование какое и отсюда вытекающей второй вопрос по 50 с необходимо периодически перес шифровать базу случае изменять там увольнение админов и так далее как в этом случае происходит с доступностью замечательный вопрос во первых мы в очередях не храним панель мы не имеем право хранить пан вообще где-либо в открытом виде в принципе непомнящий поэтому мы используем специальный сервис мы его называем кей демон это сервис который делает только одно он принимает на вход сообщения и отдает сообщение зашифрованы и мы вот этим зашифрованным сообщением как бы это все и храним соответственно там длина ключа у нас вот мегабайт вот чтоб это было так прям серьезно и надежно он не под мегабайта килобайт прошу прощения до два надо сейчас уже 2 килобайта надо вроде недавно еще было 256 на куда уже соответственно это первое во вторых вот это вот решение которое есть она поддерживает процедуру перед шифровки то есть там две пары кеков которые дают деки которые зашифровывают какие-то ключи но какие-то производной от ключей которые зашифровать и в случае инициации процедура процедуру это мы проходим регулярно там от трех месяцев до там плюс минус каких-то вопрос загружаем новую пару кеков у нас происходит перед шифровка данных у нас есть отдельные сервисы которые все данные выделяются пережевывают по-новому и до ударных рядом хранится идентификатор ключа которым они зашифрован соответственно как только у нас все данные пришел на новыми ключами мы старые кучу дали то есть если пришел не за этом возврат по какой-то операции да то вы все равно его шифру типа к старым ключом крайне при сюда и тогда еще так и может маленький вопрос когда происходит какой-то сбой падение инциденты да и необходимо протолкнуть транзакцию в ручном режиме бывает такой ситуации бывают вот откуда вы берете эти данные и или вы сами ходите вы ручками водохранилища нет ну понятное дело у нас есть некая бы кофе система которая содержит на трофей со для наших для нашего саппорта и если мы не знаем в каком статусе транзакция мы не знаем например если вот платежной системы тайм-аут мне ответила мой априори не знаем то есть мы финальный статус присваиваем только при полной уверенности то в этом случае на транзакцию сваливаем специальные статус на ручную обработку с утра там на следующий день как только сахар получает информацию о платежная система остаться с таких транзакции они вручную их обрабатывают в этом интерфейсе спасибо да у меня такие пару не пару вопросов вот один из них продолжения зона писи одессы соответственно как вы носите логе из контура это такой вопрос потому что разработчик влоги мог положить что угодно это первый вопрос второй вопрос как вы выкладываете хотфиксы ну вот ручками в базе это один вариант но могут быть какие-то фриков хотфиксы и какой там какая-то процедура и третий вопрос наверное связан с ртр по у вас доступность на было 97 порядке четыре девятки но я так понимаю что у вас там и второе дата-центры 3 дата центры 5 дата-центр как вы занимаетесь их скажем так синхронизации репликация my всем остальным прибыли вот у меня три вопроса давайте начнем с первого первый вопрос прологе был да да у нас когда пишутся логия у нас стоит прослойка которая все сенситивной данные маскирует она по маске смотрят и по дополнительным полям соответственно на слоги выходят с уже замаскированными данными из контура пися dss это одна из регулярных задач которые вменена дело тестирования они обязаны каждую задачу в том числе проверять на theologie которые они пишут и это одна из регулярных задач при code review для того чтобы контролировать что разработчику да чтоб не записал это первая последующая проверка этого осуществляется и регулярно отделом информационной безопасности примерно раз в неделю выборочно логе берутся за последний день они прогонять через специальный сканер и анализаторы тестовых серверов чтобы под проверять это все второй вопрос а второй вопрос про хотфиксы был проход фиксы это включено у нас в регламент дипломов у нас отдельно вынесены пункт про хотфиксы мы считаем что мой ход fix диплом круглосуточно тогда когда нам это надо то есть как только версия собрана как только она правда на как только у нас есть артефакт у нас поднимается дежурно стями администратор по на звонку и саппорта и он делает это в тот момент когда это необходимо и про четыре девятки вот 1 цифра которая у нас сейчас есть по-настоящему она была достигнута и мы к ней стремились ещё на 1 до центре если что сейчас у нас появился второй этот центр мы начинаем роуч между ними и вопрос кросс до центра репликации это действительно вопрос нетривиальные мы пытались его решить свое время с разными средствами мы пытались использовать например тот же тарантул у нас не зашло я сразу говорю поэтому мы пришли к тому что мы делаем закат солнца вручную у нас каждое приложение по факту в асинхронном режиме необходимой к синхронизации чанга данных гоняет между доцентами ну если уж у вас появился второй то почему не появился третий потому что сплит brain еще никто не а у нас нет спид брейна из-за того что каждое приложение догоняют нас multimaster вот нам не важно в какой центр пришел запрос да мы готовы мы готовы к тому что в случае если у нас один дата-центр упал мы на это закладываем сяп основан на данный момент что если нас один дата-центр упал и в середине запроса пользователя пользователя переключил на 2 дата-центр мы готовы потерять этого пользователь действительно это единица будут абсолютной абсолютно единицы спасибо добрый вечер спасибо за доклад здесь вы рассказывали про свой дебаггер который продакшене гоняет некий тестовые транзакции на аккорд а вот расскажите про тестовой транзакции как глубоко она уходит она проходит полный цикл всего компонента то есть для компонента нет отличия между тестовой транзакции и боевой транзакции а с точки зрения логики это просто ну некий отдельный проект в системе до на котором только тестовый транзакций гонять где вы отсекаете город мы законом в данном случае для тестовых транзакции у нас есть такое понятие как роутинг core знает какую платежную систему надо отправить да мы отправляем фейковую платежную систему которую просто вот мы там кота подбивку дает сразу и все спасибо можно просто скажите пожалуйста у вас как приложение написано одним огромным монолитом либо вырезали его на какие-то сервисы даже микро сервис у нас шутка по этому поводу у нас не монолит конечно у нас сервис ориентированное приложение у нас шубка что нас сервис из монолитов они действительно достаточно большие это микро сервисами язык называется не над ним не поворачивается назвать от слова совсем но это именно сервисы кота внутри которых вот работают worker и worker распределены по машин тогда у меня следующий вопрос даже если бы это был monolids рынок вы сказали вас есть очень много инстансов этих серверов все они в принципе обрабатывают данные и на вопрос такой в случае компрометации одного из инстанса в этих серверов либо приложение какого-либо отдельного звена у них есть какой-то контроль доступа кто из них что может делать кому обращаться за какими данными за га несомненно требования безопасности они достаточно серьезные во первых у нас открыты движения данных и порты только те по которым мы заранее предполагаем движение трафика если компонент общается с базой данных emc мускульным по пять четыре три два и мы будет открыт только пять четыре три два и другие порты другие направления движения трафика они не будут доступны кроме этого надо понимать что у нас в продакшене существует около десяти различных контуров безопасности и даже если скомпрометирована было приложение каким-то образом не дай бог то пользователь минуту мы шли ник не сможет получить доступ например консоли управления сервисом сервером потому что это другая зона безопасности стелла мне в данном контексте больше интересен момент что от вас же есть некие контракту сервисов что они могут делать через какие action они могут возвращаться к друг другу и в нормальном пол какие-то определенные сервисы запрашивают какой-то ряд перечень action of на другом другим они как бы не обращаются в нормальной ситуации и они как бы с другие зоны ответственности у них если же один из них будет компрометировал сможет ли он дернуть экшн и другого с этого сервиса дергать нормально связи я понимаю если в нормальной ситуации с другим сервисам сервером коммуникация вообще было разрешено то да по swagger контракту мы не мониторим что тебе разрешены только первые три экшена от 4-4 экшена тебе не разрешены это наверное для нас избыточно потому что у нас и так четырехуровневая система защиты в принципе да контуров то есть мы предпочитаем защищаться контурами а не на уровне внутренних внутренность от спасибо да я хотел чуть уточнить момент по поводу переключения пользователь из 1 этот центр на другой просто ну насколько я знаю виза и mastercard работы по бинарному синхронному протоколу су-85 83 да там стоят медуз и вот и хотел узнать вот сейчас имеется виду переключения это именно до непосредственно visa mastercard или все-таки до платежных систем но для до процессинга этот доме псов mips оставят нас в одном в центре то есть много боря если есть у вас одна . выключен если один хост довези и master card да просто потому что виза и mastercard требует достаточно серьезные вложение в инфраструктуры в заключении отдельных контрактов для получения второй параметров например они они резервированный в рамках одного дата-центра но если да если у нас и дай бог сдохнет дата-центров которым стоят микс для подключения виза и mastercard одну связь виза и mastercard матери серверу насколько я знаю виза разрешают только один connect принципе держать они сами поставляют оборудование случае нам пришло оборудование которое внутри железно резервированный ну то есть стойка александр счет как в этом случае если вас это центр попадает как вы дальше пользователи просто останавливается нет мы в этом случае просто переключим трафик на другой канал который естественно будет дороже нам естественно будет дороже клиентам но трафик пойдет не через нашу прямое подключение к визе master card об чем не знаю через условный сбербанк учу интрига и поймем спасибо еще все к сожалению вынуждены закончить но последний вопрос я думаю будет да можно еще раз повторить о нем услышал я дико извиняюсь если я действительно задел сотрудников компания сбербанка но по нашей статистике из российских банков сбербанк падает чаще всего не проходит месяц и чтобы сбербанк еще нибудь не отвалилась"
}