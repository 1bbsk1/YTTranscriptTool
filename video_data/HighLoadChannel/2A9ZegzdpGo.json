{
  "video_id": "2A9ZegzdpGo",
  "channel": "HighLoadChannel",
  "title": "Платформа для Видео сроком в квартал / Александр Тоболь (Одноклассники)",
  "views": 649,
  "duration": 2741,
  "published": "2017-04-22T14:46:45-07:00",
  "text": "здравствуйте так меня уже представили меня зовут александр я разработчик компании одноклассники и сегодня я вам расскажу как компания одноклассники пошла нестандартным путем и реализовала свое видео платформу на java и сделать достаточно успешно поехали значит так что такое сейчас представляет собой видео на одноклассниках ну во первых видео на одноклассниках доступна с десктопов с мобильных устройств это ну видеосервис стандартные социальной сети единственным таким интересным аспектом на видео у нас является наличие видео витрина в отличие от других соцсетей у нас есть стопы новинки у нас есть каналы с партнерским контентом и много всякого интересного так же как и на других социальных сетях у нас доступна пользователям загрузка видео у нас есть такая штука как пользовательская модерация видео то есть пользовательское видео попадает в топ и в каналы только после прохождения модерации собственно варя это нам позволило их построить чистыми вот там не знаю каких-то извращений ну дальше пользователь соответственно просматривают это видео такой стандартный видеосервиса youtube теперь текущие нашей аудитории объемы к нам в день у нас 10 миллионов уникальных смотрителей видео 60 миллионов просмотров видео в день ну и в день к нам пользователи загружают порядка 50 тысяч новых видеороликов чтобы так понимать наши объемы сейчас в нашей базе содержится порядка 30 миллионов видеороликов исходящий трафик спеки час пик у нас среднем до 100 гигабит доходит пользователи к нам загружают в день в пять терабайт нового видео мы его пережимаем свой внутренний формат получаем порядка двух терабайт на выходе сохраняемся и плетей шин фактор 3 и того + 6 терабайт видео каждый день на одноклассниках собственно варя на чем все это работает у нас swanage видео парк состоит примерно из 200 серверов ну вот основные группы и примечательны на слайде это хранилищем хранением пользовательских данных у нас занимается 70 серверов сейчас суммарный объем по данным порядка пяти петабайт дальше у нас есть базы данных и каши для хранения метаданных парк трансформеров еще 60 серверов которые пережимают пользовательское видео в удобный наш внутренний формат и еще порядка 30 серверов занимаются раз загрузкой польского видео и раздачи обо всем этом детально сейчас рассмотрим так история с чего это все начиналось ну где-то в январе 2010 решили делать видео на одноклассниках изначально у нас не было свои видео платформой пользователи могли только постить видео расположены на других видеохостингах впоследствии мы запустили видео в одноклассниках поверх дружественно на mail.ru group и и как бы сервис стал расти у нас появились новые требования к сервису и где-то в январе 2013 года мы поняли что без своего видео сервис она мне как и решили разработать свою видео платформу новая платформа собственно говоря и вот требования к ней основная основное требование как и ко всему портала у нас это надежность надежность как при отказе сервера так при отказе дата-центров также понятное дело что мы хотели возобновляемую загрузку видео для пользователей сейчас много полиции с мобильными устройствами они загружают у них пропадает сеть они в метро еще где-то они возвращаются к нам через несколько часов и в бэкграунде загружают свое видео ну собственно говоря воспроизведение видео с любого момента времени без предварительной буферизации ну тот самый видео псевдо streaming который на предыдущем докладе рассказывали всем и это хотели видеть на мобильных устройствах ну и также так весь портал одноклассники там на 99 процентов написан на java мы очень любим java мы умеем эксплуатировать java сервисы и требованием платформе тоже было максимально использование java посмотрим как выглядит наш видео конвейер изнутри пользователи загружают к нам видео на некоторое оплот сервер мы его дальше будем от стрелочки вверх обозначает на диаграммах это видео складывается во временное хранилище после завершения загрузки она нарезается в удобный на внутренний формат и дальше download сервер мы его стрелочки вниз обозначим уже раздает видео пользователя вот такой конвейер ну будем есть этого слонам по частям так загрузка что мы хотели хотели возобновляемую загрузку хотели отказоустойчивость при вылете дата-центров серверов хотели гарантий обработки видео если полить загрузил нам видео целиком то рано или поздно оно обработается если он загрузил половину видео то она будет какое-то время хранится мы его не потеряем так тут можно было бы конечно мне возразят можно было бы использовать и джеймс у него есть модуль по загрузке но engine сработает с файлами и конечно можно было что-то докрутить но наше требование было такое мы хотим делать репликацию сразу же то есть если у нас пропал сервер пользователь в процессе загрузки вы за продолжу загрузка на другом сервере и данные не будут потеряны давайте посмотрим что у нас получилось из загрузки на примере работы так загрузка значит копается и нажимает добавить видео на портале мы ему водоем некоторые линк на загрузку которого есть некоторые домена или дальше через наш dns джессел by сервер но дже су бин она знает global service лот balancing такая штука которая позволяет вам рисовый доменное имя в айпи и при этом учитывает загрузку по дата-центром их доступность соответственно этой штукой вырисовываем вайпы одного из дата-центров дата-центров на картинке 3 мы обычно работаем с тремя но у нас их несколько больше соответственно дальше мы разрезал велись в айпи один на данной картинке и дальше внутри каждого дата-центра у нас стоит тсз с ему еще прячемся сервера в принципе что нам такая штука дает если мы теряем один сервер у нас лвс просто перебрасывать нагрузку внутри дата-центра на другой сервер если теряем дата-центр целиком the dns обе перестанет резолвится эта дата центр и пользоваться есть рано или поздно загрузку в другом дата-центре все бы здорово но такой вид балансировки подходит только для стоит лиц сервисов мы посмотрим как у нас эта штука получилась дальше помним что мы хотели обеспечить отказоустойчивость на уровне сервера или дата центра причем на частичной загрузки то есть предположим сценарий пользователь грузит видео мы его сразу же сохраняем блоками наши распределенное хранилище по разным дата-центром случае потери дата-центра серверов дата-центре или еще чего-то пользователь перебрасывается при помощи вот того механизма балансировки на сервер другого дата центра и он восстанавливает сессию по состоянию в нашем распределенным хранилище таким образом если даже у пользы готовился к нам к нам вернулся через 6 часов мы возобновим загрузку если мы потеряли дата-центр мы тоже возобновим загрузку если у кости творился коллег здесь будет еще проще ему не нужно будет восстанавливать состоянии из распределенного хранилища он просто продолжить загрузку на сервер так и что мы в итоге получили мы получили отказоустойчивость по дата-центром по серверам мы почти не писали код то есть наш загрузчик работает над там кати а все балансировку за нас делает джессел двс так рассмотрим дальше следующую часть видео мы загрузили наше временное хранилище как реализовано временное хранилище я расскажу попозже это маленькая копия нашего постоянного хранилище преобразование значит что мы хотели от трансформации мы хотели преобразовать видео внутренний формат который можно будет воспроизвести на различных браузерах различных платформах требования к нам конечно стояла подержать различные качества там порядка 4 ну и как всегда отказоустойчивость масштабируемость стандартной требование поехали начнем с выбора видео формата посмотрев на таблицу используемости браузеров и платформ android ios понятно что отделаться чем-то одним нам не получится поэтому наша платформа должна поддерживать как и html-файл псевдо streaming там через байт range так и flash хорошо теперь выберем внутренний формат если посмотреть что поддерживает flash и html-файл собственно говоря выбор невелик гарантированно нам нужно поддерживать mp4 h264 12 audio codec для того чтобы это было доступно как слышу так и html5 ну хорошо будем преобразовывать в этот формат так собственного ряд дальше дальше вот у нас есть транскодер парк транс кодиров у нас порядка 60 машин пользователь через оплот сервер полностью закончил загрузку во временное хранилище ну и дальше его нужно просто раздать нашим transformation сервером на трансформацию для этого мы используем некоторую очередь требование к этой очереди понятное дело что мы должны нарезать видео только на одном трансформеры одно видео и мы еще хотим обеспечить отказоустойчивость дело в том же transformation сервером могут вылетать + спа запущены на них тоже может быть нестабильным еще что-то сеть дата-центра опять все и так далее букей как мы делаем пишем очередь transformation сервера приходят в эту очередь если у них есть свободный ресурс забирают задачу скачивают задачи временного хранилища и постоянно пингует наша очередь статусом говорят что я нарезаю так а это видео в ответ на этот статус мы всегда возвращаем возможность продолжения нарезки для чего мы это делаем если у нас пропадает transformation сервер то другой сердце тоска через некоторое время возвращается в очередь другой сервер забирает эту задачу начинают ее нарезать и тут неожиданно появилась сеть или восстановился the transformation серых который раньше нарезал делал эту задачу так как мы хотим исключить возможность параллельно резки он приходит в очередь говорит буду продолжать резать эту тоску а нам говорит нет не надо вот он берет другую задачу ну собственно говоря вот такая простая вещь все как вы прекрасны и здорово но у нас появляется single point of all your а это наши transformation tool тут конечно все скажут что есть много различных решений для формирования распределенных очередей но это новая инфраструктура и мы нас до этого не было никакого опыта эксплуатации распределенных очередей мы решили сделать некоторую хокку мы позвали вот этого дядьку слабо daisuki пера и решили сделать очередь на лидере ну наверно многие из вас слышали лидер election да то есть это у нас есть механизм который позволяет какой-то распределенной системе выбрать основной сервер и в течение какого то времени все сервера эту систему придут консенсуса что лидера ты лидер он знает что он лидер и все клиенты знают что он тоже лидер вот собственно говоря нам за кибер позволяет это сделать в зуке перед стандартных примерах есть решение для лидер lektion он прямо под него заточен если кто-то там чуть-чуть знаешь такой за кибер там sequence of мерной но до используется они выстраиваются все сервера в очередь на друг дружку подписывается все прекрасно посмотрите пример если будет нужно все здорово но состояние очереди мы не реплицирует то есть во первых в какие здесь плюс и у нас нет никакой дополнительный структур инфраструктуры для очереди мы таранные на серверах бизнес-логики то есть сервис очереди очень простой он может перетекать с одного сервера на другой нас есть дополнительно вот этот вот список кандидатов на перри и собственно что у нас произойдет если пропадет лидер соответственно звуки pernod обеспечит лидер олег шим через какое-то время я засекал но это меньше чем через пол секунды то есть там через 300 миллисекунд все уже сервера у нас приходит консенсусу ну по правилам звуки пера это будет там порядка двух-трех секунд но происходит это сильно быстрее в реальной жизни приходят к консенсусу по поводу нового лидера все здорово но этого лидера нету состояния очереди но состояние очереди у нас есть во временном хранилище и все сервера время от времени пингует своим статусом нарезки мы делаем небольшой период задержки ретенция там порядка 10 секунд на котором мы обходим наше временное хранилище и слушаем все сервера и через 10 секунд наша очередь полностью готова к работе и мы возобновляем начинаем раздавать задача таким образом мы здесь немножко подходили заварка раунде ли в общем на самом деле штука с лидер lektion достаточно полезно иногда позволяет что-то как бы иногда обойти так собственного ряд трансформация после того как это все видео успешно доберут добралось до transformation сервера что он делает он скачивает видео временного хранилища считает для входных данных мт5 проверяет что у нас точно такого же видео уже нету если есть просто ленку it ничего больше не делает если нету то мы нарезаем нарезаем ff бегом выбрали моего как наиболее всеядный tool принципе это единственной нативная штука не нажали которую мы используем во все видео платформе нарезаем и как я уже говорил в mpeg-4 h264 нарезаем 4 качества нарезаем картиночки извлекаем различные mp4 теги mp4 парсером основные грабли которые здесь были но это поворот видео с мобильных устройств матрица поворота то есть мы смотрели ее переворачивали видео еще как-то ffmpeg достаточно важная штука иногда съедает всю память иногда улетает ну основные все работы были именно строки нгам процессы нарезки все эта штука тоже написано на джаве это как некий wrapper вокруг ав ав ампера дальше еще интересный грабель на которой мы наступили это коррекция длины оказалось что в пятнадцати процентах вид загружаемых нам видео длина в заголовке не соответствует реальному длине видеопотоков так как мы дальше используем эту длину нам необходимо было ее скорректировать так собственно говоря поставок наш transformation сервер все это нарезал он складывает наше хранилище у нас есть такое мега хранилище но у нас используется просто повсеместно временное хранилище у нас она же постоянно хранилище она же это one blob старридж его уже на холоде презентовали в 2011 году если я не ошибаюсь вам раздавали такие книжечки с презентациями там я сегодня утром посмотрел на 348 странице есть детальная презентация по тому как работает убээс сейчас немножко остановлюсь на том что она у нас представляет видео это порядка 70 серверов по тридцать шесть дисков на каждом и ты доктора бат на сумме пять терабайт можете меня перепроверить как работает нашу б.с. каждый диск не некоторые сторож на каждом диске расположены некоторые партийцы вся эта карта дисков хранится в зуке перри клиенту приходя точно знает на каком стороны какой сторож и сохранять данные зависимости от айдида там и где их искать все полностью отказоустойчивого сохраняем с реплики шин фактор 3 при потере дисков берется другой диск вы на него восстанавливаться данные более детально все можно будет посмотреть вот даже по книжке так важно что у нас здесь используется в принципе тот же за киппер кластер что и на очереди наша главный пункт был это максимальный ю с того что у нас есть сейчас в системе чтобы быстро-быстро запустится так дальше следующая интересная часть во всем этом конвейере это раздача видео пользователем download сервер представляет собой такой многоуровневый кэш над нашим временно над нашим постоянным хранилищем постоянное хранилище медленно там жесткие диски серверов много они не самые производительные так собстна говоря какие были требования к раздаче ну понятно отдачи видео с любого момента времени или с любого смещения без предварительного какого-то при каширования на клиенте высокая нагрузка мы ожидали порядка 150 гигабит в секунду в пике и отказоустойчивость то есть все как всегда там один дата-центр один сервер как бы система не должна этого замечать немножко расскажу про сидя в видео псевдо стриминге собственно говоря как хранится видео в mpeg-4 у нас есть ну такая упрощенную модель у нас есть видео и аудио данные для того что пользователь начать воспроизводить видео с любого момента времени без предварительной загрузки всего этого видео видео чередуются видео аудио данных есть опорные кадры вот такие блоки вот дата 1 дата 2 изображены соответственно при нарезке мы можем частоту смены этих блоков указать все бы здорово но чем чаще мы меняем эти блоки тем больше у нас заголовок соответственно предварительно еще на стадии нарезки мы подобрали некоторые некоторые частоту смены чтобы ограничить вот длину блок у нас там порядка 5 6 мегабайт научилась дальше еще такая штука что трансформатор у удобнее всего когда он вам нарезает как 4 разместить мува там в конце почему потому что все смещения на дата 1 дата 2 даты но у него появляются только известны только в конце уже записи всего файла поэтому обычно он его кладёт в конец тут вот для того чтобы обеспечить именно псевдо streaming нужно переместить начала дальше псевдо streaming на флешей на html5 html5 достаточно умная штука она сама берет мува там пара ситовой если пользователь уже имеет мы в атомы перематывает видео куда-то в середину html5 сам партии данные понимает по какому ops эту нужно получить данные то есть все что нужно поддержать на сервере для работы html5 video player это bad request окей сделаем с флэшем картина чуть чуть хуже flash не понимает мува там поэтому он перри запрашивает данные пин временем секундах star time в параметре url и тут наш сервер должен будет создать новый мува там который является каким-то подмножеством изначально вам его тома есть фактически если вы смотрите видео там s60 секунды то для flash а наш сервер должен отдать полноценное видео 60 и секунды которая полноценно вам играется из полность полностью файл отрезаны от этого места вот такую штуку еще нужно сделать на сервере джинкс как раз и говорили что они эту штуку вроде бы поддержали но вот собственно ряд не наш случай значит что мы получили получили что нам нужно псевдо streaming дальше такая штука исходя из того что я рассказывал что пользователи могут смотреть фильм в начале потом перемотать в серединку в конец и понять что-то вообще не то видео которое нужно и не смотреть его больше то понятно что все что мы хотим запиши ровать это ну наиболее каширу ему части должен быть заголовок потому что заголовок нужно всегда хотите вы смотреть его сначала середины лист за голов наиболее часто каширу и матчасть поэтому мы подумали что им джинкс и его модуль который позволяет нам делать псевдо streaming не очень подходит потому что оперирует с файлами может быть если есть есть ребята из engine к ним не возражал будут возражать но на тот момент нам показалось достаточно сложно его будет допилить под наше решение наше решение было таковым что мы будем кэшировать видео с сегментами ну выбрали такой то размер там 256 килобайт ну и соответственно делаем это все на джаве так смотрим что мы как мы строим нашу раздачу ну это понятно многоуровневый кэш сейчас у нас ramp стоит на этих серверах 96 гигабайт и там ssd-дисков набрано порядка 4 терабайта как это все работает пользователь приходит нам на download сервер мы проверяем если у нас данный в памяти есть из памяти отдаем здорово нету из памяти проверяем на диске нету на диске идем в наше постоянно хранилища и все оперируем главное блоками по 200 56 килобайт есть если пользу и запросил только заголовок моего везде протянем скачаем из временного хранилища в память дальше в памяти у нас реализован url при его тиснение за у нас не такой немножко шумный кэш он не вытесняет на диске все подряд он смотрит количество запросов именно по конкретному сегменту то есть если у вас видео супер супер ход и его все все все смотрят потом перестали вдруг смотреть то при вытеснении блоков этого видео на диск они будут вытесняться все если видео смотрели редко и не целиком то скорее всего вы тесниться только заголовок ну если видео было крайне редко и мы его соответственно выбросим просто в трэш так значит для раздачи мы использовали наш раздающий сервер это open source на я доступно библиотека ванне разрабатывается в рамках одноклассников что там интересного там на джаве есть обертка для не блокирующих сокетов там селектора и там есть готовая их этапа сервер который поверх селекторов не блокирующих соки под вам позволяет писать высоко производительные сервера для раздачи данных с этим ванне на open source на гитхабе можете посмотреть так дальше по тому на чем мы будем реализовывать эти каши ну с ssd дисками крайне просто тут можно использовать все что угодно мы пока используем фифа особо каких-то супер требования к этому не было а вот к работе сразу требует достаточно такие критичные то есть размеры у нас порядка 100 гигабайт сейчас у нас мы экспериментировали с густыми гигабайтами мы хотим чтобы этот эти данные в раме никак не аффект или garbage collector хотим мы там видите у ну и собственно варя запросов но сейчас там порядка 20000 в секунду к этому кашу идет и еще хотели добавить к нему такое требование перри системности то есть чтобы можно было обновлять наш видеосервис раздачи без потери этого каша так чтоб можно было обновить сервисного версию a cash у нас сохранился посмотрев на различные решения мы пришли к выводу что большие объемы там порядка 100 гигабайт требует использования архип посмотрели варианты решения для архип а то есть это там джиной то дайрект буферы memory mapped файлы в джаве вот 2 и 3 пункт не пошли не подошли потому что у них есть физическое ограничение на размер этот тип int и это не больше двух гигабайт нативный код не хотели использовать он так не переносим зато есть интересная штука man сейфе нам позволяет соответственно писать данные по абсолютному адресу читать их оттуда и если к этой штуке еще добавить shared memory то вообще получается здорово но shared memory немножко хитро делаем мы делаем шарик memory файл делаем через закрытые методы map этого файла наши адресное пространство и дальше через инсеев по прямым адресом с ним работаем все это тоже есть в библиотеке ван него тоже все пан source все можно посмотреть так в итоге результат то есть наш ванне а сервер это т п + вот такой that he вот такая штука на джаве конфигурация сервера здесь указано вот там вот циpкa этого сервера 4 мы ставили 10 гигабитных карты наружу 1 10 гигабит ную карту к нашему постоянному хранению данных эффективность каша у нас 80 на 20 но при больших раздачах у меня 90 на 10 бывало вот все характеристики есть несколько упираемся в циpкa на 30 гигабит ах то есть принципе 30 гигабит в секунду с одного сервера мы успешно отдаем дальше у нас проблемы с пол в принципе по джаве можно сказать что все упирается в то что к нашему постоянному хранилищу колбасу если еще обратитесь к докладывай предыдущей презентации там у нас клиента пашмина на несколько неэффективная и стерилизацию прогоняет через sip вот та желтенькая часть использования циpкa это все фактически сирле зация на почти не рано или поздно мы перепишем на нашу библиотеку ванне а доступ к нашему хранилищу я думал что ты там избавимся а вот остальные вещи по раздаче были прекрасные доклады на этой конференции про использование сокетов попробуем использовать зиру окопе вот вот такая штука 30 гигабит она отдает самое интересное что это на джаве giovanni всегда медленно так все здорово но серверов у нас таких в дата-центрах много хотим обеспечить балансировку хотим обеспечить отказоустойчивость можно было применить тот механизм раздачи помните который я вначале рассказывал dns джессел b + с все бы здорово но тогда пользователь за одним и тем же видео ходили бы на разные сервера тогда чем больше серверов у всех но фактически у всех серверов кэша содержал до примерно одни и те же данные мы так не хотим мы хотим чтобы кэш был эффективным понятное дело протекционизм видео по серверам то есть разбиваем видео на partition здесь они обозначены п 0 п к каждому серверу псевдослучайно назначаем партиции понятно дело что у каждой партиции делаем реплики реплики тоже псевдослучайно разбрасываются по всем серверам это на случай отказа то есть сера отказал то вся нагрузка равномерно ложится по другим серверам а каждый серый отвечает только за конкретную партицию если так делать жестко то получается что фактически весь объем суммарный рамка сша равен сумме всех граммов всех серверов все здорово но при потере сервера еще куда ни шло а вот при потере дата-центра нагрузка на наше постоянное хранилище очень сильно возрастает и этапа реально является проблемой для этого мы еще добавить некоторый коэффициент того что для наиболее частых видео палец или ходит ненулевую реплику иногда еще на первой на вторую то есть они прогревают кэш других серверов для того чтобы уменьшить нагрузку на наши постоянно хранить хранилище в случае отказа например целого дата-центра вот ну вот в общем в этом случае мы вынуждены были написать свою балансировку на достаточно эффективно и нам нравится так еще куда у чтобы сидена то есть последнее время все рассказывают просидел ну наверно многие знают да content delivery network у нас тоже есть силен и собственно говоря на них так же работает наша версия раздатчика она отлично портировать кроме того что теперь вместо постоянного хранилище она ходит на наше шоу основную площадку пользователь по айпи адресу отправляется на наиболее ближайшую к нему площадку по раздачи контента площадка соответственно дает ему из сша или ходит на основную площадку за этими данными ну все отличие что место постоянного старриджа у этих площадок у нас main площадка площадке тоже внутри себя позиционировал то есть такие маленькие площадки по два сервера соответственно один сервер дает одну половину видео другой сферы дает половить другу по на видео немножко только прогревает так что у нас по псевдо стримингом не забываем сделать псевдо streaming ну с html5 все понятно все просто а для фарша нужно перестраивать mba там тут мы попробовали и mpeg-4 парсер но это гуглово я штука генерала очень много гарбич а в памяти эффективность была крайне низка ну понятна эта штука такая наиболее общие мы решили написать свою наша реализация получилось там в десятки раз быстрее понятно что мы изначально знали наш формат что у нас там не знаю на видео 1 аудиодорожка и так далее но в итоге тоже пришлось реализовать самим работает эффективно там хедер порядка 3 мегабайт формируется там на сервере там за 4 секунды например и один сервер прекрасно выдерживает порядка там не знают 200 запросов в секунду и в общем-то я не видел какого-то супер аффекта на цепова именно по перемотке так ну собственно говоря что у нас было на проекте то есть вот та история развития нашего проекта вот январе 2013 мы начинаем писать свои видео платформу и к май 2013 мы ее запускаем на весь портал всего три месяца порядка трех разработчиков работают над этой платформой мы считаем что в принципе это успех за три месяца реализовать видео платформу с 60 миллионами просмотров в день эффективно отказоустойчивых что мы используем зоопарк технологии пытаемся по максимуму минимизировать то есть все почти все java библиотека ванне открытой можете посмотреть за кибер на java танк от используем в при загрузке гугловый mpeg4 парсер только для извлечения тегов при нарезке единственная нативной штука f em back это нам не нравится достаточно важное мы ее постоянно tracker смотрим чтобы она не съела все ресурсы нашего сервера нарочно не останавливался на кассандре потому что мета сторож по роликам у нас уже был на момент запуска сервиса вот с ним у нас особых проблем не было мы его не писали в рамках платформы давайте ещё раз взглянем на наш конвейер видео и посмотрим что мы использовали соответственно для загрузки у нас сервер на java запускается на там катя временное хранилище tbs очередь у нас на звуки перед transformation of a fanfic дальше это в постоянное хранилище опять vbs раздача наша библиотека ванне open source на и на джаве можно посмотреть на дисках тоже маленькая версия биоса для кэша то есть минимум технологий работали только с тем что с чем фактически уме результаты добились работоспособность сервиса при отказе соответственно там сервер или дата-центра пользователи теперь могут возобновлять загрузку видео сейчас настроена в течение 12 часов просмотра видео возможность в различных девайсов доступно перемотка как вы во флеше такого html5 с одного сервера сейчас отдаем порядка 30 гигабит в секунду до все это работает на мобильниках круто здорово что ещё можно сделать сейчас у нас в разработке идет html5 player можно дополнительно ffm пегу добавить каких-то модули для работы с какими-то новыми кодеками хотя и так и тату наиболее всеядный ну и мы никак не используя метаданные ролика там например кита gps данные которые у нас извлекаются пока мы их просто храним вот что ещё хотел сказать что из нашего решения ну наиболее стандартные стартапы которые используют для них задача максимально быстро запустится не изобретать велосипеда взять готовые модули из них быстренько собрать попробуйте этот интегрировать и запустить мы как имея какое-то уже экспертизы в работе различных сервисов решили пойти по другому пути то есть решили попробовать написать чуть больше кода в начале но сэкономить впоследствии на интеграции на запуске потому что это наш кот нам с ним легче работать плюс у нас есть очень много различных оберток для эксплуатации джалла сервисов которые нам позволяют собирать статистику много всего интересного вот собственно говоря наверное все можете вас есть вопросы вы сказали что напитки вы выдаете 150 гигабит на первом слайде было сейчас у нас порядка 100 мы ориентировались на 153 разработки down серверов сколько сейчас сильно запасом 12 на моей площадке изначально так как бы железо заказывали до того как написали написали полой провели нагрузочное тестирование то есть изначально рассчитывали на там 150 гигабит соответственно 12 серверов а отказ одного дата-центра гарантированно остается 8 серверов то есть 160 габит планировать на 20 гигабит сервера а получили в итоге 30 то есть ну немножко можно было сэкономить а из каких соображений выбрали вот такую величину 32 бит мы ее не выбрали мы ее получили то есть мы ориентировались на 20 вот ведь и 160 хотели раздать 12 серверов 3 дата центра один отказал осталось 8 серверов 20 gigabit server 30 мы сейчас получили т.е. скорее всего можем получить больше но это пока нам сейчас не нужно это такой как бы теоретический максимум будет я увидел там достаточно дорогие технологии там четырехкратные создана примерами это нечто автор обаятельный ssd это там 8 ssd по 500 гигабайт это не что это не 1 4 то работин ssd от 8 чтобы понятно но тем не менее хорошо понятно а скажите пожалуйста вот очередь на звуки вы рассказали про момент когда переключается на другой сервер это очередь при падении 1 и данные собираются из стен кого хранились или стран со стороны озера а если сообщение задача данного транскодирования была еще в очереди вот момент она пропадает так а может быть чуть чуть не рассказал трансформер стирает задачу с временного хранилища поставок он ее успешно все нарежет и сохранить постоянно хранилище ответственности трансформер не успел этого сделать для нарезки до конца сохранит она все еще лежит во временном хранилище тогда наш оператор обойдет временное и восстановит ее в очередь тогда можно поподробней что такое задача есть что лежит закипели что лежит окей внуки перине лежит ничего кроме того что он занимается лидер олег шина то есть он выбирает сервер на котором будет работать очередь очередь пропала вы были новый сервер что он делает пошел во временное хранилище и тренирует временно хранилище там сейчас остались только те задачи которые либо еще в очереди их никто не обрабатывает либо задачи которые сейчас обрабатываются другие transformation серверами он их все собрал и получил эти тоски теперь он подождал пингов transformation серверов и теперь понял что отсюда у него сейчас реально работает и соответственно получил это очередь на там задержка стоит 20 30 секунд то есть для пользователя после завершения загрузки видео 20 30 секунд при потере дата-центра ну я думаю что это не так много а время у нас публикация среднее для там 100 мегабайт нова ролика сейчас порядка там 10 секунд то есть здесь секунд поставку вы загрузили 100 мегабайт на ролик вы его уже сможете смотреть ваши друзья его увидят просто такое провести за кипром пользуетесь то я не знаю как он отрабатывает нет сплит и то есть если у вас скажем сеть развалится на два адвоката на два куска хан которых там будет по несколько лидеров есть у вас такая ситуация отварить или вы ее когда у вас такой нет сплит я понимаю что наконец сплит или это головная боль про который просто еще не хотелось думать что будет когда у вас два лидера начнут в другом разбираться кто же то кто же кто же как будет командовать хороший вопрос чисто я не могу ответить дело в том что как бы за кибер кластер у нас изначально есть моего эксплуатируем и вот с этой проблемой не сталкивался не спит это сам больная проблема со всеми этими или других лидер легче нами что существует что происходит когда вас два лидера оказывается и вторая штука правильно ли я понимаю что вы часа проводите все вот уже современными штуками то что услуга называется html 5 то есть java script and laughter и которую java скрипта оплотом насколько у вас такая вот ваша ваша статистика потом по тем клиентам которые не могут этим пользоваться то есть если у вас люди которые все еще пользуются там есть и не хотят и что вы делаете с ними то есть про статистику к сожалению не вспомнила у нас есть соответственно флеш алый загрузчик на случай если у клиента html5 не поддерживаться java script анкета проблема есть мы соответственно лидером и дальше либо грузим нашим новым загрузчиком с нагрузкой либо старый flash новый загрузчик да он без загрузки ну я может быть совру но порядке там 5 10 процентов до десяти процентов пояс или есть которой пользуется этим флешом загрузчика то есть может 35 процентов от окно могу собраться и те кто и импульс не могут а то уже совсем кайт исчезающий увеличу сюда хорошо спасибо я тогда вот здрасте значит вопрос такой у вас видео значит позиционируется то есть за каждый каждый видео файлик отвечает какая-то своя группа rip ну серверов вы сейчас кроха про раздачу значит я так понимаю картовод парк портится не равана она у вас как бы ну статическая да то есть там том же звуки перед наверно лежит как то так да то есть я имею ввиду вы как-то предусматривали вот скиллинг дальше серверов то есть да можете добавлять ну грубо говоря еще там у нас скиллинг поддержан отвечу да на этот вопрос понял у нас есть некоторая задержка которую нас настраивается там оно указывается в минутах у нас есть старая версия распределение реплик по старому кластеру мы добавили новых раздать у меня не слышно вернулся и дальше соответственно у нас в течение какого-то времени эти партиции плавно перетекают там некий sing происходит да мы плавно перебрасываем партиции по чуть-чуть перебрасываем партиции соответственно весь роутинг у нас на моей площадке происходит при том когда пользовали только обращается за видео и тут вот наш роутер это пересчитывает спасибо рокер тоже наджиев привет во первых спасибо шизо доклад это пока лучше чем видел этом холоде манере по всему значит вопрос следующий сроки были ограниченной да буквально квартал а поэтому вы сказать очень нудистки все откинули взяли максимальный одесского технологии который ваш собственник использовали а если бы была возможность да немножко вот по использовать что другое было бы немножко побольше время эту какую сторону бы смотрели может быть 1 2 технологии вот да хорошо но понятное дело мы сейчас и активность исследуя мы все-таки хотим внедрить какую-то систему распределенных очередей у себя то есть начать ей пользоваться следующий еще интересная штука конечно мы посмотрели в сторону engine ксо чтобы посмотреть по производительности раздачи что у нас получится но с учетом того что мы хотим кэшировать по максимуму именно блоками если кто-то использовал и джеймс у кого-то есть статистика по железу по трафику кто-то его использовать так как мы то есть когда у вас есть большой сторож который измеряется просто петабайт ами вы kosher уйти данные то есть вам все время ужин часть подтягивать у вас есть кэш вот и сколько вас трафика при этом отдает engine с я бы хотел бы такие данные иметь к сожалению интернете я их нигде не нашел спасибо за доклад вот скажу the engine создает нормальную десятку сервер ну вайпер из поставщик и просто мне волосы еще десяток грубо игры на серверах и вот десятку стоп это создаем видео 10 гбит а это статическое видео либо его потом подтягивать у нас есть это кэш второго уровня грубо говоря то есть у нас есть 100 рад что же большой-большой он кэширует притягивает видосы время от времени горячий потом их вытесняют на уши пам бла бла это не интересно а вот до такой такая интересная нет а если у вас от большой видеофайл дает допустим но не соответствует названию такое часто социальных сетях бывает то есть допустим какой-то там у нас это административно решено а вот та соответствует и у нас возникает проблема что мы не хотим все эти два гигабайта видео себе сейчас подтянуть для того чтобы сделать там 1 сиг то есть мы хотим работать с блоками если вот это вы джинкс можно сделать это было бы здорово у нас только вроде нет у меня вот вопрос собственно говоря по картинке со статистикой там как то все полочками идет статистика это какая-то агрегация период роста часовой или о нет это буквально там не знаю десятисекундной так как ти работать то есть мы просто запускали сервер как бы то есть ну и спасла мой вопрос собственно говоря что вы делаете делаете литую с вечерами ваших видео которые предоставляют это видео у себя на тито страничках на сайтах используют грубо говоря вашу площадку как сторож его ну наверно это не секрет я расскажу что мы делаем мы подписываем дымке мы их x паре тесно если возьмете наш линка и течение 12 часов видео или там 24 я не помню вы его не не посмотрите то этот линк спорится его больше понимание сможете смотреть вот ну наверно это не очень эффективно то есть какие-то боты могут перри запрашивать у нас опять же таки эти ссылки данного пока мы так боимся еще может оказаться вопрос по поводу каширования здесь то есть если у вас такая проблема когда появляется новый контент и принципе его ни на каких серверах еще нет я так понимаю что это глобальный кэш промах и все обращения каким образом вы их redirect стена какой-то там центральный сервер для как а вы про сидим сейчас удаленная площадка или просидит говорю то есть но у вас фактически же контент должен распределить по всей сети дамы гондри находилась новый контент который еще по сети не разошелся игре и пусть вот каким образом вы не пуше мы используем конечно использовали приходит на свой сервер он смотрит что его там нет а сервер идет на моем площадку main площадка подтягивает его из хранилища ну собственно горя первому чайку крупно не повезло ну понятно ну все остальные если это идет трафик со всех узлов то они у вас тут на центральную узел куда на центральный узел соответственно центральная площадка уже подтянет в кэш она будет быстрее отдавать а с удаленной площадке тоже со временем у патент спасибо еще вопросы вся спросить как вы решаете проблему с так скажем неподходящим контентом а что-то неподходящее обычные люди сидят рассматривать каждое видео вас я вначале наверно вам рассказал что у нас есть пользовательская модерация то есть у нас есть сами дарме чает видео как абьюз там да да да там то есть есть некоторые а не как в какую-то карму себе зарабатываю тем что они правильно отмечают там если ты по 10 пальцы решили что это хорошее видео 1 что эту порнографию значит хороший пользователи могут осуществить хорошее видео ну значит она попадет в топ и но соответственно конечно перхоть ну там буквально первые там может быть там 100 кто-то их осматривают какие-то модератора остальное все видео конечно это невозможно смотреть только польской модерация еще вопросы по поводу генерации вот превью картинок то есть вы просто используете первый или последний кадр интеллектуально выбираем то есть мы нарезаем несколько картинок и выбираем каким-то алгоритмом наилучшую картинку которая нам кажется что у неё там лучше соотношение яркости на там не черная там и так далее понятно еще вопрос от может быть избыточно сразу несколько нескольких качествах видео сохранять можете кайт качестве никогда вообще не будет за прошлый никем ну никогда никем наверно это вряд ли но это избыточно но посмотри сравнению с максимальным качеством нарезать еще три качества поменьше это не такой большой вверх от ну да может быть спасибо александр спасибо за доклад"
}