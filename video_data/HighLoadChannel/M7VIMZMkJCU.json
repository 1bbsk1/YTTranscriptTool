{
  "video_id": "M7VIMZMkJCU",
  "channel": "HighLoadChannel",
  "title": "Архитектура ВКонтакте: там, где данные / Илья Щербак (ВКонтакте, VK)",
  "views": 378,
  "duration": 2472,
  "published": "2023-10-06T07:16:08-07:00",
  "text": "ребят Всем привет превращается во что-то невероятное А здесь знакомых лиц Мне кажется больше чем в офисе а можно попросить поднимите пожалуйста руки те кто на хайлоуиде впервые ого ого круто очень а теперь ребят небольшой усложнение Поднимите руки те кто когда-нибудь разрабатывал базы данных серьёзно OK Хорошо тогда я расскажу о чем я буду рассказывать прежде всего мы поговорим о том где мы храним данные данные ВКонтакте с какими проблемами мы встречаемся расскажу несколько баек а потом расскажу может быть что-то еще прикольное что вам понравится Итак сразу хочу сказать что мне как бы доклад не нравился Я хочу понимать когда он закончится так вот будет небольшое интро и потом 6 глав Когда скажу что заметки для хозяйки считайте что мы финалим окей Тогда погнали да о себе Меня зовут Щербак ли я родился учился чему-то научился где-то работал сейчас я работаю ВКонтакте руководителем разработки баз данных и инфраструктуры Наверное это самое интересное обо мне а теперь о команде команда баз данных инфраструктуры что это такое но но мы любим сложные задачи мы любим проекты которые горят умеем их делать умеем и делать срок также с командой нам Иногда приходится чинить падение прода из бара когда вся команда находится в баре об этой истории тоже сегодня Расскажу чуть позже а вообще вот эти герои часть ребят находится в зале часть смотрят онлайн ребят очень круто работать с вами я на самом деле горжусь очень крутая команда Теперь давайте поговорим чем же мы занимаемся Саша добрый рассказывал про технический Вектор команды у нас кроме метрик как скорость разработки поддержка продукта утилизация ресурсов есть две главные фичи это отказоустойчивость и быстродействие на них мы фокусируемся Почему Но потому что ВКонтакте становится все более и более информационным и коммуникационным ресурсам поэтому мы не хотим чтобы пользователь видел 500 на своей странице быстродействие Да потому что это напрямую влияет на потребление контента насколько пользователь может посмотреть клипов может посмотреть видео может посмотреть полезное ему информации Итак чтобы поговорить о том как хранятся данные Давайте сначала рассмотрим общую архитектуру ВКонтакте выглядит она выглядит она примерно так же как любой интернет-магазин который продает Но что надо то и продает а фронтен сервера занимается терминацией htps трафика у нас это enginex кстати у нас есть отдельная команда команде инфраструктуры которая занимается разработкой Джинкс Почему так получилось в какой-то момент админы поняли что они хотят дополнительные функции и мы подумали а не купить ли enginex Plus посчитали количество серверов количество дополнительных лицензий и поняли что проще Как купить команду Игоря Сысоева и мы с Игорем так как поступить не могли даем вряд ли бы согласился поэтому мы начали разрабатывать сами и сейчас это выросло в команду cdn инфраструкцию Андрей привет которая занимается разработкой Quick протоколов разработка дополнительных модулей сейчас работаем над антигидосом на самом деле это не топик сегодняшних сегодняшнего разговора Но я не мог обойти этой стороной а что происходит Дальше http запрос идет на backend где срабатывает логика и происходит некоторая магия и мы забираем данные из наших баз данных вот наша база данных называются движки о них мы поговорим дальше Давайте сначала начнем с цифр всего движкам обращений 250 миллионов в секунду это я не посчитал внутреннее общение между движками Но потому что это сложно и сложно потому что у нас 720 кластеров кластер это отдельный стап какого-то движка для продуктовые задачи Окей сколько нас движков у нас 53 вида базы данных Но на самом деле это не обычные базы данных Их бы не было 53 Наверное это какая-то специализированная база данных которая решает определенную бизнес логику Окей какую часть мы рассматриваем мы рассматриваем ту синюю часть которая не серая Что делают движки Отвечая на этот вопрос самое правильно ответить что они хранят все данные пользователей То есть это фото аудио аудиосообщения сообщения комментарии лайки все подряд они хранят то чтобы вы не запустили на сайт также они занимаются контролем кэширующего слоя занимается распространение служебной информации есть движки которые занимаются тем что определяют сетевую модель и подсказывают пользователи Откуда лучше забрать некоторые трав что напрямую влияет на cdn и на раздачу трафика Окей сразу задается вопрос зачем разрабатываем движки есть технология XS технология Y есть технология K мы их Соединяем и все Казалось бы работает прежде всего мы начинаем говорить про гомогенность эксплуатации что это такое для нас движки выглядят одинаково а для админов тем более движок сообщения не отличается от киеверия сторожа движок таргетной рекламы не отличается от движка лайков они все одинаково индексируются они все одинаково ведут они себя показывают и мониторят одинаково это супер важное устройство второе пожалуй конкурирующая с этим свойство Я считаю это экспертность в разработке когда все идет хорошо это скучно надо чтобы ушло плохо Обязательно все пойдет плохо А когда все идет плохо Мы не хотим терять данные пользователей Мы хотим быстро ответить на вопрос как подкастылить что убрать или Что добавить чтобы восстановить работоспособность как я уже говорил мы часто с командой занимаемся тем что восстанавливаем ВКонтакте от сбоев и прочее немаловажным фактором в плане быстродействия является знание о том как мы храним и какие храним и данные Это позволяет нам эффективно кэшировать эффективно хранить и эффективно запрашивать эти данные отдельным аспектом стоит отметить что у нас общая кодовая база можно сказать это фреймворк на котором строится все движки то есть улучшая сетевой слой вы улучшаете сетевой слой для всех движков Это довольно хорошее свойство Итак как выглядит типовой движок на самом деле он не отличим От любой базы данных У нас есть входящий запрос есть какой-то клуб который занимается его обработкой после срабатывания некоторых таймеров у нас попадает событие в бизнес логику бизнес логики если это овраят события мы записываем начинаем записывать Лог ивенты так называемые мы обновляем состояние мы обновляем состояние метафайлов которые совпадают снапшотом Что такое белок белок это классический writellock только он бесконечный То есть он существует сначала времен если других базовых данных он ротируется то у нас нет у нас он просто режется складывается где-то на бэкап и но мы можем тем не менее его проиграть чтобы Представлять что такое бинлок стоит обратиться к следующему слайду У нас есть табличка здесь есть операции здесь есть User и здесь есть value допустим это движок который хранит голоса и мы хотим знать сколько всего голосов у юзера мы можем каждый раз ходить и просчитывать общее количество смотреть Сколько было добавлено Сколько было отнято и потом вводить сумму но хранить Это довольно избыточно поэтому периодически мы делаем snapshot и мы делаем просто суммирование и получаем общее число значений голосов которое у нас есть и можем его потом забрать процесс это называется индексацией Окей с этим разобрались на самом деле я писал первый пункт Как выглядят движки они выглядят чуть сложнее там чуть больше функциональности и чуть сложнее логика Но базово это выглядит все так и этого хватает для нашего дальнейшего обсуждения особенности движков они у нас однопоточные и Это скорее Плюс То есть это структуры данных многопоточные они сильно проигрывают однопоточным и поэтому мы стараемся делать таким образом чтобы у нас все шло шортировано по треду процессора все прибивается на тред и на нем работает мы умеем хорошо делать решат поэтому Для нас это не страшно далее да здесь стоит отметить что оно со звездочкой Почему Потому что когда нам нужно сделать какую-то сложную операцию распараллелить мы это естественно все Делаем дальше расскажу Каким образом также мы можем асинхронно записывать на диске делать разные штуки Но логика именно прокручивается данные и бизнес логика находится рядом это очень важно поэтому они называются движки нам не нужно никуда ходить чтобы понять кто есть в чате нам не нужно никуда ходить Чтобы понять какая Какая логика будет дальше у этого события дальше стоит отметить что особенности социальной сети в том что чтение больше чем записей люди больше просматривают И это экстраполируется также на движке индексируется У нас все в отдельном потоке то есть делается Fork мы опираемся на копию on Write и дальше пошел у нас индекс и мы параллельно читаем и пишем запросы здесь все отлично также стоит отметить что у нас данные все персистентные но они в основном Почему Да потому что юзер в основном просматривают свежие данные никто не будет смотреть старые фотки никто не будет смотреть старые посты поэтому все скорее находится в памяти и здесь играет роль то что я ссылался на знание о том как у нас хранятся данные охраняться они специфично для каждого кейса поэтому можем за кэшировать наиболее эффективно и зафиксировать даже именно изменения которые Нас интересуют ещё один момент движков в том что скорость важнее консистентности что это значит но консистентность на чтения у нас все реплики асинхронные они отстающие не важно если пользователь поставил лайк не так важно если другой пользователь его увидеть чуть позже есть примитивы синхронизации такие как барьеры которые позволяют сделать так чтобы мы все-таки получили этот синхронный момент Но это сразу будет замедлять бэкен замедлять нашу систему что нам не интересно OK с движками разобрались следующим компонентом у нас является rpc Proxy что это такое Дело в том что backend как бы кэнд общается с движками он с ними не общается вообще Он общается через rpc Proxy которая выполняет функции по сути маршрутизатора Да прокси знает о топологии связи она знает о таргетах она знает шортировании она стоит локально к бэкенду и занимает всего один хоп для того чтобы быть Достучаться до движка к примеру если меняется топология меняется топология прокси если меняется шергирование меняется шодирование прокси но про это ничего не знает только про актер Окей в чем здесь проблема Проблема в том что каждый ходит в каждого и у нас все бэкэнды неотличимы что это значит Значит что если начинает тупить один кластер то у нас начинают тупить все бэкенды почему но предположим время ответа кластера лайков было две миллисекунды потом он начал проседать у нас начали случаться тайм-ауты и начали случается они 200 миллисекунд Что это значит Это значит время работы воркеров бэкэнда также увеличилось поскольку оно у нас не бесконечно нам приходит предел и бэкенды просто все заняты engines начинает 500 пользователей получает 500 не могут пробиться каши охлаждаются сайт умирает мы все возвращаем начинается рестарт но пользователь не могут опять зайти почему Ну потому что каши холодные на холодных кашах мы не работаем Окей что с этим делать примерно происходит такая история когда мы взяли и залили все трафом И у нас все полилось надо это как-то контролировать Каким образом у вас возникла идея где-то год назад мы сидели с товарищем За обедом и обсуждали Квик И что там есть Flow Control и канзашим контролы и прочие штуки и возникла мысль А давай с транспортного уровня мы кондэшный контроль поднимем на Application level Поднимите руки пожалуйста те кто знает что такое кондейшн-контрол Ага не так чтобы много Окей это супер простая вещь Представьте трубу и туда идет вода Если вы будете подавать воду мало то соответственно Используйте трубу если будете подавать воды много вы затопите соседей и вероятно платите им ремонт что тоже не круто поэтому ваша задача лить столько сколько туда может пролиться в этом основана вся идея контроля и не переполнение буферов а что мы делаем мы внедрили на rpc прокси механизм когда если у нас сервер становится недоступен мы превентивно отвечаем дропам без ожидания тайм-аута То есть как это выглядит вот У нас есть User это запрос Q4 он пришел в модели conshen Controller для него есть место он отправляет запрос запрос помещается в очередь это очень перемещается в Network очередь и уходит дальше в движки приходит другой юзер и видит что очередь уже занята он делает sent А ему возвращается сразу дискард в этом отличается базовая модель поведения то есть мы разделили модуль контроля на две части на congestion контроль и на сетевую сетевую занимается отправкой и так далее Контрол только контролирует окно Итак мы переходим к понятию окна а как выглядит диаграмма бкнд в данном случае клиент шлет запрос этот запрос транслируется с rpc Proxy на Engine Engine его processic и отправляет ответ вот время между тем как rpc прокси послала запрос и получил ответ называется дилея Мы берем этот дилей и смотрим насколько он соответствует Таргет дилею мы для каждого сервера можем для каждого кластера можем посчитать желаемой Таргет дилей и Смотреть насколько Если дилей меньше чем Таргет то мы окно соответственно сужаем если наоборот то окно будет расширяться базовая идея конечно контроля Итак выглядят примерно так если очередь свободно мы пропускаем если очередь забита мы делаем discard Окей что еще мы получили благодаря конечно контролю Мы также получили Application level баланс балансинг То есть это балансировка без знания разделяемого состояния мы на прокси можем оценить Какое у нас размер окна и в отстающую прокси реплику отправить запрос Итак что мы видим У нас есть одна реплика в которую забили запросы что мы делаем мы следующим движением Просто перекидываем их на другую реплику поскольку окно увеличилось мы это взяли информацию и постепенно все прокси это приняли и без разделения состояния о том что происходит они просто начали слать больше на вторую прокси и это равномерно все раскидывается Итак Каким был учили результаты при стопроцентной нагрузке CPU мы получили утилизацию и утилизация канала серверов мы получили процент отбитых пакетов и процент успешных пакетов но с нужной нам letones это очень важно при замедлении реплики Мы также получаем перекидывание с одной реплики отстающий на другую соответственно менее нагруженную как это выглядит на графиках если раньше мы видели вот эти пилы когда мы врубались в летнсе И она начала начинала просто скакать То сейчас мы видим довольно гладкую картинку то есть мы повышаем повышаем повышаем порог сервер уже перегружен но он выходит на пик и просто обслуживает то что может обслужить Окей с этим думаю Понятно А скажите пожалуйста а кто знает что такое мем кэш Поднимите руки пожалуйста Окей будем считать практически все но можно сделать вывод о том что мы написали свой мим кэш и научили его некоторым прикольным трюкам но например беатрический до умело выбираться из могил Им конечно же тоже умеет что мы сделали Мы туда накрутили жареную память И теперь прио-ме мы не получаем холодного старта минкаша холодный старт мелкоша возможен при нескольких случаях если произошла внутренняя ошибка если пришел ом Киллер если произошло что-то еще если пришел админ если произошел рестарт и так далее если память осталась консистентной на этом на целостность мы ее также проверяем то рестарт будет успешен файле как это выглядит мы написали небольшой локатор сразу скажу что здесь шарит Memory она не совсем шарит она эксклюзивно принадлежит одному процессу поэтому мы не получаем никаких Пенальти на производительность и как выглядит локатор это по сути слаб локатор где у нас есть таблица сайз классов и мы бинарным поиском ищем Какой нам сайт класс подходит бинарный поиск Здесь нам не страшно потому что все дальше у нас есть указатель указатель на Пейдж Как выглядит Пейдж Пейдж выглядит как префикс 512 бит это Да а префикс выглядит следующим образом это 512 бит которые забиты единичками есть специальная процессорная инструкция с помощью которой мы можем вычислять Какая единичка ближайшая и отдавать нужное офсет тем самым формируя смещение то есть мы знаем Пейдж мы знаем офсет умеем вот туда адресоваться умеем уже туда записываться а дальше Пейдж может оставаться на другой Пейдж тем самым мы увеличиваем к опасен число этих пейджей у нас вычисляется при старте и основывается на статистике в итоге мы получили примерно такую картинку И что здесь важно здесь у каждого сайта из класса есть свой лру то Здесь мы получаем фернанс по размеру если мы считаем что больших ключей должно быть много они будут вытесняться быстрее чем маленькие в стандартном счет такого нет Итак общее правило что разделять хорошо и жаренная память нам здесь помогает или Старт Мы переживаем и это большой выигрыш я расскажу еще потом дальше почему это важно Окей что еще мы сделали мимикшом мы его научились реплицировать помните говорил про две задачи задача отказа устойчивости задача производительности так вот это решение которое отвечает обоим задачам мы умеем с одной стороны при падении дата-центра переключаться на другой на геты нас это конечно тоже вот что как это происходит разберем идею с гетами Мы берем для каждого rpc прокси и вычисляем ближайшее расстояние до минкаша статический для каждого дата-центра после того как мы его вычислили мы начинаем туда ходить с большой очень вероятностью как только он выпадает мы начинаем ходить в другой мим кэш в другой этот центр с гэтом если понятно то как реализованы сеты с этой реализованы на механизме часов то есть мы когда Хотим засветить мы шлем везде И после этого вычисляем локально намек кш Было ли Ключ уже с предыдущим клоком если новый клок больше чем старый то мы перезаписываем если меньше то дискорде идея простая Окей Почему мем кэша приедет это важно с точки зрения производительности Ну потому что было вот так когда каждый дата центр ходил в каждом кэш стало вот так я не знаю флаг какой-то страны получился но стало в общем лучше то есть мы видим что красный цвет это локальный зеленый это ближайший к нему и синий Это тот который находится дальше всех Окей дальше хочется поговорить о том что бывало у нас в продакшне бывало разная обычно причина такая с точки зрения разработки первый случай Я думаю здесь нужно объяснять что такое бекон отейшен но мы получили примерно следующее Дело в том что лист сайз по идее в одиннадцатом стандарте должен всегда выдавать от единицы но не всегда Если старый гцц то он не будет ломать abi и будет честно считаться за отн А у нас есть проблема есть сборки которые используют старый Linux Джесси и там старая как мамонта Гц В общем он это использовал на графиках мы это сразу увидели сначала понятно что мы это как-то закачивали в коде А сейчас мы перешли на решение когда подкладываем Новый клипси к старому и указываем нашим функциям С какими лучше линковаться понятно что решение костыльное но контейнеров у нас нет с другой стороны админы обещают гомогенность нам в окружении в этом году и будет везде новый Linux это не может не радовать вторая второй инцидент это когда я обещал рассказать историю как мы пошли в бар Я честно говорю что мы только пришли хотели написать друг другу Кто где есть и поняли что ВКонтакте не работает а дело оказалось в том что один человек не пошел в бар Привет в этом была проблема И он в этот момент решил поэкспериментировать все бы ничего но получилось так что он послал форме пакет Ну это норма но дело в том что в тот момент в нем кэш закрылся бак который там существовал некоторое время И в этот момент пакет стал пакетом убийцей и он положил весь мем кэш у нас ложится мемк кэш у нас холодный старт сделать мы ничего не можем Но Окей потихоньку останавливаемся потихоньку встаем Окей а какие были как это произошло Дело в том что Бага не существовало никогда потом в какой-то момент он появился и в этот же момент короткий произошла выкладка мелкаша народ зачем-то и потом про него все забыли бак уже починен мимо кэш не обновлен И тут решаемся на этот эксперимент идет мой форменный пакетч падает То есть это большое стечение обстоятельств которое произошло что-то было дальше дальше дальше нам дали с бочку естественно поменяли процесс тестирования и выкатки на продакшн и мы начали думать что нам можно сделать мы профазировали сеть Обновили везде критические компоненты и помните мелкая шарит мемори Вот блин не просто так он появился Окей хозяйки на заметку о чем стоит помнить Когда вы разрабатываете высоко нагруженная система а компактность объекта влияет роль не только на сколько памяти вы занимаете Но и на использование каша процессора он дорогой и быстрый его жалко поэтому не забывайте выравнивать объекты сортировка по типу и следить за падингом далее avx-инструкции часто могут дать выигрыш несколько раз Для нас это показало например яргон 2 что он чувствительный к использованию avx 512f и выигрыш идет порядка трех раз также avx используется при текстовых операциях текстовых партингах В общем его использовать можно довольно активно на хэллоуиде было много докладов про эту историю а еще момент что переходы по указателям не бесплатны также как и локации если про локации никому не надо объяснять то в свою очередь мы заменили std-сет на b-дерево и это дало нам выигрыш x 10 раз по этой рации локальность обращения очень важна одно дело когда мы прыгаем по указателям в рендом Access другое дело когда мы бегаем по массивам и работает да дерево место мы умудрились все вершины все информацию служебного вершины сложить в 8 байт А у нас есть некоторые ограничения под дерево но она у нас скорее устраивает Если сравнивать с сетом то мы видим что при б Факторе 32 у нас один страница 48 байта ассет например 40 байт опять вспоминаем про кэш и быстродействие Окей используем устройство указателей указатель у нас 64 Бита про это знают все но используется 48 А что делать 16 А еще есть смещение это можно использовать для тагирирования еще есть Помните у нас бы дерево вершина занимала 8 байт 8 Прошу прощения ее можно тоже туда запихать вот дальше если вы говорите О дата-центрах то Стоит подумать про UTP протокол сейчас есть масса вариантов например Квик который реализует надежную доска в как tcp но при этом он не живет буферы и лучше в плане had of line blooking растущего хвастала интенси важная история у нас например реализован кастомная версия UTP протокола которая Надеюсь скоро будет переработана но это было сделано только потому что раньше не было сейчас Клик есть если у вас много соединений на один сервер подумайте об этом когда записи много Стоит подумать про Россию немного конечно это если мы говорим уже про многопоточку я упоминал что у нас есть сервисы которые используют распараллеливание И в чем идея rsu это когда мы у нас есть структура и её Все читают но когда нам нужны изменения Мы сначала копируем потом апдейтим а потом меняем ей вершины через Кас операторы там есть ряд проблем Мы свою очередь решили отказавшись от счётчиков ссылок для сущности Но об этом будет в другом докладе сейчас вместо заключения мне хотелось бы дать какие-то пару Советов первый этап бенчмарк бенчмарк бенчмарк в жопу всех экспертов доверяйте никому если вы хотите писать на черный код Используйте фазеры и пропит типа тестирование Будьте аккуратны с продакшна ошибаются все главное сделать правильные выводы а еще Берегите здоровье Всем удачи Спасибо просто красавец Спасибо большое спасибо еще и время сэкономил друзья время вопросов ручки в небо а пока вопросик из чатика Почему вы не используете какой-то альтернативу для доступа к базе пока прогревается кэш и если такая возможность вообще какой альтернативу Уточните пожалуйста в Telegram Боте Какую именно альтернативу вот друзья вопросы задаем Дайте пожалуйста микрофон вот юноша и потом вот потом справа от прохода Привет Спасибо за доклад скажи пожалуйста вы для выравнивания трафика использовали как же Control вам не помогли два предыдущих фокуса это и бюджет но бюджет redrive здесь скорее не причем здесь происходит следующее что у нас когда затупливает одна реплика мы просто встаем просто время начинает увеличиваться то здесь не дело в том что Мы удалим ритм Дело в том что у нас среднее время начинает расти Спасибо вот прям рядом с вами Да добрый день спасибо за доклад два вопроса Первый totalocker который вы сделали памяти вы его прямо заменили системный и молодцы только данные персистентные Нет мы используем его только для локатора и только для так и второй вопрос Я правильно понял что у вас все все запросы внутри одного шарда и кроме вообще нету да вот бежит смотри класс раз Приходите сюда вот нам пожалуйста Вопрос У тебя в докладе было сказано что движке работают на сто процентов же это не опасно В случае если будет какой-то скачок по запросу Нет это возможность Окей это не баг старик так у кого микрофон махните рукой вот справа и там же был вопрос где-то Ладно Привет Спасибо за доклад вот вопрос про движки их 53 ты сказал но при этом какая-то очень большая часть общей когда база А нельзя сказать что это как бы ну одно решение с разными настройками они прям чем-то существенно между собой отличается они отличаются внутренним хранением и внутренним представлением объектов У тебя есть разница хранишь Ты пост или хранишь ты лайк хранишь ты фотку или хранишь ты аудиозапись Ну и так далее получается тоже там хранить то есть видео контент тоже в этих движках отчасти видео хранится сейчас другой платформе платформе Одноклассников это платформа единого видео аудио и фото хранятся здесь да видишь вот это худи это значит человек имеет отношение когда у него логотип хайлоу во всю грудь это значит наш человек вот справа Добрый день продвижки можете чуть подробнее под капотом это полностью ваш или там какие-то субдд движки Используйте Нет это полностью наше решение оно тянется с 2010 года поэтому Да полностью наш Захаров Алексей который спросил про альтернативы написал еще одно сообщение если без ход узнал о возможных альтернативах я бы не задал вопрос возможно есть более медленные подходы прямого доступа к базе Пусть медленно но хоть что-то Ну наверное нет я не знаю такого подхода наверное ещё один каши Но это хороший ответ из серии мы пока не нашли достойного решения это правда вот еще вопрос можно ближе микрофон а то мы не слышим Спасибо за доклад очень интересно собственно на Application level и ты рассказывал про конгошина Control а явно ссылаясь на Квик на транспорт соответственно вопрос Вы э в сети есть какие-то оптимизации особенно в шифрования трафика поскольку как бы DP накладывать некоторые ограничения А ещё ограничения от вашего соответственно э решения для того чтобы снизить Lightning В то время как толс достаточно много его добавляет Спасибо Да ограничение я правильно услышал что шифрование теперь шифрования vdp есть у нас весь трафик ходит шифрованный со звездочкой Вот и второй вопрос OK OK у нас не Квик вот такой ответ Он знаешь он как бы намекает что должен прийти на стенд и вы Поговорите поподробнее вот на картинке с архитектора движка данные хранятся прямо движки вы сказали что он никуда не ходит вы на данных вас много вы используете какой-то шарнирование балансировщик знает каким именно движка можно обратиться чтобы достать нужные данные Да здесь Наверное момент который я не явно проговорил этим занимается всем rpc Proxy Proxy знает про шардирование когда ты посылаешь запрос запрос У тебя есть какой-то ключ и по типу запроса RP прокси может определить какой-то ключ то ли это User ID то ли это будет пара User idpress примеру то ли это можно будет взять какой-то строки и зашарировать его каким-то образом в общем этим всем занимается rpc Proxy чатик продолжает смотреть Петр Минаков Фоменко спрашивает каким механизмы используются для прогрева кэша перезаполнение или по факту данными из запросов Но для проблем для прогрева кэша У нас есть Cold Start То есть когда я просто запускается часть пользователей и эти части пользователей прогревают кэш то есть такие партизаны Спасибо И вот финальный вопрос Давайте зададим большое за доклад вопрос Следующий Я так понимаю у вас есть что-то типа балансировки трафика и в любом механизме для балансировки всегда есть обычный мастер тот станет мастером после смерти мастера у вас реализован какой-то механизм Лидер selection Это хороший вопрос наверное эта тема будет следующего доклада сейчас это происходит переключение вручную то есть админ умеет команды хатсвопа умеет команды Смена мастера Но это происходит по команде админа сейчас разрабатывается система по автоматической детекции и переключения в консенсусе о том Кто является мастер Спасибо И теперь к тебе вопрос последний кому подарим то что ты принес А что вот у девушки есть смотри а это значит Ну вот принес Вот ты принес у девушек значит нету у девушек подарок для тебя за какой вопрос отдаем игрушечку кажется самое классный вопрос был про автоматическое Приключение потому что это тема следующего доклада Я думаю его стоит ждать если можно Подойдите пожалуйста к спикеру и Возьмите игрушечку из его рук Да я не вижу просто нам светит два фонаря один отсюда второй отсюда третий это видимо бэкап и поэтому мы очень плохо видим зал тебе спасибо большое памятные призы конференции Илья Спасибо красавица"
}