{
  "video_id": "cr7EziE98P8",
  "channel": "HighLoadChannel",
  "title": "YDB Topic Service: надёжная и масштабируемая очередь сообщений / Ильдар Хисамбеев (Yandex Cloud)",
  "views": 337,
  "duration": 2422,
  "published": "2023-10-06T07:23:00-07:00",
  "text": "а Спасибо за интерес к теме Очень приятно видеть Как заполняется залы на highload меня зовут Ильдар я разработчик в компании Яндекс занимаюсь системой поставками данных сегодня расскажу вам о сервисе очередей сообщений и поставки данных потоковые поставки данных idb topics db topics мы так сокращенно называем более официального дибитопек сервис что же такое в двух словах это наша написанное в Яндексе на платформе vdb решение оно заменила предыдущую предыдущий инструмент для поставки логов предыдущей инкарнации она использовала под капотом а пачкавка но в какой-то момент мы переписали его на платформе сейчас этим сервисом пользуются большинство команд а какие-то цифры по продакшена инсталляции на слайде мы предоставляем много инструментов API для работы в частности доступны в Облаке в API Яндекс Data streams это Amazon кинезис совместимый протокол над сервисом топиков И немаловажно что не так давно в составе всей платформы сервис вышел в мир Open Source то есть теперь каждый может смотреть код экспериментировать внедрять для своих продакшн задач приносить обратную связь и тоже управлять развитием платформы а это первый раз когда мы публично рассказываем Про Сервис топиков и поэтому доклад будет такой ознакомительный обзорный я расскажу для чего мы вообще его создавали Какие элементы Какие архитектурные решения мы принимали по ходу разработки покажу крупноуровнево как из блоков элементов кода собирается действительно Production Ready система Зачем же вообще писать свое решение для каких задач Какие задачи перед нами уже стояли на какие мы смотрели наперед Какие функциональные не функциональные требования ставились перед нами рассмотрим несколько классов задач вообще паттерн очередей сообщений Это такая универсальная абстракция которая много для чего годится Но вот несколько таких основных применений первый наверное по объему трафика это задача поставки логов практически каждый сервис отгружает батчами какие-то объемные Лог файлы в дата варихаус для будущего аналитики и это емкая по трафику задача Кроме этого есть система аналитики в реальном времени то есть Какой пример пользователь например пользовательские клики на сайте нужно максимально быстро получить реакцию на это И тут для систем транспортных важна низкая задержка Кроме этого давно известно из рекомендовавший себя паттерн обмена сообщениями между между сервисами тоже строится на центральном брокере сообщений Это удобно иметь общий протокол общий брокер и подписать Например сразу несколько потребителей на события своего сервиса кроме таких задач которые непосредственно Вот бери и применяй мы задумывали технологию как потенциальный компонент для переиспользования в будущем все части платформы и кода взаимоувязаны в частности вот как пример приведу есть протокол change Data Blitz это пересылка изменений таблицы для поддержания синхронности версии в разных базах и он реализован как раз поверх топиков Какие свойства можно прикладывать к таким задачам Ну первый комплект базовых наверное свойств если мы Передаем сообщение из пункта в пункт Б Мы хотели бы чтобы они не терялись для многих приложений крайне важен в потоке сообщений порядок сообщений Мы хотим настраивать ФИФА порядок То есть если сообщение В каком порядке сообщения были записаны отправителем в таком же они и вычитывались и отдельно скажу про такой Святой Грааль систем передачи данных это семантика доставки exect levance я о ней поговорю чуть больше ниже Мы хотим поддержать на своей стороне дедупликацию при записи в наши топики и обеспечить возможности для публикации Следовательно такого практического экзаклеванс конечного пользователя масштабируемость это свойство распределенной системы при добавлении нового железа новых хостов расти линейно по многим параметрам по суммарному трафику по количеству партиций во все во всей установке В общем по количеству пользователей которые могут одновременно работать с нашим кластером при этом немаловажный такой фактор что мы хотим доращивать свою инсталляцию для Сколь угодно больших размеров там тысяч и более хостов и мы хотели бы чтобы такие гигантские инсталляции конфигурировались обслуживались обменивались таким же такими же силами как и маленький костра чтобы издержки на эксплуатацию суммарно не росли доступность еще один класс класс требований к распределенной системе это сильно зависит от задачи от конфигурации кластера для которой мы строим в зависимости от наших требований мы выставляем какую-то модель отказа если мы говорим про кластер а масштабов Яндекса опять же 1000 хостов множество дисков и от сбои отказы выпадения дисков даже хостов становятся не каким-то экстраординарным явлением о чем ты почти ежедневным к чему нужно Быть готовым Мы в свою модель отказов заносим выход от строя целого дата-центра мы знаем что такие случаи бывают к ним нужно Быть готовым Яндекс регулярно проводят учение подключением дата-центра соответственно наше решение должно автоматически без каких-то ручных действий дежурных переживать это продолжать работать и не деградировать а вот после этого набора требований в совокупности что же может быть критериями для производительности решения по здравому смыслу очевидно что нам нужно замерять суммарно пропускную способность обычно в системах передачи данных меряют срупутно запись и у нас уже на руках задачи которые совокупно хотят закат перегонять через нас десятки может быть потенциал потенциально сотни гигабайт каждую секунду это это байты в сутки Вот и поскольку мы заявились что что мы будем решать задачи для Реал тайм-приложений крайне важна задержка то есть время от того как сервис клиент писатель отправил в нас сообщение и На другой стороне другое приложение его получила для realtime приложений нужно целиться в субсекундную задержку это тоже даем так Ну наверное к этому моменту встает закономерный вопрос на рынке очень много решений которые позиционируют себя как система именно для таких задач и в коммерческом мире его Pen Source первое наверное в голову приходит апачкавка Она кажется наиболее популярна на сегодня Зачем же писать свое Зачем строить велосипед Почему не развивать этот опыт проект скажу так предыдущее решение где-то до 2015 года у нас и использовала Кафка поэтому мы на практике уперлись в какие-то моменты но нужно оговориться развилка для нас и старт своего пути это уже несколько лет назад и Кафка тогда и Кафка сегодня это два совершенно разных продукта Я думаю что многие из вас смогут мне рассказать про встречно про тот путь который прошла Мы тоже следим за развитием проекта с большим уважением потому что Это пример Как нужно развиваться на встречу пользователям но тем не менее речь о тех старых годах в первую очередь мы упирались во внешнюю координацию через на практике это означало что priescale Up выше там сотен тысяч партиций объем мятой информации который прокачивается через гипер в координатор становится слишком большим и кластер деградирует Какие могут быть варианты решения строить несколько специализированных кластеров как обеспечивать какую-то федеративную логику над ними кроме этих Кафка кластеров вас живут азу Кибер кластера отдельно все это нужно поддерживать Это провал на этой набилете Кроме этого на тот момент в тех версиях ноль с чем-то не существовало известных на сегодня фича свойств которые по которой мы опирались по многим конкретным задачам наверное больше чем на этом списке и наверное даже не знаю все когда мы выбираем путь своего решения мы вместе с этим получаем какой-то ряд преимуществ Конечно мы тратим больше инженерных усилий но при этом мы сами управляем тем В какую сторону мы развиваем развиваемся Какие фичи добавляем Какие оптимизации под максимально нужные нам задачу вносим и Кроме того платформы тоже есть стратегия обеспечить под одним зонтиком максимум сервисов которые между собой очень хорошо дружит поэтому переиспользование компонентов из свою очередь дать свои компоненты для переиспользования другими продуктами это часть общей стратегии Давайте поговорим о том как очередь выглядит для пользователя какие-то сущности по каким протоколам клиент действует Когда в соответствии с какими-то своими сценариями хочет писать в топе или читать из топика Итак начнем с самого начала из потому что с песней слова не выкинешь Хотя вот в следующем разделе я уверен многим из вас будет материал сильно знаком Действительно это сознательное решение узнаваемой сущности и узнаваемая паттерны повторить и у себя Итак пройдемся есть сервис который хочет передать сообщение другому использует нашу систему логически он свой Поток Поток данных группирует в именованный топик отправляет читатель читает внутри это топик разделен на партии элементы параллелизма и уже каждая такая партиция которая показана на картинке она представляет абстракцию распределенного Лога такая Лента сообщений сообщений записывается в строгом порядке и учитывается передается читающей стране В этом же порядке сообщение это неделимая информация неделимая кусок пользовательской информации есть какие-то байты пользователя которые мы не интерпретируем и метаданные которые нам важны я Некоторые из них покажу ниже когда сообщение попадает в партицию оно обретает уникальный и возрастающей в рамках партиции офсет по сути возрастающее число порядка и номер как же мы пишем в топик наш протокол потоковый мы устанавливаем двусторонний поток rpc и называем это сессии записи если запись устанавливается ровно с одной партии одного топика И как же выбрать распределить Какая партия будет выбрана для этого есть одно из полей которое устанавливается которая задает клиент при установке сессии у нас она называется Messenger пойти по сути это ключ протекционирования зачем это нужно Если в нашей логике какие-то события должны прийти до потребителя в строгом порядке то мы группируем их в один тогда они гарантированно попадают в одну партию сервис серверная сторона позаботиться о том чтобы не было двух активных сессии установленных в одну партицию с одним и тем же мессендж групп ID старый вытесняется при попытке установить новые и таким образом внутри партиции все сообщения данной группы оседают в строгом порядке на записанное в потоковых запросах данные сервер Понятно возвращает Аки при этом сессии с разными ключами конечно могут писать в одну партицию конкурентно потому что между сообщениями с разными ключами порядок мы не наводим Поговорим немного про экзакт леванс и дедупликацию тоже отступим и поговорим с самого начала что это за проблематика есть много приложений в которых самое важное что для нас есть при доставке это чтобы ни одно сообщение не было потеряно ни одно из сообщений не было доставлено дважды не было бы дублей как добиться первого свойства которое даст нам так называемое от листва семантику поскольку мы знаем что в модели ненадежной сети и ненадежных узлов компонент доступны разного рода сбои моргает сеть Ари стартует процесс клиентский то если Клиент не уверен в том не получил явного акка на то что его порция данных сообщения была доставлена он обязан повторить эту отправку и возможно сценарий когда на самом деле это сообщение дошло второе летит повторно и если с этим специально не бороться они записываются в партицию дважды появляются дубли поэтому вторая половинка решения для exect levance это распознать и устранить дубли для этого мы тоже снабжаем записи еще пары параметров при установке сессии мы добавляем ID продюсера и в рамках этого продюсера клиент все свои сообщения нумеруют каким-либо монотонным последовательности порядковых номеров тогда если сервер получает сообщение с порядковым номером меньше либо равным чем последнее для данного продюсера ID о котором он знает он такое сообщение пропускает это просто проще увидеть на простом простой иллюстрации допустим у нас есть партиция в которой пишет один продюсер он назвал себя X когда установил сессию и начинает последовательно записывать сообщения вот он записал первое сообщение Я на картинке Это обозначаю как Просто пара продюсер X первый номер получил подтверждение понял что может у себя продвинуть порядковый номер пробуют записать второе оно действительно успешно записывается но до того как был получен успешный акк допустим продюсер перезагружается он просыпается и тут Важно обозначить небольшую деталь реализации Что за ID продюсера в нашей архитектуре отвечает сам клиент сам продюсер Поэтому если для нашего критичного к отсутствию дублей приложения мы должны какое-то детерминистический способ восстановить себе ID продюсера после роста обеспечим то нам не нужно обходить эту партицию На чтение чтобы узнавать последние сигналы просто поднимаемся с таким же продюсером и вот на примере повторно пробуется отправить второе сообщение на что мы отвечаем специальным ответом что такое было такое сообщение уже было доставлено можешь спокойно продвигать себе в своей последовательности продвигаться до третьего сообщения ну нужно поговорить о чтении сессия чтения также потоковые rpc который параметризован ID consumera это тоже симметрично но при этом сессия чтения когда устанавливается ко всему топику она получает в общем случае данные сразу из нескольких партиций у нас реализована клиент серверная балансировка то есть серверная сторона динамически перераспределяет партиции топика по всем сессиям которые подключены с данным консер ID естественно Когда речь о независимых читателях которые ничего друг о друге не знают и Уникальны то их Прогресс Их чтение топиков друг от друга не зависят сервер следит за прогрессом и последним вычитанным офсетом по каждому из них когда мы в потоке возвращаем порции данных мы к ним прикладываем офсеты этих данных этих сообщений и по набору топик партиция офсет Возможно дедупликация на стороне клиента уже Мы не знаем как он это сделает Но вариантов много потому что единственная задача которую нужно решить это хранить последние обработанная все для данной партии еще немножко про балансировку зачем она нужна если мы для увеличения своего сруб это открыли несколько партиций то наверняка мы это сделали для того чтобы читать из них в параллель клиенту достаточно легко и удобно просто открыть несколько сессий чтения с одинаковым консуми ради и посмотреть как сервер по ним распределяет партиции Тоже небольшая иллюстрация если одна сессия устанавливается к топику состоящему из двух партиций то она получает данные сразу из обеих если подключается вторая то сервер сам отберет например грейсфуле партицию о первой сессии и распределит между ними Аналогично при выпадении сессии наступит какой-то момент в который из нулевой портится никто не читает и она переподключится к оставшись В общем случае оставшиеся сиротами партиции распределяться по всем оставшимся сессиям тоже равномерно Так мы немножко поговорили про эту модель данных теперь давайте рассмотрим как реализуется свойства которые мы заявили раньше надежность доступность отказоустойчивость в этом мы сильно опираемся на архитектуру idb которая освещалась моими коллегами и раньше в предыдущем докладе поэтому я все еще и сам сервис топиков является компонентом vdb поэтому мне начать нужно с того чтобы кратко попробовать осветить архитектуру всей платформы вот верхнеуровневая картинка которая показывает составные блоки в idb даже на самом крупном уровне Она получилась довольно развесистой видно что на одной и той же инфраструктурной платформе реализованной не только Таблицы с распределенными транзакциями но и наш сервис для потоков данных и к примеру networkblock Store все эти штуки внизу имеют под собой один ключевой строительный блок это абстракция реплицированного конечного автомата мы называем ее таблеткой пользователю она предоставляет интерфейс локальной отказоустойчивой базы данных и локальных транзакций а внутри она опирается на подлежащий но отделенной абстрактный слой распределенного хранилища и вместе с этим слоем внутри зашит зашитые репликация И консистентность для нас она такая отказоустойчивая база даже вся платформа в idb это в каком-то смысле Во многом реализовано как облако различных по своей логике небольших узкоспециализированных таблеток которые являются легковесными инстансами сами не стоит вас и хранилище отделено поэтому они могут инстанцироваться динамически На разных нодах и обеспечить балансировку и загрузку железа и между ними среда коммуникации и способ писать код это акторная система Какая же таблетка нужна нам Мы берем основную таблетку как базу и Наша задача построить таблетку которая отвечает за пользовательскую партицию за один apient onlog пользовательских данных вкратце что нам нужно построить поверх интерфейса таблицы для удобства мы первым шагом строим Киеве или Интерфейс это пуд Get Delete и поверх этого интерфейса и механизма локальных транзакций собственно реализуем логику своей партии тут за скобками осталось много деталей и оптимизация которая туда внесены Но в общем мы дописываем новое сообщение удаляем старые сообщения ВКонтакте пачки сообщений вместе для оптимизации тема наверное дальнейших докладов и кроме таблетки которые отвечают за порцию за партицию пользовательских данных в алгоритмах участвуют еще несколько действующих лиц так скажем которые тоже живут на разных узлах и взаимодействуют между собой это прокси который принимает клиентский rpc запрос Но это таблетка спартицией и подлежащий дистрибьютор сторож в который мы ходим через кэшблабов для оптимизации запросов это компонент который хранит мета-информацию по топиком и обеспечивает роутинг до нужных партий и эта таблетка балансировщик на чтение которое обеспечит как раз серверную балансировка которая говорил раньше это можно типичный запрос можно показать на такой картинке и посмотреть как между разными узлами перемещаются запрос на разных стадиях вот пришел пользовательский клиент в прокси прокси выяснила Discovery в какую идти партицию за данными отправил пользовательское сообщение в нужную партицию и уже та через слойкаша распределила уже кейвелью пары вытащила из пользовательское сообщение распределила по репликам в свою очередь когда она получила кворум реплик они по обратной цепочке возвращаются собираются вакан и сообщаются клиенту возвращается на чтение могла бы быть похоже картинка но достаточно сказать про специальную таблетку топик Red balancer Все установленные сессии чтения в топик идут в неё регистрируются у нее в рантами есть вся информация про текущее сосед чтение и она может по каким-то эвристикам количество партийцев количество сессий текущая загрузка текущая лаги определить оптимальный способ как раскидать нагрузку между сессиями чтения так поверхностно прошлись по архитектуре И мне осталось заключение просто показать какие-то цифры и картинки про их при использовании idb основной клиент это конечно же наши внутренние наше внутри Яндекса внутри яндексовской установка Вот они какие-то параметры подробнее это картинка это график записи чтения за последнюю неделю Мы видим что Пишем сообщение в самый крупный инсталляции суммарно до 70 гигабайт каждую секунду читателей в среднем больше И вот мы вычитываем до 150 ГБ секунду кластер состоит из более чем 1000 хостов то есть Это пример Scale Up нами пользуются практически все сервисы внутри Яндекса и суммарно это сотни тысяч партиций а letones я не добавил на слайд наша Средняя персентиль палетонси это до 300 миллисекунд Вот и дом с кросс-доца гарантия а мы предоставляем ряд инструментов чтобы с нами работать основной нативное из детей которые мы разрабатываем на c++ также выпущена из DK на Go и ведет с работы над с дикими других языках есть консольная утилита это общее утилитовая dbk У нее же есть протокол для записи чтения истопиков для создания каких-нибудь наверное скриптов в манере командной строке Кроме этого мы у нас выставлены для пользователей несколько API надстроек одна из них это совместимость кинезис в наших сервисах в том числе в облаках она называется Яндекс Давайте подведем итоги доклада я рассказал как мы реализовали распространенный дизайн очередей и потоковой передачи данных на платформе vdb это решение уже многие годы в продакшене используется как основной фреймворк для передачи распространения данных этому сервису Яндекс не боится доверять там самые чувствительные к потерям данные и мы пока не подводим теперь это в мире Open Source и Мы очень хотим совершить какой-то разворот будем рады коммуникации с комьюнити встречать ваши запросы ваши случаи использования и приглашаем вас всех развивать экспериментировать и внедрять себе для своих задач у меня все спасибо Меня зовут Ильдар на слайде несколько моих а также точка входа в айди би и стандартный qr-код на обратную связь супер спасибо Эльдар Давайте вопросики если есть зале вот я вот на первом ряду вижу Давайте Спасибо большое за очень интересный доклад не такой интересует проводили какие-то сравнения справка и ваша система На каких-то тестах на каких-то платформах там вот чтобы понимать когда если выбираешь систему то грубо говоря мне должно быть таблица Это очень хороший доклад это действительно часть работы которую нужно провести у меня на текущий момент нет аккуратного сравнения Но вопрос сложный потому что нужно подобрать конфигурацию в которых обе системы Дали бы и схожий набор гарантий то есть чтобы никто из нас не срезал какие-то углы и на кластера с примерно одинаковыми свойствами под пользовательскую задачу мы смотрим и занимаемся задачей аккуратного бенчмарка Я думаю что поскольку в дизайне у нас ради лучшего выделения абстракции были какие-то трейдов и по ряду параметров сразу мозрительно сказать можно сказать что мы уступим например на картинке про обработку записи видно что сетевого трафика между нами у нас скорее всего будет больше кавка надо отсильнее оптимизирована но на аккуратные цифры я согласен смотреть другая часть ответа что и наша система и Кафка по классу сравнимые и я думаю что узким местом будет становиться запись на диск а не CPU но этого конечно недостаточно хочется смотреть аккуратные цифры Спасибо большое за вопрос Давайте тогда Следующий вопрос Филиппа Спасибо большое вопрос Можно ли делать общую транзакцию для и ДБ базы и ДБ топика Ну Стандартный вариант работы транзакцию в конце положить события топик это наша фича номер один в нашей текущей работе как повторю слова коллеги пока мы здесь выступаем люди сидят и пишут Код да транзакции между топиками и топиками а также топиками таблицами у нас в разработке а но на данный момент конечно скачать из github развернуть и попробовать такого нет Маленький вопрос Можно ли делать на решение много-много очень маленьких очередей миллионы десятки миллионов маленьких и хватит на что все это помещалось на один-два маленьких Но почта не маленький очереди Я думаю что для такого сценария использования лучше подойдет другой паттерн решение у нас есть система которую я не говорил это sques совместимый совместимое как раз очереди точечных сообщений про него меньше знаю но мне кажется из общих соображений Что такое кейс должен скорее решаться другой системой Спасибо молодой человек и потом микрофончик дальше через ряд Да я хотел спросить про возможности Ну retention то есть как каковки удалять нам данные что нельзя наверное вечно хранить петабайты в очереди Да у нас есть конечно настраиваемый ретеншн Он наверное это важная деталь которая стоила сказать У нас сейчас дефолт 18 часов и этот тоже часть самого паттерна вот этого очередей на основе логов с попсапом что да а в общем случае последние 18 часов записанных данных хранятся и можно э-э нового потребителя на них подписать и отправиться в прошлое прочитать оттуда и вот как кавки ещё нету или процессор композиции чтобы я схлопывал по ключу какие-то значения если по-моему даже упомянул Но действительно из слайдов вытащил это как деталь реализации мы компактимные э-э при записи в партицию есть также оптимизация транспортного уровня Мы в протоколе данные передаёмчами и так же как в сказке поддерживается архивация на потоке Спасибо большое за вопрос и от микрофончик через ряд пожалуйста будьте добры Спасибо за доклад не интересует вот вы отказались от эзукипера Вот вы что-то взамен и стали использовать Или вы просто ушли от выбора функциональность выбора лидера и консенсус у нас зашиты с самого начала даны нам на старте через базовый функциональность таблеток в idb То есть у нас координация осуществляется специализированными таблетками которые базы для самой платформы у нас это все из коробки Спасибо так вот здесь попросите пожалуйста У меня небольшой на самом деле ответ на вопрос Филиппа что транзакция между базой и топиком особо не нужно потому что базу можно закоммитить попробовать после того как записали в топик и если это не получилось повторная запись в этот э-э жетопик с этим же обсетом Не сработает уже выйдет увлекаться там это обнаруживается биллинг Яндекс облако ровно на этой технологии сделано поэтому Гарантируем что ничего не удваиваем Поэтому да два Спасибо Эльдару и за интересный доклад из-за технологию спасибо Я хотел вопрос в следующем состоит вы предоставляете API kinesis совместимый и какой-то свой собственный разработанный протокол и какие фишки протокола В чём он удобнее что есть такого чему программист будет рад что нет здесь а я наоборот больше знаю про наш и меньше про кинезис поэтому может быть мне нужно Подготовиться и на стенде на это ответить Но про наш протокол скажу что если на него смотреть Непосредственно достаточно сложный но тем не менее На нем можно реализовать как простые сценарии там с однопоточной синхронной записью чтения так ивент лупы которым к разным типам моментов цеплять специализированные калбеки и в принципе любую логику навешивать но понимаю что это не полный ответ и чтобы полноценно сравниться с кинезиосом давайте наверное отдельно обсудим Спасибо большое спасибо и молодой человек пожалуйста очень раз два три раз два три а микрофон не работает не работает микрофон да давайте другой микрофон раз два три Отлично Отлично Что вы сделали дедупликацию вот Я насколько помню Кафка очень долго с этим мучилась и долго это реализовывало особенно больной Вопрос вот деда бликаться на уровне разных реплик не знаю вот подскажите как это реализовано Пишем сообщение в одну реплику она отваливается от клиента но успевает реплицировать во вторую реплику и клиент переключается на вторую реплику и пишет туда дубль Как вы вот это разрешаете у нас архитектура в этом в этой части отличается от архитектуры Кафки где реплики это независимые брокеры и между ними есть там Лидер фолловер у нас возвращаясь к слайду с архитектурой репликация репликация к воронная она устроена уже для нас зашита внутри таблетки мы не думаем мы когда пишем код сервиса топиков мы не думаем про репликацию на самом деле мы пишем сразу в таблетку А внутри у неё это кворум То есть алгоритмически это равный по равносильной алгоритм по гарантиям семейству рафтпаксос немножко по-другому сделанный Но если прищуриться то там все алгоритмы из этого семейства используют одни и те же идеи Поэтому нам не придет подтверждение пока не Соберется к ворам реплика которая успешно записали про обстоятельств Так слушай давай вот еще вопросик маленький от меня буквально знаешь вот смотри ты Обозначил вот одну из фишек которая прямо сейчас в работе вот из среди Пула задач Ну любой разработчик Он же всегда вот какая-то есть любимая фича Вот такая любимая фича вот прямо тебя сейчас которой бы ты сейчас вот бросил бы конференцию и занялся бы именно ей следующая фича которая мне досталась в Black Lock это автоматический динамический сплетный чтобы пользователь вообще мог забыть про конфигурацию партиций и на лету в зависимости от нагрузки вот опять же Это в соответствии с облачной парадигмой при уменьшении нагрузки часть партий сжималась бы в одну И наоборот сплителось если это портится становится высоко нагружен Блин круто так Я думаю мы сделаем следующим образом во-первых мы сейчас выберем вопрос который тебе больше всего понравился среди всех вопросов которые были Да я бы тоже подсказочкой воспользовался Вот тебе подсказочка угу так и остальные вопросы можно будет задать в экспертной зоне или на стенде Яндекс Клауд куда можно будет потом подойти После этого я наверное хотел бы отметить самый первый вопрос про сравнение с кафкой потому что он действительно самый животрепещущий и понятно что чтобы идти в для во внешних людей такие сравнения действительно хочется они у нас горят так хорошо Кто у нас сдал Вот про сравнение с кавкой пожалуйста сюда и у нас кстати организаторов есть небольшой презент для докладчиков мы сейчас его тебе вручим и спасибо тебе большое за доклад Спасибо Давайте ещё раз поблагодарим докладчика и на этом заканчиваем это выступление в этом зале До скорых встреч Приходите здесь будет интересно"
}