{
  "video_id": "hUFFsLoCRNU",
  "channel": "HighLoadChannel",
  "title": "Развитие баз данных в Dropbox / Слава Бахмутов (Dropbox)",
  "views": 4613,
  "duration": 3602,
  "published": "2018-07-19T04:57:18-07:00",
  "text": "всем привет меня зовут слава бахматов я работаю сиквел обелить инженер в команде дро бокса также являюсь тех ли там в команде баз данных очень люблю иногда появляюсь подкасте главных шоу прага я не мальчик алди бей и про майская или вообще ничего не знаю так что если у вас есть вопросы про мой скальп тут спрашивать у петра зайцева который здесь где-то бегает и у других людей а собственно структура этого доклада кратко я расскажу об архитектуре друг бокса как он вообще работает потом мы перейдем к истории развития баз данных и к текущей архитектуре как она устроена обсудим какие-то простейшие операции которые мы выполняем над базами данных и таффи лавры бэкапы промоушены потом мы пойдем к автоматизации что управляют этим всем базами данных и что запускается эти операции и кратко рассмотрим мониторинг и тестирование стейджинг собственно об архитектуре друг бокс ту бокс появилась 2008 году и по сути друга кс это облачные файловый сторож и когда только дру бокс запустился пользователь на hacker news прокомментировал что реализовать бокс можно нескольким ваш скриптами и с помощью ftp и guitar но тем ни менее дубов развивается сейчас это достаточно крупный сервис у нас более полумиллиарда пользователей более 200 тысяч бизнесов и огромна конечно file save скажете несколько миллиардов как друг бокс выглядит по сути у нас есть несколько клиентов это web-интерфейс у нас есть api для приложений которые пользуются дру боксом и доступное приложение и все эти клиенты пользуются неким и пиаре и общается с двумя большими сервисе можно так логических разделить это metaserver блок серы нами то сервере хранится эта информация о файле это размер этого файла какая какие-то комментарии к этому файлу линки на этот файл / боксе какого-то такого типа информации блок сервер они с информации только файлах это папки где они хранятся где файлы хранятся вот это все хранится блок сервере собственно как это дело работает есть у вас файл видео вин с каким-то видео и ваш клиент он дробит этот файл на несколько чанков в данном случае по 4 мегабайта подсчитывает контрольную сумму и после этого он отправляет и metaserver запрос вот у меня есть файл avi я хочу его загрузить вот такие-то hash-сумму им то сервер mu возвращает ответ у меня нет этих блоков давай загрузить либо может ответить что у меня есть эти блоки или бы мне есть вот эти блоки и загрузить только вот эти блоки если вы уже загружали другого например устройство с этого клиент идёт блок сервер и отправляет уже hash-сумму и сам блок то есть блок данных и сохранять его на буксир бог сервер говорит все окей самом деле это очень упрощенная схема потакал намного более сложнее там есть организация по сети внутри сети есть какие-то драйвер ядра и есть возможность различать ковать коллизии так далее он достаточно сложный протокол но по простому он работает примерно так и когда клиенты конечно то metaserver metaserver ты всю информация сохраняет mask ель блок сервер информация о файлах о том как не структурирована из каких блоков составить тоже хранит мы с келли также благ сервер хранится сами блоки где-то в каком-то блочном стороже и этот борщ несторович информацию где какой блок лежит на каком сервере как он обработан данный момент тоже сохраняется мы съели у нас есть экзабайт и данных именно пользовательских файлов и все это создает примерно несколько десятков петабайт данных в базах данных и порядку шить на порядка шести тысячах серверов как развивались база данных у нас труба все собственно все началось с одного места сервера и 1 глобальный базы данных и всю информацию которую другого нужен блока сохранять весь этот стоит он сохранял вот в глобальном базу данных москве так продолжалось недолго потому что качество пользователь росло и какие-то базы какие-то таблички внутри этой базы разбуханию больше чем другие поэтому в каком-то году 2011 например а были вынесены несколько табличек это по табличке юзер где была информация эта информация о пользователях а когда они логине лись какие-то востоке не так далее и таблички ход где хранилась информация о ок сервера файлах также добавил с табличками со которая не была в продакшене то есть они трафика production решил но там какие-то большим джогу крутились всё такое но в две тысячи лет после тысячи двенадцатого года другом сначала сносить очень сильно расти и смена 2012 года мы растем на 100 миллионов пользователей в год и к сожалению нужно учитывать и то то есть такой огромный большой рост и поэтому 2012 в конце 2011 у нас появились реальные шарды база sharp появилась состоящий из 1600 шар дав изначально это было всего 8 серверов получается по двести шагов на каждом сейчас это 400 серверов 400 мастеров по четыре шага на каждом а в 2012 году мы поняли что создавать таблички и обновлять таблички в базе данных каждый раз на каждый какой-то бизнес-логику до которой нас просили это очень сложно муторно и проблематично поэтому тысячи двенадцатом году мы изобрели свой собственный графу и стоишь который мы назвали штор и с тех пор вся бизнес-логика и сами эта информация которая генерирует приложение она кладется высочество штор он по сути абстрагируется москве лет клиентов и клиенты у них есть некие entity которые соединены между собой какими-то ссылками и записей пик хочешь тур кору а уже он эти данные преобразуют в москве или каким-то образом там хранит в основном он дает все это дело и спеша а 2015 году мы сказали что мы уходим сумма зоной стримы разработали собственный больше сторож назвали у magic pocket и magic pocket информация о том где какой блок файл находится на каком сервере когда он перемещает эти блоки файловыми серверами прикрыта обработки он хранит мы склеили но использованы miss kelly очень хитрым образом по сути он использую как большой распределенных и штабов поэтому это очень разная нагрузка это рандомная нагрузка на чтение в основном и сервера очень сильно пострадает om 90 процентов there'sa цевьё на их текущая архитектура база на как у нас устроено во первых мы во-первых мы сразу же определить некие принципы по которым мы строим карикатуры нашей базы данных самый главный принцип это что от нас ждут клиенты то что мне должны терять данные то есть у нас самый главный принцип это надежность и долговечность данных а второй главный принцип это оптимально из решения то есть это например backup это они должны делаться быстро устанавливается быстро решений должен быть очень простым как архитектурно так с точки зрения обслуживания с точки зрения дальнейшей поддержки разработки и обязательно нужно стоимость владения да то есть мы если какое то решение очень сильно оптимизирует наши если что-то оптимизирует очень решение но стоит очень много ну например slave который отстаёт да и для бэкапов это очень здорово у нас есть слоев который наденет встает и мы можем в любой момент это стоит прокрутить куда нам нужно установить наши данные но это нам дает в как бы k6000 на серверах нужно добавить еще тысячи серверов костюм из владений очень сильно вырастает свете принципе они должны быть верифицируем и и измеряемый то есть на все эти принципы должны быть метрики если это стоимость владения мы должны считать сколько у нас серверов например уходит под базы данных сколько серверов уходит под бэкапы и сколько это стоит в итоге нам другого и когда мы уверяя новое решение мы тоже это подсчитываем и все время у нас есть это стоимость для других принципов то же это все работает и при выборе любого решения мы как бы руководствуемся полностью теми принципами при любом решении как страны сама база данных есть у нас в самом деле центре у нас есть мастер в которой происходит все записи мастера есть два слова и на эти слова angry pets у него происходит 7 сен трипле к цена оба слова эти слова находятся в других мастерах то есть кластер это отдельное совершенно комнату дата-центре которая не связано друг с другом за 1 ком то сгорает 2 комнат остается вполне себе рабочий почему у нас два слова потому что сервера у нас часто умирают порядка 10 серверов умирают в неделю и поэтому с одним слоем очень тяжело жить что если своих умирают осень сильно больше не работает у нас на мастер больше запись не идет также другом то-то центре у нас есть так называемый псевдо мастер которые на самом деле просто слоев к мастеру и у него есть еще свой slave почему выбрана такая топология потому что если например хотим если у нас умирает 1 это центр во втором это центр нас практически уже повод полная топологии которое нужно мы просто меняем а в discovery все адреса и в клиент уже могут работать у нас есть специализированные топологии это первая топологию мальчик пакет а потом у него топологии стали знал мастер и двух слов почему так сделано в том что самый же pocket он дублирует данные среди зон если он теряет один кластеру может установить через ranger коды все эти данные с других zoom 2 топология костанае тот пологий актив актив который используется в ваши шторы и по сути в ней есть мастер два слова в одном да старый мастер два слоя другом за центре я не являются словами друг друга это очень опасная схема но и штор на своем уровне разруливает то есть он и штор точно знает какие данные на какой мастер он может каком уровень записать поэтому этот пологи не ломается собственно у нас достаточно простые сервера установлены это по пять терабайт и ssd диски в райдеру в радирую потому что нам проще заменить целый сервер чем за меня диски намного быстрее и 383 его эта память достаточно простой серверу 4 пятилетней давности конфигурация на этом сервере сейчас у нас есть один институт большой москве или на котором находится несколько сортов этот майская лист из он сразу же выделяет себе практически всю память там и какие то другие процессы которые запущены прокси на этом сервере есть какая-то статистика логе еще что-то а это решение хорошо тем что им просто оперировать если нам нужно заменить сервер да мы просто заменяем сервер а просто очень full делать фолловеры другой стороны очень проблематично то что операция любые операции происходят только он один из солнцем и съели сразу на 7 шар даме например все нужно сделать бэкап мы делаем backup сразу всех сортов если нужно сделать flower мы делаем flower сразу всех четырех ярдов соответственно как бы было били страдает в четыре раза больше и последнее проблем это то что во-первых репликация не параллельная в москве эликсир линию и все шарды работают в один поток реплицирует друг друга и шарды очень сильно влияют друг на друга если у нас 1 шард эффекте садат остальные шарды становятся жертвами у нас там приходят какие-то демоны которые начинают убивать транзакции могут убивать транзакции совершенно не связанности с теми транзакций на который реально шар перезагрузить перегрузили поэтому сейчас мы переходим на топологию типом мальте instance где у нас запущена нам сервере сразу несколько инсов мозг ель в каждом есть там по одному шагу допустим а в чем это лучше тем что мы можем проводить операции только на на нем конкретный шар дом то есть нас нужен flower mayflower им только 1 шард нужен backup делаем backup только наш адрес значит операция очень сильно ускоряется и ускоряется четыре раза для 4 равно сервер мы можем миксовать различные категории баз данных лиц например и штор очень много места занимает все четыре терабайта например magic pocket занимает всего один терабайт но у него утилизации 90 процентов можем миксовать различной категории который использовать по-разному использовать по разному ресурсы машины также улучшается репликация прошу у нас получается уже четыре потока репутации уже работает быстрее конечно же в этом решении и минусы самый большой минус это намного сложнее этим всем управлять то есть нужен культурный среду лир который будет понимать куда он может вынести этот тест с где нагрузка на tss будет нормально вот поэтому если только сейчас переходим на это решение клиенты должны как-то узнавать о том кому подключаться какой базе данных поэтому у нас есть discovery который должен во первых очень быстро notify целовать клиентам изменения меняли мастер своего клиента должны знать об этом практически мгновенно топологии это не должна зависеть от топологии мы искали потому что при некоторых операциях мы меняем топологию мы с келли например мы делаем когда сплит подготовительном шаге мы на таргет мастер куда мы будем выносить чаша родов мы часть слоев перекид мы часть своего в перенастраивай это таргет мастер и нам не нужно чтобы клиенты знали бы вы об этом все поэтому а не должна зависеть пологи miss kelly и нам важно чтобы была тамара с операцией и верифицируем с ним не нужен так чтобы у нас был два сервера мастером один джемами что-то не то а каким образом discovery у нас развивала сначала это был очень просто был адрес нас данных в исходном коде в конфиге и когда нам нужно было обновить адрес то просто все пришлось очень быстро вот к сожалению перестал работать когда стало очень много серверов и перед первый самый discovery который нас появился а у нас были скрипте баз данных которые изменяли табличку в конфиг baby балок и табличку москве или в отдельном а клиенты уже слышали этот слушали ту базу данных периодически пулю ли оттуда данные исходили из этого табличка очень простая танис категория баз данных а есть шард 3 шард + шар до есть класс этой базы данных мастер своих прокси и есть адрес этой базы данных и по сути клиент запрашивал категорию класс база данных чертей ему возвращался адресом и скверны по которому уже мог подсоединяться к только сервера стало очень много появился memcache клиенты стали общаться уже с малышом потом мы все это дело переработали и не знаю видно все здесь видно а потом это все дело переработали по сути у нас москве ли скрипты начали общаться через записи через тонкий клиент с сервисом назвали а режиссер сервис и когда какие-то изменения происходили этот режессер сервис он понимал то есть него была очередь он понимал как применять эти изменения это роде сыр сервис сохранял данные bffs это наша внутренняя система построена на базе на базе звуки пера до этого когда второе решение которое здесь не показано напрямую использовал звуки pir это возникало это создавало проблемы потому что у нас каждый шар ду нот и звуки переката клиенты например 100000 клиентов у нас которых закипел подключаются если они вдруг умирали за пул табага все вместе потом сто тысяч запросов закипел приходил и клала его прост его не мог подняться очень поэтому была разработана система fs который пользуется весь дух бокс и по сути она очень она абстрагируется работу звуки перов для всех клиентов она локальных трудятся на каждом сервере представляет очень простой api видов файловой api вида создать файл удалить файл запросить файл получить нотификации на изменения фразой компромиссов об операции то есть можно было попробовать заменить файл с какой-то версии если эта версия поменялось процессе смены да то вы получали файл по сути такая абстрактный звуки пираний был у быков локальный джиттер алгоритмы уже нила железу кипер всеми запросам с этого эфесу мы снимали бэкапы выстрелив гид и потом собственно сама fs локальной демон офисы рылу клиентов о том что мы изменили эти как-то изменились и данных в данные хранятся виде файликов по сути ну то есть это api файла систему вот например здесь у нас есть файлик шар своих профи самый большой он порядка 28 килобайт занимает и когда мы изменяем категорию шарф из live rock city класс то все клиенты которые подписаны этот файл получает модификации они перечитывает этот файл я это файл простой про табов файл в котором есть как раз вся эта информация которая нужна это по shortcake может получить категорию соответственно учитывает этот файл и переинициализация них базе данных простые очень операции которую мы используем как операция выглядит операция такая простая стоит машина заходим операцию мы производим этот чек либо это спиннер чек который несколько раз проверяет по таймауту о том можно ли нам выполнить сто perazzi после этого мы делаем к это подозрительное действие которое не влияет на какой-то какие-то внешние системы собственно сама операция если операция какая-то проблема с не возникнут а все шаги внутри этой операции некие soul backstab отмены и он эта операция пытается установить систему исходное положение все нормально то какой то какая так ли оно происходит и операция завершена такая простая стоит машина на любой операции у нас собственно промоушена это очень частая операция базы данных были вопросы о том как делать альтерна горячий горячий мастере который работает он живут встанет колом просто делается все эти операции на свадьбах и потом своих меняется с мастером поэтому эта операция промоушн очень часто нужно обновить керну с wap им свои с мастером нужно обновить версию москве ли обновляем на словах своим с мастером одновременном street мы добились очень быстро промоушенов у нас например для 4 шард сейчас промоушен порядка 10 15 секунд и был ability можно здесь посмотреть этом графике то для предыдущего промоушена выл ability пострадала на три сотых 1 процент то есть это очень такой низкий вела билете очень страдает мало при промоушена в таких нормальных нормальный про машины не так интересна потому что это обычная операция которая выполняется каждый день интересно флауер флауер это значит что у нас база данных umi.ru если сервер действительно умер это просто идеально это простым она какая-то небес на самом деле серверы бывают так что они иногда живы иногда мертвы до сервер очень медленно умирает у него отказывается raid-контроллер дисковая система какие-то запросы возвращают ответа какие-то трейды блокируются какие-то трейды не возвращают ответы бывает такое что мастер просто перегружен он не отвечает на наши холсте кира но если мы сделаем промоушен то новом мастер бы тоже перегружено это как бы будет только хуже у нас flower и мастеров происходят примерно 45 раз в неделю flower полностью автоматизирован никак никакого чернику интервенты человека не нужно он занимает примерно 30 секунд критическая секция и в нем есть куча дополнительных проект проверок о том жив ли мастер на самом деле и может он уже умер вот и в общем это такая простая схема как работает fila вверх вот в этой секции вот в этой секции мы перезагружаем мастер почему мы перезагружаем мастер потому что у нас mais quel 56 в москве или 5 6 7 сен complication на ней lossless поэтому возможно рандомный фантом рид на мастере поэтому нам нужно этот мастер даже если он не умер как можно быстрее убить что клиента от него от коллег телись поэтому мы делаем hard срезается через и фирмой и отправляем мастер ебут это первая самая важная операция которым нужно сделать 57 версия наверное нам это уже будет не так важна следующая синхронизация кластер зачем нужна синхронизация кластера из вспомнить предыдущую картинку а наши топологию у нас получается что три слова у мастера два в одном дата-центре 1 slave другом это центре при промышленном нужно чтобы мастер был в том же дата-центре в основном нашим но иногда так бывает при 7 senki когда у нас славы нагружены 7 sings life появляется сами sing sing sing for становится слоев в другом это центре что он-то не нагружен поэтому нам нужно синхронизировать сначала весь кластер а потом разделать промоушена slave который нужно наш нам дата-центре все делается очень просто во-первых мы ну незавидна здесь нет во-первых мы останавливаем среду на всех словах после того как мы установили исраэль и мы уже знаем точно что мастер он readonly потому что мастер отключился 7 sing она мастер бусинка записать не может люди могут еще прочитать что-то него на записать уже не могут после этого мы выбираем слой с наибольшим рецептуры какие-то джетте одесситам с наибольшей по сути транзакции которой он либо скачал ли будет применил перенастраиваем все слои вы на этот слив выбранный и они синхронизируются запускаем опять и рисует они синхронизируются мы ждем пока не синхронизируются после этого у нас весь кластер становится сарни zero ван проверяем в конце что у нас везде забьют и джи ти одессит установлен на одно и то же одно и то же . а это вторая важная операция синхронизировано кластера и по сути дальше начинается уже основной промоушен каким образом он происходит мы выбираем любой своих нужном нам дата-центре мы уговорим что мастер запускаем операции стандартного промоушн мы перенастраиваем все свои вы на этот мастер останавливаем там репликацию применяем акулы убиваем пользователей останавливаем кита прокси что-то перезагружаем в конце концов мы делаем readonly 0 то есть мы заговорим что теперь мастер можно записывать и обновляем топологию с этого момента клиент уже идут на этот мастер вы все уже работает дальше у нас есть какие-то пост шаги обработки в ней мы в них мы просто какие-то сервисы переезд рестор темно этому кости какие-то конфигурации перерисовываем еще какие проверки дело что все точно работать что нас прокси работают например и после этого как бы вся операция завершена на любом шаг шаги из у нас произошла ошибка мы пытаемся делать rollback до момента до которого можем то есть мы не можем сделать рубик для ребута ну потому что он происходит где-то там но те операции которые можно сделать например перед назначением de master когда мы делали можем назад завернуть мастер на предыдущие операция backup & backup это очень важная тема база данных я не знаю делаете любой коту вот но мне кажется все должны делать укладку это уже шутка такая избита собственно паттерн и использование выказав самые главные паттерн и использования которые лично мы используем это предъявлении нового слова мы просто устанавливаем из бэкапа это прямо постоянно иногда достаточно часто пользователи удаляют данные приходят к нам говорит нам эти данные нужно восстановить он пример пользователь удалил в свой файл и к нам пришли продакшен и сказали вот нужно восстановить этот файл и из базы данных тоже восстановить это все автоматизировано у нас но это достаточно часто операции восстановления на данных на точку прошлом и восстановить кластер с нуля эта операция ну для чего все думаю что нужны бэкапы вот восстановить данные да вот с нуля на самом деле эта операция практически никто на метле был последний раз мы использовать три года назад когда один инженер сделал примеров на всем кластер и собственно какие у нас гарантии бэкапом бэкапом мы смотрим на быка по как на продукт поэтому мы говорим от клиентам что из гарантий у нас есть гарантии того что можем установить любой любой базы данных 1 терабайт за 40 минут и мы можем остановить любой базы данных на любую точку в течение прошедших 6 дней прим не любую точку то есть это наши основные гарантии которые мы даем нашим клиентам почему за 40 минут 1 терабайт вам что ограничение по сети мы не одни на этих стойках нас уже продакшен трафик на этих стойках есть такой тротлинг у нас есть мы ввели такую абстракцию как цикл в цикле мы стараемся то есть в одном цикле мы стараемся забэкапить практически все наши базы данных у нас одновременно крутится четыре разных цикла первый цикл выполняется каждые 24 часа мы бы копим все наши сортированные базы данных наш dfs то есть это порядка тысячи с чем-то хвостов мы как бы делаем бэкап и не ждите скажи 24 часа каждые шесть часов мы делаем бэкап и для орды то это базис нас еще есть некий данный на global юзер еще до сих пор работает мы хотим очень от них избавиться в сожалению не еще есть до сих пор а также мы каждые три дня сохраняем полностью всю информацию на ис-3 сортированных баз данных и каждые три дня сохраняется информация не сортированных и храним и это все в течение нескольких циклов соответственно допустим если мы храним трицикл тоже dfs у нас есть последние три дня ps3 у нас есть последние шесть дней как раз мы выдерживаем наши гарантии например как они работают вот у нас в данном случае два лунка цикла запущена это которые делают backup hr деле на базу данных и один шорт сайкл при завершении каждого цикла мы обязательно верифицируем чтобы к поработать то есть мы делаем recovery на какой-то процент базы данных к сожалению мы не можем восстановить все их какой-то процент мы обязательно проверяем у нас есть какие-то определенные шарды которые мы всегда восстанавливаем чтобы смотреть скорость восстановления чтобы мониторить какие-то регрессии есть рандомные а шарди котором сторон просто рандомно что проект что не восстановились работает ну и плюс мы еще восстанавливаем при клонированием установить рядов мы устанавливаем за бэкапов то есть мы знаем что они тоже работает собственно сам процесс backup а сейчас у нас происходит ход backup мы используем при конскую инструмент экстры backup запускаемого в режиме stream is by стримы и он она возвращает по сути некий то есть на рабочий байдан который работает сейчас на слове где-то на славе где-то на мастере а мы запускаем этот со backup она возвращает этот стрим бинарных данных дальше у нас есть скрипт сплиттер который этот бинарный поток делить на 4 части и дальше мы собственно сжимаем этот поток как ни странно мой сквер хранить данные на диске очень странным образом и у нас получилось компрессия порядка больше 2 2 икс из мы компрессируем больше два раза база данных занимает 3 терабайта то получается где-то в результате backup 1 терабайт 500 например дальше мы собственно in козима эти данные чтобы никто не мог прочитать отправляем вождей css3 в обратной стороны работает абсолютно точно также мы подготавливаем сервер куда будем устанавливать backup достаем этот backup в излив если ps3 декодируем его де компрессируем его splitter сжимает это все дело и отправляет экстры backup который собственно восстановить все данные на сервер потом происходит крыши recovery некоторое время это очень самое главная проблема ход бэкапов то что краше recovery занимает достаточно длительное время по сути нужно проиграть все транзакции за то время пока вы делаете бэкап и после этого мы проигрываем и ноги чтобы наш сервер стал на том же уровне транзакций matter текуч сосна как на сохраняем берлоге раньше мы сохраняли файлики бен логов мы собирали на мастере файлики родителях каждые 4 минуты либо по 100 мегабайт и сохраняли их ваш dfs сейчас сейчас у нас используется новая схема мы у нас есть некий гель-лак бы copper который просто является подключен к треплите щенка всем базам данных и он по сути постоянно сливает обе ноги к себе и сохраняет лишь dfs соответственно в предыдущую в реализацию могли потерять 4 минуты о бинарных логов дальше потеряли все пять серверов в эта реализация мы теряем буквально секунды все зависит от нагрузки этой системы и сохраняя ваш дпс в с3 и все это хранится в течение месяца где-то примерно а какие у нас планы на бэкапы во-первых мы хотим перейти на холодный бэкап и почему мы хотим перейти на холодный бака потому что скорость каналов на наших серверах стало больше было 10 гигабит достала 45 гигабит хочется восстанавливаться быстрее из хочется что скорость установление было надо быстрее потому что нам нужно более умный скейлер для младенцев и хочется очень часто перекидывайся славы и мастер сервер на сервер его смотреть очень быстро ну и самый главный пункт это то что при холодном бэкапе мы можем гарантировать чтобы как работает потому что когда мы делаем холодный баков мы про просто скопировали файл потом мы запускаем эту базу данных и как только она запустилась мы знаем что этот backup работает он может запуститься как минимум ну и после 5ти яблочек сам мы точно знаем что данные на файл с теми тоже не кукарачи а собственно гарантии которые у нас получились при холодным бэкапов наших экспериментах это один терабайт восстановление за 10 минут ну что это просто копирование файлов и мне нужен краж recovery делать это самое главное проблема и года гарантия по восстановление то же самое и как это работает вот в нашей топологию нас есть slave другом на сале который практически не фильм делает мы периодически восстанавливаем делаем холодный бэкап и запускаем обратно все очень просто какие планы у нас еще есть прям дальним дальним будущем когда мы будем делать обновление наших ar2r парка дамы мы хотим добавить на каждый сервер дополнительные шпиндельный диск hdd порядка десяти дара байт и делать бэкапы на это шпиндельный диск и делать экстра backup крыш recovery прям на этом диске и после этого загружать уже backup совета у нас будет бэкапы на всех пяти серверах одновременно в разные точки времени это как бы конечно усложнит всю эту обработку да и оперирование но это очень сильно улучшит во-первых по цене потому что ищите и стоит копейки какие-то огромные кластеров dfa100 и достаточно дорого клонирование как я уже говорил очень простая операция либо восстановление из бекапа либо это процесс backup из разных срывается сервер то есть этой диаграмме который мы видели где мы наш dfs копируем это копирует спроса на другой сервер здесь ресивер который принимаете данные и устанавливать очень простой процесс какая у нас автоматизация вокруг всего этого конечно же на 6000 серверах никто ничего не делать вручную вот поэтому у нас есть различные скрипты автоматизации сервис автоматизации их очень много вот основные у нас это авто реплей себе менеджер и есть еще два сервиса на уровень holes собственно авторе place что это такое это как этот скрипт нужен когда сервер умер серверу меры нужно понять что за проблема правда ли он умер может и сеть поломалась можете все что это нужно решить как можно быстрее а собственно что такое доступность доступность это функция от времени между ошибками да и временем за с которой вы можете починить эту ошибку детектировать ее починить мы можем очень быстро у нас в рекавери очень быстрый да и нужно теперь очень быстро зафиксировать эту ошибку на каждом сервере москве ли у нас стоит сервис который пишет херби ты в маске ель херби ты просто time stamp и который мы еще и смотрим на сколько слоев отстаёт секунд от мастера есть также другой сервис который пишет другие информации есть у нас некие предикаты которые мы точно знаем например если это мастер то он должен быть with bright и мы проверяем этим сервисом втором этом эту информацию после этого второй сервис отправляет некое центральное хранилище этот heart beat и у нас есть авто реплей скрипт такой простой схеме по сути что здесь происходит у нас есть основная лук в котором мы проверяем и орбиты которые вот если кто это глобальный базе данных here битов которые отправляются серверов с морем зарегистрировали этот сервис или нет дальше мы подсчитаем heart beat например сентября и 30 секунд мы посчитаем если 2 heart beat к примеру за эти 30 секунд здесь мы подсчитываем их смотрим подходит ли они под наших трешхолд если не подходит значит что-то сервера мне тогда почему не послал херби то после этого а мы делаем реверс чек на всякий случай вдруг эти два сервиса умерли вдруг что-то сетью вдруг вот эта глобальная база данных не может почему-то записать here by the river щеки мы по сути под соединяемся к этой базе данных которая полом она и проверяем ее состояние мы проверяем просто средство этой базе данных проверяем если локальные записи here битов если уже ничего не помогло мы смотрим двигается ли вообще мастер вперед из мы проверяем мастер позишн прогрессирует ли он или нет происходит на него запись или нет если ничего не происходит то все этот бокс эта коробка она уже поломана и последний этап это собственно авторе place он очень консервативен и он не хочет никогда очень многое делать автоматических операции поверх мы проверяем не было ли операции с топологией недавно может быть от серы только что был добавлен и может быть что-то на нем еще не запущена было также мы проверяем не было ли каких-то уже взамен в этом же кластер в ну в какой-то промежуток времени проверяем какой у нас фигура лимит то есть если у нас три или четыре проблемы или дальше больше 10-20 мы не будем автоматически все решать потому что так может случиться что мы возьмем мы сделаем flower ли всего нашего кластер всех с нашим узнал поэтому все очень консервативно решаем только одну проблему за 1 совет анализ слова мы запускаем клонирование удаляем просто и вас топологии если это мастер то мы запускаем фолловеры либо и промоушен это муж такой простой ски скриптов to replace что такой тебе менеджер по сути сервис который хранит source of trust полностью управление нашими базами данных в нем есть планировщик задач который точно знает когда какую джуббу запустить в нем есть логии вся информация кто когда что запускал в нем . синхронизации baby точно знаешь на этом серверу можно запустить эту задачу она этом не может а что там запущен к другая важная задача он очень достаточно просто архитектурно в нем есть тебе менеджер есть клиенты это либо деби и который ходит через web-интерфейс что-то делают либо это скрипты которые они salix приходит потерпите еще внешней системы вроде wheel house и науру который ходит потерпите в этот тебе менеджер и по сути в нем есть только скейлер который очень по-умному по-хитрому понимает какую операцию где можно запустить когда и собственно запускает эти операции есть очень тупой worker который по сути к нему приходит операция он ее запускает проверяют по пиду worker может перезагружаться эти процессы не ломаются после перезагрузки все горки расположены как можно ближе к серверам на которых происходит операции чтобы например при промоушене при обновлении оков нам не нужно было делать много район трипов мы сохраняют эти логике куда-то и на каждом из кель холсте у нас есть некий деби агент это рпц сервер когда нужно провести какую-то операцию на сервере мы делаем рпц запрос мы не делаем ssh мы не делаем суда на сервере всего этого мы не хотим делать это происходит иногда когда нам нужно река реально что то что нету вертись и такое случается очень редко у нас есть веб-интерфейс для этого дебила жира где можно посмотреть текущий запущенные задачи какие то логика этим зачем столько дачу запустил какие операции были проведены для сервера конкретный базы данных и так далее есть кло интерфейс достаточно простой где можно запускать эти задачи также просматривать с кем-то удобными вопрос никами еще у нас есть ли медиации то есть когда у нас что-то поломалось например какой то вышел диск из строя либо сервис q это не работает у нас есть науру науру эта система как работает во всем друг боксе всей пользуется и она именно построена для таких вот небольших задач про науру я рассказал в предыдущем докладе в 1017 году еще у нас есть символ house которая основана на базе ставить машины она для долгих процесса примерно нужно обновить ядро на всех москве елях на всем нашем кластере 6000 машина и вывод well how сундук четко делает он по машине обновляет обновлять нас левее промо учиться свойственно с массивом является мастере и вот эту операций который может занять месяц или даже два водку how занимается как раз это тоже есть предыдущему яндекс мониторинг мониторинг это очень важно я не знаю знаете ли вы нет но если вы не мониторить и систему то значит с тем у вас работает скорее всего мы мониторим все мы съели всю информацию которую мы можем получить из мозг ели она у нас сохраняется где-то сохраняется можем получить к ней доступ к по какому-то времени то есть на сохраняя информацию по инн и деби статистика по запросам по транзакциям по длине транзакции пирс интеле на длинные транзакции по репликации по сети все-все-все-все огромное количество метрик практически все собираем у нас есть alert у нас здесь написано что у нас 992 leur то настроена вот это значит что но метрики очень никто не смотрит то есть не знаю если люди которые приходят на работу и такие начинают смотреть на график метрик мне кажется таких людей нет есть более интересно задача поэтому есть alert и которые настроены три школы и они при достижении определенных школ ef собственно стратифицировать нас 992 leur то чтобы не случилось мы об этом узнаем собственно сами инцидент у нас есть париже льете это такой сервис через который мы эти alert и распространяем плане божий на самих on call of вот и анголы получаете over the у них звонок происходит еще что ты они начинают чинить в данном случае у нас зафейлился mercy промоушена и сразу после этого стремился лерка там что мастер мастер упал после этого on call пошел посмотрел что свалилось на мерсе промоушен и сделки это ручные операции обязательно для каждого инциндента которые происходят например предыдущий инцидент у нас есть некий on-call хэндов каждую среду где мы разбираем каждый инцидент почему случился для каждый инцидент у нас есть тоска даже если этот сын dent эта проблема в наших абортах мы тоже создаем тоска ножки с проблема валютах fresh холоде надо менять они не должны просто к alert людей и портить им жизнь лед это всегда больно это в 4 часа ночи приходит alert ты просыпаешься и тебе неприятно тестирование как и с мониторингом тестирует все скорее всего как бы мне кажется пять лет назад только поднималась эта тема чтобы ответ делаете и не тестируем все-таки ну может сейчас фиксирует все помимо unit тестов который мы покрываем наш код у нас есть интеграционные тесты прессованных тестах мы тестируем все топологии которые у нас есть и все операции над этими топология me то есть если у нас есть промоушен операция мы тестируем это персону то есть промоушн операции если у нас из клонирование мы делаем клонирование для всех topology которые нас ты пример топологию нас есть топологии на все случаи жизни 20 центра с multisense сами с шарами без родов с кластерами один дата-центр вообще практически любое топологии даже те которым мы не используем просто посмотреть интересно в этом файлики у нас просто есть некие настройки какие сервера и с чем нам нужно поднимать нам нужно понять мастер не говорю что нужно понять мастер с такими-то данными инстансов с такими-то базами данных каких-то портах и все это дело у нас у нас все практически собирается базиль им и собственно базиль запускается на базе вот этих файликов он создается топологию запускает этим уэскер сервера после этого запускается тест тест выглядит очень просто но газовым к топология у нас используется в данном тесте мы тестер move to replace мы создаем авторе place стартуем его дальше убиваем мастер в нашей топологии ждем некоторое время и смотрим что торги tv live который вот не мастер он стал мастер если нет то стс зафейлился такие операции обязательно всегда также у нас есть ты же окружения stretch окружения это базы данных которые вертятся в продакшене на них не трафика продакшен на них есть некий синтетический трафик который похож на продакшн через лейку персону playback икона playback она плохо работала она практически не работала никогда и никто не пользовался в прошлом году один на наш разработчик ему заняться было нечем он подчинил а сейчас перуну playback работать ста процентах случаев вот так что пользуйтесь спирко не playback на записан трафик потом проигрываем это трафик на вот этих встреч инвар ментах проигрывала с различной интенсивностью можем в два раза быстрее проиграть в 3 раза быстрее то есть искусственно но очень близка к реальной нагрузки зачем этот stations ужин потому что интеграционных тестах мы не можем протестировать наш production мы можем тестировать alert можем потестирую что нас метрики работает встречен gem этой сферы mallet и тестируем метрики тестируем операции мы периодически убиваем сервера смотрим что они собираются нормально и плюс мы еще тестируем все автоматизации месте потому что вентиляционных тестах скорей всего тестируется какая-то одна часть системы настей тенге все автоматизированные системы работают сразу и у вас есть какие то вы думаете что система поведет себя так а не иначе на самом деле она может вести себя как-то иначе также мы проводим тесты в продакшене ну прям на реальном продакшен называем это deze app store ковры testing для чего это нужно потому что мы хотим протестировать наши гарантии это делают многие крупные компании например google и есть сервис и не помню как-то назывался он работал настолько стабильна вот сто процентов времени он работал стабильно и все сервисы которые вы использовали решили что это сервис реально сто процентов стабильно никогда не падаете поэтому гуглу пришлось этот сервис каждую неделю ронять специально чтобы пользователи при использовании сервер учитывали что может когда-нибудь упасть так и мы у нас есть гарантия что мы склеили работает иногда не работает да и вот у нас с границами работ какой-то промежуток времени поэтому клиенты должны это учитывать периодически мы убиваем production and master либо если мы захотим сделать фил over иногда мы убиваем все славы что посмотреть как придется missing репликация и вот у нас мастер уже не может ничего записать донос еще доступна чтения клиенты готовы к этим ошибку то есть она заменой смерть мастера и клиента готовы почему это хорошо потому что ну у нас был такой случай недавно который привел к отряду пудра это когда считают что бизнес компании под угрозой находится у нас происходил промоушены одна система при промоушене 1 шар до чечни 4 рядов 2600 было берите прогибался до 20 процентов за 4 шага к царству не так из четырех шагов 1600 должны какие-то другие цифры но вот этим сила веры для этой системы приходили срочно редко 1 месяц примерно ну это flower ну какая-то ерунда ну бывает в какой-то момент при когда мы переходили на новую систему один человек и а ты расскажешь нас два сервиса которые пишут их орбиты он решил оптимизировать этот момент увидите два сервиса объединил в один объединил в 1 и этот один сервис сделал еще что-то и в конечном итоге он умирал и переставали хоббиты запуска записываться вот так получилось что вот для этого клиента у нас было восемь фигур в день и все лежал 20 процентов выловили вот code red база не работают что что вы делаете в команде баз данных разогнать вас все оказалось оказалось что в этом клиенте тайм-аут был keep-alive 6 часов соответственно как только мастер умирал нас все connection держались к этому мастеру еще шесть часов соответственно нас пул не мог дальше работать у него каноиста держится он ограничен и не работает это починили делаем flower опять уже не 20 процентов на все равно там 80 процентов что все равно не так должно быть меньше оказалось что бага в реализации пул там пула при запросе он ходил вам на ваш ардов потом соединяла то сидел и если какие-то шарды фил уверились происходил какой-то рейс кондишен в коде и весь пул забивался все эти шорты больше не монсеррат поэтому disaster recovery тестирование очень полезно потому что клиенты должны быть готовы к этим ошибкам клиенты должны проверять свой код когда у вас система не работает ну и + adidas recovery тестер нездоровый тем что это бизнеса часы дежурство меньше стресса люди знают что сейчас произойдет они это не ночью что-то происходит это очень здорово это прям манна небесная застрелю ковры тестирование не заключения что нужно делать во первых все нужно автоматизировать никогда не лезь руками каждый раз когда у нас фаталист руками у нас все умирают сил понравится каждый божий раз простые операции вроде умер один slave человек должен был добавить 2 slave вместо того чтобы и он решил удалить тут своих умерших руками из топологии вместо того слова он скопировал живой своих у нас мастер без слов остался это такие операции не должны делать вручную тесто должно быть постоянной автоматизированный какой-то человек буквально позавчера мы обсуждали и вот он говорил что вот мы начали используя систему вот мы и проверили она работает вот система же меняются до ваша система меняется ваш инструмент и если в один раз проверили она вроде работать и знать что он быть завтра работать поэтому нужно постоянно каждый день автоматизированное тестирование делать в продакшн и в том числе обязательно нужно владеть клиентами библиотеками потому что пользователи могут не знать как работают базы данных пользователей могут не понимать зачем нужны тайм-аут зачем нужны keep-alive и поэтому лучше владеть этими клиентами и вам будет жить вам будет спокойнее и собственно нужно определить свои принципы по которым устроить определить свои гарантии всегда по ним действовать таким образом можно поддерживать 6 серверов силами всего 5 человек вопрос спасибо слава большое спасибо технологий трубок что она вообще у нас есть спасибо очень много вопросов но самое самое первое что если дисбаланса нагрузки на шарды то есть какой то какая там эта информация о каком-то файле оказалось популярнее если возможность этот шарф расплетите ли именно нагрузка на шарды она не отличается нигде на порядки не отличается на порядка на практически нормально распределена и у нас есть тротлинг то есть мы не можем перекрыть еще раз по сути наносим на уровне клиента но вообще бывает такое что какой-нибудь тимати или еще кто ну ладно не тимати какие-то американские звезды потому что другой популярный в америке выкладывать фотографию и это шар взрываются практически за мы баним это еще просто пожалуйста бресте вопрос по allure там кажется говорили 992 у вас до их собственно поподробнее что это такое-то из коробки или это создается если создается то это ручной труд или что-то вроде может быть машину обучения режим обучения спасибо за вопрос это все создается вручную нас есть собственная система внутренняя называется word- хранятся метрики в ней поддерживать солер ты есть некий файл описания яму в котором написано что вот из какой-то условия например что бэкапы должны выполняться каждый день и если это условие выполняется не происходит выполнялись извините название какой еще раз назвали как называется vortex на это внутренняя система она ваша разработка да потому что никто не умеет хранить столько метрик как нам нужно спасибо можно я здесь вопрос такой насколько крепкий должны быть нервы чтобы делать dirty ну так вот уронил code red и не поднимается из каждой минутой панике все больше и соответственно ну вообще работать базы данных это реально очень боль потому что если у кого-то что-то падает до и это сервис какой-то там небольшой ну ладно фиксе если база данных упала такой сервис не работает весь другой все работы да это реально боль dirty а полезна тем что это бизнес часы то есть я готов я слежу за рабочим столом я выпил кофе я свеж я готов все что угодно сделать хуже когда это происходит в 4 часа ночи и это не 9 например последнюю сильные аутрич у нас был месяц назад в общем при вливании новой системы мы забыли вы ставите won't score для наших мы склеили и там еще была другой сервис который читал берлоге почему он читал биологи другой вопрос но читать берлоге чтобы потом делать нотификации при изменении кисту данных на специальных анализов неважно какой то момент наш оператор вручную я опять же говорю причислила делает вещи он запустил команду по удалению перка ночи ксанти был такой информации просто удаление обычная операция эта операция породила огромный белок это этот сервис прочитал этот блок в память его ямки ли я пришел и думает кого бы убить а мы забыли вам скоро выставить и он убивает москве у нас в 4 часа ночи иначе умирают 40 мастеров когда умирает 40 мастеров это реально очень страшно и опасно 9 это не страшно и не опасно и мы лежали целый час и как стать и 9 это хороший способ от репетировать вот такие моменты чтобы мы точно знали какая посвятили здесь нужно сюда филдс добрый день с позирования клад хотел бы подробнее узнать про переключение мастер мастер во-первых хотелось бы узнать почему не используется кластер к примеру кластер до кластер баз данных то есть не в мастер слоев с переключением мастер мастер аппликация чтобы если один упал то и не страшно вы имеете виду caitlyn галеру что-то вроде вот этого стал армандо мне кажется через ну группе как я ещё не готов к жизни про коллирую мы к сожалению не пробовали у всех этих решений они выглядят то есть это здорово когда failover есть внутри протокола вашего да есть форму танцы работает но к сожалению у них есть очень много проблем вот и не так просто перейти на это решение кажется в маскел 8 есть что-то типа и надо бы кластер не пробовали у нас сих пор еще стоит я не знаю когда мы придем на 8 карт вопросы хорошо в таком случае если у вас есть один большой мастер которому подтягиваются своего и нужно переключить одного мастера на другой то получается на свадьбах под высокой нагрузкой скапливается очередь если мастер погасит надо чтобы очередь добежала чтобы своих переключить же мастера или как-то по-другому это делается но slave ой мастер он нагрузка на мастер регулируется 7 сен com правильно то есть хельсинг ограничивается запись на мастер производительности слоев соответственно славы могут конечно может быть такое что транзакция пришла с вы прочитали 7 сен кат работал но славы очень долго проигрывает эта транзакция нужно подождать пока слоев проиграет о транзакции конца ну тогда на мастер будет поступать новые данные и когда мы запускаем процесс а про машину вот этого мы отключаем моют ряды на словах после этого мастер не может еще записать с missing репликация все мастера readonly в него могут делать в нему могут прийти фантомное чтение к сожалению но это другая проблема а тогда запись начинается в слой в которой становится мастером да после этого мы стараемся убить этот мастер старый и в новый slave как только мы все операции выполнили поменяли право доступное состав добавили пользе для записи что-то изменить и после того как мы поменяли readonly на ноль и поменяли в топологии все начали записываться живут в новой своей спасибо спасибо вам да а еще вот та 5 все красивые стоит машины на чем написаны эти скрипты и как сложно добавить новый шаг вот в какой-то момент мы поняли штамм нужен еще конь промежуточный шаг вот что что нужно сделать тому кто пишет эту систему скрипты написаны на питоне все скрипты которые просто делают какой-то скриптовый работ они написаны притоне все сервисы написан могу этому автор place или менеджер сервис он написан на java скрипт написан на питоне это наша как бы такая политика а мы ни разу не добавляли я имею ввиду вот стоит диаграмма что сначала выключаем о его триады потом еще и чет делаем вот насколько сложно поменять каталоге это несложно это просто в петровском ходе сделано и по нему раз генерироваться диаграмму это не то что там с этой диаграммы по нему а можно еще подробнее про тестирование то есть как написаны тесты как они разворачивают ноды это виртуалке это контейнер и до тестирование у нас все собирается базиль им у нас есть некие настроечные файлы и джейсон и и базиль поднимает скрипт который поэтому на строчному файлу создает топологию для нашего теста там разные топология пить виртуалке или это или прямо наливает на машине не либо у нас это все работой в доки в контейнерах либо это в кантине стать грешно работает либо наряды в боксе у нас есть система d-box мы все разрабатываем на неком удаленном сервере и это может на нем работать например там а это тоже запускается внутри базиль внутри докер-контейнер все это либо в sandboxie базе да базе очень сложный но прикольный базиль это система google а для сборки для сборки и там много много интересных вещей можно делать описывать сервисы а зависимости сервисов как они запускаются друг друг от друга добрый день и спасибо за доклад фильм есть вопрос когда вы сделали на одном сервере 4 installs они потеряли вы его эффективности использования памяти эффективность использования памяти но каждый из нас он стал меньше в общей сложности до соответственно чем меньше память мы искали оперировать тему проще жить любой системе проще оперировать небольшим количеством памяти нет в этом месте мы ничего не потеряли у нас на низком ограничения у нас есть простейший группы который ограничивает по памяти эти инстанции но нет спасибо второй вопрос есть у вас шесть тысяч серверов хотят база данных можете назвать сколько миллиардов петабайт хранится ваших файлах и гигабайтов это десятки экзабайт я не могу сказать но мы мы переливали данные сама зона в течение года оси большой удачи спасибо можно вопрос у вас получается начать было 4 серия 80 у них по двести шагов получается потом соответственно 400 сироп на них по четыре шага до у вас вот 1600 шагов это жестко какое-то заданное значение вы больше не сможете ни как сделать ну то есть я папа изначально было сделать жестко и очень больно будет приезжать там есть вам понадобится три двести шагов скорее всего да это было сделано 10 лет чуть меньше десяти лет назад и до сих пор живем у вас еще есть четыре шар до 4 раза мы можем еще увеличить место ну и собственно есть мне есть операция сплит шортов это очень простая операция если com если вам интересно мы буквально две недели назад месяца заделались сплит у нас есть наш кластер мы добавляем по три новых своего в каждый кластер после этого мы выбираем куда таргет там написано до выбираем таргет будущий мастер репер интима майской или репликации на этот таргет после этого запускаем промоушен то есть мы выносим часть шар дав на этот мастер как мы их выносим мы просто удаляем здесь шарды кто здесь не должны быть переименовываю их в ремонт базы данных и наоборот другой стране таким образом у нас происходит сплит сплит нескольких петабайт данных вот лишь стороны давно вас произошел мы сделали за одну неделю все автоматизировано данной недели без каких-то потерь выловить спасибо можно еще про статистику как к умирают сервера под ким в основном причина что чаще происходит что реже и особенно интересно происходит ли и вот спонтанный коробки блоков вот типчик сумма не сошлась и ну типа по случайным причинам основном ну самое главное это все-таки диски вылетают да у нас raid 0 диск вылетал все мастер умер от самая главная проблема но нам проще заменить это сервер google у проще заменить это центр нам сервер пока вещь вот коробки and check sum у нас практически не было такого я если честно не помню когда последний раз такое было да по обычно мы не было достаточно давно такого вот просто мы достаточно часто мы обновляем мастера то есть у нас время жизни мастер ограничить на 60 днями то есть он не может жить больше 60 дней после этого мы его как-то изменяем на слив на новый сервер потому что с почему-то мы склеили постоянно что-то накапливается накапливается через 60 дней мы видим что начинает проблемам происходить мы склеили может быть не майский или может быть в линуксе мы не знаем что это за проблема мы не хотим с этим разбираться но мы просто в течение 60 дней ограничили время и обновляем как бы весь по сути стык не нужно примат пригибать к одному мастеру нужных 1 2 спасибо за доклад у меня такой вопрос вы искали что кажется за последний там шесть или семь дней можете восстановить из них резьбу капота на любое состояние да то есть например если человек залил там мы предположим кодекс одним название потом зайдет такой же портной измененные то вы можете достать там первую версию велик от за это время ну то есть вы храните version ность файлов получается это ну какими то данные с версиями или как мы храним информацию о файле облаках то есть а мы поделили на блоки имеют а человек если попросят я хочу достать первая версия файла вы сможете ему это дать да мы можем вдруг бокс есть возможность устанавливать файл есть встречи вопрос будет такой как вы потому вычищаете это не потом проблем с имитации на диск так далее ну то есть много данных стирается с диска получается чистка это время когда версия становится ненужной протухшие верху смысле допустим человек зайдет на десерт файл там поочередно заливал тысячи видно через там семь дней в бэкапе вы поймете что вам первые 6 версий уже не нужны и как большой из первых версий скорее что она просто потереть или как или они вечно хранятся незнаком что там какие-то гарантии есть другого за какой промежуток времени по моему скуку версия хранится просто помню это это немножко другое то есть если система которая умеет восстановить файлы там файлы просто не удаляются сразу несколько корзин кладутся до информация проблема когда совершенно все удалено файлы есть но в базе данных нету информации от нас есть блоки а как из этих блоков файл собрать вас есть скрываться как раз нас была проблема что при параллельном какой-то записи мы напортачили и у нас есть блоки а собрать мы ищем раньше и тогда мы решили эту проблему тем что зная структуру файлов описания как мы пытались собрать файл кроме а домовских autocad не смогли собрать и вот как в такой момент мы можем проиграть да ну до какого-то момента до остались на шесть дней проиграли до момент когда этот файл был удален не стали его удалять восстановили этот файл и отдали пользователю пассивом спасибо"
}