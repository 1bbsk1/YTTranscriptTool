{
  "video_id": "nMp5IscWEB0",
  "channel": "HighLoadChannel",
  "title": "DNS в Facebook / Олег Облеухов (Facebook)",
  "views": 1300,
  "duration": 2661,
  "published": "2019-05-14T14:36:04-07:00",
  "text": "всем привет спасибо что пришли и не знает 10 утра на второй день это прям испытание целая самом деле пришел на все хорошо надеюсь все попили кофе сейчас ли вы довольны ok можем начинать всем привет меня зовут олег я работаю production инженером фейсбук занимаюсь я многими проектами один из которых dns о котором я вам сегодня расскажу презентация будет довольно техническая так что я надеюсь все присутствующие имеют хоть какое-то представление о dns ok если у вас останутся вопросы или вы просто заходите со мной связаться пообщаться там пожалуйста свяжитесь со мной через linkedin мой лагерь логин указано на слайде все знают пути слайды будут на английском но вы не удивляйтесь я буду все равно все объяснять и показывать если вас станут вопрос пожалуйста хранить их до конца презентации спросите там окей я думаю с приветствием закончили можем начинать фейсбук мы оперируем огромном масштабе миллиарды пользователей и терабит и трафика заставляют нас постоянно оптимизировать систему и искать пути чтобы уменьшить нагрузку и улучшить качество сервиса и даже казалось бы такой простой понятный древний сервис как dns становится большим интересным проектом сегодня мы поговорим о трех основных использованиях dns у facebook как мы балансируем нагрузку и причем здесь dns как ресурсные записи попадают в нашу глобальную инфраструктуру и наконец что такое dog food in и как мы используем его как мы используем dns в его организация просто чтобы напомнить в самом простом случае клиентский запрос выглядит следующим образом клиент делает dns запросы получает api вы 6 запись на самом деле мало активно продвигаем айпи вы 6 и у нас даже есть регионы где нету а пиво 4 вообще как вы знаете обычно в dns используется в dp хотя в некоторых случаях может использоваться в себе следующим шагом является установка ищите без соединения и отправка того самого get запроса выглядит все просто и привычно все мы видели это много раз в далеком 2011 году команда трафика решила измерить качество работы facebook во всем мире основной метрикой выбрали сетевую задержку соединение с серверами facebook в то время у нас было только 3 дата центра и все они находились в сша как вы видите у пользователей северной америке блины лучшее качество сервиса однако чем дальше пользователи находились а дата-центров тем хуже становился опыт использования facebook мы не могли так просто взять и построить дата центры по всему миру поэтому команда стала думать как уменьшить сетевые задержки и улучшить качество пользования сервисом на текущий момент мы анонсировали 12 дата-центров ситуация разумеется улучшилась однако как вы все еще можете видеть большинство из них находятся в штатах давайте посмотрим на простой пример где пользователи скорее подключается к одному из наших датацентров в сша сетевая задержка в одну сторону через океан то конечно примерная задержка но более-менее правда правильная составляет 75 миллисекунд что не так долго казалось бы но мы с вами скоро увидим почему это плохо давайте разобьем один простейший ишите пьес запрос на составляющие и посмотрим с точки зрения протоколов мы все знаем что установка те типы соединения потребует от нас рукопожатия состоящего из трех шагов сян сян ak и aq для этого нам понадобится как минимум 1 round trip что займет 150 миллисекунд затем нам нужно установить тела и соединения это 4 этап на рукопожатие которая потребует а нас два дополнительных round trip ну и наконец сможет править тот самый ищите пи запрос ради которого все и затевалось для этого нам понадобится еще один раунд rip и так чтобы просто сделать простой базовый запрос нам понадобится ужасающей 600 миллисекунд решение так называемый point of presence точки присутствия или просто по по факту несколько серверов роутеров которые терменируйте себе и tls и прокси рует оригинальный запроса дата-центры они разбросаны по всему миру и мы постоянно добавляем новые мы стараемся устанавливать их максимально близко к конечным пользователям давайте теперь посмотрим как улучшится сетевая задержка для все того же пользователей скорее если те цепи и tls будут терменировали серверами в том же регионе они зато центром у нас есть паб в азии круговая задержка для которого составляет 30 миллисекунд установка тисе пи и tls теперь займет всего лишь 90 миллисекунд вместо 450 и теперь мы можем отправить тот самый тебе запрос и прокси ровать его через океан общее время составит всего лишь 240 миллисекунд просто из-за того что мы добавили несколько серверов поближе к пользователям мы уменьшили задержки более чем в два раза а как же устроен поп внутри на самом деле примерно так же как и дата-центр только в гораздо меньшем масштабе клиентский запрос попадает на один из наших роутеров и по и семьи на балансировщик и нагрузки основная разница между поп и дата-центром здесь в том что у нас нет веб-серверов и пользовательский запрос просто прокси руется в основной дата-центр по заранее зашифрованному туннелю все это звучит здорово да но как же мы решаем на какой поп отправить пользователя очевидно нам нужно какой-то дополнительный метод балансировки нагрузки на которой мы можем активно влиять нам на помощь приходит dns и наша система картографа которая постоянно изменяет и измеряет наше введение карта интернета как же мы решаем на какой по подправить пользователя вот несколько метрик и критерия в которыми мы руководствуемся близость к пользователя если вы когда-либо пробовали делать trace рот-то могли заметить что маршруты в интернете не всегда выглядит так как вы себе их представляли провайдеры зачастую оптимизирует сеть под свои нужды не сильно заботясь о пользователях конкретного сайта опять же очень важно понимать что физическая близость к поп не значит что он самый близкий по сети загруженность как мы уже знаем поп состоит из серверов роутеров ресурсы которых конечно один поп физически не сможет обслуживать всех пользователей в регионе поэтому мы как минимум должны обратить внимание на утилизацию поп состоянии помимо очевидного выхода из строя сам поп может быть в отличном состоянии а вот провайдер испытывать трудности наиболее частые причины отказа бывает обрыв кабеля или даже отсутствии электричества особенно в проблемных регионах в некоторых локациях даже электричества периодически выключают на ночь если у нас недостаточно данных мы все еще можем из учитывать физическую удаленность так как же мы мелем ту самую близость к пользователю при помощи нашей системы которая называется sonar примерно каждые 40 1000 запрос мы отправляем на ней оптимальный поп например вместо ближайшего папа азии ваш запрос на аватар например может быть отправлен на и ведь даже на другой континент несмотря на то что это происходит не так часто и пользователи не замечают этого мы все еще получаем очень много данных и очень неплохой сетевую карту интернета и так картографа эта система которая получает все эти данные создает карту и решает какой айпи адрес вернуть пользователю на запрос на dns запрос вся карта обновляется примерно 1 минуту и мы можем активно реагировать на любые изменения в сети dns карта еще известно как сплит horizon dns где зависимости от айпи адреса клиент а мы можем возвращать разные результаты это открытый стандарт и каждый может его внедрить у себя на этом слайде вы видите еще одно представление процесса где пользователи приходят через разных провайдеров и получают айпи адрес наиболее оптимального для них поп процесс доставки dns-записи и карты на сервера выглядит следующим образом картографа генерирует dns карту формате для тайне днс dns publisher компилирует этот файл в базу данных сидибе запускать целую кучу тестов и начинает раздачу через нашу торрент инфраструктуру все это оркестре руется через а по часу кипер на самом сервере все еще раз тестируются и база данных подменяется на этом слайде показан картографов действия зеленая линия подписано как capacity лимит указывает на лимит производительности до появления картограф ира особенно пиво по вечерам нагрузка на некоторых поп здесь оно отмечено красной линией подписано как юзер demand была значительно выше чем они могли обеспечить однако другие поп были не dogru жены в это же время в некоторых случаях это вызывало отказ в работе или плохом качестве сервиса однако как только мы смогли использовать данные стоп мы смогли лучше балансировать нагрузку и использовать оборудование максимально эффективно синяя волнистой линии прямо на границе с лимитом производительности демонстрирует успешность работы картография так выглядит утилизация попав в европе вы можете видеть как в разных временных зонах просыпается все больше людей и в какой то момент мы достигаем лимита производительности картографа отлично реагирует и балансирует трафик между регионами сдвигая пользователи на альтернативный поп а это как выглядит утилизация на глобальном уровне где слева на графике видно успешная утилизация по в сша а справа в европе остальных частях света надо все же упомянуть о проблеме с публичными dns резольвер ами такими как google dns справедливости ради конкретно с google dns дела обстоят не так плохо как с другими риэлторами как вы помните наш корейский пользователь подключался через пап в азии однако если он использует публичный dns резольвер то ситуация становится гораздо хуже совсем не факт что публичные dns-сервера находится близко к пользователю вернее очень часто это совсем не так давайте представим себе что пользователь попал на сервер публичной dns инфраструктуры где-нибудь в канаде этот сервер начал рекурсии и разумеется попал на ближайший к нему поп из-за того что клиентам для нас является сервер публичной dns инфраструктуры мы вернем адрес папа не более оптимального для него они для оригинального пользователя ну и теперь пользователь обречен ходить через океан для каждого запроса теперь давайте поговорим о том как устроена внутренняя dns инфраструктура сразу отвечу на главный вопрос зачем отдельную инфраструктура все просто безопасность и производительность например мы не хотим делать доступными имена серверов извне ну и конечно же dns-сервера сервис dns должен быть максимально близко к сервисам в дата-центре в то же время до сна внешне dns инфраструктуру не должно влиять на работу внутренних сервисов хоть с другой стороны конечно внутренне и внешне эта структура организованы довольно схожим образом давайте рассмотрим типичные dns-сервер по и не каст запрос выпадает на open source найти широкий и рекурсивный inbound который пытается получить ответ от автор это тивных dns-серверов те в свою очередь разделены на шарды и таких шар дав у нас много для быстро ты распространения и дублированы для отказоустойчивости каждый шар это авторе то тивной тайне dns мы используем стаб зон чтобы балансировать трафик и распределять его самба у напоним вы могли заметить что он bound local рекурсии только на внутренний dns-сервера это сделано сознательно для безопасности в то же время у нас есть отдельный резольвер который только отдельные проекты могут использовать и который уже умеет рекурсии интернет вот так выглядит типичный остап зон конфигурация для n bounce правда для этого sharding пришлось организовать по dns зонам что вылилось в довольно большой конфиг для on баллона около двух мегабайт если хоп на релат занимает довольно продолжительное время хотя в целом от использования нба у нас только положительные впечатления он идет в ногу со временем и даже поддерживает и dns о котором мы еще с вами поговорим таких типичных dns-серверов у нас очень много и чтобы балансировать трафик между ними мы используем экзо би джи пи демон который через бюджет и сессию стар или top of rack switch анонсируют одинаковый и vip с каждого сервера работать заказа бюджет и очень просто нужно всего лишь создать файл на диске и обновлять его каждые несколько секунд контента два файла тривиальный вам просто нужно указать айпи адрес который вы хотите анонсировать и адрес бюджеты и устройство которое умеет принимать сессии из нескольких источников если обновление файла не произошло в соси устаревает и switch удаляет маршрут до хоста или же вы можете удалить маршрут сами просто заменив анонс новых дроу файле кто же делает изменение в dns записях ну во-первых сами инженеры делаю записи изменение записях покупка новых компаний валидации доменов и так далее все это требует возможность изменять записи вручную однако люди могут допускать ошибки наиболее действенным методом поимки которых является кадре view и тут даже тут мы автоматизировали процесс у нас есть ли интер который проверяет на самые распространенные ошибки такие как дубликаты не правильный формат данных или циклы после того как изменение попали в конфигуратор в наше внутреннее управление систем управления фигу рациями оттуда они попадают уже на dns сервера вот пример совершенно в валидный с точки зрения dns записей которые могут ладить быть логически неверными и если первая запись совершенно верно и то две другие представляет собой не очевидные для человека ошибки которые могут теоретически положить весь сайт например мы не хотим отправлять трафик на айпи адрес который нам не принадлежит однако люди могут сделать такую простую ошибку но в основном конечно же сами сервера изменяют данные в dns каждый новый сервер контейнер попадает в нашу систему конечно же мы должны убедиться что этот процесс максимально быстрый безопасный и надежный для того чтобы запись из серверов попадали в наш dns у нас существует специальный фреймворк написанным на питоне у меня отдельно отношение к бетону но мы его используем просто потому что это очень популярный язык и все команды могут легко его освоите использовать отдельной команды могут создать свой уникальный task который будет экспортировать данные из абсолютно любого источника данных к нам в систему хорошим примером здесь будет апекс данный task следит за микро сайтами расположенными на внешнем облачном провайдере после получения айпи адреса он добавляет запись в наш dns существует множество других to suck так и как а по discovery например который добавляет адрес а сетевых карт и устройств подобного рода тоски через наш open source рпц 3вт пишет в dns мьют интер сервис написаны на гол который агрегирует его лидирует данные если новые данные не прошли проверку юпитер оставляет старую версию и сообщает об ошибке дежурному если данные прошли проверку они записываются в зуке пир нужно упомянуть что в зуке коране все еще хранятся независимо друг от друга и даже хранятся в разных отдых после того как данные попали место хранения мы переходим к следующему шагу объединению и логической валидации presenter сервис написаны на go который множество независимых источников генерирует один файл в тайне dns формате заодно он проверит что между записями нету дубликатов конфликтов если они все же есть презенты расставить приоритеты где например записи созданные вручную имеют наибольший вес вот так выглядит полная версия dns pipeline как и в случае с картографом publisher превращает результаты работы presenter в базу данных сидибе запускает тесты и начинает раздачи черри нашу торрент инфраструктуру тайне dns поддерживать все cup no reload что еще больше ускоряет процесс еще пара презентер publisher продублирована для разных швартов что тоже ускоряет процесс несмотря на то что pipeline выглядит довольно сложно с большим количеством проверок и компонентов большинство данных проходит через нее и распространяется на сотни dns серверов по bittorrent всего лишь за пару минут как вы можете видеть как минимум в пяти местах у нас производится проверка данных чтобы помогает предотвратить серьезные инциденты в какой-то момент записи даже распространялись настолько быстро что мы с ужасающей регулярностью начали убивать ssd-диски чтобы избежать этого мы переместили базу данных сидибе в нпф с тем самым сэкономили кучу ресурсов и еще больше ускорили работа dns прелесть этой papillon еще в том что нам совершенно не нужно ничего менять если мы добавляем или удаляем dns-сервера каждый клиент каждый сервер имеет полностью независимо представление и чтобы добавить новый сервер нам просто нужно установить нужный софт для него все же надо сказать пару слов про сидибе про которые я уже несколько раз упомянул сидибе расшифровывается как константа the bass и не случайно ее невозможно изменить можно только пересоздать написанная даниелем bange ней нам автором тайне dns это безумно быстрая база данных за счет неизменяемости и отсутствие log off однако с ней не очень просто работать код сильно запутан где например есть множество 1 буквенных переменных ну и конечно же такая чистота обновления в основном репозитории не может не смущать однако несмотря на это так никто и не сумел ее хакнуть и получить свою обещанную награду которая станешь было 1000 долларов сама сидибе получается из текстового файла записано в определенном формате у меня лично этот формат вызывается ужас и недоумение если немного разобрать формат для пива 4 записи то первый символ тут он указан как + означает эта запись а типа летел вместе станем с ним могут выполнять разные функции если указать и тел в ноль the time stamp будет как раз тем временем когда запись появится онлайн довольно удобно л.о. как раз является тем самым механизмом для сыплет horizon dns который так эффективно используется картография его по факту вы можете указать клиентскую потеть и только для нее вот эта запись будет возвращаться теперь немного статистики по нашей dns большинство запросов и это измерено с клиентом выполняются менее чем за десять миллисекунд суммарно у нас несколько гигабайт dns записей с таким количеством проверок ошибки очень сложно попасть на пруд и честно говоря я не помню ни одного серьезного инцидента за долгое время прохождения через pipeline занимает до пяти минут обычно около 1 минуты это значит что с момента как вы как инженер создали запись в своей базе данных в своем приложении мы будем отдавать айпи адрес в течение 1 минуты за счет сортирования достигается еще большая скорость распространения данных нельзя конечно же не упомянуть и о надежности нашей системы на каждом сервере facebook у нас установлена специальная проверка которая резолвится тестовые записи причем эти записи рандомизированы чтобы избежать получения ответа из кэша он bound все эти данные отправляются в нашу систему мониторинга и мы получаем alert и если качество сервиса ухудшается причем поскольку сервис все еще написать все еще использует в основном ю т п потери могут быть связаны как сетью так и с загруженностью самого клиента например несмотря на это в общем случае запрос клиента не приходят только на один из десяти миллионов запросов что является довольно неплохим результатам бог ходим это очень интересный термин который широко используется войти кругах по факту это означает что если бы мы производили собачий корм то были бы настолько уверены в его качестве что ели бы его сами так и с нашими сервисами мы настолько уверены в них что используем альфа версии чтобы тестировать последний vici и баги так причем с dns спросите вы как вы помните ли как вы помните пользователи делают запрос к нашему внешнему dns и получают айпи адрес наиболее оптимального для них для их расположения во внутреннем же dns оперативный сервер спрятан за inbound который обеспечивает рекурсию это значит что все запросы атон bound будут приходить к тайне dns и тайне dns не будет знать кто оригинальный клиент на самом деле многие могли заметить что это та же самая проблема что из публичными dns резольвер совсем недавно еще эта проблема решалась оригинальным методом у нас был dns идти мы его называли состоял он еще с одной пары unbound этой не dns on bound boston фигурировал таким образом что если на него приходили запросы от для facebook.com instagram.com или других бог пудинг доменов мы возвращали ответ с локального тайне dns который возвращал докладе gps в остальных же случаях он рекурсии впрос это хитрость и доработал довольно стабильно но добавляла сложной системе вообще вопрос с рекурсивными серверами между клиентом и after the team нам сервером волнует не только нас google amazon другие компании все сталкивались с этим ограничениям и также как вы помните это периодически влияло на работу картографов счастью не так давно и постепенно набирает все большую популярность расширенный стандарт для dns так называемый и dns суть проста клиент или артрит активный сервер извиняюсь клиент или рекурсивный сервер добавляют оригинальную клиентскую подсеть прямо в день с пакет который через сколь угодно рекурсивных серверов доходит до стрита тивного сервера если вам сложно это представить можете подумать о export for http заголовки из веб мира вот пример тисе пи дампа без и dns и с и dns где в dns пакете отчетливо видно сеть которую мы передали теперь мы можем с легкостью избавиться от dns сайте и уже знакомая нам схема будет выглядеть значительно проще где рекурсивный on bound просто добавляет и dns информацию о подсети в оригинальной подсети и передает ее of the ride активному серверу и теперь мы можем с легкостью вернуть доков уделяет и нашим сотрудникам и production айпи внешних заключение бы хотела сказать что несмотря на то что протокола dns уже более тридцати лет на его основе можно делать очень серьезное улучшение которые помогут экономить много времени и денег мы смогли уменьшить сетевые задержки для внешних пользователей и обеспечить доков один для внутренних пользователей и поскольку потакал не стоит на месте очень скоро вы можете увидеть интересное решение такие как dns о грехе tps практически все эти решения вы можете собрать самостоятельно на базе окон сложных компонентов большинство про большинство я уже рассказал некоторые упустил например прокси джейн этого заговора у него балансировщик нагрузки вы можете сравнить его с яндекс одной из функций эти чем это наш runtime для нашего собственного диалекта печки которая называется шаг айпи вес мы используем для низкоуровневого балансировки ну про стальной я более-менее рассказал на этом все спасибо за внимание привет я так вопрос вот и ты рассказал схему доставки изменения dns-записи до ваших серверов там получалось примерно две минуты но для 3 балансировки для третьего или для серверов физических все более менее понятно тайминги достаточно но вас наверняка внутри есть контейнер систем управления контейнерами и каждый контейнер как правило и всегда своей пи адрес и сервиса скорее всего часто сделал discovery на нужен sbs discovery и хочется понять как вы в этом с такой интеграции живете именно в контейнер нам мире потому что кажется что две минуты плюс каширования которые еще один assisted элем происходит она уже дают большие задержки по сравнению с той скоростью с которой контейнеры по системе теплиц да хороший вопрос на самом деле то есть как во первых сами dns сервера у нас не использу контейнер мы используем только в эрмитаже лески вот действительно нас постоянно появляются исчезают новые сервера им нам нужно контейнеры и нам нужно добавлять для них активно записи вот на текущий момент это действительно происходит в течении одной минуты то есть мы использовали разные шарды как я упомянул и вот именно контейнеры они используют отдельный shard и нас насколько насколько я помню то один из меньших шар дав то есть на самом деле propagation для конкретно вот этого шарда для the power это право это наша система контейнеризации занимает около 1 минуты вот то есть это пока что минимум который мы можем обеспечить но у нас в планах есть большое улучшение которое например поскольку у нас используется такая система которая называет у нас две системы на самом деле есть одна называется айла про которую есть много разговоров публичных и вторая называется api пор task то есть у нас есть контейнер и каждому добавляется api вот и те айпи мы можем предсказывать на основе каких-то параметров и у нас сейчас есть в планах чтобы мы эти api прямо отдавали из самого dns а генерируя по тому же алгоритму то есть у нас не будет про повешена вообще то есть это приходит ds мы даже лука в базу данных не будем делать в сидибе мы будем напрямую это отдавать то есть вот это есть план но пока что да у нас есть ограничение в одну минуту то есть контейнер стартует и да но справедливости ради контейнеры мы подключаемся не по dns в основном у нас есть специальные утилиты которые с кейджу лири это тех самых контейнеров обеспечивают и тебе предоставляют то есть ты делаешь cw как топор в ссср дальше контейнер и он уже попадает потому что он ходит с пейджером он знает айпи адрес то есть мы ходим без dns в контейнер и то есть это не то что прямо острая супер проблема но до хорошего дал хороший вопрос спасибо здравствуйте здрасте у меня вопрос по всего 1 чит за доклад у меня вопрос про составную часть dns конкретно против с я видел на 1 с кем у вас менее 100 миллисекунд на их и джейк и хотелось бы может быть если вы знаете приоткрыли завесу как вы достигли результата 90 миллисекунд для handshake сейчас я промотаю туда по быстренькому то не то но мы сейчас говорим про этот слайд да за раз какая да то есть 4 этап на рукопожатие с точки зрения сетевых задержек этом мы рассматриваем да то есть про то что там генерация занимает некоторое дополнительное время мы здесь не упоминаем но это за это приходится платить в любом случае но как еще есть происходит оптимизация и в принципе мы используем это насколько я помню что в некоторых случаях можно обойтись даже 2 этапным хан шейком если этот сервер когда-то недавно уже был использован то есть вот это вот время которое может добавиться на генерацию ключей мы используем на мы отсекаем на том что это повторное подключение и мы спали только двойных и джейк да но я очень много про это рассказать не смогу сожалению я не в этой команде работаю вот я больше именно по dns у специализируюсь но да все все все что я могу ответить на это вопрос спасибо за доклад меня зовут артём из компаний одноклассники хотел бы спросить последнее время dns стал тем местом куда больше всего через которое хакеры проникают сети и атакуют клиенты за счет подмены и так далее не валидации кэша и поэтому есть прям тенденция и на самом деле она уже довольно-таки старая защищать dns появляется dns скрипты подписи с днс зоны так далее можете вкратце рассказать про это не дрянной или нет у нас было довольно много x3 dns и каждая я понял да у нас было довольно много экспериментов с этим опробовали единый скрипта dns cd и на самом деле мы активно кантри beauty прямо сейчас вот почему я упомянул денисович дпс вот то есть вот эти все методы защиты мы действительно пробуем и пользу каком-то в мире их тестируем проблема в том что мы еще пока не выбрали потому что если мы делаем это мы делаем это хорошо и dns всех все еще в довольно плохом состоянии т.е. например тайне dns из коробки его не поддерживает нам приходится его самих самим имплементировать вот и если мы сами имплементировать мы можем это сделать не очень хорошо из-за этого это может быть какая-то опасность еще больше например да когда все доверяют а мы плохой сервис проводим вот из-за этого мы все так и не анонсируем это и мы еще не говорим что это готово но мы активно работаем действительно в этих направлениях но пока что денисович дпс выглядит наилучшим образом мы активно участвовали в россии когда у стакане были что конкретно мы хотим чтобы что мы видим в будущем вот то есть в будущем и steppes скорее всего я понял спасибо большое привет спасибо за доклад вопрос такое вы говорили что на записи настолько много что базы получаются в районе нескольких гигабайт их перезагрузка занимает довольно долгое время не приводит ли это к тому ну вернее монет уж не перед как вы решаете проблему с тем что данный сервер вынуждена постоянно перезагружать большие базы и заниматься только этим да то есть тот тут немножко мы перемешали парочку терминов то есть рилот долгое время занимает он бандана его конфигурацию потому что 2 мегабайта конфликт большой конфликт бан просто некогда по факту не используется с таким даром конфигом с сидибе конкретно с базой данных для тайне dns у нас таких проблем нет потому что она поддерживает сикхов no reload и по факту поскольку мы еще храним это втм pfs то этот сетап занимает миллисекунды то есть тайный dns он действительно тупой и он действительно однопоточный и он действительно подвисает на миллисекунды потому что мы делаем ему сикхов но по факту этот секрет только м м ap то есть он просто перематывает файлик который уже лежит в нпф из памяти то есть это происходит реально моментально и мы действительно это это вообще не проблема для нас то есть обрело занимает миллисекунды вот привет спасибо за код алексей check engine вот ты сказал что с помощью еды с вы можете получать айпи адрес исходного клиента да то есть но 4 курса попадают либо например как осуществляется гиа балансировка то есть вы используете какой-то своих партнеров допустим или вы сами вычисляете гиа расположения ну то есть вот близость клиента к попу сейчас я тоже на самом деле этот так ну ладно неважно на самом деле то есть у нас много пользователей пользуются фейсбуком и в вашем регионе ну предположим вот прямо из москвы возьмем пример да москве много людей пользоваться фейсбуком и мы знаем примерно из какой точки москвы сколько задержка происходит потому что мы постоянно каждый 40 1000 запрос как я упомянул вы постоянно отправляем на различные поп и пробуем их и у нас есть базы данных в нашей внутренней системе которую мы да это наша система который мы храним эти записи и мы говорим о кей вот из этого региона от этого провайдера более-менее вот до этого места такая вот задержка и мы постоянно это пробуем и например если поп какой-нибудь очень сильно загружен мы говорим о кей пользователь ты можешь двинуться на соседний потому что там загрузка тоже довольно более-менее приемлемое спасибо то что вы сделаете сами это какая-то необходимость ну так просто потому что допустим ваши партнеры не за тот же команд занимается тем же самым вы это делаете сами почему-то или вот так не знаю так исторически сложилось то есть 2 разных есть тоже вещи akamai это седин которую не только ну мы за ним и мы его используем как как сидел это очень мало на самом деле там меньше чем какой-то малюсенький процент сейчас логическая чуть лишние но не важна суть в том что вот картограф фирмы используем то так называемых динамических запросов когда пользователи приходят получить там основную веб-страницу когда им нужно статика и например вот ваш news feed ваша лента новостей когда вы на нее смотрите мы уже заранее inject им вам туда потому что мы знаем где вы находитесь более-менее мы туда inject им адреса поп который более-менее для вас оптимален враг в районе да то есть там плюс минус полмира вот и это 2 то есть это отдельный разговор про это можно вести как на шоссе дейэн работает но поскольку это всё сконцентрировано на поп то по факту 1 динамический запрос мы балансируем просто выдав вам правильный адрес а потом ваша news feed которая она формируется абсолютными юрий ламин на ближайший к вам поп окей вот все просто спасибо за доклад вопрос такое правильно я понимаю что их за бюджет используется для связи n баунти тайны dns то есть тех забить жди это отдельный демон который запущен на сервере и интерфейс не взаимодействие один из интерфейсов это просто файл мы в этот файл пишем вот этот айпишник который может быть в россии 1917 если правильно помню до внутренней api адреса приватные пи адреса мы просто ему говорим пожалуйста анонсирую вот этот айпишник с меня на switch мы и экзо бюджеты и и вот на этом мои печеньки может быть запущена нба und может быть запущен тайне ты не сможешь запущен ваш собственный сервис апачи что хотите не важность просто айпи адрес любой рандомный вы просто говорите пожалуйста switch который наверху за меня анонсирует что я здесь нахожусь вот и много серверов так делают и switch уже просто балансирует нагрузку между айпишник ами вот этими подсвечен показывается минут нет нет я то есть это физическое устройство мы называем top of rack switch то есть у нас есть стойка в ней куча серверов на самом верху у нас действительно самом верху стоит свечь который принимает gps сын bound как вот на сервере которая запущен это просто сервис это можно можно травмироваться от него представишь это apache или что это тайне и что это он bound что это что нибудь другой не важно он должен работа на каком-то пи адрес и правильно что мы делаем это мы анонсируем этот ай-пи-адрес к свечу и говорим я здесь нахожусь а до меня такая то метрика хлопов сколько там до меня прыгать вот и вторая часть второй вопрос про бюджет и рассматривали ли вы вариант следующий когда клиенту возвращается некий айпи адрес вот и этот ай-пи-адрес через бит gdp соответственно наносится там со мной а с другой так далее же провайдер выбирают кратчайший маршрут рассмотрели вариант вместо карт ургов карту грифера и вместе с ним ну то есть это было дока картограф мира по факту то есть мы изначально пользовались так это да это самый простой метод на самом деле когда вы анализируете одинаковое peace кучи разных мест и провайдер уже сами решают куда вам идти но во первых провайдере делают плохо и это очень долго контроль очень сложно контролируется и долго обновляется то есть если вы захотите выключить этот паб или на нем огромная нагрузка вы начинаете там меньше анонсировать пока это все располагаться по интернету ваш папа уже дымится будет вот мы его используем вот этот метод для того чтобы изначально вот там был слай когда в внешне как когда публичный dns-сервер пытался зари курсить и вот этот вот самый публичный dns-сервер он попал на ближайший паб по dns то есть он у он узнал наш автор это тивной dns у него а . н.с. facebook.com вот этот api адрес который он действительно работает вот именно по такой вот системе как вы описали то есть вот этот вот первая попасть на любой ближайший dns работает именно так ну дальше мы используем карта гроттер просто потому что это а быстрее bb mmorpg более предсказуемо и у нас лучший контроль override спасибо за вопрос алексей мтс сказать просто как вы определяете собственным ближайшего клиенты вообще адреса если dns не поддерживается поблёкли долларом white так что не присылает простых много не до конца понял вопрос в плане если если к вам не приходит одна единая 100 ну то есть мы по умолчанию работаем без и dns то есть к нам приходит то есть обычно клиенты ходят через своего провайдера то есть вы сидите дома у вас есть какой-то там провайдер этот провайдер приходит к нам как вот recourse рекурсивный dns-сервера да у них есть как правило провайдеров они приходят к нам и говорим о кейс этого провайдера мы знаем что примерно у такой талей концы и мы просто записываем это всем наша база данных и поскольку клиентом является именно провайдер не конечный подать наш конечный пользователь скорей сидит за натом у него там куча фаерволов и прочего к нам приходит именно рекурсивный сервер от провайдера и мы уже знаем что вот это согнет который нам провайдера слэш не знаю по пиво 4 там слэш пусть будет на 4 неважно приходит на мы знаем что для слоев 24 вот это вот сеть мы позже мы вот вернем вот этот адрес паб это вы говорите про провайдер ская игры про публичные да про публичный dns если они нам не передают и dns то эта проблема действительно и чего делаете вот вопрос собственных в этом ничего не делаем то есть мы ожидаем либо едины и и честно говоря то есть я упомянул что у гугла не так все с этим плохо по двум на самом деле причина первая google передает и dns то есть когда мы приходим к когда клиент приходит к dns они записывают ваши вашу потерять оригинально и передают ее то есть если вы поддерживаете вы уже можете этим пользоваться и второе это то что что google тоже делает они выход из ес их инфраструктуре их публичных на сервер ты тоже не один единственный сервер не с восьмерок и нам приходят вот что они делают они на самом деле понимают где мы находимся и они присылают нам пакет с айпи адреса который максимально близко к нам то есть они эту часть тоже оптимизируют но это только google так делает то есть там если вы пользуетесь какими-то странными dns резольвер ами то этого конечно не будет и вы будете мучиться потому что мы не можем определить другая ситуация если пользователь ну понятно да да спасибо за вопрос секунду скажите неужели вот такой оптимизациях и джейка действительно стоит свеч да очень стоит во первых это гораздо лучший опыт использования сервисами то есть когда пользователь не сидит и не ждёт загрузку и полсекунды 600 миллисекунд чувствуется действительно для пользователя и запросов мы делаем очень много естественно то есть каждая как каждое обновление то есть это это много сильно запросов это 10 чувство разным а записям к разным да то есть множество запросов к различным на нас есть дополнительные сервисы естественно то есть есть веб есть дополнить дополнительно какие то есть мобильное приложение она ходит не взять на к одной точки только то есть она может ходить к разным дополнительным . спасибо"
}