{
  "video_id": "A-Mcve9qYMM",
  "channel": "HighLoadChannel",
  "title": "Хранилища данных на службе BI / Александр Крашенинников, Алексей Еремихин (Badoo)",
  "views": 5166,
  "duration": 3158,
  "published": "2020-04-27T13:29:15-07:00",
  "text": "так что же такое bio их ну с точки зрения бизнеса самый короткий ответ это люди делающий графики из данных конечно же это очень узкое видение но к сожалению это так бей нужно уметь команде бей нужно уметь работать с данными работать эффективно и знать данные понимать данные так ну с данными конечно можно работать в их цели но на практике все таки данные находятся в хранилищах но и хранилище это базы данных так что в этом докладе поэтому практически синонимы хранилище базы данных какие бывают хранились ну бывает встроенный совсем маленькие базы данных это иску light up hdr мечту и это но такие специфические базы данных которые в г практически не применяются бывает обычный базы данных с которым все хорошо знакомо это москве или этапа сброс у кого-то это может быть обеим дебюту но общее у них это то что один инстансы базы данных работать на одном сервере бывает параллельные распределенные базы данных которые рабу могут работать на множестве серверов некоторые в облаках некоторые нет и с этими именами но наверное с половины из них вы хорошо знакомы с в россии еще более или менее известно vertica но здесь еще присутствует их засол который будет в дальнейшем то о чем я буду рассказывать она применима ко всем базам данных из этого списка и не принципиально для конкретного вендора ну если сравнить размеры и сложность баз и соответственно данных них но естественно распределенной базы данных ворочает огромными количествами данных и делают это хорошо буду бы в этом докладе мы будем рассматривать как стандартной базы типа mais quel вас грея так и распределенные базы данных конкретно вот эти то есть mais quel hadoop распределенная файловая система и куча обработчиков над ней и тосол аналитической базы данных и клик house данных которые претендуют на то чтобы стать аналитической и возможно через сколько-то лет она такой станет так это лишний слайд значит сейчас придем классическому и целью еще что такое и только первый раз когда я услышал эти буквы я вообще ничего не понял экстракт transform load каждый раз когда открываешь какое-нибудь описании что же такое детей в видишь картинку вроде вот этого то есть у вас есть какой-то набор приложений которые работают свой лтп базой данных потом данные перекладываются вала базу данных из зала база данных идет какой-то reporting блин но непонятно вообще не фига не понятно что если заменить пару аббревиатур на этом слайде на что-нибудь более адекватное у вас есть набор приложений которые работали с mais quel базы данных даны прикладываются в базу данных экзо sol язык зашел делается reporting но стало понятнее так что же такое отель отель это перекладывание данных из одной базы данных другую базу данных очень коротко но на практике это эльф в больших проектах он чуть чуть более сложной и данные нужно перекладывать не только из морской или нужно перекладывать из ходу по из файлов из каких-то внешних опиши какие-нибудь особые сервисы партнеры компании которые предоставляют данные все это нужно собирать и засовывать в аналитическую базу данных чтобы делать reporting последнее время последние годы все больше говорят о том что используется ниги т л а ю л т суть это или состоит в том что данные вытащились из источников как-то между собой привились к виду удобному для reporting для reporting и легли в обычную базу данных в аналитическую базу данных для reporting а суть ей lte состоит немножко в другом этот подход подразумевает то что данные были взяты из кучу разных источников их положили в аналитическую базу данных после этого сделали какие-то трансформации преобразование подготовили данные для удобного reporting и после уже случается reporting так как же лучше всего укладывать данные из одной базы данных в другую какой формат для того формат данных подобрать чтобы это сделать но кто-то скажет о том что можно использовать отдых буду bcd pc drivers это в принципе правда это очень гибкий формат но проблема в том что он совершенно не производительный и для и не все базы данных имеют драйвера под подходящий для того чтобы быстро перикл перекладывать данные через задуматься и джитибиси что я буду продавать я буду продавать формат с в формат tsv вот он звучит как csv то есть comma separated но на самом деле он так separated это очень удобный формат удобен для компьютеров в нем нет проблем которые возникают с форматом все свои это экранирования запятых кавычек если вы думаете что усе что реализации сейфы ровно 1 и есть какой-то стандарт нет на самом деле у каждой базы данных в excel а кучу разного софта будет свое собственное видение как надо экране ровать переводы строк как надо экранировать кавычки и приводить из одного формата в другой это головняк вот когда говоришь штаб separated values все прямо начинают работать одинаково да конечно собственно основные разделители то бы переводы строк сложно встраивать в данное нет какого стандартного формата но сейчас говоря в данных бей таких данных немного также с форматом tsv очень удобно работать просто в командной строке если вдруг надо сделать какой-то отель при необходимости вместо того чтобы подключать какие-нибудь инструменты для реализации отеля можно подключить простые инструменты командной строки которые работают офигительно быстро с потоками данных формат tsv поддерживается везде вот чес-слово я видел много аналитических баз данных я видел много различных инструментов и у каждого из инструментов есть набор и у каждого из хранилищ данных есть набор инструментов для того чтобы работать с так separated потоком эффективно то есть быстро ну и случае чего если исходные данные приходят не из базы данных и приходит из какой-то описки самому все реализовать данные опишет втс в тоже не составляет никакого труда так что же все хранилище данных которые есть слева можно перед все данные перевести формат tsv засунуть в экзо sol применить какой-то набор из quelle а чтобы скомбинировать эти данные и предоставить reporting но как пример например у вас живых например у приложения есть так называемые push-уведомления у android свои ухо у эппла свои но не суть суть в том что как правило приложения составляет где-то себе логе о том что вот такие to push и были отправлены таким-то пользователям вот тогда-то эти пользователя открыли пуше вот собственно 2 logo надо затянуть в базу данных получится две таблицы две таблицы надо с джой нить между собой и получить собственно уже reporting покушаем например почитать какой-нибудь alton рейд или еще какие-нибудь метрики бизнесовые которые будут к этому времени мы разберем простейшую маленький элемент этой задачи как перегрузить данные из одной базы данных в другой конкретный из mais quel век засол но самым тривиальным простейшим но не самым эффективным способом будет перекачать данный через временный файл то и запустить одну команду mais quel сказать друг мой дорогой вы выберем не все поля из этой таблицы и положив файл ну я это у себя запустил это заняло 30 минут это скопировал 100 гигабайт данных по роллу диски мог сколько это времени заняло заняла продолжаем двигаться дальше после этого я говорю экзо солу ведь засов дорогой возьми пожалуйста данные вот в этом временном файле и импортирует это за него еще двадцать минут нехитрого вычислениями мы узнаем о том что для того чтобы переложить данных из одной базы данных другую базу данных мы потратили 50 минут но конечно же это можно сделать быстрее и очевидно что это можно сделать используя pipe как это будет выглядеть точно также мы берем экспорт данных из mais quel а берем импорт данных в экзо sol соединяем их paypal и все работает но в первом примере мы команде xd lol передавали имя временного файла и по умолчанию команда не могла есть данные из студии на путем нехитрого хака доступного в операционной системе linux можно можно открыть стандартный поток ввода как файл и работать с ним пример рабочий вот да я рассказал про двс т.н. пример рабочий но здесь важно не забыть о том что когда работаешь с вашим дерзнул использовать pipe где-нибудь по ночам важно еще указать опция о том что по excel иначе баш как результат выполнения команда вернет результат выполнение последний команды в пайпе это как правило не совсем то что нужно проверять нужно проверять все сделали сделали это за 30 минут то есть ограничение было вызвано тем насколько быстро мы вообще можем вытащить данные измаильского или но это был один сервер майской а что если у нас объем данных находится не на одном сервере а каким-то образом посажен размазан на разных серверах и нам нужно обойти эти сервера вытащить данные и засунуть их вокзал собираем нехитрый конструктор берем несколько команд mais quel объединяем их с конверт чтобы они запустили друг за другом и все это пойдем во внутрь как засола который берет данные и за сталина но основной недостаток приведенного подхода состоит в том что каждый измаильского ли серверов будет в процессе данные получили сначала обработается первый сервер потом второй потом 3 блин ну не хочется столько ждать хочется загрузить то это все побыстрее тем более что можно же обрабатывать параллельно и тут на помощь приходит замечательный инструмент найдены когда-то на гитхабе называется в диллайн комбайны в делаем комбайн это очень простой инструмент который берет несколько потоков и стадионов мер сжатых полу переводом строк так чтобы строчки были не порвана и отправляет в и хвосты дал очень производительный очень стабильный инструмент очень маленький соответственно пример который был перед этим можно переписать с использованием мы сделаем комбайн опять же это рабочий синтаксис баша но честно говоря когда вы начинаете работать с башен и начинаете писать такие конструкции вы и сразу загуглите есть такая страничка баш pitfalls очень легко гуглиться ну кто скачает слайда может кликнуть вот из этой конструкции лично я не нашел как отловить экзит коды всех команд можайску эля и поэтому не рекомендую такой подход в продакшене в качестве прототипа работает отлично в продакшене такое нельзя отчет а что надо делать production of products не может сделать весь этот функционал баши который мы рассмотрели но используя соответствующие ну используя более распространенные более общий языки программирования например в пхп использовать функцию про колпан в питоне это со процесс пил пан в яви и в прочих тоже есть какие-то альтернативы но для тех языков которые не умеют вот так вот легко работать с процессорами с процессами в операционной системе всегда есть набор work раундов вот то что файловый дескриптор на самом деле можно открывать просто как файлы которые находятся в пути можно создать pipe и дни безымянные как это делала баша сделать именованные pipe и соединяться с ними ну и уже использованный hacks это про dfa100 dnd в стыда от отличный способ если у вас в языке программирования слишком кросс-платформенным нет возможности получить и стыду ним и и стыда от вы просто открываете файл tv-102 двс ты дауд и просто работаете с ними опытный разработчик наверняка спросит а почему бы не использовать гану параллель ведь классные инструменты делают вроде бы все тоже самое но здесь первую очередь надо отметить что гнул параллел создавалась немножко для другой задачи он создавался для того чтобы запустить много команд параллельно и получить вывод вас туда у такой как если бы они запустились последовательно то есть он сначала накопит результаты а потом их заплачет в том порядке в котором они должны бы быть это почти правда и углу параллель соответственно есть опция которая разрешает мер жить output и данных но это работает неэффективно я не знаю в силу каких причин и как они реализовали на pearly что это работает неэффективно но гну параллель на обработки потоков данных живет гораздо больше ресурсов чем и в диллайн комбайна рассказал вот одну параллелям тоже еще есть такая история то что он сложен в отладке вот честное слово вот вроде бы запустил команды в параллель вроде бы они как-то выполнились а потом начинаешь проверять в каком порядке они выполнялись они раз выполнялись последовательно в силу того что ты где-то атрибут какой-нибудь не передал ну и вообще классика наверное с любым софтверным продуктом мы находили баги и очень неприятно находили баги потому что мы теряли данные мы запускали команды в параллель получали их результат и результат оказывался меньше чем в чем отдавали команда это пофиксил из какой-то версии гнул параллельно но репутацию нему подмочил очень сильно так почему мы пришли мы пришли к задача когда мы из нескольких mais quel серверов параллельно забыл затащили данные в один процесс который называется их делаем комбайн и эти данные каким-то образом засунули вокзал здесь стоит чуть-чуть добавить понимание что такое экзо sol как зашел это все-таки распределенная система и это не один сервер это множество серверов и мы получили вообще типичный болтовне когда у нас слева много серверов справа много серверов посередине один в делаем комбайн который будет являться ограничивающим фактором но на самом деле он очень производительный и гигабит даных протаскивает буквально использую 15 процентов цикл но вообще архитектурным и когда думаешь о том что все это будет скейлится хочется не иметь таких штук и захотелось сделать так а что если у нас данные примут напрямую будут причем в этой задаче вот на слайде нарисовано 3 mais quel сервера у нас речь идет про то что у нас сотни mais quel серверов у нас есть десятки экзо sol серверов и было бы здорово очень данные тащить непосредственно в их засол здесь уже конечно пришлось рассмотривать возможности специфичные для базы данных access all что он может что он не может в последних версиях наверное год назад добавил замечательная возможность писать хранимые процедуры причем очень общий хранимой процедуры на любом языке программирования и эти храним ки позволили нам просто организовать вот такой конвейер когда 20 серверов экзо sol обходят сотни моей склеили с почти одинаковыми запросами собирает данные комбинирует это между собой и этот подход оказался в пять раз быстрее чем стримить данные через одну машину это отличный результат теперь вернемся к исходной задаче когда у нас был блог отправки пашей лог открытие пашей из него нужно было сделать какой-то reporting но вроде загрузили данные одни да мы загрузили другие данные написали какой нибудь как репей был с select join другая таблица все поехал а потом detail чуть усложняется и добавляются другие таблицы которые тоже нужно загрузить для того чтобы сделать этот reporting и через несколько итераций приходят приходится смотреть вот например на такой граф но рассказ вот про этот граф и как с ним работать он выходит за рамки этого доклада поэтому я заканчиваю свою часть и передоверять александра александр вы в эфире отлично я веду свой рассказ с водопада затапливай холодный коридор в центре но на самом деле водопад не более чем и химизм когда мы говорим о потоке событий потому что поток это водопад и это мощно круто и так далее откуда поток событий ребята из продуктового отдела они привыкли приходить в бей и задавать довольно неудобные вопросы а сколько у нас ты этого а сколько этого его на 100 в данных происходит и все такое и не всегда у нас достаточно данных чтобы ответить на вопросы совместно мы начинаем собирать какую дату эту информацию с нашего приложения как у нас все ведут пользователи треккинга их поведений прочее прочее ещё особенность продуктового дело что не очень очень очень любит apts ты и по дабы тест у них в голове рождается какая-то новая метрика это все надо высчитывать это надо строить reporting и так далее и обычно это не обходится без того чтобы в graph который лишь так что показывал добавить еще немножко энтропии занести еще один прямоугольничек и начать процессе ну очевидно что из мы хотим сохранить данные где-то в нашем политическом стороны надо его по altered и все такое все такое потом еще кто-то должен прийти и настроить продуктом отдел даже борды и это бесконечный выглядит как довольно похожи на обезьяне труд который хочется автоматизировать мы все-таки инженеры мы за то чтобы меньше работать руками и пусть вкалывают роботы не человек чем мы решили сделать мы понимаем что для того чтобы автоматизировать потоки данных их нужно сначала описать описать в таком виде чтобы они были понятны и человеку и машине на в 1 очередь машине дальше мы хотим автомат пировать доставку данных до хранилищ и мы хотим автоматически репортить мы не хотим больше ничего нигде кликать не хотим программистов погасить писать дашборд и формальное описание событий что это это одно место куда приходит аналитик и говорит ребята значит все наши мобильные устройства или прочие клиенты теперь нам в одном и том же формате с одним и тем же набором атрибутов всегда присылает одну и ту же статистику если пользователь проголосовал за другого то с любого устройства мобильного должна прийти одна и та же статистика и это должно понятно и человеку и машине то есть вот пример скриншотик из я я который говорит как для человека аналитика управляющего протоколом статистике выглядит вся эта история вот например у нас есть авторизация события и авторизации обычно есть какой-то способ могут представлять себе корочка войти через это это это можно было между собой договориться что ребята это у нас будет называться там facebook это email но более круто будет разумеется если описать это фиксирован набором значениям то есть йена мам и после этого с этой штукой можно будет творить невероятные истории например места циферки 1 написать слово email бу довольно понятный банально окей мы все это описали нам как-то клиенты наши договорились и стали слать события в нормальном формате согласовано но по-прежнему это пока еще не появилась нашем хранилище что требуется сгенерирует это событие преобразовать их в понятные для хранилища формат и загрузить нашем случае события прилетают в бэг-энд приложение оля не знаем все на реках знаете про яндекс метрику google analytics примерно такая же история то есть на пользовательские denis ты приложений и реверс статистика прилетает на бэг-энда дальше поскольку мы уже говорили что мы довольно большие данных много поэтому мы не будем рассматривать никакие мои scoinы и так далее мы это уже путь прошли выросли и мы будем сразу эти данной класть в ходу как она хранится в ходу по ходу пи мы используем функционал под названием hive который нам предоставляет абстракцию у sql доступа к слабо структурированным данным у нас есть одна табличка над всеми данными в ходу с трекингом пользователей и все атрибуты событий просто быть и между собой различаются хранятся в генерале зерном виде в mapi отображение строка в строку и чтобы это все эффективно хранилось используется бинарный к мощный формат арк какие преимущества нам эта штука дает я говорил о том что мы хотели избавиться от мартышки novate.ru да да и действительно если мы изменяем формат статистических событий нам не надо производить alter нашего хранилище потому что все в принципе резервного виде круто то что для того чтобы построить общую статистику над всем множеством ивентов например какие у нас браузеру полис где используется вообще из каких наших брендов или приложения ios android больше к нам приходит пользователей это все делается запросам одним по всей таблице то есть не надо делать и unions join и и прочее и это довольно круто и в этой штуке разумеется есть свои недостатки а именно что поскольку мы сделали генерализации там на хранилище оно не такое эффективное как если бы мы хранили атрибуты int в нативном формате но мы с этим живем и нас это устраивает поэтому надо думать и понимать подходит ли такой случай вам или нет понимать что бесплатный сыр только в мышеловке и всегда будут свои трейдов и как загружаются данные в в большой большой большой сторож живой ходу пи мы с клиентов принимаем данный формате джейсон преобразуемых загружаем в hadoop также в виде джейсона но орг сам по себе себя не сгенерирует надо построить какую-то инструкцию которая с джейсоном сделай торг мы это делаем несложной командой в huevito есть иска или запросам и цель виде hyip запроса океюшки и у нас получается ду новые данные так доезжают это все круто да но хочется аналитиком делать более сложные запросы и поэтому нам эту штуку надо переложить в специализированное аналитическое хранилище где истории про сложная конные функции прочие реализуется из коробки и гораздо быстрее чем если бы мы с этими данными ворочали в ходу пи аналитической базы такая штука с которой такие трюки с обобщенным хранением данных уже не подойдут надо нормально делать нормально будет сделать под каждое событие свою нормальную колоночку атрибутики тю-тю-тю-тю тю всё разложить и тогда это будет работать быстро круто эффективно но здесь мы применяем один hand вот у нас есть два события которые нас авторизация пользователя и просмотр профиля но них есть общая информация что это был какой-то пользователь у него был пол мужской женский или еще какой-нибудь это было в какой-то стране и так далее и вынесение общей атрибутики в отдельную таблицу нам позволяет экономить диска пространство которое в случае с нашей аналитической базы стоит денег но как правило в любой политической базе вы будете платить за диска и пространство или за лицензию основанную на дисковом пространстве поэтому об этом стоит думать какие мы бенефиты получаем от такой схемы хранения да мы эффективно разложили в int и в соответствующем колоночки строки в строки везде где можно под и подрезали поджали и отдельный reporting по события будет очень быстро и потому что тоненький тоненький хорошенькие колоночки их аккуратненько с диска вытаскиваем они там в с пейдж влезают все круто быстро недостатки то что если мы захотим поменять схему это обычно то о чем неудобные вопросы про которые редко распространяются да действительно надо будет пойти и сделать alter таблицы но поскольку мы решили что конфликт у нас машиночитаемой мы можем автоматизировать это и мы не можем сделать сквозной reporting по всем событиям потому что все данные лежат в разных табличках конечно можно написать около сотни или там трех сотен union олоф но делать этого конечно не будем потому что это очень странно выглядит как мы вы наставлять данные из одного хранилище в другой тут преимуществом что аналитические база как правил нам представляют кислотность то есть нас есть acid у нас есть транзакции и мы можем в одну транзакцию взять приложить данные из одного хранящих другой но и select insert commit просто как нарисовать сову рисуем раз кружочек 2 кружочек все осаго на деле все не так просто у нас есть довольно ощутимые объемы данных для нас это вообще кажется что многие такое собирают за год мы такой набираем за полчаса и все-таки это по-прежнему байтики если с ними на как над как-то аккуратненько поработать и кажется так что здесь пора подключать машина работы с большими данными у нас это spark мы его любим ценим холим и вообще стараемся если можно его то использовать нам он нравится и он классный мы выбираем данные из hive для того чтобы реплицировать их хранилище получаем некий dataframe в thermex парка и для каждого из событий у нас есть инструкция в какие колоночки он это сложить в базе но здесь важно помнить о том что конфиг конфигом реальность никто не отменял и может быть хранилище аналитической базы ещё не готова она не приведено в согласованность мы помним об этом что и об этом тоже надо думать надо сходить выяснить в какую схему мы будем засовывать данные формируем крутейший который уже алексей продавал формат тсв кладем это отдельными файлик ами в pdf с импортируемых засол поскольку я уже говорил что у нас классный формат арк для хранения данных то есть напрямую из орг в аналитическую базу не получится ведь нужен какой-то промежуточный формате который понимает аналитическая база мы используем функционал в pdf с потому что большинство современных вас умеют ходить куда-то во вне плохо т.п. и выглядит так как будто это не иллюзорный способ заработать перфоманса каждое событие кладется в свою отдельную папочку туда кладется более чем один файлик это важно потому что таким образом аналитическая база может в несколько поток в процессов неважно как она у вас там под капотом работает забирать данные из ходу по мы размазали дисковую нагрузку на чтение на запись мы их отлично мтс масштабировались парк job и потому что он так работает из коробки мы организовали опять взаимодействие между кластерными базами данных но даг ноги и мы можем таким образом перезаливать только часть событий например мы где-то по дороге свалились не смогли импортировать все и мы только какие-то события будем перегружать в следующем перезапуске то есть мы обеспечили дым патент ность кто знает обо как называется оба из этих мостов которые нарисованы картинках может не назвать просто поднимите руки довольно мало хорошо первый это мост на остров русский 2 это золотые ворота причем тут мосты зачем они нужны круто то что каждый из базы данных хороша по-своему то есть она умеет делать сосед вещей и делает это круто но не все задачи можно сделать решить используя функционал кадра представляет базы данных которым белых круто поэтому требуется обмен между большими кластерами базами и это требует наводить между ними мосты мы с вами рассмотрим три интеграции между распределенными базами это клик house hadoop и их засол каждую из них я очень люблю и вкратце расскажу вам про достоинства и недостатки каждой из них что мы знаем про клик house мы знаем о том что он не тормозит нормально делай нормально будет опять же все можно упоротее заставить работать плохо но если хорошо делать тонн обеспечивает сверхбыструю выборку в пределах одной таблице то есть и клайн шел скан мы туда можем реал тайме доставлять данные и можно делать сравнительно мелкие вставки почему я это сейчас говорю я раскрою чуть позже просто держа в уме что если редко то можно не очень большими пачками туда доставлять данные к сожалению в терминах египте которые шьют аналитики join там представлена в недостаточной степени и также есть мы все хотим что-то joy нить нам нужно думать о том чтобы данные у нас легли локально на одну ноту и таблиц котором будем же не должны лежать надо рядом принципе это справедливо для большинства баз данных чтобы не гонять данные по сети но включаюсь это особенно выражена что мы знаем про hadoop то что эта штука в котором можно кидать данные в любых форматах то есть это абстракция на уровне файловой системы хотим ксв хотим tsv хотим джейсон хотим еще что то мы его легко масштабируем докидываем узлы и все круто то есть это сравнительно дешевый способ масштабирования удобный удобно какие-то данные удалять выставлять больше репликации то есть какие-то дан остаются горячими и холодными и так далее и можно делать любые join то есть функционал который представляет нам хаев настройка над файловой системой любые join и но они далеко не всегда эффективны и никогда это быстро не будет то есть надо думать о том что ваши запросы никогда не будут быстрыми даже если вы будете использовать новомодные надстройки оля приста или еще что то мы не очень можем туда доставлять маленькие данные то есть мы это можем делать у нас неизбежно будет расти индекс файловой системы он весь в оперативной памяти надо думать о том как самому в бэкграунде маленький файлики сливать в один большой и это не очень удобно с точки зрения эксплуатации и нам надо думать как мы данные партиции ruim чтобы в один прекрасный день все таки удалить потому что ничто не вечно под луной и надо иногда подтирать что мы знаем об их засол аналитическая база данных которая заточена под хитрые мозги аналитиков умеет любые запросы выполнять делает это быстро или старается делать это быстро но есть одна вещь дисковой лицензии за не надо платить означает что мы не можем там тоже все вечно хранить мы можем удалять данные в любым способом по-любому предикату в.р. та-та-та-та-та-та но это нужно делать и особенности две базы ну это вообще в принципе у больших аналитических штук часто видно что тла надо лить большими пачками в случающиеся засолом это еще и нельзя делать в много потоков и так далее потому что число коннектор нас лимитировано общем мы с вами рассмотрели стороны видим что у них есть классные сильные стороны и какие то недостатки и всегда будет так что в одной базе лежит большой datasette который надо с джо нить чуть поменьше dtc там или еще что-то и мы будем брать от каждой базы лучше и доставлять недостающие данные интеграция cliff house и hadoop есть два способа первое это в cliff house не так давно появилась нативной интеграция с pdf и сам и второе мы можем всегда использовать наш любимый spark для того чтобы сделать то чего базы данных любая не представляет из коробки импорт из pdf с мы используем так назову табличную функцию под названием hd fs мы указываем url файла куда мы должны за ним прийти в каком формате будет и описываем схемку как у нас лежат данные в этом файлики это круто это работает внутри cliff house она работает даже не через в pdf с а через хищную рыбу либо из dfs то есть используют немножко другой протокол сейчас нет поддержки множество файлов поискать wildcard в этой папочки все файлики возьми и но эта задача находится в разработке в рот мы рано ли поздно на неизбежно будет сделано нам надо думать что писали то мою входу по одним способом а читаем коли хаусом и надо как-то шарить знание о том какие файлики в каких форматах у нас лежат и к сожалению это абсолютно не утилизирует бенефиты кластер моей базы данных потому что вы послали запрос какую-то ноду и все данные из pdf с у вас прокачиваются через одну машинку как можем быстро перевести не быстро а более производительно перетащить данные мы зачитаем все если house of spark преобразуем и каждую из пачек данных перепишем перепишем из ходу павкой cows используя гбц коннектор и нестандартный метод работы с бинарным протоколом почему именно его потому что реализация gtc из коробки в кли house драйвере не предназначены для того чтобы прокачивать многомиллионные объемы данных потому что банально чтобы перевести daytime в яву там производится что парсинг текстов представления строки что разумеется очень грустно и неэффективно это дает нам бенефиты такие что раз у нас spark мы можем чего угодно ещё с данными по пути преобразовать из hadoop of clear house высокий параллелизм утилизация кластерных фишек и так далее но нам нужно катить конечном инженеры любит отдела я сам это довольно уважаю и люблю это ограничивает наши скаут технологии с пользованием spark соответственно с сопутствующим отжимаем языками накладные расходы на то что мы из одного стороны читаем здесь и реализуем в память какой-то момент потом обратно сир лизу им отправляем в другой бла бла бла но в общем и целом я пока не нашел более эффективного способа для решения задачи из cliff house of ходу также использую интеграцию в коми хаусе либо через парк в кли хаусе опять же создаем уже не используем не табличную функцию создаем таблицу с определенным движком и туда вставляем данные проблемы все те же самые утилизации только одной ноты запись только в один файл и даже с внедрение функционал gala бинго я не уверен что все равно мы сможем писать в несколько файлов в хадисе используя spark опять же берем же tbc коннектор но не стандартный метод оттуда который нам позволит читать данные в бинарном формате в этом случае можем решать из парков какие ноды мы будем ходить и клик хаусу каким и данные боем select откуда и так далее то есть можно находить все и вы будете ходить все если вы пойдете по этому пути теперь как мы можем интегрировать cliff house какой-либо аналитической базы данных здесь к сожалению мы завязываем ся на функционал предоставляемой нам со стороны аналитической базы то есть существует у нас функционал импорт-экспорт который утилизирует хтб протокол как я уже говорил каждый уважающий база данных я считаю должна поддерживать мы обращаемся к как лихо асуна иваи степи интерфейс запросам select а если мы хотим выгрузить из аналитической базы вк ли house мы делаем тоже самое только insert и только не импорт и экспорт несложно здорово и когда у нас потребовалось пир вас интегрировать cliff house засола мне кажется задача была за полдня решена и уехала радостно в теле еще одним квадратиком графе дешево просто ничего не стоит реализации но опять ребята кому он у нас кластерной базы нас но дату ноды у нас 10 гигабит на интерфейс и все а мы прокачиваемся через одну машину исправлять можно нужно ходить и пока мы не доросли того чтобы начать ходить потому что она неплохо работает из коробки и так и нам остается 3 интеграция это hadoop и аналитическая база частично я уже упомянул о том что мы используем в pdf с данный раскладываем в csv и запускаем экзост импорт и опять же всегда можно подключить machinery виде спарка это в принципе не собрать это hyip в последние несколько лет уже перестать быть хайпом а просто кладется в копилку и отеля как ну молоток которым надо забить гвозди если мы используем зашел экспортом и один запрос можем экспортировать в ходу более чем в один файл то есть автоматическое распараллеливание идет у нас но к сожалению он ограничен только с в ну то есть tsv как подмножество s vs опциями и настройками и другой вариант у нас использовать иудеев про который говорил алексей то есть user define фанкшн в которых можно находить все хотите на lua мне нравится но я редко этим занимая к сожалению можно на яве можно на питоне в общем большинство распределенных аналитических бас предоставлять тает тыльной возможность функции скрипта вания и написание yusifov можно экспорте данный в любых форматах например мы стали писать и заказал салон сразу в морг то есть раньше мою форки формировали в хайве или спарки there мы это делаем с на стороне ю.д. и это прям очень круто потому что нам как минимум посети надо гонять меньше данных у мужчин в бинарный протокол шлем в ходу что я хотел вам рассказать бей это круто не только потому что это большие данные а потому что приходится решать очень необычной задачи с достаточно необычный способ вспомнить тот же самый line комбайн кто мог подумать и вообще до того пришел в бей не думал о том что так можно попить и между собой собирать мы знаем что есть круто большое многообразие различных систем кладно и не клубные есть ли хаос который неистовый hyip последний год-два но мы помним что у каждого хранилище есть свои сильные стороны и слабые стороны и надо подбирать их под задачу именно ту которую на но мы помним что если у вас есть две штуки то можно брать лучшего каждый из них не боятся экспериментировать потому что эксперимент это двигатель прогресса и инженеры всегда должны улучшать свой продукт мы будем делать эксперименты и в нем родиться что-то новое спасибо вам за ваше внимание спасибо что пришли доклад мы готовы ответить на ваши вопросы здравствуйте спасибо за доклад а вот такой момент вот много-много-много баша да как бы то есть такой некий кудри when you till dawn это исторически как-то сложилось да вот почему не взять какой-нибудь apache мифе бросить 3 квадратика запустить скрипт и как бы и все работает сова зачем писать в ваш начиналось это вообще все spin the data integration до моего прихода и понтах adaptation показанию отвратительную производительность на этих задач более того все трансформации которые там надо было делать если вставала задача прокинуть там еще на какой-то набор полей это огромнейшее количество пиксель хантинга о том что вот здесь вот надо не забыть здесь поправить все это записалась в xml который в убогом виде лед лег в git и непонятно кто какую правку сделал вот баш всего лишь скажем так удобный зэк прототипирование того как это можно сделать на одном из слайдов после баши я показал о том что нормальных языков программирования из функций который позволяет все вот эти позволяет собирать все вот эти конвейеры и не заниматься в рамках этого языка обработкой данных а просто собрать конвейер запустить процесс и соединить их между собой и дождаться их выполнение ну просто как бы то есть вы огромный контора наверняка у вас полно всяких умных слов оля damaged by the quality прочее то есть как бы построить как бы там да это длина edge папашу это невозможно наверняка аналитика приходится вопрос к а этот показатель и откуда и по скрипту его вытащить нереально мета моделей и теле тузов это в принципе выгребается как бы от в конечной точке отчета до инстанса базы данных но над прозрачностью мы сейчас работаем и особых трудностей в принципе не видим потому что как правило везде искры леску или скалы и склеим сложно но все-таки парсится и можно найти где источник данных и проследить полную цепочку от таблица reporting а до какого источника данных кто нам эти данные дал и вплоть до того что указать из какого компонента и какие разработчики компоненты отвечают за то что мы видим business report денги то есть прозрачность но все-таки выстроили пользу сразу здрасте спасибо спасибо за доклад вы сказали что у вас репорты строится автоматически то есть не нужно ничего там делать руками то есть правильно ли я понял что у какого менеджер отомсти холдера нет возможности пойти через какой-то интерфейс нам накликать нужно ему report либо какими-то простенькими да и с целями что-то сделать то что это же по сути частая задача типа дайте инструмент а я дальше сам решу какой грех я хочу смотрите моим далее данные лежащие хотите в ходу пи потоки лежат они там лет за пять десятки сотни терабайт сюда ходить хотите оперативный reporting из политической базы за последние 90 дней и мы им дали запилен apache запилен ей где мы должны сделаны что людям не надо получать кредит шился для доступа они получают привилегию в интернете хочу смотреть данные все открывается окошко сюда пожалуйста из кори свой пишите и визуализируйте данные как вам угодно мы для вас все положили если вы не заказали какой-то сложный продукт не всегда владеет всей информационной моделью данных которые у нас есть как за соль и например аналитической базе плис там сотни таблиц и только это знание обладает продуктовой аналитики отдел специальный к ним можно прийти с вопросом но если не хочется ждать можно прийти и сказать shut up and take my sql и пожалуйста его исполнить это есть да такая функциональность есть пожалуйста не надо ничего заказывать идите даже не в скучном каком-то программистом черно-белом экране с графиками и все вот этот историй теперь справа еще немножко расширить этот ответ александр говорил что у нас есть формально язык описания потока событий этот формальные мета-описание очень хорошо позволяет еще до появления первых данных подготовить даже борды автоматически еще в процессе проработки т.д. какие метрики нам нужны уже формализуется и часто уже под стороны под разные стороны под разные системы например под их зашел под хаев под пресс то мы можем уже просто генерировать ускорили запросы прям как часть технического задания на только разработку метрики на разработку фич и продукта спасибо спасибо за доклад хотел спросить насколько хорош и правда line комбайн в случае если вот этапу transform происходит на уровне никого приложения вот в моем случае написано wanna go вот и насколько вообще такая схема скажем легально философией теперь вот прослойка написанная на чем-то еще как на ну вот для тест форма данных как бы сказать мы по сути отказались от философии и цель и перестали использовать инструменты которые предназначены для детей а потому что ну была плохая история может быть сейчас появилось что-то лучше с касательно трансформации как правило базовой трансформации базовые преобразования можно выполнить еще в хранилище источники то есть если выбираешь данной из mais quel а можно заранее себе пакости типы можно заранее переставить столбцы при необходимости что-то да мер жить если это будет работать мои ско или быстро это можно сделать вот если хочется строить какой-то свой инструмент в этот pipeline это можно сделать но и скажем если у вас прям какая-то совсем адская бизнес-логика вы можете это сделать вы можете эту ласку бизнес лойко засунуть в аналитическую базу данных если получится но как показывает практика если что-то базовое можно сделать стандартными инструментами linux вот эти вот в кассеты проще лучше сделать ими как бы меня сейчас тут не ругали я просто ради эксперимента взяла попытался повторить вот команда есть такая вот эммануэль почитать количество строк в файле сейчас слова написать самостоятельно носи код который будет быстрее чем в c минусе очень сложно или хотя бы сравним с ним в два в три раза медленнее в моем случае просто работа с данными более сложное чем подсчетом строк и вот не выходит обойтись без вот этот прослойки попахивает шифрованием где-то канале туго много thread'ов делаете и раз параллели войти по возможность утилизируйте инструменты на самом деле вот эта параллельная обработка она у нас работает ни на одной машине у нас в pipe их который забирает в том числе есть ассаж на как бы у нас есть отдельное понятие data api который нам предоставляет такую некую абстракцию между b и мы большой системой баду большой инфраструктурой баду когда мы с точки зрения бей просто говорим а вот вот у нас короче эта пешка который сейчас вернет tsv и просто дергаем команду которая нам вернет tsv за реализацию за обратная совместимость за поддержку уже отвечает владельцы компонента откуда берутся данные они могут управлять этим спасибо можно параллельно хорошо можем принять еще два вопроса первый день спасибо за доклад меня интересует стрим данных которые вы получаете от своих приложений вы их храните в клик хансен то есть накапливаете в хаусе а потом не совсем не совсем так мы принимаем их на генерируем на букинге либо принимаем через and point at клиентских приложений дальше мы используем лсд потому что мы упоролись не хотим использовать краску л л с д т live streaming демон который позволяет нам с тысячи серверов агрегировать данные на кого-то меньшем количестве например на 8 это пока не хранилище у него нет ни копья и просто у нас откладываются файлики а дальше с этими файлик мы можем делать что хотим от них не дано им и заливаем в клик house другие мы заливаем в хату чтобы они потом приехали в экзо sol то есть я бы сказал что в качестве long тером стороны чтобы данные не терялись у нас hadoop заливаем джейсон преобразуем в бинарный арк и храним вечно нет файлики например вот файлик сервал там 300 мегабайт за не знает 15 секунд 40 секунд закончили его писать лсд отдал нам все мы его тут же отправили в ходу да это не по ивент на по мере возникновения данных в кли house точно так же у нас закончил файлик писаться мы его взяли перелили в crack house есть прочитали и преобразовали залили crack house мне в тот момент было ощущение что вывел из до использовали только ради название не мы его создали мы воссоздали дам спасибо за доклад у мой вопрос принципе перекликается с первым вопросом и сейчас вы тоже на него отвечать уже начали как раз хотела спросить почему вы пишете сначала файл и потом собственные из файла в дальнейшем отличный отличный вопрос всё дело в том что backend наш когда он принимает сообщение и запросы какие-то порождают статистику ему не нужно ходить куда-то даже в демона который служит локальный socket диск у нас есть всегда мы записали данный диск backend пишется в статистической событий в диск все потом кто-то следит за этими файле коми и агрегирует их на меньшее количество серверов агент лес то есть приложение вот в тот момент когда надо драгоценный или секунды лет он сына ответ пользователю мы не тратим их на то чтобы куда-то переслать данные вы простых записали в диск все свой локальный вот на applications сервере то есть у вас именно приложение на диск пишет да я так понял по презентации допустим у нас есть мой успел выжимаю скрыли загружаете файл файл передать это дуги которая делает в рамках теперь процессов но вот глюка до вас происходит авторизация на сайте какое-то серверное событие произошло там мы не знаю просто разработчики решили что-то померить какую-то статистику собрать это происходит так что я добавляю в коде вызов который записал в диск события да у нас в php пишем диск а для этель а как объяснимо использования файлов потому что получается фактически два раза читаем одни и теже даны два раза режим одни и теже данные суть в том что мы пишем с десятков и десятков ватт нас и тысяч серверов потом она сама дам доезжает на меньшее количество серверов образуется большие пачки данных с которыми удобно работать и пэйлин то есть это уже не streaming кабатчик вас получается то есть стриминговый модель событий с рода стекается откладывается файле коми которыми удобно работать понятно в принципе по производительности возможно это чуть хуже но это удобнее в плане использовании правильной да да это диск пока я не придумали человечьим хранить данные на дисках спасибо спасибо большое аплодисменты отличный доклад красиво какой вопрос был отличный он вот молодой человек который первый вопрос даровал нам и 1 до думаю да хорошо еще раз спасибо примите наши благодарности числом 2 поскольку вас двое до"
}