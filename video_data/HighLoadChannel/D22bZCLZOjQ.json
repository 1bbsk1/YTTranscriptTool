{
  "video_id": "D22bZCLZOjQ",
  "channel": "HighLoadChannel",
  "title": "Compute/Storage separation в Greenplum / Андрей Бородин (Yandex Cloud)",
  "views": 229,
  "duration": 2483,
  "published": "2024-10-29T03:08:38-07:00",
  "text": "а сейчас на сцену приглашаю человека который в нём кое-что понимает гораздо больше чем я Я приглашаю нану нашего следующего ведущего точнее докладчика Андрея руках с Всем привет да Яре Я занимаюсь тем что и базы на основе поса в интересах Яндекса Яндекс облака клиентов Яндекс облака Ну и в целом человечество допустим так я делаю Это не один вот эти ребята Они тоже вместе пришли у нас довольно большое подразделение в Екатеринбурге несколько человек в Москве 100% нашего кода в каждая рока с из периметра Яндекса и летит куда-нибудь в какой-нибудь Гид публично доступный это достаточно принципиально для нас позиция мы разрабатываем только базы данных моё подразделение разрабатывает только база данных которые родились и выросли в сообществе мы умеем общаться с сообществом мы слышим проблемы сообщества баги мы чиним которые случаются на нашем продакшене но чиним так чтобы помогать всем также мы развиваем пог в интересах облака И вот достаточно типичная облачная идея что база данных - это система которая функционирует на некоторой пирамиде на некоторой иерархии типов памяти Когда у вас есть дешёвые быстрые кши о дорогие быстрые кши маленького размера большие диски в которых объём уже не так дорог и в Облаке логичным продолжением продолжение в Облачное хранилище раньше мы говорили что самый дешёвый способ хранить данные это на ленточной системе или в каких-то таких системах совсем холодного хранения но сейчас всё-таки это облачные сервисы которые адаптированы для недорогого хранения данных когда мы Решаем проблему удлинение этой пирамиды У нас есть два подхода понятный подход справа это прозрачное кэширование Так мы работаем с кэша процессора они просто есть и мы о них не Думаем а также хотелось бы и решать проблему для базы данных просто сказать что наши данные живут совсем где-то в облачном хранилище а блочное устройство под базой данных - это просто м временный кэш но к сожалению это длительный процесс и такой вот подход справа это может быть и правильный подход но реализовать сразу его довольно сложно подход слева - это то что мы всегда делаем первым в первую очередь Когда у нас есть некоторое горячее хранилище в котором данные которые нужны прямо сейчас и некоторое холодное хранилище на уровне x П 1 где размещаются данные к которым доступ может быть не такой оперативный не такой быстрый вот проблематика использования облачного в качестве хранения данных лучше всего пожалуй рассмотрено в статье вышедшей на vb пару месяцев назад команда из мюнхенского университета технического во главе с Томасом Ной маном выпустили замечательную статью Если вы хотите разобраться в теме лучше не слушайте меня лучше почитайте томоса про эксплуатацию облачного хранилище где систематический даже не Инженерный а именно научный подход к вопросу Мы тоже примерно в то в той же среде работаем что и тум и примерно теми же вопросами задались Единственное что статья вышла Именно тогда когда мы уже выложили свою реализацию в Open Source в статье есть классная картинка которая показывает нам архитектуру облачного хранения Когда у нас есть балансе есть stateless сервера которые занимаются координации и два хранения одно объектное хранение которое Отт за целостность файлов и доступность и одна база данных которая реализует хранение метаданных то есть запоминает где конкретно те файлы лежат которые вы назвали определённым именем и если внимательно посмотреть на эту структуру мы увидим что эта структура очень типично для веб-сервиса так же как вы писали какой-нибудь Не знаю там ну там музыку Такси Маркет что-то ещё вы примерно также и создаёте Облачное хранилище как Просто обычное веб-приложение Вот и гарантии предоставляемые облачным хранилищем они очень похожи на гарантии базы данных целостность и доступность то что вы можете переживать отказ оборудования то что вы можете масштабировать использовать разные паттерны доступа к данным Вот но при всей похожести сервиса хранения данных на базу данных стоимость кардинально отличается заметил стоу Breaker в начале в конце 2010 годов но до сих пор этот Парадокс сохраняется и как-то человечество его достаточно агрессивно игнорировала то есть хранить данные в реляционной базе данных Ну вот на гигабайт горячих данных в 10 раз дороже чем в С3 тут у меня скриншот из статьи Томаса номана но если мы возм ту публично доступную object в Яндексе она прямо повторяет Один в один то что нарисовано номана то есть у нас есть некоторый слой за балансера который метаданные хранит в шарди прогрессе и пишет в МДС Облачное Облачное хранилище И если мы посмотрим на те же стоимости хранения этих данных в рублях Мы тоже самое обнаружим что вот приблизительно рубль на гиба месяц о Это значительно дороже чем хранение данных в пост гресе или в Грин ПМЕ или в в любой базе данных получается дороже кажется что надо двигаться в ту сторону чтобы те данные которые э которым мы не слишком часто Обращаемся какая-то не критичная часть аналитических данных всегда автоматически оказывалась в облачном хранилище а не на дорогих быстрых дисках под базой данных Так мы жили в начале этого проекта полтора года назад а об актуальности этого вопроса может ответить актуальность этого вопроса ещё показывает то что у нас несколько петабайт данных скопилось в Грин ПМЕ которые в основном расположены на быстрых дисках на самом дорогом типе дисков которые у нас есть сейчас это данные только публичного облака это не данные гнп внутри Яндекса только пользователей которые к нам пришли И что-то там анализируют на этих Грин ламах Вот Но то что всё к этому идёт и будет дальше расти Мы понимали полтора года назад а и 2 года назад да и да нам сразу всё понятно было что уж там Поэтому ещё до начала работы над менеджер Грин Плам мы поняли что нашему подразделению нужно работать над экспертизой в области разработки Грин плама также как мы работаем с прогрессом тем более что нпм - это просто много прогрессо очень удобно когда вы писали один один код пришли на новый проект а там тот же же самый код надо просто писать его с другой стороны наиболее такая проработанная система у нас именно нашими руками это система резервного копирования в постгрес поэтому первым делом мы систему резервного копирования взяли из пос греса и принесли её в нпм до того как эта технология появилась в Грин ПМЕ бэкапы делались по праздникам буквально то есть не потому что люди рады были потому что нагрузки не было в празднике Ну было остановить кластер и аккуратно сделать резервное копирование мы добавили технологию консистентной нагрузки бэкапа который обеспечивает почти Point and Time Recovery Единственное что нам надо в логах каждого сегмента расставлять точки консистентность на которые можно восстанавливаться их можно расставлять произвольно часто но но их надо ставить опять же в момент э установки точки консистентность нельзя чтобы чтобы были коммиты в прогрессе не транзакции в прогрессе а именно вот операция комита не должна в это каждая операция комита должна случиться до точки консистентность или после запустив эту технологию в Production мы обнаружили что низковицы и там лежат данные за 2010 год 2011 год 2012 год они очень редко меняются и не так много запросов прилетает в эти в в эти файлы поэтому мы сразу же сделали А дедупликация которая Э не не Занимает место многократно тем более что данные Грин плама нельзя ещё раз сжать потому что они уже сжатые кпы пос греса занимают совсем мало места потому что данные гресса расположены так чтобы удобно было их изменять Green plam append optimized э в первую очередь движок использует поэтому э Изменения к ним проходят редко вот э Глядя на эту оптимизацию нам стало понятно что многие пользователи настраивают доступ к данным которые лежат в облачном хранилище в object стод в виде отдельных CSV файлов Или там каких-нибудь авра или каких-нибудь ээ паркетов чего-то ещё А через pxf и через технологии доступа к данным которые живут совсем ври и казалось что вот эту штуку надо как-то автоматизировать как-то сделать поудобнее и одновременно с этим Мы заметили что в опенсорс появилась технология компании Neon что в этом интересного Неон - это аналогичная прямо идея взятая для мира поса полный аналог АС Авроры за исключением того что она развивается на гитхабе Вы можете на это повлиять и она пишется совместно с сообществом всё как мы любим мы начали взаимодействовать с коллегами из неона тем более имена оказались внезапно очень знакомыми вот рассмотрели что они делают конечно это транзакционная в первую очередь база но вещи которые нам казались полезными для аналитики То есть например одна из первых фишек которую они предоставляют - это ment и bottomless storage то есть э база больше не ограничена каким-то дискам которые сложно быстро доставить к серверу Ну например вот летом у нас кажется Toyota полежала за из-за того что у них диски закончились и быстро доставить под базу дисков новых не получилось несколько часов простое заводов и просто на такой вот старой проблеме что данные по-прежнему живут на блочном устройстве а не где-то в в сервисе Ну кроме того Neon решает понятную задачу авто слинга то есть они стремятся сделать дешёвым доступ к небольшим базам а Для нас это такая же задача только наоборот к большим нам нужно упростить масштабирование с с большой базы Да очень большой базы вот Ну вот у нас Кирилл здесь сидит он пришёл с интересным интересные идеи вот коллеги из неона принесли патч который делает заменя пое наст в среду разработки где разработчики обмениваются новым кодом для постгрес ему этот патч так понравился что вместо того чтобы просто написать ревью какое-то ревью он конечно написал Он принёс этот же патч в п и коллеги из гпма посмотрели на патч то что мы можем теперь думать о файле как нечто о чём-то расшир который может быть подн в зависимости от обстоятельств файл мы можем искать необязательно на файловой системе этот патч был принят и тогда же началась разработка расширения которое Кирилл назвал или не Кирилл ну в общем кто-то рядом с Кириллом назвал Изи Кирилл это ты был Нет ну в общем мы не знаем почему так но кажется Изи называется Easy что его просто использовать У вас есть одна Функция которую вы вызываете говорите вот эта таблица короче отсюда в ири или или обратно вот эту таблицу пожалуйста Короче нам на файловую систему по факту там происходит замена йл спейса она шифруется и уезжает в Облачное хранилище но при этом для селекто апдейт вот всего чего чего мы любим работает так же как раньше вот и идея была такая что Мы настроим автоматическое охлаждение холодных данных то есть автоматическое охлаж короче вот проект назывался автоматическое охлаждение Э мы даже запустили где-то на пятой и Шестой итерации стабильной версии мы запустили то что называется Stage это уже бизнес название того же самого То есть у нас есть технология открытая eas и она запущенная на называется вот E доступен в отдельном репозитории на гитхабе в этом же рери есть прокси Кирилл Что это значит Y Proxy Ну что-то значит короче Y Proxy Это про него я чуть попозже расскажу есть расширение Easy и есть Y GP это Green plam с небольшими доработками то есть вот той замены storage менеджера Хоть она и была ключевой оказалось немножко недостаточно Вот пример того чего Кирилл недавно фиксилэр в во вю в системной ВХ которая описывает Table спейсы таких Фиксиков сейчас набралось порядка нескольких десятков мы когда-нибудь рано или поздно их все понесём в астрим когда Easy станет более популярным сейчас надо для того чтобы у себя использовать Easy надо использовать э greenplum с актуальной версие greenplum 6 там 25 кажется 26 Сейчас с минимальными доработками которые не влияют на функционирование кластера не связанные с E То есть если вы себе эти патчи наложите ничего страшного с кластером не случится А кстати иногда нам приходится бкпорт какие-то кусочки из пос гресса которых тоже не хватает кажется что их бкпорт в гнп будет проще всего потому что они уже прошли ревью в сообществе греса самое интересное что мы провели Бенчмарк и в интернете можно найти пост с бенчмарка прямо в репозитории он лежит где мы проводим сравнение eas с pxf Понятно оно работает там существенно быстрее чем pxf но потом мы решили сравнить его ещё с нпм обычным на быстрых локальных дисках и Выяснилось что это не технология автоматического ж данных Это технология которая заменяет локальные диски на простых запросах E оказывается на 20 25% медленнее чем быстрые SSD диски это уже космический результат но на сложных запросах разницы между E и данными хранящимися локально нету это открытие которое случилось у нас летом пере то есть весь нейминг у на вот так вот по потому что же что чтобы данные не охлаждали Всем надо делать не кэширование холодных данных а надо делать First CL Citizen в облачном Реже и ээ Вектор развития E стал немножко меняться в направлении того что данные видимо будут жить совсем быстри примерно так же как и происходит с коммерческими сервисами типа redshift snowflake dat brix Они же все сделали так уже давно просто нам было не очевидно что хорошая вот э не всё получилось так как мы хотели изначально мы шли от бэкапов и идея была такая что Ну например при восстановлении кластера с Easy таблицами мы просто так вот стрелочку туда на тот файл который уже лежит в облачном реджен Но тот факт того что каждый кластер содержит уникальный ключ шифрования нам эту технологию немного поломал Поэтому при ресторе Easy кластера мы вынуждены скачать те данные из ри старого кластера из бэкапа которого мы восстанавливаемся расшифровать их снова зашифровать и снова положить в ри может быть здесь каким-то решением будет то что один кластер должен хранить может хранить несколько ключей шифрования Но кажется что безопасники не будут рады этой идее что у нас теперь все ключи все ключи шифрования доступны всем кластерам а вместе с тем все остальные части работают так же как и работали в обычном Грин ПМЕ то есть работает обычный вакуум Гай там что-то ещё работает также как раньше плохо работает expand ну он раньше Плохо работал и так же продолжит плохо работать но это мы и займёмся этим вопроси ком вот также работают альтеры и всё такое потому что нпм думает что файлы живут здесь соответственно и статистика также работает и орка также работает оптимизатор никакой разницы не не видит Мы думали что это проблема которую надо будет решать но внезапно это проблема которую не надо решать отлично хорошо когда работу можно не делать Вот если дочитать статью Томаса номана до конца то в конце он говорит это C effective штука это будет там в 10 раз дешевле чем хранить данные на быстром диске для аналитики но ещё система не будет работать без прокси мы запустили уже систему и поняли что да система не будет работать без прокси Вот примерно так Ээ как сказал кажется фейма сказал что месяц работы в лаборатории сэкономит вам 2 часа чтения статей Вот Вот Ну да теперь Ну мы-то сами догадались Мы же не не читали выводы нойман А пришли к этому через пальцы и клавиатуру э чего работает плохо и это сложно починить эпан работает плохо хотелось бы чтобы expand выглядел как-то Так что у нас есть P optimized File он оказался охлаждённым в стод мы его где-то там вот в облачном стод перешагивает поменять схему хэширования то что внутри называется cdb хш сейчас жёстко прибит к количеству сегментов Это означает что если мы хотим проблему победить нам нужно менять Менять Access Method optimized таблиц и это мы можем сделать только в седьмом гнп а не в шестом значит это разработка новой штуки кажется Кирилл или Юра её назвали inade Что это значит Это что-то из греческой мифологии В общем продолжение идеи с одиссеем этор что-то вот главная цель сейчас следующих разработок технологии Easy 2 или en - это получить автоматическое масштабирование мы бы хотели чтобы кластер можно было на ночь сдуть с утра раздуть и чтобы это стоило как можно дешевле в общем Кирилл говорит про ноль но ну как бы это Константа которая иногда оказывается слишком большой константой Возможно что в этой технологии получится автоматически э использовать шеринг между таблицами шеринг таблиц между кластерами Они же всё равно уже живут в облачном хранилище соответственно э часто есть такой запрос предоставьте нам для Грин плама систему отказоустойчивости систему то что называется standby cluster Зачем нам standby cler Если у нас данные и так уже в облачном хранилище мы просто те же самые файлы в другом кластере подключаем к подключаем к системе расчётов и у нас исчезает такая проблема Как которая известно как Data Gravity у нас нету необходимости данные хранить близко к месту возникновения запросов теперь задача подтаивать - Это задача разработчиков S3 разработчиков ОБД классно когда работу можно не делать или поручить смежники паттернов доступа и оказывается что в случае с ltp системой отказоустойчивость - это что-то что должно решаться внутри базы данных а в случае с аналитической системой это что-то что можно поручить другому сервису Очень удобно Вот такой есть план Ну кроме того есть и другие идеи что мы бы хотели в гнп увидеть проекции что в принципе Облачное хранилище теоретически Нам должно позволить иметь более удобный тат Ну пока мы не окончательно решили с тем как это как именно это будет организовано вс-таки первый приоритет у нас это автолинг и туда мы будем двигаться Кстати у нас не очень подробная документация у E но Ну что мы обычные Open Разработчики у нас достаточно подробная документация в hy То есть то что видит человек когда этим пользуется но она это документация к сервису но зато вот настроим который проверяет быстрое поднятие кластера в тестах и настройку его для доступа к S3 в виде ме и это в общем то что можно назвать само документи кодом самый простой способ воспользоваться eas это сделать то же самое что сделано в тестах всё обсуждение наших планов происходит в стандартном средстве планирования сообщество это рассылка gpdb Dev если у вас какие-то комментарии идеи и вы хотите принять участие как э визионер Да вот вы видите какую-то потребность которая как-то сходится как-то ко линеар нашему движению то Приходите в рассылку и Давайте вместе с всеми разработчиками Грин плама это обсуждать А если вы не готовы приходить в рассылку то в принципе можно написать и Ну кому-то из нас например мне или Кириллу или ри или в общем любому человеку в нашей команде или найти нас в каком-то из чатиком нпм Раша Поша или где-то ещё А есть очень много разных сервисов управления данными они все работают они все интересные но лучшие сервисы будут построены на основе технологий с открытым кодом Мы в это верим мы исследуем этой идеи и Давайте работать вместе с нами сделаем лучшее управления данными в мире Если есть какие-то вопросы Я был бы рад на них ответить Спасибо большое тебе за доклад так и Давайте поднимем руки у нас есть совершенно замечательные барышни Давайте сначала первый вопросик здесь на первом ряду и потом вот сюда микрофончик Андрей Спасибо большое за доклад У меня два вопроса Первый вопрос вот если оператор при вычислении вынужден сбросить данные на диск это пойдет в локальный диск или В3 при использовании E И второй вопрос вот в каком сейчас состоянии шда предикатов до3 мы это начнём с первого вопроса это не сейчас у нас нет системы рования кото было необходи сбросить что-то Нади чтение файла последовательное переведём в Читающий запрос Читающий запрос В3 в принципе нам бы хотелось сократить количество г запросов в какой-то момент у нас возникнет система локального каширования на диске она не ускоряет кластер это это не про локальное кэширование я Почему спрашиваю потому что в статье про snowflake они говорили что вот У них когда оператору нужно тем дату куда-то сбросить у них сначала не было просто на локальные диски а потом они это добавили М я понял речь идёт видимо О промежуточном результате хэш джоина А слушай интересная вообще тема Мы хотели почему вот возникла здесь вот этот вот может быть не очевидно что связь этого слайда с твоим вопросом вот там есть тема с проекциями откуда они возникли а самая сильная сторона Грин плама в сравнении например с кли хаусом то что он хорошо выполняет Джой о всё делает не очень быстро но джойн он умеет он может больших таблицы вот делает это не всегда оптимально И самое главное что он в этом случае ограничен производительностью не диска не процессора а сетевой инфраструктуры он утилизирует сеть в полку Может заставляя там все соседние виртуалки дропать пакеты потому что он хочет быстро-быстро передать много данные в квадратичного количестве для того чтобы эту проб ложить сообществу стандартное решение проекции Когда вы можете в некотором случае поменять ключи шардирование так чтобы ш не приводил к большому количеству обмена данными по сети и это снизит количество спилов то есть мы могли бы вложить свои силы в оптимизацию моушена и в оптимизацию спилов но вместо этого мы хотим их устранить не сжа дела пач на эту тему но он пока обсуждается в сообществе но лучше это не ускорять а устранять совсем вот наш план такой в целом временный отправить временные файлы в нормальный план Но тогда лучше бы эти данные и забирать из3 сразу без некоторого промежуточного хранилища Вот это бы ВОМ Во а потом остальное можно в кулуарах обсудить вот микрофончик сюда а вот этот микрофончик пожалуйста туда будет Вот мужчина стоит Да вот сначала здесь пожалуйста Андрей интересный доклад услышал очень важную фразу автоматическое охлаждение про охлаждение понятно то есть мы переносим данные туда либо переносим там их назад а Раскройте автоматическое есть какие-то политики охлаждения там чтобы Нет это устаревшая идея мы её закопали закопали Нет по-моему два раза назвали там мы хотели сделать автоматическое охлаждение мы ожидали что работа дисков будет быстрее чем работа S3 по факту паттерны доступа к файлам которые используются нпм из S3 в случае формата файлов оказываются настолько же эффективны как чтение с локальных дисков это что-то что у мне до сих пор в голове не укладывается Но это полностью зачеркну необходимость какого-либо охлаждения про но что-то медленное оказался процессом переноса в такое же его надо делать не для каких-то ненужных данных а для всех поэтому автоматики у нас больше нет у нас была в планах автоматика по времени автоматика которая анализирует паттерны нагрузки автоматика на основе алгоритмов вытеснения это всё оста иде во Здра Спасибо за доклад такой вопрос аленд Вы сравнивали соответственно ваш Грин план поверх S3 против какого-нибудь трина поверх айсберга С3 Нет спасибо короткий вопрос короткий ответ нам надо было получить некото обоснование что мы сделали что-то нужное который был а он вроде как нужен и получилось лучше значит как бы у нас все базы нужны все базы которые были зачем-то написаны они были написаны не просто так и у них есть сильные стороны у нас нет необходимости кого-то победить нам надо сделать что-то прикольное короче и полезное супер спасибо большое за вопрос вопрос здесь и Следующий вопрос от молодой че тя Руку вот сюда микрофон Здраствуйте Спасибо за доклад ческое масштабирование как мы знаем Сейчас gpx только возможен типа обратно его не сжать и вы его планируете наживую и какая вообще идея за этим стоит Потому что насколько я знаю сейчас координатор это в принципе не может поддержать с точки зрения сиго Кода да спасибо вопро Какая идея идея пока что это технология автоматического масштабирования на стадии проектирования нам нужно уметь сжимать кластер иначе пользователи будут бояться его разжимать То есть ты как бы начинаешь платить облаку больше И это необратимый процесс ты 10 раз подумаешь прежде чем это делать Это не будет автоматическим масштабированием если оно будет Только вверх вот э идеи которые лежат сейчас в основе проектирования такие Но это проектирование код конечно Кирилла очень сложно остановить он пишет код ещё до того как мы поняли что мы хотим это часто помогает принять правильное решение потому что ну потому что те решения которые он уже принял оказались правильными или нет Ну какая разница Вот хотелось бы уметь сжимать это Но нам нужно к сожалению от всей автоматики от всей механики хэширования отказаться весь cdb хэш идёт туда же куда пошло автоматическое охлаждение в историю и интересные идеи на будущее или прошлое в данном случае неважно вот нам нужно сжатие короче ответ такой но мы хотели бы его увидеть у себя так спасибо большое Так значит сейчас микрофончик здесь а вот этот микрофончик вот Поднятая рука Да вот вот вот вот мужчины Добрый день пожалуйста да а здесь Справа справа от Вас Угу вижу Да Иван Санин компания зн Если я правильно понял то вот у вас были супер быстрые диски супер дорогие диски вы увезли данные в медленные диски дешёвые и у вас получилась одинаковая скорость вопрос Где кроется секрет то есть это сильно не оптимально сейчас делаются запросы или Короче как так может быть коварный вопро есть несколько догадок Почему так получилось во-первых паттерн доступа гпма такое что все фишки SSD дисков они как бы не очень раскрывается он последовательно читает фай отлично с этим справляется потому что у вас там смотрят какой-нибудь КиноПоиск они последовательно читают файл или не знаю или пы восстанавливаются из3 они последовательно читают файл Вот ребята из команды S3 потратили кучу времени на то чтобы под этот паттерн оптимизировать то есть для S3 он удобный а для диска он как бы ну ничего такого интересного вот другая часть состоит в том что S3 тоже имеют свои кши они могут быть довольно большими они могут быть существенно больше чем ресурсы кластера и S3 потом за отдельные запросы как бы забили то есть по факту у вас вобще ресурсов получается Больше чем ресурсов только на том железе на котором ваш кластер находится ист может применить какие-нибудь там эмели для того чтобы оптимизировать догадаться что вот это данные нужные Вот это ненужные мы даже как бы не особо Разбираем что там внутри файла а они могут это сделать они могут поделить его на чанки понять что вы там префикс читаете они могут я не утверждаю что они это делают Но самое простое под ними есть ещ много железа которое может быть случайно использовано именно для того чтобы зашивать то что вы бенчмарка ете а внутри блочного устройства диска этого кша нету поэтому оно вот так вот ну просто типа S3 технология будущего короче а диски технология типа давно извест хорошо спасибо большое Так сейчас у нас значит вопрос там итом Давайте Вот микрофон вот а вот слушайте Да у нас девушка Не задавала ни один вопрос сюда вот микрофончик потом сразу хорошо Дмитрий тех впечатлён до глубины души цифрами и такой вопрос А какие требования к инфраструктуре к сетевой части вот для взаимодействия между адб Ну точнее гмом и ТМ Это не адб это ванильный гнм э так требования к инфраструктуре типа нужна сеть что ещё но мы оказ да тут есть как бы подводный камень м до этого на хж Джой нах мы были упёртый а а теперь мы упёртый в сеть два раза то есть производительность кластера уже была ограничена сетевой инфраструктурой и теперь она ограничена в двух местах сетевой инфраструктурой Э мы пока у нас в продакшене был один кластер этой технологии все остальные были тестовыми и о подводных камнях К сожалению я пока рассказать не могу Не потому что я это скрываю потому что мы ещё недостаточно по ним по граблям походили вот мы походим сколько-то по этим граблям научим научим шишек и честно про это расскажем пока технология вот из-под пальцев Кирилла буквально недавно там он написал Ну типа там релизная версия была выпущена 25 сентября если я не ошибаюсь до этого у нас были как бы беты всё вот ээ и не знаю где где подводные камни тут что-нибудь какие-нибудь проблемы Обязательно будут Но точно пока про это сказать сложно то что у нас много больших планов на будущее это не потому что мы уже хорошо по граблям походили Мы только начали тут если честно супер А давайте сейчас вот э девушка вопрос и Давайте Следуй вопрос сюда молодому человеку Анна ккс вопрос тоже до глубины души Какие типы воркфлоу вы тестировали что результаты получились Ну близкие по производительности Ну высокая конкарно низкая конкарно чтение запись там мы не тестировали Ну то есть мы тестировали с точки зрения того что ничего не сломается но в бенчмарках был обычный тест New York такси с миллиардом строк тут нигде не написано короче я не положил ссылку на слайды ой на бенчмарки В общем в гитхабе У нас есть Папочка Notes там есть анонс и там есть детально скрипты которые мы выполняли там можно посмотреть какие запросы выполнялись и я сейчас не стал рассказывать потому что просто не помню что значит эти селекты А ну вот там название какой-то смысл есть Ну в общем на самом деле производительность простых запросов проседает существенно то есть там десятки процентов падения произвольности произво сложных запросов оказывается ограничена не только дисковой системой но и не оптимальность Эктора греса гпма он исследованный от поса то есть условно материализация в этой В этой системе tle Table которая называется она долгая она требует все вот эти дату развернуть и эта не оптимальность позволяет системе выглядеть также эффективно на Вот вот таком способе хранения когда Станет таким же переоптимизация Давайте Пусть это будет последний вопрос чтобы остальные можно было задать в кулуарах ты жето никуда не убежишь я я не взял с собой приз за лучший это лучший вопрос ме два будет один от облака один отлично Давайте тогда вопросик Да спасибо заклад Саша Попов У меня вопрос такой можно следующий слайд открыть он немножко уже отно к тому что задали в самом поводу запросы они как бы не содержат в себе фильтров кроме Ну наверное партиции это Ну наверно не совсем честно А вот именно на каких-то рандомных фильтрах рандомных полей мы не делаем Ну не я спрашиваю больше про производительность сравнения то есть вот эти запросы они Да долгие тяжёлые но как если в этот запрос добавить какой-нибудь фильтр который чма и опубликовать его Если вдруг окажется что в этом случае ничего не работает мы подумаем Как сделать лучше Вот в целом Я призываю сообщества попробовать короче потыкать в нашу технологию Может это всё не работает но я не знаю тестировал не я бенчмарки которые Мне принесли архитекторы сказали ну работает нормально СБО болье во докладчика за отве на вопрос А тебя попрошу выбрать вопрос который тебе понравился за подарок который мы дадим прямо здесь а второй Ты просто захватишь кого-то кого-то человека потом уве его отдельно на наш НД А какой вопрос задавали я не помню Да я вот запомнил что я хотел Вот это ненова пол что-то где-то рядом так отлично Пока тыми и второй человек спросил почему же про то что у нас на самом деле кши от3 приходит вот я хотел ещё вот вам дать подарок от Яндекс облака но мне надо будет вам вместе с вами дойти до стенда Я там его вам вручу хорошо Давайте ещё раз поблагодарим А подарок подарок в студию"
}