{
  "video_id": "LeHOMN8kqBM",
  "channel": "HighLoadChannel",
  "title": "CPU-изоляция по memory bandwidth / Лев Плинер (Yandex Infrastructure)",
  "views": 395,
  "duration": 3206,
  "published": "2024-04-17T01:10:20-07:00",
  "text": "про железо чуть-чуть такого железного хардкора посмотрим какое влияние наше железо может такая и то как мы его настраиваем будет оказывать на утилизацию облаков и расскажет нам сегодня об этом Лев пленнер из Яндекса Приветствуем нашего спикера Привет Меня зовут Лев Клинер Я работаю в инфраструктуре Яндекса я сегодня расскажу об одном ограничении которое мы обнаружили и героический побороли имя этому ограничению Мэри бэндвис что на русский можно перевести как скорость шины памяти Зачем в названии написано его изоляции я расскажу чуть позже сперва немного про контекст Я работаю в инженерной платформе Яндекса это множество сервисов которые инженеры и продакты индекс используют для создания сервисов Яндекс здесь все от ищут трекера и срез сборки до системы контроля версии вычислительного облака Кстати как раз про вычислительное облако Я буду рассказывать сегодня наш облако контейнерное в отличие от того которая получает коммерческие заказчики в Яндекс клауди мы не используем гипервизерную виртуализацию но мультитентная в том смысле что несколько потребителей могут оказаться на одном и том же железном Хосте и оказывать друг на друга негативное влияние за счет того что используется в принципе одни и те же ресурсы Вот как раз под изоляцией мы понимаем наши усилия чтобы этих этого влияние было как можно меньше чтобы два контейнера запущенные на одном и том же Хосте оказывали меньше влияния Ну и не трудно понять что CPU изоляция это как раз влияние связанное с использованием общего процессора более подробно если после моего доклада у вас останутся вопросы о том как устроена приватная вычислительное облако Яндекса Вы можете прочитать публикацию на хаббре моего коллеги Сергея Фомина Либо вы можете вчера сходить на доклад нашего сетевого Дмитрия липина который рассказывал про платформу и про вычислительные облака Я работаю в инфраструктуре контейнерного читательного облака приватного вычислительного облака Яндекса в Яндексе принято сервис делить на продукт и инфраструктуру группа которая занимается продуктом имеет фокус на то чтобы как можно быстрее деливерить свечи команда которая занимается инфраструктурой фокусироваться на надежности эффективности фокус немножко разные поэтому целесообразно команды разделять как раз работаю в инфраструктуре приватного вычислительного облака в зоне ответственности инфраструктуры три момента три разработки Первое это контейнер runtime в Яндексе используется контейнер порта собственного производства его Исходный код не только выложен в интернет но и опубликован под опенсурстной лицензии Блин я думал кроме собственно контейнерного рантайма мы делаем плагины которые готовят сторож для контейнеров готовят акселераторы готовят сеть мониторят за сетью обеспечивать ответственность за сервис мониторинг за их контейнерами словом хозяйство довольно развесистое Если вы хотя бы раз в жизни заглядывали под капот контейнерного рантайма Вы знаете что в линуксе для этого используется все группы и namespace часто бывает что какие-то возможности под системы segroup Нас не устраивают и мы патчем и друлинус собственно У нас есть вторая ответственности в инфраструктуре это разработка ядра в нашей версии Linux примерно 160 приватных патчей которых нет в стриме кроме фокуса на контейнерную изоляцию мы повышаем баги и третья зона ответственности инфраструктуры это эксплуатация задача команды эксплуатации в том чтобы весь наш флот как можно больше времени проводил в состоянии готовом для запуска пользовательских воркутов в команде эксплуатации всего шесть человек а Наш флот это Примерно 100 тысяч серверных в 100 тысяч нот расположенных в нескольких центрах Яндекса в разных географиях ещё пару цифр про наше вычислительное облако А многие эрогенные Я прошу вас Запомнить этот факт потому что он дальше как ружье на стене у меня выстрелит у нас около 10 разных моделей процессора есть современное грубо говоря вчерашние модели типа например amdzen 2 и совсем скажем так уже устаревшее но мы продолжаем их эксплуатировать и благодаря этому мы предоставляем нашему внутреннему потребителю сервисом Яндекса беспрецедентно дешевое протест В современных нодах два процессора каждый по 64 физических ядра в случае трейдингом получается 128 два по 128 256 в приватном вычислительном Облаке ядра примерно поделены поровну между Real Time сервисами и не реалтаем сервисами Если вы были вчера на докладе коллег из войти заурус Вы наверное слышали про то что в Яндексе есть большой хаос грубо говоря это кластер в котором лежат очень большие данные структурированном виде и на этих кластерах Мы производим машинное обучение для того чтобы улучшать наши продукты так вот примерно половина облака это войти заурус вторая половина это Real Time сервисы это сервисы которые принимают живую пользовательскую нагрузку и должны отвечать примерно за десятки миллисекунд есть сервисы которые должны отвечать за единицы миллисекунд есть сервисы которые могут отвечать за сотни миллисекунд Но примерно вот так типичный сервис в 99 должен отвечать за десятки миллисекунд здесь под Real Time сервисами я понимаю поиск рекламу Алису такси КиноПоиск музыку все эти сервисы имеют довольно жесткие по временам ответы поэтому мы сами называем наше облако runtime Clouds мы облако для runtime сервисов половина ядер у нас под не потерял тайм об одной верхнеуровнево проблема про которую буду говорить сегодня выглядит следующим образом представим себе что какой-то сервис решил заняться горизонтальными масштабированием наверное сложно себе такое представить соответственно для того чтобы обрабатывать больше пользовательских запросов в секунду он заводит себе не один контейнер со своим Ворк лодом 35 100 300 и каждый из инстансов каждый из контейнеров благодаря балансировщику нагрузки обрабатывает какое-то свое количество запросов в секунду так вот если какой-то один из контейнеров на моем примере два три контейнера и вот второе выбивается у него утилизация квоты на CPU выше чем у остальных а если это происходит то ответственность за сервис не знает сколько еще копосить его кластера Сколько еще можно допустить на этот кластер пользовательских запросов прежде чем начнется деградация потому что деградировать начнет второй контейнер на него польется больше пользовательской нагрузки чем он состоянии выполнить с учетом своей семьи квоты и он начнет просто проливать например он перестанет укладываться у него вырастет очередь он начнет Просто 500 возможно любая абсолютно деградация и ситуация когда контейнеры ведут себя сильно по-разному вредно мой доклад будет устроен следующим образом Для начала я сравню наши приватное облако скажем так один кластер rtc с типичным кластером губернатиса которые возможно развернут у вас а дальше я еще раз очень подробно поговорю про проблемы с которой мы сталкиваемся на самом деле Вот то явление которое я показывал неодинаковая утилизация может возникать по разным причинам я сегодня буду говорить только про одну проблемы с Memory bandes так вот дальше я расскажу В чем конкретно проблема с Мэри бэндвис и конечно же перейду к выводам Куда без выводов Итак начнем со сравнения кластеры rtc с кластером кубернетиса который вы все могли видеть у себя на работе или где-то еще первое отличие то что у нас не куберетесь у нас все детали собственного производства Но на самом деле мы обводили губернатиз когда и делали центральным элементом кластер является сервер в котором хранится база данных того какие контейнеры заноцированы и на каких нодах они запланированы пользователь приходит в оркестратор пользователям мы видим инженеры Яндекса который разворачивает свой сервис пользователь приходит в регистратор и заказывает локацию какого-то количества контейнеров определенной геометрии скажем Мне нужно 100 контейнеров в каждые по 10 ядер по 10 гигабайт оперативной памяти по 10 мегабит в секунду сети указывает Каким должен быть слой Как должны выглядеть файловая система контейнера в момент запуска Ворк лодов рассказывает как этот как его контейнер можно мониторить что нужно запустить для того чтобы проверить что контейнер жив что нужно выполнить для того чтобы его выключить и так далее регистратор превращает запрос пользователя в виде заказа на контейнеры через Запись сервер они попадают в базу данных дальше начинаются отличия от того кластеры которые вы могли видеть дальше во-первых наш дуллер не работает в режиме работает в режиме то есть заказанные пользователем контейнеры он размещает туда где В принципе можно разместить бывает что ресурсы настолько фрагментированы что в моменте некуда разместить контейнер который заказал пользователь и ничего не делает он поднимает руки но когда он это делает просыпается дефрагментатор к слову дефрагментатор также просыпается и по крону но не суть важно Задача дефрагментатора в том чтобы переносить пользовательские контейнеры по кластеру при этом не сломать сервисы переносить пользовательские контейнеры по кластеру для того чтобы маленькие кусочки ресурсов которые размазаны по всему кластеру с оптимизировать и превратить в более-менее свободные над для того чтобы в дальнейшем шедоллер смог туда что-то зашедлить примерно по такому же принципу как дефрагментатор работает хиллер Это подсистема которая устраняет проблемы которые создалер Ну вопрос том как создать проблемы на самом деле часто бывает что проблема возникла уже после того как контейнер зашедлили скажем была она Да на нее зашедлили там три контейнера пользовательских потом перезагрузилась мне стало меньше памяти И в этот момент возникает оверка МИД по оперативной памяти чиллер это видит и пытается кого-то оттуда выселить процесс называется хилинг волю оркестратора и дуллера на ногах выполняет постоянно приходит сервер на самом деле это аналог билета поднимите руку кто догадался что это аналог билета прекрасно как минимум треть зала подняла руку постоянно ходят сервер или спрашивает целевое состояние своей ноды если целевое состояние отличается от того что видит контейнерный runtime но до Агент развоплощает контейнеры создает контейнеры меняет свойства контейнеров и так далее для манипуляции с контейнерами над Агент использует API контейнерный runtime который как я говорил в нашем случае самописный второе важное отличие кластера rtc от того что вы могли видеть в мире губернате состоит в том что у нас поддерживается вложенные контейнеры помните рассказывал что половина кластера это войти заурус в котором собрано большая система так вот rtc сам по себе мультитонатный в нем есть разные потребители поверх этой системы коллеги из войти создали свою имущественную систему которая запускает пользовательские джипы во вложенных контейнерах а поверх войти коллеги зубайкюль создали еще одно мультиленную систему То есть у нас есть кейс когда благодаря вложенным контейнерам у нас появилась мучтенная система поверх мультитанатной системы поверхностентной системы и каждый раз был какой-то супервизор который сам находился в контейнерах и создавал под контейнеры для того чтобы изолировать друг от друга запускаемый джипы третье важное отличие на самом деле на него ориентировались когда принимали решение о том что нам нужно делать что-то свое это то что наши технологии позволяют делать большие кластеры самый большой кластер у нас состоит из 40 тысяч нот мы считаем что предел на который в состоянии губернатиз это 5000 теперь еще одно важное отличие как я говорил наш кластер гетерогенный то есть в нем есть примерно 10 разных моделей CPU на самом деле примерно такая же ситуация совсем оборудованием но не суть и для того чтобы пользователь не думал о том какие мы придумали идеальные виртуальные ядра ответственность за сервис заказывает себе вычислительные ресурсы процессорное время не указывая конкретную модель процессора есть кейсы когда указывать приходится но про них говорить не буду соответственно если я заказал под свои контейнер по 10 виртуальных ядер в случае если мой контейнер заселяется на какой-то совсем древний сервер он получает там 12 физических ядер Ну цифру условная просто для того чтобы понимали Как это работает Если это там предпоследняя модель значит он получит 10 физических ядер если совсем какой-то свежий сервер то он получит 8 физических ядер И вот теперь давайте еще раз взглянем на проблему про которые говорил в самом начале с пониманием того как устроена модель процессора в rtc и того что артистирогенные если я залоцировал два контейнера в каждом из которых требовалось 10 виртуальных ядер то в первом случае мог получить 12 физических ядер которые под моим ворклодом утилизируются на 58 процентов для проценты семьи один используется во втором случае это будет 10 ядер из которых утилизировано 8 то есть 80 процентов в третьем случае это 8 из которых утилизировано 5 то есть 63 процента из-за того что утилизация разная как я говорил ответственность за сервис не знает какой в точности запас по РПС есть у его кластера и он соответственно не может утилизировать выделенное залоцированные процессорные ядра полностью То есть он это попозже поговорю он не может создать везде одинаковую утилизацию и соответственно везде там допустить в каком-то экстраординарном случае стопроцентной утилизации копнем эту проблему еще глубже Почему утилизация может быть разной в идеальном мире Если бы наши виртуальные ядра работали как мы хотели мы получили такую ситуацию что на Древнем процессоре где мы выделяем 12 ядер Вместо 10 виртуальных пользовательский код выполнял собой со скоростью условно два попугаев в секунду я здесь сделал такие условно девицы на самом деле это миллиарды операций в секунду если мы возьмем который примерно 1-2 будет на частоту скажем это два Герца мы получим примерно 2 миллиарда операций в секунду на одном ядре если это более современное ядро оно должно прожевывать пользовательские инструкции со скоростью 2.4 попугая в секунду То есть примерно 2,4 миллиарда операций в секунду если это совсем свежее ядро то со скоростью 3 Что будет если средняя модель прожевывает пользовательский инструкции чуть медленнее неважно По какой причине в этом случае как раз и произойдет то о чем я говорил мы получим разные утилизацию и из этого не до утилизацию ресурсов Вот теперь как раз хотел продемонстрировать эту это явление в цифрах это живой пример кластер из контейнеров из рекламы в одном из сервисов рекламы есть 300 контейнеров каждому из контейнеров алоэфировано по 40 виртуальных ядер представим себе что всего 10 из них работают всего на 10 процентов хуже чем все остальные в этом случае простаивать будет всегда примерно тысячи виртуальных ядер то есть 10 процентов квота этого сервиса пользу сервис занацировал 12 тысяч виртуальных ядер а использовать Может на 9 процентов меньше Это довольно много и все ситуации Естественно что причина может быть много потому что модель с виртуальным ядрами протекает иногда Проблема в том как пользователи собирают свои сервисы иногда Проблема в том какие они используются инструкции скажем на каком-то процессоре какая-то инструкция может быть реализована в микрокоде работать медленнее Но если проблема носит системный характер ее приносит нам в инфраструктуру И просит решить на всем кластере и вот как раз о такой проблеме я буду говорить сегодня чтобы начать не очень сложного Я нарисую такой простенькую картинку я рисовал ее своему сыну когда он учился в пятом классе Он спросил меня папа Что такое процессор вот я нарисовал такую картинку здесь Красная собственно процессор вычислительные ядра зеленые это кэш-процессора это та часть памяти который процессору проще всего Достучаться а синие это модуль оперативной памяти я объяснил своему сыну что запрос в кэш работает примерно в 10-20 раз быстрее чем запрос в модуле оперативной памяти сейчас мой сын заканчивает 8 класс он пришел ко мне и спросил папа Что такое 200 серверы мне пришлось ему нарисовать более сложную картинку на которой есть два процессора у каждого из них свой кэш и стрелочки стали некоторые стрелочки стали пунктирными на этом на этой схеме у меня изображено два процессора и две нумы над и сейчас объясню что такое надо левый процессор Давайте будем называть его нулевым обращается к двум левым модулям памяти для загрузки данных в L3 кэш быстрее чем к двум правам потому что два левых модуля находится с ним в одной ноге а правый процессор первый обращается к двум правым модулям памяти быстрее чем двум левым потому что они находятся с ним в одной ноге это явление называется ну и соответственно левая правая части моего рисунка это две ноды для того чтобы посмотреть на это на практике Вы можете через FS на линуксе посмотреть содержимое таких файликов честно говоря как это сделать Windows не знаю но думаю что тоже какая-то есть возможность Вот пример в котором я исследую сервер с двумя но монодами в нулевой но мы Наде 128 ядер на самом деле их 64 Просто на этом сервере включен гипертрейдинг поэтому ядра с нулевого по 63 128 по 191 относится к нулевой воде в третьей строчке так я хотел посвятить указкой но в третьей строчке Я спрашиваю у ядра как она себя представляет запросы из нулевой воды в модуле памяти подключены к нулевой ядро считает что запросить данные из модуля памяти процессу запущено на нулевой ноге а потребуется 10 попугаев не знаю В каких единицах а для того чтобы обратиться к модуле памяти из первой 32 32 попугая на самом деле это не очень точная цифра она не меняется например под нагрузкой поэтому можно считать что это такие модельные цифры как себе представляет leton производитель Если вы хотите получить настоящие цифры нужно использовать что-то под нагрузкой Можно например использовать Intel Memory leton Checker есть такой инструмент в мире AMD есть тоже его аналог немножко еще посмотрим на то как ядро видит разные в файле нумасстат ядро рассказывает нам статистику Каким образом страница памяти свойственно хит ядро подсчитало для нас Сколько страниц памяти было выделено на той же самой новой ноге на который был запущен процесс то есть процесс работал на нулевой ноге он пытался лоцировать память нулевой ноге в результате получает доступ к ней за 10 попугаев Мисс это соответственно противоположная ситуация что такое форум рассказывать не буду подробнее можно почитать в Мане полномостат и meminfo ядро прекрасно знает что на нулевой Наде есть столько-то памяти столько-то свободно и Сколько памяти есть на 1 так вот видя эти цифры Мы подумали что проблему которую мы наблюдали А проблема очевидно была в районе латентности доступа к памяти можно решить если у страдающего сервиса у того сервиса у которого утилизация вырастала за счет того что был ниже можно этим сервисом можно выделять память на то же самое для этого Попытка номер один Мы решили покрутить виды ручку которая называется эта ручка против по сути это еще одна пи для ядра и еще несколько ручек логически связанных с ними работают следующим образом представим себе что у меня есть две новые и запросы к памяти этих распределены следующим образом вот красненькая это запросы в модуле памяти нулевой над и от процессов запущенных на нулевой на моде А беленькая это запросы к модулям памяти нулевой моноды от процессов запущенных на первый то есть кросс-нома запросы соответственно у них выше и с первым с первой такая же ситуация задумка Memory balancing состоит в том что рано или поздно можно все привести в такую ситуацию можно мигрировать страницы таким образом чтобы все запросы в модуле памяти были локальными для этого ядро ведет специальную статистику оно смотрится тем как часто процесс запущенные на одной обращаются к фреймам запущенным На соседней периодически устраивает И во время этого перетаскивать данные Мы думали что это решит проблему причем довольно дешево но оказалось что все снова приходится такое состояние А причина в том что в индексе кроме называется это процесс в ходе которого пытается сделать утилизацию ядер примерно одинаковый для этого он перетаскивает 3D из одного тоски в другой если он видит что в каком-то tasq мало Idol то есть мало времени в тоски никто не выполняется оно берет каску из этого Из этой очереди и перетаскивает соседнюю соответственно шедоллер постоянно перетаскивал 3D между нума нодами и появлялся снова не локальный трафик проблема никуда не ушла Окей подумали мы нужно запретить как я уже говорил под капотом у контейнеров всей группы мы начали объединять все процессы в еще одну группу навешивать на него сад-контроллер и для сервисов Которые страдают из-за высокой высокого времени доступа к памяти мы начали навешивать в результате все 3D страдающих сервисов всегда были выполнены всегда выполнялись на одной поскольку Он должен был с мигрировать все страницы этого контейнера на ту же и должно было возникнуть светское счастье здесь нас снова ждала фиаско мы называем это фиаско Memory bentles деградаш вот сейчас нужно быть очень внимательным начинается сложная часть моего доклада прошу всех протянуть привязные ремни не курить Итак что мы понаблюдали представим себе в левой части экрана а ну мы наду у которой Memory bandy составляет 100 Гб в секунду Это означает что по запросу по запросу процессора со скоростью 100 мегабайт в секунду данные могут предаваться из модулей памяти в L3 кэш если к тем же самым модулем памяти возникают запросы из соседней драматически сокращается мы видели сокращение примерно на 40-50 процентов почти в два раза еще раз если возникают хотя бы какие-то кроссумы запросы то страдает в которую читают они из которой читают это было довольно неожиданно в этот момент мы решили открыть спецификацию процессора на котором мы пронаблюдали такое явление эта схема процессора AMD семейства синенькие квадратики это чиплеты это такие ядра блоки из восьми вычислительных ядер и L3 кша 8 таких чеплетов объединяются в один процессор в котором 64 ядра для того чтобы объединить их на чипе есть Матрица ввода/вывода Матрица Memory Матрица ввода вывода которая и обеспечивает доступ каждого из вычислительных ядер к модулям памяти другими словами у чипов интерфейс памяти был не ddr А вот это восьмерочка на самом деле это знак Бесконечность здесь какая-то amd-шная внутри процессорная высокоскоростная шина Она кажется называется со словом Инфинити Все любят очень все производители очень любят слово Инфинити потому что это ощущение что полоса безграничны на самом деле они тоже есть определенные ограничения так вот когда кому-то выполняющемуся на одном из ядер нужно получить память запрос идет через матрицу памяти через синенькую синюю линию с бесконечностью в один из желтых банков Вот они видите слева желтые банки а если запрос идет в PCI Express соответственно он уходит вниз нас это сегодня не особо интересует линии которые вводят вверх могут использоваться как для PCI Express connectivity с периферийными устройствами так и для создания двух сокетного сервера Вот пример 200 в каждом чипе по 8 чиплетов по 8 ядер и у каждого процессора есть свои модули памяти так вот если ядро если ядро из левой ноды хочет прочитать данные модули памяти подключены запрос проходит через два через две матрицы ввода вывода мы считаем что в zn2 Запрос который приходит Запрос который приходит в матрицу ввода вывода от соседней матрицы ввода вывода требует Примерно в три раза больше ресурсов чем запрос от собственного ядра подключенного напрямую Почему так мы не знаем у нас тесты которые это показали важен экшнайтом Мы решили поделить все серверы в которых есть такой эффект полностью на 2 нады линии которые невозможно пересечь То есть если пользовательский контейнер аллоцируется на такой ноде неважно страдает он от Memory datance или не страдает мы не знаем мы тут же выставляем ему cpu-set либо на первые 63 ядра либо на вторые 63 ядра далее за счет данные этого контейнера мигрируют в лонному наду и удаленный трафик дальний трафик ремонт трафик через шины памяти пропадает На самом деле не пропадает потому что любое завершение ввода вывода будь то жесткий диск SSD или сеть приносит данные со внешнего устройства на которой они должны быть однопроизвольно грубо говоря где у вас прерывание сработало Где у вас ядро поймала прерывание там он его обработает там на памяти выделит но благодаря сингу и благодаря тому что основные потребители шины памяти у нас были прибиты к ядрам мы существенно уменьшили Импакт этого явления Окей что же дальше здесь Мы подумали немножко на будущем и предстали себе ситуацию Когда у нас появляется два сервиса которым каждому требуется по 60 гигабайт Memory Ben 10 секунду как я говорил вот на этих мощных серверах семейства Zen 2 в одной есть ресурс на 100 Гб в секунду Что будет если на одну минуту упадут два контейнера каждому из которых требуется по 60 Это довольно серьезный вызов Потому что если такая же ситуация на самом деле может возникнуть и на более старых моделях процессоров если скажем в Z2 уже реализован процессор в сервис который позволяет лимитировать аппаратно потребление процессами то на более старых моделях такой возможности нет здесь мы придумали Да это как раз случай когда два контейнера по 60 упали которые ресурсы на 100 здесь Мы приняли следующее решение мы заведем как ресурс Но это будет скажем так слабый ресурс я объясню что такое слабый ресурс на примере с cpu-time если пользователь ответственность за сервис заказывает у нас вычислительном ядре защитном облаке Прошу прощения ядра то он явно указывает сколько ядер ему нужно и мы ставим лимит в этот момент потребление вычитается из квоты пользователей квота выше который не может прыгнуть и оверкамид недопустим то есть шеделлер Никогда не позволит заселиться контейнером на один хост если им суммарно требуется больше ядер чем есть на этом Хосте Memory bands Мы решили поступить Иначе если мы скажем пользователям друзья на нашем классе появился новый ресурс у нас были процессорные ядра у нас была оперативная память у нас была полоса сети теперь у нас появится новый ресурс который называется скорее всего нас не поймут потому что никто не знает сколько если мы всех заставим померить эту величину и постоянно поддерживать в актуальном состоянии Ну скажем так решение будет последним для меня для руководителя базовой инфраструктуры поэтому мы решили в качестве Point of trues считать не заказ пользователя а фактическое потребление Мы решили замерять Каким образом контейнеры утилизируют Memory bandes соответственно у нас не будет квоты и не будет лимитов что нам даст это измерение с помощью хилера в шеделлере про который я рассказывал на одном из первых слайдов мы будем видеть ситуацию что до какой-то но мы найди происходит превышение по memys брать какой-то из прожорливых контейнеров и отселять его на соседневноду либо вообще на другой хост Я бы хотел чтобы после моего доклада вы ушли со следующими выводами во-первых процессор нужно налаживать Казалось бы налаживать нужно какую-то периферию какие-то сложные устройства на самом деле современный процесс Это довольно сложное устройство его тоже нужно налаживать Если для вас это сложно как минимум в случае проблемы с производительностью обращайте внимание на локалити особенно если речь идет про большие серверы с большим количеством ядер или любые современные модели Большое спасибо за внимание пожалуйста вопросы Спасибо поднимайте руки получаете микрофон и задавайте вопросы да спасибо за доклад а может быть у вас уже есть статистика Как часто Как часто клиенты выходят из квоты вот насколько это а В каких единицах Ну например Насколько процентов чаще вы стали реалицировать под клиентов после того как ввели Вот это правило что при увеличении Я не говорю что мы уже начали мы пока только прибор сделали для того чтобы видеть проблему проблемы на самом деле довольно редкая Мы скорее сделали впрок то есть когда проблема начнет воспроизводиться мы включим хилер а правда что зависит скорее не от клиента от модели процессора влияния Memory bandvice на Вот это точность целая но замри влияет от того зависит от того от другого если мы берем допустим Intel это в intella фундаментально в двух сокетных серверах одна наманада То есть разработчики из Intel подумали о том что на их процессорах будут выполняться прожорливые в плане мемори бэндвис Ворк лоды и ну скажем так развальцевали чтобы она не была бутылочным горлышком то есть чаще на самом деле проблема возникает на AMD Но я могу себе с легкостью представить что у нас когда-то появится очень прожорливые сильно оптимизированные сервисы которые на интелах будут такой Проблемы создавать Простите я неправильно задал Вот вы говорили в начале что войти это типа один из ваших больших потребителей Правда же что в большинстве задач большинстве контейнеров которые занимают выйти заурус одинаковый паттер доступа к данным он в принципе делает одно и то же и ну одинаково на него Влияет Мэри бэнс и вы когда начнёте их расселять то окажется что у вас Ну некуда селить Потому что везде на всех хостах есть уже войти и Да я понял ну смотрите Начнем с того что уйти Нет проблемы с мемри бэндвис Потому что это не Real Time система это балл система то есть задача войти состоит в том чтобы утилизировать все ресурсы Казахстана сто процентов при этом никаких гарантий по поводу того сколько Как быстро выполнится джеба уйти нет и они буквально типа заваливают жопами так доверху всё что им выдали все ресурсы которые у них есть они забивают доверху Поэтому если мы Селим войти с real-time сервисами на один хост то мы возводим вокруг it несколько эшелонов защиты для того чтобы они создавали влияние на соседей то есть войти не будет по этому принципу расселяться это на самом деле решение для realtime сервисов и спасибо ваш вопрос хотелось спросить Вы же наверняка закупаете процессоры какими-то огромными пачками нет ли у вас тут взаимодействия с производителем хотя бы форме обратной связи что есть вот такие вот проблемы и говорят они какие-нибудь идеи решений взаимодействие с производителем в последнее время сильно затруднено Я не вижу тогда я задам Лев спасибо большое за доклад Ну вот я разработчик простой я читаю без практики пишу хороший код потому что таскать умные люди подумали и презентовали на конференции Что мне сделать в своем приложении Чтобы поменьше потреблять ресурсы На каком уровне это стоит делать уровень живого кода там runtime на samлере надо писать Для начала нужно вообще понять если проблема может быть Battle necom в случае вашего сервиса является одним из есть прекрасная статья про то что бутылочное горлышко в процессоре бывает четырех видов не помню только кто авторы по моему кто-то из инженеров Intel это декодирование инструкций то есть прогнозирование того какие инструкции будут выполняться дальше математическое устройство и Кэш соответственно нужно понять в случае с вашим сервисом Что является батлэком А если бы неком является кэш значит проблема где-то в районе доставки данных кэш или может быть того сколько грубо говоря сколько в моменте нужно памяти для того чтобы кот мог продуктивно исполняться точная рекомендации про Java код я конечно сказать не могу Но поскольку мы вместе работаем в Яндексе Я постараюсь что-нибудь найти Спасибо У каких сервисов У каких типов сервисов проблемы с моим весом чаще всего это проблемы сервисов которые активно гоняют по памяти грубо говоря если бы это была высоко нагруженная база данных там с ба деревьями предполагая что она активно использовала потому что она не состоянии была уместить все данные пользователи просто не знают где вода найти памяти лежат Да они отправляют произвольно соответственно их произвольно читаете они произвольно считываются или вымывается L3 кэша Я предполагаю что любой сервис который активно гоняет по памяти будет скорее всего иметь проблемы спасибо спасибо ваш вопрос Спасибо за интересный доклад картинки вообще зачет Спасибо Буду демонстрировать А вот такой вопрос это вот все сказанное оно относится преимущественно к AMD или я больше того скажу Это относится к amden 2 потому что amdzen 1 устроены по-другому принципу у них каждый процессор уже является двумя номанами что для нас было огромным сюрпризом То есть их нужно сразу же делить на две один процессор а у интелов как я говорил в Intel стараются чтобы Memory banvast не было бутылочным горлышком поэтому зачастую 200 сервер Ну и высокий до 200 сервер будут одной молодой То есть их можно запустить в режиме одной когда леденцы будет одинаковым даже если не зависимо от того с какого ядра вы обращаетесь к какому банку памяти поэтому эта проблема Я считаю более актуальна для AMD Ну и на самом деле она для Zen 3 тоже актуально сервис zen3 Мы тоже делим по границам и спасибо ваш вопрос Добрый день спасибо за доклад отличная реклама машины MD такой вопрос вскольте проходили на схемах мимолеров и тому подобное А можете рассказать чуть подробнее прошу дуллер например по каким измерением И как вы боретесь с фрагментацией ресурсов на машинах мыши долим по всем основным ресурсам это ядра оперативная память сеть дисковое пространство и диск bandvice у диска на самом деле тоже есть полоса скажем если это жесткий дисков там в прыжке это будет 150 мб/с если SSD там будет 400 если это nvme то 1 Гб это все ресурсы которые про которые знают пользователь про другие он не знает что дуллер в момент когда он аллоцирует контейнер на хостел он видит ресурс каждого Хоста по всем пяти измерениям и пытается найти свободное место если его нет как я говорил возникает дефрагментатор о том как правильно дефрагментировать Гугла была чудесная статья если коротко то для того чтобы хорошо дефрагментировать не нужно пытаться запускать дефрагментацию на всем флоте нужно флот делить на маленькие кусочки и фрагментировать их отдельно тогда вы сможете на каждом кусочке флота решать полную задачу полным перебором оптимизировать и находить лучший способ дефрагментации плюс несколько эвристик например Каким образом относится одно ядро к одному гигабайту памяти одно ядро там ну и надеюсь объяснил Спасибо Спасибо ваш вопрос а есть множество алгоритмов в которых специально делают упор на то чтобы умещаться в кэш-процессора Так что в принципе со стороны пользователя есть возможность пытаться уменьшать свою потребность в Memory bandes хотелось спросить а как вообще Планируется ли как-то доносить до пользователя информацию о том что у них необычная высокое потребление пропускной способности памяти Или как это будет как это должно работать ваше идеальное картине мира То есть у пользователей будут какие-то метрики чтобы за них смотреть вы будете сами это мониторить то есть с учетом того что такое скрытый параметр то будет ли какое-то взаимодействие здесь с потребителями это будет я страдал от этой проблемы и вот и вот решение На каком объеме вот эти проблемы с ума Вы заметили В какой момент начали их решать мы их заметили когда мы ввели в эксплуатацию 1000 серверов к нам массово пошли сервисы у которых вот так сказать утилизация на intellag понижена MD повыше не знаю ответ ответил здесь в целом когда появились разные процессоры в системе и вот примерно там от тысячи узлов то есть вопрос Какой я наверное попытаюсь Еще раз понять ваш вопрос иначе насколько сложным должен быть флот для того чтобы проблемы начала воспроизводиться можно так сказать насколько сложным должен быть флот чтобы на эту проблему имела смысл обращать внимание понятно как только у вас появляется второй производитель например AMD рядом она тут же появится То есть если сохраняется консистентность у нас всех остыть там их 10 тысяч они все одинаковые то она не будет не должна проявляться так ярко скорее всего Вы долго будете не заметите эту проблему то есть не потому что не замечаете потому что она не возникает но опять же может быть кто-то особо люто оптимизировать свой код и выкатит в продакшн сильно оптимальный код который будет не знаю как всегда есть он грубо говоря он будет оптимизировать по утилизации процессорного времени но все остальные ресурсы он будет есть Сколько им дают вы об этом узнаете когда он уже выкатится понял Спасибо за ответ Спасибо вот вопрос у нас в середине Спасибо за доклад Вы когда показывали слайд когда объясняли проблему там Насколько я понял что главной части этой проблемы является то что у нас ожидаемые попугаи расходятся с рассчитанными Ну то есть реальными попугаями как обрабатываются и насколько меня представляется эта проблема же не только может возникнуть из-за проблем с нума Да у нас там может быть ядро дефектно Или там чеплет или еще какая-то проблема вот мне хотелось бы понять это является проблемой Если да то как вы эту историю мониторите или это вообще единорог это является проблемой если эта проблема затрагивает только один из десяти тысяч сервисов то мы помогаем ответственным за сервис её раздеба жить и найти какой-то способ решения Ну грубо говоря мы говорим Да мы виноваты типа наша виртуальный ядра плохо работают Вот вам немножко квоты А дальше делайте более равномерную нагрузку балансировщиков а если эта проблема затрагивает все сервисы весь кластер то решение уходит в нас Мы должны его системно решать на весь кластер но причина на самом деле Да всяких разных может быть очень много Например Слава Богу что никто не использует у нас инструкция vx3 потому что в X3 снижает тактовую частоту На физическом ядре Ну это наверное для следующего доклада уже можно Тогда еще немного Давайте дадим возможность задать другим людям вопрос потому что время у нас уже выходит ваш вопрос будет последним большой за интересный доклад У меня вопрос касается наверное больше архитектуры процессора Но может тоже подскажете а почему модули памяти не селят равноудаленно от двух Ну монот то есть на диаграмме они были где-то слева ноды и справа Ну и понятно что по шине они дальше чем локальной локальные моды Понятно есть две причины первая экономическая мой рисунок на котором я изобразил схему процессора было восемь чеплетов и одна одна Матрица ввода вывода так вот на самом деле в точности такие же теплеты только в единственном экземпляре используются в десктопных моделях AMD грубо говоря в МД придумали чеплет а потом придумали Как использовать этот чиплет для сервера для серверных процессоров и для десктопных соответственно дальше нужно было какое-то одинаковое решение для доступа памяти на каждый чиплет они не могли зашивать работу с модулями памяти по ddr потому что это сложно поэтому им нужен был какой-то контроллер от типа они для серверов изобрели контроллер видео этой матрицы Беги стопы там типа отдельный контроллер который материнской плате стоит и из этого уже все остальные проблемы возникают первые на самом деле причина экономическая для того чтобы сделать сейчас придумали в порядок когда несколько ядер одновременно работают с одними теми же данными им нужно реализовывать специальный протокол который называется Месси Для обеспечения конкурентности То есть если у тебя есть разные каши то процессоры либо должны знать что ну там либо блокировки ставить и так далее очень дорогостоящий и его экономические целесообразнее решать как его решают то есть сами лифтом в любом случае это грубо говоря экономия транзисторов которая напыляют на процессоре и маркетинг Спасибо нам надо с тобой выбрать лучший вопрос который мы хотим отметить так я не могу выбрать никого из коллег из Яндекса Извините а вот оттуда был доклад про то когда проблемы на каком масштабе начнутся отнимите пожалуйста руку чтобы вас нашли спасибо я чувствую что у этого человека рано или поздно начнутся проблемы не нужно поддержка книга на самом деле она как раз про то что делать если у тебя проблемы как сфокусироваться как выдохнуть тонко Ну что спасибо вам за ваше внимание ваши вопросы Спасибо тебе"
}