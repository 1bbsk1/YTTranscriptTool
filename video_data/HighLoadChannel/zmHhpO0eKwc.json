{
  "video_id": "zmHhpO0eKwc",
  "channel": "HighLoadChannel",
  "title": "Tips & Tricks for Fast Neural Net Inference in Production / Дмитрий Коробченко (NVIDIA)",
  "views": 827,
  "duration": 3070,
  "published": "2019-05-14T15:03:44-07:00",
  "text": "привет всем меня зовут дмитрий коробченко я вот в компании nvidia занимаюсь нейронными сетями машина бы ним задачами связанными с графикой но сегодня мы будем говорить больше про такую вещь как inference то есть что такое вообще inference для тех кто вообще случайно сюда зашел она смотрите есть у вас какая-то задача в да это сайнс вы обучаете какую-то модель тренируете подбираете гипер параметры общем занимаетесь в общем всяким таким речь о чем исследованиями так далее потом вы хотите вашу модель отправить в продакшен где она уже будет в боевом таком режиме работать то есть внедрить ее за тепло и и вот там уже ваша модель будет работать в режиме inference а то есть прямого распространения теперь почему мы хотим сделать быстрый inference во-первых существуют такие вещи как real-time какие-то приложения где у вас очень важно чтобы быстро ваш алгоритм работал ваши нейронная сеть или какая-то модель во вторых бывают устройство какие-то или еще какие-то ситуации с ограниченным количеством ресурсов вычислительно это мог быть мобильные устройства какие-то встроенный девайс embedded и так далее но иногда у вас просто какие-то ограничения на пропускную способность и или задержку что ли ты нас еще называется имеют в общем очень много из кейсов когда вам нужен быстрый inference ну понятно что все мы хотим чтобы работал быстро хорошо они плохо и медленно хорошо вот как сделать чтобы ваши нейронной сети работали быстро во-первых давайте разберемся что кроме скорость еще есть другие показатели ключевые которые определяют общее как у нас все работает хорошо ли плохо кроме скорости работы которая упирается в задержку как быстро мы начали обработку данных и пропускную способность то есть как много данных мы можем обработать в единицу времени еще немаловажный критерий это качество и размер моделям но размер модели понятно почему важен понятно что меньше лучше меньше памяти занимает меньше вычислений ещё это может быть важно потому что если ваши данные например помещаются в какую-то быструю память например веса вашей модели то тоже вас меньше обращение в медленную память тоже потенциально это все быстрее будет работать но немаловажен и здесь тоже такой критерий это качество мы пытаемся ускорять нашей модели наши нейронные сети но не в ущерб качеству или и карусели какая-то там у вас еще другая метрика качество хорошо вот этой наши ключевые показатели за которыми мы будем следить теперь неочевидный факт быстрый inference это задача обучения то есть как только вы начали конструировать вожу нейронную сеть вашу модель начали и обучать очень часто в этом в этом моменте да это сантис ты забывают вообще напрочь про скорость работы вот мы сейчас обучим поиграем все класс получилось хорошее качество запускаем в продакшен ой подождите что-то у нас медленно все работает надо как-то урезать надо что то делать ребята не делайте так скорость обучения проверяйте еще на этапе обучение и вообще задумывайтесь о том какой у вас бюджет по времени укладывается ли ваша модель какие нужно оптимизации сделать еще на ранних этапов во время обучения сразу после обучения и так далее его всем при этом мы тоже параллельно смотрим на качество чтобы находить некоторый баланс между качеством модели и скоростью работы то есть задумываемся о скорость еще на этапе обучения еще такой короткий совет он покажется очевидным но на самом деле не многие до этого доходят проверяйте скорость обучения скорость inference это обучения на случайных параметров на случайных весах то есть вы сконструировали какую-то модель и скорее всего им потом уже не будете менять если вы допустим не будет использоваться трюками о которых я расскажу дальше уже в этот момент можете понять вообще она вам подходит или не подходит по времени работаем не нужно для этого оставить его на обучение на сутки то моя тайна неделю только потом у киева получили классное качество потом проверили не отчет она медленно работает сразу проверяйте скорость работы там кажется очевидным но на самом деле мало кто так делает теперь давайте чтобы комплексно подойти к набору трюков которые мы будем использовать для ускорение ровных сначала общем вспомним какие у нас есть нейронной сети чтобы понять какой набор трюков нам для этого нужен вот так вот условно можно разбить все типа нейронных сетей или там 99 процентов нейронных сетей которые используются в реальных приложениях на какие-то простые полна связанные сети или многослойный персептрон и рекуррентной нейронной сети свёрточная нейронная сеть и все остальное какая-то экзотика то есть даже если у вас какая-то комбинация из нейронных сетей или какие-то вас ганы или еще что-то как правило все равно основные важные компоненты ваших неровностей будут состоять из вот этих вот кусков допустим вот тут у меня кусочек свёрточная тут у меня кусочек конкурента и так далее в общем это можно сказать и и типы нейронных сетей и основные компоненты большинства нейронных сетей давайте коротко по ним пробежимся но многослойный персептрон все просто у нас есть вот такая простая неровно сеточка в которой есть только полна связанный с лаять и каждый нейрон с каждым связан с каждым другим в двух соседних слоях и одна операция внутри одно условие по сути задается матричное умножение мы там прибавлением вектора то есть это полна связаны такие вот слои дальше у нас есть еще очень известный всем вам надеюсь свёрточная нейронная сеть и там кроме полна связан слоев еще есть другие типы операции например очень немаловажный слой и операция это свёртка свёрточная слой кроме того еще pulling слои и другие слои ног от но на самом деле мы сегодня будем больше парить про свертки и полна связанный свои но просто вы понимали что одни и те же операции используются в разных типах нейронных сетей дальше у нас есть рекуррентной нейронной сети которые хороши для текстов для последовательности так далее и что мы видим там на самом деле тоже используется полна связаны с лаем да там есть какие то более хитрые связи между слоями какие-то connection опрокинут опрокинутые в разные части сети но основной батл на iq в плане вычислений это умножение матриц это полна связанной слои все остальное это как бы меньшего порядка вычислений опять имеем полна связанные слои и теперь какой отсюда вывод надеюсь ни для кого не было секретом за предыдущие три слайда что у нас есть основное это свёрточная слои полна связные слои и операции которые им соответствуют свёртка и умножение матриц и все остальное на самом деле тоже важно иногда мы тоже какие-то трюки для других вещей тоже с вами сегодня рассмотрим но основное на что мы будем еще внимание это свёртка и умножение матриц так как это основные операции которые замедляют ваши нейронные сети так давайте сначала мы строим нейронную сеть еще даже не обучали и и вот просто на этапе построения как уже сейчас сделать нашу сеть оптимально чтобы у нас потом все классно круто хорошо быстро работала во первых общей такой совет идите от простой модели к сложный они наоборот очень часто люди делают наоборот они делают очень сложную модель со всеми вещами какие только известные науке вместо этого удар подходы сделали получили качество какое-то хорошее может быть даже не самое лучшее а потом к ним приходит менеджеры говорит о вы знаете вот классная вас модель качества высокая но вот мы хотим это все на устройство поставить на на мобильное устройство но извините как тот не влезает давайте вот чтобы такое же качество но чтобы модель поменьше и вы начинаете и из там с горем пополам как-то урезать уменьшать опять начинать все сначала в общем это поход подход не только сложно но еще и депрессивный как тихо стало да лучше и те простой модели к сложным то есть сначала сделать какой-то минимальную модель и во первых посмотрите что она в ваш бюджет по времени влезает во вторых наращивайте сложности и в зависимости от того каким он скрыт потребности по у качество не используйте прибамбасы ради прибамбасов и второе не заставляйте сеть учить то что вы так уже за нее знаете то есть люди иногда думают но вот машинное обучение это черный ящик моему с корнем данные она все выучат если есть какие-то вещи например какие-то нормализации входных данных какие-то другие слои которые вы знаете как явно запрограммировать не тратя на это ресурсы и емкость сети сделать это явно иногда может быть вообще вам нужен какой-то гибрид классических и теплер ных методов для того чтобы опять же уменьшить ту часть сети которые вы обучаете и как-то более оптимально имплементировать те вещи которые вы уже знаете как должны работать не оставляйте сеть учить то что можно задать явно теперь набор конкретных трюков уже прям совсем прикладных для того чтобы делать компактные и быстрые модели вот смотрите вот допустим делаете zver точную нейронную сеть у вас какой-то момент получилось очень много карт признаков например вы делаете конкатенацию с различных веток вас получилось много карт признаков и чтобы дальше это все не тащить потому что это на самом деле если потом еще поставить и сверхточный слой у вас будет много входных карт признаков ой быть дорогая получится операциям на этом этапе можно это количество сократить с помощью например сверток один на один то есть если кто не слышал про свёртка 11 а такая звучать на операция которая по сути на выходе вам дают тоже какое-то количество карт признаков которые есть линейная комбинация прямо по пиксель на предыдущих карт признаков ну то есть вот просто в пространстве вот в канальном пространстве у нас происходит такая полна связанная маленькая сеточка а каждый пиксель и работать независимо таким образом мы практически за дешево в общем без вот сложных сварочных слоев получаем уменьшение количества карт признаков и потом вы можете уже дальше ваши свертки замечательно это все это накручивать дальше очень частый кейс это свёртка и pulling по сути свёртка нам нужно для выделения признаков pulling для инвариантности для отсеивания всяких шумов не для значительной формации так далее понижение размерности вот это все пару можно заменить на самом деле на свёртку с астрой дам что вы здесь получит вы здесь тоже получите и некоторые выделение признаков потому что вас есть обучаемый фильтр сверток и вы получите понижение размерности за счет как бы понятно что съемка со строевым например straight 2 у вас примерно в два раза в пространственном измерении тоже все ваших вечер map и сожмутся но здесь вы получаете во-первых меньше операции во вторых меньше обращение к памяти потому что редко у вас будет свёрткой pulling за один как бы проход вычисляться все-таки это как два отдельных слова часто идут и опять же вас памяти все туда-сюда перекладываем чтобы этого не было используйте страдает can валюш дальше функция активации на самом деле целый зоопарк функций активации сейчас есть и вот вам над хорошо задуматься вообще нужны ли они вам если вас это ваша цель это быстрый inference а не скажем так плюс полпроцента в качестве то используйте самую простую функцию лилу они всякие разные функции по двум причинам первая лилу практически самая простая функция в плане реализации и быстродействия вторая причина очень часто если вы будете использовать какую-то библиотеку где реализованной операции для нейронных сетей там функция конкретно функция rifle у вас на сегодняшний день уже смержи нас фьюжена с разными другими слоями например на свёрткой с другими функциями активации такое не получите это будет считаться как отдельный слой опять же смотрите навсегда на ваших качество если все-таки вам так нужна что ваши конкретно нейронной сети вот прям вот очень нужен там ли кириллу там пример там в гаага но вы используете ли еще что-нибудь вот конкретно уверен что силу обычным не взлетит и вы попробовали не взлетел тогда конечно используйте но начинаете стрелок дальше вот всевышних слоях у вас есть такое понятие как padding и очень часто люди борятся с ограниченными артефактами когда вас по краям картинки появляются кит артефакты с помощью какого-то хитрого умного по 1 га который там делать reflection в разные стороны так далее тоже этого лучше не делать используйте либо зиру padding то есть нулями обрамляется картинка либо вообще padding не используйте то есть используйте вылет padding в котором вас фильтр ставится прям в левый верхний угол картинки без всяких там были за не за его границы если все-таки у вас есть проблемы с ограниченными эффектами например вы обработку картинок делаете то не нужно делать такой хитрый padding на каждом своем на каждом слое делаете на каждом слое обычный простой padding а делать хитрый padding в самом начале когда вы исходную картинку просто берете и допустим немножко раздуваете чтобы края и сделать получи чтобы потом их отрезать и делайте crop в конце чтобы вас floss есть опять же вы делать например сеть которая картинку отображает в картинку чтобы вас волос весь этот мусор по краям не попадал делать укроп в конце вот и опять же зиру padding эта вещь которая достаточно быстро реализовано сейчас на различных низкоуровневых в ремарках новый лет по 1 понятно почему быстро потому что там вообще ничего не надо вылезать нам еще один совет используйте то количество карт признаков который хорошо дружат железом например 8 там 16 так далее общем что-то кратное 8 ну наверно это как бы для большинства людей это очевидно но для людей которые вообще недавно пришли в компьютер сайнц и вдруг начали заниматься deep лингам им кажется ну меньше лучше а при чем тут вообще 8 в степени двойки икра 8 но действительно как бы железо так реализовано скорее всего и ситаджи пью или тоже там может быть семью или еще что-то там есть всякие различные вещи за заточки под конкретные размеры 8 будет работать быстрее чем 7 и вот такие вещи то есть если вы хотите ужать вашу юную сеть это не значит что если вы там из 16 сделали 15 то будет хорошо посмотрите внимательно какое количество карт признаков или может быть даже размер лидер вам надо взять как правило в какой-то библиотек которую использовали типа куда н н а еще где-то указано как и оптимально брать размеры а дальше идем по трюком смотрите кроме сверток обычных вот таких вот классических есть специальный тип и сверток и среди них есть специальные типы сверток которые заточены на быстроту например такая d.va из свертка которая по сути работает поканально каждый двумерный канал входной она сворачивается каким-то двумерным ведром и не делает потом сумму по всем каналам как это делается в обычной съемки то есть нас просто такой набор двумерных сверток они 1 трехмерная съемка как в обычной свертки дело сделаем такую операцию в принципе не очень много потеряли мы в качестве зато существенного ускорили операцию дальше возникает вопрос а вот у меня был на входе 3 каналы на выходе 3 канал да я же не могу менять количество карт признаков в такой операции да но остается константа вот ничего не мешает вам потом сделать еще свёртку один-на-один чтобы как я говорил ранее изменить то самое количество каналов то есть это просто тоже такая дешевая операция которая направлена на изменение количества каналов и все это вместе называется внезапно по канальная separ обильная свёртка и вот это основная идея которой используется в неровности под названием мобайл нет то есть по сути у вас как бы такая separ обильная свёртка разложилось на сначала двумерную свёртку потом свёртку один на один в в канальном таком измерении дпс и пара сепары вулканов очень классная штука используйте и ну и дальше начинаются со всеми за тические трюки а если коротко то всю эту идею можно описать следующим образом вот здесь у вас есть какой-то слой и он задан каким-то количеством параметров и очень часто все эти параметры они избыточны и можно как-то параметризовать какой-то слой и чтобы сделать меньшее количество параметров потенциально этого мы в памяти и в компрессии поможет и в ускорение работы вот как пример допустим нас есть полна связанный злой это просто какая-то там для простоты возьмем квадратная матрица н.н. и вместо того чтобы хранить н квадрат значения давайте хранить только n значение а все остальные допустим то есть допустим первый столбец а все остальные столбцы это циклический стык первого столбца то есть получается такая циркулярная матрица циркулярном и это допустим наши наш полна связанный слой если у вас допустим не квадратные матрицы то можно сделать блочную матрицу из таких кусочков а то есть таким образом мы избавляемся от избыточности и люди проверялись ведь на такие штуки работают они еще быстрых давал считаются потому что такие циркулярные матрицы умножение с таким матрицы можно легко и имплементировать с помощью преобразования фурье в общем такой как бы уже хаки на уровне вроде операцию оставляем как она была у нас и размерности все остались но избыточность в данных все равно убавилось кроме того это ещё поможет вам для борьбы с переобучение так переходим к следующему очень интересному методу ускорение ваших герон их сетей это прореживание или чаще всего услышите термин owning же такой вот есть у вас нейронная сеть . есть замечательные ножницы и вы начинаете нейроны в ней отрезать все это еще началось с давнишних работнику на под названием optimal brain damage то есть как бы подрезать нейроны вашем мозгу чтобы все еще как бы осталось более менее нормально а на выходе получается какая-то вот такая вот разреженная сеточка с меньшим количеством связей с одной стороны кажется ноги как бы меньше вычислений с другой стороны те кто из вас когда-либо вообще работал с нейронными сетями с тем как они реализованы наверное представляет что это будет не слишком оптимально потому что такая структура от которой здесь первым пунктом разреза не структурированный пру ming она не очень хорошо дружат железом то есть до в железке у нас или в каком-то несколько уровней фреймворк у нас как правило это все реализовано в такими большими параллельными потоками умножаем большую матрицу на большую матрицу или свёртку делаем а то что у нас матрица будет разрежь наверное не очень нам сильно это поможет как раз возможно на борту вверх от ток какой там появится поэтому такие штуки будут хорошо работать только специализированных устройствах во всяких отсеках и так далее где вы уже примут сами connection и между нейронами как-то указали вас все классно быстро работает однако если вы все-таки это делаете на каком-то general пепперс оборудование то вам поможет структурированный pruney нг в котором мы не просто какие-то случайные нейроны удаляем а удаляем прям целые строки и столбцы случаев полна связанных слоев в матрицах удаляем или прям удаляем целые карты признаков или фильтры или еще какие-то очень структурные компоненты в разных слоях общем так чтобы у нас это все еще было хорошо работал на железе все еще была в рамках исходного так исходные парадигму то на свёртка или вот у нас полна связанная плотная матрица и так далее теперь как собственный сок этот прутик burning сделать итак начнем с того что нам нужно нейроны или там фильтры или те вещи которые мы собираемся удалять как-то проранжировать то есть чтобы собственно понять какие вещи какие нейроны вносят минимальный вклад в качество работы нашей модели то есть например мы можем этапом точности по какой-то метрики отлавливать теперь как собственно это сделать что ли когда к ним не мне и сделай комбинаторный взрыв не перебирая всё на свете на самом деле можно просто каждому нейрону опять же или блоку отдать соответствие важности по какому-то критерию например нормы этого фильтра то есть и фильтр близок к нулю то скорее всего он никакого вклада не дает либо если это отдельный нейрон то тоже если в нейрон значение в связи между двумя нейронами близко к нулю то можно собственном связь и отрезать более продвинутый методы используют норму активации или градиентов по активациям или какую-то комбинацию на природная с последних работ и нвидии про комбинацию активы и шин активации на каком-то слое и градиентов по активациям это произведение этих вещей дает нам представление о том вообще можно ли целую вот эту карту признаков двумерную выкинуть и вот теперь собственно когда мы знаем что мы хотим выкидывать в чем же собственно процесса стоят у нас было исходная обычная модель мы отранжировали нерона или опять же в структурном случае отранжировали фильтры карты признаков удалили их но на этом мы не останавливаемся это не значит мы выкинули ненужное и пошли с этим в продакшен скорее всего вас в таком случае очень сильно упадет качество вам нужна ваша модель до обучить этот dotsub допустим это дело с помощью fine тюнингом мы ее до обучили и вот уже у нас получилось та самая маленькая модель на выходе уже обучена из повышенный качества с повышенным качеством хорошо работающая теперь если бы мы сразу брать взяли будут эту маленькую модель и обучали бы ее с нуля то результат был бы хуже чем если мы сначала обучали большую модель вне чего-то отрезали ее немножко докрутили до обучили но самом деле даже более того это все в один шаг не делается мы это делаем маленькими шажками то есть мы сначала отрезаем по чуть-чуть да обучаем еще чуть-чуть и отрезали добычи или таких операций может быть несколько как только вот мы уже все максимально все что могли отрезали выходим из этого цикла и получаем про реже ную высококачественную и в то же время маленькую модель хорошо кроме пру инга еще есть такая замечательная вещь как матричные разложения но вспомним или узнаем для для кого-то да тут есть такая замечать на меч как сингулярное разложение вот она слева записано по сути у нас некоторые матрица раскладываться в произведении других матриц и это за разложение замечательно тема первых она просто магическая супер крутой разложение самое лучшее которые когда-либо встречал она применяется просто в огромном количестве задач и различных алгоритмах и так далее но в конкретно сегодня она нам пригодится для низко ранговой аппроксимации матриц то есть мы хотим получить вот нас был исходная матрица w мы хотим получить другую матрицу которая такого же размера но меньшего ранга мы это можем взял с помощью сингулярного разложения на самом деле просто вот в матрице у ивы или там с в том числе мы как бы за 0 им некоторые строчки или столбцы или вообще их выкидываем теперь как нам поможет в случае с нейронными сетями вот у нас есть полна связаны слои там есть допустим была какая-то большая матрица после такого сингулярного разложения и выкидывания некоторых столбцов и строк который соответствует маленьким сингулярным значением у нас получается что большая матрица она примерно представим в виде произведения двух таких маленьких мадрид это не точное будет разложение но вот в плане там фробениуса венорм это будет лучшее представление для такого ранга r вот мы выбрали такое число р это вот собственно толщина соответствует этих вот кусочков для него мы сделать такое разложение у нас мире как бы уже вместо одного полна связано слоя 2 полна связанных слоя или просто допустим там если мы в посередине не будем делать функцию активации это как бы не совсем два парня связанным слой но это как бы две таким а . но операция здесь будет уже существенно меньше и мы опять это все да обучаем то есть мы не оставляем как есть разложили выкинули не нужные строчки или оставили как есть нет мы потом эти матрицы берем как начальная инициализация еда обучаем fine tune все это опять же на наших данных чтобы они немножко скорректировались теперь что делать есть у нас свёртка там уже не двумерной матрицы там уже какие-то простите тендеры или тензора как там но допустим было у нас обычная простая свёртка это такая вот трехмерная она на самом деле свёртка потому что нас есть на входе двумерной карты признаков их у нас н так как мы делаем после двумерных сверток еще суммирования по каналам этой ковалентно трехмерной съемки как будто если бы мы в исходном пространстве признаков бегали бы таким трехмерным окном и ему соответствует какое-то одно значение в выходной карте признаков вот так бы работал бы обычное свертком вместо этого можно вот эту трехмерную сверху как бы разложить то есть эта аналогия или даже потом прямое соответствие той самой separ обильной свертки который мы уже видели в мобайл нет по сути мы сначала делаем одну свёртку с таким вот одномерным ядром ну точнее одномерно в пространственном измерении и понятно у него еще есть измерение некоторое в каналы это вот красненькая потом в результате мы делаем другую еще свёртку в другом измерении это близко все параллельно свёртка отличия в том что у нас как бы разное количество каналов в первом и втором случае разная глубина вот этих вот первые 2 кубика но по сути мы опять же уменьшили количество операций сделали как бы такое аль any scar on приближение но в большем скажем так большего порядка и вот например так можно можно на этом не остановятся пойти дальше вот например использовать каноническое тендерное разложение в котором у вас уже буквально все это представляется такими 1 ранга вами цензорами да пусть сначала мы сделали свёртку один-на-один потом сделали пространственную свёртку по высоте не трогай при этом канале потом канала не трогай при потом сделали в горизонтальные так далее в общем разложили на такую композицию очень маленьких операций опять же все это следует из тендерных разложением можно разные попробовать вот целый список различных тендерных разложений наверно какие то даже два из них это одно и то же в общем разные вещи люди пробуют и делить на удается сжимать нейронной сети с помощью этого с небольшим с небольшой потерей качества и опять не забываем что как только мы разложение тендерное сделали все это еще немножко нужно докрутить до fine tune то есть вот эти все параметры до обучить но опять же если мы просто случайно инициализировать вот такое разложение то получилось бы не так хорошо потому что мы все-таки инициализируем эти все параметры вот во втором случае тем что получилось в результате разложения они берем случайные значения куча тендерах разложением вот это все что касается построение модели давайте теперь вот мы модель уже построили все что до обучения вы могли сделать сделали но иногда мы конечно еще потом то обучали но опять же это все что связано со структурой теперь посмотрим как собственно делать оптимальное обучение как во время обучения нам сделать так чтобы у нас inference хорошо работал во первых лосс функция очень важно то есть иногда вот вы поставили какой-то лосс которую функция которого минимизируете функция потери и в принципе добились хорошего приемлемого качества которое выше например порога который вам задал раза до лифта за еду нет ну хорошо хватит нет смысла дальше поднимать качество но на самом деле нужно использовать тот лосс который дает максимальное качество на маленьких моделях то есть чтобы ваша сеть эффективно использовала свою емкость то есть не надо делать так что вы сделали большую сеть и какое-то взяли лосс абы какой not из-за просто из-за размера сити у вас все сошлось сделать упор именно на лосс функцию для повышения качества они на наращивания модели но опять же это конечно зависит от задачи если у вас например просто какая-то классификация и вы там дальше чем кросс энтропию вы ничего может быть сделать не сможете а может и сможете но и чего сложной задачи например обработка картинок или еще какое каких-то других данных смотрите внимательно на функцию потеря например от ганы люди придумали это принципиально новый лосс который существенно помог люди даже не думали что вообще в генеративных нейронных сетях такая мощность заложено что кажется они могут такие классные вещи генерировать достаточно было просто лоссом класса накрутить в общем обращать внимание на лосс а еще такая вещь достаточно advance avaya довольно старая у нее есть много вариаций общее такое название канала тч distillation дистилляция знанием а смысл такой что вы хотите опять же на выходе получить маленькую сеточку но не знаете как ее обучить если вы будете в лоб и и обучать на ваших данных получится не очень хорошо а вот оказаться оказывается что работает плюс-минус хорошо следующий подход у вас есть какая-то большая uber сеть которая очень классно обучается на вашей тренировочной выборки вы ее обучаете потом как только в эту сеть и обучили вы как бы делаете некоторые синтетические datasette который по сути характеризуют работу от этой вашей обычной сети то есть по сути исходный datasette он тоже как бы и характеризует да потому что неровно сети то есть аппроксимация вашего to set a но мы хотим еще больше это расширить то есть на генерить каких-то рандомных синтетических данных прогнать через нашу сеть посмотреть чего она на выходе выдает то есть как бы такую табличку составить а собственно как бы что вот это наша большая модель собой представляет в табличном смысле это когда мы такие уже новые данные получили мы их используем для обучения маленькой сети и вот там мы на самом деле много разных вариаций у этого метода есть но оказывается что такие вещи работают иногда для того чтобы обучить маленькие сети не натравливая их на исходные ваши данные она травле в их как бы на данные который соответствует работе более сложные модели у этого подхода есть много вариаций если загуглить там есть студенты учителя вот так и terminology разное хорошо а теперь мы все обучили зафиксировали даже сделали может быть пару минком что мы ещё можем смотрели сделать not есть такая вещь как понижение точности как правило вы все ваши модели обучаете во флоте нгпу n 32 вот в таком диком диапазоне 10 38 но зачастую вам этого на сам не слишком много у вас нет задачи там в точности рассчитать какой-нибудь ядерный реактор очень часто вы делаете какие-то предсказания где вам вообще все в итоге округляется и вам неважно вас ответ sti был 095 или 096 в плане в каком-то там нейроне да вот такие маленькие погреешься допустим и поэтому вы с легкостью можете все это точность понизить допустим да fighting on 16 16 битный флот если конечно у вас ваша реализация вашей библиотекой ваше железо позволяет вам такие операции вычислять по это что это будет работать быстрее но можно пойти дальше и вообще сделать еще и понижение до интеджер до целочисленных каких-то значений начиная с int 8 потом вообще можно in 4 и вообще есть примерно unique вы слышали бинарных нейронных сетей где вообще просто параметры модели это какие-то просто бить и кеном 01 или там один минус 1 в общем понижать диапазон и точность ваших впечатлений с помощью их то квантизации тут тоже можно много чего придумать а квантизация какой-то в корзины разные веса класс there'sa вать и так далее ну по сути это связано с понижением диапазоны и квантизации однако если вы уходите от флотов и приходится фрагментом то поэта чтобы вас все умножения все еще правильно работали чтобы не перед не уходила все из диапазона допустим вот and 8 это не очень большой диапазон до 127 а может потребоваться перекалибровка параметров вашей модели то есть так как из калибровать чтобы все рабочие диапазоны всех активации они не выходили за тебя по зону вашего значения некоторых фреймворке например за помогают делать автоматически теперь вот опять же все мы уже наша сеть полностью подготовили теперь начинаем ее запускать и вопрос теперь начнем мы ее собственно запускаем какие алгоритмы мы используем под капотом нашей нейронной сети вот начнем со свертки но и в принципе ей же и закончим вот есть опять же вспоминаем наша исходная свёртка это если мы и делаем по определению тут у нас есть такое скользящее окно зелененькой мы идем по исходному нашему трехмерному тендеру в таком скользящим стиле в каждом окне в каждой позиции окна вычислим скалярное произведение ответ пишем выходной массив это вот так бы мы делали что называется хрестоматийно как это можно ускорить на самом деле есть несколько способов ускорения свертки и один из них очень часто применяющийся это свёртка через умножение матриц вот если мы в лоб так будем реализовывать свёртку как я до этого показывал это будет не так оптимально как если бы мы это делали следующим образом сначала мы сводим задачу свертки к задача умножения матриц потом быстро умножаем матрицы потому что это как правило obtain операция которых хорошо за оптимизирована на железе на каком-то там специальном процессы и так далее быстро умножаем матрица потом обратно все это немножко разворачиваем с помощью там простого решили по и опять получаем тот же результат который если бы мы получили в результате свертки в чем здесь идея но смотрите опять же как я сказал нас свёртка это такое скользящее окно в каждом из которых в каждом позиции мы вычисляем скалярное произведение пишем ответ на улицу что если мы такое каждую позицию такого окна вот этот кубик возьмем эффекта рисуем таким образом нас получится набор патчей который вот так вот остается вот эту матрицу зелененькую это операция называется int укол и так как нам нужно просто с каким-то керном я из с ядром свертки вычислить скалярное произведение вам это можем опять же видим матричного матричный векторного умножения записать то есть просто зеленым матрицу мы умножаем на первом есть у нас несколько ядер сверток то есть несколько выходных карт признаков у нас это целая матрица кернов оранжевенькие вот мы то все классно умножаем получаем другую матрицу которую если мы потом немножко опять развернем даже разворачивать особую не надо там по сути просто нужно от такого одномерного представления перейти в двумерное мы получим выходную карту признаков другой способ похоже операцию сделать через матрица теплицы это без имт укола прям сразу напрямую мы строим такую blu но матрицу теплица которое по сути тоже соответствует вот этой матрицы из патчей сверток и все это быстро умножаем окей второй способ который тоже часто на сомали применяется это через преобразование фурье вот есть такая замечательная теорема теорема отвертки которая гласит что если у вас есть тверд свёрточная оператор вот этот звездочка вас есть две функции какие-то f и же если вы их вот одну свернули с другой если вы от этого всего возьмете фурия преобразованием то это будет эквивалентно поэлементно он просто произведение но если это функция то просто произведению фурье образов каждого из ваших сигналов f и же в случае в нашем дискретным и там даже в двумерном это тоже работает то свёртка это наша свёртка преобразование фурье вы можете взять там допустим дискретное преобразование фурье который быстро реализовано с помощью быстрого преобразования фурье fft вычислить а фурье образы вашего исходной картинки вашего карты признаков и вашего ядра свертки например f и же потом их просто перемножить по элемента не надо ничего суммировать ничего reviews никакой не делать никакого в обычной свертки любого умножения матриц просто по элемента перемножили две матрицы это этого взяли обратно и фурье и получили как бы результат свёртки можно на этом не останавливаться и вообще сказать а зачем нам вообще нужно зачем нам нужно преобразование фурье делать обратно почему бы нам все вычисления не делать прямо с в пространстве фурье если там свёрткой ковалентно поэлементно умножения там просто фокус будет с другими слоями но об этом как бы уже отдельно по читайте в статьях как функции активации на все это травить и так далее но в целом данным ничто не мешает работать спектральной области где свёртка эквивалентно просто по элементам умножению отличная идея ну и на закуску есть такой алгоритм виноградом по сути изначально он был просто алгоритм быстро умножения некоторых маленьких матриц специальных как бы если просто мы обычные матрицы умножаем на матрицу на вектор умножаем там сложность у нас n в кубе и вот с помощью алгоритма винограда там сложность о большой от n в степени 2 , что то там в общем короче чуть более быстрый алгоритм умножения матриц которая там сокращает количество умножений зато чуть больше там сложение и его тоже действительно применяют в звездочных нейронных сетях когда у вас какие-то определенные специальные размеры лидер и там допустим пещер map of в общем в случаях когда это действительно будет быстрее работать чем другие методы кстати во многих библиотеках например в куда н.н. вот из этих трех методов выбор происходит практически автоматически если у вас известны размеры всех ваших лидеры или например железо известны в общем очень часто просто есть правило если у вас размер и такие-то выбираем такую-то алгоритм и так далее хорошо что ещё мы можем сделать на этом не конец можем еще оптимизацию графа сделать итак смотрите что то что можно сделать с графом ну во первых можно сделать слияние слоев то есть чтобы у вас не было такого вы его вызова слоя от одного за одним можно делать слияние нет низких слоев в один слой самое простое что вы можете сделать это следующий если у вас есть батч нормализация это по сути просто как вы все там зафиксировали обучили это по сути просто некоторые линейные слой такое аффинное преобразование все умножить на какое-то какие-то числа и ко всему прибавить какие-то числа и вот эту операцию можно просто очень дешево вшить и припаять к свёрточная условия просто так как у вас свёртка это что-то там умножили к что-то прибавили и байт реализация тоже потом сверху без всяких там нелинейности прям сразу потому что очень часто большой вставляется перед функция активации большом тоже что-то умножить что-то прибавили все это вместе расписали в за за один как говорится проход сделали в более сложных случаях если у вас опять же реализация позволяет вы можете прям пачки делать еще из функции активации например свёртка бас лилу или свёртка большой брелок но это опять же если у вас реализация такая есть вашей библиотеке если вы сами это пишите дальше еще есть горизонтальное слияние это как правило работает случая когда у вас несколько слоев имеют один общий вход и чтобы это не считать вот допустим как несколько слоев я на следующем слайде покажу а конкретный пример на общем грубые когда несколько слоев имеет общий вход все это можно объединить в один такой широкий слой с помощью горизонтального слияния ну и третье это удаление нужных слоев кроме тех слоев которые просто ничего не делают и подходит и причине у вас остались после обучения но все-таки холостую они почему-то работают вы можете еще избавиться таких вещей как конкатенация сейчас покажу почему вот пример работы всех вышеперечисленных методов слияния слоев и и тут нас допустим слева некоторый блок модели inception справа после оптимизации узнать у нас все свёртка боясь ярилу смерть такие булочки 3 на 3 cbr а вот эти вот нижние свертки 1 на 1 3 штучки слева в самом низу они смерть такую широкую свёртку один-на-один потому что мы можем как бы все это просто с конкатенировать и третье у нас на самом вверху слева было опираться конкатенации что он по сути дела она просто нет некоторые буферы памяти отчитала потом записывал в другое место и так далее ну чтобы этого не делать мы можем просто предыдущем своем сказать а пусть они сразу пишут в нужную область памяти там где у нас будет выходная конкатенация и просто избавиться от лишнего копирования кроме того просто такое тоже советует не не столько про ускорение сколько про менеджмент памяти переиспользовать и промежуточный буфер то есть у вас есть 10 слойный нейронная сеть это не зачем вам нужно там за 11 буферов промежуточных вы можете сделать так что вас допустим из просто два больших буфера и вы вовремя inference а первый слой допустимость 1 буфера во второй пишет потом обратно из сторон буфера в 1 так вот пинг-понгом вы в двух буферах только работать у вас этим и на память повлияет потенциально на скорость может повлиять есть у вас например опять же все это умещается в маленькой какой-то объем быстрой памяти на каком-то специальном устройстве в общем тем менее полезная штука если вы можете такие вещи и реализовывать внутри вашего там программного комплекса лев фреймворка так ну теперь вот с трубками покончено а теперь нас нас на чем же собственно все это запускать делать если вы допустим не хотите все это делать руками начнем с железом ни для кого не секрет что вот есть такие специальные две штуки или три на которых все классно летает и машинное обучение и inference и все вот начнем с того что джипе юн с недавних относительно пор но относительно недавно люди распознали как одно из таких вещей на котором сидит на быстро все работает потому что изначально g пью были заточены под графику там много та же задача таких рассчитанных на массовых параллелизм ну и в наших задач машинного обучения тоже есть массовый по релиза нам нужно умножать большие матрицы делать свертки так далее все это классно ложится на gpl если пойти дальше то хочет уже какого-то специализированного железо и все это можно реализовать на каких-то ft j или оси как то есть в общем когда вас конкретная специальной архитектура вашей неровности вы можете это уже реализовать на специальном железом но там есть определенные сложности в плане джипе вот например у и nvidia есть куча разных замечательных карточек и чтобы вы не думали что все ограничивается каким-нибудь же for some it исламе хотят есть справа тесла которая специально для inference и заточена несколько такого низко мощного то есть с маленькой мощностью с высокой пропускной способностью вот внизу два примера это драйв и джексон это по сути тоже чуть-чуть уже специализированные решения одно для автомобилей стал drawing app драйв и джексон это вот самое только на самом деле мне кажется подходящим вот этот доклад вещь эта штука которая как раз для таких вот для мобильных роботов для каких-то мбт устройств по сути там почти полноценный gps такой же практически мощностью просто точнее пропускной способностью но с пониженной мощности с пониженным тепловыделением теперь есть такая штука как тендерная ядра и вот появились они с момента архитектуры вольта что это такое но джек you это у так и также массово параллельная такая штука но можно на сам не это все еще немножко ускорить с помощью таких специальных уже хардкорных акселераторов которые быстро умеют умножать матрицы 4 на 4 и там тоже прибавлять матрица 44 они все это делают с пониженной точностью вплоть young под 16 но зато как бы делает это просто мгновенно то есть как только вы используете fp 16 у вас есть поддержка тендерных ядер и то есть в архитектуре вольта и during вы можете использовать вот эти тендерная для быстрых вот таких матричных умножений в по нежной точности но и в тюрингии там еще появились даже вот and 848 раньше был и там еще появился int четыре то есть прям совсем можно быстро все умножать если вас супер понижает очень сказать in 4 это немножко такая уже экспериментальная вещь но как как задел на будущее почему нет но то из одной статьи взял такую вещь сравнение различных вот этих вещей как процессоры в пи джи джи пью по мощности и по пропускной способностью дышать же пью это такие мощные вещи но в то же время производительные но вот если взять джейсон про которой от этого раз кого-то по сути вещь сравнимое там sfp j может быть по тепловыделению но зато сравнимо с g пью по скорости ну и в заключении пару слов про софт который вы можете использовать если вы не хотите треть наверно из всего что я рассказал реализовывать самостоятельно вы можете это взять в готовом совсем так как мы много говорили про джип ну почему бы не говорить про софт который классно работать на джипе поту и nvidia есть куча разных классных библиотек основные из них это куда and tender rtm ну там кубло с для матричных умножений но откуда не что такое мысли текут н.н. это уже реализованы примитивы такие как свёртка pulling матричное умножение фулика на этот слой и так далее иногда там даже есть уже фьюжен с функция катим от с функциями активации но основная те в том что вот эти вот все быстрый алгоритм и например сверток про который вам рассказал там уже реализованы и куда практически сам выберет нужный алгоритм и отдельно хотел бы сказать про библиотеку тендер rt это вот второй в серединке сверху что делать н д а р т по сути она делает ряд оптимизация про который вам уже говорил первое она делает калибровку точности то есть если вы понижаете точность двое впо 16 или int 8 все это можно сделать там в один щелчок пальцев и автоматически будет перекалибровка весов в случае с интером кроме того тендер ты тоже делает о слиянии слоев от все step если я не про который рассказывал удаление конкатенации слияние вертикально и горизонтально все это есть там автоматически вы просто скармливаете свой граф и все вам делать автоматом и третья важная штука это автоматический подбор наилучших реализации в зависимости от вашего железных есть например у нас конкретная нейронная сеть конкретный слой youth конкретная железка конкретный размер бочче конкретное все конкретное конкретная конкретно и библиотека знает а на какую конкретную реализацию нужно запустить для этого конкретного слоя ну и много разного другого вот а если вы хотите прям inference такой продакшене делает не просто по одной картиночки гонять вы хотите прям серьёзную вещь что вы можете воспользоваться тендер the engine сервер который по сути представляет такое полноценное решение вас на входе приходят это запроса сделали пожалуйста inference штука это автоматически все распределяет по джикию расстреляет по контейнерам с разными фреймворке в общем вот весь менеджмент делает где тоже как бы ботаник иногда присутствует так на этом в принципе у меня все вы творите есть ещё куча литературы для тех кто будет эти слайды потом смотреть не знаю кому кому нужно сфоткать я вам дам секунд до 15 это вот 1 слайд пока фоткайте еще немножко про тензор rt inference сервер понятно почему это важно для быстрого inference а потому что быстро интернет это не только прогнать модель от входа к выходу а это еще вот собственно взять картинку вашу или что-то от пользователя обратно ему отправить в общем чтобы это все тоже оптимизировать тендер the entrance сервер литература 1 литература 2 на самолет на первом слайде там были первые пару ссылок это замечательные блок-посты одного товарища который вот буквально примерно 50 60 или даже 70 процентов из того что рассказала он там все это тоже описал вот литератур твое общее резюме что мы сегодня с вами увидели мы увидели что такое структурная оптимизация как построить модель как сделать трунин как сделать применить то размер на и разложение потом как сделать оптимальное обучение как потом понизить точность какие есть оптимальные алгоритмы обучения сверток как потом оптимизировав на ком железе все это считать и какой софт для всего этого использовать на этом все спасибо большое наверное чуть-чуть вопросов можно успеть задать спасибо за доклад вот вы все рассказали так все здорово поделились трюками скажите пожалуйста может быть вы приведете один там или два примера где вы реально это применяли и насколько это помогло там поскольку количество раз результаты улучшились да конечно вот смотрите во первых сразу дисклеймер почему я не говорил ничего от числа что вот вот этот трюк дает такой это прирост и так далее по двум причинам первая я считаю что это достаточно сложная работа все вот эти все методы эти проверить или собрать по ним данным данные это прям ценят на отдельную статью такую обзор на второе что вот в литературе которая привел там люди говорят типа вот мы этот метод попробуем вот мы в конкретном нашем примере что-то получили все это просто под а почему я не приводил неких чисел теперь на моем личном опыте вот например я занимался как минимум двумя задачами где использовал вот эти приемы во-первых я занимался обработки изображений и там вы понимаете да что допустим нас на входе картинка на выходе картинка у нас свертки в отличие от классификационных сети где у нас все сводится так скажем на в такой вниз к размерную штуку на выходе свертки очень много места занимали и вот и pruney нгам и скажем так чего там еще то было вот как быстро реализация сверток тоже все это там использовалась это задача такой супер сверх разрешение там antialiasing style трансфер вот это все частично там применяется и вторая задача вот например предыдущий доклад был про облака там мы использовали библиотеку тендера rt которые тоже нам помогла где-то на нем раза в 2 наверно по сравнению с эндер flow все это улучшить хотят сеть там тоже было достаточно простая ну то есть вот смотрите если у вас интерес интересует числа то это от двух раз для каждого конкретного там трюка если вы их де в комбинацию тает у вас уже там может быть n 1 и где-то видел заголовок такое что типа каноническое тендерное разложение ускорило нашей съемочной сети в 3000 раз ночью зависит очень сильно спасибо ведь еще парочку вопросов успеем вот вопрос по поводу софтверных модулей то есть довольно богатый выбор ну как то вот на скидку кажется что вроде бы какие-то задачи которые они делают должны пересекаться или они как-то друг другу перри используют или там и ну это за независимых эти три частично перри используют где-то пересекается но не переиспользовать но если кратко то вот еще раз могу сказать что кубло с это просто но глаз то есть операции линейной алгебры в qt н.н. там по сути это самая такая широкая вещь в плане примитивов то есть там есть свертки но нет не просто форма то еще backward ко всем операции то есть куда н.н. он еще и для обучения применяется то есть весь у вас каком-то фреймворке есть обратно распространение через какой-то слой там это все тоже атомарная реализовано но например вы можете просить о чем куда н.э. на тендер ты отличает святой то для быстродействия там сверток тех же отличие основной в том что куда н.н. он заботится только вот атом арно об одной операции отверткой например и так далее мы не важно что вообще снаружи происходит и поэтому там больше операции больше их такой вот зоопарк бизерты смотрит на весь граф в целом и он по сути оптимизирует целый граф несмотря на то что он выбирает до наилучшие какие-то реализации чего-то у него меньше sap сет вот этих операций потому что в интернете как правило но нам не нужны backward и через инволюции нам нужны какие-то другие сложные вещи в интернете под множество операций меньше и поэтому в тандыр это не все операции реализованы зато их больше различных вариаций под конкретное железо то есть если qd н.н. это одна библиотека которая работает просто везде но смысле библиотек и понятно что одна и между одна и та же реализация выбрали shot хочу свёртку допустим через фурия вот она одна и та же будет везде то в тендере rt это будет там не за 10 различных вариантов этих сверток через фурье просто по-разному реализованы в зависимости от того какая у вас железка поэтому для тендер т важно чтобы вы ему еще сообщили на каком железе вы все это запускаете что конкретно под него было оптимизация и наверное это значит что она только на видео железе работа нанес с другой стороны если например я ну пользователь я строю обучая сеть через какой-то framework там да то есть это вот куда-то встраивается во всех практически фреймворков 99 процентов фреймворков под собой использует qd н.н. то есть у вас уже это автоматом есть более того тендер rt ну вы тоже можете использовать но с недавних пор например есть интеграция тендер tf tender flow прям можете из tensorflow его вызвать спасибо вообще правильно говорит тендер arte"
}