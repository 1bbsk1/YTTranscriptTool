{
  "video_id": "o-g8bFoLdSA",
  "channel": "HighLoadChannel",
  "title": "Трансформация Enterprise- и Highload-приложений в HPC / Дмитрий Кирий (Техцентр Deutsche Bank)",
  "views": 603,
  "duration": 2602,
  "published": "2021-10-04T02:45:36-07:00",
  "text": "за у нас она дмитрия серия из дочь и банка который нам расскажет про трансформацию enterprise эхолот приложение htc htc он тоже нам расскажет привет привет рассказывай меня слышно да да отлично так значит изначально доклад хотели я точнее говоря хотел сделать много более техническим чем он есть сейчас но когда мы осматривали мой доклад мы решили что мы его сделаем больше больше таким философским и о том что и как мы делаем и вообще что у нас интересного происходит первый вопрос какой у нас количество в зале разработчиков поднимет просто руки листе которые пишут код отлично те которые занимаются инфраструктуры всякими коммутаторами сторожами и все все прочее хорошо поэтому у нас будет battle of на самом деле между инфраструктур щеками и программистами о чем мы будем говорить мы будем говорить о такой штуке как трансформация enterprise и highload приложений в то что называется high performance компьют версии приложения из писи приложения на самом деле эти приложения это если мы вспомним всякие там моделирования как погоды до или моделирование землетрясений или еще тому подобные вещи то есть те штуки которые написано очень эффективных алгоритмов загружают память сеть вычислительные ядра надо на сто процентов как мы знаем в enterprise и файлов приложениях загрузка инфраструктуры очень очень далека от 100 процентной очень далек от стопроцентной сетевики например могут сказать что типичные сетевая инфраструктура загружена на 2 3 5 процентов и это уже хорошо о чем мы поговорим мы погорим о том что происходит в современных больших классических энтерпрайза на текущий момент времени и каким образом они вообще двигаются в будущее с точки зрения и software & hardware типичный enterprise это мы говорим о я напоминаю что не все на самом деле знают что такое большой enterprise компании мы говорим о организациях где от 500 до 5000 приложений неважно самописных не самописных и тому подобное от одного до пяти или до 10 миллионо я der то есть циpкa ягер хранение от 10 до 100 петабайт информации порядка пяти до двадцати дата-центров совокупная стоимость оборудования несколько миллиардов долларов безумное количество технического персонала который занимается поддержанием всей этой инфраструктуры и что больше всего беспокоит энтерпрайз на текущий момент это о том что стоимость функционирования инфраструктуры сейчас стало очень очень очень большой для классических энтерпрайза highload приложений классический enterprise тратят до полумиллиарда или порядка миллиарда долларов в год и это существенные деньги можно сказать о том что ну у классического банка там условно говоря там 100 200 до триллиона долларов оборот и вроде бы миллиард на обслуживание фра структуры не так много но мы говорим об обороте а есть а это фактически чистые издержки мы должны сравнивать с прибыль и на фоне 3 5 миллиардов долларов чистой прибыли миллиард издержек это очень много каким образом развивалась enterprise крупные банки и большая часть финансовых и технологических хронизации появились в прошлом 20 веке и где-то вы 40 30 лет назад у них появилась сайте это было мэйнфреймы это были так называемые средние машины as400 полный зоопарка пир операционных систем зоопарк это на самом деле так сказано для красного словца каждый из этих операционных систем имеет некие плюсы и минусы но к сожалению они просто устраивают то есть aix packard unix spark solaris и тому подобное очень естественно за 40 лет очень много чего написано на си си плюс плюс и к боли все это крутится на мэйнфреймов и все это прекрати все это необязательно например на текущий момент выведены из эксплуатации с точки зрения инфраструктуры ключевой фактор того что делает энтерпрайз это надежность деньги беспокоит но беспокоит не очень сильно потому что грубо говоря как они какой-нибудь попавший упавший набор серверов которых trading flow информацию перекачивают дата центра до означает то что скажем нью-йоркской бирже идущий банк не может торговать да и там 3-4 пятикратное резервирование этих несчастных серверов да это совершенно другие деньги по сравнению с тем сколько банк заплатит если его клиенты не смогут торговать не с кибер поэтому большой фактор инфраструктуру надежность это как правило парочка дата-центров находящихся рядом где в одном дата центре находится активно-пассивных наливали билетик в другой в другом дата-центры находится полный такой же комплект оборудование на suicide' завтрака very и там же на как правило находится среда для разработки и тестирования и тому подобное и естественно многократное дублирование сетевых устройств на всех различных уровнях ключевой фактор который крупный enterprise концентрироваться раньше это очень быстрая ли зация быстро задач лишь игра бизнес-задач почему потому что так устроен рынок что когда ты изобрел что-то новое что то прям самое современное именно это приносит прибыль и это приносит прибыль небольшое количество времени потому что конкуренты смотрят на то что ты сделал естественно все это невозможно запатентовать или запатентовать точно сложно и нереально естественно тоже все это у себя делают на своей платформы что означает очень очень простую вещь до что постоянно разрабатываются как какой-то новый функционал который потребляется клиентами именно на это на скорость разработки на скорость разработки бизнес функционала направлен большая часть разработки энтерпрайза что же у нас произошло в результате подобной организация естественно что мы говорим не обидели 10 devil и прах которые делают бизнес фича типичный enterprise это 1 2 3 10 тысяч разработчиков по всему миру 40 лет таких подходов к разработке породили собственно закономерный результат что сама инфраструктура которая было наработано то есть это десятки дата-центров куча различного оборудования и оптимизации всем этим никто практически не занимался почему потому что классический диалог в энтерпрайзе о том что вам надо чуть-чуть эффективно что-то делать ответ отца в твоей девелоперов простой вот если мы сейчас это не напишем мы не сможем заработать 20 миллионов евро вот поэтому давайте мы этим будем заниматься потом слова потом это самое классное слово она продолжалась 40 лет подряд и вот сейчас наступило время когда больше потом нет потому что оборудование не влезает дата-центра сложность сложность всей этой системы достигла запрись запредельной короче короче говоря сложностей с точки зрения инфраструктуры с точки зрения самое главное обслуживания да то есть когда на все технические работы в организации свое время из условно говоря 10000 девелоперов полторы тысячи технический человек который организует на местах просто всю эту работаю до что все это просто работала и достаточно много это деньги которые постоянно платятся людям и вторая проблема которая возникла it о том что инертность да мы все знаем как очень сложно меняться да вот у нас была комната там какого то вот не знаю хорошего бежевого цвета и он уже не в стиле да уже стиль какой-то модной серой и вот и жена уже говорят нури пора уже перекрасить это такой думаешь вроде все нормально было десять лет жили чего там перекрашивать но зачем бизнес относится также дата в 40 лет вроде ничего не происходило все было прекрасно зачем вот что сейчас такое произошло а произошло как мы знаем очень очень много и мы об этом поговорим чуть попозже что же мы имеем в результате активной разработке бизнес свечей мы имеем среднюю нагрузку циpкa от 5 до 15 процентов средняя нагрузка сети 15 10 процентов значит во всех наших 20 дата центрах и все они естественно полностью забиты оборудование и инфраструктур структурой такого масштаба уж сложно управлять то есть у нас очень сложно и топология сети и начал очень много старого оборудования которые занимают очень много места у нас старые плохие системы хранения нас плохая скорость сети и в общем с этим надо что-то делать и когда стоимость когда уже в таких организациях время доставки оборудования от того как ты заказал до того как она приезжает составляет 2 года надо что-то уже делать и собственно все эти проблемы привели к тому что бизнес начал интенсивно спрашивать эти сколько вы хотите денег на то чтобы все приделать как вы хотите все переделать вообще что вы хотите делать и прежде чем говорить о том что enterprise хочет делать давайте посмотрим на совершенно другой спектр на совершенно другую часть айти под названием high performance computing то есть что делают люди там в такой high performance computing нужно с вами поговорили это всякие моделирование ядерной энергетики тому подобное классические драйверы этой из индустрии это американский государственной лаборатории тип оукридж не супер компьютер центр или лоуренс ливермор или тогда lake порядка там 15 они все починят большая часть подчиняется министерству энергетики сша это и там находится крупнейшее один из крупнейших суперкомпьютеров в мире не считая китайских и что очень интересно что если мы вдруг сравним эти супер компьютерные центры по одному простому критерию а сколько у них компьютерных игр то окажется что я dirt и у них примерно на столько сколько в энтерпрайзе то есть порядка там вот допустим если в enterprise мы говорим 15 миллионов то скажем суперкомпьютер сами 24 миллиона естественно там намного меньше приложений потому что они узкоспециализированные если я там чуть чуть больше хранения да то есть если в энтерпрайзе я от допустим 10 100 петабайт то классические супер компьютерный центр хранятся без сто-двести и в частности сейчас строится например система хранения на экзабайт на систему которая будет заменять супер-компьютер сами что интересного здесь что для того чтобы все это все эти ядра обслуживать и так далее в энтерпрайзе 20 дата-центров которые требуют людей проб банально начиная с электро как тут тех людей которые примыкают к белле да и ставят оборудование в то время как у кричишь супер компьютерный центр это 600 квадратных метров то же самое количество вычислительной мощности находится на 600 квадратных метрах они в 20 дата центрах и это принципиальная разница с точки зрения обслуживания да с точки зрения управляемости и тому подобное и естественно эта гигантская разница с точки зрения цены потому что и это открытые данные супер-компьютер саммита порядка 250 миллионов долларов когда он был построен в то время как стоимость оборудованием энтерпрайзе это один два три четыре пять миллиардов совершенно разная стоимость тех денег которые мы платим каждый месяц если мы говорим энтерпрайзе у нас 350 1500 человек то супер комната саммита обслуживают порядка 30 сторож network и unix инженеров и все это выражается в простейшей вечер long kast если enterprise тратит полмиллиарда долларов то рано cost суперкомпьютера саммит составляет 50 миллионов долларов в год и enterprise очень очень хочет 50 миллионов и каким же каким же как же мы этого будем достигать если мы гипотетически вообще можем этого достигнуть с нацелен на том что нам пример как какие плюсы находятся все супер компьютер центрах это у нас есть 30 инженеров поддержки то есть у нас нет практически никакой сложности в поддержке нет диверсификации оборудования малая полная стоимость нет проблем с расширением очень быстрые сети и гиперскоростные системы хранения это все нужно энтерпрайза это все нужно хай-лоу да но мы-то почему-то не можем построить не файловый день и в enterprise почему об этом поговорим если вы были на предыдущем докладе кто был на предыдущем докладе здесь никого не был здесь докладчик говорил о анапских свечах и например скажем возьмем классический дата-центра несколько этажей и так далее и в классическом дата центре находится порядка тысячи свечей условно говоря 500 тысяч и так далее когда мы говорим о устройства такого класса сетевых на их достаточно всего два на дата-центр основной резервной ну они работают ecть эффектов и так далее вся сеть кардинальным образом улучшается вам не нужны 3 4 уровневые сети и все тому подобное у вас есть 800 портов на 200 гигабит полная пропускная способность 320 терабит и я я полагаю что это парочки таких коммутаторов я полагаю что это примерно 100 это скорость которая скорее всего где то есть в ядре у гугла здесь это всего находится на двух устройствах эти устройства в тот не запредельные деньги то есть они порядка стоят два три миллиона долларов в сборе при стоимости дата-центров существенно за десятки миллионов долларов за сотни я бы даже сказал бы да строить сеть на таких устройствах которых всего 22 может быть три это совершенно другого класса сложность и вот если вы посмотрите на сетевиков которые сидят в глазами и не верят вообще что такое можно потому что они боятся потерять работу вот это это действительно то оборудовани который существует сейчас да она сухим сейчас она будет развиваться его действительно можно использовать она стоит разумные деньги просто где бояться его использовать мы погорим как его дальше по использовать с точки зрения хранения естественно все знают о том что система хранения на основе жестких дисков уже отошли да и у нас уже по из all-flash массива но это я вам оставлю в качестве домашнего задания взять открыть современные системы хранения халед паккарда делая msi и посмотреть сколько в них ssd просто вот 10 штук например в одном юните какая у них пропускная способность и сколько же эти системы хранения отдают наружу и вы удивитесь что интерфейс и к этим система хранения условно говоря на 100 гигабит а внутри пропускная способность ssd один например один терабит то есть в 10 раз например отличается то есть где и в 10 почему потому что proprietary системы хранения просто не рассчитаны на современную производительность нвм я что например делают тип текущие вендоры спесь и оборудования они делают совершенно простую вещь они вообще даже не заморачиваются производством сложных контроллеров всех вот этих вот вещей которые делает pellet pack dlc они просто выпускают шоссе до с двумя ли там четырьмя процессорами amd или intel куда можно вставить там 32 например ssd и где наружу торчат 2 10 гигант в 20 гигабитных интерфейса и все of organization hail его или билете dior и всего прочего вот этих области устройств хранения предполагается выносить на open source сторож устройства на всякие кластерные файловой системы типа люстра назад fs в качестве систем которые собственно управляет волю мумию непосредственно внутри этих устройств хранения produce сальная ситуация что сама сборная до некоторой степени самосборные устройствах ранений в десятки раз производительный современного enterprise оборудования все что нам нужно нам нужно давать деньги инженером которые будут соответственно управлять кластерами файловыми системами который будет управлять задав с мы так далее и самое поразительное что это намного дешевле чем платить вендором за лицензии ну просто вот в разы особенно когда мы говорим о там 10 ст или 100 петабайт охранения сервера здесь показан классической классическая стойка атас был цикла на это и частичная стойка в чем ее суть в ней находится безумная плотность лидер то есть это порядка 12000 циpкa ядерно стойку и климат вообще как работают чем отличается принципиально бленды от серверов с точки зрения вот подключения сети это и тому подобные руки поднимите рост мало так тогда я значит об этом расскажу расскажу нет не расскажу значит сервера и все прочее оборудование у каждого сервера для того чтобы прасини к сети нужны порты да это логично сетевые порты 12 там или сколько надо так вот у плодов и в частности например вот этих плодов как стоит у них углу льдов нет никаких портов сетевых как это сделано внутри этой стойке есть отдельная шина из этой и к этой шине с другой стороны не с той стороны куда праслин и блины а с другой стороны шины присоединяются коммутатора то есть никаких индивидуальных портов до в блюдах нет они все присоединены к общему коммутатору который находится за стойки о чем это о чем собственно почему почему это хорошо это означает о том что вот сейчас у нас 400 гигабит новые соединения да и мы естественно не можем каждому серверу который загружен на полпроцента в 10 гигабит ах 400 гигабит их будет загружен на 100 процентов давайте каждому по 400 гигабит что мы сделали что мы делим мы можем на одну стойку вот просто вот в эту стойку от центрального вот этого коммутатора который дает 200 гигабит или 400 гигабит просто привести один провод к всей стойки и весь этот провод 200 гигабит он был шарится абсолютно всеми бла всеми вот этими благами если нам нужен еще один провод сами понимаете провести один провод от небольшая проблем три четыре пять шесть проводов до или тому подобное но это принципиально отличается от десяток из сотен километров сетевых проводов которые проводятся в современном дата центры и не используются для тут мы говорим о том что здесь у нас сеть перестала быть тем устройством которые соединяют сервера у нас сеть это то что соединяет стойки между другом и количество этих соединений в колоссальное количество раз меньше чем количество портов которые раньше был у серверов у серверов естественно есть внутреннее соединение типа там пися экспресса да вот но это совершенно другое другое другая сложность сети нам не нужны никакие многоуровневые многоуровневую такие вот системы где у нас есть top of direct свечи spine свечи лив свечи core свечи и так далее нет нас есть просто 1 уровень свечей куда воткнутые абсолютно все оборудование и этих портов как мы с вами смотрели на предыдущем платит 1600 портов по 200 гигабит до этого вполне достаточно чтобы делать дата-центра достаточно большого класса всего на двух сетевых устройств все это прекрасно оборудование у нас есть все это здорово но как же нам теперь писать software если мы software продолжим писать в том стиле которая ничего не использует а именно используют 10 15 процентов циpкa и использует сеть то мы просто не сможем построить компактные центр мы опять все построен дикое количество стоек которые загружены на непонятное количество процентов нет не то что нам надо нам надо по-другому писать software developers мы говорим девелоперам ребята вы с каждой машины можете получить 200 гигабит если надо 200 гигабит центральной коммутационную скорость у вас дверь 320 терабит вы ничем не ограничивать производительности нвм я стоик например 1 терабайт в секунду вам должно быть достаточно давайте попробуем по-другому писаться в твои в каком стиле давайте попробуем постели . естественно мы с вами должны пройти все пять стадий да то есть отрицание чо за фигня кто это придумал не такое нельзя гнев почему мы собственно должны что-то делать торг может быть мы сделаем чуть чуть по другому возьмем наш tomkad 32 ядерной запакуем в микро сервисы и вытащим не не так тоже не будет депрессия потому что надо все таки что то делайте наконец-то принятие что нам надо писать софт по другому сам хорошая новость в том что большая часть что мы уже на самом деле как software developer большая часть согласилась что мы будем переписывать софте микро сервис да то есть монолита то уже не то что не модно она по понятным причинам менее работоспособными микро сервиса и мы знаем там десятки плюсов почему микро сервисы лучше я сейчас не буду перечислять глаз мы знаем что перехитрить и переводить наши монолита или это то что крутиус виртуальных машинах на микро сервисы это сложно да это вот примерно вот такой объем работы для того чтобы у нас нашими красивой заработали как hpc нужно примерно дополнительно столько же работа которая направлена на простую вещь когда вы делаете микро сервис какое-то условно назначим это в 4 ядра и 32 гигабайта памяти он вот эти четыре ядра должен грузить на сто процентов и память он должен выразить на сто процентов и сеть которую вот только вот максимум которую можно должен грузить на сто процент почему нам это надо потому что современных контейнерных фреймворков у нас есть возможность сделать автоматическое масштабирование то есть ваше приложение которое разбита на микро сервиса она можно сканировать вверх и вниз и увеличивать количество инстансов реплик точнее говоря ваша контейнеров да если у вас растет нагрузка на на ядра например да это сказать просто простейшая вещь связана с кубер нить есть другие средства масштабирования с каждым по бизнес-логики ну то есть например у меня скажем не знаю там на 1000 сообщениями надо поднять еще один инстансы обработчика автоматически масштабирование надо пользоваться и надо его не бояться если раньше у нас было стать акадо то есть нас что-то за тепло и на на виртуальных машинах и на серверах и все она вот работает утром вечером в нём и так далее правильный вопрос себе задать зачем у вас ночью работает что то что не используется как правило ночью нужно масштабировать вниз приложение которые обслуживают пользователей и на них запускать каким быть нвд риск вычисления моделирования какой-нибудь оол об объединении тому подобное ук номер два к эти микросферы со нам нужно написать разделение процессов сохранения данных обработки то есть мы не можем себе позволить просто вот эти вот классический крут приложения даст читать что-то из базой поменять и сделать insert базу данных и нажать commit все это должно быть бочарова на все это должно отъезжает на другой уровень кэширования каких-нибудь распределенных кашей и конечно уже это должен писать бочками в базу данных совершенно другой производительности совершенно другой задержкой совершенно другим фра потом а вы как клиент как клиент кластера да вы избавляетесь вообще от необходимости писать что-то в базу данных да то есть вас получается разделение очень быстрых процессов работы с памятью и медленных процессов которым занимается специальный слой который сохраняет эти данные асинхронности пакет пакетная обработка это вообще прекрасная вещь которая появилась волнующе все рулетах 30 когда я бросил активная программирование 10 лет назад и до сих пор применяется достаточно слабо что мы говорим про синхронности и пакетной обработки это значит что у нас есть некий клиент и к нему пришло там тысячи запросов доната чтобы что то сделать а если это микро серой стоит одна и та же функция ему пришло тысячи функции с разными просто параметров и вместо того чтобы делить тысячу раз сделать до из он берет эти 1000 запросов агрегирует в один и отправляет их куда-нибудь либо для этих тысяч запросов по чем загрузить какие-то данные для всех для всей 1000 или всех их куда-то записать или их как-то всех центрально обработать пакетная обработка синхронность она невероятно увеличивает скорость обработки данных кто то может сказать о том что если мы начнем делать совсем chrome надо что-то начал синхронно это значит каких-то клиентов мы должны задержать до чтобы у нас накопился вот этот матч в тысячу запросов да однако если мы смотрим на то оборудование которое есть да вот эти вот 200 гигабайт секунду вот эти тысячи order to the время который мы будем накапливать нашим приложением эти тысячи запросов во-первых мы перейдем от тридцати двух ядер в четыре или в два да и на них очень быстро такой все тысячи запросов и это 1000 запросов накопится 50 миллисекунд за 20 миллисекунд за десять миллисекунд для клиентского приложения bathing он будет не видим а как отправить по сети мы всегда можем потому что у нас фактически сеть в топология если она 1 уровня на безлимитное вы не опасаетесь за то что вам не хватит сети все эти данные которые вы накопили запись от миллисекунд и всегда сможете очень быстро отправить естественно никакого http протокола и всего вот этого вот почему потому что все эти протоколы и тисе пи пи и еще тебе это все протоколы эры где у нас были милисекундные задержки вот 10 миллисекунд 20 миллисекунд это вот это вот время их ищите при к сожалению это время прошло сейчас задержки у нас микро секундные и стоит скоростью которой развивается ssd они скоро будет на на секунды задержки например в милан огс коммутаторах которая показала там реально задержка 83 на на секунды при переключении пакетов внутри внутри самого свеча это очень быстро это очень быстро то есть концепция о том что вы теперь можете подождать 20 миллисекунд или там 50 миллисекунд получить информацию с базы данных это очень медленно этот в тысячу раз медленнее чем вы можете на самом деле это сделать сейчас в тысячу раз медленней поэтому нам надо переходить на низкоскоростные протоколы естественно нужны фреймворке контент и контроля бизнес и технических показателей ты что имеется ввиду мы должны контролировать количество запросов в секунду на этих микро сервисах сколько мы памяти потребляем циpкa и тому подобное чтобы у нас не было наших любимых out of memory капюшона вот это вот всего да ну это было 15 лет назад и как-то с этим уже пора что-то делать до чтобы мы буду хватит уже естественно полный мониторинг я бы сказал бы даже что мониторинг вот есть у нас был test driven development микро сервиса это по сути мониторинг времени development то есть если вы написали какую-то фичу оно не показывается интересно и понятно на мониторинге которые ее не видно на мониторинге нет нет не так как надо писать потому что когда у вас есть десятки сотни сканируемых микро сервисов да и все вот это вот копаться в логах даже агрегированных но это так себе затея честно говоря ну прям совсем так себя я не большой любитель копаться в логах и я получу предпочту что у меня для функция был какой-то график где понятно она работает или нет и только если она не работает у меня будет отдельно конкретно где-то error она которой я смогу посмотреть и все ну вот не люблю гигабайта логов глеб и вот это все это вообще не мое я из таких вот глупых гламурных java developer в которой всего это хочет избежать усилий при масштабировании мы поговорили до что это есть прах более-менее простые варианты в зависимости от загрузки цпу или сети и сложный вариант автомасштабирования в зависимости от бизнес-процесс соответственно про сохранения данных тоже поговорили что нам надо развязать запись данных в память я отправка их в базу данных и это должны быть две разные операции на двух разных слоях про асинхронность пакетную обработку мы тоже проговорили что кстати что вот во что сказать про про пакетную обработку асинхронность когда у вас есть асинхронные каши да и вот это все скорость опираться на этих кашах ограничено количеством блоков которые вы можете сделать в распределенной системе а количество блоков распределенной системе которые вы можете сделать для чего нужны локи но допустим нам нужно записать в массив да нам надо записать в лист распределенной данном на записать какой-то целью и лет поставить блок для того чтобы записать локи у нас работают не быстрее чем задержки в сети плюс накладные расходы конкретных сервисов до что означает это означает что если птенцы у нас всех наших операций одна миллисекунда да вот но на все нашим кластеры то ему можно делать всего тысячи операций в секунду для и когда мы говорим про пакетную обработку это значит следующее что если мы вот по одному объекту добавляем то мы добавили их тысячу раз а если мы добавляем папочки в 100 объектов то мы добавили за те же самые тысячи операций в 100 тысяч объектов да это принципиально другая друг друга я скорость работы распределенных кашей то есть там делается все патчами и тогда мы добиваемся нормальная скорость и это я при этом говорю не по не говорю про хай-лоу да вот в хай лоу день свои впечатления говорю просто про обычный enterprise вещи которые существуют в джаве все плюс плюсик sharp этом нет никакой магии и тунберга ред про задержку одна миллисекунда как только мы переходим на современные задержки из писи и в 1-10 микросекунд у нас появляется возможность сделать сто тысяч операций на распределенном каши и до миллиона а если мы это блокируем то у нас натуральным образом 100 миллионов объектов может пробегать через дистрибьютор лист просто просто вот с минимальными ухищрениями да без всяких дополнительных дел то есть правило простое одна операция должна максимум работа должна быть все операции должны быть максимально патчей и работать с десятками и сотнями объектов низко латентные высокоскоростные протоколы и поговорили они есть на рынке их надо смотреть фреймворке контроля бизнес технические показатели поговорили мы здесь боремся с перегрузками аутов на рецепшен ограничение количества запросов которые могут выдержать наши нормальные микро стресс и v2 там четыре ядра до естественно мы организуем также проверку связанности с другими микро сервисами появились давно достаточно так так называемый flow интерфейсе от которые позволяют на цепочке микро сервисов которые друг 1 друг от друга зависеть чтобы тот microserver с которой вы зовете говорил не-не-не я больше не могу я вот не могу тысячу объектов я могу только 590 это это стандартные интерфейсы скажем джаве они так и называются по моему java java java util flow например который позволяет вам балансировать всю эту систему которая случай из писи будет очень высоко нагружена и поэтому естественно такие вещи как тротлинг вам тоже нужен полный мониторинг платформ это естественно мониторов индивидуальных инстансов мы говорим о мониторинге естественно внутренних параметров которые всем известны как там горбач коллектора его скорость циpкa юзать мониторинг бизнес-показателей и тому подобное но когда мы говорим о монитор до следующего класса мы говорим о новых мониторингах которые нужны это мониторинг типа тип inspection то есть мы должны мониторить наши дистрибьюторы листы с этими п.и. то есть низкоуровневые объекты который является дистрибьютор либо глобальными да для того чтобы наконец-то понимать что происходит в системе сейчас это делать не так сложно у нас очень много фреймворков которые все это делают и наконец что же мы получим в результате моя команда взяла ту платформа которая нас было под рукой а именно платформа горит менеджмента то бишь это платформа которая в себя принимает задачи на посчитать и разбрасывает их на 5 10 тысяч ягер который собственно делают риск вычисления и мы переписали эту платформу в стиле htc и запустили тест на 40 на 40 4 часа мы смогли обработать 60 миллиардов task-ов о чем здесь идет речь это не это не 60 миллиардов запросов типа get какой-нибудь простейший параметр бретон окей нет это 60 миллиардов полноценно про крученых task-ов на вычисление риск процесса внутри а внутри наш платформы естественно там никто не тратил время на и реально вычисление рисков да потому что у нас была задача по 3 ст платформу вот но это все прекрасно работала с таким образом что центральная часть когда кстати надо поговорить о том собственно во что это трансформировалось это платформа стала состоять из шести микро сервисов каждый микро сервис в пике имел порядка 20 инстансов по 4 ядра 6 на 20 на 4 где-то так и порядка 400 до вот и вот эта штука и выдерживала запросы с 20000 циpкa который собственно приходили туда за задачами на посчитать и вот как мы видим по этому прекрасному графику который нигде не ломается это означает что эти 60 миллиардов задач прокрутились на нашем оборудовании до 44 часа и ничего при этом не сломалась не было никаких overflow и все было прекрасно вот собственно к чему приводят да и если мы собственно переведем это в цифры до аж такое 60 миллиардов запрос это порядка это чуть больше для даже не знают и больше в 10 тысяч раз чем-то сколько потребляют столько сколько нужно по производительности нашей организации больше чем 10000 раз то есть запас производительности и того что у нас ничего не сломается на простейших современным простейших микро сервисов здесь гигантские да то есть мы избавились от того что у нас что то сломается потому что нас не хватает performance на свои его хватает с большим запасом это не уверен что это это вот это кстати и мониторинг который мы прикрутили когда на эту это у нас из кто знаешь такой гнать супер значит выгнать какой-то очень плохой внутреннему ребенку мы прикрутили мы собственно этого password поэтому мы прикрутили туда мониторинг глобальных объектов игнайта с точки зрения сколько там выполняется операция на дистрибьютор объектах и сколько сколько стоит каждая операция например сколько стоит операция путь в миллисекундах сколько стоит ремувалы сколько там происходит синхронизация между нодами да и тому подобное здесь будет слайд здесь слайда не будет потому что реальных данных банка лучше не говорить и поэтому переходим к вопросам ответ можно звук о спасибо дмитрий спасибо за твой доклад он очень-очень интересный и мы переходим соси вопросов по сути во втором than the microphone спасибо большое за доклад и очень у меня больше вопрос по калькуляции костов вы описывали но вот эти системы стойки с текущим энтерпрайза то есть с холода все хорошо а что будет с х его любили ти ну то есть грубо говоря у нас одна стойка там заменяет сотни серверов тысячи серверов да но стойкий выходят из строя и из лютеру саму плей до для понять давай медведь и нахваливали бельке да значит для вот этих вот этих стоек эта вещь просто если здесь у вас работают контейнера да и у вас таких стоек ну возьмем если мы говорим о современному the prize допустим мы говорим скажем миллион лидер да то есть вы все оптимизировали с 10 или там 20 миллионов я беру поля в один и нам надо миллион лидер то есть это сколько тут 100 стоит до 100 стоек по 12 хедер на них развёрнут кубер найти и внутри этих стойка соответственно вам появляется хайло вы любили тебя с точки зрения того что вы ваши как и залог ты господи ваши контейнеры до переезжают с одной строки на другую при этом надо понимать очень интересную вещь это стойка целиком ломаться не может до и тут очень важно понимать почему кто понимает почему они не могут ломаться целиком руке она очень мало людей значит 10 или 20 лет назад была изобретена такое понятие как пассивная шина до чтобы пассивно яшина это просто печатная плата на которой разведены проводники лама ломаться в ней ничему так вот все вот эти стойки да они сделаны так что есть вот такая вот штука куда с одной стороны воткнуты блады а с другой стороны воткнутые сетевые коммутаторы поэтому что-то из этого может ломаться сетевые коммутаторы блэйду и так далее но вот эти центральные так называемые пассивные шины они не ломаются что их blade центрах что и здесь что их кстати в коммутаторах которые по нормальному построены где есть пассивная связь всех портов там тоже вот это ничего не ломается вот поэтому случае отказа обладает а сама все это значит н вами стойки там тоже есть полная халява liability то есть вас каждый юнит там в нем какое-то количество терабайт данных но поверх этого всего существует какая-то система файловой типы люстра я до которая полностью организует халы вы любили те dear между вот этими всеми индивидуальными устройствами хранения ну а с точки зрения мило докторских коммутаторов тут тоже все вполне понятно да каждая вот это вот каждый вот этот blade это собственно такой же независимый сетевую blade который тоже встроив пассивную mellanox узкую шину на 320 на 160 терабит и практически мило нокс тоже целиком сломаться не может вот просто потому что сделал точно так же с одной стороны сущее сделан из основные свечи с другой стороны вот он the spine свечи который обеспечит коммутацию и все и поэтому надежность современному оборудование построена на у таких вот модульных конструкций с пассивной шины они целиком не ломаются и поэтому халява добились с точки зрения переживания все это с одного сервера на другой она уже достаточно понятно как делать смотря на производство например рейдов да ни разу никогда и ни разу не выходила пассивно шиной сестра что ни разу не видели ни разу и не выходила из строя пассивная шиной которая связывает байды с дуэйном был и дыбы и да естественно вылетали это их нормальная система нет я именно про выход стойки целиком и снова вот стойка целиком и даже стойка blade центра до из строя не выходил некогда я вам скажу даже так количество запасных вот этих вот центральных шин blade центра на любых складах или от паккарда их равно нулю их просто нет вот они настолько никому не нужны потому спасибо нашим просто life"
}