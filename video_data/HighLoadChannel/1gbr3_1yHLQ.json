{
  "video_id": "1gbr3_1yHLQ",
  "channel": "HighLoadChannel",
  "title": "Надежная раздача видео минимальными средствами / Евгений Потапов (Сумма АйТи)",
  "views": 164,
  "duration": 1582,
  "published": "2017-04-22T14:46:45-07:00",
  "text": "Всем привет Меня зовут Евгений Потапов И я расскажу о том как мы делали видеосервис для компании камтв Ну если кто не знает то это ролики 10000 и B comed и подобные ролики а необходимый дисклеймер перед тем как я начну доклад А мы занимаемся технической поддержкой серверов и видео передачи Для нас это сайт проекту а то о чём я расскажу как мы сделали этот сервис в принципе это можно использовать не только для раздачи видео но и там для раздачи какой-то статики раздачи файлов и так далее А когда я буду говорить о ЦД Я не говорю о том что мы таргетинге офи к пользователю мы просто а делаем Content Delivery Network то есть мы доставляем контент через сеть и речь не идёт о видео стриминге мы раздаём файлы которые заранее транскодирование МТВ А в чём смысл у МБА есть видео на Ютюбе которое они монетизируется и видео на сайте которое также монетизируется рекламой до прошлого года Они использовали сторонний цдн Однако их не совсем устраивало несколько несколько моментов которые они хотели бы исправить во-первых цдн был полностью Техническая сторона цдн был полностью ими неконтролируемой и они хотели бы понимать Насколько надёжно Они передают видео то есть понятно что стопроцентной надёжности добиться нельзя но можно понимать Насколько надёжно мы её транслируем Они хотели бы иметь возможность кастомизировать видеоформат кастомизировать плеер и примерно понимать как у них сейчас идут расходы на видеопередатчик Нея А это довольно интересная часть истории которая ну несколько поучительная то есть изначально мы думали что транскодирование в работе Будет очень много а мы ожидали что будет большая загрузка файлов будет Параллельная загрузка соответственно у нас есть какой-то сервер транскон который постоянно работает а мы будем отслеживать количество заданий на транскон и Исходя из этого количества заданий мы будем подключать инстанс в Amazon Web Services а то есть мы смотрим Что у нас ушло там четыре задания на транскодирование транскодер Пока ещё справляется приходит новое задание и мы из ходя из оценки времени понимаем что пора добавлять инстанции а сервер очередей там получает сообщение от транскодер о том что это будет занимать ещё столько-то времени Он передаёт в клиент к амазону о том сообщение о том что пора бы задеплоить несколько новых инстан сов которые будут использованы для транскодирование инстансы деплоя и на них также отдельно даются файлы мы это делали Ну довольно какое-то продолжительное время не фатально продолжительно там несколько недель а в итоге Оказалось что в течение дня процеси Пять файлов эти пять файлов поце не параллельно и по сути до сих пор несмотря на всю эту систему работает фактически Работал бы просток я хотел бы сказать о том что в тот момент когда вы там планируете архитектуру иногда стоит всё-таки даже там начинать с каких-то минимальных решений даже если вам кажется что будет много работы по по нагрузке вторым вопросом после того как дивали файлы вопрос том как мы будем раскидывать файлы по серверам в настоящий момент это реализовано через LSN у нас есть три дата-центра а по которым файлы распространяются файл сначала кидается на один из серверов в одном из дата центров которые объединены в кольцо Эл синка а сами дата-центры объединены также по одному из серверов л синком между собой То есть файл например летит там в в Амстердам 2 уходит в Amsterdam 3 Amsterdam 1 уходит Затем в DC и затем уходит в в ЧМ довольно простое решение выбирали исходя из простоты и это самое как бы это главный большой плюс есть просто берём настраиваем довольно простая задача для админов после чего файлы распространяем самая большая проблема две большие проблемы одна из которых с которой мы сталкиваемся постоянно вторая с которой мы скоро столкнемся простет у падает что файл не будет распространён по всем серверам а то что файл заливается последовательно приводит к тому что с ростом количество серверов в сети файл начинает распространяться всё дольше и дольше А подумав об этом мы решили что Ну раз мы раздаём видео то наверное можно попробовать тон а то есть в качестве трекера мы пробовали он трекер в качестве клиента пробовали тмин мы раскидали Рамин по серверам в а где лежат собственно статика и файлы а трекер пробовали на отдельной машине где лежит управлял транскодер изначально мы думали то не сильно много времени тратить и попробовать использовать просто клиенты с которые будут обнаруживать друг друга там в пределах сети а мы раскидывал по этим клиентам А через SCP по идее когда клиенты открывают этот файл обнаруживают друг друга пос они находится в пределах одной локалки в пределах центра то дальше они начинают качать друг с друга но по сути оказалось так что закачка была фактически секундочку Да закачка была нестабильно то есть мы ожидали того что файлы раскиданы по всем машинам чно закачивается друг с друга но вот если мы посмотрим на график то мы увидим что сначала файл залился с мастера на первую машину а затем с первой машины он полился на вторую машину То есть мы по сути получили Ту же самую последовательную цепочку А с трекером получилось всё именно так как мы и хотели то есть с трекером файлы после после раскидывал раздачу что важно мы пока ещё не включили этот механизм поскольку по скорости раздачи а оно не сильно быстрее чем В текущей архитектуре но главным преимуществом будет то что с ростом мы сможем использовать докупка инстан сов например в амазоне то есть мы ожидаем что выйдет какое-то очередное шоу и к выходу этого шоу где там трафик достигает там порядка сейчас 14 Гигабит А мы можем докупить на несколько часов инстанс в амазоне которые с подключённым тон клиентом получат только этот просто файл и прицепи вшись к трекеру смогут загрузить с других узлов ЦД на нужный контент А теперь поговорим раздачи основной вопрос был в том что сильно много тратиться на трафик не хотелось А хотелось получить высокую пропускную способность площадки хотелось получить большое количество включённого трафика за который там не сильно надо много платить хотелось получить надёжную Ну не надёжную а просто хоть какую-то обзо устойчивость чтобы если там придёт жалоба хостер выдрал провод не сразу и При всём при этом получить техническую устойчивость решения А вот как я сказал То есть если смотреть по техническим требованиям площадки мы отбирали площадки по тому чтобы на ней должна быть высокая пропуска способность в том что мы могли бы получить там не один общий канал на все сервера а получить например канал к каждому серверу мы ориентировались на выделенные сервера а получить дешёвый трафик и получить небольшой Пинг до России потому что люди хотят смотреть всё-таки видео А по каждому из хостингов которых мы выбирали мы смотрели чтобы большой проблемой было то что большинство дешёвых хостингов не разбираются с азами а то есть мы в итоге остановились выбор на софт как на надёжном хостинге на 100б как реле Соф который продаёт трафик очень дёшево но при этом за озы просто выдирает провод практически сразу же и зарезервировать ещё одним хостингом который К сожалению там по условиям договора не могу называть А который на которой просто паралели также и используется он только для того чтобы если 100 ТБ умрёт мы могли работать и на другом хостинге а большая проблема с дешёвыми хостинга которые продают трафик за дёшево много и на большой скорости то что чаще всего там есть небольшая техническая поддержка которая там ответит в течение суток или двух то есть надо исходить из того что м что какая-то из площадок может вылететь и не работать долгое время то есть вот мы остановившись на трёх дата центрах ожидаем предполагаем что один из центров может вылететь Ну то есть как там там по три машины но три машины могут вылететь А что мы в итоге выбрали А у нас есть софт как площадка для транскодирование 10 гиби пропускной способности при раздаче сервера у есть серверы в софт как он их получил непонятно поскольку софт там берёт за тот же объём трафика в сумму там в 100 раз большую А и у 100 тба есть сервера в Лондоне их собственном дата-центре которые в последнее время показывает Там не сильно большую стабильность но которую мы взяли просто на тот случай если 100 ТБ там контракт у 100 теб флером поломается и ещ один доцент который там находится в Голландии который называть мы не можем потому что стриминг они официально Запрещают Аа Но если поговорить с ними то в принципе можно договориться фактически на самом деле Ну с любым костеро вопрос идёт в основном в с точки зрения переговоров То есть вы получаете какой-то офер от одного хостинга может быть это просто то что написано у них на сайте и идёте к другому хостер у которого там условия чуть хуже но который вам может быть нравится больше с точки зрения технической надёжности вы идёте с этим офферов А что вы можете прела большинство хостингов в таких условиях нормально договариваются о каких-то идут навстречу То есть в принципе если там хостинг говорит о том что стриминг идн запрещены то вы можете сказать Давайте подумаем может быть как-то можно сделать так чтобы можно было это включить Итак поговорим о раздаче дорого фик остальные можно упасть и нам нужно не слома очередь мы думали о балансировке то есть софт железо или НСО если софт если железо то получается что у нас на железо надо минимум три устройства железо стоит дорого ни один из там дешёвых хостингов не будет вам предлагать железные балансе в там за за какие-то небольшие деньги и авария на одном устройстве равняется аварии на ДЦ и мы упирается по трафику оставшийся д центра Большой риск А если балансировка софт всё та же самая ситуация то есть а мало того порт на сервере с балансировщика должен равняться сумме всех портов кэндо то есть там условно говоря если у нас есть три сервера по 2 гиби то порт на сервере с балансе должен быть там гигабитный на этот кастом тоже уже довольно сложны Т дешёвые хостинги и опять же Single Point of Fail в итоге мы остановились на DNS робине с Ro 53 и с дми солера что здесь хорошо у нас при надёжном хостинге не мы ставим низки ktl и получаем относительно лёгкое изъятие упавших серверов то есть Понятное дело что при падении сервера и при смене там при убирания из балансировки Изра roby на а сервера из DNS там остаётся порядка в России 5% а людей которых провайдеры закачивают DNS записи на большее время на сутки обычно но там 5% Мы готовы пренебречь на фоне того что остальные люди будут а получать видео аа а что оказалось интересно именно в специфики видео раздачи А поскольку файлы занимают долгое время поскольку люди смотрят видео изъятие аварийного сервера - Это довольно быстрая процедура А вот изъятие сервера который работает для тенса довольно долго то есть вот мы там в начале графика убираем сервер из балансировки а люди продолжают смотреть ролик который длится ещё 20 минут и вот всё это время в принципе у нас выбор либо обрубить сервер и и оборвать пользователем соединения либо дожидаться пока всё это закончится при этом нужно понимать что вот это вот верхний график - это порядка там полутора Гигабит трафика соответственно даже когда она там низко падает это там ещё сотни мегабит которых люди там получают и до сих пор там остаётся вопрос рвать им соединение или нет то есть до полного ухода трафика у нас ушло получается порядка 2 часов просто ожидания а что мы хотим в будущем сделать чтобы это избежать Мы хотим сбалансировать на уровне плеера то есть плеер знает там получает каким-то образом через API число серверов которые находятся в балансировке и соответственно Дальше он там регулярно получает какие-то апдейты куда он может обращаться Куда Нет а по поддержке там Понятное дело что надо мониторить сервера а-а что из не типичного для простого хостинга э мониторинга у нас появилось А у нас много трафика причём дешёвые хостинги которые продают много трафика очень сильно чарт за превышение этого трафика то есть вот есть stbc он там берёт 300 долларов в месяц за сервер со 100 терабайта Но если вы превысить эти 100 траб то о начнёт вам Чар гигабайт по-моему порядка пол цента за гигабайт То есть вы там в течение короткого времени попадёте на деньги там в три раза больше чем Вы заплатили за месяц А естественно что при этом хостинге не сильно хотят предоставлять информацию там Алтай потреблении трафика потому что у этих дискаунтеров у них в принципе отдельная часть денег зарабатывается на превышении а Поэтому приходится а приходится там для одного хостинга мы парсим прямо веб панель как она выглядит и там добавляем её в мониторинг делаем алерты и при превышении там определённого порога начинаем перебан сирова сервера сами руками а вторая большая проблема - это мониторинг последней мили то есть на графиках всё выглядит хорошо трафик идт люди приходят в комментарии жалуются люди соответственно не самые не самые френдли по отношению к проекту Ну то есть там у меня ничего не работает у меня ничего не показывает Сделайте что-нибудь это комментарии приходят люди расстраиваются там там это происходит Ну потеря потеря Как сказать привлекательности бренда что Тим сейчас сделать это пока у нас в туду Мы хотим на плеере отслеживать там тайминг проигрывания по отношению к таймингу на компьютере и соответственно высылать эту информацию в мониторинг и отслеживать там по регионом насколько там где и как работают узлы ЦД потому что вот реально то есть в ТБ в Лондоне а для нашего мониторинга для всех там наших шести точек Откуда мы проверяем показывают хорошую работу в Москве при этом для нескольких больших провайдеров начинаются серьёзные тормоза там у людей лаги Аа вот пожалуй всё что я хотел рассказать вот а Жень значит смотри вопросы такие во-первых Я может пропустил А какие у вас характерные потоки с сервера одного сколько у вас Гигабит примерно А в настоящий момент порядка полутора Гигабит с одного сервера мм почему вам просто не обратиться к тем хостера которые э ну как-то короче это не очень много То есть это примерно Почему бы не обратиться к тем хостера которые конкретно вот с этими вещами занимаются именно вот вам каналы продают потому что продавать э гонять видео при расчётах за терабайт это очень очень бывает кучеряво потом Ну Ну то есть смотри получается стоимость 100 ТБ в 100 ТБ - это порядка 200 долларов Ага вот любой цдн русский берёт порядка рубля за это понятно Вопросов нет вот как бы соответственно плюс хочется контролировать плеер плюс есть какие-то форматы которые нужно добавлять тог как раз не про сины говорю я говорю именно про Хосте которые Ну вот сейчас например там у меня огромное количество клиентов которые массово скупают там там много гигабитные каналы в Голландии и в Германии но не конечно не в хере А в других хостингах Ну в общем я про это ещё буду рассказывать вторая штука - это вот насчёт насчёт выключение машин ДНС вообще говоря конечно это нерабочая штука то есть это ну а как вот как как балансировать значит самый простой вариант который работает на сайте который у тебя собственно говоря каталогом видео тебя дол просто хранится информация о том на каких машинах на каких хордах на этих машинах лежат файлы и просто при открытии у тебя человек ну отдаётся то есть у тебя человек нажимает на кнопку посмотреть видео в этот момент вычисляется Как раз на сервера где оно сейчас лежит и что сейчас работает самый простой вариант есть проблема Я не уверен насколько она вот решается в этом плане вопрос к разработчиком скорее очень много видео идёт с мбда ВКонтакте и насколько я понимаю там может быть за Каширова с то есть вконтактов ское видео там прилично трафика которая транслируется с этих же машин Ну понятно В смысле что вы сейчас вставляете МБЕ ВКонтакте уже с урла на и при этом у вас у вас не стриминг А вас псевдо стриминг Ну ну то есть кото продат Да ну просто скажем это может решаться либо редиректа либо переходом на стриминг Ну да да варин плеер который там Александр вы упомянули Amazon вот через что вы с ним работаете А у амазона есть API который можно создать ла улиты да да да ну фактически там есть PHP API есть там Ruby API есть Java API и там мы довольно долгое время с ними работали плотно А и В основном мы используем именно Для таких случаев когда там приходит много трафика и надо от масштабироваться то есть смысл В чём очень много трафика это не постоянный поток это не телеканал очень много трафика приходит на выход каждого шоу то есть выходит шоу люди начинают там лайкать это во ВКонтакте в Фейсбуке и в течение короткого времени там приходит порядка там час 12-15 Гигабит трафика наверное в остальное время этого трафика Может быть там в 10 раз меньше соответственно там время от времени возникает вопрос а не могли бы мы например там сэкономить на о том чтобы на эти короткие промежутки они очень короткие А докупать машины то есть может быть будет выгоднее держать в амазоне там и платить дорого за трафик но там не платить за то количество машин которые сейчас существует чтобы просто его выдержать Потому что если у нас приходит пик на выход шоу и Карамба не вывозит это шоу то там Люди перестают лайкать перестают шерить и всё и трафик теряется А вот хорошо вот по поводу трафика вот как вы его контролируете пото треть в амазоне это не про Карам если говорить мы регулярно для своих клиентов смотрим по тому какая производительность на инстанса на амазоне там год назад У меня было выступление как раз про клады а очень свой балансировщик нет нет нет то есть по сути вот машины которые деплоя в зоне которые мы хотим деплоить это будут очередные там узлы этого ЦД которые там добавим в DNS Robin там или в другой балансировщик там который будет а другая Проблема в том что Amazon не говорит Какая пропуская способность на инстанса причём instance m1x large Может быть медленнее быстрее чем instance M1 xlarge 2 там или как он называется они могут меняться со временем они могут меняться с производительностью аккаунта то есть там например на новый аккаунт может быть скорость медленнее чем на аккаунт который прожил уже какое-то время на аккаунт который упирался в трафик скорость может быть рано или поздно быстрее чем на аккаунт который никогда не упирался в трафик и таких вещей очень много то есть буквально вот там недавно клиентов мылись то что на m1x lge скорость была на всех инстанса 150 мегабит А у другого точно такого же клиента Там же в Ирландии скорость на этих же самых конфигурациях там порядка 250 Почему никто не знает как бы ну и это вопрос экспериментов то есть там мы делали график там может быть что M1 медиум быстрее чем m1x large m1x быстрее чем mge 2 сейчас они стали делать таблицу где всё-таки есть и Дале но какая там реальна скорость не знает никто дахо проблема понят решението есть тестировать то есть ну слава Богу можно взять нагрузить Нетворк посмотреть какая там скорость если там почему-то Скорость ниже того что мы считаем что на нашем аккаунте в амазоне может быть мы гроха этот и делаем новый ть какую-то гео привязку пользователе А пока нет пока в принципе люди там довольны Мы хотим рано или поздно когда-то это сделать но по сути до Голландии там до отовсюду одинаково Да я понял а какая-то у вас есть проблема с людьми живущими за Уралом Ну мы с Иркутска сами Ну в смысле что й то есть ну люди довольны Да довольны то есть вы вам не приходится делать там до всяких городов типа там типа какого-нибудь Томска делать какие-то местные местные репитеры то есть такого нет проблемы Здесь даже если будет желание Я думаю что мы упрётся в цену трафика как раз вопрос о том чтобы его не платить то есть чтобы до одного места довести а там дальше уже Дешевле нет такого пока такого пока нет ну тумач тумач пока Угу то есть короче свой cdn вам не приходится дите Ну да Понятно может быть интересно будет когда-нибудь это в Москве Но трафик Я очень извиняюсь ещё маленький вопрос А Вы сказали что вы файлы раздаст никакого нет не а шоу выходит а файл конверти файл кладётся на genx файл раздаётся плеер качает А вот файл какого формата то есть MP4 или что вы использу MP4 MP4 да то есть это в основном html5 плееры соответственно а фш тоже фш флэш не очень хорошо насколько я знаю Ну да ладно вот то есть а вы с hls не экспериментировали вообще с аским форматом нет Понятно то есть на самом деле вся суть проекта была в том что ребята хотели начали попробовать получится или нет соответственно получится или нет при этом не заплатить деньги Ну не заплатить много денег а И после этого уже решить что делать дальше соответственно там в качестве узла cdn получился просто ngx сервер с файлами которые на нём лежат А по трафику у вас как-нибудь плеер может переключаться то есть ну от канала пользователя то есть конкретно а сейчас нет сейчас Нет есть битрейт а которые там люди могут сами выбирать Аа может быть мы на это Будем ориентироваться но пока просто зависит от того сколько людей жалуется Ну то есть люди в основном жалуются всё-таки на совсем тормозящий канал Аа до того место где тормозит видео где лежит видео если нам плохо то тогда мы этим сервера убираем и больше люди в принципе не жалуются на то чтобы ну то есть это в любом случае а было работать быстрее чем то что работало до этого а и это в принципе сравнимо с Ну то есть с тем к чему человек привык То есть он нажимает Play и смотрит видео А вот такие устройства которые не поддерживают MP4 Ну то есть Firefox например который vbm То есть вы о них не заботитесь просто нет ой"
}