{
  "video_id": "nrJT8IeHRok",
  "channel": "HighLoadChannel",
  "title": "Face Recognition: From Scratch To Hatch / Эдуард Тянтов (Mail.ru Group)",
  "views": 1663,
  "duration": 3091,
  "published": "2018-08-16T04:38:54-07:00",
  "text": "добрый день коллеги меня зовут эдуард центов и сегодня я вам расскажу про то как мы в mail.ru group так сейчас я порежу запущу как мы в mail.ru group запустили высек огни шин с какими проблемами столкнулись на какие грабли наступили ну во-первых зачем нам вообще в если конечно мы первой задачи который стоял перед нами это внедрить весь reccagni шин в облако mail.ru то есть пользователи заливают в облака свои файлы в том числе фотографии и наш алгоритм должен автоматически за детектив лица распознать их и рассортировать фото по людям соответственно пользователь который смотрят фотографию например своим другом ему продукт наш показывает и еще другие фотографии с этим же человеком он может и посмотреть ее там залипнуть исследуя свой архив как это сделал я когда мы запилили эту фичу вот этот продукт мы запустили вы можете скачать мобильное приложение наша и посмотреть как она работает вот так же у нас в компании много соц сетей и потенциально мы можем внедрив если к внешним туда например чтобы метить фотографии тега друзей на фотографиях поднимите руки кто понимает как устроены свёрточная сеть так ну и каком на на минимальном хотя было ну я тогда я коротко очень быстро расскажу буквально один слайд значит какие признаки выучивают сеть на примере фэсс reccagni шин то есть свёртка графически кодирует некий признак например там наклонная линия градиенты какие-то на картинке и вот мы видим что с сети во время обучения поддаются картинки с лицами и первые слои выучивают такие базовые элементарные признаки как вот наклонные линии градиент вот такой самой примите самые примитива из этого формируется так называемый опцион активационной карты фичи maps и они подаются на вход следующим слоем сети воду в данном случае второе уже на базе примитивов выделяет части лиц и 3 слоя уже на основе частей лица выделяет уже целые модели людей и все это обучается and went то есть мы на вход сети подаем лица на выход у нас какие-то метки например как лад классы как ему какие люди там на фотографии и мы ничего не задаем больше только архитектуру сети эти все признаки выучиваются сами вот чтобы что-то распознавать лица нам надо сначала научиться детектив то есть у нас задача разбивай разбивается на 2 на 4 задача достаточно сложная как на этой картинке надо найти всех китайцев достаточно сложная задача потому что их много вот и они под разными углами разными разрешениями ее там с разным освещением вот дополнительную задачу которую мы решали это нахождение так называемых landmark авт листочек лиц я объясню на примере бритни то есть мы берем и фотографию выделяем у нее глаза уголки губ и нос получаем 5 точек и теперь мы знаем как можно повернуть и выровнять фотографию таким образом чтобы лицо было анфас смотрела в камеру и таким образом мы в будущем облегчим задачу после как низшему потому что таким изображением и уже легче работать обучаться мы будем на нескольких популярных дата сетах это veider и силе бы 1 состоящий из 30 тысяч картинок сфокусирована в большей своей степени на большом количестве людей на одном изображении а второй там гораздо больше картины 200000 там люди под разными углами освещения my и так далее нам там больше всего интересно именно лечил инмарко в которой нам пригодятся в будущем тестироваться мы будем на fttb это исторически на нем все сравниваются небольшой datasette но он достаточно сложен и и тоже там на фотографиях много лиц вот достаточно сложно datasheet значит существуют разные подходы к детекции лиц и их разбиваю на как бы старую школу и новую школу то есть старые классические алгоритмы и новые использованию с использованием съемочных сетей вот один из таких алгоритмов виолы джонс называется он работает на основе признаков которые вы видите слева на слайде такие квадратики прямоугольники черно-белые и физический признак это разница между суммой белых пикселей и суммы черных пикселей вот почему это для полезно для выделения лиц потому что какие-то области на лице они могут быть светлее или темнее соседних то есть например глаза темнее чем область под глазами и там нос светлее чем область рядом вот наш алгоритм учиться надо то сети мы берем datasette берем фотографию и для каждого патча то есть для каждого кусочка на этом изображении вычисляем все возможные такие признаки их получается 160000 что очень много и такой считать для каждого патча я слишком накладно ни в какие лимиты мы не уложимся поэтому мы даем возможность алгоритма да будет выбрать наиболее значимые признаки и уже вот эти шесть тысяч которые получились на основе них мы будем принимать взвешенное решение лицо это или нет вот но 6000 признаков это все равно много потому что надо вычислять на каждым патчем вот и поэтому авторы алгоритмы предложили поделиться эти признаки на так называемый 100g и чтобы распознать лицо как каждый stretch должен сказать что это лицо таким образом пачек где нет лица не отвалятся на первом же стоит же и мы сэкономим огромное количество вычислений есть отличная имплементация этого алгоритма на open cv она работают примерно 100 миллисекунд насыпью в зависимости от разных там конфигурации самого сепию но она недостаточно точно на этом тесте в дтп пристав используя алгоритм имеет 45 процентов приколы вот что достаточно низко то есть мы будем из всех лиц на изображении детектив только 45 процентов вот это мало поэтому нам нужны какие-то другие подходы перейдем водка swear точным этот так называемый регион поездка на вершину networks это целое семейство алгоритмов их много разных вариаций модификации вот я расскажу на примере одного из них фас террорист н.н. в чем основная идея значит мы берем картинку целиком даю некоторую сеть это может быть в г гарри знает любая популярная архитектура и на выходе мы получаем активационной карты фичи maps и свечами по нам описывают кары изогнул признаками изначальную картинку и этих фич мая по мы подаем в следующую сеть рпн которая нам уже накидывает где по ее мнению могут быть почесать лицом вот их достаточно много и мы в целях оптимизации вырезаем ее в изначальном фичами p области ну вот этих пропал залов и отдаем по одному на классификацию в финальный классификатор который говорит есть нам лицо или нет вообще диета кст снесли сравнивается классическими алгоритмами то как я уже говорил виллу джонс 45 процентов приколы имеет еще алгоритму фок это гистограмма направленных градиентов тоже один из классических алгоритмов умный ist die liebe вот он дает 70 процентов прикола я про него не рассказывал вот и тот алгоритм который мы посчитали самый быстрый из семейства регион bass трлн он дает уже девяносто два процента приколов что гораздо выше но закрыты естественно не бесплатно за это мы платим вычислениями теперь у нас inference сети занимает 40 миллисекунд на ужин и джованни насыпью как было в предыдущих алгоритмов хотим мы меньше 10 миллисекунд на джипе и потому что у нас сеть физ-ре корнишон работает выглядит в этом районе если мы позволим сити фэйс detection работать там аж 40 миллисекунд это будет огромный бутонах нашей системе поэтому наша цель 10 миллисекунд вот но режим bass ну вот ир сен который говорил о работает 40 миллисекунд соответственно нам нужны какие-то альтернативы вот один из интересных подходов это mt седан это каскад из нескольких сетей каждая сеть делает предсказание и передает следующие для уточнения предсказания вот наш мы берем картинку с келли мы ее в разные разрешения и придаем 1 сети который ищет лица фиксированного размера 12 на 12 и делает очень грубые предсказанием вот свои про пазлы на передать следующие сети которая на более высоком разрешении уточняет эти регионы и отдает последнее которые уже на там самом большом разрешении 36 на 36 делает финальный prediction где есть лицо и заодно выделяет лан market точки лица если сравнивать rovsen то мы немножко проседаем по точности 90 процентов против 92 но при этом мы выиграем в скорости в 17 миллисекунд против сорока вот огромный минус в этом алгоритме то что невозможно сделать batch processing в лоб программеры потому что мы подаем в первую сеть пирамиду картин на разных разных картинок на разных разрешениях им нам их надо по одной прогнать вот это минус потому что обычно batch processing дает нам ускорение в районе там 15 20 процентов поэтому при прибегли к помощи тензоры рта и это для тех этом видео которое занимается оптимизацией совершенных сетей но оптимизировать их структуру потому что современные фреймворке популярны они дают а поверх это определенно при инфляции вот так же он позволяет запускать сети на половины и четвертины и точности на джипе что кратно ускоряет perfumes вот на примере блоку inception посмотрим что делает именно тендер ртс у нас есть вот клочок издам инволюции бориса и релуи там в разных фирм орках это может быть отдельными прямо слоями и будут лишние операции копирования тендер то это все объединяет делает за одну операцию не делаем лишних копирования также есть горизонтальным например если однотипные операции типа сверток один на один мы их тоже объединяем чтобы оптимизировать с путем матяш матричных умножение наши вычисления ну и убираем лишнее копирование есть r конкатенации которые не нужен можно сразу копировать на на выход как как здесь ну это вот эндера то есть определенные недостатки именно то что например принимает модели в кафе и только недавно начал принимать тендер т у ф взрыв wow и вот мы используем там торжка и торт и для нас огромная боль и конвертировать все эти модели в кафе ведущий сложно то можно потратить кучу времени те кто с этим столкнулся очень хорошо это знает вот второе это то что все фиксирована фиксированную входная картинка фиксирован бача для полна сварочных сетей которые принимают любое разрешение это может быть проблемы и третья тоже ты реализованные базовые layers достаточно много но не все то есть какие то там слои который у вас есть в сети может не реализовывать вот и мы сконцентрируемся на двух последних минусах потому что не читаете сидел у нас принимает пирамиду изображений и на разных скейл их но мы стандарты должны определить фиксированный размер картинки поэтому укладываем все картинки пирамиды в одну такую большую огромную картинку зависимости от разрешения может быть но оно оптимально уложится или нет может остаться какое-то пустое место но это не так важно зато теперь у нас было изначально реализация на питоне который занимал 17 миллисекунд мы прописали на плюсах и делали и сделали этот проход одной картинкой получили 12 миллисекунд дела c7 вот и добавили просто batch processing то есть подаем пачкой изображение получили уже 10 7 миллисекунд что очень близко к тому что мы хотим вот я там еще не применяли тендер rt чтоб примет тендер ты нам нужно избавится от при лилу слоев которые у нас есть в дефолтной сети мтс и нам приучила вместе с рылу слоями и получаем то чтобы во-первых попутном у лучшее качество немного донес 1 и 2 процентов и сократили время inference и до 8 8 миллисекунд то есть мы получили от нзт прирост 20 процентов ну и соответственно мы получили то что хотели нужную точность нужную скорость и можем переходить дальше к физ-ре как низшему под задачах если конечно обычно понимаются 2 это верификация когда мы сравниваем две фотографии нам надо понять один и тот же этот человек или нет и винсе фикации когда мы ищем человека среди могу других фотографий и то и другое может быть достигнуто метрика то есть функцию как который мы хотим выучить она принимает на вход два изображения говорит похожи эти лица или нет это можно выучить на размеченным дата сети с людьми и суть состоит в том что мы учим сверх ночную сеть которая нам в итоге выдает так называемый embedding это вектор в данном случае ста двадцати восьми футов которые описывают нам картинку то есть это какие-то признаки сто двадцать восемь штук которая нам описываю картинку и позволяют их сравнивать между собой и теперь сравним не фотографии виктора и если мы все сделаем правильно то вы бы динка спейси в пространстве будет он будет обладать хорошими свойствами то есть фотография одного человека будут близки и фотографии разных людей будут при этом далеке вот ну и самое важное потому мы учимся на каком-то определенном дата сети с конкретными персонами и нам важно чтоб она нам надо работать на всех в мире людях и поэтому если появляется 5 новых фотографиям должно падать в какую-то другую точку пространства не мешаться с остальными научиться мгу им из цели by это да крупно такой datasette 10 миллионов картинок от microsoft составлены с помощью поисковых движков там где-то топ-10 100000 celebrity вот но он достаточно шумный потому связи с тем как его составляли вот тестируется был на rfw этот классический dt сайт на котором исторически все тестировались но он на самом деле не достаточно сложный потому что современная все алгоритмы имеют порядка 100 процентов в точности примеру сто процентов точности на нем escalade момента сравниваться на нем не имеет смысл поэтому будем сравниваться на челленджи мега фейс которым надо найти нужные лицо среди миллиона других это уже гораздо более сложные задачи если мы посмотрим leader board текущие то увидим что в десятке решения которые имеют от 77 процентов точности до 91 на первом месте компаний российского корт который очень сильно опережает всех остальных основе все остальные участники в основном крутится вокруг 80 процентов и мы будем ориентироваться на эту цифру в качестве нашего дизайн соответственно перейдем к самому метро клинингу как на вы выучите эту функцию метрики лишь можно применить и классе наивный подход просто выучить классификацию на то есть мы по фотографиям предсказуемым bending in после mb dengan и на его основе предсказываем какие именно классы вот и будем надеяться и молиться на то что пространство которое получится будет удовлетворять нашим условиям но на самом деле мы получим вот такую картину так как то алгоритмы классификации the soft макс будет зовут макс кассационный слой на выходе будет заботиться только о том чтобы эти классы разделить вот и при этом будет все равно на то что там около разделяющей поверхности могут быть точки которые достаточно близкие по метрике зато неразделимы поэтому когда у нас будет какая-то другая фотография она запросто может попасть на пересечении этих классов и будет полный мясом и ничего с этим не сможем сделать нам на самом деле нужно помимо разделению разделяем асти классов которые став макси нужно еще и компактность классов в деле чтобы все точки 1 чтобы все до точки одного человека были в одной точке вот есть несколько подходов как это делать один из них триплет loss он не занимается классификации а учат метрику напрямую мы берем фотографию какого-то человека в качестве опорной точки берем другие фотографии как позитивные и лица других людей как негативной минимизируем расстояние до первых максимизируем до вторых и нашло составляем таким образом чтобы позитивные пары были были ближе на константу чем негативные так чтобы у нас был такой зазор между классами длиной в альфа вот этот параметр самая большая проблема в этом алгоритме это то как выбирать вот эти триплеты позитивные и негативные пара потому что и их на самом деле очень много на dt сети и а нам нужны те на которых мы сможем чему-то научиться на чем сеть сейчас ошибается вот и чтобы ускорить сходимость можем заниматься так называемых арт негатив майнингом то есть мы берем опорную точку берем все позитивные фотографии и среди негативных лиц мы берем мы простые которые на текущий момент сеть уже умеет распознавать не рассматриваем ищем только те которые сложены достаточно для сети в текущий момент и на них уже обучаемся вот и нам мы не можем это вычислять на всем дата сети поэтому будем делать это онлайн и огромными мини матчами то есть мы берем там тысячу картинок условные среди них еще вот эти вот нам необходимы и hard негатив пара но можно попасть встрять на проблему если в какой-то момент для сети будет слишком сложно разделить людей если люди между собой очень похожим для сети и будем составлять такие пары что в глазах сити это и позитивные и негативные они очень близки между собой она их не может различить и одновременно мы минимизируем и максимизируем да ну да них расстояния в таком случае нас сеть между место не нужны картину может выучить но она может либо застрять в начале обучение в комнату локальном минимуме ему ничего хорошего не получим либо она вообще чаще может коллапсировать просто внук в ноль то есть все точки будут в нули вот что бороться с этой проблемой мы идем на компромисс не берем простые негативные примеры не берем очень сложные а берем сэмми hard так называемый которые достаточно челленджем для системы не слишком вот это как раз они как раз будет лежать вот этом зазоре альфа между классами если подвести итог под триплет лоссом то в самом большая проблема и завод выбору этих триплетов он сильно замедляет а сходимость потому что мы в делаем много разных вычислений чтобы найти эти триплета вот ведь собственно со мной концерн есть различные реализации upon собственно этого алгоритма это в принципе популярный алгоритм для файз велик огни шина первым его применил написал статью google свои сети у вас нет они получили на rfw 99 63 и на мигов и системе с плагины вот опу собственно реализации очень плохие они дают 92 процента она i love w вот мы написали свое на торце и смогли выжить из нее 65 процентов только вот такой такая разница между результатами скорее всего без опаски обусловлено тем что google использовал свои большие dtc то то есть мы учились на msc ли бы публичный доспехи у них свой приватный большой скорее всего разница в этом вот ну мы смогли выжить только 65 процентов нам надо двигаться дальше есть другие подходы один из них это центр волос или крайне просто им а во время обучения стягиваем к центрам обучаемым классов то есть все точки стараемся стянуть в одну вот такая простая идея и для сети не слишком много меняется мы у нас есть embedding мы стягиваем центра лоссом к центрам классов и но если мы так воспользуемся только этим лозам то все ценится в одну точку поэтому нам нужно чем-то уравновесить в данном случает софт макс мы просто ну мы добавляем классификацию и получается что финальная нашлось это некая взвешенную сумму этих двух и если посмотрим что получается при разных весах у центра лосс если мы поставим очень маленькое значение то очевидным получим тот же сорт макс что я был по мере того как мы будем увеличивать эту константу мы получать все более и более компактные классы вот собственно центра лосс лечат проблему софт макса то есть было раздели масть он добавляет еще компактность это очень здорово и также центра лосс у нас работает на очень многих досках мы применяем ещё эту штуку купить мы пробовали или сделаны очень хорошо работы в принципе легко заводится есть его оригинальной реализация там на кафе который до 65 процентов на мега фейсе мы написали свое на торчат не очень сложно на самом деле 71 на 7 его получили это уже достаточно высокая точность и с этого момента мы начали думать чем можно еще какие трюки добавить чтобы улучшить точность одна из них это обмен тация то есть во время inference и мы берем картинку исходную и флип его по иксу прогоняем их обеих через нашу сеть получаем 2 имбилдингом потом усредняем и получаем один финальный такую соединенные биллинг вот это снижает дисперсию и улучшает качество значит 2 напомню что мы у нас есть точки от мтс и на пять штук а мы можем поворачивать изображение то есть выравнивать его до этого мой центр у вас учили на не вырванных изображения и очевидно что сети обучение происходит гораздо сложнее если выравнивать и применить предыдущие аргументацию мы получаем еще прирост до 73 процентов на мегаполис вот в какой-то момент у нас были процентными проблемы с солнцезащитными очками мы воспользовались точками которые у нас есть и просто накладывали при обучении текстуры очков на фотографии вот это тот момент поборола эту проблему такая есть интересная аргументация мы потом от нее отказались потому что перестали испытывать проблем сеть при обучении может выучивать но какие-то разные признаки мы можем даже не знать на самом деле какие вот но почему вы нам как старые добрые времена не заняться fitch инжиниринга и помочь нашей сети выучить что интересное которую что поможет и разделять классы ну и такое такой признак мы решили что этот цвет глаз и для celebrity у нас есть это в есть это информация в открытом доступе и мы добавили в сеть помимо того что оно классифицирует персону использует центра лосс мы добавили еще предсказание цвета глаз там 10 штук их было разных цветов и вот эти три комбинации лоссов они собственно ну ему и кучу на всех этих трех лоссов в итоге это нам дала немного полупроцента вот семьдесят три и пять теперь у нас на мега фейсе им опробовали добавлять всякие другие признаки ну мы пришли к очевидному выводу что если добавлять какие-то простые вещи типа не знаю пол человека там мужской или женский то сеть это очевидно так и так вы лучше лает никакого профита не дают вернемся к метре клинингу есть еще один алгоритм который позволяет улучшить результаты центра loss называется но индусов макс или и софт макс суть его состоит в следующем в при обычном софт максим но не в данном случае для 2 классов на основе весов двух классов софт максим и проводим какую-то разделяющую плоскости вот и уравнение там внизу написано если мы от нормируемым вендинге до единицы то есть поделим на сумму квадратов координат то все точки в 7 бединге лягут на такую круг на окружность и но в n-мерном случае это будет гиперсферы и тут мы можем видеть что за раздели масть классов отвечает угол между векторами и соответственно мы можем переписать нашу переформулировать задачу таким образом чтобы уже оптимизировать угол между классами вот и формула показывает что мы можем сделать с помощью несложных преобразований и разница косинусов будет определять разделяющую плоскость но 1 перед формулировки нам недостаточно потому что мы просто совершили кит преобразования если мы будем оптимизировать этот угол никакой компактности классов мы не получим чтобы получить компактен компактность нам надо каким-то образом зафорсить больше угол между классами это можно сделать путем добавления константы м то есть она отвечает за то во сколько раз мы хотим чтобы угол между классами был больше чем внутри класса стресс но там добавляется в косинус из посмотреть что получается в итоге при разных им прямо равных равными единице мы получаем то же самое что и в софт макси то есть классы разделимы но точки разбросанные по сфере прямо равна 3 и получаем компактную картину то есть люди разные разъехались по различным областям сфер вот как написано и как я вам рассказал этот алгоритм он не работает вот как это ни странно и авторы имеют другую публикацию которую они рассказывают какое можно завести делается это примерно так мы во время обучения имеем два лосса прямо равно единице и прям равно 3 например его и по ходу обучения мы постепенно переходим от м1 км3 то есть такая взвешивая у нас суммы получать этих лоссов nm3 концу обучения нарастает то есть сеть плавно учиться увеличивать угол между векторами но это работает только на маленьких dt сетах хотя это дает отличные результаты то есть автор статьи получили там достаточно высокие скоро на мега phishing вот мы поправили чуть матан чтобы улучшить сходимости смогли завести на нашем большом дать с этим из цели по траве и софт макс и получили sims 4 2 процента на мега сиси это больше чем на процент лучше чем центра лосс тоже мы выслав макс уже не применяли и вот этот а elos вот значит если подвести итог под метр и клинингом то у нас центры лосс получилось лучше чем триплета и софт макс лучше чем так central основе софт макс очень сложно завести мы потратили кучу времени заставить его работать вот поэтому по умолчанию я рекомендую использовать центра лосс который очень легко работает впишется в несколько строчек на пайтор ч очень стабильные очень удобно и мы его используем даже просто в задачах классификации когда мы хотим чтобы класса тоже жались внутри чтобы было лучше обобщающая способность вот после этого мы начали как-то внедрять нашу сеть чтобы смотреть какие ошибки вообще что происходит сетью получают фидбэк и вот вы наверно замечали что маленькие дети все на одно лицо вот это действительно так и сеть с этим наши солидарно была она просто пихала все точки ну все лица в одну точку детей прям вообще их не различала практически вот и вторая проблема то что сеть была не толерантно casio там вот узнали мы об этом когда залили в интернет размеченный корпоратив и коллеги наши азиатской внешности указали нам что вот мы не толерантны к нашему стыду вот поэтому пришлось срочно фиксить разбираться чего не так вот про детей мы конечно знали что что существует такая проблема это проблема выходит из data set а потому что она у детей celebrity их как бы нет вдт сети азиатов их там достаточно мала еще там дофига мусора вот поэтому нам нужен нужно делать с данными мы взяли откачали в контакт то что там самый лучший api можно брать аватарки вот и составлять datasette но люди заливают в качестве аватара кучу всякого мусора не только себя там друзей и всякие демотиваторы поэтому все это надо чистить мы предлагаем такой алгоритм то есть мы берем все фотографии одного человека на аватарках выгоняем числа из detection и получаем вырезанные лица дальше мы пользуемся текущей моделью фэйс reccagni шин то есть который у нас есть на текущий момент и получаем бы деньги дальше мы проводим кластеризацию но не жесткую чтобы оставить следующей модели возможность выучиться и берем просто максимальный кластер это и будет наше персоны весь мусор выкидывает и прелесть этого алгоритма в том что мы можем повторять после каждого луча улучшениях is recognized то есть постоянно улучшаем наш data set вот мы на самом деле первым делом прочистили исходные dts этом из цели он составлен с помощью поисковых движков и например он есть всякие поисковые новости которые приводит к ошибкам например сценарист джойс техас там поссорился в публичном пространстве с мэлом гибсоном и вот вмс цели by это просто один человек также там есть такие вот персону как мужчины зачем-то перемешиваются женщинам вот тут мы получаем ответ почему мы имели проблемы с азиатами потому что движок microsoft очевидно тоже их не различает вот все подряд намешано есть вообще трешовые кластер а где какие-то картины мужчины этом женщины отсекут трэш но даже применив алгоритм даже такому трэшу мы получили тону сумели выделить вот спортсменам и добавит в дтп таким образом мы больше чем в половину сократили наш data set до 46 тысяч person со 100 вот и мы обучили на этом наш алгоритм и получили уже 76 и 23 софт макс ну мы мы начали немного в контакт и добавили в dataset 150000 person для vkontakte и составили свой тест по аналогии с мега фейсом называли омега в к только теперь надо искать людей из в контакт а вот и замерили текущие алгоритмы как они нам на нем работают и получили ну цифру 60 процентов что гораздо ниже нами так то что на мега фейсе то есть получается что мигов с не отражает в полной мере разнообразие всех возможных лиц то мы и люди в солнцезащитных очках там в макияже и ну вот разные вот это магов из плохо отражает мы обучились на вот этом дата с этим с цели плюс vk и получили уже еще август к точности до семидесяти семи с половиной процентов на мега фейсе un amigo вакама получили еще выше цифру но оно само по себе мало что значит потому что данные так или иначе могут быть коррелированы и на ее нет нет смысла там опираться на конкретное число вот участники там соревнований в том числе мега фейса часто используют ансамбле модели для улучшения качества точности мы воспользуюсь самым простым вариантом мы просто взяли наши две лучшие сети которые были обучены мы с целями и на совместном dts эти прогоняем часов без эти получаемым бединге и конкретизируем и их и получаем финальным bedding двести пятьдесят шесть футов и на этом бединге у нас уже точность на мегафоне получается 7 9 6 что почти 80 процентов которые мы хотели изначально но в бой мы покатили одну модель и то есть которое имело 770 половиной процентов потому что ну если мы будем хотеть ансамбль то у нас двойные расходы на память двойные расходы на этом вычисления на все чего мы не можем позволить но еще оставалась небольшая проблема с детьми и как бы алгоритм гораздо лучше с ними начал справляться после обучения навыка но все еще недостаточно хорошо вот поэтому решили немножечко подкаст или заметили что все дети в принципе расположены достаточно компактно в им бы не экспрессия и взяли доцент с детьми посчитали им бединге почитали среднее и провели сферу которую мы говорим что внутри сферы если попадает точку значит это ребенок вот и внутри это с фермы можем уже ужесточить порог на сравнения потому что сеть детей путают вот мы уже встречаем порог и решаем эту проблему при этом общий порог мы можем упустить потому что на десерт на надеть их у нас свой порог и он был достаточно завышен как раз именно из-за детей то есть мы еще слаб расслабили в итоге общем последнее что я скажу prometric leaning это как ну что делать вообще с большими даты сетами google учился на огромных дата сетах и но он использовал триплет но они использовали триплет лосс там нет такой проблемы но если мы все наши алгоритмы лучшие они используются вт макс а он потребляет ресурсы то есть этого классифицируйте сети который классифицирует занимает память занимает вычисления линейно вот поэтому в тупую нельзя сделать миллионные слои есть решение так называем сов макс аппроксимации достаточно простая идея мы берем наш data set и кластер жую с помощью cummins допустим карам внутри на там три кластер дети мужчин и женщину вот и далее мы в нашу модель добавляем еще один софт мог слой которые нам предсказывает кластер в данном случае что вот эта мужчина a soft макс на персоны мы уже предсказываем человека внутри кластера то есть мы получается вот этим к управляемым как какого у нас максимального размера вот этот софт макс для person вот я решаем эту проблему из плюсов алгоритмы у нас не съезжаются друг другу кластер а потому что если мы выкинем клуб кластер софт макс и будем менять периодически там при обучении разные софт макса то у нас ведь кластером могу запросто начать съезжаться вот что в данном алгоритме не и так же мы делаем что-то вроде hard негатив майнинга потому что мы сравним мужчина с мужчинам женщин женщину сети как бы более сложную сложные примеры вот ну и дополнительно мы можем задавать эти класс террасами руками если хотим какой-то определенной компактность или разделения кластеров например детей или азиатов после этого значит на вот этот сок макс аппроксимации мы еще работаем результатов я не могу сказать вот мы предыдущую вот модель 770 половина начали внедрять в рот и увидели там различные ошибки значит первое то что детектор отлично работает и детектив за бурильной фотографии но для фэсс recognise она недостаточно лицевых признаков чтоб разделить людей он начинает их клеить средстве есть простой к стильный вариант это посчитать вторую производную по картинке так называемого пластины посмотреть на его дисперсию для размытых картина комбайну дисперсия будет низкое области она для четких картины когда будет высокая и можно соответственно поставить под порог отсечения и достаточно точно отсекать всякий размытых фотографий а вот на более серьезная проблема это когда детектор действительно фолдит и определяет в качестве лица например там какие-то конечности пальцы рук ног вот эта проблема потому что еще добавляются но другие такие ошибки и там пользователь смотрит и видит что там вы любимого ребенка поклевок с кулисы и головы мужика вот он офигевает от этого и закрывает приложение вот и или мои любимые грибы которые тоже почему-то распознаются как лицо вот наряду с диаграммы которая кру лыжи вот и в детектор не видел в этих картинок и вот тоже их распознаёт как лица ещё там можно за фиксики можем заметить шов из recognition склеивает эти фотографии месте значит что их что-то объединяет а именно мы вспомним что когда мы pdf фотографии все эти получаем беден кто самым байден код какие-то признаки лицевые до которые сеть вы выявила и на основе них мы сравниваем поэтому если мы подойдем гривы это на самом деле эти признаки срабатывать не будут будет очень слабые активации ну то есть в районе нуля и это приводит к тому что норму таких фазовых картины будет очень низкая и норма о нормальных фотографий она обычно вот и это еще с математической точки зрения форсится тем что во время обучения на софт макси у нас софт макс оптимизирует в том числе произведения скалярное произведение весов на им embedding и там участвует норма чем выше норматива с большей вероятностью i'm wading попадет в какой-то класс вот это отлично работает там рука укус семь процентов даже лучше работает для за блюдах фотографии чем улыбалась она полностью решает проблему с непонятными кластером какие клевые результаты мы получили ну внедрив нашу сеть что мы увидели что она умеет значит первое не так давно путин назначил 2 губернаторов которые как капля воды друг на друга похожи когда я увидел вот эти фотографии даже не понял что это разные люди вот надеюсь путин их отличает вот наша сеть от точно их отличает и и дмитрий с глебом разносит по разным персонам вот второй прикольный результат когда мы учились надо достать его к средину аватарок людей люди выкладывают свои фотографии за разные возраста и сеть научилась учитывать возраст при распознавании и вот как данном случае мы склеили мускула стилизовали фотографии там детские с юношескими и что особенно приятно это было сделано на облаке нашего руководителя обычному у нее ловим всевозможные ошибки получаем по голове но в данном случае мы отработали клёво и такой вот интересный результат даже человек человеку будет очень тяжело отличить но действительно то один человек или нет по похожий пример то есть по фотке паспорт склеили современные фотографии там старым вот если подводить итог под всем рассказам то первое можно и нужно оптимизировать со времен сывороточные сети потому что фреймворке дает overhead и мы с помощью тендер rt смогли получить плюс 20 процентов прироста еще попутно мы оптимизировали еще сильнее вот для метро крайне нга рекомендую использовать central os потому что он просто в употреблении в отличие от эйсов макса которые рекомендую использовать только если вам важен каждый процент задачи и вы готовы потратить много времени третье это как всегда очень важно важны чистое данные потому что как показывают недавние статьи сети способны выучить вообще любой шум данных в данных в том числе вообще рандомные лейблы при обучении поэтому чистая dtc всегда приводит bust of performance и надо всегда думать надо понимать что происходит при обучении чтобы происходит при интернете чтобы направить в нужное русло или как-то пофиксите ошибки вот заключение скажу что сейчас машина обучения активно развивается и внедряется во все сферы нашей жизни но есть большая проблема это недостаток обобщающие способности то есть способность моделей работать на данных которых она не видела при обучении и это приводит к тому что нам при обучении нужны либо будет больше выборки и долго и нудно тестировать на продакшен данных чтобы словить все фазы и до обучить сеть поэтому использование метрик леоненко она позволяет как раз частично решить эту проблему и улучшить обобщая способность сделать модели более радостным за последние пару лет дмитрий клименко сильно развился вот на примере высеры как низшего в частности и эта задача действительно трудно интересное потому что она тяжелые с точки зрения исследовательской потому что там много математики есть где развернуться что придумает из инженерной как эти всю математику оптимально посчитайте как это как результат всей работы внедрить production вот поэтому изучайте метре cleaning использовать его на практике спасибо у меня всем здравствуйте подскажите пожалуйста а вот если у нас потоковое видео и на нем изображен человек но вдруг попало либо зеркало lizard либо зашла сестра-близнец как в этих случаях определить кто есть кто на кадре но это зависит как мы обучали вот например apple заявляет что они заявляли что они близнецов различают на своей верификации потом правда заявили что нет ну я думаю что на текущем качестве вряд ли мы будем различать близнецов это будет несколько позже а вот именно что видео обучение и система сама учится его запоминаем родинки текстуру сохраняет но мы можем например учить ну сеть точно так же на фотографиях но при этом мы можем например учить несколько сетей которые например работать на разных частях лица например там на глазах на там ящиках вот на разных патчах лица таким образом вас будет ансамбль модели которая реагирует на более пристально на какие-то детали и как показывает практика это даёт нам достаточно существенным бустом но тоже мега facit может быть там 2 3 4 процента здравствую спасибо за доклад вопрос какая следующая бизнес задача важнейшие но я не могу раскрывать наши бизнес-план она скоро мы планируем еще одно внедрения будет интересно а техническая трудность в смысле техническая трудность и не понимаю вопрос хорошо бизнес задача оставим за кадром какая следующий техническая сложность который вам надо преодолеть но отверг научиться обучаться на очень больших дата сетах это основная сложность но дальше там основные сложности это как внедрит внедрить это в продакшен у нас там есть свой компьютер или жена пью там всякие очереди базы тарантулов и так далее и вот там тоже очень большую проблему как это все оптимально сделайте ускорить можно вопросик так в один может быть спасибо большое потрясающее выступление про себя по поводу базовых точек возможно это как-то не за преобразование хафа или как это находится и для чего нужны ну и соответственно нахождения этих базовых точек на лице ну мы их находим с помощью свёрточная сети которые мы просто учили на выборки и они нам нужны только для того чтобы повернуть лицом то есть могут вот вот так у сфотографированы и нам надо для вас reccagni шин его развернуть чтобы флиса и конечно было легче принять решение ну как какой-то человек а там дает один-два процента точности можно этого не делать соответственно нахождение при свёртке на последнем этапе да то есть мы просто регрессию решаем в какой толь в каких точках находится и теперь спасибо за доклад а справа вкус вывод не или что такую проблему я тоже это читал эту статью где вот зависимости от capacity да ну то есть сети к пасте сети и количество do the point of nirvana сеть может но любовь данный луч от любой шум от нас случайно да да да а что насчет обзор сервиса смысле помогает ли они с этой проблемой пигмент вообще adverse ареала таки такие атаки когда у нас есть еще одна сеть которую специальным образом хочет изменить картинку чтобы надурить сети то лучше с активно развивающаяся область и чтобы сделать сеть устойчивый к таким adverse ареала так и а так им есть несколько способов в частности можно вам во время обучения держать вот такой ансамбль вот этих атакующих сетей и аргументировать таким образом данных также есть различные техники там например mix up так называемые когда мы картинки во время обучения просто складываем и лэйбл лейблы на выходе складываем и это получается такое обычно не использует отдельный ведь это он сам были из пуль да-а-а doors are all 7 может только тот цвет который распознает то есть если у вас есть доступ то называем это в лайтбокс но если они обучаются в свои боксом по сути я уж так тяжелее вот вы не дело мы этого еще сделали ряд то есть мы думаем вообще в принципе для применения этой технологии в наше всех продуктах вот но пока мы еще там по исследовали не делают даже если не знают люди делают если мой прикольные примеры когда там специально удерут раскраска лица if is recognized люди это знают но не делают потому что там от кори скорость думают падает но принципе вот недавно была соревнованием гипса на dance не знаю вот дефанс со стороны дефанса легче потому что со скажите если смотреть производительности на самом деле там легче и со скорость не падает особо спасибо пососи а можно еще одинок а вы не использовали метод векторных опорных век векторов для от классификации опорных векторов qaynana вывезено не нужно методу машин опорных векторов или так далее просто близ вектор с вами мне не использовать не используем то есть принципе или каких-то причин и тайные то есть не просто не пробовали таится сеть сама всего учует выучивает метрику этого достаточно на основе метрики достаточно легко сравнивать людей нецелесообразно вы считали используя да ну нет мы учим метрику если мы все-таки классификация у нас как бы неизвестное количество классов неизвестное количество людей нам нужно метрикам спасибо скажете вот такой вопрос я думаю что у гугла но у них результаты потому что у них есть свой частный архивчик но свой частный datasette да как-то ну понятно почему то что они могут сделать все о земного шара дам а вы говорите о том что вы взяли его силе и еще до кучи взяли в контакт ну вот а вот насчет контакта такой немножко странно вопрос на любой может как бы или я не 0 мы просто взяли апперкотом помада пользователи должны там что-то она условного если они открыты а большинство пользователей как минимум половину открытый можно проскочить мы просто взяли откачали откачали своих машину всем никаких проблем а все я понял да спасибо баня можно составить свой такой честный dtc если знать как все вопросы видим окончена время но давайте последний спасибо еще раз шикарно доклад я хотел спросить вы же получили определенного выбор куда обучив нейронную сеть обучающего метида да да извините я говорил вот у вас есть какое-то ранжирование между значимостью фичей даффи чем апах то есть одни фичи важнее чем другие но мы нет ни следовали вопрос мы просто метрику замеряем между вот а не было ли у вас такой мысли наиболее значимые фичи на основе выделения наиболее значимых вещей разработать методику такой съемки людей чтобы повысить эффективность можно применять например писи и ну то есть этот вектор схлопываться меньшее количество признаков вот в какой-то момент когда мы учили самом начале виктора длиной 512 на примере писи и да там 100 примерно у нас получился будет от этого то есть я заработал лучше в принципе это работает cba спасибо"
}