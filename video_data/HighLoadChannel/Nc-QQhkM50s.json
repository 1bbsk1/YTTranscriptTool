{
  "video_id": "Nc-QQhkM50s",
  "channel": "HighLoadChannel",
  "title": "Почему распределенный SQL сложнее, чем кажется / Станислав Лукьянов (GridGain)",
  "views": 2445,
  "duration": 2746,
  "published": "2021-10-04T02:42:08-07:00",
  "text": "спасибо что пришли всем привет меня зовут стас лукьянов я работаю в great game мы в регейн занимаемся в венгерской поддержкой распределенной базы данных который называется apache и игнайт кто здесь слышал знает систему apache и игнайт поднимите руки пожалуйста о прилично приятно спасибо а кто кстати кто был на докладах вчера на докладе рома кондакова про сквере на кольце ти вы знаете вот вижу пару человек а кто-нибудь на докладе вову озеро вопрос quelle в хозяйстве вот вам будет немножко попроще всем остальным тоже будет легко вы уже знаете некоторые части некоторую часть часть проблем с которым мы сегодня будем бороться так вот мы помогаем пользователям запускать системы на apache и игнайт и часто люди когда приходят с обычной базы данных с какой-нибудь ссора класс пост гаи на распределенную базу соски елим они просто приливают данные просто запускают свои старые запросы и видят что у них некоторые запросы не работают а некоторые запросы работают не быстрее или даже медленнее не говорят у нас жить у нас был один пост гриша теперь у нас пять узлов игнайт и должен быть в пять раз быстрее правильно но в реальной жизни так конечно не бывает и вот сегодня цель моего доклада это рассказать почему же это так не работает что же там за проблемы которые возникают действительно в распределенной системе и как их можно обойти как их можно решить с чем может помочь база а что нам придется как разработчикам приложений делать самим итак поехали начнем с простых вопросов что он вообще будем в рамках этого доклада понимать под распределенную с гелем распределенные сколь системой как она работает и как понять что игра вообще стоит свеч зачем мы можем ее применить начнем скажем что мы начинаем с простой обычной rdbms она у нас имеет какой-то дисками это какой-то процессор память оптимизатор хранилище индексы runtime выполнение все это у нас уже есть и теперь мы захотели масштабироваться мы захотели долить еще диска еще процессора еще памяти и захотели чтобы наш стиль запросы какую-то пользу из этого нового железа извлекли чтобы это случилось мы должны как люди создающие всю эту систему распределенного эскивель а мы должны ответить на несколько вопросов 1 это что ж за данные мы будем распределять как мы нашу базу которая раньше желаю одним куском сделаем несколькими кусками во вторых нам надо понять как мы эти куски будем держать в консистентной состоянии что мы будем понимать под консистентную и так далее также нам нужно понять как мы будем распределять вычислительные ресурсы то есть мы одинаковые вещи на всех узлах будем делать или разные вещи спойлер конечно же немножко разные и наконец нам надо понять как мы все таки будем наш исходный вопрос который у нас раньше выполнялся монолитно на одном узле что значит распределить эскивель запрос по системе и сразу disclaimer о чем сегодня не будем говорить во-первых мы не будем сегодня говорить про так как данные реплицируются алгоритм репликации эту тему не на один она кучу отдельных докладов вот прямо сейчас мне кажется в зале номер три рассказывают про pax ас в картинках вот это отдельные темы сегодня будем говорить про select и в распределенной символьной базе не будем так же говорить про то как нам делать дизель как нам управлять конфигурации как нам изменять схемы это все очень интересный задачу действительную распределенная база здесь ставят дополнительные челленджи на сегодня поговорим в первую очередь прочтение ну и перед тем как мы перейдем он к вопросу как же конкретно мы будем все эти вещи имплементировать давайте скажем зачем это нам вообще может быть нужно все в общем сводится к двум основным вещам мы хотим чтобы было быстрее мы хотим быстрее и больше то есть мы хотим что масштабироваться увеличить производительность увеличить количество данных которые мы можем обработать и мы хотим чтобы это работало надежнее то есть мы хотим большую отказоустойчивость если это не нужно если текущий параметр по производительности по емкости по отказоустойчивости устраивают не надо гнаться за модой не нужно прибегать на распределенную систему она принесет вам если не новые проблемы то хотя бы новые вопросы вам конечно как инженером будет интересно но с точки зрения проекта это может быть не не лучшим решением и так считаем что распределенная база нам все таки нужно уперлись мы в возможности вертикального роста и хотим масштабироваться как будем распределять данные тут все просто все механизмы давно известны и и не в распределенных системах они известны репликация простейший механизм берем одну базу делаем еще несколько таких же получается у нас много копий одной и той же базы тут мы уже можем сделать некий такой не до распределенный эскивель уже можем получить некоторый benefit от того что мы сделали несколько копий поставим load balancer перед всеми нашими базами начнем запрос балансировать уже стал немножко побыстрее доступность сильно увеличилась у нас три независимой копия но главная проблема масштабироваться мы пока что не можем потому что каждая база хранит полный datasette значит больше данных мы в такую систему не воткнем опытные инженеры все знают что если больше данных воткнуть нельзя значит надо сортироваться шарди рование тоже старый механик тоже везде он есть берем одну лозу и эту одну базу распиливаем на несколько кусочков каждый из этих кусочков у нас каждый из этих кусочков у нас хранится на отдельном узле значит каждый узел хранит немножко меньше данных значит каждый узел немножечко больше немножко больше сможет в себя вместить вот мы уже и смогли таким образом за масштабироваться однако тут получается что каждый узел уже полный результат запроса нам вернуть не сможет для того чтобы выполнить такой системе запрос нам нужен какой дополнительный компонент который мы пока что условно назовем склеивать это дополнительный компонент будет принимать исходный запрос пересылать его в наши узлы базы данных получать частичные ответы и склеивать их в какой-то финальный результат проблема такой системы в том что если у нас один узел выпал то мы уже не сможем полный результат получить и получается что если в масштабировании мы и выиграли то в доступности мы конечно же проиграли что делать для того чтобы получить и доступности масштабируемость тоже все очень легко комбинируем рипли к репликацию и шарди рование говорим что мы каждый sharp будем хранить не в одном экземпляре а в нескольких экземплярах там например первый узел будет хранить шард 1 и 2 второй узел хранить шарды 2 и 3 3 1 и 3 и тогда потеряв один узел мы все еще можем шарды который на этом узле охранялся забрать какого другого узла отлично все получили и доступности масштабируемость с масштабируемость стало немножко похуже чем варианте без репликации с чистым шарди рования понять потому что мы диска теперь больше используем но в общем задачу на свое решили тут два слова про то как это работает во почернеет я пачек знает буду использовать как пример такой распределенный база данных с поддержкой сиквел а я его хорошо знаю мне близок мне проще от него оттолкнуться но более-менее также это будет работать любой системе в играйте все таблицы сортированы по умолчанию шарды в и найти называются партициями и есть два режима таблиц есть таблицы которые называются parties and то есть профессора не раваны и шарди рованные таким таблицам можно указать сколько копий их хранить но сколько backup копий по умолчанию вот на примере на экране мы будем хранить две копии одно основное один backup есть таблицы который называется репликейт то есть полностью реплицировали их число копий она всегда равно числу копией число наших узлов в системе то есть каждый узел хранит полную копию данных реплика этот таблицы таким образом если нам нужно обязательно масштабироваться по хранению какой-то таблице мы обязаны ее сделать против нет то есть хранить неполный datasette на каждом узле если таблица небольшая мы в принципе можем сделать ее рипли кайтинг и еще такой момент это распределение партиции то есть шар дав по узлам она полностью автоматическая и она динамически меняется динамически пересчитывается когда у нас меняется в пологий то есть приходит тузлы уходит тузлы мы понимаем что у нас теперь распределение данных должно быть другой и системы она сама это все перекинет вот дальше немножко опустим вопрос про репликацию про хранения нескольких копий по про доступность дальше будем говорить первую очередь про шарди рование и мы сказали что для того чтобы формирование у нас завелось нам нужно уметь каким-то образом склеивать частичные результаты запросов в конечный результат для пользователя и на самом деле вот посмотрев на эту схемку я уверен что многие уже поняли что вот это за паттерн какие-нибудь варианты мапри deus конечно же мапри дьюс здесь не имеем в виду то что google когда-то сделал не имеем ввиду ходаковский map requires имеем ввиду сам паттерн то есть такой паторну распределенных вычислений когда мы вычисление разбиваем на две фазы 1 фаза у нас делается полностью распределена на каждом узле по отдельности потом все частичные результаты скатываются в одно место собираются и там конечный результат добивается и возвращается пользователю в случае с сквер запросам типичное распределение работы будет выглядеть примерно так на этапе map точно будет сам скан то есть мапперы это условие которые хранят данные они сканируют базу по индексу делают какую-то первоначальную фильтрацию совершенно точно то есть vr скорее всего будет выполняться на мотив большинстве случаев join и агрегация будут где-то между mapreduce между мапо мариту сам гулять-то выполняться нам а питона радиусе мы дальше посмотрим от чего это зависит ну и конечно reviews должен сделать какую-то финальную вот эту склейку данных пользователя их отправить давайте посмотрим на примере опять же apache игнайт как у нас режиссер устроен внутри какие на какие какие задачи должен решить на какие вопросы ответить делаем запрос простенький запрос попадает на радиус r на apache apache игнайт reviews are это тот узел который как бы является первым узлом принял принявшим запрос то есть тот кто обрабатывает запрос тот режиссер чуть позже проекта подробнее скажу в радиусе вопрос изначально попадают в компонент как же называется к вере сплиттер ну естественно тоже нам сложно устроен но главное что он делает он парсит исходный запрос и он разбивает его на 2 части собственно на часть map и на часть радиус часть map после этого уезжает на мапперы и результаты и после того как мапперы обработали возвращается обратно на режиссер наледью серия создается под результаты мам запроса создается специальная виртуальная таблица это не совсем таблица в привычном понимании это скорее такая в ухо над курсорами который мы получили на запросы в мапперы то есть это там такая в юха котов в которой мы можем проецироваться потому что нам mapper вернули и вот на этой вот нашей виртуальной в ухе мы исполним второй запрос который нам к варе сплиттер выдал вопрос радиус и результат этого запроса мы уже вернем пользователя получается что у нас были review серб получил запрос раз парсел разбил отправил его на map и режиссером при этом мы можем назначить разные узлы это на самом деле вы знаете зависит от того как мы запросу пускаем их и запускаем и как мы к базе к ногтям ся например грубо говоря из тонкие клиенты и толстые так называемые тонкие клиенты они тупые они подключаются к какому-то узла базы и узел база за них будет режиссером то есть он будет 310 все запросы пришедшее от этого тонкого клиента так например джитибиси работают выгнать а есть умные узлы но умные клиенты также ну этот толстый клиент и например на основной java клиент так работают он подключается в кластер как но да как у как узел кластера и он сам может себе быть режиссером и часто в этом случае само приложение режиссером будет выступать а значит мы можем как бы выбирать кто у нас будет кто у нас будет процессор на и время тратить на запрос будет само приложение или это будут серверной ноды или мы вообще выделенную ног сделаем и мы посмотрели что mapper он же работает локально то есть каждый узел на фазе map он только про себя что-то знает и там все работает как в обычные rdbms там так обычно работают оптимизации все узлы делают эти оптимизации независимо теоретически каждый узел вообще может иметь разные планы запросов локальные на фазе map точно также как обычный or dbms кстати работает вот этот запрос в виртуальную таблицу которую мы создали на review siri единственная сложность всего этого механизма то что делают распределенной скилле возможным это работа квари сплиттера то есть то как мы разбиваем запрос на две фазы и давайте посмотрим на то как как же мы можем это сделать и какие у нас тут будут сложности да и еще один момент который конечно не сказал это то что мы определись уже есть ну фундаментальная проблема ready to battle ног и когда мы будем развивать разбивает запрос на мапри deus нашей главной целью будет попытаться как можно больше на фазу map скинуть потому что режиссер у нас один это не сканируется of азамат у нас делается вот на всем железе которой у нас есть смотрим как мы можем наивно выполнить запрос вот тот вопрос на который мы уже посмотрели или например там запрос заменим select deselect каунт наивно можно было бы просто все данные вытащить с мапперов то есть сделать делать в мапперы всегда select звездочку вытаскивать все данные и по этим полным данным уже наш финальный запросто есть все вычисления наряде у siri делать естественно это не сканируется мы присылаем целую кучу данных на реки сервере тусили тратить много процессов во времени нам это не надо мне ни для этого распределенные стиль запускали как сделать лучше мы можем посчитать агрегаты прямо на на пирах ну вот например для каунта как это сделать тут понятно считаем на каждом матери сколько у него есть данных а потом это все суммируем с одной стороны все понятно и стал гораздо лучше с другой стороны в общем как мы из каунта получили сам но очевидно что робот проект ничего не знаю тут надо немножко человеку подумать и на самом деле если смотреть на простые запросы то можно сказать что вот скан и фильтрация этом по индексу не по индексу она нам об вообще отлично уходит почти всегда агрегации уходят хорошо но им нужна какая-то специальная обработка то есть нам нужно прямо за хардкоре что вот если мы считаем каунт там и на мапри будем делать каунта потом мы это все просуммируем если мы считаем а веришь то нам нужно получается забрать суммы со всех узлов county со всех узлов и одно на другое поделить на review siri это все ну нам надо немножко захар колить но это естественно работают и нам как разработчикам базы данных это может быть сложно не очень сложно на для пользователя вообще легко у него просто все работают и достаточно быстро что работает сложнее конечно же дроиды join и самые сложные через базах наверное и в чем здесь проблема вот представишь нам надо теперь нашу таблицу person за joy нить с таблицы компания в таблице 1 сон мы храним людей и айдишники компании в которых они работают в таблице company мы работаем храним ну сами компании садишь никами соответственно хотим в результате получить четыре строчки как всех людей и компании где они работают мы уже знаем что мы наряди усилив все делать не хотим и вспоминаем про то что у нас система сортированная распределенная не хотим делать наряде у siri что будет если сделаем join прямо на матери если сделаем joint прям на матери то видим что вот с первого узла у нас join получился вернулся вася с 3 узла у нас получился join вернулся вова а со второго узла вернулся только петь и о маше потерялась потому что маша работает в компании fb компания fb хранится у нас на третьем узле а маша хранится на втором узле получили ошибку делать no reviews плохо делать на матери не работает но как сделать так чтоб нам api разработала и тут есть волшебное слово которое на самом деле уже мелькала спойлером где-то в начале доклада кто-нибудь запомнил волшебное слово colocation волшебное слово которое где-то в самом начале мелькала colocation это когда мы храним данные которые друг с другом связаны рядом clock к лоцировать можно например по ключу например мы можем сказать что все данные с все люди с одним и тем же ком пойди 3 и все компании иди три будут всегда хранится на одном узле если мы этот инвариант гарантируем то мы можем сказать что таблица person скала сырого на с таблицы компания по их ключам айди и ком пойди если это если это выдерживается то тогда мы можем делать join локально на каждом узле и всегда получать правильные данные сделали быстро все сделали локально и получили верный результат классно есть ещё один способ сделать к локацию и это colocation вместо того чтобы клацай ключу к локация с помощью репликации в этом случае мы вместо того чтобы сортировать правую таблицу таблицу компания мы ее просто реплицирует мы храним полную копию table1 из таблиц на всех узлах тогда тоже мы делая джоин сначала идем по левой таблицы делаем lookup и в правую правое у нас полностью есть локально на каждом матери и мы тоже получим правильный результат то есть можем скала целоваться по ключу можем складироваться вот одну таблицу ты отказов отказавшись от сортирование одной из таблиц как выбрать по ключу colocation хорошо работает когда есть вот такой классный крышку котором мы можем складироваться для лтп систем это очень часто так например если у нас есть если у нас вся работа построена вокруг обработки каких банковских транзакций каких-нить операций с пользователями у нас вот эти сущности транзакции ли пользователь они могут быть хорошим хорошей основой для такой колокации кола цирка локация с помощью репликации хорошо работают например для схемы звезда когда у нас есть одна центральная таблица фактов она постоянно растет она огромная например это тоже там банковские транзакции или покупки или какие-нибудь там сессии звонков а к этой таблице у нас join это целая куча маленьких справочников маленькие справочнике меняются редко сортировать нам их особо не надо потому что они маленькие мы можем их просто на всех узлах хранить и тогда у нас join и отлично будут работать и еще стоит сказать что у нас кроме джайнов с помощью колокации могут может ускоряться могут ускоряться и другие части спины запроса и другие сложные конструкции например если посмотреть на грубой то если мы бы не делали joins компания и спросим если бы мы делали грубой по компаниям то есть но хотели бы посчитать там не знаю средний возраст сотрудников компании то это тоже бы заработалась к локаций потому что у нас все данные для получения полной группировки у нас бы лежали на каждом отдельном узле то есть colocation это прям хорошая такая серебряная пуля для проблем с распределенными с распределенным сиквелом но серебряная да не серебряная конечно повторим ещё раз то что мы на что мы посмотрели вопросе оптимизации regius что мы можем сделать сначала мы пытаемся все что можно спихнуть на фазу map вообще наивно вот как она была в исходным запросе мы его прямо так и скидываем vr так хорошо уйдет сканы так хорошо уйдут агрегация уйдет ну чуть более сложно нам здесь нужна какая-то кастомной а логика дополнительная но для аккаунта a very джо это сделать несложно совсем для всяких distinct of ордеров это можно сделать с помощью всяких там мерч алгоритмов если у нас есть предварительную сортировку по индексу в общем частично это как-то можно сделать дальше начинаются сложности и здесь уже нужно базе данных немножко помочь дальше нам нужно использовать к локацию и для того чтобы colocation у нас завелась нам нужно адаптировать свою модель данных нам нужно придумать модель colocation как у нас какие данные с какими должны быть клонированы и возможно даже нашу модель данных или модель запросов немножко поменять а все что мы не смогли таким образом с оптимизированными kingdom of нам придется сделать наряде усилия и если reviews будет сложной то работает медленно и в реальной жизни reviews в какой-то момент будет сложной и тогда нам тоже нужно иметь какой-то план б что делать если reviews сложный тут два решения фундаментальных одно решение это попытаться заз коллировать у нас есть один батл ник один режиссер если есть один батл ник попробую группа масштабировать может что получится это работает хорошо для цепи системы то прям часто работает хорошо потому что если запросов нас много пользователей много приложений много то мы каким-то о балансе нгам можем вот эту reviews фазу по всему нашему железу равномерно размазать другое решение это многофазные запросы и тут как раз подробно вчера рассказывали hv2 докладах и про то как это делалось для игнайт и про то как это делалось для хозяйка 100 коллеги рассказывали если коротко то подход примерно такой мы вместо мапри дьюса используем более сложные алгоритмы планирования и здесь помогает хорошо играет а пачкой сайт это фреймворк для выполнения sql-запросов который очень с очень большими возможностями кастомизации и вот на нем хорошо писать вот такие вот сложные runtime и и планировщики которые можно которыми можно сделать сложный граф вычислений для sql запроса и сделать его не в две фазы а сделать его вообще много фаз раскидывая каждую фазу на какой-то узел вот на какую-то часть нашего железа которая у нас есть но это конечно требует поддержки на стороне базы и конечно же это тоже не серебряная пуля это все равно потребует от нас пересылки данных по сети и все равно как бы если colocation даже у нас не полетела то вот это уже будет таким work раундом все равно это будет все равно все эти скорее всего будет упираться еще немножко про apache игнайт как это сделано в vape чем найти естественного гноить и есть все базовые вот эти механизмы как разбить к он как разбить override как там спихнуть фильтрацию нам об это все заработает join и вы знаете по умолчанию считаются склад и рваными то есть игнайт не будет пытаться вытащить моря бисер данные таблиц если они у вас не скала церовани будет пытаться заменить наряди у siri потому что в реальной жизни join регистре особо не работают но он просто медленный слишком будет но есть возможность включить специальный такой финт который сделает другой вариант запуска запроса как про который немножко дальше поговорим и есть вот такой стиль движок на папочкой сайт то есть уже отход от мапри deuce и этот движок должен вы играете 30 к концу года за релизиться вот в октябре должна быть альфа с этим движком уже на борту два слова про то как colocation делается папочек знает сказали что нам нужно как-то подготовить свои данные и модель колокации написать может быть не просто а вот сделать это просто технически как настроить и так на самом деле очень легко локация по ключу есть такое понятие вы знаете который называется афинити ки мы выбираем просто поле которое для нас будет вот этим дескриптором того данных которые мы должны держать вместе вот в нашем примере это было бы polycom пойди для таблица person если не указываем афин текке то его роль играет праймари ключ то есть для таблицы компании это был бы идеи как нам и нужно для колокации с помощью репликации мы уже тоже видели этот пример просто есть реплицировали таблице вы знаете они отчасти для этого и нужны они вот с этим и помогают и такой пример не спонсировано и join и вы знаете как работают mysql оцифрованные join и это вот этот специальный режим когда мы можем агна эту сказать что колобка локацию мы не смогли но ты нам все равно данные за джони он это делает с помощью расширенной фазы map в этом случае мапперы сначала сканировать левую таблицу а после сканы они на самом деле уже понимают все полный набор данных который им надо затянуть других узлов и они это и делают и они просто запрашивают у других узлов данные которых им для join a недостает доделываю join локально и эти результаты отправляют на ряде юзер это казалось бы работает вообще классно все сделали на фига нам colocation но на самом деле это все равно очень много сетевого трафика мы очень много данных будем притаскивать по сети и это хорошо работает не для 100 процентов ваших запросов а вот когда у вас все работает хорошо и есть какой-то один такой запрос неприятный он часто это какой недельное чет или что-то такое какая даже бордовом иногда надо отрисовать или выгрузку сделать и вот для нее можно включить для вот такого запроса там раз в неделю в тихие часы можно вот такую штуку сделать даже если она нагрузить сеть подведем итог поговорили сегодня про то что начали с того что нам нужно сортирование для того чтобы масштабироваться пришли к тому что нам для этого может подойти мапри deus как самый простой такой паттерн обработки данных в распределенном режиме но reviews это battle.net и мы посмотрели как с ним бороться colocation данных это идеальный способ с ним бороться с какие-то оптимизации для нас может сделать база с колокации придется повозиться самим какие выводы мы можем для себя сделать во первых это наверное было очевидно и до доклада ну понимать как работает ваша база а иначе вы не поймете почему она у вас тормозит если вы переходите на распределенные сквер ваши join и могут тормозить и вам придется скорее всего адаптировать модель чтобы и join и агрегации не тормозили и colocation этот как раз тот способ адаптации она нам поможет colocation это круто но нужно быть готовым и и правильно разметить придумать и возможно поменять манную модель данных и поменять запрос спасибо за внимание круто-круто стать спасибо тебе большое слушайте получается недостаточности запросите писать надо еще понимать как там данные внизу лежит ну иногда приходится к сожалению но это и но это если надо быстро если надо медленно то не надо спасибо спасибо тебе огромное за доклад у нас для тебя есть памятная табличка с персонале нашей благодарностью худи и значок с хайло дам друзья поднимаете руки задавайте ваши вопросы то есть то ли все понятно то ли непонятно ничего а вот есть рука спасибо огромное на пятом ряду и потом на первом ряду есть рука онлайн присоединяйтесь нажмите кнопочку выйдите эфир в эфир и задайте свой вопрос 1 раз здрасте спасибо за доклад подскажите вот вы рассказали о том что я могу написать какой-то свой запрос и к репортерам него как-то само перепишет а на основе чего это сделано то есть вы как-то раскладываете запрос на какой-то сент x3 переписываете его и сравниваете что он делает примерно то же самое мы не сравниваем что он делает пример то же самое действительно партиям индекс 3 но там нет там все преобразования они гарантированы эквивалентные и они но грубо говоря они захардкожены то есть мы знаем что если мы например делаем грубой то нам нужно будет сначала сделать этот грубой на матери а потом сделать этот грубой наряду siri если мы делаем а веришь то у нас вот прямо вот захардкожены что вот надо сначала посчитать это нам а потом посчитать это нас нарядился чтобы это есть запрос будет чуть сложнее чем просто диванной группой с например какими-нибудь swindle фонтанами и группировка с группировкой из обертками развили винду фанкшн и в и найти не поддерживаются и ну да работу и винду фанкшн и не поддерживаются есть некоторые виды запросов которые действительно не полетят то есть большая часть вопросов летят window on couch не поддерживаются вот промо кондаков как раз как мотивацию к тому чтобы переделать движок см определился на кольце он как раз рассказывал вчера о том что есть запросы например скс коррелированы про под запросами когда у нас есть под запрос и для того чтобы этот под запрос сделать нам надо полностью сделать уже мо фазу reviews а потом результатов озарит юз должен каким-то образом попасть обратно фазу map и у нас получается такой как бы цикл то есть вот мо придешь сожалений может нам здесь помочь нас такие вот такие запросы не работают их обычно можно переписать в ну либо вас кипели либо их приходится переписывать на уровне предложения то есть разбивать один запрос на парочку на это прямо такой маленький процент спасибо онлайн пока не лидируют по количеству вопросов ребята выходите в эфир хотя бы не пишите в чат мы сможем зачитать сейчас у нас вопрос 1 ряда дальше я вижу руку там здравствуйте спасибо за доклад вопрос такой вот радиус r он совсем один одинокий нету способов его как-то там размножить что будет если он упал они 3 сером может выступать радиусе это тот кто принял запрос то есть в такой самой типичной такой арди пмс подобной схеме у нас например есть джитибиси клиент он подключается к какому-то серверу с данными вот этот сервер с данными и будет для него режиссером если сервер укус данными упал то джитибиси клиент переподключиться к другому серверу этот будет для него есть режиссеры то это не какая то роль вечная для узла это вот под каждый запрос выбирает свой ридус супер еще руки поднимаете да спасибо за доклад вот вопрос такой а если у нас есть какой-то вот join colocation но если у нас одну и ту же табличку надо joy не с разными большими таблицами то есть какой способ в этом случае предпочтительно этот имеет две копии с разными affinity или есть какой-то способ другой еще не дублировать даны если нам нужно сделать одну и ту же таблицу джонни с двумя родной в разных случаях двумя разными таблицами которые тоже вот как-то распределенные и в реке этот не укладываются а 1 ук а укладывается в репликой идеально если первый выкладывается прямую я тогда не города не но да нет и если действительно если в этом и проблема то есть нам придется придумать какую-то модель колокации когда мы к лоцируется вода вокруг от вокруг чего-то одного если мы это сделать не можем ну может быть нам придется выбрать какой то из этих запросов которые нам нравятся меньше который нам жалко меньше и включить его вот в этом режиме которые в конце показывал с низко лакированными joy нами которые нам больше трафика прадед в сети так делают так делают в общем-то достаточно часто мы не то чтобы это рекомендуем на потому что работаю медленную сеть и сеть тут и забивается ну вот как вариант либо да либо действительно просто делать несколько копий таблиц то есть у нас как будто бы есть одна копия всей нашей схемы и там части схемы скала церовани по вот этому полю есть копия части с кем скала церн и поэтому другому полю а ну и при агрегаты конечно то есть у нас может вообще там результат этих дроидов он может храниться вообще в отдельной таблицы и мы можем ее просто на лету как-то обновлять стати ты помнишь что тебе автономно which is вычислитель фоне там выбирает лучший вопрос у нас да да да да да конечно не здесь и следующий рука вот на втором на третьем ряду да спасибо за доклад мы говорили про селекции чтения но совершенно ничего про запись с этим все круто или есть какие-то нюансы но про запись там я в самом начале на самом деле делал дисплей мир что до что и этот от этого ткани на самом деле запись работает в общем то так же как ну в любой распределенную систему то есть есть там куча распределенных протоколов как нуб атак протоколов распределенной репликации которые позволяют там в либо и сид режиме либо там вы венчал consistent режиме нам консистентной sti добиться вот нам их нужно использовать но это в общем там случае с игнайтом например из келий insert просто wrapped тот же протокол который у нас используется там для нескольких записей там для пилинга то есть это хорошо спасибо и опять тот же самый наверное похожий вопрос по нашему склеивать или его можно как-то резервировать или дублировать not он один ну как то хотелось бы чтобы это не было точкой отказа но вот этот на на самом деле да я тоже так коллега сейчас тоже спрашивал у нас режиссер он не существует постоянно то есть это не какая-то роль которую мы на конкретный узел назначили и он всегда так живет режиссер выбирается под запрос и я говорю если у нас джитибиси клиент подключился к какому-то серверу вот этот сервер для него будет радиус то есть к серверу с данными переподключиться к другому другой будет режиссером а сервера у нас резервированный обоим спасибо ребята если вы не поднимите руку прямо сейчас вы можете mills перед задается вопрос круто здрасте такой вопрос мне человек как погрязшему rdbms базы данных не совсем понятно ничего бы я мог использовать apache игнайт например dbms я также могу сделать парте церовани и а подчиняет выглядит как парте церовани из коробки мой дизайн как бы можно было применить то есть какой-то взгляд на это спасибо як я я попробую переформулировать а вы кивните у меня postgres тоже умеет сортироваться так нафига мне apache игнайт есть такие есть такое-то количество распределенных систем которые сами распределенные и они говорят что они вот сиквел движок сделали и говорят мы теперь распределенная сиквел система на самом деле вот в этом в реке у сирии в том как запрос разбивается вот в этом распределенный планировщики в нем вся соль когда я слышу что вот у нас распределенный сиквел но он как бы на каждом узле свой а данные вы сами склеиваете и сами придумываете как потому как потому то все будете делать ну вызывает никому не которую улыбку то есть да это вполне все можно сделать вот все что вот я говорю про склеивать ильмо previews вот это все это можно заходить на приложение вот у вас прям приложение может ходить в несколько шарда впо сга риса и сама там склеивать данный я уверен что у многих так работает это нормально и хорошо что это так работает просто в каких системах то есть из коробки у нас есть еще один вопрос или два поднимите руку кто хочет да спасибо за доклад хотел услышать пару слов про то как можно найти аномалий допустим если у нас есть таблица которая реплицируются меж нескольких баз данных но за счет того что ивент еще не обработался как вы говорили при абсурд это данные туда приходят не полностью то есть если документы достигнуто согласованность как понять что запрос допустим можно за ретро этой серии ли это хорошо вопрос в играйте прямо сейчас нет поддержки изоляции для сиквел то есть изоляция вы знаете сейчас работают там на уровне рядками that то есть ну вот что за комитете получили что в базе была готова то мы прочитаем поэтому в принципе для игнайта но ответ что вот да мы просто живем в изоляции рядками ты-то сиквел у нас получается как бы ну не венчал конце consist от 0 там не стой изоляции которая финальную но которую вы скорее всего ожидаете для того чтобы сделать это хорошо нужен нужен какой-то алгоритм который нам позволит изоляцию сделать для данных в играете начинали делать этот на основе сиси молча вершин конкор ansi control то есть так что каждый запрос он как бы фиксирует версию версию данных на которой он должен выполняться и вот вот данные этой версии они везде живут таким образом мы достигаем изоляции то есть они сохраняются но это до конца доведи но не было она в бете живёт сейчас в исходном коде но как это для продакшена не рекомендовано есть еще вопрос онлайн я читаю николай я не уверен что смогу такой интонация как вы бы это сделали вопрос такой а что насчет распределенных транзакций возможно ты уже частично ответил ему тем не менее они выглядят сложнее в такой схеме я имею ввиду соблюдение требований весит да ну вот вот вот это оно и есть да вот конкретно вы знаете пока не ареале не реализована как реализовать над пилить им весь еси на надо пилить и месси для вашей базы супер последний шанс задать вопрос какой ты запросто был лучшим правда я думаю что вот да я думаю что в вопрос про вестись и я думаю что вот вопрос про транзакций давайте скажем что первые вы часть не онлайновой последние оффлайновый вопрос от пути последнюю fly новый вопрос отлично поздравим победитель этого маленького розыгрыша друзья вы сможете продолжить еще какие-то свои может быть более менее публичные какие вопросы задать онлайн кулуарах который конечно же находится слева при выходе из зала"
}