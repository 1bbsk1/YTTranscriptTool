{
  "video_id": "t-2osArBp-w",
  "channel": "HighLoadChannel",
  "title": "Подготовка данных поиска Яндекса, какую библиотеку и процессы мы сделали / В.Агапитов, К.Хамитов",
  "views": 381,
  "duration": 1247,
  "published": "2023-04-28T06:10:46-07:00",
  "text": "Здравствуйте Уважаемые коллеги Меня зовут Камиль моего коллег зовут Василий Мы из службы подготовки больших данных для аналитических расчетов Яндекс поиска мы расскажем вам про библиотеку и наш процесс построения этих данных и как ко всему этому все пришли собственно небольшой глоссарий перед всем докладом Когда у нас будет звучать такая слово как батч это имеется ввидуума preduce Когда мы будем говорить про стриминг процесс это что-то аналог Кафка стриминга патронтаем разбиваем операции из которой выполняется во время похода При запросе в поиск То есть вы задаете вопрос поиск и внутри какой-то сервис задает запрос каким-то данным которые нужны когда мы говорим всех это страница выдачи результатов поиска всё очень жаль Пейдж В общем рассмотрим вообще что происходит очень упрощённо Когда вы создаете какое-либо запрос вы задаете вопрос про котиков запрос попадает на балансе балансе и сразу же видя вас пишет нам аксо-лок о том что вы зашли вы задали такой-то вопрос У вас был такой юзерагент у вас были такие-то куки вы это вы в Акса слог далее Поэтому запросу все отправляется в компоненту собственно которая будет осуществлять наш поиск это репорт секса ситапер неважно Как называется оно ходит по различной иерархии поисков достает вам результаты по которому вот мы решили что они нужны вам исходя из каких-то алгоритмов ранжирования эти результаты которые мы вам показали они тоже пишутся влог этот блок ответов реконцов Когда наша выдача в том плане что ответы сформированы Они отправляются на рендеринг э-э гендерингляется компоненты она осуществляется по сути делает серверную вёрстку то есть она делает макет того что вам будет показано и вот этот макет и вот приблизительное расположение она тоже пишет влог Таким образом у нас сформировались три серверных блога в хотела на клиент на клиенте она как-то там преобразовалась и тут мы решили кликнуть по какому-нибудь результату мы кликнули на котика сам факт клика улетел в еще в одиннадцатом с другого сервиса Клик Демона и записал факт того что мы прокликали и ваши другие взаимодействия с точки зрения клиента тоже улетели в отдельный Лог все Локи достаточно большие вот клиентского взаимодействия тоже очень большой и все логики К сожалению из-за того что поиск эволюционирует имеет свойство меняться посмотрим как выглядела страница допустим примерно 10 лет назад и как она выглядит сейчас очевидно причём страница на один и тот же запрос там вот про контекстную рекламу видно что у нас больше динамических элементов больше ответов не из поисковой выдачи реклама может располагаться теперь не только в первых там четырёх позициях где-то над выдачей Или под выдачей реклама может располагаться где угодно элементы стали другими также если мы грубо говоря возьмём совершенно другой запрос история про Кафе Пушкин нас появляется там компонент который называется объектный ответ еще более динамичный элемент много кнопочек много сценариев взаимодействия со страницей но много-много мест где мы можем повзаимодействовать по всем этим местам наши аналитики хотят считать метрики хотят узнавать какую ценность та или иная то или иное взаимодействие приносит пользователю при взаимодействии с поиском В общем все это нужно учитывать как раньше вот просто рассматривать страницу что у нас просто есть там 10 результатов какие-то из них рекламные просто там джоннить его к ответу слогами слогами кликов не получается потому что нужно знать относительное расположение нужно понимать а был ли запрос вообще показан потому что с того момента с десятого года собственно появляется запреченные запросы э-э запросы которые показываются пользуются мгновенно и собственно основные блогом из-за этого становится блок серверный вёрстке Это только один из примеров такого мажорного изменения которое было там за последние 5 лет таких мажорных изменений можно перечислить довольно большое количество и посмотрим как вот собственно у нас изменилась логирование допустим что если раньше у нас в старом логировании просто пиков писались какие-то атрибуты и влоги запросов писались другие какие-то заикоженные атрибуты оппозиции и мы по этой позиции могли склеиться то сейчас у нас вся информация на самом деле хранится в этом большом блоке Северске и сохранится в атрибутах кстати нам нужно добавить один новый Lock Join нужно как-то э всё это разнести нужно собственно э всё это э донести до потребителей нашей аналитиков которые пишут расчёты сами логики тоже очень большие там даже Вот рассмотрим старые там 2015 год А вот серверный вёрстке 2,5 ТБ за день аксессулок два тебайта блока ответов поиска там десятки терабайт э-э с различными факторами очень большие данные расчётный метрик затрагивают глубокую историю то есть аналитики могут захотеть пересчитать эксперимент за несколько лет а-а и посмотреть что вы там вот с этой новой метрикой менялась А в этих многих годах вот эти изменения могут быть раскатываться совершенно неравномерно более того если каждый аналитик пишет Работает сырыми логами он к сожалению имеет такой факт то есть человеческий фактор может изобрести свою новую концепцию запроса клика результата и соответственно скоростной аналитикой напишут расчеты про одно и то же совершенно по-разному ну и плюс ещё один и тот же код нам нужно как-то уметь записать запускать и в бачей в стриминге первое решение было принято ещё очень давно что мы из этих данных будем варить агрегаты будем варить агрегаты по пользователю потому что большинство метрик либо сводится к пользовательским либо нужны именно пользоваться по пользовательские метрики агрегатов вычищаем всякую ненужную информацию ненужные куки не нужны допустим дампы ранжирования и так далее а-а и э-э добавляет специальной очистки от отрода и так далее аналитики получают более структурированные данные но также остаётся проблема с тем что даже по этим почищенным данным мы можем считать все как это в лес под дрова поэтому у нас появляется в трусы библиотека агрегации запросов а-а библиотека агрегации запросов по сути даёт тебе не материализованное то представление а поисковый вычет и какие-то это представление независимо от той даты за которую мы запускаем расчёт то есть мы можем видеть расчёт за 2015 2020 и при этом мы будем получать один и тот же набор классов Возможно с разными данными Но грубо говоря принцип работы останется одним и тем же почему мы не можем вместо этой библиотеки Просто каждый раз перекладывать данные потому что у нас данных очень много мы не можем Каждый раз при изменении вёстки допустим запускать миграцию старых данных новые эти классы едины в бачей в стриминге и позволяют там осуществлять в новом пока он в мире э-э опциональные заказы колонок исходя из того что запрашивает пользователей Ну допустим рассмотрим пример Ой простите э простите простите рассматриваем пример если грубо говоря то без этой библиотеки аналитик писал какие-то вот такие костыли э из серии а я посмотрю сначала Если бы у меня расчёт такой-то до какого-то временном интервале находится мы считаем так а иначе так-то то в с помощью этой библиотеки он пишет один единообразный код и запускает его за любую дату чтобы посчитать клики по title выдаче соответственно так как мы знаем вот что Дан входят в термины этой библиотеки Мы можем с помощью него них формировать опциональные заказы колонок то есть читать не всю таблицу наших агрегатов а только выбранную часть и чем экономить цепу пользователи этой библиотеки то есть метки считаются быстрее эксперименты можно запускать быстрее все счастливы результаты экспериментов видны гораздо раньше но проблемы со стримингом всё ещё остаётся как это решали ввели в стриминг режим работает библиотеке мы можем получать те же самые данные из входного потока но джоэн и библиотека не выполняет То есть джоины если вообще Джона выполняется в памяти джое есть ложатся на самого пользователя мы здесь можем получить оперировать точные теми же самыми сущностями библиотеки всем тем же самым ножом лежат точно также можем получить грубо говоря в рам-тайме стримы нужных нам данных получить из них и соответственно посчитать стрим с этими метриками когда мы захотим вдруг этим же кодом проверить что-то в боче мы запускаем на нашей таблице запускаем их по тем же сущностям библиотеки которые мы заказали и получаем результирующую таблицу метрик сравниваем что-нибудь дебажем там выясняем что чего-то не хватает в заказе или там и какие-то сущности недоучитываем и выкатываем там новые релиз изменёнными метриками в стриминг режим А вот почему грубо говоря мы никогда не можем получить расчёты в стриминге дальше расскажет власть так Привет значит как рассказал Камиль наши эмали разработчики смогли использовать библиотеку ралип готовить данные за большую историю прошлое обучать на этих данных модели и собственно класть эти данные панели ближе к runtime где они применяются и все бы хорошо но сами buch расчеты выполняются Достаточно долго А рантаймом часто требуется информация о том что делала пользователей значительно быстрее например вот пользователь зашел на поиск сделал какой-то Запрос к поиску увидел выдачу понял что ему Она не нравится сделал ей запрос И в этот момент поиску было бы хорошо понимать что предыдущий ответ не устроил пользователя и не надо ему эти узлы заново показывать и соответственно Казалось бы давать тогда перейдем к классической лямда архитектуре где у нас данные за историю готовятся в батче свежие данные готовятся каким-нибудь стриминг процессам и поставляются в эмаль модель которая обучилась на данных в батча что тут критично Дело в том что данные на которых применяется ml-модель и данные на которых она обучалась должны быть похожи иначе будет посадка качества модели Можем ли мы сделать так чтобы эти данные были похожи Ну кажется да вот у нас на самом деле процесс похоже но не совсем Дело в том что интерфейс на расчеты в баче и расчетыми различаются если мы говорим про ту же Map или стейтлос операцию тогда они поход можно запустить любую встретилась операцию как Map расчетного периода или же наоборот и все будет работать А вот что касается redews State for операции тут начинается проблемы Давайте посмотрим на них подробнее как нас редис выглядит библиотека ралип он потребляет на вход сразу набор данных за какой-то период времени кладет его оперативную память строит объекты по этим данным делает связи между этими объектами отдает пользовательский код который выполняет расчет если мы попробуем запустить такой в режиме как процесс то мы столкнемся следующие проблемы мы не знаем о том Будет ли по данному показу по данному ключу еще приходить данные или нет то есть вот пользователь зашел на страницу это мы увидели влогах пользователь куда-то кликнул это мы увидели влогах пользователь закрыл вкладку мы этого блогах не видим соответственно единственное На что мы можем рассчитывать что если по какому-то показу достаточно долгое время не приходило никаких событий то скорее всего в дальнейшем они тоже не придут и тут мы получаем тогда задержку В готовке данных но не в единицы секунд которые нужны runtime Окей как же тогда делать процесс но они делаются немножко иначе процесс читает по шортированные очереди для этих людей достает идентификаторы показов по этим идентификаторам ходит в Киева или сторож откуда достает информацию о том что мы видели по данному показу ранее все это представлено в виде стейтов дополняет этот стейт вновь пришедшей информации кладет обратно в Киева сторож и нашим потребителям отправляет в очередь идентификации информацию о том что вот по такому идентификатору показа пришло вот такого типа события с точки зрения пользовательского кода это выглядит следующим образом они считают это очень смотрят на тип события если тип события интересен они обращаются к его или стордж идентификатором показа получает объектную модель выполняют расчеты результат Ну если не интересно такой событие они просто ничего не делают если мы теперь попробуем запустить такой процесс как обычный процесс на маппе идет к власти достаточно большом то он просто положит киевалю storage и надо тут каким-то образом ограничивать число обращений видно что интерфейс на наш редьюс и стоит процесс существенным образом различается Давайте держать Это в уме теперь немного посмотрим на то Как развивались наши стриминг процессы то есть чего Мы начинали в компании наш тренинг процесс Какие проблемы мы видели как эти проблемы пробовали решать и какому решению пришли на данный момент изначально у нас было Два кластера один продакшн другой резервный на случай выхода из строя Production кластера или же каких-то работ в DC и он находится процессы на этих мастерах были полностью аналогичными данная на вход поступали точно такие же Но поскольку данные не были синхронизированы между кластерами то Возникала проблема в случае переключения из продакшн кластера на резервный и обратно рантай мы могли Как получать дубли в данных так и не получать какие-то данные то есть иметь потери Теперь давайте посмотрим чуть Подробнее Как сами процессы выглядели на этих кластерах наша часть это были обычные стейтс операции которые готовили данные а вся бизнес-логика жила в процессах наших потребителей мы к такому пришли взяв за основу нашей процессы и просто перенеся схему как есть в стриминг режим надеюсь на то что все заработает пришли мы в этом случае к тому что данные в нашем батч и steaming режимах были идентичны Нет не пришли а дело в том что процессы которые эти данные готовят отличались а именно в патч режиме у нас потребители готовили данные используя библиотеку ралип в стриминг режимах были варианты это как вариант использования библиотеки раллип запущенное На некотором окне событий так просто какие-то функции из этой библиотеки или же вообще код наших пользователей которые библиотеки рарит не имел никакого отношения Кроме того эти процессы могли Как сохранять свои промежуточные стейты в Киева или сторож так и не делать этого естественно не о каком глобальном изменении бизнес-слойки единообразным говорить не приходилось а соответственно первое наша попытка сделать данные единообразными была следующая во-первых Мы перешли от двух кластеров к одному между cкластеру Который жил в трех локациях и в случае работ в какой-то одной из локаций он замечательно продолжал работать переживая Это совершенно прозрачным образом для потребителей во-вторых мы стали готовить данные для нашего батч режима в стриминг режиме это позволило нам при изменении В готовке данных перелизах и при каких-то переключениях на разных источниках данных делать это синхронно и данные конечно стали ближе в стриминг и мы вынесли самое важное Как нам казалось джойн и самая крупная из ралиба в виде отдельных процессов наш стриминг режим это конечно привело к тому что данные стали похожи Но вот до конца вот эти вот данные не стали совпадать и мы стали задумываться А почему вот такое происходит Дело в том что мы не могли целиком взять и переложить библиотеку рамип в наш стильный процесс и класть стейт целиком в сторону потому что он получался слишком большим слишком большим он получался по той причине что вот у нас есть Лог реканс запросов и ответов поиска и там кроме запроса и набора ответов есть довольно подробное описание Почему поиск считает что данный ответ стоило показать он настолько подробный Что снимает до 70 процентов от всего объема Лога вторая интересная особенность этих данных то что она нужна относительно малому проценту потребителей и только в матч режиме поэтому мы сделали следующее мы разделили Лог реканс на две части первая часть маленькая и там лежат данные которые нужны для всех наших потребителей хоть стриминг хоть режимах а вторая часть большая и Расскажу чуть подробнее дальше и по данным мы наконец смогли готовить единые профили показа которые доставляется до всех наших потребителей то есть данная наконец-таки в тех наших потребителей стали доходить единообразно сама библиотека ралип из пользовательского кода переехала на этап готовки данных но пользовательском коде она сохранилась для того чтобы те объекты уже новые которые мы предоставляем пользователям представлять в виде понятных привычных им классов библиотекера лип чтобы пришлось переписывать код что касается потребителей которым необходимы были данные из тяжелой части нашего лого они это тяжелая часть Лога уехала в исключительно бач режим и эти потребители просто дополняют профиль показа этими данными в тех процессах это необходимо не сложно видеть что данная схема отличается как от классической карпа архитектуры где данные готовятся в стриминг режиме целиком так от классической лямбда архитектуры где у нас есть некоторые runtime который для которого мы готовим данные как стриминг так и в режимах То есть у нас скорее вывернутая схема где есть набор бачек процессов для которых мы готовим данные стриминг и бачрежимах скорее всего вас по данному нашим решениям будут какие-то существенные вопросы или вы даже предложите какие-то свои варианты этих решений мы с радостью выслушаем ваши вопросы ответим на них и обсудим ваше решение Спасибо за внимание Может у кого-то есть вопросы это все было совсем непонятно или все абсолютно понятно Спасибо Здорово"
}