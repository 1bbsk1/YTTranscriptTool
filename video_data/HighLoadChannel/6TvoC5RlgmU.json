{
  "video_id": "6TvoC5RlgmU",
  "channel": "HighLoadChannel",
  "title": "Как мы Change Data Capture делали / Василий Тюбек (Mail.ru Group), Александр Деулин (Nexign)",
  "views": 1247,
  "duration": 3056,
  "published": "2021-10-04T02:36:56-07:00",
  "text": "я приглашаю на сцену 2 докладчиков с докладом как мы чем ждать коптер делали вы видите это на экране также на сцену василий тигр и александр доволен пожалуйста прошу рассказать коллеги привет начнём немножко с рекламы тарантула у нас собрана и все материалы по докладам вот поэтому qr коду вот меня зовут александр дюмин компания nexen я тимлид команды фабрики микро сервисов мы разрабатываем решение подключен на базе микро сероватой архитектура для целиком оператора для компании мегафон доклад у нас совместный мы давно работаем фактически как одна команда уже около трех лет с командой mail.ru вот но и расскажем собственно сегодня про то как мы вместе чинче to capture сделали этот доклад фактически продолжение предыдущего доклада где мы взяли в тему вширь рассказали каким образом появился появились микро сервисы и пиши ландшафте телеком оператора зачем это вообще нам была нужна ну а сегодня продолжение предыдущей темы и мы капнем вглубь как видите архитектура микро сервисного слоем как виктора некрасова достаточно многослойная и очень важную роль в ней играет слой кашей для чего он нужен но он прежде всего нужен для того чтобы готовить удобно данное делать нужные представления для наших каналов мы поднимаем дать достаточно большой слой данных и score систем в каше и дальше уже делаем нужные композиты и отдаем уже быстро и с хорошим в этом все собственно в конце концов канал и наши данные вторая проблема которая решает слой к шее это изоляция данных совсем недавно у нас появились два высоконагруженных очень требовательных потребителей это триггер и от системы к processing где мы в пике видим четыре с половиной миллиона событий в секунду часть триггера входят в эти каши их arrows engine от проекта real-time маркетинг вот которые выставляют требования на в 50 миллисекунд лейтон сына композитных запросах и просит 30к рпс получить так так так же но без деградации сервисов вначале мы делали достаточно простые вещи с кашами мы либо просто забирали каким-то условиям из master system данные поднимали их тарантул либо так же подписывались на какие-то события в любом брокере сообщение да и так и у нас еще работал над налаженный процесс поставки данных поставки данных из энергетических хранилищ все это достаточно было просто типовые решения раскатывали там в течение недели-двух мы получали результат но все все было прекрасно до тех пор пока не случился упс вам потребовалось сделать систему данных над продуктом да извините за тавтологию правда product инвентарь и это такая master system в которой лежат все данные по подключенным продуктам и услугам абонент в компании что у нас в мастер системе на входе ворог ли лежала около 30 терабайт сырых данных от трех до пяти ктп с изменения в секунду и самое интересное что эти данные кроме как и 40 невозможно было забрать изменение происходили президент изменения клиенты написали напрямую в базу задача была в следующем создать провокационно целостную витрину и обеспечить усилий по задержке не более 30 секунд задержка репликации от master system и ну и понятно что в конечно нужно было поднять не все данные нам нужно было фильтрация ну и где-то с 30-ти работ нам нужно было взять где-то 1 терабайт ну и кэш достаточно часто обновлялся ну собственно мы приступили к задачей сели там понаписали себе запросов разных да и пошли на нагрузочный стенд чтобы погреть кэш посмотреть как это происходит в результате мы прогрели oracle oracle стало плохо в это большая нам в про образная система вот не любит он запросы свой а вот ну и время в общем то там прогрева нашего каша оказалась порядка двух недель то есть к моменту загрузки мы уже моменту прогрева мы бы получили уже там полный полный неактуальный средств данных ну и плюсом к этому до отсутствия отсутствие любого брокера но нас тоже немножко остановила мы начали думать как каким образом нам делать решение сначала мы вообще думали может быть мы откажемся от каша давайте поднимем еще один с тумбой почитаем данные напрямую 40-го первое не получилось потому что данное разложено по разным табличкам нам нужно joy нить а как я уже говорил у нас 50 миллисекунд на композитный запрос то есть нам нужно еще до богатеть эти данные чем-то вот и отдать за 50 миллисекунд 40 а у нас это не получилось ну и второе собственном во все банально просто система большая значит много ядер и получается что мы увеличиваем лицензионную нагрузку то есть нам нужно нужен еще бюджет на то чтобы за лицензировать эти дополнительные ядра отказались от этого решения пошли смотреть бесплатные версии того как мы можем получить поток изменений и оказывается у раков сделал такую технологий она называется континиус коварен notification и вначале у нас все прекрасно работала когда мы проводили тесты на функциональных зонах когда у нас не было большой нагрузке секрет при прекрасно справлялся кратко в общем то суть его заключается в том что в рок ли есть тоже внутреннюю очередь и на эту очередь можно тут можно подписаться на модификаций которые происходят в базе когда мы вышли на нагрузочный стенд в общем то обнаружили интересную историю про то что про то что профи цианирование таблицы изменения на протекционизм в таблицах не отлавливаются секретом если вы закрываете одним кометам более 100 изменений мы сразу получаем историю с разрушением традиционной целостности она не поддерживается то ли это баг или фича не знаем открывали free крест в oracle собственно там нам ничего хорошего не предложили мы на этом закончили потому что там бесплатные бесплатный сыр только в мышеловке не хочется наступить на какие-то грабли непонятные напротив ну и тут на сцену вышел наш старый добрый golden gate казалось бы где микро сервисы модно молодежно да и старый добрый golden gate но оказалось что в общем то нам придется нарастить экспертизу и погрузиться в эту технологию потому что только с помощью goldeen где-то мы решили эту задачу бы обеспечивать традиционную целостность мы получили нормальную поддержку и тель процессы где можно по фильтровать по трансформировать данные мы сделали в общем-то кэш самым малоинвазивным способом мы не да не давали никакой нагрузки на базу не вмешивались работу мастер системы но и плюсом получили экспертизу воде берег потом оказалось что в общем-то это решение стало масштабируемым потому что кроме вот первый и первый мастер системы у нас еще появилось еще появилась задача сделать 22 кэша в разных предметных областях и в общем то на шаркал golden gate работает теперь нам наполняет каши для логически разных систем мы подключаемся к разным базам и делаем в общем то но захватываем изменение легко и непринужденно ну для дополнительных потребителей но все же любят работать с данными этот поток изменений интересен кроме нас интересен еще многим потребителям мы еще плюсом бонусом сделал еще одну точку в кафку вот и теперь у нас такой вот получился универсальное слой сидиси да где есть 40 of golden gate который в общем-то поддерживает загрузку данных тарантул вот и есть еще кафка которая для для тех потребителей кто не может подключиться кто кто кому интересно и к кому интересен этот поток также вот первый поток подход снаряду у нас был в общем то через выгрузку csv-файлы файлов мы решили проблему с нагрузкой все таки вот начали сделали по простому грузии или из оракла в csv и погружали ладдер погрузил все это все это в тарантул и репликацию настроили виде значит преобразование трое файлов xml формат вот второй вариант у нас получился более такой технологичная скажу расскажу расскажу более более промышленные которые мы сейчас везде используем это прогрев каша напрямую из оракла и использование процедуры из exid это фактически . кастомизации в голдэн гете вот на практике ничего сложного с голодным детям нет надо просто почитать документацию пообщаться с baby i'm и в общем то вперед у вас все получится вот дальше про мясо рассказываешь расскажет василий тебе команда mail.ru вася тебе слово отлично слышно так коллеги меня зовут васили тюбик я расскажу про технические аспекты реализации этой штуки во первых с самого начала обычно представляет из себя что обычно кэш работает от запроса то есть у нас есть какое-то приложение которое сначала пытается достать данные спеша затем а если не находит бы ты смог сходить в мастер систему болот класс складывает данные обратно в кэш и уже от этого работает в нашем случае историю слегка отличается на то есть вот этот концепция каша сбоку так называемого когда мы не переделываем уже существующие приложения которые ходят в мастер систему oracle существующей а всем новые приложения общаются столько спешим на достают данные только оттуда соответственно что в этом случае представляет из себя работающий кэш он прогрет на то есть он заранее загружен данными эти данные актуальны и актуальность этих 9 данных поддерживается регулярно причем поддерживается за счет репликации из мастер системы ну и он обвешен всякими разными характеристиками типа он абдирова был maintain a ball потому что нам нужно понимать до работает он актуальные данные в нем насколько он сильно отстал от master system и вот это все поэтому по порядочку первый наш доход был прогрев с выгрузкой csv на то есть что нам нужно было да то есть требования к этой выгрузки они довольно просты и во-первых нам нужен только срез данных исключительно да то есть не весь 30 терабайтный набор а только его часть во-вторых мы должны были в процессе выгрузки данных из ого класса сдавать на него минимальную нагрузку и в-третьих выпуска должна была происходить за какую-то адекватное время на то есть там например время две недели явно неадекватная такой куш никому просто не нужен первое что мы пытались сделать да это выгрузки по условию выгрузки по условиям просто не сработали то есть мы получили время вы выборок такое что и нагрузку еще на исходную базу такую что это просто неприемлемо история что еще нам эта штука обломала она нам обломала возможность раз параллели со по условиям на то есть если бы мы хотели выбрать вот из этого 30 ты работнова массива на данные кусками да чтоб например ускорить выгрузку мы могли сделать мы могли данные починкой да например пользователи выгружать там по тысяче но здесь мы этого сделать не можем потому что условия просто тормозят что нас здесь спасла нас спасло парте цианирования на то есть исходные таблицу исходные данные были парте цианирования причем позиционирование довольно хорошо то есть несколько сотен прямо портится до которой мы начали в общем-то загружать целиком на соответственно мы выгружаем каждую партицию параллели но соответственно единственное что здесь важно важно ограничить количество этих процессов которые в параллель выгружают партиций потому что банальные смотал банил выгрузку в 100 потоков мы вытер 100 потоков нагрузим баз данных источник и второй момент мы по-прежнему здесь работали без условиях на то есть мы выгружаем все данные которые у нас портится их лежат на круг мы получили вот такой вот pipeline на то есть первым шагом для выгрузки мы проходимся по всем сучков карте всех таблиц партиции sap партиций которого у нас есть в базе данных источники из этих данных мы создаем пачку искали запросов каждый из которых вот простой select на либо всего либо набор полей и конкретную партицию дальше мы набор запросов скармливаем никому процесса экспортеру который уже начинает там несколько потоков выгружать данные в csv файла почему все свои файлы ну потому что их довольно удобно данные этапе отладки гонять ты один раз выгрузился дальше ты их можешь загружать на экспортировать и импортировать делай с ними что хочешь в итоге чем получили чтобы выгрузить данные из оракла быстро без нагрузки во первых никаких фаеров вообще надпись только на загрузке фильтры во вторых должно быть адекватно и парте цианирование на самой базе данных источники на то есть если мы мы сами сталкивались на самом деле у нас была табличка в 2 партиции буквально то есть причем одна partition была маленькая 2 на несколько ярдов записей ну понятно да выгружали мы это долгое важное значение имеют всякие на строчки урока на в нашем случае нам помог пресечь при подключении тоже такие детали непосредственно подключение к уроку но тоже важный момент и для себя мы ограничили количество потоков на выгрузку в штатном режиме это 10 если мы грузимся экстремально экстренно максимум 30 для нас вот такие цифры сработали уроков мы не нагибаем мы получили примере таких вот водных выгрузку за шесть часов в зависимости от того с какой таблицу мы работаем от 60 до 700 тысяч скорость выгрузки в cs вышки и это одна целая две десятых терабайта сырых данных csv на выходе собственно данные выгрузили теперь нужно их загрузить здесь что важно здесь важно учитывать модель хранения этих самых данных с которыми мы работаем во первых в нашем случае так как данных многом и сразу из коробки приняли решение что будем сердиться на соответственно у нас данные локализованы по каждому абоненту и шарди раваны то есть на каждом шар не лежат лежит набор данных того пользователям которого там лежит не смешаны в кучу что важно важно дальше сама модель данных есть абонент на у которого есть вот эти желтые карманы это платные услуги его подключенные и синий круг маны это все что как-то привязана к этим платным услугам особенность в чем самих платных услуг там 4 ярда записи дополнительных данных синих кружочков 12 на то есть 13 м в чем особенность особенность в том что когда мы вставляем платные когда мы пытаемся загружать уже данные платных услуг в наш кэш мы точно знаем в какой шар то не кладутся у каждой платные услуги есть поле в котором написано сабскрайберами такой-то на соответственно можем сразу вычислить нужный чарты отправить данные туда все остальные данные приходится загружать с поиском а то есть у них нет низ адская верой дени они только знают о своей платные услуги соответственно чтобы анны загрузить нам нужно сначала найти к чему их привязать на соответственно вся загрузка в этом случае превращается в мо придешь на поиск шарда в которой нам нужно положить дополнительные данные и insert в один соответственно в чем вот этот самый момент да где мы начинаем уже грузить с вышки где вот все это происходит у него есть вот такая вот особенность на то есть мы грузим данные поэтапно сначала мы загружаем основной массив данных затем все остальное к чему идти далее привязываются уже в общем то такая вот особенность спас моделью данных нужно считаться для нас это что во что вылилось для нас то вылилось то что у нас данные продуктов подключенных загружаются буквально за час с копейками на то есть не просто взлет 400 гектар все остальные данные загружаются еще 11 часов просто потому что мы их сначала ищем потом загружаем вот это все дальше и вот здесь уже как раз появляется golden gate с чем ждет коптером данные загрузили нужно поддерживать как-то актуальность сам golden gate сам из коробки довольно простая штука условно простая если в квадрате х кружочках его рисовать вот зеленый квадрат по середине этого долги его задача реплицировать данные из базы данных источниками 40 db в таргет db применяя какие-то там провел и конфигурации важно нам что было важно что она умеет работать с разными топология my на то есть она умеет агрегировать данные в одну банку разливать данные по нескольким банкам в общем все что надо и поддерживает семантику и отеля до то есть на каждом этапе можем объявить какие-то фильтры трансформации сказать что нам эти данные не нужны а вот те данные нам нужно преобразовать во что-то другое нас это несколько раз вытаскивала особенно при работе с датами на то есть при проведении дат которые горок и более в приведении до от которого у нас подуем в tarantul на и кэш и работает это все довольно просто на на базе данных источники происходит какая-то транзакция происходит commit дальше молдинги этого и процесс extractor это транзакцию ловит и складывает получившиеся данные в файл вью файл что это такое это транзакции но то есть как его golden gate ведет для себя то есть insert и долетая blade и по порядочку как они происходили на мастер системе ну этих файлов может быть много да то есть по мере там добавление transition не просто дописываются от нуля двоен дальше golden gate новый процесс replicate эти 3 файлы слушают на соответственно учитывает из них эти транзакции и накатывает их на базу данных целевую позже собственно простая довольно штука для чего важно знать реплицировать данные позволяют трансформировать и самое важное для нас штука как оказалось это он реплицировать только за комичными транзакции не за комичные транзакций мы просто не увидим из коробки сам по себе golden gate из коробки умеют oracle oracle для того чтобы запилить штатов тарантул мы немножечко извернулись на первый заход мы сделали вот такую штуку на то есть был golden gate как он есть зеленый квадрат банка источник банка целевая мы половину golden gate а по сути отрезали на ту часть которая накатывает изменения на целевую базу и запилили ее собственную на таран туле на своею под под себя что называется на и вот это вот красный квадрат тарантино репликатор это вот та часть которая заменяет golden gate новый процесс replicate и пишут в общем-то в целевую базу она же тарантул это сработало потому что троил и в которое пишется лог транзакции могут быть в нескольких основных форматах из коробки это бинарный проприетарный формат какой-то аскиз кролик есть и для нас сработал xml xml сам по себе выглядит вот так на то есть от просто список до бэббит есть данные есть мета-данные есть понимание что это за то близких за таблицы какого типа операции прилетели какие-то данные то или после транзакции сами данные выглядят довольно просто то есть мы знаем что это за колонка какое значение в метаданных мы можем увидеть всякие дополнительные прикольные штуки например время к метро на то есть когда произошел commit на базе данных источники все это на этом можно заканчивать да то есть мы просто написали xml парсер который разбирает троил файлы в сомали и накатывает это все на тарантул было бы сильно просто если бы было так просто чем получили на первый заход на первый заход мы получили загрузку данных из рисуя то есть заливы сливаем zeiss мышки загружаемая слышать мы получили репликацию через xml просто написали xml парсер все круто нам нужен смотри сами в этот xml начеку видеть что там происходило понять почему у нас выше такие данные теперь второй раунд это было у нас первый заход на 1 кэш дальше у нас появилась к 100 метров индекс так называемые системы специализированный индекс для поиска абонентов тут мы решили подумать чем можно сделать лучшее из того что у нас есть потому что у нас было несколько нюансов во первых во первых нам не нравилось вот этот machinery загрузкой на 3 ст вот эти хореографии с валиками файлики выгрузив файлики загрузи файлики там сохрани и так далее во вторых мы написали половину golden gate то есть в буквальном смысле мы остановили там ту часть которая на golden gate может на трейлах и остальную половину мы просто написали его ручками самостоятельно чу это означает это означает что мы часть функциональных возможностей априори потеряли потому что голый греет этот матерый продукт который развивается очень давно мы наступили на всякие разнообразные прикольные штуки связанные с самим форматом xml на то есть например вот так вот выглядит поддержки с лобов на то есть блобов соловов xml адрес прямо посередь xml golden gate фига чивает нам ошибку что он дед не поддерживает все мы даже это распарсить не можем мы еще наступили на всякие против ские ошибки да то есть все равно это файлик который нужно по хитрому читать да вот так вот это это лак на репликацию вот так выглядел наш первый подход к снаряду то есть красная планка это наше сало и видно да то есть каждый раз если там посмотреть на время это ночь каждый раз в ночь мы это сало и пробивали в чем был прикол на причем до ближе к выходным да то есть тренд такой что чем ближе к выходным тем выше мы пробиваем тем больше у нас лак репликации в чем подвох был подвох был в том что мы когда данные из xml и читаем их читаем пачками по 1000 записей пока пищу записи не наберём мы не отправляем их дальше в processing соответственно ночью или ближе к выходным нагрузка с базы источника спадают им априори автоматически получаем лак выше чем могли бы это тоже нужно закончить на то есть мы похудели мы сделали да но с этим тоже нужно считаться это тоже нужно иметь ввиду что есть вот такие штуки от этого всего хотелось избавиться переложить это все на golden gate ну их последний момент наверное самые важные во всей этой истории мы сломали эксплуатационные домино то есть есть 2 есть специалисты по golden gate у которые поддерживали каждый свой загончик и тут пришли мы мы пересеклись с golden gate чеками начиная от настроек операционной системы прямо от создания образов учетные записи безопасность на и те и те должны иметь доступ к одним и тем же файликом одни должны писать другие считать короче в общем этого всего хотелось избежать с прогревом версии 2 подходить было довольно просто да то есть мы просто вырезали файлики а мы оставили исходную часть как она есть потому что спарте цами работать так и так приходится вот так на свой ответ но он здесь место файликов мы сделали и сквозь то есть у нас была очередь загрузки файлов а стала шире загрузка и стоит ограничение все те же самые на то есть порядок партийцы здесь ничего нового с самим golden gate а вот здесь мы зашли немного серьезнее а мы запилили юзер exit так называемый чуть такое из коробки golden gate выглядит вот так первой итерации мы обрезали половину во второй мы сделали вот так вот мы его дополнили golden gate позволяет расширить позволяет расширять за счет использования зиракс чуть такое это просто смешная библиотека которую мы можем в конфигурации golden gate у сказать использую юзер exit вот такой то и путь доставки там в сушке мы можем под закончит свое нося на где мы можем получать транзакции которые golden gate обрабатывать как ты их по-своему обрабатывать пусть чугуна делать мы сделали вот так вот то есть мы перекрыли основной поток нас на на целевую базу надо сказать что-то не надо туда писать чуваки и развернули собственно tarantul ным клиентам запись непосредственно в tarantul ные каши на то есть здесь тоже в целом ничего сложного есть нюансы там скандирование насер там и так далее в общем все укладывается в целом доки расскажу немного про то как мы с этим совсем боролись за наши силы как мы обслуживание этого всего налаживали в общем все это самая интересная часть дома первый заход на репликацию у нас выглядел вот так это графика логарифмы кации в секундах на соответственно там вот видно что слева это уже минуты с олей планка красная внизу вот так выглядел наш первый заход такая вот пила медленно нарастает и потом резко уходит вниз до нужной на цифры в чем проблема проблема в том что есть оракул исходный к нему golden gate мы подключили есть такое понятие интегрированный экстракт так называемый с точки зрения настройки golden gate а это означает что база данных источник помимо того что она занимается тем чем она должна заниматься на накатывает транзакции так далее начинает работать лук майнером то есть оракал база данных сам по себе ведет свой блог транзакции и вот когда мы подключаем golden gate токио федерации интегр интегрированного экстракта какой мы его подключили мы получаем что база данных источник начинает сама разбирать свои логе подготавливаете для уроков google и гейта короче мы его опять начали гореть чтобы это обойти надо соответственно чтобы этого не происходило мы получили потока архивных логов то есть просто 30 минут условно нам отстреливается новый лог на то есть 30 минут мы сидим курим бамбук ничего не делаем ждем пока к нам подъедет этот влог пока начнут разбираться в общем чтобы не грузить основной базу данных так жестоко обходится это довольно просто для того чтобы не греть базу данных источник можно поставить отдельную грелку на то есть на которого мы сливаем этиологию и где мы их парсинг то есть по сути мы проблему с циpкa на базе данных источники меняем на проблему с сетью на то есть если у нас терабайт изменений в сутки значит этот терабайт изменений сутки теперь нужно будет прогонять по сети на базу данных майнинг она уже там как то это дело обработает в общем как оказалось проблемы с сетью решить гораздо проще на то есть инфраструктур щеки и решили за нас делать там ничего не пришлось за счет того что мы с архивных логов перепрыгнули наряду vogue так называемый в этом режиме мы получили вот такой вот лак на то есть 18 секунд это полка в штатном режиме которую мы видим в обычный день не нагруженный день она еще меньше бонусом бонусом мы получили майнинг работает как слой изоляции но то есть если в конфигурации подключение golden gate а напрямую к базе данных oracle случае если урока отвалится мы получим собственно говоря какой-то там эксплуатационный сценарий на восстановление этого села в случае если у нас стоит майнинг перед нами маленькой томат может выступать слоем изоляции у нас есть основной источник есть резерв случае если основной источник происходит свечой или failover мы за счет того что перед нами станет майнинг мы даже не чу нужны такой хороший финт ушами в общем за счет вот так вот небольшого find a важный момент в этом все истории в чем вот этот база данных майнинг это не реплика да то есть это числа грыз который не хранит полную копию данных он просто получает logis базы данных источников разбирает их и отправляет golden gate все вот это важно потому что иначе бы мы залипли на те же самые терабайт которая у нас есть вывод из этого всего по умолчанию если вам не нужно сильно быстро реал тайме можно работать на архивах и вот это вот этого всего можно избежать если нужно как в нашем случае это вот так вот можно сделать и тогда мы будем получать реальное время среду лагами все хорошо дальше дальше вот такая вот фигня 21 год сказалось бы на вроде уже решено несколько раз нет а когда мы делали все на икса 0 все было очень просто мы в golden gate и в конфигурации говорим in котику tf и забываем как страшный сон получаем получаем просто нормальной валидный xml 100 f am в случае с юзеры axe там с golden gate of golden gate сам может конвертировать из кодировки в кодировку на то есть он детектировать что кодировка источника и целевой базы расходятся значит мы ее с конвертом есть несколько ограничений по этому поводу например что там целевая кодировка не может быть уже по символизму составу чем исходная на то есть ну например цепи 1251 можем screen вернуть вот f8 без потерь utf-8 обратно в степи 1251 уже нет нюанс на в нашем сочетание версией golden gate оракла это не сработало у нас появились боги нам пришлось скатиться в историю сайгон вам в этом случае нам повезло да то есть у нас на тарантула есть встроенный модуль айкон в то есть мы что делаем с базы данных источника через golden gate мы получаем данные как они есть той кодировке которые они есть конвертируем уже у себя а то есть вопрос закрыли и того данные загрузили real-time сделали проблему с целостностью данных порешали выводы на этот момент xml самый простой вариант бороться с кодировками юзер exit конвертирует но не без нюансов айкон надежный вариант тогда когда iconv работает на то есть aig он запросто может там за 20 лет работы базы данных в энтерпрайзе там скапливается много всего неожиданного мы пару раз выхватывали что а iconv у нас просто падает то есть это нужно просто предусматривать и один наверное самых сложных моментов failover всей этой машины rain сам по себе кэш само по себе витрина по его верится из коробки на то есть у нас вот эти серые квадраты это допустим 20 до две стойки там две машины как удобно есть мастер часть каша зелененький квадрат и есть реплика то есть синенький квадратик большой соответственно файла говорится все у нас там автоматически написано это все на тарантул картриджи у него есть встроенный механизм переключения мастеров реплика общем все это работает базы данных источники тоже работают потому что то она стоит рык кластер как там сидит свечу рифаил улицу и вообще его в области ответственности дебилов мы об этом ещё не знаем вот эта вот часть самое интересное первое что мы сделали само собой мы поставили просто резерв этого всего и забили потоки репликации из источников вот так вот крест на крест в оба майнинга чтобы каждый майнинг в любом случае мог получать и с основного оракла из резервного оракалом данные дальше вопрос чудилось когда это все падает например там падает майнинг или падает golden gate так как все это держится на файликах на то есть trail ее логе транзакции вот это все это есть штатный вариант да это подключиться ходы между основным и резервным судом то есть репликация дисковых массивов и все нюансы этого всей истории в чем у нас с аллой по-прежнему должен был оставаться 30 секунд цеха да сама по себе а штатное переключение во первых 30 минут занимает заявленное время во вторых это процедура чуть ли не пол и ручная на то есть идем от монтируем примонтируем подключаем запускаем так далее она как получаем все описала и забыли кэш уже не актуальны чем сделали мы перевели проблему собственно из вышестоящих компонентов на себя мы сделали два активных потока репликации на то есть в каждый конкретный момент времени и майнинг и golden gate в обоих судах работают если в случае с и ходы на в случае сходи работает только один при переключении 2 чается в нашем случае работают оба разница в чем разница в том что работает только зелененькая часть на то есть у нас в кэше на запись доступен мастер соответственно в мастер мы пишем в реплику мы получаем оббивку с мастера нам поступают данные о тех транзакциях и стенах которые мы уже отработали на для того чтоб реплика могла резервному потоку golden gate и отвечать что да там вот эта часть отработали дальше все очень просто мы для триггеров и а вера мы переключаем мастер на своей стороне в каше витрине на соответственно таким образом у нас одна половина за становится доступной на запись на второй половине у нас запись перекрывается автоматически мы получаем переключение потока репликации то есть там где краник был мы его закрутили открутили в соседнем судят автоматически вода полилась история довольно сложные с точки зрения реализации но она во первых дешевле за счет исключения из хода репликации дисковых массивов этого всего во вторых она она срабатывает в рамках снг то есть переключение мастера на крыше у нас происходит в рамках 5 или 10 секунд все и дальше мужа дальше работаем своего миром как-то так дела обстоят то есть в рамках с алоэ решили единственное что мы его вот так вот сами сделали каким-то нештатные механизме но она автоматическая быстрой и последнее наверное чего хочу рассказать это про эксплуатацию коротенько важный момент во всей этой машине в том что у этой штуки обратной связи нет никакой на то есть транзакция произошла на одном конце на другом конце а она накатилась и как-то там она через что-то пробежала обратной связи нет то есть мы из тарантула не подключены вокал не проверяем ничего что это означает это означает что если например происходит разрыв где-нибудь между golden gate а мы тарантулом для нас для каша для тарантула это будет неотличимы от отсутствия транзакции на то есть транзакции нет сеть порвала одна и та же история соответственно мы не узнаем до что нужно предпринимать чет переключить на а нам это важно чтобы сохранить наши силы чем сделали мы включили heart beat rap beat транзакции это синтетические транзакции который раз в минуту бегает на в какую-то левую табличку которая так и называется херби соответственно туда вставляется запись важно что она происходит раз в минуту важно что там есть какой-то to my time stamp из которого мы можем понять то что теперь мы можем на эту штуку положиться мы знаем что раз в минуту у нас пробегает какая транзакция и в общем то от этого мы можем уже смотреть мониторинг да то есть если мы видим что последнее было две минуты назад значит у нас чуть не так соответственно и последний момент это мониторинг intent лога рипли кации то есть как она там произошло на источники и до кыша как она доехала это важно в первую очередь для потребителей в кэше потому что они могут полагаться на это время здесь все очень просто кусок максимального отрыва то есть те же самые данные убегают через визируется мы знаем время коммита на базе данных источники соответственно для того чтобы посчитать лак нам достаточно просто из времени здесь сейчас у себя витрине вычесть время коммита на базе данных источники и все теперь мы знаем лак собственно касательно мониторинга и наверное вот две самые важные вещи на то есть можно там обложиться разные всякие метриками лагами и так далее но две штуки обеспечить непрерывность на которую можно положиться на 1 в какой-то интервал времени и лак репликаций мониторить причем лак рипли кации мониторить не на каждом там участке до and winter креплю кации итого технически важно при построении таких штук держать в уме характеристики источника если у вас нет парте цианирования там еще какие-то ограничения на это можно запросто наступить важно что именно да какую именно витрину мы строим от этого может страдать там время загрузки или прямо во время создания витрины вот эти нюансы xml с gold англии там легальны и самый простой способ на то есть проще просто не тут берем пишем парсер xml сделать репликацию куда угодно по сути юзер exit способ более промышленный но он же и более сложный в общем-то и последний момент важно эксплуатация то есть вот эти вот несколько моментов прокату и рассказал на них нужно обращать внимание на что витрины работали все по технической части sage подведи итог что мы получили на круг 1 раз но в итоге наверное можно сделать такой вывод что если у вас есть большой монолита вы не знаете как к нему подступиться поставьте кэшбэка это новый архитектурный поттер вот и меньше страдаете получается удовольствие цен спасибо коллеги диски кракен вопросы вопросы есть это хорошо добрый день у меня такой вопрос не зовут анна и прах орбиты интересная технология на самом деле и у меня такой вопрос существует то есть два параллельных каких-то процесса inserto до 1 процесс это идет вставка через вот эти три ресурса как вы показали схемку в конце автор антон ложится и параллельна параллельна получается фиксируется какая-то транзакция раз в минуту вот соответственно вопрос может ли получиться так что первая транзакция где-то провалилась то есть не прошла полностью по схеме до тарантула при этом фиксация in sort of прошло корректно то есть мы со своей стороны считаем что у нас все в порядке insert и каждую минуту про продолжается но фактически данные валяться где-то там по каким-то причинам спасибо за смотреть и целостность транзакции которая происходит на базе данных источник и их согласованность и вообще что там происходит это задача непосредственно приложение от того которое из транзакции там поворачивает соответственно если она failed to the она будет вылетит для клиента то есть эти данные нам априори в кэше точно не нужны а потому что клиент точно так же увидит зафейлился транзакцию это я думаю что должны мониторить гибели уже на тебе разработчики приложений которые oracle пишут до нас важно что эти штуки не доезжают до то есть чтоб там не взорвалось мы получаем только валидные за комичные в общем как то так но и мы наверное в каком-то горизонте времени все таки получим деградации по хирурги там соответственно тоже поднимем alert а есть еще вопросы да есть вопрос александр васильев спасибо большое за презентацию я хоть астана что так лучше вопрос наира больше к александру каким но поскольку кашу нас не резиновый на каким образом кэш чистится и как это повлияло на техническую составляющую хороший вопрос спасибо не чистите столько же говоря бревна на кино смотрите из реально реально я читаю вина была было миграцию на базе данных источники которые поднимала все данные вообще на что начиная там с начала времен на соответственно вот это все скопом начали получать и мы запилили процедура очистки которые вот так вот раз в какое-то время массово расходится по всему классовый начинает по каким-то условий выпиливать данные потому что рост пошел вот так вот прям на то есть у нас был там условно наш терабайт штатные уютный которого мы привыкли плюс минус там и тут вдруг поехала поэтому да мы запилили очистка очистка на уровне тарантула это ваш наконечник который раз в какое-то время запускается и данные по какому-то критерию удаляет как то так а почему не почистить все и заново погреть там потому что миграция которую мы вот в том конкретном случае миграцию она была длинная там чуть чуть ли не месяц на то есть этого целый месяц вот этим заниматься соответственно вычистить там терабайт гораздо проще чем трогать залить за то есть это было было бы опять холодная загрузка дает догнать репликацию вот эти там 12 часов на загрузку вот это все почувствует дешевле спасибо 1 раз collect спасибо александра компаний sky box у меня такой вопрос я так понимаю у вас кейс если использование то есть вы храните актуальные сведения по абонентам да например 1c свой парик на 1 абонента да и там описываются какие-то услуги нет мы мы мы не храним это только используется при провели нам нужно это один раз выгрузить то есть ну как как работает вообще весь процесс то есть мы сначала подписываемся на изменениях кайтом вот а параллельно делаем выгрузку вот и у нас эти два процесса сходятся то есть мы сначала но подписались на изменения у нас уже есть блок транзакций который нам нужно будет проиграть на базу в начале нам нужно ну забрать весь объем терабайт данных поднять в память вот и вот эти cs вишни кида когда мой первый реализацию делали мы их выгружали и ссоры проделали экспорт в csv импорта тарантул вот и копили лог изменений то есть сначала ну как бы вот накатывали вот базу базовую часть потом проигрывали накопленный лог изменений над всем этим вот а потом уже после этого у нас кэш стабилизировался мы понимали что он прогрет там все все транзакции мы догнали он там попадает уже достали по влагу репликации в нужное после этого мы туда переключаем запрос пользователей а вот такой вопрос а как собственном вас и витрина формируется то есть если у нас есть изменения какие-то базе при следующем запросе нам нужно метаданные у допустим какого-то клиента связать то есть вытащить из oracal и как их хранить где-то чтобы оранта ли у вас просто хранятся файлики по сути то есть там никаких аналитический запрос мы не сделаем то есть вот вот когда вот это сделано вы смотрите во первых наши крыши конечно же это не просто хранил к на тарантулы байтов да конечно же это арест приложения до из круто там есть r они доступны на чтение для потребителей всегда то есть вы всегда можете сделать get запрос в сторону в витрины и получить данные из кэша то есть ну там не надо писать коннектор тарантула это во первых во вторых но василий про это она быстро проговорил мы подготавливаем средств данных то есть вот в исходной системе данные лежат вообще не компактно скажем так то есть для того чтобы нам собрать данные по оппоненту по всем его услугами по всем связанным сущностям нам надо joy нить достаточно много то есть берем данные из разных таблиц janym вот этот процесс join a мы фактически перенесли на загрузчик то есть когда у нас есть то есть мы забираем полностью все партиции а дальше мы меняем модель данных потому что нам нужно поменять представление сделать его компактным чтобы в один запрос в один хлоп получить все данные которые относятся к но к нашему ну как бы к нашему запросу к нашему оппоненту вот и мы провели достаточно большую аналитическую работу чтобы вот взять из мастер системы эти разрозненные данные и положить в нужную структуру данные рядышком в 1 шард чтобы можно было этот эти данные забрать мы над этим есть естественно rest и вот так всё это работает то есть это не просто кэш там нолик им единичками да это полноценное приложение которое работает отдает даны на чтение с очень хорошими показателями в прекрасном real-time am a вот и историчной саму вот у нас какие то данные поменялись она мне надо вот смотрите вот в кэше как раз нам нужны нам не нужны исторические данные то есть от каша требуется отдать актуальный срез данных но так чтобы он ну вот не вышел за 30 секунд то есть наши потребители требовательные готовы ждать 30 секунд их устраивает вот эта задержка вот поэтому у нас там только актуальные данные и именно поэтому нам приходится и фильтровать ну кроме того что там для нас много лишних данных есть вот мастер системе да там мастер системе как раз хранится вся история за истории пожалуйста холодные хранилище можно к мастер системе обратиться можно потом в девич садить потому что все там отбрасывает ну отбрасывается потом в аналитические базы вот наша задача была сделать в общем то горячие поднять горячие данные в кэш потому что всех интересует горячие данные а историю в каше матрица терабайт но куда мы по памяти съедим но в паркете если хранили бы они сжимались вместо zeiss вы не стал заниматься света за занимает ну смотрите cs вы это все-таки жесткий диск да мы в любом случае жесткий диск он дешевле чем память все-таки хдд vanich не сравнялись по стоимости с памяти александер спасибо спасибо большое за ваш доклад"
}