{
  "video_id": "88TMuCb2gs4",
  "channel": "HighLoadChannel",
  "title": "Катастрофы больше не страшны / Иван Раков (GridGain)",
  "views": 2771,
  "duration": 3214,
  "published": "2019-06-28T08:09:26-07:00",
  "text": "меня зовут иван я из компании great game и сегодня мы поговорим реплика в распределенных системах пару слов про то чем я занимаюсь это кто такой стас уже на прошлом докладе частично про это рассказал но для тех кто пришел с обеда я занимаюсь продуктом apache играет это open source распределенные су бды платформа для вычислений кэширования она может использоваться как самостоятельная база данных с персис танцем и частыми транзакциями так и как кэш для ускорения работы с каким-то z5 сторож в качестве ключевых приятных особенностей игнайта я бы отметил простое позиционирование данных вы просто загружаете данные и affinity функция сама считает в какой партиции окажется ваши данные и на каком узле окажется ваши partition если вы меняете топологию то партиций приезжают в фоне выгоняет есть честные распределенные транзакции которые дают eset гарантии и это то почему мы начали делать традиционную репликацию также выгнать вы можете к лоцировать данные если у вас есть похожие данные ну например там вся информация каком-то пользователя вы можете сделать так чтобы эти данные оказались на одном узле и направляет вычисление близко к данным прямо на этот узел таким образом можете сокращать количество сетевых hop off with game это компания в которой работаю это основное контрибьютором apache и игнайт и также это одноименный плагин который дает некоторые enterprise на и фича который облегчает использование игнайта в реальном продакшене например там есть дайте snapshots в возможно сделать полноценный распределенные бэкапы обладающий консистентной есть примочки для security ну и вот в ближайшем времени появится традиционная репликация я старший разработчик в компании great game я также помидор в apache и игнайт и я руководил разработкой транзакционный репликация о чем я вам сегодня и расскажу я поделил свой доклад на несколько частей начнем с вопроса зачем зачем вообще люди пользуются репликации какие проблемы своего бизнеса не хотят этим решить также поговорим как я проведу некоторую классификацию решение по репликации мы посмотрим какие они могут быть и чем они друг от друга отличаются таким образом вы можете благодаря этой классификации охарактеризовать вашу репликацию и понять какая вам нужна далее из нашего контекста и наша мотивация и я покажу какие варианты репликации мы выбрали верили реализовали у нас как треугольным камнем нашего решения является продукционной целостность я расскажу что я под этим понимаю и как мы в итоге добились транзакционные целостности на реплики и в конце я расскажу про таки уже более maintenance вещи то есть вот мы научились делать индукционной репликацию но все еще могут случаться неприятные события я расскажу как мы переживаем какие-то выходы отдельных узлов и какие-то глобальные события такие как изменение топологии всего кластер начнем с того зачем зачем люди пользуются репликации ну всем очевидно что скорость света конечно и предположим что у вас есть какой то я распределенный сервис и вы хотите вас есть какая то база данных какой-то datasette и вы хотите работать с этим dat ass этом с разных уголков мира если ну то есть вот кабы картинка с пинками между разными финансовыми центрами мира и где бы вы ни расположили вашу базу данных где-то будет до нее очень большой пинг то есть решение этой проблемы это хранение данных ближе пользователям то есть вы можете настроить вместо одного intenso база несколько инстанций и просто настроить между ними репликацию то есть база данных в фоне будет осуществлять репликацию между инстанциями данные будут догонять друг друга а вы просто будете работать с конкретным эстонцем следующий повод это масштабирование доступа к данным то есть если предыдущая картинка была про клад ansi то эта картинка про трубу то есть какая то база ваш сервис имеет к ней доступ и рано или поздно база перестает справляться ваш сервис растет количество нагрузки растет логичным решением это логичным решением будет поднять несколько инстансов и настроить между ними репликацию соответственно вы будете внешним образом балансировать нагрузку база уже сама пасибо само позаботиться о том чтобы данные доезжали благодаря репликации последний но далеко не самый маловажный повод для использования репликации это защита от катастроф предположим у вас есть какая то база какой-то кластер и вы с ними работаете она держит нагрузку у вас есть серьезные с алей который вы не можете нарушать вы потеряете кучу денег но в какой-то момент как показывает практика любая база любой plaster может превратится в тыкву то тогда делать как у вас восстановить работоспособность сервисов обычно люди делают bacopa данных и вы можете честно восстановить заранее снятый backup на ваш кластер которого ступал и после восстания либо копы вы продолжите работать но это занимает существенное время и соответственно стоит денег то есть если данных у вас много то пока все эти данные передаются на ваш кластер вы потеряете время логичным решением в такой ситуации будет поднятие 2 инстанса который будет резервным настройка репликации между этими инстансами и если случается катастрофа вы просто не паритесь и переключаете нагрузку на резервный incense соответственно временем простое будет не время восстановления bacopa время переключения нагрузки друзья одну секунду ok какая мотивация была у нас почему мы решили делать индукционную репликацию наш бизнес контекст это банки и винтик и они используют нас как распределенным sbd с транзакциями они хотят какие-то финансовые операции прогонять через нас у них могут быть большие кластеров в которых может быть много данных например у нас есть deployment и где есть более 100 узлов суммарным объемом информации более 50 терабайт главная цель это защита от катастроф у нас до этого уже была возможность снимать бэкапы но этого недостаточно пользователи хотят время простоя в случае катастрофы меньше при этом репликация не должна ощутить производительность то есть как бы будет плохо если кастамере привыкли каким-то показателям какому-то количество транзакций в секунду эти показатели сократятся для нас важен и быстрое восстановление сервиса допустимым является потеря данных за какой-то короткий промежуток времени ну то есть если была какая-то история транзакций ничего страшного если транзакции потерялись за последнюю минуту но при этом абсолютно недопустимо является потеря консистенции данным то есть тоже транзакции потерялись за минуту это не страшно из какого-то внешнего журнала марк сможем их восстановить но если транзакций между собой перемешаются останутся отдельные кусочки появятся какие то причин естественные парадокс это тогда мы встретим то есть тогда банк уже придется проводить какое-то существенно анализа того чтобы затем разобраться давайте пробежимся по тому какой разный может быть импликация вот и попробуем как-то это дело по классифицировать начнем с того что репликация может быть внутри кластерное меж мастер да то есть тут на самом деле граница довольно размытая но ключевой ключевым различием является то от чего мы страхуем ся при внутри кластера и репликации мы страхуем ся от выходов отдельных узлов то есть внутри кластера из праймари бэкапы данные внутренних переливаются и там если выходит какой-то узел то праймари становится другой узел и данной мы не теряем выгоняет уже есть такая внутри кластер аппликация речь пойдет в основном не про неё промер кластер например кластерной мы защищаемся от катастрофы всего кастера целиком также расскажу про важную метрику которая всегда упоминается когда мы говорим про защиту от катастроф это вот две две такие под метрики recovery points облике в рекавери time объектив вот рассмотрим на примере допустим у нас есть история и в рамках это историю на случается катастрофа мы теряем данные за какой-то промежуток времени безвозвратно при катастрофе этот промежуток времени называется rp recovery потоп джозеф временной промежуток за которой в течение которого мы восстанавливали наш сервис это эрте а вот соответственно благодаря этими двумя метриками любит оперировать бизнес и ими вы можете охарактеризовать качество вашего процесса защита от катастроф вот довольно очевидная классификация но все-таки я упомянул вот репликация может быть синхронный и асинхронный при синхронная репликация мы целимся на консистентной когда приходит клиент и засылает мастеру какой-то апдейт он сразу же отправляется на реплику и только после того как мастер узнает что реплика этот апдейт обработала она отвечает клиенту при таком и только при таком сценарии мы можем получить rp равный нулю вот вопрос в зал какие вы видите недостатки в таком подходе пожалуйста да действительно низкая производительность а именно увеличение latency то есть если нам нужен низкий watenshi то мы жертвуем тем временем в рамках которого мастера реплика общаются между собой противоположным подходом который не имеет этого недостатка является а синхронная репликация то есть сначала мастер отвечает клиенту что апдейт успешен после этого он уже занимается внутренней кухни до общается с реплика и все это дело синхронизирует при этом варианте соответственно у нас не возможно достижение rp равной нулю поскольку просто по определению если после ответа клиента мастер сложится то он может не успеть до сна титана реплику и реплика будет в непонятках и этот апдейт пропустят еще один одним методом различия решения по репликация является masters live или мастер мастер при masters life в варианте у нас для каждого кусочка данных для каждого ключа для каждой партиции что угодно есть один сервер 1 instant системе которой является источником апдейтов то есть все апдейты всегда проходит через него он несет за это ответственность тем не менее клиенты могут посчитать рид запросы на другие инстанции но в райт пройти всегда приходят вот через мастер по разным данным могут быть разные мастера но по конкретно конкретному куску мастер всегда один противоположным подходом является мастер мастер вариант когда у нас может быть несколько мастеров и они могут принимать апдейты на одни и те же данные если вспомнить пример проге распределенный кластер то это решение может нам помочь если нам нужны быстрые апдейты из разных концов света то есть при мастер слоев варианте мы вынуждены будем тратить большое количество latency на апдейт если у нас мастер находится в другом континенте но у такого варианта тоже есть существенный недостаток как вы думаете какой пожалуйста да мастера не могут договориться точнее не всегда могут договориться очевидно если есть 2 мастера по одному ключу то могут быть конфликты мы можем попасть ситуацию когда придут два клиента к разным мастерам попытаются обновить один и тот же ключ и система будет в таком вот разобранном состоянии часть узлов будет считать что значение 1 часть узлов будет встать считать что значение 2 и системе потребуется некоторое время чтобы договориться внутри себя какое на самом деле значение является истинным это означает на практике падения ваших гарантий по консистенции до event shall консистенции то есть в течение какого-то короткого промежутка времени данные могут быть неактуальным но вы можете построить систему таким образом чтобы в итоге все данные синхронизировались и системы стала консистентные давайте продолжим как еще отличается разные варианты репликации когда мы говорим о репликации данные каким-то образом попадают с мастера на реплику давайте рассмотрим каким образом они могут это делать первый вариант это самый простой и тара убийст репликация когда изменение передаются в явном виде мы меняем какой-то ключ и значение этого ключа просто пересылается на реплику очевидный подход но он не самый емкий по количество информации другим вариантом является репликация на основе недель или демон кто понимает что такое недель и делали поднимите пожалуйста руки окей замечательно речь идет о том что если клиенты как бы ходят на мастер с помощью каких-то запросов эти запросы просто дублируются на реплику исполняются там какие тут есть подводные камни во первых результат выполнения может быть недетерминированные sli запросы включаются в себя какие-то рандом функции ли какие-то поминания текущая timestamp а результат на реплики может получиться другой но даже если функции dither минирован и то вы можете попасть ситуацию когда параллельное исполнение одного набора запросов на мастере и параллельное исполнение одного запроса такого же за набора запросов на реплики придут к разным результатам просто за счет конкурентного доступа то есть вы должны понимать какие гарантии вам нужны если они вам нужны то как-то упорядочивать или стерилизовать собственно исполнения ваших запросов на реплики еще одним вариантом является лог shipping любая база данных которая поддерживает краж recovery пишет свой журнал для того чтобы восстановиться в случае локальной авария обычно называется в рай то hotlog соответственно этот в рай то hotlog можно брать и передавать на реплику и применять там журнал бывает разное как бы если это физический журнал где отображаются бинарные изменение структур данных то вам нужно позаботиться о том чтобы этот журнал подошел к реплики потому что на реплики может быть друг могут быть может быть другая версия на реплики структуры данных могут быть в эквивалентном состоянии с точки зрения логики но физически они будут как-то по боевикам отличаться тогда ваш журнал может не побоится об этом тоже нужно помнить есть еще другие варианты трансфера например трансфер благодаря использованию общей файловой системы трансфер по файлам когда мы просто фиксируем там физический файл портится и передаем его их может быть много но вот это основные которые там в целом описываются насколько это может быть все разнообразным ok давайте я теперь расскажу как из нашего бизнес-контекста мы вывели вот какие из этих вариантов нам нужно выбрать в бергене так как нашей мотивации является защита от катастроф то мы выбрали наш кластер ной мастер слоев репликацию да тут как бы выбор довольно простой зачем нам вводить какие-то дополнительные вводить дополнительные сложную логику если нам нужно стенд бай реплика которые не не пишет только понимает какие какую-то нагрузку на чтение performance на мастер не должен деградировать из этого следует что нам подходит а синхронная репликация синхронное неизбежно увеличит watenshi и уменьшит количество транзакций в секунду так как у нас могут быть очень большие кости раз большим объемом данных то мы скомбинировали подходы робость и лог shipping в вариантов и добавили компрессию то есть сейчас я объясню что что я под этим понимаю мы достаем из нашего брата had logo непосредственно логически обновление данных вот и фильтруем лак от всех остальных далее мы сжимаем файлы журналов и передаем на реплику то есть благодаря вот такому подходу у нас есть там существенной watenshi на то что апдейты доедут на реплику но данные передаются очень эффективна в red hat логе с логическим записями сжимается отлично это 10-20 раз на практике вот и поэтому суммарно большая нагрузка на мастер приводит к небольшому количество информации передаваемого на реплику реплика должна сохранять транзакционному целостность то есть так как канаше kenda пользую с транзакциями они хотят за established какие-то инварианты вот и все эти варианты должны сохраниться на реплики чуть позже я подетальней про это расскажу р п о р т а который нас устраивает это не больше минуты то есть клиенты соглашаются с тем что данные потеряются за минуту и сервиса становится за минуту вот при условии что транзакционный целостность сохранится если изобразить вот эту схему на картинке то получится как-то так есть мастер он отправляет отфильтрованные сжатые логе в какое-то надежное для реплики хранилища называем его трансфер folder на практике это может быть какая-то общая сетевая директория типа нас это может быть по могут быть просто примонтирован и и папки на реплику в целом не важно главное если мастер что-то успел отправить под полностью та реплика это уже увидит и не потеряет в рамках репликации у нас зеркальные топология это значит что для репликации клиент должен настроить такую же топологию реплики как и на мастере вот чем это помогает это помогает тем что журналы с узла x на мастере полностью подойдут как узлу x штрих на реплики кого так сказать аватару потому что выгоняет статическое распределение партиций то есть по набору узлов affinity функция назначит партиции на те же самые узлы то есть мы просто берем журнал и он подходит к другому узлу это замечательно очевидным минусом является то что клиент может захотеть сделать реплику поменьше то есть нагрузки на него нету поэтому он хочет какое-то меньше количество узлов физических для него использовать очевидным решением будет это deployment нескольких узлов grid гейна на 1 физическую машину то есть такой подход будет продолжать работать начальная загрузка данных такой схеме осуществляется через restore backup а то есть мастер делает backup в какой-то момент и сразу же начинает стримить в right to hate логин на реплику с этого момента рано или поздно этот процесс заканчивается реплика делает restore этого бака по и в этот момент она уже полностью соответствует мастеру но отстаёт от него на несколько часов чтобы сократить эту разницу реплика начинает накатывать отправлены журналы из трансфер folder а когда-то она догоняет мастер и вот они находятся в таком режиме дальше то есть мастер стримит реплика постоянно чуть-чуть от cerrad мастер и но постоянно его догоняет что стоит заметить то что вот это решение как бы ну тут пока ничего особенного не туда ну мы просто шутим локи до что сложного но пока этого не достаточно пока мы все еще не обеспечили традиционную целостность файла журнала может оборваться посередине транзакции до транзакциями могут быть перемешаны журнале то есть такой бы . мы не докатились на каждом узле транзакционный целостность целостности мы не получим поэтому надо этот вопрос развивать и как-то эту проблему решать давайте я расскажу как но начну с того что именно да вот сейчас я объясню какие именно инварианты пользователь хочет увидеть когда он говорит о том чтобы все должно быть трансексуала целостно для начала я вкратце расскажу как устроены распределенной транзакция выгнать они реализованы по классическому протокола туфа из комет когда пользовательский поток уже сделала все обновления и решает комитет транзакцию с узла инициатора отсылаются request и на начало фазу припер да то есть мы готовим транзакцию в камеру на фазе при прм мы приобретаем исключительные блокировки на ключи которые участвовали в транзакции на всех узлах участниках транзакции после этого после получения всех припер response of инициатор формирует commit request а он уже может утверждать что транзакции полностью готова и вот только на этом этапе мы фактически обновляем наш сторож и пишем апдейтов на журнал когда это все произойдет узлы участники отвечают инициатору что коммент успешен и по только после этого он возвращает управление пользовательскому потоку что из этой картинки нам важно и пригодится это то что есть отношениях и пинги for между всеми припер фазами и всеми commit фазами просто благодаря тому что вот у нас есть . линеаризация . когда инициатор принимает решение о том что при пару спешно делать коммиты и такое же отношение есть между всеми партнерами и всеми фактическими апдейтами именно фактически апдейтами будем накатывать у журнала именно они нам интересны и так что же мы подразумеваем по традиционной целостностью начнем с базовых гарантий месяц и за исида нас в данном случае интересует буковка л а именно атомарной то есть рассмотрим на простом примере классический для банков кейс карточный перевод мы переводим 100 долларов с аккаунтом 2 на аккаунт один если атом аппетит этом на транзакция и на узлы то есть вот есть же узлы 1 и 2 и и в журналы то это будет как то так мы запишем в рамках транзакций обновленное значение количества денег и для аккаунта 1 и 2 аккаунта 2 и какую гарантии мы тут хотим видеть это обратное следствие что-либо все либо ничего апдейт вся транзакция должна быть ибо атомарной видно на реплики либо атомарная не видно этого не всегда достаточно давайте поговорим про причинную согласованность по ключам под ней я понимаю такое свойство что если ключ x изменяли транзакции а и б по порядку то ситуация когда бы видно а а не видно недопустимо как это может нам испортить жизнь чуть-чуть усложним наш пример с карточным переводом теперь будем там как бы как делают обманщики переводить деньги через буферный счет есть на этот раз уже 3 аккаунта аккаунт один аккаунт 2 аккаунт 3 и сначала мы переводим деньги на аккаунт два после этого мы переводим деньги на аккаунт 3 аккаунтов в промежутке был пустой то есть на нем не было денег соответственно то что мы хотим видеть это следствие что если результат транзакции 2 виден то результат транзакция один тоже должен быть виден если это будет не так-то просто кгц это не будет иметь никакого смысла мы просто будем переводить деньги со счета на который мы вам на котором их никогда не было еще один один вариант это причина согласованность по пользовательским потоком в чем тут разница да то есть два каких-то действия для каких-то транзакции могут быть с не связаны по ключам явно но они могут быть связаны пользовательского викой предположим мы также переводим деньги а после этого на пустом аккаунте мы хотим осуществить операцию закрытие аккаунта то есть для этого мы должны записать какую-то другую таблицу информацию о том что аккаунт один переводится статус козел так как это другой кэш другая таблицы по связи по плечам тут не будет но пользователь как бы подразумевает что сначала он делает одной операции потом другую и не логически между собой связаны вот в данном случае мы хотим добиться того что если это происходит из одного потока то будет следствие что если результат транзакции 2 виден то результат транзакций один тоже должен быть виден окей как все эти инварианты удовлетворит на реплики то есть есть в целом у существующих вендоров разные подходы то есть репликам могла бы как-то проанализировать лог там посмотреть на версии транзакции если и реализован эм би си си то на благодаря snapshot и завершено могла бы какой-то консистентной view построить но вестись и у нас все еще в бете поэтому мы воспользовались другим подходом мы заставили мастер заботиться о транзакционные целостности как это работает мастер строит мастер выполняет глобальный алгоритм в рамках которого строится консистентной срез да то есть это некий срез если разрезать журналы по нему то результат их на кота будет транзакционных целостном соответственно журнал вратах от logo размечается то есть там ставится дополнителей пометки которые подскажут реплики как именно накатить журнал чтобы добиться инвариантов реплика применяют журналы дискретно то есть есть срезы и реплика видит что если срез доступен то мы можем перейти в следующее транзакционных целостное состояние и собственно соответственно все накатить соответственно если случается катастрофа та реплика просто видит своим надежным хранилищем какой срез последний если он еще не накачан то на его накатывает давайте это все проиллюстрируем то есть сущность срез которая представляет собой mapping из узла то есть срез это локальная сущность для каждого узла я для каждого журнала и в качестве вылью тут the pale из двух указателей на вал то есть указатель на вал это просто номер сегмента в рай the hat logo и и смещения то есть начало никого фазе бордера конец никого фазе ордера и эти фиксаторы транзакций которые мы при накатке пропустим внутри этого фазе бордера то есть накатка по фазе boarder то что мы заранее посчитали она как раз даст нам целостное состояние the sea реплика будет себя вести следующим образом сначала она увидит то что срез один готов накатит все до старта фазе бордера сплошняком и внутри фазе warder а она уже накатит все кроме тех транзакций которые мы решили пропустить при накате кота и 1 на следующем этапе мы в рамках вот этого фазе border будем катить весь журнал но по обратному предикату то есть все транзакции которые мы пропустили при накате кота один при начале накате кота 2 мы уже не пропустим и накатим только их далее до начала следующей фазе бодрым и выкатить все и повторим логику уже вот на в конце на фазе борода рикотта 2 мы будем катить только то что нужно кроме и идите к транзакции которая пропущены на этапе 2 вот давайте теперь поговорим собственно сам самое сложное как добиться на мастере вот этого конкретного среза ну как бы хорошо что на мастере у нас есть runtime контекст транзакций то есть мы в целом понимаем что происходит для этого запустим глобальный алгоритм он будет состоять из двух этапов на первом этапе мы для начала зафиксируем на каждом узле набор транзакции который находится в состоянии prepared если транзакция в состоянии prepared to это значит что фактически она уже могла записать какие-то апдейта выраженного потому что апдейты пишутся между состоянием ну как бы сначала приобретается препарат состоянии пишутся апдейты потом закрывают состоянии комитет итогу мы запоминаем все транзакции которые препарат и дожидаемся их завершения дожидаемся пока они действительно запишут все свои обновления когда это произойдет мы инициируем следующий этап нашего консистентная среза и на этом этапе мы зафиксируем транзакции которые пока еще не находится в состоянии комитет то есть они могут быть в любом состоянии если они не комета то мы зафиксируем это вокальном дождь и транзакций далее узлы обменяются этой информации они отправят координатору набор идите фиксаторов транзакции которые у них не закончены координатор построит объединение этих множеств и разошлет всем коллекцию глобально пропущенных транзакций то есть чтобы сути происходит по сути на этапе два узлы сообщают в друг другу какие транзакции у них частично закончена частично какие все еще находятся в процессе если все узлы такой информации обменяются то в итоге мы получим множество транзакций которая хоть где-либо ещё не закончено по идее если все узлы договорятся то мы решим проблему от омар настя мы сформируем множество транзакций которая big fat где-либо не закончена и это даст нам соответственно вот то что мы хотели получить набор пропущенных транзакций прошу прощения да вот он набор пропущенных транзакций это хорошо но нам все еще нужно сформировать два пойнтера начало и конец спать в ордера логичным следствием из картинки будет то что эти фазе vor der и будут просто сформированы на нас той же 1 ст2 то есть в рамках работы этого алгоритма мы запишем вал какой то какой то запись зафиксируем на нее указатель и будем использовать как начало и конец этого фазе border а зачем нужен вообще этап 1 да то есть нам хочется чтобы транзакция которые пропущены частично чтобы их куски не были левее чем этап 1 да потому что нашу сложную логику по накату с предикатом мы примем только между этапом один этап м2 если какие-то куски окажутся левее то мы пропустим то есть мы частично накатим эти куски и транзакционных целостность и не получится вот на самом деле то есть тут есть следующая цепочка отношений то есть вот рассмотрим транзакции как бы из множества глобально пропущенных чисто по определению протокола туфа из комет их обновление в которой не пишет журнал то есть реальные данные находятся правее чем их препарат маркеры чем в моменты перехода их состояние препарат с другой стороны так как на этапе один мы подождали конца всех транзакций которые были в состоянии препарат то все примеры этих транзакций находятся правее от ap1 то есть ну от противного да если бы какая-то транзакция и которая здесь комете лась какие-то куски были бы левее то мы бы просто дождались этой транзакции вот собсно на этапе 1 и она бы не попала в это множество то есть то есть таким образом мы ограничиваем окно в рамках которого транзакция могут быть кусочно то есть и вот эту сложную логику по накатке мы исполняем только в рамках максимально короткого окна для чего это полезно почему нам хочется сделать это окно коротким потому что нам и все еще нужно вот мы уже добились от омар насти нам все еще нужно добиться причиной согласованности чтобы добиться причина согласованности по ключам мы делаем просто мы между этапом один этап м2 строим обратный яндекс по ключам предположим транзакция приходит красная транзакция которая обретает ключи 2 и 3 эти апдейты попадают журнал и по нашей логике мы должны не накатывать транзакцию 1 так как частично она находится правее границы stage 2 то есть по нашей логике она не комитет момент stage 2 ее нужно откатить допустим есть синие транзакция x2 она обновляет только один ключ ключ 3 и по нашей логике мы не должны откатить поскольку она целиком левее границы стоит 2 но между ними есть зависимость они зависимы по ключу 3 то есть если такое происходит как мы это детектирует в момент пополнения обратного индекса если мы кладем туда маппинг и там есть уже момент по такому ключом то мы понимаем что она случился матч что у нас есть зависимость и мы достраиваем и пополняем наш граф зависимых транзакций то есть мы добавляем туда туда вершину тег с 1 вершины x2 и ребро между ними которое свидетельствует о том что у нас есть зависимость что могут потом с этим графом сделаем на этапе когда мы узлы на этапе 2 обмениваются информацией о том какие транзакции локально пропущены мы добавим к эта информация ещё и граф зависит зависимых транзакций соответственно узлы обменяются своими идентификаторами пропущенных на за акции и своими графами и в итоге получится глобальный граф и в момент когда мы будем формировать множество пропущенных транзакций мы замкнём это множество относительно нашего графа то есть мы поймем какие транзакции надо откатить и посмотрим как бы вдруг у нас в этом в нашем окне есть еще транзакции которые надо откатить потому что независимы и соответственно мы посмотрим при тренируемся по нашим транзакциям вычислим те которые нужно откатить так как они зависимы от тех которые надо откатить и построим полное множество то есть мы замкнём наше множество транзакций кого надо пропустить относительно графа зависимости вот как то так то есть благодаря такому вот механизму то с ним немножко по извращались и добились того что три инварианта которых я говорил в секции транзакционных целостность они будут выполнены давайте теперь поговорим про такой вот maintenance вещь а именно о том вот хорошо мы научились там традиционно целостно катить логе но все еще могут могут случаться какие-то события например узлы могут выходить так мы с этим живем у нас есть инвариант что репликация переживает выход backup фактор -1 уникальных узлов что такое backup фактор в рамках одного кластер а если каждый ключ каждая partition хранится как минимум в четырех экземплярах это значит чтобы как фактор 4 что значит уникальные узлы это значит что если там два узла а и b выйдут на мастере два узла c и d выгодной applique это значит что репликация становится но если на реплики выйдут их аватарка их так сказать зеркальные копии а штриха b-штрих то привлекаться продолжатся потому что уникальных узлов всего лишь 2 соответственно мы треком изменение топологии на мастере и в наши консистентные срезы которые вот является пк2 двумя planter миновал и набором транзакции мы добавим эту информацию о топологии мастера если узел выйдет на мастере реплика узнает это и среза и остановит накат если узел вернется та реплика запустит внутреннюю и балансировку и за счет себя самой в основе данные давайте вот на такой вот истории посмотрим как это работает есть backup фактор 3 есть три узла и случается беда выходит узел один на мастере узел 3 на реплики допустим узел один после этого довольно быстро входят узи ну я реплика при этом узнает это поскольку она получает информацию о том что узел один зафейлился и она останавливает накат потому что у журналов просто нет их некому генерировать потенциальным рано или поздно узел один приходит в себя и на следующем связи реплика об этом узнает потому что она добавляет следующий с информацию мастер добавляет следующий срез информацию о том что узел один снова же соответственно реплика это понимает и продолжает уже накат журнала именно из трансфер holder а вот и все теперь все еще есть промежуток времени за который она пропустила апдейты поскольку вот между этим этим срезом мы не применяли dts master а вот она их восстановит благодарили балансировки то есть узел 2 все еще обладает всеми апдейтами поэтому мы запустим ребаланс и данные просто затянуться на узел один су-22 узел 3 на реплики через какое-то время тоже может зайти обратно тогда он соответственно тоже пропустил какую-то долю апдейтов и он тоже благодаря и балансировки догонят весь остальной кластер то есть узел 2 которые не выходил и обладает всей информации он снабдит узлы которые испытывали кита проблемы всеми апдейтами поговорим еще про то как мы можем в рамках построенного процесса репликации поменять топологии кластеров под топор я между существенный скилл кластера когда у нас будут перезжать партиции с узла на узел у архитектурой игнайт есть особенность что при глобальных событиях которые подразумевают изменения маппинга партиций вас останавливается традиционная нагрузка соответственно если нагрузка остановилась то будет что что нам нужно сделать мы можем создать легковесный срез так называемый то есть и если вот по этому срезу реплика до котят журналы то она окажется в транзакционный целостном состоянии поскольку этот срез будет снят момент когда нет нагрузки то есть мы вас уже нет нужды запускать наш глобальный алгоритм мы просто воспользуемся тем что мастер уже находится в транзакционных целостном состоянии и как бы просто в этот момент будет вызван call back который создаст этот легковесный срез реплика если увидит такое легковесный срез она увидит какое событие его породило то есть в нашем случае это изменение топология и просто продублирует это событие и среза и после того как она продублирует уже продолжит катить журналы соответствии с новой топологии что имею ввиду предположим у нас был построен процесс репликации с кластером действо узла на кластер где есть два узла какой-то момент администратор решил по спас коллировать кластер добавить третий узел он исполняет команду на то что вот теперь топология новые партиции должны переезжать в этот момент транзакционные нагрузка на мастере останавливается и она создает легковесный срез да то есть этот легковесный срез видит реплика в какой-то момент она доказывает до него все изменения видит что этот срез несет себе информацию о том что то по воде теперь другая replicate аналогичным образом изменит топологию и продолжить катить журналы уже в соответствии с новой the plague то есть благодаря вот таким вот механизмом легковесных средств можно дублировать не только изменение топологии но и какие-либо изменения схем данных да то есть добавление новых таблиц и так далее то есть все глобальные операции выгодно это которые оставляют нагрузку они могут быть таким образом продублированы на реплику вот соответственно реплика тоже запустит ребаланс и будет продолжать катить им дайте окей что у нас получилось в итоге какие выводы мы можем сделать в вариантов репликации много репутация может быть разной и как бы для вашей задачи может подойти какая-то из них то есть нужно ориентироваться в том какие репликации бывают и понимать под ваш бизнес контекст какой вариант будет лучше всего под наш бизнес контекст краж и кавери подошел вариант лог шопинга и мы сделали транзакционный лак shipping благодаря следующим хаком то есть на мастер мы возложили дополнительную нагрузку по периодической разметки его и построению срезов для получения транзакционные целостного состояния реплика видя эти периодические срезы осуществляет дискретный накат отрезок сразу вот таким образом реплика ответ от мастера но всегда остается традиционно целостный вот в целом это все что я хотел вам рассказать спасибо за внимание теперь я готов ответить на ваш вопрос здрасьте спасибо за доклад есть два вопроса во первых почему вы решили использовать именно двухфазный коммент почему не l1 консенсуса например тампакс из ровд речь идет про механизм транзакция выгоняет донесли проникать году ну здесь скорее всего мотивацию у нас историческая да то есть у нас довольно зрелый механизм транзакций вот и двухфазный комет это простейшие решение которое позволило нам действительно в распределенной системе сделать acid то есть поясните пожалуйста поясе показ он как бы позволило бы избежать на познавака метода ну в частности он позволил вам обойти ограничения на то что все реплики должны быть онлайн ну по факту потому что это естественное ограничение которое будет накладывать двухфазные коми ну то есть я думаю что тут просто исторические причины что мы сделали таким образом так и еще проводили ли вы какой-нибудь формально или неформально и доказательства корректности вашего алгоритма ну в частности с графом зависимых транзакцию вопрос хороший то есть формальное доказательство там с использованием какой-то человек у вас какой ну да там отдай или чего-то такого мы не проводили то есть мы просто выписали математическое доказательство и долго на него смотрели вот и суммарно решили что как бы никаких подводных камней или код контра примеров не найдено но фарм неформальную верификацию ну вероятностью там джексоном пошатать джексон это на самом деле отдельная история да ну то есть мы проводили как бы просто тестирование наших стендах вот мы не считали джексоном мы как бы шатали то есть сетью там как бы ему выводили узлы сами вот вы спросили какие то сетевые проблемы вот садятся на том какие то проблемы мы нашли их и я не были подчинены да то есть абсолютно использовали спасибо вам спасибо за доклад поясните пожалуйста каким образом инсталляция но то что на одном узле можно развернуть несколько инстансов видно это позволит а разную топологию обеспечивать вот как как это ограничение можно обойти я не совсем понял смотрите что имел ввиду то есть у нас есть ограничение что и зеркальной топология и узел идентифицируется в рамках топологии каким-то идентификатором до обогнать он называется от consistent айди то есть именно благодаря тому что он есть узлы мастера и реплики видят друг друга да то есть узел на master is consistent айди отправляет журналы узел на реплики с consist отойди а таким же эти журналы применяют то есть мы можем на мастере узлы с consist in the idea и позади плотно разные узлы она реплики эти узлы за тепло и на один физический сервер но соответственно они все друг друга найдут спасибо еще вопросы коллеги к вам бежит уже барышня спасибо за доклад вы сказали сам начале что у вас время на поднятие сервисов около минуты а каким образом у вас slave гиа кластерный становится мастером кто мониторит это и каким образом да спасибо большое за вопрос действительно я про это не упомянул то есть тут есть два варианта переключения словам мастера либо это так сказать запланированное переключение да то есть мы условно говоря либо это запланированное переключение со стороны мастера то есть на мастере мы делаем последний съезд прерываем репликацию вот и реплика автоматически видит то что репликация закончена и переходит мастер состоянии второй вариант это то что оно случается катастрофа на мастере и мастер уже не как реплику опустить не может в этом в этой ситуации администратор уже дает команду на реплика что у нас аварийное переключение и на как раз выполняет логику про которую говорил им она видит последний срез накатывает его и соответственно переходит мастер состояние в эту минуту еще и укладывается звонок администратору да ну реплика в любом случае не может сама принять решение о том что нужно переключаться потому что единстве протокола коммуникации с мастером это журналы которой она видит вот то есть вы естественно на практике внешним образом можете настроить мониторинг который случае того строфы будет давать реплику команду ну то есть в этот функционал такая опция не входит спасибо большое спасибо за отличный доклад маленький вопрос вот момент на кота реплики применяется ли механизм двухфазного комета или нет как обеспечивается целостности и вот именно висит в брики реплика как раз избавлен от необходимости применять двухфазный конец она просто накатывает отрезок срезу да то есть все что нужно сделать это накатить журнал по отдельно как бы размеченному же высадятся на это хотя бы не согласен вот смотрите вы превратили пример тот же пиривет перевод сумм есть у нас там баланс клиента а лежит на ноги а баланс клиентам н н о д п соответственно если мы успели накатить на но duo изменения то на ноги бенни консистентная состоит я понял ваш вопрос да спасибо смотрите мы решаем задачу защиты от катастрофа то есть если мы поймем что случилась катастрофа та реплика сможет быстро прийти в консистентной состояния но при этом в промежутке от однако тогда на кота оно временно не консистентная пока она непосредственно катит то есть есть опция заполучить аппликацию и перевести ее вход стендбай режим когда она может принимать нагрузку и и давать там транзакционных целостный ответ да но как бы где то вам нужно еще поставить на паузу спасибо спасибо за заклад вот вы хорошо так рассказали как с мастера там sleeve там запускается становится мастером замечательно а как вы обратно ты включаете мастер кластер то есть он там глаз на одну минуту убежала будущее так если у вас правильно понял вы просто что и предположим что мы переключились с мастера на реплики теперь реплика стала мастером и простом как это делать обратно в случае когда это делается по команде с мастера то есть мастер говорит все репликация окончена у нас есть опция чтобы бластера поменялись местами то есть при таком плановым переключения реплика станет мастером и начнет автоматически катить апдейты на бывший мастер новую реплику да все верно соответственно этот процесс можно повторять сколько угодно раз вот если у нас произошло аварийное переключения мы автоматически подразумеваем что мастер у нас уже раздал иван и как бы возобновить и репликацию в таком случае можно только через bootstrap заново да то есть мы выставимся snapshot а и делаем все так как в самом начале здравствуйте спасибо за доклад у меня вопрос следующий замеряли вы сколько идет до конца среза и на каких объемов данных эти замеры проводили какие результаты получили переходу на слои вот вы гаити до минут вот интересно это минута от какого объема thrive он и активности вопрос сколько наполняется съезда сколько с момента создания средств до момента сверки и переходу на слои проходит времени а я понял смотрите тут зависит все от характера нагрузки до то есть прежде сначала мы на играете пишем а какой то отдать журнал потом он фоне после того как он полностью отстреляться и мы сменим сегмент начинаем его отжимать потом фоне мы его начинаем пересылать вот на практике там мы воспроизводили условия когда у нас там мы долин нагрузочное тестирование мы дали нагрузку на мастер что у нас было порядка 15 гигабайт чистого же брать на hotlog журнала в минуту вот в таком варианте у нас cts на реплика отставала от мастера на время мне больше 50 секунд когда у нас вот время интервал между срезами был 30 секунд то есть это за счёт того что благодаря высокой нагрузке журнал успевал отстреливаться вот и сжиматься и просто перебрасываться вообще хранилищ теперь поклон и аплодисменты красавец спасибо большое вам спасибо это была великолепная кому подарим книжку за лучше в обладает человек который спрашивал про джип сон однозначно он вот отлично"
}