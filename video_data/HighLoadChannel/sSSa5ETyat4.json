{
  "video_id": "sSSa5ETyat4",
  "channel": "HighLoadChannel",
  "title": "Как устроена многопоточность в Hazelcast  / Владимир Озеров (Hazelcast)",
  "views": 892,
  "duration": 3108,
  "published": "2020-04-14T11:15:03-07:00",
  "text": "коллеги всем привет меня зовут владимир озера в гарвардском по не ходил к 100 мы сели погорим о том как у нас в нашей системе организована многопоточность кто из вас работал сейчас котелка сам поднимите руки кто незнаком систему хорошо как кто не знает козелка стопа а платформа для распределенных вычислений в памяти так называем класса систем которые иногда на называть он м ред это гриц янамари computing platform не суть важно и все том что моя главная цель нашей платформ ускорять работу ваше предложение путем перенесения вычислений данных памяти и для того чтобы достичь этой цели нам необходимо уметь хорошо масштабировать нагрузку во первых в рамках ресурсов в во-первых масштабируйте между машинами которые имеются в кластере во вторых в рамках одной в рамках одной машины и сегодня докладе мы с вами не будем говорить о том каким образом мы распределим данные между разными машинами кластером эта тема отдельная а покурим исключительно о том каким образом мы добиваемся хороший утилизации сепию и хорошего труб это именно в рамках одного сервера за века вперед до могу сказать что он хитер их и заказ тв представляет собой набор титулов которые взаимодействуют друг с другом и которые и каждый из которых специализирован под выполнение конкретного вида задача такие как операции с сетью и операции операция с ключами операция связана со стрингами так далее так и такого рода архитектура вы можете встретить вот сети на как правило называют сета то есть элемент ривен архитектура и наша задача сегодня с вами разобраться поговорить не столько о том что хабблкаст ему так работы то разобраться почему мы пришли именно к такому варианту и какие альтернативные способы организации обработки запросов от клиентов вы можете встретить других похожих системах ну давайте давайте двигаться давайте скажем что перед нами стоит ну что мы работаем с классическим это им распределенного кэша которые есть в качестве украшения обычно операции под get еще какие-то и вот представьте что у вас есть огромное количество клиентов которые за большого количества потоков берут отправляет нам эти самые операции нам нужно в рамках одного сервера каким-то образом фиктивно очень их обрабатывать и в идеале и быстро отвечать и подавать высокий трупу самый простейший подход который мы можем лет в использовать это использование блокирующего ем которая предполагает что на каждый клиентский поток который хочет исполнить операцию мы открываем серверный поток который просто блокирующему режиме сначала ждет запрос от клиента потом исполняет потом отвечает обратно ну большим преимуществом такого подхода является то что он достаточно просто реализацию прямолинейный там тупой но при этом понятное дело что под большой нагрузкой если количество клиентов или потоков растет то соответственно у нас будут расти накладные расходы связаны с таким подходом потому что такое решение и во-первых требует памяти при создании все большое количество потоков чем становится больше тем больше в вас становится накладные расходы на скиду ленка этих потоков на переключение контекстов поэтому на практике такие решения вы в таких возраста системах конечно не часто встретите да и поэтому более распространенный вариант это использование не блокирующего юсб при котором с помощью мы можем фактически мультиплексирование количество клиентских потоков на один серверный поток используя используя опять не блокирующего и операционной системы или торрентов котором вы находитесь вот соответственно в этом случае у нас имеется ну представьте что у нас есть один поток на на сервере в котором крутится винт лоб который сначала слушает события слушает сетевые события как только один клиент нова прялку то запрос мы его расспросили исполнили ответили обратно если второй клиент соответственно хочет нам тоже отправится просто мы можем вас спокойно обрабатывать в этом же потоки этим самым по большому счету нам одного потока может хватит для обработки даже очень большое количество клиентов и такой подход уже значительно чаще встречается на практике псевдокод в терминах джавы да это вот мы у нас есть есть те циклов которому все время крутимся у нас есть дневники селектор из которого мы получаем события которых на пришли по очереди быть исполняем даже если они от разных клиентов и отправляем обратно характерные такие фигурки которые полагаются на такую архитектуру это самый известный atrides но джесс да потому что вы от них наверняка слышите что-то такие классические то поточное приложение хотя ну если быть совсем точным то они ну по крайне regisseur на по точным является мы еще дополнительные с служебные потоки но тем не менее вот именно базовой обработка происходит в одном потоке преимущества такого подхода состоит в том что вы можете большое количество клинских потоков обрабатывать потом серверном потоки и это достаточно хорошо масштабируется до тех пор пока вам хватает ресурсов 1 secu для того чтобы обрабатывать и и сеть и непосредственно склонять операции который вам вам прислать клиенты но рано или поздно ресурса носите его может не хватить в стоит вопрос как каким образом нам это дело дальше масштабировать значит здесь это одна проблема и вторая проблема каким образом нам справляться с разнородными с разнородной нагрузкой про которые я покажу немножко позднее итак давайте сначала посмотрим а теперь представьте что у нас есть вот один северный поток в которого гуляю запросы большое количество клиентов и какой-то причине secu этого одного потока нам уже не хватает самое простое решение что мы здесь можем сделать это за стартовать ни один на больше потоков serving который будет которые будут отвечать за процессинг клиентских запросов это простой понятной решение но здесь главный вопрос который возникает а каким образом нам теперь балансировать нагрузку между этими потоками и один из вариантов которые приходят голову это давайте мы просто будем всякое клиентские подключение допустим каким-то образом распределять между северными потоками например по round robin и это можно делать но на практике опять же чаще вы встретите другой подход когда задачи распределяются не в соответствии с тем от какого клиента не пришли а в соответствии с тем каким данным они пытаются получить доступ как это обычно выглядит представьте что у вас есть есть какие то данные которые хранятся допустим в памяти до есть большое количество ядерщик у которых этим данным пытаются получить доступ некоторые потоки хотят их прочитать некоторые потоки хотят их изменить и если вы свою систему построить таким образом что каждый поток может читать изменять все данные системы то у вас очень кому что возникнут контент шина на этих данных потому что ну представьте два потока у вас хотят изменить один тот же ключ чтоб не потерять целостность данных вам необходимо соответственно организовывать каким-то образом защищать доступ к этим данным например используя критической секции и на практике это будет править к тому что в общем-то вас будет может значительное количество ресурсов систему ходить просто на синхронизацию да поэтому чаще на практике вы в систему встретите другой подход когда каждый поток работающим на этом определенным ядре отвечает строго за свойства все данных то есть мало того что мы бьем данные попортишь нам в рамках всего кластера номером в рамках одной машины мы тоже говорим что конкретный поток работает только задуматься этом парке шинах ну и соответственно с конкретным с рассветом данных тем самым и и преимущество этого подхода заключается в том что с каждым конкретным ключом у вас гарантированно работать не более чем один поток поэтому по большому счету вы можете практически всю синхронизацию на доступе к данным из вашей системы убрать и это будет работать у вас достаточно достаточно быстро но этот подход тоже не является универсальным почему потому что если вы работаете с простыми операциями например get по вы делаете get по по ключу ли делать и путь по ключу то есть вам необходимо обратиться только к одному пар тишину в этом случае как правило все будет работать хорошо и быстро а вот если вы исполняете например запросы с предикатами какими-то до которые потенциально должны потрогать данные с большого количества про тишина здесь возникают проблемы почему потому что так как мы заведомо сказали что только один только один поток может работать конкретным ключом чтобы исполнить вот такой запрос предикатом нам скорее всего потребуется наш изначально запрос разбить на с ну скажем так на под запросы каждой из которых вы адресуете уже в конкретные поток потоки и независимо друг от друга своей части запросто исполняют и потом собирают обратно и это оказывается зачастую достаточно дорого поэтому это решение хорошо работает если вы осуществляете доступа ключам но немножко не очень хорошо себя показывает если вы исполняете запросы это первое второе так про это сказал дальше каким образом обычно таких системах организуются меж поточное взаимодействие иногда возникает ситуация когда этому потоку необходимо получить данные который владеет другой поток и в таких систем в таких системах обычно если эти два потока которые взаимодействует находятся на разных машинах использовать сокеты если же потоки находится на одном машине то можно использовать любые механизмы меж под меж процессом или меж поточного взаимодействия да то есть это могут быть и из о китае шерп memory что угодно значит ну и опять же да то есть преимущество такого подхода что достаточно хорошо стелется пока пока вам хватает лидер и вас практически нету накладных расходов на координацию доступа к данным от разным потоком и доступа к данным из разных потоков примеры где это используется atrides это сцилла вот они очень активно эту архитектуру продвигают в последних версиях да это stax вы тоже можете встретить у нее есть одна проблемка не очень приятно она заключается в том что ну давайте представим что у нас есть один клиент который исполняет простые операции типа get put которые исполняются довольно быстро а представьте что у нас также есть второй клиент который хочет теперь исполнять какие-то запросы которые бегут достаточно долго и в этом случае если так получится что этот долгий запросу нас в очереди задача оказался раньше чем короткие запросы на путь куда-то вот эта операция под она так иначе будет вынуждена ждать пока первая задача отработает и только после этого мы сможем никто не приступить этим самым если вот такой смешиваете разнородной операции то у вас практически неминуема будет расти их увлекаемся даже даже если в принципе вас еще я свободно какие-то собой на себе у которые в теории могли бы как-то попытаться эти операции вы выполнить в параллели и например если вы откроете документацию редиса где-то проблем описывать сам одна из рекомендаций а как с этим бороться они предлагают давайте вы для каждого типа операции сделать нам свой отдельный кластер скопируйте данные и отпускалось те запросы которые просты идут в один кластер запросы тяжелые который влечет лет инси поскольку в другой и но это решение имеет право на жизнь но она предполагает естественно более сложный deployment больше расход ресурсов потому что вам нужно фактически данные продублировать вот поэтому это такая не очень приятная проблема которые в таких системах встречается альтернативный подход альтернативный подход что то можно сделать можно использовать как раз таки так называемый states and even архитектуру которую мы применив взял кости которая заключается в том что вместо того чтобы исполнять задачи сразу же в топ с но потоки сразу в потоке в которой мы получили сетевой запрос мы делегируем исполнения задач и в какой-то другой thread пул который уже независимое их из их исполняет и выглядит это так что вот когда клетка нам предстоит запрос у нас есть специальный ее пул это набор потоков до который отвечает исключительно за сетевое взаимодействие чтение и запись мы получаем этот запрос его каким делом какой то предварительно парсинг определяем его тип после чего перекладываем в труппу который уже отвечает непосредственно за данный тип задач и соответственно пока это задача исполняется другом поля на шее и поток может продолжать вычитывать новые тоски и распределять их распределять уже под под по другим маркером а значит и преимущества такого подхода главное преимущество пожалуй его заключается в том что он сразу дает системе очень высокую гибкость потому что под разные типы задач вы можете организовывать разные triple и с разной внутренней структурой про которые я вам сейчас расскажу и тем самым гибко адаптироваться под разные виды на под разные виды сценариев использования в частности входил качеств у вас есть четыре основных трек пула 1 purple это так называемое apple всего всегда у нас начиная с исполнении конецкого запроса это полу который соответственно слушает о китае пишет обратно в китай и у нас есть три специализированных пола уже под конкретные типы задач есть так называемый generic полу который исполняет произвольные компьют операции которую можете к нам в кластер отправлять есть специальный пол который отвечает за изменение данных то есть за применение за исполнение в или операции и есть еще один полк который мы используем для исполнения с стэн для стрим для stream процессинга и как мы видим дальше все титулы работают внутри совершенно по разным принципам но тем не менее в данной архитектуре мы можем их легко ну очень легко интегрировать в систему ядов и адаптироваться под разные типы нагрузок так значит всегда исполнении задача у нас начинается с ее пула который выглядит следующим образом у меня в системе всегда застрахован ровно один поток так называемые и цептор и который в которой просто слушает входящее соединение от клиентов как только и у нас кроме того есть набор так называемых гоев у worker of когда клиент хочет открыть коннект к серверу он сначала приходит в рецептором задача которого просто создать создать connection и передать его в соответствующего рокером и тем сам од на данном слайде окнам представьте пришло одновременно два клиента мы открыли соединением после чего эти connection и передали уже в ее квартиры для чего нам это нужно необходимо дело для того что если к нам если к нам открывается очень большое количество соединений то нам необходимо каким-то образом их сбалансировать по разным квартирам потому что 1 квартира в принципе может не хватить на обработку всех запросов и здесь вот мы изначально используем простейший простейшие round robin чтобы распределить connection и по по квартирам после чего worker и уже работают скрин с клиентскими истринским с опытом напрямую значит далее каким образом дальше мы работаем с запросами в рамках одного worker а представьте что нас есть два клиента которые у 2 клетки соединения которые работают с одним и тем же квартирам эти соединения отправляют нам запрос и запросы могут быть разбиты на один или более пакетов и соответственно дальше у нас в каждом ее worker есть тебя клуб которые просто периодически слушаний не в не слушает не появилась у нас на вход новых пакетов после чего пытается эти пакеты собрать знаю длину и как только очередной пакет и у нас оказывается полностью собран мы зная его тип отправляем вас это и сущего рокера в соответствующей 3 пол именно на данном слайде ответе даже у нас пакет сперва клиента оказался разбит на две части ну например потому что он оказался слишком длинным вот поэтому так получилось что пакет что запросто 2 клиента мы собрали раньше отправили после чего на следующем цикле вы и глупо мы уже собрали цель ко второй пакет собрали запроса перва клиента это же отправили вас этой существует пол для того чтобы такая механика работала нам приходится ну вас достаточно тривиальная поддержка на уровне протокола то есть у нас в заголовке запросов всегда присутствуют длина потому что с помощью длиной мы понимаем сколько еще пакетов от этого запроса нам необходимо получить и в заголовке присутствует тип этого запроса потому что на основе типа запросы мы понимаем в какой-то пул его отдать дальше особенностям на первое что на практике на практике таких ее worker of требуется не очень большое количество то есть обычно вот мы по дефолту стартуем 3434 потока который отвечает за и о взаимодействии для очень большого количества сценариев этого хватает сполна и и на графе тебе количество этих потоков можно даже уменьшить иногда тем не менее под большой нагрузкой и worker и не справляются их над стартовать больше и ещё одна особенность заключается в том что иногда между этими варварами может возникать некий дисбаланс например связаны с тем что представьте что вы маркером каждому подключилась например по 10 по 10 разных клиентов но 1 рокера вдруг вы вдруг 8 из них уже своей свои коннекта закрыли получается на первом матче с 80 лет на первом два клиента на котором 10 тем самым а для того чтобы избегать такого перекоса в нагрузке мы используем там простейшие джобс telling алгоритм который может перекидывать конечно между маркерами чтобы выравнивать нагрузку между ними дальше интересно что такой же подход со стадиями можно применять и на стороне клиента да потому что если вы это сделаете то во-первых даже если у вас на средне клиенты есть большое количество потоков которые хотят исполнять какие-то операции вы можете ограничиться всего навсего одним одним socket подключением к серверу с вами требуется скажет о потока новой новый новое соединение открывать вами требуется создавать никакие мы использовать никакие the red bull и фу не использовать пулы соединение вы просто работаете с клиентскими пойму все для вас происходит абсолютно прозрачно второе преимущество заключается в том что не блокирующий мастер не клиенты облегчи реализовывать более сложные протоколы и или например асинхронной операции потому что в случае с блокирующим ее как правило мы провели операцию ждем ответа провели операцию ждем ответ в случае с неба кричи моим эти две стадии отделяем друг о друга и поэтому реализации и реализации не блокирующего вызова на средних лет остается очень простой вот поэтому для многих наших клиентов которые от из коробки в продукты идёт полный flow запроса выглядит примерно следующим образом да то есть мы изначально стартуем на стороне клиента с пользовательского потока делаем с обитаем задача после чего на она переходит в и потока клиента на отправку отправляется попадает в поток сервера после чего он исполняется на сервере в соответствующем поле после чего по обратному пути идет обратно в клиент вот и на практике нам такая такая конфигурация зарекомендовала себя достаточно хорошо и в плане масштабируемости плане гибкости теперь поговорим о том какие уже специализированные трату у нас имеются в системе первое во первых у нас есть так называемый generic пол это просто пул в которой вы можете исполнять произвольно операции то есть если вы посмотрите на перед ней и приходил к 100 то вы увидите что там есть возможность исполнять просто произвольный код если вы знакомы с java ту это можете просто реализовать интерфейс рана была ли call был просто эту его отправить в кластер на исполнение и в этом случае мы используем такой прям очень очень примитивный простейший дэдпул в котором между ее потоками и потоками которые занимаются исполнением компьют задач находится обычно блокирующий очередь из этого цена потоки которые исполняют эти тоски они просто повод но они просто берут верхнюю тоской из очереди и и и и исполняют здесь что-то более сложно придумать проблематично и мы что-то более интересное эффективно и потому что мы не знаем природу тех задач которые вы к нам отправляете то есть некоторые задачи у вас могут быть короткими некоторые долги и некоторые допустим могут блокироваться некоторые нет и поэтому так как мы никаких предположений сделать не можем мы используем такой простейший тпу дальше что касается операции над ключами от операции put операции get здесь давайте представим что мы бы таким же образом попытались организовать работу с ними использовали бы об обычно трипуру да тем самым операция над любым ключом может быть исполнено в любом потоки здесь возникает проблема том что может возникнуть такая ситуация когда два клиента пытаются изменить один и тот же ключ и здесь мы приходим кроме той же проблеме про которая горел в начале доклада когда нам не пойдем каким-то образом организовать конкурентный доступ ключам таким образом чтобы мы не сломали внутри носить система то есть если мы это сделали таким образом нам бы потребоваться это синхронизации на операциях тут get то есть в теремок java мы бы скорее всего там использовали какие-нибудь рендера блокеры синхронно если что-то еще вот это как я уже сказал не очень эффективно поэтому здесь мы в общем то тоже используем подход в котором мы бьем все множество в котором мы говорим что конкретный поток из нашего пула может работать только с определенным цветом ключей и тем самым опять же все проблемы с многопоточном доступу чем у нас автоматическим автоматически уходят значит с точки зрения внутренней организации это выглядит так что у нас месяца опять же и и пулы которые посмотрев на ключ понимает каком он находится посмотрев на ключ понимают в каком портишь неё находится зная номер портишь но мы определим номер потока который должны быть который отвечает за за ключи находящийся в этом в этом пар тишине после чего мы отправляем запрос переадресуем этот запрос конкретного в тот поток который конкретно кто-то worker и который отвечает за этот ключ и для этого у каждого брокера имеется отдельно очередь в этом случае можно использовать такую боль оптимизированную не было бы липкими zero ванную очередь мог ты у продюсера сингл консьюмер потому что здесь фактически да у нас есть много продюсеров это все ее потоки каждый из них может в текущий поток что-то вставить в его очередь но с данного из данной очереди забирает задачей только один потока поэтому мы здесь использовали у более оптимального реализации очереди дальше но преимущества и недостатки в принципе уже вам сказал теперь давайте расскажу какие структуру данных мы используем в этом пуле если вы работали с жабой или с каким-то фигурками основанными на джаве вам там скорее всего встречался терминал хип хип это такая такая боль и java и связано с тем что современные garbage collector и не очень хорошо работу с большими типами поэтому если вы берете какой-нибудь такой боли мы более-менее солидный сервера и если у вас имеется на борту хотя бы там 32 гигабайта или больше а это сейчас можно очень часто встретить это не так дорого стоит the garbage collector может начать достаточно сильно тормозить работу вашей системы да и поэтому ответ на эту проблему со стороны вендоров у которых решения базируется на джаве как в нашем случае это так называемое архива так называемые хранение данных в of happy happy это означает то что на хранятся хранится вне типа java и соответственно горбач коллектор ничего не знаю про эти данные и поэтому соответственно он не тратит время пытаясь их там попытаюсь потоси их собрать вот поэтому как правило в таких в такого рода системах типа хохолка ст вот часто встретите несколько способов организации хранения данных можно хранить happy боже храни устану можно хрень if he пейджа вы можно хранить в них и поджала и вот для хипово реализации ну естественно используем там все коллекции которые нам дает java с некоторыми поправками но суть одна и та же то есть у нас имеется базовая структура данных в терминах жавотта конкурент haш миp который хранить непосредственно непосредственно по рыбке и волью и у нас имеются вторичные миксы для ускорения запросов для которых мы используем обычный обычный конкурентный skip листа а вот для архип а мы используем уже однопоточный структуры данных и в частности для хранения кивали пар мы используем однопоточный hishum em с открытой адресацией а для организации индексов мы используем она по 1 поточное красно-черное дерева и тут возникает интересный вопрос об чем мы связаны вот это отличие да потому что как мы с вами проговорили в принципе так как у нас каждый worker и отвечает строго за свой сайт ключей нам бы даже в типовой реализации хватило бы однопоточный структуры данных в принципе по большому счету сейчас можно взять и с очень небольшими усилиями возил кости изменить вот это те как con carne структуру данных на 1 поточные и ответ такой что вот эти хиповое многопоточной структуры данных они ну скажем так остались продукте по историческим причинам но практика показывает то что очень хорошо что не там остались почему потому что как я уже сказал вот это так называемый шире на нас young организация доступа к данным когда вас один поток отвечает за свой собственных она действительно плохо работает если вы хотите исполнять что-то более сложное чем операции дедпул например вы хотите вспомните скальный запрос да и поэтому вот в текущей в текущем варианте оказывается что мы мы мы используем это свойство и в продукте а вот если вы работаете с типовыми структурами данных то мы как раз используем тот факт что он является потока безопасными и запрос исполняем с одного потока потому что мы заведомо знаем что наши запросы не изменяя данные и поэтому мы просто смело тренируемся по этим структурам а в случае с архипова и реализации мы действительно обязаны при моем именно обязаны взять ее изначально запрос которые вы к нам отослали разбить нас на составные части распределить их по всем потоком которые обращаться к данным исполнить их независимо друг от друга и после этого собрать из-за чего соответственно производительность таких запросов конечно оказывается зачастую ниже чем мы бы хотели поэтому на практике скорее всего в будущем будем что-то с этим делать и без того чтобы типовых структур и делать она поточными мы с мы с гораздо большей вероятностью архиповой структуры сделаем многопоточные несмотря на то что в общем-то для get под операции это не особо-то и требуется дальше у нас есть ещё один важный 3 пол есть только продукт называется хабблкаст джед это платформа для stream процессинга и особенности стриминговых задач заключается в том что они ну а ты до могут исполняться достаточно длительное время в общем случае они никогда не завершаются потому что вам постоянно приходят какие-то данные новые вы как-то перемалываете отдаете дальше и поэтому общем-то худшем случае стриминга ваза задачи работает бесконечно долго вот и возникает вопрос а каким образом нам организовать исполнение таких задач если их потенциале можешь приходить очень многое десятки сотни значит первый подход который можно рассмотреть это создавать и телик поток на каждую такую тоску вот нам опять же это такой подход у вас достаточно быстро перестанет масштабироваться потому что чем больше потоков из создаете тем больше у вас накладные расходы на следуем к этих потоков альтернативный подходит про который можно было бы подумать это может быть как-то ограничить количество таких потоков сверху то есть мы говорим что старту стартуем не больше чем n таких потоков да тем самым проблемы соске другом может решиться но если мы это будем делать то может легко оказаться так что допустим представьте что мы ограничили количество таких потоков двумя от вас две задачи пришли стали исполняться при приходе 3 тоска ей в общем то нету потока куда мы можем его отдать поэтому на плана просто ждет не учит а то что стриминга вы и задачи могут исполнять реально долго она может ожидать потенциальные неограниченное время вот поэтому ни первый ни второй подход здесь не подходят да и вместо этого мы используем мы полагаемся на так называемую кооперативную многозадачность то есть для jetta у нас есть red bull с фиксированным количеством worker of обычным xsd мб по количеству ядер но каждый этот worker каждом этому маркеру принадлежит сколько-то задач эти задачи выполняются в карб в кооперативном режиме это означает что мы постоянно и tury.ru и мся по списку задач которые принадлежат данному маркеру и пытаемся каждую задачу из пытаемся скажу так продвинуться в исполнении конкретной задачи но периодически мы останавливаем и исполнение и передаем контроль уже другой тоски то есть мы самостоятельно принимаем решение когда вам переключать контроль между разными тасс коми в рамках этого потока вот обычно это приключение происходит тогда когда либо ваша стриминга vaio тоска ожидают какие-то данные а другого узла но тебя на еще не пришли то есть мы в этом случае ни в коем случае не будем блокировать поток мы просто отдадим контроль другой тоски или же другая ситуация может возникнуть это когда ваша задача хочет записать данные на какой-то удаленный узел этот узел например щас и не готов эти данные принять ну потому что допустим сработала сработал потому что допустим мы и так уже слишком много данных отправили мы ждем пока он их исполнит и только после этого проперло продолжим записывать данный ему вот поэтому таким вот образом у нас работает же и в общем то вы видите наверное да что по большому счету но для каждого типа задачи у нас имеются свои предписанные особенностями до которые в текущей реализации системы очень легко создавать встраивать в нее и тем самым сама система получается достаточно гибкая так это пропущу тир самый главный слоя и каким образом это выглядит с точки зрения цифры на самом деле конечно бенчмарков можно придумать огромное множество в данном случае я попытался симулировать операции под с достаточно небольшим валью что-то типа там 128 байт машина я взял их на авось и но такую средней производительности у него 96 виртуальных ядер за стартовал 8 клиентов на отдельных машинах и стал просто наращивать количество потоков на клиентах да и в принципе от чего смог добиться практически из коробки это то что наша система практически линейно с масштабировал ась да она практически линейно масштабировались до 1000 клиентских потоков после еще одну масштабировать этом скажем так сгладилось отчасти да и уже росло нет не так быстро как бы мы хотели но тем ни менее тысячи потоков это уже очень хороший результат и в пике общем то с этого сервера без какого-то супер тюнинга я вот вытащил два с половиной миллиона операций если посмотреть на то сколько каждая операция съедает сети и как вообще пропускная способность сети я могу сказать что на самом деле с эту машину можно было бы выжать где-то у два раза больше операции то есть тут на самом деле надо было бы еще по разбираться с чем-то связано что мы перестали дальше и линейно масштабироваться я думаю что одна из ряда причин это клиенты потому что клиенты в данном бич марки были не очень мощные там вот c4 2x отчёта по это 4 и физических ядра и вполне возможно что мы в них уперлись в целом в целом система ведет себя очень даже очень неплохо да выдавая миллион операций в секунду значит тем не менее тем не менее можно не только хвалиться но говорить о проблемах которые есть в таких таких решениях в наш и в нашей архитектуре и главная проблема это накладные расходы на нотификации что-то значит каждый раз когда вы хотите переложить задачу с-1 потока в другой вам нужно каким-то образом принимающий поток модифицировать о том что появилась новая операция на исполнение и обычно мы обычно что что мы используем для этого мы мы используем какую-то синхронизацию и эта синхронизация она отнимает ресурсы к сожалению если извне это выглядит как просто дополнительный отдыхает на операции как-то можно увидеть вот например flame граф который показывает так как себя ведет и и потоком то есть это поток который читает данные сок этой пишет данные в супер и перекладываться ответственно тоски в квартире на исполнение и внесет мы их правой части мы здесь видим на что вот мы у мы делаем систем комната чтобы взять данные соки то вот мы пишем данный пакет вот мы висим на селекторе ожидая поступлением данных а вот а вот слева есть какая-то странная фигня которая съедая достаточно большое количество времени и здесь в данном случае это оверхед на то чтобы модифицировать тот worker которому мы передали тоску о том что то тоска к нему пришла да и здесь это занимает примерно в 30 процентов времени на самом деле конечно это синтетика эту сетку специально пытался сделать максимально маленькой легковес операции чтобы хорошо подсветить да на практике при эксплуатации вы конечно такой жести не увидите но тем не менее важно понимать что такое overhead есть могу вам показать обратную ситуацию вот это пример потом графа сварки ра который наоборот исполняет phuket операции и после исполнения хочет сказать о ее потоку что я perros исполнил пожалуйста профи вот этот ответ на клиента и опять же мы видим что вот слева происходит непосредственно где то вот здесь вот происходит не посредством операция put out on справа справа это как раз таки опять же overhead на то чтобы чтобы найти fits и ровать а ее поток о том что пришло новое операция на исполнение вот и поэтому конечно это является недостатком station thriven архитектуры вот эти дополнительные на накладные расходы здесь очень важно понимать что эта гибкость до которого на 6-м присутствует она не дается нам бесплатно с этими и накладными расходами можно и нужно бороться до общей подходит здесь заключаться в том что нужно всеми силами пытаться уменьшить количество нотификации для этого можно использовать bathing можно использовать какие-то были хитрые оптимизации углом пару примеров продемонстрировать что мы делаем чтобы с этим бороться пример 1 представьте что вы находитесь в потоке и вот вас-то мясники в клуб и вот вы получать новые тоски с буду получать новые пакеты от клиентов и можете брать и по одному такому пакету передавать в worker тем самым вы будете вот здесь вот когда вы будете делать сабит операции worker вы на каждую пакет будете worker на каждый пакет будете делать одну нотификацию что можно альтернативно сделать альтернативу можно попробовать вычитать максимальное количество пакетов которые сейчас у нас есть в сокетах далее завопить эти пакеты уже на конкретные маркеры после чего уже в мочевом режиме с помощью пиратским условности цветов передать сразу много пакетов в worker и тем самым при таком подходе вы каждый worker будете модифицировать не более чем один раз в сумке the notification становится меньше это один вариант что можем делать второй вариант можно пытаться адаптироваться под эту нагрузку которую вас есть вот здесь вот мы используя вот этот station and rewind подход для клиентов исходим из пессимистичного предположения что на стороне клиентов есть много потоков которые которые все активно сами тут какие-то тоски одно иногда оказывается так что на самом деле на своими клиента большой нагрузке нету и без того чтобы на клиенте перекладывать задачу сначала с польского потоковое поток можно попытаться отправить ее напрямую из пользовательского потока сразу на сервер и тем самым минуя вот этот самой ее поток и в некоторых случаях если нагрузки нету мы там через простейший compressed это это дело проверяем и иногда можем миновать эту дополнительную фазу далее иногда можно косвенно добиться оптимизация оптимизировав сам сам протоколов сетевого взаимодействия и вот например исторически в хабблкасте когда вы делаете по рацию put если у вас имеется бы капля то есть дополнительная копия данных то сначала клиент отправляет запрос на узел который является праймари для данного ключа после чего с праймари мы отсылаем запрос на backup после чего обратно обратно цепочкой засыпка по отвечаем на праймари и и после чего отвечаем на клиент и на каждым вот этой фазе передачи запроса или response а мы проходим через все вот эти фаза ее поток worker а ее поток снова и так далее здесь будут конкретно для данного случая можно оптимизировать оптимизировать сам сетевой протокол и сделал так чтобы мы с bacopa отвечали не на primary узел отвечали сразу же на клиента обратно но потому что по большому счету никакого большого смысла отвечать сначала на примере нету и вот последней версии мэтт оптимизацию сделали и несмотря на то что главное преимущество в производительности здесь появляется из-за того что мы уменьшаем количество сетевых запросов на один с 4 до 3 тем не менее косвенно мы уменьшаем количество вот этих фаз через которые мы наша тоска должна пройти дальше еще интересно то что вот если вы посмотрите на продукты то в большинстве которые предполагают наличие большого количества потоков то как правило у них у всех достаточно такие кривенькие дефолты и заключается это в том что как правило мы то есть при представьте у вас есть сколько тот бред плов и вам нужно каким-то образом определите сколько потоков вы хотите давать тому или иному полу и это количество потоков часто коррелируются с тем насколько много или мало нагрузки будет приходить в этот пол так как заранее профильной нагрузки неизвестен очень часто продукты просто пессимистично стартуют скажи на поле потоков по количеству ягера и на это привели к тому что суммарное количество потоков созданных становится больше чем количество ядер если так иначе все типа только находится в работе то это приводит к тому что у вас возрастают накладные расходы на скиду линк и особенно болезненно это проявляется от особенно на системах с виртуализацией то есть если вы сидите за каким-то гипервизором то это может достаточно болезненно бить по по производительности вот поэтому здесь очень важно что когда вот вы сталкиваетесь такого рода системами заниматься неким тюнингом то есть грамотно определять количество потоков которые требуются патрульную стадию из и стараться делать так но особенно то есть если вы работаете не на голом железова скорее всего вам нужно стремиться сделать так чтобы суммарное количество активных потоков не превышало количество лидером вот еще один интересный момент что конечно на производительность часто влияет и топология топология процессора и опять же системы с station древней архитектурой более подвержены проблемам здесь почему потому что как правило все таки потоков у вас больше масштаб поток вас довольно много и все они так иначе активное и они могут быть активными на разных соки такси пьем и тем самым представьте что вы получили запрос на в вы об обрабатывайте сетевой запрос на одном соки эти маркеры которым вы передаете на дальнейшее вполне находится на другом тем самым вам приходится одну процессору приходится лезть в память другого процессоре kanso и из-за чего производителя снижается опять же это в большей мере проявляется в тех случаях когда вы работаете не на голом железе а работаете в какой-то детализированной среде поэтому это приходится учитывать по большому счету от этого страдают ну так иначе практически все вот эти все все системы но вот стоят что в древней архитектуры это все-таки встречается чаще поэтому здесь с этим необходимо бороться и на данный момент могу вам сказать что байму скажем так преимущественно продукты определяя количество необходимых потоков использовать некую статическую конфигурацию то есть вы должны на старте узла указать сколько потоков вы хотите чтобы вас было и так как профиль нагрузки может изменяться это работает мы скажем так это работает но это доставляет определенную головную боль вам как администратору этой системой конечно на практике гораздо лучше если система умеет динамически адаптироваться к профилю нагрузки или к тому железу которое на использует и вот мы их на данный момент мы тоже используем статический подход то есть количество потоков вам необходимо задать заранее но это скорее всего до дня одним из важных направлений улучшений в ближайшее время у нас чтобы сделать так чтобы из коробка вас все работа уже в оптимальном режиме без того чтобы просить вас каким-то образом высчитывать сколько потоков отдать сюда сколько потока подать сюда итога давайте быстренько еще раз пройдемся по тем принципиальным архитектурным решением которые но через которое потенциально вам может потребоваться пройтись и при проектировании такого рода системы то есть 1 мы говорим о том что мы можем использовать либо блокирующие или бани блокирующие блокирующие и хорош тем что он просто реализация то есть если можете его использовать пожалуйста используйте но если система находится под большой нагрузкой предполагается что будет много клиентов то конечно не блокирующего и это наш выбор и большинство современных систем использовать именно его далее задача можно заполнять непосредственно в том же потоке которые работают соки там как это на пределы travis rice цела преимущество подхода что нет накладных расходов на нотификации потоков как я вам показывал да но негативной стороной является то что задачи могут ну что вы можете получать в личный 17 если вам приходит много разноплановых задач например как задач короткие какие-то длинные записи вас может не устроить альтернативный подход это использование стоишь в древесной архитектуры в котором под каждый тип задач у вас есть отдельный поток это естественно более гибкая более гибкая архитектура которая хорошо адаптируется под разные виды нагрузок но с ней необходимая зачастую повозиться в ее такой более тонкой настройки юлию есть есть на накладные расходы на передачу задачами между потоками но что интересно что на самом деле в рамках иного продукта оба подхода не являются взаимоисключающими то есть тот факт что мы сейчас используем station 3 вон архитектуру не отменяет того факта что мы можем взять и переписать наше наш обработчик киева или операции на первый подход то есть нам ничего не мешает открыть еще один открытие ещё сколько-то сока collection of с клиента который будут уже напрямую сразу же исполнять эти тоски может быть в ближайших релизах мы это рассмотрим и наконец последнее тоже вал важное решение это каким образом вы организуете доступ к данным то есть ли вы можете или либо сказать что у нас каждый поток может обращаться к данным может их и и и читать изменять в этом случае у вас вам нужно каким-то образом заниматься синхронизация доступа к данным но тем не менее такой подход хорошо справляется с операциями которые обращаются большому количеству ключей например со своей вскоре запросами и оперативный подходят это такой прям чистый ширад настолько когда у вас каждое ядро отвечать за свой суп со данных плюс есть опять же в том что он идеально подходит для маленьких задач которые работают только с одним парте шинам но этот подход показывать не очень хорошую производительность при тяжелых операций которая требуя которой от рук требует доступа к нескольким практичным вот ну архитектура хазел касты the station древняя архитектура преимущественно шире наценка почему преимущественно потому что так как мы используем в некоторых местах конкурентной структуры данных запросы мы иногда можем исполнять с разных потоков не переходя в партию потоки ну и конечно важная рекомендация заключается в том что при использую таких продуктов очень важно понимать как или устроено внутри потому что в зависимости от этого вы сможете понять как как правильно их тюнить потому что тюнить их современная современной инфраструктуре когда у вас есть и железо и виртуализация тюнинг очень важно потому что порой с него можно вытащить просто на пустом месте увеличить производительность в близкой к 1 поэтому используйте все доступные вам способы ну на этом все спасибо за внимание а вопрос у кого-нибудь есть владимир спасибо за доклад очень познавательно у меня вопрос по трек прапор тишину если возникает ситуация каждый против к и каждый пропишем там имеет свой цвет который только кто который может у него писать правильно и каждый такой трек имеет какую-то очередь перед собой а если парте шин становится весьма горячим то есть там много где-то часто меняющейся данные и этот трек не успевает и очередь перед ним начинает расти как такие ситуации разруливаются ну это это действительно проблема данного подхода да то есть у разных эндеров могут быть разные рекомендации у некоторых даже есть там специальные тулы которые могут вам показывать такие про тишины которые находятся под больше нагрузка это и здесь но здесь как бы выход только один это каким-то образом делать ришар ding данных то есть где то это можно делать в динамике но как как правило все таки предполагается что если вы так с такой ситуации столкнулись вам нужно сесть и переосмыслить функцию который вы используете для распределены данных например если вы используете распределение данных похожу то может быть вы можете для хэширование к ключу там при приписать как дополнительный там суффиксы дактру и тем самым эти кучи ключи которые сейчас находятся на протяжении распределить по разному но как бы это слабая сторона данного подхода практически праздника steel что произойдет out of memory или там нет а вот в море здесь не произойдет просто вы увидите что у вас трупу но если вы действительно в вы действительно уперлись то есть то есть допустим все запросы по сегодня про тишина 4 все пошли в один поток значит мы видим что у нас не более чем один цепью используется системе то есть мы все перлись и на практике это будет означать что рано или поздно вы увидите что трубу перестал у вас расти в этой сила стала расти разгрузка система находится на не очень высоком уровне а вот с клиентскими запросами которые мы хотим сможет встать в очередь ну не можем вставить они просто мы даем а тут какой то что тот клиент пишет много данных да и watch либо нам выбор либо очередь увеличивать либо я не знаю просто ему отказывать в обслуживании как мы не можем той запрос обработать переполнилась ну в данном ну это уже другая проблема да это это я проблема не горячего прадеш на проблема того что слишком много слишком ага я скажу колецки запросов приходит пора тишины вот на практике она выстреливать не так часто потому что почему потому что пользователи предпочитают так или иначе использовать синхронный 5 это означает что в моем примере допустим даже две тысячи потоков травят операции так будто к колесе сохранные в каждый момент времени одновременно на стриме сервер исполняется не более чем 2000 операций что принципе можно ну психи размеры череде ты этого хватит прекрасно проблема здесь могут возникать если используются асинхронный к я тоже сосин кроем это может даже с одного потока заставить те сервер так при un ete скажем так вот в кости сейчас какое-то супер супер продвинутого супер продвинутого быть причин для этого случая нету то есть я затрудняюсь сказать как система себя поведет наверно да по среду что документация говорит вот но скорее всего в блистере лесах это будет как-то изменяться улучшаться спасибо еще маленький мир к теме опрос если не секрет эксперимент на выяснил дорого вышел не очень конкретно долларов в 40 нет ну вот этот эксперимент дадут долларов 30 спасибо большое ещё есть вопросы ну если вопросов нет и если вдруг они возникнут а в дискуссионной зоне вы можете все обсудить пользуемся давайте все-таки поблагодарив докладчик каждый доклад"
}