{
  "video_id": "15lWiZkyweA",
  "channel": "HighLoadChannel",
  "title": "Практика использования NOSQL в высоконагруженном проекте / Дмитрий Ананьев (Mamba)",
  "views": 515,
  "duration": 2553,
  "published": "2017-04-25T08:02:58-07:00",
  "text": "на сцене дмитрий ананьев компания мамба с докладом практические вопросы использования но и скилл высок а нагруженном проекте встречайте здравствуйте уважаемые коллеги меня зовут дмитрий ананьев я работаю в компании мамба сегодня я хочу рассказать вам о том какие но squeal решения мы используем в мамбе как именно используем с этими проблемы сталкивались а также поговорим о ряде сопутствующих вопросов почему возникла необходимость использовать на успел давайте посмотрим на примере одного из наших сервисов сервис мессенджера это сервис личной переписке вот представьте у нас пол сотни мой скил серверов причем под довольно существенно нагрузкой баз данных горизонтально масштабированы виде шарден га на каждый сервер идет порядка 350 запросов в секунду но четверть всех запросов идут на работу со счетчиками сообщений у нас есть сообщение они группируются по контактам контакты группируются по папкам вот у каждой папки отображается количество непрочитанных сообщений посмотрите какие запросы при этом выполняются это же обычный инкремент i get по ключу зачем имя нагружать базу данных ну мой скил в частности с этим же прекрасно справится но успел так вот именно от этих запросов мы и хотели избавиться дело в том что нас аудитории постоянно растет и вот при этом наши мой скил сыра уже переставали справляться с нагрузкой ну не так хорошо по крайней мере поэтому это имело смысл и так мы начали искать такое но успел хранилища которое бы позволяло нам который бы удовлетворял удовлетворяла нашему профилю нагрузки ну конечно с одной стороны мы могли взять тупо memcache но дело в том что он мало того что не персистенции так и хранить все данные в памяти а при нашем профиль нагрузки как раз это было лишнее потому что большая часть данных было не востребована дело в том что у нас в плане сидит порядка одного процента пользователей ну естественно они общаются преимущество между собой на тот момент на рынке уже было несколько решений но скилл ну вот посмотрите мой бешеном сразу не подходил как я сказал он уже был не persistent ней и к тому же роман ли редис прекрасно справлялся с перси стен то стью но однако он тоже был роман ли а вот такие решения как memcache db это кейтеринг прекрасно нам подходили они были и персистенции и позволяли использовать базу данных размером больше чем ram как мы тестировали мы взяли инструмент с названием routes он написан на php поэтому это очень гибко решение она легко расширяется под свои нужды мы запускали его на 30 серверах на каждом сервере в 30 потоков и выдерживали нагрузку примерно 30 минут нагрузка подбиралась исходя из характера нагрузки на сам сервис ну вот нагрузку на messenger вы сейчас видите на слайде под нее рассчитывалась результаты тестирования показали что моим конечно б не очень хорошее решение но она мне очень подходит по крайней мере дело в том что при частой записи во время синко данных на диск быстродействия у него очень сильно падает ну где-то раз в сто таки serving не показал конечно каких-то заоблачных результатов но тем не менее он довольно легко справлялся с нагрузкой в 10000 тысяч чтение в секунду и 5 тысяч записей записи в секунду поговорим немного о деталях реализации дело в том что во время тестирования токи тиран то выяснился один неприятный момент при отключении сервера из розетки вся база данных терялась да но мы не пристрелялись мыши заткнуть эту дыру архитектурно сейчас объясню как мы взяли два сервера на каждом из них подняли по два демона токио there in the master и slave при этом слоев 1 сервером брал данные с мастеру другого сервера сами запросы мы отправляли только в мастер сервера мыслей естественно используя лишь один для этого со слов сделался только backup ну это довольно хорошо повышал отказоустойчивость например при отказе одного жесткого диска на спасала зеркалирование при отказе целую сервера она спасала перекрестная репликация мы в общем то ко всему были готовы но только не к этому да у нас упали сразу оба сервера два раза так вот сделать на самом деле ситуация очень простая кто-то убирал там мыл полы и выдернул розетка одна стойка у нас упала по питанию мы естественно переставили сервер в соседнюю стойку потом вырубилось электричество всем дата-центре так что стало очевидно нам нужно искать какое-то новое сказал решение потому что тогда мы потеряли всю базу пришлось откатываться на последнюю точку б к по но это не очень хорошо каждый раз делать на тот момент на рынке уже существовала достаточно большое количество но скилл решений но на самом деле с одной стороны они были несколько сложны для наших задач и в основном избыточные счастью именно в это время разработчик talked серванта выпускает ну точнее он проделывает работу над ошибками и выпускает улучшенную версию токи теру это называю kitty кун но мы решили не изобретать велосипед и дать второй шанс каковы основные отличия тетку на тоги терм то ну во первых конечно теперь уже баз данных была консистентной даже при падении сервера мы сами производим тестирование мы нагружали очень сильно сервер подходили и прям дергали из розетки нормально всё сохранялось до наученные горьким опытом ну так же кроме этого у него было выше скорость уменьшились накладные расходы на запись данных правда поменялся протокол вместо здоровым каша теперь использует http кроме этого у него появился удобный интерфейс и можно было расширять функционал j теку на с помощью плагинов в том числе написанных на lua можно было носить писать на лоб мы кстати пользовались этой возможностью например очень легко можно было при обрабатывать данные когда получаю запрос на запись например класть сразу несколько баз данных или строить какой-то более сложный индекс ну и так далее еще мы написали плагин который позволяет допустим реализовать свой сетевой протокол то есть мы могли не только по http соединиться но и поют zippy допустим напрямую отправлять запрос какие то что просто сэкономить на сетевых операциях результаты оказались впечатляющими на самом деле вот для небольших баз данных киото тикун по производительности был примерно такой же как мы мкэш но с ростом числа значений в базе производительность несколько снижалась но все равно нас она очень сильно и устраивала однако возникла проблема когда мы попытались записать сразу тридцать миллионов значений подряд то производительность начала резко деградировать ну то есть вот этот график показывает что когда мы записали 2 мили значение только то скорость записи упал до 250 в секунду на самом деле серого практически не отвечал почему это произошло а вот в чем дело просто киото тикун использует кэширование по кластер на ну вот предположим у нас есть plaster в нем ну к примеру 100 значений при попытке обратиться к одному этому значению весь этот кластер загружается в оперативную память при этом вытесняется предыдущий 100 значение к счастью именно в этот момент google выпускает в open source свою встраиваемую но скилл библиотеку базу данных л д б мы решили уже смотреть в сторону движков в чем особенность лавал db как он хранит данные он хранит данные высота ебал файлах что это такое но это обычный файл в котором последовательно записаны ключи и значения в отсортированном по ключам виде кроме того этот файл неизменяемый предположим допустим что это нам дает очень просто ну во первых конечно очень быструю скорость линейного чтения последовательного ну а если мы построим простой индекс еще к нему тогда мы также сможем использовать быстрое случайно и чтение а что же записью записи все очень просто запись происходит в тот же самый состав был файл только находящийся в оперативной памяти назовем его m people тогда вся запись происходит только в оперативной памяти а при чтении уже данные сначала проверяются если они в метель была потом только высасывал наглядно можно представить себе работу lldp таким образом у нас всегда есть дерево хранящиеся на диске и другое дерево которое находится в оперативной памяти при этом вся запись естественно дела в дерево которое находится в оперативной памяти а при чтении мы сначала проверяем дерево в оперативной памяти а потом смотрим только на диск такой алгоритм называется л см3 он довольно уже не новый периодически вот эти ветки под деревьев которые находятся оперативной памяти они сливаются с ветками которые находятся на диске вот вы видите внизу для тестирования мы воспользовались теме тестами которые идут уже в поставке л л д б какие тестовые данные мы использовали мы взяли 100 миллионов записей размером 100 байт в итоге база данных получилось размером 11 гигабайт обратите внимание что вверх он состоял всего 10 процентов но так же мы использовали сжатие при этом уже размер базы данных сокращался до 1 гигабайта ой простите до 6 гигабайт один гигабайт мы использовали размер кэша мы тестировали 3 движка вот синий график это киото кабинет 3 красных график это киото кабинет хэш оранжевый это ловил д.б. ну вот киот кабинет движки это те которые использовались и в kitty куни так вот результаты оказались такими при случайном случайные записи а л д б показывает просто огромное превосходство над киото и случайно и чтение примерно у всех одинаковые но все-таки у сша сильно больше но мы помним что при росте базы данных в киоте производительность будет деградировать вот ул л д б такой проблемы не оказалось обратите внимание какая быстрая последовательная запись и последующей последовательное чтение ул д б на самом деле вот эта скорость в общем-то сравни линейному чтению записи на диск оказалось то есть что это означает для нас как для разработчиков если мы захотим загрузить базу данных целиком или допустим сделать бэкап то мы это сделаем очень быстро ну понятно google выпустил отличный движок а что сделали мы мы взяли библиотеку level db дописали к ней сетевую часть реализовав протокол джейсон рпц но для удобства также сделали мастер слоев репликацию еще мы добавили дополнительный функционал например это метод encode что он делает он инкремента значение в базе но при этом если в базе не найдено этого значения он его просто добавляют но это удобно это происходит а там 1-ой второй метод это апдейт пакет он позволяет обновлять данные которые хранятся в виде стилизованного массива например если мы хотим ну к примеру проникли метить несколько значений массиве то мы можем воспользоваться этим методом и это тоже произойдет от омар на следующий метод great range он позволяет получить сразу целый диапазон значений из базы данных при этом достаточно указать только начальное и конечное значение ключа эти запросы оказались не такими быстрыми поэтому по умолчанию мы все отправляли на slave на слайде результаты то есть как у нас сейчас какая нагрузка идет на a-level db в продакшене это порядка пяти тысяч запросов на чтение в секунду при этом средние время одной операции чтения занимает 0.6 десятых миллисекунды 1600 апдейтов и так далее размер базы уже превысил 200 миллионов записей при этом никакой деградации производительности не наблюдается как в киото было сейчас также можно тестируем базу данных на 2 миллиарда записей тоже пока отлично все идет напоследок я хочу обратить внимание еще на одну особенность две шкалы л д б дело в том что периодически во время слияния по деревьев вот в памяти и на диске среднее время ответа на запрос и немного увеличивается ну вот посмотрите синий график это среднее время от это на get запрос и вот здесь это происходит каждые десять минут от чего это зависит это зависит на самом деле от того сколько какой объем оперативной памяти вы выделиться для буфера записи у нас этот размер составляет 128 мегабайт всего если его бесконтрольно увеличивать тогда может это сильно отрицательно сказаться на производительности lldp во время слияния по деревьев тем не менее 95 процентов всех запросов у нас выполняется менее чем за вот одну миллисекунду а 99 процентов всех запросов менее чем за две миллисекунды что в это время происходит с утилизацией диска она возрастает до 25 процентов для мониторинга мы использовали zabbix вот от графика от него также мы использовали свое решение btp это предыдущий график доступный на гитхабе подведем итоги совершенно необязательно все простые запросы отправляют на мой скейлз с ними прекрасно справится но успел сначала мы попробовали использовать стойки тиран но у него были проблемы с отказоустойчивостью затем на смену ему пришел тот секунд и стало ясно что он подходит хорошо только для небольших баз данных на больших у него начинает производительность деградировать тогда мы написали свой сетевой демон поверх готового движка л д б он показал отличную производительность на самом деле вот я рекомендую попробовать спасибо за внимание задавайте вопросов подскажите пожалуйста на сколько увеличилась нагрузка на мой стиль после перехода на этот движок то есть у вас была проблема именно в этом насколько понял да ну я отказал уже что нас четверть всех запрос оставляли как раз это запрос который мы перенесли на но успел но у нас было при этом в пол сотни моя спел серверов вот мы с них четверть нагрузки сняли и сейчас в общем эта нагрузка ушла только на один сервер слал д.б. то есть таким образом освободились ресурсы понятно и еще один вопрос не могли бы вы поподробнее рассказать про из st paul то есть там была такая фраза что это не изменяемы файл каким же образом тогда есть мнение это происходит да хорошо на самом деле там конечно есть еще ньюансы но основной момент что из остывал это ну просто файл в котором последствии записано ключи и значения ключа отсортированы когда этот файл читается в оперативную память то из него очень легко построить дерево на самом деле то что значение уже отсортированы неизменяемый он только потому что мы так решили что давайте не будем его изменять это даст нам как раз вот те плюсы с очень быстро последовательным чтением у него есть индекс у этого файла и вот за счет индекса еще и случайно чтение тоже быстрая запись происходит как я уже сказал целиком оперативную память при этом когда заполняется этот буфер записи то эти файлы просто на диск целиком последствия записывать причем в отдельном потоке то не мешает там чтением другим операциям ну то есть вы в качестве ключей какие значения используйте в качестве ключей да у нас стройки в основном где-то порядка там 10 15 байт не это имею ввиду какая структура у ключа то есть какие данные он представляет собой ключи вы имеете ввиду индекс что-либо составе благо вы сказали ключ-значение хранится ключи по отсортированы по ключам правильно в качестве ключа используется что какой-то идентификатор или что то есть вы храните там используется просто строка бинарные фактически и вот они потрясали записаны там ключ-значение значит да да бен бинарность 40 вы храните и количество просмотров то есть какие-то счет о и количество сообщений какие-то счетчики и эта строка должна быть каким-то идентификатором который указывает на это счетчик правильно а ну да хорошо вот счетчики они привязаны к папкам то есть фактически мы храним пользователя идентификатор папки вот из него получается такой ключ в виде строки соответственно если мы знаем их папки за треть мы сможем напрямую обратиться в нашу и посмотреть какой там счетчик до то есть получается это как бы сама база данных она уже является индекс правильно ну поскольку отсортировано по ключам файл сортируем по плечам успел нет нет но имеется ввиду лдп там же как бы я имею до базы данных это вот этот файл который состоит ну или там несколько файлов я не знаю как хранится простите ладно не важно изучая а вы можете подойти после просто выступлениям и в кларк остаться так можно пожалуйста к вопрос вы говорили что реализовывать дополнительно повергли шкалы vdb свой набор функций частности была функция апдейт p-cad которого говорит что она позволяет модифицировать атомарная и реализованные данные такая щийся в каком виде они реализованы данные там хранятся джейсоном ходят ну то есть вот сейчас я расскажу немножко подробнее на самом деле это очень интересный функционал потому что он позволяет бороться с рейс кондишн ими вот в принципе такие атаманы операции мы делали для этого понятно что php это многопоточное приложение все таки вот и у нас есть массив на несколько сразу счетчиков но в примере вот на примере сервиса мессенджера можно сказать что это счетчики прочитанных сообщений непрочитанных и в принципе все счетчики касательно одной папке соответственно когда пользователь один пишет пользователь другому то мы увеличиваем сразу два счетчика и общее количество сообщений и непрочитанных плюс к этому мы ещё обновляем поле последнее время изменение этих счетчиков мы его даже не приедем просто с этим так вот и вот поэтому нет стать ситуация очень часто и вот чтобы не было сказать мы решили сделать это можно красы такой где мы указываем просто какие счетчики бринкли иметь насколько какие установить тоже какое значение дефолтные значения указываем и это происходит то там одна а так вот а получать если вырезать свой сетевой уровень то есть действенных пациент поверх него д.б. и вот при обращении какой конкретной записи у вас выстроится очередь запросов к этой записи то есть прирост два пользователя одновременно послали сообщение то есть получается нужно обновить счетчик два раза то есть то есть две атомарные операции путча да какой прирост производительности был бы если вы спать прям не джейсон который над каждый раз заново расспросить из текстового вида примеру там сели тонкую бинарный формат свой вы не рассматривать такой вариант до рассматривали такой вариант но вот по производительности самих движков понятно что если мы говорим об операциях апдейт пакет то видно что вот наш дом и сейчас работает ну отрабатывать одну операцию апдейт пакет за ноль целых шесть десятых миллисекунды вот так что в общем то очередь откуда же не копится с одной стороны с другой стороны на самом деле вот эти сейчас вот эти запросы которые идут это не то чтобы get по одному ключу и не то чтобы отдать пакет по одному крючок мы по умолчанию сделали такую архитектуру сетевой что можно указывать сразу кучу значений то есть get у нас там допустим обычно сразу шесть включить мы спрашиваем если отдать пакет аналогично мы там два или три сразу обновляем то есть на самом деле производительность еще выше если работать только с одним плечом эти наработки куда нибудь потом будут выкладываться open source да мы сейчас работаем над этим будет доступна вот в этом репозитории нашим на гитхабе mamba.ru вот мы выложим его туда хорошо спасибо здравствуйте у меня такой вопрос вот я как бы посмотрел ваш доклад и вы в 1 искали что мы отказались от radisson о в конце в принципе у вас получилось все то что есть в одессе из коробки то есть тот же атомарный климент мастер slave редис также умеет здравствуйте на диск вот вы сказали что вас там 11 гигабайт данных но эта проблема найти сервер 16 оперативной памяти то есть почему все-таки дело в том что вот процесс я говорил о том размер на котором мы тестируем то есть мы запускали тесты выбрали три движка и и делали универсальный тест то есть одинаковое чтобы они были чтобы можно было как-то их сравнить друг с другом на реальных данных вот сейчас у нас уже получается там порядка 20 гигабайт причем этот сжатых данных вот и это продакшне уже работает мы тестируем еще больше есть мы планируем очень сильно увеличивать эту нагрузку размер базы данных уже вот тестирую на мели на двух миллиардов записей и в конце концов тут бесконечно не получится но у поставьте два сервера но просто оперативной памяти сейчас нет проблем по магия понимает что нет проблем но вот мы изначально подумали какой сервер нам подойдет идеально под нашу под наш профиль нагрузки то есть у нас в онлайне к примеру сидит ну вот как я говорил один процент пользователей соответственно данные актуальны только вот для этого одного процента пользуется остальные они вы просто веселье в памяти ну и все и ничего с ними не происходило нам показалось что это как то не сильно оптимально и вот поэтому мы начали смотреть в сторону решение которые бы не упирались в память случае чего reds до отличное решение добрый день алексей палочник компания skype я хотел может быть продолжить вопрос предыдущий на первом слайде вы сказали также что вы отказались твоим каждый б он в свою очередь использует bird vdb который довольно гибко настраивается пытались ли вы это делать или сразу от не отказать ну может быть не так сильно долго мы это пытаясь сделать как хотелось бы но какое-то время действительно в той все настраивать но все равно из-за того что очень много было операций записи у нас получалось так что все равно вот во время высокой нагрузке когда и чтение и запись идет когда происходит sing данных на диск в этот момент всего все вдуплил причем вот ну реально очень сильно вот в сто раз примерно там ну то есть до десятков записи в секунду такой деградации мы не ожидали производитель именно в эти моменты но и скажем такой пример у нас получалось что 1 в пять минут у нас вдуплил жесткий диск причем утилизации возросла на сто процентов а то мне на 25 примерно в течение 30 секунд и так каждые пять минут вот и но это оказалось совсем как-то неправильно долго не разбирались на самом деле может быть сейчас уже это было уже достаточно давно года три назад может быть сейчас уже там оптимизировали что-то и как-то это решение развивалась дальше я не следил за этим уже мем каждый бы не развивается уже он за три год там не было ни одного к метро но то о чем вы говорили в принципе настраивается спасибо спасибо здравствуйте спасибо за доклад как я понял вы используете ну и стоит для хранения счетчиков сервиса сообщений после такой вопрос где вы храните сами сообщения то есть данные если вы храните отдельно от счетчиков как вы добиваетесь синхронизации счетчиков и данных и второй вопрос если счетчик изменяется вместе с данными значит эти сообщения пишутся и читаются так же часто как и счетчики на вы храните где-то в другой вместе спасибо спасибо за вопрос ну на самом деле очень просто сами сообщения хранятся в майские базе данных в обычно таблица вот о счетчике хранится у л д б добиваемся ну понятное дело что сделать транзакцию чтобы она была распределенной то есть смотреть что в маске ел закончилась потом gdb но это довольно сложно реализовать возможно поэтому мы в общем то когда комиссиям транзакцию в маскел допустим добавляем сообщения и увеличиваем там счетчики то после этого мы делаем апдейт влдп таким образом ну конечно там есть процент ошибок и мы его отслеживаем дело в том что у нас есть еще такой механизм когда ну при прочтении данных мы сравниваем действительно ли вот топ что пользователь видит на экране допустим у него там 3 контакта а счетчик показывает 5 вот если после такой можете наблюдать мыться остаться тоже отслеживаем и сразу запускаем корректировку счетчиков вот и как показывает практика такая корректировка происходит ну ночи редко то есть в основном все-таки нормальные данные при этом если они начинают биться то мы видим там всплеск этих корректировок сразу что-то фиксе то есть это не такая большая проблема на самом деле и второй вопрос был про не могли повторить 2 часа вопроса и я спрашивал самих часто читаем да спасибо вот на самом деле сообщение читается где-то 1 ну там до 1000 в секунду а счетчики у нас показывается практически везде то есть когда у кольца из кита непрочны сообщение мы ему их показываем сразу в меню и поэтому практически на каждой странице где происходит хит все эти счетчики загружаются и показывается если что поэтому сообщение считается реже гораздо чем сами счетчики дрозд вы сказали что вы реализовывали мастер слой трипле к цию но в тот же момент вы сказали что вы используете один сервер могли бы вы об этом немножко рассказать и правильные понимаю что исходными данными для репликации вот вот вот те два процента вот этот про это hotlog вы оттуда данные берете для репликации то есть для слива или нет да вот данные для репликации мы храним оперативной памяти и отправляем на a slave почему искал что один серго используем но на самом деле их 2 то есть master и slave но основная нагрузка идет строго на мастер то есть это все апдейты с догнали а вот нас леев очень мало идет он у нас в основном для бэкапы для чего такого это в сравнении с тем чтобы когда мы реализовали архитектура готовить erp нам мы вынуждены были два сервера использовать и между не мешать нагрузки здесь такой необходимости нет так как консистентной данных сохраняется в случае падения серы я понял ну а вот по поводу откуда слоев берет данные то есть у него данные обновляются в момент записи на то есть у него какой-то очередь из которой эти два нуля так есть то есть а вот эта очередь она не этот рай то hotlog который мы ранее донецка и тает у него свое эго череда hotlog это вот тут функция который уже реализован в лтп был свой собственный а мы реализовали чуть повыше уровнем просто копим очередь и отправляем ее нас live и сколько выручить храните как какая-то какой длины на то есть пока слоев не прочитает пункт когда читают ее вычищает или ну когда он когда он читает туда то эти данные оттуда вычищается ну если она там сильно заполнится то мы можем записать на диск тоже не version as a slave может быть один только сливов может быть сколько угодно на самом деле поддерживается там даже два архитектурных решениях первое архитектурное решение это когда у нас есть master и slave при этом слова могут быть их много причем они могут быть каскадно даже то есть вот один мастер там два слова дальше еще там десяток слоев от этих сливов и второй вариант это когда мы можем реплицировать мастер мастер то есть можно на самом деле сделаться так тоже него спасибо спасибо очень интересный доклад мы используем он где бит для математических расчетов и есть проблема с тем что когда мы заливаем то 5 миллионов записей пока она строит индексы она перестает отвечать как здесь это получится ли на месте мы перейдём на другую собой по бороться с этой проблемой да вот как раз таки пример того о чем я и говорил на самом деле из и та же проблема мы ушли от kitty куна потому что мы туда попытались записать ну очень много данных то мы ожидали конечно он просто запишется и все она она казалась так что он перестал отвечать на get запрос и и там уже не 1000 в секунду запись показывала там всего 250 максим и при том что это мы в один поток записывали последовательно при этом использовали киото именно хэш-таблицу вот славу db как раз такой проблемы нет почему я уже сказал то что вся запись происходит в оперативную память ну вот представьте вы вам нужно записать последовательно там несколько миллионов записей они идут на лауда бы на сервер записывается в оперативную память причем тоже последовательно в общем то потом раз какое-то время заполняется буфер записи этот этот кусок данных скидывается на диск в параллельном потоке вот собственно нагружается 25 процентов кушать утилизации точнее а в это время серы продолжает так же спокойно отвечать ну единственное вот то о чем я сказал что средняя скорость ответа может колеблется ну вот пике есть до одной миллисекунды среднее время ответа но в основном все равно вы видите что среднее время ответа во время даже мер g веток она примерно 07 ну вот этот по верхней границе если судить о периоды между слияниями деревьев она вообще 05 это помнили секунды на слои войне-то синхронно если мастер потеряется the slave будет ждать пока он поднимется все очень просто когда поднимется он заново перри коллекции перечитает данные заставлю вас потрясающе история спасибо за вопрос конечно мы очень просто если у нас падает сервер мастер да ну понятно что мы можем там два мастера поставить или там два слова но на самом деле сейчас уже три сервер мастера если три слова но вот в частности если мы говорим про сервис messenger то это один мастера dislike если мастер совсем упадет то есть вот он даже не захочет подниматься после этого там и очень просто мы тогда даем нагрузку на слив и говорим что теперь ты мастер вот и все и у нас тогда остается пока один сервер пока мы другой поднимаем ну вот действительно это вот нерешенная проблема сейчас остается если взять и совсем вот убить мастер и потеряется вот та очередь которая не успела отправиться на слив но дело в том что на слив обычно вот в наши я прошу прощения тут вот подсказывают среднее время ловил на репликацию может проще так будет а это меньше миллисекунды вопросов нет можно еще вопрос и последний от меня вы сказали что вы используете два сервер да под лтп правильно понимаю да мастера из и они эти сервера используются выделено подлого тебе ну то есть на них только у тебя нет ну да а почему тогда нельзя туда этот мой стиль тот же понятия отдельные об отчете cqi использовать его насколько понял из проблема была в том что вы этими запросами грузили другие сервера с данными уже почему бы не используем мыска или этих счетчиков и используя только его для счетчиков так но дело в том что мое тело не выдержит таких нагрузок то вот мы могли оставить в принципе мой скил серво от сервера там же где они были и вот использовать те 50 который у нас есть но мы как раз и хотели снять с них четверть нагрузки вот со всех этих 50 серверов для этого заюзали всего лишь тут два сервера вот получились даже то есть можно увеличивать базу данных и число запросов на лорде бы очень сильно то есть у нас очень много ну не только вот щёчки сообщения у нас разные флаги там другие какие-то данные относящиеся пользователям у нас та же плавно переезжает уже vdb то есть использовать выделено серым выделен апать счетчики на мыске ли это был не вариант мы не тестировали его и выплюнул принципиальные мы тестировали его даже потому что ну какая разница 1 7 скилл взять под счетчики еще один дополнительный 51 или там и со второй если у нас и так уже из 50 серверов и там эти счетчики тоже хранятся там ведь были другие данные другие запросы которые тоже и грузили tessera да но я как уже сказал что четверть всех запросов но даже на самом деле еще больше это составляли только на работа со счетчиками кадры хандлер soccer вопрос такого так как же это серебряная пуля как хандлер socket пробовали да сейчас мы его уже протестировали успешно собери собираемся даже внедрять ну не для счетчиков скорее для того чтобы авторизовать пользователей потому что сейчас распада используется отдельный демон действительно хорошо это очень хорошо работает на тот момент он показал меньшую скорость когда мы тестировали и плюс он какой-то немножко сыроватый то есть например шаг влево шаг вправо у нас получалось с помощью ходлер сакита на создавать кучу записей и допустим первичный ключ у них был один и тот же вот то есть очень осторожно нужно с ним работать особенно запись к примеру потому что потом баз данных просто не встанет придется что-то делать спасибо здравствуйте меня интересует у вас очень много память выделено на кэш значит вы его хорошо использующие а как у вас холодным стартом при этом приседайте ну да понятное дело если у нас используется в маскел ммк что к примеру когда все падает он все поднимается все лежит заново но вот slow db в частности и последовательная особенно последствии чтение происходит вообще со скоростью чтение с диска вот поэтому на самом деле если нас все падает то все поднимается то проблем слова дабы возникает ну точнее она не успевает возникать в принципе я думаю он очень хорошо справляется с и со случайными числе ними тоже вот у нас есть вот эти результаты тестов вот здесь л л д б на случайном чтение показывает производительность 15000 чтений в секунду старте на холодном но это просто теста они запускаются разные нагрузка дается с нуля записывается там файл читается и так далее ну понятно что это данные тестируем самого движка сетевая часть конечно дают дает свой эффект негативный и снижается эта скорость но в принципе то сам будет сопоставимо в любом случае я так понимаю что это момент когда мы записываем то есть это момент когда у нас еще все в оперативке а если мы уже скинули на диск и стартовали у нас этого всего в оперативке нет и нам приходится все поднимать в оперативку соответственно выстраивать там индексы и это все равно займёт какое-то время такие таинство проводили смотрите во время старта холодного к примеру идут куча запросов в это время файлы с диска подтягивается в кэш причем вот там есть отдельный буфер про который говорил буфер запись 128 мегабайт и кэш мы подпишем уделяем вообще 40 гигабайт то есть там памяти дофига на сервере 40 гигабайт он просто выделить docash вот поэтому как только начинают случайно чтение происходить или там последовательное на холодном старте то вся эта информация сразу поднимается в кэш я туда уже берется спасибо спасибо"
}