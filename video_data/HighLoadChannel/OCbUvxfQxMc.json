{
  "video_id": "OCbUvxfQxMc",
  "channel": "HighLoadChannel",
  "title": "5 новых способов использовать данные в вашей Kafka / Андрей Серебрянский (Райффайзен Банк)",
  "views": 1320,
  "duration": 2662,
  "published": "2024-10-29T03:08:38-07:00",
  "text": "а ребят Всем привет Я сегодня попробую рассказать вам про какие-то новые способы использовать данные вашей Кафки Возможно вы их знаете Возможно вы их используете но возможно мне Е С чем-то новеньким С вами поделиться А кто Я вообще такой я работаю стриминг платформ ором райн банке То есть я оунер стримингового платформы Что за стриминговая платформа такая это платформа которая позволяет другим командам разработки не думать о том как стримить данные из базы как стримить данные из меседж брокеров как их залить из Кафки куда-то как эти данные использовать то есть мы даём инструмент для того чтобы легко взять настроить cdc из базы настроить выгрузку в другую рацион настроить выгрузку в dat Lake или куда-то ещё всё легко Приходите делаете конфиг данные потекли всё также вы получаете алерты инфраструктуру в общем всё для пользователя должно быть просто и а платформа такая запустилась только в январе этого года а за это время когда м пришло довольно много команд и эти команды приносили с собой какие-то свои кейсы они говорили нам нужно вот это а добавьте Вот это и из этого я набрал этот доклад в котором рассказываю как разные команды в Йен банке по-разному используют кафку Давайте начнём в начале у нас была только кавка в райфен банке в неё кто-то писал продюсерами кто-то читал консьюмер и в целом всё этого хватало это классно работало но в какой-то момент возникла ситуация что появились данные которые в каку очень хоте занести А это были данные карточных транзакций То есть когда вы что-то оплачиваете прикладывается свою карточку к терминалу эти данные попадают в эту орак базу очень хотелось эти данные оттуда выгрузить и загрузить dat Lake при этом хотелось это сделать именно через кафку потому что ну не только в dat Lake хочется а много Куда ещё и тут у нас появляется первая история о которой я хочу вам рассказать Это история про быстрые данные для аналитики собственно с этого началась как раз стриминговая платформа к нам пришли сказали ребят о Надо данные из оракла стримить а потому что мы задолбались анализировать за предыдущий день мы хотим анализировать в рамках дня данные А почему вообще нельзя это сделать стандартными инструментами Почему нельзя взять и далеком ходить каждый день каждый час каждую минуту в базу карточных транзакций Ну потому что это очень большой и сложный Запрос который сильно грузит базу Мы селекти полностью базу это тратит её ресурсы процессора и она начинает медленнее отвечать на запросы согласитесь Если вы в магазине прикладывается карточку к терминалу райфен банка а оплата не проходит потому что мы выкачиваем данные для аналитики Ну вам это не очень понравится а поэтому загрузка проходит один раз в день ночью когда нагрузка на базу небольшая и естественно данные А Да собственно база краснеет Когда происходит нагрузка и естественно а данные появляющиеся на следующий день для аналитики Ну не всегда удобны иногда хочется прямо в рамках дня посмотреть как клиенты righting банка тратит свои деньги а как стали делать Мы мы стали использовать такой инструмент как cdc а это change Data capture или по-русски захват изменений в данных Я знаю что было много произнесено этой аббревиатуры на лоди но я на всякий случай всё равно расскажу как это работает внутри А на примере вот Представьте вы прикладывает к терминалу в магазине в этот момент а операция летит в нашу базу данных операция туда записывается не сразу а сначала база данных записывает её в так называемый Right Head Log то есть Запиши сначала Log а операция попадает туда И после этого становится доступно в табличке Ну откатывается на табличку тоже самое у нас происходит с апдейта ет апдейт он записывается в й headlock а затем становится доступен в табличке так да затем становится доступен в табличке всё этот хло можно взять и выгрузить в кавка топик на диске уже этот Файлик лежит Мы его можем взять избрать на самом деле если честно мы конечно используем не прямо тот который использует база для собствен аппликации А это инструмент отказа устойчивости баз на самом деле другой А и к нам данные попадают только после того как они стали доступны в таблице то есть Давайте на примере нам прилетает insert он становится доступен в табличке и только после этого он попадает в кавка топик то есть мы получаем данные после того как не Катились на базу а соответственно Таким образом мы стали забирать данные из оракла это работает быстро это не нагружает базу Мы задержками в пару секунд данные в топике получаем А значит если данные у нас в топике уже есть мы можем их в любой момент лег выгрузить Lake соответственно наши дата аналитики могут получить данные с той задержкой с которой они хотят Ну слишком часто в dake выгружать данные тоже нельзя потому что создают Слишком маленькие файлики Да это не любит но условно Раз в час выгружать Бачи данных вполне можно как мы это запили запили мы это вот таким образом идёт запись Oracle мы его проприетарный вендерс инструментом выгружаем для него нужна платная лицензия А после 24 февраля такую лицензию купить нельзя в России но не переживайте я дальше расскажу как то же самое сделать на осор сны технологиях соответственно дальше после гон гейта начинается уже полный Open Source мы кавка коннектом данные выгрузили в кавка топик А зачем опять же он Source нам коннектором из кавка коннекта залили А в наш dat Lake а Супер это заработало аналитики получили свои данные и вы тоже так можете сделать взять данные из Кафки и с помощью осор сного коннектора там не знаю за 5 минут поднять его в докере и начать грузить данные в ваш Hub или аш S3 сделать данные доступными для аналитиков к нам пришла команда и говорит очень клёво Ребят вы прям так быстро грузите данные Зорак А можно Нам тоже самое для постгрес не хотим свой пост грузить тоже хотим его логи читать и тут мы приходим ко второй истории можно использовать данные в кафке для того чтобы оптимизировать ваш датасет под конкретно вашу Рид нагрузку расскажу на примере Что именно мы сделали в какой-то момент к нам пришла команда рублёвых платежей и говорит на самом даже м прила у них давно уже это данных куда попадают все переводы по рублёвым счетам в йн банке То есть это нагруженная база данных в неё идут какие-то записи какие-то чтения оперативных данных Когда вы там делаете перевод туда попадает запись и к ней же прилетают большие сложные запросы на исторический анализ типа Дай мне все транзакции за последние 3 года по этому клиенту Посчитай средне за последние 10 лет по всем клиентам В общем эти запросы естественно сильно базу нагружая решили чтобы не грузить равную базу и не удлинять путь перевода Рублёво в righ бан и разделить эти базы на две в одну прилетают все оперативные запросы маленькие в другую прилетают все архивные запросы которые как-то анализируют данные внутри которые сильно базу грузят вопрос только в том как две эти базы между собой синхронизировать А ребята ответ на этот вопрос легко нашли они взяли вендора IBM стали использовать дебиту репликатор чтобы из одной объемов ской базы залить в другую объемов вскую базу это это работало пока IBM из России не ушёл а потом Ребята смиг с дебету на постгрес и у них стал вопрос А как данные использовали залить в s400 который у нас остаётся А вопрос с этим вопросом Они как раз пришли к нам и мы им нас запили то же самое решение только теперь не с венским Годен гейм А с орным де безум и опять же Это всё из коробки полного Open Source вы забираете этот образ кавка Connect раскатывается зимовский раскатывается конфлюенс коннектор jdbc который может в любую реляционная я уточню что is400 - это вот такой вот кто не знает огромный мейнфрейм сервер откуда-то из семидесятых или восьмидесятых не знаю в общем огромная коробка а Но она понимает jdbc протокол и мы в него смогли довольно быстро начать писать нам на самом деле пришлось доработать одну штуку нам пришлось форматы тайм смпо изменить потому что в посе форматы тайм спов одни А в is400 другие Но кроме этого ничего это на самом заняло один Java кла на там по-моему 50 строчек вот и всё это полетело при этом Вы не обязаны заливать из одной таблички в другую Вы можете в другую реляционную табличку Вы можете оптимизировать ваш датасет в вашей реляционной базе под ту нагрузку которая вам удобно то есть не знаю залить другую реляционной а если у вас много киве запросов именно read нагрузки Вы можете залить в кэш если вам нужно для анализа данные пожалуйста залейте их в Вы даже можете какие-нибудь графовые базы залить главное что один раз загрузив данные из вашего постгрес например а они хранятся в кавка топики и вы можете их распространить по тому количеству хранилищ которое вам нужно даже если у вас там несколько разных один раз загрузили потом распределили по куча разных а оптимальных для вас хранилищ этот паттерн называется это может быть реализация паттер cqrs то есть Command and quy responsibility segregation вчера про это был кстати довольно классный доклад рекомендую если не посмотрели посмотреть записи про то как перевести ваше приложение на приложение использующее cqrs и вот как раз с помощью Кафки можно вот так взять и данные раскидать по соответствующим хранилищам тут надо Уточнить что есть очень важный момент Я думаю раз вы пришли доклад про каки то вы прон знаете но на всякий случай я расскажу потому что Ну вдруг для кого-то это будет новая информация и какой-то какая-то полезная фича Итак в кафке по умолчанию данные удалятся через неделю Что это значит Это значит что у нас общение в день допустим приходит в октябре когда у нас приходит сообщение за восьмое Да за 8 октября сообщение за 1 октября помечено к удалению то есть оно оно не сразу удалится кавка удалить Чуть позже но оно удалится То есть когда нам придёт сообщение за 9 октября А за первой уже данных не будет Почему Для нас это важно Ну потому что если мы хотим синхронизировать два хранилища допустим вот мы использовали выгружаем и заливаем в какое-то ещё то первое хранилище синхронизируется отлично но потом через неделю данные из него удалятся и если вдруг к нам придут клиенты а стриминговая платформа она как бы загружает данные из разных источников и распространяет в разные источники и допустим к нам придут новые клиенты и скажут ребят нам нужны тоже эти данные Давайте нам их отдадите мы скажем конечно Подписывайтесь но вы получите только новые обновление вот допустим там по первому юзеру что-то пришло новые Ребята это получат А по второму юзеру больше там допустим апдейта не прилетит и всё ребята потеряли второго юзера как с этим быть Ну довольно просто можно использовать compaction в кафке а кокш в кафке на всякий случай рассказываю как Как это работает а мы выгружаем данные из топика заливаем в нашего первого потребителя до Том приходит новое сообщение по первому юзеру и предыдущее сообщение по ключу предыдущее сообщение для первого юзера удаляется для второго же юзера по которому ничего не приходило данные будут храниться столько сколько вы настроите их хранить по умолчанию вечность соответственно когда к нам придёт Новый потребитель на ти данные он получит последнее сообщение по ключу для каждого из ваших юзеров То есть он сможет накатить тебе актуальный слепок данных из базы что довольно круто но не всем и всегда подходят данные хранить вечность в кафке например в рафе десятки и сотни миллионов транзакций прилетает в день уникальных транзакций и хранится только в кафке нам просто ну не нужно зачем нам так сильно кафку нагружать А поэтому можно настроить чтобы данные хранились какой-то заданный интервал времени и с этим можно по-разному играть Я на слайде привёл пример того как можно по-разному это настроить можете удалять данные по ключу предыдущие сразу Может через какое-то время можете хранить данные ском пакте вечность А можете хранить год или Сколько сколько вам нужно в общем с этим можно поиграть но Допустим мы храним данные вечность или даже год А что делать с теми данными которые удаляются Что произойдёт если в таблице на источнике данные удалились в таком случае на самом деле по умолчанию всё будет отлично работать то есть об этой проблеме даже по идее можно не думать потому что по дефолту если пог видит что у вас произошло удаление Он отправит такое сообщение ключом будет первичный ключ а будет написано что Operation Del но он отправит не только его а ещё одно сообщение в котором написано что ключ первичный ключ А value ну кавка когда такое сообщение увидит удалит все сообщения с таким ключом в этом топике соответственно Ну не сразу А после определённого периода на самом деле Тут ещё небольшая ошибочка удалит он последнее сообщение с оно называется tombstone или надгробие где val оно удалит его через Delete retention MS супер Да настройку то надо показать вот а как быть с удаления если мы хотим их транслировать в целевую базу Да тоже всё на самом деле работает из коробки нужно только включить настройку по умолчанию настройка выключена удаление игнорируется но её можно одним флагом в конфиге включить и у вас Удаление из sce базы прольются в целевую базу Вот нуно это работает конкретно для gbc для других коннекторов надо смотреть конфиги коннекторов но надо сказать что стриминг на самом деле такой вот он работает классно быстро вы накатывает изменения на другое хранилище но подойдёт не для всего тут обязательно нужно помнить про eventual consistency то есть какое-то время базы легко могут быть некон систентки приведу пример Представьте что у нас есть две базы две таблички в одной базе users и emails такая вот супер нормализованная база имейлы лежат отдельно В отдельной табличке делаем запись делаем запись транзакционная естественно в обе таблички одновременно обновления прилетают они улетают в кавка топик и затем кавка топика мы его накатывают накатывает для нашего имейла К сожалению там не знаю уборщица выключила сервер из розетки случилась какая-то недоступность в общем данные не долетели какое-то время они поживут в кавка топике через какое-то время они даже долетят и база снова станет консистентной состоянии но какое-то время консистентность будет достигнуто это важно потому что для некоторых кейсов не знаю Если вы там проводки Потам передаёте вам обязательно нужно чтобы база всегда во всех табличках была в конном состоянии Ну или хотя бы нех табличках супер ребята на это посмотрели Как можно быстро данные стримить в кафку из Кафки стримить куда-то ещё и говорят офигенно очень нужно такое только теперь давайте между командами Можно мы эти данные которые ребята и так уже для себя стримят себе заберём А конкретно на самом деле пришла команда которая отображает данные в приложении ба говорит у вас есть информация о комиссиях рублёвых платежей нам такое очень нужно давайте мы это приложение отобразим залейте это нам в базу Окей конечно Просто взять тот же самый топик и накатить на базу другой команды а но с этим есть небольшая проблема Представьте что в базе источнике ребята решают Ну это же наша база наша табличка делаем Что хотим удаляют обязательную колонку естественно целевой базе это нормально не проживёт и целевая Ну заливка в целевую базу упадёт что не очень хорошо ребята хотят быстро данные получать соответственно чтобы такого не происходило база источник завязывается на базу потребителя вы раскрыли детали своей имплементации теперь Вы вынуждены любые изменения в своей собственной вашей базе согласовывать с другими командами это неудобно О'кей давайте по-другому сделаем давайте мы с нашего бэнда будем писать в базу которую мы мигрирует писать в кафку сообщения которое мы будем эволюционировать очень-очень аккуратно А хорошо но это к сожалению дольше для наших пользователей потому что теперь уже не только в базу записать но и в кафку но и это не самое важное и не самое страшное А самое страшное что теперь нам нужна какая-то нная транзакция ЕС Мы хотим записать обязательно и в базу и в кафку нам нужно что-то придумывать что если мы в базу например записали А в кафку запись упала нам нужно делать лбк на базе или нам нужно делать Дубль В общем этот вопрос он не очень удобен А и тут приходит на помощь паттерн который называется transaction outbox мы его всем рекомендуем использовать Потому что тут не возникает проблема с тем что вы раскрывайте далее имплементации своей базы И не возникает проблемы с тем что у вас распределённая транзакция вы пишете в один и тот же постгрес например в рамках одной и той же транзакции вы пишете в свою табличку которую вы можете эволюционировать как хотите меняете там типы переименовывается колонки дропа обязательны колонки что угодно и пишете в отдельную табличку которая называется outbox табличка и в неё вы уже пишете аккуратненький которые вы аккуратно эволюционирует предупреждая всех о том что происходит эволюция и вы уже развязали с другими командами Соответственно по конектор Кафки вы натравлю набо табличку супер научились с вами Ну научили команды интегрироваться между собой так чтобы не раскрывать детали имплементации к нам приходит новая команда говорит ребят Вот эти данные которые у вас в кафке лежат Мы очень хотим визуализировать на телевизоре Мы хотим принимать решение в реальном времени основываясь на данных Кафки но проблема мы как бы не очень умеем кодить мы аналитики Мы хотим просто вот визуализировать Дайте нам инструмент для этого Окей тогда мы можем использовать эти же самые данные в кафке для того чтобы строить красивые дашборди с помощью Series базы данных какой-нибудь приведу вам пример кейс команд поддер ф То есть вы Когда пишете в приложении в поддержку как раз вот эти ребята вам отвечают они себе настроили вот такой дашборд графа Давайте его поближе покажу на нём указано сколько прямо сейчас находится человек в очереди в определённый сегмент поддержки например не знаю в Инвест много людей пришло в очередь или куда-то ещё и соответственно менеджеры поддержки каждые 5 секунд это дашборд у них обновляется видят Ага Сейчас у нас в очереди столько-то людей Давайте перекинется на этих данных которые прилетели из Кафки Как это работает мы загружаем все данные в кафку затем отправляем его в Time Series базу мы Для этого используем timescale и потом визуализирую через фан timescale - это такое расширение постгрес которое классно данные позиционирует Time Series данные а данные Точнее с СТМ меткой партиционирование чанк он вообще хранит в памяти поэтому доступ к последним данным а идёт очень быстро Вот но на самом деле вы не обязаны использовать timescale он для нас был очень удобен потому что мы хорошо умели работать с постгрес сом а Клик House команд из шести человек спить не хотелось вот мы взяли та escale но на самом деле кха timescale показали на наше нагрузке одинаковые а цифры Вот и вы можете выбрать для себя ту базу данных Time Series которая подходит больше вам а да О чём Вам нужно будет не забыть в любой базе которую вы бы не выбрали а о том что пользователи если хотят быстро данные селекти и часто то им скорее всего понадобятся какие-то индексы А значит им нужен инструмент как к этой базе подключиться если это ваша база им нужно как-то уметь индексы накатывать второе О чём нужно подумать это то что если данных туда летит много а у нас например информации о активности агентов поддержки летит где-то 1500 сообщений в секунду то в какой-то момент база может и переполниться если мы ничего удалять не будем соответственно нужно как-то поработать с удаления Ну и последнее нужно как-то разобраться с квотами Потому что если много команд сразу начнут в эту базу писать или напишут какие-то не оптимальные Селект запросы то база может и и как бы потратить все свои ресурсы как мы это решаем мы решаем это а довольно просто через liquibase и gitlab сегодня Мой коллега Женя Харченко показывал как А в райн банке устроен CCD pipeline он у нас там коробочный можно его взять и из кубиков собрать тот который удобен вам в том числе там есть а миграции базы данных вот мы такую как раз штуку используем и накатывать через gitlab миграции после review вот для ретеншн в тайм скеле есть супе доб ная функция Вот конкретно э проведённая на экране удалить данные в этой табличке через 2 дня Всё из коробки оно заработало точно так же через к Base накатили А С квотами И лимитами мы планируем разобраться только в следующем году потому что мы ещё маленькие у нас несколько команд всего пришло и Пока таких проблем нет но они должны начаться в следующем году как раз мы начнём этим заниматься благо много экспертизы о том как там например в Грин ПМЕ ограничивать и давать квоты на запросы у нас в банке есть да переходим к последней истории ребята посмотрели на то что происходит в нашей стримингового платформе и сказали Очень классно классно что у вас столько данных мы их все хотим Но они нам вообще Категорически не подходят Мы хотим себе данные чтобы они были Вот другое переименовать колонку А вот здесь мы хотим добавить к этому топику Вот этот А вот здесь вообще агрегацию хотим и Средние считать О'кей переходим к последней истории как е это засе тапи мы засе апилифт мы загружаем все данные из источников в кафку натравлю на это какой-то фреймворк какой я расскажу чуть позже и отправляем уже получившиеся данные опять снова в кафку результирующий топик с этими данными можно делать очень много чего интересного Например можно эти же данные использовать для того чтобы залить в реляционную базу но это как-то слишком просто можно использовать их для того чтобы делать какие-то алерты Ну не знаю вы считаете среднее количество операций покупки клиента за последние 10 секунд За последнюю минуту за последний день и анализируйте все эти операции такие а нет ли там фрода если фрод есть отправляйте Арт соответствующую систему Или например можно использовать для нотификации польз То есть вы получаете какую-то авторизацию покупки смотрите что на неё начисляется кэшбек раз можно отправить пользователю уведомление клёво Ты получил кэшбэк Ну как пример вот ну и последняя функция которая тоже клёво в стриминговый кейс - это триггеры не знаю вы посчитали что у вас какое-то количество много логов накопилось раз создали Арт или там сообщение триггер на него навесили запуск какого-нибудь не знаю вычитки из каки этих логов Как пример Какие стриминговые фреймворки этого существует А довольно много на рынке всякого разного есть у них свои разные плюсы и минусы но как мне кажется наиболее популярными сейчас являются два это cka streams и aplink а притом у cka streams есть классное решение kcq db которое позволяет с помощью SQ SQ данные обрабатывать описывать обработку над данными с помощью SQ мы для себя выбрали apach Link Хотя кавка streams тоже немножко используем просто потому что apach fink старше больше в нём больше функций и например в cka streams на момент нашего выбора не было та Series Джо инов то есть что такое Time Series Join это когда мы конкретному сообщению жаним ближайшее к нему сообщение из другого топика Как пример вы покупаете что-то в магазине там не знаю 10 октября в 12:00 И вам хочется посмотреть на стоимость это покупки в долларах на 10 октября 12:00 соответственно вы прижмите курс рубля к доллару в 10 10 октября 12:00 вот такое штуки вка streams не было сейчас на самом деле уже есть вот но мы всё равно пошли в сторону фнка а в том числе огромным преимуществом фнка является то что он умеет в SQL А некоторые наши пользователи они не хотят писать на Джаве они не хотят писать на питоне они хотят описать преобразование на том языке который они уже знают на SQ вопрос А чем SQL на линке отличается от SQ в базе данных да на самом деле Ничем вот это тот же самый SQL только Селект идёт не из таблиц А из топиков каждое конкретное сообщение обрабатывается по заданной логике такие вот есть селекты вы описываете там Какие колонки каких типов такие же есть джоны притом Джов может быть несколько и джоны бывают не просто Left Right А ещё и всякие необычные типа как я описал temporal дна А есть конечно же условия есть даже подзапросы которые работают и конечно же есть иср insert опять же идёт не в табличку а не в табличку А в конкретный топик а соответственно мы описываем для Линка это как будто бы эта табличка Но на самом деле флин когда получает от неё inert он отправляет сообщение в кафку что нужно знать пролин чтобы его у себя запить с ним на самом деле кое-что может пойти не так много чего во-первых нам нужно где-то хранить наш стейт если мы делаем не знаю группировку агрегацию среднее зад считаем нам нужно эти данные где-то хранить и делать это отказы устойчиво чтобы приложение Даже при перезапуске это стоить не потеряло К тому же если мы хотим хотя бы или даже может обрабатывать данные нам нужно где-то про х опять же если мы запускаем Лин кубее нам нужно чтобы приложение пережило перезапуск пода надо куда-то это ВС сохранять во флин есть встроеная поддержка S3 благо в рафе тоже было S3 соответственно с этим проблема Не возникла Окей подняли S3 баке для этого А где нам это запускать тут тоже Нам повезло всё супер У Линка есть отличный кубер оператор поднимаете раскатывается на кубернетес всё отправляете манифест в он вам поднимает соответствую этому манифесту вво ещё одна проблема - это то как разделить доступ Ну фн банк организация большая чувствительных данных в ней много если мы сделаем один большой классный флин кластер и к нему дадим всем доступ то все смогут всё селекти все там карточные транзакции переводы по счетам всё что угодно такое нам конечно же недопустимо Поэтому нам приходится сдавать по отдельному кластеру для каждой команды что как бы в целом не проблема но требует много автоматизации Потому что много рутинных действий нужно сделать Ну и последняя проблема с которой мы столкнулись - это как пользователя обучить SQL конечно сэлем всё клёво Но если вы селекти данные из базы то привыкнуть что каждый Селект обрабатывает одно конкретное сообщение а не бежит и возвращает вам результат довольно сложно и тут конечно простого пути нет нужно просто обучать пользователей рассказывать проводить митапы чем мы потихонечку и занимаемся и потихонечку команда к нам приходит и флинк начинают использовать даже если они как бы даже с какой не знакомы просто хотят данные каким-то образом преобразовать Итого Как ещё мы можем использовать данные в нашей кафке Ну во-первых мы можем писать туда не только продюсерами а стримить данные из самых разных мест например из баз данных или из брокеров сообщений всё это можно сделать с помощью кавка коннекта в ОРС существует огромное количество орны коннекторов к самым разным базам источникам и брокерам источникам и обычно можно сразу в опенсорс найти и запить Хотя нам всё равно пришлось пару штук Дописать самим второе Что можно делать с данными можно их синк для аналитики если до этого у вас допустим dat Lake забирают данные раз в день то с помощью стриминга и с помощью Кафки Вы можете оттуда данные загрузить и начать их анализировать в рамках дня Что гораздо быстрее даёт ценность в-третьих Вы можете использовать данные для оптимизации вашей Рит нагрузки залить в то хранилище которое лучше всего подходит по чтению этих данных не знаю в кэш в Кра неважно Аа Вы можете отдать данные другой команде Но тут надо не забыть о том что вам не очень круто было бы раскрывать детали имплементации своей базы И нужно подумать как вот эту ответственность разделить Можно например использовать pattern transaction outbox наконец Вы можете задеть данные в Time Series базу Как пример timescale или Click House или какую-то ещё А И эти данные а пользователи смогут вводить себе куда-нибудь на графики на телевизор принимать решение прямо в реальном времени на меняющихся данных Это довольно удобно и даёт много ценности Ну и последнее о чём я вам рассказал это про то что можно использовать Stream process in Framework они бывают разные Но они позволяют данные со всем со всей организации взять объединить поджо инить из разных команд Да соединить это даёт много дополнительной ценности А самое главное что это всё Open Source все эти штуки Можно попробовать прямо сейчас попробовать развернуть где-то что-то может быть понадобится доработать но в целом это не так сложно засе тапи много кода для этого написать не придётся а пожалуйста пробуйте Надеюсь это вам чуть-чуть поможет или какие-то Новые вам юзкейс открыло а у меня на этом всё спасибо вам большое за доклад Спасибо я вас сфотографирую Извините пото что вас очень много Я никогда так много людей не видел о вижу руки Спасибо да да Спасибо от Сбербанка тебе врай почему почему вот вопросы можно пожалуйста микрофон на третий ряд бежит Лисичка Да будьте добры третий ряд слева от нас да може вставать Да Спасибо за рассказ Меня зовут Константин Я системный архитектор промо связьбанка смотрите вот в плюсах при рассказе Об использовании Кафки чаще всего рассказывает о чудесных возможностях подписываться на один и тот же топик несколькими продюсерами и соответственно Таким образом мы переиспользовать а потому что вот в отдельности всё что вы рассказывали можно сделать другими путями и зачастую гораздо более оптимальными Да там начиная с пе мы используем не складываем в каку а берм Gold ко да сразу писать входу например да конечно Тут отдельно сначала наконец вопрос отвечу что ценность именно в платформенной инструменте То есть вы приходите и можете залить данные в кафку а потом их использу так как вы хотите и касательно переиспользование данных например данные карточных транзакций очень многим командам нужны и у нас насколько я помню где-то пять или шесть команд себе эти данные Забирают из одного и того же топика то есть один раз мы эти данные залили а потом Спасибо Добрый день спасибо за доклад очень интересно это у нас второй ряд чуть левее центра для для менеджера фонаря Да несколько вопросов первый вопрос как боретесь с ребаланса и с дублированием который возникает второй вопрос Ели у варен иб Как вы его делаете и Давайте по очереди первый вопрос был с ребаланс правильно дада да а на самом деле мы ещё всё-таки довольно маленькая платформа сколько я слушаю про объёмы Яндекса переживаю там 80 Гб в секунду ребята грузят вот у нас сильно меньше Вот и пока что с ребаланс мы сильно не боремся мы короче таких задержек не возникает не возникало вот мы настроили для наших косме например в кавка streams а там периодически такая проблема была политику распределения партиции на стики asser по-моему или как-то так которая очень помогла То есть у нас постоянно между подами в кубернетес на кавка streams перераспределяться что было супер неудобно один под умер перезапустил на него снова партиции перекинулись когда мы включили кисар и Время ожидания перераспределения партиции увеличили то эта проблема немножко ушла Вот то есть на статическое партиционирование пришли ну оно не статическое всё-таки именно стики То есть он какой-то время ждёт и если какое-то время пот не поднялся на том же месте он пере перебрасывает Да статистики оно там висело просто и никто бы ничто а второй вопрос был ре процессинг репроцентр мы вычитали данные мы хотели накатить одно а накатилась вообще другое надо ре процеси у нас по умолчанию данные в проде хранятся один месяц а всякие словари данные которые вполне можно хранить вечно по ключу Мы храним вечно соответственно когда Людям нужно накатить ре процессинг они создают тикет который сейчас пока что к сожалению вручную дежурный наш берёт и сбрасывает осет коню и они репроцентр нка были какие-то проблемы Ну то есть с входящей нагрузкой Если честно субъективно мне гораздо больше нравится кака streams Вот Но почему именно потому что он ограничен Если вы например в режиме exactly on работаете количеством чекпоинтов и там эти чекпоинты каждый раз заливаются S3 соответственно ограничена скоростью заливки в S3 а кавка streams комитет все сво весь свой Прогресс в кафку это ограничено скоростью записи в кафке Что быстрее вот поэтому с перформанс фнка Мы немножко столкнулись мы настроили чекпоинт раз в секунду и пока что у нас нет кейсов которые требуют большей скорости соответственно пока что мы всё просим нормально но если потребуются сотни миллисекунд или десятки миллисекунд нам придётся что-то придумывать скорее всего переходить на кавка стримс в этих кейсах Спасибо большое спасибо Вот теперь слева от нас э четвёртый или пятый ряд Спасибо большое за вопрос Да добрый день спасибо за доклад Подскажите пожалуйста как бороться или как жить с не консистентные данными То есть можно ли в какой-то момент узнать что данные внезапно стали консистентные или есть какие-то практики которые позволяют синхронизировать потоки данных отличный вопрос Спасибо большое постоянна эта проблема вот данные дотех А вот из этой таблички не дотех Почему а-а во-первых хотел порекомендовать вчерашний доклад видео по-моему от ребят из теньков они рассказывают про то как решают проблему это через доменные ивенты они не стримят не через cdc все данные а они отправляют один большой ивент который объединяет все данные из разных табличек соответственно консистентность не нарушается что касается вот вашего вопроса как может быть получить уведомление Что все данные загрузились пока мы этот вопрос не решаем Но нам нужно будет обязательно реши в следующем году А мы планируем может быть это делать через оконные операции в линке то есть мы копим батч проверяем что со всех топиков данные пришли и только потом триггере кастомным триггером по линке можно строить кастомные триггеры отправку Ну выли вку заливку этих сообщений в топики то есть мы ждём что всё агрегировать батч точно пришёл и потом тригери вот но мы пока это не обкатывать не тести это гипотеза вот может быть придём расскажем получилось или нет Вот спасибо пятью рядами выше вот в той же секции под камерой прямо да Будь стоб раз спасибо за доклад кавка коннекта которая у вас там обозначена как вот обмен всего совсем в чей зоне ответственности кто их конфигурируется если это стриминговая команда Вы ещё не стали бутылочным горлышком Угу суперский вопрос Я кстати про это немножко рассказывал на джокере может быть скоро появится запись этого доклада А как не стать бутылочным горлышком кака Connect само приложение именно вот Java приложение в наше ответственно мы его поднимаем мониторим поднимаем если оно падает а запуск коннекторов - это ответственность команд самим то есть мы дали интерфейс когда они в gitlab котят config и этот конфиг автоматически нашим CD раскатывается на кластер Валиди ется создаются топики всё такое Соответственно в этом моменте мы не являемся бутылочным горлышком потому что система создания коннектора автоматизирована То есть вы смогли обучить команды конфигурирования и сказа Да написали инструкции и попросили попытались эти конфиги максимально упростить убрать лишнее вот у нас всё ещё на самом деле там много всякого мусора в этих конфигах но мы планируем прямо ограничить чтобы в конфиге Вы писали условно адрес постгрес список таблиц первичные ключи всё данные потекли сейчас нужно конечно побольше указать ребята указывают Да спасибо в том же луче левее смотрю вот да А спасибо за доклад Меня зовут Владимир А вы обучили ли свои команды каким-нибудь инструкциям в роде профа который можно навесить поверх навку и собственно чтобы понимать какие у нас там данные какие у нас там типы и все вот эти вот штуки Спасибо у суперский вопрос в большой организации огромное значение имеет эволюция схемы если кто-то свои данные поменял хорошо бы об этом узнать мы используем для этих целей не прота бав А авра хотя прота бав тоже бы отлично подошёл и используем вместе с авра сма registry сма registry - это такое Java приложение которое бежит И всегда знает какие схемы в целом в этой кафке существуют соответственно в каждом сообщении которое в кафке а летит существует дишни это схемы дишни занимает всего 4 байта немного и когда сообщение высчитывается из Кафки оно идёт приложение идт Reg и накатывает схему на это сообщение Вот мы используем авра схему Да надеюсь ответил Спасибо большое за вопрос спасибо Вот теперь фонарь чуть-чуть правее значит через через проход Да пролёт вот пожалуйста чуть выше Спасибо за доклад очень интересно хотел бы задать такой вопрос с самого начала вы начали использовать cdc то есть бези Но это вообще довольно контро тема та же проблема с тем что вы раскрываемой вы отчасти решили outbox паттерном многие вообще cdc ассоциируют с чем-то вообще Legacy против него как вы пришли к тому что использовать Именно cdc они возможно какое-то другое решение возможно напрямую из приложения в кафку писать И почему Спасибо О'кей спасибо большое за вопрос а Лично я когда-то когда прочитал igna intensive applications просто вот у меня челюсть отвисла я такой Вау можно реально через cdc стримить данные со всей организацией у всех будут актуальные данные круто меня это супер впечатлило и поэтому мы когда-то тогда пошли в этот паттерн вот пожалел ли я Да честно говоря нет действительно многие команды отказываются Говорят мы не будем через cdc писать мы будем писать напрямую в кафку сами свои доменные ивенты но у них по крайней мере есть выбор они могут если они не умеют например Это вендоров ся база а не могут писать в кафку сами они могут настроить cdc если хотят сами не раскрывать имплементации Да Пожалуйста пишите пишите в авра Вот это не проблема вот так что наверно как-то Так отношусь к cdc Мне кажется это хорошая альтернатива спасибо да спасибо Вот ниже пятью рядами Да Следующий вопрос пожалуйста Да здравствуйте Спасибо за доклад Фёдор Сбербанк такой вопрос Да вы показали там ну такие новые необычные способы как там использовать каку вот нар ва с какими-то не типовыми задачами которые вы считаете например не целевыми Ну например у вас есть база аналитики они говорят сделайте нам там на линке быстренько обра пото что знаю нам так нравится и так быстрее вопрос в том как вы ограничивайте команды в этом Да чтобы как бы ну не распыляться на все возможные там кейсы которые в it существуют дахо Спасибо болье Поня вопрос пару примеров негативных кесов мы смиг из одного поса в другой поз с помощью вас Мы зальём ВС в каку и потом из каки накатим но тут Обычно просто берётся инструмент поса стандартной репликации и он накатывает второй как раз привели пример отличный это когда давайте мы в риал тайме всё проанализируем но за последние 10 лет вот это такое к сожалению не помещается обычно в память приложения Ну или работает очень медленно потому что спи на диск И мы такие ке стараемся тоже ограничивать как ограничивать Тут у на есть красный отдел архитектуры вот здесь даже коллеги сидят в первом ряду Да Привет Артём они очень помогают в этом то есть н банке можно прийти в отдел архитектуры спросить ребят как лучше это сделать И архитектура посоветует Какие существуют способы интеграции Вот и обычно ребята из архитектуры рекомендуют что-то другое больше подходящее потому что инструментов много есть и для аналитической обработки для стриминга и для не знаю просту интеграций и можно выбрать что-то подходящее для себя вот так ограничиваем Ну и просто говорим всё время с людьми ребят Не надо Давайте лучше Вот по-другому спасибо чуть-чуть левее Да Андрей привет Спасибо за доклад интересно Меня зовут Виталий эксплуатация Сбер тут был вопрос про консистентность данных между базами между базой и топиков куда пишется Да а как контролируется консистентность данных между потребителем и источником потребителем То есть получается источник событий кавка и потребитель событий э как-то контролируется ли то что потребители Ну несколько систем они все получили точно то там в точности до записей атомарных э всё то что отправилось А понял кажется Понял Вопрос на всякий случай уточню вот у нас есть один источник мы его заливаем в шесть разных систем Гарантируем ли мы что все шесть систем находятся на одном и том же вотермарки они получили вот Ровно до этого офсета все Да в этом вопрос Угу Нет не гарантируем каждый консмед команда читает сама независима от других пока не было таких запросов вот поэтому не могу то и над методами восстановления тоже не не рассматривали если данные потерялись да А если данные потерялись рассматриваем А ну во-первых понятно мы там всё заранее настраиваем чтобы оно реплицировать хотя бы на три брокера Вот и стараемся ничего не терять А и не теряли пока что во-вторых мы искренне верим в cdc что оно ничего не потеряет потому что оно пишется в headlock базы если мы на правильно настроили репликацию на базе чтобы она всё нормально переключала плечи то и там мы тоже не должны ничего потерять Вот Но а мы это пока не используем хотим использовать для критичных данных есть ещё один инструмент можно опенсорс тем же кавка коннектом настроить например Син данных из топиков S3 и точно так же сорсинг из S3 обратно соответственно можно довольно легко в пару кликов сделать бы копирование данных в S3 вот мы смотрим на это Но пока не делаем потому что Когда можно данные перекачать Спасибо и финальный вопрос а потом вот это движение означает что вы берёте спикера и идёте с ним в дискуссионную зону потом и он уже не под запись говорит вам правду Да вот здесь вот наверху Да да спасибо Андрей за доклад классно зовут Владимир архитектор Газпромнефть подскажите вот кон платформа Она сама по себе платная получается И решение которое там представлены в том числе конектор они там тоже с лицензией особенной это один вопрос какой-то обошли ши И второй вопрос в целом импортное законодательство как обошли с лицензиями каки Привет Спасибо большое за вопрос насчёт импортного законодательства рецензии Кафки Что это значит кавка же а 2.0 Почему её нужно как-то Нет не ко мы не используем каку мы используем оную каку что касается первой части вопроса про коннекторы действительно в России использовать и оплатить нельзя используем confluent platform мы просто посмотрели лицензии на сайте confluent и они отличаются у разных коннекторов например отличный пример - это hdfs connector конектор hup есть hdfs connector 3 который проприетарное нельзя только по лицензии есть hdfs connector 2 который похуже но использовать Можно пожалуйста бесплатно по apch 2.0 Вот то есть там у разных решений у confluent разные лицензии даже вот есть такие преобразование Single message transform когда перед записью в конектор данные немножко можно преобразовать даже та есть те которые они сделали проприетарный оказывается платный вот короче мы просто вычитывать только те которые мы реально можем использовать спасибо Угу Спасибо Спасибо по-моему Это было великолепно Давай те определим А можете все кто вопросы задавал махнуть рукой Давай определим Кто получит постгрес очень много вопросов было ребят Спасибо вам большое кстати за это у меня есть подарок отн банка тут много всяких подарков на самом деле целый мешок штук семь А я их подарю э кому-нибудь вот мне кому же давай какой вопрос тебя максимально зацепил где ты прям думал вот я просто даже не помню насчёт думал я просто помню что Отметь у себе Вот мужчину в третьем ряду нет подождите мужчину в а Поднимите руки пожалуйста а вот мужчину во втором ряду я отметил В голове так да вот тут надо бы подарить Я правда не помню Про что был вопрос Я помню что их было три да А про ребалансинг Да вот короче тут мне захотелось вот мужчина с лиом хоккеист получается полу Спасибо большое за вопрос Нужно второй выбрать ещ ещё один сувенир сувенир от конференции уже Кому Да я точно же помню что в голове был отмечен мужчина в третьем ряду в красной футболке Спасибо вам большое за вопрос опять же не могу вспомнить какой А про переиспользование по про переиспользование потоков тоже отметил что архитектурно важно целый час говорили про каку как бы вот да можно можно пожалуй вот на третий ряд сувенир от сувенир от конференции и тебе конечно же Андрей Тебе тоже сейчас будет памятный приз от от конфы Вот видишь за за призами уже идут Друзья у нас небольшой перерыв потом пред записано интервью и потом следую"
}