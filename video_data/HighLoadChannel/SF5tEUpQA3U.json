{
  "video_id": "SF5tEUpQA3U",
  "channel": "HighLoadChannel",
  "title": "Как мы держим миллион RPS в рекламе, троттлим трафик и не теряем при этом деньги / Михаил Кириченко",
  "views": 339,
  "duration": 2833,
  "published": "2025-01-17T02:25:09-08:00",
  "text": "Привет всем меня зовут Миша Я работаю в ВК 13 лет я техлит рекламной сети и так получилось что отвечаю за весь внешний трафик который приходит в рекламную систему это запросы на подбор рекламой это пиксели редиректы разные ошки трекеры поки статистики сегодня будем говорить только про запросы за рекламой это самые тяжёлые запросы с точки зрения ресурсов вычислительных Ну и с другой стороны это самые важные запросы для нас потому что если мы рекламу не подберём дальше не будет ни пикселей ни редиректов ни статистики Ну не денег в итоге а запросов за рекламы сейчас у нас 1 млн РПС в Пике даже чуть побольше и я считаю это довольно большая нагрузка которую надо как-то держать прибегая к разным ухищрения и ну Один из таких подходов - это троттлинг трафика и сегодня мы посмотрим как троттлинг на помогает держать нагрузку Ну и в целом лучше перформия сервера Куда приходит вся наша нагрузка эти сервера у нас Термини с сль и балансируют трафик на фронты которые находятся дальше 280 фронтов с них начинается обработка запроса там мы запрос распаковываем дешифрування дальше мы запрашиваем наши рекламные движки чтобы они вернули нам рекламу устраиваем аукцион низи отдаём ответ вот у нас есть два рекламных движка это рекламный движок Target Да бывший mail.ru и рекламный движок ВК и ну и также у нас есть десятки внешних партнёрских ДСП которые мы подключаем к себе и тоже можем их деман получать к примеру Яндекс у нас подключен как ДСП вот в рекламных движках это уже счёт идёт на тысячи серверов там более нано 3.000 серверов входят рекламные движки помимо серверов движков У нас есть инфра это те сервисы которые нам возвращают информацию о пользователе предикт сом наши хранилища тарантулы э кластер тоже довольно большой более 1.000 серверов а сервисов этих а более сотни на всё это приходит запросы 1 млн РПС - это все запросы из них как я уже говорил 1 млн именно подбор рекламы и всё это порождает примерно 2 млн событий статистики в секунду которые соответственно тоже нужно обра теперь немножко Вот Перед тем как мы перейм к ролин Я немножко расскажу о рекламе как мы к этой задаче приходим и почему тролинг в рекламе он немножко отличается от троттлинга в обычных там ошка я нарисовал самую простую схему рекламной системы которую смог это один квадратик в который входит два потока и один поток выходит пото вхо это за это за ко приход изн приходит от наших проектов внутренних там в Сервер это те запросы которые мы должны обработать Да другой поток - это креативы та реклама которую приносят нам рекламодатели в наш кабинет это тот деманд который мы можем показать запросы встречаются с креатив врек системы в результате мы Зарабатываем деньги за Ной си заработать Дема зат возвращаться по ходу презентации так получилось что ни потоком запросов ни потоком креативов мы не управляем запросы могут вырасти по причинам а которые от нас не зависят подключился крупный проект мы вот купили там зн и он нам принёс очень много запросов упал какой-то там крупный проект в интернете люди пришли в ВК трафик вырос вышла новость люди пришли трафик вырос всё это может привести к определённым проблемам поток креативов он не меняется так быстро как поток запросов но всё же возникает ситуация когда у нас на каком-то среде трафика нету подходящего деман к примеру и мы запрос обрабатываем нагружая инфраструктуру но при этом ничего не покажем денег не заработаем а нам нужно деньги зарабатывать это тоже вызывает некую проблему которой мы будем бороться дальше по ходу презентации Давайте подробно посмотрим что происходит при перегрузке А что приходит при не подборе Ну при перегрузке когда мы запрос обрабатываем мы ходим во внешние сервисы во внешние хранилища ходим с неким таймаут нагрузка на машину растёт время ответа мы начинаем таймаута не выполнять начинаем закрывать соединение открывать новые соединение как известно новый конек Он намного дороже чем лаев это в свою очередь приводит к е большему Роу пуру приводит к е большему росту нагрузки ВС это входит в цикл и в итоге у нас превышает нагрузка там на 10-20 а машина начинает деградировать практически полностью деградирует одна машина деградирует её Группа деградирует Группа деградирует кластер И в итоге при небольшой перегрузке Мы можем потерять Ну там 80% рекламы или даже больше и денег не заработать что для на очень плохая ситуация это ситуация когда у нас трафик приходит мы его обрабатываем а рекламы нету и в принципе мы могли бы заранее предсказать что вот на этот запрос мы ничего не ответим и не отвечать И тем самым сэкономить ресурсы и потратить их на более полезные запросы причём этот не подбор случается не только когда у нас нет деманд к примеру может быть плохая интеграция с площадкой как-то поставили код криво площадка перегруженной рекламой засунули На десятую страницу рекламный блок и в итоге запросы есть показов нет или есть показы а нету целевых действий в итоге нет денег и всё плохо эти две задачи нас приводят как раз к задаче троттлинга троттлинг - это ситуация когда мы не обрабатываем запрос отбрасывая некий заведомо Ну некий пустой ответ это помогает нам во-первых при перегрузках сохранить наш кластер живым а также при не подборе позволяет нам избавиться от бесполезных запросов которых на самом деле тоже много теперь про особенности троттлинга в рекламе вот троттлинг в пишка в обычных - это взять и выбросить просто часть запросов случайных он не подходит для рекламы в рекламе каждый запрос имеет свою ценность с точки зрения вот тех денег которые он нам может принести то есть какой-то запрос больше принесёт какой-то меньший и было бы здорово если уж нам пришлось тротлит запросы начинать троли самых невыгодных да И постепенно подниматься подниматься подниматься Какие запросы можно тротлит Давайте посмотрим на те вот типы запросов которые существуют которые мы не очень любим Ну первое - это фрот это мошеннический трафик трафик с подделкой идентификаторов попытки накруток трафик из под нето этот трафик Как вы знаете в интернете присутствует его довольно много он никак не монетизируется И даже если мы эти запросы будем обрабатывать они потом будут отброшены статистики никаких списаний по ним не произойдет это запросы нам не нужны и мы можем их отбрасывать 100% невыгодный трафик трафик когда обработка запроса стоит дороже чем этот запрос принесёт нам денег нами посчитано стоимость наших серверов нашей инфраструктуры и есть некое число в системе которое означает вот сколько нам стоит копеек обработать запросов если средст трафика прит больше денег есть обва если он приносит меньше денег наверно есть смысл как-то отбрасывать запросы но при этом мы не можем как в случае фрода отбрасывать все 100% запросов Почему Потому что ситуация может поменяться рес может плохо перфор потому что ну плохо компания настроена Да блок коза плохо стоит или там деман нет Да ситуация может поменяться поэтому мы должны всегда оставлять некий контрольный трафика чтобы ситуа е же растлить этот трафик да И опять начать его принимать и обрабатывать следующая категория очень интересная - это пере запросы он вот этот трафик с пере запросами очень характерен для видео видео сетей видео партнёров они стремятся лучше монетизировать встраивают множество систем в плееры настраивают медиации и возникает Интересная ситуация что мы заходим на страницу открывается видеоплеер и к нам система прилетает там 3 4 5 а бывает и 10 20 30 запросов от одного пользователя Прим при том трафик нормальный его можно монетизировать на нём можно заработать но нет смысла отвечать на все запросы потому что реклама не будет показана есть смысл ответить на один из этих тридцати а остальные отбросить и всё и мы денег не потеряем и денег заработаем иреу и ресурсы Не потратим ну и последнее - это такая последняя линия обороны Случайный троттлинг вот если мы выкинули фрод выкинули невыгодные запросы выкинули пере запросы А у нас всё равно большой боль много запросов и у нас кластер страдает мы должны начать троли случайные запросы Ну это лучше чем ничего Да лучше чем деградировать Каким должен быть троттлинг Ну немножко очевидных вещей он должен быть быстрым он должен быть эффективным и точным быстрым с точки зрения срабатывания правил Да как только мы обнаружили фрот должны тут же его забанить эффективным с точки зрения ресурсов трот ролин нужен чтобы ресурсы сэкономить Да если он будет ресурсы тратить он ну потеряет смысл да поэтому никаких тяжелых моделей никаких тяжёлых вычислений мы здесь делать не можем Всё должно быть очень предельно просто предельно быстро и точно с точки зрения выбора разреза трафика Ну если мы видим фрод с какого-то одного адреса нет смысла резать там всю подсеть да и с одного фтора пойдём дальше мы будем всё ниже и ниже опускаться к технической реализации как мы тролим немножко про устройство нашей сети У нас есть разные роли серверов по типам запросов которые они обрабатываются но в целом их можно поделить на две больших группы это сервера которые принимают запросы из интернета и сервера которые принимают запросы сервер на sts с наших Ну с наших проектов ВК окн и так далее вот перед теми серверами которые принимают запросы с интернета у нас находится слой у тех самых прокси балансировщика которые СС Термит и балансируют трафик между экст фронтами Да а перед внутренними фронтами Internal fronts У нас нет таких балансировщик и балансировка возлагается на гинс которые стоят на стороне проектов сами же фронты себе анонси е CD А гинс через Аси модуль астрим забирают и Знают куда ходить вот дальше за нашими фронтами Дары зелёненьким за которые как раз отвечаю я и моя команда находится наша Ну инфраструктура скажем так там находятся наши сервисы наши хранилища наши баннерные движки там же находятся другие ДСП которые мы от нагрузки спасать и будем трот ли мы на наших фронтах почему во-первых это можно сделать однообразно на фронтах доступна доступны все данные о запросе на фронтах мы можем сходить в наше внутреннее хранилище чтобы получить дополнительные данные о пользователе Ну если Вдруг пригодится правила троттлинга каждое правило троттлинга состоит из трёх полей это средств трафика то есть та часть трафика на которой это правило будет работать это действие которое мы применяем и это опционально значение если оно этому действию соответствует средств трафика он определяется набором из нескольких полей они могут комбинироваться случайным образом не случайно мни определённым образом тип интеграции и рекламное место потому каждое рекламное место может на сайте форми по-разному Да может быть баннер в шапке хорошо форми там баннер где-то внизу плохо форми Мы должны уметь тротлит один и оставлять покое другой IP подсеть - это в основном тролинг фрода идентификаторы пользователя опять же либо тролинг фрода либо троттлинг на уникального пользователя Иво - это те основные параметры которые наверно настраиваются в любой рекламной кампании первыми которые хорошо бьют наш трафик на кластера и при этом эти кластера перформ по-разному Да допустим Один гео на одном сайте региональном будет работать Совсем по-другому как там гео не в этом регионе действия в принципе соответствуют тем типам трафика который мы хотим от которого мы хотим избавляться действи заставляет запрос отбросить и забыть про него действие задаёт полосу пропускания от нуля до единицы если мы включаем это действие мы бросаем кубик кубик больше мы запрос обработаем затм меньше обработаем правила заводит ограничение на количество запросов от пользователя отника в секунду ины интервал времени это то классическое ограничение на маль Ну эдим мы свои площадки там УК ВК потому что считаем Рафик сни довольно чистым эдим некоторых партнёров с м есть договорённости но мы не можем Элю потому что ну был Горький опыт Если дать ручку налю очень много и ПС перестанет работать и кластер не будет защищён дальше мы рассмотрим как работают каждые правило Почему они такие быстрые на самом делени довольно просто устроены и справляем задачей Давайте начнём с негодного трафика самое интересное Как его найти У нас есть сервис так D который как раз нужен для того чтобы искать невыгодный трафик и обновлять коэффициенты пропускания вот на срезах где мы этот трафик обнаружили этот сервис принимает наш Лог запросов и Лог событий запросов у нас миллион в секунду событий у нас 2 Милн в секунду все эти 3 мл в секунду приходят в сервис и он начинает строить агрегаты по статистике из каждого события он выбирает срезы к которым события относятся да комбинируя те параметры которые были на слайде до этого и из статистики по событиям по показам там есть поле сколько мы это показ денег должны списать начинает суммировать деньги и суммирует их в некие интервалы на данный момент это интервалы по 5 минут и считает сколько запросов было со среза Сколько было денег с этого среза срезов у нас примерно сотни тысяч когда мы интервал один заканчиваем Начинаем писать следующий интервал Когда заканчивается последний интервал это кольцевой список переходим к первому стираем его Начинаем писать заново таким образом этот сервис постоянно держит в памяти текущее состояние по всем нашим сотням тысяч срезов Сколько было денег и сколько было запросов вторая сть Серви она как раз про обновление коэффициентов раз несколько минут запускается эта часть она агрегирующие начинаем наши коэффициенты пропускания либо увеличивать либо уменьшать и каждый раз когда мы коэффициент меняем мы прогнозируем А сколько трафика он нам даст или сколько трафика он зарежет и как только мы лимит выполняем сохраняем коэффициенты в базу ждём следующий интервал Когда придёт новая статистика сравниваем и обновляем ещё раз дальше теперь про ограничение наника эта схема немножко сложнее потому что здесь надо чтобы ролить по уку нам нужно ходить во внешний сервис мы могли бы использовать мы не можем использовать модуль потому что у нас много идентификаторов пользователя у нас много проектов Да у нас есть ки У нас есть почта У нас есть УК ВК и пользователь идентификатором к машинам не прибитые Поэтому если мы хотим ролить на ука мы должны иметь какой-то сервис координации который нам будет сообщать А сколько я этого пользователя видел за последнее время такой сервис у нас называется урек Инн на запрос в него посылают ключ пользователя первичный ключ для данной площадки в ответ получают ответ А сколько я этого пользователя видел за последнюю секунду например и уже фронт может решить от роли запрос или не Троль в зависимости от лимита внутри он работает примерно также как и сервис агрегаты статистики строит агрегаты только уже по секунде каждую секунду начинает новый Когда заканчивается последний возвращается к первому и в принципе за о1 Да из вот этой ш таблички мы можем получить значение для любого пользователя любого идентификатора сколько мы его раз видели через серс у нас прилетает примерно половина запросов где-то 500000 туда прилетает и где-то ядер 30 он занимает то есть довольно эффективно мы его написали мы сделали ным чтобы уменьшить запрос ответ срук без синии работал дальше дальше ограничение по купс Ну если у нас другие правила не сработали нужно нам и попс поролить как-то вот здесь уже можно не ходить никуда наружу Да в сер синхронизации и сделать всё на одной машине машина у нас анонси анонси вмест своим весом и каждая машина может узнать свой вес В общем трафике если есть лимит на срез мы можем умножить его на вес и понять Сколько одна Эта машина должна трафика пропустить дальше алгоритм Довольно простой получаем срезы для текущего запроса и для каждого среза смотрим Ани задан ли уже коэффициент пропускания по псу если задан кидаем кубик тролим или не тролим запрос коэффициенты хранятся в шарен памяти даёт отличный для работы шано памятью потому что работает во много воркеров да и нам нужно как-то между ними синхронизироваться вот у нас одно хранилище в шаре най памяти если мы запрос тротлит отлили а если мы обрабатываем мы должны как-то его учесть что мы его он пришёл Да и мы его обработали вот если мы запрос обрабатываем мы должны увеличить счётчик что этот запрос был обработан Там же в шаре най памяти по ключу в виде среза плюс текущий тайм СМП мы инкремент счётчик наступает новая секунда мы переключаем на новый счётчик Каждый раз когда наступает новая секунда мы на одном на одном процессе это заметим чтобы обновить коэффициент наступила Нова секунда смотрим Сколько было запросов в прошлую секунду мы это п наш умножаем на вес делим на реквесты и наш коффин пропускания умножаем на этот на это число то есть мы моментально за секунду приводим коэффици к тому виду который ожен быть ИМТ память чтобы первая часть алгоритма уже с этим работала вот этот такой подход Он простой но он он Не сработает если у нас трафик резко меняется Да если у нас в одну секунду там 10.000 в другую секунду 5.000 но у нас реклама в интернете да запросы рекламы они не меняются быстро они довольно плавные Да ну не меняются там два раза за секунду они довольно плавные плюс у нас 300 машин Они между собой как-то этот трафик разбрасывают и в целом Вот такая простая схема довольно хорошо держит лимит qps если он срабатывает на самом деле когда qps срабатывает правило это для нас проблема у нас Арт происходит потому что qps когда срабатывает Это плохо Это значит мы теряем запросы которые могли об работать а возможно полезные запросы Да и мы должны что-то сделать как-то железо перебан сирова найти какие-то ресурсы для этого дальше А вот это было как правила работают а теперь как же мы эти правила доносим до наших фронтов у нас этих правил 500.000 500.000 разных правил троттлинга и с нашим миллион мы не можем Они лежат в базе данных SQL и с нашим мы не можем в базу данных ходить потому что мы её свалим потому что на каждый запрос ещё у нас каждый запрос может входить там в 10 срезов для каждого среза проверить Пять правил ито мил данные из базы мы определённым образом готовим в первой версии мы их селекти и писали в специальную ш табличку мы е разработали это такая хш табличка которую можем положить в плоский файл и этот файл Отправить на фронт мы этот файл доносились буквально какие-то микросекунды даже если не не наносекунды практически моментально Единственный минус такого подхода был в том что э-э данные долго Доезжаю это занимало десятки секунд бывало минуты и мы придумали другую схему мы сделали Сервис К vmap СРВ этот сервис Он подключается к базе данных лип слейм и получает Лог изменений и каждый раз когда что-то меняется он эту хэш таблицу обновляет прямо на той машине Где inin находит То есть он находится прямо на машине с инсо обновляет ш таблицу кладёт новый файл ин видит что новый файл появился мат его в память и вот у нас уже новые данные такая схема позволяет доносить нам данные от тролинг за менее чем за секунду в принципе вот вот эта кв Мака это файл используется не только для троттлинга там же работают а тесты там же все фичи флаги там же там динамические бит очень много-много полей в этих таблицах у нас доезжают и в принципе очень быстро работает и вот этот подход с бинарными файлами которые доносятся э до машины и мапи в память у нас используется довольно широко в рекламе и этот подход уже десятилетиями себя зарекомендовал как очень быстрый эффективный немножко статистики что получилось у нас ну я нарисовал несколько разрезов Чтобы показать что трафик у нас бывает довольно разный Вот это статистика видео сети у нас ну как видите че запросов режет правила ПС ой правила рек то есть на видеос очень много пере запросов три запроса из четырёх это пере запросы которые можно не обрабатывать на само деле там есть некий такой дисбаланс есть площадки нормальные А есть площадки которые присылают там десятки запросов в секунду немножко троли также в правиле негодного трафика мобильная сеть здесь уже нет пере запросов зде 10% уходит на невыгодный трафик примерно 5% уходит на фрод Ну ПС везде по нулям потому что у нас сейчас всё хорошо мы в принципе можем держать и там пол милна ПС сейчас у нас чуть меньше трафика чем у нас железо поэтому мы справляемся и правила ПС не работает Ну и самое интересное если мы просуммировать да чуть больше в Пике примерно 50% запросов это все наши правила отбрасывают то есть мы избавились от половино нагрузки и при этом ничего не потеряли то есть ФД мы не монетизируется поло запросов и в принципе при наших при нашем парке серверов это тысячи и тысячи серверов сэкономленных в принципе что приносит большие деньги заключение Что же нам тролинг помогает ну две очевидных вещи которые следуют из доклада сэкономить железо за счёт за счёт того что мы отбросили запросы которые денег не приносят трон помогает нам уберечь кластер от перегрузки и два неочевидных это во-первых повысит показатели нашей рекламы для рекламодателей если они смотрят свою статистику А мы откинули фрод откинули невыгодный трафик Да откинули пере запросы они будут видеть более чистый трафик который лучше перформ и ещё один пункт - это а мотивировать издателей улучшать интеграции опять же если издатель видит что фрейт низкий Да и он может в кабинете увидеть А сколько же было затратно запросов Да и подумать А может быть мне что-то поменять поменять настройки положение рекламного блока что-то сделать чтобы заработать больше Да чтобы мои запросы не трот чтобы запросы не троли Да ну что в заключени можно её сказать если у вас есть рекламная система если у вас много запросов Я рекомендую посмотреть а все ли запросы полезные приносят ли не деньги Возможно вы тоже найдёте очень много запросов которые можно отбросить и вашу систему Спасти ну или как-то ей помочь на этом У меня всё можно задавать вопросы Спасибо Михаил напомню что зрители онлайн-трансляции могут кликнуть в плеере на кнопку перехода в чат и с хэштегом вопрос задать нам вопросы Я их зачитаю пока что вопросы Из зала Давайте молодой человек на первом ряду раз раз Гергий Большаков лофти Мы тоже занимаемся рекламой и для рекламодателей критично важно именно и самому рекламному так рекламодателю в рамках рекламной кампании не показывать не давать один и тот же трафик два раза но при этом к вам этого пользователя Ну можно приводить по несколько раз таким образом Если вы просто отслеживается сколько раз пользователь видел какую-то рекламу на какой-то платформе вам вот именно чик уникальных посещений не сильно поможет Может быть вы как-то поумнее это дело считаете то есть в разрезе отдельных рекламных компаний дспк и так далее и каким образом это технически решается потому что тогда у вас увеличивается количество счётчиков которые вам нужно одновременно проверять за вот собственно один троле смотрите Эта система троттлинга Она работает самом начале отбрасывает невыгодный трафик Да и пере запросы и так далее дальше когда мы уже рекламу подбираем вот там уже встаёт вопрос об ограничениях если в рекламной системе в рекламной компании есть ограничения и будет показ Да эти ограничения или там будет запрос будет показ эти ограничения будут записаны к пользователю сохранены в серверное хранилище и мы будем знать сколько он видел этот баннер этот компанию это рекламное место и на все эти объекты мы можем ставить ограничение чтобы на следующий запрос эта реклама не была подобрана и рел не увидел этот запрос То есть это просто другая система которая работает позже именно на показы на запросы на клики и на события То есть это на уровне уже ваших вот платформ после троттлинга уже про это происходит позже уже при подборе рекламы мы на этапе вот в моей части мы грузим все данные о пользователи сом Там и так далее в том числе мы грузим ограничения Я называл вот эти ограничения - это та самая история что он видел чтобы потом когда мы баннер Ищем все Те кто не подходит под ограничение отбросить Но ведь тогда получается это лишние запросы к дпм Вы можете в принципе в дпш с запросом не ходить Ну и в свои собственные рекламные платформы и Это столько вызовов сэкономит А мы не пойдём а то есть всё-таки это на том же сервере происходит где тролинг ещё до похода Да загрузка всех ограничений происходит там там же можем решить если ограничения не сработали там же не ходить дальше за деман дом а Спасибо Давайте молодой человек иду за колонной долго тянет руку Да спасибо огромное интересный доклад хотелось уточнить вот rps реально ужасающий можно так сказать А lency у вас какой Вот то есть сколько вы себе даёте время на ответ по рекламе смотрите зависит от интеграции и где-то есть одни ограничения где-то другие ограничения где-то нужно ответить там за 100 м секунд где-то там за 30 мсн но в среднем Я думаю ответ где-то 50-60 миллисекунд Но это в среднем есть тяжёлые интеграции и тогда вот интересно применяете ли вы где-то внутри условно бачин потому что ну вот есть такое ощущение что где-то с 50 миллисекундами Я скорее верю что можно упираться ещё и в сеть Где вы просто очень много запросов единичных пуе используем что бачин Ну например вы там Повали вот эти уже хороший запросы накапливается там условно 50 запросов из одного г батчинг нет нет батчинг мы не используем мы успеваем всё обработать Спасибо Давайте девушка в последнем ряду Я здесь Спасибо за доклад У меня два вопроса А первый скорее продолжение про время вот насколько время обработки всего запроса увеличилось при включении этой вот всей системы весь пайплайн и второй не решается ли проблема одной High системы введением другой High системы у которой тоже могут быть проблемы смотрите А во-первых про доп нагрузку Это хороший вопрос никакой доп нагрузки она Это не несёт вот сходить в кв маку в оперативную память это настолько дёшево по сравнению с остальными действиями что в принципе никакой доп нагрузки это не принесло за исключением проверки У потому что нужно по сети сходить в сервис но он очень оптимизирован буквально 2-3 миллисекунды уходит на поход на проверку ука но и уники нам помогают срезать половину трафика То есть это выгодно это первая часть А про другую систему вот дальше идёт у нас баннерные движки Да Таргет наш движок ВК вот там задача гораздо сложнее подобрать по эти Коста 500 рену рек если мы вот каждый запрос наш сэкономленные Это огромный плюс тан молодой человек во втором ряду ближе к проходу Да спасибо А спасибо большое за доклад а тоже там с личной болью он соприкасается захарин Игорь сбр и А у меня собственно два вопроса Первый запрос а когда показывали где-то категорию вредного трафика и там в колоночки фрот а Ну были графы IP там и подсети то есть как бы это получается категоризация трафика определяется на Ну некоторые внешние ресурсы Да там условно с Intell что вы заранее определяете какие-то айпишник с родом и бывает Вот ещё момент когда при допустимом количестве запросов важно их качество да то есть условно какой-то метод который грузит он условно Ну там какой-то вложенный особенно или ещё что-то и лидие ли вы такие запрос то есть условно количество запросов допустимое Они вроде бы легальные но негативно влияют на всю систему и ели у вас какая-то там предва вот сталкивались уже с такими или просто тип трафика у вас несколько иной поэтому это не нужно так нет какой-то пред валидации у нас нету И таких запросов которые бы сильно отличались Да по нагрузке не приходит и не обнаружено А вот про фрод Да мы ищем аномалии в трафике дальше эти аномалии изучаются На данном этапе в основном вручную и мы ищем эвристики по которым мы можем потом этот фрот найти и эти эвристики вносим в систему и она дальше нам начинает находить фрод но правда сейчас мы отрабатываем более умную систему которая сможет фродить автоматически Да на основании поведения запросов пользователя Но это в будущем И вот вопрос как раз по коэффициента по доставке коэффициентов Ну понятно как они доставляются А как они размещаются то есть ну логика реализована сразу в инсе он как-то умеет там в логическую конструкцию их подкалиберные ЕТ по плеву на сервис сервис пишет файл файл мапи в память А в гинс уже наш код который мы написали он умеет ходить в шарю память и эти правила оттуда искать Угу всё понятно Большое спасибо Давайте в эту часть зала молодой человек В в третьем ряду Спасибо за доклад А можете немножко подробнее объяснить как устроен экспортёр из мускули внутрь самого нса Это что-то самописный скрипт нало или на чём он выгружает именно из музку и вот в эту вот SH первая часть сервис который он слушает он по протоколу й коннектится к мускулюс ВОМ и получает Лог изменений и на основание этого лого изменений он начинает апдейт своё внутреннее состояние Да и скидывает его на диск в виде этого файла в виде кв маки Угу а ining уже следит mtime файл обновился обновился и он его мапи в память А старый отмаливания дом сидит Здравствуйте спасибо Я хотел спросить вы показывали графики Где в Пике было милин 100 а про ограничение Говорите пол милна Ну кажется с первого взгляда что не очень большой запас в плане если условно умрёт ДЦ или какие-то форс-мажоры когда люди начинают Много ходить я правильно понимаю что для вас это просто не ну не первая цель держать такие вещи или как-то это обрабатываться умеет ну у нас есть запас и по цпу мы стараемся его не держать там выше 30% и у нас есть много ДЦ Да если один упадёт это не не Половина наших мощностей Ну упадёт ДЦ запрос на других вырастут Начнут работать правила купс пока они работают спасают остальные машины Мы что-нибудь придумаем Спасибо А давайте молодой человек во втором ряду Здрав здравствуйте Дмитрий у меня такой вопрос Он наверное немножко и про взаимоотношения смерчом который поставляет вам трафик и про нагрузки вот к вам может ну быть площадка мерч которая даёт трафик и даёт его довольно много но при этом там допустим маленькая проходимость или много повторных запросов отсюда два выхода мы можем порекомендовать ей не показываться Я так понял у вас есть ТБ статистика которая показывает ему что-то и может же какой-то допустим менеджер по рекламе Есть ли какие-нибудь люди которые накидывают значит Ну вот аналитики которые могут связаться с источником это один вопрос а второй вопрос Он про кэширование запросов или их суммирование допустим у вас пришёл ну мерч пришёл с одним клиентом 10 раз Вы же можете отдать ему один и тот же ответ как бы если Это насколько релевантен такой подход Вох менеджеры Да есть менеджеры которые прикреплены к площадкам клиентам особенно крупным площадкам и в случае чего Ну в случае вопросов Они ищут способы если Фрей низкий как его повысить Да начинают конечно проверять а не троле площадка А про каширование Нет мы не каширу и считаем Ну каширование для слабаков получается весь трафик который пришёл он либо хороший и пошл обрабатываться либо весь ушёл Ну Ну не весь а ушёл в троттлинг То есть если это с одной площадки один клиент он всегда либо в троттлинг либо в обработку отдать тому же клиенту допустим Ну с разницей в секунду тот же ответ вы нет нет Мы так не делаем но не обязательно там есть правила которые работают по вероятности qps request Rate они работают по вероятности То есть он раз не получил рекламу на следующий раз может получить да А если это фрод да там 100% отрезается кстати небольшой спойлер если кому интересно что происходит с оставшимся п там полмиллиона запросов в секунду завтра будет доклад моего коллеги Артёма Букина про баннерный и как они уже оптимизируются оптимизируются чтобы на эти все запросы ответить молодой человек В первом ряду у нас ещё есть время на вопросы всё хорошо спасибо за доклад А хотел спросить про фильтр по ПС он пересчитывается раз в секунду и не и какие это секундные окна не лини на кластере и не приносит ли это каких-нибудь спецэффектов в начале или в конце астрономических секунд они не синхронизированы между машинами это раз И это даже хорошо для нас приносит ли всплески знаете я не замечал не замечал и всё что я видел это ровная линия пса если мы правила включили понятно то есть они рассинхронизировались спасибо спасибо четвёртый ряд Здравствуйте Огромное спасибо за доклад Алексан подскажите пожалуйста вы ролите там до пки то есть или ничего не выдаёте или выдаёте сразу или у вас есть какой-то ряд результатов которые Ну вы там не смогли быстро достаточно отработать хорошим какой-то хорошей системой которая подбирает рекламу Ну пробуйте попроще ещ попроще нет наша система это прямо вот первый первый первый шаг на самом начале мы либо затролили Вообще либо запрос обрабатывается а если вместо троттлинга стать какую-нибудь возвращать Ну около рекламную становится хуже не пробовали Мне кажется площадки удивляться если мы начнём статику возвращать Ну там не знаю наво площадке можем мартика вот на стороннюю сеть это будет странно Спасибо второй ряд Третий третий Да спасибо за доклад может быть в самом начале говорили начало пропустил А сколько ресурсов выделено вот под эту систему троттлинга смотрите сервера которые вот в моей команде это 16 первых Проси балансировщик и 280 фронтов которые идут дальше то есть 280 серверов они в том числе тротлит Они грузят данные о пользователе они проверяют ограничения они определяют как запрос вообще Бут выполняться схема выполнения запросы запрашивают ДСП получают ответы аукцион устраивают низи ет То есть тролинг - это вот маленькая маленькая часть того что они делают за счёт того что эта система вот написана так оптимально Она работает очень быстро она не тратит ресурсов практически никаких Ну то есть это сервера как бы не только по трон А да да это сервера выполняют кучу работы это ролинг - это мелкий шаг но очень полезно Как видите половина трафика Мы за счёт этого экономим Понятно спасибо пожалуйста первый ряд Михаил Спасибо У меня такой вопрос может быть даже шутка но он меня очень сильно беспокоит многие годы Вот вы рассказываете про систему там миллионы запросов в секунду РПС Да ну вот у вас в схеме мускул Почему мускул Почему не постгрес почему не Oracle Возможно это не к вам вопрос меня просто очень он беспокоит знаете он был 20 лет назад мускул и до сих пор там сохранился это исторически сложилось и мы же не ходим в него с этим ПС то есть мы всегда его ограничиваем как-то данные из него перевариваем готовим вот эти мап которые носим У нас их много не однакова мап Ну работает не трош Спасибо пожалуйста четвёртый ряд Да здравствуйте Михаил Спасибо за доклад У меня вопрос такой Как вы считаете стоимость обработки потенциального показа рекламы динамическая одинаковая ли для сегментов и рекв спасибо не совсем ко мне вопрос Ну этот коэффициент он один на всю систему и он рассчитывался Каким образом Мы считали стоимость стоимость серверов стоимость сети стоимость аренды машин то есть довольно большие такие расчёты проводились Ну потом поделили И ткнули пальцем в небо сказали вот столь копе стоит запросов крутили Когда нам нужно было когда вот ещё не было системы поиска Вот это негодного трафика и когда мы просто могли либо троли либо не троли Да вот в этот момент мы если хотели ролить больше мы говорили А давайте запрос стоит у нас там на Копейку больше сся запросов если надо растро уменьшали сейчас мы эту ручку уже не трогаем у нас есть вот сервис который умеет ролить начиная от самых плохих То есть в принципе стоимость особо не нужна Давайте в первый ряд Да здравствуйте Спасибо за доклад у меня такой вопрос про фрод э на этапе троттлинга очевидно там часть фрода отваливается но часть проходит дальше Вот какой Ну насколько короче эффективно это делать на этапе троли процент который пост уже обрабатывается трот Ну вот уже когда там Сата пришла что угодно Нако в второй антифрод который уже работает по статистике То есть когда мы уже запрос обработали пришёл статистический поток и там мы можем тоже обнаружить что что-то мы там должны за фродить есть ещё и третья ступень когда мы оффлайн после уже постобработки можем что-то найти и обратно это заро да и обратно деньги начислить А вот сколько я не скажу потому что это уже не в моей зоне ответственности это уже Вопрос статистики дахо Спасибо Давайте девушка в последнем у меня ещё один вопрос а можно к слайду про вот срезы Да ещё назад Вот он где-то в начале был вот а вот как бы с действием понятно как будто бы вот срезы они Понятно Вот как они возможно добавляются Да а вот управлением вообще вот всей этой информации по правилам ком ф Не фрот а вообще в принципе трот осуществляется вот он кем управляется вот всё это дело управляется разработчикам и определяем Ну нам приносит задачу что вот мы должны на учиться тротлит по связке рекламное место плюс там не знаю гео плюс там индификатор площадки какой-нибудь мы добавляем новое новый ключ среза и у нас весь трафик начинается на это средст тоже проверяться А всё это потом всё это дело удаляется как-то ну информация она как-то обновляется вот если правило не нужно мы можем удалить его конечно а оно вручную удаляется Да это ручная работа Спасибо А мы уже немножко вышли из тайминга я вижу несколько поднятых рук пожалуйста не постеснялся в кулуарах А сейчас нужно выбрать лучший вопрос и одарить вопрошающий надо было трот литься большое спасибо всем за вопросы э дальше Спасибо"
}