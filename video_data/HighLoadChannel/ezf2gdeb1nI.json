{
  "video_id": "ezf2gdeb1nI",
  "channel": "HighLoadChannel",
  "title": "Кластеризация с помощью etcd / Петр Растегаев (AGIMA)",
  "views": 329,
  "duration": 2433,
  "published": "2025-01-17T02:27:54-08:00",
  "text": "Я здесь чтобы поговорить с вами про кластеризация и про способы которыми её можно достичь началось всё с того что перед нами встала очень амбициозная задача нам надо было разработать систему контроля гипервизоров поддерживала кластеризации была бы доступной и Отказ устойчивой какие у нас были требования к системе нам надо было во-первых хранить состояние самого гипервизора В этой системе то есть любую виртуальную машину любой контейнер который мы там создавали мы должны были отслеживать его и давать пользователю возможность им управлять нужно было хранить всевозможные конфигурации это конфигурации серверов которые работают в системе конфигурация самих виртуальных машин нужна была система се Discovery это всё должно было работать под высокой нагрузкой предполагали достаточно большие нагрузки там десятки или даже сотни тысяча маета отка устойчивым си первое что рассмотрели это был Консул это распределённый сервис от Он позволяет сес Discovery там есть хранилище его многие используют он удобный там много плюшек но у него есть один большой минус это лицензия в наши времена всё что не подр зау в проекте у нас ещ помимо всех требований которые я назвал было ещ одно требование пройти лицензирование в стек такая вот Страшная штука и требования встк Консул Категорически не соответствовал дальше мы посмотрели зупер старый проверенный продукт используется много где вроде бы стай коды это проверенное зрелое решение но есть у него свои минусы по производительности достаточно много проблем написан на Джаве и несёт с собой много всего что связано с проблемами производительности в Джаве утечки памяти и прочее и опять же вот все эти страшные аббревиатуры типок почему-то не очень хорошо смотрят на как-то сложилось что не очень они их любят мы стали искать дальше думали даже написать сами что-то думали целых 10 минут и передумали Нет свой свой велосипед иногда это вариант но не в такой амбициозной задаче Это значительно Удо бы нашу разработку потому что опыта ни у кого из на разработки такой штуки не было и най человека кото такой опыт на рынке тоже задача не тривиальная и стоить он будет как крыло от самолёта и не факт что всё равно у вас получится Хотя честно сказать я даже начал набрасывать свою реализацию алгоритма ф но быстро передумал Ну и в результате мы рассмотрели etcd etc реализует алгоритм он простой и надёжный отказоустойчивый и он автоматически восстанавливается после сбоя это очень антихрупкость Он написан Наго и достаточно быстрый производительный Шустрый Ну из минусов с которыми мы столкнулись которых я позже расскажу подробнее Это для распределённого сервиса требуются дополнительные пляски с бу ёмкость хранения данных ограничена 8 гиба Ну и как таковой горизонтальный скейлинг скажем так очень сильно ограничен результате мы выбрали etcd Он поддерживает и распределённое хранение конфигурации и распределённое сохранение состояния сервиса и Отказ устойчиво а к тому же это тоже достаточно зрелое решение уже третья версия выпущена сама система появилась получается получила популярность где-то в середине десятых используется тоже много где в частности в кубернетес и в Оке Ну и у неё достаточно простой опи под которой написаны библиотеки для практически всех языков сейчас поговорим про то как мы собственно реализовывали эту нашу кластеризации нашу систему уже используя hcd сначала надо чуть-чуть в теорию погрузиться объяснить что такое кворум ная операция в etcd для все практически для всех операций требуется так называемый кворум большинство нот должны согласиться на операцию дать своё добро то есть для кластера из узлов кворум мы считаем по Фоме по формуле n раз на 2 П 1 кворум очень важен и для консистентность данных потому что мы должны убедиться что у нас на всех нода нашего кластера одно и тоже и для предотвращения такой ситуации СТ взаи ко ляется на два то есть возникает из-за какого-то сбоя в сообщениях между узлами два как бы два кластера которые каждый из которых Думает что он тот самый единственный кластер двоевластия так собственно Мы приступили к созданию кластера что нам нужно для кластера Нам нужно три узла или более Ну Один узел Как вы понимаете это ни разу не кла во при этом в кластере всегда должно быть нечётное количество узлов такое правило чаще всего берут п или се узлов Ну се - это максимум который вообще рекомендован к использованию чаще всего для большинства средне нагруженных систем достаточно трх узлов для высокой нагрузки подключают пять Мы в своих нагрузочных тестирования проводили их с тремя узлами нам этого было вполне достаточно то есть 5 и7 - это больше теория которую я подчеркнул из Гайдов также Для обеспечения High avab есть так называемые узлы лё неры это узел который не участвует в голосовании он не участвует ни в каких кворум операциях кворум операции у нас это и чтение и записи и удаления но он будет синхронизироваться постоянно с кластером и в случае если у нас Один узел выпадает по какой-то причине мы можем быстро этого Лёне воткнуть на место таким образом обеспечивается High avab также по дата центрам если говорить про геон если у нас два дата-центра то в случае если дата-центр выбывает вот на вчера был доклад там спикер использовал очень хорошую аналогию там экскаватор с дедушкой альфонса или Альфреда который копает что-то и переруб все кабели идущие к дата-центра вот если такая ситуация происходит Мы автоматически теряем кворум в случае двух дата-центров тогда как нам быть нужна так называемая кворум площадка в каком третьем дата-центре мы просто организуем виртуальную машинку можно даже на на своей инфраструктуре там вплоть до ноутбука системного администратора Ну чаще всего просто в серверных заказчика поднимаем ещё одну ноду и таким образом обеспечиваем кворум даже в случае падения дата-центра так Ну допустим мы подняли наш кластер мы его настроили Казалось бы уже пора переходить к написанию кода взаимодействую с самим etcd что тут можно посоветовать Ну по записи рекомендуется избегать использования больших значений в value вообще ограничения Как такового нет но по умолчанию стоит 1 Мб но ничего большого лучше в не хранить если есть нужда хранить какие-то большие документы лучше использовать там Мон или какую-нибудь другую документо ориентированную базу или хранилищ всё-таки у нас для относительно небольших значений предназначен опять же метаданные которые хранятся в etcd лучше изменять редко то есть для часто изменяемых данных рекомендуется использовать тоже что-то ещё также в etcd есть очень удобный инструмент аренда это ключи Time to нам это очень сильно помогает Об этом я расскажу позднее но создавать ино может привести если создать их слишком много Это тоже может нас привести к проблемам с производительностью поэтому аренды рекомендуют разработчикам переиспользовать так это мы поговорили про особенности записи в etcd чтение в etcd можно считывать много ключей с одним запросом если передать массив ключи или так называемый стартовый ключ то есть мы можем оптимизировать наш запросы к at CD получая множество значений сразу А что на высоконагруженных системах тоже очень важно а в apcd есть такая директива Watch она нам позволяет отслеживать ключ этим тоже рекомендуется пользоваться это работает оптимальнее чем а ситуация когда мы постоянно ДГА запрос и проверяем ключ также тут надо заметить что в atd поддерживается кворум и не кворум чтение по умолчанию чтение у нас кворум то есть мы читаем какие-то данные запрашиваем опрашиваем все узлы опросили все узлы они нам подтвердили что да данные именно такие у нас есть консистентность данных и мы получаем данные но это не всегда быстро а если мы хотим консистентность пренебречь в пользу скорости тогда мы можем передать специальный флаг и прочитать с данные с определённого узла Да Возможно они будут устаревшими Возможно не консистентные Но это будет быстро по умолчанию чтение все кворум для специальный флаг так собственно мы начали параллельно с разработкой смотреть думать а как быть с отказоустойчивость Что будет в случае отказа к чему нам готовиться Чего нам ждать так если у нас чтение из невозможно такая ситуация теоретическая мы е рассмотрели но то если убить кластер совсем То есть если у нас погибают все наши дата-центры падают там все ноды тогда Да чтение невозможно так пока хоть одна нода жива у нас хоть какое-то чтение хоть не консистентная ли не реализуемая но будет а так А если вот запись невозможна ситуация эта ситуация случается чаще Ну во-первых это будет если мы при высли лимит хранилища То есть просто у нас кончилось место которое ограничено напомню 8 гигабайтами тогда кластер переходит в так называемый режим обслуживания это почти как Но с возможность удаления соответствующие чтобы не допустить такую ситуацию дальше если мы её всё-таки допустили то Чистим место запускаем так называемую дефрагментацию когда эти сиди преобразует ужи мает свои данные и всё у нас восстановлен кластер если был потерян кворум да Такое тоже возможно большинство узлов лежит кворума нет деградирует переходит в режим токо чтения Ну какое тут у нас решение пере создам ВС из снапшота спш поддерживает их надо регулярно делать опять же задача для наших псов это настроить снапшоты настроить метрики настроить автоматическое развёртывание дело в шляпе намдо хранить конфигурации в что для этого нужно делать вообще название есть такая версия происходит от юнисов ской директории Где хранится конфигурации различных сервисов и приложений это у нас плоская система иде подходит для конгу настрое То есть у нас ключ конфигурация значение то что мы храним да система плоская но мы можем использовать префиксы как многие привыкли делать в редисе мы можем использовать диапазоны ключей чтобы обеспечить иерархичность Выделите определённые диапазоны под конкретное значение так вот дальше это была простая задача а вот дальше уже сложнее хранение состояния приложения У нас есть множество наших гипервизоров у нас есть вещи которые относятся гипервизора сети диски что там ещё всяческое железо на котором всё это крутится нам надо отслеживать состояние всего этого сохра приводить в соответствие с контрольным планом Сейчас расскажу об этом поподробнее во-первых состояние которое хранится у нас в любой базе в Где бы то ни было ещё всегда будет отставать хоть на миллисекунду от того что на Земле потому что сам сервис нет адекватно себя постони отслеживать нужен сторонний сервис даже я бы сказал два сервиса один сервис просто отслеживает и наблюдает далее он СВ это сервис наблюдатель или сенсор далее он связывается со вторым сервисом контролёром который уже имеет знание о том в каком состоянии доди тот или иной элемент системы скажем у нас есть виртуальная машина она должна быть включена сенсор проверяет и видит что она по какой-то причине там находится в состоянии выключено или ушла в ребут он сообщает об этом контроллеру контроллер в свою очередь анализирует это и в соответствии со своей бизнес логикой уже пытается привести этот элемент систе в соответствии с тем состоянием которое записан непосредственно в базе данных то что я сейчас описал называется таким термином Rec по-русски это будет что-то вроде петля согласования или петля петля согласования наверно правильнее всего то есть прит к состоянию которое у нас требуется К источнику истины собственно тут мы да храним храним все данные о сервисах vcd используем Watch для отслеживания ключей используем ttl для отслеживания используем для создания ключей с онм сроком жизни так называе аренды такой термин так Собственно как Discovery Ну собственно все те же инструменты и используется и для Service Discovery то есть мы сохраняем вни Так давайте поговорим про проблемы и подводные камни с которыми мы столкнулись так производительность всегда важно смотреть на производительность когда разрабатывает настолько сложную высоконагруженные Ну в частности у есть это скорость и так по сети во-первых надо следить чтобы не превышать лимит размера данных по умолчанию этот лимит полтора мегабайта его можно увеличить конечно но производительность скорее всего упадёт не рекомендуется это делать нам полутора мегабайт вполне хватало и мы не стали экспериментировать тут по дискам Ну рекомендуется использовать отдельный выделенный диск использовать SSD или nme ту тут тоже всё понятно скорость записи решает также важны настройки файловой системы скажем ограничения журналирование может помочь так вот геон то что для нас было важно и оно вплотную Вт скорость сообщения потому что работает по принципу кворума то есть нам жизненно важен Пинг между узлами если Пинг будет слишком высокий по скорости мы упадём это нас в общем-то ограничивает и в геора спредер где-нибудь в Австралии другой где чисто теоретики в то работать Это явно не будет как бы скорость света к сожалению не обманешь физика самая бездушная бессердечная наука Ну если рассматривать дата-центры расположенные скажем на территории России то вполне можно найти компромисс между пингом и между скоростью и расстоянием что ещё ещё мы столкнулись с такой проблемой она была во втором etcd который мы изначально использовали потому что на него там уже была было лицензирование в нём использовался рипт для аутентификации B и ему требовалось 100 миллисекунд для сравнения Шей паролей за этого э аутентификация очень тормозила Наско Я знаю третьей версии это уже точно исправили просто имейте в виду если кому-то понадобится Legacy версия hcd Так масштабирование что можно сказать по масштабированию кластер у нас ограничен по горизонтальному масштабированию потому что опять же нам требуется консенсус как для записи и для чтения если мы вотк там условно 100 узлов Они не смогут за вменяемое время друг с другом договориться никогда поэтому максимум узлов которые рекомендуют гайды официальные документации hcd - это сем узлов возможно можно сделать и больше но мне Сложно представить Под какую задачу это нужно нам вполне хватало и трёх насколько как я уже сказал все наши всё наше нагрузочное тестирование все наши нагрузочные тесты вполне проходили при узлах для особо высоконагруженных систем рекомендуют использовать пять узлов но по факту можно сказать что в etcd нет шардинг то есть если у вас перед вами встанет задача что вам этих семи узлов недостаточно вам придётся создавать ещё один кластер и делать какую-то свою систему взаимоотношения между двумя кластерами которая будет асинхронно работать у меня были такие натки по архитектуре Но это просто не пригодилось по по тестам нам вполне хватило трёх узлов пять узлов было уже за глаза геора под геосм кластером я рассматриваю кластер где у нас задержка больше 100 миллисекунд Потому что если задержка там будет очень большая то как бы никакие настройки нас всё равно не спасут всё будет работать медленно но потютьков это интервал между между Хард битами между обменом ртами в узлах мы его подобрали для работы в геосм кластере в нескольких дата-центра 200 миллисекунд по умолчанию там стоит 100 с нашими настройка время через которое начинается новый выбор лидера в кластере atd то есть ноды начинают выбирать нового мастера тут хорошей практикой считается ставить от в 10 до 50 раз больше чем хабит мы поставили 5000 миллисекунд так ёмкость хранилища я несколько раз об этом говорил вас наверное это смутило почему такие вопросы про ёмкость хранилища это по сути K value для Service Discovery для хранения настроек что он там собирается хранить одним из требований нашей системы было хранение журнала событий всех событий с гипервизора за 3 года то есть в etcd Конечно такой объём информации вы хранить не сможете он не для этого предназначено это и не нужно как как решили мы мы просто подключили мо db Ну можно использовать по сути любую базу данных все горячие данные Мы хранили в для хранения холодных данных для хранения архива применяли ещё одну базу ограничения на размер Илью пара Ну как я уже говорил нам это не понадобилось Но если кому-то понадобится есть специальная опция которой это можно увеличить поб происходит если у вки кончилось место если метрики не помогли мы вовремя это не отследили тогда по кластеру рассылает так называемое тревожное оповещение То есть все ноды оповещаются что переходит в режим обслуживания в этом режиме разрешено чтение и удаление Ну далее Это уже работа для девопса почистить ме снять Рени обслуживание дефрагментировать Так ну и про безопасность надо сказать это тоже важно и если вы хотите лицензировать да и просто очень важно Для любой продакшн системы поддерживает настройку пользователей поддерживает аутентификацию и есть даже шифрование если хранить ключи и пароли необходимый миним вроде бы больше и не надо все все проблемы безопасности с которыми мы столкнулись были связаны отнюдь не с Ну Таким образом мы с помощью etcd разработали систему управления гипервизора которая была и доступная отказоустойчивый антихром на разработке своей сие достаточно быстро смогли разобраться с а с работа То есть у системы очень низкий порог вхождения Ну и без проблем писали и на пайтоне и на потому что оба эти языка имеют поддерживаемые библиотеки Спасибо коллеги за внимание большое Надеюсь было интересно Надеюсь я не очень вас утомил буду рад отзывам спасибо Ну что ж обязательно делай записи по поводу вопросов потому что да самое сложное для спикера не готовиться несколько месяцев а выбрать лучший вопрос поборемся первую супрематист скажите пожалуйста вот не рассказали про кэш использование или использовали вы допустим Такой сценарий если так вышло что цода действительно очень далеко а надо работать быстро может быть имеет даже смысл какую-то надстройку поставить чтобы кэшировать данные для быстрого обмена Такой сценарий рассматривали вот Хороший вопрос Спасибо Это было как раз в той архитектуре которую я начал разрабатывать Я рассчитывал Да сделать настройку которая будет помогать сообщению между двумя разными кластерами и асинхронно переносить там кэш из одного в другой вот такую штуку я начал писать но мы провели тесты и по тем дата-центра которые у нас были в которых предполагалась работа системы мы смогли настроить всё-таки at CD для работы в одном кластере но да план такой был микрофон если какой-нибудь редис поставить над чтобы он широва всё в себя вообще всё и через него работать так обдумывали Такой сценарий Ну ну тоже да но там проблема ещё в том что кэш получается должен быть персистирование было Довольно простой удобной низкоуровневое системой Но после того как кубы вот стали популярные в etcd начали добавлять очень-очень-очень много всяких свистел переделок и она вот увеличивалась увеличивалась по твоему мнению как человек который вот непосредственно etcd А много использует насколько она сейчас блот раздутая или это всё тако сказки Мне кажется она до сих пор достаточно простая там есть всё что нужно практически всё что я рассказал и свистел перделки они практически не влияют на производительность их можно просто не использовать то есть в результате это просто система удобная распределённая очень быстро не забывай записывать вопросы Следующий вопрос начальная сть Даня Новиков МТС Судя из доклада вы как будто разработали частичку кубернетес сами вот там у вас и Act des есть и процесс реконсиляция не думали изначально просто взять кубернетес и написать к нему свой оператор нет задача была именно разработать свой велосипед я ждал что кто-то скажет даже не про кубернетес про Open Ой простите Да да это похоже на кубернетес да это похоже на ОК но у нас была задача разработать свой велосипед у а вот буквально на секундочку для наших слушателей в онлайне А вот можешь чуть-чуть это раскрыть Вот почему была именно такая задача Вот чем примерно руководствовались что и в кубы задача была я несколько раз упоминал в стек это нужно было Для для государственного это нужно было Для военного поэтому Естественно они не хотели инвестировать кубы конечно сертифицировать это такое себе прошу Здравствуйте спасибо за докл проводили вы нагрузочные стрельбы перед тем как начать использовать в условный зупер вы сказали то что он медленный там старый Вот это всё но мы просто у себя используем Я поэтому вот хочу спросить сравнивали ли вы как-то есть какие-то чиселки чтобы посмотреть Да сравнение проводилось и по-моему бенчмарки есть даже в открытом доступе Мы тоже у себя смотре и по технологическому стеку вс-таки быстрее В общем быстрее я не хочу уходить в но это так наверно да однозначно однозначно быстрее и по нашим тестам тоже отдельная тема поговорить в ларах следующий вопрос из центральной части честно говоря там грустная история проект закончился до срока по независящим от нас причинам урезали финансирование но сертификацию мы можно сказать что прошли Да это очень медленно но сертификат там в конце бы полу можно вре бы Като наработка с точки зрения отказов валов кластеров именно на продуктивной инфраструктуре там частота обращения там частота не знаю восстановление сшов и так далее или это не продуктивное решение какое-то Ну как я говорю до прода К сожалению проект не доехал такой статистики Нет я рассчитываю все эти знания потом возможно где-то переиспользовать физиологи потому что у меня как аутстаф есть корыстная цель обучать разработчиков лучше чтобы они писали для наших клиентов код ещё лучше и вот в науке есть огромная проблема что до пода до собственно говоря статей доходят только эксперименты в которых гипотеза подтвердилась А все эксперименты в которых гипотеза не подтвердилась их не публикуют потому что про топовые журналы такой не берут Ну кому интересно читать о том что вот выдвинули интересную гипотезу провели кучу экспериментов на крысах и нет не подтвердилось это даёт огромный баес и Мне бесконечно нравится что на наших конференциях мы обсуждаем интересные с технической точки зрения не только те которые вот дошли до прода и показали свою работоспособность в проектах с но и какие-то другие штуки которые потом можно обсудить В кулуарах Так давайте завершающий вопрос этой сессии и отпустим себя на кофе потому что всё-таки после вчерашнего Нужно чуть больше кофе в организм прошу в левой части зала Да Пётр Привет Спасибо за доклад вот у меня вопрос про хранение данных itc как я понимаю Там места мало вот думали ли вы о том как управлять как сказать ну теми данными которые нужно хранить именно в эти си во-первых мы использовали активно Лизы аренды это ключи с ttl мы их переиспользовать то есть старые ключи удалялись новые на их месте создавались во-вторых мы отслеживали метрики и с помощью метрик контролировали объём хранилища В общем как таковой проблемы именно с объёмом данных у нас не возникало для чисто для Service Discovery для хранения конфигурации для хранения состояни 8 ГБ вам хватит за глаза вот вопрос журналированием подключения отдельной ба аплодисменты спике Я вижу ты сделал Некоторое количество записей несите наши призы в студию Выбери пожалуйста того кто получит супрематический вопрос про редис мне понравился кто задал вопрос про редис Поднимайся на сцену прошу матрёшка и нашему спикеру подарок за то что победил в конкурсе докладов подготовился выступил и сейчас пойдёт в дискуссионную зону чтобы уже в кулуарах Обсудить с вами те подробности которые нельзя на камеру Спасибо тебе и до встречи Спасибо"
}