{
  "video_id": "TLFbmZnaJvA",
  "channel": "HighLoadChannel",
  "title": "История одного инцидента: как парализовать работу 20к сотрудников / Эдуард Александров",
  "views": 666,
  "duration": 2840,
  "published": "2024-10-29T02:38:24-07:00",
  "text": "я приглашаю нашего следующего докладчика Эдуарда Александрова и он из Яндекс арак и расскажет о том как в Яндексе сотрудники играют в тетрис встречаем Привет Итак Меня зовут Эдуард и в Яндекс Я занимаюсь сервисом который называется треке скажу пару слов о нём собственно изначально для трейдинга задач в Яндексе использовали ро но в какой-то момент жира перестало справляться с нагрузкой и попытки решить проблемы с масштабированием ни к чему не привели даже привозили людей из алая но безуспешно и приняли решение разрабатывать свой собственный трекер задач и примерно 10 лет назад появился Яндекс трекер его начали использовать для разработчиков для трекинга задач Но со временем мы добавляем Фуна наш сервис и сейчас он превратился в полноценную систему управления проектами и процессами и практически все внутренние процессы Яндекса построены на трекере Ну например чтобы поехать не в командировку или чтобы благоустроить офис Ну естественно все девелоперские задачи всё работает внутри Яндекс трекера мы поддерживаем несколько инсталляций самое крупно из них - Это внутренняя инсталляция и е наручные характеристики приведены на слайде у на примерно уникальных пользователе поддерживаем базу данных которая примерно ну чуть уже больше чем здесь 3 ТБ и соответственно для поисковых сценариев мы поддерживаем специальный поисковый индекс на основе elastic Search сейчас мы предоставляем Яндек в виде B2B решения и все желающие Пожалуйста приходите к нам ссылочка приведена на слайде скажу Пару слов про архитектуру сервиса архитектура Можно сказать она немножечко leg мы постепенно от этого избавляемся она достаточно типична для распределённых нагруженных систем мы поддерживаем несколько версий AP одна из версий I мы называем её Front для нашего веб фронтенда и одна из версий IP то что мы называем V2 IP - это IP для роботов и интеграций именно благодаря наличию v2p Яндек трекер столь тесно интегрирован со всеми процессами которые у нас есть в Яндексе Кроме того У нас есть worker Pool асинхронный планировщик и многие запросы которые прилетают к нам по IP приводят к что мы создаём задачу её планируем на Work Pool и она там соответственно асинхронно исполняется в качестве источника данных и нашего источника истины мы используем шардирование дашборды строить различные отчёты и для того чтобы такие сценарии эффективно реализовывать мы поддерживаем поисковый индекс на основе сначала elastic Search исторически Мы начинали фактически с первой версии эластика сейчас у нас в продакшене эксплуатируется шестая версия эластика и примерно полгода назад в результате того инцидента про который я сегодня расскажу мы таки перешли на Open Search Ну или точнее переход на Open Search в том числе поучаствовал вот в этом инциденте и сейчас Мы практически полностью перешли на Open Search Мы в параллель эксплуатирует кластера эластика Мы собираемся в ближайшее время отказаться буквально пару слов скажу про особенности нашей инсталляции наши кластера содержат порядка 100 машин соответственно на этих машинах развёрнуты индексы на каждом индексе на каждом кластере находится по одному индексу а индекс шардирование узле находится один шард или копия шарда ну и соответственно эластик следит за тем чтобы при поиске запросы отправлялись на правильные шарды и там правильно собирались результаты одной из важнейший особенности нашего индекса является то что документы которые хранятся и которые мы затем ищем они достаточно широкие схема документа в ластике называется пингом ну и у нас Пинг достаточно большой онде более 20.000 полей почему так произошло потому что тикеты которые содержатся в трекере они содержат как системные поля Ну это стандартные поля типа создан тикет кто исполнителю тикета названию тикета и так далее и так далее и Кроме того пользователи могут создавать свои собственные поля под свои конкретные задачи поскольку инсталляция большая пользователей у нас много с различными сценариями соответственно количество полей которые мы поддерживаем тоже достаточно велико Ну и в результате это отображается на такой гигантский Пинг это приводит к тому что поисковые сценарии достаточно тяжёлые И несмотря на то что может быть у нас не такой Великий rps поисковый на sech ВС равно за счёт вот этого размера пинга поиск достаточно тяжёлый моя основная задача в трекере - это обеспечение его стабильности соответственно Яндекс уделяет очень болье внимание стабильности своих сервисов у на существует такая активность которая называется процесс 4 девятки которая описывает различные требования к сервисам критерии которым должны удовлетворять сервисы процессы которым должны удовлетворять сервисы для того чтобы обеспечить их высокую доступность всего существует четыре так называемых тира стабильности или тира доступности тир наивысшей доступности это тира это тир который должен обеспечивать пресловутые и к этому тиру относятся критичные публичные сервисы Яндекса например поиск внутренние сервисы Яндекса и в том числе Яндекс трекер относятся к тиру б соответственно тир б должен обеспечивать 99 и 95 для сервиса которые находятся в тире B должны быть настроены мониторинги скажем что мы мониторим должны быть организованы дежурства должны быть формализованы формализованная работа с инцидентами тоже покажу что это означает Ну и самое главное требование что все сервисы которые находится в тире б естественно в тире а тоже должны переживать падение одного ДЦ пользователи при этом не должны видеть никаких спецэффектов и такой переход для них должен быть максимально плавным соответственно мы поддерживаем падение одного ДЦ за счёт того что выполняем резервирование и наш код ожидает что в любой момент поды могут оказаться поды с эндом поды базы данных могут оказаться недоступными и соответственно мы это отрабатываем именно поэтому все наши базы данных как мо так и elastic sech растянуты на ри DC в каждой ДЦ есть копия шардов Ну И при пропадании одной из ДЦ наши сервисы продолжают работать кста пожива такую инсталляцию растянутую на ДЦ для эластика Хотя по большому счёту это естественно не рекомендуемое его развёртывание вместо этого рекомендуют в каждом ДЦ иметь по копии кластера ну и соответственно вручную либо кодом либо внутренними механизмами реплицировать индекс но мы подумали что достижение консистентность в этом случае будет намного более сложным потому что поисковый сценарий часто который пользователи от нас ожидают выглядит следующим обм что я завёл тикет и сразу же я этот тикет вижу в фильтре или сразу же я могу его найти в строке поиска если мы будем поддерживать разные кластера в разных ДЦ вот этому требованию нам будет достаточно тяжело удовлетворить Ну и самое главное с точки зрения оперирования кластерам и всем сервисам мы должны постоянно подтверждать что мы действительно выдерживаем работу при падении ДЦ проводя регулярное учение У нас есть специальный инструмен закрыть Ну и соотвественно посмотреть что при этом происходит К сожалению примерно год назад по организационным причинам эти учения перестали проводиться в частности это и привело к тому инциденту про который я буду рассказывать но сразу после него мы эту практику возобновили по поводу того что мы мониторим мы мониторим как стандартные практи мы мониторим различные параметры оборудования Ну часто инциденты и часто то что мы начинаем складываться под нагрузкой видно по пу Кроме того мы мониторим различные специфичные параметры баз данных в частности для elastic sech его здоровье очень чётко видно По состоянию поисковых тред пулов и по состоянию поисковой очереди для мы многие инциденты детектива по графику или по мониторингу открытых курсоров теперь по поводу работы с инцидентами вся работа с инцидентом э делится на две части это работа во время инцидента и соответственно всё то что происходит после того как сервис установлен и а инцидент завершён во время инцидента автоматика по настроенным мониторингом и по настроенным алертом заводит автоматический тикет в трекере в который прикладывает графики тех на основе которых сработали мониторинги и в котором в этом тике затем ведётся работа с инцидентом Ну понятно что если падает трекер он недоступен то соответственно автоматика умеет придерживать создание такого тикета и тикет будет создан позже когда трекер станет доступен снова Кроме того у нас имеется внутренний сервис он называется Ира этот сервис по сути большой Дэш борд о состоянии всех внутренних сервисов Яндекса и пользователь если у них что-то не работает может прийти в этот дашборд увидеть что действительно сейчас в сервисе происходит инцидент он недоступен Ну и открывается специальный чат который называется протокол на самом деле протокол - это не только чат но и вот всё то что показано на слайде то есть тикет иф и так далее И в этот чат автоматика призывает дежурного S Яндекса призывает дежурного сервиса и в этот чат могут приходить пользователи приносить какие-то дета если они есть соответственно после того как сервис восстанавливается мы начинаем формально разбирать инцидент и заполнять тикет протокола в этот тикет мы пишем во-первых Тайлан то есть что когда произошло что пошло хорошо что пошло не так мы пытаемся расследовать руд коз и очень много усилий уделяем тому чтобы действительно докопаться до сути проблемы мы вычисляем метрики инцидента сколько пользователей было задето сколько смежных сервисов было задето Сколько времени лежали вычисляем различные интегральные метрики на основании которых определяется насколько серьёзным был инцидент и затем мы рассказываем об этом инциденте на специальной встрече если инцидент оказался достаточно серьёзным то затем следует ещё дополнительные встречи по разбору в которые вовлекается всё большее и большее количество специалистов для того чтобы понять Можно ли было предотвратить этот инцидент и что нужно сделать в будущем для того чтобы подобные инциденты не повторялись ну и вопрос причём же здесь анонсированные Тетрис Тетрис - это такая пасхалка которую добавили в сервис ребята из команды фронтенда и когда энд в течение некоторого времени не отвечает фронтенду соответственно frontend сначала показывает страницу с ошибкой Ну типа там 500 Internal Server Error и так далее и через несколько секунд эта страница пропадает и открывается Тетрис в этот Тетрис Вполне себе можно поиграть а мы в этот момент видим то что мы называем красной стеной вот такой вот дашборд это дашборд как раз того инцидента и естественно нам надо с этим дашбордов люди которые давно работают у нас в сервисе даже по состоянию дашборда могут примерно сказать куда поку мы знаем структуру нашей нагрузки Ну например мы знаем что количество поисковых запросов у нас составляет примерно 25% ну и соответственно если наш сервис пятисот на 25% то есть вот эта красная стена на 25% соответственно надо идти в эластик и смотреть что он скорее всего полк Итак давайте Расскажу уже непосредственно про анонсированные инцидент ему предшествовал процесс перехода на уже несколько лет эксплуатировали кластер elastic sech шестой версии на железе и у нас было с этим кластером несколько проблем одна из проблем заключалась в том что мы долгое время не могли перенос То есть у нас индекс был создан там изначально при начале эксплуатации сервиса и за время его эксплуатации размеры шарда очень сильно разрослись собственно sech рекомендует разме шарда в 50 Гб в результате мы в процессе эксплуатации получили размер шарда больше 300 Гб это с одной стороны замедлялось с другой стороны очень сильно усложнилась администрирования в частности эластик любит если по умолчанию ничего не настраивать Как управлять шарда соответственно он любит при пропадании части узлов начинать релоцироваться процес релокации занимает больше 3 часов соответственно очень часто получается история что узлы пропадают шарды начинают переезжать потом узлы возвращаются Но они всё ещё переезжают индекс там в непонятном состоянии и чтобы решить эту проблему мы отключили автоматическую релокации и по сути администрирование производили руками то есть мы следили за состоянием рдо бы настроены на это иу востановление узлов шарды правильно аллоцировать эластик на машинах различного класса а известно что эластик работает со скоростью самой медленной машины придумали такую схему кастомной локации шардов что на машинах послабее мы запускаем одну инсталляцию соответственно на эту инсталляцию лоцируется о на более мощное машины Ну поскольку не имеет смысла её гонять в холостую Да всё равно более медленная Машина будет определять производительность кластера мы соответственно запустили две инстанции или два экземпляра эластика и тоже на них пороли по одному рду в принципе это тоже была не самая лучшая идея машины не боролись за ресурсы процессора мы настроили affinity но боролись за ресурсы диска поскольку не на всех машинах можно было разнести различные инстанции на различные диски Ну естественно когда мы эксплуатирует кластер у нас в железном кластере было Порядка 70 машин Естественно С этими машинами постоянно что-то случалось Ну из-за того что мы использовали вот такие странные кастомные схемы с локацией селф хилинг эластика фактически не очень работал Нам тоже приходилось постоянно за этим следить Раз уж Мы решили отказаться от elastic sech 6 и переехать кстати не сказал вернусь собственно основной идеей такого переезда было переехать из железных машин в manage db Man db - это внутренний сервис Яндекса который предоставляет Database сервис в том числе он предоставляет мон и собственно наша инсталляция Мон находится в mdb и они предоставляют elastic Search Open Search Но сейчас они планируют отказаться от elastic Search на Open Search в пользу Open Search вот по причинам указанным на слайде сейчас Невозможно нормально лицензировать elastic Search и плюс ко всему El sech очень дорогой соответственно с Open sech всё намного проще и мы естественно хотели поучаствовать в этом просе и пере что 6 это достаточно старая система Open является РМ от elastic sech 7 но за время развития и elastic Search сейчас там актуальная версия 8 что-то 12 или 814 что-то такое и соответственно Open у него актуальная версия по-моему 2.8 и она ближе к версии послед верси 8 развити сервиса Там возникли несовместимости в AP то есть AP elastic 6 То которое поддерживали мы было несовместимо с новыми версиями AP которые поддерживается в Open Search Это потребовало от нас миграции в коде Ну и естественно мы не можем взять индекс с эластика и положить его в Open Search нам этот индекс по сути надо перенос то есть мы берём данные из монго обогащаем их нормализ у нас в монго Несмотря на то что это документ на СУБД мы всё равно поддерживаем там внутренние какую-то внутреннюю нормализацию для удобства работы соответственно мы всё это дело де нормализуя обогащаем и льём в Open Search Проблема была в том что изначальная версия кода которая это делала она это делала примерно за полтора месяца Ну понятно что о практической применимости такого кода было Ну не не очень хорошо соответственно Мы оптимизировали наш скрипт и в конечном итоге мы налили Open Search всеми нашими данными за 5 дней Итак 2 недели после того как мы это сделали Мы тестировали Open Search и не были готовы вывести его в продакшн по нескольким причинам во-первых мы не до конца были уверены в том что он держит нагрузку У нас есть код который во-первых позволяет параллельно писать в два кластера это понятно с другой стороны мы можем реплицировать часть поисковой нагрузки на вторичный кластер как раз в роли вторичного кластера выступал Open Search и смотреть его performance характеристики к сожалению не всю нагрузку с прода можно нормально реплицировать На вот такой вторичный кластер без дополнительных усилий потому что эластик поддерживает как ST запросы так и stateful запросы соответственно для запросов репликация тривиально А вот для запросов Это не так-то просто сделать поэтому мы реплицировать только Stat у нас таких запросов большинство мы почти были уверены что Open sech выдерживает но как бы конечной уверенности у нас не было и плюс ко всему Мы ещё не исправили все несовместимости с I у нас некоторые запросы которые нормально отрабатывали на elastic Search 6 на Open Search не выполняли соответственно мы видели какой-то определённый процесс ошибок и работали над этим Но к моменту развития инцидента соответственно Мы ещё до конца готовы не были Итак всё началось с того что мы получили уведомление от нашей службы ДЦ что они собираются проводить регламентные работы с нашими машинами которых развёрнуты эластик соответственно машины в этот момент будут выключены и ноды будут недоступны и нам надо было убедиться что на оставшихся узлах мы выдержим нагрузку мы проводим учение отключаем этот DC и видим что нет нагрузку мы к сожалению не выдерживаем соответственно нам надо что-то срочно делать переконлива эластик добавить машины в тех ДЦ которые остаются для того чтобы мы нормально пережили регламентные работы мы это сделали К сожалению не быстро потому что ну надо найти машины надо их налить Надо нарова на них шарды это всё занимает время понятно что у нас есть скрипты для наливки машин и так далее и так далее но всё равно там пару дней это требует соответственно наливаем Мы наш эластик поднимаем запускаем это всё получается готовым к вечеру среды к вечеру заканчивается нагрузка спадает и протестировать лаер под Мы не успеваем Ну и закономерно на следующий день в начале прайм тайма мы имеем вот такую прекрасную красную стену наш кластер ласика просто напросто складывается Окей деваться некуда быстро починить его мы не можем нам нужны ещё машины нужно их наливать это всё требует времени соответственно мы принимаем решение что пора пора переходить в проде на Open sech переходим отлично ВС поднялось но видим небольшой пронт обо изза того что запросы которые мы не успели починить они фейля Ну ладно мы договариваемся с пользователями пожалуйста Потерпите немного временим и мы вам всё починим далее простояли до конца Пятница Пятница как-то нас миновало возможно нагрузки было меньше возможно что-то произошло что наш кластер продолжил работать Мы уже практически выдохнули и подумали что хорошо сейчас быстро уже сделаем фикс для не выполняющихся запросов всё будет хорошо В выходные проблем нет но в понедельник начинается что-то непонятное Мы видим что наши поды бэнда начинают Странно себя вести соответственно в нашем бэнде есть endpoint чека который ходит в базы смотрит что база доступна этот endpoint выполняется в штп пуле которое обрабатывает пользовательские запросы и этот endp дёргает система dey dey - это внутреннее облако Яндекса в котором развёрнуты многие сервисы в том числе в деплой развернут Яндек трекер соответственно если эта ручка таймаут Я кстати иногда буду говорить ручка в Яндексе энпо называют ручками и очень быстро к этому привыкаешь и начинаешь также говорить соответственно вот этот endp или эта ручка таймаут и деплой показывает вот такую картинку что этот под становится недоступен На самом делей показы такую же картинку если под просто падает например по Out of memory и соответственно автоматически потом переподслушано на этот под смотрит его состояние падал он или не падал смотрит состояние сервиса внутри пода и снимает ТМ по тред дам мы определяем Есть ли какие-то проблемы в нашем штп пуле или нет начинаем анализировать этот ТМ действительно там проблемы есть и мы видим что фактически все потоки АТП Пула которые обрабатывает пользовательские запросы блокируются на фьюче которая обрабатывает запросы Open у драйвера орча такая архитектура что он использует внутри Невский пул потоков для того чтобы обрабатывать соке отно соединения А ожидает обработку соке ного соединения он фьюче которое собственно выполняется в пуле запроса это для синхронного IP Ну для синхронного IP там архитектура немножко другая но самая печаль заключается в том что это фьюче ждёт без таймаута и если вдруг возникает проблема где-то на стороне онсер CH он по какой-то причине не отвечает или например как-то странно дропается tcp соединение И это не детек уровнем сокета то тогда собственно э фьюча зависает или вот этот вот Лог в этой фьюче зависает навечно именно эту проблему мы и наблюдаем починить мы её всё ещё не успеваем и в результате на следующий день наши пользователи наблюдают вот такую прекрасную картину практически целый день Они получают возможность погра в м раз что же мы можем сделать наша первая задача вернуть сервис к жизни Ну и починить все Те проблемы которые сервисом могли бы возникнуть для этого мы собираем звонок в звонок приходят Все ребята из нашей команды из команды трекера и также мы зовём туда ребят из mdb чтобы они со своей стороны посмотрели насколько живой кластер орча Может быть там есть какие-то проблемы с виртуальными машинами Может быть там есть какие-то проблемы с физическими машинами на которые это всё крутится и так далее вырабатываем три направления работы во-первых нам надо срочно починить блокировки в http пуле чтобы даже в случае если Open sech там по каким-то причинам не отвечает наш http пол не блокировался и мы хотя бы другие сценарии продолжали отрабатывать далее мы просим ребят из mdb разобраться с очм и мы естественно хотим восстановить свой кластер эластика и в случае если не успеваем починить что-то с он серм соответственно переключиться на эластик возникает вопрос почему мы не починили эластик раньше К сожалению Когда в дата-центре массово выключаются и включаются машины есть такая статистика там до 30% машин по разным причинам может не включиться Ну там батарейки садятся ещё какие-то аппаратные проблемы с памятью и так далее и так далее и некоторые Машины наши на которых был развернут наш эластик не включились соответственно инженерам в ДЦ Пришлось идти до них ногами разбираться в чём там проблема и так далее и так далее и фактически только ко вторнику То есть к моменту разворачивания нашего инцидента эти машины у нас появились снова нам надо было их опять же наливать заново нам надо было на них лоцирование приходят какие-то люди и пишут ребятки Ну вы ВС равно Лежите залите нам пожалуйста таблицу рекордов а то как-то теряет смысл всё что тут происходит Да сначала нам удалось починить elas sech оказалось самое простое что было Из всего нашего списка соответственно Мы вернулись на него таким образом Нам не пришлось реализовывать таблицу рекордов и чемпионат мы прервали в течение недели мы чинили всё что только можно мы соответственно разобрались с проблемой с блокировками и починили её мы до конфигурировать наш кластер орча и через неделю в понедельник мы смогли убедиться что действительно все проблемы которые мы обнаружили мы исправили у нас упало питание в другом уже ДЦ соответственно мы польку починили всё что можно починить переключились на Open Search и с тех пор у нас уже получается полгода как эта инсталляция Open Search работает без каких-либо проблем какие мы из этого всего вынесли уроки Ну во-первых любая большая проблема не возникает Случайно она развивается постепенно причём к этой проблеме приводят как организационные какие-то моменты для нас основной вот этот организационный момент был отказ от учений соответственно мы не понимали Как работает наша система при падении ДЦ мы не были к этому готовы и различные технические проблемы соответственно мы слишком много всего сделали мы промивають но как бы в трудную минуту все средства хороши нам это просто пришлось сделать и несколько уроков собственно что мы сделали в коде и что мы сделали с точки зрения улучшения процессов с точки зрения с кода скажу пару моментов на которые обычно не обращаешь внимания Но которые во время инцидента оказываются очень важными мы по рекомендации наших друзей из соседних сервисов реализовали прогрессивную деградацию раньше мы е не поддерживали собственно в рамках прогрессивной деградации мы теперь можем очень тонко отключать либо пользователей либо роботов либо какую-то часть функцио сервиса Ну и если мы видим Что где-то развивается инцидент например падает поисковый сценарий падение поискового сценария чаще всего связано с тем что кто-то либо начинает либо начинает открывать какие-то очень сложные дашборды Ну либо применять какие-то очень тяжёлые фильтры соответственно этих пользователей мы можем теперь быстро идентифицировать и их отключить все остальные пользователи продолжат нормальную работу а мы аккуратно разберёмся с теми людьми которых мы отключили И как мы выяснили наше логирование оказалось никуда не годным потому что Понятно первая точка присутствия во время инцидента после того как мы увидели инцидент на графиках мы идём в логи смотреть что там происходит и в логах мы видим всё что угодно только не то что нужно то есть ошибки от различных подсистем от различных участков кода наша любимая TR lor Причём тут этот еррор как он помогает является ли этот еррор там связанным с инцидентом или это возможно просто какая-то ошибка валидация пользовательского запроса и так далее И теперь каждый раз когда мы что-то логи мы обращаем внимание на то Насколько этот Лог будет полезен во время расследования инцидента и если этот Лог не полезен да то есть он не помогает дежурному что-то там увидеть мы соответственно это теперь не пишем с скорее всего мы это напишем War А может быть даже рейсом Ну и естественно процессное улучшение что мы сделали мы написали новые инструкции для дежурных для того чтобы можно было более качественно и более быстро понимать по каким нашим графикам по каким мониторингом и артам куда идти куда смотреть что чинить вот это вот как бы процедура То есть если раньше у нас в инструкции было написано сходи посмотри теперь у нас там есть Прямо команда которую надо скопировать и вставить и соответственно по этой команде Можно либо сразу же что-то починить либо получить какую-то информацию далее Мы вернулись к регулярным учениям теперь Раз в месяц мы блокируем ДЦ и смотрим Как изменяется поведение нашего сервиса мы стали намного более стабильно и практически А во время всех учений мы Ну какие-то мелкие проблемы детектива но больших инцидентов во время учения у нас больше нет мы улучшили процесс расследования соответственно если раньше мы писали протоколы Ну чуть-чуть менее формально менее указывали тко Теперь мы обращаем на это внимание Ну и очень часто получалось так что те экшен айтемы которые мы вырабатывали во время инцидентов они тонули в блоге теперь у нас это наивысший приоритет если у нас есть эм по инциденту который не выполнен мы соответственно первым делом чиним Именно его так а что же произошло с нашим тетрисом Мы решили что Тетрис - это такая уникальная фишка трекера которую надо развивать и продвигать Мы в конце концов реализовали таблицу рекордов мы сделали поддержку джойстика и презентовали эту важнейшую бизнес фичу на конференции Яндекс Scale соответственно наш стенд оказался самым посещаемым стендом и произвёл фурор Так что теперь можно сказать что Тетрис - это такая положительная фича которая улучшает имидж нашего сервиса больше узнать о Яндекс трекере можно по вот этим ссылкам Приходите к нам в Telegram чат Там есть СОШ архитект с ними можно поговорить о внедрения и о возможностях нашего сервиса Спасибо за внимание голосуйте за мой доклад Спасибо Эдуард Давайте поднимайте руки у кого есть вопросы к Эдуарду так вот я вижу там справа в углу Добрый день Вячеслав СР подскажите пожалуйста у вас правильно понял нет периодов повышенной нагрузки на системе То есть вы к ним не готовитесь сезонных годовых сезонных нет то есть у нас получается что у нас есть очевидный мтайм это рабочее время там примерно с 10 до Ну 78 вечера но нагрузка примерно постоянно Да потому что это внутренний сервис он как бы эксплуатируется примерно в одном режиме Да есть ли у вас норма утилизации по железу И как вы поняли что вы не были готовы к переходу кам Метрика лизации В каком смысле Ну то есть сейчас я это попробую сформулировать то есть какого-то ограничения на железо что мы должны там условно работать на 100 машинах у нас нет мы следим сами за нагрузка и понимаем что если у нас нагрузка повышается за счёт того что растёт индекс Там и так далее и так далее мы можем заказывать машины через стандартный процесс Яндекса и эти машины соответственно получать и вводить в эксплуатацию уточню норма по минимальной утилизации чтобы у вас не было избыточная мощность чтобы машины воздух не грели есть такие нормы у вас в принципе нет отдельно нет так ещё вопросы вот ви молодой человек Спасибо за доклад Максим Меня зовут А вы можете как-то поделиться а своей практикой дежурства То есть я так понимаю вы уже ну на дежурстве уже там не первый год там может не второй не третий наверняка уже есть какие-то наработки в том плане в каких-нибудь самых банальных моментах там например со скольки до скольки должен дежурный там дежурить с полуночи или с 10:00 утра например или какие Вот например у вас подобные вот мероприятия помимо Вот таких вот стресс нагрузок как вот отключение ДЦ раз в месяц вот происходит может ещё что-нибудь Ну тут про какие-то практики именно дежурство наверное сложно сказать то есть у нас получается так что опять же прайм тайм у нас - это рабочее время да потому что ну понятно основные пользователи приходят туда расписание ещ и получается что ну дежурный начинает дежурить условно там в 8:00 утра и где-то до 8:00 вечера он постоянно на связи потому что Вот наши пользователи ожидают работу в этот момент с другой стороны если развивается инцидент Ну всё стандартно приходят эсэмэски там женщина начинает звонить и это может происходить в любой момент времени Ну дежурный должен на это как-то отреагировать есть политика эскалации собственно если не реагируют дежурный то там женщина начинает звонить другим людям которые эта эскалация настроена и до кого-нибудь он в конечном итоге дозвонится В общем как обычно Я просто какая-то Больная там была тема которая родила ещё какую-нибудь Фракцию Ну как-то в общем каких-то ноу-хау здесь у нас нет всё всё стандартно Да так вот и слева у нас вопрос да да Эдуард Добрый вечер А насколько глубоко у вас проработан план управляемой деградации Ну graceful degradation может быть есть какое-то готовое дерево принятия решений оцен ли Импакт Что произойдёт если вы отключите ту или иную фичу и какие риски например Вы теряете там не знаю персонализированные дашборды или что-нибудь такое Ну то есть Ели у вас готовы порядок действий и Готова ли дежурная смена исполнять эти шаги без команды но у нас получается порядок действий есть то есть мы определяем где у нас проблема и собственно смотрим графики логи и ки бану Да в ки бану идёт наш алог и мы можем по ней понять какие пользователи Какие эндпоинты дёргают С какими задержками там это всё происходит и собственно мы можем Вот это всё посмотреть и проблемного пользователя или проблемную часть функциональности можем отключить без каких-то дополнительных действий коман отдельный Импакт мы не оцениваем потому что ну не то что как бы не видим в этом смысла Но это потом может быть включено в расследовании инцидента просто как факт что мы вот отключили такую часть функционала и таким образом пировали проблему Так вот здесь посерединке ещё что равномерно этот самое я туда сюда вопро Э спасибо болье за интересно было посмотреть на ваш опыт решения проблемы Вот Сочувствую что так долго получилось всё это А у меня вопрос два вопроса Первый вопрос по теме а второй не очень Вот первый если вот вы говорите про эти учения Да когда вот вы вырубает ДЦ Да есть ли у вас такое такие учения как типа маки тест То есть когда случайным образом врубается там либо ДЦ либо там какой-нибудь сервер либо какой-нибудь отдельный кластер например да то есть такой вот да это классный вопрос сечас этого нет но мы про это думаем то есть мы прочитали красивую книжку про ха иннинг в какой-то момент и решили что нам оно нужно но с одной стороны кажется что хау инжинеринг - это больше история про микросервисы когда там вырубаются да случайным образом какие-то именно их части мы Ну можно сказать условный Монолит понятно что там это всё развёрнуто на множество подов за балансе и в общем думаем это вть пока не внедрили А по поводу отключений DC То есть это всё у нас происходит по регламенту учений Ну как бы когда матест то есть там необходимо убедиться в том то что вообще система работает нормально вся чтобы при нерабочем ДЦ ещё второй не положить Ну да И второй вопрос У меня вот касаемые монго то есть монго она прикольная штука вообще прикольная база данных То есть у вас там три центра и скорее всего вас есть Ай который следит за всеми этими ребятами Вот И там всё здорово переливается да может возникнуть такая ситуация когда допустим станут все secondary Да ну бывает такое вот и нельзя будет писать никуда вообще Угу Ну и также Вот хотелось бы узнать каким образом вы занимаетесь тем чтобы восстановить монго с нуля Потому что как я знаю И П И рер - это очень долгая история то есть особенно у монг это какая-то проблемная у неё штука Как вы решать эту проблему да это правда тут есть хитрость связанная с тем что мы её немножко не решаем потому что мы развёрнуты mdb и мы радостно делегированы им собственно Ну то есть получается мы как бы администрировании средствами монго Ну и они могут относительно быстро её восстановить и мы на самом деле сейчас давно не проводили учение по восстановлению нашей Мон из БПА тоже хотим к этому вернуться и заодно посмотреть как это вообще всё работает Спасибо большое спасибо Так давай правую сторону зала чтобы равномерно было Ну спасибо очень подробный технический рассказ о том как вы решали вот технически А интересно просто один день Вы полежать значит с у вас уже 99,7 То есть даже не три девятки и вот интересно с точки зрения бизнеса как бизнес проблему то есть там может промокоди пользователям раздавали или какая-то коммуникация была Да да спасибо Это отличный вопрос А ну бизнес нас ругал Но поскольку Это внутренняя инсталляция соответственно внешние пользователи никто не пострадал пострадали Только наши а внутренние пользователи Ну они просто нас не любили как бы всё на на этом на этом всё закончилось поэтому Тетрис Да так давайте Извините я чуть-чуть ещё отвечу на ваш вопрос вот по поводу соответствия вот этим тирам на самом деле автоматика каждые полгода пересчитывает соответствие тиру То есть если мы не выдержали 99 и 95 из этого тира можно ещё и вылететь Ну и то есть мы сейчас надо уточнить по-моему Мы из него вылетели но мы вернулись и в результате Ну вот то есть как-то Так а ну давайте тогда с этой стороны один вопрос Да вот а выделяется ли вы в эластики Ну в эластики в этом самом О собственно тёплую и архивный уровни хранения Если да По каким критериям Это отличный вопрос Нет не выделяем потому что нам нужен доступ фактически ко всему на самом Ну то есть пользователь может в любой момент вести любой поисковый запрос Мы за все там 10 лет существования трекера должны ему эту поисковую выдачу выдать понятно что есть тикеты Ну там более свежие и вероятность обращения к ним выше но как бы всё равно получается мы должны поддерживать И то и другое мы на самом деле думали про то чтобы действительно сделать холодное хранилище например для тикетов там за 7 лет а за последние 3 года ну или вот что-то такое разделить но качественно мы пока не придумали как это сделать ня вопро тес был какой-нибудь приз таблица рекордов не было Ладно давайте а вот на скеле Кстати да на скеле мы раздавали врем у меня такой вопрос Вы сказали что у вас теперь политика логирования есть и вы гиру ете только те эксп которые могут помочь дежурному разобраться в ситуации как на этапе написания кода Вы можете понять како какой эп будет полезен и если вы можете заранее предупредить такие места Может быть вы сразу видите проблему её можно починить тогда Это отличный вопрос на самом деле действительно на этапе написания это часто бывает сложно определить Зато очень легко определить какие эксп будут не полезны и вот всё что не полезно Ну например какие-то долги Вот раньше писали долгие запросы влак на уровне Во теперь потому что оно практически там ну ни о чём не говорит многие обращения к некритично внешним системам получается Мы очень сильно интегрированы как к нам много разных смежных систем ходит так и мы во многие разные внешние системы ходим соответственно многие походы во внешние системы у нас тоже логировать как еррор и чаще всего это было неважно сейчас мы это тоже не логи Ну и так далее и получается что ну большинство того что остаётся оно или иначе связано с какими-то именно системными проблемами которые полезны во время инцидента то есть скорее мы идм от обратного Так ну Давайте ещё один вопрос Вот здесь Да я быстренько один вопрос Можно пожалуйста а что у вас следует за выхо тестирован То есть если к пример у вас вот этот сбой случился вы вышли из 9995 Какие последовали для всех Я так понимаю какие-то финансовые какието состав ничего не последовало но пальчиком нам погрози То есть постоянно выходить из тира очень сильно не рекомендуется но Мы надеемся мы не узнаем такие что последует Вот ещё здесь вот вопрос Здравствуйте спасибо за доклад скажите вот ваше учение Когда вы гасите один дата-центр вы это делаете в прайм-тайм под нагрузкой и что происходит с запросами пользователей в это время они пятисот или как-то трается в живые ДЦ там ну как бы действительно мы это производим в мтайм Иначе просто смысла в этом нет потому что мы не узнаем как сервис ведёт себя под настоящей нагрузкой мы сами с ретра В общем всё сложно то есть мы сами свои запросы не ретра но у нас ретра запросы балансера и нн то есть мы полагаемся на вот эти два механизма То есть получается пользовательские запросы не пяти пользователь запрос не п с кото можно будет в кулуарах после доклада Да а Эдуард надо выбрать два самых интересных вопроса так ну был хороший вопрос про наказали ли нас или нет и это вот вот мне кажется Да и мне понравился вопрос про логирование там максимально сложный и неоднозначный про логирование Мне кажется вот да Всё поблагодарим докладчика спасибо большое Эдуард Спасибо вам"
}