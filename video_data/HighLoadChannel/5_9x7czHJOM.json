{
  "video_id": "5_9x7czHJOM",
  "channel": "HighLoadChannel",
  "title": "Шаблоны проектирования микросервисов на примере Авито / Фрол Крючков (Авито)",
  "views": 72199,
  "duration": 2999,
  "published": "2020-04-27T12:22:18-07:00",
  "text": "всем привет меня зовут frome я тимлид команды занимается там перед тем как я перейду к теме из своего доклада я хотел бы поместить вас в контекст который привел нас монолитно архитектуры в микро сервисную и так а в 2016 году компания авито составила приблизительно из-за разработчиков которые contribute ли в один большой монолит один большой репозиторий который содержал приблизительно 2 миллиона строк кода также мы релизе ли 1 или 2 раза в день это в лучшем случае когда мы развили два раза в день и и также в тот момент у нас уже были зачатки микро сервис на архитектуры как вы можете себе представить 100 разработчиков которые контролируют в один репозиторий с большим количеством фич а приводит к тому что доставка какой-то маленькой фичи приводит к тому что релиз нового функционала происходит очень долго так же это касается и пример если у вас какой то маленький баг на фронте вам также необходимо собрать большой монолит с большим количеством шагов pipeline от тестов интеграционных тестов чтобы выкатить и пофиксите баг конечно это было очень болезненно для того чтобы разрабатывать и вести быструю разработку в компании и также удовлетворяет потребности бизнеса также такое количество функций в монолите проводила к тому что кот стал становился очень связаны и разработку было вести очень тяжело поэтому компания решила о том что нам необходимо переходить от монолитная микро сервисной архитектуре которая с большими нахлёстом перекрывала все проблемы нашего времени как вы знаете достоинства некрасиво скорректирует заключается в том что мы заключается в том что комп команды могут вести независимую разработкой доставлять тысячи в продакшен гораздо быстрее если если бы они делали это в монолите также это достаточно сильная важная характеристика которая обладает прекрасной архитектурой который заключается в том что если часть вашей системы упала или не работает то вся система продолжает работать уже в ограниченном функционале но при этом оно продолжает работать так же вы более эффективно и утилизируйте ресурсы компании то есть вы можете исцелить ваши серверы микро сервис эти сервисы исходя из нагрузки которые есть и также более гибко удовлетворят потребности бизнеса в виде более гибкого б тестирования к на реечных релизов на или заменять какой-то функционал 1-ую другой прозрачную для бизнеса о чем собственно будет мой доклад когда вы что я перечислил достоинством микро сервисной архитектуры но эти достоинства очень легко превратить в ее недостатки об этих проблемах я бы хотел поговорить на нашем опыте я хотел бы подогреть поговорить в этом докладе а также поделиться решение данных проблем и так наш первый подход микро сервисной архитектуре мы решили идти таким путем когда мы стали брать очень мало связанные компоненты системы и выносить их в микро сервисы как я сказал нас был небольшой plaster каберне this который мы решили часть функционала выносить туда например 1 предел следующим образом пользуйтесь заполнял какую-то форму а пички получал post запрос сохранил это в базу затем у нас был крон который родился сделал выборку и дел непосредственно рассылку в делом непосредственно рассылку в продавцам мы решили что вот этот фон со функция по отправке емейлов этот хороший кандидат для того чтобы быть маленьким независимым сервисом который бы выполнял только свою функцию это как бы хорошо сыграла и таким образом вся бизнес-логика находилась в монолите при этом при этом отправка почты находилась в отдельной микро сервисе следующий этап был более крупный микро сервис такой больше бизнес ориентированный а у нас был кейс когда мы получали как кадастровые номера при подаче объявления делали из этого проверку смотрели на сколько пользователь корректно заполнять данные опции собственности с рост реестром мы также подумали что это очень хороший кандидат я вынес в отдельный микро сервис и расследовали такому же flow когда у нас вся бизнес-логика frontend находился в монолите но при этом сам самопроверка и валидация кадастрового номера и параметров объявления с данными росреестра происходило в в микро сервисе новый функционал шел таким же путем например садист адресов очень хороший кандидат мы вся бизнес-логика находилась в монолите формирование запроса подготовка запроса сам а сам asam-sa jest был микро сервис на архитектуре микро сервисе как бы получилось ли на тот момент у нас микро сервисной архитектуре я думаю что мерил мерил вам такого их характеристики будет если мы пройдемся по достоинствам и красивой фигуры и убедимся что мы ее приобрели легко ли стала доставлять фичи командам в данном случае ответ скорее нет чем да потому что когда у вас такие маленькие независимые микро сервисы бизнес-логика размазано по нескольким проектам и командам только становится тяжелей вести разработку надо какой-то над какой-то функции проекта и поэтому здесь это желаемое сходство мы не получили также что насчет малой связанности проектов здесь также эта характеристика не было достигнуто потому что при условии при условии что монолиты у нас падал или другой микро сервис падал то наша функция не выполнялось мы не отчаивайтесь не отчаивались и продолжали работать в этом направлении в чем основная была проблема проблема заключалась в том что мы не выделили домен для нашей проблемы не выдели основные сущности ни абсаль ее поведением и всего лишь взяли очень узкий у скифов модуль который малыш был связано с чем-то следующим этапом мы начитались domain-driven design от эванса и стали действовать немного по-другому второй этап проходил под эгидой просто национальный команд мы переход к компания переформатировали из-за функциональных просто национальной команды что позволяло командам работать и владеть всем функционал функциями для того чтобы доставить к какой то конечно блок функций для пользователей вести разработку изолированное автономно а второй момент который на спился это ключевой элемент микро сервисной культуры это и пики и твой который позволял нам крутить всю нагрузку в обход монолита и он также выполнял очень важные функции когда делал проверку авторизован ли пользователь или нет или например firewall который по сути дела тоже ходил в маленький микро сервисы делал небольшие запросы следующей нашей целью был сервис магазин для пользователя сервис магазин заключался в том что когда пользователь имел большое количество объявлению но хотел их объединить в один общий в одну общую страницу где пользователь бы мог смотреть только объявление этого магазина и когда он попадает на страницу объявления этого магазина все рекомендации также были из этого магазина мы подумали что это очень хороший кандидат для выноса в отдельный микро сервис определили домен для этого для этого для этой этого модуля также описали сущность описали ее поведение описали все связанные контекст и для этой сущности и я думаю что вам понятно что у нас также не получилось почему мы когда мы стали выделять только понять как bound контекст то есть граница микро сервиса и сущность которой он над которой он хранит и производит операции мы наивно стали следовать тем моделям которые у нас были в нашем монолите то есть у нас был там модель пользователя модель магазина модель подписок и мы слепо стали думать что наш нашу модель это хорошие кандидаты и мы начали выносить слепо эту этот репозиторий с этим с этой функцией в отдельный микро сервис это выглядело следующим образом мы репозитории обернули в интерфейс сделали 2 имплементации одна была старая и ходила через через базу данных а вторая через клиент но у микро сервису и и делал запрос уже через микро сервис в этом в этом мероприятии нам помог и такой понять как фич это был то есть по мере того как мы переносили бизнес-логику из монолита в микро сервис мы могли динамических фич это и переключать трафик из баз данных в в микро сервис что здесь было не так как бы кажется что все хорошо команда владеете своим микро сервисом базы данных может вести разработку независимо владеет fontaine дам но проблема заключалась в том что когда мы начали распиливать бизнес-логику в базе данных то встретились с такой проблемой когда вся бизнес-логика при ипре и развязывание выяснилось что наша сущность магазина очень тесно связано с сущностью такой как подписка и получилось что наш при все критические вызовы до нашего микро сервиса привели к тому что мы сделали fall back обратно в монолит что вызывало большую связанность этих двух систем и не позволяла команде вести разработку независимо и деплоить свой проект и салерно то всего остального так же такое появилось такое понятие как и поехал когда у вас появляется большое количество внешних зависимых вызовов без которых ваш сервис перестает работать как правило это все изменяющие вызовы которые приводят тому что ваша команда по сути дела не может вести разработку изолирована от основного проекта или других микро сервисов также не всегда есть возможность развязывать бизнес транзакции и это тоже одна из больших проблем что здесь мы сделали неправильность мы пасхи и последовали нашим моделям то и организации кода который у нас была в монолите и без анализа зависимости мы мы вынесли это в микро сервис хотя по сути дела это несколько моделей были тесно связаны также очень серьезная проблема которая идолам разработать микро сервис заключалась в том что этот не красе разделил общие ресурсы с другими сервисами или монолитом в данном случае следующий подход у нас был конечно удачный мы учли все наши проблемы выделили это был сервис прав профиля пользователя в которой мы также выделили домен для этой сущности команда который над этим работает и также абсол bound контекст но на этот раз мы очень четко проанализировали связано с нашей сущности данных пользователя с тем без чего эта сущность не может существовать например выяснилось что данная сущность не может никак работать она бесполезна если допустим идет работа с компании пользователя или имейлами пользователя или например социальными авторизации пользователя то есть если бы мы это она неполноценна без всех этих моделей как мы думали в нашу монолите но по сути дела это одна общая сущность что у нас получилось когда мы сделали основной расцвел у нас получилась большая изолированная базы данных в не было порядка 12 или 13 таблиц и котор и сервиса держал до 60 in point of возможно многие скажут что мы у нас получился менее монолит но по сути дела у команды появилась возможность а вести разработку над своим доменом изолирована и не зависеть от других компонентов системы таким образом они могут тепло и твистеров тепло и разрабатывать внутренней функции функции сервиса и прочие элементы независимо от всего все из других команд в компании они путь получили возможность тепло из и независимо ввести в вести разработку также они получили только свойства как малую связанность или независимости от других компонентов потому что они владели и только группой микро сервисов которые реализуют исключительные фонтан функции но также и базу данных и все так все стороны что позволяло не позволял при падении внешних компонентов и их это никак на них это никак не влияло таким образом мы получили это желаемые свойства также у них появилась возможность стелется в зависимости от нагрузки и делать тестирование и выходку к на реечных релизов или проводить аб-тестирование какой основной принцип необходимо соблюдать для того чтобы достигнуть желаемых достоинств micros террасной архитектуры они заключаются в том что вы не стремитесь сделать вашим микро сервиса как можно более гранулярными маленькими конечно это очень желаемое желанное свойствами красной архитектуры но в первую очередь вы должны задуматься о том как какой бизнес процесс инкапсулирует ваш микро сервис или группы ваших микро сервисов для того чтобы команда могла вести разработку над своим проектом автономно и изолирована мы поговорили о том как описать bounded контекст и что поместить в нашем микро сервисы но когда у вас большая компания и достаточно большое количество микро сервисов то внешние большие цифры начинают играть большую роль здесь и у вас постоянно начинает отказывать ибо сеть либо какие-то сервера выводят из строя выходит из сервер и выходят из строя и получается такая ситуация что у вас постоянно часть системы не работает для того чтобы это чтобы это не как они сканировать и и приводить к отказываюсь работы всей системы необходимо ли зовый такой такие ряд мер как который называется грейс пул degradation в него входит таки паттерна как на logic parton использование sarkic брокеров и правильно настраивать half чеки что такое налог джек паттерн для того чтобы объяснить это гораздо легче показать на примере предположим у нас есть сайт и для того чтобы отобразить имя пользователя наш наш проект должен сделать сайт должны сделать запрос микро сервис который это имя содержит и как правило этот статус бар отображается на всех страницах сайта если при этом наш сервис пользователя откажет и не сможет сказать не могу которым под которым пользователь авторизован то все страницы не будет работать конечно это достаточно наивный пример иди того чтобы показать но представьте что какое чтобы отобразить какое-то поле на главной странице за это ответственность сервис который находится десятом в цепочке вызову сервиса и если не правильно реализовать или не реализовывать на logic паттерн это приведет к тому что это ошибка одного маленького микро сервиса придет к выходу из строя всей системы и такая поведение называется каскадный отказ микро сервисов чтобы этого не допустить как правило делается такое вносит такие изменения сервис когда во всех готов если какой-то не критичный сервис вашим вызове микро сервисов downstream отказал то вы это эту ошибку обрабатывайте корректно вы подставляете либо какие-то дефолтные значения либо эти поля вообще не возвращайте сервису вверх то есть но при этом нужно иметь в виду чтобы сервис сверху смог то также обработать эту ошибку корректно когда у вас не приходят какие-то данные наивная имплементация такой такого паттерна выглядит когда вы начинаете а перехватывать все ошибки на вызов данного микро сервиса и и таким образом проверять заводить 2 flow когда у вас а получилось получить эту сущность или нет но как мы знаем если отказ пока сердца происходит но не так часто это не постоянный режим работы микро сервисов и если вы разделили fork lift какая-то бранче вашего кода не постоянно используется то как правило она не работает для того чтобы этого не допустить и используется такое понятие как на лоджик паттерн а когда вы реализовывать интерфейс к сущности который возвращается от внешнего вызова и делайте ее 2 имплементации первое это при успешном возвращение когда сервиса вернула ответ и вы действительно генерируете настоящую сущность а вторая эта заглушка сущность заглушка и таким образом вы убираете из вашего кода эти фотки которые необходимо покрывать дополнительными юниты или интеграционными тестами следующий паттерн про который хотел бы сказать это sirpi breaker если предыдущий патрон как бы скрывала ошибку сервисов вниз по цепочке то этот паттерн предотвращает усугубление ситуации для сервисов которые находятся дальше по цепочке это примерно выглядит следующим образом когда вы делаете делаете запрос этот запрос проходит через сроки breaker и нас если ваш сервис ответил успешно то он прокси ru это свет обратно если нет то делает возможно делает ретро если это не помогает тогда сервис выходит из-под балансировки или или делается запрос другой инсцес микро сервиса на что здесь обратите внимание в совокупности такой паттерн в совокупности с ретро и автоматическими ретро яме и неправильные настройки таймаутов это может привести к тому что вы только убьете micro series и вниз по цепочке предположим такую ситуацию вас есть два микро сервиса у одного тайм-аут 50 миллисекунд и 2 1 секунда вызов внешних внешних ресурсов тапира базы данных или это кэш и когда мы делаем в сервис сделает вызов нашими к сервис то мы занимаем worker of в данном случае маркеры могут быть как маркеры могут быть как fpm worker если это печке или например пул connect в базе данных или или кашу таким образом сервис который не дождался при возникновении проблем с сетью сервис не дождавшись ответа а сделал ретро и взяла 2 вызовы после 50 секунд но маркеры в другом micro series себе продолжают работать над предыдущим запросам таким образом вы скором времени вы видите строя практически весь микро сервис и тем самым только усугубите ситуацию для того чтобы это не допустить как правило требуется настройка такой понять как каскадный настройкам таймаутов и она выглядит следующим образом когда вы ваши тайм-аут и настраиваются все меньше и меньше в глубь как вы туда как gps проходит вдоль цепочки вызову микро сервисов возможно это достаточно сложная проблема с большим количеством коннектов достаточно большим количеством микро сервисов но я думаю что когда команда работают как правило не знают кто их вызывает и кто вызывает их вниз по цепочке таким образом если каждая команда из договориться о своих потребителей и зависимых сервисов то эту систему можно решить и последний патрон про который я хотел бы рассказать это hal чеки перед тем как я расскажу вам что сделать с hal чеками необходимо понять еще не используется перед тем когда вы стартуете ваш микро сервис перед тем как он попадет по балансировке и на него пойдет живой трафик лот балансе проявляет делает вызов на хавчик и проверяет насколько проявляет готов ли ваш сервис принимать запросы для того чтобы это понять вы в своих оппонентах с колчаком делаете запрос и зависимые ресурсе микро сервис это например может быть база данных либо либо кэш и после того как у все ваши внешние ресурсы вы имеете доступ к внешним ресурсам вы говорите о кей и и лот балансов начинает давать на мега сервис нагрузку в чем здесь может быть проблема важный в момент вы должны разделить hell чеки для старта приложения и для режима когда сервис находится под балансировкой потому что возможно ваш микро сервис может работать в нашем случае например когда у нас 95 процентов запросов это читающий запросы и всего лишь 5 пишущих то при отказе базы данных мы все еще можем сервировать допустимы и читающую нагрузку при условии что у нас большой hit rate в кэш поэтому когда у нас происходит hold когда у нас отходит базы данных когда плод находится под балансировкой мы мы не отдает нахал чеки что мы недоступным у нас однажды была такая проблема когда в коде была допущена ошибка и при реализации библиотеки и на патент на stiu мы съедали все connect и базе данных и при этом не возвращали их обратно через какое-то время hal чеки перестали проходить для подав и вот балансер вывел их под нагрузки тем самым вы все микро сервисы которые могли предоставлять доступ на этом все спасибо за внимание буду рад выслушать ответь на вопрос фрол добрый день меня андрей зовут подскажите пожалуйста вы сказали вы сказали что 60 м поинтов в сервисе при этом работало несколько команд и при этом удалось улучшить доставку этого сервиса то продакшена как вам это удалось что на одной сущности с мульти командной разработкой у вас улучшился этот процесс на самом деле разработка велась одной командой просто мы не стали делать скажем так супер маленькими к сервисам мы посмотрели что наш связано с этой сущности она тесно связана с другими и поэтому для того чтобы не распиливать одну транзакцию не прощать дистрибутивный транзакцию скажем так это было легче забрать в одну базу данных тем самым команда уже на ранних этапах могла имела возможность независимого вести разработку над этим микро сервисом при этом при необходимости могла уже изолирована возможно разносить какой какие-то функции в отдельные мини микро сервисы то есть по сути в один момент времени велась разработка над одним надо одной micro функциональностью нет мы вот сделали а наоборот опять смотрели зависимость одно одной сущности мы поняли что бизнес процесс который работает он не включает в себя только допустим исправление каких-то там персональных данных по сути дела он был тесно связан с тем как пользователь регистрируется через социальные сети а как что у него есть компания и когда ты забираешь скопом все эти объекты которые там тесно связаны транзакции внутри себя и выносишь этого отдельный кусок у тебя появляется возможность быть независимым от других микро сервисов но если враг понятно и весна я понял спасибо спасибо за доклад можно два вопроса первый это стратегия как бы переноса общих данных и пробросов синхронизация общих данных между микро сервисами сейчас подразумевается тип подход транзакции или это как приносили данные когда разделяли микро сервисы если например какой то представить что 10 две базы но они используют какой-то общую сущности она должна синхронизироваться в результате какого-то 3 3 функционал административного не знаю ну здесь мы мы сделали стремились к тому чтобы минимизировать вот этот общий функционал между микро сервисами чтобы state они не разносить по низким к сервисам но при этом у нас много было докладов про такой подход как сага то есть у нас есть внешний так вот брокер который отвечает за цепочку вызовов in service on point of service of который позволяет сделать такой уменьшил consist если подход между функционалом допустим вам нужно списать деньги я предполагаю предположим вы заводите определенную сагу которая а в которой прописаны несколько допустим вызовов мы сначала идем сервис пользователь там меня и маму статуса участника допустим там на компанию а потом применяем там какое-то действие в другом микро сервисы где мы не можем сделать транзакцию и этот внешний микро сервис питается проиграть этот сценарий то есть если первый раз у него не получилось применить изменить статус пользователя он приходит там делает ретро и через какое то время потом следующим шагом он делает уже изменения в в другом микро сервисе то есть у нас получается такая гарантированный event shall consistency для вот таких распределенных данных которые необходимо чтобы они были синхронизированы частично на второй вопрос ответили то есть я правильно понимаю транзакционных именно как бы из коробки не всегда удавалось обеспечить поэтому иногда просто вы как то вручную tamb очищали если что-то ну нет здесь смысл в том что у вас получается транзакцию если вы сколько-то сущность забирать в 12 к сервисам у вас есть одна базы данных и там внутри одного micro series и одной базы данных изолированы для этого microsoft и там соблюдайте транзакцию как это в привычном мире но когда у вас есть какая-то так бизнес-процесс который действительно влияет изменение в нескольких доменов в нескольких командах тогда 10 мы используем вот этот сценарий когда у нас есть внешний сервис который которому мы прописываем шаги для того чтобы так бизнес-процесса выполнялся то есть поменяли пользователя там поменяли в магазин и чтобы это и вышел венчальная консистентную выполнилась то есть офицер за это отвечать если у него второй шаг не получился в этом случае мы делаем операцию онду то есть второй шаг не получился он потом приходит при определенных конфигурациях делает в первом сервиса онду ну таким образом как бы откат откатывает амарны чтобы изменение были атомарного времени спасибо больше добрый день дмитрий информация стинг алекс я два вопроса первый касается ограничение доступа на к сердца и то есть вы используете и пегий твоей которые сути всегда проверяет основные доступ запросто одолеете запросы идут микро сервисом микасе-сан наверное тоже нужно как-то определишься запрос авторизованы видим авторизация приложения от которого пришел запрос и на этом все ограничивается или могут ли микро сервисах быть какие-то дополнительные распределения правильно ли я понимаю что вопрос в том что как мы ограничиваем доступ внутри кластера одна их микро сердца к другим на до такой механизм есть это мы используем кубер нить из ее на основе namespace of можно задавать на улице и там секретов на мойке то как как каким другим нас пей сам данный микро сервис имеет ну или and space другими нмс пей сам в первую очередь это именно ограничение по 6 даст какие контейнеры имеют ну да мы как бы в данном случае здесь гранулярный это гранулярный является т.н. space один микро сервис имеет 1 м space то есть несколько инстинктов 1 микро сервиса вы находится в одном нам спейси во первых у нас здесь через систему через вольт имеет мы определяем доступ к базе данных то есть у вас не получится использовать то есть мы на стадии тепло и подкладываем конфиги и кредит шо вы для базы для базы данных конкретно этого микро сервиса то есть во первых 10 идет изоляция и на уровне что только от микро сервис имеет доступ базе данных к этой базе данных то есть это на уровне тепло и делается на автоматизировано второй момент что мы также работаем над тем чтобы 1 x micro сервисов в не мог по htp и паштет из 3 часа другом микро сердцем внутри кластера но это еще насколько знаю пока не сильно-то считает желание находится в доверенной зоне как бы до между собой и могут нормально взаимодействие да а вот вы говорите о доступа к б д то есть несколько микро сервисов могут напрямую обращаться в одну и ту же базу до нас получается нет это наоборот мы и стремится чтобы этого не было то есть у нас на данный момент один микро сервис одна база данных и только он имеет ней доступ спасибо и второй вопрос касается wear санирование микро сервиса да то есть если команда выкатывает какие-то изменения микро сервиса например опять битва еще и к этому не готов да соответственно должна быть возможность как-то ipega твоей этому некрасиво сказать за каким типом или версии данных или как он обращается ну то есть естественно что может быть север санирование опять да пока там работаем но если версия много взгляд на накапливается большое количество обработки логике здесь во первых такой момент что у вас допустим в различные моменты тепло и вашего микро сердца то есть как правило мы используем rolling апдейт когда у вас в один момент времени может находиться две версии разные версии одного микро сервиса то есть у нас допустим 30 подав 1 микро сервиса мы начинаем выкатывать новую версию мы потихонечку выключаем кабинете выключая допустим 5 кодов старой версии поднимает 5 кодов новой версии и как минимум уже на уровне программного кода мы соблюдаем чтобы чтобы это все корректно что чтобы выкатка новых версий не поломала потребителей других клиентов также мы активно используем для кора micro series of такой парень как сидиси тестирования continuing контракт с тестеру не который гарантирует что выход к новости что контракт нового и версиями к сервиса соблюдается для и всех их потребителей то есть мы гарантируем чтобы сам когда выкатывается новая версия микро сервиса и если по какой-то причине там программист не не понял что его изменения могут как-то повлиять на формат ответа или на формат ответ это вот этот садись и тестирование капрала ловит такие ошибки 10 речь идет о том что опеки твои обновляется по сути параллельно с вами красиво ну как правило здесь и пик этого немного там костаная история потому что как правило те пункты которые смотрят наружу их не даст не так не так много и как правило мы управляем всеми потребителями эти эти кто потребляет этой api то есть это как правило фронта или мобильное приложение и тут в такой в полуавтоматическом режиме это происходит то есть вы описываете какие компоненты смотрят наружу и автоматически при тепло и они прописываются они там прописываются есть наверное ситуации когда микро серво сработать на несколько приложений например тот же сервис от отвечающие за рассылку и разные потребители может быть не готовы к тому чтобы обновиться прямо сейчас их в принципе все устраивает но это только распределением но versio нирования самого если ребята из нового вы катушки новый микро сервис поломали контракт для вот этих старых потребителей они просто не смогут выкатиться то есть они просто не пройдут интеграционную стентирования здравствуйте спасибо за доклад я здесь здесь здесь налево налево налево здесь вижу исходя из доклада в текущей я бы назвал это транзакционные зависимой архитектуре у меня вопрос такой достаточно немаловажны а каким образом вы обеспечивали отказоустойчивость системы объяснил свой вопрос на примере то есть у нас есть скажем м количество пользователей выполняющую одну операцию в один временной так если представим что тот или мы сервис упал на какой-либо причине куда делся запрос то есть он потерялся по факту или нет понял хороший вопрос здесь я не совсем скажем так все рассказал во первых для того чтобы система сделать более отказоустойчивый микро сервисной архитектуры большая часть взаимодействия уходит во первых в асинхронное взаимодействие то есть использование какой-то общей шины про которой в данный уровень я прерву извините я прерву ваш ответ вы не рассказали ничего про асинхронность раны джинсов то есть каким образом они взаимодействуют начиная от шины и так далее так далее так далее да ну не тяните накладывала но у нас нет у нас нет традиционной то есть если грубо говоря нам пришел запрос мы его потерять как бы здесь имеется ввиду если пришел запрос с фронта то как минимум мы делаем а пирог запроса сейда понтент на и если мы и так вот изменяющие запросто мы сделаем ретро его единую доставим для клиента если эта часть какой-то транзакции ну такой дистрибутивной транзакции которых у нас не так много на самом деле мы их их можно скажем так их там меньше десятка то у нас есть сервис который грубо говоря если он сделал запрос и получил тайм-аут и непонятно там применилась это изменение нет то здесь во-первых работает механизм retrieve во вторых механизм опять же как sld патентные запросы чтобы при ретро и у нас estate опять не поменяется и если все-таки там на каком-то этапе мы не получили ответ от этого микро service to the мы сделаем манду от всей этой операции хорошо это скажем один цикл если предположить что ретро ну я полагаю что ретро я вас там парит лимиту скажем три раза то есть три три попытки что если предположить что downtime того или иного сервиса превышает предположим час в этот промежуток времени 3 литра и произошло то есть тем самым запрос теряется так или иначе нету не теряется потому что этот сервис сага о на хранить себе и сработать во-первых работают они рты у ребят кто поддерживает этот сервис потому что он скажет что вот эти шаги мы не смогли их выполнить течение там одной минуты то есть их нужно либо перезапустите либо откатить и либо понять где мы рассыпались если я понял что есть некая очередь туда складываться все запросы кто не обработал какой сервис не отработал запрос перри поднимается так далее да да либо и там уже принимается решение подкатываем и это все ну вообще отката же происходит в автоматическом режиме но при таком длительным downtime скорее всего там нужно будет ручное управление и небольшая ремарка и сколько вот в вашем опыте это downtime происходил то есть максимальное значение но если могу сказать что может быть минут десять у нас какие-то внутренние сервиса бывают что больше от развивать его вопрос про вопрос такой вы сказать что у каждого микро сервиса у вас своя отдельная база соответственно дублируйте ли вы какие-либо данные между микро серво сами по может быть одна из двух причин первое это для гришин 1 пользователь и упал и чтобы показать имя имя за дублировали да и второй второй сценарий для эффективности чтоб не делать лишних запросов хороший вопрос во первых если какой-то другой домен другой группы micro series of действительно требует имеет такой широкой сущность нам например messenger то мы используем шину которая по большое которую которая какое-то изменение нашей основной сущности транслирует стоит там стоит bass стою здесь как ариец state shine то есть когда у нас как это присходит event мы вши не передаем event с изменением до и после и это отражается на всех подписчиков то есть они могут обновить данные о сущности с кем они связаны да здесь происходит дублирование но здесь изначально проектируется только такой сценарий для тех сервисов когда они действительно толерантны к тому что данные будут не актуальны то есть это изначально закладывается в архитектуру если же ребята не могут на устаревших данных работать тогда они делают api вызов в другой микро сервис каждый раз и здесь вопрос о том что как избежать этих большого количества запросов как я сказал 10 когда мы необходимо грамотно выделить вот этот домен чтобы как можно больше операций могло происходить внутри микро сервиса и тем самым минимизировать вот это асинхронное взаимодействие между сервисами то есть здесь правильно выделение доменов играет роль вопрос касаемо от тестирования уже немного говорили про сидиси tasty не внесли какие-то сценарии которые предполагают мерсер но тестированием из сервисного взаимодействия то есть полностью приложения в целом но в рамках всех его микро сервисов как это реализовано на данный момент момент у нас есть нтн тестирование которое по сути делать делает скажем так мы вызвали отправили создали объявление и таким таким образом потом что-то с ними сделали как правило это проходит по цепочке микро сервисов но мы стремимся такой ситуации когда мы можем тестировать один микро сервис изолированная то есть мы во первых у нас есть стейджинг среда для этого и когда мы прогоняем нтн тесты то есть все сервисы которые участвуют есть вроде они дублируются стейджинг инфраструктуре и прогоните этих тестов нтн тестов касается всех микро сервисов в стайлинге но когда ребята ведут разработку надо непосредственно изолированным микро сердцем мы стараемся соблюдать как правило делать изолирована тестирования микро сервиса и делаем моки внешних вызовов ну делают тупо делаем make a внешних вызовов микро сервис то есть мы используем насколько я помню сейчас hover fly для того чтобы мы могли то есть если мы допустим это какой-то конечный микро сервис который входит в базу данных то это как обычным делается фиг накатывается текстуры и таким образом проверяются делать интеграционное тестирование при микро сервиса которые делают зависит от других утра сервисов то мы тут используем матирование поднимаем сервер которая отдаёт моки нашему микро сервису спасибо за доклад подскажите пожалуйста вот в первой части доклада у вас родился микро монолит которые самостоятельно способен работать в изолированный сергей и там вроде как все хорошо но потому что монолита это прекрасно но дальше у вас появились вызовы по цепочке микро сервисов каскадной отказы и не получается ли что это приходит возвращается к тому же самому монолиту просто уже распиле нам у на отдельные сервисы но которые точно так же не могут взаимодействовать без какого-то одного еще не дай бог связано не только по каско дунае циклически я понял вопрос и здесь принципиальная разница когда допустим нас есть цепочки как правило эти цепочки выражаются не в том что допустим вам пришло прошел event вам нужно изменить state а потом этот еще в каком-то сервису другому изменить какой-то стоит то есть это как правило цепочки порождаются читающий нагрузкой и при отказе каких каких-то сервисов н.и. это не критично если вы скажем так не доставите часть часть ответа вверх по цепочке но именно пишущий нагрузки как правило эти цепочки неглубокий ну и на самом деле их нет потому что это все прикрывается вот сервисом сад то есть когда каскадные отказа это когда совсем скажем так базовые принципы не были реализованы то есть это как правило читающий нагрузка поэтому это не настолько критичны и сюда же там сияние отвалятся флоп спасибо вам большое за интересный доклад хотелось бы узнать о как вы храните но ведете так называю matos описание микро сервисов то есть кто за него отвечает ну вы поняли да бы его как-то ведете или у вас все-таки есть кота разнородная документация которая просто там просматривается понял хороший доклад как мы видим метаданные нашим наших микро сервисов у нас ведется работа юнитам архитектуры мы разрабатываем свой пас платформ как а за сервис который позволяет скажем так разворачивать очень легко мега сервис и с допустим печки голый бетон который как правило поднятиями к сервиса автоматически то есть чтобы развернуть разработчику новый размер асель если мы достаточно там выполнить команду хочу новый микро сервис на таком языке с таким таким языку нам такой закон с таким названием и как правило все там бот или сервер делает новый репозиторий заводит даже варды в графа не заводит там ошибки даже борт для центре с ошибками и у него появляется нас есть сервис такой рост реестр micro series of и опять же он там регистрируется и как правило вся большая часть этой информации о том как что за об этом об этом micro series 3 rus суд на странице которые автоматически генерируется при создании века сервис вот этим ботом и в такой момент еще что там вся необходимая информация о целое сервиса о том как там покрытые тестом каком мой какой команде у над вы относится кто за него ответственен то есть это все практически в полуавтоматическом режиме спасибо очень интересные решения друзья остальные вопросы в кулуарах это за что вопросов много и поэтому накидываете на фрола прямо сейчас и прямо несите на руках у в столовку через десять минут wear a да подожди frost кому за лучший вопрос мы дарим книжку какой был самый неудобный самые конструктивные или это разные тестирование микро сервисов когда мы да все верно"
}