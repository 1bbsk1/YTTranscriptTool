{
  "video_id": "FoLFZZXYjds",
  "channel": "HighLoadChannel",
  "title": "Обучение бота поддержки для банка: качественные данные и тысячи интентов / Ирина Степанюк (Тинькофф)",
  "views": 168,
  "duration": 1907,
  "published": "2024-10-29T03:07:35-07:00",
  "text": "Я хочу пригласить на сыну Ирину которая расскажет а чем могут быть большие данные полезны для поддержки Ир прошу раз-раз Да всё работает Всем привет Меня зовут Ира я занимаюсь NLP для автоматизации суппортов Тиньков и сегодня буду рассказывать про то как мы обучаем Бота поддержке поговорим про то как мы собираем качественные данные и поддерживаем на проде более тысячи интен тов в начале хочется рассказать про особенности автоматизации поддержки в Тиньков в тиков есть большое количество продуктов самое известное - это жёлтое приложение ли также есть Tin of Mobile Tin of инвестиции и другие и сейчас робот работает на де направлениях в чате и трёх направлениях в Голосе у нас выходят новости выходят новые продукты условия работы банка меняются и соответственно вопросы клиентов в чате и на звонках тоже меняются и нам нужно быстро адаптироваться к изменениям так как нам нужно выдавать как банку верную актуальную информацию клиентам быстро а как это будет работать допустим у нас выходит новость то что выпустили стикер для оплаты смартфоном клиенты сразу же начинают писать в чат а что вообще такое стикер как он работает как его можно заказать и таких вопросов одновременно может быть очень много и мы хотим сразу же начинать помогать то есть отвечать клиентам на их вопросы потому что таких вопросов одновременно может быть очень много и мы не хотим загружать линию операторов и не собирать очереди на линиях как это будет работать с точки зрения наших сервисов у нас приходит какая-то первая фраза клиента Как заказать стикер мы по ней намерение пользователя и запускаем какой-то сценарий в сценарии происходят до уточнения задаются различные вопросы и затем робот уже формулирует финальный ответ клиенту который как раз выдаётся ему но у клиента могут возникать какие-то ещё дополнительные вопросы И если мы не можем определить интент в таком случае то мы будем переводить на оператора это один из сценариев по одному интен нох св и каждый сценарий - Это сложный бизнес-процесс нужно учитывать большое количество бизнес когда мы их создаём то есть Какие вопросы именно задавать клиенту Как именно формулировать ответ и банк должен выдавать верную информацию то есть поэтому все сценарии ответы пред записываются людьми то есть мы не можем подключить туда генеративную модель которая будет генерировать ответ на ваш вопрос потому что нам нужно выдавать верную информацию А моделька может люци и выдавать что-то Невер и также мы поддерживаем на проде по разным направлениям более у нас более 4.000 интен соответственно О чём же будет мой вообще доклад мы не будем здесь обсуждать сценарии Что у нас там происходит скорее сфокусируйся на классификации интен тов потому что в нашем домене С таким количеством интен тов это уже является сложной задачей поговорим про то как мы поддерживаем базу интов актуальный для того чтобы отвечать на количество вопросов пользователей как мы получаем чистые данные по интен чтобы у нас было верное Хорошее распознавание о том какие архитектуры используем для интент классификации чтобы как раз быстро создавать интен и что вообще поменялось после выхода лмо чат gpt И как мы внедряем сейчас м в наше решение в начале хочется вспомнить вообще о том как обычно решается задача интент классификации То есть к инженеру Т говорят вот у нас есть данные давай пожалуйста обучим интент классификатор что же ML иннер будет делать Ну в начале ML иннер пойдёт сделает выгрузку каких-то данных сделает предварительную аналитику покриття разметку данных после этого у него уже будет какой-то финальный датасет который он может использовать для обучения дальше ML иннер сможет обучить модель допустим использовать условный Берт предсказывать вероятность каждого индента на этом датасете и затем после обучения модели будет выкатывать эту модель напт с помощью Тритон свинга например и если будет нужно сделать какой-то быстрый фикс починить какую-то важную ошибку по интент то инженер может написать регулярное выражение В таком случае то есть в целом такой пайплайн он довольно стандартные для многих задач и компаний и мы довольно долго жили в такой парадигме делали большое количество итераций переобучения пересобрать данные искали ещё интент и довольно сильно расширили количество интент в которым мы покрываем продо поток но получили после этого какое-то количество технических проблем про которые сейчас поговорим Первое - это 90% работы инженера у нас была работа с датасета то есть вместо того чтобы погружаться в какой-то льный контекст читать статьи ML инженеру нужно погружаться в бизнес логику понимать вообще банковскую специфику чем один интент может отличаться от другого также у нас может получаться зоопарк моделей так как каждый инженер хочет попробовать какую-то свою архитектуру и из-за этого у нас будут различные модели по разным направлениям если у нас есть зоопарк модели то у нас могут не сходиться зависимости в продо вых сервисах потому что инженер будет использовать разные библиотеки они могут использовать например разные версии порча из-за этого нам постоянно нужно решать какие-то конфликты зависимости и разбираться с этим и также у нас может быть долгая итерация обновления потому что как я сказала нам нужно каждый раз пересобрать датасет переобучить модель её пере деплоить и соответственно в таком случае интент на проде появится не сразу А через какое-то время а со всеми этими мы проблемами столкнулись и поняли что Для дальнейшего масштабирования и развития вообще системы нам нужно что-то менять и к чему же мы в итоге пришли первое На чём мы поняли что нам нужно фокусироваться - это создавать единый технологический стек то есть ML инженер фокусируется на улучшении алгоритмов допустим у нас выходит какая-то новая статья мы проводим эксперименты смотрим насколько эта модель действительно является классной и если она побеждает наши предыдущие архитектуры то мы говорим что это какое-то внутреннее сото решение и договариваемся что вот именно эту модель мы будем использовать теперь везде по всем направлениям Таким образом мы унифицировать - это работа с данными Как нам вообще создать хороший датасет и вторая - это алгоритмы интент классификации и соответственно ML иннер фокусируется на создании платформы для работы с данными и на улучшение алгоритмов интент классификации и затем команда аналитиков уже может использовать эту платформу для создания интен тов обычно у аналитиков больше есть продуктового контекста И за счёт платформы можно гораздо быстрее создавать интен и доводить их до прода поговорили про проблемы зачем мы это делаем теперь хочется углубиться в каждую компоненту и подробнее рассмотреть а что мы вообще там делали Первое это платформы данных поговорим вообще что мы понимаем под платформой данных и какие проблемы Она у нас решает первое как я сказала у нас вопросы клиентов регулярно меняются и нам нужно адаптироваться к изменениям И поддерживать нашу базу интен тов актуальной А что такое актуализация интен тов то есть Нам нужно поддерживать нашу базу для того чтобы она соответствовала какой-то подовой ситуации соответственно мы регулярно удаляем какие-то старые тенты расширяем наши текущие интен и создаём новые интен как же нам можно искать новые формулировки и искать новые интен для этого мы используем алгоритмы кластеризации и выделения ключевых слов про которые как раз хочется поговорить а Первое - это кластеризация здесь можно видеть интерфейс который у нас сделан на Стрим лите этот интерфейс сделали инженеры то есть никаких фнн скилов здесь не нужно этим и удобен МД и соответственно можно просто загрузить какой-то ваш CSV файл нажать на кнопку отправить на кластеризации и получить такой результат здесь можно посмотреть как у нас выглядят кластера наши и видно что там схлопывается какие-то небольшие кластер по которым можно посмотреть и Какие данные туда попадают но также есть какое-то количество алаев в синим выделены как раз алае это какой-то шум в данных который не выделяется в кластера И если мы посмотрим какие кластера У нас есть то здесь в целом видно что схлопывается хорошо похожие сообщения пользователей о том как вообще поменять личные данные А как у нас вообще работает кластеризация по каждому тексту сообщению пользователя мы получаем его динг для этого используем наш предобрий с помощью алгоритма hdb Scan он как раз удобен тем что выделяет алае вот эти шумы в данных затем визуализирую с помощью алгоритма umap с помощью этого как раз получаем картинку с проекцией сообщений на 2D плоскость и также все параметры кластеризации мы подбирали под чаты и под голос то есть использовали для этого датасет классификации смотрели насколько хорошо наш алгоритм кластеризации мат в кластера одни и те же интен и за счёт этого подбирали параметры и подобрав параметры один раз можно их таким образом уже постоянно использовать и не перебирать каждый раз а дальше про выделение тегов теги в нашем случае это какие-то ключевые слова в текстах которые хорошо описывают смысл сообщения То есть например по сообщению Я потеряла карту хотела бы перевыпустить тегами будут потерять перевыпустить и карта и с помощью тегов мы можем группировать тексты между собой то есть понимать какие темы у нас ещё не покрыты интен Тами И за счёт этого рять текущие интен и искать новые интен как это будет выглядеть у нас на платформе у нас выделяются ключевые слова существительные глаго глаголы и по каждой паре тегов мы можем посмотреть их частотность затем посмотреть какие сообщения туда попадают и здесь видим что у нас очень хорошо собираются сообщения где клиенты говорят что они потеряли карту таким образом эти сообщения мы опять же можем расширять каким-то образом текущие интен или добавлять новый интент если его ещё не существует А таким образом э алгоритм выделения тегов позволяет выделять частотные комбинации слов мы это делаем с помощью статистических методов э tfidf и й эти методы удобны тем что они не требуют обучения то есть они основаны на статистика по частотности а слов в текстах и таким образом э там это является хорошим бейслайн вам не нужно обучать какие-то модели использовать большие модели для этого достаточно в целом этих простых статистических методов и после использования этих алгоритмов мы делаем какой-то специальный пост процессинг то есть отбираем существительные глаголы удаляем стоп слова то есть Каким каким-то образом Чистим наши ключевые слова и уже у нас получаются хороший набор ключевых слов которые можно затем использовать и агрегировать по ним информацию идм дальше Допустим мы позвали наш данные выделили теги и у нас получились кластера похожих сообщений Но на самом деле данные могут быть шумные кластера могут быть шумные и Нам нужна какая-то дополнительная валидация для этого мы используем разметку данных можно использовать какие-то краудсорсинга платформы У нас есть внутренняя платформа где разметчики являются операторы они хорошо знают доменную область и как раз очень удобно это использовать как у нас будет выглядеть наша ция то есть мы просим разметчик определить намерение интент клиента мы перечисляем наши интен и просим отнести сообщение пользователя к одному из этих интен тов или сказать что ни один из предложенных интен тов не подходит поэтому можно нажать другое Не уверен то есть я не совсем знаю к какому интен Это относится для того чтобы запустить разметку нам нужно собрать какую-то изначальную информацию об интенс Мы обычно придумываем какое-то название говорящее для этого интен подробное описание А что вообще должно туда попадать и позитивные негативные примеры которые позволяют хоть как-то сделать ограничение А что они и что не должно попадать в этот интент и соответственно после разметки мы получаем уже какой-то хороший датасет чистых данных которые можно использовать для обучения Но конечно же с разметкой бывают проблемы разметчики очень часто ошибаются в таких заданиях а почему это происходит в нашем случае у нас довольно сложный домен есть большое количество интен тов и часть интен тов похожи друг на друга сложно провести какую-то линию где они отличаются и часть разметчики хотят очень быстро сделать задание получить деньги и как бы пойти дальше А что мы в таком случае делаем используем различные способы для улучшения качества разметок это перекрытие то есть обязательно каждое задание размечают Три разметчик это обучение раз то есть мы в начале учим разметчик справился с этой задачей их Ани поты - это какие-то пред замеченные нами сэмплы которые мы подмешивать в основные задания и смотрим насколько хорошо разметчики вообще справляются с этими заданиями отлично поговорили про то как мы собираем данные собрали допустим наш хороший датасет теперь хочется поговорить про то какие архитектуры мы используем как обучаем модели А здесь тоже есть свои проблемы То есть как я сказала нам нужно быстро добавлять интент на прот и если вспомним про какие-то классические подходы то там к сожалению не всё так просто первый подход который приходит на ум для того чтобы быстро создавать интент - это регулярные выражения то есть достаточно просто написать регуляр на интент и запустить его но к сожалению регулярка не учитывает семантический смысл и если будет какой-то очень сложный интент то у нас будет огро неподдерживаемая регулярка Также можно обучать допустим Берт предсказывать вероятность каждого интен Ну как Мы уже обсудили такая у такой модели Будет долгая итерация переобучения то есть нужно каждый раз переобуть модель пере деплоить её Хочется как-то работать с минусами обоих подходов то есть с одной стороны быстро запускать интент а с другой стороны учитывать семантический смысл как это хочется делать например мы нашли какие-то срабатывания на проде или сри допустим 30-50 примеров на интент мы придумали для него название добавили эти примеры и запустили интент на проде Какая архитектура Может в таком случае под капотом работать можно использовать для этого алгоритм на основе эмбеддинг и knn вспомним Как это работает допустим по какому-то интен у нас уже есть какой-то набор примеров которые туда относятся мы получаем по этим текстам их эмбеддинг и затем складываем в knn индекс на основе н индекса уже сможем потом искать ближайших соседей и получать наши предсказания То есть когда нам приходит какая-то новая фраза с прода например хочу узнать статус возврата мы получаем её динг с помощью кна ищем В Н индексе ближайших соседей находим какие-то ближайшие по смыслу тексты которые там лежат и метрику близости затем агрегирующие предсказать его нашим сервисам и соответственно начать помогать клиенту такая схема в целом хорошо будет работать но достаточно нужно для этого подобрать какое-то количество параметров Чтобы она это делала хорошо и ключевым элементом в данной система является сенн Bader то есть Насколько хорошо он будет мапить ваши текста в векторное пространство настолько хорошо В целом и будет работать данная система поэтому хочется поговорить про то как мы обучали Дин изначально у нас была базовая модель Роберта которая была предо на mlm на банковском домени Но на самом деле как базовую модель можно брать осные модели для русского языка они тоже будут хорошо работать такую модель плохо использовать как р потому что она ничего не знает про ваши интен поэтому Давайте адаптируем такую модель с помощью Ринга на датасете ин классификации напомню Что такое Le У нас есть две головы мы подаём туда две фразы на вход у нас получается два вектора и затем с помощью Лоса мы будем приближать эти вектора в пространстве или отдалять А как это будет выглядеть с помощью контрасти Лоса допустим у нас есть две фразы которые принадлежат одному интен и они будут являться позитива друг другу контрасти ифлос в таком случае будет приближать их в пространстве если у нас есть две фразы которые принадлежат разным интен тамм они будут негатива и контраст Лос будет Оля их в пространстве таким образом с помощью Лоса можно учить р хорошо мапить ваши интент и похожие тексты по смыслу в векторном пространстве и в целом для такой задачи можно использовать не только contrastive L Но другие лосы И вообще про эксперименты параметры можно почитать в статье сенбе там довольно хорошо и подробно это описано мы провели большое количество экспериментов попробовали разные лосы и для того чтобы это сделать нам что нам нужно сделать библиотеку для обучения Динго её мы делали на основе осной библиотеки sent Transformers там уже реализовано как раз большое количество лов Train val loop но мы решили дополнить её какими-то вещами которые нужны именно Нам для нашей задачи первое что мы добавили - это генерация датасета пар то есть на основе изначального датасета ин классификации мы получаем датасет парзи негативов которые как раз используются для обучения Бера также мы добавили выбор параметров обучения То есть все параметры удобно задать из конфига вашу базовую модель Los и затем после обучения Бера положить рядом конфиг и таким образом все эксперименты будут воспроизводимые и также валидация решения является ключевым элементом потому что нам каким-то образом нужно сравнивать сенбе между собой и смотреть как они вообще справляются с нашими downstream задача поэтому хочется как раз поговорить про то как мы лидировали наши сенде первую задачу которую мы смотрели это поиск ближайших примеров мы использовали датасет вида текст запроса То есть это какое-то пользовательское сообщение и похожие тексты которые у нас есть мы прогоняем текст запроса через эмдер и косинус ную близость и смотрим какие у нас похожие тексты попадают в топ если у нас в похожих текстах содержится тот же что и тек запроса то у нас будет увеличиваться Метрика метрики которые Мы считали - это метрики ранжирования mean average precision и mrr и также все метрики Мы считали на отложенных интен тах то есть часть интен тов мы не использовали в начале при обучении нашего Эдера мы их специально отложили и потом смотрим С каким качеством у нас работают на таких примерах наш sentence eder тем самым мы воспроизводить продоются какие-то новые тематики на проде и наш эмбеддер должен хорошо на них работать следующее На что мы смотрели - это распределение расстояний а смотрели на расстояние между позитива и на расстояние между негатива получили вот такую картинку синим - это расстояние между позитива красным между негатива То есть можно посмотреть что расстояние между позитива у нас гораздо меньше чем между негатива тем самым можно валидировать Эмбер и говорить о том что он Да действительно хорошо разделяет ваши интен и не нужно придумывать какие-то сложные механизмы для того чтобы разделять затем интент между собой достаточно подобрать какое-то оптимальное пороговое значение и в целом этого уже будет достаточно а что же дальше у нас выходит чат gpt выходит gpt 4 и другие мки и появляется В целом мнение что можно как бы забыть про все наши кастомные модели можно просто использовать LM как я сказала мы не можем использовать генеративные сети для генерации ответов клиентам Но кажется можно использовать LM для инт классификации то есть Давайте просто напишем промт чтобы м предсказывала нам наш интент но есть определённые ограничение у нас регулярно меняются интен и нам каким-то образом нужно актуализировать знания М понятно что переобуть каждый раз мы не можем это очень долго и дорого в контекст в промт всю информацию про отправить не получится потому что доменная область очень большая интен тов очень много и нам каким-то образом нужно это вместить всё А какой же выд выход приходит Давайте Будем подавать в ЛМ только нужную информацию то есть действительно ту информацию которая поможет ей принять решение как это может делать как это можно делать У нас есть сообщение пользователя мы как раз с помощью нашего алгоритма эмбеддинг икн получаем какой-то топ потенциальных контентов к которым может относиться наша пользовательская реплика и затем подаём эти потенциальные интен в промт и просим LM Выбери пожалуйста один из этих потенциальных контентов И скажи Или скажи что ни один из тентов не подходит И тогда там мы переведём на оператора и таким образом LM вернёт намм наш финальный интент и соответственно мы получим предсказание такой эксперимент мы на самом деле ещё Тестируем смотрим насколько это ВС будет работать но на самом деле уже кажется что можно интегрировать старые подходы вместе с новыми подходами и надеемся что это действительно позволит нам увеличить наши продо вые метрики дальше какие выводы из выступления хотелось бы сделать Первое - это то что единый СТК позволяет развивать разные направления одновременно на каком-то этапе развития системы Это в целом уже может сильно ускорить и у простить процессы дальше не забываем про классические NLP со статистика в инструменте по выделению тегов видно что такие простые алгоритмы довольно хорошо справляются и они позволяют получать какие-то важные инсайты из ваших продо вых сообщений А дальше то что качественный эмбеддер позволяет сильно улучшать метрики на вашем домене в нашем случае он позволил хорошо решать задачу кластеризации и выделения интен тов и также такой подход как Дин ин можно использовать как поиск по базе для того чтобы подавать нужную информацию в ЛМ и таким образом внедрять это новое решение И за счёт этого прокачивать ваши метрики у меня на этом всё всем спасибо спасибо большое тебе за доклад Скажи пожалуйста а вот Q который здесь это на оценку доклада или твои контакты это на оценку супер А тогда друзья пожалуйста сканируем QR оцениваем доклад и переходим к секции вопросов и ответов А вот пожалуйста первый ряд сразу приветствую Спасибо за доклад Меня зовут Пётр компания русал Ну и Самый наверное очевидный вопрос может быть кто-то уже хотел его задать если использовать м модели для классификации и поиска нужного нам ответа то как решить вопрос с безопасностью если в запросе содержатся какие-то важные или персональные данные клиента думали ли вы об этом Может быть какой-то пред процессинг Вот можете рассказать об этом более подробно Спасибо А да Отличный вопрос на самом деле у нас выход такой чтобы использовать свои ЛМ поэтому у нас сейчас есть трек по а так вот из этой половины зала давайте я пока проверю вопросы есть ли в чате а добрый день большое спасибо за доклад он был весьма исчерпывающих во многих аспектах Я подготовил вопросы заранее начну с самого короткого А вы показали рейтинг интов относительно промто А какой уровень confidence процента именно определение интен будет являться пороговым до лбк интен что мы ничего не поняли Давайте свяжем вас с оператором Ну в ЛМ мы подаём уже без порогового значения и м нам к сожалению не выдаёт там какое-то пороговое Значение То есть можно её попросить здесь имен именно в схеме мы указали Как именно там текстом она будет генерировать нам ответ и соответственно там какой-то пороговое Значение оно нам не будет выдавать но можно её просить это делать Это тоже помогает то вы уже отказались от базового эмла и используете большую модель А на проде нет на проде сейчас не используем то есть рассказали именно про то как мы Тестируем и что такой подход в целом возможен для решения такой задачи коллеги я предлагаю дальнейшее обсуждение чтобы это не превращалось в диалог перенести в дискуссионную зону А сейчас дать другим людям возможность тоже задать свой вопрос пожалуйста в середине и следующий в том углу зала будет Да здравствуйте Спасибо за доклад было очень интересно у меня один два быстрых вопроса во-первых а вы показывали как гиру ете отзывы то есть с помощью ТФ DF и других cbas методов А что делать если там есть какие-то отпечатки м Если какие-то отпечатки Ну нам нужно как бы да чтобы они там если они там есть на проде то они значит и будут на проде Поэтому в целом Это хорошо что они есть в данных в начале какой-то агрегации то есть мы с опечатка ничего специально не делаем просто на них учимся то есть передо обработку для обучения и для прода вы не собираетесь пока внедрять А ну если мы говорим вот именно про там методы про выделение тегов А да понял Спасибо И второй вопрос вы не пробовали чач ой мки использовать для генерации данных по интен а пробовали м скажем так нет каких-то сейчас классных вещей про которые готова была бы рассказать но наверное если у нас что-то будет Мы обязательно расскажем пока Это нигде не прокрасивости кация интен - Это история исключительно про языковую модель или какой-то контекст пользователя Ну например наличи у него кредитной карты там или ещё чего это используется отличный вопрос на самом деле Вот про текущую парадигму про кото без контекста сейчас вот мы тоже у на стримы Как нам учитывать контекст для того чтобы улучшать это всё и ведение диалога в том числе Спасибо за вопрос коллеги Есть ещё вопросы у кого-то в этой части зала может быть нет так можно микрофон поста доверяете что всё корректно работает вот вы собрали да всю эту модель и потом же вам надо как-то это посмотреть вот у вас 4.000 этих контентов всё ли вы правильно собрали и и там пере собрали Ну там и после пересборка что Всё старое работает всё новое тоже работает да Отличный вопрос тянет на целый доклад если честно Потому что очень сложная задача проверить то что всё действительно хорошо у нас там есть много каких-то этапов есть оффлайн метрики которые мы смотрим на каких-то наших датасета специально отобранных там тестовый датасет или какой-то продо вый СМЛ с продаж то мы всё верно работаем также у нас есть какие-то онлайн-оценки когда мы там размен загружаем какой-то определённый набор текстов каждый день и говорим вот пожалуйста Оцените верно или неверно мы ответили классификатор мы с счёт этого тоже метрики можно собирать Спасибо сейчас я ещё раз проверю нет ли у нас вопроса из онлайна нет вопросов нет Ир Выбирай лучший пожалуйста Два лучших Сначала давай подарим от газпромнефти от нашего партнёра подарок а потом от теньков вот мне понравился вопрос про ЛМ и Безопасность и также был интересный вопрос про контекст у молодого человека Вот спасибо большое и не убегай У нас для тебя тоже есть подарок Спасибо большое за то что ты делишься с нами своим опытом приходи и рассказывай ещё и я предлагаю вам перейти в дискуссионную зону чтобы более глубоко пообщаться на эту тему Всем спасибо"
}