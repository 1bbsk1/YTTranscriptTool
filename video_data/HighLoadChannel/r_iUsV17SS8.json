{
  "video_id": "r_iUsV17SS8",
  "channel": "HighLoadChannel",
  "title": "H264 жив / Иван Емельянов (Дзен, VK)",
  "views": 90,
  "duration": 1890,
  "published": "2023-10-06T07:20:32-07:00",
  "text": "что Всем привет Я из дзена Я в день руковожу разработкой видеохостинга собственно Давайте поговорим о чем я сегодня Вам расскажу во-первых мы поговорим немного о метриках визуального качества и о том зачем они нужны дальше о том какие бывают способы пережатия видео и о том как не совершать наших ошибок не набивать наших шишек но сэкономить до 30 процентов трафика на Казалось бы уже старых и забытых технологиях пару слов о цене что вообще такое Дзен Дзен это мультиформатный блок платформа в которой больше полумиллиона авторов которые регулярно что-то постят Дзен это до одного терабита в секунду видео трафика и около 10 миллионов загружаемых видео в год задача экономии трафика для нас очень важная во-первых трафика становится все больше и больше и как бы это довольно дорогая операция раздавать трафик емкости сиденов ограничены оборудование дорогое соответственно крайне выгодно экономить на раздаче трафика как собственно всему миру с другой стороны есть пользователи которым мы отдаем видео файлики и чем меньше размер этих видеофайликов тем меньше проблема пользователя с воспроизведением меньше прыгает разрешение в процессе воспроизведения меньше буферизации и соответственно пользователь становится более счастливой довольный и лучше Возвращается на сервис чтобы посмотреть видео как база работает видеохостинг есть Автор который приходит загружает какой-то контент этот контент оседает у нас на серверах После чего мы его каким-то образом транскодируем и показываем зрителям Что такое транспонинг И вообще зачем он нужен трансконнинг это некоторое операция над видео которая во-первых ужимает его а во-вторых делает так чтобы можно было видео смотреть на произвольном девайсе в интернете и когда мы говорим про экономию трафика Мы в первую очередь думаем про кодеки про реализацию кодеков история кодеков на самом деле довольно длинная аж до 64 придумали аж в 2003 году и с тех пор он обрел массовую популярность впоследствии были придуманы более современные кодеки более совершенные какие-то были открыты например vp8 вп-9 и av1 какие-то проприетарные h265 сейчас разработки 266 но у современных кодеков есть некоторые набор проблем Какие четыре обладает очень хорошей поддержкой девайсов практически любой девайс интернете умеет играть этот кодек при этом у современных кодеков такой широкой поддержки пока еще нету во-вторых для того чтобы кодировать в эти Новые кодеки нужно тратить сильно больше мощности CPU больше энергии больше денег так как алгоритмы становятся более сложными и соответственно вычислительно емкими Ну и в-третьих для того чтобы доставлять пользователя максимальное удовольствие от просмотра нам нужно как-то заранее кэшировать и сохранять эти видео в разных лодок и соответственно нужно в сторожах больше места тратить на то чтобы хранить разные кодеки Окей А можно ли как-то менять битрейт но при этом чтобы визуально оставалось видео нормальным и соответственно стандартном кодеке мы типа подрезаем качество И на самом деле да можно можно взять видос и руками перебирать параметры и увидеть что Да действительно видос остается нормальным мы сколько-то трафика экономим размеры файл уменьшается но что делать если таких видео у вас десятки или сотни тысяч в день отсмотреть глазами мы их уже физически не можем человечество не стоит на месте оно придумывает всякие способы например придумал метрики визуального качества это такие метрики которые собственно помогают нам как-то автоматически отсматривает такой объем контента существует два основных класса таких метрик это безреференсные такие которые как бы меряют некоторые сферическая вакууме качество видео например Nike типа идея в том чтобы как-то анализировать контент и смотреть не знаю сколько там артефактов или чего-то еще проблема такого подхода в том что если пользователь загружает вам как это видео в низком качестве то на самом деле очень тяжело работать с этой метрикой с этим подходом потому что мы не можем понимать насколько мы хуже или лучше сделали мы не можем этот файл улучшить и не можем никак с этими метриками работать нормально с другой стороны есть референсные метрики которые берут исходный видеофайл берут результат транскодирования пережатия и покадрово сравнивают два видеофайла на выходе считают некоторые потери качества например самая популярная это paysonar Sim или вот у нас есть метрики визуального качества А почему бы нам не взять какую-нибудь целевое значение и все наши видосы в это целевое значение не закодировать например Давайте возьмем каком-то не знаю вымов возьмем какой-то порог и все видео будем туда фигачить как это сделать берем бинарный поиск и бинарный поиском перебираем B3 до тех пор пока не придется значение целевой метрики мы попробовали разные варианты метрик попробовали разные варианты существующих меток попробовали разные варианты их агрегации всякие минимальные средние перцентили и прочее и поняли что на самом деле это не очень правильный подход потому что метрики показывают разные результат для разных видосов Например можно пережать видео достаточно хорошо визуально но при этом метрики будут давать плохую скор с другой стороны есть видео на которых метрики дают хороший скор но при этом видео визуально плохое видно артефакты что нас не устраивает получается что метрики на самом деле не всегда коррелируют с человеческим восприятием что они несовершенны но можно сделать другие выводы что на самом деле Чем выше метрики у нас получается относительно одного и того же файла тем выше визуально качество сохранится если у нас метрики более-менее одинаковые то визуально мы не заметим разницы А если Метрика падает то непонятно насколько сильно визуально деградирует файл он может деградировать до появления артефактов Окей Можно ли вообще взять битрейт и как-то его уронить И что с ним сделать Мы подумали Можно ли взять и изменить вообще как бы вопрос Можно ли уменьшать битрейту видео и при этом оставлять хорошее визуальное качество попробовали построили распределение построили зависимость визуального качества от битрейт и поняли что блин Нет нельзя К сожалению всегда когда мы уменьшаем битрейд при стандартном подходе Мы всегда теряем сколько-то визуального качестве получается что мы на самом деле не понимаем сколько мы можем потерять чтобы оставалось хорошо соответственно мы либо очень мало теряем визуальном качестве и не экономим достаточно битрейты либо теряем больше Но тогда файл может очень сильно деградировать мы такие блин Что дальше делать непонятно а давайте Может посмотрим как конкуренты делают говорят вот на YouTube есть и качество хорошее и битрейт вроде оптимальные Может посмотрим как они делают может быть попробуем также мы значит посмотрели взяли один видос свой попробовали его закодировать примерно с тем же Кашин Как кодирует YouTube и посмотрели собственно что получается на выходе и увидели что действительно нас есть разница что у YouTube в некоторых местах качество визуально лучше чем у нас и начали разбираться Почему вообще такой может происходить мы попробовали измерить параметры кодирования попробовали оставить старые но поменять подход перешли от двух проходного кодирования к однопроходному в чем вообще разница между этими подходами однопроходное кодирование или кодирование с РФ это типа такой специальный фактор который пережимает все кадры видео примерно одинаково соответственно с одной стороны хорошо что визуально более-менее стабильная картинка с другой стороны Плохо что результат такого кодирования он не гарантирован то есть файл может превышать наши там заданные какие-то ограничения двухпроходное кодирование работает по-другому первым проходом мы снимаем какие-то статистики по видео и вторым проходам энкодер кодирует файл в заданное ограничение соответственно за счет того что от CRF это однопроходный алгоритм то он работает примерно в полтора раза быстрее чем двухпроходная Но с другой стороны соответственно результат не гарантирован файл может весить сильно больше вот мы значит попробовали перейти и увидели что Ого а у нас получается примерно так же как и на YouTube картинки становятся более-менее похожими или практически одинаковыми мы такие задались вопросом а Можно ли взять и подобрать CRF для всех наших видео и как это делать Это автоматически что для этого нам нужно для этого нам нужно собрать dataset собрать фичи и каким-то образом предсказывать собственно CRF как Мы собирали dataset мы взяли короткие видосы с ними проще всего работать нашли на видео похожие видосы по текстам описаниям на видео поиске И дальше с помощью фингритов Нашли уже точную дубликаты Чтобы построить картинку целевых качеств Как искать фичи мы поисследовали и нашли ряд статей где описаны Какие фичи над видео можно вообще снимать Один из таких подходов Это Давайте возьмем видеофайл базовые его стандартно откодируем и снимем какие-то метрики статистики результат этого кодирования например битрейт выходного файла или процент мокро блоков на выходе помимо статистик базовых мы можем еще посчитать какие-то формулы и посчитать какие-то такие фичи которые описывают динамичность и сложность видео Что такое динамичность это когда у вас видео очень сильно меняется кадра кадру Что такое сложность это когда у вас очень много мелких деталей внутри кадра соответственно Нужно больше информации чтобы сохранить значит построили фичи получили модель обучились все хорошо вроде как предсказывает какие выводы мы дальше сделали не все так радужно как оказалось но первое в принципе сам по себе CRF неплохо и он может ускорить нам кодирование в некоторых местах второе что на самом деле его можно подбирать и можно подбирать таким образом чтобы ограничивать размер файла и делать это как регулярно И третье что этим может нам помочь имэль Мы можем обучить такую формулу которая будет собственно предсказывать этот но проблема в том что таким подходом мы все еще не экономим достаточное количество размеров файлов недостаточно количество трафика и для того чтобы экономить больше нам нужно уже как-то терять визуальное качество А мы не знаем насколько много Мы можем потерять мы не знаем как это зависит от как размер видеофайлов как размер качества зависит от польского усмотрение мы не знаем как они друг друга влияют мы такие Окей хорошо что еще можно сделать Можно ли вообще как-то еще вот покрутить параметры что-то придумать такое чтобы можно было все-таки добиться какой-то экономии и на самом деле есть такая идея что у реализации энкодеров у них очень много параметров ручек которые можно дергать которым можно влиять на результат кодирования Например у x264 их больше 70 есть даже статья от лаборатории компьютерной графики МГУ которая рассказывает про то что можно подобрать такой набор это оптимальных пресетов для видео который будет эффективнее работать Чем стандартный пресеты например они могут быть быстрее Или качественнее или быстрее качественнее относительно стандартных подходов в качестве смысле битрейты быстрее В смысле производительности вот такие интересный подход хотим также что нам для этого нужно сделать во-первых нам нужно собрать параметры кодека во-вторых нам нужно из этих параметров найти пресеты в-треть их нам нужно собрать какой-то dataset Видео и дальше обучать какое-то предсказатель который будет собственно в этот пресет кодировать видос как мы искали параметры видео для того чтобы их найти мы собственно попарсили специализированные сайты по энкодингу видео подчеркнули оттуда поисследовали статьи про кодирование видео почерпнули оттуда и преследовали стандартные пресеты посмотрели как еще можно сделать какие там еще есть параметры В итоге собрали что-то около 50 параметров кодирования Как собрать пресеты 50 параметров это очень много это бесконечно долго вычислительно считать перебирать невозможно но есть генетический алгоритм который могут это сделать за нас и сделать это достаточно эффективно в итоге мы умеем кодировать один видос в кучу пресетов Искать эти собственные пресеты и дальше с этим работать что собственно такое генетический ритм и конкретно nsg2 это такой алгоритм достаточно стандартный который отбирает лучших поколений и повторяет цикл но благодаря именно этому мы можем оптимизировать не одну метрику а две в нашем случае это скорость кодирования и результат результирующий битрейт соответственно на выходе кодирования мы имеем некоторый трехмерное пространство какие у нас измерения это битрейт это метрики визуального качества и это скорость кодирования при этом визуальное качество мы пытаемся сохранить примерно на том же уровне как у нас было в оригинале как у нас было до всех изменений и вот фиксируя целевое значение визуального качества Мы научились строить такие же картинки по распределению пресетов и увидели что даблин клево У нас есть наш подход который работает относительно неплохо и есть куча других хороших пресетов которые работают и быстрее и лучше чем наш стандартный подход как собственно фиксирует это самое целевое качество Да все также мы Бин поиском перебираем в данном случае уже CRF и ищем такой который бы максимально нас удовлетворял как мы теперь собираем dataset для того чтобы продолжить работу берем 100 случайных видео снимаем по ним наши текущие метрики визуального качества после чего запускаем после чего запускаем генетический алгоритм и получаем некоторые результат пресетов плюс метрик эти метрики пресетов мы каким-то образом агрегируем из чего получаем новый набор пресетов И дальше повторяем цикл то есть берем 100 новых случайных видео накладок на них наши полученные пресеты видим что На некоторых видео пресеты не работают или работают плохо и эти видео мы еще раз прогоняем через алгоритм генетически и таким образом повторяем цикл как мы агрегируем метрики мы убираем те которые не дотянули по визуального качества мы убираем те которые оказались слишком долгими по производительности и выбираем топ по битрейту в итоге мы собрали что-то около 3000 видосов в нашу корзинку 3000 видосов картинки Это примерно 150 пресетов на выходе мы такие Блин наверное 150 как-то много наверняка какие-то из них очень похожи между собой наверняка можно как-то их проредить Да и в принципе Выбирайте 150 довольно сложная задача мы провели исследование поняли что для каждого для каждого разрешения А у нас их больше одного для того чтобы максимизировать удовольствие пользователя можно подбирать свой пресет и это может давать какой-то вклад в экономию трафика тогда мы взяли все наши видео Плюс разрешение и закодировали их во все 150 процентов дальше мы для каждой такой пары выбрали топ-1 по битрейту И жадно набрали топ по топ пресетов который максимально нам экономит трафик получилось что-то около 15 пресетов и вот у нас получается классный хорошо размеченный dataset видосов плюс целевых пресетов и CRF что дальше дальше нам нужно научиться выбирать оптимальный пресет сходу приходит в голову идея что можно просто взять и просто закодировать во все эти пресеты Но это довольно дорого и сложно тогда можно как-то научиться предсказывать Какой пресет нам нужно выбирать но для этого нужны фичи каких мы использовали Ну во-первых мы взяли те фичи которые у нас получились в подходе с Ютубом во-вторых мы добавили еще новых тяй и сайфичей которые помогают нам лучше понимать про состав видео в итоге мы получили полный dataset свечами обучили модель попробовали разные подходы разные алгоритмы победил свм с некоторым качеством и дальше сделали аналогичный подход для того чтобы подобрать модель для CRF и закодировали ну получили модель srf Почему у нас две модели по сути получается так что пресет на помогает определить категорию видео и таким образом мы выбираем категорию А CRF это уже некоторое надстройка следующий этап когда мы как бы для данного набора параметров определяем какой CRF нам нужно подобрать чтобы он укладывался в нужные нам ограничения по битрейту или для достижения правильного метрики визуального качества что в итоге получилось Получилось какая-то монструозная формула кодирования видео для какого-то произвольного исходника вот мы значит взяли исходник получили вот такую формулу исходник довольно высокого качества и применили кодирование что мы получили а получили мы что если раньше у нас видос кодировался в 5 мегабит секунду то теперь у нас кодируется в 3 мегабита в секунду мы такие Вау Очень круто а что еще начали Смотреть дальше и поняли что на самом деле Этот новый подход он не только улучшает старый подход с тем же кодеком но еще и улучшать наша кодирование Вы по 9 со стандартным набором настроек а это значит То что мы можем вообще от него отказаться и больше в него не кодировать мы такие супер надо это катить в Пруд как это делать как у нас раньше работал механизм транспортирования есть некоторые сетка битрейтов константная для каждого разрешения и мы двух проходным кодированием эту сетку накладываем на файлы Как стало мы считаем для каждого разрешения свои фичи после чего для каждого разрешения определяем свой пресет затем свой CRF и однопроходном кодиром кодируем файл для показа пользователя супер класс выкатили увидели что да клёво уронили значительно трафик удалось значительно сильнее пережать файлы но есть некоторые проблемы какие на самом деле наш подход оказалось что замедляет кодирование в итоге это из-за этого мы смогли запустить только кодирование ночью и соответственно пережимать только какой-то топ по трафику Ну и естественно мы не всегда успеваем за популярностью видео в итоге нам приходится как-то их догонять Мы хотим все-таки максимизировать наши результат что делать Ну что делать ускоряет какой у нас узкое место узкое место Это расчет свечей фичи считать это еще одно кодирование базовое на основе которого мы собственно статистике насчитываем а это значит что у нас получается на круг еще 5 кодирование для каждого разрешения для каждого разрешения свое и приходит идея А можно ли Вообще переиспользовать фичи и считать их один раз они несколько мы проверили Да действительно можно причем можно брать не просто от какого-то там среднего типа 480p ч а брать сейчас самого низкого разрешения и переиспользовать их благодаря тому что мы берём сейчас самого низкое разрешение мы на самом деле значительно ускоряем этот процесс расчёта типа чем разрешение ниже тем быстрее пробегает расчёт свечей и Соответственно в этом месте мы типа раз пять сэкономили а что в итоге В итоге оказалось что суммарно нам удается не просаживать скорость кодирования и соответственно мы можем внедрять этот подход на всем потоке как у нас теперь выглядит процесс транкодирования Мы рассчитываем один раз фичи дальше для каждого разрешения выбираем пресет затем CRF и затем кодируем файл что в итоге В итоге получилось что мы на 30 процентов В среднем уронили трафик для всех видеофайлов и до 45 процентов экономика на высоких разрешениях на Full HD клёво что дальше Что Дальше что можно делать дальше на самом деле Можно повторить подход Для каких-нибудь более современных кодеков например 265 дальше Можно повторить подход для каких-нибудь GPU семейств кодеков и соответственно лучше кодировать эффективнее кодировать на GPU и Можно повторить подход и глубже залезть файл и заняться Персен кодированием соответственно разбивать файл на отдельно независимые сцены лучше кодировать каждого из этих сцен и на выходе максимизировать результат в принципе подведем итоги что получается мы сэкономили трафик мы сделали это на тех же условиях Мы закодируем в то же время на тех же мощностях Мы за счет эффективной эффективного кодирования лучше сохраняем еще и сторож мы меньше денег тратим на каждый файл по сути И что самое классное На самом деле смогли даже отказаться одного из современных кодеков потому что он стал проигрывать нашему новому подходу и соответственно мы смогли сэкономить себе в этом месте и ядра и сторож в принципе на этом У меня все спасибо ставьте лайки голосуйте за мой доклад и приглашаю вас всех на автопати к нашему стенду так для наших онлайн слушателей я Напоминаю что мы ждем ваши вопросы в наш онлайн чат или вы можете задать их по telegram-боту давайте пока у нас онлайн вопросов нету начнём с зала Здравствуйте спасибо за доклад а Простите хотелось бы задать вопрос наверное по поводу того Каким образом типа планируете дальше масштабироваться смотрите то есть полезете ли вы в ту же самую ходу на гпухи Или например может быть возьмете ну к примеру в пи вот мы насчет вот этого не думали потому что все в CPU там даже голды не думали думали но мы хотим на гобу попробовать мы на самом деле уже сделали подход но пока не получается экономить трафик без деградации визуального качества То есть пока там придётся видимо разменивать как-то вот Но вот с новыми кодеками да возможно и там как раз есть планы в них вложиться вот ну и плюс вот пирсон кодирования на самом деле может дать еще больше Профит потому что можно будет каждую сцену сильнее опережать Независимо друг от друга если Ну там есть проблемы что потом плеер может Сойти с ума и неправильно наполнять буфер Но кажется что подход более-менее рабочий так можно пробовать дальше Спасибо за доклад вопрос 254 кодек это стандарт и у него есть разные имплементации здесь два вопроса какой имплементацию вы используете и используете ли вы хардварные имплементации мы стандартную 264 в беге использовали и оптимизировали Спасибо прерву немножко вопросы зала вопросам из онлайна сейчас продолжим вопросами зал А почему для изначального отбора пресетов использовали генетический алгоритм пробовали ли что-нибудь другое генетический алгоритм нам помогает не отбирать пресеты а формировать их типа у нас получается дофига параметров у каждого параметра очень много вариантов опций это размер значений и в итоге перебирать их очень долго собственно генетически алгоритм делает это эффективнее чем бортфорс А что-то еще пробовали кроме генетических алгоритмов а нет не пробовали поисследовали вроде как генетическую получает самый эффективный взяли заработало чуть-чуть экспериментировать я понял дальше издала Добрый день спасибо за доклад в своей работе вы ссылались на исследование кафедры компьютерной графики МГУ И относительно недавно они приводили тоже другое исследование в котором сравнивали корреляцию референсных меток субъективными и выяснилось то что в принципе что их можно хакать то есть там пиксель буквально поменять и Метрика будет другая и второе что для современных кодеков они уже не актуальны то есть по факту между субъективным и значением метрики может быть отрицательная корреляция то есть картинка стала для человека хуже Метрика показывает что она лучше Ну вопрос-то собственно в том как-то дополнительного это анализировали и может быть у вас какая-то своя группа для опроса была там голосования после просмотра видео вы показывали вот в этом направлении на самом деле на самом деле мы сделали примерно то же самое мы выяснили что вот как раз итоговые метрики они как-то очень ну Независимо друг от друга влияет на результат типа у нас есть как раз примеры когда вроде как метрики Норм но визуально не норм и мы на этапе вот исследовательски работы в этом месте как раз и обнаружили вот этот вот сходство Ну и дальше со статьей знакома действительно корреляция у них не очень собственно вот хаканье метрик по сути происходит в наши первые идеи когда вот мы пытались подогнать под целевое значение в микрофон Можно по итогу к чему пришли что вы все-таки докрутили свои пресеты чтобы мы пришли к тому что типа мы не должны метрику ронять но мы можем как-то зафиксировать и типа либо в неё кодировать либо выше вот там можно какое-то Дельту задать там на типа третье четвертый знак в метрики но в целом вот главное не сильно ниже кодировать тогда будет норм ну и ещё один небольшой вопрос там приводили примеры команды действительно используете 264 свободным доступе или у вас какие-то свои там патчи улучшалки поверх него а-а нет в этом месте мы используем стандартный x264 и на нём выжимаем всё стандартные какие-то патчи У нас есть в районе расчётов по сути вот в процессе расчётов фичами как раз кодируем стандартно и дальше там какие-то статистики вытягиваем Спасибо друзья рядом по центру еще вопрос Здравствуйте спасибо за доклад У меня два вопроса первое не пробовали ли метрику который называется певка певка Ну соответственно тоже самое ну в смысле дает метрику качество И второй вопрос они работали со звуком не пытались звук пожать чтобы тоже чуть-чуть хотя бы поменьше сделать Спасибо так первая певку не пробовали надо будет попробовать посмотреть поизучать про звук думали я постоянно про него думаю но пока ещё никакого подхода не сделали Пока ещё стандартно вот со звуком работаем Спасибо И вот разбавлю вопросы за онлайна а планируете ли вы использовать кодек av1 И что в целом о нем думаете когда-нибудь планируем прямо сейчас у него очень низкая поддержка среди наших пользователей и не очень целесообразно прям бежать в эту сторону Да мне кажется тут главный вопрос Это широта охвата использования кодеков Да и там условно даже если есть технологические преимущества но конечные устройства пользователей не поддерживают этот Ну да куда деваться друзья еще вопросы Вань тогда по нашей традиции либо человеку зала либо человеку из онлайна надо выбрать лучший вопрос и достанется небольшой Презент так А можно еще раз песка вопросов онлайн У нас был про кода кв-1 и про генетический алгоритм для пресетов оффлайн соответственно вот три вопроса которые у нас здесь звучали наверное больше всего мне понравился Вопрос вот товарища который про исследование от кафедры МГУ и вот ну про метрики про то что мы делали тогда Какая подарок получается своего обладателя Вань мы тебя благодарим за то что ты рассказал нам про прекрасный эксперимент и про положительные результаты соответственно от организаторов конференция программного комитета тебе небольшие памятные подарочки чтобы ты на них любовался и приходил к нам еще и еще рассказывать про интересные темы друзья ловите ванюшку ларах конференции там дискуссионная зона А на этом зале H4 У нас заканчивается программа первого дня и спасибо"
}