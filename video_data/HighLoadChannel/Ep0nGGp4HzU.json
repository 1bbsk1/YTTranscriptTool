{
  "video_id": "Ep0nGGp4HzU",
  "channel": "HighLoadChannel",
  "title": "Как выжить под нагрузкой, имея 100 Тб в нешардированной MongoDB / Андрей Комягин (STM Labs)",
  "views": 2303,
  "duration": 2441,
  "published": "2023-04-28T06:19:52-07:00",
  "text": "друзья Всем привет Меня зовут Андрей комегин и я ситео в компании Steam labs мы занимаемся проектированием и разработкой с нагруженных систем сделали достаточно много систем в том числе государственных федеральных и сегодня тоже я хочу вам рассказать про такой кейс который из реальной жизни но немножечко о том чем мы занимаемся как мы это все проектируем мы очень широко применяем различные наверное сейчас все их применяют Ну мы не исключение этой базы данных и брокеры очередей и кэширования и все что угодно В принципе все построено на опенсорсе конечно это все допиливается дорабатывается потому что ну у каждого проекта своя специфика и нельзя просто взять решение применить как есть но иногда Можно иногда нет чаще всего нельзя немного предметной области то есть прежде чем перейдем проблематике Что за проект вообще какая специфика предметной области это такой класс систем как который называется Track and Race для чего они вообще нужны Да что они делают может быть кто-то первый раз вообще слышит такое понятие система это такая большая глобальная система которая обычно развертывается масштабах страны нескольких стран которые занимаются обеспечением единого непрерывного процесса отслеживание статуса товара логистического логистических цепочках начиная от процесса производства и заканчивая тем этапам когда товар собственно попадает покупателей вот там продают на кассе и так далее там или уничтожают списывают все что угодно любое конечное состояние вот примерный план движения товара вы можете видеть на экране конечно говоря кейсе который мы с вами сегодня поговорим о проблематике нельзя буквально в двух словах не остановиться потом пай плане который используется в системе для процессинга данных в принципе он применяется Ничего здесь нового то есть источник данных эти данные поступают входящую очередь входящую очередь построена на базе Кафка и дальше собственно идет микросервис который высчитывает эти данные из очереди передает на процессинг и результатом процессинга является некий новый State некий новый State но некий новый статус у сущности который нам нужно сериализировать и сериализируем А куда в mongo db ну исторически сложилось то есть выбрано было такое решение стерилизуем это монго тебе и Казалось бы да монго тебе достаточно современная СУБД где здесь могут возникнуть проблемы потому что вендор все продумал Да изначально из коробки все имплементировано в части возможности горизонтального масштабирования шортирования все в общем-то есть из коробки да Но как это часто бывает наверное не видел ни одного проекта где все продумано спроектированной модели нагрузки все просчитано все всегда чаще всего на начальном этапе в погоне за функциональными возможностями мы напрочь просто забываем как у нас развернута база данных там в каком она режиме там шардированном это там просто сингл реплика сет или так далее там георепликации Я вообще никогда не видел на начальном этапе проекта Ну собственно Наш кейс не исключение все ровно точно так и было То есть исходная ситуация исходная топология СУБД это база данных развернута в режиме сингл реплика сет Что такое сингл реплика сет это просто обычный кластер который реализует механизм мастер-слей стандартный и автоматическое переключение между узлами собственно вот в таком режиме собственно все и жила скажем так но куда мы идем Конечно хочется иметь целевую топологию СУБД то есть полноценный шарнированный кластер со всеми элементами Да там конфликт серверам он газ и так далее То есть несколько шардов но мы имеем что имеем то есть мы имеем то что нарисовано вас на слайде с левой стороны Итак давайте поговорим о тактико-технических характеристиках вообще о той нагрузки которая падает на систему об объемах данных ну нагрузка на запись медиана это порядка 10 тысяч операций в секунду нагрузка на чтение конечно же побольше это порядка 15-20 тысяч операций в секунду топологии как я уже сказал ранее это сингл реплика сет и размер тасета порядка 100 терабайт чуть побольше немножечко уже начинаем напрягаться до 100 терабайт это многовато Ну и конечно же важно поскольку у нас доклад ограничен временными рамками Мы тоже ограниченное имя я не буду рассказывать о некоторых аспектах То есть я не буду рассказывать рецептов построения архитектур здесь все зависит от специфики от конкретных требований и очень важно эту архитектуру подбирать под конкретный проект под конкретное решение не буду рассказывать про горизонтальные масштабирование как его делать Для различных компонентов системы в целом не буду останавливаться на масштабировании самого кластера Кафка не буду рассказывать про семантики доставки в Кафка да то есть вот эти вещи а может быть кто-то и первый раз сейчас услышал такой термин семантика доставки в Кафка что это такое есть три семантики это листванс это Most Vans и exect levance почитайте Кстати это очень важная история ее тоже лучше продумать до того как вы стартанули проект про это про гарантии доставки Ну и конечно я буквально вкратце коснусь сама архитектура СУБД отказоустойчивости масштабирование то есть об этом мы конечно же поговорим но это буквально вкратце предполагается что это этот момент вы знаете но не буду вас томить перейдем к основной проблематике Итак был прекрасный солнечный день ничего не предвещало беды система перемалывал довольно интенсивную нагрузку Ну технические характеристики Вы видели все работало хорошо Ну в один прекрасный момент все ломается сборная система мониторинга сходит с ума служба эксплуатации засыпает алёвертами В общем полный Ахтунг вот здесь у меня изображен рост лагов Кафка рост входящих очередей то система не вывозит входящую нагрузку всё что же делать Давайте разбираться первое что мы сделали Мы естественно включили онлайн профилирование то есть включили профилирование в приложении Надеюсь вы тоже предусмотрели что возможно включить профайлер прямо напроде если не предусмотрели обязательно Это сделайте включили профилирование и ничего нового узким местом у нас оказалось что BD то есть обращение к базе данных вспоминается слова классика никогда такого не было И вот опять Ну лезем ищем дальше очередной душборд в системе мониторинга данный экспортера mongo db Ну и конечно нас интересует блокировки что мы видим на блокировках мы видим собственно полку Все ожидаемо мы видим полку То есть у нас блокировки мы уперлись Манго тебе используется движок Кто не знает Wire Tiger и для организации конкурентного доступа там есть такое понятие как read and tickets то система выдает так называемый тикеты на определенные атомарные задачи с базой данных вот собственно мы имеем такую полочку прекрасно Да паникер уже все все кричат все пропало помогите но мы не опускаем рук включаем холодный Разум и начинаем думать дальше Давайте вспомним Как устроена схема хранения файловка тебе именно при использовании движка дефолтного Wire Tiger тут очень просто все в монго тебе вместо таблиц используется такое понятие как коллекция фактически аналог таблицы напомню что Манго тебе это у нас документы ориентированная база она хранит документы значит документы хранятся в коллекции вот на одну коллекцию у нас приходится фактически один файл то есть одна коллекция один файл с расширением WT то же самое и с индексами То есть каждый индекс это отдельный файл залезаем в папку с данными и что мы там видим мы там видим файлы размер которых измеряется собственно терабайтами все Приплыли чуйка нам уже намекает что проблема именно в размере файлов работать с большими файлами мягко говоря проблематично в общем-то Здесь проблема тоже в этом начинаем что как-то решать эту проблему да то есть у нас напомню проблема в том что у нас деградация при вставке в очень большую не шортированную коллекцию собственно это подтвердили размеры файлов Ну и давайте думать над решением задача оно в принципе на поверхности она напрашивается Давайте делать Сплит то есть пилить Вот это огромный файл на кусочки Давайте идем к инфраструктуре говорим инфраструктура Дай нам пожалуйста серверы под Ну под развертывание шардированной базы данных структура говорит Извините у нас как бы оборудование нет инфраструктуры нет немножко тревожно становится Да думаем что же делать Давайте пилить плит на уровне приложения То есть фактически превратим эту одну большую супер коллекцию куча маленьких коллекций и запилим шортирование на уровне приложения Ну вариантов нас собственно не осталось никаких других А давайте так и сделаем Ну надеюсь у вас приложение есть выделенный слой по работе с данными если он есть то в принципе здесь все достаточно просто просто запиливаем маршрутизатор мы взяли фактически функцию хеширования То есть как обычно шортирование устроено в СУБД Там просто используется алгоритмы хеширования широко и кэш функции позволяет распределять данные фактически достаточно равномерно если правильно подобранных функция прохожесть функции их выбор мы говорить тоже сегодня не будем Потому что ну это тема отдельного доклада Мы в своём решении выбрали функцию Адлер 32 и вот по алгоритму который вы видите на экране мы вычисляем фактически хэш и берем остаток от деления от N где N Это количество кусков которые мы хотим собственно разбить нашу большую коллекцию классно Давайте реализуем не буду здесь останавливаться на деталях реализации если кому-то будет интересно можем отдельно обсудить после доклада в принципе мы это реализовали достаточно быстро в течение дня прогнали это на стенде нагрузочному тестировании сняли профили все нормально никаких проблем нет но Осталось одно но да то есть вот все это решение у нас заточено под то что у нас есть пустой taset То есть когда мы выкатываем это на провод Мы у нас все эти вот коллекции они пустые данные все еще находится Вот в этой большой супер коллекции Что же делать да мы же не можем просто сейчас взять все это остановить увести в даунтайм и там начать миграцию данных растаскивать их по кусочкам нам никто это не позволит сделать Окей тогда будем делать это прямо в runtime по мере обращения к данным То есть как только мы Обращаемся каким-то данным которые лежат в большом блоке супер коллекция мы эти данные сразу же на лету иммигрируем собственно в наш вот этот вот кластер новый Да в новый набор коллекций которые мы здесь придумали конечно же важно оценить плюсы и минусы такого подхода Да когда он подходит когда его можно применять когда его нельзя применять такой подход применим Когда вам надо потушить пожар да Когда у вас нет возможности от шардироваться на уровне базы вот прямо сейчас да у вас нет инфраструктуры вам бизнес говорит Хрен тебе не даунта им у нас все 24/7 у нас там четыре девятки вот это вот все да Ну как обычно ну плюс очевидный что мы не останавливаем систему Да у нас нет Down Time Все работает все летает заказчики практически ничего не замечают Ну и подход позволяет ускорить ставку за счет того что мы уменьшаем количество блокировок в базе и не требует начальной миграции Так иммиграцию мы делаем прям фоне прям в ран тайме по мере обращения к данным минус очевидно тоже они есть какие минусы минусы то что мы фактически все управление данными перенесли куда на уровень приложения изобрели велосипед очень плохая история но это та жертва который нам пришлось принести деваться тоже некуда Теперь стало сложно искать данные Да потому что вот у нас теперь 100 кусков и где лежит наш документ мы не знаем чтобы определить где он лежит нам надо вычислить Хеш функцию вот это вот всю математику который я привел на предыдущем слайде Да поэтому быстренько напиливаем утилиток которые нам локализуют эти данные отдаем их там техническую поддержку всем кому они нужны и все все счастливы ну и вопрос такой в зал Кто считает что в принципе можно расслабиться и больше ничего не делать Поднимите руки все верно потому что все равно мы рано или поздно упремся возможности вертикального масштабирования то есть они есть у нас как бы сервера не бесконечные и этих пределов мы практика показывает скорее рано чем поздно Мы их достигнем Поэтому двигаемся дальше думаем дальше что же нам делать да Как нам прийти к целевой топологии и кейс номер два это Значит нам нужно каким-то образом запустить осуществить вот эту миграцию Да перейти на целевую топологию на шардированный кластер и мы здесь подумали и пришла такая гениальная идея Почему бы не запустить параллельную эксплуатацию двух суббоды одновременно да то есть взять старый шарнирный кластер наш оставить его в покое оставить данные в нем в покое Пусть он все работает Пусть там все живет поднять параллель новые шортированный кластер и все свежие данные собственно лить в новый шарнированный кластер классная идея осталось только придумать как это все сделать да а здесь все тоже достаточно просто смотрите То есть у нас фактически в Старой не шортированной базе с точки зрения ключей доступа все статично То есть если мы перестаем с определенного момента лить туда данные дата набор ключей доступа А монго тебе это фактически киевелью у нас остается неизменяемым и это нам наводит нас на Мысль о том что можно реализовать маршрутизацию запросов между этими двумя базами данными на основе классического подхода какой классический подход маршрутизации это использование статической таблицы маршрутизации но вы скажете как же мы так и построим такую таблицу огромную там же миллиарды записей все верно Да Файлик нам тут не поможет нам придется что-то мудрить а почему бы нам не поднять еще один кластер под хранение статической таблицы маршрутизации так и сделаем поднимем третий кластер и так целевая архитектура решения с параллельной эксплуатации Она выглядит у нас следующим образом с левой стороны у нас шардированный кластер Да вот этот большой блок в котором там монгосы конфиг сервера шарды вот это вот все Все по красоте все то что должно быть в шарнирном кластере значит соответственно с правой стороны наши старенькая не шортированная база монструозная и мы ее оставляем какая она есть такая Она остается то есть там что у нас есть не шортированный кластеры у нас есть фактически Праймари Узел это Мастер и секонд-ри узлы это слоевые Но если говорить терминологии классической СУБД Ну и что у нас в центре В центре у нас собственно наш хэштет до наша еще один кластер СУБД тоже в режиме реплика сэд подняты это наш маршрутизатор Ну тут мы не хотим делать опять наломать дров Да сделать маршрутизатор как-то не очень хорошо Мы все-таки решили подумать как нам оптимально какую схему хранения выбрать чтобы там все быстро работало и место немного занимало наша любимая хеширование в помощь давайте от хешируем все ключи в нашей старенькой базе не шортированной воспользуемся какой-то функцией Да записи очень много их там они там десятки сотни миллиардов и нам нужна какая-то нормальная хеш-функция Адлер 32 нам тут не очень подойдет возьмем Хеш функцию семейства S ha Даша алгоритмы Почему Потому что ну основной критерий потому что эта функция обеспечивает минимальное количество коллизий Да и она подходит так поступим возьмем эту функцию и с помощью нее рассчитаем всех иши для всех записей старой базе Да мы это сделать можем до того как мы собственно запустим эту параллельную эксплуатацию про то всю миграцию То есть все заполнение вот этого хэштета до статического статической таблицы маршрутизации мы можем сделать до того как мы запустили параллельную эксплуатацию это классная история пусть она там длится неделю неважно сколько Мы это можем себе позволить и мы это сделали но опять же Да я вот сказал что минимальное количество коллизий но мы все не можем позволить коллизии в принципе то есть нам Мы не можем сказать Окей там у нас там вот столько лизий Ну ничего страшного нет мы должны Как бы это с этим поработать и что мы сделали Мы тоже применили статические классический подход который очень часто применяется мы фактически рядом с этим хэшом вычисленным сохранили список исходных сырых ключей доступа по которым идет собственно обращение И тем самым при возникновении коллизии мы всегда можем посмотреть в этом списке А если этот ключ собственно к Лизе разрешить всё классно Все работает осталось это имплементировать опять же дорабатываем наш Data accesslayer Слава Богу у нас он один как бы на приложение у него унифицированный Ну тут Чуть побольше доработок чем в первом кейсе дальше все это прокатываем на стендах накрученного тестирования ребята кстати поднимите кто катает свои решения на стендах нагрузочного тестированию У кого есть накрученное тестирование прошел многих есть это классно если есть возможность всегда обкатывать свои решения на стендах нагрузочного тестирования Особенно это актуально для highload систем потому что ну без этого мне кажется вообще жить невозможно Итак мы реализовали второй этап нашего решения и конечно же ну важно отметить плюсы и минусы того что у нас получилось Из плюсов Из плюсов опять же у нас нет downtime мы не останавливаем систему мы все делаем финт ушами в воздухе так сказать в рантами и никто практически ничего не замечает конечно замечают но влияние минимальное не нужно заниматься перешартированием перешардированием на таких объемах оно может занимать там неделя месяцы и больше в общем это такая достаточно неопределенная история Ну естественно перешардировании всегда требует даунтайма в большинстве СУБД запускаемся мы опять же на пустом датасете то есть у нас вот эти шарды шарнирный кластер который мы запустили Он пустой нам не надо его заполнять там подрываться писать какие-то миграции вот этого всего не нужно Ну и продолжаем эксплуатацию старой нашей старенькой не шортированной базе то есть её не трогаем пусть себе живёт минусы конечно тоже есть но основной минус это просто дорогое решение почему то есть у нас сейчас три кластера вместо одного это тупо дорого ну и плюс естественно эксплуатации такой такого большого кластера То есть это три кластер это куча машин куча инфраструктуры это тоже дорогое удовольствие это все надо мониторить и так далее обновлять поддерживать в актуальном состоянии Это очень дорого но вариантов не было конечно приходится идти на такие издержки Ну и второй минус он тоже очевидный архитектурный это статическая таблица маршрутизации она есть теперь раньше ее не было Теперь она есть и она требует инициализации инициализации достаточно сложная длительная но плюс в том что мы можем сделать до запуска параллельной эксплуатации классно классно вот мы прошли кстати вот эти вот этапы Да все все вроде бы стало работать да Но мы сталкивались с другими аспектами связанными с эксплуатацией вот СУБД такого большого объема до 100 терабайт это все-таки многовато Да ну и в том числе для Манго тебе это достаточно большая история компактифация до компактив акции очень важный очень важный элемент эксплуатации нужно обязательно делать контакте фракция позволяет вам уплотнить данные то есть уменьшить След данных на диске да А Мы все знаем что размер файлов да чем он больше тем Ну как бы больше проблем блокировки вот это вот все поэтому естественно попытались сделать мы попытались сделать до попытались сделать после ничего не работает штатные утилиты не работают ничего не сжимается мы показали Ok что же нам делать да надо все-таки как-то это сжать и в Манго тебе есть такой механизм он называется нишал Синг это механизм который используется для создания новой реплики да То есть вы когда делаете создаете новую реплику есть такой механизм называется нишал Синг Мы подумали а может быть и не шел симку нажмёт эти данные пока он там переливается сдает эту реплику запустили Да он проработал неделю Он создал эту реплику долго печально но он её сжал и дальше Мы подумали что же нам теперь каждую реплику создавать по недели там по 10 дней мы не можем себе позволить такой роскоши Давайте что-то с этим делать Ну подумали А почему бы нам просто взять и скопировать её просто прямым копированием жахнуть и создать новую реплику быстро попробовали создали подключили ее кластеру и оно заработало подумали Вау классно рассказали об этом вендору вендор тоже сказал Вау Ребята вы крутые Так что в принципе безвыходных ситуаций нет и компуктифацию тоже можно сделать хоть и вот таким вот способом есть и другие аспекты которые связаны с эксплуатацией СУБД такого большого объема Это бэкапирование и так далее нас к сожалению рамки доклада не позволяют об этом поговорить Сегодня хотя это тоже очень важная тема которая всех волнует Ну и перейдем к выводам подведем итоги хочется дать такой напутствие Да ребят Я конечно понимаю да что все проекты стартуют одинаково мы начинаем там в запаре что-то писать запуститься надо было вчера у нас ничего не готово поэтому все думают про функциональные требования Ну стоит хотя бы чуть-чуть если есть возможность подумать и про какие-то фундаментальные вещи Ну например построить модели нагрузки да Или спрогнозировать скажем так объема с OBD который Вас будут там через три пять лет ну Не говоря уже о том что попытаться постараться не наступать на одни и те же грабли а спроектировать и поднять субэд сразу же Ну в целевой топологии То есть это шортированный кластер там понятно что георепликация - это такая история на будущее но хотя бы шарнированный кластер должен быть на старте проекта Ну и напоследок хочется сказать что все эти вот трюки фокусы выполнены профессионалами ребят Не наступайте на эти грабли Пользуйтесь нашим опытом Ну и нет нерешаемых проблем все проблемы можно решить не опускайте Рук из любой ситуации можно выйти Всем спасибо готов ответить на вопросы и первый вопрос справа Смотри вот уже а добрый день спасибо за доклад интересно с вашего позволения сможете вернуться на слайд с графиками будет здорово а вот в начале Да вот отличный слайд начал смотрите на вот этом графике видна странная картинка в 11:30 система работает великолепно в 12:30 они работают совсем Как вы думаете может были какие-то другие еще рядом параметры мониторинга которые могли бы нам заранее подсказать что вот вот оно сейчас скоро уже перестанет работать Спасибо но безусловно у нас есть и другие параметры мониторинга можно посмотреть на бизнес метрики можно посмотреть когда само интенсивная нагрузка на систему там выделить там часы наибольшие нагрузки Кто знаком там с теории Телематика вот это вот все но мы сразу пошли Как говорится к цели и посмотрели на блокировки и все поняли все было понятно и нам очевидно у нас на вопросы в целом минут 15 есть Поэтому можно все кто руки поднял пожалуйста Спасибо большое за доклад хотел узнать какой размер функции вот этой ша в битах Ну то есть какую вы взяли функцию вот эта душа А мы взяли 256 тогда Странно что вы коллизии ищите потому что ну не найдете 100 миллиардов смешное совершенно как бы число для коллизий для США 256 Конечно мы закладываемся на теоретическую возможность их возникновения Ну совершенно смысла никакого если найдёте такой коллизию то звоните ещё как бы такой вопрос Если у вас приложение выбирает какой шард положить по сути данные то нельзя ли атаковать это ну на приложение там не знаю положив товар как бы в другой шард предположим Ну я же могу на стороне клиента раз я Вычисляю номер шарда там то я могу это дело подкрутить как бы положить данные в другой шарф как-нибудь от этого защищайтесь Ну во-первых мы не используем скажем так натуральные идентификаторы поэтому шарнирование идет по ключу который синтетический и мы не даем возможность управления этими ключами соответственно клиенту поэтому шардирование как в этом плане защищено Привет Андрей Спасибо за доклад у меня вот поход доклада возникло два вопроса вопрос номер раз когда вы сделали жордирование вы применили тот самый sha-256 для того чтобы строить контрольные суммы распределять нагрузку А не проще и подняли для этого целый отдельный кластер с таблицами Вы же знали В какой момент вы произвели разделение вы не могли посмотреть максимальные значения текущего индекса и просто сделать обычную проверку больше и меньше индекс значения primary Intex больше туда меньше туда Всё одна математическая операция я не специалист по монго Я сначала подумал раз это такие вэйлу и наверное там нет Прайма индексов Но вот пока ты рассказывал специально залез в документацию посмотрел и написано что для каждой коллекции он автоматически автономный экспериментальные праймери вайлы формируют но все верно Да но мы не хотели закладываться на диапазоны Тем более если придется делать какой-то перешарнирование Вот и какую-то вот эту логику городить не хотели поэтому сразу больше в итоге мы сделали второй кластер сделали таблица хеширование вместо просто одного знака больше не совсем так если Хочешь расскажу потом в деталях после доклада там есть нюансы хорошо так и второй вопрос Ты сказал вы пришли к службе эксплуатации они сказали больше ресурсов нет И в итоге вы подняли а-а шардированную базу где там же там же А если вы Подняли там же почему Там же сразу нельзя было это как его добавить мы Нет ты говоришь сейчас про кейс номер один э там мы вообще не переходили там мы не меняли топологию то есть мы там остались как были на сингл реплика с этим так и остались То есть фактически мы просто рядом у нас как бы диск позволял организовали вот эти n-коллекции вместо большой они сбоку лежали Ага вот то есть мы не меняли здесь топологию топологии мы уже поменяли уже на этапе 2 а там уже нам не важно было что там старый отдали еще оборудование да да Кстати когда тебе задают трольные вопросы смотришь глаза и говоришь Следующий вопрос пожалуйста Спасибо за доклад очень откликнулся потому что мы в X5 как раз таки занимаемся процессингом товаров маркированных нас хуже архитектуре что хотел уточнить вот когда вы на сингл реплика с этим поделили просто коллекции Да там были минусы И вот я думаю там был еще один минус вы потеряли возможность использования уникального индекса например да если он у вас был заложен в бизнес-логике и может дополнить Ну вот раз такой кейс был с чем вы ещё столкнулись Ну так вышло что слава Богу у нас не было на этой коллекции самый монструозные индексов и она чисто такие value это нам просто повезло а да на самом деле то есть если у вас там есть индексы какие-то там аналитические запросы не дай Бог гоняете на оперативной базе это очень плохая история не гоняйте Вот Но в общем у нас на самом деле это не Наш кейс потому что у нас просто обычная киевелью там было и нам не пришлось ничего придумывать все прошло гладкое Да спасибо у меня есть еще вопрос Спасибо за доклад интересно было я не троллю Да я просто спрашиваю вы сказали про три кластера не это как максимальная конфигурация или вы думаете про 5-6 какие-то косяки найдете или есть какие-то просто отдельные решения которые вы ещё не попробовали это один вопрос и второй У меня вопрос по поводу при потере данных теряете какое время восстановление у вас заложено при такой конфигурации в трёх кластерах смотрите внутри кластера здесь не случайно выбрано просто потому что это такое вот решение эксплуатации оно требует трех кластеров То есть у нас первый кластер это собственно наша старая СУБД который мы не трогаем она у нас как бы есть и в ней данные лежат и они там продолжат лежать собственно новые шортированный кластер и маршрутизатор то есть никаких там 4 и 5 не планируется наоборот То есть как бы цель такая что мы постепенно выводим данные старой субдд мы их там архивируем и мы от нее избавимся то есть мы уйдем на целевую топологию и у нас Останется только один кластер конечном счёте Ну спустя там какое-то время про потерю данных про потерю данных Мы опираемся чисто на механизмы репликации мы не используем какие-то там ну то есть мы ничего не штатного такого здесь не применяем то есть стандартные механизм репликации у нас достаточно много реплик То есть каждый шарт Это что такое Это по сути реплика сет то есть там есть Праймари секонд-реплики Нет мы ничего здесь своего не писали здравствуйте Да спасибо за доклад такой вопрос как я понял вы заменили заменили запись в один файл на запись несколько файлов принципе да но объем данных он продолжает расти в связи с этим Вы же в какой-то момент упретесь опять же Да нельзя же бесконечно наращивать количество шард в связи с этим вопрос Если стратегия вымывания и вот как вы себе придумали все верно но опять же когда я говорил о том что мы это распилили Это был этап номер один Да мы конечно же упремся возможности вертикального масштабирования диски не бесконечны и там бесконечно писать в эти файлы они будут расти Да но мы просто отказались от записи в принципе в старую базу данных и стали писать все новые данные только в новую базу Вот то есть у нас фактически рост данных в Старой базе он как бы есть там есть небольшие апдейты Но вот такого какого-то роста стремительного там нет то есть Она постепенно постепенно количество апдейтов в ней уменьшается количество обновлений тоже то есть там данные не растут а в новой у нас нет такой структуры в новой у нас как бы шарнированной базе мы отказались в принципе мы используем средства шортирования самой БД и финальный вопрос Вот это мерч ВК но не факт что это сотрудник ВК никогда не знать такие есть только у сотрудников какая-то внутренняя как мерч поэтому можете быть в этом уверенного но вопрос такой смотрите у вас такая архитектура которая как бы и венчурился сойдётся к протесционированный какой базе но в реальном мире Она никогда не сойдётся если вы её не дожмёте на самом деле то есть вас будут ключи которые как бы никогда к ним не будет обращаться скорее всего и никогда не переложатся типа в настоящую базу данных нужно будет дожимать этот процесс каким-то ручным скриптом или ещё чем-то и соответственно продвигаться по всем изначальном и потом куда-то перекладывать вы думали это какое-то будет процесс не совсем понял почему это и венчуле ну потому что у вас каким-то ключам можно вообще не происходить больше обращения вы никогда к ним будет в их положили как-то они не очень часто используются и никогда к ним никто не обращается и соответственно никогда не будут приложены в ту базу А вы имеете в виду то что у нас накоплено в Старой базе данных да Ну конечно Вы же как бы все-таки хотите ее от нее используется для этого собственно мы ходим к бизнесу и спрашиваем смотри у нас как бы есть данные которые не востребованы и они не востребованы уже полтора года может быть мы сходим заказчиком и спросим Нужны ли им эти данные и мы да но потом вам нужно будет как бы пробежаться по всем базам и типа сделать их септ то что вы переложили и того чего не переложили и вот типа вычислить хотя бы те которые ключи которые не ходили То есть это тоже нагрузить всю эту систему как минимум Ну это можно делать постепенно не обязательно это там грубо говоря делать там в момент там за неделю перетащить на самом деле не планировали пока такой процесс такой мы такой не планировали долгосрочное планирование в этом плане Но это как бы незадача номер один великолепно А смотрите еще один вопрос вот рядом на галерке Давайте зададим а здравствуйте Компания сбертех Мы в свое время делали тоже в другой компании некий Киеве или сторож и вот с такими всякими проблемами сталкивались скажите вы физически диски наращивали то есть кластер 2 он дополнительную физику добавляли Да конечно диски Да но мы изначально после того как у нас случился вот этот факап назовем Так естественно мы прочитали сколько у нас будет в Горизонте 3-5 лет данных потому что мы уже в принципе знали модель данных она стабилизировалась К тому моменту и до заложили но мы еще тестировали сценарий добавления новых дисков на самом деле проблема-то больше не в объеме не в размере то есть вот если доступ по диску часто прыгает головка Ну соответственно все становится грустно Ну да есть такая проблема но это проблема абсолютно любых баз данных в принципе дисков мы решали Так что мапилены физические диски просто Спасибо Спасибо большое Сколько у тебя призов и за какие вопросы ты их отдашь Ох У меня два приза Как определить возраст Мне очень нравится вопросы Какая серия Спасибо большое за доклад старик Я тебе сочувствую ты Ну ты молодец так два классных Таких вот мешочка отлично всякие призы интересные Давайте по-честному поступим часть призов в эту половину зала часть призов Это половина зала в этой половине зала приз справляется человеку на первом ряду Да в принципе просто задаешь вопрос уверенным низким голосом и тебе сразу приз Да нормально так и так так так так так так так так так так кто-то у нас там спрашивал интересный вопрос про количество кластеров такой троллинговой хочется подарить за это приз четвертый пятый вот отлично Да пожалуйста Да Он хотел проверить троллицы спикер или не троллицы вот отлично это сотрудник компании 1С вот сейчас можете обняться как раз спасибо большое спасибо замечательно подожди стой поощрительный приз от конференции обязательно потому что"
}