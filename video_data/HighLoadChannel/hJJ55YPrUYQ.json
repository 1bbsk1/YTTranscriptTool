{
  "video_id": "hJJ55YPrUYQ",
  "channel": "HighLoadChannel",
  "title": "HighLoad для \"маленьких\" / Олег Романенко (MediaSniper)",
  "views": 904,
  "duration": 2454,
  "published": "2024-04-17T01:10:20-07:00",
  "text": "раз раз раз раз во работает да Всем привет Меня зовут Олег Романенко и я директор по разработке highload решений в медиаснайпер компания занимается интернет-рекламой и я думаю да вы все наши баннеры видели в интернете если вы в него ходите без от блока сразу скажу честно я туда хожу с блоком как и многие наши работающие компании но и как у всех кто занимается в том числе я думаю есть миллион браузеров и какие-то из них без блока компания позволяет покупать показано тысячах разных наших площадок в интернете вы видите вы видите баннера от Таргет mail.ru какие-то баннеры заведены у нас Я думаю достаточно большое количество давайте для того чтобы я так понимаю Не все же из рекламной сферы далеко не поэтому немножечко определимся как вообще все работает из терминологии смотрите есть две стороны рекламодатель который хочет вам что-то показать какой-то рекламное объявление допустим продать вам квартиру и желательно Если вы кликнули сразу купили все в онлайне и которые смотрите сайт прикладыватель заводит к своей рекламные объявления в какую-то админку таргетную Директ в наши Сто Пятьсот тысяч админок пользователь смотрит на сайте На сайте владеет владельцы сайта Или те кто ими управляет паблишеры Определи должны определиться где в каком месте сколько и так далее рекламных объявлений они вам покажут хороший паблиш разместит очень красивую контекстную рекламу и вас это не будет напрягать не самый хороший сделает вам как Fish в Одессе Коран в нем загрузится видосик видосики загрузится еще один видосик Итак в целом матрёшка бы это закроете А у вас там еще будет баннер на весь экран дальше ну Такова жизнь ставят себе на сайт какие-то рекламные сети многие ставят несколько рекламных сетей потом из них пытаются каким-то скриптами другими вещами выбирать ту которая приносит им больше денег рекламные сети интегрированные с уже с бэкендами Где производится аукцион Ну мы называем его потому что изначально это все было именно серверная часть сейчас у многих это все реализовано в том числе аукционы отправляют формируют запрос в каком-то стандартном варианте и отправляет это все покупателям покупатели у аукциона может быть много но запрос плюс минус стандартизированный покупателям наше объявление попадают из админок из трейдинга Если покупатель хочет купить конкретный показ Мы помним что мы торгуемся за конкретный показы конкретного пользователя за конкретный сайт и даже там за конкретную позицию рекламное место на этом сайте если он хочет он формирует ответ стандартным записываю туда код рекламного баннера Ну хоть e-mail или что-то еще в зависимости от формата обмена И самое главное ставку тот креатив который поставил больше всех кто-то побеждает ну почти все мы не берем естественно всякие предукционы и так далее чуть-чуть о компании У нас есть весь стек всего что только можно и DSP все что о чем я сейчас говорил и админки и так далее и тому подобное но я вам буду здесь рассказывать именно про сам именно про действие самые нагруженную часть и приходим к самой нагруженной части Что это значит для нас смотрите как только вы начинаете становиться покупателем Вы должны с интегрироваться с огромным количеством аукционов но вчерашний момент когда я делал Дополнительные настройки в системе у нас было более 400 подключений это активных среди них сотни хэдербитинг подключений и около 570 активных подключений к ssp Server соответственно это один и тот же бэкент обрабатывает все это очень много запросов Что значит много для нас вот сейчас настоящее время это полмиллиона запросов в секунду мы очень хотели дорасти до миллиона в двадцать втором году но не получилось Google отключился отключились другие платформы которые сейчас не работает с Россией сейчас как-то все начало потихонечку восстанавливаться Посмотрим может быть в следующем году станет лучше Российский рынок практически весь мы забираем себе ну почти Мы с ними работаем не могу сказать что Для нас это сейчас секунды входящего и один исходящего трафика это только запросы Я не говорю про статику не говорю про какие-то картинки или что-то еще 50 тысяч постоянных соединений 10 тысяч коротких сессий очень хорошо когда ssp работает с вами сервер сервер сходербингом чуть похуже потому что запросы идут напрямую от самого клиента и мы их получаем конечно Классно что мы видим браузер видим весь контекст видим куча хидоров с другой стороны мы видим короткую сессию которая потом отключается потому что юзер от нас уходит 3 миллиарда просили пользователей Да я понимаю что в России не 3 миллиарда жителей в мире не 3 млрд тех кто ходит те сайты которыми мы обслуживаем профиль постоянно обновляются куки меняются в разных фишках есть разные айтишники и так далее Поэтому в среднем сейчас я информацию которая выгруженная система это 3 млрд 25 ТБ в сутки приходят В архивах Ну и на внутренний сервисы так как они тоже используются друг с другом это где-то полтора-два миллиона запросов сразу скажу мы к этой архитектуре пришли не Прочитав где-то в книжечке А через боль и страдания то есть итерационным путем спустя несколько релизов Да мы дошли до того что нам нужно что-то менять И вот уже потом прочитали в разных книжечках Почему так потому что как и многие стартапы в самом начале У нас не было четко поставленного ТЗ У нас не было Четкого понимания что завтра к нам придет там 100 тысяч запросов секунду мы с интегрируемся с Гуглом нет вначале на старте про Google никто не говорил нагрузка была около 500 тысяч запросов в секунду это работало на одну двух серверах и все были счастливы Прикольно а потом пришел шеф и сказал мы подписали договор с Google минимальная 10 тысяч КПС и тестируют они двадцаткой и к концу года были минимум 100 начали думать надо что-то с этим делать потому что это уже не те цифры которыми которые мы даже рассчитывали чуть-чуть про шаблон любые такие распределенные системы вообще любые распределенные системы чаще всего упираются данный шаблон он показывает что как можно что-то сделать если у вас нет Центральной базы данных здесь в каждом Unity одинаковая память которая с помощью каких-то определенных механизмов общих реплицируется на каждой другой Юнит То есть если один Юнит делает на себя изменение эти изменения потом доходят до остальных Да это не стронг консистенции я понимаю нам она здесь не нужна к счастью у нас не требуется нигде stron это прям прекрасно Если бы она требовалось это были бы совсем другие затраты и железо и ресурсов и всего остального очень интересным всяких книжках читать про всякие архитектуры как это все работает прекрасно там расписаны огромное количество плюсов когда ты делаешь это архитектуру как бы получить эти плюсы и нет грести бы всех минусов которые мы получаем при реализации Давайте посмотрим что у нас получилось Я не говорю что мы не захватили минус что у нас получилось у нас вот допустим этот самый главный модуль ядра само действий это Монолит изначально вообще он целиком был монолитом потом из него сервисы все-таки вытащили Монолит достаточно большой осталось он делает основные задачи он делает паршинг запроса Он переводит какой-то внутренний состояние потому что все ssp почти присылают тебе запросы плюс-минус одинаковым стандарте и все предпринимают его по-разному ну прям 90 процентов поэтому стандарт есть Он написан у него есть куча версии 25 2630 и каждое подключение это в любом случае кастом вот так этот запрос дополняется различными данными которые у нас На многие есть Как локальными так и обращения в другие сервисы после того как мы запрос дополнили мы по нему под него подбираем подходящее объявление к объявлениям применяем какие-то ограничения Ну допустим лимиты бюджеты ограничения на уникальность применяем ограничение к самому запросу потому что многие вспышки допустим не выдерживают если отвечать на все запросы есть определенные кастомные схемы когда счетчики которые встраиваются в объявления не разрешает в течение там нескольких минут отвечать несколько раз на одного и того же пользователю тоже там что-то не успевает считаться этот весь странный бизнес логика с помощью костылей где-то там реализовывается И что в этом всем хочется сказать Монолит он нужен для того чтобы собрать кучу данных которые у нас хранятся внутри ноды и найти этот с помощью этой кучи данных объявления которыми Мы хотим ответить обычно это одно объявление сейчас рассматриваем банку нужно найти всего одно объявление десятков сотен тысяч которые настроены в системе максимальное подходящие Какие данные у нас для этого есть что для этого сделаем на первых у нас есть внешний апель Да мы с вами говорим про spacebasto архитектуру и все-таки здесь есть внешние Вызовы Как это связывается смотрите К сожалению мы не можем Сохранить все данные Какие бы мы хотели на ноде по двум причинам первая причина это потому что финансы это очень сложно Ну допустим смотрите 3 миллиарда пользователей которые мы просили которые мы хотим сохранить это если хранить их распайк только для индекса потребует около 200 Гб там по 64 по 64 байта по-моему на одну запись требуется Это я не говорю про сами данные Это только индекс плюс есть данные которые нельзя никуда Ну как сохранить потому что они вычисляются на каждый вопрос данные которые сервисы алгоритм предсказания предсказание цены они высчитываются могут от предыдущих ставок от предыдущей истории то есть там иногда задержки в обновлении данных секундные в самих алгоритмах то есть мы их пытаемся закошировать но тоже редко это получается поэтому у нас есть в котором мы Обращаемся Но главное требование при разработке это если я не отвечает весь система продолжает работать как работала То есть я всем этим докладом хочу донести одну главную вещь Мы очень архитектуру и подход если у вас не отвечает ничего снаружи ваша система все равно должна хоть как-то продолжать работать то есть допустим если на у нас упала все BD балансировщик хоть один должен выжить иначе запросы не придут то есть у нас остался один несчастных балансировщик и одна несчастная нода и ничего больше в системе нет это надо все равно продолжит отвечать а если у нас не ответила API что делать много разных сервисов допустим самая простая цепочка приходит запрос в нем есть внешняя айтишник юзера мы по внешнему айтишнику получаем внутренние айдишник наш юзера потому что у каждого источника свои какие-то идеи мы их на свои так далее и по этим айдишкам мы получаем профиль теперь мы знаем что там куда ходил что он делал достаточно большой такой мне персонифицированный справочник так вот на каждом этапе что-то может сломаться мы можем не получить один из справочников мы можем не получить из матча году и так далее если мы не получили вообще ничего или что-то отвалилось Это значит что из огромного списка креативов которые у нас есть в системе то есть там допустим 100-200 тысяч объявлений мы какую-то часть вообще не сможем применить данному запросу Ну да не сможем ничего об этом страшного нет система приходит во-первых полмиллиона запросов в секунду во-вторых покупать все запросы не требуется к счастью мы покупатель мы не ssp часть которая должна обслужить все запросы что-то показать на сайте Мы покупатель мы можем отказаться и практически на все случаи есть какой-нибудь рекламное объявление которому не требуется профиль пользователя или ему не требуется там уникальность или не требуется ещё какие-то настройки и вероятность от этого отказа Мы конечно ее считали оценивали Но поскольку я помню Сейчас на случай отказа API это где-то 30 процентов отваливается креатив в текущий момент допустим на прошлый месяц как там что настроено то что сильно меняется да 30 процентов креатив отваливается во-первых не страшно во-вторых j5 достаточно неплохой по крайней мере достаточно быстро могут поднять там память самое главное и самое то что чаще всего используется в каждой ноде есть своя шара это какая-то память это все у нас реализовывается с помощью распайка для тех кто не знает что такое распайк радист астероидов скажем так распак это такая кевильо бдшка которая очень быстрая очень очень быстрая ребята очень любят хвалиться тем что они мегабыстрые Они любят очень показывать свои бенчмарки с миллионами kps и да да Она прикольная если уметь готовить то есть с ней Достаточно долго пожить она очень хороший показатель еще на очень прикольно умеет реплицироваться поэтому мы живем на распайке кластерах еще с момента когда они называются труслив То есть это по-моему уже лет восемь с ними у них очень хороший комьюнити версии поэтому если кому-то интересно смотрите тестируйте профиль площадок динамические всякие листы ограничения лимиты данная конкретная ставках это все живет в памяти каждый всё обновляется ежесекундно и все это реплицируется с помощью встроенных средств кластера распайка статика Мы очень любим статику Мы начинали со статики как я думаю множество всяких таких стартапов которые хотят обрабатывать много данных и которых нет вообще ресурсов они все стараются закашировать на одной ноте в виде файликов виде всего чего-то что не чтобы твоя нода не ходила на каждый запрос или на каждые 5 каждую минуту не ходил какую-то поддержку А мы получаем файлика снапшотом этой бэдэшки мы получаем Диф от snapshot или что-то еще сохраняем его на ноги и мы не боимся того чтобы если вдруг с ней какие-то проблемы если у неё есть задержки если она упала многие начинали искать если у тебя вдруг выросла или у тебя вдруг переехала в соседний дата центр случайно никогда такого не было у нас было три раза у тебя дэдэшки вдруг стал Пинг не двинули секунды а около 130 насколько я помню после этого уехали Ну уже в другой центр статика нас очень много и теперь представим что у нас есть допустим 400 для оборовного счета 400 тысяч КПС и 20 машинок которые их обрабатывают на одну машинку нам приходится 20К машинки допустим двухпроцессорный по 40 всего 40 ядер Итого на каждое ядро 500 ППС или как мы видим 2 миллисекунды процессорного времени тратится на обработку одного запроса куда если по последнему профайлингу мы вот у себя видим что на фильтрацию применения правил и так далее где-то половина этого времени у нас уходит почему это показываю потому что это будет самым последним в списке выводов предварительная оптимизация правда-правда Мы заменяли у себя одна из разработчиков друг нашел очень прикольных функций ты один по-английски функции классные Все работает 50 процентов буста мы получили на наших строках с юзер агентами относительно 64 Шикарно Все круто потом второе второе изменение было мы переходили с анордарет Map обычного на Робин хударет МАПП тоже процентов 40 получили выигрыша при загрузке в мапой из диска когда мы не знаем как сколько данных мы будем загружать тоже очень круто и вон тот видите один процент вот мы его оптимизировали получилось то есть когда вот этот профайлинг сняли Люди так посмотрели сказали классно у нас теперь не один процент а один с половиной Это нам помогло примерно никак но цифры при бенчмарках были обалденные То есть когда тебе приходит программисты показывают цифры смотри у нас выигрыш 120%. Классно Давай внедрим вы конечно внедрили потому что это было внедрение с помощью 20 строчек кода но профита нам это не дало на загрузке на старте ДСП с Холодного старта Да вот достаточно большое при обычной работе никакого как с этим жить как жить когда у тебя есть две миллисекунды на обработку данных в принципе ты Терпимо Я думаю те кто встроенными системами хоть раз занимался вообще без каких-либо проблем те кто занимается веб-сервисами мы запрещаем серии gxp запрещаем длинные списки правил длинные строки длинные строки Это юзера когда вводят по 4 килобайта зачем-то строку и они ищут каких-то дезорагентов Я не знаю но это все из реальной жизни Если вы думаете что невозможно руками заполнять список из доменов из 20 тысяч записей Возможно если невозможно думать что невозможно вести много версий этих списков без Гита и потом применять их на каждой на каждой конкретной возможно я не знаю зачем пожалуйста Используйте словарей и все это у нас было реализовано мы просто нашли поймали того менеджера из-за которого слёг бэкент Ну как он начал в два раза медленнее работать посмотрели графану начали говорить Что такое происходит И выяснили что где-то там какое-то ограничение наш очень упорный человек смог обойти пожалуйста когда у тебя одна строчка намного приятнее все такое намного лучше чем огромнейший длинный список а еще на предыдущий вернусь пока вот здесь как жить если тебе нужно что Ну не регать Ладно что-нибудь нужно хотя бы поиск под строки в строке у нас одна из вариаций ахакарасика работает когда мы строим по строке облако всех возможных подстрог и в принципе из него строим словарь Так выживаем пробовали редактор пробовали кучу различных библиотек в принципе если легкий риск это классно либридва или гипер скан это хищные Очень приятно Все работает очень классно как только ты разрешаешь менеджером пользоваться тебе сразу кто-нибудь пишет точку звездочку и на этом заканчивается счастье числовые данные тоже надо как-то группировать переводить их каким-то категориям Ну типа Если у вас есть 150 объявлений видосиков в которых есть куча битрейтов мы их приводим там битрейт до 1.000 битрейт тысяч до двух и так далее То есть все стараемся сводить каким-то категориям Почему Потому что мы потом все правила которые в системе настроены приводим комбинации Да иногда иерархический как вот здесь пример с гео Но все равно это все плюс-минус Зачем нам такие нужны листы мы все эти списки правил приводим к битовым массивам то есть вся все вот эта вся подготовка данных нужно для того чтобы построить кучу битовых массивов где каждый бит это один рекламный одно рекламное объявление Почему если вы применяете 50 правил 5000 рекламных объявлений вам нужно сделать 250 тысяч проверок много а если мы применяем 1000 битовых операций типа НТО массивами из десяти килобит это всего 45 микросекунд если у вас будет меньше она линейно масштабируется если вас это не устраивает Ну Напишите на интенсинках сами Хотя Boost Очень неплохо в интригинке раскладывается вот это вот посчитано было на наших реальных серверах на Boost Dynamic битовые операции битовые операции Я тоже утащил массивы объявления в них Я тоже тащусь какого-то одного из древних хайловодов не помню кто уже выступал К сожалению могу сказать огромное Большое спасибо у нас столько рекламных материалов на входе на каждый запрос из индекса Мы выбираем один одну три пять тысяч после применения правил остается вот где-то сотня то есть мы получаем битовый массив которым очень много ноликов и всего 100 единичек опять же мы не перебираем его весь целиком и продам производится операция поиска первой единицы и так далее следующий это тоже все достаточно быстро после фильтрации остается там несколько десятков фильтрация это те операции несчастные несколько штук которые мы не смогли переложить набитого набитовые массивы либо это какие-то очередные костыли которые нужно срочно проверить и никто не хочет их неделю перекладывать на битого операции когда у тебя стало 10 креативов их дальше уже алгоритм ранжирование просто оценивает в ответ когда-то система была маленькая а потом когда к нам подключили не один не два не три 10 даже не 50 источников вот к чему-то такому она пришла То есть сейчас там плюс-минус 18-20 с небольшим нот Я не могу точное количество сказать потому что они там под горячие горячем режиме стоят и ждут вдруг что-то упадет на них переключится нагрузка и так далее в качестве бэкапов около 500 тысяч приходит и в среднем Ну 20-30 тысяч на некоторых нодах еще разное количество процессоров сожалению Ну даже бесплатный позволяет промежуточный слой промежуточном слое несколько различных кластеров которые выполняют свои задачки балансировщики мне говорили что возможно здесь будут какие-то вопросы на тему Почему вы не используете аппаратный балансировщики мы использовали разные за 10 лет мы пробовали много что у нас были в 5 Мы у нас была Барракуда у нас были аппаратный балансеры от дата-центров у нас была haproxy у нас был один большой кластер джинса в итоге мы пришли к двум Джеймса Почему не хабруксе не отвечу потому по-моему потому что админом больше нравился в тот момент и я не стал спорить с админами что им нравится поддерживать Пусть поддерживает но производительность на тот момент было принести минус одинаково Почему программный потому что запускаются на тех же самых нодах мы всё в любой момент можем снять любую ноду из ядра если она нам не используется перекинуть её быстро в пластырь джинсов Если там у них что-то сгорело и всем будет счастье Это займёт у нас 20 минут а замена нам было рощиков в последний раз обошлась 2 месяца два месяца это долго два класса Почему два мы поделили один кластер который держит типа Live Long polling соединения с эспэшками соединение живут больше часа и один кластер который же в котором соединение от юзеров приходит там кратковременные сессии в одной опять же как как на них запросит распределяются просто домен просто раунд Робин почти все ССП уже научились вычитывать из домена айпишки Ура 2023 год наши клей Ну не смысле наши клиенты А клиенты тех с кем мы работаем http-шный научились вычитывать из домена весь список айпишешь и распределять трафик на них потому что многие годы Google нам свал запросы на 2ip Ну так перехват ip-адреса упавшего сервера здесь работает то есть где-то 3 серверов выпадает из кластера все живет дальше мы начинаем бегать и пытаться разобраться что с ними случилось обычно балансировщик обычно блоки питания это так уже по известной информации к сожалению промежуточный слой API так как реализовывается внешними сервисами здесь маленький кластер из парамашинок которые просто синхронизируют настройки всякие кэши и так далее Здесь особо ничего такого интересного нет статика здесь у нас живет кластер который готовит диффы для ядра и для clic House могу сказать так Боже благословит Клик Хауса Яндекс который его сделал потому что мэрш 3 это очень классно Мы очень любим Хаус я несколько лет поддерживал примерно похожее решение вертике реализованное на коленке и Это ужасно Это настолько ужасно что с ним что-то происходит это прямо плохо тяжело по-другому это не скажешь быть единственным несчастным дебаа который хоть как-то что-то в этом понимает еще более грустно хорошо память здесь живет распайка всякие его утилиты вся его оснастка и здесь живет небольшой кластер который отслеживает такие данные у нас распайки и насколько они актуальны обновляет Почему Потому что есть устаревание данных еще что-то такое или всякие дефрагментация Ну мы стараем у нас есть свой сервис У нас есть ttl который ставится он сильно завышенный и свой сервис который пробегает по данным в течение нескольких лет Такая схема нам показалось сильно удобнее потому что иногда даже устаревшие данные все же нужнее чем вообще отсутствие каких-нибудь данных если у тебя E5 отвалился служебные сервисы по всему этому мы подключили кучу служебных сервисов Да мы просто их вынесли из гидра Я понимаю но так есть здесь есть сервисы которые выдают объявление сами здесь и сервисы которые подготавливают данные для детей сервисы которые обслуживают клики показывает так далее от пользователей они все работают в принципе вот тем же промежуточным слоем и предоставляет данный либо забирают от них вот в них внутри уже есть очереди там где-то в одних и сервисах есть Кафка там в них есть я не ошибаюсь У кого-то там еще живет Но это все внутри них есть этот сервис отвалился ничего страшного ничего страшного не произойдет Почему мы вытаскивали сервисы потому что мы очень любим плюсы но на Гоа писать маленькие сервисы сильно проще найти разработчика себе сильно проще который умеет это делать год достаточно производитель веб-сервиса очень даже хорошо хорошую нагрузку может держать и производить код review маленького сервиса мне как главным ответственность за все это сильно проще и ответственность меньше и диплоить этот бой сильно проявить когда ты делаешь на две машинки не на 20 на 40 но некоторые сервисы теперь у нас нагружены по 2 миллиона Да даже Гоша причем сервера из 5-7 машинок никаких проблем Это тоже сервис они вызываются не просто в любом порядке К сожалению какой-то определенной последовательности то есть допустим как я уже раньше по-моему говорил у нас есть внешняя по внешней диске нам нужно получить внутренняя диск по этим двумя дишкам нужно получить профили и со всеми этими профилями прийти в сервис прогнозирования Сказать скажи-ка нам А что ты думаешь по оценке этого запроса Какую бы нам здесь вот думаем или стоит ли нам него вообще ставку делать или не нужен этот запрос на каждом этапе что-то может сломаться на каждом этапе мы можем получить тайм-аут поэтому у нас ограничения внешне это 67 Как быть как жить когда у тебя есть 70 миллисекунд мы уже жили с двумя миллисекундами но теперь мы живем с 70 миллисекундами сетевой части естественно не блокировать текущий процесс и Поток Поток никогда не блокируется Он обрабатывает следующие операции следующий запросы А все что у вас отправлено запросы в другие сервисы они просто ждут всякие обработки событий нам потребовалось htp сервер А что тебе клиент и все остальное чтобы оно все живо в одном цикле обработки событий Все взаимодействовал между собой В тринадцатом году мы что-то похожее не нашли пришлось реализовать свое свое решение Как ни странно почему-то оказалось очень неплохим мы с ним живем Достаточно долго я его не рекламирую никому и так далее То есть они говорят что вот юзайте вот эту либо классная Ну да Она быстрая Да она крутая Дана там вот тех компаура держит 7 миллионов Круто да но она подходит конкретно нам потому что сделана под наши требования например не закрывать соединение с сервером если мы получили какой-то определенный набор ошибок То есть если у тебя полностью вообще нет Окей мы закрываем Better Quest получили закрываем А если тайм-аут мы это соединение откладываем определенный пул и ждём немножечко потому что что потому что сервисы могут чуть-чуть притормозить иногда могут диски уганули ssd-шечка луганула на ssdшечка мы получили сервер cou сервис что-нибудь заработала на этом роде ещё что угодно может произойти у тебя на пару секунд северону и получаем что вся все сервера все там классные из 20-40 машины которые бился от него Ну зачем после того как мы такую штуку сделали нам стало чуть-чуть полегче дальше мы начали выносить сервисы дальше мы начали агрегировать запросы чтобы а-а передавать сервис только один запрос и это всё как-то работалось распаком но опять же умная каширование с помощью данных мы получаем из dmp расчеты различные Какие профили стоит поднять в кэш Какие профили стоит убрать из кэша потенциальные Какие пользователи могут нам прийти на следующий час какие нет это конечно помогает Но иногда это просто прогнозирование погода на Марсе Иногда работает иногда не работает самое главное еще раз вот здесь же любой сервис может не ответить вы всегда получите тайм-аут просто Живите с этим поднимайте какие-то кэш делайте какие-то группировки поднимайте себе еще что-то все что может сломаться обязательно Сломается Все что у нас могло сломаться ломалось она сломалась вообще все она сломался до пациентов целиком вместе с франкфуртом у нас сломались корневые маршрутизаторы нас выдергивали провода У нас есть между нашими стойками отдельная своя сетка которая нам проводили проводил этот центр и специальный кабель ее тоже ломали у нас горели два блока питания в одном сервере несколько раз балансировщиках у нас сгорела материнка материнская плата много тоже я не говорю что одна Я говорю прямо в балансировщиках прям рядом у нас выгорали серии жестких дисков поэтому ваша нода должна жить так что вообще все вокруг не работает к сожалению Если вы можете зарезервировать все в огромном количестве круто мы не можем Всем спасибо Спасибо большое Олег прежде чем перейдем к вопросам ответам Я хочу сделать маленькую ремарочку я вот залы ввиду уже вам лет пять Я первый раз вижу доклад последний доклад второго дня на котором люди стоят двери друзья мои вопросы Давайте Спасибо большое за доклад Мне очень понравилось Особенно потому что я пишу на плюсах и хайлоуит у меня на плюсах вот у меня следующие вопросы вы говорили про оптимизацию и то что вы там заменяли нордер мэп и насколько видел вас сбитыми очень жесткой оптимизация в плане алгоритмов насколько сильно вы здесь упирались в память и упирались его вообще реализуют такие алгоритмы ни разу не упираемся а именно скорость доступа скорость доступа у нас машинки очень старые многие то есть DDR3 и честно говоря здесь мы проблемы особо не видим то есть менять машинки на новые мы не готовы нам проще добавить еще машинки скорость доступа к памяти Ну может быть но мы намного больше опираемся в себе Мы упираемся всегда Поэтому если есть какие-то по памяти проблемы мы их редко замечаем то есть мы упираемся в b64 какой-нибудь дикодинг Мы упираемся в кэш Мы упираемся в мд5 Хотя тебе приходит клиент и говорит А теперь я хочу чтобы во всех ссылках которыми вы отвечаете было на пять хэш А ну может кто-нибудь нормальный конечно D5 круто мы прочитали про М5 Где вы прочитали прямо пять Ребят 2023 вы что Ну им до 5 классно потому что в каком-то другом счётчике md5 И мы сравниваем Класс у меня ещё будет сразу ещё до вопросов который я хотел задать так э-э второй вопрос про Клик Хаус Я так понимаю он стоит на каждой ноге нет кликауса отдельно с статистикой мы пишем у нас формируется дифф Ну то есть приходит куча статистики в Серых влогов она агрегируется после агрегации формируется дифф от предыдущих данных и этот дифф отправляется в плехаус Мерс три и отправляется на ноты Но ты подгружает его в память и тогда еще Последний вопрос вы когда писали свою библиотеку для веб при такой большом количестве нагрузки она у вас на потоках или на корутинах она на потоках каротин тогда еще фишечки не было Ну они в густе конечно были не ну конечно пусть это все было Нет она чисто на потоках обработки события поднимается в каждом потоке и каждый поток Ну Единственное что там этот с помощью гидро раскидывается на потоке запросы всё плохо японский флажочек ставится для друзья еще вопросы вообще мы изначально первые самые версии но ушли производительности но 12-13 и если правильно понял вы занимаетесь и развитием продукта и архитектурой еще проводите код ревью программистов Олег из тех времен когда слово продукт еще не существует Как вы находите время проводить коды View И причем так глубоко понимая все детали но куда деваться Мы же стартап нас изначально вообще там трое было сколько стартапуль лет Олег но сейчас это уже не стартап все-все совершенно летний стартап как только к тебе приходит инвестиции Да спасибо за доклад Я извиняюсь Может просто слушал невнимательно можете сказать почему именно была выбрана spacebase архитектура Что именно вам дала честно Потому что получилось Я же говорил итерационно мы пришли к тому что нам нужно на каждой ноги изолировать все данные только возможно Мы сначала сделали прототип изолировать посмотрели а так классно и работает и начали думать а как делают умные люди то есть мы сделали вроде прикольно но надо почитать как умные делают почитали А правда так Правда умные люди делают и уже требование чтобы была максимальная изоляция данных Конечно мы жили в начале времён и там падало все у всех тут мне кажется вопрос не изоляции данных а именно от каждого устойчивость максимальной если у тебя данных нет под ногами то по факту всё Олег из трёх прекрасных людей Выбери человека с лучшим вопросом первый вопрос соответственно Давайте подарим приз за лучший вопрос наши прекрасные помощники уже бегут ребята ребята вот пока вот вы здесь все Сидите пожалуйста вот этот qr-код Достаньте ваши телефончики Наведите сюда телефон и Оставьте отзыв оценочку и отзыв это очень важно и мне и на всему программному комитету очень важно ваше мнение вот молодой человек соответственно ему книжку по надежности как раз Олег от программного комитета тебе прекрасную толстовочку с прекрасными наклеечками там как раз про надёжности и все такое прочее друзья а мне остается только отпустить Олега с этой сцены сказать спасибо спасибо вам всем спасибо за то что мы все вместе провели эти замечательные два дня в городе на Неве следующий наш highload будет Московский он будет у нас осенью в конце ноября До новых встреч друзья Спасибо вам большое"
}