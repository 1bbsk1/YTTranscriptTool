{
  "video_id": "82mzi0iN_Uk",
  "channel": "HighLoadChannel",
  "title": "Трансформация подхода к хранению и синхронизации писем / Андрей Колесников (МойОфис)",
  "views": 189,
  "duration": 2430,
  "published": "2023-10-06T07:21:09-07:00",
  "text": "Доброе утро Нам выпала честь открывать второй день хайлоут и давайте сделаем это с интересом поговорим о трансформации подходов к хранению синхронизации писем в компании Мой офис в частности о развитии почтовых систем в нашей компании На текущий момент на рынке у компании Мой офис представлено сразу несколько почтовых серверов Это мой офис Почта сервер для нагрузки до 30 тысяч пользователей и корпоративный почтовый сервер миллион последние нагрузочное тестирование было проведено для 600 тысяч пользователей позднее мы поговорим об этом подробнее Привет Меня зовут Андрей Колесников Я являюсь руководителем инженерного отдела я отвечаю в компании за внедрение продуктов компании также за их последующее техническое сопровождение Я работаю в компании более семи лет и буквально на моих глазах были свои взлеты и падения наших продуктов продукты технические развивались становились оптимальнее функциональнее и хочу начать свой рассказ буквально С чистого листа на заре работать в компании Я прекрасно помню тот теплый Весенний день когда я находился в офисе работал занимался инсталляцией продукта за окном пели птички я попивал кофе и все было прекрасно Ровно до того момента пока система мониторинга буквально не взорвалась красным цветом телефон разрывался от звонков пользователей и руководства модификации приходили буквально каждые 10 секунд что же произошло одна из наших корпоративных почтовых систем достигла своего предела производительности Неожиданно как для нас так и для разработчиков кластер баз данных вышел из своего кластерного режима и не хотел Никаким образом входить Обратно мы в аварийном режиме искали новые конфигурации и буквально этот и последующий день носили систему на руках это не добавляло оптимизма моему рабочему дню какой стык технологий был на тот момент нашей почтовой системе метаданные хранились в пост рискуэль конверты А тело письма хранилось в openstek Swift за кластеризацию отвечала связка paysmaker + карасинг где пояснэйкер это менеджер ресурсов А crossing это менеджер транспортного уровня создан для управления информационными потоками Давайте немного разомнемся поскольку у нас самое утро и проведем небольшую зарядку Пожалуйста поднимите руки в зале кто когда-либо работал с пост graceql так неплохо А кто работал с openstex Swift отлично Вот и познакомились едем дальше Давайте посмотрим как вы могли получить доступ к нашему письму на тот момент для начала надо было посмотреть ID в ID письма в логах почтового сервера после этого с ним пойти в постгальскую Эль оттуда получить Пит файлы с этими данными обратиться в S3 объектное хранилище openstex Swift скачать объект И после этого открыть на просмотр это будет тело нашего письма проведя ряд нагрузочных тестов на одном из наших корпоративных стендов мы достигли предела производительности в две с половиной тысячи rps что эквивалентно одновременно и работе 100 тысяч пользователей мы не хотели останавливаться на этой цифре Поэтому решили пересмотреть свой подход к почтовой системе что же конкретно пошло не так что не дало нам масштабироваться и обрабатывать большее количество нагрузки в частности мы были вынуждены признать признать ряд архитектурных ошибок кластеризация баз данных была ограничена только тремя нодами только одна из них могла читать и работать на запись при этом также узким местом стал редис его однопоточная природа обработки запросов и также были проблемы масштабирования подведем итоги связки пост gree SQL и openstex Swift Из плюсов это безусловно команда которая на тот момент создавала продукт и его поддерживала также что позже что openstec Swift это проверенные временем технологии надо сказать что и по сей день что позже что Swift существует в ряде успешных проектов в крупных компаниях также у обоих продуктов крупное комьюнити И большинство вопросов можно решить в короткое время Из минусов как я уже говорил это архитектурные погрешности а также на тот момент Почтовая система находилась в рамках более крупной системы которая называлась Мой офис частное облако и был было принято решение о том чтобы вынести почтовую систему в отдельный продукт который будет называться Мой офис Почта в рамках проектирования обновленного подхода к почтовой системе мы выбрали компонентом MD почтовой системы дав код для репликации писем использовали до вход desync соответственно продолжим нашу зарядку Кто в зале работал с дав код Поднимите руки так в таком случае Вы наверняка знаете что до вклада симка работает достаточно простым способом куда бы Клиент не направил свое письмо оно будет среплицировано на противоположную ноду какое-то время мы существовали в такой парадигме пока не столкнулись с конкретным кейсом у заказчика заказчик в большом количестве шарил свои ящики между пользователями буквально тысячами и возникла такая ситуация если на одной ноте пользователь редактирует письмо говорим о кластерном режиме а при этом на второй ноте второй пользователь работает с этим же письмом то возникает не консистентность данных индексов что приводило к недоступности писем эту проблему нужно было решать и поскольку на тот момент мы нашли этот баг в бактекере давкот и разработчик не собирался фиксить ее в ближайшее время то были вынуждены искать оперативно замену а также когда вход у нас накопились еще некоторые скажем так претензии в частности в коде симки есть проблемы с масштабированием один из наших инженеров предложил нам решение на которое мы в итоге перешли но между прочим хочу заметить что сейчас дав код нашел решение проблемы с жареными ящиками в кластерном режиме путем введения новой сущности до вход директор как раз и занимается сейчас распределением запросов клиентских iMap сессий для доступа к шаринным ящикам но повторюсь на тот момент решение не было и мы были вынуждены искать замену один из наших инженеров предложил внедрить кластер FS и мы точечно внедрили у конкретного заказчика эту файловую систему кластер FS полностью закрыл потребность проблему которая существовала синхронизации писем и мы решили проведя еще ряд тестов на совместимость внедрить кластер FS централизованно во всем продукте Мой офис Почта и Давайте завершим нашу зарядку 3 4 кто работал кластер Fest есть в зале есть отлично Поговорим немного подробнее какими сущностями оперирует гластер FS минимальная единица хранения власти РФС это брики Они же диски брики формируются в логические комбинации логические дома в терминологии кластер FS это называется Volume кластер FS может работать с различными типами репликации мы используем тип replicated это аналог зеркалирования При таком типе при сохранении объектов в систему письмо будет сохранено минимум на два брика или диска если опуститься на уровень ниже на уровень ядра файловой системы то кластер FS это файловая система которая работает в пространстве пользователя а с ядром общается и с виртуальной файловой системы посредством модуля Fuse подведем промежуточную черту Из плюсов кластер FS это быстрый Старт мы проверили это на себе также кластер FS лежит в основе корпоративного продукта Red Head Stories Server Что дает некоторую надежду что завтра разработка не закроется Из минусов Мы периодически отлавливаем баги которые помечены в бактрейкере Как нерешенные так и как решенные и обновление до последней версии не всегда помогает а также замечено следующее поведение когда клиенты просят нас поставить кластер FS кластером режиме не на 3 но думаю ноды для кворумного кворбная конфигурации А на две в целях экономии дискового пространства и при этом в сети заказчика есть задержки то одна из нот отваливается и не возвращается автоматически в кластер приходится это делать вручную параллельно с внедрением и эксплуатацией кластер FS мы начали разрабатывать корпоративный почтовый сервер на или он и в рамках этого проекта был дан Старт собственной разработки объектного хранилища дисперс object Store Dos Наверняка у вас появится вопрос Если еще не появился зачем мы начали делать собственный объектное хранилище скажу так что на тот момент А это был 2017 год ни одна из файловых систем ни одной из объекта хранилищ не удовлетворяло нашим требованиям к системе в частности одной из требований было наличие дедупликации данных если пройти по ссылке из qr-кода то можно попасть на нашу статью на хаббре где подробнее можно узнать об архитектуре объектном хранилища а также там есть некоторый сравнительный анализ части Open Source продуктов которые мы рассматривали если проанализировать особенности корпоративной переписки то не сложно заметить что многие письма частично или полностью совпадают вы отправляете письмо оно сохраняется Как у вас в исходящих так У вашего оппонента во входящих корпоративные рассылки шаблонные письма длинные цепочки цитат в переписке и многое другое при анализе структуры электронного письма Мы решили делить его на части или парты как мы это называем нашей внутренней кухни то есть заголовок письма у нас уходит в хранилище метаданных А тело письма делится на те самые парты основной текст первый парт подпись второй парк картинка в подписи 3 вложенный файл 4 и так далее Давайте сравним как хранятся письма в нашей почтовой системе и как они хранятся в популярных почтовых форматах распространенный формат m-box подразумевает под собой парадигму один почтовый ящик один файл на файловой системе не менее распространенный формат mail Deer существует в рамках следующего правила одно письмо один файл то есть ваш почтовый ящик это директория с под директориями и файлами письмами в них в нашей системе в нашем объектном хранилище как я уже говорил заголовки мы отправляем хранилище метаданных А тело письма делим на парты и отправляем в объектное хранилище в нашей системе мы поддержали дедупликацию давайте рассмотрим как функционирует дедупликация на уровне объектов простой пример к нам поступает Первое письмо Почтовая система делит его на части и сохраняет отдельными объектами в объектном хранилища приходит второе письмо сервис обработки входящей почты также парсить его разбивает на части и предположим тело письма это уникальный объект он сохраняется в объектном хранилище новым объектом а второй объект предположим что это та самая картинка из подписи уже есть объектом хранилища поэтому уже у существующего объекта увеличивается значение счётчика копии плюсы такого подхода очевидны это экономия как дискового пространства так и сетевого трафика между нодами Из минусов это конечно усложнение логики клиентского сервиса то есть сервис обработки входящей почты должен уметь сличать объекты уметь сравнивать их делить на части работать со значением со значением счетчика копий при помощи простых эвристик наши объекты хранилище умеет определять Тип и размер данных и в зависимости от этого пускать по различным пайплайнам обработки и оптимизации забегая вперед сразу скажу что данный малого размера мы К данным малого размера мы не применяем никаких оптимизаций поскольку на основе исследований сделали Вывод что эффект от них незначителен а затраты на аппаратные ресурсы превышают полученный эффект от оптимизации поэтому когда к нам поступают большие или средние данные если это текстовые то мы направляем их в конвейер чанкинга где они делятся на части посредством алгоритма контента и компрессируем То есть сжимаем если же это бинарные данные то бинарные данные обладают достаточно высокой тропиностью и без понимания что конкретно внутри бинарного файла будь то аудио видео изображение сложно подобрать оптимальный алгоритм сжатия Мы приняли решение не компрессировать бинарные данные и делить при помощи тривиального алгоритма на чанке равного размера наш объектное хранилище горизонтально масштабируемые и отказоустойчивы поговорим подробнее мы реализовали несколько подходов к избыточности первый условно назовем его полное избыточность это аналог зеркалирования когда файл хранится в нескольких экземплярах на разных нодах это достаточно расточительный подход поэтому мы используем его для метаданных поскольку метаданных в нашей системе значительно меньше коэффициент избыточности здесь является целым натуральным числом большим или равным двойки второй подход это дробное избыточность здесь мы используем коды коррекции ошибок в частности коды Рида Соломона реализация следующая когда письмо попадает в хранилище оно делится на части посредством кодов Рида Соломона к ним генерируется дополнительно избыточные фрагменты после чего все части распределяются по разным нормам если одна из нот выходит из строя то данные на ней могут быть восстановлены с помощью тех фрагментов которые остались на других нодах коэффициент избыточности здесь варьируется от одного до двойки и это достаточно экономичный вариант с помощью него мы экономим большое количество дискового пространства Давайте посмотрим весь лайн обработки письма в нашем объектном хранилище письмо попадает на вход после чего делится на чанке чанки компрессируются сжимаются после этого попадают на вход функции кодирования Рида Соломона где генерируется дополнительные избыточные фрагменты и только после этого сохраняются на диске в своих исследованиях Мы заметили достаточно большое соотношение метода данных К данным и это соотношение варьировалось как одного к трем так и до 1 к 500 но можно сделать один вывод с точностью что метаданных значительно меньше чем самих данных поэтому для метаданных мы позволяем себе использовать полную избыточность Давайте же посмотрим сколько же в цифрах мы выигрываем применяя наши оптимизации У нас есть поток входящей почты назовем его resift это весь поток данных предположим который должен храниться в почтовой системе Он поступает на вход почтовые системы где парщица и к нему применяется наши оптимизации в частности дедупликация на уровне объектов в результате объем данных уменьшается примерно в 13 раз далее наши данные поступают на вход функции кодирования Рида Соломона где реализуется избыточность и генерируется дополнительные вспомогательные фрагменты после чего объем увеличивается примерно на треть в результате разница между входящим потоком resift и тем объемом данных которых был сохранен на диске Store в 10 раз достаточно весомая цифра теперь посмотрим сколько же в терабайтах и в деньгах мы экономим по сравнению с другими форматами хранения данных с другими почтовыми серверами серверами соответственно в нашей почтовой системе будем рассматривать обе почтовые системы в кластерном режиме в нашей почтовой системе коэффициент избыточности для метаданных будет равняться трем для самих данных двум стопроцентный объем данных поступающей на вход в результате оптимизации дедупликации чанкинга компрессии превращается в 16 процентов от исходного значения и сохраняется на диске для реализации аналогичного уровня отказоустойчивости нам потребуется трехкратное резервирование в системе с форматом хранения mail Deer то есть объем хранимых данных Нам необходимо будет умножить на 3 на 3 ноты в итоге можно сделать вывод что наше Почтовая система 18 Раз экономичнее в плане потребления дискового пространства если перевести это на деньги то разница уже не столь значительна поскольку мы используем Некоторое количество SSD дисков а SSD диски дороже и экономический эффект в данном случае то есть стоимость хранения данных в нашей почтовой системе в нашем объектном хранилище 8 раз ниже Интересная история что для некоторых заказчиков наличие в нашей системе использование SSD дисков становится таким стоп-фактором далеко за примером ходить не надо буквально в начале этого месяца мы ездили в достаточно крупному государственному заказчику и устанавливали свои SSD диски в сервере заказчика и только после этого он смог начать пилотирование нашей системы такая история вернемся к слайду и еще раз повторим что наша Почтовая система наш объектное хранилище в 8 раз экономически эффективнее Давайте посчитаем сколько это будет в долларах в таблице представлена стоимость за 1 ТБ HDD и SSD дисков на текущий момент интересно кстати что здесь есть прогноз и 2026 году стоимость SSD HDD дисков сравняется а далее SSD диски продолжат дешеветь Ну о причинах можем поговорить в кулуарах пока вернемся к слайду и предположим что одна нода в системе Male Deer должна хранить 500 терабайт данных поскольку у нас три ноты в случае кластерного устойчивого режима умножаем на 3 получаем полтора петабайта умножаем это на 19 долларов на стоимость одного терабайта в этом году и получаем сумму в 28,5 тысяч долларов за кластерную дисковую подсистему при хранении данных в формате mail Deer вспоминаем что наш Почтовая система с объектом хранилищем 8 раз экономически эффективнее и видим цифру в три с половиной тысячи долларов результат достаточно внушительный как я уже говорил последние нагрузочное тестирование которое мы проводили на почтовой системой меллион и нашим объектом хранилищем диспер Jack Store было на 600 тысяч пользователей профиль нагрузки было предусмотрено что пользователи могут заходить с различных клиентов отправлять принимать письма совершать действия в календаре и другие манипуляции для того чтобы собрать кластер на 600 тысяч пользователей нам потребовалось 46 серверов серверов и общий объем данных который был сгенерирован в процессе тестирования равняется 135 ТБ на следующем слайде представлена средняя нагрузка которую мы подавали на наш объектное хранилище в течение 8-часового рабочего дня по нашим подсчетам нагрузка в 600 тысяч пользователей эквивалентно 6000 rps Давайте посмотрим как распределялись запросы в нашем объектном хранилище по оси ординат у нас время ответа в миллисекундах 99 процент Итак 58 процентов запросов шло на изменения счетчика копию данных Это говорит о том что отрабатывала дедапликация 99 процентов запросов выполнялись быстрее чем 30 миллисекунд 28 процентов запросов шло на запись данных и 14 процентов на чтение в результате первого раунда тестирования мы не получили чистого результата в системе были замечены задержки которые не позволяли системе функционировать полноценно мы проанализировали текущую конфигурацию задержки на слайде изображены фиолетовым и после анализа конфигурации выявили несколько моментов которые можно поправить а именно увеличили количество подключений к базе и настроили кэширование после чего в результате проведения второго раунда тестирования система показала стабильную работу при 600 тысячах пользователей поговорим о плюсах и минусах собственного решения Из плюсов это безусловно контроль над разработкой над приватизацией задач которые важны Как нам так и нашим ключевым заказчикам мы много поговорили об оптимизациях нашей системе и нашим объектом хранилище мы реализовали дедупликацию и компрессию данных благодаря этому наш объектное хранилище эффективнее как экономически так и дисковым экономии по сравнению с рядом аналогов а также мы стараемся соблюдать оптимальный баланс аппаратных ресурсов буквально в сентябре на петербургском хайлоут Мой коллега старший разработчик объектного хранилища Виталий Исаев рассказывал о баланс драйвон подходе к конфигурации нашего хранилища это интересно посмотрите Из минусов это конечно сложная и дорогая разработка распределенный stateful Service и все что с этим связано Сегодня вы прошли со мной большой путь от зачатков почтовой системы в связке позже и openstex Swift далее мы разобрали работу до вход dsync поговорили о кластер FS и детально посмотрели механизмы реализации собственного объектного хранилища за это время буквально на моих глазах продукт становился функциональнее на этом Уважаемые слушатели мой доклад подходит концу Я хочу пожелать и вам чтобы ваши продукты становились оптимальнее и удобнее как в эксплуатации разработки так и использовании Я прошу оценить мой доклад Это поможет стать лучше как мне так и организатором конференции Спасибо Андрей Спасибо огромное для меня всегда это было как черный ящик письма приходят письма ходят А что внутри творится непонятно Друзья давайте позади вопросы и знаете что за лучший вопрос будет подарок от компании Мой офис вижу Спасибо за доклад у меня Да здравствуйте у меня такие такой вопрос в начале доклада вы говорили что две с половиной тысячи фпс Это соответствует 100 тысячам пользователям в конце доклада вы говорили что 6000 с копейками соответствует 600 тысячам пользователей как-то оно друг другом не сходится каким образом вы пересчитываете фпс количество пользователей а здесь напрямую зависит от реализации API это абсолютно два разных продукта Как вы видели правильно что есть в начале один продукт один продукт а в конце есть другой продукт и то количество запросов которые было в первой системе для обработки одного пользователя соответственно не соответствует тому количеству которое было вот В текущей реализации объектного хранилища это отвечает на первую часть вопроса А если говорить про соотношение количества rps и количество пользователей Да там десятков сотен тысяч как О чём мы говорим сейчас то мы провели скажем так сняли статистику с одного из наших ключевых заказчиков по использованию в их организации почтовые системы и на основе этого экстраполирует эту модель применяемая по сей день Спасибо Спасибо тебе сегодня каждый задаст вопрос Прекрасно Я с удовольствием Добрый день Андрей Спасибо за доклад Добрый день Меня зовут Павел компания nfr у меня такой вопрос возник Я интересуюсь нагрузочным тестированием в принципе и в принципе в нагрузочном тестировании почтовых систем не понимаю ровным счетом ничего хотелось бы поинтересоваться каким образом вы эмулировали нагрузку 600 тысяч пользователей Ну это продолжение предыдущего вопроса отчасти У нас есть определенная модель которая соответствует работе одного пользователя скажем так то есть мы знаем сколько и каких запросов в среднем Ну это прикидываем скажем так приходится на одного пользователя далее мы масштабируем наши данные и соответственно уже применяем в инструментах нагрузочного тестирования экстраполиры на всю нашу целевую модель в данном случае 600 тысяч пользователей Но изначально все исследования были взяты из работающей системы Одного из наших заказчиков как я уже сказал Да спасибо еще один вопросик есть Они могли бы рассказать про целевую аудиторию своих клиентов это именно какие-то я не знаю год заказчики госкорпорации там допустим если вот у нас есть маленькая компания И сейчас мы пользуемся например гугловыми почтовыми сервисами там естественно там закупать там какое-то оборудование устанавливаете себя в офисе именно почтовую систему особо наверное смысла нет то может быть можно как-то просто оплатить я понял вопрос на самом деле есть абсолютно разные подходы и сейчас в текущих условиях их становится все больше и наши возможности расширяются скажем так наши основные заказчики это действительно на текущий момент это действительно государственный заказчики Мы также работаем из корпоративным сектором надо сказать успешно работы Если есть желание для небольшой компании перейти на наш продукт то не обязательно устанавливать его он примет скажем так да на своих мощностях можно воспользоваться услугами наших со с партнеров но текущий момент плотно взаимодействуем как минимум с двумя провайдерами из большой четверки Спасибо Спасибо за доклад у меня такой вопрос Если у вас какая-то логика по удалению исторических данных там например по времени там да Или по объему данных что вообще делается Когда хранилище заканчивается Да и клиент например не хочет хранить больше определенного объема Как работает удаление особенно в условиях дедупликации которая применяется Да конечно все это заложена Но тут есть такой ответ многогранный на самом деле смотря с точки зрения кого смотреть ролевая модель Да если вы пользователь то для вас файл может быть удален для начала в корзину а потом вы удаляете его из корзины и для вас файлы не существует На самом деле для администратора имеется доступ ко всем удаленным файлам и файл сразу не удаляется естественно после того как Вы удалили из корзины он попадает как бы в такое расширенный пул Где хранится еще какое-то время администратор При желании может восстановить этот файл если мы хотим скажем так зачищать наши данные то у нас запускается как в автоматическом режиме так и вручную можно запустить скажем так горбич коллектор который подчистит все данные вы можете указать например За какое время там чтобы больше месяца информация или письма у нас не хранились А так хранилище реализовано целая логика по поводу счетчика ссылок и когда на один из объектов перестает ссылаться Он после прохождения горбич коллектора также будет удален из системы точечное удаление потом хранилище можно и так и так это как точно удаление то есть вручную администратором для удобства администрирования непосредственно почтовой системы А также есть и централизованный инструмент который делает это в автоматическом режиме как правило запускается ночью когда нагрузка меньше на почтовую систему на таких объемах Как там 600 тысяч пользователей сколько вообще этот процесс длится Да и сколько он какой он какой какой нагрузку он создаёт эту систему когда этот процесс запускается в фоновом режиме в нерабочие часы мы рассматриваем если обратили внимание на восьмичасовой рабочий день был повторённый то как правило ночью нагрузка в разы меньше поэтому для системы в общем прохождение фонового процесса для удаления Писем не является проблемой и нагрузка там возрастает в среднем допустим если максимально сто процентов то это 10 процентов из 100 Это если очень усреднять очень абстрактно говорить спасибо спасибо за доклад у меня вопросы Какой у вас самый большой инсталляция хранилища по объему и связи с этим связанный Вопрос какие вы применяете инструменты для балансиров которые писать вот в момент записи и для устранения потом перекосов по свободного места например на этих дисках то есть связано например с докладами Яндекса и ВК про объектные хранилища там для почты в том числе Я был на них Вот и поддерживаете ли вы гибридные факторы репликации когда часть данных например X2 X3 а часть полтора по порядку начну со второго вопроса гибридные факторы репликации поддерживаются могу вернуться к слайду Где рассматривалось избыточность и у нас для метаданных репликация является как раз двукратный трехкратный либо пятикратный это выбирается буквально на этапе инсталляции по договоренности с заказчиком с администратором полтора как рассказывал как раз вчера представитель mail.ru Да у них также используется у нас фактор репликации полтора может быть выбран для основного объема данных для то есть тел писем и за счет этого происходит достаточно большой экономии дискового пространства так второй вопрос Напомните пожалуйста Он был первым по счету как вы выбираете диск в который писать как вы устраняете перекосы например по месту или по перегрузке например как выбираем диски соответственно есть соответствующий алгоритмы прямо сейчас там не готов воспроизвести можем в кулуарах сразу после чуть подробнее поговорить об этом а механизм выбора записи информации в зависимости от загруженности нот также заложен и он функционирует Поэтому данное распределяются и записываются в первую очередь на те ноты где например на новые добавленные ноды Где больше дискового пространства туда также будет идти запись в первую очередь друзья еще вопросы Ну а показал думает У меня тоже возник вопрос я как человек который очень любит Open Source хочу спросить рассматриваете ли вы возможность публикации этого продукта Ну субпродукта именно объектного объекта на хранилища как Open Source решения Это первый вопрос а второй вопрос возможно ли его использовать для хранения не только писем но и других типов документов и сделать для него доступ в виде API как S3 compatable например Хорошие вопросы Отвечая на первый вопрос про Open Source на самом деле насколько я знаю есть серьезные намерения выложить часть скажем так нашего кода в Open Source и это дело там ближайшего будущего поэтому Время покажет Я знаю что сейчас некоторые наши инструменты наши коллеги также выкладывают в Open Source для некоторых исследований в частности для исследования подхода Я знаю что коллеги писали некоторые адаптеры на гол для того чтобы можно было эффективнее анализировать данные и это сейчас доступно на нашем соответственно в нашем репозитории в открытом доступе и второй не менее Хороший вопрос по поводу по поводу По поводу Можно ли там хранить другие типы объектов и интерфейс в виде S3 compatable про S3 compadwell пока что в планах скажем так ближайших не заложено но Вполне может быть это действительно такой вариант рассматривался и Вполне может быть реализован и первая часть вопроса Если еще раз разные типы объектов это заложено буквально ближайшие перспективу сейчас у нас в объектное хранилище используется для почтовой системы ближайшем будущем планируется также расширить для частного облака и для других продуктов Супер это просто великолепно друзья у вас остались еще вопросы Я так понимаю нет И теперь у тебя самые важные задачи среди заданных тебе вопросов выбрать самый лучший Который больше всего понравился Давайте про использование наших продуктов для маленьких компаний и про нагрузочное тестирование был такой комплексный Вопрос вот Новый Человек мало человек могу его спросить подняться чтобы наши помощники вам вручили приз от компании Мой офис Да спасибо и я прошу вас ко мне подойти даже после я к Вам подойду точнее про подробнее поговорим про распределение данных У нас есть дискуссионная зона в которой вы сможете все это обсудить в приватном режиме Спасибо тебе большое и от конференции холод для тебя тоже есть маленький президент за такой классный доклад Спасибо большое друзья ждем"
}