{
  "video_id": "iGXwDaFni0M",
  "channel": "HighLoadChannel",
  "title": "Современные архитектуры диалоговых систем / Анатолий Востряков (Segmento)",
  "views": 445,
  "duration": 5913,
  "published": "2017-04-22T14:48:18-07:00",
  "text": "Ну как сказали компания сегменты и одно из управления чем мы занимаемся это диалоговая системы Ну общеизвестно как чат-боты больше такое слово и умные планировщики в том числе и чтобы начать заниматься в этой область по что тоже мы новички Мы сначала в общем-то перерыли много литературы статей что у нас сейчас есть в области нейронных сетей в этом направлении вот отсюда соответственно появилась тема доклада Да хотел предупредить Да это двухчасовой фактический доклад будет математика будет формулы но я буду очень стараться пояснять Вот Но я считаю что упрощать до какого-то момента можно но но не совсем то есть есть какой-то предел упрощения после которого уже непонятно как это вообще работает вот ну вас попрошу сконцентрироваться как-то и у нас да организационно Через час примерно сделаем перерывчик чтобы немножко так расслабиться и можно позадавать вопросы 5 минут и потом я ещё продолжу до конца и потом уже финальное состояние вопросов Ну ладно давайте начнём А ну первое что несколько слов скажу чего не будет в докладе что Тема очень Широкая диалоговая системы я не буду охватывать то что было Вот раньше R bas системы или шаблон ещ системы к сожалению не посвещаю ничего от интересным подходом семантическим сист антологии что в принципе хорошо тоже работает не могу сказать какую-то одну архитектуру нейронных сетей которая решит все проблемы и сможет разговаривать на уровне человека с человеком Да вот пока по-моему такого ещё нет архитектуры то есть тема ещё достаточно ну динамично развивающаяся Ну по сравнению с графикой Да где уже в принципе всё почти решено Вот хорошо распознают и там и видео даже здесь ещё будет большое развитие вот даже сказа там ещ не могу осветить нарека что-то не вошло И к сожалению не могу Ну почти Его не будет Вот Хотя очень тема интересная именно для диалогов которые нужно достигать какую-то цель То есть например это служба поддержки То есть когда звонит пользователь Ну там или он текстом пишет Вот он решит хочет решить какую-то проблему свою да то есть он не просто поболтать хочет Да он чтобы мы решили проблему или ответили на его вопрос то есть достигли какой-то цели вот в этом направлении Я считаю будет оче по крайней мере применятся for Learning вот эта тема очень развивается что хотел сказать и Ну про неё Наверное нужно сделать один доклад когда может через полгода Я надеюсь что-то будет я сделаю такой доклад вот Ну в любом случае это отдельная тема А ну что всё-таки будет Ну то что я лучшее нашёл из области нейронных сетей для диалоговых систем по состоянию на где-то сентябрь 2016 года разбита на три категории то есть система котора генерирует ответы система которые выбирают ответы из какого-то набора уже заранее известных Ну набор не обязательно это маленький какое-то количество ответов Да это может быть миллионы ответов То есть это реальные куча диалогов сделаны Ну и некая комбинация этих двух подходов в виде двух архитектур Ну начнём сведения Какие бывают в принципе виды диалогов Первый вид - это вопросы ответы самый простой и здесь возможно нейронные сети это векил может быть то есть есть другие подходы которые позволяют эту проблему решать Ну либо на том же уровне либо даже лучше Вот Но для двух других видов диалогов нейронные сети Мне кажется более подходят это диалоги которых нужно вот достигать цель как я сказал м ну и для бизнеса яркий пример - Это любая служба поддержки где сидят много народу хорошо бы её автоматизировать и сократить по крайней мере Ну в перспективе може полностью убрать людей из этой деятельности Ну третий вид - это диалоги без цели Это например там в фильмах люди говорят между собой да то есть какой-то Явный такой конкретной Цели нет ну то есть она как бы есть там подразумевается иногда Но это по крайней мере не бизнес Ну или какие-то чаты форумы и так далее Это в третью категорию но в любом случае мы здесь везде работаем с текстом То есть у нас есть текст на входе текст на выходе или так Ну дальше с чем мы имеем дело а ну я возможно повторяюсь да то есть был доклад до меня введение нейронной сети там возможно много это ну рассказывалось но я думаю Неплохо бы повторить потому что всё-таки конференция не профильная по нейронным сетям вот поэтому возможно что этого не было понятно Я постараюсь это более подробно рассказывать А ну по сравнению с предыдущим докладом А ну то есть э хочется немного сказать про то как мы собственно работаем с текстом то есть текст мы не можем непосредственно подать на нейронную сеть мы должны его преобразовать в какой-то набор чисел вот ну и два варианта которые будут рассматриваться в Да докладе это первый - это Back ofs называется А ну здесь вот на примере показано что если у нас существует всего два предложения например возьмём самый простой вариант да вот и если мы соберём все уникальные слова из всех этих двух предложений то мы получим вот такой словарь ну или другими словами вот такой Вектор из слов Вот и далее используя Этот словарь мы можем закодировать например Второе предложение в виде последовательности единичка налей единичка значит в данном предложении было слово Ну то есть вот на первой позиции было слово там единичка и там не было слова предположим Да во втором предложении там стоит нолик Ну и так далее да то есть это просто последо едини и ноликов длиной сколько у нас слов в словаре Вот так мы кодируем предложение в самом простом варианте Ну второе которы нака Многие слышали Это векторные представления в 2013 году эта тема началась я не буду вдаваться подробности но суть там ВМ что мы для токена В общем случае это токен Ну почему токен Ну это не обязательно слово То есть это может быть символы это может быть даже под слова то есть слова можно разбивать на какие-то их кусочки и к каждому этому кусочку тоже присваивать Вектор вот поэтому в общем случае это правильне называть токеном нем ще в докладе буду говорить слова что там это в общем случае тон Вот и было придумано много алгоритмов два из них которые очень вот известны На слуху этог не вдаваясь как они работают суть их такая что после То есть просто на сырых текстах какого-то английского языка или русского языка обученный то есть там нет никакой нотации в этом тексте они получат для каждого слова некий Вектор оден кото самого начала при обучении то есть там 100 200 там 300 чисел и вектора вот эти обладают таким свойством что два вектора слов которые часто встречались в одинаковых контекстах Ну то есть слова вокруг них были примерно одинаковые они в векторном пространстве оказываются рядом то есть угол между ними Да если взять только многомерным Ну так я руками показываю двухмерное пространстве многомерном пространство это угол между будет маленький да тем более у них одинаковый контекст вот а косинус угла соответственно у них будет наоборот большой стремиться к едини и вот эти вектора если сложить вместе то мы получим матрицу вот такой размерности В на Ну её ещё называют Матрица я вот это слово буду использовать дальше в архитектура вот есть это набор всех векторов всех слого язы сложенных вот так вот одно под другое Да вот что такое матрица Ну следующий строительный блок я ну повторю Да это уже было кто уже присутствовал это курент нейронные сети очень популярны вот при работе с текстом Ну при любой последовательности Ну Суть в чём Ну в отличие от обычных нейронных сеточных и фиксированного размера То есть это может быть либо Вектор числовой либо Матрица Ну как картинка да то есть картинку можем превратить некую матрицу чисел вот кутх нейронных сетей там не так у нас есть последовательность и мы не всю сразу последовательность подаём на вход рекут Нейро сети мы её подаём по словам а Ну на самом деле не по словам Да по векторам вот этих слов То есть каждое слово мы заменяем на его Вектор в которым мы заранее обучили то что я на предыдущем слайде рассказывал соответственно внутри происходит вот эта вот математическая операция в самом простом случае рекут нейронной сети а ну я постараюсь её пояснить что ничего страшного здесь не было а вот это вот у нас Вектор слова текущего на шаг Т Да на временной это Матрица wx приумножение мы получаем вектор А здесь тоже происходит перемножение некоторого внутреннего состояния Ну у любой курент нейронной сети есть внутреннее состояние или hidden State е по-английски называют вот перемножаются на другую матрицу тоже получается Вектор два вектора складываются между собой и от этого вектора берётся поэлементно нелинейная функция какая-то Но в данном случае здесь гипер тангенс вот это вот закодировано ну бывает целый там набор нелинейных функций тоже за пределами моего доклада вот ну и вот эту вот операцию мы получаем то есть после вот этого получаем следующее скрытое состояние которое мы передам как бы на следующий шаг И вот так вот по кругу подавая последовательно слова постоянно крутя как бы по кругу его внутреннее состояние из предыдущего состояния вычисляя следующее и так далее мы можем пройтись по всей последовательности и на каждое состояние на каждый вход мы получаем некое скрытое состояние hidden State 0 H State 1 то есть Сколько длина последовательности слов столько у нас хн СВ на выходе Ну вот это правило оно работает Для любой рекут нено Стих там много видов есть вот вот это самая простая как я сказал что ещё тут забыл сказать что вот эти вот две матрицы это и есть те параметры нейронной сети которые обучаются там в процессе обучения да такой масло маслено сказал а а да И вот последнее что я забыл сказать Вот недостаток вот такого простого вот этой формулы заключается в том что она достаточно быстро начинает забывать что у неё было в начале последовательности Ну то есть если в начале было какое-то важное слово после которого мы вот что-то там хотим узнать про это слово в начале то она уже забудет через там 5-10 слов Вот и чтобы эту проблему решить в дено сем году был придумал блок вот намного сложнее да чем предыдущий Вот вот эти формулы я тут привёл только для того чтобы не то что сечас понимать будем все последователь тут шесть вот ну просто показать что ничего сложного в блоке в принципе не происходит чисто математи вот э вот операциях векторов Следовательно в один длинный Вектор да Ну дальше у нас опять перемножение Ну всё что W большое - это матрицы это те параметры которые будут обучаться то есть они в начале инициализируется какими-то случайными числами просто вот а здесь ну то есть умножение опять матриц на векторов сложение векторов опять полимет взятие нелинейной функции в данном случае это сигмой вот такой буков G обозначается Ну и так далее да то есть я там не буду всё это расписывать Ну просто фактически математические операции простые Вот то есть для этого я их и привёл вот Ну ещё раз скажу что вот это вот у нас значит обучаемые параметры вот это вот байс это вектора это тоже обучаемые параметры вот остальное - это входные данные и внутреннее состояние hidden State А ну lstm хорош до какой-то длины последовательности Он сохраняет информацию по которой он прошёлся то есть как бы её прессует другими словами можно сказать в один Вектор понемножку Вот но у него тоже есть пределы то есть там до ца где-то слов нормально работает но Дальше он тоже начинает забывать что было в самом начале вот а более простой вариант это Gate recurrent Unit называется тоже ещё один из строительных блоков Ну тоже курит но сетей Ну тут на две формулы видно меньше То есть он вычислительного у него Ну только слегка хуже чем стма его тоже достаточно часто применяют в архитектура Ну в частности там будут примеры где архитектура которых этих м где он применяется А ну последний строительный блок который я не буду детально расписывать это вот свёрточные нейронные сети которые я надеюсь были на предыдущем закладе А ну очень популярны в графике В общем если нейронные сети и графика то там всегда будет CNN ну здесь на картинке Там две простые математические операции на самом деле под этим сином В общем Кто хочет более детально легко найдёт в интернете или из этого доклада по ссылке пойдёт и прочитает как это всё работает а ну вот перейдём к первой архитектуре начну я с генерирующих архитектур а они уже в принципе упоминались в каком-то некотором виде то есть или Другими словами это sequence To squ архитектуры А ну перед тем как вот детально по вот эти квадратики расскажу как всё это работает а хочется сказать что с ними сложно их сложно обучать ну я там ещё много перечислю но у них есть одно очень большое преимущество почему оно всем интересно вот они преимущество в этом тексте заключается они действительно могут в принципе сгенерировать ответ которого не было в обучающей выборке Ну то есть там оператор так не отвечал да вот в диалогах записанных между пользователем Ну так дословно пока не отвечал вот то есть они могут генерировать действительно новые ответы Ну в каком-то смысле это начинает действительно напоминать какой-то разум какойто ител Дава пори Как это работает на этой картинке Ну тут двумя цветами как бы она разделена на две части вот коричневым показан энкодер а синим показан декодер вот энкодер он закодируют входную информацию последовательность Ну или запрос пользователя Ну в общем смысле это не обязательно вопрос Это может быть какой-то запрос пользователя Да вот и декодер соответственно декодирует это в ответ То есть генерирует другими словами ответ вот Ну вот эту вещь в принципе я уже описал на предыдущих слайдах это обычный СТМ блок Ну или курент нейронная сеть там в общем случае сказать А часто она двунаправленная то есть последовательность слов хорошо проходить двух направлениях одновременно Ну в зависимости от языка иногда там немецком иногда что-то формулируется в конце А в начале ещё это непонятно про что там ну и так далее То есть в русском тоже порядок слов любой Поэтому лучше проходить в двух направлениях а получать два набора хитов это то есть две независимые рекут нейронной сети одна в одном направлении то есть они независимые В смысле у них свои параметры все вот эти матрицы внутри свои Вот Но получается два набора хитов которые потом контини вместе и могут передаваться на следующий слой вот на второй Вот это да как вход То есть у первого слоя вход это вектора слов а второго слоя вход - это предыдущий его н сй и предыдущего слоя Да понятно надеюсь Да вот ну и в самом простом случае мы проходим в эту последовательность вот эти все хиден стей которые промежуточные последнего слоя нам не интересны пока мы берём последний и его передаём А в начальное состояние хн сй декодера Ну своего слоя то есть там тоже несколько слоёв это ну независимые тоже куре нейроны сети тот же принцип то есть здесь мы подаём вектора слов только уже ответа то есть они из запроса Да ну начиная специального такого токена начало предложения потому что Как это работает то есть мы сначала подаём токе начала предложения знаем некое спрессованного Н стейта вот и из него генерируем первое слово ответа потом это это сгенерировано слово его Вектор точнее мы передаём опять на вход Ну и так далее да то есть мы вот постоянно вот такой змейкой двигаемся туда-сюда пока мы генерируем символ конца специальный такой символ конца предложения по которому можно понять Всё мы сгенерировано не сгенерировал генерировать слова Вот так вот по кругу то есть примерно это так работает Ну и вот на выходе декодера У нас есть тоже набор НВ мы вот этин ты прогоняем через такой for То есть это фактически тоже перемножение матри на Вектор Плюс некий Вектор баса эти они обучаются w и берём от этого операцию Soft не знаю насколько все знаете что это за операция Но если у нас есть Вектор чисел то после сома у нас получается фактически Вектор вероятности то есть распределение сумма этих чисел будет равняться единице вот что делаться в Макс по сути То есть можно сказать что это распределение вероятности если выход у нас равняется словарю всех слов у нас поделение вероятности по всем словам Ну там русского языка и где вероятность всех больше значит это слово скорее всего правильно сгенерировать то есть мы его и как бы генерируем вот то есть просто по этой вероятности выбираем у кого слово самая большая вероятность вот обучается эта штука с помощью вот этой функции Её называют то есть мы берём логарифмы от вероятностей Какие должны быть слова правильные мы заранее знаем какие слова когда мы обучаемся есть эти примеры всех запросов и ответов Ну то есть у нас есть предположим корпус диалогов службы поддержки Ну какого-то там крупного банка например да ну тиков банк например выкладывал свои датасет реальных диалогов и соответственно мы можем знать какие слова должны быть в каждый временной шаг Ну в какой посредственности Мы можем взять логарифм Какой сейчас реально получился вероятность этого слова просуммировать их получить сонно там вот правильно заметить знак минус есть потому что вероятность Она всегда от нуля до единиц а логарифм соответственно от минус бесконечности до нуля будет да математически вот чтобы нас получить от плюс бесконечности до нуля нужно там ещё знак минус поставить вот ну и мы вот эту функцию фактически минимизируем взяв там производную от неё ну я там не собираюсь сечас углубляться как собственно обучаются нейронные отде рассказывать вки про архитек Да я забыл сказать Что На каждом слайде там в углу где там если что-то интересно почитать собственно статья приводится где эта архитектура используется и где я нашёл код Ну реальный код можно просто взять код и с ним начать играться вот на каком-то из фреймворков Ну это не всегда один и тот же фрево но чаще всего я старался найти именно для как самого популярного фреймворка на данный момент Ну хотя и свежего Да ну и вот как и было в предыдущем докладе здесь недостаток этой архитектуры что мы всю длину последовательности с как бы закодируют последний hidden State и от него начинаем как бы разворачивать генерировать ответ вот это не очень хорошо потому что есть предел Как бы Сколько можно прессовать в один Вектор вектора кучу других слов да то есть если у нас длинная последовательность то мы что-то явно потеряем Поэтому соответственно в 2015 году был Придумано такое понятие ан Ну точнее оно не было придумано в этом году нуно как оформилось а математически что ли Как это применять а ну здесь вот на схеме показано То есть это предыдущая архитектура то есть только мы взяли один слой ну здесь два слоя здесь я упростил чтобы это не загромождать представим что у нас здесь у энкодера и декодера всего один слой А и мы берём в какое-то состояние декодера его hidden State в какой-то момент времени Т А и дальше мы его как бы перемножаемых случае это просто перемножение двух векторов с каждым хн тейтом декодера с каждым и получаем в принципе число на каждое входное слово мы получаем некое число то есть его вес фактически данного слова входного насколько оно влияет на генерацию выходного слова Вот за счёт просто перемножения двух векторов Ну это Линейная алгебра То есть я тут не буду пояснять Что такое Линейная алгебра вот ну вот там приводится вот эта самая простая операция Ну можно что-то посложнее делать врете мы равно получаем число одно потом мы вот этот Вектор чисел прогоняем через опять через Ма вот здесь кстати расписано математически что за операция это сома там экспонента здесь сумма экспонент и получаем соответственно вот Вектор весов каждого вот этих входных последовательности НВ насколько они влияют как бы на генерацию данного конкретного слова при выходе Вот соответственно дальше мы вот эту математическую операцию делаем то есть мы со своим весом State умножаем и все их складываем в один Вектор C назовём Ну или Вектор контекста его можно назвать Вот и вот этот Вектор C здесь по стрелочки дальше мы контини с исходным н йм декодера вот прогоняем через слой Ну вот этом просто перемножения на матрице Ну берём там нелинейность Ну и дальше как мы в предыдущей архитектуре мы можем ещё прогнать на один слой там и получить распределение по всем словам Ну на данный момент шаг времени Т вот Ну надеюсь Примерно это понятно как работает ну в другом случае если так посмотреть на эти формулы то это в принципе просто перемножение векторов оден последо суммирования векторов какие-то нелинейности это тоже сильно улучшает как бы качество генерации ответов доста также архитектура архитектура же самое вот э же архитектура которую в принципе рассказал то более больше слоёв там немножко больше деталей всяких вот ну суть у не такая же использу сейчас вот Google которы они в сентябре залили для не для всех правда пар языков но в частности английские китайские пары у них в продакшене уже вот именно эта архитектура работает когда обучили на нескольких миллионов на корпусе субтитров из фильмов английских Вот и задали тему Ну то есть стали спрашивать некие философские вопросы здесь сказа то она в принципе стало выдавать какие-то вполне осмысленные ответы на это Ну то есть там В чём смысл жизни служить более хорошему в ЧМ смысл существования жить вечно Ну и так далее есть и вот этого скорее всего не было в где-то в субтитрах да то есть она сама Действительно сгенерировал это в каком-то смысле то есть Она собрала какую-то вот кучу кучу миллионов диалогов людей реальных и начала генерировать что-то более-менее ме смысле Ну а с там с вашей стороны значит с правой стороны Да там из службы поддержки как раз тоже диалоги тоже несколько миллионов там было диалогов реальных службы поддержки Ну это открытые датасеты в принципе их можно тоже самим обучаться на них вот ну и там получается в принципе какая-то достаточно осмысленная беседа которая даже решает как-то вопрос пользователя но посередине Вот реальное применение это Gmail клиенте у Гугловский то он как раз только в этом случае срабатывает а ну здесь ФОКа Ну да е как раз последнее состояние показано то после того как вы открываете письмо на которое вы хотите ответить то он предлагает тамри вот архитектура которая генерирует эти ри варианта ответа это то что я рассказал только что вот просто Бер три самых лучших варианта это вот Google таже архитектура в принципе там с некими модификациями ещ использ gole некий межер мало популярны воз вот один вопрос и один ответ но если у нас есть реальный диалог это обычно последовательность Обмена информацией то есть пользователь что-то спрашивает оператор уточняет а пользователь дополняет информацию и так вот несколько раз они могут это уточнять и в конце Там оператор может ответить и там пользователь сказать спасибо и оператор попрощаться да то есть есть некий диалог вот Ну вот в эту архитектуру это не очень хорошо как бы всё входит Но единственное как это можно было бы вот в этой архитектуре если ничего не добавлять шить то все вот эти предыдущие высказывания пользователе ответа оператора выстраивать вот такую длинную последовательность то есть начиная с первого вопроса пользователя потом ответ оператора Следующий вопрос пользователя всё вот это вот последо подавать каждый раз да и ну генерировать следующий ответ на последний вопрос но с учётом всего диалога который был до этого вот ну и более как бы правильно сделать было бы вот это следующая архитектура можно сказать нейронные сети Ну в целом она похожа на предыдущую Единственное что здесь как бы есть два логических уровня первый уровень - это уровень вот слов в предложении и второй уровень - это вот уровень уже предложений Ну вот здесь как бы одновременно на картинке может немножко сбивает показано два примера одновременно то есть вот лум расе этова разных Сония ко первый раз задал вопрос и система первый раз сгенерировал ответ потом если мы сечас перейдём дальше состояние диалога человек не понял что ему дальше делать То есть где там всё-таки это булочна вот он начинает уточнять Вот и мы начинаем генерировать следующий ответ но с учётом всего что было просно до этого вот ну здесь каждый квадратик там на всякий только единственное что мы здесь по каждому предложению отдельному каждому высказыванию с обоих сторон мы проходим отдельно и берём только самый последний State каждой Да вот то есть там у первого мы берём Ну и так далее И вот эти последние нй мы переводим на следующий уровень где мы ещё второй раз про ним проходим он уже Отдельно курент неровности ещё отдельным СТМ и получаем тоже опять набор хитов Мы тоже опять можем взять Последний из него начать генерировать ответ вот что здесь показано вот в остальном в принципе это архитектура которая вот ну всё то же самое как в предыдущей архитектуре только как бы у нас есть уровень слов предложений каждому отдельно проходим потом уровень предложений по которому мы проходим только по последним хине стой тамм этого этих предложений А ну ссылка код ну ссылка на статью Ну всё на английском К сожалению Ну как и предыдущий закладчик все темы все статьи более-менее интересны всё на английском вот если кто не знает Надо учись так дальше соответственно ну две такие самые актуальные архитектуры на данный момент из генерирующих какие у них есть проблемы Ну первоя это соответственно нужно очень много данных реально что доходит оно обучается на нескольких десятках миллионов если даже не сотен миллионов примеров есть ЕС для диалоговых систем то нужно несколько миллионов реальных Чтобы достичь какого-то качества более-менее осмысленного а не просто чтобы она генерировать что-то смешное более-менее там ну осмысленная но никак не отвечающая на вопрос пользователя вот а как эту проблему решить когда Ну обычно мало у кого есть такие датасеты несколько миллионов соответственно вот этот Transfer Learning если обобщить работаю данной область То есть все вектора слов мы обучаем просто на каких-то неструктурированных текстах получаем уже более-менее осмысленный вектора слов Вот и дальше Мы можем взять если у нас есть любые логии человеческие не обязательно по нашей теме Ну например там если мы ту же службу поддержки для какого-то банка делаем мы можем взять диалоги Ну вообще там из любых магазинов каких там или вообще из фильмов то есть диалоги между то есть главное что это были люди с обоих сторон говорили то есть обучаясь на них мы уже какую-то информацию об окружающем мире нейронная сеть выучит вот а потом после вот этого обучения мы можем учить уже на нашем небольшом каком-то датасете из нескольких там ну овно это должен быть несколько сотен тысяч десятков тысяч даже с таким предо скорее всего ничего не получится вот ну это такие как можно по крайней мере недостаток данных решать но вторая проблема никак не решается это Вычислите сложна нуж GP на пу почти нет никаких шансов это всё обучать даже на самых мощных даже на кластерах поэтому ми GPU иногда может не хватать если у вас много данных ну в общем придётся либо покупать либо арендовать третья проблема такая технологическая - это то что они склонны генерировать общие ответы на любой запрос Ну потому что в датасете когда действительно большие объёмы то люди На многие вопросы отвечают одинаково и она начинает скатываться при обучении каким-то общим фразам Ну вот например пример когда обучали в самом начале SM вот от Гугла Ну вот эти на письма отвечать то чаще всего после обучения если ничего не делать Она начинала генерировать Я тебя люблю Ну почти на любое письмо Да более-менее там короткое соответственно вот эта проблема есть Ну и вторая проблема что она всегда обучается на правильных данных и когда в реальности в продакшене она начнёт генерировать какой-то ответ и сгенерировать какое-то неправильное слово может то после этого она нач генерировать бред потому что она никогда в прос Викой ситуации то есть никогда не встречалось какое-то неправильное слово Ну и она не знае как себя вести начинает генерировать какой-то достаточно бессвязный текст Ну что можно делать с последними двумя пунктами несколько вариантов хотел бы уже минут 10 посветить вот этому может 15 то есть ну первый вариант тут начинается такая жёсткая первую формулу посмотрим а первая формула - это то что мы до этого делали это наша loss function то есть мы фактически если так словами сказать не в давай с эту всю математику как бы то что написано мы стараемся максимизировать ответ в зависимости от запроса то есть S - это Source по-английски тут то есть запрос пользователя некий или вопрос Т - это Таргет то есть ответ то есть мы мы обучаем так неровную сеть чтобы максимизировать ответ который Ну нам был известен уже с диалогов в зависимости на его запрос вот ноно более правильно в принципе оказывается максимизировать не вот эту функцию А максимизировать их взаимную как бы информативность вопроса и ответа Ну по-английски это называется вот это вот э вот эта формула Ну тут за счёт Ну тут теория Верности конечно используется и вот эти вот вот эта штучка и де она превращается вот в эту вероятность математически превращается в разницу логарифмов чи математика вот мы получаем вот такую функцию нашей потери которую мыть то есть вот это то что мы делали до этого вот это логарифм вероятности ответа на запрос в зависимости от запроса Да минус какой-то гипер параметр лямбда который мы сами руками просто зада вообще ответ так скажем То есть это в NP только тся то есть психология не имеет отношения это называется Ну сейчас чуть попозже скажу что это может быть за этим ну второй вариант вот если мы вот вместо вот этого подставим вот эту математическую операцию там немножко сделаем неско преобразований полум нци потери кою будем оптимизировать вот то что тут двумя красными подчёркнуто это ну здесь я уже сказал это Mod или в принципе ну который можно сделать на граммах не буду сейчас даваться подробности что это такое или это просто может быть тоже не ну нейронная сеть А вот это тоже нейронная сеть которую до этого рассматривал только наоборот мы на вход подам не запрос пользователя а уже ответ и генерируем запрос пользо мы переворачиваем вот то есть мы обучаем ту же архитектуру как и до этого сейчас я Отмотай немножко Вот эту Да только мы меняем их местами то есть вот это идёт на вход А это мы начинаем генерировать то есть две независимые нейронные сети обучаем на тех же данных только одни правильно расположены да то есть из запроса ответ мы генерируем а другие наоборот тот же засе То есть просто перевернули и обучили две нейронные сети вот тот то что там ещ в первой формуле было показано вот этот логарифм от P Отт это если представите у нас нет энкодера вот здесь у нас есть только декодер и мы просто обучаем эту нейронную сеть генерировать свои же ну свои же предложение только со сдвигом на одно слово то есть мы подаём как бы начало предложения генерируем первое слово первое слово подаём на вход генерируем второе слово и так мы просто прогоняем предложение тем самым мы можем в принципе создать langage Model которая будет как бы в конце вероятность вообще такой фразы существования насколько она человек может её сказать то есть другими словами так Ну вот возвращаясь к этому Ну более подробно опять же статья про это Ну вот это вот подход Он позволяет решить одну проблему по крайней мере Вот эту вот генерировать уже не общие ответы а более релевантные к запросу но не решает вот эту проблему в принципе то есть если она ошиб то она начнёт генерировать так же бред как и раньше а вот чтобы решить все проблемы что ли А эти последние две по крайней мере вот интересный есть подход который был Гуглом применён Сентябрьская статья Ma Translation это генерировать Ну как бы использовать reinforcement Learning при генерации не знаю на кто лько слышал что такое reinforcement Learning но там есть несколько концепций которые сейчас расскажу то есть Сначала мы используем SE и обучаем так же как и до этого то есть никакого reinforcement ленга с помощью вот этой функции я её приводил Раньше просто здесь фроки ну обозначения немножко другие То есть теперь X - это запрос пользователя вот а Y - Это ответ ну или там сне Да сгенерированный ответ А это L function но она уже была до этого просто немножко по-другому записана вот а после какого-то момента мы начинаем подключать ещё вторую L loss function то есть функцию потери Вот которую мы должны минимизировать сейчас я постараюсь её рассказать вот это вот P там в скобочках игры так икса зависит это мы генерируем после какого-то обучения 15 случайных последовательностей Каждый последователь когда мы генерируем мы на каждой вот позиции ответа Мы выбираем случайно слово но с учётом его вероятности То есть то что мы уже обучили то есть слово которое более вероятно сечас генерировать оно будет чаще выбираться случайным образом вот ну то есть такое сапли делаем и так вот мы 15 последовательно генерируем для каждой последовательности вот это вот начинается мы считаем некую функцию рда вот что это может быть за функция в данном случае Для текста это может быть например то что Google назвал glore то есть какой-то клей ЕС по-русски перевести он считается и снева ранее знаем ответ мы его разбиваем на граммы Ну то есть Идите прямо и направо да предположим если у нас исходная фраза И если мы Разобьём на две граммы это текст то у нас получится такие граммы Идите прямо прямо и и направо да то есть вот такое 4 грама получится и предположим мы сгенерировать предложение с ошибкой Идите прямо и налево да там не направо Вот соответственно сколько-то Гра покажутся одинаковыми ри из четы од неправильна да то есть и вот мы посчитали Сколько было правильных ри поделили Сколько было неправильных то есть не сколько неправильных А сколько было Гра В правильном ответе вот ну потому что система может разной последовательности сгенерировать как бы ответы не обязательно той же длины Да ответ будет сгенерирован который должен быть может быть длинее может быть короче кочево заданном точнее и то же самое количество совпа всех нграм на определённое на количество нграм у сгенерировано ответа и потом от этого мы просто берём минимум Какое из этих двух чисел оказалось меньше и это в общем-то фактически наша награда м То есть если она стремится к единице то значит мы сгенерировать действительно абсолютно тот же ответ который и должен быть Ну если ноль Значит мы не ни одна грамма не совпала то есть мы полностью отличное э предложение сформулировали Ну вот он считается для каждой из эти последовательностей там все суммируются есть вероятность после умножается на для каждой вот последовательности все это суммируется такая но отдельно Не используют Они они складывают вместе с предыдущей получается вот такая скро которая уже добу нейронную сеть более релевантных ответов не более обобщенных и она так как она генерирует сама предложение то она иногда Может сгенерировать что-то с ошибками соответственно она более устойчива к своим же ошибкам становится в продакшене если общем это реально то что используется в Google шить эту проблему генерации обобщённых ответов если мы представим что у нас есть диалоги которые разбиты уже на какие-то категории Ну то есть у нас есть например форум большой и там уже все диалоги между пользователями там ну или между пользователя пользователями или там службы поддержки происходит уже ну разбита на категории Да соответственно мы можем обучить отдельную нейронную сеть пока не связанную с нашим вот архитектура Ну и хорошо достаточно себя зарекомендовали точны сети а я не буду сейчас подробно рассказывать её как это всё тут работает вот но суть В чём она обучается просто по запросу пользователя какому-то Да а просто его относить к какому-то классу Ну какой-то теме точнее да как это существует на форуме А ну и на выходе сколько у нас там классов нужно предсказать последний слой это вот столько там у нас будет выходов у у неровной сети но нам не это нужно мы берём не последний выход а мы берём предпоследний слой это рекут ой не рекут точнее Вот это CNN архитектуры и его используем качестве вектора контекста Вот и дальше мы этот Вектор можем как бы вот эту генерирующую архитектуру А ну так микшировать разными способами то есть запихивать в разные места sequence to sequence Ну первый подход это то что называется Contex lstm это если вы помнили сес назад отмотать У нас есть декодер это тоже последний слой если взять то это в принципе СТМ вот ну там вот если ещё немножко вспомнить с этими формулами Да и сейчас вот если назад вернусь то вы увидите что это те же формулы только то что вот подчёркнуто красным это вот у нас новое ещё добавляется туда внутрь в четырёх местах тот Вектор который получен из предыдущего слайда вот это вот CNN обученного Вот то есть он кши Вектор контекста при генерации каждого слова при декоре вот Ну тоже самое можно его этот Вектор контекста микшировать последний слой при получении распределения по словам в текущий момент времени Ну самый лучший результат показывает всё-таки замешивание его механизм вот ЕС помните Я просто использовал hidden State декодера какой-то на данный момент времени т а Но мы его не сразу используем А мы его вот такой математической операции смешиваем с этим вектором контекста C каждым своими весами получаем вместо вот hidden вот этот HT Gate T А дальше так же как это всё в теншен происходит примерно Ну то что я вот описывал на этом слайде только вот здесь вот где у нас было HT зде формула где-то если встретите там будет тепер GT как бы то есть это уже смесь нй какого-то слова декодера с вектором контекста смешанным вот а Но если кто-то будет когда-то читать эту статью где это вот метод применяется то там они что-то более сложное делать но что детально к сожалению не рассказывают у них та коммерческая тайна вот ну я и письмо в смысле писал и не признаются вот этот Вот это седьмое Да вот это вот вот Ну некая нелинейная функция так ЕС вот не сильно удаваться то есть вот это тоже нелинейная функция то там гипер Но это конкретные формулы То есть это не нам написано это действительно формула все Какие предыдущий рассматривал просто как туда в СТМ запихивает что-то ещё дополнительное вот ну примеры диалогов которые вот эта архитектура вместе вот с этим вектором контекста когда обучается такая статья но где-то они её применяют в реальности толь Ну скорее там для китайского языка они это делали в реальности но они демонстрирует что на уних стала действительно более релевантные ответы давать Ну вот это вот таким жирным выделено это вот ответы сгенерированные уже машиной и она стала более устойчивая к всяким ошибкам то есть они специально добавляли какие-то там ну и просто там где-нибудь да Или или там Чайна неожиданно вставляли ача да то есть какие-то случайные слова но всё но тем не менее не сбивалось и генерировать воды там или поесть или там чай какой-то китайский выпить и так далее А давайте вот здесь сделаем перерыв если какие-то вопросы чтобы пока не устать дальше мы перейдём к выбираю архитекторам которые выбирают ответ уже то есть они не генерируют да Если какие-то есть вопросы то да я так сделал неожиданный шаг есть вопрос такой там где была вот последовательность энкодер декодер Да можно слайд Да вот там где была вот последовательность слов в правой части вот Идите и прямо то есть получается мы просчитывать вероятность что после идите идёт прямо правильно Ну да каждый раз Мы учимся максимизировать вероятность слова прямо после слова идите но учитывая весь вопрос конечно да то есть да а вот я хотел спросить то есть мы будем учитывать вот скажем связку Идите и и то есть через одно слово и вот такое вот то есть там идите и направо через два Ну то есть более такие сложны большим количеством шагов чем один Не ну это всё зависит от того какая у нас вот на чём он тренируется То есть если там были такие примеры то она будет как бы учиться генерировать разные варианты если на один и тот же вопрос у нас было два разных ответа но она к какому-то общему ответу будет скатываться что ли Да скорее всего более короткому если не применять никаких там трюков которые Ну я тут не говорю вот чтобы ну то есть скорее всего они ещ будут более коротким ответам скатываться То есть получается что она может Как переучиться так и не доучиться зависит от данных да Ну как я отметил данных доже действительно Гене систе очень много миллионы реально миллионы Спасибо скажите пожалуйста вот Как вы оцениваете системы типа А вот которые именно семантику позволяют Ну мнени очень нравится есть я в кон доклада хотел бы сказать возможно эту проблему не решат Да диалоговых систем возможно Вот именно какой-то комбинация нейронных сетей и например комна с её антология с Вот это деревьями возможно покажет лучший результат НОК сожалению я пока не знаю как это интегрировать то есть ком хорошо Например у них есть продукт вы знаете это То есть когда даётся текст и они там с помою в принципе самос правил которые человек создат могут эть из структурированных текстов какую-то информацию там Ну например если это какие-то договора то они из него могут истратить там главу компании компанию Ну позицию там не знаю её там доход этой компании и так Дале Ну да не логично ли Вот на диалог именно понять смысл и потом отвечать да то есть не генерить там 100 млн примеров То есть как не Ну логично Да я не спорю что это логично Вот почему до сих пор как бы нет прогресса хорошего Ну такой уже философский вопрос Почему нет прогресса А ну то есть вот Акана я там ничего не увидел то есть какую-то реальную задачу там нереально Не ну я покажу вот в выбирающих архитектура Там есть ну забегая вперёд Memory networks вот их можно э комбинировать с данными которые были вытащен например компрено из неструктурированных текстов Ну например из Википедии можно не буду сейчас детально потому что я й тоже буду рассказывать вот а и вот этот knowledgebase можно использовать А как бы при генерации ответов Ну или выборе точнее ответов и существующих а Ну почему Ну в смысле я сказал пока эта проблема Не решена То есть наверное достаточно сложная потому что она в принципе наверное на уровне действительно интеллекта человека то есть мы пытаем а"
}