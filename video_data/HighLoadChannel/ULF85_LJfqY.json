{
  "video_id": "ULF85_LJfqY",
  "channel": "HighLoadChannel",
  "title": "Высокопроизводительная и отказоустойчивая архитектура фронтальных систем / М.Зелинский (СберТех)",
  "views": 3538,
  "duration": 3111,
  "published": "2017-04-22T14:48:19-07:00",
  "text": "максим зеленский и он расскажет вам об архитектуре я единой фонтаны системы генных фронтальных систем прошу любить и жаловать всем привет сейчас включу презентацию нокий так всем привет опять меня зовут зелинский максим я бы хотел бы рассказать про отказывать свою архитектуру единой фронтальной системы я занимаюсь созданием и развитием платформы для единой фронтальной системы в компании герман технологии до этого я больше пяти лет работал в духе банки и моя главная специализация эта архитектура высокопроизводительных я отказоустойчивых систем итак первую для кого этот доклад кстати кто-нибудь вообще читал тезисы этого доклада что принесет я приношу извинения там доклад on the sets a немного сумбурно и если честно я не знаю кто их туда скомпилировал это из моих какие-то предыдущих докладах я пытался то напряжение последних пару дней их поменять но тем не менее я надеюсь доклад будет интересно потому что на клад на самом деле шире чем те тезисы которые там указаны стать еще один quiz кто-нибудь вообще избил тех и тут есть поднимать меня вергара в уже уже не бьют за продукты сбербанка поэтому можно свободно говорю что есть брать эхо таки я и сдачи банка кто-нить выше края матвей ok соответственно у меня доклад и разбит наверно можно сказать на две части первая часть это такое более-менее теоретическое выручайте она ориентирована на разработчиков которые делают франсин applications возможно не такого большого объема как например делаться сбивая кинг и кому интересно вообще про архитектура подхода как сделать вообще from phishing приложение отказу случае мы производим авто часть докладывает уже для северных архитекторов это я расскажу про некоторые наши изюминки которые мы применяем в компании сбербанк при создании наших систем сожалению 1 часть 2 часть устроить без первой части сказать невозможно поэтому если кто знает основы я приношу извинения придется немного и послушать какие давайте начнем что с того же что же такое единой фронтально система применительно к сбербанку начнем с со слова фронтальная фронтальная система это система часть которой клиенты заводиться с услугу с услугами банка то есть это интернет-банке у нас их три это для физика для юриков для финансовых институтов это банкоматы системы в отделениях колл-центра и колл-центра тоже фронтальная система и так уж сложилось исторически что в банке для каждого из фронтальных систему была своя реализация то есть каждая фронтально система представляет собой отдельно независимую как бы систему которая была разработана либо in-house либо это продукт какой-то сторонний либо это вендоры разработал а тем не менее банк он пытается идти в ногу со временем и вводить какие-то современные новые продукты на рынок при этом банк хочет выводить продукты во все свои каналы текущая ситуация означает что банку приходится обновлять все системы все интернет-банке и которого деление стоят и все-все-все и оптимистично если очень сильно повезет продукт выводят где то за год обычно выводит какой-то новый продукт на все каналы примерно ввода за 22 это еще и еще хуже вот это первая часть вторая часть это то что банк хочет чтобы во всех каналах был единый набор услуг то есть что вы могли например вклады открыть и в интернет-банке и в отделении через банкоматы через колл-центр через вообще любой по сути канал взаимодействие с банком опять-таки когда мы имеем отдельной фронтальной системы для каждого из каналов канал и ford снимать по сути одно и тоже иногда буду менять слова и тот час же бу это означает что в каждой фотосистеме на нарисовать вот эту новый продукт каждый раз по новой то есть мы реализуем там открытие вкладов нотами один раз потом открытии вклада в 3 банки 2 1 3 1 4 1 5 1 и бывают курьезный случай когда эта реализация иногда бывают разные то есть хотя они даже до сих пор остались некоторые нюансы когда в одном канале например функционалов по-разному работает а ви таки это влияет на тайм ту market опять-таки бизнесу приходится 5-6 раз переделать ну по сути реализовать одно и то же что же виде нахватали система для банка это как раз попытка ускорить вывод новых продуктов путем создания некоторые единые фронтальной системы которая объединит все текущие фронтальной системы при этом это объединение на самом деле это большая случаев переписывание на используя уже некоторые единый стек технологий единую архитектуру и используя так называемый единые сервисы бизнес-логики то есть одна из главных концепций это то что мы делаем сервисной об открытии вкладов один раз и в каналах в фонтан в каналах уже там этой банке в теме мы его просто докручиваем под специфику канала тем самым бизнес реализует логику один раз это где-то 75 процентов всего вверх и до 25 процентов это он закручивает до специфики канала а все остальное отказоустойчивость производительность системные сервисы уже предупредила перед предоставляет по сути платформы которые вот я и занимаюсь ok наши показатели в одессе 16 год я тут привел цифры только по одному из наших об одной из наших фантазий системы интернет-банк для физических лиц соответственно с общее количество пользователей сейчас это 85 миллионов общее количество активных пользователей 48 миллионов это пользователь которых вы хотя бы раз зашли из за три месяца в интернет-банк и у нас один из миллионов финансовых операций в день при этом именно операции и опять-таки финансовых это означает то что на самом деле транзакции гораздо больше если мы говорим то минут она транзакция там базе данных эту цифру можно спокойно умножить на 15 но тем ни менее цифр для банка довольно-таки приемлемо и ну например там для какому супер-супер-супер хайло до нашей количество операций в день они конечно не является большим показателем зато количество активных клиентов у нас сравните высокая поэтому тем не менее у нас подходы к масштабированию такие довольно серьёзны о кей наш технологический стек frontend у нас если мы говорим про деск топ про веб это java script то есть мы перешли от концепции сервер сайт отрисовки концепции полностью single page applications java script мы взяли технологию react сейчас текущие фронтальной системы некоторые уже мигрируют на эту те налоги но с приходом фронтальной системы дефлятором система уже всех все во всех каналах если мы говорим про рыб будет именно сингл больше при кеша для мобильных устройств мы использовали нативная разработку для клиентов для сотрудников мы сейчас пилотируемой отметив как тоже не который такой инструмент по использованию логике между десктопом мобайлом ну там есть свои плюшки могу рассказать кому-то нем подробнее если это интересно так как по сути банком далеки ип сказал консервативные что супер сбербанк что дочь и банка что альфа банк backend у нас на джаве при этом мы не используем джи и я тут ремарка почти мы используем только datasource и экономичный factory's а все остальное у нас spring потом ну и куча куча куча других наворотов о чем по сути полном open source is инфраструктуры мы используем помимо железных балансировщик of которые не так интересны engine как так называю умный он и балансировщик и поэтому потом расскажу и как к сервер который дает нам статику вы все рубрики он сервер это у нас или положение extreme скилл это дистрибьютер на рынках римским мало кому знакомо но тем не менее мы его используем и с этим как бы страдаем иногда но им анкеты у нас для синхронного вина сообщениями ну и плюс 40 как хранение данных то есть такой принципе а и б м курская так объем oracle стек наш сервис овал agreement так как одна из главных таких челленджей это то что если сейчас например у нас есть много фронтальных систем у них разный слой например у интернет-банка целый аналогичные как тут ну например там для call центра силы более мягкий и сейчас когда мы делаем продано системы конкретно под какой-то канал мы можем как-то нивелировать где-то например подходы другие применять но когда мы делаем иди на фонтан систему это значит что нам надо взять самые как бы строгие своей которые вообще во всех каналах есть и по сути его развернуть на все каналы все остальные каналы это вот один из главных челленджи наших итак у нас слайд 24 на 7 доступность четыре девятки то есть это означает наша недоступность может быть только 52 минуты в год не больше технологически на кону нас нету то есть бывает такой хак в некоторых историях есть у почитайте там хостеров там еще там финансовых берс там бывает такое что как бы там чуть ли не 6 девяток там 50 секунды на собственных год но зато у них есть технологического окна 4 часа в неделю когда они просто остановлю систему ставят почти обновляют но вне этих рамок у них там шесть девяток окей у нас такого чит чит они-то то есть у нас есть один унесу все в год включает себя и установку патчей установку новых версий системы и отказывай все все все это он на карту виконта наш актив это как быстро система может вернуться в строй после падения у нас одна минута arpu это как это сколько мы закупили у данные данные мы можем потерять в случае падения у нас 0 минуту так как мы работаем с финансовыми данными у нас недопустимо и какая потеря каких-либо данных самое интересное на самом деле не за старика very у нас тоже не более одной минуты вот когда работал в банке у нас все эти принципы силой были за исключением то что на 90 recovery был четыре часа одежда recovery это когда случается это открывает врата ада то есть упал дата-центр начались военные действия там потоп на снег выпал в москве в душе банки это нормально к этому относились соответственно переезд и зануда центр дуга сандра занимал там обычно часа два-три но вообще свой был четыре часа и за мы свою практику один раз только до центра рухнул и вот вот вот этот где-то 40 сработал сбербанке это как банк относится к своим клиентам очень щепетильно банк поднял как бы плашка планку то есть резьбы бара и все новые системы они идут вот с таким тревоги за сколько я не более 1 минуты то есть у нас падает дата-центр мы должны приехать другой дата-центр одну минуту и это как раз очень сильно усложняет архитектура отказоустойчивости то все остальное она не так сильно влияет как вот последний пункт окей год сейчас как раз та небольшая прелюдия перед основной частью это я хочу рассказать про типичную текстуру к типичного тайной системы ну допустим вот мы не знаю там делаем какой-нибудь from phishing applications не замена там за интернет-банк нет нет рыбак интернет-магазин допустим точно классика да тут у нас есть веб то есть у нас есть браузер у нас есть большой нагрузке сера приложение база данных какие-то внешние системы с которыми мы взаимодействием вот такой курс точки отказа то может сказать какие тут точки отказа у большинства сказала все задай они правы плюс браузер окей что мы можем придумать до в соответственно самый классический способ защититься от отказа внешней системы это вставить очередь то есть асинхронный обмен можно использовать вызовы с тайм-аута my но самая надежная это вот именно синхронный обмен сообщениями о сбербанке это основной паттерна интеграция между системами как раз использоваться в офисе рынку в какой то момент мы мы скорее всего перейдем на кафку но пока мы используем mq при этом надо понимать что даже очередь иногда вас не может защитить если уж система которая вот внешне с полностью вышла из строя и она высоко нагруженная то есть вас может очередь переполнится и например случаю свинку если не настроенная ретенция полисе и не настроен соответственно максимальный объем очереди то можно просто вылететь по памяти у нас такой был один инцидент когда система становилась на 4 часа все как-то расслабились забыли что никакого лечения в сферы пока лишь сообщений нет она в старый сообщение отбрасывает и просто переполнилась и банк там часть банка небольшое встала на где-то на час то есть это же не надо забывать что очередь это уже надо настраивать которая понятна классе когда это вот б.д. почему тут можно много решений применить в банке сбербанк и применяется решение 99 случаях из 100 это репликация на уровне сходи то есть актив cold когда у нас соответственно репликация происходит на уровне массивов я это немного потом попозже расскажу какие ещё там есть варианты и какие у нас были опыты с этими вариантами также соответственно очень часто ошибка например когда мы рассчитываем какой-нибудь сайтик ну примерно 100 не знаю там 10 миллионов клиентов мы посчитали там по нашей модели в производительности получается что надо где-то 100 серверов иметь из 100 серверов по сути и закладываем в проект но не надо забывать что на примете у нас выйдет из строя какая-то часть этих сыров наглядно 10 то на оставшуюся часть повыш по пойдет повышенной нагрузкой но в самом критичном примере когда у нас 100 серверов у нас например 2 дата-центром и писать серого фото центра 50 другом в центре расположили его случается у нас завтра отказывает этот центр то вторая часть получает вообще-то разбудишь нагрузку и все падает соответственно есть много решений начиная от n + 1 + 2 + 3 но так как опять-таки у нас disaster recovery банки конкретно на минута мы используем так называемый дуайен то есть мы держим по сути в каждом дата-центре количество серверов равное чтобы можно было обслуживать всех клиентов из 12 центров пойдет соответственно есть еще отказ сервер приложений самом самом по себе обычно это не так плохо но клиент ухудшается user experience то есть если мы не используем а какое-то внешнее хранилище для сессии то есть и подателю приложения где клиента обслуживались того клиенты их обычно выбрасывать вообще то есть все что они делали пропадает и приходится логиниться заново и заново перри набивать свои там какими платежки сбербанке мы используем соответственно к sims кейс для хранения внешних сессий при этом мы используем не механизмы которые вы в сфере встроены в интеграции с extremes телом а мы напрямую работаем через клиента extremes кем если кто-нибудь хоть когда-нибудь буду использовать сын с кем-то он может ко мне обратиться за советуемся что там очень много советов и могу дать большие нагрузки тоже может отказать я тут не особо большой гуру в частности сетевой инфраструктуры но такие два два три ключевых слова dns на virtual ip то есть в банке и применяя цвету это то есть когда в одной dns-записи отдается несколько печника в разных блокировщиков и они там еще поддерживаются через virtual pet то из них там соответственно обслуживает клиентов и одна из по сути наших тем которые появилась в связи с переходом на java script это то что мы можем агрессивно кэшировать данные на клиенте это означает то что какая-то окраина недоступность вообще может быть клиенту вообще клиенту останется незаметный то есть если мы упали там или переходим там за одного режима в другой там минута полтора то есть польза агрессивное кэширование мы просто показываем данные которые уже на клиенте отображены в принципе это позволяет нам вообще как бы скрыть все люблю любое отказа вот если вы меня слушали внимательно на первую часть может быть кто-нибудь может сказать что еще тут не хватает вот учитывая наши силы и и показатель производительности по клиентам чудовище на добавить фрезерование канал связи база данных я слышал очереди смотрите давайте она я не очень хороший человек который подогревать подогревает аудиторию остается у нас проблема с у б.д. это масштабирование и designs to recovery то есть мы используем исходы там и за ну минуту не перейдем никуда если упадет база данных или упадет дата-центр где была активная база данных и еще одна проблема это опять вспоминаешь что у нас и солей четыре девятки это обновление сыров приложения то есть как нам достичь того чтобы мы обновляли всю систему без остановок это сервис discovery ну сервис сервис discovery у нас простой то есть у нас есть блокировщики ну давайте потом вы спросите я вам отвечу ok давайте начнем с базы данных отказоустойчивости масштабирования скажу стучит какие вот вариант вот опять таки в банке применяется в основном как раз на уровне сходи то есть это самая такая надежная репликация за последние 20 лет вендоры наконец научились у делать безбожно то есть никаких по сути ошибок не возникает во что все реплицирует просто на уровне работы с массивами с массивами вот с железками второй уровень это репликация средствами чтобы д то есть когда например мы используем оракал там у него есть продукт варкала dg актив до таггарт когда сам сам особо dm сама занимается репликацией то есть у нас шипит логе она соответственно не комитет транзакции пока не прошла репликация то есть это делается как бы на уровне сова сова максом саму суду посуды а не на уровне схд и преимущества тут в том плане что исходя она реплицирует все абсолютно все все что происходит с дисковым осевыми а приказ на уровне базы данных там есть всякие бти мизуки то есть он может там как-то с хлопнуть транзакция который там идут параллель но в общем уменьшить трафик по репликации и третий способ это репликация средствами приложения то есть на самом деле только со средствами приложений это когда мы уже на уровне приклада своего занимаемся какой-то счет дополнительной репликации это именно дополнительно репликация потому что она никогда не заменяет репликацию либо на уровне с кадыри любит на руины и субд и я буду расскажу как мы это применяем но минусы на самом деле опять-таки применительно к нашему случаю исходы это то что так как ходы поддерживают только режим актив cold то есть ни одна база данных не оракал томительно дебита там не погреться не позволяет работать двум экземпляром базы данных с одним файлом то есть всегда у нас один экземпляр 11 файл с данными с данных а грамма реплицировали схд это означает что 2 ын су насаживают потушен иначе в общем ничего не заработает и когда мы глушим например есть у нас пошла деградация если у нас просто база упала или дата-центра выругался там все просто то есть все пропало ok но когда у нас это большинству случае с которой мы сталкиваемся начинается деградация то есть база начинает медленно умирать это и и угри слуша да он занимает огромное количество времени и подъем разогрев cold in сноса тоже занимает огромное количество время потому что у нас база данных очень большие там с огромными терабайтами буфер пулами и как бы там ни старались минимум кто могут достичь в сбербанке это минимум 30 минут на переезд из одного дата-центра в другой для базы данных в принципе что касается и логической репликации она по книжкам по как бы документации должна быть гораздо быстрее потому что при логические репликации на уровне база данных мы можем второй экземпляр держать в режиме ход ну например режиме редон и то есть он уже включен buffer pool его разок разогреты но на самом деле наши тесты изыскания с раковыми специалистами показали что никто предугадать насколько быстро переключение будет не может то есть каких-то случаях при хороших денег остается приключения происходило за 5 минут в каких-то случаях когда была очень большая нагрузка на базу данных мы пытались симулировать деградацию переключение происходил точно так же долго как с схд то есть база останавливалась 20 минут бристл садом и переключение при доброй тоже занимала там 5 5 10 минут и ни один специалист например 40-го никогда не подпишется под тем что при любом всем стоять из приключение будет пять минут никто-никто свою зарплату или бонус под это не подпишет как мы не старались подписать готовых специалистов не получилось ok опять-таки привыкли catia средствами приложения мы должны использовать какой-либо из из репликацию которую я рассказал раньше потому что привыкли к средствам приложениям и реплицирует только некоторую часть енот ну например мы можем реплицировать там только справочные данные а все операции мы например так как мы используем вот мы решили просто их не реплицировать все равно их надо гарантировать их new отказоустойчивость поэтому когда мы используем ри прекрасное приложение у нас получается 33 по сути базы то есть первое это активная 2 это у нас там либо ход либо cold это если из ходы или субд и а третий это уже вот наша еще одна активная база но уже с какой-то ограниченным набором данных которые мы регулируем вручную я потому скажу как мы это используем масштабирование тоже наверно по сути гораздо сложнее чем с отказоустойчивостью если мы используем классическую субд эта есть несколько решений но все они на самом деле дэнт мы можем использовать уроков orel опекаешь на кластеры либо аналог уплывем есть своя версия там я уверен для других систем при когда когда vendor говорит что он к strether баз данных он кластер визирует только инстанции этой базы данных то есть эти инстанции они все равно работают с одним файлом то есть но с 1 tbsp сам который хранится там на какой-то файловой системе и соответственно только небольшое количество из кейсов выигрывает от а тут такой масштабирование то например если у вас очень много хранимых процедур это как-то может там вам что-то дать при этом кстати оракал ритвик очень классно еще использовать для отказоустойчивости я не стал там на первом слайде вы показываете сейчас расскажу это когда например мы хотим обновлять оракал накатывать пачек видосики security почти без остановки вот реала приказчика странам это позволяет то есть он такой конкурс презирать инстансы мы можем инстанции обновлять под папой поодиночке то есть у нас там триеста sa1 essence активный соответственно мы его отключаем переключаемся на второй инстанции первым составляем это без без без downtime ok второй подход а флот нагрузки это когда например вот опять таки там теперь премиум к уроку когда мы используем вторую базы данных в режиме редон и многие баз данных этот режим поддерживают это позволяет нам снизить нагрузку но только в специфичных кейсах то есть когда у нас есть то есть мы видим что у нас производительность упирается что что кто-то запускает репу репорты отчеты строить по нашей базе и у нас все-все-все просаживается вот тогда мы можем в принципе как-то с масштабироваться используя вторую второй экземпляр в режиме ребенка и снять снять вот какую-нибудь такую паразитную нагрузку но это работает когда у вас самого основная база не упирается например по объему то есть когда объем уже превышает там все все все допустимые пределы даже вот этот а флота не всегда спасает вот и третий способ это масштабирование в средствами самого приложения то есть так называемый шарлин это когда само приложение сама контролирует какую базу данных например пишет соответственно минусы кстати от один из основных минусов урока варю так очень классно это то что его два года пытались в банке внедрить оракал просто на на для нас писал определенные почти специальная машина сама была большая инсталляция это наш был processing который все платежи по визе обрабатывает в общем oracle на нас отваживался в конце концов она платился это худо-бедно запустилась но не в том режиме в котором все рассчитывали запустил столько в режиме по отказоустойчивости то есть нагрузку на кластера так никто и не пустил потому что все она продолжала падать нокий но самое фиговое что это работает только внутри одного дата-центра то есть так как кластеризация база данных означает что получил появляется дистрибьютор локи очень большой interconnect это все работает только от просто пределах стоят на там если мы это разносим там на 10 километров на двадцать километров то все кто бы чтобы не говорил есть это большие нагрузки это просто не работает а float как и говорил это определенные потер не подходят под под этот пот это кейс к сожалению у нас только в одном в одном случае это помогло когда вот именно репорты запускались для бизнеса они строятся там с реплики но для остановки если у нас это не подошло ни разу и мы выбрали именно sharding вариант шар лингам я потом расскажу уже чуть подробнее ok zero dawn тем deployment то есть классика когда мы обновляемся без без остановки классический подход это мы сначала то есть ну там либо сначала либо потом сначала ну допустим давайте сначала обновляем сера приложений на котором по кластерам по по кусочкам новую версию потом обновляем структуру базы данных при этом нам надо гарантирую что эти обновления не обратно совместимы по базе данных а потом повторяем ну например пусть о как вы накатили апдейт на базу данных нам надо все же колонки какие-то поудалять и мы снова начинаем обновлять нашу систему потом удаляем эти колонки в общем процедуры такая в несколько шагов но главная проблема в нашем случае с которыми столкнулись это то что если база данных хранилища перестраиваются радикально а это у нас случалось и не раз потому что они всегда угадывали структуру изначально вот всею space и то вот этот подход он становится настолько сложно что возникает очень много проблем и приходится все же останавливать систему то есть не всегда базу данных можно обновить на лету даже тот же самый уроков даже самой последней версии там все же есть случаи когда надо просто выключать ее и обновлять по сути в оффлайн режиме наш ответ это blu green deployment я потом расскажу как мы применяем на blu green это классическом случае это значит что у вас есть две версии то есть два экземпляра ваших вашей системы и соответственно там blu версия blu экземпляра не стоит текущая версия и на не обслуживаются клиенты нагрянуть на нагрев экземпляре вы наказываете новую версию а потом когда вы ее на коте ли вы переключаете соответственно используя там балансировщик с blu на грин и по сути повторяйте этот процесс следующий раз когда вам надо опять обновить версию если вы используете amazon тома в с либо azur то это вообще очень все просто вы просто под по сути платите очень маленькую касту за это то что вы можете поднять потом погасить инстанции их убить и я лично вот очень совету придержаться и ноутбук горин deployment а я расскажу как мы это у себя применили хотя у нас не amazon используется итак наше решение такой финт кто хорошо глаза мы используем как я говорил sharding мы используем репликацию средствами приложения и мы используем blu green deployment вот опять я картинка проведу я тут все очень сильно упростил для чтобы проиллюстрировать значит там картинкам сейчас ума сшедший то есть у нас есть браузер у нас есть балансировщик у нас есть сервер приложений у нас есть база данных standing это самом деле термин который действительно придумали в сбербанке я нигде я не нашел его когда мне сначала говорили про станнер не совсем путал со стенда я а сами стендин это по-английски это дублер и действительно интересную вещь которые коллеги до меня на самом деле придумали мы просто по сути используем эту экспертизу и так что же такое это некоторая комбинация подходов грим блюде племен то и соответственно репликации на стороне приложения то есть что это означает то есть у нас всегда есть два экземпляра системы один экземплярах системы который и эти экземпляры содержит 40 приложение база данных тише очередя тут просто для иллюстрации я указал самые ключевые компоненты и эти экземпляры два один из них это праймари который как бы обслуживают клиентов во второй тест н так называемый в случае соответственно и перед этими экземпляром стоит некоторые может этот умный балансировщик роутер который зависимости от режима пускает клиентов либо например либо на стенды что это означает то есть это означает то что у нас в случае падения праймари то есть например у нас упал дата-центр мы переходим на уже раза активный по сути экземпляр стенды на при помощи вот этого ротора просто по сути моментально там нашей последовали зации это занимает 5 секунд второе мы можем использовать стендин и праймари именно в режиме blue green deployment а то есть например когда нам надо обновить систему мы клиентов перемещаем на стендин обновляем наш праймари это может занять там не ни одну минуту этому занятий полчаса и два часа и три часа после обновления мы спокойно клиентов со стендином переключаем на праймари догоняем а те данные операции которые клиенты на стенды не сохранили при помощи у наших скриптов репликации the prime и по сути клиенты уже работаю с новой версии при этом так как стендин мы в этот момент до новой версии не догоняем у нас остается вариант перейти обратно на старую версию если у нас произошла какая-то какой-то критический дефект например мы заняты радиусе ли которую на quite фазе не не поймали и это кстати пару пару раз спасала команды и так плюсы да то есть disaster recovery то есть отказ база данных отказ дата-центра у нас обрабатывается по сути за одну минуту при помощи вот этого роутера blue green deployment из минусов у нас поставит оборудование то есть у нас мы нам приходится по сути держать стендин виде по сути простаивающие экземпляры системы он на самом деле по базе данных не такой большой как основной как правильно и потому что на стендом мы копируем только те данные которые требуются для проведения критичных 1 критичного набор операций и кстати да вот забыла уточнить вот этот нюанс в нашей практике большинстве случаев отказавшего заданных это обычно деградация деградация из-за того что когда структуру базы данных проектировать десять лет назад никто не ожидал что в общем будь такой бешеный успех там у какого-то продукта и когда там мед центр банк начинает там шерстить банке у нас там увеличивается клиентская база все к нам бегут и в общем тесто все структуры которые давным-давно были спектр иногда не справляются и возвращает деградировать при этом если бы у нас была бы репликация всех этих данных один в один из праймари стендин это бы означало что у нас бы и стендин база просто за деградировал точно также поэтому мы специально ограничиваем объем данных котором реплицировали стендом чтобы избежать вот этой ситуации т.е. даже если у нас если вы понимаю что завтра не оптимально мы когда перемещаемся на стенды standing просто окно полупустой да то есть там гораздо меньше данных и мы то можем продержаться там и знаю там неделю там месяц в отличие от праймари базы где там данный уже за пять за шесть лет накопленный ok и тут проблема то что не решается проблема масштабирования то есть окей мы решили проблему 10 recovery с версиями но если упираемся в просто физические возможности базы данных то в стенды нам ничего не поможет многопоточность этот следующий как бы термин который у нас придумали на самом деле я вас что-то про мага обычно когда-то читал нас на блоге хай лот такой есть блок там старый статья кого двенадцатого года про фейсбук как они там мой скверы раскачивали у них что-то подобное кстати проскакивал но тем не менее люди которые тоже это придумали не тот блок точно не читали и уверен то есть мы облачность это по сути развитие действо стенды на но с подвешиванием шарден га то есть у нас опять таки есть независим экземплярами системы но у нас их не 2 а.н. мы берем нашу пользовательскую базу на некоторые пакеты то есть у нас на меня там 100 миллионов клиентов мы говорим что каждый экземпляр система обслуживает только 10 миллионов соответственно нас например 1010 газом 1 системы по сути мы используем те же самые подходы что из-за стендином у нас есть роутер но он уже были умные то есть он использует информацию о маппинге клиентов на блоке что это нам дает то есть мы решаем проблему с простоем оборудования тем самым у нас каждый блок на самом деле может подменить другой блок ну это как бы в идеальном варианте сейчас тем не менее все равно у нас есть стэн yanofsky блок но он маленький он реально маленький то есть ему не надо там говорить у нас 100 миллионов клиентов нам не надо держать это дымовский блок в холостой который 100 миллионов плену сможет обслужить нам надо держать стану русский блок который только 10 миллионов может обслужить но в дальнейшем и конечно стараемся перейти все же подходу когда каждый блок взаимозаменяем и решается проблема с масштабированием до точки это по сути классическое шарди рование то есть когда у нас соответственно мы клиентов разбивая на какие-то баки то и по сути мы можем масштабируется до бесконечности мы можем и 20 30 экземпляров дрожать проблемы как таковой существенно нету сразу сразу хочу сказать туза слайд 1 пропустил когда я делал что вот это шар ринке вот эта нога блочности она подходит только когда у вас нету interconnect of между блоками то есть когда вы можете все например кого клиентские данные поместить в один блок и блоки друг другу не ходят потому что если блоки начинают друг друга ходить у вас возникает другая проблема опять с масштабированием потому что один блок как в один блок если там какой-нибудь очень горячий клиент в этот один блок могла манусу все остальные блоки опять мы возвращаемся к шагу 0 что блок может просто не выдержать нагрузки в нашем случае случае фронтальной системы в 90 опять-таки 5 случаях из 100 мы можем клиентов локализировать на каком-то блоке у нас есть конечно юридические клиенты у которых есть холдинге и и холдинги ну то есть организации дочерней могут быть в разных блоках но таких из кейсов мало и мы можем в по сути применяя некоторые там допущение разрешать межблочные взаимодействие но также на очень пристально наблюдая на производительностью вот это всё хозяйство так какие по сути решение мы применяли для ре зация много- блочности опять-таки мы сделали свой роутер вот этот хитрый и мы пашем они не сертификацией роутер это наверное тренд мы опять-таки опять в тренде благодаря пример то же самое с нами была год назад когда мы выбрали java script и react тогда был реактор не помню 11 что или 011 все там крутили у виска что много чего qq но зато как бы муж мы не прогадали вот то что мы сейчас injuns а мне кажется происходит учит его сколько докладов по индексу и плюс ло я прослушал мы не заговаривать взяли вот этот магический магические как комбинацию что это как это вообще работает то есть у нас есть пользователь нас есть один из сервисов модификации которые тоже в банке разрабатывается это чтобы можно было модифицироваться физиком юриком там партнером через как в единую систему они как сейчас когда у них его разные лаги на разные точки входа ну не важно о чем это едино сервиса дефекации при той самой фикации возвращать на магических клиента они джинкс обращается к нашим сервером приложений которые поставляют наружу rest интерфейс эти серого положения лезут в хранилище в котором находится mapping клиентов на блок там есть нюансы вот этого хранилища в том что у нас есть разные все же источники информации но это такие очень низком уровне детали тем не менее мы используем тут распределенный кэш перед хранилищем для двух вещей даже так как хранилище по производительности может не выдержать и второе для отказоустойчивости то есть если упала хранилища то по сути все принципе встанет то есть вот этот на шутера откажет это как бы посвятить единой точкой отказа тут мы используем распределенный кэш чтобы иметь возможность работать какое-то непродолжительное время без самого хранилища то есть в кэше хранится по сути вся вся реплика пусть того как mapping сад и отдался с яндекс отправляет уже запрос клиента как приводом блоку и выставляет в cookie номер блока чтобы клиент чтобы потом вот это все процедура по обращению к хранилищу не выполнялось при этом у нас есть там защита от дурака ну не а дурака entre ужин потому что купили любой может подменить если кто-то подменил куклу из и ушел на тут блог где выданных нету то система это фиксирует сбрасывает модификацию этого пользователя и тут же пишет сообщение в anti-fraud системам что мы что-то у нас не так и в большинству чайф клиента блокируют поэтому не играйте с этой куклой если хотите вы пользоваться услугами даже в исследовании целях окей это что значит что касается роутера репликация между блоками я т.е. какие данные мы вот вообще реплицируют и у нас есть два набора данных основных во фронтальной системе то есть мы фронт мы работаем с клиентами мы по сути принимаем от них заявки и дальше уже их отправляем в либо в медаль либо в бег системы то есть у нас каких-то проводок со счета на счет нету то есть мы фиксируем ваше желание провести эту проводку но исполнять она в бегах поэтому у нас наша модель данных гораздо проще то есть у нас есть справочники они бывают внутренние и внешние и операционные данным person даны это вот заявки по сути и так как мой реплицировали справочнике между блоками мы тут у нас сейчас текущая версия возможно не особо красивая то есть у нас есть схема со справочниками внутренними мы эту схему реплицируется через golden gate на блоки и внешние справочнике к нам прилетают через топике тут очередь написан сонгюль через топики он подписано все блоки и они уже у себя сохраняют эти данные блин же время когда мы унифицировать наконец-то наши api по работе со справочниками потому что сейчас просто прикладные проекты они напрямую работу софт винчи джи-би-си мы не можем никак контролировать это вы всё как мы унифицирует и мы также будем в справочнике обновились пайщиков через топике отправлять на блоке ар гал golden gate хорошие на самом деле решения но в нашем случае у него есть две большие проблемы 1 проблемы это то что им очень сложно управлять не использовать консоль уроков то есть консоли oracal отлично и там все есть там всё переключается все настраивается но так как у нас система большая мы хотим делать свои dash горды и уметь управлять нашей инфраструктуры снаружи а у оракла угол другие то нет никаких не rest api и там ничего нет если бы там надо хотите ихнюю консоль как-то чтить и request и поддерживаем подделывать либо использовать команду line вызовы ну тоже тоже не очень красиво и с мониторингом тоже угол donghae то есть небольшая проблема все-таки отлично мониторится через консоль но внешний мониторинг вот именно тех параметров которые нам надо для перца для нашей эксплуатации мы вывести не смогли поэтому мы планомерно наверно будем а только углу другие тот отказываться и то есть есть есть причины операционная данная тут все просто я уже сказал то есть мы на самом деле в рабочем режиме операционной данной не репетируем то есть у нас если мы используем просто стендин без мага блочности то стаду нас пустой тоже рисую только справочники если мы используем блоки то то же самое между блоками никакой репликации авиационных данных в рабочем режиме нету есть репликация только когда клиент . на блок упал клиент перешел на другой блок он там что-то на колбасе а потом нам надо перейти обратно вот тогда у нас есть обратная репликация мы ее сделали опять-таки самым простым способом которые смогли через sql script и через де белен как это наверное выглядит не так красиво но она работает мы хотели использовать дней для этих вещей но тут 3 проблема возникла у golden gate и потому что убил бандита невозможно контролировать не начала его репликация не конец то есть привлекаться можно остановить за паузе но вот именно сказать что вот сейчас реплицирует а вот и скажи мне когда ты закончишь без какой-то sap set at golden дети это надо там придумывать всякие маркеры вставлять какие-то искусственные вставки делать по которым мы можем засеять что он наконец то что туда реплицировать но это настолько free jal что общем гораздо проще и надежнее то можешь сказать простые сельские скрипты что мы как бы и используем многие кому когда я рассказываю про наш решение спрашивает а покажи на ушки на . выезжать ну хорошо в этом как бы динозавры может может у нас обвинить используйте только сумма dj amor около любит дебету используйте там кассандру мангу где все как бы есть то есть и sharding есть автоматический и репликации есть и все-все-все че вы паритесь вы придумали какие-то велосипеды просто за того что у вас ограничение есть какие-то в плане используя технологии но на самом деле нет то есть нет вот все падает до то есть я когда тучи банки работал я на моем проекте я устал наблюдать как к hiren's разваливается то есть кластер коференции мою datagrid вроде бы тоже там мафию офигенный явно ранговый но сначала развалился потому что мы их орбиты неправильно настроили срабатывал говоришь коллектора и выбивались ноды классно перестраивался в общем приводил это просто падению тупо потом у нас будет проблема что кто-то перенастроил люди пии на роутерах там пошел в общем не туда здесь поток тоже опять упала третий раз там and deep care ендовы кто-то подключился такая la live messenger система которая cherry pie работает начинала там короче конфликтовать эти эдипе потоки забивать роутер и опять все падало то есть система подала как бы постоянно хотя как бы кластер ни единой точки отказа но чего-то нифига не работает то же самое может случиться принципе с любой класс резервной системы с мангой с кассандрой с чем угодно если у вас конечно есть некоторые я сказал не такой жёсткий целый когда-то например вы можете себе позволить там 5-6 проколов когда вы там все же настраивайте unity свою настройку и вас там не лишат не примени бонусов то в нашем случае у нас как бы такого нету то есть мы не можем на клиентах экспериментировать если у нас все упало мы силой пролетели то это будет плохо и клиентам и нам поэтому мы исходим из того что все падает даже когда мы будем использовать там кассандру и лимон body by я уверен что в каком-то будущем это будем делать все равно у нас будет механизм вот именно вот этих блоков чтобы защитить от падения всю систему потому что всегда есть . отказа абсолютно во всех решениях и в данном случае мы к этому подходим можно сказать с религию религиозным таким ажиотажа вот выводы такие до вальке банальные на больших объемах архитектура становится нетривиальной то есть если мы там делаем интернет-магазин мы можем ограничиться там третьим пунктом где-то вначале расскажу когда мы говорим о 100 миллионов пользователей и очень большому силы и то как бы вы попали получается то есть если у вас комбинации большая нагрузка и сумасшедшие целый то вам приходится придумывать как-то изворачиваться и находить какие-то нестандартные решения и самым последним как бы пункт который вот меня научил на разом моей последней там 10 лет как я занимаюсь отказывающим систему что все падает абсолютно все вообще ничего я не видел ни одну систему которая была бы будет прав где бы не было какой нибудь инцидент и там в конце концов там 1 34 года все равно она я каяться из-за каких-то там причин в основном кстати это не сетевой причина основном просто настройки кто то поменял сетевую инфраструктуру перестроил чтобы свечи выпадали там или блокировщики отказывали это вообще ни разу не случалось а вот именно настройки и поведение системы она она меняется со временем и вот это изменение но приводит тому что падает какой-нибудь критический блок вашей архитектуры и все за ним как рушится вот но в принципе я завершаю наверное мой доклад cappy-х лука вопросам-ответам мои контакты из кому интересно но задержался спасибо большое максим если есть какие то вопросы пожалуйста задавайте здрасте максим а вот спасибо за доклад скажите вот какие признаки роутер используют для переключения вот схеме стенды это какой-то объективно и там число которое или зависимости от нагрузки на самом деле вот кстати про these as the recovery во всех книжках пишут никогда не автоматизируйте де за стариками 90 recovery оно должно быть automated automating автоматизированным но не автоматическим я не про рассказал то есть переключение у нас происходит по сути вручную до старта человека до человек потому что когда мы переходим на стендом мы получаем ограниченный функционал потому что например там нету история операции и вот этот переход он должен происходить все же осознанно если просто начнет мигать все то это будет очень плохо спасибо обычно когда начинает деградация когда начинает падать как бы система это по мониторингу ну где-то там защита 4 за пять уже видно что деградирует баз данных когда юг падает тоже дата-центр это тоже случается не моментально то есть начинается отказывать какие-то там системы и дежурные администраторы которые сидят там 24 на 7 а не успел hyip нажать кнопку надо сначала позвонить босса ну укладываемся в 52 минуты мы укладываемся на спасибо за доклад очень интересно сколько у вас дата-центров и как вы реализуете вот стандарт технологию сколько у нас два дата центра причем один дата-центр сбербанк сам владеет это тир ван дата-центр автор газа центра сколько я помню арендует то есть у нас пока еще два дата-центра в таком случае да да да да а что будет если в момент ну у нас вчера приложение у нас единственное что принадлежит эта центру это активный экземпляр база данных а то есть вот у нас есть праймари он размазал между двумя дата центрами то есть на сверх серое положение в одном и в другом дата-центра у нас они по технологиям равнозначно считаются но в праймари дата центре находится как бы праймари база данных но активный экземпляр а в в резервном соответственно cold в случае 100 со стендином у нас в резервном активное of primary cold поэтому если падает у нас поймает этот центр мы переживаем на резервный этом стендом в активном режиме но его часть базы данных и то есть тем самым как бы мы ну нам не надо ничего поднимать а почему не сделаете активно и они они активны просто вчера приложение да они так и работают я так сказал нас в блоки и стенды и праймари они размазаны но только сервер базы данных так как субботы классическая может быть в активном режиме только в одном дата-центре а серого приложение они активны и там и там и балансировщик у нас ран / янгом по сути разбрасывают между двумя дата центрами до синхронно спасибо максим я хотел бы задать вопрос можно спасибо большое я хотел задать вопросы по поводу blue green вы сказали что эта технология вы применяете для того чтобы собственно говоря решить проблему поддержки обратной совместимости при обновлении баз данных и вывод красиво сказали что у вас есть green система blue system но потом вы сказали что когда вы переключаете после переключения польстили на стенды и собственно при обновлении основной системой праймари вы потом реплицирует и данные которые собственно в этом станет и не польский спиртом на колбасе а к собстна как это соотносится с абсолютной диаметральной my структурами баз данных мы делать мы делаем репликацию на уровне средствами приложения то есть не сильно нам приходится делать скрипты которые трансформируют из одной версии данные в другую версию данных но так как объемы которые переливаются они не такие большие и в принципе ну это такой как бы ну сопутствующий ущерб на приходится делать вот скрипт эмиграции из одной структуры в другую но это происходит как бы в игровом процессе то есть клиенты в этот момент продолжают обслуживаться ну что ж дорогие друзья спасибо большое за вопрос и давайте сегодня раз поблагодарим максима спасибо ему за выступление"
}