{
  "video_id": "q5jo-Hhmrlw",
  "channel": "HighLoadChannel",
  "title": "SOA: строим свой service mesh / Иван Круглов (Booking.com)",
  "views": 5717,
  "duration": 3522,
  "published": "2018-08-16T03:56:34-07:00",
  "text": "всем привет на те щас круче таймер я ваня я из booking.com и сегодня хочу рассказать немножко про то чем я занимаюсь вообще booking almera вот уже много пять лет и там было много проектов которых я участвовал например я был в команде которая делала поиск для букинга я об этом не рассказывал год назад здесь на highload в этом докладе я хочу немножко раскрыть тему коммуникации между сервисами особенно того аспекты как сделать так чтобы эта коммуникация была безопасная предсказуемой принципе простая чего у нас сегодня стоит на повестке дня тут как бы все достаточно просто но у нас есть проблема в перед которой мы стоим я хочу вам ее показать рассказать вам про нашу боль потом собственно играя какие решения были рассмотрены чего там есть интересного потом собственно говоря как как мы это решение внедрили в практику то есть какие-то такие более реальные данные с графиками ну в конце какие-то какое-то количество заключительных слов и свой рассказ про проблему я хочу рассказать я хочу начать с такого примера небольшого давайте представьте что вот вы работаете в компании вот у вас есть такой монолит вот он такой большой-большой да там много кода он писался очень долго там с каких-то там не знаю начала двухтысячных годов пишется вот у вас за время как бы роста компании начали выделять какие-то сервисы чисто как как естественный рост например там вас выделился там сервис то же самое поиск и например у вас выделился сервис взаимодействие там с сервис хранения юзера аккаунтов но в принципе как бы вы понимаете что как бы вот у нас по-прежнему есть монолиты с ним связано куча проблем например там сложно деплоить куча зависимости там в одном месте поправила там в другом есть и отвалилась короче вы приняли решение что вот все мы короче вот будем делать при на так мы будем распиливать вам монолитно какие-то кусочки на какие-то сервисы да но это собственно говоря здесь можно говорить про сервис-ориентированной архитектуры если продолжить движение вот в этом же направлении да и продолжить перейти как бы эти кусочки на более мелкие части получится как-то примерно так да и здесь уже можно говорить про микро сервисе наверно как бы вам немножко сложно все представив таких деталях наши как бы кусочков много поэтому давайте немножко сделаем шаг назад вернемся к этой теме да когда у нас сервис-ориентированной архитектуры и своем как бы в таком виртуальной компанией давайте представим что вот это вот нашу нашу архитектура хотим как бы раскрасить в двух цветах например например там один цвет это какой то ваша исторический стык например то не знаю php perl что угодно а новый цвет этот такое старое такой вот одесситам монолитом php на оного это такое модное прогрессивная клёво этом не знаю колледжа вы вот там все что с ним связанно это вот как бы то что вы хотите сделать do not на самом деле мне кажется что вот получится как-то вот так вот учитывая то что как бы компания такая большая до людей как бы очень много вот и здесь как бы то есть у вас как бы скажем так на моей практике получается что всегда как бы есть какие-то какие-то кусочки кусочки ваших архитектуры которые примите раваны каком-то другом языке или например у вас даже в рамках одного как бы языка одному фреймворк у вас есть какие то там разные версии там разные не знаю там разные веб-сервера разные блюда китайской ручек раз так получается такой неоднородный далеко не однородной и теперь дайте как бы наши кусочки они как бы по-прежнему должны выполнять вот у первоначальной функции который собственно крану для когда которая создана наша организация то есть вот все то что дело наш монолит должны выполнять q кусочки понятно что они должны как-то между собой там взаимодействовать они могут взаимодействовать как то вот так вот да то есть как то в какой то в рамках иерархической модели до или они могут взаимодействие как то вот так вот в рамках пир toupper коммуникаций да когда все вот эти все сервисы все кусочка не абсолютно равнозначны ну или там совсем и такой модный хипстерские в эту коммуникации между сервисом через какую-то шину данных так вот во всех этих подходах есть как бы одна фундаментальная причина это фундаментальная причина то что если раньше у вас были вот эти вот со составляющей кусочки вашего applications надо они взаимодействия между собой через простой вызов функции простой просить укол а теперь у вас есть ремонт просить у кого да и вот эта вот буковка r ремонт она там много чего портит там много чего нехорошего может случиться за время полета вызова там от вашего клиента к вашему серию суда от сервиса от к сервису бы давайте более подробно как бы посмотрим а что может случиться-то такого плохого но давайте возьмем какой-то гипотетически там сервер консьюмер enter и провайдер да у нас там есть при кешен вот там облик вечность какая-то клиентская библиотека на там где в транспорт там они там как-то транспорта между собой взаимодействовать но собственно говоря идет сервере и обратно да и вот сделан такой зум как бы вот эту часть вот именно клиентская библиотека тире транспорта то есть контакт что нужно по моему мнению учитывать вот этим вот в этом условии чтобы как то более менее скажем чтобы сделать так чтобы наш консьюмер смог достучаться до провайдера сделали zum ende в первую очередь нам нужно как бы нашему клиенту нужно понять а а куда сходить доносов ты нужен discovery да тут думаю все понятно потом нам нужно как бы когда мы поняли куда нас ходить нам нужно лот бантинга ну там round robin или чего там еще есть более умного нам нужны тайм-аут и на такой очень простой необязательный фактор каждый удаленный вызов должен должен быть вместе с тайм-аутом нам если там про все-таки что-то отвалилось или тупо то надо до которой мы данный момент решили пойти как покинутую причине недоступна да нам нужно сделать какой-то ретро и если мы идем какой-то ретро и обязательно нужно сделать каким-то отступом на какой-то быков если таких вот и трофей и бэкапов быков и становится сильно много дату может вообще стоит как бы клиент сказать на узоре вот на наш сервер наш сервер провайдера он короче у нас выпал все даже не пытайся ходить по сети потому что это бессмысленно то вот суть серки брейкеры нам нужен так как мы делаем в удаленный вызов нам нужен какой-то tracing мониторинг там тупо нужна авторизация до 13 фикации потому что раньше так было все едино чё там авторизована как бы но мы всегда знаем кто вызвал ваши функции котов начиная ходить по сети до появляются такие проблемы это на клиентской стороне на серверные там прямо тоже самое ну то есть не то же самое какие-то такие кирпичики и нужны какие-то очереди и тайм-аут и было хорошо на стороне сервера иметь какие-то приоритеты между запросами было хорошо также иметь какой-то функционал с monkey который бы собственно там периодически все отстреливал чтобы понимать что на клиентской стороне этой вообще-то все корректно обрабатывается да если все становится уж совсем плохо то было хорошо за иметь какой-то рейтинг чтобы там сервер мог сказать что вот вот короче вот я могу принять только одну тысячу запросов в секунду аутсорс верху я не могу принять я даже не не буду этого делать также авторизация короче там много чего есть и про это все я более подробно рассказывал на high low junior и полгода назад очень рекомендую вам такая небольшая самореклама рекомендую вам посмотреть на такие интересные графики вот значит смотрите вот это вот в принципе это самая проблема которую мы сейчас решаем booking.com до booking.com большая организация один большой монолит написанный на перле дает мы его распиливаем на кусочки как там поделились мы там выделили а сервис например там сервис сервис в 100 грамм бронирование сервис юзер сессии сервис юзеров сервис там которые представят картинки ну короче какой-то там пару десятков наборов сервисом но нужно сделать так чтобы вот эти наши кусочки по-прежнему могли выполнять функции большого монолиты так чтобы все это работало надежно и тут как бы в чем в чем еще как бы дополнительно сложности то что я уже упомянул дата сперла у нас реально много пирло нас много почему такой огромный кусок 1 5 миллионов строчек кода который никогда не писался с учетом того что это будет какой-то сервис ориентирован архитектура чтобы какие-то удаленные вызовы вот этого никогда не было все у нас лежит в одном repository вот реальный хардкор такой мандат гид четыре с половиной гигабайта тупо чтобы сделать checkout нужно порядка 7 минут вот кроме этого вот эти вот те самые seo сервисы которые как бы в рамках как бы такого естественного роста мы стали появляться не там появляются каких-то новых языках нас есть какая-то часть написана на яве там какие-то системные вас к про зрительные демоны есть ногой там наши админ из дайте с аметистами очень любит питон поэтому вот там в питон тоже есть да и сверху это все как бы скажем так наложено сверху что там полторы тысячи артистов а эти персонала да то есть но вот примерно так и чего еще то есть мы как бы идем вот это вот светлое будущее сервис-ориентированной архитектуры да и понимаем что вот облака все модно круто да то есть будет как а там какие-то новые сервисы по большей части а скорее будет размещаться там то у нас есть настроиться внутреннее облако но кроме этого как бы есть очень четкое понимание что какая то часть какая-то часть во-первых старого кода просто не не будет с мигрировала и также как бы есть очень тяжелые сервис то же самое сервис поиска которая уже несколько раз вспоминала да он реально очень тяжелый там на каждой ноду там десятки гигабайт состояние и как бы вот записывать это как в облака и в контейнер ну скажем так это реально но это такой для меня это личная спорный вопрос да то есть скажем так там очень много чего надо смотреть хочу делать вот значит вот это наша проблема какие мы себе видим решения решение номер один такое очень простой и понятной давайте мы напишем там ряд библиотек до которое вот нам вот эти вот весь функционал собственная а не будет реализовывать то есть библиотечки там для пирло мы можем понятно в перву нас особенный в конкурсе молча есть мы напишем счет свое softy граната часть какая то есть там для какие вот другого стыка можно использовать что-то уже готовы какие тут плюсы ну во первых для меня лично я такую плюс тот ну очень все просто понятно да то есть к этому все мы понимаем что такое библиотек и как она работает берем библиотеку как-то там смотрим настраиваем тю ним все хорошо и такой существенный плюс это то что мы можем взять уже готовое решение до которой я перечислю на экране а чего тут с минусами с минусами это во первых то что вот эти вот библиотечка которые перечислил они как бы все решают в принципе одну и ту же задачу да они все умеют делать там ретро не все мои заставлять тайм-аут и понятно они все мои делать быков они умеют комбинировать какие-то метрики да то есть они все это умеет делать но каждый из них это делать немножко там своим способом да то есть и если угроза в рамках большой организации по попытаемся их внутри нас получится какое-то такое некое системное поведение она что-то более того от это не консистентной может проявляться в каких-то критических случаях да . бы там 90 процентов времени он все равно хорошо а там в 10 процент случай когда нам реально нужно понимать как систему себя ведет просто настолько хватало до мог бы прибавить какие-то различия 2 это для меня такой лично для для меня как человек который занимается инфраструктуры важный аспект потому что получается вот этот библиотека она как бы она может быть написана мной до но она является частью приложения и если в этой библиотеке нашлась какая-то блемы тамбова или там какой-то security fix да мне нужно ножками короче мне нужно достучаться до этих полутора тысяч и коллег и объяснить им что пожалуйста люди обновите приложение на то есть это это как бы ну вот я могу там короче мне нужно будет как-то до людей донести эту мысль и во-вторых дождаться того как бы про продолжительный процесс который будет занимать там не знаю может пару недель в лучшем случае на также чего тут на мой взгляд не очень хорошо это вот интеграции со старым годом на то есть представьте nude то же самое монолит да там какой-то код который там десятилетней давности него никто не хочет лезть а если ты залезешь то будет еще хуже надо чтобы взять это библиотечка вставить в тот же код это так как такая нетривиальная операция да и последнее для меня пожалуй это наиболее такой важный фактор это то что ты библиотечка на получается как бы немножко опциональная а потому что почему потому что но вот у нас есть какие-то эти компоненте кида то есть всегда найдется какой супер умный программист который скажет вот короче говоря вот я знаю как сделать сервис discovery я приду к тебе на прямую и вот чтобы вот это вот как бы момент то есть что вот этому момент как бы нивелировать не нам придется городом карнизов со стороны какие-то тренинги да ты как-то людям объяснять и тут тут как бы в чем в чем такая сложность тотема хочу рассказать небольшую такую историю в этом году так получилось я достаточно много летаю в этом в этом году я лет совершит нам порядка 40 перелетов и вот на каждый на каждом рейсе я вижу одну и ту же самую картину вот вы сели в самолет да там делается объявление штамм пожалуйста выключите телефон и и вот там и за зажигать какие-то там табло делается несколько несколько объявлений сути пожалуйста пассажиры выключить телефон и да блин каждый раз каждый раз найдется там один или два человека который блин не выключили он телефон ну вот он надо ему на то есть вот вот кисть на мои на наш случае получается что мы можем написать берем эту библиотеку можем там как-то там сделать git этом статьи круто рассказать что вот люди пожалуйста используйте на но принц сигнал есть какой-нибудь супер умный программист который будете тупо привет карлом да вот это по поводу библиотечки а второй подход достаточно интересный подход с локальным прокси на то есть идея тут собственно вот эту самую библиотечка мы начинаем предоставлять как сервис то есть мы ставим какой-то рядом с приложением локальный сервис на то есть получается applications взаимодействует с вот этим сервером локальным прочные простым и ттп да это локальный взаимодействие 100 грамм так вот буква р там сильно напакостить не может просто мы ходим там палубах интерфейса ну и прокси взаимодействуют с внешним миром чего здесь хорошего это ну собственно это решение для любого стыка да то есть если это мы говорим про прах это прокси любой язык умеет говорить пахах это т.п. да ты шо он там послал его локальный запросе и все хорошо нас автоматически появляется консистентной поведение да потому что ну прокси один имплементации 1 он становится доступен для любого языка на и вот контролируемый апгрейт на то есть теперь получается как бы вот это вот логика она не является частью приложения оно является частью прокси которая контролируя на и соответственно там чем же самому папе тому кто там чему вас там отгородиться локальной пакеты да я могу там накатить новую версию в случае если в этом есть необходимость и хорошая интеграция со старым годом дата исполчать как бы если нас какой то код на он там куда то там ходил мы можем взять и просто сменить импланты и сказать что вот вместо уж ты ходил к там в удаленный склад куда то там вот теперь заходи к нам на локальные там на локальный адрес на локальный порт и мы все сделаем для тебя хорошего никакое решение не лишь но минусов да то есть чего здесь не очень хорошо это первое это накладные расходы то у нас появляется какой-то хоп да да дополнитель который должен понять наш запрос и как распарсить и выполнить дальше да ну и такой более нетривиальный сетап сложно сетап нужно понять нам как сзади плоти прокси какого контролировать вот смотрите на данный момент мы с вами как бы рассмотрели проблему и посмотрели эти два варианта решения когда у нас есть клиентская библиотека есть они крымского лютика какая то умная бью библиотека и есть какой-то локальный прокси это сотни игр вот то те два подхода на которых строится сервис мышь сервис марш это вообще а сама концепция как бы это сеть сервисов это якобы не сервис mashita сети сервисов и в задачу сервис меж входит вот как раз таки предоставления как бы это и инфраструктурный слой который предоставляет вашим сервисом простые способы простые и надежные способы взаимодействия друг с другом на то чтобы когда сервиса хотел сходить сервис ибо это было все как бы просто если там какие то возникли проблемы в полете да то есть наша инфраструктура его обработал а вот значит 100 гр а на втором подходе с вариантом прокси для booking.com мы мы остановились я хочу вам сейчас более подробно рассказать как это работает то есть опять же возвращаясь к этому на нашем кейсу сервис консиллерами сервис-провайдером а теперь они нас взаимодействия через какие-то прокси да и кроме то это вот то что в терминологии через плей сервис мышь называется дейта plain plain по-русски переводится как плоскость а то как бы тот не тот уровень где где у нас текут данные то есть происходит взаимодействие против такой дополнительной вещи которая называется контру plain то есть на самом деле вот эти вот прокси в терминологии сервис мыши они обычные динамические конфигурируемые да над ними стоит какой-то такой дирижер до которой всем говорить что вот вот вот у нас если ты увидел за вопрос который нужно послать на сервис б вот иди в ту сторону сейчас на нас этом слайде станет это более понятно контру play нам в во первых можно сказать практиках бы там что слушать на тихо портах там как какая политика по ссылке запросов есть там что то делать случай того что если появилась какая-то ошибка там а ну собственно сам сервис discovery и ещё много чего он имеет делать что это как бы такая основная диаграммы сейчас опять же хочу сделать ряд таких люменов чтобы более подробно рассказать вам про его составляющие компоненты и 1 просто я хочу рассказать им это вот что такое прокси да то есть какие там варианты есть на данный момент как бы индустрии рынок предоставляет нам 2 варианта вот этих вот прокси-серверов это link or die и in white lingerie диета java демон который основан на сенегала на библиотеки от twitter и инвайт разработка лифт есть такое конкурента опера в силиконовой долине вот они написали это дело на плюсах если вы посмотрите на вот эти вот фичи вот этих вот проксей то там короче говоря одно и то же они функционал у них там есть конечно различия но функционал у них в целом абсолютно идентичен и мы у себя букинге пинка когда вы выбирали variants прокси мы рассмотрели их оба и в итоге мы остановились на варианте с in cardiff почему потому что у него есть извините мы остались на на на варианте с ян го ими отказались от linker де почему извините почему потому что смотрите так такие как бы на два критических моментов для них во-первых link or die построен целиком для облака и по этой причине его нет грейс пул рестарта есть он не умеет и стартовать себя не оборвав клиентские connection и что как бы для локального прокси очень важно он он он построен для того случай когда у нас говоря деп деп ловится приложение там в тот же самый кубер найти статус наставлять какое-то количество входов если нам нужно что то сделать приложением ты мы берем шадоу ним под целиком и поднимаем где-то новый там процедуре старта не нету и второе это то что linker де это только через прокси тогда как и инвайт в том числе он имеет проектировать просто эти соединения это не совсем то что нужно прямо сейчас но скажем так такой хороший задел на будущее потому что в теории уинго я такая модульная структура и он позволяет создавать плагины для пар сеня по сути любых протоколов для проектирование любого про такого например данный момент там есть фильтры для редиса для mongo db для динамо что там еще было как бы вот с рестарта мы тут прямо без чего мы не могли жить потому что на собственно вот наш кейс с сбр металл с машинками дату когда нам нужно тупо взять и при старте прокси так так чтобы не оборвать сомнения с этапами прочь это больше такой задел на будущее вот а это про прокси и значит смотрите такова особенность вы видите то что прокси по идее как бы в в концепции сервис мышь ставится на кабина обосную на обе стороны на сторону клиента и сервера мы как бы на данный момент решили немножко сделать себе немножко упростить задачу да и сказали что вот мы пока только будем экспериментировать в и внедрять прокси на клиентской стороне а то есть на серверной стороне у нас тупо остался индекс которого он там и был ну и собственно грамм он понятно на данный момент он не контролируется контрольная панель кстати там через несколько слайдов когда я буду говорить про доступные на рынке контрольной панельки есть проект который называется индекс мишон на данный момент очень в зачаточном состоянии но он как бы позволяет вот идея в том чтобы интегрировать вот этот контрольную панель и engine и чтобы можно было engine.exe динамически конфигурировать вот значит окей про прок я вам рассказал про то что у нас engine x на серверной стороне тоже рассказал давайте посмотрим собственно как выглядит тот как выглядит полеты за запроса от нашего от нашего костюмера к нашему провайдеру фокусируемся на утро на взаимодействие значит вот все то же самое сервис консьюмер себя в провайдер есть мантра plain наша applications хочет послать какой-то запрос да то есть у нас идет на локальный порт говорит что вот я хочу сходить там в в корень да и вот хост что важно хост этот мой поисковый сервис я буду рассматривать показать вам пример на примере поискового сервиса значит в этот момент in вою нужно понимать а куда идти на то есть ему нужно понять что ко мне пришли с хвостом search . сервис мне нужно куда-то сходить для этого в рамках control plain есть такое понятие как road discovery которая как бы посылает данные в в инвойс проксю говорят о том что вот короче говоря если ты увидел вот такой-то домен все что . сервис и нужно сходить в такой-то кластер что такое кластером буквально через секунду причем если ты когда ты пойдешь пожалуйста поставь тайм-аут в 2000 миллисекунд в две секунды если ты встретишь какие-то ошибки на пирре там какую-то 500 ошибку пожалуйста паре троих но ретро и не более трех раз это такая как бы policy на которой вот которая как бы контрольно под панели диктует прокси что делать теперь смотрите у нас как бы мы поняли что такое домен но мы ввели новое понятие кластер и для кластера есть еще одна вещь которая теперь уже называется кластер discovery который опять же говорит нашей прокси что вот короче гора если тебе понадобится такой-то кластер то вот у нас к нему нужно применять вот такую-то политику лот браунинга например рандом и когда ты будешь устанавливать персистенции соединения об этом буквально опять же 4 шаг то есть январь короче когда январь буду стараться не с индексами что-то пойдет не так то вот при меня вот такой то конечно тайм-аут и последнее что здесь есть это такой сюрприз а сервис discovery который уже нам дает понимание о том а что такое кластер в этом случае вот наш кластер и вот он состоит из вот такого то наборах оставь ты что тоже такой классический сервисным discovery теперь как бы уинго и есть четкое понимание что если ко мне пришли из цеха стоимостью существует . сервис мне нужно сходить вот в такой-то кластер который состоит из 2-х став там 10 11 110 112 на их как-то там рандомно сбалансировать и применить правильный тайм-аут вот наши двое пошел он пошел по дтп с нато sisley локальные взаимодействие похоть и т.п. то вот между процессами можно сделать это тпс он там установил percent на и соединение достучался до и джинсы достучался до самого провайдера и вдруг провайдер нам предоставляет отличается ошибкой в этом случае январь понимает что у меня пришло 500 ошибка но у меня есть policy который говорит что если 3 500 ошибку сделали трой да и он как бы делает ретро this абсолютно прозрачна для нашего приложения опять же запрос полетать вынуждены с провайдеры вдруг там по какой-то причине там например там то база данных которая была там за за этим провайдером поднялась и провайдер нам ответил успешно а то есть запрос прилетает обратно вы двое и наконец-то нашей двое отвечает успешным ответом нашему изначальному при приложении такой workflow как выглядит в прочном виде как выглядит процесс отправки request a и тут такой как бы важный момент дело в том что то есть получается вот на каждый запрос январь он не ходит контрам plain да то есть вот contra plain пласта диктует правила по которым следует выполнять те или иные операции то есть грубо говоря если у нас control plain отвалилась то ничего не произойдет то есть январь он кэширует последнее состояние ответственно будет просто бы следовать старым правилам то я проверял реально nanopro практике там посмотрим нас есть там какой-то такой не очень маленький диплом тыс я реально shadow не во всей ее составе клон counter uplay на и все про продолжала работать прекрасно окей значит это вот что у нас есть по поводу взаимодействия возвращать к бы дело с ума от на предыдущую схему давайте теперь пару слов про эти вот что из себя представляет контра plain значит и себя она представляет следующую штуку значит да тут стоит сказать что контру плинтус если в этот прокси новой мы взяли как бы open source на из рынка to control plain мы написали свою собственную значит в ней есть эти вот три компонента которые по сути по сути является как бы rest api sky в которую периодически стучится in воина перед каждые пять секунд он ходит он спрашивает а если что новое и контру plain она она отдает ему результаты из локального кэша который берет например для нашего для нашего стыка с бормотал стека да она берет данные и pir это наша старая система сервис discovery у нас в букинге есть своя приторная system service кадры которые основаны на закипит в этом плане как бы контрплан просто является практик ней да и для нашего облака на контуром plain подключается к обернитесь ким мастером там слушает notifications выставив там картина подписывается на на правильные уведомления от водку вернитесь и по сути проектируют эти запросы проектируют эти сообщения в январь я уже упомянул дату что контра plain с самописный но на рынке есть такой очень интересный продукт который называется истер продукт этот это по сути контра plain от гугла эти вот google данный момент насколько мне известно достаточно интенсивно инвестирует в развитие самого как как самого январь как от этого ключевого компонента за сервис мышь прокси так и в контрольную панель q для него это панелька клёвая она очень умная умеет делать много чего нами идиотом очень сложные правила роутинга bird м она умеет сказать прокси что типа если ты видел такой то завоза запросто сделать там какой-то трафик сплит например на может нам сказать сделай сделай короче так сказать то есть если сервиса нужно было сходить в сервису бы версии 1 то контрольную панель куми сказать прокси что сделай когда ты поддержка сервису б версии 1 также мы пошли копию в в сервис б 202 открывается клёвой возможности для как вот такого смог песенка когда у нас мы берем просто реабилитируем клинский трафик при этом не не у не учитывая ответы обратно но с этой клёвая интересной панелька есть как бы две проблемы на первых на на мой взгляд проблема номер один то что она очень умная но оно очень сложное там реально как бы очень много составных частей в них там своя dsl свой язык который позволяет описывать эти все правила мы как бы для себя на нее смотрим очень-очень так интенсивно и как бы оцениваем насколько насколько вот тот функционал которой она предоставляет нам нужен это как бы такой открытый вопрос мы продолжим ее оценивать и вторая проблема с этой панелькой в том что она не production вроде google обещаю сделать ее продакшен ради до конца 2017 года и на данный момент вот эта панелька умеет управлять как своим так и link or die и что интересно вот опять же я упоминал этот проект яндекс мышь engine меж который по идее должен научить engine x управляться с вот это вот панельки так ладно про панельку рассказал в тертый я вам рассказал как что какие подходы есть все в построении сервис мышь что у нас есть библиотечка у нас есть прокси на мы посмотрели как он проходит запрос вот как бы пункта а в пункт б а теперь я поделюсь с вами какими-то такими практическими данными на что там есть интересного в продакшене как а у нас есть сетап во первых на данный момент in varna сзади play на порядка тысячи серверов и когда мы сделали такое первоначальные внедрение мы мы мы столкнулись такими интересными сложностями при первой сложности вообще на уровне как бы анекдота кто знал что кто знает что хоть этапа эфедры кейс in сенситив я не знал до и короче игра у нас есть у нас в букинге были самописные внутренние клиенты которые то момент не учитывали и короче grow in vain он когда через себя проектируется запрос он делает около варфейс на всех ядер и кстати наши как бы них и ттп не rfc complain клиенты tube отвалились потому что контент лент где си большая и л большая не равно контент здесь и маленькая ел маленькая это как бы такое немножко анекдотический случай ну короче если у вас есть какие-то не раз цепным complaint клиенты вы будете у тебя-то внедрять вот вот вы наступите на ту же самую граблю мораль не используйте не используйте и самописные хотят по клиенты и второе это то что мы от январе хотели получить от от всего сервис мышь хотели получить как бы везде белить и то есть как бы мы хотели получить метрики о том как взаимодействуют наш наш клиент сервер надо причем так как прокси оставить на клиентской стороне то это будет метрики именно с клиентской страницу с не будут включать как бы все данные про round trip и мы реальных получили но получилось так что мы мы получили их очень много дело в том что а то есть инвойс стоит на клиентской машине он знает про какое-то количество кластеров например в нашем поисковом сервисе каждый шар дует отдельный кластер то есть этих кластеров много и на каждый классе он генерирует порядка 100 метрик то есть если эти все цифры как бы там ну три тысячи серверов тона на десятки кластеров на сотни метрик если сектор может а port такие внушительные числа короче у нас на графике закончилось место мы как бы экспортировали эти метрики в graphite нас тупо там закончилось место поэтому данный момент мы как бы скажем так мы решили эту проблему просто снизив рецепшен но но как бы это сказать-то кого workaround в перспективе нам нужно переходить на тоже самый мы рассмотрим вариант перехода на направилась который умеет как бы все дела агрегировать и хранить в более компактном виде на значит так как у нас у прокси является такой как бы ключевым компонентом мы реально уделили много времени чтобы проверить то что вот то что она обещает она реально делать но как может звучит немножко как бы наивно not реально это было для нас важно поэтому реально проверили что грейс пул restart работает в продакшн я пробовал то есть когда льется production in traffic обновлять версию делать restart все прекрасно и то есть клиенты не отваливается от прокси и проверены то что я уже упоминал до то что контрольная панель к отвалившийся контрольная панель к не влияет на коммуникации ну и софт эти все как бы и тайм-аут и ретро и быков на игра работает вот значит давайте теперь посмотрим на следующий график смотрите то есть вот когда я рассказывал про январь и первым минусом это было то что у него есть в архиве на этом графике изображена перси флейте . перси встретиться значит это лайкните которая как бы то как видите и client dataset лет миссис клиентской стороне который включает все вот такие round trip и там полный поход тут у нас есть 5 перцентиле и 50 и 75 90 и 105 99 на и вот это вот тот момент когда мы включили январь в продакшене и вот стрелочками у нас получается что оверхед на на вот на на одну прокси составляет в районе одной миллисекунды а вот какие получать у нас 30 или немножко выросли вырос немножко 75 95 99 и это собственно говоря прямо коррелируется с тем что заявляет автор инвайта что каждый как бы hop in воет добавляет в районе одной миллисекунды это было хоть и т.п. давайте посмотрим на ттп с это уже другой сервис в котором которым клиент и сервер и общается похоти tps все те же самые перцентиле да то есть папа по сетке у нас помнили секундам что там в самом низу 10 миллисекунд самом верху 40 это вот тот момент когда мы включили январь да и тут начался как будто подъем как бы запросы стали немножко медленнее но это ожидаемая почему ожидаемый потому что у нас двое начал устанавливать президент на и соединение с с индексом на другой стороне то есть появились хан шейки которая занимает какое-то время и после того как эти хан шейки как бы ну то есть январь установил достаточное количество percent они не все собственно говоря нормализовалось а и тут опять же у нас получается что где-то оверхед на вот этот вот хоп на самый 0 составляет в рамках одной миллисекунды это то что мы видим другой значит пример по степи you по загрузке значит смотрите это вот этот опасный трафик ноты которые вы делать порядка двух тысяч запросов в секунду и на shingo и выедает порядка 3 ядра до порядка 3 одного ядра то есть как бы не очень много как бы для нас это полностью приемлемо ok значит мы с вами я вам уже тут рассказываю 40 минут меня уже начинает заканчиваться время давайте я подведу какие-то итоге мы у себя на данный момент внедряем вот эту идею сервис мышь на основе на основе проксей то есть контроль plain самописная новой прокси и в целом опыт у нас положительный она тут стоит заметить то что мы как бы в середине пути на то есть мы начали этот проект где-то с полгода назад в продакшен мы ушли где-то три месяца назад то есть мы как бы впечатление положительные ожидания большие что из этого в конце концов получится думаю будет интересно это хват на следующий лот при любом случае если тобой положительный опыт если вы будет отрицательный опыт чего мы ожидаем от всего этого проекта это в первую очередь унификации наличие и из коробки управления трафиком а то есть на данный момент у нас есть как бы букинге возможность управлять трафиком но только как бы когда запрос входит как бы в саму организацию на уровне как бы высокоуровневых лол балансиров а вот на уровне взаимодействия как бы компонентов друг с другом как бы такого большого контроля нет то есть нет возможности как бы сказать что вот ты ты корчишь сервис если ты у тебя есть если ты консьюмер ты хочешь сходить комнату к провайдеру этот провайдер есть в трех инстанциях да то есть в данный момент если ты хочешь там какой-то один теперь сходи в другой или там как то сделать но трафик split что там 50 процентов идет туда 55 или сделать какой-то трафик мира унификации наличие из коробки механизмов надежности на так так как и действий в том что любое взаимодействие между сервисами будет идти через прокси то это а как бы единственный способ клиентам клиенту до достучаться до сервис тойота как бы та часть нашей инфраструктуры которую мы контролируем полностью и которую и и в котором мы можем выставить какие-то правильные дефолтные тайм-аут и правильные дефолтные там стратегии ретро я быков а им им и мы увидим как бы хороший набор метрик который опять же есть по дефолту которые есть на клиентской стороне есть планы большие это как бы такой выводы для меня для для как бы все всего проектов букинге что вы можете вынести для себя из моего доклада 1 как бы такой мамам место если вы делаете какую-то сервис-ориентированной или microserver текстуру пожалуйста уделяете выделяйтесь существенное внимание проблемы взаимодействия сервиса эти все ретро и быков и тайм-аут и мониторинг tracing с monkey это все маст хэв вы можете использовать существующие библиотеки которые это все дело реализует то мгуки сестры книгу герпесе на мой взгляд если у вас в компании гомогенная инфраструктура съесть вас говоря одним так вот вы решили что вот мы пишем только на яве и все либо вы какая-то небольшая компания на то есть вы можете как бы там между собой да да говорите что если у нас там какие то есть разные компоненте кита мы будем см и сделать так чтобы они работали одинаково или вот у вас есть какая-то такая сильная культура что вот ну короче говоря вот мы делаем так и никак по-другому шаг влево вправо расстрел и последний пункт на мой взгляд когда лучше использовать какие-то решения на клинских библиотеках и когда вам реально нужно сильно высокая производительность а потому что какой бы там высокопроизводительной прокси не был он добавляет какой то оверхед это это в случае клиентских библиотек случае прокси на мой взгляд опять же если вы какая-то крупная компания от вас реально там много людей и вас или у вас гетерогенность так какие-то разные компоненты и такой момент на мой взгляд проект вести очень интересный инвайта же . бы следить и смотрите что из этого получится окей у меня все остался только один единственный слайд я уже упоминал до то что несколько раз то что мы как бы находимся середине пути если у вас есть какой то опыт подобных проектов мне было бы очень интересно услышать а как это делать как вы это делаете с какими проблемами вы столкнулись и как вы их решаете пожалуйста напишите подойдите я очень буду рад с вами пообщаться ну все на этом все спасибо большое да спасибо за доклад 1 раз на такой вопрос делали ты какой-нибудь стресс тест на собственно двое потому что но там 3 пил понятно зависит от того сколько запруда и так далее значит и да и нет я как бы ожидал этот вопрос комментария как бы не сторонник таких вот синтетических стресс-тестов что вы давайте сейчас как будто мы сгенерируем на нагрузку и и как-то сделаем да поэтому с этой стороны нет но как бы я ждал вопрос поэтому да я сделал как этот тест вот по моей практике короче говоря январь получается чуть чуть иметь ну не то чтобы медленнее он где-то на 25 30 процентов медленнее чем индекс он по-прежнему быстро короче на 1 ядро то есть например такая же диалоги то что есть мастер процесс варки процесса в каждом варки процесса свой event лук и на каждый этот worker мне удалось выжить из него порядка там 16 18000р пасеку но это как бы мои чисто такие наши эксперименты на коленке поэтому как бы пожалуйста не надо это воспринимать как хотя такой адекватный тест спасибо за доклад у меня вопрос было то есть у они пока мере у них написано что можно настроить безопасность то есть весь трафик монетизировать через и таким образом настроить что клиент а может направлять запросы только на сервис и b больше никуда не может и соответственно сам опроса без если это возможно ну скажем так по идее да то есть потому что получается короче края панелька скажем так протоколы опишите позволяет вам пушить сертификаты в январь я не помню там по моему там стопудово можно пушить server certificate когда у вас январь выступает в качестве лист венера можно ли пушите клиентский сертификаты я не помню по моему можно однозначно не могу сказать нам в любом случае как бы у него очень модульная архитектура то у него по сути как бы он является типе проксей и над ним плагином стоит как бы их и т.п. бартер то есть если там в него встроятся на и короче вы можете короче то по такой конструктор в котором вы можете сделать все что вам надо и он писал на здоровье спасибо классный доклад вопрос 2 первый вопрос зачем там to alice ну кажется что этаж внутри сети запросы ну там вот но собственно вот первый ну скажем так смотри ты лес у нас используется под какие-то важные вещи связанные security например под хранилища кредитных карточек ну просто маст хэв под хранилище персональных данных просто маска то есть какие-то такие чувствительные вещи это ответ номер один ответ номер два это то что теоретически скажем так вот это облако которую мы строим внутри мы тоже могут короче телец вам позволяет не зависеть от того где находится ваше облака на то есть она может быть как внутри так может быть и снаружи в этом случае у вас уже весь трафик зашифрован и собственно там ходить по интернету на фиксе ну то есть это приятное дополнение не обязательно весь трафик т.д. ну то есть в security вещах это как бы маской авто например есть сервис от модификации сервис авторизации да только нему весь трафик зашло ну понятно как бы по другие вещи да ты как бы такое ну вообще смотри то есть для себя в компании мы решили то что мы хотим как бы все новые сервисы строить стелсом как раз для того чтобы иметь гибкость и возможность выносить как бы сервис и вне компании понял спасибо и второй вопрос тоже почему балансер на стороне прокси и лот balancing они на стороны не на стороне собственно сервиса потому что кажется что если прокси и будет довольно много если к этому сервису будут ходить довольно много к land of town в какой-то момент может рассинхрон эболой балансировка не так будет удар или хотя бы в в контроллере не смотри начиная как бы сзади то есть контроль play ни в коем ни в коем случае не не участвует во взаимодействии сервисов она участвует только ну точно как бы выдает правила игры все да . этот момент почему на клиентской не на серверной то есть в принципе можно использовать два варианта что еще раз то есть помогать мне ответит появится . отказа на серверной стороне то есть еще один ход еще одна еще один балансер который там настоятельно в целом понятно спасибо парню вам спасибо за доклад у меня два вопроса первый вопрос по поводу того что in вот как я понимаю используется как прокси для всех ну клиентских запросов то есть у нас не под каждый интерфейс для конкретного сервиса отдельный инстанции двое для всех один да то есть эта единая точка отказа которая в случае падения просто изолируйте здесь и диалоги примерно такая то есть во первых как бы прокси она стоит на локальной машине на соответственно если на отвалилась то отвалится только вот вот только сам клиент понятно да и в этом случае как бы подрывается что если прокси отвалилась то но до считается нукаба мертвый какие-то завязываете ну скажем так в середине пути еще пока нету нов новым вообще да то есть вообще как бы в планах то что грубой если договор получаться как если бы все соединение хочешь про проксю если практиковать эту по цене не не дойдут как бы здесь на самом деле можно же и хавчик они кого-то не не надо но но тут уже может быть вариант когда сервисом самодостаточный а вот конкретно этой проксю им пользуется в каких-то особых кейсах воду с одной стороны может отвечать нормально на какие-то запросы на на какие-то запросы не может потому что прокси недоступна но мне кажется здесь мы немножко как бы то есть ты говоришь про случай когда у тебя с сервера обслуживать входящие запросы или и входящие запросы в сервис и любом случае поступает запрос а дальше за тем чтобы обработать запрос он может как на своей стране решением проблемы так и сходить куда-то за дополнительными да я понял абсолютно прав то есть получается если как бы у нас получать как бы два пути что исходящий и входящий запрос идут через проксю во все входящие идут мимо практику учатся такой случай то что так и мы можем суть вопрос это как бы то есть да это проблема на данный момент мы никак не решаем потому что скажем так в теории исходящие то есть такие входя и входящие запросы по ней тоже должны идти через комп roxy возможно я об этом что-то еще не думал руки ответил вопрос спасибо большое и второй вопрос по поводу думали ли вы об этой прокси как способе обогащение допустим дополнительными данными но более сложная игра не просто да да то есть например одним из способов которому то есть у нас короче когда запрос пролетает через проксю выставляется ряд кедров которые например нам однозначно идентифицирует запрос а то есть прокся выставляет x request иди и выставлять ему запрос это 1 вторых мы мы мы мы выставляем ряд кедров который нам позволяет узнать как бы откуда пришел за просто что на самом деле в крупные организации важный например мы выставляем фейдер что вот вот этот запрос пришел с такого-то дата центра до или он пришел из какого-то сервиса а то есть героя на стороне при приемке мы как бы то есть прокся находится под нашим контролем мы можем очень четко доверять этим вещи кедром и знать что вот их выставила как бы наш доверенный агенты если там стоит что там запрос пришел и там из какого-то дата-центр один точно знаем что он оттуда и пошел куда бы он не был там zara один частично у нас тут я имел ввиду более сложную логику допустим мы отправляем запрос какой-то сервис на этому сервису для того чтобы в работах запрос нужно еще какие-то данные из другого сервиса и мы вот на уровне прокси описан нитку нет прокси это транспорт то есть она даже не смотрит в поила то есть максиму что она делает она парсит кедры а все что как бы будет а вот ну как бы абсолютно the transparent то есть как бы ничего ничего такого умного с спас приводом не делается все понял спасибо и вам спасибо большое за доклад вот вы говорили про примерно тысячи серверов в продакшене это имеется ввиду контейнеров то есть на каждом контейнере стоит свой прокси и единстве микро сервис запущен да то есть это нет это имейте ввиду именно железных серверов то есть почему так потому что то есть опять же мы в середине пути и integrated сам январь это тоже самое все они очень хорошо дружат скутер нить из у них очень тесной интеграции поэтому как бы как это все будет работать там это скажем так менее менее интересный вопрос против в вопросах а как это будет работать нашем старом сайте который нам нужно поддерживать поэтому как бы на данный момент мы фокусируемся вот именно на старом стаки и поэтому вот эти тысячи машин это 1000 обычных железных серверов тысячи железный серверов по даже запущенную какое-то количество когда нет ни контейнеров а вот там грубо сайт booking.com она как бы есть но вот та куда за диплом инвайта монету спасибо за доклад очень интересно получилось у вас были пожелания к системе в том числе авторизация влажная как-то решено не как будет ли ну вот скажем так идеологически очень очень хотелось бы например вынести авторизация между между и сервисами на уровне именно прокси и джинкс и и 00 до с помощью серверных и клиентских сертификатов и вопрос поводу можно ли в январе порт указать кровли hasta можно спасибо можно я про авторизацию спасибо за доклад скажите пожалуйста почему отказались от паттерна какого-то опять битва и он позволит решить проблему даже добавить какое-то слой там каширование хотеть не очень хорошо то есть на самом деле мы от него не отказались мы просто сказали то что albea гетры у нас находится на локальной машине ну и тем самым получили проблем с авторизацией внутри сервиса почему ну только что сказали что нет авторизации между церкви и и и как бы не было мы мы только строим ее а так это был централизовано удобно просто ну то есть у вас получается как бы весь трафик пройдет через пойдет через и на точку отказа но такое и можно масштабировать но так вот мы и с масштабируемыми про сказали то что вот все эти ноты которые бы у нас были копы часть вот этого айпи битвы у нас как бы они стали локальными у нас как бы один есть единая система контроля который позволяет нам вот эти все там тысячи десятки тысяч машин оркестре ровать в едином стиле понятно своеобразно спасибо можно про авторизацию дальше поражу то есть сервис к сервису вам сможете авторизовать там на уровне что это вопрос в том что если клиент обращается изначально к одному сервису этот ко второму тот к третьему и так дальше по цепочке эту информацию о об авторизации клиента 100 она остается только на первом правильно я понимаю lilo как то есть смотрите россии по клиентским сертификатом вы можете авторизовываться как бы service to the service вы не можете авторизовать там человека к сервису то есть из за того что в тогда чего как сервис вам нужны какие-то более сложный механизм обычно там когда от вас запрос который знает что вот я в я ваня я прошел booking.com там выдается ему какой-то там токины потом это talking как бы везде вот передается по по всей цепочке это как бы немножко другой уровень ну вот то есть это правильный подход будет и вы придете да то есть я тонул скажем так очень хотелось чтобы мы к нему пришли у меня вопрос . я прошу прощения мы модератор у нас уже 1400 наверное уже да давайте мы с вами вот буквально 210 метров и я отвечу на ваш вопрос хорошо спасибо большое-большое спасибо ивану за интересный хорошо так вот"
}