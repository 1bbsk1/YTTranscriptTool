{
  "video_id": "_ZbwXm8-VsE",
  "channel": "HighLoadChannel",
  "title": "Балансировка HTTP-трафика / Антон Резников (Mail.Ru Group)",
  "views": 5417,
  "duration": 3157,
  "published": "2017-06-28T07:08:38-07:00",
  "text": "меня все еще зовут антон резников и я работаю над проектом облако mail.ru но мы поговорим о балансе арте трафика немного других примерах давайте начнем что нужно для балансировки трафика ну как не странно это не всегда гаяне сетевое оборудование в первую очередь его источник эффективный легальный бизнес социальных ограничений думаю хотите нам подойдут далее нужна команда познакомьтесь это менеджер майк лица его второе имя по gummies павел он немного перфекционист и очень любит писать красивый кот а еще большего переписывать и администратор тони в мегу ленив но ответственным и профессионален и конечно же нужен план мы будем говорить о запусти социальной сети для любителей пустить и смотреть фотографии котиков нет дотка это или просто на tkat план амбициозен и достаточно просто начнем мы запуска бета-версии по инвайтам далее захватим москву через нее пойдем на россию а там и до мирового маску господство недалеко в качестве платформы был видал выбор ламп linux падь москве и что-нибудь на букву п в данном случае нас больше всего интересует apache потому что он использует модель iphone ну собственно почему при и почему фок fok потому что каждый процесс обрабатывает единовременно только одно пользовательское соединение потому что выяснилось что невыгодно запускать процесс во время установки соединения процесс запускается заранее и обрабатывать несколько соединений с конфета многие из вас скажут ну да апатия это ж не модно и неэффективно но скажем по джанга или печка и пэм лежит та же самая модель как много типов и можем запустить ответ достаточно прост нужно взять размер свободной оперативной памяти поделить его на максимальный размер уокера и мы получим некая н повышать которые крайне нежелательно как много фпс мы можем получить с одного сервера мы берем количество киров делим на средние время обработки запроса и получаем некое число не нетрудно заметить что при увеличении времени обработки запроса например если в маску или ваш немного подтормаживает эффективность сервис сервера сильно падает и так павел он все еще программист допустил прототип сети на своем компьютере станица получилось достаточно легкой но содержащий большое количество файлов павел любил раскладывать все по полочкам после чего сайт был выгружен на тестовый сервер чтобы показать его менеджер отдыхающему в таиланде сожалению сайт работал на тестовом сервере сильно медленнее чем работа на локальном компьютере давайте разберемся почему каждый браузер имеет лимит на количество соединений когда муха the tape echo 100 стоит отметить что лимит этот сильно разнится так как до недавнего времени все имел явное ограничение если не ошибаюсь максимум 6 соединений с недавнего времени формулировка на меня на очень обтекаемое разумное количество соединений чем например воспользовались разработчики интернет и но подождите у нас все еще есть канал 100 мегабит которыми же почему не очень дешево стоит неужели провайдер нас обманывает вообще-то нет давайте рассмотрим процесс передачи файла на примере файла лапка . png чтобы передать данные по сети файл нужно разбить на пакет и конечно же в реальности это происходит прямо при записи пакетов по и записи данных сотен но ради красивые картинки мы разбили его задние первое что должен сделать клиент установить соединение это происходит в виде так называемого трехступенчатого 10х пока пожатия и тратится на это около 1 раунд и type или пинга также вот на этой стадии передается запрос далее сервер должен отправить данные но сколько пакетов отправить насколько широкий канал клиента мы пока этого не знаем поэтому серые начиная с небольшого количества пакетов так называемого тисе пи окна и отправляет их клиенту в данном случае это 2 в реальности конечно значение побольше если не ошибаюсь современных минусах свои 10 получив подтверждение от клиентов себе понимает что канал не перегружен отправляет еще данное решение тем самым окно на последнем этапе он управляет последний пакет и соединение нужно еще закрыть примерно так выглядит изменения ширины тисе пи окна с течением времени в рамках одного соединения конечно же это идеальный случае в данном примере мы предполагаем что у нас потей обусловленных только перегрузка и канала но никакими другими факторами в реальности очень часто размер окна будет просаживаться из-за того что на где-то на магистральных каналах происходят пиковые нагрузки и пакета теряются из всего этого вытекает достаточно простое соотношение вы не можете передать через одно тисе пи соединение данных больше чем длина тисе пи окна деленное на pink именно поэтому у со вокальная машины сайт работал достаточно быстро из таиланда очень медленно и именно поэтому популярны разного года давно вот он же которые скачивают большие файлы несколько потоков то не быстро нашел решением был включен тип его им но что поменялось изначально все также мы также устанавливаем соединение также передаем данные также расширяем тисе пи окно но нет закрытия соединения а через некоторое время если не истек таймаут на keepalive браузер может снова воспользоваться этим соединением уже установленным уже разогретом с более высокой передаче с более высокой скорости передачи данных и так майк был очень доволен результатом решил приступить бета-тестированию за я что-то пошло не так и очень скоро пользователи получившие инвайты начали жаловаться на проблемы с доступом к сайту да мы все еще используем и фок модель все еще используем apache и при включенном тебе вайф пользователи нет пуск не развивали соединения что приводило к тому что даже небольшое количество юзеров исчерпала количество активов полностью и новые клиенты не могли установить соединение то не был очень не доволен тем что майк не предупредил его запусти но быстро нашел решения на самом деле он давно хотел установить engineer но проклятая пакости нации не давал это сделать что же изменилось engine x использовать другую модель обработки запросов он может на одном в акте выкинуть тысячи и даже десятки тысяч соединений его активов можно запустить ни один и даже медленно и клиенты которые раньше нагружали наш сервер не будут создавать нам проблемам так как ответ от apache будет вычислен в буфер engine.exe и отдан с требуемой скоростью и только engine x превратил мужество соединение от пользователей в минимально необходимое количество запросов к патчу ok пора нам на москву ну до начала нам нужно увеличить количество сигналов увеличить то не решил выбрать достаточно простую схему балансировки через dns как то происходит к нашему донести вы обращается пользователь из спрашивает что за одессу этой замечательной социальные сети на tkat 7-му отдает 2 пи адреса после чего клиент случайным образом выбирает себе сервер и отправляет заказ и за счет того что клиентов много и особо закономерности выбор и sega нет мы получаем равномерно размазанную нагрузку как много серверов можно балансировать на такой схеме согласно действующему ныне и все 1123 dns сервер должен поддерживать в гипер и передавать через него данные размером не более 512 мбайт тисе пи желательно поддерживать но необязательно чем пользоваться ноги и системные администраторы на самом деле обязательно 2016 года но вспомните по ipv6 и попробуйте спрогнозировать когда данное требование стандарта воплотится в жизнь итак согласно тому же стандарту мы должны потратить около 16 байт на одну а запись и около 100 байт на заголовок и только мы получаем двадцать пять серверов в пределе при использовании убитым но есть нюанс между клиента почти никогда не обращаются к нашему серверу напрямую обычно между нашим сервером и клиентом есть dns сервер провайдера этих клиентов возможно еще локальный кэш и вещи dns у него на машине извините не приключился сайт поэтому крайне редко кто либо масштабирует таким образом более 4 0 6 адресов кроме того при балансировке серверов таким образом стал вопрос с тем что дело с картинками во-первых не хотела сохранить их в 4 копиях а именно чистые сервера планировал поставить тони да и при передаче картинок соединение до севера забивались и маленькие запроса копии вынуждены были ждать своей очереди достаточно долго сайт становился менее отзывчивым поэтому статика клиентский контент был вынесен на отдельный сервер это не только позволило сэкономить на дисках но и вы еще скорость работы сайта итак сервер работал на 4 итак сервис работу на 4 северах поддерживающих api но однажды ранним утром понедельника всех дельта u шоу не вернулся хуже того отель а это время в течение которого клиенты могут не перри запрашивать информацию об ip-адресах сайта был выставлен в один день 25 процентов пользователей начали испытывать проблем с доступом к сайту что же делать-то не вызвонил дежурного специалиста из data center и попросил поднять любой сервер с этим опиаты сам чтобы боксировать запросы с поднятого сервера на рабочие все являться почем но как только себе был поднят тут же проблема ушла сама с собой что произошло с любой современный клиент способен передать все адрес все адреса из ответа отданное сервера если ваш сервер лежи и не отвечает никаким образом в течение тайм-аута нас одеяние клиент будет жать вдруг ему ответит ваш сервер если как в нашем случае серых поднят нем нет не запущена никакой веб-сервис и никто не понимается дней не на 80 порту клиент получает отказ фактически за время главной одному pingu после чего клиент прозрачен для пользователя идет на другой сервис собаковода очень не любили данную социальную сеть как-то котики котики котики ни одной собаки и однажды они обнаружили что поисковые запросы работают очень медленно они подумали а что если мы направим apache benchmark это утилита для тестирования производительности вашего сервиса на эту социальную сеть ведь утилита не спрашивают ваш вы сайт тестируйте или нет они так и сделали они создали большое количество запросов к методу апиа и search и севером стало плохо моя скрыли работал все медленнее и все вело к тому что сайт русского имени должен был полностью лечь хуже того так как на те же серверах хранилось куча стать и тех же с css файлы нельзя было использовать лимиты на connect и клиента бы не смогли быстро загрузить файлы не подходил не над фильтр на самом деле надфиль то можно было решить проблему но какой одни сад ранним утром в понедельник сможет это настроить яндекс также не умеют работать с лимитами на путь с автор no limit и реализовывать когда ваш все пытается приложить достаточно поздно поэтому тонет и не волевое решение лучше потерять одну функцию чем потерять их все и поиск был отключен по результатам данного инцидента тони и павел решили перенести всю статику на отдельный сервер и это не только упростило жизнь это не так как теперь он мог легко лимитировать количество запросов но и увеличила скорость доступа к сайту россия и начнём мы пожалуй стоит за в новый dns в новые дата-центр с хорошим усилий то непредусмотрительно я выставил отеле название в пять минут и рассчитывал что в течение десяти ну максимум 30 минут все пользователи пи-пи идут с api адресов самых ставим dc на api адреса и снова в dc как бы не так к сожалению значительная часть пользователей так и продолжал пользоваться старыми dns серверами что между прочим летела в копеечку менеджер майк вынужден был заплатить за 2 дополнительных недель i ain't и и он требовал объяснений и строя казалось просто я занят на в городе было ты популярных провайдера один из них хоть телеком не вносил никаких изменений в трафик и с ним было все хорошо у телеком экономил на трафике поэтому все запросы в dns были принудительно за кашель он и на один день скрутили ком экономию на системных администраторов поэтому большая часть их конфигурационных файлов представляла собой компиляцию сосцы к верху и каширования загадочным образом была настроена на в одну неделю также встал вопрос с семенами стать контента пользователей фоточек было много и их нельзя было хранить на одном сервере поэтому было решено купить еще два как же балансировать нагрузку между ними тони и павел долго спорили то не предлагал распределять за косы и дефектами ведь мы в базе нашей базе данных точно знаем где лежит файл и можем отправить на нужное хранилище но этот способ совершенно не подходит для повода файлов поэтому павел предложу специальный механизм на джесс который умным образом выбегает нужны серверы но это решение было не очень управляемым поэтому то не начал настаивать на выделение отдельно вопросы которые бы зная где лежит файл начинал отдавать пользователю несмотря не зависимо от того знает пользователь на каков совершенно лежит или нет в итоге было принято трехслойная схема свой хранилища был не доступен пользователю над ним лежали прокси в этом случае кстати надо не забыть отключить кэширование engine.exe на диск иначе получается очень весело прокси-сервер знал где лежит нужный файл и пи апраксин запрос пользователя к нужному стоит опционально там можно было настроить кашель некая чего контента чего очень хотел тони боясь высоких нагрузок механизм на клиенте периодически обновлять информацию о доступных системах что позволяло относительно быстро вывести фокси сервис на гости или добавить новый также стал вопрос об масштабирование селюков под апиа и 27 дюймов забивать dns никто не хотел поэтому не выделил отдельности вида назвав их балансирами и поставил там яндекс яндекс равномерно распределяю запросы между серверами собачьим все шло замечательно более того если по каким-то причинам один из типов ломался прозрачно для пользователя перенаправлял запрос на другой работающий себе ты не нравилось любил рассказывать г-н дженкс особенно он становился синим сентиментальным когда вспоминал про то как играх сысоев лично пожал ему лапку но это все лирика давайте пойдем более интересной истории до часто в высоконагруженных проектах тяжелая воде к реализуется на языках более низкого уровня так было нашем случае специальный код для добавления стикеров на фотографии был написан на си плюс плюс к сожалению одно из исключений был не было обработано и получил с буйный jpeg такой код падал вместе с мостиком что же делал и джинкс решил что 7x буйный помечал его на 10 секунд как недоступным и приходил к новому серверу бак замечательно воспроизводился на всех семьях кластером пока хоть что-то работало после чего сайт становился недоступен на 10 секунд то не кусал себе локти он был очень недоволен но он понимал что дело не в engine x а в том что он не прочитал документацию всего лишь нужно было ограничить количество попыток к обращению другим серверам достаточно было 2 ну максимум 3 зачем было обходить весь кластер так собственно тони и сделал а также решил что после одной ошибки совершенно помещать как с буйным у нас осталась еще одна почка отказа это сам сервер сам жены кс если что-то идет не так обучиться дата-центры запинается кабель или пригорает блок питания к сожалению сайт становится недоступен для примерно для половины пользователей то не решил что ему прекрасно подойдет в р.п. это решение в котором есть так называемый виайпи виктор p-cad которые обслуживают два сервера выделяется мастер и backend мастер обслуживает запасы всегда backend обслуживают запросы если что-то пошло не так к сожалению данная схема требовала наличия l2 сети между серверами да и то не недавно купил новое оборудование фирмы tissot киса вот поэтому он и шёл переделать схему на работу с о бюджете на самом деле он мог использовать и другой протокол на самом деле например новый сплав но а бюджет и был короче в данной схеме уже не требовалось наличие l2 сети между серверами но также был виталий пим который анонсировался двумя сериями связано метрика и если мастер работал то трафик шел на него так он анонсировал описать меньше метрикой если что-то шло не так to master the backup принимал на себя нагрузку ну что замахнемся на версию постите россию мы уже прошли тони майк грезил его до жены миллионы людей и все в белых штанах поэтому экспансии решили начать с бразилии для увеличения отзывчивости сайта были подняты всегда вы его даже нега пользователи точнее трафик от них раз поднялся с помощью дело dns все шло хорошо но некоторые пользователи из россии пользовавшиеся публичном dns попадали в бразилию ну точнее попадали не пользователи пользователи бы не возражали против такого расклада попадали их запросы и сайт у них работал сильно медленнее чем у тех у кого запроса попадали в россию это было вызвано использованием публичных сивов это иногда случалось из-за того что база d1s обновлялась недостаточно быстро схема в принципе рабочий но требует активной поддержке поэтому тони всю своей лености решил попробовать другую схему была куплена автономная система автономная система это набор api адреса в который управляется одним администратором под адресата нужно иметь в виду не вашего сисадмина а некую организацию которая и покупает данный набор api адресов имей автономную систему вы можете анонсировать ее в нескольких частях света своим соседям если вы договорились с соседями то трафик может через них может пойти и со всего мира так и было сделано автономная система была поднята в трех точках в принципе таким образом нужно обслуживать все api достаточно отдать клиенту абу хтмл к в которой уже прописаны все нужные данные куда сходить вы его вы мыска или в не йог за счет того что провайдер выбегает для своего клиента точнее для трафика своего клиента оптимальный маршрут в этом случае надо помнить он выбегает маршрут с наименьшими тыкай это не всегда расстояние иногда тофика появляется куда дешевле иногда он отправляется по более быстрому канала но в подавляющем большинстве случаев это наиболее быстрый маршрут поэтому пользователем и замазка начали попадать в навсегда москве пользователи из бразилии пользовались с играми выводы жена его и так майк повел свою команду к намеченному намеченной цели но он смог и дальше а мы подведем небольшой итог чего хотелось бы сказать в первую очередь не пытайтесь найти решение универсального решения для балансировки трафика трафик бывает разное это бывает и опять с короткими запросами задержки в которых очень чувствителен для пользы для пользователя это бывает и контент задержки на соединение которое ему не очень чувствительны но важна быстрая скорость передачи данных это бывает и а под от пользователя если вы разделяете эти задачи и для каждой из них ищите отдельные решения вам будет намного проще имейте plan b a лучше b и c так как какое бы дорогое оборудование вы не использовали какой бы надежный сорт вы не писали рано или поздно что-нибудь сломается что-нибудь поиграет если не у вас то у соседей и затронет вас и не забывайте читать документацию даже с таким замечательным со в том как яндекс достаточно просто выстрелить себе в ногу если вы не убедились что ваш конфиг работает именно так как вы этого хотели с докладом все и я думаю может перейти к вопросам антон большое спасибо за такой фундаментальный доклад я прям прослезился четыре года наших бессонных ночей все в 5 котиках очень кого-то большое спасибо отлично от меня все правильно описано честно вот ровно так так и надо строить если новизне так вопросы я вас очень прошу вас много я уверен что там было много там непонятного там как х ттп с трафик например балансировать или вот как стать иван сердечки разницу но разница на самом деле не настолько велика дело в том что х ттп с требует большей нагрузки на циpкa потому что трафик и должен быть зашифрованы и дешев и он поэтому кроме более мощных цикл особых нюансов нет ануида до сих пор есть клиенты которые не умеют работать с ансипа он называется техника когда на одном ее пи адрес и можно поднять несколько из этих тифе катофф до вопросы друзья мои неужели вот все настолько понятно что вопросов не осталось можно идти делать автономную систему здрасте спасибо за вот так вот а вот там слайде клиент с помощью скрипта посылать на нужные прокси запрос в этот жесткий тонна себя что представляет произвольный код его задача запросить с сервера список доступных фокси и выбрать их случайным образом в принципе вы можете из сервера каким-либо образом но скажем hаши вы по они клиента выдавать ему не весь список прокси серверов а например 2 которыми он может пользоваться тут важно что вам нужно вывести в любом в любой момент вам может понадобиться вывести сверяясь нагрузки либо наоборот добавить еще запросы на контент они работают ну достаточно долго и понятное дело пункте нельзя просто выключить участие клиентов не до грузятся картинки но если ее убрать из списка в течение минуты все клиенты обновлять список доступных фокси то на данной машине останутся только соединение заживающие свое и через ну зависимости от контента там через десять минут возможно в течение пары часов все на севере не останется никого и его можно будет выглядеть у меня маленький вопрос а вот какое время реально отели для dns и но там для работы то есть мы в практике например через 15 минут при минимальном за ты лишь наблюдаем 80 процентов что она переключилась а вот за сколько превращает 90 95 99 процентов трафика если мы выставили ttr там в три минуты для dns балансировки зависит от профиля клиентов конечно же но в целом большая часть клиентов должна переключаться в течение отель то есть если у вас три минуты то 90 процентов но я думаю достигнет ну че из 6 точным дальше в зависимости от того как вам повезло с клиентами и скажем ну в москве на и нажимал кто экономит таким образом тазики пытается что-то сделать в дион ах но еще случается повезет в принципе да скажите пожалуйста антон вот те способы отказоустойчивости балансировщик of которые вы предлагаете они связаны в общем с управлением сети и какой бюджет и как в с pf коктейль 2 а давайте теперь представим что у вас небольшой стартапы у вас нет человека с компетенцией там всего в инженеры чтобы вы и все оборудование у вас арендованный или вообще виртуальные чтобы вы предложили в этом случае как обеспечить отказоустойчивость проекта спасибо тесно вопрос ну если у вас оборудование виртуальные и вы скорее всего платите деньги и за балансировку и за циpкa и завтра фиг ну я думаю вас такой проблемы нет если у вас оборудование арендовано и реальная то чтобы скажем использовать монтировку через яндекс ничего более не нужно если мы идем дальше и нужен наверху или фолд elegance через обе джипе да придется либо арендовать сетевое оборудование либо договориться с данным дата-центром все очень зависит но поверьте все очень сильно зависит от того где вы снимаете антон такой вопрос смотрите вы показали нам несколько способов блокировки ну dns round robin он понятен и как бы мыло ставим следующий это было балансировка на яндекс а вот что делать когда исходящий трафик то есть входящий трафик мы сбалансировали и раздали на несколько патентов над багандов порождается исходящий трафик а что делать если мы запираемся на этом он же никс чтобы раздать его есть вариант с использованием ну например и пвс эта схема в которой ваш баланс их понимает только входящие запросы я работал в варианте когда трафик через api dapi туннель котировался на любой из брендов на бэк андах на балансе и на брендах был понят один тут же вижу . и эта схема достаточно хорошо работает единственное так как тофик так как балансе видит только входящий трафик он достаточно долго помнит клиента поэтому клиенты активно использующие над не очень быстро уходят с серверов даже когда мы его вычеркиваем и загрузки говорим нет балансе никогда больше не посылать туда новых клиентов только тех которые дорабатывают свое иногда клиент умудряется пойти 23 газа потому что он идёт с того же адрес из того же факта спасибо и вопрос пола следующему способу сиротки это бюджет автономной системы вот можно по ним поподробнее потому что ширине было понятно в каком именно месте как она вообще в принципе работа хорошо давайте у нее мтс откроет мы подумаем автономную систему в нескольких местах и а насилуем ее соседям естественно с которыми мы должны договориться в покупке каналов как доступна в данном случае это ты и точки после чего но на самом деле вы клиент можем мог бы обратиться к любой из-за точек где поднят это нужная пи адрес но маршрут от клиентов выбегает не он маршрут убирает в первую очередь правой так провайдеры пользуются все тем же бюджеты и его оборудование выстаивает маршруты для всех адресов интернет если адрес доступен из нескольких мест то выбирается маршрут с наименьшими такой ну как правило это число подключенных автономных систем между источником и получателям если не сильно вмешивается политическая конъюнктура я в молодости помню случай когда между двумя соседними зданиями алтайском крае трафик шел через германию то тофик пойдет ближайшим образом если клиент находится где-нибудь в подмосковье то у него шансов попасть на другие сервера вне маска ну примерно 0 спасибо спасибо за доклад такой вопрос почему до покупки автономной системы не попробовали g1s ну где один из групп опробован и в принципе это работающая и даже пожалуй ковалентно и решения на дешевле дешевле но требует постоянной поддержке вы должны следить за актуальностью база данных должна что-то делать с публичными серверами например запомните откуда приходит от них запасы и отдавать ну что-то там средние например и мыска вместо какого-то локального сервера спасибо здравствуйте спасибо за доклад у меня два вопроса первое это а что будет на этой картинке если падает дата-центра которые в москве а второй вопрос принципе какие существуют варианты балансировки между это центрами как я понял это можно делать на уровне клиента то есть java скрипте получать дотация список дата-центров и на случай найти второй вариант это с помощью dns и подходит би джи пи и если еще какие то варианты в случае если дата-центром искал падет бюджет и сессии будут разорваны соседями и более они не будут видеть маршрута к нужным айпи в этом месте ну я думаю что нью-йорка в данном случае намного ближе чем бразилия и трафик пойдет туда естественно все будет работать медленнее в качестве балансировки да такое решение может быть использовано но только для коротких запасов это либо отдача маленькой статике либо ну какие то быстро работающие методы api ну мы выходим выносим за скобки то что если у вас в нескольких местах поднятая пивом нужно как-то между ними синхронизировать данные для длинных коннектов например вы отдаете не знаю видео в два гигабайта это подходит не очень хорошо изменится маршрут и this присоединения пропадет клиент постить он ни в чем не виноват поэтому уж лучше ему отдавать отдать адрес конкретного сервера например ну если мы знаем что клиент пришел к нам изначально на дата-центры вы маска отдадим и пи адрес который доступен только из-за мыска до если мы отдавали несколько жирных файлов и серве неожиданно выключился тисе пи соединения погнуться но в противном случае никаких проблем с и балансировкой мы не словим спасибо спасибо за рассказ такой вопрос про ваши опыты проводил балансировку потому что мне никак не получается равномерно равномерно сортировать по гео балансировать знаете если честно у меня тоже не получается 60 на 40 это норма меньше никак ровнее то есть если стоит два сервера подождите вопрос именно по делу то есть у вас в двух очках ну dns балансировка да у нас гиа dns но это не суть важно проблема именно в том что если мы отдаем 2 и и адресов dns-записи 60 на 40 запрос уважаться нифига не 50 mixed я последнее время немного отошел от вопросов эксплуатации на ранние как я помню была замечательная бага в windows vista которая приводила к тому что dns балансировка цеха съезжало возможно та же проблема существует и сейчас и до часть клиентов например пытается сортировать ваш айпи адреса если для вас принципиально и требуется разделить 50 на 50 можно найти решение которое бы отдавала один ip адрес ну не совсем по стандарту но как раз с распределением 50 на 50 это дарит где-то до 52 на 48 ну мы же понимаем что стопроцентной точности не бывает engine xm точнее хорошо спасибо значит это не неправильно готовлю это жизнь такая извините можно вопрос значит вопрос заключат следующим по поводу bdp джипик балансировки вы говорите что в принципе маршрута gdp они достаточно предсказуемы и определяются минимальным временем прохождения трафика между соответствующими каналами которые обслуживают этот маршрут до что будет если на пути маршрутов встретиться хай джек или или бюджет pillar так называемый то есть будет третьей стороной вмонтирована ваша сеть допустим да и перенаправлен ваш трафик каким образом вы будете значит анализировать перераспределение трафика в данном случае и балансировать эти канал но пример такое допустим компанию куратор часто используют своих технологиях хай джек да когда perry переопределяя трафик в свои сети дать то есть чужой трафик свои сети соответственно его там балансируют своими методами и определяя отправленных на хаст и клиентские обратно до как в вашем случае будут работать значит перехват этого трафика ну перехват трафика очевидно будет работать если кто-то подымет уже автономную систему другом месте хотя мне кажется это сво пускай и по сетям нежели именно по методам балансировки хата то птк ну вы хотите это сбалансировать соответственно в данном случае балансировки вас не получится потому что то что вы хотите получить вы получите с большими задержками так как пройдет через узлы про которые вы не рассчитывать в настоящее время да если кто-то по договоренности ли за нами и на подымет данную сеть можно нарушить будто балансировки вопрос к чему если на узлах какие-то дополнительные точки анализы би джи пи и соответственно таблицы маршрутизации без dp и значит сравнение их с какими-то эталонами и перераспределение трафика у уже согласно этих таблиц скажем так систему мониторинга которая определяет правильность прохождения маршрута в настоящее время я боюсь что я не отвечу на этот вопрос потому что всегда у нас был отдельный сетевой отдел естественно мониторинг использовался в той или иной мере спасибо за ответ по поводу мониторинга маленькое дополнение вы всегда можете с клиент собрать метрики из браузер до шире там из любого клиента там пар по сетевым задержкам пока никто и посмотреть какого ip адрес у вас из какой под стиль ипотеке у вас выросли connect и время время соединения там какая-то аномалия то есть против нормально россия россия потом бац где-то выросла и посмотреть уже по ним там точечно есть там просадки именно по маршрутам и маршруты там в полуавтоматическом режиме или даже автоматическом просто перекидывать но это все собирается с помощью клиентских метрик это это решаемая задача это мы в частности так делаем может куратор то что делают другие метрики вопрос а тот спасибо за классный доклад про котиков и провал анкеровку мне вопрос не всем по докладу скорее про облако mail.ru скажите пожалуйста как много у вас сервер обслуживает собственно ваш сервис и какой-нибудь неболшой инсайд сколько-то центр сколько компьютеров какой рпс спасибо бойся метан точных данных потому что они могли бы через них можно было бы получить и размер аудитории но савинов на самом деле не так много на график их менее 100 дата-центров так ты или четыре любом случае на хабы и вот буквально в ближайшие дни выйдет пост по отдыха я думаю более подобно можно будет обсудить в комментариях видимо коммерческой сильно закрытой информации мы сейчас на антона тамино довели до кантона тайный сейчас переживает трансляции чтобы понимали все пишется он сейчас чинг ляпнет а потом бац завтра приходит в этом заявлении новости он у него красивые котики на что же делает здрасьте а я вот с этой стороны спасибо хороший доклад единстве вопрос что до что после bpm то есть вот эта часть почему-то никто собственного не затрагивать ну хорошо затронули вопрос безопасности вопрос того что интернет действительно ломают примеру лет пять-шесть назад яндекс прям на одной из конференций заявлял что у них я сдача баллов тайтлов для карты происходит как раз вот в разных регионах с одного и того же а печника по сути вот последним вашим способом отсюда вопрос на который они кстати не смогли ни как ответить есть пограничные места в сети где два маршрута по сути являются равноправными то есть можно пойти налево и провайдер на правый провайдер и принял и получить одно и то же в этом случае трафик пользователя один и тот же к в рамках одного соединения отправляется сразу часть предположим москву предположим там еще куда-нить вот вы мама 6 головой я уверен что это не так никто не проводит доказательств как вот сделать на city чтобы это не происходило так вот если действительно какие-то способы мониторинга может быть тоже кого-то как-то проверяет или как-то статическим анализом что-нибудь а если я совсем навский вопрос судя по выражение лица человека задала ну убедить меня что это не так ну расскажите что дальше-то после об этом спасибо дальше луна и романтика дайте отдохнуть менеджер что касается вашего вопроса ну в первую очередь это мониторинг того как работает положение на стороне клиента если вы раздаете трафик для мобильного приложения пусть она пишет куда-либо мид количество ошибок если вы испытаете срyб вы также можете писать количество незагруженных той таваф ну данном примере то есть если так случилось что sin пакет ушел в один дата-центр ок с запросом ушел в другой ну это означает что соединение не будет установлена и кусочек не будет загружен это отлавливается на клиенте да нет я потому что соедини уже установлена мы там не знаю там часть данных уже получили отправляем какие-то данные но по факту кусок отправился немножко в другом направлении это не на нашей стороне это на стороне непосредственно правой 1 га то есть пока трафик идет просто по двум каналам просто в одном канале он за вернется в одну сторону другом в дугу они никак не согласованы и вот честно вы мне не докажет обратного судя по всему вот пока не убедительно ну с монитором понятно то есть мы загружаем некую страничку на нем снимаем какие-то показатели потом отправляем статистику действительно ли это единственный метод для того чтобы понимать вообще что в интернете происходит или все таки нет ну насколько я знаю практически единственные то есть да вы можете на своем сетевом оборудовании какие-то данные со бегать но извините да например увидеть что у вас какой то с каким тогда этом приходят пакеты от совершенно левых соединений но наилучший мониторинг что все работает это сана клиента потому что вы совершенно не права это не проблема провайдера и не наша проблема это проблема клиента у него не работает софт и если вы а виноват на чаш конечно нет нет эта проблема клиента у него не работает софта проблемы клиента должны жать поставщик услуги то есть если ваш сайт скажем 99 процентов случаях работа это в одном пациенте не работает то нужно искать тех клиентов у которых не работает и попытаться применить другой метод я собственно к чему вот если у вас несколько точек присутствия в выкладывайте 1 и печник ты бы идеи хорошо было бы сделать research в каких регионах у вас конкретных провайдеров одинаковые по скажем не по сложности о по сортировке маршруту то есть грубо говоря как в одном отдельно взятом провайдере выбирается маршрут как яд надо вас а какую проблему это решает но это решает что весь интернет у человека работает нормально а какой-то один взятый сайт будет работать некорректно а что значит не корректно это значит что трафик будет разбиваться просто на две локации он будет в каждом случае не работать нет такого не будет но как ни бури стройка таблиц маршрутизацию потанин нет перестройки выполняется не каждую секунду у нас не будет такого что нас там один сайт работает нестабильно как-то это узкий вид на сетевой вопросы сейчас вспоминаю нашу опытом последних четырех лет по постройке то аналогичной системы вот у нас нет такого что там вот у нас пользовать пишу нас не работает сайт на больших файлах когда будет рваться соединение из-за того что пилинг между провайдерами где-то упал и один маршрут заменился другим на больших файлов при передаче больших там два гигабайта он прост файл поломается но этому пользователю не повезло да и тут как бы задачи чтобы маршрут и перестраивались как можно реже это задачи провайдера но на маленьких файлах такой проблемы примерно нет я не об этом я о ситуации когда нет перестройки маршрутов есть два маршрута 2 pro 2 ap стрима у провайдера куда трафик расходятся от провайдера намеренно равноправно но у него делит вот физически делит он 50 процентов трафика отправляет одному ops 3 ему а 50 процентов трафика давай отдает другому австрии мы не ловили ну а как хочет так и делит под пятьдесят процентов да бывает всякое нет но на самом деле я тут соглашусь бывает всякое потому что если человек на компе россию чего-то откуда-то ну я думаю этого можно добиться но в реальности ребята не ловили я такой проблемы тоже не припомню если придется столкнуться можете оставить визитку после презентации но у нас а так нам вопросы по существу доклады уже закончились я предлагаю вот совсем узкие технически вынести уже в кулуары поблагодарим еще раз антона как правильно сказал спасибо ближайшее время я буду около стенда тарантулом"
}