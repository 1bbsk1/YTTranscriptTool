{
  "video_id": "FESHGMCKQ7s",
  "channel": "HighLoadChannel",
  "title": "Переход к Platform as a Service в Яндекс Вертикалях / Константин Касев, Мария Васильева",
  "views": 695,
  "duration": 2801,
  "published": "2024-04-17T01:10:27-07:00",
  "text": "Мы хотим вам рассказать о том как служба эксплуатации превратилась в службу разработки пас доклад будет интересен всем тем кто стоит в начале пути системная администратором который задается вопросом что дальше тем кто строит свой пас Возможно они смогут избежать наших ошибок а также тем Кто уже построил свой пас может быть они подчеркнут для себя что-то новое пардон Извините если говорить эксперт А вторую Яндекс аренда Яндекс недвижимость Яндекс путешествия и другие все это крутится более чем на 1000 серверах ты сейчас серверов это более 30 тысяч ядер более 100 терабайт оперативной памяти на всем этом располагается более 8 более 2000 микросервисов И эти микросервисы занимает более 8000 контейнеров извиняк нам прилетает 10 тысяч РПС а внутренний трафик составляет 150 тысяч РПС Или 200 тысяч одновременно коннектов у полторы тысячи микросервисов есть свой балансировщик доклад будет состоять из нескольких частей первая часть это то какие мы были следующая часть это переходный период на пути пас но и следующие две части это то каким мы стали Наши планы на будущее Итак 5-7 лет назад мы были отделам системы администрирования мы держали парк чуть менее чем 500 серверов мы как классический администраторы занимались всем в полу ручном режиме если нам надо было расширить кластер или создать новый то админ брал железку отправлял ее в переналивку железка переналивалась на не устанавливалась какая-то операционная система с минимум программного обеспечения на нем админ прописывал ДНС перечеркивал ванны админ прогонял Анти был типа этой железке если разработчики говорили что им нужен балансировщик над их сервисами на этом кластере то админ отправлял заявку службы ног ноки делали какую-то сетевики они делали какую-то свою магию потом было много ручной работы релизились мы с помощью dep пакетов разработчики писали код упаковывали его в пакеты ставили тикеты на выкладку утром приходил дежурный администратор и раскатывал новую версию пакетов Ну как иногда раскатывал иногда не раскатывал если багет не раскатывался то админ смотрел Почему не раскатывается и если была проблема на стороне эксплуатации починила если проблема на стороне разработки то он собственно связывался с разработчиком который стоял тикет на выкладку и дальше они либо ход пиксель либо откаты либо что-то еще делали для метрик мы использовали графит для отображения использовали графаном впрочем графаном и сейчас используем Она нас полностью устраивает для логов у нас был поднят отдельный кластер на кластере стоял Пусть сервер а на остальных мастерах она стоял Пусть клиент у серые пыж клиент это внутри индексовые решения Яндекса Пусть клиент натравливается на файлы слогами читает их и отправляет Пусть сервера Пусть сервер предпринимает данные раскладывает их по каталогам и файликам в этих каталожках мы были единой точкой входа мониторинга когда что-то ломалось или работало не так как планировалось то дежурным администратору звонил мониторинг дежурили мы 24 на 7 звонки принимали по всем сервисам когда мониторинг звонил админу то он смотрел что сломалась но опять же если это было на стороне эксплуатации ТО админ чинил если это было на стороне сервисов то админ связывался с разработчиком ответственным за этот сервис иногда бывало так то что тикет на выкладку ставился месяц назад собственно админ связывался как правило с тем разработчиком который ставил на выкладку вот этот разработчик мог быть в отпуске или он мог вообще отсутствовать в Яндексе уволиться или это вообще была ночь и тогда админ кому-нибудь там позвонил разбудил его оказалось Он разбудил не того человека сервис лежит админ кого-то разбудил В итоге это стресс какой-то и прогресса нет короче какая-то жизнь была тогда у нас была автоматика но ее было немного у нас были какие-то на коленочные скрипты у нас были системы которые отслеживаются состоянием железа например на всей системы которая смотрит за состоянием дисков на сервере если диск неисправен то создается заявка инженером дата-центр инженеры получают заявку меняют сбойный диск вытаскивает старый вставляет новый сервер И в этот момент бывает так то что сервис ходит в ритуале это может быть например сервер с мастером SQL собственно мастерской уходит freedonly админ опять звонит мониторинг админ там просыпается Или там Если не спал то достает просто свои скрипты переключает мастер на делает слоев резервный слоев новым мастером старый мастер отправляет переналивку потом опять Буки потом брал из бэкапа дамба за данных восстанавливал старый мастер как Слышь его догоняться когда Слыш догонял мастер то ставил под нагрузку работы много было ее да И при этом у нас кластеров было много было какой-то зоопарк баз данных в каждом кластере были какие-то может быть многим из вас это даже знакомо а На этот период у нас было много проблем но Первое это то что много ручной работы было админы были неким таким бутылочным горлышком если им ставили тикеты на выкладку админ в этот момент занимался мониторингом или катил другой тикет какой-то релизной проблемный то все остальные ждали его просто очереди и на этом этапе на самом деле было очень много стресса у нас нам это все надоело Мы решили автоматизировать то что можно автоматизировать стали искать так называемые точки роста как я уже говорил когда сервер отправляющий переналивку то после при наливки по нему надо прогнать антибул плейбуки Это вроде автоматика такая не руками скрипты конфигера складываем но требуют действовать администратора мы подняли отдельный кластер сделали на нем сетевую шару в эту шару выкачивались последние версии антибл playbok и системы контроля версий далее Все другие кластера конектились к этой сетевой шаре и подтягивали к себе нужные плывучки затем по крону раз пять минут каким-то рандомом они теплые Буки сами по себе прогоняли стало немного легче то есть админ отправляет железка переналивку железки переналивается поднимается подтягивает плывут и сама по себе их прогоняет и сама встает под нагрузку примерно в этот же период Мы решили то что релизы могут катать самые разработчики мы дали им такую возможность а еще решили то что они могут катать 24 на 7 тоже стало полегче участь администратора не требуется но бывало иногда так то что релиз катят выходной день релиз не самый удачный что-то ломается и опять мониторинг звонит администратором на тот момент докер уже стал популярным мы его тоже использовали какие-то тесты все гоняли Как вы знаете докер позволяет создавать изолированную среду причем подготовка этой среды она на стороне разработки собственно нам оставалось только автоматизировать развертывание докер контейнеров мы посмотрели в интернете несколько систем оркестрации контейнеров остановились на двух из них Это куберетесь на тот момент крайне плохо или вообще не умел работать А у нас все инфраструктуры это сервера собственно наш выбор Пал на хешкорд работает и собственно Все работает друг друга Они отлично интегрированы У консула есть очень удобная и очень удобный а из минусов хочется отметить то что у них не такой большой комьюнити как у кабернетиса и Консул становится нашим вендерлоком он Действительно удобный мы действительно него сильно завязываемся но при этом пока что планов отказываться от него у нас нету в итоге мы развернули кластер из 20 нот клиентов погоняли на них докер контейнеры замерили ТТХ и нас все устроило Мы решили завозить в ном от нашей нереалтаемые сервисы запускалкой Job nomad мы выбрали Левант подсовываешь Леван то шаблончик Джо б нам от передаешь аргументами Какие параметры например версию докер контейнера говоришь где писька но надо и собственно левот диплоиджоба нам это запускалке левента мы выбрали дженкинс потому что нам было не важно где нажимать на кнопки на самом деле наши разработчики запоры вечеров накидали Грузии скрипт работал с файликами в определенном формате файле указывающие имя сервис соответственно разработчиков их телеграм-каналы грузинский гриб запускается на серию с женкисом и появляется кнопочка для тепло и того или иного сервиса собственно этот момент разработчики уже стали диплоить контейнеры без участия администраторов по запускав некоторое время не реалтаемовые сервисы Мы решили завозить на материал Time сервисы и встал вопрос как подавать на них трафик на тот момент у нас уже был хобброкси мы Обновили его до последней версии и взяли такую штуку как Консул template Консул template это тоже решение хешкорпа оно работает сервисами в консуле оно умеет генерировать файлики и мне это выполнять какие-то действия например релотка прокси но файлики собственно прокси все было хорошо первое время до тех пор пока контейнеров стало количество контейнеров выросло настолько что консультимплей делал релотка прокси при каждом запуске и тут вылезла две баги первая Бага это прокси платил тренды есть 3 лодка прокси появляются копия трейдов так происходит несколько раз в итоге на сервере возрастает его речь нагрузка в вся система подтормаживает а вторая Бага это сервис отправляет запрос через копрокси Консул templay делает релотка прокси И в этот момент у него срывает крышу вместо того чтобы отдать ответ сервису на его запрос kprox отдает ему свои акции слоги это было больно это было неприятно мы стали смотреть Чем можно заменить haproxy посмотрели в интернете несколько систем и остановились на янвой инвой по тем нашим замерам он добавлял всего лишь две миллисекунды для трафика на 3dc и 4 миллисекунды для кроссы трафика собственно нас это устроило где-то за месяц-полтора два мы написали так называемый Control plain для него январь работал сервисами в консуле он собирал адреса докер контейнеров адреса сервисов объединял их группу только те адреса у которых есть одинаковые тег и генерировал для них конфликт балансировщика А дальше периодичностью конфликт балансировщика подтягивал и собственно сервиса появлялся балансер для удобства работы мы от кластером запись и собственно тоже стало немножко попроще разработчики сами делают свой сервис сами выставляют нужный тег их сервисы диплоиться балансировщик участие администратора вообще не надо графит Мы со временем стали закапывать переходить на прометеос прометеоз имеет удобный sdk он также умеет работать сервисами в консуле Ну динамика такая в какой-то момент времени у нас графит просто-напросто закопался мы от него полностью отказались а налогов Мы тоже перешли на динамику это мы стали использовать файлбит файлбит направился на каталог на новом от клиентов на каталоге слогами сервисов читал их и отправлял ноги в кавку далее наш самописное решение простенькой самописное решение оно читало данные сказки и раскладывала логи по кластеру слогами в нужной каталоге и файлики собственно на тот момент мы пришли к такому решению жить стало проще разработчики сами диплоится администраторы отвечают только за свою часть разработчики создают балансеры строят дашборды по метрикам своего сервиса админы не такие уже нервные это решение можно брать внедрять тем кто только начинает строить пас Вот Но мы на нем не остановились Мы решили пойти немножечко дальше до дольше но дальше продолжат Нария Спасибо всем Привет Очень вас много тут это очень радует Давайте поговорим о том почему же мы пошли дальше прежде всего ответим на именно этот вопрос Мы хотели сфокусироваться на написание кода и развитие платформы а не на никечестве и системном администрирование в классическом его виде мы начали наконец-то задумываться о гибкости нашей платформы и о том чтобы нашим пользователям нашим разработчикам было удобно работать с платформой наша золотая Мечта это снизить порог входа в инфраструктуру чтобы разработчикам вертикалях не приходилось задумываться Насчет того как эта инфраструктура под капотом работает мы пришли Вот к такому стеку который представлен на слайде И сейчас мы поговорим про него немножко подробнее мы не стали Далеко ходить и решили использовать Яндекс для того чтобы селить наши виртуальной машины именно там собирали образы виртуальных машин с помощью пакера и тепло или с помощью терраформы такая классика но в тестинге мы сделали Ход конем и начали использовать там авто масштабирование и значительно сэкономили наши ресурсы для сети балансировки мы продолжаем использовать о котором рассказывал Костя с их помощью подойдем трафик непосредственно на инстанции приложений а наверху используемые джинсы которые умеет конфигурировать кажется что примерно каждый админ и с ним работает достаточно просто надеюсь вам очень нравится этот слайд здесь хотелось бы рассказать на самом деле об одном факапи который случился с нами один единственный раз с одним сервисом с одним инстансом приложения но это было очень неприятно спойлер такого больше не было Что произошло пользователь внешне заходил на сайт Яндекс объявлений и кнопку разместить объявление внезапно его директила на сайт auto.ru да не просто так еще с 301 редиректом который кэшируется в браузере примерно навсегда Что случилось под капотом мы выяснили Жил был контейнер Яндекс объявлений был у него какой-то ip-адрес но внезапно решил переселить его на другую ноду и поселил туда контейнеров Тару произошла коллизия контейнер auto.ru получил тот же ip-адрес что и контейнер Яндекс объявлений имел до этого не наша балансировка ни нашу метрики не узнали о том что они обращаются к новому контейнеру и поэтому мы получили этот очень неприятный редирект И на самом деле мы думаем что произошло это из-за того что дефолтный докер плагин выдает первые свободные ip-адрес с новым контейнером эта проблема починили бы достаточно просто написали свой докер плагин который выдает айпишники по раунд Робину и мы планируем его танцор сеть Потому что его очень удобно использовать в облаках он очень гибкий с базами данных мы продолжили нашу традицию переезда в облако мы здесь использовали менеджер решение в Яндекс облаке благодаря чему значительно сократили наш зоопарк баз данных Оказалось это очень удобно но нужно было как мы жди вовсе на тот момент были нужно было автоматизировать все что можно мы написали еще немножко автоматизации первое Консул Discovery что он делает он ходит собирает через запишечку которая у нас есть и регистрирует их консульт ставит чеки сам представляет из них мастеру репликой на выходе мы получаем очень удобно запись преимущество такой длинной записи заключается в том что она очень быстро переключается с мастера на реплику и наоборот Ну и вообще-то у нас ее использовать удобно так как он использует все второй автоматизация это х2п это утилита которая предоставляет ssh тоннель до базы данных с компьютера с локального компьютера разработчика мы имеем на входе системы права доступа на выходе мы эксплуатация получаем Локи и историю запросов которые мы можем впоследствии использовать для аудита мы как многие жили на примете очень долго Ровно до тех пор пока перестали упираться лимиты оперативной памяти на прометился нужно было что-то с этим делать Оказывается можно было создать масштабируемую конфигурацию кластера но делать это надо было еще до того как мы начали внедрять вас но мы сделали не масштабируемую систему поэтому надо было на что-то съезжать выбор очень невелик был на тот момент мы начали использовать Виктория метрикс которая очень хорошо мигрирует под прометиус здесь мы использовали Консул Discovery для того чтобы обнаруживать с которых метрики нужно собирать слогами произошла примерно такая же проблема Как с метриками мы перестали успевать их ротировать на тот момент было что-то около 50 терабайт логов в день Это очень много написали велосипед написали Локс драйвер для Джокера написали специальный сервис logs коллектор они из sada yester контейнеров собирали логи и писали в кафку Кафка отлично работает на запись а другой сервер из сказки бочами перекладывают логик Хаус что же мы получили в итоге мы сейчас умеем читать логин прямо из графаном оттуда можно читать метрики и еще мы умеем читать долгие медленные блоки за большой период времени через SQL подобный язык вообще наша команда считает что логики не нужны очень Рекомендую вам сфотографировать этот слайд доставить свой телефончики и после окончания автопатии видимо почитать о том Почему наш коллега считает что логи не являются основным средством первобилити а ими стали Трейси так вроде как все сфоткали если что Поделитесь друг с другом фотографиями Поехали дальше мы не любим брать на себя очень много точек входа по разным проблемам мы начали делегировать наш мониторинг нашим разработчикам вместе с ответственностью за база данных их приложений Нельзя просто отдавать нужно еще нельзя только брать нужно еще отдавать мы сделали автогенерацию графиков и олертов для наших разработчиков и теперь нам звонят только по проблемам с инфраструктурой И на самом деле мне не звонили ночью уже больше года я считаю что это отлично теперь самое главное это снова про регистрацию и диплой мы продолжаем на текущий момент использовать мы используем дефолтную конфигурацию мы отключили все неиспользуемые фичи но Посмотрите на эти огромные успехи которые нам приходилось писать это было долго это было долг это было больно и еще это было очень сложно это очень простой сервис это мордочка закипера Не пытайтесь читаться в этот конфиг потому что не нужно просто посмотреть какой он большой и чтобы побороть эту проблему мы написали Шива это ядро нашего пас это абстракция на одном этом которая занимается валидацией конфигов которая создает окружение для наших сервисов А еще она предоставляет очень удобный API интерфейс для диплоид сервисов Так на что же поменялись эти большие конфиги конфига стала два один из них это конфиг для людей написаны людьми здесь можно увидеть кратенькое описание сервиса и куда бежать в случае какого-то факапа и второй конфликт который принимает непосредственно участие в процессе диплоя это манифест диффле по сути это Спектр который указываются необходимые ресурсы и переменные окружения которые используются для конфигурации сервиса мы имеем диплоид не только обычные сервисы мы еще умеем диплом периодические задачи которые часто используются для автоматизации какой-то рутины и мы предоставляем несколько стратегии тепло Первое это бронхи это стенды песочницы которые можно развернуть в том числе и в продакшене для того чтобы тестировать новые фичи очень удобно и кенне-диплой диплоид нашим пасе можно по классике через telegramвод можно смотреть статус своего сервиса производить с ним любые действия подписываться на события связанные с этим сервисом в общем-то можно делать все что угодно Сидя на диване Но это мейнстрим и в этом месте Мы тоже пошли дальше мы написали свой собственный webuy очень хочется отметить наш Босс попросил нас об этом Прости чтобы чтобы юань наш он не является админкой для того чтобы диплоить сервисы и проверять их статус это гораздо общее понятие и предоставляет сильно больше функционалом прямо сейчас приглашаю вас здесь на хайловоде за диплоить бранч сервиса вместе со мной и познакомиться с нашим веб-кивай заходим наши веб-.ua Как вы видите здесь есть список моих проектов хочу выбрать один из них чтобы с ним поработать здесь появляется карточка конкретно нужного нам сервиса и Давайте перейдем на его страничку Как вы видите функционал достаточно широкий и можно делать очень много чего можно заказывать доступ напрямую к этому сервису чтобы я могла обратиться к нему через с компьютером напрямую можно копировать кучу разных переменных в том числе переменных окружения можно изменять фичи флаги если мой сервис с ними работает можно смотреть по классике особенности диплои и историю диплоя можно заниматься даже профилированием своего сервиса и переходить связанные инструменты для работы с этим сервисом сервисом например центре Или с авто сгенерированный dashboard в графане непосредственно к диплом я перехожу на страничку диплоид одна достаточно понятная кнопка на неё нажимаю и заполняю коротенькую формочку Как вы видите разработчик Может по пути при определить некоторые предметы окружения указать например очередь в трекере и так далее нажимаем кнопку запустить у нас появляется карточка диплоид этого сервиса если бы сервис был чуть сложнее то здесь бы еще и статус Прогресс бар отображался и через несколько минут наш сервис за диплоин Так что же поменялось начало нашего пути если оглядывается на начало нашей презентации мы были классическими системными администраторами делали очень много чего руками было больно нервно и очень неудобно Сейчас же мы не можем назвать себя системными администраторами по понятным причинам Мы вроде как Неди вопсы мы внезапно не поддерживаем себя сети наших разработчиков наших сервисов и внезапно Мы даже не сырья потому что мы не являемся единой точкой входа мониторингов мы считаем что мы можем назвать себя платформу инженерами потому что мы начинаем думать более высоких вещах наверное как-то так и продолжаем развивать наш ПАЗ наша платформу мы будем продолжать автоматизировать все что только можно стабилизировать будем думать о высоких вопросах из разряда оно уже ли нам сервис mash будем стабилизировать И поддерживать те решения которые работают Хорошо будем обращать архитектуру чтобы нам инженером было просто с ней работать и будем продолжать делать webuy круче И точка входа в наш ПАЗ Спасибо вам большое Я зову Костя чтобы мы вместе смогли ответить на ваши вопросы если вам оказался полезен наш доклад проголосуйте по qr-коду за него Спасибо за доклад своими традиционно по максимуму все что можно будет Мы будем сверсить за вашими анонсами чтобы понять официальных каналах Яндекса скоро будет уже Сейчас секунду Спасибо за доклад Михаил витех у меня такой вопрос есть такое приложение Называется бакстейдж от Spotify Почему вы не стали использовать его Какая Великая но классная Случайность мы вот прям докладом с человеком знающим это обсуждали на самом деле ребята когда создавали вот этот веб-юа и его предыдущую стадию которую вот чисто админкой была оказалось что нужно Ну слишком много всего доработать в backstage чтобы предоставлять тот функционал который задумалась на тот момент Но с другой стороны при этом у нас было достаточно доработок наработок чтобы развивать их Поэтому не стали использовать решение Спасибо Следующий вопрос ребят Спасибо огромное за доклад два вопросика первый это я когда вижу что-то на ямле так сказать сказано что это описание сервиса хочется подумать пробс Давайте хранить оно само будет Хмм Вот можете рассказать что вот как бы в вашем решении возможно невозможно Это вообще не о том Если ты про конфликт карты сервисов то нам кажется он достаточно удобный для человека но и собственно он пишется людьми для людей вот приходить на что-то другое пока не думали Но если поймем то что Это удобнее то Конечно перейдем Пока таких планов нету сейчас где его редактировать Спасибо и второй вот на слайде с было формочка для наката нового релиза Как выглядит дистрибутив я разработчик что я должен такого там указать как это собрать чтобы получить артефакт артефактом является докер контейнера как-то его собираешь Если честно мы предоставляем много инструментов но мы об этом не думаем Ты можешь волен брать то что тебе удобней а для диплоида всего лишь название сервиса как правило совпадает с названием докер контейнера и его версия Ну то есть там какой-то маппинг на формат Докера Да там registry Slash там все Да это в трех полях Спасибо И у нас следующий вопрос Здравствуйте спасибо за доклад У вас был слайд про то как вы Насколько я понял перешли с файл бита на свой сервис для сбора логов Вот вы сказали что вы ему спали ротировать Я так понимаю что вас логики перезатирались просто потеряли логи можете рассказать поподробнее почему Новый сервис что не так файл бетон там было просто много данных на самом деле со временем диски тоже устаревают замедляется что-то еще у нас было много проблем с тем то что срабатывал мониторинг что на серию заканчивается место Когда Мы начинали разбираться почему это происходит то мы видим что сегодня приезжает новый 50 терабайтов А вчерашние 50 терабайтов еще не ротировали снижались и старое место есть это было достаточно часто и я не знаю мы с этим пожили пару недель стали думать то что надо найти новые решения вы не поняли Почему в файлбит не справлялся и нет нет заканчивалось место на кластере слогами заправлялся файл бит отправлял в кафку все нормально наше решение вычитывалось Кафки и складывала их на Локс кластер А налог с кластер уже не успевал ротироваться почему тогда Новый сервис который как я понял читает логи с фодов там не знаю хостов почему свой сервис двух концов это все системы немножко изменили парадигму Потому что раньше мы напрямую читали файлы слогами во-первых Мы перешли на чтение здесь нужно уже первое звено в этом месте менять поэтому здесь получилось может быть сразу напрямую как на тот момент это было с коллектор и мы с конца тоже пошли и мы уже перешли от хранения логов на серверах на железе в быстрых базах которые можно быстро хранилище одно хранилище это каска в которой можно очень быстро и очень много писать а другой хранилище из которого можно было очень быстро читать Спасибо замечательный ответ на следующий вопрос да спасибо за доклад Вопрос такой кажется что ваша инфраструктура достаточно независимо от большого Яндекса и наверное вопрос Вот когда вы выбирали но мот консула энси был вообще пробовали искать решение внутри Яндекса для регистрации Если да почему они вам не подошли Но на самом деле это было давно когда мы нам перешли вот в Яндексе есть тоже свое решение порта но мы несколько более свободны в выборе решений и мы выбираем то что нам удобно в конкретный момент времени мы постоянно смотрим на решение в Яндекса мы постоянно примеряемся к индексовому диплою в том числе мы постоянно смотрим на кибернетизис Ну мы не забываем про эти тоже сторонние решения если они в какой-то момент станут более удобным и перейдём на них друзья напоминаю Не забывайте голосовать за доклад Давайте обратную связь Это очень важно для спикеров для улучшения и для строителей конференции чтобы знали что нравится у нас вопрос Здравствуйте спасибо за доклад очень Прямо так прочувствовал в нем практическую бой с которой вы столкнулись на самом деле один из вопросов сейчас человек задал который я хотел вам задать Почему вы не использовали Яндекс Но тогда я Следующий вопрос из него задам вытекающий как я понимаю у вас мощности сейчас по большей части своей или на яндекс.науде находится в феврале два месяца этого года мы закопали почти все железо что оставалось но в резерве на всякий случай оставили несколько железяк Вот но можно сказать да мы живем в Облаке в Яндекс облаке в Яндекс Облаке Да у вас получается вот это ваша пас система она развивалась так параллельно развитию яндексаблока да то есть там какие-то фичи появлялись у себя что-то делали То есть это как-то перекликалось или нет На самом деле от Яндекс облако у нас только по большей части виртуальной машины мы используем достаточно ограниченное число тех решений которые предоставляет Яндекс облако потому что очень многое у нас уже есть свое и миграция в Яндекс облако произошла достаточно поздно по таймлайну вот и это просто сделано Во имя удобства потому что здесь мы делегировали долго и нудное слежение за железом мониторинги вот эти замены дисков делегировали это инженером Яндекс облако И теперь мы очень быстро стартовать машины если мы видим на своей стране Что намечается какой-то факап то достаточно быстро можно его оккупировать именно за счет того что это виртуальная машина с ними работать просто проще у вас база данных в том числе на инстанциях Яндекс облако сейчас находится Да но не менедж так еще другое У меня вопрос другого характера Скажите как вы работаете с секретами где они его сохранятся Ну вот при диплоида откуда достаются И кто к ним вообще имеет доступ а у нас есть общий индексовый сервис который который используется для работы с секретами при диплои по сути никто не имеет доступа к этим секретам если это особенно сервисы которые попадают какие-то аудиты разработчики сервиса иногда бывают имеют доступ к сектам Вот Но если мы говорим именно про ПАЗ то вся работа с секретами делегирована собственно шире которая ядро нашего пас и сервисом в этой экосистеме шивы вот так вот диплоисервисшие проверяют э-э делегированные для секрет сервиса и дальше мы в военных помещают этот секрет разработчики почти не имеют доступа к этим секретам Ладно спасибо у нас вопрос чата расскажи подробнее про техническую реализацию кеннели в вашем инструментарии cannelly где пламя большая часть логики она сделана на нашем Control plain'е на янвой января собственно кенере это как копия сервиса только с другим именем Ну к названию сервиса мы добавляем с флешка enery мы выставляем определенные теги и наш Control playing янвайп он генерирует конфликт для балансировщика добавляет туда некоторые поля которые говорят что вот эта часть это основной сервис трафик подается туда но при допустим заголовке там Генри труд то часть трафика заворачивается в неё или если разработчики выставляют определенный флаг например что они хотят получить процент трафика то собственно и генерирует конфликт в котором 1 2 3 там сколько надо процентов трафика подается уже в Новый сервис названием спасибо спасибо ребят Кто поднял тему слогами я вот Java разработчик и у меня есть Джесси логин и всякие разные хип-дампы продампы продам фасте дал пишутся и действительно есть в облаках без практики собирать стыда отправлять логи А вот эти факи теряются либо их нужно собирать какими-то кастомными апм тулами можете рассказать как у вас извиняюсь заранее Правильно я понимаю что вопрос про различные дампы сервисы Например если там джавом случилось не так то мы генерится какой-то Лог либо же силок это который всегда генерится по которому можно спалить что не память кончается вот по поводу первой группы вот их можно посмотреть их проанализировать у нас следующий вопрос Спасибо за доклад у меня пару вопросов первая прология Где бы их храните используете лить для этого Ну например если мы говорим про какой-нибудь рекомендательную систему то логи это по сути основная штука которую надо хранить там мы используем эти Отлично так и по диплои у вас там было по кнопочке это конечно все мило автоматически покамитные релизы например это можно реализовать есть опиальная всем У вас есть система Да конечно у нас наверное стоит начать сначала помимо вот этих всех красивых вещей для разработчиков которые глазами который может сходить разработчика там уже есть на текущий момент сиди есть возможности Есть скрипты есть вот еще по диплою Я так понимаю бинарники вы выкатываете как докер Да но у нас все сервисы в Докера Да но если например у нас есть рекомендательные системы Мы периодически там модели обучаем хотим их как-то отдельно чаще обновлять как это устроено можешь перефразировать вопрос не знаю как это в остальном Яндексе это называется динамические ресурсы Когда мы можем как-то выкатывать какие-то данные например индекс для поиска там рекомендательной системы не меняя сам бинарник приложение ну по факту тебя просто получается то есть контейнер есть Спасибо за вопрос и у нас есть следующий два даже Спасибо за доклад Если не ошибаюсь вы сказали что автоматически генерируете дашборды и аллерты расскажите пожалуйста подробнее в карте сервисов можно задавать инструментарий но указывать на каком языке то сервис написан и мы стараемся чтобы все сервисы у нас были несколько похожи например на 81 порту сервис отдают свои метрики собственно имеете данные плюс Имея данные про название сервиса опять же с карты сервисов карта сервиса всем голова вот мы можем с этим названию сходить в метрики балансировщиков и дальше уже по шаблонам генерируем какие метрики у него надо брать известные дальше куда ходить или Джаве там плюс-минус один и тот же функционал и мы теперь считаем что это мы на репа является частью паста как мы поддерживаем развиваем различные автоматизацию удобства в ней и Недавно мы сделали там мониторинги умные они стали сильно проще чем раньше нужно было разработчику достаточно такой приличную спеку написать Или там самому собрать выражение Теперь если разработчики используют например какой-то условный бейзаб на скале то им автоматически генерируется ряд альбертов которые они впоследствии могут использовать вот такой Мы тоже умеем и у нас последний вопрос Привет Спасибо большое за доклад расскажите пожалуйста первый вопрос Шива у вас получается занимается управлением всеми контейнерами правильно там шивы управляет контейнерами через записку нам дата и не только интересно где она сама располагается вообще в кластере или где-то рядышком Шива это тоже сервис Да живет на воде Окей круто то есть получается управляет она сама себя раскатывает правильно Да Окей хорошо еще такой вопрос последний Как происходит вот новых виртуальных машин которые Ну если вам надо увеличить мощность кластер и соответственно это как-то автоматизировалось или руками делаете в тестинге у нас настроены авто скеллинг мы по метрикам аллоцировано использованного использованных ресурсов мы собираем эти метрики и прокидываем Яндекс Клауд в яндекс.клауде мы живем группах у них тоже задаём параметры когда надо расширять и такой степени можно расширять единственное с группой Да а для природа мы автосхельнинг не включаем но у нас есть запас вот если нам надо срочно расшириться то используем стандартный террофор меняем цифру там одну на вторую и тот же появляется на машине через второй формирование инфраструктуру хорошо спасибо большое друзья Спасибо за замечательные рассказ я узнал что хотят живой вам подарочки от организаторов для осенней питерских вечеров самое интересная вещь у нас будет три приза два приза от ребят симпатии от конференции выберите три вопроса кажется мы можем выбрать по одному все вопросы я вот хочу отдать свой приз вам за самый подготовленный вопрос и одна Самая подготовленный ответ сейчас принесут второй вопрос Честно мне понравился вопрос про хранение логов прям вот так прочувствовала и третий предлагаю вам принять решение"
}