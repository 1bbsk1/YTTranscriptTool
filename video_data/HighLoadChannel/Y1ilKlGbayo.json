{
  "video_id": "Y1ilKlGbayo",
  "channel": "HighLoadChannel",
  "title": "LLMops: что есть, кроме ChatGPT, и как ты можешь развернуть это / Ирина Николаева (Raft)",
  "views": 404,
  "duration": 2929,
  "published": "2024-10-29T03:04:26-07:00",
  "text": "попрошу поприветствовать под Аплодисменты Ирина Николаева ра так отлично меня слышно я во-первых вас всех Приветствую здесь сегодня как уже было сказано этот доклад последний поэтому что я могу сказать давайте кайфам наверное хотелось бы начать с небольшой истории о том как около полугода назад мы в компании raft стали смотреть в ai направление более концентрированное что вообще есть на рынке и что из этого можно использовать посмотрели сначала начат gpt какие-то продукты или решения изначально мы делали именно с ним но из-за недостатков чат gpt Мы решили смотреть в Open Source решение опенсорс на рынке сейчас LM моделей достаточно поэтому выбрать было Из чего но у он сорса есть всё-таки недостатки Это большой размер непонятно где их хостить не совсем было понятно сколько это всё будет стоить и как можно это оптимизировать поэтому мы с вами сегодня поговорим немножечко такой будет в начале обзор а затем я расскажу про техники которые мы непосредственно используем и что из этого работает и что из этого вы тоже можете заюзал не помогают с кликером вернусь вернусь давай Конечно конечно А давайте проведём наверное небольшой опросить а Поднимите руки пожалуйста Те кто уже пробовал делать какие-то решения на основе чат gpt Так отлично хорошо Так я прости пожалуйста я тебя немножко рву потому что нас прервался сигнал нас нет в онлайне поэтому я я думал этот момент никогда не настанет на технологической конференции технологическая ошибка мы сейчас буквально 2 минуты подождём восстановим сигнал И потом я к вам вернусь и мы будем продолжать А сейчас я дарю Вам эти 2 минуты тишины чтобы вы написали своему начальнику что конференция огонь Вот и вы не зря сюда приехали вот кликеры я тебе оставляю с ним всё в порядке Я когда можно будет продолжать мокну либо скажу выйду Всё Ребята простите пожалуйста если быстро поднимем никто не узнает что падало Обычно это так не работает но да ладно давайте ко второй части моего вопроса пока нам всё равно чини доступы Поднимите руки тогда пожалуй те кто использовал что-то кроме ча gpt в своих решениях есть ли такие товарищи Боже ВС те же руки теже руки прекрасно прекрасно Ну что пока мы всё ещё находимся не в онлайне давайте наверное Немного поговорим м как вам вообще сегодняшнее выступление Здорово насколько докладов максимально успели сходить я буду сзади смотреть горит это конечно сильно Ладно я если честно Я никогда не мог вос докладов просидеть кроме сегодняшнего дня потому что я ведущий У меня просто Выбора нет четыре доклада Мне кажется это достаточно хорошо Итак вы видите семь тезисов основных которые мы будем сегодня использовать для обсуждения первое Я немножко расскажу о том как же мы вс-таки вра используем расскажу прова основные продукта и немножечко про их архитектуру во втором пункте мы с вами обсудим жизненный цикл моделе в mls Казалось бы здесь причём тут MS спросите вы меня хотя надеюсь что вы помните доклад называется LM Ops это такая новая зарождающиеся отрасль Конечно она очень похожа на старую и в целом входит в MS Но уж очень красивое название это про то как раз как вы можете вернуть хостить ваши большие языковые модели в пункте три мы немножечко быстро пробежимся по основным моделям и их особенностям буквально на секунду я уверена что вы это и так уже знаете четвёртым будет классный вопрос над которым я хочу чтобы вы тоже подумали а именно Всегда ли нам нужно использовать большие языковые модели и может быть всё-таки зады мысль что в каких-то случаях мы можем обойтись без них поговорим в каких имен в пятом пункте поговорим конечно про обзор лицензии и требований к железу скорее даже не столько требований сколько я расскажу про то как на практике какие цифры мы вообще получили и На чём можно разворачиваться пункт шестой мой любимый там про квантизация и Файн тюнинг Файн тюнинг и если прям буквально в одном слове это когда вы хотите уже Трей модельку обучить на вашем каком-то узком домене причём обучить на том что раньше она не так хорошо знала или что не знала вообще например и в пункте се поговорим про векторные базы данных и про L Chain L Chain очень крутая технология очень классный фреймворк половину прототипов в нашей компании мы написали на чейне он вообще хороший и вы можете достаточно быстро написать Бота с большой языковой моде на чейне дня за 2-т и это вообще не не шутка как бы здесь Поднимите руки кто уже использовал чей Ну в целом можно было предугадать наверное да А ну что давайте начнём первый продукт про который я хочу рассказать - это Audio Исай Если вкратце то есть и другие подобные решения которые решают ту же проблему а именно проблему Какую мы хотим грузить в нашего Бота например MP3 записи это часто может быть Полезно если у Вас бизнес который имеет какую-то линию поддержки голосовую и вы хотите знать сколько из ваших А сколько из ваших коллег из вот отдела прослушивания или поддержки насколько Они вообще хорошо работают посмотреть kpi посмотреть какие-то другие метрики и часто в таком бизнесе есть критерии по которым оценивается звонок условно если мы например возьмём дилерский центр то мы хотим знать пожелал ли например наш сотрудник Спокойной ночи Если звонок очень поздно или мы хотим узнать пожелал ли наш сотрудник посоветовал ли он прийти на тест-драйв той или иной модели и автомобиль соответственно не модель машинного обучение так вот и таких критериев у нас может быть очень много как и звонков запи То есть я немножко не расскажу вам Сколько в среднем звонков совершается в день но тем не менее вы хотите их анализировать и прослушивать и просматривать это всё глазками человеческими стоит достаточно дорого Да И долго в целом и здесь мы берём MP3 запись грузим её в spe to текст модельку то есть мы получаем в первом шаге на выходе текст транскрибацию этого самого звонка затем мы вместе с протом то есть с критериями критерии прописываем в нте и закидываем это ВС уже в мку и часто критерии у нас бинарные то есть сказал Не сказал сделал не сделал позвал не позвал и это помогает нам дальше получить достаточно понятные метрики которые мы можем уже дальше хоть Сколь угодно как угодно представлять и крутить так вот первое наше решение было основано на видеть снизу и в дальнейшем мы поняли вот какую историю мы использовали опиш Chat gpt Достаточно долго и в какой-то момент мы задумались может быть нам стоит перейти на Open Source стали смотреть в эту тему и выяснили интересную штуку что вам как бизнесу точно нужен Open Source моделька если у вас чувствительные данные и вы не хотите чтобы они уходили в чат gpt это в целом очевидная штука менее очевидная штука - это если нагрузка раст есть чем больше у вас нагрузки идёт на модельку на apic модели тем больше выгоды вы получите от Open Source Наго решения типа той же ламы второй например так про Audio insights Мы вроде как поговорили Вроде я всё рассказала что хотела Давайте дальше пойдём второй продукт достаточно интересный а мне он очень нравится ког Дато сайентист он скорее про то как обезопасить ваш бизнес от пользователя Казалось бы Зачем обезопасить ва ваш бизнес от пользователе но в случае с модельками это бывает Нужно Я не знаю знаете ли вы новость есть даже отдельная когорта людей которые взламывают различные нейроном чтобы нейронка А в частности большая языковая модель отдавала вам какие-то секретные данные которые в неё заложил бизнес или её разработчики При этом они не хотели чтобы эти данные вовсе попадали в ваши руки но они попали либо что из интересного часто Если вы берёте большую языковую модель из коробки ОРС Ну имеется в виду например вторую ламу или любую другую Мы проверяли на второй ламе и просите её рассказать вам про кредитный скоринг скажем и она действительно вам что-то рассказывает про кредитный скоринг и дальше делает необратимые вещи она вам закидывает ссылочку которая была Видимо использована при обучающем датасете о котором мы к сожалению или к счастью ничего не знаем мы ничего не знаем про тот объём данных и скорее всего те данные которые закидывали в эту сеть прямо с большой точностью поэтому здесь есть вероятность что нейронка отдаст эти данные пользователю вообще с лёгкостью Я не знаю Кстати как когда только выходил чат gpt и подобные нейроном она и сейчас в целом есть и она как раз отдавала всякие немножечко такие а биологически химические непотребства которые и послужили закрытием этой модельки на какое-то время Поэтому этот продукт он отвечает Как раз за то чтобы лишние ссылки Не проникали чтобы модель не показывала нашим пользователям какие-то персональные данные о других пользователях такое тоже может быть и вообще в целом чтобы не показывало какие-то данные такие чувствительные которые нам важны Давайте пойдём дальше дальше у нас жизненный цикл модели в mls когда я придумывал этот доклад у меня была такая прекрасная Мысль о том с какой же точки зрения излагать то вообще это всё то есть нельзя же просто прийти и сказать вот я вам сейчас расскажу про всё что мы попробовали что из этого Получилось Хотя в целом так тоже бывает и я решила выбрать парадигму именно MS штука не новая кстати Достаточно давно в дата сансе мы её используем и есть такой конвейер жизни модели о том что надеюсь что вы не думаете что мы можем один разок обучить модель дальше использовать её всю нашу жизнь и никогда не быть онлайн такого К сожалению наверное не получится Скорее всего Потому что рано или поздно нам придётся придётся заливать в неё новые данные снова её переобуть или добу чтобы она подхваты новые тренды и чем как бы более такие узкоспециализированные тренды тем чаще нам придётся проделывать весь этот конвейер собственно здесь Наверное останавливаться супер сильно я не буду скажу лишь только одну интересную штуку которая нам в дальнейшем поможет а вы видите что последний шаг - это inference Модели inference Модели - это когда вы хотите вот вы её до обучили и всё теперь вам обучать её не нужно У вас есть архитектура модели файл Весов и и вы можете её использовать где угодно Вы можете её скинуть своему другу и он её где-то заходит Вы можете показать её вашему шефу он оценит и тоже возможно где-то захват эту модель А так вот Почему здесь так много инфин сов Хотя главный пайплайн у нас один потому что плюс в том что инфин Сить мы можем сразу на разных устройствах чем например мы не можем похвастаться на предыдущих шагах Так ну вот мой собственно любимый вопрос Всегда ли нужны у нас с вами мки есть есть сейчас небольшой хайпят такая тип А давайте использовать CH gpt Давайте использовать все большие языковые модели для абсолютно всех задач А что если попробовать заменить часть задач нашего льо отдела тем что мы просто будем закидывать данные в чат gpt Почему бы и нет если она такая крутая как все говорят Может быть это сработает Но на самом деле хочется здесь сказать про то что если у вас модель закрывает одну или две задачи точат gpt и другая м вам не нужна они действительно очень грузе даже если вы квантуется вам не нужно поднимать часто всю эту большую архитектуру все эти веса для того чтобы решить пару задач условно до смешного доходит если у вас задача классификации то я бы наверное не советовала всё-таки закидывать данные в чат gpt а сделать такую стандартную классическую Да Old But Gold модель а классификатора собственноручно её натренировать Но зато вы будете Точно А уметь работать с выходами этой модели вы сможете каким-либо образом её видоизменять и если у вас вдруг например отдел рисков то там стопроцентно не нужно использовать LM потому что LM и CH gpt в частности дают вам каждый раз разные ответы Да они лежат все рядом Да они все означают одно но если вам нужна воспроизводимость результата то использовать LM сейчас не видится логичным вообще лучше использовать вместо этого старые добрые модели которые ещё никто не отменял Ведь они для этого и были созданы встаёт другой вопрос Когда же нам тогда использовать мки они безусловно хороши в задачах генерации саммари зации текста анализов текста плюс если мы хотим быстренько Зафари то есть чем они хороши тем что их уже обучили э за нас и до нас На огромном количестве данных Поэтому если мы хотим сейчас на каком-то небольшом домене добу то мы вполне быстро можем это сделать какой-то быстро получить хороший результат А что ещё это общение с пользователями всё что касается общения с пользователями а именно это могут быть чат-боты э различные всякие прочие боты Если вы хотите например блок завести в котором текст будет генерироваться вам точно стоит использовать ЛМ потому что благодаря хюман фидбек который был э использован при их обучении они действительно такие классные и могут генерировать текст очень близко с человеком плюс можете дополнительные всякие настройки накручивать типа там хочу деловую речь а нет хочу чтобы ты смайлики ставила в конце каждого предложения здесь можно как угодно изворачиваться быстренько буквально посмотрим на этот слайд я уверена вы с этим сталкивались я сегодня уже об этом говорила все модели делятся на открытые и закрытые из открытых моделей те что мы часто используем это Лама Мистраль остальные две Поменьше проприетарные есть классные от антро недавно кстати завершился грок тоже от икса вот в целом Я думаю не нужно здесь говорить о разнице между проприетарный лучше пойдём дальше после того как вы решили какую же всё-таки модельку вы хотите использовать открытую или закрытую вам нужно посмотреть насколько модель же всё-таки хороша в вашей задаче чем-то же они всё-таки отличаются и всё верно модели различаются датасета на которых они обучались и методиками обучения поэтому что Вы должны сделать это пойти в интернет лучше на проверенные сайты и рейтинг например и посмотреть какая модель хороша в какой задаче и какой у них вообще Рей есть вот эта прекрасная Круговая диаграмма которая Я люблю здесь конечно нужно условиться что gpt 4 представлена не представлена например Лама 70b но представлена 13b она у нас последняя поэтому немножко Странно что четвёртая gpt сравнивается с тринадцатой ламой Но об этом мы поговорим чуть подальше про ламу и про е параметры кстати говоря про бенчмарки не все бенчмарки всегда можно воспроизвести в этом К сожалению отдельная заключается боль поэтому слепо полагаться только на бенчмарк Я бы здесь не советовала часто хороший пример - это выделить несколько эллэ МОК три например или четыре и проверяться уже с ними конкретно для ваших задач потому что на конкретной вашей задаче скорее всего никто ещё не проверял Итак перед тем как вам хостить вашу модель появилась такая интересная штука не работает больше просто скопи пасти код и скачать файлы ренто вам ещё нужно знать про лицензии м моделей Если вы конечно хотите использовать для личного пользования то наверное можно почти всё Если вы хотите всё-таки для бизнеса использовать то есть разное лицензирование которое говорит вам о том как можно использовать ту или иную модель что с ней можно делать мы с вами все представим сейчас что в первом в первой строке третьего столбца У нас тоже есть плюсик где M лицензия собственно mit такая хорошая Она позволяет Да свободное использование вам не нужно за неё платить Но единственное что вам нужно сделать это рассказать кто автор этой модели скопировать лицензию И вам вам получается нельзя нет вам можно использовать зна и логотип этой модели у рейла почти тоже самое но вам даже не нужно аннотацию автора включать у apch 2 в чём о на Суть в том что мы не можем использовать знаки и логотипы у рейла по-моему ещё такая история что в целом всё очень открыто но при этом мы не можем использовать модель если дело касается каких-то обходов законов или каких-то плохих дел именно почему-то с несовершеннолетними То есть у рейла Вот такая лицензия которая всё уже предусмотрела за нас Ну и проприетарная - это собственно самая закрытая лицензия по типу чата gpt Когда вам чаще всего нужно заплатить за то чтобы использовать модель и там бывает тоже достаточно а разные дополнительные условия которые вам нужно сделать То есть не просто заплатить а ещё что-то возможно и указать авторов возможно и нельзя логотип использовать Но это всё очень специфично для каждой модельки отлично теперь переходим к железу для ламы второй быстренько пробежимся у ламы второй есть три версии это 7 бит 13 и самая большая версия 70b циферка которая стоит рядом с буквой B отвечает нам на то насколько большая модель и насколько большом датасете она была обучена И насколько много она знает соответственно Таким образом семидесятом получается знает больше всех супер Отлично Тринадцатая Лама знает поменьше 7 7b знает Совсем немножко и Казалось бы чем больше модель тем больше ресурсов нужно для того чтобы её захо обычно мы используем карточки NVIDIA 100 без разницы скорее всего часто 40 ГБ это или 80 Да конечно 80 памяти побольше но на 40 тоже можно работать вот и таким образом вы можете увидеть количество часов сколько вообще эти модели обучались и примерно прикинуть а стоимость снизу есть для обучения каждой из этих моделей это если вдруг вам когда-нибудь придёт в голову мысль обучать мки From Scratch так сказать чтобы выкидывали сколько нужно так вот сразу скажу здесь интересную особенность у всех L у кого параметры меньше 10b а это именно Седьмая Лама есть ещё седьмой местраль есть такая проверенная информация на практике что у них не очень хорошо работает Human edb который как раз являлся их такой визитной карточкой поэтому 7b использовать на самом деле не советую хотя бы нужно начинать с 7 прв хорошо Вот Но для того чтобы за Инфинити семидесяти нам например не хватило четырёх карт а 100 по 40 ГБ потому что очень много памяти почти 80% уходит на то чтобы просто загрузить веса в которых и градиенты и другие артефакты на которых моделька обучалась Поэтому если мы хотим в неё грузить контекст больше 2.000 она просто выходит за пределы памяти можете посмотреть цену на NVIDIA 100 и поул немного тому как дорого сейчас стоит видеокарта или погрустить но советую всё-таки поул так вот по деньгам примерно так немножечко мы прикинули поэтому это выходит чаще всего дороговато Даже если мы хотим поставить свой сервер с четырьмя сотами ну Ну дороговато выходит да А если мы хотим Файн тюнить Да обучать модели то прямо на постоянной основе стоит всё-таки наверное задуматься о покупке своего сервера Хотя если мы не постоянно это делаем то можно и в облаках разворачиваться есть решение от henf есть у реплике тоже сервисы поэтому решение присутствуют русскоязычные тоже но вот мы посмотрели на всё это чудо ро чудесное поняли что большие языковые модели реально огромные чтобы хостить их нужно слишком много Как мы можем здесь ухи или оптимизировать поговорим про квантизация получается я вам напомню что в чём у нас с вами хранятся веса и в целом на чём скажем так базируется работа всех нейронокс ски либо перемножение сложение матриц Ну со сложением ещё нормально с перемножения иногда бывают вопросики так вот чаще мы используем именно тридцати двух битный тип Т 32 и он такой у нас можете сами видеть прекрасный огромный интервал в 4 млрд чисел и это всё значение тридцати двух битного тензора и мы такие а зачем нам так много может быть мы Во мы сделаем и действительно такой Стандартный вариант квантизация - это квантовая в 8 то есть мы берём и как бы сжимаем весь наш лот 32 в in8 в чём вопрос-то вопрос-то в том что вместо четыр млрд мы получаем то что мы можем использовать только 256 различных значений и понятно что стоит делать маппинг но какой э равномерно как-то их брать выглядит странновато может быть есть какая-то подсказка и она действительно есть если мы посмотрим с вами на веса моделей то часто они находятся именно вот их основной скоуп находится в промежутки от минус одного до одного из интересного веса модели имеют нормальное распределение это прям как-то не знаю прикольно и мы берём и чуть больше уровней в этом манге выделяем для вот вот Максимум для вот этого костяка который находится от минус единиц до единицы а хвосты левый и правый У нашего нормального распределения мы просто схлопывается иметь несколько чисел из рити двух битного отобразится в одно восьмибитное Вот такая история Да здесь можно как раз на Катино про кванза изначально самое Верхнее - это наше распределение весов чуть пониже у нас получается логарифмическая квантизация Как видите у нас по краям достаточно мало значения Кванта нулась и по центру чуть побольше в целом они все так и Ну вот соседняя по крайней мере сохраняет своё распределение пойм пожалуй дальше это была такая ту про кванти про е практическое применение чуть попозже буквально после этого слайда по-моему Вот мы с вами Кванта нули модель на Ирен её запустили и вот вроде всё прекрасно есть даже библиотечка для квантование по-моему за это отвечает B И чего мы хотим ещё смотреть хотим как-то то есть дооб и для дония Яя State of the Art это ФТ эффективный по параметрам тюнин что вообще такое ФТ это некоторая библиотечка некоторое сборище Я бы даже наверное назвала методов для того чтобы Зафари Как вы видите множество достаточно огромное И если вдруг вы успели заметить то вот снизу в таком персиковом кружке есть Лора скорее всего если вы сталкивались с то вы работали именно с лорой и заметьте как удивительно что кроме Лоры есть ещё куча других техник которые позволяют нам с вами эффективно Файн тюнить эффективно до обучаться Сегодня мы рассмотрим только лору и лору Я надеюсь что в следующий раз я расскажу поподробнее что у нас вообще есть Какая информация с другими методами насколько она проверенная и Рабочая про лору р у нас с вами расшифровывается как Low R adaptation я Напоминаю Вам Мы хотим тюнить нашу модельку Это значит что мы хотим её обучить ном каким-то данным в неё закинуть и Казалось бы мы должны использовать всю Вот эту вот архитектуру все веса все слои чтобы сделать это Но на самом деле нет мы можем добавлять довать информацию к уже существующей а именно вот вхо закидываем X W - это Матрица наших изначальных весов Ну вот такая абстракция представим и получается что мы должны ведь на каждый слой докинути получаются выгодно и оптимально если у нас Мы можем сложить две матрицы Если только у них одинаковая размерность то есть Нам нужно чтобы W было той же самой размерности но прикол большой здесь в том что матрицу W мы можем представить в виде перемножения двух других матриц с низким рангом если Вам станет очень интересно или скучно Вы можете посмотреть про сингулярное разложение матриц из линейной алгебры и удостовериться что это действительно так то есть ранг я вам напоминаю ранг Матрица - это максимальное число непересекающихся строк или столбцов то есть такая суть основная суть нашей большой матрицы Если вы знаете то Матрица весов она достаточно разреженная то есть там может быть много строк и столбцов Но вот действительно какой-то такой главной информации может быть мало И все И большинство из этих элементов могут быть заполнены числами около нуля поэтому мы здесь берём делаем трюк и представляем нашу матрицу в виде двух матриц но поменьше а информацию при этом теряем не сильно И вот эти две наши матрицы поменьше нам уже удобно их и перемножать и другие операции над ними производить тоже и делать сврт если вдруг нам захочется её делать отлично Давайте теперь посмотрим на лору это в целом вообще почти тоже самое Единственное только мы добавили квантизация мы с вами говорили вот из fp32 в int8 можно кстати ещё квантова в fp16 Я бы советовал вам квантова в fp16 потому что это немножко такой золотой стандарт Вы немного теряете информации при этом моделька уже получает ускорение но в Лори поступили э немножечко более рискованно сделали двойную квантизация с помощью которой сейчас мы можем аж квантова в in 4 это небывалая роскошь которая позволяет в том числе нам запускать LM на Маке на нашем компьютере CPU Вот только время ответа и память которую МКА подхватывает контекст во время вашего диалога на практике пока очень сильно падает поэтому я бы наверное не советовал здесь квантова в Н4 А начать с каких-то более больших размерностей что ж э вот вы определились с квантизация Кванта нули уже всё уже всё заан И теперь хотите узнать как как в итоге хостить Есть ли какие-то фреймворки может быть чтобы удобно обращаться к мке может быть есть какие-то базы данных специаль для того чтобы поработать с ней есть это векторная база данных Как вы наверное уже знаете Все сейчас почти знают про векторное представление слов или картинок или вообще всего чего угодно Мы из текста картинки аудио делаем векто размерность зависит вашей задачи вас и собственно вашей нейронокс переводит в вектор дальше Мы представляем что вот в этом пространстве многомерном Ну вот сделали Мы Вектор 128 размеров размером они у нас все где-то лежат вот в этом 12 двухмерном пространстве и как-то друг с другом соотносятся и вы хотите знать тако насколько близко каждый Вектор лежит вот как бы нам его вать так чтобы дополнительную информацию из этого вытягивать для этого есть собственно дополнительные методы достаточно классические Давайте на них сегодня Останавливаться не будем можно посмотреть про снижение размерности и pca это если нас не устраивает наша размерность вот этой В итоге получившейся базы но главное что мы должны тут уяснить что есть Вектор они представляются в пространстве и слова которые похожи друг на друга по смыслу будут у нас лежать рядом а слова которые не похожи друг на друга вообще далековато будут друг от друга лежать давайте так скажем И в чём ещё здесь большое преимущество в том что если вы складывается в базу данных вектора то после того как не россет или обвязка над ней идёт в базу данных она и Запрос к ней тоже вектори зуе То есть если вы пойдёте уча gpt Спросите подадите е контекст и ответ она вам преобразует и контекст и получается ваш запрос к ней преобразует вектор и пойдёт в базе данных искать похожие ответы это достаточно оптимально это достаточно удобно плюс таким образом вы можете ещё историю чата подгружать что немножечко уменьшает ционис мки про гоци лмо тоже интереснейшая тема из такого что мы уже обсудили н тоже решает проблему галлюцинации тюнг векторной базы данных Осталось совсем немножко времени L Chain А это фреймворк который уже содержит в себе по факту всё что вам нужно там есть модельки Вы можете выбрать Там же есть темплейты для промто индексация по документам э устанавливаете себе Long Chain заходите выбираете модельку пишете телей прота грузите в него например вашу корпоративную ки или справо пон и погнали собственно разворачивай на стрем каком-нибудь фронт и можете Вполне себе развлекаться с этим есть также память и чены тоже классная штука из которого следует что вы можете как угодно настраивать внутри чейна взаимодействия компонентов друг Между другом так вот это на вами последний срм вки ному слайди и здесь что же Ново с приходом ЛМ то есть вопросики вызывают на самом деле инфе понятно что схема и так достаточно общая но тем не менее изменения у нас в инфе мы можем теперь Ирен по-разному можем инфин на нескольких задачах о Простите на нескольких видеокартах в тестировании у нас теперь тоже достаточно много вопросов потому что в тестировани лмо нам нужно здесь ещё закладывать мты и наверное немного мной недооценённый промт инжениринг здесь включается в полную силу Поэтому если вдруг вы задумается проверять какая лэм лучше для вашей задачи обязательно на Первом шаги подумаете про методологию проверки А именно как вы будете пытаться сравнивать ответы которые от неё приходят иначе можно очень много на это потратить времени сравнивать их глазками ручками и и всё равно ничего не понять А в обучении модели тоже есть изменения вместо обучения прямо м Скретч сейчас больше значения наверное даётся всё-таки Файн тюнингу за счёт того что быстро и не так много нужно данных Если вы будете с нуля обучать Ну и побыстрее это всё происходит этап подготовки Я думаю остаётся примерно таким же он и так достаточно общий Ну что у меня Всё я вас поздравляю вы выдержали Теперь ещё буквально немного времени на ваши вопросы Я накинула для вас буквально три вопроса если вам будет не о чем меня спросить ина Спасибо большое ребята Аплодисменты очень классный доклад можете пользоваться вопросами которые указаны там можете задавать свои я напомню что у нас за лучший вопрос Мы Дам подарок ите У кого есть вопросы начнём с центра Спасибо большое Очень интересно мой вопрос очень похож наверное на третий вопрос Вот что делать А если векторные базы не помогают уменьшить э условно говоря размер а мы хотим в промт запихать настолько много информации А И при этом уже воспользовались векторной базой данных для того чтобы не знаю там улучшить точность там или какую-нибудь фактологической историю улучшить вот всё равно этого контекста не хватает вот есть ли какие-то способы для борьбы с этим здесь Наверное хотелось бы следующее посоветовать А какая сетка у вас GP вы не можете сказать какой Ще не не могу сказать гига чат Ага а ну смотрите здесь интересная есть штука я изначально Хотела Вам ответить про разные техники промтис у вас всё не влезает в один большой промт возможно вам стоит разбить их на шт прон на несколько промто которые вы последовательно в неё передаёте здесь минус только в том что сейчас все статьи которые есть по промтис по факту проверены на Chat gpt и например мы тестировались на ламе и у нас было одно и то же решение Все прочие равные условия мы меняли только gpt на ламу вторую семидесятом то уже до говорит и промт оставался тем же и вот промт который мы писали поча gpt он не работал вообще с ламой второй Мы убили где-то неделю фул тайма на то чтобы найти оптимальный пром для ламы то есть здесь вам возможно нужно и смотреть сторону разбивки пром Если вы передаёте там какие-то параметры возможно их как-то немного пожать и переименовать в базе то есть часто бывает так что клиент передаёт колонки или какие-то названия которые у него есть например часто встречаются вин номера или там история вин и ча gpt не видит этого потому что там для него история Вина - это абхазские лыхны какие-нибудь и вы должны вот этот момент тоже прямо отследить работа с промто там есть что посмотреть Спасибо большое спасибо за вопрос можно пока передать сюда а я напомню что если у вас нет вопросов то вы можете пока проголосовать за доклад и оценить его и это очень важно для для нас и для конференции Спасибо Да пожалуйста если вы ещё не голосовали сегодня то я бы попросила вас проголосовать за мой доклад Простите что вас перебило продолжайте Здравствуйте Лукьянов Антон наверно две компании вкус автомакон а у меня вопрос как раз-таки для последней конференции этого дня в начале своего выступления вы сказали что ваш любимый вопрос - это к сейчас квантизация и Файн тюнинг а потом в середине выступления вы сказали что ваш любимый вопрос Всегда ли нужны ЛМ внимание вопрос Какой всё-таки у вас любимый вопрос и почему вы решили В вести заблуждение всю аудиторию Я кстати тоже подумала об этом я вот когда смотрела и увидела что квантизация Файн тюнинг Я сказала что это мой вопрос оказался что не он но это наверное как вы двум с твоим любимым детям в разное время говорите кто из них ваш любимый или когда родители просят вас сказать Кто из него любимый если это Мама вы говорит что это она если папа говорит что это он Спасибо за вопрос Спасибо за вопрос я могу точно сказать что это мой самый любимый доклад Сегодня я вот весь день веду этот самый любимый Да надеюсь других спикеров нету здесь у нас есть первый ряд вопрос Добрый день Ирина спасибо большое за вам за интересный доклад А вот не могли бы мне подсказать следующей ве вот тут интересно вопрос векторным базам данных то есть предположим если понимаю что векторная база данных это в принципе уже некие бинарные данные да которые можем туда сохранить Можно ли вот у нас есть как проект мы сейчас там Дума обдумываю например Когда у нас много источников например ну из интернета вещей Да у нас много сенсоров очень много там причём Разно качественных там температура влажность не знаю там вот каких-то там достаточно большого объёма если мы можем каким-то образом вектори звать эти данные и сохранить в ламе Можем ли мы прота находить различные аномалии находить какие-то связки между вот этими векторными вот потоками временных рядов которые идут от сенсоров и попытаться найти какие-то закономерности именно используя мты используя ламы вот для таких вот какой-то аналитики Здесь наверное вопрос такой хотелось бы на две подзадачи разбить вот смотрите Вы записываете в ламу Вот вы записали Виктор которые представляет условно каждый какой-то объект А дальше Вы хотите эти исследовать и вот что-то в итоге Есть ли какая-то гипотеза которую вы проверяете Спасибо большое А например вот связка Ну возьмём здание какое-нибудь большое да и где-то у нас есть Температура там не знаю внешняя температура погоды Да есть температура внутри помещения есть помещения причём в разных помещениях разные температуры и Например мне нужно понять А как это зависит от например от стороны света а в помещени То есть например северная сторона по идее должна быть более холодная Да там или влажность как-то Это зависит вот у меня есть куча Вот таких сенсоров различных типов Да где идут временные ряды и я могу их просто сложить между собой и например проверить туда добавка Сколько идёт обогрев Да я могу понять что Ага вот если здесь в какой-то комнате пошёл резкий обогрев но при этом там падение там температуры то это значит там скорее всего какая-то авария или там где-то грубо говоря форточка открыта да то есть вот какие-то вот такие аномалии либо связка Что например Чем больше мы там тратим ресурс тем соответственно больше там отопления Но как это зависит между разными помещениями то есть вот какие-то такие сложные запросы промто Да я так понимаю что Лама она вот понимает себя эти все временные ряды у неё в принципе должны быть из-за того что она сжимает эти хорошо данные сама себе вот всякие текстовые Да вот то вот и здесь вот эти примеры должны тоже мне кажется Хорошо отработать на этих временных рядах Спасибо я бы наверно такой вопрос Вы же наверное уже что-то пробовали в исследовании То есть если вы классические принципы и подходы которые нам показывают корреляцию между фактами у нас куча на самом деле инструментов для этого я бы на самом деле во временных рядах советовала бы использовать именно их другой вопрос если они не работают Вопрос хороший нужно смотреть Конкретно где использовать для этого ламу Честно говоря я не думаю что она нам расскажет много информации из этого ведь если мы берм один объект бы на самом деле с вами после доклада ещё обсудила но если мы берём один объект его кладём в базу он у нас уже в виде вектора то мы можем доставать в основном их близость То есть их общность то есть насколько У нас вот этот объект близок по именно мнению нашего а нашей сетки которая делает вектора насколько близко Они лежат то есть такую информацию можем Но как будто то что вы говорите Вы хотите как будто признаки между собой исследовать вот А вот если признаки то лучше их в отдельности рассматривать Я очень извиняюсь у нас просто есть ещё несколько желающих Я хочу дать возможность задать ещё один вопрос это будет последний вопрос и потом мы в дискуссионной зоне сможем продолжить спикер никуда не денется Я надеюсь Да Ты же никуда не денешься Ну я надеюсь если только не очень много людей будет иначе очень много я видел в руки всё будет хорошо ладно пожалуйста Ирин Добрый день Спасибо большое за доклад очень интересно глух Евгений вот у меня такой вопрос интересно узнать ваше мнение То есть я понимаю что когда я там хочу по генерировать у меня там много текстов скорее всего там больше там двух задач которые я хочу решить с помощью там меля я могу использовать например мку и я работаю с чувствительными данными Да соответственно Мне нужно какое-то там решение допустим Лама интересно узнать ваше мнение с точки зрения информационной безопасности когда я понимаю что у меня в рамках одной компании есть разделение то есть есть какой-то набор общедоступных данных Да есть какой-то набор там допустим данных составляющих там ну данных для служебного пользования которые также сотрудники внутри этой компании не все могут там видеть при этом Да как бы когда мы говорим про мку мы Ну так или иначе говорим про некоторый такой Black в который мы там пием очень много данных и вот как работать с этим разделением То есть как контролировать выход здесь если у вас такой достаточно жёсткое разделение и мы хотим точно знать что данные из одного отдела Не узнают сотрудники другого отдела то мой совет - это делать Это в обвязке ламы всё-таки ручками то есть какие ещё были варианты условно когда мы фант там на каких-то данных мы можем сделать допустим дополнительный столбец Где помечать бы отдел или какой-то уровень секретности данных но не хочется Мне наверное здесь отдавать это на откуп самой ламы если мы можем сделать это в обвязке то есть фильтровать уже данные её в зависимости от того в каком отделе мы просматриваем Спасибо и пришло время выбрать лучший вопрос мне кажется лучший вопрос это вот безусловно задали про то как у меня так две любимые я я понял я понял уровень ирони у нас сегодня но лучший вопрос так лучший вопрос Э спасибо спасибо вам за вопрос и конечно у нас есть подарок от конференции тебе спасибо большое за за то что приехала выступила Это было очень классно ребята Аплодисменты Ну а"
}