{
  "video_id": "X8XYSAqWQqY",
  "channel": "HighLoadChannel",
  "title": "Как повысить уровень доступности сервиса и подружить Сеть с ИТ / Евгений Фоменко",
  "views": 442,
  "duration": 2371,
  "published": "2023-01-19T05:55:16-08:00",
  "text": "приветствуем друзья всем еще раз прошедшим днем связи а сегодня речь также у нас пойдет про связь про связь сложных распределенных приложениях который очень часто доставляют проблем и айтишником ну и стедикам не менее того на самом деле сейчас представлюсь первых я руковожу эксплуатации но вы из операторов большой четверки уже более пяти лет и на самом деле те ну паттерн и те вещи которые сегодня расскажу это по сути набор рецептов такие более эксплуатации которые у нас периодически возникали и что из себя настоящее время вообще представляет телекома оператора современный бизнес темы телефон императора это достаточно сложные распределенные приложения которое находится хочется больше чем на нескольких тысячах серверов распределены по судам по стране объединены обидно но это все сложную вычислительную сеть ну и сейчас конечно же куда без микро сервисов сверху еще стоят микро сервисы которые множество различных услуг предоставляет как внутренним заказчика так и внешним потребителям нашим абонентам и одна из более которые часто сталкивается айти специалист это сетевой проблемы на самом деле часто очень владельцу сервисов кажется что во всем виновата сеть единство структура или еще кто то кто то но только не приложение и такой небольшой спойлер на самом деле 80 90 процентов случаев результате расследование инцидента фаворит сетевых окажется совсем наоборот что все-таки приложение очень даже причем и нужно искать где-то внутри по и как же разобраться с той ситуации когда у нас есть очень сложная сетевая проблема которая проявляется раз в какое-то время которое сложно диагностируется и при этом стиви ти только пожимая плечами говорят но ребята смотрите где-то в стороне по мы тут вообще как бы не при делах так вот то тот подход это приложение которое было реализовано это результат труда нескольких 3 инженеров нашей компании которые столкнулись с подобной болью и решили что-то сделать сделать автоматизацию сделать софт который позволит позволит выявлять сетевые сбой в рамках распределенной приложений которые на находится на десятках на сотнях ростов и распределены еще под сводом на самом деле основная сложность которая сталкивается владелец сервиса когда видит что его приложение начинает тупить или где-то тормозить есть грешок в сторону сети это очень сложная диагностика потому что вам нужно момент сбоя поймать тот самый момент когда воссоединение проходит медленно либо когда пакеты с листовой не все пропадает либо еще хуже того просто происходит на ментальный разрыв между всех связей и последующие соединения время реакции плюс время на брошен на обнаружение проблемы в это время привык проблемы уже уходит что делать в этой ситуации так же бывает ситуация с повторяемость если будет происходит на крупных сумм элементе он может себе проявляться по-разному как правило то проблем никуда у него входит она просто какое-то время скажем так маскируется потом проявляется по новой влияние таких крупных сборов может быть вполне разрушительным то есть у вас одновременно отваливается все соединения и при этом приложении чувствовать себя совершенно по разному кто то уме с этим справляться кто то нет но тем не менее совершенно нам не позволяет не добавляет как бы простоты в его диагностики и когда подключается ребята из сетевого сектора а технику порой бывает очень сложно объяснить а что же на самом деле произошло где а где у вас тут маршрутизатор который дал сбой где тот сетевой элементы трафик воруется как правило это две совершенно параллельной реальности и сетевая сетевой мир мир ikea не месяца вот скажем так слабо пересекаются ребята разговаривают на разном на разном языке тот момент когда все происходит бывает так что владелец сервиса психует и горят буду писать king валку честно напишу программку которая будет пинговать соседней хвост поставлю ее в какой недугах и пусть она засылает письма прямо в голову там руководителю транспортного цеха сетевой службы пусть он разбирается если письмами когда у меня пинг не проходит на самом деле вот чтобы ответить на это вопрос кто виноват во время сбоя сеть или эти ключевую ключевая мысль заключалась в том что на это вопрос отвечать мне нужны потому что владелец приложения должен в первую очередь посмотреть на проблему стороны и вместе с ребятами из сети сесть за один стол переговоров на того чтобы сблизить скажем так свои позиции понять что же нужно данный момент времени сотруднику сетевиков для того чтобы разобраться в проблемах что мы так и сделали по сути была очень сложная и серьезная сетевая авария которая длилась можно сказать несколько дней для того чтобы ней разобраться нужно было просто выровнять наши позиции вместе разобраться в этом посмотреть что же нужно нужно не так много что нужно нужно во-первых момент он уже ли проблема уметь снимать сетевое trace того чтобы показать специалисту сети на каком хоппе какой именно узел тормозит нужно иметь карту связанности сервисов чтобы ловить аномалии на основе тех коннектов которые положения данный момент имеет то есть по сути собирать тесте вы связи которые есть их как-то агрегировать и потом следить за ними приложение должно быть недорогим эксплуатацию понятном должен и как и на unix стакана windows server охотиться и по сути уметь работать такой распределенной среде и по сути когда мы сели за общество переговоров выписали все эти требования стал все стало достаточно предельно ясно и прозрачно ребят из снг отдела родился вполне понятный план что он необходимо написать приложение которое будет удовлетворять всем требованиям то что мы проговорили с ребятами сетевиками так появился проект and move но клапана распределенная система которая ставится по хостам приложение и умеет собирать информацию о тех сетевых связи которые данный момент активны мало того это распыленные приложение но умеет смотреть в каждый момент времени проверять успешно соединения и в момент когда что-то происходит когда происходит какой-то определенный сбой снимайте у диагностику снимай все то что нужно дальнейшем для разбора и интегрируется с различными каналами модификации может отправлять информацию шлак в telegram зависимость это то что чем пользуется эксплуатацию для нас это канал telegram агрегировать alert и в случае сетевых сборов и присылать четкую подробную информацию на самом деле если разобраться очень подробные то вся конструкция она построена на основе стек are open source чего-то сложного здесь на самом деле нет приложение это поставляется вместе с zabbix агентом то есть мы написали небольшую программку нагул который умеет выполнять совершенно простые вещи как trace pink ну и через нейстат собирать информацию об открытых соединениях некая серверная часть которая управляет агентами и собирает карту сети ну и по сути может сказать выбирать себе информацию о том что происходит с вашим приложением и отправляет это все в графу базу данных использовали в качестве карты сети не f и j этого графа база данных который достаточно хорошо работает сотнями и тысячами вершин связи может легко достаточно проводить различные алгоритм обхода и по сути моделировать работу приложения встроить тусить на основе которой дальше уже идет мониторинг и базам и базаров к которой хранятся метрики база которое использовать для мониторинга но у нас-то была виктория metrics которое по сути ну имеет достаточно большое количество связи большое количество alert of в минуту и фиксирует всю ту историчность которую дальнейшем может быть полезным того чтобы разобрать какой-то происходящей сетевой инцидент или посмотреть как всегда приложение как ваш сеть вилла в предыдущем как она сейчас себя чувствует и ряд каналов на дефекацию то есть первую очередь это вывод определенных дашбордов графа ну и канал и alert manager это предложение которое агрегирует событие может быть про интегрирована с корпоративной стены мониторинга либо с каналами нотификации такой получился достаточно не замысловатая конструкция приложение и была разработка это сделал нас фактически двумя инженерами которые столкнулись с этой болью посидели посмотрели каким инструментария мини обладает и решили реализовать такой механизм но основная прелесть всей этой структуры заключается в том что приложение достаточно простое получилось в администрировании то есть вам не нужно знать топологии сети или не нужно ставить на мониторинг каждый новый хост который появляется в вашей микро сервисной среде каждый новый сервер который вводится в эксплуатацию он достаточно установить агент и систему mchost и она сама включится в общую структуру построить граф связи покажет какие связи в единицу времени есть вашем приложении и предложат администратора вполне удобные механизмы для того чтобы этим управлять чуть подробнее остановлюсь на том как устроена компоненты системы на то чтобы понять что как это можно сделать и что тут нет ничего сложного это можно сделать по сути любой инженер который алюминий обладает уметь программировать знает может программировать на баш либо в нашем случае мы нагу написали по сути что мы делаем в агенте мы собираем открытые сетевые соединения с помощью команды на достаточно стандартные команды который есть как unix и так и windows операционных системах и сбрасываем эти данные в хронику то есть на основе данных там сотен холстов и тех связей которые открыты настоящий момент мы строим карту сети после ты после этого агент получает информацию о тех кастах которым надо опросить и 1 единицу времени нашем случае там раз в 30 секунд открывает соединение с теми кустами которые у него должны быть доступной по сути делают соки конев по определенному порту и проверяет проверяет насколько доступен этот порт это приложение может ли эти трафик ну и дам данном случае она ловит информацию о том сколько времени заняло соединений есть ли какой то там connection diffuse либо все хорошо водные в случае если мы имеем замедление соединения с хвостом либо имеем какую-то ошибку сетевую то в этот момент как раз работают правила что данном случае нужно делать самый простой вариант что мы делаем мы это делаем 3 срут рисовал до проблемного хоста снимаем хоп и и складываем это сетевую диагностика в отдельный файлик которые дальше уже кант кидаю на в канал энди фикации чтобы специалисты могли с этим разобраться в месяц и и техниками что же все-таки происходит второе что может сделать приложение tapping пакетами разные толщины потому что бывают ситуации когда вас небольшие пакеты через есть проходит а уже более толстая скажем так сообщения начинают интересы либо застревать бывают случаи достаточно сложной которым и отдельную ну скажем так стали обрабатывать качестве исключения это когда приложение на соседнем посте достаточно чувствительна к соединением например если взять oracle то если вы просто соедините сделайте там телнетом за connects на порт 1521 то уроку будет не очень хорошо будет ошибки сыпать log поэтому для таких исключений для исключительных портов мы знаем что здесь находится такое приложение который не любит когда просто так без дела к нему по сети ходят мы делаем например вещи для oracle это ты нас пинг ну то есть каждое приложение есть свой определенный способ как она может быть опрошено через сеть и после таких свечений не очень много агент распространяется по серверам и по сути строят карту сетевых связан с сама карта сбрасывается в базу не f и j хотел два слова про него рассказать это очень на самом деле интересный инструмент от огров его я база данных которая оптимизирована под обходы под различную работу с вершинами с ребрами связи используют язык хайбер клей ленгвич он на самом деле поглубже немножко окунуться язык очень интересный и немножко приходится как голову поломать для того чтобы написать запрос обхода графа либо вывода информации о приложениях но инструмент для нашей эксплуатации он стал достаточно популярно скажем так то есть инженер хочет посмотреть как работает его предложения какими связями она в настоящий момент оперирует можно без труда сделать запросик на сиквеле и посмотреть распределенную структуру вашего приложения в данном случае для нас вершины графа являются хасты а ребра это те связи которые были открыты в течение определенного времени который поймал агент для того чтобы дальше можно было с этим работать кроме этого само по себе не f и j достаточно легко настраивается масштабируется можно без труда обогащать узлы дополнительной информации то есть можно добавить дополнительные свойства узлов которые нам нужны для работы например для нас мы используем площадку приложение сервис так дополнительно формат котором мы обогащаем граф связи приложения что потом с этим можно было проще работать например в область выбрать все вершины все сервисы которые расположены на определенной площадки в суде вот это очень легко делается но не напрягает базу и по сути добавления какой-то дополнительный сущности в эту в эту ну метрики в данную вершину это совершенно простая операция и если сравнить работу не fuji epam с классическими реляционными базами то в случае работы с графами конечно она на порядок превосходит и запросы по десяткам тысяч вершин они занимают доли секунд времени в общем очень рекомендую эту базу для того чтобы объединить события и вывести информацию для инженеров в понятном виде в понятный канал унификации используемый вид менеджер очень простая в эксплуатации штуковина которая позволяет получать события с различных источников и группировать и превращать их в определенные человека читаемый инцидент либо человека читаема события которое дальше может быть передана в разные канал нотификации это может быть telegram это может быть slug это может быть ваша система мониторинга корпоративного на штука достаточно простая и может быть по сути скажем так незаменимым инструментом для того чтобы поработать с каким-то инцидентом то здесь же самом alert manager дальше вы можете настраивать важные групповые политики мониторинга когда приходит множество метрик с разных ростов вы можете подвергнуть все в одно событие раз пять минут который не будет с память скажем так инженеров а может один раз отправить сообщение и ждать резала о подтверждении закрытия закрыть а я какой-то проблемой метрики и так что что мы получили мы под мы после запуска приложений приложения сама начинались начинает стран строить онлайн карту работы вашего сервиса через какое-то время карта сервис разрастается течение суток в течение двух мы получаем полноценную в карту работы сервиса что нужно сделать у министра то что он сделать администратору сервиса это посмотреть внимательно эту карту и скажем так вычистить те связи которые мы не интересны поставить в их вас ключения для того чтобы получить целевые скажи боевые связи которые данный момент должны быть активны в данном случае мы после того как построили карту связь и начинаем уже приступаем к мониторингу для того чтобы уже по отработанным рабочим связи приложением начать следить за их поведением такие собственно говоря наш опыт заключается в том что мы снимаем стиву диагностику с приложений с разрывами связи и начинаем эту информацию водить дежурную службу для того чтобы ребята момент проблемы смогли на нее отреагировать но связь может быть недоступно 2 случай то есть либо вас действительно что-то сетью либо что-то с приложением спорт может быть просто закрыт например когда проходит какие-то планы в работы или установка релиза и в этом случае самая система нотомб оказалась очень полезной для того чтобы вы itis плановых работ из когда релиз устанавливается все приложения на своих местах вы полностью уверены в том что сервис восстановлен ну дальше инженеры не поленитесь загляните в нет мофт для того чтобы убедиться в том что действительно трафик пошел что все эти связи которые были до начала плановых работ они пришли нормальное концертное состояние то есть по сути это приведение приложение боевую готовность ему мы уверены видим что приложение собрала все те метрики все эти связи которые до выхода плановые работы были открыты они не нарушены они продолжают нормально работать таким образом у нас в единицу времени момент действительно какой-то сетевой аномалии срабатывает тригер приложение после того как очередным socket коннектом не может достучаться до приложение она снимает trace в отдельных случаях может сняться стива диагностика и складывает в папочку с того как проблема случается подключает сетевой специалист смотрит у него для работы все есть вот тебе файлик в котором написано на каком именно хоппе на каком именно маршрутизаторе у тебя случилась проблема вот сетевая диагностика и разбирайся красота что еще удалось нам получить для того чтобы по сути какие еще дополнительные проблемы помогла решить помог решить данный подход после того как мы построили карточную связанности было обнаружено что часть приложениям имеет а маршруты которые первоначально скажем так не планировалось реализовывать этом были доступ и между приложениями жду какие-то страны между какими-то которые идущие со странностями маршрутизаторы какие-то доступа шли вообще через интернет и по сути подход и тот позволил инженером разобраться в том правильно ли работает приложение правильный через какие скажем так связи идет построение графика но у нас по сути произошло несколько обрывок вы увидели что приложение а ходит часть между площадками через публичную сеть и это конечно был быть большая дыра я очень оперативно закрыли что позволило не только найти специалиста мной стиком найти проблему в сети о чем в общем то и говорит этот слайд далее какие еще бывают проблемы которые нам удалось столкнуться и разобраться проблема была очень серьезно с firewall когда приложение тормозило долгое время никто не мог разобраться что же происходит проблема происходило раз примерно в 15 минут когда просто пакета начинали правильный момент тормозить либо отваливаться причем для того для совершенно скажем обычных сетевых запросов между кастами firewall является совершенно прозрачным то есть вы никогда не увидите firewall когда пытаетесь сделать пинг или trace между хостами его можно увидеть только на каких-то групповых 100 будках вот когда мы скажем так увидели что это этот firewall он влияет сразу на группах остался что вот именно в этом месте должна быть какая-то проблема удалось обнаружить неправильно там настроенная цели firewall ли после чего общем-то в течение там суток на этом проблема была решена это снял очень большой головную боль для эксплуатации позволило общество разобраться с проблемой что еще мы увидели увидели ряд нелегитимных доступов не приложение когда летишь них начинает разбираться своим сервисом с картой сервиса он видит что используются какие-то deprecated методы legacy методы которых по идее не должно быть но он их видеть на карте связанности виде что это скажем так уже давно забытые методы все еще продолжают работать по каким-то совершенно левым сценариям это позволило нам дополнительно поработать скажем так с тех долгом подключать разные legacy нас 3 которая в нормальном нормальным равно как бы рабочем процессе времени на отслеживание подобных аномалий не остается ну и плюс второй момент это скажем так доступ и между приложениями дополнительная нагрузка на приложение которое быть просто не должно т.е. дополнительно подспорье для службы безопасности посмотреть это все но по другим углом и посмотреть какие есть связи какие то что нужно порезать какие стивы ограничения нужно выложить еще один момент который нам позволил который в общем то очень часто с чем мы сталкивались когда проходит плановые работы бывает так что вам необходимо переехать на какую-то активную на вторую площадку куда у вас приложение распределена пора пора по нескольким сайтам вам нужно произвести дерпи и при этом а вы совершенно не понимаете а будет ли приложение работать после того как база праймари с мастера 3 стоит у вас начинает срабатывать совершенно другие сетевые маршрут другие доступ и вам необходимы и как бы каждый момент времени у вас происходит изменение очень много изменений пришлось этим инфраструктуре гарантировать что приложение распределенная стоящей сотен хвостов переедет на резервную площадку при этом также стабильно будет работать вы конечно не можете потому что жизнь идет сеть меняется и нет мофт как раз таки нам помог сделать слепок работы приложения на активной площадки и его проверить на резервы для того чтобы обезопаситься и момент приезда вы точно понимали что на резервы площадке положения будет себя вести не хуже чем на основной и это является по сути дополнительный проспаться пурим для службы вашей эксплуатации для того чтобы убедиться в том что после переезда всей связи поднялись и все с этим хорошо то есть ребята нашли еще дополнительный способ использовать эту технологию для подстраховки себя перед крупными плановыми работами дерпи перед переездом на резерв ну и либо какой-то история с аварийностью срочно на переход резерв уверена что там где вы связи заступ доступны точно не будет проблем все включаем рубильник переезжаем на томов нам в этом является отличным подспорьем говорю о том что площадка на куда мы решаем отлично работает а и так что же получил владелец сервиса при использовании над вам но самое ключевое мы бережем нервные клетки эксплуатации при работе с подобными проблемами потому что она там делает пуля предсказуемым работа эксплуатации вы четко понимаете что у вас есть проблема видите сити вы видите свой диагностику и можете уже без труда объяснить специалистам что вот здесь вот на этом вот именно хоппе происходит проблема смотрите на верность и маршрутизатор вот у меня все есть все под руку под руками вообще в компании мы пропагандируем такой подход что владелец сервиса он должен отвечать за весь стек предоставление сервиса то есть начиная от приложения вот микро сервиса от приложения от базы данных и заканчивая ну скажем так железками на которых все это находится если происходит проблем у нас тему уровне то ну как бы не нельзя так говорить что там ребята эдик сетевикам с ними разговаривайте это вообще не моя зона ответственности эта сеть нет есть инструмент есть правильные паттерны работы системами и есть все необходимые скажем так возможности для того что владелец сервиса с этим тоже по разбирался кроме этого владелец сервиса получил наглядные даже борды графами где можно смотреть не только текущее состояние работы системы но исторические данные то чтобы разбираться в каких то моментах которые были после плановых работ либо когда сетевые специалист проводят работаю позволяет по сути страхователь эксплуатацию как от каких-то ненужных звонков ночных либо от тех скажем так каких-то сбросах со стороны соседних подразделений с лотом приложение 5 плохо работать не вот смотрите у нас планово работа идет сеть все еще не поднялось но через час все будет хорошо это диагностика которая происходит в момент сработки проблемы и наглядность это граф в который можно очень легко просто поразбираться посмотреть как устроена приложение и по сути построить карту своего сервис посмотреть на стороны посмотреть какие-то лишние связи может быть что-то надо постреливать возможны какие-то старые а те запросы там остались те связи которые быть не должно это позволяет все уверена что приложение вот оно такое красивое распределенные там только те связи которым не нужны ну и самое ключевое наверное это наши коллеги наши друзья в эксплуатации сети которые очень оценили данную реализацию это позволило им тоже нервы сократить то что экспорта совершенно инфраструктуре в архитектуре приложения слабо разбирается им больше интересуется карта сети и здесь мы получили очень надежных друзей и союзников как вместе с которыми нам удалось сделать подобную технологию которая показала себя для нас очень неплохо я всё ребят давайте вопрос спасибо вот у нас вопросы прямо в первом ряду досмотрите вас нимфа джей есть связи на в другую сторону у вас снова forge и связи вас на картинка был тут просто написано relations вы как-то тип relations обозначаете и направление вызова может протокол асинхронный не синхронны до для связи у нас на самом деле хранится эту информации направлении связи имеет значения но связь она скажем так она важна не только для тех состав где расположен агент мы также все свежим связи куда-то во внешний мир но словно если микро сервис например работает с каким-то внешним поставщиком внешним сервисом какой-нибудь delivery либо такси еще кто-то то нам мы также можем еще посмотреть внешние связи которые открывает приложение связь все дополнительно обогащается информация о направлении управление протокола но вот в том плане что что-то триггером вызывается что-то получается это за это указано на схеме то есть где то вы получаете информацию от сервиса где-то вы сами вызываете get запросам это тоже указываете да да это тоже указывается то есть вот все что можно собрать о связи из ацетата вам не очень много на самом деле информации мы можем собрать добрый день скажите пожалуйста как вы разлить разделяете и кто вообще такой владелец сервиса и тех кто эксплуатирует это одни и те же люди или в вашем понимании это разные хорошего просмотрите на самом деле у нас давно по было принято скажем так такая парадигма когда эксплуатация они же отвечающие за диплом сервис они же отвечающие за его отказоустойчивость это одни и те же люди то есть собранный таки devops команды которые отвечают за работу сервиса целиком ну и по сути для нас руководителей команды эксплуатации он же является владельцем ну владельцы сервисом который скажем так держит сервис своих руках спасибо следующий вопрос у нас весь приветствую меня зовут алексей вопрос у меня тоже такой с подводочкой вот я у вас видел много нот на графе для вас много узлов много связей как определить что допустим где то есть роутер у которого может быть сотни интерфейсов или это сервер у которого сотни api-интерфейсов или о какой-то определяете еще раз у вас есть условно железка на которой ни один ip адрес у вас всех там может быть десятки сотни но это при этом одна физическая коробка можно ли это как то видеть на вашей системе хорошего просто самом деле мы это но получается что для тех приложений которые имеют по несколько ip-адресов для нас это будут выглядеть как несколько различных узлов потому что для нас является вершины графа этапе адрес потом отслеживаем все эти адреса с которыми работает приложения как правило такой большой прям беды это как эксплуатации сильно не доставляет ну ладно далее у вас опять же е5 каната между ними есть не допустим мониторится ли связь внутри сетей того же кубер а потому что если он сядет рейс из сети допустим контейнера steam или рту не скажет примерно ничего потому что то себе готовила и 4 этого не видит ну да самом деле для как бы вопрос понятно потому что наверное нет смысла внутри кубера мониторит связи которые между разными тампонами поднимаются для нас кубера важны именно внешнюю связь то есть когда работа там смешанной интеграции скрыто базы данных мы такие вещи их но кабине ставим на мониторинг если они внутри одного большого кубера открывается закрывается то есть по сути для нас кубера то один хост который с внешним миром как-то связан хорошо итогам заключающий вопрос ведется лишь разработчикам работы по улучшению то что называется а верность да то есть знаете они как что вообще сеть может не работать что она может работать не так как они представляют то есть я просто вижу ваша система это походит на то что сетевиков достали собственно притенить разработчиков что вот у нас что то сломалось разработчики хотят получить хоть какую-то на горят здесь с позиции там меня как сетевой инженер а тут тьма недостатков и и тьма вещей которые можно улучшить ведется ли с разработчиками работа в том направлении что вот как допустим правильно обрабатывать и как обрабатывать правильно долго эти хиппи connect и как tune например в тоже linux вы ядро чтобы она правильно обрабатывала нужное количество connectors перегибе ну на самом деле вот если про сама про разработку это говорить то она была достаточно написано двумя инженер достаточно быстро для того чтобы закрыть одну кну как бы определенную конкретную боль мы на покидали очень много на самом деле направление куда можно развиваться в том числе вот часть из них то что то о чем вы говорите потому что но не все так топорно нельзя просто так между приложениями там без это сама без незаметно как бы открывать соединения и приложение там тоже есть определенные то самое скажем недостатки вот разрыв работа ведется то есть у нас сам бак лук достаточно большое что нужно еще что нужно еще сделать и мы тоже в плотно связке с ребятами из эксплуатации рабочий сидел обучайте потому что вот что вообще из себя представляет и как linux работают сети но конечно да то есть ребята очень плотно работать и веками у нас спасибо следующий вопрос еще один вопрос края скажите граф уже может быть замкнутой и когда сервер ли сервис влияет 1 на 1 и как учили из одного может получиться зацикленная цепочка вы с какими-то такими ситуациями сталкивались и как вы из них выходили если они были бывают до на самом деле если говорить вот про как бы схему работы приложения то что я показывал это совершенно очень упрощенный вариант это с кем-то очень серьезным фильтром обычно но реальной жизни там получается огромное количество связей между приложениями на клубок такой связи на самом деле папа ходит если мы видим cootes крупную сетевую аномальной крупностью из будет у нас сразу весь граф становится красным одновременно вот и тут важно по ну как бы разобраться в череде событий последовательности то есть как правило практика показывает что первое первый обрыв сетевое 1 связи где мы detect но детекторе первую скажем там к нашим diffuse либо замедление соединения то тот узел откуда нужен цепочку дальше раскручивать то есть историчность с работа к она имеет такое большое зна но большее значение но надеюсь его рассказал понятно и у нас был я помню был вопрос дальнем конце зала вот человека рука приветствую хотел задать такой вопрос а как происходит сбор данных с узлов вы сказали что его можно установить на любой хост вот и каким образом происходит потому что вы еще сказали что добавление новых узлов как бы его не нужна не нужна ни где фиксировать то есть том же прометея не нужно указывать в конфигурации там этот хвост но на самом деле у нас есть серверная часть которая согнул работы с картами и она же собирает информацию о сработке ноту и дрессировки которые собрал конкретный агент то есть есть серверная часть у нас мы нажмем называется которая опрашивает хасты периодически эту проблематику ну такой вот у стива диагности который снимает постоянно с них собирает и дальше уже кладет нас по специальным папочкам и дальше метрику отправляет в базу виктории metris на жизнь достаточно просто указать нет менеджер в виде точек информации самом деле по дефолту скажи весь конфиг фуу агента и там в этом конфиге прописывается ну как бы несколько и пикников которых находится над менеджеры но этот разных судах они как раз таки для тех случаев если сбой происходит и при инициализации свои работы агент вам всю необходимую информацию получает и дальше начинает уже диагностировать сеть я отправлять на серверную часть но результатом netstat а из включается в цепочку в общем на этом менеджер он отказа устойчив то есть там какая-то репликация происходит сказали да там есть основной скажем так основной хостов и резервные что крупная легло сеть совсем легла то начинает обхода действует через соседний параллельный над менеджер там находится в изолированном 70 street у нас три площадки через три площадка собираются метрики мониторинг и последний вопрос здравствуйте спасибо за доклад вопрос про историю появления этого этого инструмента как я понял разработчики столкнулись с какой-то проблемой не получили помощи от сетевой службы и чтобы доказать что проблема в сети сделали этот инструмент клёвый а чем закончилась эта история тема была в сети или в приложении хорошо спасибо да на самом деле началось с того что разогнулась сергей инженер задумал написать king лавку то есть не мог доказать сетевикам проблематику и сказал я сидела пинга валку которая сейчас будет выявлять проблем у него все получилось он проблема проблематику скажем так доказал но практику решил он был эту развитие то есть тебя получилась хорошая история давайте сделаем теперь клиент-серверной части постепенно постепенно приложение обрастала различными модулями то есть появилась историчность появилась графа база данных но и вот таким образом через скажем так подручные средства на базе у таких у план собственных технологий превратилась все в такую распределенную систему"
}