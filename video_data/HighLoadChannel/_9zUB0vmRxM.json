{
  "video_id": "_9zUB0vmRxM",
  "channel": "HighLoadChannel",
  "title": "VShard - горизонтальное масштабирование в Tarantool / Владислав Шпилевой (Tarantool)",
  "views": 1836,
  "duration": 2811,
  "published": "2019-05-14T14:49:32-07:00",
  "text": "всем привет меня зовут владислав шпилевой и работаю в тарантул разработчикам и сегодня я буду рассказывать про горизонтальное масштабирование авторан модули в шар план доклада следующий сначала я немного расскажу о том каком именно из множества возможных горизонтальных масштабировании мы будем сегодня говорить о том что было до в шар да зачем появился в шорт какие проблемы он решает как он работает как вы использовать масштабирование бывают двух типов горизонтально и вертикально и горизонтально и делятся на 2 типа на репликацию на sharding репликация служит для масштабирования вычислений шар нигде не масштабирование данных шарлин делится еще на 2 типа на шарлин диапазонами и sharding хэшами пожарники диапазонами мы разбиваем мы от каждой записи в нашем кластере вычисляем некоторые sharp ключ что отключи ложатся напрямую который бьет на диапазоны и диапазона ложатся на разные физические узлы шар деньгам кашами все проще мы берем от каждой записи в кластере считаем хэш-функцию из записи с одинаковым значением хэш-функции ложатся на один физический узел и сегодня мы будем говорить про горизонтальное масштабирование при помощи шарди рования пух и шам первым модулем грант ального масштабирования в трамп или был тарантул shard это такой sharding хэшами очень простой который шарди руют ну который считает shortly учет он первично выключаем всех записи в кластере вот и принципе он был достаточно простой вполне себе работал но у тарантула появилась задача с которой тарантул shot оказался не способен справиться по трем причинам достаточно фундаментальным во-первых самая критическая причина в нашей задаче требовалось локальность связанных данных h логически связаны суть в том что когда у нас есть данные которые связаны логически мы хотим хранить их на одном физическом узле всегда как и как бы ни менялась топология кластера состав как бы вы не добавляли не удаляли узлы как бы ни сложилась ли балансировка я хочу чтобы данные лака логически связаны всегда были физически вместе так не sharding это гарантировало тарантул shard этого не гарантируется то что во-первых он считает по только в первичным ключом хэш и во вторых при балансировке даже значения с 1 даже записи с одинаковым хаш он могут какое-то время на какое-то время разъехаться следующая проблема это медленный и sharding это классическая проблема всех шар шар дав всех сортов наши шах суть в том что когда мы меняем состав кластера добавляем узлы удаляемых у нас обычно меняется шар функцию потому что она как правило зависит от количества узлов и когда она меняется нам приходится пройтись по всем записям кластер ее пересчитать шар функцию заново и какие-то записи возможно перенести кроме того пока мы переносим эти записи мы не знаем заранее любого конкретно взятого запроса данные которые он ищет они уже перенеслись или еще нет или они сейчас в процессе переноса мы не можем от заранее сказать и поэтому во время ришар деньгах графических sharding их приходится на каждое чтение делать запрос по двум шар функциям по старой по новой это все очень медленно запрос становится в два раза медленнее как минимум это было неприемлемо в задачи поставлены следующие нестабильные чтения в этом особенность этого модуля реализации у него такого что при отказе некоторых узлов в рипли кассетах он показывает плохие результаты на доступность на чтения вот и я хочу подробнее остановиться на проблеме локальности данных то из за чего то ron paul shark больше всего не подошел вот рассмотрим пример у нас есть банк есть клиент банка есть считаю клиента в этом банке и я хочу клиента его счета всегда хранить физически вместе чтобы за один запрос их всегда можно было прочитать чтобы за одну транзакцию их можно было вместе как-то поменять со счёту насчет там перевести если использовал классический sharding включая тарантул сорта нас и счетов у клиентов значения шар функции будут разными в общем случае они могут лично разные физические узлы и это сильно усложняет и чтение и транзакционных работы с таким клиентам и для решения поставленной поставленных трех проблем появился тарантул в shard ключевое отличие тарантул в шар до шарда в том что в нем уровень хранения данных виртуализованных то есть у нас появляются виртуальные стороны поверх физических сторожей и и записи они распределяются к виртуальным сторожам они по физическим и тарантул сорта он оперирует этими виртуальными сторожами которые называются бакетами он пользователь уже работы с бакетами он не заботится о том что каком физическом и зле лежит его шар при помощи bucket of green в шар небо китай атомарные неделимой единице данных как классическом шар денги один то пол и г-sharp всегда bucket и хранит целиком на одном физическом узле и переносить их тоже всегда целиком вместе все данные одного баки то там орда во время ришар линга за счет этого обеспечивается локальность нам нужно просто положить данные в один bucket и мы можем быть уверены всегда что эти данные будут вместе при любых изменениях кластера каким образом можно положить данный в один bucket модифицируем схему которую мы раньше вели для клиента банка добавим в нашей таблице по 1 1 1 новый закон по одному новому поле баки тайги и этапа либо китай диу связанных данных нужно сделать одинаковым и тогда мы будем мы можем быть уверены что эти записи будут в одном бакте преимущество здесь в том что мы можем эти записи с одинаковым баки to die хранить в разных space ах даже в разных движках у меня эти этапы могут ну эти записи могут лежать могут лежать на диске могут лежать в движке в памяти у нас таран плетут у другой есть вот и уровень bucket он абстрагируется уровень движка уровень таблиц уровень space of space и эту таблицу в tarantul а вот обеспечивается локальность паба китай и независимо от того откуда эти законы как эти записи хранятся что дает локальность данных почему мы так не стремимся на примере чтение если у нас есть классический sharding то у нас данные могут разрезать разъехаться в общем случае по всем физическим сторонам какие у нас только есть нам придётся при запросе например всех счетов какого ты пользователя от клиента сходить на все узлы это будет сложно сечение от and given это количество физических хранилищ это ужасно медленно что нам дает тарантул в шарф со своими багетами локальность you the bucket айди мы просто задаем у связанных данных одинаковые баки тайги и всегда можем прочитать их с одного глаза один запрос независимо от размера кластера вот я говорю о том что баки то иди надо засчитать одинаковым все будет хорошо каким образом его надо засчитать как как его считать смогу преимущество ну кому то преимущество кому-то может недостаток типа недостаточно автоматики но не считают преимуществом что вы можете сами выбирать функцию как вы будете считать баки тайги каком удобнее вот в примере на слайде я использую crc32 от идентификатора клиента тогда у меня для щитов и для клей для клиентов будет bucket айди одинаковые но вы можете вместе crc32 взять любую другую функцию какая вам только нравится единственное требование к ней чтоб она так как любой хэш-функции в классических sharding их на кашах чтобы она равномерно распределялась свои значение по прямой потребны по по своим значениям короче возможно вот ну вот я выбрал эту функцию bucket айди вот такое начал заливать данный на свой кластер заливаю данные заливаю все залилась у меня место уже начинает не хватать теперь надо придумать как мне эти данные перенести на новые узлы я хочу добавить узлов чтобы данные на них сами переехали как это делается в шарди сначала вам нужно эти новые узлы стартанули нужно просто физически запустить там сервер запустить процессы тарантулов запустить вот например антибан каким-нибудь каком удобно после этого нужно обновить конфигурацию washer да и конфигурациях которые описаны все участники кластеры все реплики рипли кассеты кто-то мастер кто не мастеру какого такие юри в этих новые узлы добавляете в конфигурацию вот в 15 строчки описал новый реплика set состоящий из двух реплик я указал кто там у него мастер кого какие виды у кого какие именно там разные вещи здесь можно указывать только то что на слайде представлено много всяких настроек есть вот и и конфигурацию мы применяем на всех узлах кластера на старых она новых позвав функцию в шар сторож цпг она эта единая точка конфигурацию через который конфиг обновляете на всех углах вы просто зовете она дальше все сама делает вот как и помните когда я говорил про проблему классического шарден говорил что когда он изменяется количество узлов меняется шар функция и приходится данные все пересчитывать перетаскивать и что свой шар дом если он если он делает то же самое получается он проблем не решает но то же самое не делают в шардже количество виртуальных хранилищ bucket of она фиксирована это константа который вы выбираете когда кластер запускаете может показаться что это как-то ограничивает ваш ресурс масштабируемости на самом деле нет количество bucket of вы можете выбрать совершенно колоссальным его можно выбирать тысячами десятками тысяч сто тысяч пятьсот тысяч пакетов можно выбрать главное чтобы у вас количество байтов было примерно на пару порядков больше чем максимальное количество реплика сетов которые у вас когда-либо в кластере будет вот и за счет того что количество виртуальное хранилище не меняется и шар функция зависит только от этого количества то у нас физически хранились можно добавлять сколько угодно и шар функцию пересчитывать не придется ни когда и как вот это вот делается что bucket и сами разъезжаются физическим хранилищем вот именно что разъезжаются они сами руками это трогать не надо когда вы зовете его шансы туровцев этом дальше все сама делается с вашими данными в баке так первое что происходит когда вы зовете ваша 100 часов игре на одном из углов просыпается процессоре балансировщик это такой аналитический процесс который считает идеальный баланс класть он ходит на все физические узлы спрашивают у кого сколько bucket of смотрит около bucket of что-то слишком много считает кто кому сколько бойцов должен и строит маршрут и макетов маршрута кто кому сколько должен отправить багетов эти маршруты он посылает сторожам который переполненный говорит и отправляет омута столько ты ты кому-то столько они начинают отправлять отправка идет bucket и по одному постепенно переносится в какой-то момент они переноситься закончит кластер сбалансирован кластер сбалансирован в классическом смысле что везде одинаковое количество данных одинаковое количество багетов но бывает так что у нас реальной жизни такое понятие сбалансированности она для нас не очень работает можно сказать другое понятие идеального баланса к примеру у меня я нахожу хочу на одном рипли к сети хранить меньше данных чем на другом у меня просто не знаю жесткие диски там по объему меньшую мне хочется там хранить меньше данных в шар думать что все хорошо у меня на самом деле не все хорошо у меня сто раз уже вот-вот переполнится его шар предоставляет способ с этим бороться вы можете использовать веса у каждого рипли кассету каждого стороже выбирайте вес и когда ри балансировщика принимает решение о том кому сколько bucket of отправить он берет в учет отношении всех пар весов попарно то есть но для каждой пары сторожей смотрит отношении их весов и считает сколько один тоже должен сохранить bucket of относительно другого фараджа к примеру у меня на одном стражи вес 100 другом 200 тогда у меня сторож средств то будет хранить два раза меньше bucket of чем сторож с весом 200 обратите внимание что я говорю именно отношение весов абсолютное значение весов тут не имеет никакого нам оно не влияет ни на что вы можете поэтому вы можете веса выбирать какими вам больше нравится по смыслу вы можете выбирать веса как как как сто процентов распределенные по кластеру там на одном стороны 30 процентов на другом там 70 вы можете выбирать веса как буквально число гигабайт емкость стороже которую вы готовы которые готовы принимать байки ты или вы можете как количество bucket of веса использовать это неважно главное чтобы отношение было правильное которое вам надо и под такой побочный эффект весов интересный получается что при помощи весов можно bucket из осторожно полностью удалить если у него выставить вес в ноль тори балансировщик скажет этому стражу распределить свои bucket и на остальные стороны и при помощи этого можем стороне удалять примеру мы хотим сторожат рипли кассета избавиться из кластер и вместо него другой какой-нибудь добавить но у него ставим вес и в ноль ждем пока он все свои bucket и раздаст остальным участникам кластер и он от какой-то момент сделает и мы можем после этого терять прибыль кассет мы его выкидываем из конфигурации мы убиваем там тран тульский инстанции на сервер стопим что там еще надо сделать вот прежде чем мы перейдем к описанию того как bucket удается переноситься атомарная за счет чего обеспечивается локальность данных даже во время ричардом я хочу прояснить в чем еще раз в чем ключевое отличие виртуального шарден га с бакетами от классического sharding а суть классического шарден га в том что когда мы меняем состав кластер у нас появляются два состояния текущее состояние старое и новое состояние в которое надо перейти в котором данные распределены чуть чуть иначе и когда мы между этими двумя состояниями переходим нам приходится для всех записей хэш-функции пересчитывать все это как-то переносить этот фф порождает ужасной болью что мы не знаем какие данные перенеслись какие нет возникают какие-то конфликты ошибки все это приходится ри стартовать по много раз непонятно когда данные можно удалять со старого места когда нельзя и это плохой способ потому что он очень сложный и когда мы делаем виртуальный sharding то становится гораздо проще у нас нет двух выделенных состояний кластер и как как кластера в целом у нас есть только состояния bucket of и кластер он передвигается из но он более более маневренным становится более более плавный эволюционирует наш кластер он постепенно передвигается из одного состояния в другое если они становятся не только 2 и благодаря тому что переход становится плавным мы даже ли балансировка можно в процессе квака найдется эти развернуть обратно мы можем взять удалить обратно сторож который мы только что добавили он начнёт раздавать свои bucket и в обратную сторону то есть процесс балансировки стоит свой очень легко управляемым гранулярный когда мы делим кластер на какие-то маленькие под кластера виртуальные bucket и мы упрощаем себе задачу и делаем при балансировку пошаговых они весь кластер целиком ришар деру им вот а теперь посмотрим как bucket как баки то удается удается перенестись а там арно ведь bucket он он может и реально там мегабайта весить сотни мегабайт какую на тамар на все успевает переносить вот ну вот предположим у нас есть bucket и мы хотим его перенести он состоится запросы принимает на чтение на запись и прибыл чашек говорит перенести этот bucket на другой стороны он перестает принимать запросы на запись зачем он это делает затем что если он не перестанет тогда пока он от bucket переносит его уже успевают на обновлять пока он переносит апдейты не успевают еще на обновлять пока он переносит эти апдейт еще нагуляются так это будет бесконечно продолжаться поэтому записям блокируете при этом читать его все еще можно то есть доступность на чтение все еще работают он начинает кусочками переноситься чан коми такими на новое место он переносится переносится в какой-то момент он перенесется и на новом месте начнет принимать запросы на старом он все еще лежит но он уже помечен как мусорный и в какой-то момент сборщик мусора его удалит постепенно тоже чан коми вот ffeec в чем здесь от омар насти товарность в том что у каждого баки тоска с каждым баки там ассоциированными то данные которые хранятся на диске физические каждый шаг которые сейчас описал он на диске сохраняются и чтобы не произошло со стороной при стартанет он там какая-то ошибка произойдет он всегда сможет восстановить состояние баки то на чем перенос остановился и при этом заметьте что вдруг доступность то чтения она была в сто процентов все время пока bucket переносился данные всегда можно было читать полностью целиком все связанные данные одного bucket а вот и даже если за окном что-то пойдет не так что какая-то ошибка произойдет то вы шарди есть автоматический recovery manager который bucket и будет восстанавливать которые не смогли перенестись там сразу вот ну из такой схемы все-таки возникает вопрос а можно ли запись как-то не блокировать потому что если бакен будет переноситься долго там не знаю в секунды минут и вдруг у нас в этом практике это критические данные которые вот конкретно сейчас хотелось бы не блокировать можно его потом ночью как-нибудь перенести вот днем там будет доступен и что будет запросами которые из баки там работали в тот момент когда его сказали переносить мужем не можем их просто взять закончили типы все особенно рай запросе сначала остановимся на втором потому что он просто проще реализовано вот у каждого bucket of его метаданных есть два типа ссылок ссылки на чтение ссылки на запись когда пользователь делает запросы к багету он указывает он будет с этим баки там работать на чтение или на запись ребенка перед райт и для каждого запроса соответствующий счетчик ссылок увеличивает вот что нам дают ссылки счетчик ссылок на читающий запросы они нам дают то что baked он будет переноситься спокойно перенесется потом придет сборщик мусора и скажет я хочу этот bucket удалить но он увидит что счетчик ссылок не налью удалить его не сможет вот и удалить его не сможет ему не достать количество ссылок сделать и когда наши запросы доработают закончить работу тогда garbage collector уже дропнет этот bucket то есть наши запросы могут быть читающие уверены что bucket у них из-под ног никогда не уедет что нам дает счетчик пишущих ссылок он дает то что баки даже не начнет переноситься пока ходи хотя бы один пишущий запрос с ним работает как то происходит ведь запросы читают запросы пишущие могут постоянно приходить приходить баки тогда не перенесется никогда если bucket 3 басир изъявил желание переносить то новые запросы на запись начнут блокироваться при этом старые мы ждем пока закончится в течение какого-то тайм-аута если в течение тайм-аута они не завершатся мы продолжаем принимать новые запросы на записи откладываем перенос баки то на какое-то время и так и балансировщик будет стучаться в этот bucket пока он ему не получится его перенести верен может другой вообще перенесет который будет проще перенести которые нагрузка меньше внутри балансировщик не сможет его не сможет его некуда двинуть и все это делается автоматически все эти reference каунтер и вот если вы пользуетесь в к уровням и высокоуровневым api нового шарда есть низкоуровневой api на случай если вам высокоуровневой не хватает что-то свое там хотите напилить тогда есть низкоуровневая кибаки 3 которую просто у себя в коде зовете все если очень хочется что-то самому сделать что с полным полным не блокированием на запись ну для этого bucket придется вообще не переносить если мы хотим чтобы он всегда был доступен на записи для этого есть bucket пин это такая функция которая у себя в коде зовете чтобы запятнать bucket крепли кассету на котором он сейчас живет и пока он забил он он никуда не уедет он будет всегда доступен на запись при этом его соседи bucket и смогут приезжать выезжать все с ними будет в порядке вот есть еще более сильная вещь чем bucket pinata целый лак рипли кассета он делается уже не в коде через конфиг и он делает так что с этого рипли кассета ни один bucket не увидит нужно примерно по тем же причинам по которым может понадобиться баки in все все данные этого рипли кассеты будут всегда доступны на запись вспомним какие у нас изначально были по три проблемы у нас не было локальности данных была проблема что ришар нем все замедляет а сам он тормознутый и что чтение нестабильно как мы побороли локальность мы добавили баки ты кладешь в один богaтствa локально так мы побороли что ришар ним что замедляет мы переносим все атомарная по bucket на и шар функция никогда не пересчитывается что насчет нестабильных чтений на них мы так и не останавливались пока для того чтобы побороть проблему нестабильных чтений еще ряд других проблем выше а есть под модуль под названием в шорт роутер то есть в шар дом состоит из двух под модулей ваша 2 sharp роутер них можно независимо создавать масштабировать наш на одном в инстансе в и роутер он решает за нас задачу поиска локации bucket а то есть мы когда обращаешься кластеру мышь не знаем где какой букет лежит как его искать за нашего будет искать в шар кроутер вот он работает но как обычный роутер только по баки там они по и печника мы указываем ему вместо адрес собаки баки тайги и он знает где этот баки джи-ви и он наш запрос туда прокси рует рассмотрим на примере как это выглядит вот у нас есть кластер опять клиент-банк его счета я хочу иметь возможность в этом кластере вытащить все считаю этого клиента я реализую обычную функцию которую ни в какую сеть не ходит просто локальный ищет авторан при вот так будет выглядеть функции три строчки на ищет все счета одного клиента по идентификатору клиента и теперь мне нужно решить на каком astra j ту функцию позвать вот она на всех лежит и где-то на одном из стражей лежит мой клиент где выживать я беру вот вот под одни идентификатора клиента в моем запросе считаю баки тайги и говорю ваш от роутер cold вызови мне такую эту функцию на том что раджи где живет bucket с таким табаки то иди и он это сделает за нас внутри но вся в приложении своим ваша tropical и все вот он проектирует запрос куда надо как он узнает где данные лежат куда куда надо сходить у него внутри из таблицы маршрутизации как обычного роутера you там есть соответствие какой bucket на каком рипли к сети лежит и каждому рипли кассету каждой реплике каждого рипли кассета роутер есть connect через которую наш запрос прокси рует конечно может получиться так что произойдет ришар 1к боккетти начнут переезжать вот прямо сейчас они едут и роутер он фоне постепенно эту таблицу обновляет большими кусками он опрашивает стражи спрашивает у тебя какая какая у тебя актуальная таблица байков вот и у себя в таблицу маршрутизации обновляют может случиться даже так что вот bucket прям только что переехал мы сразу делаем на него запрос роутеру еще не успел обновить свою таблицу маршрутизации тогда он ходит на старый сторож старый сторож ему скажет а у меня этого баки то больше нет и рокер тогда либо старый сторож ему подскажет куда bucket переехал он так умеет либо если он переехал относительно давно роутер все равно почему таблицу не успел обновить тогда роутер срочную возьмет им определю сам просто обойдет все стороны спросит у кого этот bucket есть ну и все это произведет внутри роутера прозрачно для нас мы даже не заметим что там произошло мисс в таблице маршрутизации то есть он автоматически решает задачу роутинга ну и задача который у нас стояла изначально 1 из 3 нестабильность чтений он решают ее при помощи под системы автоматического рид файла вера как это работает если у нас кит узлы отказали узел на крым например на который рокер вот прям сейчас ходил себе спокойно отправлял то запрос этот узел отказал он на пинге не отвечает рокером пингует периодически стражи на которые ходят и вот какой то тоже перестал пинговать он него есть у него есть горячий резервный коннект к каждому и каждой реплике если текущая реплика перестала отвечать он пойдет на другую запросами на чтение нормально работает потому что на репликах мы читать можем писать мастер откажет тогда роутер на помощь не сможет но если мы хотим читающий запрос выполнить нос может сходить на реплику и мы можем даже выбирать на какую реплику именно он будет ходить если если отказала реплика которая сейчас пользовался то есть можем задавать приоритет реплик который по которым роутер должен выбирать файл оверов для чтений делаем это при помощи зонирования мы каждую реплику каждый каждому роутеру присвоена зону это просто номер и мы задаем табличку в которой у каждого им расстояние между каждой парой зон и когда роутер принимает решение куда отправить запрос на чтение он выберет реплику в той зоне которая ближе всего к его зоне как это выглядит в конфигурацию вот у нас есть там три зоны к примеру в них есть какие-то реплики и роутеры там работают я хочу задать между ними расстояние чтобы роутер предпочитал определенные зоны когда ему нужно рид запрос выполнять я беру и вот на изи килл опишу такую табличку где для каждой пары зон задаю расстояния из первой зоны во вторую можно даже другое расстояние задать и зов из 2 в первую обратно не в конце предпоследней строчки я пишу номер зоны этого рокера и когда он он будет принимать решения в эту табличку будет смотреть куда ему ближе сходить что могут зоны означать для нас зачем может быть это надо почему нельзя просто произвольно реплику ходить в общем случае наверно можно и непроизвольно но если у нас кластер большой и сложный там много данных он распределенный прям очень сильно тогда это может пригодиться например зоны это могут быть разные разные серверные стойки мы хотим на свою серверную стойку ходить до нее ту поближе чтобы не ходить там не знаю какой маршрутизатор не грузить сеть или это могут быть географические удаленные друг от друга точки много дата-центров у нас есть там по миру и мы хотим чтобы роутер предпочитал свой дата-центр когда он выбирает реплику они куда метис россию там в америку ходил когда у него рядом реплика доступное соседней стойке и ещё это может быть нужно за например если у нас разная производительность разных реплик примеру у нас в каждом реплика сети есть одна backup реплика которая не должна принимать запросы она должна только хранить копию данных и все резерв такой и она просто будет медленно запросы обрабатывать если начнет начинать посылать и тогда мы делаем и в такой зоне которая будет очень далеко от всех роутеров вот по этой табличке и тогда рокер будет ходить на нее только в самом крайнем случае вот такое применение узон и раз уж мы заговорили про рид failover того что что вы шарда с райт файла верам сменой мастера со сменой мастеру у него уже не все так здорово он такой наполовину автоматизированной получается выбрать нового мастера вам придется самостоятельно выборы нового мастера в ваше рвение реализованы вот но когда вы его выбрали каким-нибудь внешним сервисом то мне знаки не буду кипером или еще чем-нибудь это cd там выбрали в новом мастер и вы хотите чтобы он взял на себя этот рингтон взял теперь на себя полномочия мастера обновляете конфигурацию указав у старого мастера шпон теперь мастер фолз у нового мастера что он мастер true и применяйте этот конфиг опять через шар становится f и g и раскатываете его на стороны и вот дальше все происходит автоматически у нас есть старый мастер он принимает запросы на чтение на запись и приехала указание вот пассажиров шерсти тоже часов и г что мастер должен теперь быть другим старый мастер перестает принимать запросы на запись и начиная синхронизацию он синко из с новым мастером чтобы подогнал ся потому что могли быть данные которые уже применились на старом мастере аналоговой еще не доехали ну вот он делает этот sing и после этого новой мастер начинает принимать запросы вступает в свою роль мастера старый мастер становится репликой мы все в общем то так работает raifil over ваша где как теперь следить за всем этим многообразием разных вещей которые вроде постоянно происходят какие-то внутренние подсистемы bucket репликацию что с этим всем делать много ручек наверное надо на самом деле хватит одной ручки в общем случае это ваша скоро чен фа и на роутер есть ваша роутер info в шар сторож инфо вам показывает информацию в нескольких секциях первое это секции репликации он здесь покажет он состоянии реплика сета на котором в эту функцию позвали у него какой лак три пика ционный там кем у него connect и есть кем у него репетиционных коннектов уже нет кто там доступен недоступен на кого какой мастер сконфигурирован разные рекреационные вещи здесь показывают в секции багетов и в этой секции можно посмотреть в реальном времени рамки силки будут меняться время ришар нинка особенно это заметно здесь можно посмотреть на этом реплика сети сколько конкретно bucket of прямо сейчас на него едет сколько с него сваливает сколько нам ним сейчас работает в штатном режиме сколько у него помечена как мусор сколько за pin на все это можно здесь наблюдать и сколько всего в сумме есть секция alert of это такая сборная солянка всех проблем которые в шар смог самостоятельно детектив там не сконфигурирован мастер недостаточный уровень redundancy или у нас наоборот есть мастера все реплики отказали или не знаю общем различные проблемы здесь показываются все какие вы шар смог сам найти и последняя секция эта лампочка которая загорается красным когда все становится очень плохо это просто число от 0 до 3 которая чем она больше тем хуже вот это такой самый простецкий индикатор того чтоб властям все хорошо или все плохо и есть в шартра утра инфо в нем точно такие же секции нужно что-то не немножко другое первая секция 5 репликация но только здесь не облигационные лаги здесь в основном вопросы доступности к этой информации о том какие connect и у роутера каким рипли кассетам держится какой connect у него горячие резервные на случай отказа мастера кто там у него выбран мастера на каком рипли к сети сколько пакетов доступна на чтение и запись сколько доступно только на чтение например не сейчас переносится из секции багетов где написано сколько bucket of суммарно на этом роутере доступно сейчас на чтение и запись на чтение сколько он вообще не знает где лежат и он сейчас в процессе их поиска или сколько он знает где лежат но просто рипли кассета то сейчас мы коннекта к нему нет него тоже есть секции allure тов то же самое значит только в основном там про connect и про то что failover там сработал или еще что неопознанные bucket и это же шкала более у него есть чисел k от 0 до 3 что требуется от вас если вы все-таки решаете использовать в shard от вас требуется в основном две вещи первая это выбрать число bucket of самая детальная вещь в шарди это bucket и вам нужно выбрать их количество заранее констант нам как принять это решение вам нужно учитывать что ну почему нельзя количество багетов просто в 32 макс поставить и все раз они такие хорошие гранулярными все отлично будет работать с ними потому что с каждым баки там хранится некоторое количество этой информации какое-то количество байтов на стороне какое-то количество байтов на роутере именно на сторону же хранится 30 байт примерно 30 бой с каждым баки там и на роутере примерно 16 байт с каждым баки там и чем больше у вас bucket of тем больше будет тратиться на мету информацию книжка у вас тысячи bucket of the на каждой валкири на каждом рипли к сети у вас будут килобайты это мета информации если вы делаете сотни тысяч баки так это будут уже мегабайты но зато чем больше у вас bucket of тем больше у вас гранулярный кластера тем быстрее каждый bucket конкретный будет переноситься потому что он будет меньше вот ну такой thread'ов получается и вам нужно выбрать что он удобнее здесь какой запас масштабируемость хотите заложить и нужно выбрать черт функцию как считать bucket айди здесь правила выбора точно такие же как при выборе шар функции для классических шарден гафни виртуальных потому что баки ты фактически это фиксированная фиксированная это как если бы мы в классическом шар денги фиксировали количество стара j правила выбора те же надо чтобы функция была равномерной он равномерно распределила выходные значения если мы сделаем это неравномерно вас bucket и будут пухнуть пухлые bucket и они ну ваш арт он оперирует бакетами он смотрит только на количество байтов на каждом стороны на размеры bucket of поэтому если вы не сбалансируете свою сарт функцию то будут проблемой но проблемы с тем что данное надо перекладывать потом ice bucket а в баке тушат функция менять то все сложно и больно поэтому выбирать надо аккуратно что по итогам получается что он дает в шарфе если вы решаете всего использовать он дает вам локальность данных когда вы кладете их в баке ты он дает вам гранулярный ли sharding атомарный на счет чего и локальность соблюдается и кластер становится более маневренным автоматический рид failover при помощи отдельного модуля в shard роутер и у вас есть огромное количество ручек которым вы можете пользоваться для контроля над багета мерефы пинай локи можете взвешивать эти баки так что угодно мы шар достаточно активно развивается прямо сейчас и есть несколько задач которые запланированы в работу какие-то даже уже начали выполнение каких-то даже уже началось первое это балансировка нагрузки на робота бывает так что нас какие-то запросы на чтения не например тяжелые я не хочу грузить мастера этими запросами пусть он ее реплики этим занимаются и тогда было бы удобно чтобы роутер мог самостоятельно запросы балансировать на разные реплики читающие ну вот и что еще log dog free перенос баки то есть алгоритм при помощи которого bucket можно не блокировать на запись даже на время переноса его придется заблокировать только в конце чтобы сам факт переноса зафиксировать это алгоритм уже есть и на считаю в реализации но там достаточно сложной большой патч поэтому еще вот в процессе и атомарные примение конфигов как я говорил когда вы меняете конфиг его нужно применить на всех сторонах самому через например звуки пир и быть не очень удобно это не атомарная что-то может не знаю быть недоступен в нем там конфликт не применится что-то доделать вот есть такая задача чтобы в шар самостоятельно распространял новый конфиг вы его на одном узле применяете этот узел во всем остальным распространены него там арно применит как-нибудь 2 пи си три пи си там каким-нибудь вот такая задача еще есть все всем за внимание вопросы мость дана мастер приходит запись только через мастер есть на базу male приходит нагрузка если вы там как на автомате как быстро схватили зависит от размеров ваших байтов если bucket этом размером килобайт или зенице мегабайт они почти мгновенно переносится если для вопросов 1 раз при нем нет это но есть не закончил отвечает на передадите потому чтобы вот если у вас единицы килобайты единицу мегабайт bucket принцесса почти мгновенно вот может быть даже за одно сообщение посетил нажмете перенесется зафиксироваться на новом месте если у вас баки десятки мегабайт или даже сотни туда он будет переноситься не знаю зависит от того какая у вас ну сколько у вас пропускная способность иди высокая может за минутой за секунды там за десятки секунд после банкета размеров сотни мегабайт это значит чтоб скорее всего было не так когда вы выбрали количество bucket of вот но эта проблема она пропадет как как как вообще как явление когда мы сделаем патч на то чтобы бахтина запись не блокировать туда размер пакета станет почти не важен этом зале можно ли менять количество пакетов процессе нет если она выбрана таким то на старте его понять по нельзя исток новый кластер создавать за доклад хотелось задать вопрос можно ли multimaster сделать допустим но есть какое-то определенное количество мастеров у них есть слои вода и внешним системам например через тот же х прокси сделать рид но рид только на один на ранда можно сделать multimaster если вы в конфиге в шар до укажите что явно что в этот под каждый из нас он может принимать запросы на запись бонус может принимать запросы на запись то есть мастер как роль останется один но запросы на запись можно сделать так что принимать будут все то есть это решает проблему bright color поэтому не решают проблему райт фил overwrite проблема райт fila вера выбрать нового мастера они бы выбрать роль нового мастера когда у вас multimaster установки уж проблема синхронизации данных между этими мастерами как конфликта там резул вид например все равно нужен какой-то главный мастер кто-нибудь решения принимали спасибо вдруг спасибо за доклад вот такая ситуация здесь вот есть заказчик есть у него аккаунт опытные заказчика есть заказ и в этом заказе есть товары с одной стороны заказчик и заказ можно разместить на с помощью bucket на одном шарди вот как бы связать так что по товару я мог тоже также заказали хотите но просто сделайте один баки то dieu товара его у товара в заказе и у заказы с клиентом по иску трех поясов 3 таблицу еще один будет баки то для заказчика один баки для сне у вас если вы хотите локальность то вам нужно их все в один пакет положите заказчика и заказ и товары этого заказа то есть технически как бы здесь не получится как бы почему потом по товару я хочу по товару найти заказы или чтобы эти законы по то по товару найти заказа не тогда наверное нет выкрутиться не получится но есть способ выкрутиться вам тут помогут вторичный индекса вторично индексы в шардже строить относительно больно и сложно но возможно спасибо бакет крутой и виртуальными неинтересно проволоки пины что это за класс задача такой который имеет был конкретный запрос на реализацию этих фич во-первых хотелось тестовые bucket и выделять так тестовые рипли кассеты то есть мы добавляем рипли кассету не какие-то данные переносицы и мы хотим на нем что-то протестировать того мы блокируем чтобы на него не приезжали новые данные не уезжали старые на нем что-то тестируется но там знают прошло тестирование метода разворачиваем это везде с пинами bucket of та же самая задача была только хотелось больше угрюмо реальность сделать это тестирование разработки тестирование приложения нет тестирование приложения которые используют и шер самого ваш орда тестирования объект не знает про соседние нет но предположим у нас есть не знаю даже можно не тестирование привести насечки это данные которые мы знаю что следующие несколько дней будут очень горячими например queen фильм вышел там знаю мы хотим чтобы он не переносил всегда был доступен на записи как люди комменты писали там знаю выкладывали кит рецензии на кончике на поиски против что он не переносил сумею за пинай на несколько дней потом раз пинами когда hyip пройдет спасибо дотла до меня такой вопрос правильно я понял что пока происходит модификация данных внутри баки той он не будет переноситься да не будем а какой тогда рекомендуемый размер для пати я думаю что идеальная единицу мегабайт он может 1020 ну короче меньше сотни желательно как я сказал это проблема оно временно когда bucket ты станет блокировался на запись это будет не важно можно будет ходить гигабайт поставить камеру тогда это будет я думаю в следующем году скорее всего вопрос про автоматически балансировку ребенка переходящий на реплика сайт сейчас у нас ситуация до что у нас только одна реплика может риты принимать которая приходит ну зависимости от веса на выбирается я у сейчас там на коленке скажем так за перила свое решение у меня раунд роббеном выбирается любая реплика из replicas это я правильно понимаю что это сейчас не надо будет скором времени так какое то решение будет чистой планировал вашей но можно делать его как-то но в целом там нет никого космического как бы союз там то там действительно просто 3 немного кода на просто если я вот услышал что есть работа в этом направлении я услышал что у вас есть задачи подумал что раз хуже сделаете то я может и правда делаем это пушкин open source я понял увидимся в чатике несколько вопросов а вопрос такой конфигурация в шар да она применяется целиком консенсусу тарантула могу ли я в одном инстансе этом по-разному разбить разных space of в шахт несколько катеров смысле например у меня один space там на 100 пакетов побить другой space на 10 пакетов это все в одном и наверно можете но я не знаю как это выбирать заранее вы просто считаете баки тагиева может получиться так что в одном стихе хоть хоть целиком один space может в один bucket лично вести того как вы считаете а за счет функциям уже сами выбираете банде так что можно как угодно можно хоть каждый из по иску в отдельный bucket положить хочет хорошо если тот другой например история то есть у мягкий the space с данными которые я хочу там сильно шарди ровать и есть какой-то space конфигурацией которая хочу чтобы на всех лестницах был один и тот же и просто как реплика тогда его нужно не сортировать вам нужно баки тайги у него просто не задавать то есть на одном на одном из такси можно хранить и реагируем ее данные не шарниру и когда вы сдаете bucket aedes по ису той становится флагом для вашего до того что это space надо сортировать он его берет в обработку регулировщика my горбач коллекторами всякими менеджерами recovery если вы bucket айди не задали тон это с по из как будто и не видит один шанс тарантула может находиться в нескольких реплика сетов для нескольких не только в одном ну вот то есть я смотрю the instance конфигурации которым я должен реплицироваться и быть везде одинаковым импорт руками разложить на все рипли кассеты в каждом рипли к сети а он уже реплицирует сам туда понятно но если получится автоматически не покажу так хорошо и второй вопрос а вся вот эта история с переносом баки там как она соотносится со snapshot а ми-и-ик слугами если у меня огромное количество x логов это повлиять на производительность привело собаки тов если вы используют до нее не миг влияет экологии влияют только на recovery то есть он даже не читаемых слов если используете движок который в память он читает из памяти сочетаете движок который читает диск а то он будет читать со своих дисков со своих со своих файлов не секс логов него свое там чему-то зелени слоги не мигрируют они номера собранный но когда мы на новом месте будем данные применять там просто свои услуги создадут они не переносится как файлы в одном из слуг файле может много данных лежать разных букетов как мы его будем переносить понятно спасибо спасибо за доклад вопрос такой а я вот не очень понял а как масштабируется роутер вот который запросы от кавалеров можно просто сколько угодно создавать неважно насколько старо j грудь и независимо масштабирует а как они между собой согласует душа горит никогда вам не коммуницируют поэтому роутеров можно не создавать несколько можно свой написать вовсе вот один человек не знаю написал в чате а можно создать их сколько угодно и они не как друг на друга не влияют на то есть пока если что-то реплика там реплика уходит реплика приходит то круг говоря локальный роутер допустим об этом знает а в другой локации может не знать он узнают об этом постепенно понятно хорошо спасибо"
}