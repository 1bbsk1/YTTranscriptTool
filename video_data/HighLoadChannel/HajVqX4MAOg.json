{
  "video_id": "HajVqX4MAOg",
  "channel": "HighLoadChannel",
  "title": "Устройство репликации в in-memory базе данных Reindexer / Олег Герасимов",
  "views": 654,
  "duration": 2004,
  "published": "2020-04-14T11:04:16-07:00",
  "text": "как уже сказали меня зовут али герасимов я работаю в компании ростелеком информационная технология директором центра компетенций значит мы разрабатываем и memory базу данных хари индексе ртс так сказать как один из наших проектов и сегодня я расскажу про детали устройства репликации в наши дни мари базе данных аренды ксир но собственно перед тем как начать рассказывать про репликацию наверно стоит рассказать про то что такое индексе яндекса это и memory база данных с богатым функционалом по выборкам она умеет делать joy номер полнотекстовый поиск поэтому помощью работать memory очень быстро значит где используется яндексе собственно ренди ксир появился когда мы разрабатывали бы тем для системы интерактивного телевидения ростелеком венка wink это сервис который предоставляет пользователям собственно онлайн онлайн-кинотеатр плюс телеканалы плюс телепередачи значит география сервиса от калининграда до дальнего востока то есть практически вся россия и при этом у сервиса примерно несколько миллионов пользователей которые одновременно использует сервис соответственно если говорить про контент то это у нас телеканалы это у нас телепередача то есть при матери передач и это вот контент если говорить про собственно географию то как я уже сказала сервис по всей стране работает и чтобы пользователь получили хороший юзер экспириенс необходимо обеспечить минимальный latency по ответу от серго клиента соответственно поэтому наша серверная инфраструктура расположен также по всей стране чтобы максимально приблизить точку отдача контента к клиенту соответственно если говорить про данные то данные количественно это 2000 телеканалов это 5 миллионов телепередачи собственно за месяц хранится у нас и примерно 50 тысяч фильмов и телесериалов если говорить про берем это примерно от 5 до 10 гигабайт данных то есть достаточно современным меркам но с другой стороны нам необходимо эти данные доставить до каждой ноты которые географически разбросанных по всей стране так как но для обеспечения того самого минимального лет мне нужно чтобы у каждой но до был свой кэш данных с этими всеми данными чтобы радовать локально поэтому мы в начале разработки системы вот придумали такое data flow который собственно использовался для того чтобы доставлять контент do not по всей стране значит есть некая система content management system ну скажем так это упрощение в реальности там много разных информационных систем которые нам предоставляют данные собственно допустим телепередачи мы импортируем у поставщиков данные про фильмы и про телеканала готовит наш контент редакция вот но в данном случае не столь важно важно что есть некий способ импорта всех этих данных для нашей системы значит импортом данных нашей системе занимается сервис 1 написанный нога это сервис обмена пи он получает данные из внешних систем получает 3 по расписанию там кита данные чаще это реже и записывает из после сна записывает естественно неполным ребра этом каждый раз а там как тур на вычисляет div вот и одновременно с записью данных пост грессов отправляет но дефекации через в очередь и наскребу о том что изменились такие-то таблички значит теперь что происходит на но до которой расположена по стороне значит они получив нотификацию и занозку идут в пост без которой находится в кори и выкачивают данные соответственно силами apple конечно происходит выкачивание данных и спас криса и соответственно запись их в яндексе свой memory кэш вот такая схема собственно было самом начале а какие проблемы этой схемы но очевидно что здесь будет большая нагрузка на пост при обновлении данных потому что только все но на считают из очерки сообщение события о том что прежде новые данные они все радостно кинутся качать данные и спас греха и соответствует него большую нагрузку по выборкам другая проблема менее очевидно это долгий транзакции так как но разнесенных географически то между нодами и центр может быть плохая связность медленный канал большой летать и как следствие будут долгие транзакции которые общем-то тоже по лесу хорошо не делают ну и другая проблема уже более архитектурно концептуальное что сложно поддерживать репликацию на уровне приложения по хорошим приложений должна заниматься не репликации доставкой данных а должно заниматься реализации бизнес логики вот такие мы в этой схеме видели проблема соответственно решением было данном месте импортировать данные сразу в индекс ртс мину ip-адрес тут важное замечание что эти данные для вот собственно наш системы when booking не столь важны потому что всегда есть возможность их заново импортировать с контентом измена системы поэтому здесь импорт данных френды ксир не является чем-то критичным вот и реплицировать все данные по всем но там уже силами яндексе рама для этого нам пришлось бы сделать репликацию в яндексе а вот у нас теперь такая схема значит есть тот же самый сервис обмена этой который точно также импортировать данные на место под gresso он импортирую данным же сразу в яндексе и соответственно далее яндексе своими средствами рассылает обновления на все ноды вот апликэйшен который я тебе фронта и 5 и этапе уже в процессе репликации не участвует то есть аппликации доставка данных по поляне но тут полностью унесена в яндексе собственно какой-то профит но самый понятный и простой то что при конечно оставить только бизнес-логику дальше получили не единственный универсальный инструмент доставки данных на все поля мы при этом эксплуатацию за ним нужно следить за десятью компонентами которые отвечали за в репликация по сути достаточно смотреть чтобы не разъезжалась репликация в яндексе этого будет достаточно вот ну естественно сгрузили позарез так как все эти а конструкцию же неким образом не задействует позы с все это реализовано сил мере яндексе ра теперь поговорим про реализацию репликации то есть на что мы заменили вот нашу конструкцию со москве ip-адресом когда мы теперь доставляются между станциями яндексе равно как бы если говорить про рипли к цию то все механизма они уже придуманы давным-давно до нас и один из наиболее стандартных способов репликации это репликация при помощи в рай the heat logo смысл его достаточно простой значит мастер базы данных все транзакции и все стоит монтана запись пишет в рай the heat look в районе только как правило кольцевой буфер находит количество записей но там порядка 1000000 может быть 10 миллионов зависимость от конкретного к потребностей и соответственно клиенты слоем а когда подключаются к мастеру они вычитывают изменения is right хит logo при этом есть такая важная сущность как лог sequins на мир это монотон увеличиваете счетчик записи в рай таки блоги и поэтому каждый slave знает на каком лобзик in store остановилась его репликация в последний раз и прежде чем противник мастеру вычитывается своего последнего локусе константа вот эта схема достаточно стандартная она хорошая рабочая вот но у нее есть понятные недостатки первое это то что если закуп слева был давно отключена от мастера может получиться так что вол мастера станет аудите для слива и уже в в рай хитоне не будет необходимых данных чтобы слив могут догнаться да мастер это первая проблема вторая проблема схему straight fit логом в том что оно требует теперь у на мастером то есть к приходит слэш говорит мастер мастер дай мне а там такую фигурку из рая предлагает требует кого-то процессинга на мастере требует выборки из индексов требует стерилизации данных перепаковки то есть это сыпью мастера что не очень хорошо вот это одна схема другая схема это онлай statement стрим суть этой схемы она больше напоминает работу история с publishers of скрайбер то есть в данной конструкции сабскрайберами выступают слева а мастер выступает publisher а вот работа над следующим образом когда приходит клиент на мастер он выполняет культа операцию мастер в этот момент вид как у него есть подключены если вы и рассылает апдейт на всех слева соответственно просто практически перекидывая бинарный пакетик от клиента на всех подписанных сливов плюс этой схемы в том что оно практически не требует никакого семью от мастера вот та мастер работает как банальный роутер пакетов то есть теперь у мастера никоим образом не действует и ограничение количества слов здесь у нас только по пропускной способности сети то есть есть нас на машине мастер большая сеть мы можем подключить практически ограниченное количество слоев такие плюсы этой схемы вот и собственно 3 схема в валберг схема на случай когда у нас была та ситуация когда за обиделся волк вот и slave уже реплика не может восстановить свою таблицу исходя из записи его в райских лоде а вот в этом случае используется схема полная синхронизация вот которая работает следующим образом поняли что возникла ошибка у нас за убедился в раке так мы идем на мастер просим у мастера все данные про структуру таблицы про яндекс эта блесна мира данной таблице далее создаем временную табличку и в нее медленно медленно выкачиваем все данные с мастера кусок из данные из мастера вы качались мы атомарном переименовываем временную табличку в существующие по сути этом репликация завершается вот надо понимать что этот процесс очень depay'a активные он и на мастере на слайдах и употребляется семью и к нему яндексе переходит только в случае когда у него нет никаких других возможностей использовать другие более эффективные способы репликации теперь проблемные места того что использует все три схемы сразу значит как бы если бы была только одна схему проблема никаких бы не было но она была бы не эффективна во всех случаях поэтому при переходе с одного способа репликация другой возникают проблемы основная проблема это что делать когда два потока идут одновременно и они синхронизованы то есть например ещё не закончился выгрузка данных извратит logo а уже на стрим событий вот какие возможные решения этой проблемы но первое решение но самое очевидное просто отказаться от стрима и использовать дефекации только как флажок того что то изменилось то есть по сути отказаться от клика местности операции на мастере всегда на своих братья перечитывать в райт хит log ну а история такси потому что она загрузит secu на мастере приведет к уменьшению пропускной способности по количеству слоев вот следующая попытка так сказать разрулить эту проблему это чтобы свои выслали субскрайб на поток событий после завершения организации право right ahead logo ну чтобы не было конфликта мы предположили давайте мы попробуем вначале выключить весь прайд хит лак а потом уже подписаться на события чтобы не было так сказать между ними конфликта оказалась эта схема также содержат проблему и рейс если в момент когда slave еще не закончила учитывать этлок пришел на клиент на мастер и сделку это апдейт мы получим запись который не учтем не врать итоге потому что эти не мастер он зависит от нет лака дал с точки зрения субскрайб ещё не подписался и поэтому эта запись просто пропадет и в итоге мы получим не консистентные данные на мастер и славе чтоб недопустимо поэтому я схема была отброшена другая попытка разрулить эту проблему одновременно читать и стрим событий сразу же подписавшись на события и одновременно учитывать в right it looks so мастера вот эта схема она так сказать им чуть более уст на говоря надежная но миссис тем она точно также содержит аналогичную проблему с рисом если в момент пока еще след не дочитал обратит logo из-за мастера приходит на мастер транзакция записью то на слайде будет неправильный ordering записи избирает кит логово здесь на схеме показано значит если вначале так вот значит вот здесь подсвечиваю проблемное место значит когда приходит в рай цветок запись с номером 123 в этот момент она тут же прилетает на слив с темой сам номер 123 а потом только потом завершается отдача бронхит logo с номером 122 и как следствие sleeve вначале применит запись 123 потом 122 что также не приемлем может привести к разрушению консистентной стида на он например если один из них был определит если 5 местами том получим отсутствие записи на сливе она мастер и типа будет запись то есть вот некорректное поведение значит в итоге родилось такое решение которое работает значит слои все равно сразу подписывается на стрим событий от мастера но при этом при этом если в процессе получения данных избрать хит logo прилетают события то они по факту игнорируются но при этом запоминается что были модификации и стрима и по завершению синхронизации и звол соответственно повторяется повторный запрос волк в котором учитываются все новые изменения и так до тех пор пока в процессе получения волне будет рейса photo stream а вот такая схема она собственно позволило совместите переход с оффлайн и репликации повредит лагуна online stream и при этом и сделки бесшовный то есть без потери данных и без ауре ордер ринга записи в в slave вот это собственно одна из проблем которая была которую мы решили другая не то что проблема но так сказать задача это какой-то оптимизация использования м для хранения в рай кит logo дело в том что мы говорим про in-memory базу данных поэтому у нас памяти дорогой ресурс который база должна использовать очень эффективны очень бережно это одно водное другое водное что на самом деле процентов 90 врат нет logo это ду были той же самой таблицы с данными по сути в районе ток просто содержит три принц на какие-то конкретные записи в таблице то есть что можно сделать с этими водными значит соответственно в райских лоде для записи об изменения данных можно хранить просто индекс строчке в таблице то четыре или восемь байт зависимость от архитектуры для записи удаление строчки уже так легко не получится потому что строчка жить физически нету и хранится в рай тетради только праймари кей самой записи которая была удалена тут уже зависит от конкретных данных которые хранятся в таблице но если в таблице праймари кей это им то это будет четыре байта если 648 вот и так далее вот плюс отдельной строкой идут записи a statement то есть если выполнялись кита сквере statement и меняющий много записи на типа там апдейт условиям или делиться условиям то сохраняется непосредственно весь пятно в исходном виде и он хранится как некая бинарная строчка размера мужа в зависимости от statement но здесь важное замечание что 90 процентов записей в раке коде это записи в звеньях данных поэтому два нижних случае они скорее редкие но тем ни менее существующие кейса код теперь про структуры которые используются для хранения этих данных уже час опускаемся на уровень реализации на си си плюс плюс значит мы используем собственные класса и еще вектор это скажем так с одной стороны аналог обычного есть и директора в другой стороны в нем есть мост string оптимизация которая позволяет часть данных хранить внутри самого контейнера без выделения дополнительной памяти на куче вот сам white heat look это есть ни что иное кольцевой буфер рисованный на там под капотом хранящейся в типа обычным векторе давайте расскажу про как устроена смол string оптимизация в классе и вектор значит сам по себе этот класс у такой конфигурации занимает 16 гбайт значит у него может быть два состояния первое когда в нем хранится менее 12 байт данных в этом случае у него включен флажок это старший бит последнего слова в единичку и тогда младше 12 байт и самого контейнера и то есть просто пылал для хранения данных вот случай когда он хранит больше 12 байт данных то он преобразуется в объект похоже на обычный директор в котором есть pointer есть capacity есть сайт то есть обычный гендер по сути а вот но при этом естественно pointer указывает на выделенные данные на куче он уже выпил какие дополнительные данные на куча вот но в нашем случае мы оперируем пойло у доме в 90 процентах случаев меньше чем 12 байт поэтому все случаев у нас дополнительного выделения на куче не происходит теперь собственно сама структуру которая хранится 3 этого pilauda каждой записи вол ну она достаточно тривиальна и это собственно один байт типа вот описывает что за запись скажем изменения в таблице ли удаление ли statement вот и дальше union в war and of war in the worst ринг это пришедший из google просто бабу в сущности общем они достаточно удобно позволяют занимать под sla численные типы и под строчки ровно столько байтов сколько нужно для этого они используют кодировку b120 8 достаточно стандартная история думаю про неё не стоит подробно рассказывать с ними мне кажется можно ознакомиться с интернетом и достаточно распространенные типы данных и того какой в итоге получили просят профит следующий по памяти мы на весь проект ток занимаем линейный кусочек памяти 16 мегабайт без всякой фрагментации то есть просто вот ленина 16 мегабайт тур один раз выделены дальше без перевала козы без перемещения до сюда весь край три топа занимает 16 мегабайт вот и при этом нет никакой фрагментации куча 100 как но это один кусок профит поясе пью значит так как братья тла граница кольцевой буфер в котором лобзик винтом монотонно возрастает то мы получаем константные сложности как для выборки так и для запись то есть домом случае это по сути получить остаток от деления лобзик инсайдера на размер брать из лука и получаем тоже пойду в котором можем уже добывать данные вот это реально получил очень дорого очень хорошая оптимизация на диске волк граница также комбинированном виде значит записи прострочки они соединены сами данные сами строчками просто каждой строчке добавлено 8 байт лак sequins номера которые хранятся прямо в самом объекте записи вот а записи об удалении изменения индексов о модификации метаданных и прочих прочих вещах которые не падают в записи обычные строчки они хранятся отдельно мы немного поэтому они собственно не сильно увеличивают объем значит при запуске сервиса рендерер вот волос такого виртуального формата на диске преобразуется физический формат который я показывал на предыдущем форматируем в такой формат вот и дальше уже работать полностью is in memoriam так это что касается хранения в райт хит logo еще одна интересная задача которую мы решали это разработка сетевого протокола между мастером и с клеймом ну даже он более широкий это протокол между клиентом и серым то есть клиенты работающий сером линуксе работа такому же протокола протокол асинхронный что это означает это означает что клиент может отправить на сервер там достаточно большое количество запросов не дожидаясь ответа сервера вот и получает ответа cr по мере их выполнения для идентификации так сказать от соотношения ответа и запросы используется монотон увеличивать sequence number почти кредитам бирон идентифицирует как бы и команду которая провела клиенты соответственно ответ который получил сер вот собственно здесь на картинке нарисована что они могут приходить и в разном порядке никак не опираясь на реальной потребность записи которая происходила на клиенте теперь если говорить про реализацию от реализации у нас это си плюс плюс программа многопоточная то соответственно должны реализовать такого клиента который будет принимать запросы из многих потоком с одной стороны другой стороны все это записать в одно собственное соединение получать ответ от сервера инфицировать то место которое вызвало какой-то запрос клиента о том что пришел этот сервер а вот под капотом клиентам ответа ждут в комп личностный фактор то есть по сути когда кто-то просит клиентка и действия мир дают аргументы например там если такой то запросто это из kweli запрос и плюс указатель на сильный фактор который клиент ожидает получить ответ от сердца что же произошло то есть вот такая вот модель используется соответственно если в реализацию то может быть наивной реализацию который вначале пробовали сделать значит она основана на х0 дарит map ключ sequence number то есть кот команда которой был отправлен и соответственно комп лично это тот самый functor который вызывается при получении ответа от сервера а вот но так у нас вся история многопоточная то естественно эту мэтт необходимо было вернуть beautix вот причем на каждом этапе что на записи команд в сервер что получение ответов от сера необходима именно в рай блог потому что она требует модификацию состоянии данных в мы вот минусы такого подхода они видны на поверхности вот она плохо работать нога по точке потому что есть бутылочное горлышко с миксом которые по каждому типу хватается umek это первая очевидная проблема 2 менее очевидная проблема это то что каждая отправка командой каждое получение ответа от сервера и дополнительная локация либо освобождения памяти а вот о худшем случае это еще и регулирования нардеп если возникает коллизия вот такая реализация в итоге нам не понравилось мы от них отказались местные реализовав собственный лоб френ ринга буфер в котором нет таких недостатков которые были умны для реализации с портретами значит оно устроено на собственно базе ринг буфера на базе вектором в котором хранятся записи в таком формате значит для добавления новой записи по сути клиент атомарном захватывает флажочек used просто тамар наставит true после чего записывает его свой sequence number вот она да все сказать что индекс векторе это соответственно есть остаток отделение sequins она длина прицела буфер то есть одна знаете вычисляется вот после чего разруливаются коллизии на базе переменные next случае если например клиент отправили два запроса с номером 101 и 102 а длина пальцевого буфер расскажем что то эти 2 секунд снайдер потом и чайку с индексом 1 вот и чтобы такие коллизии разрулить здесь используется связанный список на базе от механик поттеров которые точно также многопоточных хорошо работают и требуют оппонентах log то есть в итоге мы получили по сути аналог 100 дендрит небо плюс beautix реализованный на кольцевом буфере и atomic ах который не требует блоков не на чтение не на запись и при этом соответственно обеспечивает 3 ценность своей работы по разводам бич марков нога по точкам честно лучше работает чем предыдущие реализация сама джем так что вот такой подход нам понравился мы планируем дальше в использовать в других задачах где требуется а вот реализация такой lock free очереди какие плюсы и минусы ну про плюс я уже вас там сказал это нет никаких блокировок вот нет документации памятник случай smeb собственно минусы но они также достаточно понятны это более сложный код потому что одно дело стандартный код тура делает insert или david из map и другое дело код который работает lock free структуры он уже необходим он уже работает с atomic с их атомарного захватом освобождением он уже требует так скачи больше сложности при разработке и отладке на этом про особенности реализации я бы хотел завершить и перейти к собственно в нашим планом развития то есть вообще сам все репликацию на достаточно молодой механизмом появился чуть более чем полгода назад вот и сейчас активно как улучшается так и дорабатывается к отлаживать мы до сих пор ним находим какие-то баги понимаем какие-то упущения в плане скажем там средств диагностики перемещаться по итогам вводу в эксплуатацию видели ряд проблем по эксплуатации что у нас не хватает средств мониторинга средства диагностики проблем и планируем улучшить так чтобы у своего было более расширенная информация о том все ли хорошо с точки зрения синдром насти данных и состояния также мы планируем сделать репликацию мастер своих слов это когда как бы слоев морс одной стороны быть мастером для ведомых ему став с другой стороны получать данные от другого мастера для реализации иерархической репликации в случае когда необходимо скажем сделать большую кластерную систему также мы планируем сделать реализацию репликации мастер мастер вот и собственно это пока вот наши основные планы развития по репликация собственно это у меня все еще раз напоминаю что рендерер это open source проект который доступен на git hop которого можете присоединиться как как пользователи так контрибьютором вот так же у нас есть телеграм-чате который можете зайти написать этот вопрос нам по отвечаем это у меня всем большое спасибо и до ваших вопросов да спасибо большое за доклад если он не сложно можете пожалуйста вернуться поводу 24 слайд финальные схема репликации так если правильно понял здесь твой механизм работает stream где как бы мастер посылает своего его убить и своим самым вытягивает из мастера у час мы все данные перед вами почти два раза ну скажем так в момент когда идет инициализация то получается что тогда идет дубль как стрима так и волк так до тех пор пока собственно он не синхронизируется тогда уже данной будьте только раз в стриме вала больше уже не будет можете да пожалуйста еще раз пояснить что мы таким образом достигаем зачем нам именно вот два этих механизм чем у одной не хватает а значит с одной стороны стрим от механизм онлайн то есть он работает пока есть онлайн связь есть она связи между мастером и словам нету то данные могут пропасть вот собственно это хорошо видно на на вот этом слайде . на этом слайде вот здесь видно если мы у нас нету связи то момент перехода у нас может пропасть данные которые прилетели в онлайн-режиме код поэтому онлайн режим он нужен как наиболее собственно легковесный механизм с другой стороны нужен механизм строитель флагом как механизм обеспечивающие восстановление после потери связи то есть как бы один онлайн но легковесной другой на оффлайн но более тяжелый то есть поэтому 2 механизмов спасибо большое спасибо за доклад сказать по рингу буфер реализацию она работает для какого случае для многих продюсеров многих консью мегаф или только один продюсер один консьюмер и сколько там это монах операцию понять за одну операцию пуш-ап речь идет про клиента стива вода так понимаю значит там консьюмер один продюсеров может быть много то есть клиентов много а сам он соединение кроется в отдельном потоке вот соус к томатам андах операции точно не скажу но кому двери 3 сейчас точно поверх не вспомним спасибо здравствуйте здесь спасибо вам большое вообще за то что вы делаете яндексе рах это сказать что им пользуется это классный продукт и классную вас чат и классно вы поддерживаете там отвечайте на вопросы но вот репликации пока боялись пользоваться и хотелось бы уточнить а в данном случае вот master и slave вот они существуют как в случае падения мастера slave станет мастером это нужно делать руками сейчас на данный момент то есть нет автоматического перевыборам мастера ну во первых спасибо за теплые слова действительно стараемся чтобы продукт не просто был на сайт человеком и не так же им пользовалась по существу да то есть пока механизм автоматического перевыборам мастера не то часто требуется делать руками вот вы будете его делать или это мастер масса репликация решит проблем ну это скорее будет после мастер мастер то есть мастер мастер эту проблему скорее не режет мастера мастера для кейс когда в две точки входа пьется данные вот а переключения между мастерами на автомате мы скорее будем делать уже дальнейших планах спасибо еще можно маленькое пожелание чтобы у вас немножко документация поспевала за фичами а то иногда только изучать и можно выяснить себя до актуально и позвонил внутренне разработчики точно также пинаю что молчи ватин написали кучу фича не документировали да такая проблема есть знаем будем стараться исправить вопросы ну хорошо если вдруг возникнут то вы можете задать дискуссионной зоне олег спасибо вам большое это наш выберите пожалуйста лучше вопрос был наверно самый последний с такими пожелания места не могу просто не взять алаверды так сказать спасибо спасибо"
}