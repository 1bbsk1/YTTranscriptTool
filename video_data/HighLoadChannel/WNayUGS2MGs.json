{
  "video_id": "WNayUGS2MGs",
  "channel": "HighLoadChannel",
  "title": "Простая и дешёвая бизнес-аналитика на базе Google BigQuery / Алексей Паршуков (DocDoc)",
  "views": 7901,
  "duration": 2765,
  "published": "2018-01-16T12:20:40-08:00",
  "text": "Я работаю в компании dogdog и сегодня хочу вам рассказать как мы построили у себя систему бизнес-аналитики И попытаюсь доказать что это был один из наверное самых дешёвых способов И для кого мой доклад ну прежде всего для менеджеров которые устали Наверное от стандартных отчётов Google аналитики в Экселе им хочется чего-то большего И я покажу им Как вообще можно это делать и немножечко для разработчиков которые ищут какие-то новые способы по обработке больших данных Я думаю вам тоже этот доклад будет полезен и для начала у меня такой небольшой опрос кто вообще относится к разработке из вас отлично А кто относится к менеджменту Ну тоже неплохо вам прежде всего будет интересно а Итак немножко продук док это сервис который позволяет записываться в коммерческие клиники Москвы на текущий момент мы работаем в 14 городах городах России наша платформа состоит из сайта мобильного приложения и довольно широкой партнёрской сети и чтобы понимать Какой объём мы обрабатываем это около 50.000 записей каждый месяц я работаю там техническим директором и отвечаю за технологии а с самого начала существования нашей компании мы верили что создание какого-то нового функционала это не какое-то тайное знание какого-то оде нного супер гениального человека который знает как и что нужно правильно делать на проекте А это работа от данных от пользователя и от того что им нужно то есть если мы делаем новую фишку это данные должны нам сказать что эта фишка будет оправдана что она принесёт деньги что она будет нужна пользователям если эти этой информации в данных нет то надо собрать эти данные и только тогда можно сделать какие-то правильные выводы о том куда стоит двигать продукт И с чего Мы начинали начинали наверно как и все мы поставили Google аналитику настроили в ней стандартные метрики сделали в ней стандартные отчёты и в принципе это та штука которая достаточно просто и дёшево позволяет ответить на очень многие вопросы Она позволяет померить конверсию Она позволяет следить за тем как изменения которые вы вносите на сайт а влияют на развитие продукта и если вы до сих пор не настроили а какой-то такой простой похожий инструмент у себя то думать о чём-то более серьёзным А в бизнес-анализ наверное рано потому что аналитика - это просто быстро это такой базис на котором можно строить всё остальное А А после того как мы э поставили в себя аналитику и получили эти самые отчёты мы быстро поняли что на все вопросы там ответить нельзя Ну вот у меня есть пару примеров прежде всего например стоимость обработки заявки Ну в аналитики этих данных нету И такой отчёт там в принципе построить нельзя или вот второй пример - это построить воронку от например появление пользователя на сайт до того момента как деньги упали на расчётный счёт в док доке например очень много активност происходит за пределами слайда это оффлайн активности связанные с обработкой заявок по телефону и э информация Ну в принципе нет Google аналитики и построить такую воронку там было невозможно и мы стали думать как можно было бы решать такие проблемы аналитика и собственно настраивал нам Google аналитику и спросили у него А можешь ли ты нам что-нибудь предложить для того чтобы решить вот эти проблемы его решение было достаточно стандартное оно описывается во всех учебниках по ве аналитике надо просто больше данных выгружать в Google аналитику и мы попробовали и наши отчёты стали стали обширнее они стали более информативные и кое-какие проблемы мы начали закрывать но в принципе далеко не продвинулись почему потому что Google это инструмент который а создан для анализа вполне определённых вещей а он оперирует такими сущностями как пользователь посещение какое-то событие та же самая сессия и за пределы этих отчётов выходит крайне неохотно то есть всё это очень сложно Кроме того в Google аналитике существует проблема с сэмплирования данных а он не работает с конкретными пользователями он работает с их усреднённый значениями он может немножечко ошибаться А главное невозможно проследить поведение какого-то конкретного пользователя Некоторые из этих проблем устраняет например платная подписка на Google аналитика она называется Google аналитика 360 Мы тоже рассматривали для себя такой вариант в нашем бы случае это стоило примерно 20.000 долларов в год и эта штука решает некоторые проблемы Ну например она убирает проблему сэмплирование данных она убирает некоторые ограничения по отправке дополнительных данных в событи но принципиально другим от этого инструмент не становится это всё те же самые пользователи всё те же самые pvw и клики А И мы пошли дальше И что мы сделали дальше пришли к менеджерам менеджеры - это такие люди у которых восьмидесятый дан по пользованию экселем они в принципе могут решить любую проблему Ну то есть как Как это работает надо надр информации из разных источников немножечко из телефонии немножечко там из бухгалтерии немножечко из этой же самой Google аналитики вставить это всё с таблицы построить какие-то выборки оба У нас есть красивый график о том что происходит на сайте э мы пользовались наверное несколько лет Такими отчётами они позволяют действительно решать эти проблемы но у них есть ряд недостатков Ну во-первых это дико дорого и долго потому что человеческий труд оплачивается человек очень медленный А если мы хотим увидеть такой отчёт пон покажет Ну по крайней мере ситуацию на вчера и там вот прямо непосредственно в эту секунду что происходит Т на сайте оттуда Ну никак невозможно получить а потом это обработка больших данных всё-таки если мы хотим проанализировать какой-нибудь показ партнёрских виджетов где у нас сотни миллионов событий в Экселе этого сделать в принципе невозможно поэтому решает какие-то проблемы но опять же нет и мы думаем дальше и дальше мы пошли в наш технический отдел к нашей разработке и спросили что могут предложить они в плане разработки мы прошли тоже довольно стандартный путь для многих компаний и первое что мы сделали Мы попытались выгрузить вот эти самые дополнительные данные в нашу основную базу данных в базу данных приложения для того чтобы она могла отвечать на те вопросы которые мы им ставим и очень быстро мы столкнулись с проблемой которая заключается в следующем А база данных приложения Создана для того чтобы обрабатывать запросы наших пользователей быстро эффективно понятно И главное что она должна обеспечивать какую-то масштабируемость для решения чтобы разработчикам с ней было удобно работать если мы грузим туда 200-300 дополнительных таблиц с гигабайтами данных которые к работе приложения никак не относятся ни самому приложению ни разработчикам а такой идеи В общем проще не будет И мы тоже очень быстро поняли что это Тупиковый вариант и следующее решение было достаточно простым и понятным но давайте мы возьмём наш текущий сервер поставим его куда-нибудь рядышком и загрузим туда данные только по аналитике таким образом у нас будет чистая база данных приложения и отдель сервер под аналитику который никак на приложение не влияет а сделали загрузили и тоже получили ряд проблем но опять же как отчёты можно было строить в результате мы их получили но база данных которые используются под приложени например mys pog там Male они не меряют немножечко другую специфику работы и направлено на решение других задач например для того чтобы построить там какой-нибудь график по посещения если у нас сотни миллионов событий нам Для начала нужно специальным образом построить ключи а ключей будет мало надо будет построить агрегаты А и таким образом создание любого нового отчёта превратилось в создание новой дополнительной задачи на разработку которой несколько часов что-то там пилили делали какую-то магию оптимизировали запросы и оба у нас получился отчёт А честно говоря это не очень быстро не очень гибко и достаточно Дорого потому что разработчикам надо платить и мы пользовались этим решением пользовались Достаточно долго но всё равно было мало И мы думали Куда пойти дальше следующее что мы сделали это вышли на рынок и стали общаться с компаниями которые у себя решали какие-то решали эти вопросы какими-то другими способами Ну для того чтобы понять А какие вообще решения существуют и первое что мы встретили что нам рекомендовали это коробочные мощные решения называют их лоп кубами которые позволяют строить Вот Ту самую аналитику А и они действительно позволяют её строить у многих компаний это работает но с нашей точки зрения А у них есть ряд недостатков Ну во-первых это дико дорого такие это коробочные большие энтерпрайз решения которые стоят много денег потом к ним нужно купить специального человека который в этой системе разбирается а потом нужно ещё купить желез где это всё развернуть полгода готовить данные и через полгода времени и больших большого такого больших капитальных вложений мы наконец-то получим какой-то результат поскольку мы компания молодая маленькая стесненные в средствах риски внедрения вот как бы риски внедрения такой системы они казались Мне настолько громадными что мы даже не стали пытаться и ещё одно решение которые можно встретить очень часто на рынке это инструменты для визуализации данных они часто используется именно для бизнес-анализа вот тут есть два примера которые мы использовали это табло им и в принципе они позволяют сделать Очень клёвые очень качественные красивые отчёты но опять же с моей точки зрения два ограничения Есть с которыми мы столкнулись во-первых это не самые дешёвый инструменты это тысячи долларов на лицензии и вторая история когда вот эта задача Когда нам нужно нагать из разных информационных систем данных и создать такой большой единый аналитический отчёт в этих инструментах решается крайне плохо то есть где-то со скрипом где-то вообще не решается мы несколько месяцев потратили на то чтобы создать такие отчёты и начать ими пользоваться потому что мне казалось что это действительно Вот то не получилось не получилось именно потому что очень тяжело работать с разными источниками данных а и в какой-то момент я сходил на очередную конференцию по бизнес аналитике и для меня дошла одна очень простая мысль А она простая потому как она звучит но очень сложная потому как до неё дойти потому что мне чтобы понять эту историю пришлось пройти вот этот длинный путь который был длиной там в несколько лет все данные для анализа надо сложить в одну базу только в том случае когда мы все информационные системы и все данные которые там есть смогли объединить в одной структуре только в этом случае а создание бизнес отчётности будет очень простой лёгкой задачей только в этом случае её получается решать за какой-то приемлемое время и она не ломает мозг и все инструменты которые умеют там работать и визуализировать данные они сразу очень легко просто и быстро э интегрируется в эту инфраструктуру и когда мы поняли для себя а собственно вот этот правильный путь и как надо по нему идти а первое что мы сделали это задумались какой вообще Нам нужен Data Stage для того чтобы все эти данные собирать здесь у меня на слайде кое-какие требования которые мы на тот момент сложили для вот этой системы базы данных куда мы хотели влить это данные Ну для компании dogdog Прежде всего это объём данных с которыми мы оперировали он где-то примерно должен составлять был около 1 терабайта во-вторых нам хотелось чтобы база работала Как можно с более рыми данными то есть вот эти свистопляска с построением агрегаторов с вычислением индексов с оптимизацией скель запросов Мы через это были не готовы снова проходить потом нам всё всё ещё нужен был esql потому что esql обеспечивает гибкость вы никогда не знаете наперёд Какие срезы вам нужны и некоторые инструменты например не позволяют пользоваться SQL То есть это может быть какой-то полуграции которые вложили в интерфейс и нельзя выйти за пределы этих функций поэтому это обязательное требование было для нас Кроме того нам хотелось чтобы эта база давала выхлоп то есть между тем временем когда произошло событие и тем моментом когда мы получили это на графике в отчёте должны пройти минуты ни в коем случае не часы дни или недели Как это работает в некоторых больших энтерпрайз решениях и такой немаловажный но очень значительный фактор нам хотелось чтобы это было дёшево или бесплатно и Нам очень не хотелось вливать много денег вот для того чтобы построить такую инфо систему очень сильно хотелось снизить риски потому что в конце концов может не полететь А и что мы сразу откинули ну во-первых мы откинули все стандартные базы данных Я уже описывал Почему у нас с ними не получилось завести мы откинули но Scale потому что там в принципе эти задачи можно решать но нет вот этой вот гибкости и мощности которая даёт SQL нае языки Mon db тоже не про то потому что ну потому что не про то в принципе я видел системы которые строятся на решениях типа дупа и некоторые компании умудряются на них анализировать данные строить какую-то отчётность мне не понравилось очень высокий порог входа очень высокий пород входа необходимость покупать инфраструктуру там поднимать этот кластер конфигурировать всё это люди которые должны в этом разбираться всё это совсем не бесплатно Ну и поисковые движки они в принципе позволяют строить какую-то отчётность выбирать более-менее ну такие таблицы но очень сильно страдает гибкость то есть но Если нужен новый срез нужно опять есть поисковый движок что-то там поднадкостничный Стоимость это вообще ни разу дешёвое решение если что-то из он сорса это конечно кликхаус кликхаус офигенная база но имеет с моей точки зрения все те же самые ограничения это то что нужен специальный человек который её поставит настроит А ей нужно железо а может не полететь А я очень не люблю капитальные вложения А поэтому мы стали смотреть облачные решения их тоже Дофига я здесь привёл только два примера на самом деле их э невероятное множество они отлично справляются с задачами бизнес-аналитики Ну и по сравнению с Stone решениями у них есть откровенные преимущества это в том что их нельзя Не надо отменить самые распространённые из них это конечно то что предлагает Amazon и Google для Amazon это Amazon для Гугла это Google B Cloud Они во многом похожи и работают отлично но с моей точки зрения у Amazon решит есть один большой недостаток А кто не знает как работает эта штука а работает она примерно следующим образом нужно зайти в панельку амазона руками там создать сервер руками выделить ему какие-то ресурсы Если вы значит эти ресурсы не используете вы всё равно будете за них платить если вдруг там выделенных 100 гбм оказалось мало и захотелось 200 то нужно опять же зайти в панельку амазона а выделить какой-нибудь дополнительный кластер в сервер которым в котором вы выделители ещё немножко места а если там производительности стало не хватать нужно создать ещё пару серверов накидать туда CPU В общем это ручная ручное масштабирование и много ручной работы в случае если что-то меняется мне показалось это не очень круто Тем более что в Гугле всё это делать Не нужно а для того чтобы начать пользоваться Google bigquery достаточно зарегистрироваться в Гугле там ввести номер своей кредитной карты получить авторизационный токен и можно заливать данные Примерно через 2 часа вы получите уже первый результат и главное что Google думает за вас о том как это масштабировать сколько это будет стоить Если вы это не используете вы платите только за хранение если в общем абсолютная гибкость и прозрачность поэтому мы выбрали Google Bey И на самом деле ни разу не пожалели вот всё ЧМ и говорил те запросы которые мы ставили перед компанией перед вот этой системой она все их реализовала и сейчас я немножко подробнее расскажу что это вообще за штука как её правильно готовить для того чтобы можно было быстрее получить какой-то предсказуемый результат ну прежде всего что такое Google это такая Такой движок по работе с данными которые в Гугле написан Достаточно давно а и используется там ну для любой аналитической системы которые есть в Гугле Прежде всего это Google Anal Или например любая аналитика которая выпадает в Google Play там везде на фоне под под капотом этот самый Google B query основная его идея - это массированно распалить запросов То есть когда сервер получает запрос от пользователя он разбивает его на на часть отправляет следующее но следующая нода разбивает его ещё на части отправляют следующие ноды и так может происходить там тысячи раз буквально потому что B может распараллелить на самом деле сложный запрос на десятки тысяч серверов Я видел в интернете замеры некоторые ребята умудрялись раскачать этот движок до скорости когда он сканировал данные со скоростью 100 млр строк в секунду ну то есть это прям ди-ди быстро и наверно основное его преимущество причём всё это работает из коробки ничего самому делать не надо с 2005 года они решили сделать эту технологию доступно для широкого пользователя она доступна Именно под именем Google бику в облачной платформе Гугла и немножечко об особенностях использования этого инструмента ну прежде всего как я уже говорил это это штука которая сразу готова к использованию и это очень дальше стоит иметь в виду что B quy работает только со структурированными данными то есть нужно отписываться схему данных перед тем как вы сможете туда что-то загрузить а bigy поддерживает SQL он очень похож на стандартный то есть там ничего нового неожиданного и страшного не будет кроме того в него есть пару очень выгодных конструкций которые выгодно отличают его от обычных баз данных которые именно помогают строить аналитическую отчётность потому что инструмент очень сильно заточен под аналитику наверное основной один из основных недостатков Google B - это в том что он работает только с immutable данными А всё что один раз загружено в B останется там навсегда а если закралась какая-нибудь ошибка в Google B нету запросов на удаление данных и там просто Нельзя Нельзя так просто взять обновить строку попозже немножечко я расскажу как мы обходим это ограничения и ещё одна особенность в том что Google beery не гарантирует вам скорость выполнения запроса не то что этот вопрос Вообще может быть выполнен и будет как бы вернётся без ошибки чаще всего это касается запросов на ставку запросы на вставке на ставку довольно часто заканчиваются с ошибкой И это надо иметь когда вы будете проектировать свою систему но если говорить про скорость то большинство запросов укладывается в период от 300 миллисекунд до 1 секунды То есть это не прям такой могучий но скорость достаточна для того чтобы в ретай отгрузить график клиенту Правда стоит иметь в виду что большинство запросов это не значит что все угла иногда бывает плохое настроение и даже простенький запрос на выборку которую вы ему отправите Может там крутиться и 2 секунды и 10 секунд а может и все 60 такое тоже бывает теперь немножечко о том как это как её готовить чтобы получить максимальный резу но жде всего я Всем рекомендую цива особенно если вы грузите туда большие данные Это связано с тем что Google в своей системе системе биллинга считает прежде всего количество байт которые были просчитано при запросе на чтение и поскольку там нет никаких индексов единственная возможность снизить количество данных которые он прочитает - это использовать практицизм сделает это не то чтобы быстрее но в два раза дешевле А ещё одна рекомендация - это как можно сильнее снижать нормализацию данных А когда мы работаем с данными в привычной нам базе данных в привычной среде мы нам Ну как бы по учебнику И вообще принято правило хорошего тона это делать как можно большую нормализацию то есть приведу такой пример А dogdog в основном работает заявками и вот эта вот сущность заявки в нашей базе данных приложения представлена шестнадцатью таблицами когда мы все эти данные выгружаем в B мы всё это преобразуем в одну большую длинную таблицу с колонками и это отлично работает в БИК за счёт того что ну в общем ему это не мешает работать не быстро неудобно и поскольку не надо думать ни про индекс ни про вложенность вмте рекомендують именно такой способ ещё одна рекомендация - это обязательно следить за типами данных опять же bigquery всё равно как вы используете например храните например дату будет это число или вы записываете это в строку или вы используете Для этого ещё какую-то серую магию Но для аналитических инструментов для любой штуки которая будет потом визуализировать данные она к этому очень сильно чувствительно и мы с этим столкнулись нам пришлось переделывать очень много таблиц Поэтому лучше сразу типы данных называть правильно и ещё одна история связана с тем что Bey - это всё-таки обла Облачный инструмент и находится он в Штатах и не очень правильно Если вы будете туда выгружать свои персональные данные Но это на мой взгляд поэтому Лично мы все свои персональные данные сначала анимируем а потом отправляем в облако я расскажу на примере друг Дока что же мы туда выгружаем чтобы было понятно Ну как вообще сделать такую штуку и прежде всего это все события которые мы отправляем в нашу Google аналитику они нужны нам в бири для того чтобы мы могли считать Ту же самую конверсию посещение и всё что мы всё то что мы умеем делать в бику в Google аналитике И для этого мы написали небольшой скрипт который просто перехватывает все события которые отправляются в Google аналитику и отправляют их в bigy прямо в точно в таком же виде а следующее что мы выгрузили это мы выгрузили туда все справочники которые мы используем в нашем приложении для того чтобы в этих данных можно было разобраться не отвлекаясь от основной базы И ещё одна история - это все задачи из нашего тара мы используем жиру это позволило например отслеживать Какие когда задачи были выгружены в продакшн и как они повлияли на ту же самую конверсию дальше мы выгрузили туда все наши деньги из О это позволило как бы связать нем поведение пользователе с теми финансовыми результатами которые даёт даёт инструмент дальше Мы записали туда все логи с наших серверов это ACC логи и ошибки которые Ну это сильно сильно упрощает например историю с поиском каких-то отклонений на сайте чем они были вызваны дальше мы загрузили туда статистику из наших рекламных инструментов это Яндек Google adws потом мы загрузили туда все Омо нашими пользователями это телефонные звонки SMS и mail сообщения и в качестве бонуса я расскажу такую историю которую я мало где видел но считаю её крайне полезной ваш сайт очень часто представляет собой какой-то набор контента который вы загружаете пользователю и это на самом деле сильно влияет на его поведение и для того чтобы можно было отслеживать как изменения в контенте в конечном итоги отображаются на бизнес показателях этот самый контент можно сохранять в виде таких состояний в виде Слепков в частности в кдо мы по крону Каждые сутки выгружаем весь контент в Google B и это позволяет нам построить например такую отчётность как изменялась конверсия на сайте в зависимости от там количества дерматологов в Южном микрорайоне Москвы вот Очень полезная штука рекомендую использовать и немножко о том как вообще можно загружать данные в чтобы их пом можно было анализировать И с чего стоит начать это огромное количество готовых решений потому что это большой вендер это достаточно популярное решение и если вы хотите выгрузить что-то из своей информационной системы Вполне вероятно что это не надо писать это не надо делать руками уже кто-то это сделал написал какую-то хорошую тузу и выложил либо её в Open Source либо сделал что-то что-то что-то готовое и платное Ну в частности для синхронизации мы использовали готовый инструмент который стоит около 100 долларов в месяц и это на мой взгляд сэкономил на кучу времени в разработке и позволила достаточно оперативно получить эти данные для анализа а вторая история заключается в том что ну в непосредственно в Google bigquery есть хорошие удобные инструменты для того чтобы загружать данные из стандартных файлов Ну можно сделать например какой-нибудь экспорт в J в CSV и прямо в таком виде загрузить это в Google B причём конкретно за вставку из Вот таких вот архивов Google денег вообще не берёт и конечно же там есть удобная J rpc AP через которые можно вставлять данные А так вставкой наверное всё А если говорить про какую-то систему бизнес-анализа в общем случае она выглядит так в упрощённом виде она примерно так же и построена в докдок Прежде всего это данные потом это какая-то загрузка потом это Stage а и последний этап - это визуализация Ну и где-то есть рядышком обязательно мониторинг который позволяет следить за тем что вся эта система работает и ваши данные нигде не сломались А если сломались то у вас есть немножко времени чтобы всё это оперативно починить А сейчас отдельно расскажу про каждый компонент про то как мы их каждый из них реализовали в докдок и прежде всего я хотел бы рассказать немножко о нашем загрузчики а сразу скажу это не самая оптимальная схема в Гугле есть готовая очередь в гуле есть готовый ра готовый обработчик этой самой очереди для того чтобы переносить а данные в кри но когда Мы начинали всё это крутить а к сожалению эти инструменты ещё не были доступны и поэтому мы пошли По пути написания собственного обработчика Почему А вообще нужен какой-то обработчик который будет ваши стандартные данные перегружать в кри Ну во-первых потому что время вставки в квери не гарантировано и как правило достаточно медленно и если вы будете не знаю там синхронно события с сайта писать в B quy А у вас и Ну во-первых время обработки запросов может очень сильно увеличиваться потом вы начнёте где-то терять данные Ну и в конце концов не хватит производительности bigy потому что единичная вставка туда происходит крайне медленно поэтому есть вот такой вот обработчик а что он умеет ну во-первых там есть очередь а на rit MQ которая очень быстро обрабатывает все события которые происходят в информационной системе во-вторых она их группирует в чанке по 5000 записи и умеет обрабатывать параллельно это решает нам проблему как со скоростью вставки так и решает проблема с тем что там могут возникнуть какие-то ошибки Ну и самое главное этот обработчик умеет наши данные анимировать анимизар как мы Решаем проблему с данными такое бывает что в данных закралась какая-то ошибка или вам наоборот построен такой процесс что данные в принципе нужно постоянно обновлять Ну например как статистика по заявкам они постоянно меняются но меняются как правило не все а только какие-то Ну например в последнем месяце и перегружать целиком огромную таблицу из-за того что там где-то что-то поменялось было бы не очень интересно поэтому а Google позволяет а перегружать не всю её а только часть в частности её партиции Именно поэтому например партиции использовать очень важно поскольку это упрощает например решение Вот таких проблем значит что нужно сделать нужно создать рядышком новую таблицу загрузить в неё как тот кусочек а именно ту партиции которую вы хотите заменить потом старые удаляется а новое просто переименовывается ля у вас новые данные и не пришлось перегружать весь терабайт всю эту терабайт ную таблицу А теперь немножко про визуализацию после того как вы загрузили данные в B всё что у вас есть это вот такая вот небольшая консоль в которою можно написать которая доступна в веб-интерфейс туда можно засунуть свой запрос Ну и посмотреть как он примерно будет отрабатывать пользоваться этим можно только для отладки никакую отчётность непосредственно в Google B построить невозможно там нет просто для этого инструментов А и что мы что Что мы делаем И что вообще можно делать ну прежде всего можно взять и просто на сайт вывести эти графики конкретно в данном случае это личный кабинет партнёра который оперируют данными как раз из Google bigquery Если бы у нас не было Google B построили это на каком-то другом инструменте потому что эта задача очень частая и здесь нет никакого Rocket Sci но мы построили Google на Google bigquery и работает она достаточно клёво Единственное о чём стоит помнить это о том что Ну ещё раз запрос от Гугла может обрабатываться непонятное количество времени Поэтому мы делаем то же самое что и в аналитике Мы сначала рисуем интерфейс потом мы показываем proll и потом в этот proll всплывает график зачастую он там всплывает за одну секунду ну иногда можно подождать по дольше А в случае ошибки его Конечно надо перезаряди и самая удобная и если говорить про там именно дешёвые подходы к Бизнес аналитике надо начинать именно с неё это визуализация в Google spreadsheets а и тот и другой инструмент всё от Google Понятное дело что у них достаточно хорошая интеграция и первое что можно использовать есть такая украинская компания ovx она делает много всяких полезных инструментов именно для бизнес-анализа у них есть расширение для Google spreadsheets которое позволяет без каких-то лишних тел движений прямо непосредственно в таблицу выгружать данные А работает Это следующим образом открывается вот такой редактор запросов в который можно свой запрос ставить и некоторым хитрым образом его параметри Зро после чего в таблица Гугла появится справа Вот такая панелька здесь можно задать какие-то параметры запрос нажать Окей и он перегрузить таблицу которая которая отображается в Google spess sheets Кроме того ну по таблицам уже можно настроить какие-то графики которые будут более наглядно показывать информацию и здесь есть возможность эти задания запланировать То есть фактически там за пару минут можно собрать а отчёт который будет автоматически обновляться и показывать данные в Реал тайме это прикольно Это дёшево это очень просто но не очень гибко а и скорее всего очень быстро станет этого мало Куда стоит дальше пойти Ну дальше можно воспользоваться например Google App Script если кто не знает то в любых Google Диск документов есть возможность что-нибудь там программировать на жава скрипте вот в Google spreadsheets тоже можно программировать и у этого жава скрипта есть готовая интеграция с Google bigquery Вот у меня на слайде пример очень простого запроса Как можно там буквально в пять строчек кода обратиться к Google bigquery и вынуть оттуда информацию которая вам нужна ещё примерно 10 строчек кода потребуется для того чтобы преобразовать это в табличный вид и вставить непосредственно в таблицу но и опять же Google App Script можно в Google App Script можно использовать расписание для того чтобы запускать эти задания по какому-то времени Вот пример такого отчёта который мы генерируем на Google App Script он показывает статистику по врачам разбитую на Неде когда этого инструмента станет мало и захочется прямо совсем космического чего-то можно использовать один из множества инструментов для визуализации данных который умеет интегрироваться конкретно с Google B quy а Их достаточно много мы выбрали для себя а штуку которая называется Чар пример на слайде пример отчёта который мы используем это такой Центральный Отчёт для докдок который следит Ну это кусочек его который следит там за всеми самыми важными показателями такие как деньги Выру доход и заявки от наших пользователей и последнее о ЧМ стоит упомянуть это мониторинг мы используем две подсистемы для того чтобы мониторить всю эту историю ну прежде всего это к драйвер рай это часть гулого гулого облака она доступна там бесплатно позволяет у неё встроенный коннектор Google позволяет снимать все основные метрики в частно опросов там прочитанных данных скорость расходования денег и прочее и там же можно настроить уведомления и какие-то пороговые значения если вы не хотите отклоняться куда-то в сторону Ну и во-вторых мы используем в докдок Z мониторит нашу наш Вот этот обработчик который перехватывает события отправляет в Google B в данном случае мы мониторим длину очереди опять же количество ошибок и скорость отправки данных и последний вопрос наверно о котором я хочу рассказать это сколько вообще всё это стоит и Почему мне кажется что это дёшево И начнём мы непосредственно с Облака на слайде пример счёта который мы заплатили за использование Google B quy в мае 2017 года из него видно что Ну за что мы платим ну прежде всего мы платим заставку в мае мы наставлять на 140 ГБ информации и оплатили заплатили за это 7 долларов дальше видно интересная особенность Гугла У него он бьёт все данные которые вы Его загрузили на две категории это данные которые ему нужны постоянно Ну то есть быстрые данные и такой Long ST storage куда данные можно записывать куда записываются данные и м от которых не требуется такая высокая скорость доступа делает он это всё автоматически Почему эта штука очень клёвая А ну потому что когда вы строите какой-то анализ вы как правило анализирует свой хвост то есть последний месяц Ну последние 6 недель максимум последние шесть последний год а событий как правило в системе хранится намного дольше это там период 2-т лет и туда дальше Вот и эта штука позволяет как раз минимизировать расходы именно на хранение А и самая большая статья в нашем случае это именно анализ данных как я уже говорил Google Берт деньги за любой байт который вы прочитали с Ди в принципе не очень большие деньги Почему конкретно в док доке это такая большая сумма по сравнению со всем остальным Ну потому что в начале мы не использовали партицирование и две самые здоровые таблицы которые хранят события просмотров сайта и события просмотров партнёрских виджетов они не используют партиции и буквально при каждом запросе пересчитываются полностью Поэтому вот такой небольшой сразу будет поме в целом как она получилась но по моим прикидкам мы потратили на её внедрение разработку около 3 месяцев А работал над ней один разработчик и один менеджер в сумме там учитывая какие-то расходы на стол и аренду офиса Это около 20.000 долларов а если говорить про регулярные расходы ну прежде всего потребуется какая-то поддержка Ну просто потому что данные меняются иногда что-то ломается Иногда нужно что-то обновить и таких задач у нас не очень много но там 8 часов в месяц я на них закладываю А собственно непосредственно аренду Облака в нашем случае там получилось 82 доллара потом мы платим за некоторые готовые решения по перелив данных на мой взгляд там платить за них каждый день небольшие каждый месяц небольшие деньги намного выгоднее интереснее чем писать что-то своё Ну и последняя статья расходов - это непосредственный инструмент визуализации чарт на се семь учётных записей для нас обходится в 300 долларов каждый месяц так чём я хочу сказать о том что в принципе инструментов на самом деле достаточно много а и всё то о чём я рассказывал и то что я отм оно тоже в принципе подходит то есть я не хочу сказать что нельзя построить аналитику например на myq или на пост скэ или что там Amazon Shift совсем не подходит Я просто говорю что тот путь и тот способ и вот этот инструмент который мы применили на мой взгляд позволяет вот эти эти риски от разработки сложной инфосистемы куда мы вкладываем очень много денег очень сильно их сократить и наверное основное преимущество Google bigquery по сравнению со всеми остальными а инструментами это то что результат можно получить буквально за несколько часов зарегистрироваться Залить туда небольшой объём данных Зайти в Google spss построить там свой первый отчёт не один наверное из других способов Э не даёт такой офигенной скорости а У меня всё я готов ответить на ваши вопросы если они Конечно есть Алло лло У меня два вопроса Первый вопрос касается того какой принцип партиции Как вы их выделяется По какому собственно параметру И второй вопрос касается денег которые вы тоже выгружается туда вы их тоже как-то преобразователи вы показываете так как оно есть действительно я понимаю про собственно то что вы их анони зру ете А вот что собственно с суммами происходит Давай значит первый вопрос про парти меня или я зовут чтобы Ага давай значит первый вопрос про партицирование в Гугле Ну и в принципе вопрос партицирование он во многом ограничен именно задачами которые мы решаем это задачи бизнес-аналитики в принципе кри она очень про бизнес-аналитика и там есть встроенный очень удобный инструмент когда он практи данные в зависимости от даты и складывает их кусочками Вот то есть события за сегодня за завтра разделяет их днями и это там работает прямо сходу ничего думать отдельно не надо там есть возможность делать какие-то другие партиции Ну например по пользователям там или ещё как-то но для этого нужно что-то сделать а вот прямо подать можно делать очень просто а вторая история про деньги нет деньги мы никак не анимируем и в конечном итоге там Я не вижу в этом наверное никакой секретности ни какой-то особой ну особой Тайны вот пользовательские данные - это да если они куда-то Уте это будет очень страшно очень обидно и это огромные риски если там вы узнаете нашу выручку Ну да и Бог с ней не так страшно Здравствуйте А вот я видел там много Эля А есть ли варианты чтобы допустим какой-то не очень квалифицированный челок себе настроить какие-то выгрузки можно можно но это вариант как более более дорогой То есть если говорить про аналитику вот прямо на костылях как мы её собрали в своё время в Google spss то нужно немножко уметь программировать нужно немножечко знать SQL если подключить какой-нибудь инструмент типа РТО то в нём ма масса возможности сделать отчёты без без знания прямо в конструкторе и это будет кво секрет здесь вот этот который я в середине доклада вставил все данные должны быть в одном месте Вот когда они в одном месте можно взять любой визуализатор который имеет встроенный конструктор и на нём как бы всё это построить тогда будет работать Здравствуйте два вопроса Первый Есть ли у вас какой-нибудь План Б если вдруг Google перестанет работать на пару часов или на пару дней Есть ли у вас какой-нибудь кэш или что-нибудь такое Это же бизнес-аналитика бизнес не остановится если Google перестанет работать на пару дней а если вы потеряете эти данные А ты знаешь наверное наверное ничего страшного в этом нет хорошо вот поэтому по-моему у нас даже бэкапов никаких нет А значит О чём наверное стоит помнить это о том что не все системы позволяют выгрузить данные обратно вот в плане Гугла Если нам он вдруг перестанет нравиться и у нас что-то не понравится не получится с ним мы эти данные просто оттуда заберём зальём какое-нибудь другое облако или поставим там свой сервер поставим туда кха и загрузим эти данные туда вот насчёт этого Я не переживаю платформа достаточно открытая Ну а если он упадёт такое бывает один раз он упал и не работал полдня Но ничего страшного понятно И второй вопрос А что делать Нет вернее Вот вы планируете хранить данные всю там бесконечно или у вас ретеншн какой-то нет нет все данные храним бесконечно там ну потому что это дёшево А как я показал именно конкретно за хранение там у нас буквально 10 долларов в месяц уходит А мы их храним Достаточно долго Я думаю что мы ещё там сможем вперёд 20-30 лет сохранить и ничего страшного в этом не будет Вот потом там вот Обрати внимание там есть вот этот том стреч куда он автоматически складывает данные которые не использует и он стоит ещё дешевле и Ну вот аналитика Она хороша когда у тебе все данные вот вообще все за всю историю тогда любой вопрос который ты захочешь спросить он тебе даст ответ Спасибо Угу А спасибо за доклад А вы данные храните все в одной таблице или вы всё-таки разбивает То есть у вас универсальный Лок какой-то Нет конечно мы разбиваем данные то есть есть таблиц по телефонным звонкам есть таблиц по по заявкам там есть таблиц по событиям Google аналитике есть отдельная таблица по сессия Я просто о том говорю что не надо вот если представить себе Клик Stream Ну и то как он хранится в Google аналитике это вот без привлечения Это 250 полей то есть таблица с 250 полями Если бы я пытался их сохранить ционной базе данных типа по SQL я бы там там настроил кучу событий связанность там один ко многим много к одному Вот Но в БИК это всего делать не надо вот во-первых на скорость не влияет во-вторых удобнее пользоваться намного Я просто к чему спрашиваю то есть у вас получается несколько разных таблиц Ну разнородных да в отчётах вы То есть вы их не можете в конструкторе отчётов например объединить можем во-первых поддерживает поддерживает работают там отлично это если руками а если фиче редактор то вот в это всё можно сделать не работает когда у тебя в разных базах Вот я как бы хочу отдельно на этом подчеркнуть внимание Вот если у тебя события PW хранятся в Google аналитике хотя их через IP оттуда можно забрать а звонки хранятся в какой-то другой информационной системе в какой-то другой базе то в Чар свести это всё в один общий отчёт не получится в БАМе тоже не получится и в табло тоже получится фигня Оке сдела в дата сорс Спасибо Там дй невозможно сделать"
}