{
  "video_id": "EOWNpAXD3sA",
  "channel": "HighLoadChannel",
  "title": "Масштабирование сети VR-аттракционов CinemaVR / Андрей Татаринов (VRTech)",
  "views": 293,
  "duration": 2168,
  "published": "2018-08-16T04:05:32-07:00",
  "text": "привет меня зовут андрей татаринов я вам расскажу про то как мы строили масштабировались сеть 7 вверх я работаю в артеке we are tech это фонд который занимается проектами в области виртуальной реальности вкладываются в проекты делают их сам синим егор это собственно самый большой из проектов который версиях делает вот тут партнерка с варгейминга случилось это мне кажется классно доказательства что все делаем хорошо что мы делаем мы делаем вот прямо физически локации где можно прийти заплатить 300 рублей и пять-десять минут поиграть в игру свято веря площадке располагаются в основном в кинотеатрах на территории кинотеатров и торговых центров мы быстро масштабируемся такими модульными кубиками и разрабатываем контент самостоятельно и лицензируемым то что не можем разработать контент разрабатываться в сном по игра мой папа кино и каким-то брендом которые сейчас актуально вот недавно и группа смешарикам запустили запускаем сеть франшиз пара локации уже есть которая принадлежит то есть основного основное оперирование наше но есть несколько локаций которые принадлежат другим людям а мы осуществляем техническую поддержку у нас локации раскиданы по москве есть санкт-петербурге в краснодаре и мы целимся во все города миллионники что это физически такое то есть мы масштабируемся вот такими физическими кубиками пять с половиной пять с половиной метров в каждом из которых мог могут одновременно играть четыре игрока там наверху подвержены компы с виндой и подключена мажьтесь его и вам есть сервер локации есть телевизор и на которых видна во что играет человек прямо сейчас есть планшет администратора через который администратор может управлять тем что происходит на локации у нас администратор на площадках у нас в первую очередь продавцы и поэтому нам важно чтобы там все процессы были автоматизированы максимально вся поддержка сидит удаленно в офисе в целом по компонентам основная разработка у нас на рельсах игры на unity там про игру мы почти рассказывать не будем в цифрах мы на момент запуска у нас было 10 площадок которая запустились и это уже не мало потому что они все должны были запустится одновременно в прошлый новый год прим 1 января все работает это порядка 40 машин потому что нам на каждой по 4 машины 10 серверов сейчас мы выросли до 40 площадок не на всех площадках по четыре машины поэтому игровых машин сотня 40 серверов и появились облачные компоненты вот я бы хотел на самом деле про две темы рассказать про технологию про то как мы про то какие технические проблемы мы решали как подходили и пара то и про подход мне всегда кажется что важно у нас сложно мы как как и любой другой стартап никогда не знаем чем будем делать то есть у нас задачи устаканились стало понятно что примерно мы делаем не знает три месяца назад до этого задача скакали в разные стороны и поэтому нельзя все делать прям сразу хорошо фундаментально потому что мы делать будем это вечность потом окажется что это никому не нужно но вот например мы там делали поддержку под сложную многопользовательскую игру три раза перенесли в итоге вообще отменили вот чем меньше времени мы потратим на эту на эту задачу тем лучше поэтому классно делать простые решения которое можно под на которой решают задачу прямо сейчас и которые можно переделать потом но при этом важно понимать что вот есть решение которое можно откатить и их можно принимать легко и можно не особо задумываться зная что их можно поменять а есть решение которое отменить невозможно и про них надо думать хорошо окей в первую очередь про технологию я буду рассказывать про платформу про разработку игр я рассказывать не буду это отдельная команда ну и в целом это решенная задача там понятный pipeline платформа какие задачи решает платформа обеспечивает централизацию управления всем парком максимально автоматизировать все процессы которые есть и должна снижать требования к обслуживающему персоналу при этом мы должны обеспечивать работу наших базовых бизнес-процессов в любых бизнес в любых внешних условиях и самым сильным внешним условиям в нашем в нашем случае является интернет то есть вот у нас сейчас 40 площадок и это график за последние 7 дней доступности площадок через интернет вот нет такого момента в дни когда все площадке одновременно пингуются почему так даже несмотря на то что мы там стоим на территорию торгового центра в кинотеатрах не всегда можем подключиться к проводному местному интернету очень часто там безопасность какие-то еще нюансы и мы садимся на сотовые модемы сотовые модемы качество у них очень разная на одной площадке куда ездил я не мог никуда дозвониться но вот с этим надо работать и вот это вот отдельно делает задачу интересный потому что обычно бывает как либо у вас есть простая бизнес-логика где-то на на удаленных ногах с плохим подключениям ну там какой-нибудь интернет вещей и вы решаете задачу интересными синхронизации потоков данных между но да и сервером или и у вас есть какая-то сложная сложная система не знаю там офис офис продаж какого-то магазина но тогда вы можете требовать хорошего интернета и упрощаете архитектуру в этом случае а вот у нас вот такое нам на моем опыте редкое сочетание этих характеристик вот чем занимается платформа платформа в первую очередь обеспечивает то что на всех машинах в нужных местах установлен нужный софт в нужных местах на лежат нужные конфиге с корректным содержимом что игры скопированы на все машины те которые нужны мы обеспечиваем мониторинг и мы поддерживаем и бы платформу поддерживают процессы которые происходят на площадке собственно про конфигурацию конфигурация постоянно меняется и нам очень важно иметь решение которое позволяет нам его менять вот например на момент запуска мы фактически distribute равале такие коробочки которые мы наполним потом потому что всего было там два месяца на подготовку и и все и поэтому нам нам нужно сделать так чтобы мы могли ну вот удаленно менять это решение сложная она его нельзя откатить то есть там то с чем то что мы выберем она останется с нами навсегда управление конфигурации в принципе есть вот можно можно про это думать в таких терминах что есть системы типа on seba и сорта которая проактивно пушат изменения на на хост то есть вот админ написал команду нажал кнопку и изменение пошло это это интуитивно это понятно ну и все там любят анти был при этом есть есть другой класс систем у которых есть агент на каждой машине и агент сам про активно пытается достучаться до сервера конфигурации получить новый конфиг и применить нам подходит они сложнее эти системы сложнее в настройки поэтому там с ними больше возни но нам только пул систему подходят потому что интернет у нас недоступен и более того даже если мы прямо сейчас смогли достучаться до надо это не очевидно что с первого раза надо применит изменения она там пойдет файл качать соединение оборвется и конфигурация не применится вот то есть соответственно мы выбирали там между шефом и папе там был еще набор виндовых системы по управлению конфигурации но мы их не рассматривали потому что у нас гетерогенный кластер а вот выбрали шеф потому что у шефа бесплатная лицензия под винду соответственно на момент старта в шефе я сейчас прошел по рассказываю момент старта у шефа на шельфе был самый некую только необходимый минимум функционала он занимал столько фактически скачиванием игры а все остальные действия мы производили при предварительной подготовки машины в начале вот там по списку написанному вручную для людей там не так важно что там написано важно что он большой и это еще не весь и постепенно но за очень круто что у нас очень очень круто что у нас появился списочек мы смогли постепенно вычеркивай из него строчки переносить конфигурацию в автоматизацию и в целом это вот такой иллюстративный подход сначала за прототипировать по простому с минимальным у себя с минимальными усилиями а потом автоматизировать и то что реально бывает нужно то есть и анекдот про про прошив вначале мы немножко перемудрили раскидали по каждой локации отдельные сервер которые обслуживает эту конкретную локацию и выяснил что это сложно казалось что правильно если шеф клиент всегда может достучаться до локального сервера даже если нет интернета но оказалось что основной бизнес процесс у нас это внесение новых изменений и в итоге у админа появилась табличка куда он смог донести новую конфигурацию переделали сервер стал глобальной ну очевидно но важное что мы что что произошло пока мы работали с каждой локации но папа очередности мы поняли какой бизнес процесс нужен для работы с изменениями и обновлением и выкладкой новых новых билдов пришли к такой интересной схеме где у нас там к схеме где у нас отделенную разработчики контур от продакшена и в разработке от разработчики могут там ломать в том числе и шеф это было важно для нас что вы там фичи разрабатывались интегральное разработчик участвовал в том числе в разработке конфигурации про игры это это короткая история но даже на короткую игру в сессию в 7 10 минут игра их игра весит много а там от 2 до 10 гигабайт при этом в целом как оказалось игра жизненный цикл игры не заканчивается в тот момент когда ее отдали на разлив q на площадке то есть после того как мы ее налили еще там пятак раз мы ее меняем потому что нам говорят по фидбэк us площадок сделали сначала естественно по простому на этом запустились просто всей игры лежат где-то в облаке в баке кино с 3 и все машинки их скачивают как могут в целом нормально работает но на всю на весь флот новый игра льется несколько дней и ну вообще довольно сложно отследить тот момент когда везде везде новая версия по-быстрому переделали смогли ну то есть за пару дней смогли внести такие изменения в конфигурацию что на локальном сервере получил появилась в локальный кэш он скачивает игру локальной и а машины на площадке скачивают данная с локального сервера ну казалось бы да очевидное изменение почему сразу так не сделали потому что это изменение чуть-чуть сложнее чем другое и и круто ну то есть мы сделали ставку на то что такого не произойдет ну вот в этот раз не получилось но могли бы сэкономить два дня и вот так так обычно и у нас работает вы шеф у нас во многом является системой прототипирования печей то есть сначала мы в его конфиге прямо в текстовых файлах в били все списке игр с их конфигурации которой однако которая должна быть потом появились справочнике билетов очень нетривиальная конфигурация билетов потом появились интересные условия про то какая площадка как должна работать в зависимости от того там не знаю где она стоит и других факторов в принципе можно было бы опять сразу пойти и написать админку под это все но это больше это это требует больших трудозатрат на старте с неочевидным выхлопом как бы проще за прототипировать и потом материализовать в в более удобных интерфейсах то что реально полезно когда все есть внутри конфигов довольно просто преобразовать конфиге так чтобы появилась там 1 1 простынешь джейсон чекам про то как как конфигурируется вот эта конкретная площадка появляется api а потом потом не важно откуда берется вот эта конфигурация из шефа или из отдельной админке по тому же самому протоколу самое главное что мы зафиксировали протокол и мы можем уже не включая мозг сделать техническое действие отдельно с мониторингом опять начинали просто поставили все все все системы писать флаг и админ сидел смотрел на просто информации от машинок этого конечно было недостаточно появилась необходимости на графике смотреть появилась необходимость следить затем за поведением всей сети мы выбирали выбирали из стандартных компонент за zabbix на голосом мне показал мне окажутся не достаточно гибкими системами например они позволяют взять какую-то конкретную метрику за один запрос и и просуммировать показать на графике для нас это важно потому что мы ровно так делаем сели на график на граффити пожили какое-то время графит известен тем что его сложно масштабировать ну то есть он на надо как-то отдельно и знание получать прав для того как для того чтобы он функционировал после третьего апдейта сервака мы с него слезли и перешли на prometheus про метрики у нас каждой локации пишется порядка 700 метрик и вот в данный момент там каждый в каждый момент времени порядка 20 там 6 28000 метрик мы собираем каждые 15 секунд ну потому что 1 минуту я слишком медленно собираем стандартные метрики по диску по памяти по сети мониторим отдельно видеокарты в самую самую первую очередь эта температура на видеокартах потому что они портятся если греются вот и мониторим все бизнес-процессы то есть версии компонент работоспособность в каком состоянии прямо сейчас площадка находится архитектура у нас получилось почти стандартная с небольшим нюансом из-за интернета так как и из-за из-за из-за сочетания нюансов про метался и интернета у кроме то есть такой нюанс это пул систему он сам вытаскивает данные и поэтому в тот момент когда он попытался сходить на надо собрать данные не смог у него в графике дырочка очень хотелось иметь графики для отладки где дырок нет поэтому у нас появился локальный сервер который который мониторит только одну площадку вокруг и в нем и на него не действует свойство с интернетом и есть глобальной про митоз в облаке который фидера лизу it все локации и он используется для лифтинга он посылает сообщение админом и на нем рисуются графики объемы в графиках при этом там есть но как бы в данном случае это уже не важно за всем парком с машин следить это отдельная интересная задача мы искали подходы вот нашли вот такой мне кажется он интересный можно его наверное где-то еще использовать если взять весь парк порезать если взять какую-то метрику которая нас интересует не знаю например версию локального сервера на каждой площадке то можно вот взять весь парк разрезать его по и и и нарисовать какая версия где установлена там там интересные графики получаются вот видно там что пора машинок постоянно пара локации постоянно отстает по по обновлению отдельно интересно мы однажды поймали ситуацию когда пара админов одновременно вносили изменения и у нас график выглядел так версия покатилась покатилась и пошла тка и откатилась обратно ну интересно собираем до метрики я сказал папа лир там мы для мониторинга используем prometheus у него очень классно очень гибкая система настроек там вычисляемой а паря ну то там там там способ формулирования артов точно такой же как способ формулирования графиков и вешаем alert и вот на все что нужно там на систему на опять таки на на видеокарты на работоспособности и в зависимости от того что мы мониторим реагируем по-разному например на то что шеф плохо работает то есть то что последний запуск шефа был больше нескольких такое-то время назад мы даем шифтом не знаю несколько часов она то что кончается места на диске можно реагировать сразу в течение минуты отдельно интересно что есть alert и которая не имеют смысла круглые сутки например у нас была проблема что админы на локациях просто вот ради фана перезагружают все что могут в случае любой проблемы мы начали отдельно мониторить что сервера не перезагружаются это ну и за этим надо следить их надо учить но перезагрузка сервера иногда имеет смысл какая-то иногда нет потому что площадки на ночь выключаются и выключение площадке там не знаю в час ночи о включении ее в 9 утра это корректное поведение классно что опять про митоз гибкая система в него можно засунуть весь график работы площадок прям вот отдельная метрика которая пишет площадка должна работать в этот момент ее использовать при формулировании оля то тогда мы можем как бы отличить то что-то на что надо реагировать от того чего не надо это помогает потом при разборе там али ртов за эту неделю ok дальше у нас есть большой блок это управление площадкой то есть бизнес-процессами которые на ней самый главный процесс естественно это обслуживание одного клиента то есть мы должны вот он был он приходится не знаю дает нам 300 рублей мы мы ему отдаем белее от регистрируем системе запускаем игру игра заканчивается мы записываем все что нужно о том что такой-то человек поиграл для статистики и отпускаем это это должно работать всегда вне зависимости от вообще всех погодных условий мы иногда площадку на выставку отправляем там формат вообще нет поэтому идеальный вариант нам с централизованным сервером нам не подходит кстати так вот многие конкурирующие системы делают ему твой ход который наши силой и делают свою платформу для кибер клубов у него у него и центральных сервак у них классный интернет мы так себе позволить сожалению не можем у нас интернета нет итак не работает поэтому у нас функционал делится между локальным сервером которые прям стоит на сервере на каждой площадке и глобальным который визит в облаке между ними там роли делятся так что локально обслуживают площадку а глобальный реализует все процессы которые требуют полной информации и соответственно так как у нас есть несколько систем между ними должна быть синхронизация то есть у нас все все централизованные процессы типа выдачи промо кодов типа расчета лояльности там топ score of отчетности они требуют того чтобы его вся вся информация при этом нам надо обратно тоже и найти информацию о том как как людей зовут о том какие новые промокоды выпущен там и так далее и было бы классно при этом не копировать всю информацию на все площадки то есть там самое простое решение взять все секретировать она не не очень хорошее потому что у нас есть франшиза на франшизе нам бы хотелось чтобы отменную франшизой площадке не мог пойти и посмотреть статистику продаж всей сети вот я сейчас там все про промо-коды буду рассказывать поэтому вот у них там есть как как какие есть важные задачи это выдать во лидировать погасить и сформировать отчетность соответственно запускались мы просто опять как обычно мы большой список промо кодов которым мы прежде не ревновали положили на в облаке из него выдавали в этот же список скопирован везде она все площадки и каждая площадка его отдельно гасит периодически мы там псевдо ручном режиме синхронизируем списке погашена насти это классное решение она работает почему под ней esprit по если у нас промо-коды все одинаковые то мы можем действительно заранее эти все списки распространить и сэкономить на задача синхронизации но при появилась усложнения в задача промокоды стали уникальная то есть мы их начали привязывать телефону там на основании того что ты молодец и поиграл в три игры подряд они стали разные стали появляться разные промо-кампании для отчетности и там надо было их постоянно генерировать заново соответственно все таки синхронизацию надо при пришлось делать рассматривали разные интересные варианты то есть первое что принесли программисты говорят давай давай multimaster новый сквер это классное решение но это вносит больше технических компонент нам надо будет стать специалистами не только по поскорее рельсам но и специалистами по новой интересной базе данных а также по всем нюансам в которых которые в ней есть вот поэтому наверное нет из моего enterprise нова прошлого вот enterprise делает вот так он отправляет очереди сообщений от из облака ну он он бы отправил обувь очереди сообщений из центра на все надо и обратно и так это классное решение там так работает но здесь очень много точек где происходит мутация то есть а это и из моего опыта ни разу программисты с первого раза не написали систему синхронизации корректно всегда всегда возникают нюансы всегда возникает рассинхронизация между несколькими одинаковыми нотами которую прям очень сложно исправлять потому что вы вы уже не знаете где правда они все мастера и в итоге вдохновение оно пришло с такой неожиданной стороны во front-end и сейчас очень активно развивается работа с данными есть такая архитектура redux которая используется рядом с реактором и суть ясна основная суть там такая что есть ноты которые производят изменения и они посылают только дельты изменений в какую-то базу абаза в ответ ну или когда надо предоставляет надя полное состояние мира которое нужно ей для работы это классной архитектура потому что в ней только одна точка мутации . в базе и база полностью принимает решение о том что происходит а каждая надо может тупо брать состояние мира вот на текущий момент им пользоваться вот мы примерно так реализовали у нас каждая локация посылает события по изменениям в облако а облака 1 минуту от обратно отправляет джейсон с описанием всего что нам нужно знать там там активные промо-коды там перечень пользователей с их профилями там еще какая-то дополнительная информация я знаю эта архитектура взорвется она перестанет работать когда у нас будет 100 тысяч пользователей сейчас их 15 размер синхронизационные во пакета столь 50 или 100 килобайт заархивированы поэтому вообще ничего не стоит его регулярно забирать и не иметь всех проблем связанных с поддержкой множественной в записей но даже в этой простой архитектуре есть нюанс там я немножко не уследил и программисты первое что сделали повесили мета магию на активы рекорды и сказали вот у нас есть взаимодействующие сущность в базе мы прям на них на хук повесим на на метод сохранить повесим метод все реализации отправки в облака почему это плохо что представьте себе у вас есть вот такая конструкция где есть отдельная информация о том что человек пришел и принес билет там не знаю на полчаса и вот в этой информации в плеер сессии мы записали о том что его зовут вася его телефон такой-то отдельно он в рамках этой игры поиграл какое-то количество в рамках этого билета он несколько раз поиграл в разные игры и вся эта информация си реализовалась и и разными пакетами полетела на сервер что получается если вдруг нам не везет и пакеты перепутываются местами и при этом нам не везет и у нас есть логика как на пример расчета лояльности которая требует связки геймера нас плеер сессий в тот момент когда работает то у нас нет способа прямо сейчас это отработать там два варианта либо 500 и ждать что там ретро и выстроят пакеты в правильном порядке нужном нам для обработки либо терять эту информацию вот это это решается если группировать если делать синхронизацию осмысленный то есть такой когда пакет содержит достаточное количество информации для обработки бизнес-процессы связанного с ней ну то есть вот в нашем случае для game рано туда же подвязывать всегда имя пользователя несмотря на то что в идеальном мире он бы раньше прилетел по синхронизации только как в тот момент когда зарегался вот но и это не все когда мы такую схему реализовали с того понятно что площадке у нас не одинаковые то есть есть площадке есть все остальные площадке а есть вот так конкретная на котором вы пришел человек и показал вот этот его конкретный промо-код и нам очень важно чтобы вот этот конкретный промо код в течение следующих двух секунд не мог быть погашен снова есть вот такой сценарий возможен если мы все пропускаем через глобальную петлю поэтому пришлось добавлять таблицу корректировок локальных в которой мы фиксируем только столько локальное событие вот этой конкретной площадки и она служит таким буфером до тех пор пока не прилетит новое глобальное состояние про мир вот-вот примерно на эту в этом состоянии мы сейчас находимся мы наверняка это не финальная наверняка это не финальный вариант наверняка мы когда подрастем окажется что от глобального от цинка глобального состояния надо переходить на дельты но мы во-первых себе оттащили от оттянули эта а этот эту задачу не знаю на год и мы сможем собирать кейсы и думать о том что на самом деле нужно ну дольше это мне кажется классно вот про технология все рассказал выводы у меня в первую очередь такие метафизические что хорошо делать просто плохо делать сложно но при этом классно почти общую сложной системы так чтобы оно было в каких-то границах вот все так синему я расстроен вот если есть вопросы давайте до онлайн почему вас так часто синхронизацию совместимы у нас минуты это много положу ну как там есть есть базовый бизнес-процесс который пролетает через всю цепочку это это выдача нового промо-код то есть нас есть лояльность лояльность работает так ты поиграл в 5 игр получив шестую бесплатно условно и принимать решение о том что ты поиграл что это 6 игра в которую ты поиграл нужно ну на основании полной информации про все площадки но мало ли там я еще походил вот а почему синхронизировать оперативно ну потому что ты эту смазку прямо сейчас можешь получить вот ты стоишь на площадке ты поиграл и тебе во время игры пришла эсэмэска с новым промо кодом который ты можешь захотеть погасить прямо сейчас ну вот бизнес требования у нас вот так сразу бизнес-процесс все-таки локально решается и полный цикл все-таки по промокоду на локальной площадке возможен нет я вот только что попытался то есть а как когда решается вопрос промокодами если вы там на выставке интернета нет вот но мы как если у нас интернета нет то в этот момент ну мы как человеку не приходит эсэмэска до тех пор пока площадка не получает связь у нас идеальный сценарий вот такой и есть деградация с мск с промокодом тебе придет позже ну ты все убирается информирование о том что произошло это мне дольше чем вырастут на секунду так утрируя ну да ну то есть у нас примерно так и построена то есть и еще вопрос по описывали опять же push событий в рамках сессии и да почему в конце сесил там ран station есть а там финиш совершенно взять и все действие в рамках конкретной здесь и отправить на вот вот вот это вот и сделали то есть там проблема такая была что как бы ребята сначала сделали красиво просто то есть а красиво это вот есть отдельная сущность active record вот мы ей красиво сад составлен ставим в соответствии пакет с синхронизацией вот сделали все нами то магии и ну то есть самый простой самый простой вариант и реализовать вот это вот конкретную сущность которая прямо сейчас поменялось а там вот собственно решение корректны и в этот момент нужно просто как бы расширить пакет той информации которая но будет необходимо для реализации логики да последний вопрос я вот в презентацию на слой we are у вас в принципе по пиару какая-то деятельность есть ну пояснить и но я конечно в основном вы работаете по франшизе по какому-то решению ну смотрите то есть там сферам we are никто не разрабатывает самостоятельно практически то есть есть там несколько компаний которые делают базовое решение платформе на все остальное все остальные утилизируют те и те или иные базовое решения вот мы можем сейчас закончить я могу рассказать там дать краткий экскурс там какие какие платформы по верху сейчас бывает я правильно понял что вы поделились опытом сопровождении какой-то для решения и ваш приход спасибо хорошо всем спасибо"
}