{
  "video_id": "XXlrqvi5sFs",
  "channel": "HighLoadChannel",
  "title": "Точки отказа в хайлоад-системах. Backend / Константин Козловский (Газпромбанк.Тех)",
  "views": 3148,
  "duration": 2319,
  "published": "2024-10-29T03:09:38-07:00",
  "text": "Константин Козловский из Газпромбанка сегодня нам расскажет про точки отказав в системах Ура Всем привет Рад вас видеть Давайте начнём Меня зовут Константин Я хочу немножко рассказать о себе Я работаю исполнительным директором разработки в газпромбанке ци системах занимаюсь платежами переводами и всем чем с этим связано Я участвовал в разработке системы сбп газпромбанке и одной из моих последних самых интересных задач таких это мы партиционирование в бде и в настоящий момент Остановился на jav и соответственно много примеров которые я буду сегодня приводить будут связаны именно с этими языками и с технологиями которые с ними связаны Например если это РМ это будет hibernate и там например springboot какой у нас сегодня план сначала поговорим про энд разработчиков в целом как они относятся к лоду Как как они себе его представляют а затем постепенно снизу вверх пойдём по различным проблемам с которыми вы можете столкнуться с которыми я сталкиваюсь периодически коснемся памяти разберём базу данных поговорим про tcp соединения в процессе будем сталкиваться с различными инцидентами из реальной жизни я буду рассказывать какие проблемы мы решали как мы скажем так планировали с ними бороться энд разработчики в своей практике Я часто провожу различные собеседования технические собеседования мы беседуем на фите с разработчиками И мне очень нравится задавать вопрос работал ли ты в системах с системами тут можно сразу по ответу понять Как себя чувствует разработчик если это джун он начинает стесняться сомневаться там отказывается отвечать говорит не работал Если Мидл начинает спорить про ПС и про ТПС мериться цифрами сеньоры начинают задумываться и скажем так задавать встречные вопросы а что такое система для тебя как ты оцениваешь эту систему и наверняка у каждого из вас есть свой ответ на тему Что же такое лот система для меня он следующий система которая недостаточно стандартных стандартной инфраструктуры стандартных настроек и которую необходимо ткать очень тко настраивать это и есть система соответственно Если вы развернули там постгрес в докере и спокойно работаете вам этого хватает Вы не задумывались никогда про tcp соединение то наверное у вас нелот Если же вы с этим сталкиваетесь периодически то Добро пожаловать вам к нам Давайте немножко разом мы сегодня будем играть в детективов Иногда у нас есть два графика на одном графике это графики нагрузки Вот как я уже говорил Я работаю с платежами переводами Да И вот этот график нагрузки на нашу систему их два на Первом Штиль всё ровненько на втором у нас Шторм да давайте вопрос первый где-то произошёл инцидент поднимите пожалуйста руки Кто считает что инцидент произошёл на графике номер один на Верхнем Окей А теперь кто считает что инцидент произо на графике номер два здесь больше людей Окей в транзакционный системе в платежах и переводах у нас очень важно чтобы всё было спокойно и нагрузка у нас медленно повышается скажем так с утра достигает Пика днм и подходит к концу вечером вот поэтому инцидент конечно же на втором графике система перед нами которая находилась нас бойла и подавала нагрузку Нам очень плохо периодами память Я выделил несколько разделов касающихся работы с памятью и скажем так по моему мнению это самые популярные проблемы с которыми вы можете столкнуться а именно потребление памяти в ваших сервисах может вырасти в результате просто повышения нагрузки и это не всегда линейный процесс также в последнее время у нас участились скажем так внедрение систем виртуализации это куне openshift doer и не все понимают Как правильно настраивать когда мы запускаем там Java приложение к примеру на виртуалке указываем опцию xmx ограничиваем кучу и как бы с этим живём в порядке а вот когда дело доходит до виртуализации там ещё есть лимиты и реквесты на подах и если настройку сделать некорректно очень высок риск схватить неожиданные бизнес данные очень интересная проблема Когда вы пишете код какой-то процедуру Она работает всегда с одинаковым количеством данных но тут внезапно меняются бизнес условия и вам уже вместо там тысячи транзакции приходит 100.000 транзакций в это место оно к этому не готово и соответственно получаете падение Самое интересное это утечки утечки памяти Несмотря на то что в современном мире большинство языков программирования текущие которые самые популярные они уже памятью управляют самостоятельно но тем не менее разработчики иногда допускают ошибки создают цепочки объектов и ч колектор не справляется с этими испытаниями и возникают утечки продолжаем играть в диагностику Посмотрите пожалуйста на график у нас есть график потребления памяти одного микросервиса Как видно в конце он не выдержал Это был Out of memory последующая перезагрузка Давайте с вами попробуем по графику определить что же здесь произошло во-первых э ситуация на графике изображена временной период 10 дней То есть это долгий был процесс и поднимите пожалуйста руку Кто считает что проблема произошла из-за высокой нагрузки рук мало Так может это была утечка как считаете Ну смотрите как у нас все прекрасно справились Да это была утечка и Давайте теперь посмотрим с чем эта утечка была связана мы разрабатывали систему логирования и библиотека для логирования должна была фиксировать запрос и ответ разницу между ними и разработчик не ул что у нас есть е запросы в мку и в запросы не всегда возвращаются обратно есть асинхронное взаимодействие где скажем так формируется только запрос а ответа уже не поступает и в этой точке начали накапливаться вот эти запросы они не закрывались не удалялись И постепенно в течение там 10 дней это всё убивало сервис Ну соответственно здесь бак обнаружили что с этим делать если вы сталкиваетесь с утечками во-первых я бы сказал всегда обязательно в какой-то момент случится вот если вы развивающаяся система которая периодически делает новые доработки шансы скажем так не ошибиться э шансы ошибиться всегда есть вот Поэтому бояться этого не нужно и нужно проинструктировать ваше сопровождение тех людей которые сопровождают ваш пром о том как с этим бороться как только сервис начинает падать по по памяти нужно сразу снимать Hip Dump Вот Но есть нюанс Если вы п Dum снимаете с прода пожалуйста не заливайте их в паблик шары и не давайте там разработчикам на анализ потому что в дампа с прода обязательно есть в памяти могут лежать пароли могут лежать Sens данные какие-то данные пользователей поэтому анализировать их лучше тем же коллегам вашим Кто и сопровождает пром анализ на самом деле триан есть прекрасные инструменты например Memory Anal Tool от э калипса Мне очень нравится туда загружаете Дамп она сразу же вам эта штука говорит Я подозреваю что течёт вот здесь и очень часто действительно там и течёт кроме этого можно снимать периодические дампы памяти есть инструмент G прой который также позволяет сравнивать между собой дампы и каких объектов прибыло каких убыло также рекомендую с помощью например стандартных средств вроде микрометра прометеус и графа настраивать и мы у себя вообще настроили алерты что когда любой из наших сервисов достигает потребление памяти в 95% сразу же подключается поддержка сразу снимают Hip Dump перезагружаю чтобы вернуть его в норму и дальше Hip Dump уже анализируется чтобы скажем так не свалиться в ошибки ещё одно испытание для вас график уже изменился теперь на графике есть два скажем так взлёта резких временной период тоже изменился всё это произошло буквально за полчаса и интересный факт э это произошло год назад на прошлом Хай лоде соответственно вот спрашивали у нас коллеги Работаете Вы или нет да на конференции вот собственно мне звонили уточнить скажем так чтобы я помог в диагностике этой проблемы Что же у нас случилось Давайте подумаем Кто считает что причиной были неожиданные бизнес данные есть попадание А кто думает что неправильно настроили поды Окей А теперь давайте посмотрим что же было на самом деле Проблема была в неожиданных бизнес данных что за данные и кто их не ожидал выгружали отчёт из системы ВС просто Отт по транзакциям когда аналитики проектировали этот процесс и примерно прикидывать информацию было принято решение Давайте возьмём один терминал один день и будем Вот это этими блоками выгружать терминалы - это устройства которые вы все видели в магазинах вы рассчитываете с картами по этим терминалам практически каждый день и примерно аналитики прикинули что ну сколько по такому терминалу можно операций в день прокатить Ну предположим там операций в минуту это если прям постоянно кто-то рассчитывается там получается около 300 в час и 3500 в день 35 операций в день легко помещаются в памяти они не такие большие и всё Казалось бы прекрасно даже предположим если там увеличится это всё в два раза каким-то образом то будет всё всё нормально и этот бизнес-процесс прекрасно работал на протяжении или лет никаких проблем не возникало но в какой-то мо слышали что начали запускаться транспортные проекты терминалы стали ставить в автобусах там изменился изменилась механика их работы там увеличилась скорость и самое интересное что терминалы с этих операций с этих терминалов они агрегированный и тот отчёт который раньше всегда работал там с п7 тысячами транзакций теперь в него дали 500.000 транзакций миллион транзакций и соответственно что случилось на этом графике сотрудник запустил выгрузку Вот и мы получили первый пик сервис загрузил в себя 500.000 транзакций и начал их обрабатывать но сотрудник оказался нетерпелив в течение 20 минут он не дождался вс-таки своего отчёта и думает Ну запущу ещё раз соответственно запустил ещё раз и получили то что ЭБ с не вариант что мы заранее не знаем Сколько может быть операций на каком-то терминале поэтому мы начали эти операции по тысячам нумеровать ввели некий батч и номер этого батчат к каждой операции и выгружать уже начали по одному бачу то есть мы получили некую фиксированную линейную сложность и нам неважно теперь сколько будет операций по какому-либо терминалу Мы всегда знаем что их будет у нас в памяти там тысяча или несколько бачей по тысяче база данных огромная сложная гигантская большая тема которую можно мне кажется конференцию собрать на тему базы данных Почему у вас может упасть приложение из-за базы данных причин огромное количество но если хотите детально пообщаться то можем сделать это отдельно в кулуарах но сегодня я остановлюсь на нескольких оно самых частых проблемах как я считаю л соединений на иллюстрации схематично скажем так представлено Что такое пул соединений между вашим приложением и базой данных вообще честно говоря если у вас любые две точки соединяются между собой у вас обязательно есть какой-то л чего-то очень часто это соединений и он обычно лимитированный най работе там какой-то сервис скажем так невысоко нагруженный никогда в эти лимиты не упирается и вы о них даже не знаете то в хайде лимиты - это наше всё соответственно очень часто мы э с ними сталкиваемся что хочу посоветовать в этом плане во-первых э знайте о том что этот пул есть если вы работаете с двой если у вас springboot у вас под капотом там gpa у gpa hibernate у рне ри и вот hicar - это и есть наш пул который мы настраиваем а Многие знают о том что его можно настраивать э о том что можно настраивать его максимальный размер но не догадываются о том что можно настраивать Тай Out э вот есть опции idle Timeout и минимум idle это позволит вашему приложению закрывать соединения э когда они ему не требуются таким образом Когда вы разворачивается не знаю 100 микросервисов и если по дефолту каждый из этих 100 микросервисов потребляет 10 соединений то вам понадобится 1.000 соединений открытых в пост гре А я вам так скажу 1.000 соединений в пост гре настраивать по дефолту крайне плохая идея столкнётся с большими проблемами а при таких настройках можно на 100 приложений э использовать пул в 100-200 соединений его будет хватать Когда вы исчерпывает соединений ваши запросы становятся в очередь и эти моменты очень важно ловить соответственно стандартный пул мониторинга микрометр прометеус графана позволяет строить прекрасные графики вот один из этих графиков на экране всегда когда у какого-либо из наших микросервисов заканчивается пул соединений запросы которые находятся в ожидании соединения мы видим всплески на таких графиках и иногда это нормально если это нагрузка идёт скажем так БРСМ быстро и много нужно сделать несколько запросов и чуть-чуть встали в очередь это Окей но в целом длительные ожидания запросов - это по сути деградация вашей системы поэтому тут нужно подключаться и анализировать анализировать что будем анализировать вот на слайде вам пример скажем так здесь есть Первое это настройка для вашего там Java приложения которая позволяет ему сказать базе данных что это именно ваше приложение подключилось оно представляется именем и вы будете это имя видеть везде во всех запросах и статистика которые вы снимаете с базы и второе - это запрос из таблицы там PG Activity если мы говорим про постгрес это очень часто самое простое решение понять что у вас не так если вы видите деградацию базы данных делаете запрос на просмотр текущих активных сессий в данном случае этот запрос чуть-чуть подправки соединения Какие микросервисы эти соединения расходуют И дальше уже подключаете сонно либо дба либо сами начинаете расследовать В чём именно детальная проблема Кроме того что проб мот быть скажем так с базой данных в нашем случае в нашей ло системе база данных - это живой организм она сама думает Это я вот про пост гре Она иногда сама принимает решение там есть анализатор скажем так планировщик который иногда сам выбирает план и подсказать ему невозможно только сбором статистики и проблемы с которой иногда сталкивались это когда у вас ВС работает прекрасно никаких нету ничего не происходит всё стабильно и раз в какой-то момент пост гре говорит А теперь мы будем использовать другие планы и всё начинает работать жутко медленно всё начинает падать вы не понимаете что происходит иногда достаточно прибежать запустить на таблицу Алай и пог такой Ладно был неправ возвращаюсь к нормальной работе и вот я бы с удовольствием Пообщался с кем-нибудь из вас Если у вас есть опыт в таких проблемах как вы это решали но мы у себя в некоторых ситуациях нам помогало выставить опцию ш Mode Force generic Plan и где-то Это помогало где-то нет собственно ситуация такая э пуля не серебряная пуля очень полезно отчёты выгружать и уметь читать как разработчикам так и dbe и у нас был инцидент который перевернул моё представление о проблемах запросами Я всегда думал что долгие запросы приносят проблемы быстрые запросы проблем не приносят поэтому когда я вижу долгий выполняющий запрос Я всегда тыкаю Пальцем в него Ага это он виноват Давайте его оптимизируем сделаем быстрее и станет лучше и круче но перед вами есть запрос с виду несложный есть интересная конструкция distin on Это был единственный запрос у нас в нашей системе такого пла он сделан с Джоном всего на две таблицы и сделан для того чтобы используя мозги пост гре вытащить сразу все необходимые данные которые нам нужны не прибегая скажем так к обработке в коде там в Джаве но как же я был удивлён когда такой запрос он выполнялся быстро за миллисекунды но создавал по сравнению с остальными запросами сумасшедшую нагрузку на цпу в то есть грубо говоря один этот запрос он выполнялся там он занимал 5% от общего числа но создавал нагрузку 60% на цпу и исправление мы его с лёгкостью переделали заменили на два добавили обработку в джаву и наши проблемы ушли но сам факт что такое возможно Для меня был интересным сюрпризом илиш который используете же очень часто может быть причиной проблем потому что такие омы разрабатывают скажем так для мас Маркета И когда вы сталкиваетесь с ситуацией когда ваша нагрузка она уникальна она очень большая вы можете обнаружить забавные ситуации например когда мы профилировщик смотрели запросы хира обнаруживали что Перед каждым ирм он делает для того чтобы понять есть ли у вас в базе эта запись уже Действительно ли я должен сделать ирт или я могу сделать апдейт вместо инсерта и Наверное это не всегда то что вы хотите Когда у вас время вам нужно данные вставить вы думаете я делаю один запрос на самом деле вы делаете два и время у вас значительно деградирует но также мы сталкивались с очень хитрой проблемой наш любимы оет пла запросов если у вас много в вашем сервисе запросов По примеру как показано на слайде а именно мы обновляем или селекти какие-то данные и в этих данных указываем Ключевое выражение in и перечисляем какие-то идентификаторы и этих идентификаторов у вас там не не пять или четыре вариации как на слайде а тысячи То есть вы вот передаёте какие-то данные айдини в и и получаете ответ наш сервис падал один раз в месяц по памяти когда мы проинструктировать соответственно наше сопровождение вовремя сделать Дамп памяти что мы обнаружили вся память была забита планами запросов кашированные планами запросов и там везде был хибер а он Кроме этого ещё какие-то модельки туда цепляет какие-то данные вспомогательные получается что один план около меб весит и там было две там 2.500 этих планов на каждый вот такой ин соответственно есть простейшая опция enclose parameter padding которая конкретно эту проблему вылечивает и у вас э планы начинают Каширова по степени двойки то есть там 2 4 8 там 512 и вот эти промежуточные вещи они не сохраняются дополнительно хотел бы сказать если вставляете много данных вставляйте их Бача проводите профилактику вашей базы данных заботьтесь это практически живой организм а индексы есть не обычные А их по-разному называют частичные условные или предикатное довольно простая реализация не требует много места на жёстком диске очень помогает ускорить процессы почитайте если не знакомы с чем что это такое и конечно же самый лучший запрос в базу - это Запрос который не был сделан вовсе Поэтому если у вас есть данные справочники Если у вас есть редко меняющиеся данные обязательно их каширу ите и Не заставляйте вашу базу данных выполнять лишний запросы dcp очень сложная тема сложная потому что проблемы здесь случаются редко и проблемы этого блока очень легко завалить железом ну во-первых самое простое с чем мы сталкивались это если у вас в организации есть фал Возможно вы с ним не дружите или он не дружит с вами Ну у фаерволе есть такая особенность они любят рубить коннекты которые висят и ничего не делают на протяжении какого-то времени вот у нас в организации был э и есть сейчас фал который рубит коннекты которые полчаса по которым нет трафика причём Когда происходит такой обрыв приклад ничего об этом не знает и продолжает работать и использовать это соединение соответственно там ошибки которые возникают они крайне непонятные Они ужасно выглядят страшные и обычно все их пугаются решается вот парай настроек Мы заставляем Мы например используем Project Reactor и наши клиенты соответственно tcp соединений нашего клиента настраивается таким образом чтобы соедине которые находятся в пуле без действия там около более чем полчаса они автоматически закрываются самим прикладом а не файлом и тогда таких проблем не возникает также интересная история у нас была с разработчиком который хотел сделать очень быстрый очень быстрый сервис процедуру скажем так как я сказал уже у нас Pro у него под капотом крав нету там Буе которые лимитированные количеством ядер помноженная на 256 и так далее и так далее соответственно есть в системе файловые дескрипторы tcp соединение - это файловый дескриптор и их количество в системе ограничено как на процесс так и на систему в целом и у нас была забавная ситуация собственно выкатили некоторый функционал на тестовый стенд слава Богу запустили этот сервис начал порождать тысячи tcp соединений одновременно делая запросы в другие сервисы тем самым превысил э количество файловых дескрипторов вместе с другими сервисами э на систему в целом и начали начал отказывать софт на виртуалке а первым отказал ssh всё по канонам скажем так и мы потеряли доступ к серверу пришлось подключаться через систему виртуализации для того чтобы этот доступ вернуть тайм-ауты Обратите внимание если в вашей системе есть многие это знают но вдруг для вас это будет новая информация Я рекомендую настраивать тайм-ауты лесенкой Если у вас есть несколько сервисов которые друг за другом обращаются то выст в тайм-ауты лесенки лесенкой в случае таймаута крайнего последнего звена вы получите постепенный красивый отказ и каждый сервис корректно среагировать и своим обработчика там не знаю сохранит данные какие-то которые в результате таймаута зафиксирует ошибку и так далее а Несмотря на то что Project Reactor есть который позволяет cancel сигналами реагировать и на то и на сброс соединения но собственно а рекомендую всё-таки использовать классический вариант настройки очень интересный инцидент произошёл у нас как раз таки с системой быстрых платежей на слайде у нас представлено три системы система А - это мы платёжный хаб система б промежуточная система и C - это внешняя система что у нас происходило мы посылали запросы из системы А в систему б и всё было прекрасно SL у нас там был 5 секунд и в какой-то момент Мы понимали что время ответа начинает увеличиваться с одно секунды постепенно получали полторы секунды потом две в какой-то момент Мы выходили за тайм-ауты у нас на тайм-ауты срабатывал рку Breaker это паттерн специальный мы замыкали это соединение переставали делать запросы давали системе промежуточное отдышаться в течение минуты возобновляет и снова всё налаживать и снова начинали постепенно увеличиваться время ответа мы задаём вопрос в систему ребят что у вас они говорят у нас всё в порядке в системе C тоже всё в порядке У всех всё в порядке Но проблема есть начали копаться более глубоко вот если Вы посмотрите на устройство системы B то красными прямоугольниками обозначенные очереди у нас была очередь запросов очередь ответов куда приходил трафик и зелёный прямоугольник - это http взаимодействие там стоял сервис соответственно сервис принимал запрос очереди и дальше отправлял запрос по http что же происходило мы вскрыли соответственно сервис который работал с очередями а обнаружили в нём классическую реализацию mvc там было всё вычитывает поток вычитывает из очереди запускается поток перенаправляет трафик в http дожидается ответа отвечает обратно в очередь и на этом всем Всё взаимодействие завершается А по логам наблюдали очень странную ситуацию тайм-ауты в этом сервисе достигали не тайма А время ответа от системы C достигало 15-2 секунд Хотя таймаут стоит на 34 у всех глаза на лоб Как так мы же настроили таймаут 3 секунды ответы приходят через 15 Почему мы Почему мы сами не тайма сделали дам потоков посмотрели обнаружили интересную ситуацию потоков в этом сервисе висели на ожидании соединения из Пула и когда начали разбираться это было springboot приложение у него под капотом rest template rest template - это тоже абстракция у лейта под капотом был apch Web клиент и каково же было наше удивление когда у apch Web клиента пул соединений 10 этот сервис работал годами раньше и нагрузка не достигала то есть система C немножечко замедлила время ответа из-за этого соединения всё больше и больше больше начинали накапливаться и почему эта проблема очень опасна потому что когда с ней сталкиваются В любой непонятной ситуации что правильно заливай железом если это решение масштабировать этих сервисов станет больше и этих маленьких пулов по 10 соединений тоже станет больше и проблему вы в целом пиете но достаточно всего лишь добавить одну простую опцию в данном случае мы Погугли это оказалось htp Max connections увеличили пул соединений с 10 до 100 проблемы решились всё стало прекрасно всё залетала всё всё у нас пропало всё заработало посмотрев на эту проблему в системе B Мы в своей системе в системе а сразу же начали пилить мониторинг на количество tcp соединений у наших веб клиентов обнаружили что projecta по умолчанию например использует клиент у которого 500 соединений и причём не просто 500 соединения А 500 соединений на каждый эндпоинт То есть если вы подключаетесь по дети разным эндпоинт грубо говоря 10 пулов по 500 соединений вот ну с каким-то максимальным общим числом и у нас родились вот такие графики и как только мы видим что на этих графиках цифры приближаются к птм этоже к о том что либо нам кто-то начал таймаута и нам пора масштабироваться дополнительно хочу сказать что здесь Э что очень важно Выбирайте правильные http клиенты Обязательно до того как вы столкнётся с проблемами узнаете про размеры своих пулов это не тривиальная ситуация Возможно придётся копнуть в код поглубже а настраивайте тайм-ауты правильно вовремя принимайте решение о масштабировании и здесь конечно же также действуют правила как и с базами данных лучший запрос по http - это Запрос который не был сделан ширу ите и веб запросы в том числе Большое спасибо за внимание оставляйте свою обратную связь по QR коду на слайде раз здорово болье спасибо конста много времени на вопросы зададим буквально пару вопросов потом перейдём в дискуссионную зону и те кто сейчас онлайн У вас есть возможность задавать вопросы прямо в чат у спикера будет потом возможность на них ответить там письменно или чуть попозже Окей поднимите руку У кого есть вопросы о сразу вижу руку я поделишься Да супер бое СБО прекрасный опыт Да который был сведённые мами Стоит ли вообще на них смотреть apm агенты Расшифруй аббревиатуру пожалуйста apm агенты - это те агенты которые ставятся внутрь приложения могут там ну какие-то на уровне приклада Да на уровне приклада могут вплоть до скорости отрабатывая процедур каких-то конкретных показывать мы не пробовали прямо apm агентов на серверах стоят обычные агенты мониторинга которые снимают общие характеристики и вот более детальные вещи Вот например размер там tcp Пула соединений мы мониторим своими своими библиотеками то есть мы дописывая некий код который распространяем на все микросервисы и уже там например добавляя дополнительные характеристики в микрометр графа Спасибо ещё вопросы так я вижу вон там рука но можем далеко не ходить Давай сначала здесь а потом уже сходим туда хорошо а подними ещ раз руку чтобы мы тебя заранее Ага всё вижу Добрый день спасибо за адекват У меня вопрос про по соединени с базой данных есть возможность делать в спринг есть возможность ставить перед пасом а да а соответственно когда Какой способ лучше применять или может быть лучше применять сразу оба способа Спасибо интересный вопрос э скажу почему скажем так у меня есть коллеги которые имеют опыт работы с PG баунс и они его не рекомендуют Вот Но я лично не пробовал Да поэтому я могу лишь рассказать про А в комбинации с настройками который позволяют вашему приложению сокращать размер Пула соединений э когда они его Не используют гораздо удобнее регулировать именно размер Пула на вашем именно приложении вот PG bouncer всё-таки Может вам создать некую борьбу за соединение если у вас нет Если вы работаете в условиях серьёзных ограничений Когда у вас есть 100 микросервисов и одна базе в которой вам выделено 10 пул из дети соединений то без PG балансера здесь просто не обойтись Вот Но если вы себя чувствуете комфортно то конечно лучше работать без него Я думаю спасибо спасибо там вот был вопрос который я обещал пока мы несём микрофон я хотел ещё раз попросить обратить внимание на QR коды и проголосовать за доклад потому что это очень важно Это тот фидбек который мы можем использовать и делать конференцию лучше Спасибо Константин Спасибо за доклад было очень интересно а вот ты намерена э убрал мониторинг диска и цпу ну на мой взгляд вот диск - это первое что отказывает в OT приложениях наверное вопрос Что у вас за система и почему у неё отказывает диск мы вот например в своей практике может быть у нас диски хорошие вот но э в нашем случае отказ диска - это его переполнение то есть мы больше просто добавляем место на диске где это необходимо чем у нас прямо отказывает именно файловая система может быть у вас какой-то трафик там или система которая работает очень плотно с диском Ну согласен да что это такой нишевый кейс где-то Ну вот цпу цпу очень важно Вот про цпу Раньше я не обращал так много внимания Ну цпу и цпу ядра и ядра Да Бог и знает сколько их надо там работает и работает вот но когда сталкиваешься с проблемами когда у у тебя база работает на 100% цпу ты начинаешь разбираться и выяснять А что же ты такого как как же ты так её нагрузил что она умирает у тебя под твоим прикладом понял Спасибо Спасибо друзья я думаю что на этом мы с вопросами всё сейчас Константин нужно выбрать э вопрос который тебе больше всего понравилось чтобы мы вручили подарок ребят Если у вас есть ещё вопросы к спикеру их можно будет задать в дискуссионной зоне А пока у меня вопрос к тебе Я думаю про PG bouncer мне мне понравилось Я бы с удовольствием с ним поработал Вот хорошо Тогда тогда Ждём ждём ждём подарок"
}