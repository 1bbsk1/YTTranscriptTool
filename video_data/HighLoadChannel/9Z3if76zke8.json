{
  "video_id": "9Z3if76zke8",
  "channel": "HighLoadChannel",
  "title": "Опыт построения СХД на базе Windows Server / Сергей Груздов (DataLine)",
  "views": 2569,
  "duration": 2441,
  "published": "2017-04-22T14:48:17-07:00",
  "text": "добрый день уважаемые коллеги у меня зовут сергей груздов я работаю в компании do the line и занимаюсь поддержкой развитием решение построенных на базе фирмы microsoft немножко компания do the line компания the line является одним из самых крупных сервис-провайдеров в россии имеет два собственных дата центра в москве также насколько я знаю еще один то центр строится питере предлагает клиентам услуги размещения оборудования аренды оборудования аренда каналов связи и услуги публичного облака перед тем как я начну , сделать небольшое лирическое отступление рассказать немножко теории что такое программы определяемые системы хранения данных и зачем они нужны программы определяемой системы хранения с инициатива программы определяемых системы хранения данных появилась примерно в 2013 году и она предполагает отделение уровня дисков от уровня управления то есть все что вам нужно это чтобы жесткие диски предоставлялись как сырые данные для системы управления почему стоит использовать программно-определяемые хранилище вы получаете независимость от производителя оборудования так как вам все что требуется это чтобы диски предоставлялись в сыром виде вашим управляющему программному обеспечению также вы можете использовать уже существующие оборудование для построения программы определяемых система хранения данных какие на данный момент существуют программно-аппаратные решения это раечки p4000 love and одни из пионеров и msi его rail нутаникс и кластером бокс это сервис предоставлен предустановленную windows storage server историк смесь из моих существующие программные решения этой весны фильмы в область собственная инициатива сев тот же hp по 4000 лет скан предоставляемый виде виртуальных up lines of dense скилла и не совсем известный страницам и 2 решение от фирмы microsoft это стороны isis и появишься в 2017 году стоит space darex составе windows server 2016 ну а теперь собственно небольшая грустно и поучительная история о том что нельзя строить большие решения не понимая как она работает как я уже говорил раньше компания to the line предоставляет услуги публичного облака но до конца 2015 года эти услуги предоставлялись только в облаке построенного на базе технологий в мае 2015 году компания решила войти в программу партнерскую программу microsoft кого до сотни только которая предполагает построение публичного облака на базе технологий microsoft и вы из красной архитектуре содержится программно-определяемой а система хранения данных storage spaces на момент моего трудоустройство в компанию do the line к сожалению решение уже было построено имела следующая архитектура 3 кластеров управляющий вычислительный и сетевой а также система хранения данных представляющие себя два года до уповал едва сервер dell poweredge сетевая фабрика между ними 40 мегабит и как оказалось уже в момент когда нужно было сдавать решение когда должны были проходить приемо-сдаточные испытания системы хранения данных начало отказывать то есть в чем это выражалось развертывание одной виртуальной машины происходило примерно 40 минут из них от 30 минут происходило только копирования шаблона в кластерах развертывание 2 виртуальных машин приводила к тому что клиент начинали жаловаться на недоступности своих виртуальных машин то есть все пролегала полностью пообщавшись с людьми которые строили это решение я выяснил что они понятия не как это работает все было сделано по каким-то статьям в каких-то блогах nyx nyx finish и все и как бы должно было бы работать но как оказалось она не работает я имел представление что такое страсть месяц но решил познакомиться с этим поближе потому что эту проблему мне предстояло решить каклы вид архитектура строй space is на самом нижнем уровне это сразу же бот подключенный сервера вам в кластере и предоставляющий туда сырые диски на самом деле это реверсная архитектура кластерном новом диске могут подключены по любым путем самое главное чтобы они были доступны всем узлам потом поверх из этих дисков собираются сторож пула сторож плуг создаются виртуальные диски которые саботируют файлы системе in теста серии fs и дальше предоставляются клиентам как кластер шарит в альянс моем случае клиентами выступали гипервизор и немного о возможностях и организации хранения стоимость поэсис позволяет организовывать многоуровневые хранилище то есть если у вас есть диски разного типа вы можете сделать из них разные уровни и использовать либо для создания отдельных дисков в каждом уровне либо для создания диска который покрывает несколько уровней сразу тогда опять же как оказалось самый быстрый уровень будет выступать в роли в рай тока шоб то есть все данные оседают на нем потом они по расписанию ночью будут перемещаться в связи с определенной степенью горячей или холодной следующий момент это кэш как я говорил уже есть выйти в кэш по который в принципе в документации написано но это была поделена опытным путем есть right back кэш который при наличии создать для каждого виртуального диска выделяется и служат для обслуживания случайных операций записи что немного повышает производительность и также есть конечно чтение которые по умолчанию включен но имеет нулевой размер то есть такая маленькая странность от разработчиков он был выставлен максимально допустимое значение это 80 процентов доступный сервер оперативной памяти то есть и того в каждом сервере пашня четыре байта размер кэша получился больше 90 гигабайт на чтения и это все позволило существенно повысить скорость чтения однако скорость записи оставляло желать лучшего вот как выглядит организацию руни хранения на в документации на техно team возле этого я взялся за оптимизацию сетевых настроек стороны isis используют в качестве транспорта протокол smb 3 который во первых поддерживать любимому титану это если у вас серые хранилище имеется несколько сетевых адаптеров расположенных в разных магических под сетях сервере клиенты могут работать с этим системы хранения понять каким путям сразу что существенно снижает сетевую нагрузку по умолчанию на заявлено но для того чтобы это правильно работала необходимо непосредственно сервером указывать какие сети они могут использовать для если бы multichannel потому что у меня есть 30 одна управляющая и 200 хранения так вот у меня сервере клиенты очень часто выбирали мы можем он сеть для работы системы хранения данных стоп не очень хорошо отражалось на производительности также была произведена также по кокосам b3 поддерживает россе вердумом меркам может ethernet эта технология прямого доступа к памяти по сети я чуть позже покажу слайд и объясню как она работает также были был произведен на стойку настройка дата-центр bridging технологии который позволяет тегировать трафик еще на уровне операционной системы в зависимости от направления типа и сетей как работает с тобой директ то есть smb 3р дума без за дума ситуация выглядит так клиент запрашивает у сервера данные серверах читает свой бусин потом перемещает в бусин драйвера транспортного полового потом эти данные перемещаются гусева драйвер сетевой карты потом они попадают в бусах самой сетевые карты передаются на другую сторону и цепочка в обратном порядке драйвер так ты дарил по такого драйвер сервера и только тогда она попадает в приложении с использованием технологии rdma все выглядит немножко по-другому клиент запрашивает у сервера данные сервера непосредственно на карте выделяют себе область памяти в которой начинает писать эти данные тут же передаются на другую сторону в такой же буфер который выделил клиент то есть от сервер и клиент они выпадают практически мгновенно и сразу же попадают в приложении это позволяет в нашем случае позволил на 20 процентов увеличить сетевой производительностью после всех проведенных настроек скорость чтения существенно повысилась очень существ однако с операции записи по-прежнему происходили очень плохо что для чего понадобилось спуститься еще на уровень ниже и убедиться еще раз старая истине что если сделать если хочешь сделать что-то хорошо сделай это сам когда я спрашивал людей которые все это разворачивали по поводу прошивок к адаптеру и жестких дисков мне ответили что мы все проверили все и хорошо я решил это проверить сам и по странной по странному стечению обстоятельств оказалось что window буквально два месяца назад выпустил прошивку которая исправляет как раз мою ситуацию для проблемы с записью в классных ssd-дисках доски дисках которые используются в кластере после обновления прошивки на ssd дисках ситуация записью вы равнялась по крайней мере сейчас множественные операции записи не приводили к тому что исходя вставала но опять же скорость записи же оставляло желать лучшего поэтому мне пришлось спуститься совсем на низкий уровень как работает ввод-вывод физического диска жесткий диск обменивается с операционную систему используя по используя одну характеристику сектор это это объем данных который на которые делятся все данные либо на блине диска либо микросхемах то есть минимальный и максимальный объем данных за один раз как который может прочитать жесткий диск для начала какие существуют типы дисков каким типов секторов первый тип диска это на этих 512 когда логически сектор и физически сектор равны 500 20 байт логических сектор это та единица с которой работает операционная система физически сектор это то с чем работает контроллер непосредственно самого диска какими блоками он записывает данные вниз следующий тип диска 512 е это когда логически сектор 512 байт физический сектор 4 килобайта почему появилась такая разница объему дисков растут область 512 байт становится все меньше и меньше меньше при записи на диск и поэтому во первых нужно держать очень большую таблицу размещения во вторых как пишут вендора это привело к некоторым проблемам с корректировкой ошибок при записи чтение поэтому решили сделать 4 килобайта и чтобы переход не был сильно болезненным сделали диск со смешанным типом секторов и есть третий тип дисков в этом веке в 4к где физически логически сектор равняется 4 килобайта это все и на вы мои диски на данный момент и есть некоторые создать диски как с такими же характеристиками а теперь что происходит когда в пул попадают диски с разным размером логического сектора в моем случае это были на этих 512 sata диски и 512 е ssd диски на самом низу у меня диски 500 512m имеющий логический сектор 512 байт здесь стоит обращать именно внимание на размер логического сектора потому что физическим сектором в конечном итоге оперировать только сам жесткий диск поверх этого был создан том по кому-то страну не знаю алгоритму логических сектор был выставлен 4 килобайта был взят равным физическому сектор я разговаривал с производителями с представителями предводители исходя тип хулят packard и дал они придерживаются того что логически сектор 512 байт вне зависимости от того какие типы дисков внизу этой позволяет избегать проблема описанных далее а поверх всего этого создаются виртуальные диски 500 20-е то есть у них логических сектах 512 байт это делается по умолчанию то есть незаметно для человека и если нужно создать другой тип диск этот из мастер это невозможно в итоге вся вот эта цепочка когда у вышли за ешь его диска размер логического сектора меньше чем у низ лежащего приводит к тому что включается политика ритму give a white что это значит рассмотрим на примере записи 500 20 байт новый сектор то есть когда вышестоящий диск пишет нижестоящий ниже стоящая подсистема сначала читает 4 килобайта данных потом изменяет там один кусочек 512 байт а потом записывает все это обратно 4 килобайта блоком то есть вместо одной операции записи 512 байт здесь выполняется три операции ввода-вывода что очень драматично влияет на производительность в нашем случае производительность упала в 8 раз из-за этой из-за того что случилось политика ринге fight like какие выходы из сложившейся ситуации могут быть мне нужно было либо переделать у всех виртуальных дисков размер 4 килобайта я сейчас вернусь немножко вот сюда смотрите когда вот здесь значение больше чем здесь это неплохо то есть как бы это на производительности абсолютно не сказывается ринге fight сайт работает только в том случае когда вот здесь меньше чем вот здесь то есть вот отсюда сюда запись будет проходить так же быстро как есть писалось 15 боевыми блоками то есть моей ситуации мне нужно было либо здесь сделать 4 килобайта но с учетом того что там уже в облаке были клиенты важные клиенты и порядка 200 виртуальных машин я этого сделать не мог физически поэтому я перестал пул с размером 15 байт здесь к чему это привело в конце будет таблица сравнения производительности я поэтому скажу теперь какие остатки решения всплыли в процессе настройки и использования ну собственно сложность настройки то есть много неочевидных моментов которые нигде не описанные которые приходилось узнавать 1 поточная дедупликации становится стандартным дедупликации использует всего от нитку процессора что делает невозможным использование real-time где публикации если вы попробуете это сделать это приведет вас к разрушению данных отсутствие или балансировки если вы добавляете в пул новые диски и изменяете размер тома созданного в этом пуле пустые диски будут заполняться только тогда когда предыдущие диски заполнить то есть нет регулировки данных данные не перераспределяются совсем ну и масштабированием масштабированию сложно тем что нужно покупать только дополнительные дисковые полки их много не поддерживается долго и неудобно параллельно с этим велись работы на будущее по созданию гибридные системы хранения данных это обычный storage spaces и доступны в середине 2015 года виде бета-версии строй спасибо рек который является следующим сдс версии 2 в реализации фирмы microsoft чем отличается архитектура строй спесь это сторож сторож спасибо rect на нижнем уровне находится уже не дисковые полки с общими дисками а просто сервера с локальными дисками поверх которых есть программная настройка называемая софт во сто раз бас который локальные диски всех серверов делает доступными в кластере то есть каждый сервер может обращаться к соседним диском серверов а дальше все как обычно скорость спаси сторож пулы виртуальные диски теперь уже рекомендованная файловой системы рфс потому что она претерпела большие изменения и вносит очень много всяких новшеств например лазеру young быстрое выделение там фиксированных жестких дисков real time killing и прочие момента и также все это дальше презентуется как ластик шире твой линз для клиентов так изменилась резервирования данных storage space резервирование данных происходило линейном то есть файл целиком не разбивались на блоке размещался на дисках что влияло и на скорость работы с этим файлам и на отказоустойчивость 2 спасибо этот подход изменили сейчас каждый файл делится на виртуальные блоки extend и размером 1 гигабайт и эти блоки в зависимости от степени резервирования здесь показаны резервирования тройной зеркало когда для каждой копии данных хранится еще две копии эти extend и распределяется по всем серверам в кластере распределяются так чтобы копий одного и того же extend а на одном и том же сервере не находилось это позволяет во первых повысить отказоустойчивость во вторых это позволяет повысить скорость работы дисковой системы в целом потому что задействован будет ни один диск а много минимум три штуки при одной операции чтения записи ну и собственно другие изменения которые произошли просто настройки ну так написанную маркетинговых материалов на самом деле это не совсем правда но настраивать директ намного проще чем стандартный строй многопоточной дедупликации теперь do you tube lekarz используют более одной нитке более того вы можете указать сколько менее уметь так она можете спорить сколько может использовать максимум возможного там где дупликация то есть этот процесс существенно улучшен и нужен использовать различные сценарии 100 тысяц обычный возможно использовать только в так называемом конверт сценарий когда вычислительной мощности и мощности хранения разделены hyperconverged сценарий возможен но не поддерживают фирмы microsoft данном случае в случае стоит спастись директ вы можете на свое усмотрение либо делать мощности хранение отдельно либо делать их совмещенными с вычислительными мощностями все зависит от размера структуры гибкой масштабирование все что нужно это либо добавить дисков либо добавить еще серверов в зависимости от того какой сценарии вы используете что именно вам нужно добавить и одно из самых интересных новшеств которые отдельно вынесет мульти риса линз virtual disks это когда виртуальный диск может использовать различные резервирования для различных своих участков то есть вы можете сказать что вот у меня 30 процентов диска используют резервирование типа зеркало и используют диски которые находятся на ssd а 70 процентов используют резервирование типа парите находящийся и находится на sata дисках в этом случае вся запись пойдет через самый быстрый кету в диске также и используя real-time ты ринг данные будут в режиме реального времени перемещаться вверх и вниз это рекомендуется для сценариев когда нужно сбалансированное соотношение между производительностью и отказоустойчивостью в случае когда нужна большая производительность рекомендует использовать легкого случае когда нужна большая отказоустойчивость рекомендуют использовать по этим ну и вот небольшая интересная табличка это сравнение производительности как было до когда я пришел все тесты проводились внутри виртуальной машины иногда нескольких случай заставить спейси это удалось запустить на одной виртуальной машине потому что запуск позвоночного теста на 2 виртуальной машине приводил нехорошим последствиям то есть чтение внутри было 800 мне говорить секунду запись всего 100 после всех настроек перестройки пула после пересоздания по ситуация стала такая чтение то есть вот этот тест проводился одновременно 10 виртуальных машинах и все они показали пример 1 тот же результат 3 гигабайта в секунду на чтение и гигабайт на запись то есть видно что скорость выросла минимум в три раза и запись вымысла минимум в 10 раз также storage space is до рек тесты правда вообще чистая синтетика проводилась в виртуальных машинах которые имели по 5 дисков на одном диске система два диска лежали на и создадим имели типу среди используясь количестве горячего кэша два диска лежали на sata как холодные данные и в итоге четыре виртуальных машин и дали мне вот такие показатели 2 гигабайт в секунду на чтение гигабайт секунду на запись ну и собственно что я хотел сказать заключение коллеги перед тем как вы собираетесь внедрять какое-то более-менее серьезное решение ознакомьтесь тем как она работает это позволит вам избежать огромного количества проблем момент эксплуатации спасибо вопросы договорить вопрос такой это вы сказали что это сейчас в это тестирование не почему в октябре 2016 года был официальный релиз все это уже можно использовать и такой еще вопрос какая версия 2016 вот стоят это два дополнительных чего-то лицензия все это все только лицензионной политики регламентируя 107 отступление раньше в 2 вот майкрософту баллы вот эту политику то что у нас разные редакции разный набор фич вот был буквально до версии такой сейчас это вернулась редакция стандарта редакция дата-центр как раз отличаются вот наличием в дата-центре сто раз пасиз директ и некоторых сетевых фишек еще там дата центре сразу включен да конечно можете всегда их брошь спокойно использовать спасибо у меня вопрос а стоит директ в продукте фолз обрушу скоро вы самая самая основная проблема это убить руководство в том что это надо и купить дисков я забыл сказать о том что у нас готовы все у нас есть вот вещь вычислительный кластер это 14 серверов у них в буквами в бак пленить 12 мест под 2 план дюймовые диски из которых занято всего 2 то есть я в каждый то есть я могу еще 140 диск туда воткнуть и собрать огромный простор и что директу тогда два вопроса если плюс то есть может ли он поскольку просторах space рассказе запуска на 2 убивает весь кластер не не это это это запись это был было из-за того что это была из прошивки вот там ситуация была ствола из-за прошивки авто случай косо я вот по этой говорил с моим есть де себе начиная 2012 r2 где ты говоришь что у тебя для если любит трафика вот для этой подсети по порту 445 тебе нужно резервировать ему 50 процентов полосы пропускания мы так и делаем ее целиком забиваем и выставлять такой-то так на уровне уже сетевом на свечах ты делаешь qos policy горит что у тебя вот с таким тегом трафик имеет такие то вот вам пакета столько надо резервировать посмотри дизайн гайд по много нокса томате у меня вопрос все-таки провод живут на стоит да вот если мы его будет запустим синтетически guest и там есть еще 3 клиентам вообще работать сможет конечно может нет у тебя здесь здесь используются все сервера то есть есть storage spaces у тебя все используют одно общее хранилище которые вот у нас подключена потом 2 6 я битным со с контроллером и мы в итоге все он упираемся в dice с контроллером и может быть и больше бы вы жили оттуда но они больше не пропускают ну то есть у меня ограничением будет сесть локальную сеть встретить очень поиски локальный диск штука не а ее дают дадут они столько тебе выжму если под иса тоски вот но у меня все равно медленным местом в этом случае будет сеть всегда сеть здесь да здесь нужно использовать уже везде рекомендации пользователь дума для снижения латексе нужно использовать разум а вот я недавно сталкивался с моментом то что людей виртуальные машины виртуальные файловые серверы покажу неплохую производительность покинули дома вот штатском можно покидать дома внутри виртуальных машин уходит он тебя патент все залетела ремонт дома давайте я вернусь чуть назад ритма это ремонт direct memory access это если мы говорим про infiniband если мы говорим про изогнут р-россия думаю о вертолётчица за рунет вот то есть это как работает без гордума то есть у вас трафик проходит всю цепочку от начала до конца случае сыр думаю все посылки пацану система не просто откидываются и у вас приложение напрямую общается с другим приложением по сети то есть память общается с памятью и вот эту технологию в 2016 если ли вы используйте гипервизор 2016 там это возможно покидать внутри виртуальных машин и они точно так же если приложение поддерживает оси они будут общаться совершенно спокойно разрешите вопрос буду организма с нагрузкой по сети нет резкого скачка при этом если у вас что-то жирная обменивается данными друг с другом ну вы представьте себе утилизировать рака гигабитный труба у нас не получается у нас даже вот у нас выделено пока 50 процентов по трафик там несколько тысяч виртуальных машин по моему более двух или трех тысяч и мы не можем должны быть даже утилизировать то есть там это все настолько быстро походит что как-то сложно а в интернете еще есть там всякие пафосные рекламные материалы простой гигабитные сети там на директе выжимают больше 8 миллионов опусов совершенно спокойно вот говорили про с дс-2 так расстроена они что-то сделать протестировать вопрос такой как небо полки подключать уже есть инфраструктура были накоплены полки и мы выходим к тому да вот эта картинка как раз к тому что нам нужно иметь жесткие диски причем который будут не разбитой не подключены вот они отдельно стоящем и закидываем в кластер как полки закидывать там я так смотрите как бы у вас обычные полки диск вы вам нужно их подключить к серверам и диски как джобот покинуть внутри все то есть вы их прокидывайте как джуба и они будут видны всем серверам но тут здесь как бы у вас тогда получается что каждому серого должно быть подключено по одной полке иначе смысл теряется абсолютно либо вы берете сервер пустой тыкайте то есть вас есть три полки и 10 серверов вы в 3 сервера втыкаете полки имеющиеся остальные добивайте дисками из всего этого собираете пул такая схема вполне как бы приемлемо и при этом в плане директа вот как раз возвращаясь вот к этой хитрые штучки там ситуация такова у вас вот здесь там диски разного типа как только вы включает как только включается software сторож бас у вас нет он у вас все диски становятся 4k сделано для того чтобы если у вас есть диски новой мои что вы как раз не было никакого падения производительности и нужно помнить о том что у вас вот здесь должны уже должны уже быть диски 4k windows поддерживает в плане linux я думаю там тоже как бы виртуальную машину будет всё это поддерживать потому что я проводил тест даже на голых сапожках я создаю виртуальную машину и диском 512 байт логически сектор запускаю тест к ней же подсоединяю диск расположен на том же в том же пули с размером 4 к тест на 4к на чтение в два раза быстрее чем 512 на запись в 8 то есть как я и говорил то есть вот политика ритма дефо и white примерно в восемь раз убивает вашу производительность можно тоже вопрос сейчас как раз тоже тестирую вот эту технологию мы собрали гиперконвергентное решение там до microsoft совет минимум четыре сервера ну но как бы вы тестировали audigy какие-то когда у вас сервера на которых развернут стоит спеси здарэ какого-нибудь выходит из строя просто я столкнулся с тем что производится rebuild как бы уже сказали там где по копии из данных датчан куда потом убил происходит и вот этот rebuild происходит долго все у меня нас от очках даже на сапожках не очень на 15 минут у меня любил биться даже когда надо вылетает все зависит от того какую диску никаких диски oz у вас физическое количество дисков именно сколько вас диетических дисков от волны на воде по два в номер подвеса тажке в ноги сейчас вот как бы вот вы же понимаете да можно сделать всё что угодно я у меня идея такая что вот в те сервера где остается по 10 свободных мест 4 ssd 6 sata шик почему так опытным путем установлено у вас количество в ssd должно быть либо точно таким же либо по количеству минимум 50 процентов от того вас есть сад ашик то есть иначе там будет небольшой перекос плане производительности и еще можно да конечно конечно в этой сцене валери fs да и вас с ней хорошо прекрасно ну то есть все заявленные то есть вот лизи изи руин когда вы сдаю диски фиксированного размера там в 200 гигов создается за секунду это бог с ним а все остальное это все остальное вы вспоминаете про историю с 2012 годом риф с версии 1 когда ума нет я смотрела на или за у меня wi-fi работает медленнее чем нтс размер блока вся 4k стандартный какой создается и звезда на такой наверное стоит вполне может быть а вы пробовали всмысле у вас она на просто форматированного диски медленной работает она выключали file integrity нет вот выключите этот это дополнительная штука когда у вас параллельно с файлом существует еще один поток то есть запись в файл происходит не напрямую она идет через file integrity стрим в офисе для сохранности данных то есть начали данные попадают стрим потом атамана накладываются на файл и это по это есть в документации она очень плохо влияет на производительность сет file integrity папочка окей и еще последний вопрос выйди дуб проверяли на на рейс вроде microsoft говорит что он работает типа сельской робки ну допустим ну есть де дуплицирование то я делаю out each потом я включается обратно и у меня все восстанавливается но такая такая сказочная история от работает вообще ну вообще это никого не работала вы имеете ввиду вот у вас есть сервер он вздыхает выбирайте нудистском подсистему втыкаете в другой сервер все поднимается нет я имею ввиду вот у меня есть гипер conver на решение из четырех у меня есть резервирование на уровне сервера он сдох я его достал поставил новый в окно обратно у меня все должно собраться конечно даром у вас же смотрите где публикации то логика то есть он заменяет дублирующиеся блоки где тут держит таблицу размещения а дальше у вас копирование это на уровне блоков уже пойдет там вы в живую это проверяли или не проверяли нет просто то что там написано то тогда вот я интересно увидеть людей кто это делали пока нет у меня и у меня понимаете меня нет пока техническая возможность неплохой такой очень сложно как бы у кого-то убедить в том что мне что это нужно вообще компании так ну ну конечно не должны ни вопроса такое да в каких формах обитаете чтобы можно было задаем фишечку я вам увидеть куда вы можете мне писать я сам просто не специалист нашей компании чем у вас маленькая можно вот вот вот вот сейчас быстренько добегу я потом контакты я хочу сказать я могу там какие-то проза вот смотрите смотрю здесь верхней это мой рабочий email нижний это мой личный e-mail то есть как вы понимаете вот это меняется жизнь такова что она может измениться нежели это моё то есть принадлежит мне она всегда есть хорошо вещь я запишу то пожалуйста все вопросов больше нет вернемся полкам вопрос опрокидывания джей бот дано нет вы спайки вас просто полка и какой-то умная парик это огромная балка и спишь на этом 2040 она очень много чего умеет и не хотелось бы прокидывать вот это вот эти вот все возможности когда ты можешь создавать там при физическом размере там на один там гигабайт или 100 гигабайт место ты можешь там на терабайт создавать там и просто потом запихиваю туда диски и тому подобные вещи вот поэтому в таком случае здесь вы идете в таком случае в разве разрез самой концепции software дефо нет у вас полка сама выполняете эти функции то есть о чем я говорил софтовое define как раз подразумевает разделение вот именно самих дисков от системы управления у вас уже все это есть и вам наверно как бы бессмысленно вам тащите полки использовать как есть либо предоставляйте g-кодом потому что в директ в принципе теоретически можно потому туда можно подсчитать массивы уже именно рейд массивы не g-коды но вы получите не поддерживаем решения то есть будьте к этому готовы вы можете как бы этом это такое есть но не поддерживается в случае любой проблемы выкладывание вводить как то так у меня про джорджии боксов это вопрос так вот меня сервера я смотрел что у рейд контроллер в них дай бог просто не умеет то есть диски напрямую отключить юрий контроля часто диски подключить включите raid-контроллер пусть презентуется как есть поддержка с воздуха до по поводу серверов которые не умеют jboss проверено например это дело вский сервера тут похоже да да да когда я создавал ту же самую конвергентной у систему на основе vmware wisan в принципе тоже самое только в профиль пришлось делать the raid 1 на каждый диск отдельно да и вот и вот только так их прокидывать потому что джей вот он не умеет поэтому на каждый диск один виртуальный диск в raid 1 и вот их там дофига и дисков было кстати сколько 20 дисков на сервер было ну raid 0 только наверное raid 0 да конечно и и в общем как бы это довольно утомительное занятие не надо покупать такие печаль ну тогда придется парагрипп напишите которые через rest api key через контролем сидел здесь делает массово ну раз больше вопросов нет спасибо коллег за внимание"
}