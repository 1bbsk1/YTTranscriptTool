{
  "video_id": "ljjXzZUpiJk",
  "channel": "HighLoadChannel",
  "title": "Самоорганизующаяся сервисная инфраструктура на базе Docker / Данила Штань (Точка)",
  "views": 6392,
  "duration": 2735,
  "published": "2017-06-28T07:09:08-07:00",
  "text": "Всем привет Блин как тут светло я маленький дисклеймер хочу сделать я вообще-то вообще не собирался выступать уж тем более не собирался выступать в самом большом зале поэтому я могу себя вести иногда немножко неадекватно вы тут все такие если что Ой у меня тут небольшие Не обращайте внимания Про что этот доклад он довольно практический Я хочу рассказать о том как в одной конкретной компании а потом ещё в нескольких компаниях решали одну довольно простую проблему Как построить инфраструктуру для продакшена в первую очередь таким образом чтобы ну в общем чтобы разработчиков не напрягать чтобы всем было удобненько вот с чего всё началось о всё работает прекрасно на самом деле всё началось с того что я пришёл работать в одну компанию проработал там пару месяцев и внезапно совпало такое Некоторое количество факторов во-первых эта компания была довольно большим интернет-магазином она в то время входила по разным рейтингам там в топ 15 интернет-магазинов страны в этой компании была группа из ти разработчиков которые всё пилили инхаус вообще там исторически всё писалось внутри от управления складом до управления логистикой до морды соответственно всех там корзинок и так далее Вот И там был один админ который рулил всем продакшена пл код разработчики писали он там делал л пул там на разные ноды нали него бы красно было и всё это жило в амазоне на 2 и тут внезапно случился конец четырнадцатого года все знают да что было в конце четырнадцатого года Amazon стал очень дорогой не просто дорогой он стал какой дорогой и в это же время единственный админ уволился Вот и мы такие 25 человек Amazon как потом оказалось не до конца написанный огромный интернет-магазин там оборот миллионы рублей каждый день падать нельзя ломаться нельзя жалеть тоже нельзя и в общем вот вот поэтому такой слайд Э мы поняли что что-то надо менять А поскольку ситуация такая что в общем-то уже пофиг Мы решили поменять всё совсем вот отринуть Legacy и все дела Что мы хотели сделать э во-первых ээ там всегда всё пили большим большим монолитом мы хотели полить Монолит на отдельные сервисы но не микросервисы а просто сервисы то есть сервисы там продуктовые команды сделать из 25 человек в одной большой команде сделать более специализированные но при этом мы хотели чтобы все друг другу не начали мешать мы хотели потому что админ же у нас уволился А мы парни умные книжки читаем инфраструктура как код и так далее так далее Далее вот плюс опять же роть то это же ВС будет проще всё будет быстрее Всё будет прекрасно работать меньше дерьма всякого Ой я не знаю можно такие слова говорить вот ну и Разумеется поскольку у нас админ был один и он с собака уволился Мы решили что новых админов мы нанимать не будем пока совсем не прип вот вот такие вводные Куда мы пошли Ну разумеется в докер потому что ну во-первых потому что нежная любовь во-вторых потому что я уже сказал мы хотели всё сделать с ноля и круто мы модные стильные молодёжные парни докер Хай все дела Вот ээ А вообще Вот это интересный вопрос для 2017 года а кто про докер не слышал вообще прикольно А кто с докеров хоя бы на локальной машине Круто А кто докер в продакшене хотя бы там на какой-нибудь незначимый сервис запускал Вообще супер А кто в докере в продакшене живёт целиком вообще ничего другого нету только докер контейнеры Ну пальцев одной руки пока хватает Ну ладно Но тенденция положительная вот так что очевидно мы не ошиблись Вот Но я напоминаю Это был конец чего пятнадцатый год и вот у меня есть такой слайд я его в Гугле нашл по запросу Одинокий Кит Дело в том что тогда докер была прекрасная proof of Concept система контейнеры там вот это всё DSL докер файлы и так далее у него была одна проблема он вообще никак не занимался вопросом решения решение вопро работы больше чем на одной машине на одной машине вотре больше чем на одной машине вообще ничего не было точнее как были разные настрое вот там тогда был не А все понимают о ЧМ Я да нет ну это в общем такие настрое в рапе вокруг Докера которые делали сеть между машинами прокиды контейнеры друг в друга и так далее начали смотреть разные варианты поиграли с тем с другим с третьим с пятым десятым а потом внимательно накер посмотрели и поняли что он когда внутреннюю сеточку делает он поднимает контейнеры и все их сетевые интерфейсы собирает в один Бридж Ну чтобы они в такой как бы в одной локальной сеточки были Вот и уже тогда у Докера но там же ребята модные тоже они всё конфигурировать позволяет вот поэтому есть такая прикольная тема можно сказать какой бриж он использует не свой докер но как обычно а какой-нибудь отдельный и можно сказать Из какого из какой подсети IP адреса выдавать контейнером и Фишка в том что если взять Бридж прицепить его на какой-нибудь внутренний интерфейс на физическую машину а потом повесить туда контейнеры то все э Конте Вика подсеть сделать из той же более большой сети что у тебя вся локалова кало и он будет видеть контейнеры на других машинах в локалова внутреннюю сеть для всех контейнеров вот я уже говорил что мы тогда съезжали с амазона поэтому мы поехали к одному очень известному Хосте в нашей стране Взяли у него кучу арендованного железа там десяток может 15 серверов все их в локалку Ну и всё айпишник предсказуемо на машинах разложили ручками под сети на каждую машинку выделили там вот внизу ссылочка есть потом где-нибудь презентация будет или можете сфоткать правда я не знаю как вы это набирать будете неважно Там просто пример того как мы это собирали вот Итак что у нас получилось есть мы хотели контейнеры они у нас получились ши мы на них поднимаем контейнеры и все контейнеры друг друга видят одна такая большая счастливая локальная сеть прикольно уже можно что-то делать можно там паковать приложение в контейнере поднимать друг к другу ходить SL rpc что угодно дальше встаёт вопрос как мы будем эти машины Ну как мы будем определять Кто где поднялся Тем более что если мы контейнеры поднимаем опускаем кажды шки разные сказуемого нет И мы взяли все знают что это такое а поднимите руку Кто не знает что такое консул много а кто знает что такое консул А почему у вас 100% не получается Консул - это там вот есть ссылочка это эту ссылочку можно набить легко это такой софт от ребят которые своё время сделали Гран потом они очень плотно упёрлись В то чтобы создавать всякие всякие тузы для инфраструктуры именно это такой распределённое хранилище всякой информации там внутри рафт по-моему Если это кому-то о чём-то говорит Ну в общем смысл в том что мы поднимаем несколько экземпляров друг их на друга их натравлю и они все одну и ту же информацию внутри себя сохраняют распределённое хранилище поверх этого там сделали соглашение Как ты туда переда информацию а потом этот кон уме Отвечать по Ну то есть можно условно говоря зарегистрировать свой сервис а потом с любой машины которая может в этот сходить спросить сервер там сервис с именем таким-то или и тебе отдадут самой-самой древней версии по-моему ещё до 06 если такая была вот э что у нас В итоге получается у нас есть Хосты У нас есть На них контейнеры контейнеры друг друга видят и вот у нас есть такая тулз которая позволяет всем контейнерам если мы а в докере же можно DNS отдельно прописать который внутрь контейнера передаться мы прописываем вот это и на каждой железной машине Мы поднимаем Консул в режиме сервера то есть вот у нас на каждом Хосте живёт вот этот кластер консулов В общем на отдельный Консул сервер на каждой хостой машине у нас там они всякая информация друг с друга обмениваются А дальше встаёт вопрос Вот мы контейнер поднимаем И что как рассказать где оно будет где какой сервис там живёт и так далее То есть надо в Консул как-то сходить там либо через AP либо через то же самый Консул но немножко другом режиме в клиентском запущенный Вот и тут у нас случилось первая религиозная война мы её называли внутри война тупо конечка со стро конечка потому что ну сейчас это уже не так сильно А тогда в пятнадцатом году когда докер только-только начинал вокруг него Хай подниматься было очень много споров докер это вот это вот для того чтобы изолировать один процесс Ну да там пи О вот это всё да когда всё окружение поднимается там стартует бинарник в докер файле ты прописывает который вот вот вот это оно или в контейнер можно запихать больше одного процесса Но тогда соответственно сначала поднять супервизор какой-то Там Ой простите или ещё что-нибудь мы очень долго спорили э э ну война реально религиозная была вот в итоге победили не помню кто кто-то кто-то из конеч В общем мы договорились что докер - это просто какой-то кусок нашей системы совершенно необязательно один процесс вот там внутри мы соответственно поднимаем какой-нибудь супервизор вот здесь вот есть примеры того что можно тоже фоточки или там потом слайды то есть поднимается сначала в контейнер супервизор он запускает уже все остальные процессы которые нужны это может быть приложение это может быть например локальный MD который используется только для этой ноды это может быть там не знаю крон например то есть тут приложение рядом тоже кро это тоже процесс вот ну и соответственно раз мы до этого договорились то там же в контейнере можно запустить консула Агент Это тот же самый консол только запущенный в другом режиме который никак в общем кластере не участвует но информацию туда засовывает соответственно когда мы поднимаем контейнер там запускается коген в нём уже прописан ког Мы же контейнеры собираем один раз заранее там ВС сразу зашито вот он поднимается регистрируется ВМ Вот Но только если у вас тупо конечки победили ещ один момент которым очень важно про который очень важно сказать это то что конс - это не только сес Discovery и вообще в принципе се сам по себе Это довольно Бесполезная потому что ну окей вы запускали а потом вы контейнер положили есть доже быть какой-то если он хоро опускается это понятно опускается там какая-нибудь софти на которого за это отвечает посылает сигнал в кластер я пошёл до свидания все там убирают это хозяйство айпишник из ответов днса пропадают но иногда же это не так иногда оно ломается иногда оно взрывается иногда оно выходит с ошибкой и так далее потом в консуле есть очень-очень полезный инструмент который называется чеки чеки - это когда ты регистрируешься Как проверить что ты живой Ну начинают все обычно там Давайте рло сходим на порт и проверим что там по htp 200 Окей возвращается или ещё что-нибудь Но вообще-то их можно делать любой сложности вот и это очень-очень важно потому что ну то есть можно же как сказать мы на регистрируем этих Но если там что-то Мёртвое там какой-нибудь НС или любой другой сервис Он же всегда Трай может сделать на следующий айпишник в ответе днса Ээ это конечно так Но рано или поздно какой-нибудь программист Василий напишет такой софт который будет ходить брать один айпишник из ответа потому что он просто не представляет себе что в ДНСе может быть 10 айпишник в ответе сходит на него у него всё сломается и вы это будете долго и несчастливо дебажить потому что никому кроме Василия в голову вообще не придёт что так можно писать код вот поэтому Discovery иче которые будут привязаны с сес Discovery Напрямую это очень-очень важно Вот Итого что у нас получилось у нас получились машины физические там лолочка контейнеры поднимаются все друг друга видят поскольку внутри каждого контейнера где это нужно А ведь не всегда контейнеры какой-то сервис предоставляют окружающему не могут просто так запускаться так вот там где это нужно запущен агент который регистрирует сервис в общем сервис Discovery регистрирует чеки все они туда-сюда ходят в общем всё хорошо Пока всё прекрасно Вот Но всё это живёт исключительно в пределах вашей локалова А как вы собственно Ну мы же вот в том случае были интернет-магазином нам у нас был сервис который назывался интернет витрина это публичный веб-сайт сервисы какие-то должны были давать А наружу для партнёров например какие-то сервисы поднимали админки для отдельных департаментов и подразделений внутри интернет-магазина должны бы Отвечать по там по публично доступным урла Вот как это порешать во всяких модных системах сейчас это называется ингресс Да когда снаружи в кластер приходит Там балансирует и так далее мы начали просто мы взяли и сказали у нас будет ещё один сервис который мы будем запускать также как обычно Ну только ему говорить что он должен слушать публичный порт на 80 Ну там доровское пробрасывается cons templates Это от тех же ребят которые сделали кол и в общем-то у этой софти одно простое предназначение оно смотрит в Консул и когда там что-то меняется оно может у себя локально сгенерировать новый ког и запустить какую-нибудь команду ну намер вот поэтому мы можем всегда всегда актуальный конфиг джинса и всегда актуальный всегда актуальну блин маршрутизацию там запросов внутрь на конкретные сервисы снаружи получать ещё когда ты регистрируешься сопроводительные этого шаблонизатор Поэтому туда можно например домен публичный записать запихивает сервис говоришь что у него тег публичный ТТП ещё один тег там такой-то пиш такой-то такой-то имя такое-то имя и с этой стороны генерирует всё в конфиг аппетит приходит во время еды поэтому э потом мы захотели чтобы он не только все сервисы автоматически проксирование не хотим в сервис пускать просто так а хотим пускать только тех людей которые авторизованы в нашем центральном но но при этом реализацию в приложении делать не хотим например Ну например мы так ставили и прятали фану да графана фану иногда хочется посмотреть снаружи не ло кало но при этом там либо своя аутентификация нигеро Но вот потом появился крипт и стало нужно поднимать https сертификаты автоматически туда ещё что-то добавилось в общем мы в итоге написали огромное приложение внутри Эй кликер о о вот тут есть несколько ссылок последняя ссылочка - это приложение которое мы писали оно в паблике совсем не то что у нас внутри работает но в принципе вы можете спросить у меня как оно работает я расскажу ну или там там там там есть ребята которые комитет можно их тоже попытать ээ Но на самом деле это большое-большое приложение которое добавляет вот там 10% функционала который на который потрачено 80% ресурсов по сравнению с console templates который работает из коробки стабильно надёжно и 80% ваших вопросов может порешать иногда контейнеры падают взрываются машины ломаются сеть пропадает на одной машине на нескольких машинах иногда Хот строя жёсткие диски Иногда просто производит неведомая фигня Вот и здесь у нас была тоже лёгкая лёгкая религиозная война о том чем мы считаем контейнер контейнер это он Ближе к виртуальной машине особенно Лютова остроконечные кричали что Ну вы же туда уже напихали кучу кучу процессов кучу сервисов У не У вас там работат это же виртуалка просто очень лка давайте к ним так относиться или контей этос который ты здесь выкинул тут поднял здесь выкинул тут поднял никакого стейта внутри не хранится и мы не не хотим ничего оставлять Но это мы быстро э задолбали И Ну в общем мы относимся к контейнерам как к расходному материалу никакой стейт внутри никогда не хранится это Разумеется накладывает на разработчиков определённые ограничения например они ещ в локальную файловую систему сохранить никогда не могут если пользователь загружает картиночка её надо отдать в какой-то внешний сервис по-хорошему это всегда в любом случае надо делать не хранить картиночки Просто так Или например встаёт отдельный вопрос А что сделать в таком случае с базами данных Да которые по определению должны всё хранить они должны быть Ну там стоит какой-то хранится данные же лежат и так далее Ну во-первых если мы вернёмся немножко назад про сеть то она же одноранговая обычная рядом с этим докер кластером поднять такие же машины у хостера и ставить туда базы как обычно как как любой нормальный dba Там ставит реплицируемый немножко поработать но нерушимых проблем нет потому что ну например в нашем случае у нас очень активно использовался мо SQL и rabit MQ и то и то требует иногда писать что-то на диск и сохранять эти данные Да если контейнер вдруг взрывается его надо поднять Там репликацию настроить и так далее и в общем не самой большой проблемой является сделать собрать такие контейнеры которые будут работать ровно так Ты поднимаешь контейнер говоришь ему где остальные ребята как это вот во всех модных там системах типа консула происходит он поднимается накатывает стей забирает данные включается в кластер и начинает работать в случае с SQL - это делается поверх галеры э Лера - это такая синхронная мультимастер репликация она нормально работает Я серьёзно говорю вот зубы вот в случае с кроликом та же самая фигня то есть не очень сложно написать небольшой скрипт который при подъёме контейнера подключится к кластеру И ска и в него войдёт я Всем рекомендую так делать не будьте ретроград Не ставьте базы на отдельные сервера пийте всё в контейнеры это прикольно Вот про ништяки которые мы из этого бесплатно почти бесплатные ништяки получили когда мы со всем этим экспериментировали ставили контейнеры поднимали приложения мы как новую версию накаты мы Разумеется контейнер ничего никакой не делали Мы собирали лье контейнер там была новая версия кода новый конфиг консула там какие-то ещё вещи потом шли в кластер его запускали ну точнее как сначала запускали старое потом поднимали новое гоняли миграции Потом пришёл маркетинг сказал ещё раз устроите на свой апдейт Ну понятно Вот мы остановились подумали и сказали ну подождите у нас же есть Discovery который автоматически всем рассказывает Кто здесь У нас есть чеки которые позволяют определить живая нода или нет причём живая Это ведь не обязательно мёртвая и не функционирующая это ведь может быть ещё не уже Поднятая но ещё не готовая к работе и если ты умеешь поднимать и останавливать контейнеры А в случае с дором Это довольно дешёвая операция то все ништяки типа там blue green dey Roll update и так далее оно вот из коробки У тебя есть единственное что тебе нужно это убедить всех своих программистов что писать миграции на базы данных нужно такие которые ничего не останавливают Ну там совместимость версии обратная совместимость схем баз данных когда ты пишешь новую версию она умеет работать и со старой версией схемы и с новой версией схемы потом ты Всё гасишь старое потом фоне чистишь старое и так далее это на самом деле довольно простые вещи нужно только программистов убедить если у вас палка хорошая Ну или программисты понят но после этого всё работает прекрасно и Ну вот например я очень горжусь Этой темой как мы обновляли э то что у нас называлось интернет витриной в кластере запускались новые там несколько контейнеров с новой версией кода они поднимались они уже были готовы там два из трёх холковский и это Это значит что уже туда можно посылать живой трафик Ну всё здесь поднялось там опустилось если здесь что-то не так здесь опустилось там поднялось в общем довольно просто работает И самое главное в этом нет никакой магии Ведь у вас все инструменты для того чтобы это самые та та там фишка ведь в чём в том чтобы была возможность сделать это быстро большинство проблем с апдейта они состоят в том что что-то нельзя сделать быстро Если у тебя есть инструменты для того чтобы ускорить эту процедуру всё работает хорошо а вторая плюшка была когда мы делали Для всего этого мониторинг там подняли илюк графа Ну нарисовали дашборды вот а в нашей парадигме каждая команда отвечала за свои сервисы Ну собственно откуда вся эта идея с контейнерами которые вот внутри себя всё заворачивают и так далее каждая команда пилила свой сервис и делал его так как как они считали нужным они делали нужное количество там контейнеров с разным софтом который им нужен что-то брали централизованное что-то нет фану каждому тупо поднимать илюк каждому тупо поднимать поэтому он был централизованный например но в фане артин тогда не было он сейчас не очень хороший А тогда вообще не было Вот и А других инструментов мы не хотели потому что нам же надо по хипстерские есть отличная сотина которая называется катор это такая штука которую ты можешь положить в свой собственный контейнер написать там свой собственный конфиг описать там алер который нужен конкретно тебе вытащить это всё в кластер и он там будет работать также то есть каждая команда пишет свой софт регистрирует его в Service Discover В общем пишет себе окружение такое какое им надо и артин за на который реагировать тоже им тоже пишет сама так как им надо внутри этого собственно контейнера ты хочешь что-то поменять Ты собираешь новую версию Конте отправляешь е в кластер и ну собственно всё работает вот что у нас Получилось Да уже практически всё всё что нам нужно было у нас получилось были контейнеры сеть железные машины друг друга сервис Discovery чеки всё внутри контейнера каждый контейнер внутри себя то есть всё что нужно команде собрать контейнер по заранее онни очень большому потому что в общем-то что там надо нужно правильно написать конфиг для того чтобы зарегистрировать свой сервис во внутренней сеточке и в общем по большому счёту больше ничего не надо главное чтобы оно работало вот Ну дальше там возникает вопрос того как всё это на нескольких там десятках машин запускать мы взяли сварм просто потому что ну просто потому что в него удобно смотреть это одна дырка для куда можно послать команду оно там само как-то эти контейнеры разложит в более поздних версиях сварма там ещё появились фишки типа Restart Condition Когда можно э отслеживать что ну чтобы нужное количество экземпляров твоего контейнера в кластере жило и в общем-то это всё что надо поэтому свар мы взяли просто потому что он уже был но в общем своё написать Вот для того маленького маленького сабсервер чтобы их нужное количество было иногда их поднимать и опускать в общем больше ничего не надо вот ещё у меня есть замечательный слайд про слабую ими отвагу поскольку мы же не настоящие админы и нам все эти предрассудки админские до лампочки Мы в одном и том же продакшн кластере гоняли и продакшн и стейджинг и тесты и весь C тоже там жил вместе со всеми новыми нодами которые там поднимались и всё это живёт в одном кластере потому что за одним кластером следить проще чем за двумя а разницы принципиально нету ну просто сервис по-другому обзови и всё нормально работает для этого нужно немножко запас по железу иметь на всякий случай но по большому счёту Вот это наверное всё что я хотел рассказать я довольно плотно уложился время там меня ещё просили в конце доклада сказать Вот это вот ребята вот если всё так легко Как ты говоришь и в общем-то действительно довольно легко Это простые компоненты простые инструменты лишнего софта практически нет И зачем вот это вот всё Зачем ребята сотни человека лет вкалывают там туда-сюда всё это делают на самом деле ответ Довольно простой большинство проблем которые решают эти ребята через сложный код сложные конфиги сложные концепции там вот эти все поды и так далее и так далее и так далее они в нашем случае решаются на уровне соглашений соглашений внутри команды и я искренне верю что для подавляющего большинства Ну не знаю насчёт здесь присутствующих Я не знаком с аудиторией Но для подавляющего большинства вообще всех команд этого достаточно вам не нужен сложный софт не нужно погружаться в кубернетес потому что поднимите руки кто с кубернетес работал А кто в нём быстро разобрался вы админы наверное да Он он этот софт довольно сложный реально и когда начинаются вопросы на что нам потратить время там на на то чтобы разобраться с кубе или на то чтобы пописать какие-нибудь продуктовые фичи тут же начинаются вот такие вот ковыряния там носочками и говорить Ну может быть лучше не надо может мы по старинке там пушим попло на машины и так далее так да доке зна в аудитория под что на локалке доке запускает Стопудово все знают как с этим работать Вы просто туда добавьте ещё вот вот вот вот вот столичка и дальше пихай в наш кластер вот я искренне считаю что для большинства случаев достаточно довольно простых технических решений и довольно простых соглашений внутри команды и если эта комбинация даёт простой сервис для разработчиков которым наверняка не охота разбираться вот вот вот вот с этими концепциями и и не хочется иметь а"
}