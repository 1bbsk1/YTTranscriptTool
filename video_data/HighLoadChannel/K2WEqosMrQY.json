{
  "video_id": "K2WEqosMrQY",
  "channel": "HighLoadChannel",
  "title": "“Восстание машин” – это ок / Леонид Талалаев (Одноклассники)",
  "views": 917,
  "duration": 2860,
  "published": "2020-04-14T11:12:01-07:00",
  "text": "всем всем привет меня зовут леонид талалаев я работаю в команде платформа проект одноклассники и последние 3 года занимаюсь разработкой нашего внутреннего облака поднимите руку те кто ведь кто слышал в про наше облако и видела хотя бы один из докладов олега настася вам есть некоторые люди это очень хорошо а те кто не видел рекомендую очень посмотреть ссылки на доклады будут в конце этого доклада немного цифр мы запустили наши облака больше трех лет назад на данный момент большинство наших сервисов работают в облаке то есть это уже проверенная технология проверенная в том числе серьезными инцидентами в такими как пожар в одном из наших датацентров а недавно мы рассказывали про это в статье на хабре по мере роста облака по мере роста облака росла и сложность управления те задачи которые раньше решались вручную начинали отнимает слишком много времени силу но мы очень любим все автоматизировать и на наших масштабах автоматизация может дать существенный выигрыш как временных так и денежных ресурсов и сегодня мы рассмотрим некоторые таких задач мы узнаем как мы автоматизировали выкладку security патч на все контейнеры и заодно научимся собирать докер-образ за одну секунду не используя докер посмотрим как мы обеспечиваем доступны сервисов в облаке при операциях с контейнерами и узнаем что такое фрагментации ресурсов и расскажем про наш алгоритм который сэкономил на миллион долларов итак посмотрим как устроен облака ван клауд в средней части картинки это миньоны это сервера дата-центра на которых работают контейнеры на каждом меню не работает демон это сервис который управляет его контейнерами верхней части мастер процесс который отвечает за управление всем облаком он разумеется у нас отказоустойчивый у нас под у нас их по три в каждом облаке файловая система докер образов состоит из слоев те слои которые мы запускаем эти образы которые мы запускаем в облаке состоят из собственно файлов запускаемого сервиса и системных слоев операционной системы пакетов из системных библиотек у нас более сотни сервисов и около сотни разработчиков и чтобы стандартизировать упорядочить разработку мы выделяем базовые образом базовые образы создаются системными администраторами мне делается настройка операционной системы устанавливаться пакеты и так далее образы сервисов создаются разработчикам а не наследуется от базовых образов и установка пакетов у них запрещено поскольку используем остаток технологии во всех сервисах у нас очень похож мы выносим общие настройки включая точки входа и скрипты запуска в базовый образа поэтому большинство докер то сервисов у нас еще очень простые как примеры внизу слева единственно операция в них это копирование файлов от полученных при сборке этого сервиса а представим что найдено критической уязвимости в системные библиотеки например для баса сел и системный администратор назовем его увася хочет выложить security патч на весь продакшн вася вносят патч в базовый образ и далее нужно обновить все зависимые сервисы если вася ленивый то он может ничего не делать и просто уйти домой и подумать что сервиса рано или поздно сами обновятся при плановых апдейтах но мы ленивых не берем и вас работает в одноклассниках и знаешь что у нас очень много сервисов и активно разрабатывается далеко не все и например сервиса которые не содержат бизнес-логики например хранилища их написали один раз они работают и изредка что-то в них фиксится и ждать планово апдейта для них очень долго и вася придется их обновить вручную в одноклассниках более 150 типов образов и если мы предположим что сборка образа занимает 2 минуты и получится что на сборку всех образов вася потратить 5 часов не говоря ещё о том что их нужно выложить это слишком много ручной работы мы бы хотели помочь вася и автоматизировать этот процесс но реализовать его через систему continues integrations был был слишком сложно сервисов у нас много могут быть запущенной разной версия образов и далее каких-то экспериментов например и если кто-то выложил версию из бранча то мы должны ее переложить на ту же самую версию мы не можем взять и обновить на другую версию то есть нужно взять откуда контекст сборки и это очень сложный путь и мы поэтому так и не стали делать но зачем нам пересобирать сервис если мы не меняем его код посмотрим что у нас есть docker реестра у нас там есть новый базовый образ есть старый базовый образ и получается что все нужные слои у нас уже есть если мы добавим к слоям нового базового образа слой слои сервисом то мы получим в точности такой образ как будто мы бы его собирали изначально с этим базовым образом но в docker engine через docker engine мы не сможем сделать такой операции поэтому будем работать напрямую с другими сестры посмотрим как устроены представление образов в реестры docker реестра внутри делятся на репозиторий название репозитория это имя образа без того в нашем случае у нас два репозитории the service & bass и в каждом репозитории хранятся два типа объектов это манифесты и блогам для работы с манифестом и блогами есть документированной api а все объекты манифесты ебло бы имеет дайджеста даиш сет вычисляется как sha-256 хэш от содержимого является его уникальным идентификатором этого объекта для упрощения нас используются в слайдах упрощенный дайджест реальности это более длинная строка посмотрим из чего состоят образы белые прямоугольники слева это манифеста они содержат jison и манифесты можно также присваивать теги на данном слайде у нас два манифеста это мини пресс с именем манифест образа с именем pset2 11 и манифесты с именем сервис и тегом 123 у каждого образа также есть конфигурация это такой блок который содержит джейсон из от описывает докер-образ и есть зеленые квадратики это собственно слои и докер образом теперь вернемся к задача сборки доки докер образы из готовых слоев вот у нас есть новый базовый образ есть старый образ сервиса и нам нужно создать новый образ который будет ссылаться на осла из этих двух образов все слои уже в реестре есть поэтому нам достаточно создать столько манифеста конфигурации это два джейсона которые обведены красным посмотрим как устроен манифест образа для этого мы делаем такой get запрос в реестр с указанием дайджеста и также через заголовок указываем что нам нам ожидая мы ожидаем эминов мания 2 версия спецификации иначе реестр вернет совсем другой документ что мы видим что мне тесный содержит никаких дополнительных данных кроме того что он собирает вместе другие объекты конфигурацию и слои остальные поля версия mediatek и так далее они не меняются их значение можно посмотреть в спецификации по ссылке внизу то есть если нам слои и конфигурации известно создать этот документ не составляет никаких проблем теперь посмотрим как устроена конфигурация ее можно тоже запросить из реестра по ее до чувства в конфигурации есть 4 секция это метаданные параметр истории слои те кто запускал докер спектре мир в виде ли там что-то похожее на этот документ конфигурации используется runtime при запуске контейнером формат этого документа более-менее описывается стандартам iso image документацию можно найти по этой ссылке чтобы не тратить время документы будут приводиться в упрощенном виде опуская все что неважно но посмотрим вот первая половина конфигурации это метаданные тут все более менее очевидно и параметры тут тоже все просто это те значения которые меняются одноименными команда докер файл если мы их тут поменяем той эти изменения применится к докер образом то есть на данном этапе вы уже знаете как можно поменять параметры в доке роботах образах не используя докер напрямую в реестре но нам нужно менять не только не только параметры но и слои поэтому пойдем дальше вторая часть документа тут у нас история она содержит все команды из которых создан докер-образ начиная самый ранний из лаем и тут могут возникнуть вопросы что там за странные идентификаторы после команд копи но на самом деле нам не важно знать детали как именно эти записи формируется потому что мы их будем брать из готовых джейсон документов то есть как студент на экзамене нам не нужно понимать нам важно только знать у кого списать и я записи записи история которые создают слои это тут их две 1 и 2 и есть записи которые не создают свои они помечены признаком mtr слоя описывается списком таким списочком дефо и десна внизу но это не те же самые хэши которые в манифесте эти пойди вычисляется на хвосте как he shot распакованного архива со слоем но как я говорил нам не важно знать детали я как они мина не вычисляется потому что мы их берем из готовых документов некоторые команды докер файл не попадает в истории например команда chrome и возникает вопрос если у нас есть только текст конфигурациям но нету докер файл исходного как мы поймем где в история заканчивается базовый образ и где начинается образ сервиса мы решили этот вопрос так добавили в докер файл первой строчкой после фронт установку лейбл from с именем и тегом базовым образом и таким образом мы не всегда знаем где нас кончается история базового образом все что идет выше этой записи кроме того мы тебя знаем пока куба и менее того мы можем получить актуальную версию базово образа что нам будет нужно для автоматизации и теперь у нас все готово чтобы создать конфигурацию сервиса делают очень просто мы берем осла и базово образа которая выделена красным и меняем их на запись из нового база в образом после этого нам остается подменить параметры для этого нужно понять какие из параметров попали из базового образа какие были добавлены в сервисе это можно понять по историям посмотрим на историю ищем команда по меняющие параметры в данном случае это лейбл и and и эти параметры не трогаем и остальные мы меняем на параметры из базового образа далее остается подправить время создания и конфигурацию нас готова дальше мы создаем манифест для этого заменяем слое база образа и в число уже известно и прописываем дайджест осознанный перед этим конфигурации размер ну вот мы создали два документа а теперь нужно загрузить их в реестр и прежде чем это сделать нужно смонтировать слои базового образа что что это значит как я говорил докер реестр на 3 делится на репозиториям и манифест может ссылаться только на те слои который находится в том же самом репозитория например ссылка которая подсвечена красным на картинке она не валидно и такой маникюр загрузить нельзя что нам нужно сделать нам нужно сделать операцию mount при этом операции физического копируй они не происходят в реестре и просто создается ссылка и после этого мы можем можем загрузить наш манифест и того чтобы загрузить докер в доке режессер напрямую нужно смонтировать слои загрузить конфигурацию загрузить манифест и эти процессы они хорошо документирован и поэтому я не буду на них подробно останавливаться что мы в итоге получили мы получили в нашем облаке возможность меня все базовый образ на любой другой эта операция у нас называется и мы шли без новых слоев мы не создаем при этом реестр не разбух не разбухает передача слоев посетит тоже нет то все запросы легковесные создание образа занимает всего одну секунду и это очень просто реализовать достаточно уметь работать в сон и выполнять аж тебе запросы и поэтому это очень удобно использовать для автоматизации что мы и сделали и мы эту операцию вызываем непосредственно исходом в ходе мастер облака перед обновлением сервисом а мы научились создавать новые новые образы но теперь мы должны научиться их выкладывать как мы это делаем мы находим устаревшие база в образы делаем новые через и моче без обновляем сервисы и мастер облака при этом модифицирует в чат что происходит обновление и разработчики у нас поначалу спрашивали а кто это ответит мой сервисом и отвечает это робот поэтому мы назвали этот процесс восстание машин восстание машин дисциплинирует писатель надежный код так как контейнер может быть в любой внутри стартовым раньше разработчик может прийти к дежурному администратору и сказать пожалуйста не трогайте мы сервиса потому что это какая-то причина а сейчас когда роботы обновляет сервиса так уже не прокатит робот железное просьба не понимает этим механизмом предоставляем не только почти безопасности но и любые изменения базовых образов например для расследования студентов у нас в каждом контейнере есть инструмента отладки диагностики если бы заходим в контейнер и мы хотим быть уверенны что там более-менее свежие этих инструментов процесс обновления прозрачен и есть мониторинг администраторы могут следить за тем каких таких сервисах уже пошло обновление каких еще нет мы запустили этот процесс год назад с тех пор он замечательно работает и все уже привыкли к тому что в облаке что-то само собой происходит и на самом деле это очень удобно и экономит кучу времени но тут можно подумать восстание машин создает и выкладывает не прошедшие тестирование образы на все контейнеры и все это без контроля со стороны человека звучит как план положить production разумеется мы предусмотрели защитные меры во первых мы размазываем это по времени то есть перекладываем небольшого числа контейнеров дожидаемся старта и только потом продолжаем результате все контейнер облака у нас обновляется примерно за месяц во вторых обновляем только в рабочее время если что-то пошло не так команда мониторинга успеет это заметить в третьих обновляем один дата-центр в день это снижает возможно эффектов каких-то отложенных проблем и последняя моя останавливаем контейнеры только в том случае если это не приведет к недоступности сервисов и как именно мастер облака определиться по можно ли контейнер сейчас останавливать или нельзя мы рассмотрим далее одноклассники это после к доступный портал то есть при любых действия сервисов не должна нет ни при любых действиях работы сервисов не должна прерываться как именно восстание машин обновит здесь тысяч контейнеров не нарушив доступный сервисов в каждом дата центре у нас работает отдельные облака со своим мастером мастера облаков не общаются между собой и ничего не знает про инсов других облаках все сервисов одноклассниках проектируются таким образом чтобы выдерживать отказ любого числа инстансов в одном дата-центре обычно сервис размещается как минимум в 3 дата центрах если часть или все индусы в одном дата-центре не работает the instance of других дата-центрах продолжит обслуживать запросы для этого сервис должен реализовывать некоторую схему резервировали если сервис схемы резервируя бывает разный если сервис без состояния то его из нас и взаимозаменяемы у нас это фронты сервис бизнес-логики для такой схема проверка реализуется очень просто определяется значение метро нинка минимальное число работающих индусов в одном дата-центре и мастер не будет останавливать из нас если это если это приведет к нарушению мин running случае с фул сервиса всё немного сложнее 100 раджа у нас распределенная хранят копии данных в разных дата-центрах и в зависимости от требуемого уровня консистенции определенного минимальное число инстансов для выполнения запросов самые распространенные схемы это 2 из 3 2 из 3 то есть данные хранятся в 3 копьях в 3 дата центрах и для чтения достаточно любых двух если какой-то инст вас не работает то для тех данных которые на нем хранятся остается только две доступные копия в других дата-центрах если с этими копиям что-то случится то клиенты не смогут писать и считать данные поэтому мастер он не должен останавливать сны которых лежат эти копия зависимость от топология кластером в каждом дата-центре это может быть как один instance так и несколько и возможно даже все инстанции этого дата центра и если бы мастер без учета топология мастер не смог бы остановить ни один installs в другом дата центре у нас же есть кластер а где очень много очень много инстансов более сотни это них такое ограничение сильно затруднило бы работу как самим сервисом так и с любым миньонам на котором лежат en sus и этого сервиса основная сложность топологии что у нас очень много разных видов стражей очень много разных вариантов топология нам не хотелось бы мастер учить понимать все виды всех все виды топологии всех сторожей поэтому мы научили бизнеса самим сообщать мастеру а можно ли можно них сейчас останавливать или нет для этого нам немножко пришлось доработать контент сервисом работает это следующим образом каждый единство со вычисляется периодически вычисляет свой статус доступности и отсылает его мастером если статус и поменялся то он отсылается мастеру немедленно не дожидаясь следующего интервала отправки чтобы мастера всегда была актуальной информации для принятия решений всего есть три статуса доступности это резерв то есть инструкции с резервированием то есть сейчас есть достаточное число рабочих replicas копия данных этого из гипса и при его остановке сервис продолжат обслуживать запросы статус присел означает что инцесс работает на грани доступности то есть имеет минимальное допустимое число рабочих реплик и его остановка приведет к частичной или полной не доступности сервисов и failing означает что incense по какой-то причине не может обслуживать запросы доступность уже нарушена такие индусы тоже можно останавливать потому что их остановка не приведет к ухудшению доступность а перед операциями с контейнерами остановка и стартом или обновлением мастер проверяет статус доступности если статус при файл то операция не будет применена сразу вместо этого мастер и и запомнят и применит тогда когда статус контейнера изменится это нам позволяет реализовывать сложные автоматическое сценария не боясь положить production например восстание машин и другой пример это остановка миньона для работы например для замены диска администратору достаточно дать команду мастеру на остановку миньона и мастер корректно становится все контейнеры дожидаясь момента когда это станет возможным последняя тема фрагментации ресурсов представим представим ситуацию что у нас дата центре есть миньона вот показаны прямо гонщиками и на них есть задача и какой-то миньон у нас умер умер совсем и нужно восстановить работу контейнеров которые были на этом меню не перенеся их на другие миньоны но какие есть гарантия что окажется местом и других миллионов для такой операции облако у нас была одна задача на сервер и на случаи отказов серверов мы держали резервный сервер а если сервер отказывал мы заменяли в резервном и переносили на него задачу в облаке же держать резервные сервера нерационально потому что их можно добавить облака и перед перераспределиться задача волокну перераспределить задача и получается что у нас нет в резервных серверов для облака и если место закончилось то взять резервный сервер а просто неоткуда хорошо но мы же умные мы же будем будем мы можем следить за использование облака и добавлять сервера заранее пока ресурсы не закончились мы тоже так думали оказалось не все так просто в 2018 году у нас был план до конца года перенести 50 процентов сервисов с железных серверов в облако и вот в конце года мы активно переносим сервиса добавляем новые сервиса в облако и в один момент мы наблюдаем такую ситуацию что в одном из это центров у нас свободно более 15 процентов каждого типа ресурсов что не так уж и мало это более 1150 арбатов рамы и так далее но приходит разработчики говорят что вот мы не можем запустить наш сервис у волка жалуется что нет места через какое-то время ситуация повторяется других дата-центрах как же так причина фрагментации ресурсов а фрагментации эта ситуация когда свободные ресурсы есть дата центры в нужном объеме сумме но они не могут быть использованы задача потому что ресурсы распределены по разным миньонам например у нас тут есть 3 миньона накажет без них свободно по 20 ук то всего шесть цифр свободно но задача требующая трицепс не может разместиться другой пример про нотации это взаимная фрагментация ресурсов например когда на меня не есть цыпа но нет память и на другом и миньо не наоборот например если у вас есть два класса задач какие-то расчетные задачи которые потребляют много циpкa но потребляет мало памяти и каши которые потребляют мало цепь луна употребляет мао много памяти если вы будете размещать на меня задачи только одного класса то вы получите взаимные фрагментацию частный случай взаимной фрагментация про метаться редких редких ресурсов например если есть небольшое число миньонов у которых есть рыбу и позовете их задачами котором gpu не нужен то если придет задачи который этот пул требуется то она не сможет разместиться другой пример редкого ресурса это нас есть у миньона на которых есть 5 ssd-дисков таких миньонов немного и они заведены под некоторые задачи которым нужно 5 ssd и нам хотелось бы по возможности не занимать и другими задачами эту проблему можно решать правилами губерн это съесть такое понятие анти affinity которые запрещается задач смещаться на определенные миньона но это все ручная работа которую мы бы хотела избежать чем плоха фрагментация ну во-первых она снижает эффективность использования ресурсов во вторых она затруднят планировали ресурсов у нас свои дата центры и мы если нам нужно добавить сервера процесс закупка это займет длительное время и может быть это плохо если мы в это время не сможем добавить с носом на примерку что провести одну популярную онлайн-трансляцию нам нужно было в короткие сроки добавить в 200 видео раздатчиков облака если бы мы не смогли это сделать и за фрагментация трансляция могла бы быть сорвано а кстати сегодня будет доклад александра table про сетевые протоколы а вот ссылка на ее его же доклад про план платформу нашего стриминга что же делать как бороться с фрагментацией возможны два пути бороться с последствиями с помощью дефрагментации или бороться с причинами с помощью изменения алгоритм размещения задачи мы реализовали обойти этих подходом ну начнем с алгоритм размещением на вход алгоритма алгоритм размещение это такой алгоритм который работает на мастере в тот момент когда ему нужно принять решение о размещении задачи он принимает на вход задачу и и запрашиваемый ресурс это синий квадратик и миньоны на которые задача может разместиться то есть они подходят и дальше алгоритмы выбирает на некоторым критерием выбирает оптимальный миньон в реальности критериев выбора миньонов может быть очень много разных эта тема для отдельного доклада но для упрощения сейчас будем рассматривать только те критерии которые интересны с точки зрения фрагментации ну самый популярный алгоритмы to spread и или worldwide при размещении мы выбираем тот меню на котором больше всего свободных ресурсов вот у нас есть 3 миньона и вот спред выглядит вот как то так десятая задача не влезло этот алгоритм используется по умолчанию во многих системах в том числе губернатором там называется list to request a priority и dockers форм он равномерно размазать задача по миньёнам что кажется совершенно разумной стратегии на первый взгляд и мы тоже поначалу использовать такой подход с некоторыми доработками но до тех пор пока вы не столкнетесь с проблемой фрагментации насколько миньоны заполняется равномерно то вы в конечном итоге получите фрагментацию и не стоит использовать спред чистом виде какие с интернате вы есть best fit это обратная стратегия загружаем наиболее загруженный миньон выглядит как тот как то так вот видим что все задачи на этот раз влезли эта стратегия также есть dockers формы купер не тасс она плотно боковой задачей и хорошо борется с фарами тация но только с обычной фрагментации одного ресурса да и не учитывает их взаимных фрагментации еще одна проблема с бы спид то что плотной упаковкой это не всегда хорошо если у вас много свободных ресурсов и мало задач то в этом случае размещение бисквит будет иметь ряд недостатков во-первых полной изоляции задачи невозможно и взаимное влияние все равно какой то будет и чем больше задач у вас на биньоне тем больше взаимное влияние во вторых реальное потребление задачи она может быть выше тех ресурсов которые за задача запрашивать и если вас произошел какой-то рост нагрузки то может на меня лень и оказаться свободных ресурсов чтобы обеспечить этот рост третьих это плохо с точки зрения надежности так как при потере одного миллиона теряется все контейнеры на нём и в рассматриваемой ситуации может быть был бы лучше использовать тот же спред и и поэтому этот видимо поэтому тот алгоритм и используется во многих облачных решениях и стоит хорошо подумать прежде чем менять его на что-то другое нужно ли она на самом деле посмотрим как будет работать быстрее случае если у вас 2 ресурса допустим памяти циpкa если мы начнем подряд размещать задачи у которых перекос в сторону использование одного ресурса то получим взаимной фрагментации вот видим что седьмая задача не влезло хотя ресурсы вроде бы есть для борьбы со взаимной пропитаться и есть балансирная загрузка идея балансирные загрузки в том что задача размещается на тот меню где соотношение свободных ресурсов похоже на соотношение ресурсов которые задачи запрашивать например если на каком-то миньон есть перекос сторону сам-то не пришла задача с перекосом запрашиваемых ресурсов ту же сторону то она разместится на это меню не выравнивая загрузку ресурсов посмотрим как работает этот алгоритм вот как то так видим что седьмая задача влезла на этот раз и даже осталось места есть разная лизации этого алгоритма с различными формами в кабинет это называется баланс ресурса локэйшн и проблема балансирные загрузки в том что оно борется только vsauce взаимной фрагментации но никак не учитывает обычную фрагментацию и поэтому чистом виде и опять же не стоит использовать можно объединить несколько алгоритмов с помощью взвешенных сумм например под можно сделать интернет каждый из алгоритмов возвращает некоторое число от нуля до единицы оценка насколько хорошо миньон подходит для размещения затем берется сумма и получается финальная оценка но тут вопрос как настроить такую комбинацию оптимальное значение зависит от текущей загрузки облака и от ожидаемых задач которые могут прийти поэтому придется периодически подкручивать эти значения если у кого-то есть опыт настройки этого губернатор поделитесь было бы интересно прежде чем менять алгоритмы стали пробовать различные варианты стандартных алгоритмов различные настройки и сравнивали их с помощью имитационного моделирования для этого мы для этого мы взяли параметры реальных задач и и параметры реальных и миньонов которые у нас есть в облаке и сгенерировали такой бесконечный поток задач выбирая каждый раз случайную задачу и копирую ее и размещая размещаемые флаг используя выбранный алгоритм начинаем мы с пустых миньонов и добавляем задачи пока они помещаются тот момент когда я задача пир стоит влезать мы считаем фрагментацию как разницу между 100 процентами и в утилизации самого занятого ресурса например вот у нас на картинке самые занятые ресурсы по его утилизация максимальное составила 80 процентов и значит про имитация мы считаем что этот алгоритм дает на наших данных ферментацию 20 процентов подробнее про то как можно оценивать алгоритма упаковки задач в дата центре можно почитать статьи от google мы перепробовали кучу вариантов и лучший результат который мы получили на наших данных это примерно 15 процентов он отличается немножко зависимости от центра потому что у нас немножко разные задачи я немножко разные миньоны но плюс минус 15 процентов то есть при достижении облака 85 процентов при имитационном моделировании начинались наблюдаться отказы в размещении новых задач и то же самое то есть тоже то же самое что мы наблюдали на практике в чем причина почему нам не удалось добиться лучшего результата большинство алгоритмов стандартных они работают очень простой моделью ресурсов обычно это два числа памяти циpкa ван клауд очень более сложная модель ресурсов кроме стандартных циpкa память сети у нас есть еще локальные диски которые используются более чем 45 почти половина задачах используют при этом задача может резервировать не только места на диске она может резервировать еще пропускную способность то есть братья эксклюзивный доступ один или несколько дисков для того чтобы обеспечить нужный отклик ввода-вывода и это одно из ключевых преимуществ маклауд а перед другими решениями мы поняли что ни один из алгоритмов нам нас не дает хороших результатов на наших данных и решили написать мы придумать что то другое какие у нас были требования во-первых он должен учитывать все виды фрагментации во вторых должен поддерживать расширяться на сложные модели ресурсов произвольное и должен автоматически подстраиваться как под размеры задача так и под доступное место в облаке чтобы не приходилось постоянно что-то подкручивать в параметрах а после таких экспериментов и поисков мы придумали такой алгоритм мы хотели чтобы он учитывал не только те задачи которые ту задачу которую мы в данный момент вот размещаем но и как-то учитывал те задачи которые могут прийти в будущем и мы сделали предположение он скорее всего что что придет будущем она будет похоже на то что у нас уже есть им мы строим выборку из существующей задач в облаке группируя их по похожести запрашиваемых ресурсов составляем такой вот список это список в называем это лобные задачи нам данный майк в данном примере у нас три эталонная задача у них есть такие запрашиваемые ресурсы тут показан только цикл память на самом деле учитывается все ресурсы включая сеть диски gpu там и так далее и эти талоны задачи это будет параметр нашего алгоритма для каждой талоны задача определим число размещений fit команд это число сколько раз задача на месяца в облаке если мы будем ее размещать на свободном месте на свободном месте облака будем размещать только ее например здесь у нас три задача первое помещается 4 раза и яфет к он 4 2 2 и 3 1 очевидно что по мере заполнения облака вид каунт эталонных задач будут уменьшаться если ты там какой-то задачи станет 0 то задача такого типа не смог больше размещаться в облаке и это ну собственно фрагментации то есть ситуации когда задача не может разместиться в облаке и ключевая идея алгоритма в том чтобы пытаться не допустить уменьшение значений и аккаунтов наших эталонах задач до нуля собственно как выглядит этот алгоритм размещения на входе у него есть задача которая хочет разместиться это желтая задача есть ее ресурсы и сначала мы для каждого варианта размещения посчитаем какое значение fit каунт у них было вот у нас есть варианты размещения это наше миньона и для каждого из вариантов размещения мы мы считаем какое значение fit каунта будет у каждой эталонные задача после того как мы применим этот алгоритм вариант размещения очевидно что он либо уменьшится либо останется таким же и дальше нам нужно выбрать из этих вариантов как мы это делаем мы берем находим в каждом варианте размещения минимальное значение рка у на данном случае это нижняя строчка она посвященных раз там и находим максимальное значение это единица и отбрасываем варианты с меньшим значением далее берем вторые минимальные значения снова находим максим это это два и отбрасываем варианты с меньшим чем максимальное значение повторяем процедуру до тех пор пока не останется один вариант если в конце осталось несколько вариантов то мы применяем спред для того чтобы выбрать наименее загруженный миньон из них мы посмотрим на примере как работает этот алгоритм представим что у вас есть большая маленькая задача сине зеленые прямоугольники и маленькая задача наблюдает 12 1 по 6 раз на миньон и этот к вам 12 напитка он большой задача 6 она в шесть она влезает вылезает по три раза на каждый миньон и у нас есть такой такие задачи на размещение разместим первую задачу тут нам не важно куда ее размещать а для второй задача у нас есть 2 варианта это первый миньон или 2 если бы выберем первое меню то большая задача сможет разместиться всего пять раз то есть выход каунт 5 если выбрать 2 миньон то значение аккаунт у второй задача изменится на 4 и получается что в этом случае первый вариант выгоднее размещаем туда вторую задачу дальше для 3 есть опять же два варианта но здесь значение хит кантов будет одинаковые и мы используем спред выбираем наименее загруженный миньон то есть 2 далее размещаем четвертую задачу по аналогии со второй дальше размещаем пятую для 6 опять используем спред 7 8 нас размещается по аналогии с первой второй и размещаем 9 что мы здесь увидели что наш алгоритм комбинирует поведение быстро fit и спред причем стрип применяется только тогда когда это правда в рамках слата большую задача для сохранения целых больших кусков ресурсов для этой большой задача а когда слот ну заполняется применяется спред и а ну хорошо посмотрим что будет если у нас два типа ресурсов допустим есть две задача одна использует один на цифру 2 2 гигабайта памяти 2 допустим наоборот ресурсы 10 такие столбики зеленые синие и есть два миньонам где один из ресурсов больше утилизированы вот показаны занятые ресурсы на меню и посмотрим вот есть задача на размещение куда она разместится опять у нас есть 2 варианта посчитаем какие у нас были значения от каунта первая задача могла разместиться в исходном по сходной ситуации она могла разместиться один раз на первый миньон и три раза на втором и их it can четыре автора задача могла разместиться два раза на 1 миньо не и три раза на втором я редко он был 5 дальше посмотрим как поменяется эти значения если бы мы выбрали первый миньон то у первая задача you бы не смогли больше размещать на первый миньон и яфет к он стал бы три вторую задачу мы вы тоже не смогли больше размещать на первом видео не ее аккаунт тоже был в центре во втором случае первая задача опять пятка unde3 вторую задачу мы бы смогли разместить два раза на 1 миллионе и два раза на втором иафет com4 и получается что второй вариант и выгоднее посмотрим как алгоритм борется с фрагментацией редких ресурсов допустим 2 миньона на одно меню не есть два свобода gpu если то есть эталонные задачи как одна из которых использует gpu и нам пришла задача на размещение которое не использует посмотрим куда наш алгоритм не разместив опять есть два варианта считаем значение аккаунт в каждом из вариантов в случае выбора 2 1 миньона 1 за первые талоны задачу меньше ты на единицу 2 не изменится потому что мы по-прежнему можем разместить его дважды существенно выберем второй меня там вторую задача значения get caught уменьшится на единицу потому что мы и можем разместить только один раз теперь и значит что первый вариант у нас был выгоден похоже что новой алгоритму умеет бороться со всеми видно фрагментации и действительно мои по результатам имитационного моделирования наших данных мы получили что фрагментация для этого алгоритма составила всего 5 процентов и это подтверждается на практике после внедрения алгоритма мы достигали заполнение облака 95 процентов и только после этого наблюдали проблемы с тем что задача какие-то не могут разместиться в результате мы получили улучшение на десять процентов но много это или мало не понятно давайте посчитаем деньгах предположим что у нас сервис стоит 3000 долларов всего три тысячи серверов перемножим отечеством примерно получим 1 миллион то можно добавить ещё стоимость обслуживания ну много это или мало решать вам для нас показал что вполне себе хороший результат вот мы реализовали новый крутой алгоритм внедрили его запустили но в облаке уже есть задача которые запущены и они запущены ну распределены как-то не оптимально что же теперь делать один из вариантов реализовывать дефрагментации облака то есть как-то переносить задача для улучшения ситуации и наверняка многие из вас помнят такой диск где фрагмент рпа как который покажется что то делал нельзя было пользоваться компьютером но вопрос как реализовать что-то подобное в облаке конечно так чтобы это не мешало работе сервисов возникает тут несколько вопросов во первых у нас много видов фрагментации какое мы хотим оптимизировать и по какому ресурсу во вторых какие задачи мы хотим нужно двигать для этого и в каком порядке то есть какие приоритеты у нас и когда мы должны останавливаться этот процесс то есть когда когда мы можем понять что сейчас уже все хорошо и и ничего больше двигать не нужно чтобы ответить на эти вопросы представим такую ситуацию что у нас отработала как-то дефрагментация вот она переместила задача после этого у нас эти задачи допустим перри запустились и алгоритм размещения или две новых обратно понятно что такая ситуация она нежелательна что это значит что алгоритмы дефрагментации размещения должны быть согласованы то есть должна быть какая-то общая метрика которые они должны оптимизировать алгоритм у нас оптимизирует метрику финал поэтому будем использовать ее и посмотрим как мы реализовали в итоге дефрагментацию наших датацентров с помощью этой метрики работает она деформация следующим образом берем список эталонных задача вычисляет каунт в примере 3 эталона я задача оснащение 5 10 и обходим задача в порядке возрастания этих значениями а берем первую задачу в который воткнут 0 для этой задачи мы обходим смотрят все миньоны и пытаемся перемещать контейнера на другие миньоны чтобы увеличить значение with cold вот например мы нашли что можно встретила меня на передвинуть задача и освободить место под эту большую задачу цветка он стал единицам ну дальше мы берем следующую эталонной задачу м2 повторяем поиск для неё вот допустим мы нашли что можно редко он был 2 м и перенести мире принести ещё одну задачу и dcom стал 3 повторяем этот процесс для всех эталонных задач критерии остановки тут тоже довольно просто определить и мы оптимизируем только для тех задачу которых деткам от меньше какого-то лимита и останавливаем дефрагментацию если мы не смогли сделать ни одного перемещения перебрав все задачи вот собственно весь алгоритм ничего сложного мы его реализовали и сразу увидели результаты положительные во-первых он снижает дефрагментацию во вторых если представим что у вас появилась новая задача она большая и раньше наш алгоритм размещения prime про нее не знал и не оптимизировал не как место в облаке то она может не влезть и в этой ситуации дефрагментация помогает нам перераспределить задача и освободить место под контейнеры этой задачи вручную это был бы сделать очень очень сложно кроме того есть другие ситуации когда у нас возникает не оптимальное распределение например мы добавили пачку миньонов дата-центр и мы хотим как-то первых спаситель на них задача в этом случае тоже помогает дефрагментации хорошо вот ну возникает опять такой же вопрос теперь имитация она двигает контейнер и двигают контейнеры всех сервисов делать это автоматически и как сделать так чтобы это не положила production ничего не поломала но вспомним что те же самые вопросы мы уже решали поэтому мы добавили дефрагментацию как часть процесса восстания машин то есть про восстание машин это власть оптимизированы процесс который на мастер облака который затрагивает все сервисы и может автоматически обновлять или переносить контейнера для того чтобы ничего не сломал мастер следит за доступностью сервисов и так чтобы полезным и сегодня узнаем мы узнали как создавать докер образа за одну секунду не используя докер как можно обеспечивается доступность that falls сервисов облаке и почему важно фрагментации как с ней бороться спасибо за внимание смотрите наши доклады подписывайтесь на наш блог а вопрос кто нибудь есть спасибо за доклад у меня будет три вопроса я их буду задавать по одному чтобы не нагружать сильно но первый вопрос по все вопрос по распределению на самом деле по фрагментации первый вопрос учитываете ли выборные задач да то есть те задачи которые идут всегда вместе и поодиночке не мог быть но представляются как одиночные задачи свои фрагментации учитываете ли вы их в таких задач нет у нас таких задач нет хорошо вторая проблема во фрагментации получается как я понимаю вашего доклада это в прогнозировании появления новых задач то есть если бы мы знали набор там не знаю места задач да то мы могли бы просто даже прогнать в цикле и посмотреть какой вариант наиболее правильный наиболее фрагментированный и так и распределить то есть чисто информатика есть получается проблема именно в прогнозировании на какие новые задачи поступят и как заранее подготовиться к этому и соответственно вопрос вы учитываете ли как-то прогнозирование то есть если какая-то методика но известно что по опыту всегда будут скорее всего от вероятность добавления задач и x больше чем вероятность задать добавлять задачи y там учитываете ли вы это так вопрос насколько я понял был в том учитываемые вероятности с которой задача могут прогнозирования прогнозирование как я сказал у нас есть эталонные задачи которые именно есть этот прогноз задач которые могут появиться в облаке сейчас мы его строим на основе тех задач которые в облаке уже есть но в теории можно поменять этот алгоритм построении этих задач на какой-то другой если вы там у вас есть какая то дополнительная информация про те задачи которые могут появиться насчет вероятности на самом деле мы используем ну немножко-то работ на в горит здесь я рассказывал про упрощенную версию да у нас там есть некоторые pisa которые оценивают вероятности появления задача и поэтому мы оптимизируем нет значение букет каунт умножаем на них только эпицентр еще третий вопрос вот при дефрагментации да то есть при уже исправление следствия для того чтобы делать перераспределение но скажем до задачка и как приложить теперь поменять переменные местами например нам нужно всегда третья переменная ну да здесь у вас получается что всегда должен быть какой-то миньон запасом место чтобы произвести эту дефрагментацию чтоб временно переложить что-то так еще раз вопрос том что если мы что-то канала допрос если у вас миньонах оставлять или миньонов дополнительное место для того чтобы выполнять дефрагментацию чтобы перекладывать из одного в другое то есть вы не национально и жив я понял вопрос но специального места не оставляем для дефрагментации но эти ферментации использует то свободное место которое она уже как следствие всегда есть да конечно мы ну не допускаем ситуации когда у нас заполнено все настолько что мы даже передвинуть задачу не можем спасибо следующие вопросы вы можете задать дискуссионной зоне к сожалению нас время закончилось ну спасибо вам большое аплодисменты"
}