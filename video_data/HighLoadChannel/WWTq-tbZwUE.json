{
  "video_id": "WWTq-tbZwUE",
  "channel": "HighLoadChannel",
  "title": "Паттерны отказоустойчивой архитектуры / Александр Кривощёков (Яндекс Еда)",
  "views": 49105,
  "duration": 2548,
  "published": "2023-01-19T06:56:46-08:00",
  "text": "драсти коллеги как верно сказали меня зовут саша кривощеков я руководитель разработки каталога в яндекс еде так кликайте пожалуйста яндекс и да это сервис по доставке товаров и блюд из ваших любимых ресторанов и магазинов мы ставим себе целью подобрать для вас самые лучшие рестораны чтобы вам понравилось оттуда заказать что вам привезли вкусную еду чтобы ваше желание на этот вечер были удовлетворены как минимум желудку и чтобы у вас было всегда хороший доступ к ресторанам около вас с другой стороны есть и техническая проблема у яндекс карт и яндекс каталога каждый раз в нас приходят тысячи и тысячи пользователей это десятки тысяч рпс который мы должны обработать и всегда показать главная задача каталога не только подобрать для вас самые лучшие рестораны но также есть и другая не менее важная вещь это всегда быть доступным небольшая идея всего каталога что весь мир должен умереть но каталог яндекс среды должен остаться как раз этим занимается моей командой я поход доклада у нас будет огромное количество всяких ссылок я заранее подготовил для вас qr-код такой же будет в конце не переживайте это презентация интерактивная там есть несколько вещей с которыми можно будет прям поиграть поэтому если вам захочется побольше посмотреть как работает red limit и прочих по ходу доклада можете отсканировать и потом остановиться на нем а мы с вами приступим сегодня мы будем говорить не про яндекс ледовым чтобы не охватывать все что мы делали внутри яндекс еды а уместить это в более компактный вещь мы поговорим про выдуманный сервис но который основан на тех же болях и на тех же проблемах которые мы встречали в реальной жизни разрабатывая индекс виду сегодня мы поговорим про яндекс воду абсолютно выдуманный сервис если он когда появятся скорее всего это какая-то случайность но все проблемы и все паттерна которые мы сегодня будем рассматривать поход доклада основан на реальной жизни то с чем мы сталкивались при разработке индекс виды давайте познакомимся с первой версии мы начнем с классической супер маленькой системой то есть у нас есть некий монолит который отпочковался по легенде индекс вода это стартап внутри яндекс еды то есть была какая-то команда которая постепенно разрабатывала фичу и она разрастаясь и разрастаясь и разрастаясь превратилась прямо в самостоятельные решения в этом случае из яндекс еды выродилась индекс вода решение которое должно помогать пользователям находить через стороны находить ближайшие бары и помогать им заказывать из них чтобы у них был доступ не только к еде но и хорошему барон система сейчас стоит банальная из балансира пускай это будет яндекс или что угодно и из монолита неважно на чем он описан из база данных классический сетап с него мы сегодня с вами начнем перед тем как мы приступим к всем паттерном сегодня мы разберем 6 из них это ретро и дедлайны red limit и соку брейкеры пичкала int и даме на каждом из них мы останемся подробнее рассмотрим как он работает какие проблемы решает и в том числе посмотрим какого можно реализовать и так глава 1 retro и наш сервис начал развиваться пришел менеджер и говорит пацаны очень классные у вас монолит но я хочу систему рейтинга текущий рейтинг записывается где-то в баски он почти статичный то есть рейтинг блюдо или рейтинг ресторана обновляется там какими-то крон досками хочу чтобы прямо в runtime и чтобы рассчитывали честный рейтинг для этого ресторана и для этого блюда хорошо сказано сделано мы сделаем отдельный микро сервис не писать же это в старом монолите и поселим его прямо рядышком назовем его рейтинг по ходу будет несколько слайдов с кодом я решил не парить ванки вас каким-то адовым си плюс плюс кодом он скорее больше для понимания концепции то есть где-то осмысленно упрощенной конструкции не обязательно знать и плюс плюс чтоб смочь его прочитать важно обращать внимание именно на суть я ее подсвечу у нас есть скиппи клиент аж теперь клиент может сделать был без тела наши степи клиент может сделать ду с каким-то телом у нас есть ответ с которым можем почитать соответственно response и статус этого ответа также мы напишем клиент для нашего нового сервиса рейтинга банальная конструкция мы принимаем на вход я малость ослеп мы принимаем на вход складка ресторана мы формируем первичный запрос создаем запрос сервису рейтинга делаем клиенту вычитываем ответ получаем полюшко рейтинг по рс я могу как and profit как эта система будет работать в реальной жизни ну во первых когда мы запустим систему в эксплуатацию первое что мы возможно увидим что по какой-то причине рейтинг не настолько близко к микро ser kam анализу как мы ожидали он немножко дальше и между ними есть сеть ok наверное это ни на что особо не повлияет когда пользователь сделает запрос он попадет балансир балансиру прокси рвать дальше монолит монолит сходит в башку маска вернет ответ дальше монолит входит в рейтинг но по какой-то причине влогах мы увидим что рейтинг и снова микро сервиса мы получаем не всегда иногда там появляются тайм-аут и иногда появляются в принципе это странные ошибки что недоступно сети прочее а наш пользователь в эти моменты видят старый рейтинг или еще хуже полностью его отсутствия то есть выдача не отсортировано по релевантности а просто рандомно зажав лена что же здесь произошло произошло банальная мы же в мире где есть сеть и сеть не идеально у сети могут быть тайм-аут и и сетевые ошибки многие из них описанные в рдт заголовки если мы говорим про кита будут дистрибутивы и это может быть отсутствие сети тайм-аут и это может быть какая-то проблема с принимающей стороной что нам никто не дал окно лоджа или это могут быть ошибки по осудила на стороне l7 то есть это временные ошибки как недоступны сервиса недоступность битвы либо полное недоступны сервиса допустим если произошло 500к какое же здесь может быть решение решение добавить ретро и первое что мы можем сделать это начать ретро ить наши запросы которые не прошли с каким-то интервалом то есть не обязательно возвращать пользователи о плохой ответ не обязательно пользователь оставлять refreshed страницу или перезагружать приложения мы можем сделать какие-то повторы за него если часть из нашей системы не отработала и мы можем это восстановить это отличная идея чтобы это сделать в нашем случае мы добавим ретро и перед походом в рейтинг когда пользователь дойдет монолитом монолит входит в бас колбаска вернет ответы монолит захочет посчитать рейтинг если произойдет тайм-аут он не вернется уже пользователю а монолит попробует сделать еще запрос если произойдет еще какая-то ошибка еще запрос до тех пор пока хотя бы один вопрос не пройдет и не вернётся обратно конечно здесь правила из того то что не нужно делать запросы постоянно то есть если мы не успели сделать три запроса и каждый из них закончился ошибкой вероятно не стоит пробовать еще раз система уже не восстановится есть несколько разных стратегий как можно делать ограничения на ретро и классическое это ограничение на количество повторов то есть мы явно говорим что мы готовы попробовать три раза если три раза ничего не произошло окей мы вернем ошибку но вероятность такого все меньше и меньше и меньше либо можно сделать другую стратегию а именно ограничение по времени то есть мы готовы повторять запрос в течение следующих 100 миллисекунд сколько успеем сделать повторов столько сделаем это достаточно редкая стратегия и скорее чаще вам придется использовать как раз и ретро и на счетчиках как это может быть реализовано наш старый клиент буквально добавим сюда пару строк а именно будем теперь прокидывать количество повторов которые захотим делать и добавим простейшая конструкция цикл то есть теперь каждый раз как мы будем делать запрос мы будем дискриминировать количество попыток которые у нас остались и дальше если у нас произошла ошибка которую мы готовая обработать а в нашем случае мы готовы обрабатывать тайм-аут и мы не готовы обрабатывать прям уж все ошибки то есть если произошел quantic сапсан в коде либо произошла ошибка допустим 500к наверное нам не стоит делать здесь повтор по что она не повторится ещё раз но если у нас стались попытки и мы получили ошибку с тайм-аутом мы сделаем еще один повтор еще один и еще либо пока мы не получим ответ либо пока у нас не закончатся попытки отлично мы победили проблему с рейтингом теперь менеджер рад то что мы можем делать микро сервис и просят больше он говорит давайте вынесем заказы в отдельный микро сервис теперь мы можем делать не только обычные корзины ну и мультика разин и корзины которым можно поделиться с друзьями мы как хорошие инженеры конечно же думаем что стоит эту систему или развивать не внутри монолита а постепенно вносить в отдельное решение раз уж у нас есть legacy которая была от яндекс среды нужно постепенно нему ходить и выносимо в новый микро сервис заказов мы применяем здесь ровно те же паттерн и которые мы только что сделали то сделаем ретро и мы ведь знаем что серия сетевые ошибки значит неплохо сделать из 10 и 2 и делаем для него клиент примерно то же самое то что получаем какой-то заказ формируем запрос отправляем если все хорошо возвращаем ok как же теперь с тем будет работать ретро мир давайте посмотрим пользователь делает запрос в балансир балансир делает запрос крыть лимиту мы попадаем в тайм-аут и тайм-аут возвращает нам обратно ошибку мы не дошли но подождите по какой-то причине сервис заказа все-таки получил этот запрос и даже сохранил в базу не страшно наверное и даже лучше запроса вы читали при этом балансер ничего не знает что и запрос был обработан и делает еще один ретро и мы получаем еще один тайм аут и в этот же раз опять ордер и думают отлично еще один заказ 2 заказа для пользователя балансиры от до сих пор не знает что была успешной обработка и делает еще один рaз рай и наконец он проходит полностью доходит до сервиса ордеров лорда формирует нормальный ответ и возвращает его обратно пользователю что мы получили мы получили следующее что мы им благородно для пользователя вместо одного заказа сделали три то есть пользователь вместо того чтобы получить условно один бургер и один sinner получит 3 бургера и 3 сидора это в лучшем случае возможно мы еще спешим с него несколько раз деньги несколько раз пишем штуки балы и прочие конечно прикольный то что тебя тиристоре бургера но грустно что у тебя осталось три раза меньше денег и проблема здесь в следующем что ошибки могут быть сети не только на отправку ну и на получение в нашем случае как раз это и произошло то есть когда сервис когда пользователь отправил запрос к сервису этот запрос был успешно обычно сервисом и даже обработан но когда его сервис дал ответ балансир не смог вычитать произошел тайм-аут и балансер ничего не знает про это и пробует еще раз в этом случае rate лимит начинает нас подводить он начинает дублировать запросы и начинает делать ту работу которую мы не просили а именно делать какие-то изменения в системе решение здесь достаточно простая это ключи и дым патент на со включенной потребности это рандомная строка это может быть пускай ей 4 то есть это какая-то строчка которую мы будем подписывать каждый из наших запросов то есть каждый раз когда пользователь будет слать запрос он со своей стороны будет в каком this федоров добавлять рандомную строчку и дальше когда мы будем повторять этот запрос мы будем все время повторить его с тем же самым федором с тем же самым ключом и компактности и в этот раз опять произошла ошибка но в этот раз вместо того чтобы создать только заказ вместе заказом мы сохраним ключ который только что получили от пользователям эти твари опять произойдет пушта балансер ничего не знает что произошла успешной обработка и сделает еще один запрос но в этот раз мы просто проверим что этот ключ от импотент насти уже был записан на базу и не будем создавать повторных запись а следовательно сколько бы раз мы не сделали повторы если мы будем слать с одним и тем же заголовками причем 1 патент на steam мы сможем постоянно проверять что это запрос уже был успешно обработан и нам не нужно делать никаких действий больше конечно здесь хорошие идеи всегда формировать ключ как можно на более ранней стадии то есть если мы можем попросить наших мобильных клиентов или браузер формировать ключ на своей стороне это самое правильное решение в том числе если есть какой-то ретро стороны мобильного клиента он также должен самостоятельно каждый раз при литра и подписывать этим же сам причем до тех пор пока либо не изменится запрос либо предыдущий не отработает вечно в этом случае не смотря на то что мы и сделали несколько retrieve пользователь сделал только один заказ и тем самым мы сделали некую семантику it most lanes то есть даже если произойдет ошибка мы гарантируем что при ретро их все будет хорошо реализация в коде мы буквально добавим несколько строк а именно теперь есть тот самый ключ и на патент насти строчкой мы добавляем заголовок их с этим патетики и добавляется 1 вещь помимо количество оставшихся retrieve мы также проверим можно ли этот запрос ретро вич запрос можно ретро и в двух случаях во-первых если глагол этого запроса и to get hit up шанс либо насколько не вижу напомнить trace либо если у нас есть в запросе это самый x1 патент на стихи здесь важно отметить что если у вас есть запросы которые мутируют состояние системы надеты или накиды во-первых скорее всего вы делаете возможно что то не так с другой же стороны стоит обратить внимание что многие стандартные реализации retrieve и в том числе стандартные библиотеки часто полагаются на то что если у вас запросы имеет такой глагол его можно зари трать привести вам time алыча 2 же я что если у вас есть тот самый xdm патента сякие в одном из кадров то этот запрос считается адекватным с точки зрения что его можно повторить мы понимаем что мы делаем а значит можно разрешить еще раз его отправить и еще раз попробовать все-таки добить его до пользователям чуть более подробнее про кличет импотент нас не можно будет почитать здесь наш коллега из яндекс такси и описал достаточно подробную статью как ключ это по ценности работают какие с реализации и какие опять же проблем могут у вас встретить когда вы начнете все-таки использовать о самой ключи там потребности в реальной жизни а мы пойдем дальше глава 2 дедлайны дедлайн на самом деле достаточно такое модное слово для обозначения тайм-аута на клиентской стороне нн насколько мне помнится впервые в такой широкий обиход ввели во время gr5 то есть когда google решил этим словом обозначать клиентские тайм-аут а именно сколько мы готовы ждать со своей стороны на 6 м опять же развивается у нас появляется новый сервис на это роста сервис статистики то здесь мы будем хранить статистику про пользователя про его предпочтения какие заказы он и в каком условно агрегированном состоянии ему нравится делать то есть некая такая подспорье для будущего и моля сервис статистики утилитарная штука то есть он просто хранит какие-то цифирки мы напишем для него уже привычный клиент то есть ничего нового мы делаем запрос там где-то есть ретро и за комментариями и мы пытаемся получить статистику для пользователя мы запускаем эту систему в эксплуатацию и мы видим знакомую штуку это тайм-аут но в этот раз произойдет нечто другое пользователи сделает запрос балансиром баланса сделай запрос к монолиту и когда монолит пойдет за статистикой вместо того чтобы быстро понять что об сеть не доступна либо сервис недоступен наш запрос будет блуждать где-то в течение трех секунд и потом просто отвалится вернется обратно монолиту тем временем монолит видит что пользователь на самом деле уже не ждет то есть браузер даже закрылся мнение даже не пытается вычеты в наш ответ у него таймаут на своей стороне и по сути дела нет смысла даже делать ретро и пользователь отвалился по своей собственной инициативе нам никому давать ответ в итоге пользователь остался в принципе без ответа даже несмотря на то что монолит ответил вроде бы есть данные вроде бы есть рестораны в чем проблема всего лишь нет статистики не важная вещь вещь а проблема здесь в следующем что сеть не то что не идеально она может иногда нас даже подводить банально принимающая сторона может не только зарядить ваш пакет она может его дропнуть то есть оно просто не предупредит вас то что не собирается о обрабатывать это может быть обрыв соединения прямо проводом это может быть проблема то что ваш пакет физически долго идет по сети и вы действительно живете те самые три миллисекунд три секунды пока пакет дойдет это может быть зависание зависимости сеть нормальный кот тормозной то есть сервис прочитал ответ и делает тратит он при этом не отвечает ни вам ни кому он просто сидит ждет несколько секунд пока вы сами не оборвет с ним соединение туда же проблемы железом либо проблема с кодом либо ресурс который вы запрашиваете может быть очереди опять же под мьютекс он недоступен все эти проблемы имеют общее решение а именно не обязательно ждать когда произойдет что-то плохое мы можем заранее сказать на своей стороне а сколько мы готовы ждать каждый конкретный из этих случаев банальная вещь естественно это просто сделаете на своем типе клинике тайм-аут искать я готов ждать 100 миллисекунд и все никого не предупреждая это не очень хорошо то что есть системы которые заранее знают что могут не успеть обработать ваш запрос и вы их не предупредить о выемке нити запросто полови две секунды прошло две миллисекунды я не буду ждать наверное не самое удачное решение после система могла уже лоцировать ресурсы начать что-то процессить а вы даже не хотите получить отсюда есть стратегия с дедлайн про повешенном поэтому и сама стратегия с дедлайнами низко отличается то есть мы можем физически сказать что дорогой сервис мы готовы тебя ждать вот этой точке времени если ты не успеваешь не пытайся заканчивается единение либо даже не пытайся нас обработать мы не успеем то же самое касается и времени ожидания и литров то есть если у вас происходит ретро и и вы действительно готовы ждать там эту зависимость 300 миллисекунд вы каждый раз будете говорить конечному сервису что я готов тебя ждать до вот этой точки времени если ретро и будут походить все ближе ближе ближе время остался меньше сервис будет знать что может и в принципе не успеть вас обработать в идеале конечно дедлайн ставить ходит осмысленные цифры то есть не наобум 100 миллисекунд а посмотрите на статистику ответов вашей системы посмотреть на 99 перцентиль как правило если вы за тайм-ауте на своей стороне на 99 перцентиле и сделайте ретро и с большой долей вероятности что попасть в 99 перцентиль 99 перцентиле я уже гораздо ниже шанс как будет система выглядит теперь с дедлайнами мы добавим буквально один пузырек с тем самым дедлайном когда наш пользователь сделает запрос прокси дальше отправит его в монолит монолит запишет он ретро и и дальше отправит его на проверку тайм-аут а и в этот раз мы не будем ждать три секунды мы заранее скажем окей если здесь ты не успел за 100 миллисекунд возвращая обратно что и произошло в этот раз ретро я попробую сделать еще один запрос и в случае уже гораздо лучше и ответ прошел до самого сервиса сервис дал ответ и мы возвращаем пользователь не только рестораны ну и хорошую статистику по ним здесь есть важный момент мы не только позволили не оборваться единению мы в том числе немножко выиграли времени то есть здесь мы явно сказали пользователю что у нас есть какая-то гарантия сколько будет кричать наша система если мы не успеваем в это время мы тебе вернем ответ но не будем тебя заставлять ждать не нужны тебе время прост для того чтобы ты получил ничего решение здесь в ходе простейшим мы добавим буквально очень похожи на горный контекст контекст можно сказать что у него есть дедлайн до какого-то времени контекст можно сказать что его закончили в ручном режиме и мы можем запустить какую-то лямбду внутри контекста вызвав ex back решение для консоли проверка флага либо у нас есть что он уже заканчиваем либо что наступил дедлайн случае с нашим сервисом их за счет будет работать примерно следующим образом мы добавим буквально две вещи это использование текущего времени и узнаем какое сейчас time stamp а и добавим ее в запрос плюс 100 миллисекунд теперь каждый раз когда мы будем отправлять запрос нашему сервису мы будем говорить что мы тебе готовы ждать до этой точке времени и каждый раз будем прокидывайте в заголовке говорит что мы тя готовы ждать ровно до 12 00 не успел даже не пытайся скорее всего дальше мы тебя ждать не будем хендлер на своих же стороне может примерно делаю то же самое то есть если у нас есть тот самый контекст который может принять откуда-то себе дедлайн положить его в переменную и дальше за их закрутить что-то в этом контексте то произойти может примерно вот это что у нас появляется запрос баски если во время этого их zacuto произошла обрыв соединения произошел дедлайн либо console этого дедлайном и просто оборвем также и пищу которую ждали от базы и не будем дальше продолжать использовать и ожидаете это исполнение и эту ошибку которые мы получили мы можем дальше протянуть до нашего конечного пользователя сказать окей ты не готов на ждать и наверно даже не будешь читать наш ответ вот flushing тем самым мы позволит пользователю не только меньше ждать ну и повысили шанс что мы все-таки обработаем какие-то стежки пойдемте дальше голова 3 петли митинг наша тема развивается дальше менеджер не нарадуется как круто мы разрабатываем систему как круто мы исправляем любую ерунду которую сами же себе и натворили и у нас появляются новые потребностью теперь мы хотим рассчитывать дистанцию до ресторана не просто по прямой и соответственно ваще такой там банальное время по прямой хотим более умный что не дорожный граф эмаль поверх этого дорожного графа окей сказано сделано дорожный граф де стыдно классная штука и мэл числа дробилка потенциальное узкое место но мы же уже знаем как обходить проблемы то есть делаем ретро и сделаем день тайм-аут и если чего не произойдет плохо все решится руки дорожный граф имеет простейший опять же описку откуда куда возвращает время которое будет занято курьером или чем-то еще от точки а до точки б как наша система теперь работает пользователь делает запрос запросы это монолит монолит идет в дорожный граф дорогу жены граф идет в сервис простите prediction идет в дорожный граф и даже отвечает класс все паттерны и отработали пользователь получает новый ответ с правильным временем все круто приходит второй пользователь у нас уже идет там сто-двести рпс и опять же они идут дальше в дорожный граф новый трасс что-то пошло не так мы же помним что дорожный граф это числа дробилка и prediction тоже м или то же число дробилка и он банально не выдержал такой нагрузки то есть мы дали настолько большую и колоссальную нагрузку что теперь система не может ее обработать и что хуже теперь если мы будем повторять в те запросы если мы еще вышли ему запросов системе не станет лучшим мы просто будем добивать то есть придет 3 пользовать 304 стр ps но системе от этого легче не станет мы просто и добьем еще и по голове не поднимайся и пользователь в итоге не только те пользователи которые вроде бы да там могли получить правильную дистанцию и правильное расстояние но и новые пользователи из норы не будут получать и то есть мы убили сервис для всех то есть если она сломалась сломалась для каждого в чем проблемам ресурсы ограничены то есть банально мы не можем обработать весь трафик в мире особенно если у нас есть действительно узкие места действительно числа дробилки которые не готовы к тому что получать колоссальные нагрузки это может быть в том числе и не то что мы ожидаем то есть это спайка вы рост д-дос и естественный рост 14 февраля 8 марта вы пришли новые пользователи в яндекс воду хотят воды при этом это может быть и неконтролируемый рост нами же то есть мы написали плохой код делаем ретро и класс добиваем свою же систему чтобы она точно никогда не поднялась какое здесь есть решение решение это этими ты red limit и могут быть в двух разных реализациях это может быть обычный рейд лимит который будет строго ограничивать 100 рпс все что вышла за 100р ps мы обрежем это может быть ограничение либо парой тут то есть какое в среднем количество запросов получаем за последнюю секунду либо это может быть ограничения по квоте то есть квоты можно сказать мы готовы обработать 200 запросов в следующую минуту это отлично кстати подходит для всяких процессинга free shipping bound операции которые точно могут выстроиться в очередь но мы не можем переполнить эту самую очередь при этом хорошая идея в рейкьявике опять же возвращать 429 как правильный статус ответа что это не какая-то ошибка или 500к это валидно отработанная вещь которая не может быть больше тебе предоставлено и в том числе уважать ретро авто то есть если клиент видит что ему вернули 429 либо заголовок ретро автор неплохо было бы действительно к нему прислушаться и не пробовать долбить этот сервис ретро яме а чуть-чуть подождать вторая вещь мужские митинг и этот танк случай когда мы вроде бы сделали правильный red limit у нас может резко пробить спайка вaм запросам то есть мы можем сделать не стой orbeez а там тысячу запросов в 10 секунд давайте дальше как раз и посмотрим более визуально как это может выглядеть у нас есть какой-то трафик на наш сервис все идет хорошо мы 7 или 5 почему 5 показываешь хоккей идет спайка вый запрос и мы точно знаем что наш с темой не готова обработать больше чем 2 если рпс при этом здесь мы получили точно больше и убили нашу систему что мы можем здесь сделать конечно-же включите самые red limit и мы поставим 1 ряд лимит по стратегии burst то есть 100 рпс все что выходит за пределы сотни будет сразу же отрезан то есть этот прям жёсткая граница не идеально то есть здесь мы можем увидеть что если действительно пришло много запросов мы их обрежем все классно так и хотели если нам придет чуть чуть больше запросов чем мы ожидали буквально там 20 fps сверху мы смогли их обработать то есть зачем то мы вернули пользователю ответ что извини дорогой друг она недоступна наверное это не самое лучшее решение что можно здесь сделать это включить вместо бурский митинга рейки митинг допустим по стратегии с lighting виду сладим в мину говорит что мы готовы давать в долг что это значит что если чуть-чуть за пределы выходят количество запросов но потом уменьшается мы готовы простить такие паттерны и мы должны ставить подобные red limit и не на строго одну секунду а допустим вместо star peas поставить 1000 запросов на 10-ку как здесь и в этом случае действительно если к нам пришло чуть чуть больше запросов все круто работает если нам начала констант на превышать количество запросов в этот момент да действительно мы начнем резать the traffic причем достаточно правильно то что буквально там срезая немножко того что действительно могли бы обработки с другой же стороны если к нам придёт тот самый спайка вый запрос мы дадим в долг но мы помним что система не может выдержать больше двухсот рпс рейтинге конечно опомниться и потом вспомнил о квота за 10 секунд действительно заканчивается сарену сырье сразу но системе уже не помочь она умерла в тот момент когда в нее ударил бурстовый трафик решение простейших это комбинация рейд лимитов и burst лимитов мы можем ставить тот самый рейд лимит на 1000 запросов в 10 секунд но при этом поставить будет лимиты на 200 запросов в секунду и в этом случае когда к нам будет приходить чуть больше запросов чем мы ожидаем все будет работать хорошо но если к нам придет опять спайка вы мы не пропустим все что выше 200 и потом подключиться уже обычные red limit вспомнил что действительно надо бы квоту уменьшать срежет и выровняет систем чуть больше про разные стратегии red limit of можно почитать по ссылке есть огромное количество разных стратегии to fix windows lading в windows или кибаки токен baked про все это можно посчитать подробнее здесь сколько места как система отрабатывая теперь запрос приходит j дальше в limiter если метро есть квота он возвращает его дать пользователю и обрабатывает как и нормальный запрос но если пришел следующий запрос и мы превысили свою квоту в рпс и мы не будем дальше прокидывать его в сервис графа которые так умирает а просто вернем ее обратно при этом пользователь действительно не будет теперь вот и 1 показана какая-то статистика и правильная дистанция от ресторана до него но мы не убьем сервис и те пользователи которые могли бы по вот сервисом дорожного графа и получать правильную дистанцию будут продолжать ее получать как это может выглядеть давайте посмотрим исключительно разумно заканчивать время на той все пять минут и без вопросов простите пожалуйста без вопросов это дорожный граф что делает лимитер лимитер пробует сделать тэг осталась какая-то квота подробнее кот наверно можно будет посмотреть по ссылке то можно будет сделать запрос если мы вышли за этот вот он и здесь как раз та проверяется средние количество запросов за промежуток времени то есть по сути дела мы просто делим количество лимита на то время которое последние у нас было то есть мы смотрим е100 у нас там прошло условно несколько десятых миллисекунды или несколько миллисекунд между запросами их можно исполнить это самая простая реализация red limit на самом деле в реальной жизни гораздо правильно используете не там fix винду или слайде в мину в дорожном графе это будет реализовано и использована примерно так что просто мы сделаем использование нашего ряд лимита попробуем сделать лимит пик если ты вернул нам true значит мы можем продолжить и у нас есть квота если кого-то нет мы сразу же вернем ошибку что друг извини кого-то вышла тоже самое можно сделать в джинсы в яндексе есть готовые директивы чтобы работать с рыть лимитом и не писать свой собственный код эти вещи доступны из коробки и если вам прямо сейчас хочется попробовать что-то сделать с этими там у вас его нет не обязательно писать о школе можно включить он нас вашем балансе или скорее всего он уже заранее это все поддерживает могли бы поговорить но не поговорим о кубрике отличный патрон стоит если у вас открыто доклад покликать посмотри как шокер breaker открывает и закрывает рубильник эта вещь которая позволяет нам не добивать систему если мы точно знаем что произошла ошибка и это ошибка не пройдет ближайшее время пропускаем rich клиент система которая позволяет нам делать не только какие-то вещи которые будут делать сетевые запросы но и делать некий sdk то есть переносите логику сервера на клиент те самые сайт car & dev kit киты sdk которые позволяют нам перенести нагрузку стороны сервера и размазать и и равномерно между клиентами и последнее это здесь слишком много все это тыква либо даме это та вещь которая позволит нам выживать в любой ситуации когда даже все произошло очень плохо мы написать идеальную систему она умеет врать лимит она умеет все к брейкеры а нами это тайм-аут и она сама себя балансирует но что если сегодня пятница мы классные инженеры и самое время написав хороший продукт сделать его еще более классным и заработать миллионы денег так и сделаем пятница самое лучшее время для релизов поэтому мы катим релиз который сначала убивает половина системы руки не самая хорошая вещь мы подготовились мы здесь все обработаем так что пускай половине с кем мертва ничего плохого не случится сейчас мы становимся мы продолжаем катите рис не останавливает же миллионы денег и взбиваем ещё сервис заказов теперь хуже пользователь не может сделать заказы наверное не очень хорошо но посмотрим может быть все таки сейчас восстановится по крайней мере из каталог есть монолит монолит может за треть хоть чем-то и отвлечь пользователей дать ему возможность повыбирать но мы забыли что мы накатили миграцию и немножко сломали базам и нет базы нет монолита и теперь пользователь вместо того чтобы получить хоть что-то получает просто гору 5 соток неплохо зарядить или в пятницу последнее что нас волнует там где-то абэ развлекает сам собой высылает пинге но пускай что остается делать ну выдержанных наверное сервер из розетки нормально по программировали потратили 40 минут время на паттерны класс лучше не сделали проблема какая масштабный цедент мы банально сделали проблему на ровном месте и сломали систему это может быть проблемой не только из-за нас это может отключиться дата-центр сломаться зависимость может произойти какая-то проблема железом полная недоступна сервиса и полная недоступность базой вся та вещь которую невозможно быстро откатить обычно приводит таким вещам то что мы выкатили сломали базу и чтобы ее восстановить обратно сейчас нам нужно примерно но недельки две неплохо и либо это какая то очень тяжелая логика в сервисы которые просто его разламывает и дальше не дает им восстановиться решение 2 первое нормально делай нормально будет это правильная вещь второе это упрощенная копия сервиса то есть вместо того чтобы полагаться что мы идеальны инженеры и наши сервисы не падают мы сделаем подушку безопасности которая будет реализовывать супер простую реализацию нашей системы она будет возвращать супер простые данные упрощенную модель но будет что-то давать пользователю ни в коем случае этой схемы не должно быть зависимости то есть если мы полагаемся на q эту подушку безопасности очень грустно будет если у подушки безопасности есть своя подушка безопасности который тоже может сказать поэтому это последний рубеж минимум зависимости минимум логике да деградируем систему до харин достаточно плохая реализация но пользователь хотя бы получить какой-то ответ и сможет взаимодействовать системой в нашем случае когда полмира сгорело и мы добавили ту самую тыкву пользователь не просто роняет эти запросы о балансе он начал их лидере ехать обратно в тыкву она действительно возвращает хуже реализацию каталога и заказа наверное создаются в базе а пишется куда-то влог но они пишутся и мы их потом установим мы их не потеряли и когда наш админ все-таки дойдет до серверный выдернет вилку воткнет вилку и откатит релиз полмира выдохнет система начнет сама себе восстанавливать опять поэтому запросов монолит опять расцветут цветы пойдут заказу сервер заказов и тыква на впринципе то больше будет ненужное и должна можно будет отключить до лучших времен прочие паторны явно стоит об этом тоже подумать не то что мы успели все то что я сегодня хотел рассказать и тем более не успею русская сейчас провод этот список но и у важно знать особенно прямые ручкой и на этом спасибо коллеги я искренне понимаю что огромное время затянул но мне было безумно приятно вам это рассказать спасибо тебе большое спасибо тебе памятные призы от конференции у меня они от казус . у меня да у меня недостаточно прав чтобы растянуть временные рамки спасибо кормилец спасибо тебе успехов друзья сейчас самый момент когда можно на спикера накинутся его сейчас поведут в электронные кулуары и задайте ему вопрос и пожалуйста не публично тем более что нет микрофон инженер отвечает честнее и подробнее и также коллеги вы можете у меня поймать на стенде яндекса я буду там дежурить так что подходить если захотите поболтать dust and index не пропустите"
}