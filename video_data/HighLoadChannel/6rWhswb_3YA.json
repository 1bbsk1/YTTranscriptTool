{
  "video_id": "6rWhswb_3YA",
  "channel": "HighLoadChannel",
  "title": "Репликация между разными СУБД / Михаил Буйлов (Мамба)",
  "views": 463,
  "duration": 1383,
  "published": "2019-05-15T02:49:09-07:00",
  "text": "драсти я зовут миха булав я отец директор компании мамба я вам скажу сегодня зачем мы написали репликацию между маску или мы тарантулом во-первых шаткую мамба есть класс мамба это порядка 80 по хп фронтов на которых крутится php код собственно говоря это примерно 70 mais quel физических серверов на которых лежит порядка двадцати терабайт данных в основном переписка и там всякие разными этой информации про фоточки мамба отдает примерно 15 тысяч запросов в секунду в пике в пятницу вечером скорее всего им и отдаю для этого должны получать из баз данных примерно 25000 профилей в секунду собственная я про это сегодня и буду рассказывать как мы это делаем примерно вот так выглядит страница мамбы на ней много профилей поэтому 200000 профиль это еще не так же очень много мы старались как мой меч как могли мамба это dating service и поэтому наша задача показать по лесу нем как можно больше профилей чтобы они могли себе найти понравившийся но в становой нагрузка это взаимодействие между пользователями когда один пользу сделать очень за другим например палате фотографии написать сообщение поэтому основная нагрузка это одиночный get запрос и профилей один из профилей это тот кто делает действия 2 это тот какого-то если приходится чо произошло соответственно мамба была разработана порядка 15 лет назад и основной самый интересный движок для хранения данных это был мой сколь поэтому наши профиль это просто таблица маску или который потом по пристраивались более мелкие таблички в которых лежали не обязательные поля типа того что у пользователя вдруг появился телефон или там где у координаты там 15 лет назад пользователь не было телефонов кардио поэтому появляясь постепенно с течением времени но так или иначе я сейчас выбор профиля из базы данных это четырехэтажный там 5-этажный jojen вас новой таблицы и добавленных полей в которых мог лежать на ненужные нам данные по первичному ключу когда мы перестали справляться с нагрузкой если вы наши начали падать мы решили что лучше всего заказывать наши профили в memcache и не придумали ничего лучше чем читатель лукаша ничего там не ходить потом есть базу и писать в базу то что нашли восток на горе в базе я думаю что многие такой практик палец сюда из такие люди не это никто правда никто не писал базовым кэш ну 15 лет назад хотя бы классика отлично хорошо да довольно быстро мы поняли что нам нужно достать из базы данных несуществующие объекты например сам каких не смежных сервисах остались еще ладишь ники пользователи которые были удалены а вот первично карающий уже нету и такая нагрузка была довольно серьезно что нам пришлось оперировать отсутствие объектов в гонках и и научиться различать эти записи от того что ты же просто нету данных и они там испарились соответственно теперь наша дай ключи немножко увеличился и мы должны были понимать что нам пришел и смог сша пустой ответ и интерпретирую по-своему нет ли чем к нам пришел ответ от того чтобы будем каша я просто нет тут такой конкретной записи после того как мы перешли на каширование вам кушая нам нужно инвалидизировать наш кэш и мы выбрали стратегию удалении записи при изменении данных базе данных как ни странно почему это была выбрана потому что при обновлении каша с обновлением базы данных можно напороться на ошибки раз кондишен of и тогда в каше останутся не самые последние данные и там один запрос может прийти речь работу другого что будет выглядеть для пользователь как как просто не сохраненные данные какие-то поэтому мы инвалиде ruim кэш и удалить ним при любом изменении записей это инфляцию нужно делать в правильный момент например если сделать удаление после изменения данных базе ну да комета то правильные запросы могут засчитать старую версию из базы и статью в кэше тогда в базе данных будет все хорошо но пользователь то опять таки увидеть что его действие не сохранились и мы опять не увидим изменить там какой-то работы наших сервисов поэтому удалять данные из каша нужно строго в тот момент когда мы совершили камер все это было хорошо до тех пор пока на мне потребовалось доставать из базы данных профиля по таким-то вторичным индексом например по ним по телефону или почте естественно афишировать все это можно по ключу по которому достаем но тогда мы увеличиваем нагрузку на кэш в плане потребления памяти а также нам уже придется удалять два поля вместо одного что ж такое-то а почему модель соответственно если мы сохраняем в батум сразу два элемента то нам нужно знать что эти два элемента нужно удалять в каждый момент изменения конкретного объекта мы с этим сразу не стали связываться и решили поставить для морской или плагин inhaler socket я проходил сот лет еще потом расскажу судьба его была плачевно к сожалению самое сложное в этом это кэширование коллекций потому что потребление памяти увеличивается еще больше об являться становится еще сложнее в мамбе мы показываем про на каждом профиле список фотографий и для того чтобы список показать нужно выбрать все фотографии пользу сели за один раз чтобы показать их в карусели в какой-то ленте тогда если нас будет несколько выбора кто во первых нужно учитывать то что объекты могут переходить из одной выборки в другую а также в некоторых случаях найти нужно выборку вообще невозможно потому что она может быть там например каким-то необратимым хором и в принципе нельзя будет найти нужный ключ поэтому мы использовали ровно одно кэширование а все остальные необходимый хитрым и откладывали уже на нашем веке на php и отбрасывать элементы которые нам не нужны показать каким-то причинам пример фотки пользователи могли быть забанены прах андерса отец мы его довольно успешно использовали ну как успешно у нас было восемь слоев на ssd дисках они вылетали раз в полгода но при этом это работал сильно быстрее mais quel а кроме того что поддержка москва или окончилась и 56 и то что это было все еще нгб буфер intent db движок который в каких то моменты мог ходить тупо на диск потому что у меня было в кэше необходимых данных и все начинал не резко тормозить поэтому мы начали с поиска альтернатив и взяли в качестве тратил тарантул во-первых если откатить вратах этлок то можно сравниться с а поскольку сколько шум вторых из самых главных он имеет вторичные индексы теперь нам не нужно дублировать элементы каша и в-третьих самых главных для нас на самом деле как оказалось для того чтобы старте демон не нужно терять всю информацию которая была в нем до этого тарантул умеет до 5 все содержимое на диск и после старта не нужно пробивать каши заново это очень облегчило нам работу таким образом мы изменили тарантул м с это самым конечно тарантул и все было бы хорошо до тех пор пока не оказалось что мы теперь не можем понимать если в каше элемент или просто элемент отсутствует базе данных потому что наша синхронизация все еще осталось на месте и мы читали ресторан полу не видели там ничего лезли в москве ли там тоже ничего не было мы столкнулись hit me some как раз для этих целей мы и не прилипли к цию это выглядит очень просто мы просто пишем в миску и и просто читаемость тарантула соответственно репликатор переносит прозрачно все данные смазку эля в тарантул без участия bk и нам не нужно писать плойку синхронизации какую-либо что он себя представляет во-первых он основан на библиотеки lips life он напрямую пишет в тарантул и это оказывается быстрее чем реплика в mais quel и он поддерживает только рабы сертификацию потому что для стоит lambo ec пришлось бы писать еще какой-то система разбора запросов ну и он позволяет реплицировать таблицы до полей то есть мы имеем в кэше только то что нам действительно нужно они все таблицы и базы данных целиком примерно вот так работает репликатор у это отдельный демон он может ставиться на любую машину в сети и у нас есть две системы защита плен и одна для профилей которая хранит репликатор на машине сторон том и одна для фотографий которые лежат у нас в определенные кластере на 46 машинах маску или мы там репликатор защита плен и для каждой базы отдельно и 46 штук то пишут в 2 тарантула как происходит склеиваем таблиц во-первых тарантул каждое изменение записи сваливается отдельно то есть фактически приходит кусками и для нашей общей записи необходимо в момент прихода первого куска с insert нам создать новую запись tarantul и которая заполняется пустыми значениями и удалять эту запись и тогда когда все значения пустые когда все остальные записи удалили предыдущие значения своим когда мы писали нашу систему москва тарантула 19 который поддерживает нули в в индексах еще не вставал поэтому мы строго типизированные все наши данные и сейчас с выходом последних версий скорее всего мы это изменим и он будет поддерживать на любую индексы но текущая реализация то не поддерживает к сожалению мы об этом узнали очень поздно но сейчас мы пишем конкретно пустые строки в тот момент когда к нам приходят значение запись которые удаляют значение в таблицу примерно вот так выглядит профиль нагрузки христос с хроничные наших профилей у нас есть один мастер 4 единиц iso к нему слоев потому что один из нас не выдерживает на полную нагрузку от наших профилей и 20 гигабайт памяти потребляет каждый из инстансов на двух машинах вот так примерно выглядит профиль нагрузки на запросы фотографий там имеется 46 мастеров и которые запросто пишут в дворец носа тарантулы который занимает порядка 8 гигабайт на каждые отдельные ноду вот это то что мы столкнулись с самого начала мы не могли сделать уникальные индексы для наших вторичных ключей таких как поля почты или телефона поэтому мы начали писать пустые значение ну и соответственно было по большому счету плевать на то что уникальный индекс и нет на диске потому что это конечно но мы все равно упирались необходимость обновление значений в тот момент когда приходили данные за реплики которых еще не было на в самой записи после того как мы запустились первое что произошло это отказ в авторизации пользователей для айфонов потому что iphone и начали желать нам заглавные буквы емейлов и таких записей у нас не было потому что движок мой сковиля поддерживается регистр независимый поиск который мы все время как то использовали и у нас все хорошо работала поэтому для переходный тарана пришлось во-первых про облететь всю базу в которые были записаны email и в совершенно различных регистрах во вторых каждый раз при запросе в тарантул использовать приведение к нижнему регистру для поиска мы однопоточный сервер это конечно очень хорошо но нам его не хватило по нагрузке поэтому нам пришлось сделать 4 место со который фактически дублируют памяти употреблять все раза больше ресурсов если бы тарантул поддерживал посмотрите хотя бы на чтение мы бы сэкономили четыре раза больше памяти это самое интересное что мы со всем что мы столкнулись это приведение типов поскольку мы пишем на php и нас типы приводится сами как хотят и если вот такую запись в такой-то просто пролив тарантул в нам выдастся 1 рандомная запись из тарантула и все это происходит от ошибки того что один пользуюсь я отослал другому фотографию которую несмотря на силе сдаваться и в летать и ему показалась какая-то рандомно я простите голая да это вот мама в moby такое бывает сколько времени прошло до на это рекорд в холоде самый быстрый доклад в мире на что можно сказать по выводам во-первых поскольку сейчас уже никто не кошелек мой склеили то используйте базу данных но если кашина мой кашель еще базу данных но если все-таки вам пришлось то не кэшировать их мой скобелем кэшировать лучше тарантулом потому что он во первых с простыми по скорости утолив побогаче по функционалу но по большому счету мы сделали наш рипли катар как способ получения push эвентов по записи в базу данных и теперь мы можем его использовать вообще для любых целей например при передаче там совершенно различных данных для того чтобы использовать их в одном месте в одном из власти тарантула для поддержки каша для различных совершенно различных сервисов спасибо за доклад он был очень быстрой давайте кто первый спасибо за доклад это не перу я здесь я здесь за да я вижу два лапы два вопроса 1 как и зачем нет вы использовали до поры до времени хэндлер socket до поддержкой о к сожалению давно закончилась насколько я помню упаду был подобный опыт и я не помню спрыгнули они с него или нет я скажу это вопрос использовали вы прогуливали в ли вы какие-то альтернативные решения например самой сквер предоставляет низко уровнем доступа к таблицам с помощью инструкции хендлер или например там какой-нибудь мама ваше сергей до либо мкэш плагин все упирается в диск так или иначе в нтб и поэтому она все равно по умолчанию тормозни мы тестировали и у нас ничего не вышло мы даже не переключились на мойках интерфейс пришел помочь им это ничего хандри соков и второй вопрос что у вас происходит если у вас например с тарантулом free lover то есть у вас инвалиде руется кэш что вы делаете дальше с рипли котором вы прогоняете все то что от него прилетает или например просто вот стартуете с того что есть зависит от того что произошло то есть если нам нужно просто выключить на обслуживание там и дам пирсе на дисках и запоминаем последний по интер который указывает на репликацию из него стартуем но если у нас какой-то крыш произошел дат мы пересчитывать таблицу целиком заново подождите то есть вы тянете с музыка для полностью источник и из если произошла потеря ну то есть не непредвиденная ситуация такая что мы потеряли backup мы делаем и раз какое-то время snapshot кладём его на диск в этом сам шопе в счете есть указатели на аппликацию соответственно если мы поднимемся статус nightshade у нас будет указатель на репликации мы из него над продолжим чтение но если произошел какой-то совсем включайте да ну если мыть машину целиком потеряли теоретически у нас две машины разные и мы можем перетащить бэкап с 1-го mas 1 машина другую стартовать с неё вот в этом в принципе наверно сработать а в случае если у вас произошел фэйл over вы поднимаете если у вас там допустим вы поднимаете instance тарантула а в репликатор уже не может получить эти данные просто шубин логов уже нет за определенным ну тогда перечитать но мы стараемся часто д'ампеццо так чтобы что-то еще оставалось спасибо да ну вот продолжение вопроса про фолловеры сколько будет ну сколько какой возможным простую если придется преодолевать данную что что произойдет все будет стоять как то там часов во первых failover это означает что у вас несколько инстансов и тогда ничто не будет стоять просто будет стать отдельной аппликации то есть даунгрейда сердце не будет с одной стороны другой стороны все зависит от того какой у вас объем базы данных сколько вы будете вычитывать если в самом худшем случае а сама что случае вы становитесь нас ссср фото и после snapshot а просто направить импликацию ну то есть я это быстро будет есть параллельные да да у нас в репликаторы работает один поток на одну один тарантул соответственно если нам нужно сделать несколько из автора тарантула то мы закончили пико-таро да и второй вопрос плохо и знакомств тарантулом если там проблемы с со схемами домах и сальто раме майской происходит если в альтере таблицы уаз поребриков знаете моё по добавлением поля не удаляли ничего не изменяли типы у нас не было проблем а как это ловится то есть тарантул есть схемы или там да есть как как ловится альтеры в минске или как изменение москва или нет матери москве не обрабатываются репликатором это нужно делать уже не вручную потому что нет довольно серьезная операция более того наш альтеры на мускуле там могут длиться несколько часов поэтому мы восстанавливаем там сервис грубо говоря на ночью в такую полную луну и после этого накатываем сначала на мастера вольтер потом наслаиваю который нас там еще парочку осталось а потом на тарантул да еще отдельно спасибо я хотел спросить по поводу схемы вот там где вы указывали май скул репликаторы и тарантул стрелочка от репликатора обратном и сказал что на поднятие полные из без без дампа это прям стройного репликаторы можно спуститься прямо у него средствами да привет интересно мне такой вопрос вот вы сделали все через lipslide через репликацию такую а кто смотрел доклад тоже разработчика тарантулы они там рассказывали про проксирование к миску или ну через тарантул то есть запросы направляются ну прямо к тарантула всегда да у него нужно саму маску или спрашивает когда там нужно смотрели такое решение почему выбрали да это же долго просто нам этом мы рисовали кэш конечно же начать быстро если мы будем ходить в миску или в которой лежит на диске то мы сначала дошли до тарантула потом ходили майскую или фактически для этого тарантул не нужно можно сразу москва исходить ну насколько я опять же понял из доклада я конечно могу ошибаться но у них было какое-то при чтении тоже там ну то есть они заранее знают нет мы чисток ашером и кошелем все сразу это гарантированно это гарантирует быстрее чем куда-то ходить в каких случаях ясно спасибо мульти традиции пытались стимулировать как то есть ли в этом смысл нет ни пытались потому что у нас настолько большой поток репликации то есть он конечно большое он нам давал ssd но это было там в течение полугода поэтому не сильно заморачиваться с такой большой чтобы ему литра делать большой небольшое то в абсолютных цифрах вы знаете я не скажу прямо так не месяц к могу прямо у нас даже не было мысли о том чтобы в эту сторону заморачиваться на таком был небольшой то есть что вы такое можете писать в москве чтобы вам приходилось читать в несколько потоков дать какие диски должны быть в на мастере что вы все этого лежит выдержать чтения в память ну записи поить а вот там самом конце спасите спасибо скажите а ваш репликатор его можно где-то попробовать создает на упаковке от tarantula идет да да отлично когда мы написали сразу то ли туда на самом деле это не прям чисто наша разработка это разработка с новой цена куда старым кастомном не допили нам ребекка тори который использовался в мыло ручной туле рекламе то ли где-то еще которое нам выдали говорили сказать сделать с ним что хотите и мы его просто до меня его состояние для того чтобы можно было использовать в общем случае в продакшене теперь он там до фокси поставки"
}