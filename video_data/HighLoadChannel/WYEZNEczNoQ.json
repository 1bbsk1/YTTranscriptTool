{
  "video_id": "WYEZNEczNoQ",
  "channel": "HighLoadChannel",
  "title": "Что делать, если нужно обрабатывать миллиард хаотичных задач в сутки на PHP / Антон Горин (ManyChat)",
  "views": 6105,
  "duration": 2946,
  "published": "2019-05-15T04:03:47-07:00",
  "text": "всем привет очень рад видеть столько людей здесь сегодня меня зовут антон горин я сетей сооснователь компании мини-чат мини-чат как продукт это saas платформа которая позволяет бизнесом коммуницировать выстраивать коммуникации со своими клиентами через facebook messenger на сегодняшний день к нашей платформе подключена 400000 бизнесов активным образом который коммуницируют со 100 семьюдесятью миллионами своих подписчиков с помощью нашей платформы наши клиенты находятся по всему миру и они используют мини-чат для того чтобы поддерживать своих клиентов для того чтобы предоставлять им возможно сделать свой функционал какой-то для того чтобы делать маркетингу автоматизацию в общем использовать канал мессенджера для хорошо известных им хороших старых добрых маркетинговых занятий что же что же знает а значит в техническом плане в техническом плане это значит что где-то там у нас на заднем дворе стоит такой здоровый завод по процессингу огромного количества различных событий это может быть как мессинга вы и событие то есть там пользовались что написал нажал на кнопку прочитал сообщение так и десятки внутренних событий которые генерируются внутри нашей системы той самой маркетинговой автоматизацией на сегодняшний день мы обслуживаем примерно 40 процентов всех коммуникаций которые происходят между бизнесами клиентами на facebook messenger это достаточно много о чем же мы сегодня поговорим я постарался сделать свой доклад более-менее практическим и поделиться с вами информацией о том как мы работаем с нашей процессинговой нагрузкой до асинхронно асинхронным процессингом задач на бэг-энде эта история про 7 лайфхаков и один любимый нами велосипед через эти истории мы с вами обсудим несколько важных вещей во-первых все-таки мы разберемся что такое вот этот вот асинхронный processing событий мы воздадим должное очередям как одной из важнейших частей всех подобных систем горизонтальное масштабирование мы сегодня обсуждать не будем ура наконец то на хайло где они обсуждают горизонтальное масштабирование на самом деле все дело в том что в процессинговых системах масштабирования довольно сложно устроенная часть его конечно же решится постой как мы нагрузку от балансируем и свою систему под это подгоним но часть не решится и нам придётся всё равно как бы решать это каким-то глобальным способом что ну естественно выходит за рамки сегодняшнего доклада поэтому поговорим мы сегодня про балансировку нагрузки ложащийся на эту систему так как нам довелось повидать всякого мы расскажем чуть чуть я я в лице нашей компании расскажу немного про те инструменты которые мы сделали пока боролись с этой нагрузкой ну и конечно же в первую очередь очень хочется чтобы все то что сегодня прозвучит хоть как-то может быть пригодилась вам если вы не строили такие системы или может быть хотя бы дала возможность чуть-чуть что-то свежее услышать он и поднять где-то в голове если вы уже много расстроили подобные системы что давайте приступим очень коротко о том что же в чем же предмет сегодняшний сегодняшнего монолога любая система так или иначе как бы она не была устроена у нее есть какие то свои внутренние характеристики их можно наделить сколько угодно да то есть откуда берутся задача как быстро мы их процессе какие зависимости что с результатом куда мы его отдаем там толерантность к устойчивости к отказам к каким таким вещам очень много много разных вещей и на самом деле если немножечко подумать о том что же такое асинхронный processing то проще всего сравнить его с нашим стандартным любимым в blonde тот который вот мы вот только вот вчера вечером еще продолжали ходить в свои любимые даёшь key если вы bloat это что-то что нам нужно очень быстро вернуть пользователю очень быстро отработать и очень быстро вернуть очень часто с перекосом на чтение и так далее в общем всякое такое бывает the processing отличается от всего этого только тем что пользователя по сути чаще всего нет того который ждет прямо здесь и сейчас иногда бывает нужно прямо мгновенно но его нет нам нужно как-то попытаться справиться с этим где-то там внутри системы и так далее этим в этом и есть отличие поэтому собственно все о чем мы будем говорить это те самые обработчики задач обработчики событий которые так или иначе реагируют на те или иные события с помощью бизнес-логика которые у нас уже есть обычно ладно давайте перейдем к первой части что я бы посоветовал делать в первую очередь после того как мы прошли через этап инициации нас как разработчика подобной системой ну там не знаю написали первую крон jopu поняли что у нас получилось но это было конечно у каждого из вас очень давно много лет назад после этого мы написали где-то в какой-то момент демон свой первый кстати am можно чтоб чуть-чуть нам размяться а поднимите пожалуйста руки кто когда не писал демона на печке а есть люди которые считают что это невозможно а вот это неожиданно вы мне уже нравится в общем мы написали себе такой демон который смотрит в очередь и что-то разгребает что нам вообще делать сколько нам нужно сколько как нам нужно работать со своей системой со своим там не знаю монолитом например вообще в 2018 году на сколько я понимаю слова монолит нужно такой долей как бы извинения как бы в голосе произносить но но это не так мы все мы все знаем что это работает в общем есть у нас монолит давайте предположим что вот наш робот джон не хочет отправить сообщение через the messenger какому-то человеку вася есть у нас какая-то какой-то кусок бизнес-логики написанные замечательными программистами где-то там в коде там уже давно написанный что что что мы в нем делаем обычно вот просто но как бы в упрощенном виде мы дергаем скорее всего фишку фэйсбука но там или каком telegram или любого другого мессенджера получаем от нее ответ после этого по сути сообщение на самом деле в вася уже доставлена но нам нужно внутри что-то продолжить делать нам нужно например положить эту сообщен q в 100 рачка куда то в каком то виде там с метаданными а может быть после этого из этого стороны еще и проиндексировать в поиске ее мало ли может у нас поиск есть по этим сообщением а еще и бы статистику было бы классно вообще говоря обновить там не знаю добавить куда-то единичку совершенно очевидно что подобная система если она будет хорошо нагружена а про ненагруженной системы мы сегодня говорить не будем вообще про системы не при смерти разговаривать не так интересно на таких конференциях мы конечно же здесь буду это будет очень много проблем очень длинный трек того что происходит внутри вот это вот вот это вот задачи и ее очень хочется разбить очевидно что каждый из вас уже наверное даже несмотря на то что доклад вроде как позиционировался как новичков и я практически уверен что оказывает лишь каждый из вас уже это хоть раз делал признаетесь мысль мы берем и вместо того чтобы начать процессить это прямо здесь начнем начинаем отдавать это сервисом так как это не веб-сайт сервис вполне себе но как бы может быть асинхронный нам не то чтобы нужен как бы результат от него нам главное чтобы он задачу выполнил если там сильно нужно будет мы еще поработаем с тем чтобы он надежно это сделал соответственно мы там разложили эти задачи по для старриджа для статистики они там забрали какие-то процессы разобрали эти тоски в общем кто то еще в индексацию что-то отдал но потому что надо там не знаю там например индексы какие-то перенести из 100 раджа для того чтобы знать где искать потом это сообщение в общем абсолютно простая штука в чем в чем главная мысль главная мысль заключается в том что на самом деле вы несет такой сервис мы рано или поздно сталкиваемся с тем что он сам станет вот этим вот узким горлышком не всегда но очень часто этот принцип который я бы хотел посоветовать вам если вы с таким еще не сталкивались это то что за системой нужно следить и если уж вы стали на эту скользкую дорожку до тех пор пока вы можете позволить себе транзакционные издержки систему надо продолжать бить столько сколько набьется разделять ее на отдельные части и выносить в асинхронную обработку все эти части которые которые у вас в результате этого разбиения получились потому что по сути своей это синхронные сервисы всегда можно круто за оптимизировать например вообще говоря закинуть там не запечь пи и поставить туда там какой-нибудь другой язык программирования или поднять там какую-то систему до за примерами ходить долго не надо любые там статистические системы какие то поисковые системы по большому счету как бы я если вы напишете такой процессор то он будет брать из очереди какие-то эти задачи и скармливать их матчами в какой-нибудь elastic search или там не знаю crack house и это будет обеспечивать очень крутую оптимальность обработки таких событий потому что эти вы вы вы будете вывалю вы будете выжимать очень крутую утилизацию потому что эти сети процессоры всегда заточена на то чтобы очень четко делать одну классную работу и и в этом есть смысл и масштабировать их гораздо проще потому что профилях нагрузки может ну как бы очень сильно отличаться выбрав их из одной цепочке вы можете их независимым образом масштабировать соответственно ну с балансировкой нагрузки все понятно здесь даже там слишком сильно останавливаться на нет наверное нет смысла как бы мы можем быть избран байт в в эту систему поток но до какой-то степени пока там в общем совсем что-то страшно не случится скорее всего вытекать из нее будет высыпаться из нее будет там примерно примерно плюс-минус одинаковое количество все зависит от того сколько мы сможем принять новых надежность конечно же внезапно не появится просто так потому что мы эту систему таким образом разбили но ну как бы достаточно очевидно что очередь позволяет чуть чуть проще справляться с отказами особенно с учетом того что мы только что взяли и независимые друг от друга части разделили дальше нам останется работать с каждым конкретным сервисом для того чтобы он был надежен ну как бы то есть если вдруг вы у нас в этом большом кирпичи отвалился какой-то промежуточный шаг скорее всего мы бы потеряли там все все следующие шаги ну как бы там довольно очевидно я понимаю что можно реализовать внутри логику но все очень сложно то есть например вот вот вот эта операция которая здесь масла диана скорее всего далеко не едим патент на потому что если мы ретро и попробуем сделать скорее всего наш вася получит второе сообщение потому что первым же делом который мы делаем мы отправляем сообщений facebook все внешняя система но как бы можно забыть там про там железобетонную транзакционных ну и на этом на самом деле все самое главное это все таки не бояться развивать эти системы и делать это до тех пор пока транзакционные издержки позволяют сейчас пойдет быстрее погнали вторая глава все достаточно просто мы написали несколько таких сервисов они работают в продакшене и вроде как все классно но один прекрасный день нам прилетит нагрузка с которой обработчик событий они не справятся ну как бы мы можем и конечно тестировать печенюшку там что-то там добавляет спилить вертикально или еще что то но мы не ради этого собственно делаем такую машинку естественно у нас первое желание которое возникнет а давайте мы поднимем 2 обработчика который будут разогревать эту очередь и чудесным образом там как-то пытаться справляться ну может быть там ну как бы на кухне сзади ещё что-то будет страшно и сложно что-то придется радировать или еще что то но там в принципе в первом приближении это должно сработать но судя из опыта и и того как с чем мы сталкивались чаще всего когда программисты пишут бизнес-логику которая в монолите лежит раньше выполнялась там где теперь спичками в как реакция на какие-то запросы начинают появляться не от омар насти они они почти никогда не заметны когда мы это дергаем обычным но как бы в обычном виде в в обычном большом приложение но как только мы поставим две машинки которые будут со скоростью там ну там несколько тысяч ну сколько зальешь только зальешь несколько тысяч раз в секунду долбится по этой очереди попытаться из нее что-то взять и после этого обработать конкор нэнси заставит все эти не от омар насти мгновенно всплыть и просто начать фигачить во все стороны просто мигать проблемами богами и так далее дублирован ими сообщениями потерянными сообщениями чем угодно поэтому из-за того что рано или поздно практически любой такой сервис захочется начать горизонтально масштабировать лучше всегда заранее весь код который мы выносим в такие в такую обработку всегда проверять на то что он умеет работать параллельно там нет нигде никаких не от омар настей так далее особенно в работе с очередями ну здесь все довольно просто и единственное о чем наверное еще останется добавить это то что надо помнить про очередность сообщений и если очередность сообщений не позволяет нам просто поднять н worker of которые разгребают нам придётся с этим что-то сделать например по какой-то логической своей структуре к какому-то логическому принципу page or did например эти данные использовать он очередей давайте пойдем дальше веселая глава номер три философская hallowed это спорт высоких достижений это такая экстремальная дисциплина и как в любой экстремальной дисциплине на самом деле там всегда есть возможность искать баланс где-то проиграть для того чтобы выиграть еще больше как как показывает практика где-то и на уровне технологии на уровне бизнеса всегда кроется какие-то улучшения и какие-то послабления точнее который можно в итоге превратить в крутое улучшение например мы можем сказать себя ребята а мы точно на сто процентов ровно отказов отказоустойчивые просто в любой точке нашего кода мы точно готовы к той самой уборщицы которая все-таки выдернет шнур в дата-центре здесь если будет интересно зайдите можете зайти к ребятам из enterprise секции на доклад там ребята от ли отличные штуки рассказывают про там аппаратное резервирование процессора в горячую замену процессоров в каким-то enterprise системах и так далее конечно же наверное для нашего бедненького пички монолита это немного избыточная фигня поэтому другой в другой вопрос например мы можем обработать например что-то нас 100 миллисекунд позже зато мы сделаем это каким-то более сглаженным способом ресурсов в общем в поиске этого баланса на самом деле и состоит очень крутой потенциал оптимизации системы практически всегда можно учета усмотреть благодаря чему взять и сделать все еще круче leithen супер системность отказоустойчивость от омар ность не трогайте пожалуйста раз уж на то пошло 4 глава быстро я на самом деле есть две штуки связанные с обработка ошибок в системах процессинга они они такие тут немного взаимо противоположны во-первых если взять и применить к классной цепочки обработки отказ мы тут же получим классную цепочку отказа все чего нас шло дальше она перестает работать ну очевидно как бы тут далеко ходить как бы не надо за за за за за примерами и соображениями есть еще один чуть более неочевидный кейс о котором обычно забываешь потому что код ты этот писал монолиты этот писала обработку ты написал бизнес-логику год назад два года назад это собственная обработка ошибок непосредственно конкретно все наши любимые треки ч потому что в реальности мертвый processing это очень быстрый процесс инка если мы не дай бог где-нибудь там слишком хорошо словили ошибку и у нас что-то пойдет не так ну например apes к что-то отвалится и мы там замечательную сеть маме гнезда в дата-центре где нить или в облаке мы замечательным образом с офигительной скоростью просто paper ним всю нашу очередь в ноль все что туда будет течь мы мы с замечательным образом просто за полсекунды sagem и потеряем эти данные ну если мы каким-то дополнительным образом их все-таки файл сейф не не обеспечивали голова номер 5 конечно же она про очереди очередь в этой системе ну как бы очевидно является центральным звеном и без них ну практически не возможности представить такую асинхронную систему очереди очевидно и всем вам хорошо известно лежат в основе всего чем и только не трогаем все там ок в огромном количестве частей операционных систем сетевых протоколов программного обеспечения до всего и на самом деле самое крутое что можно сделать это использовать те системы очередей которые идеально подходят под нашу конкретную задачу сейчас есть огромное количество инструментов которые реализуют которые работают как брокеры сообщений шины данных просто очереди и на самом деле для различных задач очень хорошо подходят свои инструменты и их не стоит бояться использовать для этого иногда конечно не хочется разводить зоопарк но в принципе скорее всего два максимум три инструмента скорее всего даже два могут решить практически все все все виды подобных задач что самое интересное с помощью некоторых из них мы например в очень любим редис и используемого по полной программе но здесь это не совсем система очередей но ну как бы понятное дело и кто кто среди сам работал примерно себе представляют что подобная система работает in memory работы с данными со структурами данных позволяет сделать довольно много гибкой логики и на самом деле как показывает практика это очень дешевый способ реализовать какую-то процессинговую логику прямо на уровне очереди с помощью этой штуки можно сэкономить огромное количество ресурсов у нас на наши редис на наш редис кластер льется сейчас примерно где-то миллион запросов в секунду и то мне очень там очень немного тачек на самом деле они не очень сильны и и он отлично с этим справляется при том что у нас некоторые запросы просто буквально как было как быть скрипт и атомарные выполняют для того чтобы обеспечить какие-то сложные очереди например с помощью сорта цветов я не буду прям совсем там глубоко погружаться в этот вопрос чтобы мы слишком сильно не засиделись но например с помощью сорта центов то есть взвешенного множество проставив просто в качестве веса time time out точнее таймс темп плюс какой-то тайм-аут можно например организовать инерциальную обработку каких-то объектов ну например у нас каждый там 5 1 секунду падает какая кашка том что пользователь метод а пешки дернул а мы хотим раз в десять секунд записать куда-нибудь отправить куда-нибудь например то что он онлайн ну например там как один из примеров хотя мы немного по-другому это использую соответственно можно это делать через кита локи можно это делать там каким-то образом где-то сохраняя делая нам например там наши и прядь запросы немного state state full да то есть ну что не очень хотелось бы и а здесь просто с помощью буквально нескольких строчек на луи и использование редиса у нас получается делать это ну там практически бесплатно то есть если посмотреть сюда если вдруг вы программируете на печке я надеюсь что многие потому что у меня в названии пищи было написано то ну как бы вот так вот там мы пишем в эту очередь да то есть это 1 один вызов а так ну я не стал приводить php код как бы вокруг так выглядит здорово ссорящиеся процедурка которая просто атомарном образом достает самый первый который должен быть вытащим да еще и желательно чтобы он попадал в этот в это ограничение то есть и в intel юца мы доставим только через десять якут молотова если наша система насчет отказывать вдруг внезапно представьте себе что внезапно начала лица в 10 раз больше трафика но в смысле вот там этих каких-то запросов и система не может обработать что с ней произойдёт юзеров больше не стала ну их там примерно столько же в онлайне и записей там тоже не станет больше и после того как система как-то раз раз тупят там ну чуть-чуть будет больше чем было бы чем было в момент начала этого инцидента мы по сути очень быстрое и разгребём но в общем достаточно много элегантных каких-то способов взять и на очередь переложить какие-то там серьезные вопросы которые требуют от омар насть скрупулезной работы с с состоянием и так далее ну и конечно же составные очереди тоже очень крутой инструмент то есть использование комбинации там очередей лаков иногда позволяют сделать какие-то интересные штуки реализовать какие-то виртуально стелющиеся очереди и так далее я не буду прям совсем сильно погружаться множество там довольно много разных нюансов но чуть-чуть будет последний сейчас последняя секция где мы например такое используем ее там наглядно в принципе это будет видно не забываем про тамар ность все все равно 6 глава в которой мы поговорим об одном классном эффекте с которым вы скорее все столкнетесь когда сделайте если вы впервые еще будете делать такую систему вы обязательно рано или поздно с этим столкнетесь если делали наверное сталкивались а можете пожалуйста поднять руку кто знает что такое эффект шумных соседей а у ничего ся ладно наверное просто термин не знаком обижен на пальцах это это новый этап это эффект когда действие одной какой-то логической группы там пользователей допустим да для простоты affected выполнение действий для другой группе пользователей просто из-за того что она например слишком активно что делает то есть давайте себе представим у нас не знаю мы хотим в очередь положить свои не знаю там сообщения которые мы хотим отправить до бродкасты какие-то приходит 1 пользователь и кладет зелененький стикер в нашу очередь приходит второй пользователь и кладет 2 оранжевенький стикеров нашу очень приходит третий пользователь и кладет 700000 красненьких стикеров наша очередь после которого опять приходит 1 пользователь и кладет еще один стикер чувствуете с этой системой произойдет понятное дело что если мы ее просто будем пытаться разогревать нет это уже проблема это ну понятно что это придется как-то процессить но вопрос вопрос даже не в этом вопрос в том что вот этот второй зелененький пользователь будет ждать пока мы не разгребём все здесь лежащие и это действительно проблема на самом деле на самом деле вполне себе реалистичные кейс и потому что если вы по к сожалению наверное тоненько получилось это состояние глубины например нашей очереди в какой-то рандомный момент времени отправки цепочек маркетинговых сообщений как вы понимаете ну как бы здесь ну здесь не не глухо все то есть здесь происходит processing просто очередь пустая потому что мы ее успешно разгребаем там примерно 0 0 глубина получается хотя там на самом деле processing 1 каждую секунду примерно тысячу каких-то задач обрабатывает и в какой-то момент кто-то пришел и сделал что-то страшное он даже может не знает как бы что он сделал он сделал абсолютно нормальный бизнес задачу свои но нам нужно как то с этим сделать и сами и пользователей готов кому что ему 15 минут придется подождать нам нужно сделать так что другой пользователь не знал 15 минут и и когда там вы делаете например платформу это типа очень страшный эффект там приходится практически каждую часть своей системы стю нить на то чтобы справляться с подобными вещами довольно наверное напрашивающийся вариант и я вот только что упоминал как бы когда говорил про составные очереди довольно напрашивающийся вариант на верное решение этой проблемы был бы а что если мы на каждого пользователя сможем заводить очередь и туда складывать его с его сообщение еще и желательно там чтобы у нас там была вторая очередь где будет список тех пользователей для которых у нас есть очереди и желательно чтобы это все а там арно было и тогда ну как бы очевидным образом мы там например взяли и там по одной начали их отправлять и тогда там очевидно что мы по отправляем всем кому мы должны отправить и только потом будем долго долго долго разгребать вот этого вот замечательного дядьку который там решил отправить 700 тысяч им прямо сейчас какую-то рассылку и составные очереди они реально помогают такой делать при том что ну как бы по своей сути там никак не ничего ничего волшебного там нет там комбинация очередей лаков и в общем веселых позывов чего-нибудь походить ну что последняя глава такая объединяющая после всего того что мы сделали а мы послали систему покуда вообще получается и islay сеть поработали с очередями запомнили про очередность поделали еще много много разных вещей и все равно особе если у нас в этой системе реально уже работает там не знаю под сотню сервисов еще что то происходит у нас равно есть во во всем вот это он постоянном сигнале как бы происходящего в системе все равно есть очень лютые всплески периодически и они генерируются не просто каким-то сервисом а потому что на любой на самом деле сервис рано или поздно откуда тогда может прилететь большая нагрузка чем бы этот сервис не занимался но может не на любой на на абсолютное большинство ну как бы мы только что видели вот этот график на самом деле до того момента когда они не стал заниматься проектом впервые которые очень сильно был направлен на именно на асинхронный processing данных я даже нет нет не задумывался о том насколько на самом деле web нагрузка ровная и плавная хотя мы с ней боремся там годами как бы и вполне себе ну не не вид делаем то есть если например посмотреть на этот график красным цветом отмечена нагрузка на 1 наш сервак обслуживающий сайт зеленым цветом показано нагрузка на цепью на один из какой-то там рандомный из рандомная машина которая делает обработку processing вот этот вот весь нагрузка с ну просто как бы фантастически другую природу имеет и и вот с этим вот приходится бороться после того даже после того как мы взяли вычленили из нее какие-то понятные нам но моды в вот этих всех всплесков вот например здесь прошу прощения цифр здесь не видно просто я решил в общем что они может быть не так важна мне будет спокойнее графики показывать на самом деле разницы никакой нет всплески иногда бывают 100 кратными иногда бывало даже больше конечно же там эффект низкой базы тоже присутствуют но так или иначе и такие тоже бывали и там кстати не 0 как бы вокруг вот этого пика там вполне себе отлично это кстати не глубина очереди это прям количество отработанных задача там в минуту показано и в течение там скольки ты там 5 там 10 минут вот такая вот штука была что же что же вы что же получается у нас на входе в какую-то в какой-то один сервис внезапно получается набор сообщений с которым мы не особо можем справиться ну то есть там не знаю там есть сообщение которые длятся несколько секунд например а процессоров у нас три сообщение таких там опять же 700 тысяч раз уж мы начали про 700000 говорить прилетела понятно дело что мне каждые по секундам ну что если мы перемножим одно на другое получим какое-то такое время не несовместимая с жизнью но в реальности ну допустим там не 3 процессора 300 в реально качественная картинка от этого не поменяется нам будет очень тяжело и кому то что то придется ждать и проблема обычно заключается в том что такое теоретически может прилететь там на почти любой сервис рано или поздно из-за тех или иных действий пользователей или нашей системы и в итоге это все копится эти cis сервисов у вас уже 30 штук на самом деле они могут там одновременно или не одновременно как ты друг на друга повлиять а еще они между собой связаны пишут друг в друга и в общем в итоге это все заканчивается горящим дата-центром и ну в общем не очень классными классными событиями raid-массив один раз мы так сажали что же в итоге делать-то с этим такой напоследок кусочек во-первых к понятно дело что чудес не бывает если вам нужно от процессить тысячу task-ов каждый из которых на сто процентов утилизируют ресурсы какой-то там ресурс который вот он требует то вам нужно тысячи процессоров этого всего дела ну то есть вы никуда от этого не денешься законы физики там особо не позволяют разгуляться поэтому конечно же здесь нам нужно вызывать самосвал грузит туда сервера и в общем поднимать что-то дополнительно для того чтобы все таки с этим справится но обычно конечно же все не так стопроцентной утилизации там какого-то там ресурса конечно же не бывает напрашивается выбор поднятия обработчиков для каждого сервиса и ну как бы на самом деле выбор конечно же очевидным и только в начале нашей нашего разговора говорили про то что давайте там пару процессоров под нею на самом деле рано или поздно если если такая нагрузка прилетает хотя бы на половину наших сервисов хотя бы там ни за раз в месяц там в неделю то что я показал наши графики это не типа 1 месяца это происходит каждый час условно и ну там не знаю каждый день каждый час иногда каждую минуту ну каждый минут это уже выглядит как нагрузка который мы должны быть постоянно готовы как бы к обработке соответственно нам нужно уметь как-то балансировать эту общую нагрузку потому что если мы поднимем он обработчиков то рано или поздно мы столкнёмся с тем что нам не за connection of не хватит каждый из них там что-то себе по открывает как как их папский down спилить там совершенно непонятно чем делать ресурс это общее и на самом деле это все это все приводит к к такому хаку который мы реализовали в те классные в то классное время когда там рост проекта не позволял успевать нормально как-то справляться с с растущей нагрузкой но там я не знаю она там удваивалось каждые пару недель и это был был и были не нулевые значения и мы для себя придумали пару инструментов которые нам в итоге в тот момент позволили справиться заменить некоторые некоторые обработчики уже какими-то более подготовленными к этому системе системами и так далее я расскажу об одном из них он очень незамысловато называется балансер и работает он столь же незамысловато каждый из нас скорее всего работая со своим приложением в какой-то момент жизни сделал так он взял клавиатуру и написал фасции финиш request что-то там прям боевые воспоминания да ну и соответственно сказал слушать а давайте сейчас разорвем соединение как бы fp мая дальше поработаем отдадим запросу дальше проводим и это кстати ну всмысле в этом ничего нормально ничего плохого нет я не знаю я по крайней мере ну как бы там под это специальные темы инструменты даже писал в свое время когда давно и это круто и мы подумали а почему вы здесь не сделал что-то похожее и в итоге мы реализовали наша внутренняя закрыты для внешнего мира асинхронный пи ай да ну то есть асинхронные пьют очень странно звучит на самом деле там вопрос просто в простом мы должны отправить запрос сказать поработай пожалуйста для нас и ответном твой не нужен ну и теперь как записи здесь ровно одинаково не подходит в качестве описания происходящего в чем соль это печка это наш монолит это наша бизнес-логика с нашим фреймворком со всеми ресурсами которые могут взять и поработать над любой задачей которую мы сейчас над которой сейчас там этот бедный задыхающийся процесс поработать не может на что он один или там их там три а можем взять и очень легко и быстро ну как бы почему почему легко и быстро да потому что написано это все там поверх джинсов php в белом или там любой другой технология которая вы используете которые уже отлично отладили которая отлично работает отлично справляется которую мы знаем как конфигурировать наши там админы если если они у вас есть прекрасно знает как поднимать такие машины и и и все у нас до сих пор этот инструмент работает в продакшене мы его используем конечно в первую очередь для тех сервисов которые которые там ну низ масштабированы и совсем уж потому что иногда хочется что-то выпустить и сказать блин я не знаю какая будет нагрузка вот я реально не знаю вот что эти психи придут и придумают как бы что сделать с нашей системой так чтобы и завалить я не могу сейчас предсказать я не я иногда как бы не могу их предугадывать их действия после этого как бы мы начали очень активно и продолжили очень активно используют эту систему для того чтобы запускать все наши процессинга вы и новые штуки потому что оно более или менее каким-то образом вытягивает хорошие масштабирование тай тай тай системы которую как масштабирование который пока написать преждевременно ну как бы как оно работает я думаю довольно уже очевидно понятно и без картинок то есть пардон за схематическое отображение связи сын джинсы с печкой пел в общем берем задачу отправляемые на fpm fpm возвращают 2 сотку разрывает соединение и все наш процесс может делать следующий запрос сколько бы он ни работал его работу теперь выполнит fpm а он может просто делать это т.п. запросы на локальную тачку или на какой-то другую тачку из города как бы вот этих балансиров и таким образом ну как бы понятное дело что здесь никакой магии нет мы только что потеряли кучу ресурсов на network overhead понятное дело стану вся соль начинается тогда когда у нас есть кучу очередей который совершенно низшим образом переполнены который не хочется масштабировать которые начинают лупить всякими разными большими задачами которые могут длиться там и 5 секунд очередь процессор же не хочет 5 секунд выполнять какую-то задачку ему хочется взять следующую задачку и в итоге все таким образом начинает выполняться на и впрямь и он переиспользовать ресурсы переспала переиспользовать полу и так далее очевидные бонусы это то что мы используем готовый стек то что мы используем всю нашу бизнес-логику она у нас есть ну как здесь очень простой правило 1 шумы у нас есть монолит так давайте по полной программе им пользоваться потому что ну так уж совсем как бы было бы глупо не пользоваться тем этой возможностью что доступ к да у нас есть он прямо от на кончиках пальцев однородная архитектура сохраняется это очень круто потому что по сути как работают наши процессоры также продолжают работать этим эти процессы прямо там в фильме обработчики становятся тонкими ну это уже таки это уже детали и мелочи знаете мелочи жизни которые конечно же только когда там постоянно в этих обработчиков копаешься возникают но это странно круто потому что бизнес-логика начинает очень серьезно отделяться от холодной логики и разработчикам проще с этим работать да и в принципе как бы выглядит это красивее простота балансировки нагрузки ну понятное дело можем любой сервис в любой сервис вписать дополнительно пару строчек и сказать слушай ты конечно красавчик мы знаем что ты там не знаю там умеешь в 3 fete а давай так если вдруг мы видим что твоя очередь начала переполняться а давай мы разрешим тебе взять и до какой-то степени вот по отправлять туда запрос и может быть она разгребём и на самом деле это дает возможность более или менее автоматически скелет ресурсы во многих сервисах даже там где на это нет в принципе 1 самое главное надежды и что самое важное общее ограничение все-таки существует ограничение то единое то эта штука она как бы она единая на на всех метод fpm эти пулы которые мы можем там по шее пить для того чтобы там внутри как то это все разделить ну и про что там просто масштабирования очевидно как бы это абсолютно стандартный стек подняли ещё одну тачку настроили в пмм джинкс сисадмин довольны потому что то он уже очень хорошо умеет это делать и моего сильно не потревожили и все им и у нас теперь есть еще одна тачка которая готова сто процентов своих ресурсов раздать на вот такие вот совершенно не специализированные действия ну конечно же у этого всего есть недостатки ну очевидно здесь там я уже немного упомянул что транзакционные издержки конечно же играют огромную роль здесь и под огромной ролью я имею эту прям реально огромную роль если в случае с редисом там мы в овир хотите считали и думали а сколько нам стоит миллион запросов в секунду там помножить на какие то там network тайма ну как пинге в смысле там при временной промежутке там на запросы и так далее понятное дело что там один раз открывает соединение персистенции то здесь нам мы поднимаем еще framework фрэймворк вместе со всем нашим монолитом со всеми нашими ресурсами и так далее мы получили чего хотели мы теперь за это расплачиваемся поэтому ко дну ну как мы в принципе поверх этого еще пару инструментов сделали мы там научили наши процессоры иногда в тех в тех в тех обработчиков где это возможно пулить какие то какие то задачи там где-то там где просто бизнес логика позволяет сделать и так далее но так или иначе проблем остается меж сервисная приоритезация не дает нам немножечко ну как бы полностью управлять ситуацией под межсервисный приоритизации я имею ввиду следующее эта штука реально хорошо работает и вот тот сервис который я при те же ссу шумные соседи тот сервис который самой громкие сейчас он прям сильнее всего и affected машину и аффекте цвету систему и обычно как бы это приводит к тому что бывают очень сильные перекосы когда на несколько сервисов начинают прилетать большая нагрузка ну и может быть не столь очевидно с первого взгляда с теоретического взглядов проблема от которой на самом деле страдаем больше всего ну ладно не больше всего но достаточно много в общем мы нервов потрепали это то что вся эта вся эта компания норовит открыть соединение прямо всюду куда вот вот куда нужно ну потому что опять же монолит опять же мы там пытаемся стучаться куда-то и в итоге каждый этот worker по открывал соединений fpm эти маркеры юзает сильно убрать их тоже не хочет потому что всплески будем тяжело держать мы ну не хочется делать так чтобы они умирали там через слишком быстро после того как не используется но и в итоге на самом деле это выливается в то что нам приходится прям присматривать за то затем что у нас на графике открытых соединений по большинству каких-то ресурсных частей системы там база данных понятно что у вас пока bouncer стоит от каким-то редис и они там иногда очень серьезное количество connection of выдерживают по сути это такой краткий список каких-то инсайтов каких-то таких мыслей которые получилось вынести из достаточно долгой работы над всей этой системой сегодня мы процессе уже даже больше чем миллиард событий в в день если бы я не успел сделать это не знаю подать заявку на этот доклад и пришел бы с этим докладом где иметь там весной на рид скорее всего он бы назывался что-нибудь типа как мы процессе 5 миллиардов соединений задач в день это там примерно про темпа технологического роста как бы этой системы с которыми приходится работать и в реальности эти хаки этого то что получилось из себя выжить для того чтобы то что вот то что давало максимальный отклик на то что же может помочь другим людям справиться с этой системой не бойтесь велосипедов не бойтесь простых инструментов и самое главное старайтесь всегда чувствую физику системы потому что на самом деле как бы от физики системы от физики данных зависит практически все как и в любой другой экстремальной области надо четко понимать как это устроено у нас замечательная профессия всем хорошего дня спасибо за внимание мы успеваем так один вопрос хорошо да коллеги мы успеваем один вопрос пожалуйста если вдруг у кого-то будут какие-то еще вопросы или вдруг кто-то хочет мне где-то что-то сам подсказать пожалуйста словите меня где-нибудь вокруг я буду здесь кто-то хотел задать вопрос понимал руку хорошо спасибо за доклад вопрос спички из-за этого вопрос как вы справляетесь с кондишен рейс конечно чего например там сотни миллионы там запрос в базу данных которые изменяют к что-то асинхронно все это делается до на самом деле закономерный вопрос давай я может быть еще раз подчеркну то что я вот рассказываю она не имеет прямого отношения к горизонтальной масштабируемости системы на самом деле у нас система очень сильно заз келина понятное дело что речь идет там ни про один сервер базы данных и не пройден редис и что то такое у нас система поделен она вертикали ты видел наверное там было написано galaxy 5 вот эта галактика это логический способ физически заизолировать какую-то часть наших клиентов благо бизнес-логика позво ляет и у нас соответственно под каждую такую галактику это отдельная база данных в которой есть еще отдельный сервак баз данных в которой есть свои базы данных отдельные это свой редис отдельно это свои тачки которые обработкой занимаются и мы сейчас даже мы еще не дошли до этого но мы хотим еще и прокси ровать туда за все запросы на сайт текущие в скупе аккаунта для того чтобы полностью заизолировать эту штуку сделать честный grey dial roll out который затронет который как бы там практически за исключением пары структур данных как бы можно можно делать соответственно с базами ничего страшного в этом в этой галактике подобрана такое количество нагрузки которые с которой она готова справиться и соответственно это примерно как бы и есть критерий шейпинга спасибо большое если вдруг будут вопросы я где-то здесь буду"
}