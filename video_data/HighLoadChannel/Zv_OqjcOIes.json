{
  "video_id": "Zv_OqjcOIes",
  "channel": "HighLoadChannel",
  "title": "Эволюция архитектуры торгово-клиринговой системы Московской биржи / С. Костанбаев (Московская Биржа)",
  "views": 2122,
  "duration": 3006,
  "published": "2019-05-15T05:31:12-07:00",
  "text": "всем при меня зовут мой сергей и мы он про эволюцию архитектура московской бирже он начал краткую историю и и технологической части в девяносто четвертом году московской межбанковской валютной бирже сокращенно ммвб была запущена система стс это австралийская система и с этого времени началась в истории электронных торгов в девяносто восьмом году фактической архитектура была доработана до работал интернет трейдинге и с этого момента уже у нас россии прошел интернет-трейдинг примерно с этого времени система долгое время сильным образом не менялось в 2011 году произошло слияние ммвб ртс так и образовалась московской бирже и уже с этого времени прошло значительное изменение архитектуры той части которая разбана давайте рассмотрим вообще зачем потребовалось менять архитектуру это график количества заявок в день за последние 12 лет как видно нагрузка система вырос они просто в разы она порядке даже несколько порядков и связано с множеством фактором с приходом американцы трейдинга с увеличением общего числа клиентов и это потребовало значительного изменение ритик туры давайте посмотрел как и архитектуру было изначально это фактически наша классическая архитектура расскажу на использовалась используется использовалась валютного и фондового рынка фактически это 3 уровень ная система есть центральный узел который обрабатывает все транзакции есть сервера доступа и есть клиентский уровень практически котором работает все клиенты и брокеров в то время этого 90 конец конца 90 х годов 90 восьмой год система использовалась на так называемом high-end железе болельщики супер дома сверхнадежные сервера у которых дублировался абсолютно всем дублировался ёпта система дублером фактически был рейд массивы sram памяти дублировались процессоры позволяют на нету менять процесс в общем на этом можно менять все и выход железо вообще не потрогаешь что такое вообще может случиться располагались полностью что это железо называется андрейка был и основном закладываем закладывались make нибудь другие авто если сервера доступа тоже в то время были лишь при x и можем система была простая и высоконадежной а что из себя представляет вообще ядро торговой системы можно сказать что это такая очень хитрая in-memory дтп из где все транзакции хитрые биржевые транзакции она была написана чистом языке 7 зависимых всех только lip 7 естественно не было никакой динамическая локация система стартовала практически со статическим набором массивным и со статическим рио локации данных чтобы не влиять на время обработки загара все общение с базой данных было только на старте то есть при старте все данные на текущий день загружались память дальше работа была полностью из памяти все данные в системе отсортированы при старте поиске по справочникам по инструмент вообще почему они работают очень эффективные в runtime занимают достаточно мало времени все таблицы тоже с интрузивные так называемые с блесками здесь и провязано тоже с оптимизацией для поисков и первой версии архитектуры была построена на так называемом unix взаимодействие прощение это разделяемая память simm амфоры очереди ну давайте посмотрим что это из себя представляет первых был есть до сих пор два типа так скажем запросов первый запрос это транзакциям если вы хотите купить допустим доллары акции или что-то еще вы отправляете транзак свою торговую систему которая может быть ответ удалось вам это сделать нужно не удалось второй тип запросов это информационные запросы вы хотите на 1 знать какая текущая цена посмотреть книгу заявок посмотрите индексы и за все это отвечает как раз информационные запросы итак итак первая версия фактически два звена сервер доступа и центральная сервер торговая система клиент отправляет запрос он попадает в очередь в unix очередь он переправляется на процесс битвы и процесс битрейт переправляет транзакцию неизменном виде на торговый узел торговый узел отправляется в центральное звено котов дальше это торговая дальше торговая торговый engine практически обрабатывать транзакцию изменяет локальная память и отправлять транзакцию на репликацию фактически это полностью реплика ционно я модель где где твои повторяют полностью те же действия что и происходит на торговой системе где твои получают ответ от транзакции отсылает клиенту и по механизма репликации транзакций приходит где твой и также она исполняется на этом гитаре ну давайте рассмотрим упрощенную схему всю торговой части вернем квадратик и что у нас творится сбит вы фактически центральная часть вот это вот процесса битве приходит максимальная нагрузка поэтому первая доработку которую мы сделали это из ключ избавились от 1 процессу битвы и не заменили его на множество компонентов появился так называемый живой dispatch который разделяет все все очереди на очереди информационных запросов и на очереди транзакции и они уже обрабатываются независима друг от друга появился процесс так называемых живой репликатор который повторяет все действия торговой системы и отображает у себя в локальной памяти и когда происходит информационные запросы они отправляются в интро так называемые информационные битвы и которые работают на their поймите понятно что нужно здесь блокировка потому что не может процесс репликации и процесс получения рацион их данных работа над на одном и том же массиве данных и фактически эта архитектура она существовала достаточно долго вплоть до 2010 года тем временем перспектива по риск платформу уже была неочевидной и мы стали заменять все сервера на с hpx на linux потребовалось некоторое время на портирование но это было достаточно не сложно и основная проблема которая стала как заменить высоконадежный сервер на центральном узле и здесь нам помогла система горячего резервирования я делал доклад про эту систему на предыдущем hail audi но в двух словах можно сказать что у нас есть центральный узел это узел main она реплицирует все данные на a backup ные узлы они исполняют одни и те же транзакции ответ измеряются достигается консенсус и тем самым достигается надежность всей системы примерно же в это время приходит такое явление как high-frequency трейдинг и она меняется значительно меняет профиль нагрузки на ядро системы давайте рассмотрим что же из себя представляет high frequency трейдинг у нас есть небольшая транзакция который выжил вызвало значительное изменение цены ну допустим кто-то там купил там пол миллиарда долларов например и спустя пару миллисекунд все это замечают и начинают давать коррекцию известно это выстраивается в огромную очередь которую систему долго еще и разгребает в чем здесь особенность вообще на этом интервале в 50 миллисекунд если посчитать средняя скорость где-то 16 тысяч транзакции секунду но если уменьшить окном д 20 миллисекунд это у нас будет уже 90 тысяч транзакций если посмотреть на самый пик то пиковая скорость будет достигать свыше 200 двухсот тысячи транзакций и тем самым нагрузка на система такая спорадическая туринский всплеск проникая затей шаг там снова резкий всплеск и все ожидают что вот это вот очередь будет обработан достаточно быстро все как она возникает есть куча пользователей которые заметили изменение цены и каждый отправляет соответствующий транзакцию эти транзакции прилетаю таки твоем битва exe реализуют сдают некий порядок отправляют в сеть всем ешьте затар это же перемешивают пакеты тоже сдают сеть и когда все эти транзакции приходит центральное звено у них появляется некий порядок практически кто выиграл гонку то есть данном случае транзак составил играл поэтому все стали замечать что если они будут ставить не просто один раз допустим одну и ту же транзакций с нескольких битвах то шансы у них выигрываются и фактически нагрузка только multiply цитируется то есть каждый трейдер хочет каждый робот хочет это выиграть он увеличивает нагрузку на каждый твой в итоге все мы теплицы руется и получается огромная лавина вот этих вот транзакций что здесь важно во-первых нам важно соблюсти равноправие потому что если мы будем запускать в архитектуре что кто-то нарушает порядок и выигрывается соответственно это никому не понравится нужно строгое соблюдение очереди сифона обработки то есть мы не можем переставить представлять транзакции тут требовалось несколько решений первое решение которое мы пришли это перешли на real-time систему мы выбрали red hat enterprise linux дергается как практически to miss a thing real-time grid мы используем real-time расширение и все преимущества в этих real-time патчах они оптимизирует под максимальная real-time исполнении то есть все процессов там устроится в азии по очереди можно изолировать и ядра и на этой картине как раз характерник характерно это очень видно то есть красным квадратиком этом работа с очередью в обычном ядре и зелененькая это работа were all to me dry а то есть здесь нет никаких выбросов все транзакция обрабатывается очень последовательно но работа slow lighten7 на обычном кому эти железо тоже недостаточно так очевидно вообще этому тему посвятить отдельный доклад как подготовить систему из обычного серого или работал вода it low latency но я выделил самые основные моменты во первых что сильно мешает это так называемая сми я system management and rape в x86 это вся работа с скажем важный периферии там работа вентиляторов там какое-то событие перегрев датчиков я не обрабатывается в германию и обрабатываются не так называемым прозрачными сми и режимы где операционной системы вообще не видит что делает firmware есть утилиты для как бы определение вот этой вот с меня и задержки вот как правило все вендоры крупные вендоры допустим hp предлагает специальное расширение д-р firmware сервере серверов где можно отключать там модифицировать чтобы уменьшить это вот змея обработку естественно не должно быть никаких не turbo boost вов нидан динамического управления процессором так как это приводит в соответ дополнительному простую также наличие журнала файловой системе тоже наихудшим образом влияет на lighten7 потому что ты сброшу налом может порождать некие процессы в ядре и это приводит к непредсказуемым иногда задержкам естественно надо обращать на такие вещи как тепло tiny tim ты раб тоже не чему все это подготовить и тогда можно как бы достичь уже вот эти вот красивых графиков по обработке чего мы достигли практически с переходом с по риск серверов на x86 мы значит фактически не было изменение кода только адаптация настроили систему и увеличили практически три раза и время обработки было около там типичное время обработки около 60 микросекунд одна из задач с которой мы начали заниматься в первую очередь этом организация сетевого взаимодействия так называемой подсистема you она представляла собой множество процессов как бы здесь не была проблема называется 10к на множество соединений но и множество процессов но влияло на дополнительный контекст свечи и это наихудшим образом влияло на jitters на задержку поэтому всю вот эту вот кухню мы заменили на один поток в котором используется классический пол и это значительно улучшил как и скорость так и время обработки транзакций физически только на одной aio pro системе мы выиграли еще порядка 8 17 микросекунд зависимости от сценария и один поток у нас справляется намного больше чем вся остальная часть поэтому он используется до сих пор в неизменном виде ну стал вопрос как нам идти дальше то есть один процессом джона конечно хорошо но масштабировать уже невозможно как бы тактовые чисто не растет и нужно разбивать и проблема тут в том по как как сделать это разбить ее на самом деле не так то и просто потому что у нас так хитро сделано таким образом была что занимается двумя важными вещами это проверка денег условно говоря и создание самих сделок условно говоря у нас есть кошельки и у нас есть допустим там доллара европу и вопрос тут как разбить если мы просто разобьём допустим по долларом по инструментам то у нас есть один серый которую торгует долларами один серый который евро пунктам и мы отправить 2 транзакции на покупки и долларов и фунтов то у нас возникнет проблема кошельки будет рассинхронизированы система позволит и там и там сделать и нужна синхронизация синхронизации это сложно естественно правильная схема это разбить на несколько слоев отдельно сортировать практически по кошелькам отдельно по инструментам это фактически классиков надо сказать что большинство западных бирж они не используют так называемый онлайн на риск чек то есть у них проверка риск идет в оффлайне и такой острый проблемах как у нас в них нету и в данном случае как это будет работать то есть мы пока весь и покупает 30 долларов запрос направляется в рискую систему там деньги блокируется то есть они помечается текущий момент заблокированы запрос переправляется в торговую систему торговая система удовлетворяет или нелюдь отворяет данная транзакция слушай она удовлетворяет леской система помечает деньги ти разблокировали сетям рубли стали долларами ну как бы очень схематично очень простом языке на самом деле все намного там сложнее ну суть примерно та же признака направляем вторую транзакции она попадает на тот же рисковые джон он уже видит что денег то в общем то нету и отклонять данные транзак либо это идеальный случай но в конкретно в нашем примере на самом деле у нас не два слова не два слоя даже три слоя потому что первый слой это так называемый исторически сложилась эта проверка валидации транзакция нас делается проверка до после данный трейдер для данного режима торгов медленном данное право не заблокирован ли он и это все должно идти как бы да и то по проверке риски то есть в идеале надо разбивать на три слоя и да здесь на данной схеме как бы еще указал что дисковой часть аризона самой интенсивной то есть его нужно выделять очевидно в первую очередь еще проблема в разделении то что текущий код который был в то время он активно как бы использовал один тоже массив данных на этапе валидации на этапе мальчонка то есть как бы разбить на три слоя разорвать здесь достаточно было сложно поэтому стали думать как же преодолеть эту проблему достаточно легко не переписывая вообще абсолютно все 1 очевидная вещь которой можно было бы сделать просто эти две стадии как бы не пролились как бы оставить их как есть отправлять транзакцию в резкую систему дождаться результата и в зависимости от там кошелька отправлять разные рискуете но здесь мы не выиграем практически ничего потому что когда транзакция приоритетно валидацию она замрет будет ожидать ответ резко то есть с ним будет просто бездействовать и тут стал вопрос как же нам этом наилучшим образом разрешить данную проблему еще стают дополнительные вопросы то есть мы выделяем отдельные сервером нам нужно обеспечить отказоустойчивость и естественно нужно писать новые функциональные релиза то что у нас задача основное не делать как бы новая система еще торговать и дополняйте функциональность по торгам и тут мне пришла идея использовать идею как современный процессор обрабатывает инструкции то есть они разбивают ее на мелкие этапы и фактически за один цикл выполняет сразу несколько действий параллельно то есть данном случае мы разбиваем всю операцию на стадии и начинаем их исполнять параллельно естественно требуется никакой адаптация кода но как показала моделирование это сделать намного проще то есть у нас первый этап был если не выделение резко разбить ее на такие этапы и конвейере за создание такого конвейера по обработке вот для простоты рассмотрим вот как это работает в действие то есть у нас есть две системы на конвейерная обработка другая последовательная обработка то есть происходит прилетает транзакция она отправляет на стадию валидации следующий момент времени прилетает 2 транзакция в последовании схеме она просто лежит в очереди в конвейерной обработки она уже попадает на поток конвейером дальше у нас идет риск минченко в обоих системах и как бы выигрыш по скорости первой транзакции у нас нет они в обоих системах практически с помощью одно и то же время но если промотать чуть дальше то конвейерная система уже успевает обработать транзакции 2 и за время пока она обрабатывает транзакт был 2 последовательной системе мы уже выполнили 3 то есть основное преимущество данной конвейерной системы конвейерной обработки то чтобы быстрее выгребаем очередь обработки и как бы на первом моменте мы как раз из-за реализовали эту схему у нас появилась система sds plus но с конвейерами тоже не все так гладко то есть допустим у нас есть какой-то транзакция которая влияет на массивы данных в транзакции кто находится рядом мир для биржи это характерно у нас есть допустим утренние торги дневные торги вечерние торги момент перехода сильно меняется там режиме некоторые параметры и вот такие транзакций окрас смена режима например я нужно изменить какие-то глобальные данные и такая транзакции сито в к двери просто не может быть исполнено то что она может влиять на другие в теории конвейеров такая ситуация называется data hazard и решает очень просто то есть такую транзакцию обрабатывает отдельно то есть мы дожидаемся когда все те транзакции от которой находится в очереди на обработке полностью завершатся дальше запускаем эту отдельную транзакцию дожидаемся когда на полностью завершится и после этого запускаем конвейер снова у нас таких транзакций достаточно мало это меньше процента поэтому на скорость на общую картину вообще это никак не влияет но значительно упрощает код то есть можно делать достаточно хитрые действие по всем осевым данных и не беспокоиться что-то пойдет не так дальше еще один момент как это все таки называется как бы с программной точки зрения то есть у нас есть три стадии практически это три потока исполнения но как и синхронизировать между собой мы стали смотреть и разработали такую систему она основана на колесо вам эфире изредка фактически это кольцо фиксируем размера ячейка фиксированного размера нет копирование данных и села корзину для максимальной скорости есть несколько стадий допустим нас есть стадия локация то есть все сетевые пакет который приходит они попадают на стадии локации мы размещаем их в массиве и помечаем что они доступны для стадии 1 пришла транзакции 2 он узнал доступна для стадия 1 тут поток обработки один заметил что у нас есть доступной транзакции обработал перил на следующую стадию потока обработки 2 обработал 1 транзакцию пометил что данная ячейка уже полностью обработана как бы флаг d-statcom модель этот и доступным для последующего перри использования и так они как бы обработали всю очередь допустим у нас приходит еще одна транзакция 1 пруток замечает что появилось обрабатывает второй поток что обрабатывает и так далее какой какие здесь могут быть проблемам во первых так как обработка каждая стадия на занимает очень мало времени эти десятки от уединиться микросекунд то если использую стандартная схема синхронизации в операционной системой типа мьютекс думаю больше потеряем а самосинхронизации поэтому мы стали использовать спин локи естественно спин локи в real time системе это очень плохой тон и red hat строго-настрого запрещает это делать поэтому практически в нашей схеме мы смело чем ся определенный участок времени это около сотни микросекунд и дальше переходим режим семафоров чтобы уйти от потенциального дятлова в real time системе эта вся конструкция может раскручиваться там достаточно старые займет это около 8 миллионов транзакций секунду современных я уже давно не делал здесь на стали смотреть вообще как в процессе работы нет я что-то такого похожего и когда мы это разрабатывали буквально через 2 месяца нашли похожие статьи практически макс дизраптор делает все примерно та же сам идеология примерно та же самая разница была буквально два месяца как это ложится на нашу схему нас есть транспортный поток который практически находится на столе локации тесно что мы доработали что у нас может на 1 стадии быть несколько потоков исполнения не могут быть по очереди как бы выгребать чтобы усилить допустим какую-то стадию ну дальше как бы все понятно и сесть на обработка всех транзакциям кольце строго по очереди вы думали можно ли переставлять но решили что это очень плохая идея сильно усложняет код и пока сета загс исполняется в том порядке в котором они прожить и изначально с точки зрения код и все очень просто это простые операции практически локация ожидании следующего этапа мы и так далее фактически мы выиграли существенно значительная часть пиковая скорость увеличилась значительно и сделали задел на будущее но нужно было выделять рискую систему и когда мы стали думать что же нам нужно сделать в рисковой системе умы достаточно свободно подошли к дизайну решили все сделать называется правильно постараюсь авичи старые проблемы уйти а дабл где только это было возможно сделали другую графическую модель данных но стало сразу проблема как синхронизировать всю ту бизнес нулю которая уже работала десятилетиями и перетащите новую систему естественно вот этот прототип версии в один далеко не пошел потому что было невозможно все сделать за 1 поэтому то что мы сделали сейчас фактически это один и тот же код который работает in- в торговой части в рисковой части к наполнить шарик большую часть общего legacy ну если она большая проблема которая была на этапе разработки это делать git merge между двумя версиями практически у нас был один разработчик который каждый недели он делал эта операция очень долго ругалась известно когда мы выделяем отдельную систему встает вопрос о взаимодействия нужно выбрать некую шину данных и отличительная особенность в нашем случае это нужно выбрать решение которое обеспечивает стабильный джиттер и минимальную задержек и для таких решений как раз наилучшим образом подходит city in his band то есть если сравнивать время доставки сообщения в сетях и не mini band если тех 10g ethernet как бы по всем показателям он лучше то есть среднем 4 раза лучше но то что реально подкупает то есть на всех тестах которые мы проводили это время в 99 процентах случаев и максимальное время показательно значительно и это сильно повлияло на выбор у данного решения но сын филиппа там тоже не все так гладко во первых это другое прям во вторых нет всех тех что доступны место jing решений какие есть на рынке мы попытались сделать свой прототип но просто чтобы оценить насколько все это сложно это действительно очень непросто поэтому выбрали коммерческое решение но вложили как бэйби я и на будущее и соответственно стали вопрос а как нам правильно разделить эту рискую систему основная проблема здесь если допустим а просто вынесем из канджан заложим наши ну и не будет никакого промежуточного узлам то транзакция двух источников могут перемешиваться сразу скажу что в так называемых ultra low latency решениях есть так называемый ли ordering когда транзакции вот двух источниках могут на приемнике выстраивается в правильный порядок этим обладают ее объем и многие другие жены достаточно хитрые джим но мы решили на этом этапе отказаться по ряду причин во-первых понятное дело это сложно во вторых в других решениях этого не было и в третьих на все но необходимо было каждую транзакцию проставлять соответствующие метками и вот это вот times stamping был очень сложно здесь в этой схеме джона работал как бы везде предсказуемо и одинаково поэтому мы используем классическую схему с между брокером то есть диспетчер которая распределяет уже сообщение между риском и менеджерами вторая проблема которая возникла это займет ленский доступ то есть если мы положим такую же листик тура у нас есть разбит воедино языке ti2 то клиенты уже необходимо конектится к двум битва им кто встает проблема что клиент должен знать что у него теперь уже 2 guide в то есть нужно поменять уже третий слой и мы хотели уйти как бы от этого на данном этапе поэтому в текущей схеме у нас резки твой фактически обрабатывает весь поток данных это сильно ограничивает максимальную пропускную способность но на текущий момент это как бы не является существенным препятствием к эксплуатации заново однако сильно упрощает как бы интеграцию данной системы если нам потребовались добавить так называемый джо и на хранилище хотя это нашим место джинка решение имеет но по нашим тестом было это не очень не удобно использовать в качестве восстановление когда нужно было мигрировать разные дата-центре поэтому мы как бы использовать то что у нас уже имелась изначальной схемой немного доработали и как бы сделали свои места стороны дальше что мы пошли смотреть на схему так как теперь у нас много узлов у нас не должно быть единой точки отказались все системы как минимум должны быть продублированы конечно message broker должен быть продублирован это хорошо решается как раз системой а и б м а не имеет так называемый всем из кластер где два диспетчера могут работать мастер и своим режиме есть регистратор и фактически при выходе на мой строй автоматически переключается на другой следующий этап нужно продублировать все резки и тут встает еще один вопрос если мы подруги продублируем риски мы получим два потока от ветра от рисков системы от 1 или 2 и что с ними делать особенно что нужно делать если они отличаются каким-либо причинам там допустим ошибка в коде один ответил так 2 ответил так и первая мысль это добавить ники процесс шейкер тогда стоит такая же проблема что этот шейкер нужно также продублировать ириш и это добавляет еще один сетевой ход что является для нас не очень хорошие вещи поэтому мы решили всю эту логику запихнуть в торговую систему то есть торговая система принимает множество ответов делают у себя сверку и отправляет далее как бы конечно с архитектурной точки зрения не очень здорово но тем самым мы выигрываем один сетевых а как бы значительно меньшая время обработки вторая проблема с иными не бэндом то есть на 2 уже наверно иная проблема с этими б нам которая появилась практически никто не band оптимизирован для работы в локальной сети то есть ли соединение стоечного оборудования если есть два географически распределенных дата-центр от а1 и п1 сеть уже я как бы дотянешь поэтому пришлось доработать сделать так за вами были диспетчер то есть он соединяется по обычным ethernet сетям к несколько штору и ретранслирует все эти транзакции уже во вторую tony banks этим и когда нужно иммиграцию сода мы уже сможем переключатель типа с каким дата-центром мы реально сейчас работаем еще одна проблема возникла в системе резервирования торговой части у нас есть так называемые главные узлы и резервные узлы теперь мы выделили рискую систему и стоит вопрос о боли узла должны взаимодействовать то есть в чем схема в чем преимущество когда они оба взаимодействия то есть у нас как бы один и тот же код одна и та же схема все хорошо но что будет если вдруг эти два узла отправят две разные транзакции то есть тут встает вопрос что должна делать как бы рисковая система если она сделай что-то не то то как все это дело потом устанавливать у нас получится загрязнения данных во всей системе и это потенциально может порождать достаточно серьезные проблемы поэтому решили исключить этот случай и сделать взаимодействие только на главном узле при этом схемы между так ставить master est in buy i'm получилось несимметричная но зато она гарантирует 11 ту же поток в рискую систему в данном случае это является более предпочтительным доработали систему резервирования теперь дополнительно или транслирует риска вырезал то в a backup на ее мужем и backup на engine как бы дальше работает как и работал на самом деле вот я прошел такой вот круг как мы там пытались делать он был из нескольких итераций чтобы достичь наибольшего компромисса то есть нельзя увеличить сильно увеличит время обработки транзакций в то же время на достичь определенного уровня надежности системе все компания должна быть обязательно продублированы и еще один пункт так у вас новая система мы делали возможность восстановления по двум независимым источником то есть если допустим у нас message скоро покинут причину функционирует неправильно не просто один instance вопрос какое то там ошибка в логике то есть всеми же стары у нас работают неправильно то у нас есть еще один источник откуда мы можем взять блок транзакций то есть на блок транзакций есть на место растворах и но самих рисковых инженер итак у нас все было самой системе еще один момент что мы пытались сделать фактически достигли это сохранение клиентской чтобы не брокеры там никто еще не потребует значительной переделки под новый архитектура что все это было скрыто то есть если там какие-то интерфейса что туго пришлось поменять но значительного изменения модели работа у нас не произошло вот если все это к вам поместить на одну картинку будет примерно следующее изначально мы хотели выделить из торговая система клининговую на самом деле это разрослось вот в такую огромную распределенную систему из диспетчера из риском жена и свинговый бит воев из дополнительного резервного сайта и клиенты теперь уже взаимодействует могут взаимодействовать либо с торговым битва им либо скрины либо с двумя сразу хотя мы сделали прототип такой системой можем то за месяц довести ее до рабочего состояния потребовалось практически более двух лет на проработку всех систем восстановления всех систем дублирования что все это было надежно чтоб прогнать все тесты убедиться что нет никакого регресса убедиться что выход а оборудование строй не влияет на систему в их там свечей выход всего что только можно и естественно долгое время заняла пока мы делали эту систему практически фоне все это время развивался и функционал его тоже необходимо было затягивать в эту систему что же мы получили во-первых мы хотели убедиться что мы не сделали хуже как это бывает когда была один узел стал много узлов и на практически удалось это сделать то есть на небольших скоростях на небольшом потоке транзакции у нас время такой же как интенсивность начинает расти все сильно улучшается опеки еще значительное улучшение то есть если старая система взбейте при максимальных оптимизациях давала 50100 теперь уже дает 180000 и сейчас уже ограничение идет в том что нас практически от только один поток разведения заявок и для дальнейшего как бы улучшение есть два пути пришли два пути которых нужно сделать это вас провели этими чонг изменить схему работы сгибанием сейчас битвы и работают паре аппликационной схеме она практически на таких скоростях перестает нормально функционировать вот какие мы можем советуют выводы для тех кто занимается подобными вещами там в доработке enterprise систему первых к архитектуре вы как вы худшему сигналы готовится на все время она приходит всегда неожиданно и переделать архитектуру быстро как правило не представляется возможным особенно когда нужно достичь максимальной надежности множество показателем естественно чем больше узлов тем больше узлов на поддержку нужно и все специальные решения которые используются в том числе и изгибаем всякие проприетарные решения еще дополнительные ресурсы на исследования на поддержку и сопровождение ну естественно вопросы надежности и восстановление системы отбора таких случаях тоже не надо откладывать на начальном этапе проектирования их учитывать вот у меня все по вопросам насколько по вашему обеспечиваются принцип фифа вот реально все-таки для всех транзакции потому что ну все понимаю что реально все трейдеры они на цепочке их транзакции стоят еще и сначала брокерские системы потом есть еще разные лэтэн си до вас поэтому ну как бы честность и вот в этом фев он на самом деле не очень много вот у вас такой принцип формирования фифа происходит вы из-под time stamp с какой-то уровнем точности устраиваете нуб тифона сформируется по приходу фактически такого естественно приходим на что на нашу систему ну смести на наших торговый сна одно ядро 1 процессор одного потока фактически на сетевую карту главного узла естественно мы дорабатываем все промежуточные можете завтра в нашей сети есть мы не можем знать что производит в брокерской системе как они там все это оптимизирует но мы стараемся максимально обеспечить вот это основание черепа в нашей системе и в нашему соответственно зоне ответственности всегда будет одна точка входа и других способов это сделать у нас же есть множество героев естественно как бы и тогда придется синхронизировать потому что даже на уровне синхронизации времени это уже для высокочастотной торговли слишком маленькое время разрешения когда речь идёт про микросекунды то достаточно синхронизацию времени уже не хватает до хотим более сейфами виртуализацию всем остальным здесь конечно у нас нет никаких виртуализации снова критической критическом пути все там был там все аппаратные железо максимальные настройки ожидал бы не было ничего переупорядочить естественно такая проблема есть оно есть не только у нас как бы зарубежные 1 с тем же парятся были проблемы есть волокна есть попытка высчитывать клайн платности для того чтобы вот этот промежуточный слой сделать почестнее вы в таком направлении не думаете вопрос как бы вариантов много думать думаем разных направлениях но пока как ну пока используем как вот по времени прихода на наши узел да спасибо за доклад а можно вернуться на ту схему где много стрелочек вот это значит параллельно понимаю что каждый раз когда значит вот у нас стрелка проникает сквозь зелененькую полоску ингибин дауна случается здесь и реализация процесса который что-то отправляет и потом носа все реализацию с одной стороны где связаться с другом и это две разные машины до 2 разных сколько времени на это тереть и у нас там были замеры то есть хобби по инциденту в типичных условиях это 10 12 микросекунд при этом при этом утилизации не к некой нет чтобы просто памятник айкидо эти вот ну это бред вообще rdma арктическим хорошо то есть фактически получается нет но и соответственно каким принципам вот этот диспетчер сделан из печи она практически разгибает входящую очередь ставят служебные штампы и перебрасывает в другой тупик и поэтому соответственно прадипике из парашют бессмысленно потому что никого не там нет ни таки спасибо спасибо за доклад такой вопрос у вас было значительно меньше взаимодействий на первичной схеме стало гораздо больше а скажите тоска резюмировать и что вам позволил сохранить скорость хотя вывели раза больше взаимодействие какие приемы ну во первых у нас обработка резко стало параллельно то есть если у нас есть поток транзакции набивается входящую очередь это очередь стала разгребаться быстрее то есть если мы посмотрим скорость обработки одной-единственной транзакции когда ничего нет естественно вас время увеличится но на текущий на типичных скоростях потока он стал разгребаться быстрее как бы время за счет этого вы равнялась каждый машина работает быстрее от разрыва то вот этот вот количество взаимодействий оказалось не больше добавленное правильно там больше было вопросов насчет сетевых hop off то есть фактически мы добавили три стива hop но за счет применения fineprint это время сильно уменьшилась от типичных решений за счет этого нам удалось вот эти выиграть вот это вот время за счет как бы параллельно в исполнении рисковая системы спасибо спасибо вам большое за доход я пока делайте я здесь здесь вот сергей скажите а как вы столь сложную архитектуру документируйте то есть воспользуется к фактам типа togaf а или по 100 классическая документация просто классическая документация колбасу поговорка используя не паникуйте в будущем по сейчас ну это вы та та та же такой же там мейнстрим за народ интересуется думаю что то планируем как конкретно мне не известно но спасибо за идеи вам спасибо добрый день генадий коллега мужчины биржу тоже скажите пожалуйста а вот вы в начале показали картинку такую красивую с транзакциями в секунду и демонстрация того что кого-то это по сути 220000 транзакций в секунду пиковой скорости квада и соответственно раздуется очередь насколько она теперь быстрее разогревается то есть сравнение именно вот разгребание очереди крики ну так сходу у меня данных нету но можно как бы использовать тот же график то есть допустим papers interim то есть допустим на 10 тысячах уже видно типичную задержку она значительно меньше то есть конкретный hot wheels самих цифрам сейчас нету но допустим уже на скоростях к 40 тысячам мы не выходим за миллисекунду а раньше было раньше было намного больше окей ладно цифрами будут спасибо друзья есть у кого-то еще вопрос спасибо за доклад а не подскажете вот получается же он стал очень много всяких разных сервисов а может просто сказать какой теперь это все мониторить и отслеживать какие аномалии и то тут все на когда все это сосна подготавливаем мне тренер стал вопрос как все это мой мне мониторить у нас было до этого систему мониторинга которые мы используем для предыдущей версии фактически пришлось ее доработать особенно сложно присутственных не брендом курс не было ни специалистов не средств мониторинга это пришлось также дополнительно дорабатывать понимать какие типичные могут быть отказывали бенди ну и планировать аварии что называется спасибо за так вот такой вопрос я правильно понял что сейчас вы обеспечиваете сохранность данных путем просто по сути дублирование то есть например у вас отпадет бежо который то совершать транзакции таким образом обеспечивается вот продолжение работы бежит как раньше да то есть вы просто полагаетесь на то что у вас если что то умрет и для сгорит будет продублирована это поток данных практически на и фактически то есть какие-то даже физически к этому при теста может быть разное то центра или как дал естественно есть дубли дублируя нотки 1 дата-центра есть дублирование как резервный дата-центра на данной схемы бэкап сайта итак 1 returned оценок роль физически находится в другой локации лучшее заклинание больших проблем уже как бы прыск переключение быть вручную в рамках одного дата-центра допустим если у нас есть какой-то один за риск режимов умрет все это переключиться незаметно не требует срочного вмешательства если диспетчеру black тоже все это автоматически переключится и данные всегда янамари вы их храните логе только на случай наверное чего-то страшного там окей спасибо"
}