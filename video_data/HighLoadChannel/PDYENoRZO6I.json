{
  "video_id": "PDYENoRZO6I",
  "channel": "HighLoadChannel",
  "title": "MADT: Моделируем сетевое взаимодействие клиентов и сервисов / Олег Якушкин",
  "views": 559,
  "duration": 2833,
  "published": "2020-04-14T11:16:38-07:00",
  "text": "добрый день дорогие друзья сегодня я расскажу вам немного о том как мы пытаемся моделировать сеть таким образом чтобы разработчику который при не знает ничего или очень мало можно было все еще разработать свое распределённое приложение и при этом не убить миллион лет на то чтобы изучать как работает маршрутизация сетевые пакеты и тому подобное началось началась история для меня довольно давно много лет назад мы разрабатывали маленькая маленькая мама про шарик в которые бегали и прыгали мы использовали довольно крупные сервис и крупнозернистые такие и несколько вариантов протоколирования того как сервис и взаимодействуют между собой с одной стороны мы смотрели в начале норовит и мфу и мыслишь брокеры а затем мы посмотрели на то как мы можем использовать пир toupper сети для того чтобы делать то же самое в обоих случаях мы сталкивались с крайне интересными проблемами когда мы тестировали вещи локально сообщение которые передавались между нашими сервисами вели себя по производительности как спина верблюда они представляли себя интересную кривую и это кривая никак не было связано с кодом это было видно по профилированию кодов то что на локальной машине все было замечательно идеально и гладко а вот как только сеть становилась распределенные приложения масштабировались появлялось что-то очень странное разработчики они упертые люди и подумали что но это наверное проблема нашей локальной маленькой сети давайте потестируем на продакшне вывели все в продакшн поставили на нашу арендованную кластерную сеть то у нас был снят завесу взял и оказалось внезапно что что на арендованном узле виртуалке что на арендованном физическом узле когда узлов больше чем один в стойке у нас начинается интересные проблемы и типа того что сеть ведет себя непредсказуемо мы видим паттерн того как она себя ведет почему это так происходит мы объяснить не можем но мы можем заложить в нашу программу некоторый алгоритм который позволит с этим бороться некоторый алгоритм который позволит нам понять что окей если у нас латентность между нашими всеми узлами начала падать мы можем предпринять какие-то меры как предпринять такие меры как понять что это вам необходимо и что вы можете с этим бороться до того как вы выйдете в продакшн до того как вы получите первую проблему рода пользователи видят бак он очень странный вы не можете объяснить вы не можете воспроизвести но при этом он стабильно происходит из-за того что кто то в стойке параллельно внезапно начал активно работать либо другой вас же сервис в другом инсцес сен например себя начал вести себя более агрессивно и передавать огромное количество сообщений существует огромное количество компаний которые занимаются микро сервисом уже довольно давно и сегодня мы поговорим с вами о конкретных примерах файлов которые случались в основном в видимом виде когда система была уже в продакшене и была распределена на довольно большое количество узлов конкретно это google netflix у веры facebook все материалы которые мы будем показывать с вами сегодня доступна онлайн это все видео на ютюбе с примерным полутора часовым или часовыми лекциями о том а что же там происходило и все доступно по ссылкам и начнем мы с дата-центров google а как вы все знаете они используют мапри deus при поиске нам поступает поисковый запрос мы раскидываем его по огромному количеству шар дав и дальше собираем все это дело в едино как оказалось топ 1 процент запросов безусловно является по сути дела скоростью ответа нашей системы если у нас этот топ 1 самый медленный их запросов очень долгие то и иногда пользователи будут на него попадаясь огорчаться тому как заработает наш сервис и желать того чтобы что-то изменилось в случае с google решением такого такой проблемы к сожалению было нахождение бага в аппаратных компонентов как вы сможете увидеть из видео но зная имея некоторый инструмент который бы позволил им смоделировать такую ситуацию заранее понять что такая ситуация может быть воспроизведена хотя бы на небольшого skillet скилла продакшене было бы для них безусловно плюсом если вы можете увидеть это на сотни узлов они на миллионе это дает вам конкурентное преимущество более интересные кейсы были uber у них случился традиционный для микро сервисов баг в виде каскады запросов когда приходит дерево с видишь никами и это дерево следишь неким начинать разбирать по одному и проходятся линейно получается такая красивая лесенка мы получаем новые техники и идем вперед и у нас запрос растет по времени просто в путь второй интересный пример это обращение к базе данных через сервис когда на сервис приходит запрос выдайте мне просто список по некоторым пользователям или по доступному пользователю контента сервис делает 100 параллельных запросов базу база очень хорошо и быстро отрабатывает но это мог быть один запрос и это можно было увидеть до того как пользователь бы сделал это сейчас такого рода вещи что то что мы говорили про гугл что то что вы видите сейчас про uber отслеживается по сути дела только трассировкой мы доходим до того что у нас собралось огромное количество лагов мы пишем специальное средство которое позволяет нам этиологии анализировать мы смотрим на эти логий у когда они уже в продакшене а пользователи уже страдают и потенциально уходит от нашего сервиса и кроме трассировки почему-то мы особо не увидели готовых решений другое дело и netflix который столкнулся с более такой железной проблемой на новый год умирает часть серверов часть серверов крутит важные сервисы которые которым привязаны все остальные эти сервисы незаменимы и не имеют не имеется средств обхода их падения более того все unit-тест это проходятся идеально нету никаких проблем в том как работают эти unit тесты и вообще тесты этих сервисов каждого отдельно по отдельности все было стабильно не падает но просто умер узел физически включился и после этого они перешли как вы все наверное знаете какой системе house monkey который может в хаотичном режиме взять да и выключит 50 процентов узлов и сервис должен продолжать работать и все разработчики изначально ориентированы на то что 50 процентов узлов убили я а вы все еще работаете ну или чуть меньше соответственно до визуализацию они там делают с помощью интересной системе wetherall она прекрасна и делают эти милые картинки дальше facebook столкнулся с крайне важно интересной проблемой они сделали кластер по традиции он дерева и внезапно поняли что количество данных растет количество пользователей растет у них очень быстро особенно когда приходят новые регионы и встает задача как отдайте пользователю контент максимально быстро при том что количество данных на купленный у нас все время растет скорость отдачи пользователям данных не изменялось по сути дела все сервисы были с силой которая заставляла их отдавать ответы довольно быстро и этот график грубо говоря если смотреть на него у них был ровный гладкий а вот общение между сервисами количество запросов которые происходят между узлами просто с ростом данных у них росла экспоненциально появление каждого нового региона каждого нового пользователя чуть-чуть дополняла эту картину быстрого роста того сколько запросов надо сделать каждому микро сервису ко многим другим для того чтобы просто собрать информацию на ответ почему им пришлось прийти им пришлось прийти к созданию специальные архитектуры того как они организуют свой кластер для того чтобы общение между узлами в рамках кластера было максимально быстрым чтобы количество копов которые запрос любой должен пройти было предсказуемым соответственно можно ли было увидеть это до того как приложение вышла продакшен и начала показывается скилл да можно было почему они скорее всего готовились к тому что при увеличении количества пользователей которые к ним приходят у них растет не общение между сервисами а время отдачи запроса конечному пользователю что они естественно пошли соответственно нас были монолитные приложения на стамбуле логе мы к ним привыкли у нас был дебаггер профайлер могли тестировать это в нашей среде могли запустить это виртуалка хэл запустить эту себя нам надо перейти к распределенной системе в которой много сервисов которые могут разрабатывать больше чем один человек нам надо чтобы вася мог написать свою бизнес-логику на сишарпе за пять минут при том что сервис который у нас performance critical будет написан артемом иллидари на си плюс плюс за два месяца и мы можем позволить себе такого рода разбиения и нам надо что эти сервиса общались между собой у нас там появится какой-то протокол у нас там появится много логов которые приходят из разных источников известная проблема что все пишут логи по своему а нам надо потом это агрегировать у нас есть необходимость того чтобы наши узлы были готовы к тому что падает не только по то может даже и не упасть того железо может упасть особенно в большой системе и с этим всем нам приходится мириться и разработчикам надо с этим работать у них есть стандартный стек инструментов которому они привыкли любят которому они уже адаптировались за прошедшие там десять лет протоколы то как создать микро сервис как можно реплицировать базу данных что можно либо использовать брокеры либо сделать directory service и сделать при тупи взаимодействие между узлами все это довольно приятные понятные людям вещи однако то что случится системой когда она выйдет в продакшен когда этой системой реально начнут пользоваться предугадать практически невозможно поскольку вы составили свою архитектуру и своих замечательных компонентов вы рассчитали какие-то базовые характеристики которые позволят и запустится и подняться но что случится когда она выйдет в большую дикую сеть в которой происходят всякие чудеса вы заранее предугадать не можете соответственно обладая на руках языком программирования как правило чем нибудь типа система контейнеризации для того чтобы делать свой сайт для того чтобы делать свой первый релиз некоторыми стандартным пониманием о том как работает система передачи сообщений или сокеты или брокер сообщений в том языке с которым работает данный пользователь какой протокол раскиданы между нашими микро сервисами какие внешние api мы используем на этом всем наши разработчики могут создать довольно интересный продукт и когда этот продукт создается они не хотят углубляться особо сильно глубже либо потому что у них нет компетенций либо потому что чтобы дать им компетенции о том как снизу работает сеть нужно будет убит три месяца и они все равно не смогут увидеть большую картину а большая картина заключается в том что роутер и не только падают и железо не только падают но сеть просто может внезапно становится медленнее в сети могут дублироваться пакета с этим надо что-то делать к этому надо как-то адаптироваться можно делать теста у нас есть безусловно необходимо сделать так чтобы наша система была масштабируема чтобы систем поддержала возможность с ростом нагрузки динамических какой рост самой себя как правило может докатиться до того что вам надо сделать так чтобы ваш продукт продавался более чем одной организации перейти к мульти рядности а что происходит в таким а это ставится со всем интересно как ваши базы разбивается что происходит если у одного пользователя пошла активная нагрузка а другие пользователи тоже пытаются воспользоваться системой все у вас работает на одном сервере такие проблемы как забивание конечно например что происходит когда у нас географические регионы должны взаимодействовать между собой там где трафик идет совсем медленно нам приходится использовать различные алгоритма консенсуса для того чтобы гарантировать что вовсе не смог снять свои деньги из банка трижды или пять раз как вносить version ность в наши данные мы используем bass подход или мы выберем какую-нибудь сторону как треугольника что нам из этого хочется что мы можем с этим сделать мы смотрим на сеть просто мы видим что у нас есть . и на всех картинках для разработчиков это будет выглядеть следующим вас есть сервер а сервис б сервис c посередине облака магия не важно что они работают между собой по сети сеть кто-то сделал в лучшем случае мы указали там какие-то адресом и и особо не задумываемся о том что это за адреса они в какой-то сети они там существуют и мы с этим живем вот у нас есть адрес брокер мы туда все радостно пойдем брокер адрес пдн с именем его там расшифруем выберем какой-нибудь из пяти доменных имен который носил мы не хотим больше знать мы не хотим глубже лезть у нас нету ни времени на это не компетенция а менеджер дышит нам спину и говорит что все пора надо в продакшен сейчас это начисто придет голову отрубят и так у нас есть сеть которая может вести себя некорректно в ней есть страшные вещи которых программе знает мало или совсем ничего не бывает еще и сегментация с виртуализацией о которой прыгай место же знает мало или почти ничего есть сразу страшные топологии того как устроен наш кластер под капотом и опять же наш любимец не хочет думать в этих терминах он хочет чтоб точечка с точечкой соединились по адресу и сообщения передалось и желательно всегда с одинаковой скоростью ну иногда там может быть какая-то вариативностью можно попытаться учесть ну никто же не думает о том что вот здесь вот вас на рисунке 50 миллионов узлов и это было в 2017 году и 36 миллионов этой связи что интересно у вас есть регионы на данным например слайде региона здесь цветами обозначены и те цвета которые соединяют два региона это цветные точки если у вас на какой-нибудь цветной точечки 50 процентов вашего трафика радостно летает и на этой точке случайно скорость соединения упала в два раза дублирование пакетов увеличилась два раза вам на самом деле не будет дела до того как там устроена передачи пакетов на низком уровне вам будет дело до того как ваше приложение отреагирует на то что вот это случилось и между регионами у вас началась начались интересные события а вот у нас здесь есть разные точки переходов вот например здесь упал у нас трафик на 50 процентов и что что мы можем сделать дальше как наше приложение отреагирует как можно это смоделировать на уровне наших тестов можем ли мы что-нибудь с этим сделать понятные вещи такие как потери соединения да там cows monkey включили прочитали что нам нужно иметь какой-то кэш по запросам может быть у нас не корректно отрабатывает api хэндлеры поставили закрыли это дело заглушками какие-то компоненты системы могут у нас выключаться возможно специально возможно непреднамеренно мы это можем учесть мы это понимаем сразу же включим эта в архитектуру а вот удушение трафика когда роутер начал работать медленнее не знаем почему более того это может быть не связано с нами особенно если мы в облаке мы в облаке мы на рынке в реки много таких же как мы хороших замечательных нас куда-то кинули мы с кем-то работаем или с чем-то и тут внезапно параллельно с нами человек с нами не связано или организации не связанные с нами забивает внутри этого рака роутер пакетами просто забивает его долга и внезапно наше приложение начинает работать на 50 процентов медленнее пользователя прибор тут баги не вовремя пришёл ответ не пришел ответ пришел не тот ответ наши сервисы начинают не понимает что с ними происходит поскольку к ним приходит запрос и они на них отвечают а потом пользователь в истерике дублируя этот запрос миллион раз пытается получить уже ответ не на тот запрос который послал когда-то вот на свой самый последний бит и и пакетики к нам приходят задержки пакетов все эти прекрасные вещи которые происходят с нами самый банальный способ тестирования которые вам могут предложить давайте возьмем тира форм развернем у себя в облаке клон нашей системы и начнем над ней издеваться нам можем протестировать же на самом деле все как она есть развернуть его клон взять логировать какое-то количество реального пользовательского трафика и промоделировать нашу систему целиком и полностью облаке это будет стоить очень дорого поскольку нам надо будет поднять всю нашу инфраструктуру займет у нас огромное количество времени это будет стоить нам всякий раз много денег это будет не релевантно поскольку грубо говоря управлять роутерами которые у нас в той сети мы будем сравню постольку поскольку данным там выдадут sd на может быть на самом деле мы крутимся на реальном железе и тут есть вопросы что мы сможем сделать если мы в конце там оставляем только house monkey но будет он выключать нам узлы как это нам поможет с трафиком как нам поможет с удушением можем попытаться отлаживать на горячую и радостно централизовано собирает логе встраивать в нашу систему потенциальные уязвимости single point of или который будет давайте собирать нам информацию когда она упадет силою потеряются будет очень грустно можем пытаться делать life tracing и все это очень грустно наша отладка на горячую это печально можем попытаться виртуализировать всю сеть максимально детально взять каждый пакетик который там у нас должен летать и все это смоделировать с максимальной точностью пригласить 10 системных администраторов которые все это знают они построят нам сеть они смогут это смоделировать но это займёт у них три месяца за три месяца наше приложение пройдет 5 итерации изменится настолько сильно что все что они на моделировали уже будет мягко говоря не актуально и есть решение меня нет это прекрасная вещь там даже есть маленькое ответвление которое я вам очень рекомендую называется контейнер нет если вы хотите вот прямо сейчас взять что-то готовое чтобы все летала можно попытаться взять эту вас зайдет я думаю месяцев за 5 у вас получится моделировать сеть силами программистов почитав документацию описал свой контейнер поскольку вам придется изучает уровень эмуляции l2 и имея продвинутый силой имея хороший api вам придется работать на довольно низком уровне это будет занимать вас кучу времени как unit-тест это по сути дела будет несостоятельна и уровень будет для вас слишком низок другой вариант это и venga она тоже очень низкоуровневая она очень напоминает то что вы видите когда работаете с роутерами напрямую если вам нужно там по пакетикам смотреть и это опять же то что понравится системному администратору гораздо больше чем вам разработчику gns3 самые продвинутые из представленных наверно систем она будет бить вас с тем что она поднимает виртуальную машину поверх которые разрешают вам поднимать другие виртуальной машины и уровень работы опять же тоже не зак и ориентирован на системного администратора которые знают отличие свеча роутера и всего того что у нас летает в пакетиках человека который в состоянии в этом посмотреть в состоянии соединить сеть на том уровне который понятно системному администратору там можно использовать даже докер контейнеры начиная с этого года это большой плюс и меламин то все же то что мы хотим мы пошли другим путем мы пошли со стороны разработчиков и попытались сделать такое решение которое бы позволило вам написать докер файл описав там то что вам надо сказать вот у меня есть мне надо ванием пятьдесят пять узлов вот эти узлы я знаю соединены с вот тем а вот те вот узлы соединены с вот этим сделай мне хорошо объединение сеть каким там знаешь вот этот какой-то протокол общения между займем соедини я хочу посмотреть как будут вести себя мои приложения когда будут происходить сетевые события мне не так важен детальный уровень того как у меня летают пакетики посетить или какой там алгоритмы роутинга заложен между ними мне важно именно что произойдет когда сеть станет дублировать пакеты между вот этими двумя точками что станет когда сеть начнет работать два раза медленнее и еще более интересно что станет когда у меня есть мои традиционная кривая того как приложение загружено сзади и у меня есть другая кривая того как приложение по сути дела соседней ствующих моих же инстансов мы его уже приложения ведут себя в это время то есть как они загружают трафик хочу посмотреть как будут вести себя каждое конкретное сервиса в зависимости от того как эти кривые друг на друга наложится соответственно уйти от того чтобы требовать права администратора системного грубо говоря и знания который у них есть уйти к тому что будет просто будет встраиваемый систему и будет желательно интерактивно прийти к динамическим моделям в которых можем зайти и поиграться желательно подключить отладчик нашему приложению несистемно регистратор мы делаем софт мы хотим в докер записать и такой побелить и и прописать туда что мы хотим быть рейс у нас будет работать дальше отладчик мы сможем посмотреть как наше приложение работает изнутри и мы сможем понять а где же было бага если она или как обойти эту проблему с которой мы столкнулись мы хотим ориентироваться на работу с большими сетями в первую очередь думать о том что у нас есть не только наши сервисы но и наши клиенты все больше можете слышать на конференциях как end-user приложение то что на ваших мобильных телефонах то что на ваших конечных девайсах используется как система дублирования хранения как-то где у вас реальные данные системы хранятся и соответственно оттуда в случае чп данные будут вытянута обратно и перейти к масштабу больших моделей в которых у нас может быть много наших клиентов которые показывают нам нагрузку для этого мы взяли очень такой либеральный стак это докер внизу и в икстлан который сейчас у нас в подключения квагга для моделирования различных алгоритмов rolsen говорите себе конфиг для выставления их свойств мы рассмотрели разные алгоритмы того как можно сделать распределение вот этого всего по сети и для того чтобы конечной пользователи могли что-то рассказать нашей системе о работе своего приложения мы взяли самое простое что было это героем куб и это scape для того чтобы это дело визуализировать основная идея раскидали узлы рассказали что на узлах сказали системе сделай мне ротик между ними я хочу играть я хочу тыкать и смотреть как будет она работать какой проблемой с традиционно ну если вы поднимете на одном узле докер образы и будете поднимать их столько сколько получится мы хотели сделать так чтобы можно было сделать эксперимент на минимальном количестве оборудования на одном посте мы можем поднять смелых 200 докер образов соответственно сценарий будет 25 сервисов и у нас будет 170 эмулируются хлеб все документы которые вы увидите в интернете будут рассказывать вам что в принципе если захотеть несколько тысяч контейнеров можно поднять на одном узле оказывается что в случае с докер сети это не так после пятиста начинаются огромные силы деградация мы наблюдали это много раз когда вы делаете эксперимент сеть просто начинает прямо падать по своей производительности когда вы доходите до цифры 500 как мы все знаем cabernet рекомендует 200 это как бы потолок ребята на один узел но и соответственно видимо 200 это хорошая рекомендованная цифра для того чтобы можно было раскидывается нашу систему по разным углам нашли через алгоритм и который позволит взять в икстлан для того чтобы соединить компонента нашей сети и мультилок эпиграф к в марте жених фреймворк для того чтобы придав нашим узлам вес например у нас есть сервис который требует gpu и у нас есть 10 server 10 клиентов которые требуют gpu как мы можем раскидать по нашим ресурсом зная их особенности нам же надо это тоже учитывать современные сервисы это не ваш бабушкин сервис у которого только secu и рамы все что ему надо было да и базу данных один час там есть видеокарта сейчас там есть специальные ускорители там и специальные требования и специальные железо которое может требоваться вашему сервису может быть там есть доступ к кому-то api который вам тоже нужно так или иначе использовать и смотреть как он работает и для того чтобы это делать вам нужно по взвешенному графу по сути дела делать с парте shining вашей сети для мониторинга мы сделали простую систему сообщений в которых вы рассказываете статус вашего вашей системы какой у вас сейчас статус в работе вы все еще думаете или у вас все хорошо мы сделали так что можно отправлять сообщение о том что залог перед вами сейчас и соответственно все очень просто и банально с точки зрения api это давайте посмотрим на маленький пример создаем сеть говорим это будет соответственно сеть форме звезды форме звезды несколько звезд у нас будут связаны между собой и у нас будут узлы которые соединены с одним каким-то большим скоро мы увидим как это выглядит на картинках основная идея в том что мы выдаем каждому из них уникальные говорим что вот у нас есть 10 детей они соединены с одним роутером пожалуйста сделай мне вот из этого сеть дальше у меня есть десять роутеров они соединены в одну единую сеть свяжем не их сделать там протокол какой-нибудь и выдай мне то с чем я смогу взаимодействовать ну и дальше можно тестировать разные интересные вещи ну например у нас есть какая-то целевая пропускная способность и мы хотим экспериментируя с тем как у нас работает дела и смотреть есть у нас все еще наш торгуют по выдаче ответов на запросы или его уже нету или наоборот мы хотим смотреть на то как у нас происходит тоже самое для сервиса типах и т.п. до до этого там был пример с сенсорами для 10-ти годам ля яички эта система которая позволяет вам по сути синхронизировать информацию в распределенном режиме а здесь у нас это те клиенты которые как-то будут реагировать на задержку ну и давайте посмотрим на то как это все выглядит в живую вот у нас есть два узла а не взаимодействуют между собой вот тут вот вот тут вы можете наблюдать pink того как быстро она работает это просто реальный pink данный спинка который приходит на docker образы 1-го из узлов можем зайти на другой и посмотреть его лаги здесь мы выставили rear door на пакеты например и смотрим как хорошо нашим узлам как они себя чувствуют можем вставить все те эксперименты о которых мы говорили частота задержка потеря убийство более интересный сеть маленький цветочек три узла один роутер мы ставим дублируем ее пакет и смотрим как наш всем известный pink реагируют видим как реагирует наш узлы можем увидеть лук можем привязать желание зайти в любой из этих докер образов по ssh и посмотреть если это наше приложение реально и что с ним можно сделать как его можно отладить он другой пример поставим маленькую потерю пакетов малинку небольшую и обратите внимание на то что нам будет писать capitals вот она стала с 0 33 процента color холдинге это у нас соответственно тратилось мы делаем все это вживую до поставили побольше пакет лосс увидели побольше по и котлов все вполне себе доступна для человека который не знает просить ничего кроме того что там есть базовые проблемы задержки битые пакеты дублированные пакета все их можно посмотреть все их можно посмотреть комбинации и увидеть что будет делать наше маленькое приложение типа pink нужно не будем писать сервис сложнее чем pink в данном случае мы смотрим на задержку и при мы выставили довольно большой с рейтом на 300 килобит в секунду нет даже три килобит в секунду у нас начинается время на взаимодействие наших узлов довольно большое если мы его откинем назад у нас все восстанавливается и время становится снова хорошо дальше можем посмотреть на задержку реально между нашими пакетами поставить you какую мы захотим и это тоже отразится на том как наше приложение будет себя работать более того если бы ваш сервис был чуть-чуть сложнее чем банальный пинг-то вы увидели более интересные закономерности связанные сделаем например если у вас есть если на то как быстро ваш сервис должен ответить как вы думаете что случится когда у вас задержка от вашего первого сервиса до вашего 2 сервиса идет в капах дальше пример с когда мне это был первый тестовый пример которые видели на слайде здесь мы смотрим на то как если в случае падение одного узла наша чудесная всем известное замечательные дети начинает себя ли конфигурировать сколько времени это занимает мы взяли один узел сказали ему выключить и дальше вся сеть должна перестроиться и учесть то что если раньше она была полная то вот теперь она изменилась ее надо сибири конфигурировать это занимает некоторое время в научных статьях про когда можно увидеть сколько времени это занимает и в принципе она соответствует потому что мы видим здесь на медленно медленно восстанавливается и восстановится в конце концов естественно можно строить графини на три узла заходить и быстро находить то делает каждый конкретный узел заметьте да здесь нету ничего сложнее роутера и ничего проще ротора здесь тоже нету потому что в мире программиста если там есть что-то кроме роутера он уже не просто программист он уже программист продвинутого уровня а программист нашего уровня он знает только докер-образ и ролла торт какой-то него есть магические свойства что за роутер как работает не так важно важно чтобы вот здесь мы здесь мы сказали стал медленным и дальше мы увидели как наша сеть на это реагирует как будет вести себя наше приложение какие компоненты сети нашей отвалятся представьте себе большой банк у которого есть много филиалов у нас есть связи между ними мы в принципе их представляем даже на некотором по крайней мере уровне абстракции мы знаем как эти узлы раскиданы по нашей территории в худшем случае можем спросить наших регистраторов они расскажут нам и нарисую примерную картинку упрощенную модель мы сможем быстро эту модель построить и узнать что будет если например всю сибирь случайно накроют потери пакетов 50 процентов как быстро и будет ли у нас отвечает наш сервис на запросы по снятию денег или он начнет себя вести некорректно или если у нас будут дублирующиеся пакеты это приведет к тому что ничего не работают безусловно это не решит всех на свете проблем реальные данные от клиентов уникальна и воспроизвести их не всегда может быть реально когда у вас есть огромное количество динамически меняющийся трафик он всякий раз уникален у вас могут быть проблемы связанные с тем что клиенты хаотично падают и вам надо будет в это смотреть и при том что они падают они что-то успевают отправиться что-то не совсем записать все это будет довольно тяжело но по крайней мере мы можем попытаться это протестировать рост между вместе система является большим вопросом и например знаем что есть масса кейсов когда систему выгрузили сделали первый релиз она начала развиваться по мере ее развития и и модернизируют и это является логичным бизнес flow для данной конкретной системы люди понимают что бизнес-модель входе да вот мы будем страдать некоторое время нам придется авральном нанимать кучу разработчиков для переделки нашей системы но если мы заработали наш первые 10 миллионов долларов мы можем дальше пойти вперед и такого рода рост учитывает заранее может быть вашей конкретной задачи не нужно может быть вы можете его допустить и учитывать что нокии мы сделаем как-нибудь максимально быстрый видео продакшн но хорошо ли это все знают наверное что анкл бен пытается продавить специальный этот клятву программиста в которой девять пунктов будет включая то что программист должен стараться сделать максимально хороший код по своей возможности и тестировать всякий раз когда этот код пытается выложить в общий репозитории общий доступ бэн один из создателей и джейл концепции соответственно нужно ли нам это когда нам это будет нужна в какой мере детали поведение клиента в сотовых сетей тоже очень милый когда у нас едет человек на поезде приезжает много точек иногда у него падает connect иногда он наливается иногда у него просто вот так вот по красивым математическим функциям трафик его доступны падает а мы например серым ему видео в виде стрима как наше приложение будет реагировать здесь в том решение которое моим сегодня представляем идти вы можете задать такую кусочно аппроксимации хитовых это будет выглядеть disease and к сожалению не самое быстрое средство на свете и для того чтобы сделать красиво так волну того как теряется скорость надо использовать чуть-чуть другие механизмы и всегда есть основной вопрос а надо ли сразу делать хорошо или можно сэкономить время сделать больше вич вот на этом я благодарю вас за внимание все о чем мы сегодня говорили все примеры которые мы с вами сегодня рассмотреть лежат вот тут и все средства это лежит вот тут все на концерт там забавная лицензия которая просит вас обратиться к нам и рассказать о том что вы хотите пользоваться нашим продуктом и есть безусловно маленькие детских грабельки о которых мы поговорили с вами сегодня простые решаемые а есть большие самое банальное у меня есть пять миллионов клиентов они приходят в мою сервисную систему сколько вам надо железо чтобы это смоделировать вам надо поднять 10000 виртуалок вы серьезно уверены что мне это надо что мне это надо в таком виза у нас может быть плавно езды на котором мы говорили когда нас есть мобильные клиенты и безусловно мы хотим увидеть модели ваших систем конечно нам крайне интересно увидеть это используемое в реальном мире с реальными задачами мы будем очень рада вашему фидбэка и спасибо всем за внимание я буду рад ответить на ваши вопросы а появился звук да теперь мы можем теперь вы можете задаваться покойный вопрос вопрос дается просто вы поднимаете руку это микрофон и вы его задаете прошел для меня александр копирайтер labs исключительно интересный доклад я бы хотел дополнить первое профилирование в сетях реальность сейчас мы сидим здесь разработчики по большей части сетях нет даже десятой доли того инструментария которым привыкли разработчики да и стал никакой динамики почти никакой никакой динамики это первая попытка я приведу в качестве примера историю с facebook как раз когда они сканировали отошлю к оригинальному докладу питера хуса питерхаус фейсбук my майк распайка ищется в в ю-тубе первое видео где собственно говоря фейсбук при скалирование нарвалась на проблем микро спайк of то есть все систему мониторинга говорят что все хорошо при этом на самом деле ничего не хорошо бизнес метрики летят к чертям и решалась эта проблема тем что ребята да работу перед работать между бушует и рисом и интересно как проверялась решение проблем они ломали живую селить такие продукты can wash крайне-крайне своевременным до россии большой учитель с мы будем рады помочь вам всем и мы чувствуем что вот это вот проблема того что мы можем смоделировать реально сеть можем попросить администраторов показать красивую картину того вот у меня есть мы и смоделируем все пакетики разложим это не будет связано с тем как работает наше приложение и вот этот вот разрыв того как мы можем увидеть то что делают они это что делаем мы как разработчики он конечно принципиален и надо в него смотри спасибо за вопрос привет добрый вечер геями или к вискам понизим rush очень очень понравился доклад спасибо вот только у меня два вопроса первый вопрос в этом говорите потом 5 миллионов компьютеров возможно собирать все прочее но если там 5 миллионов сервер одну инстансов сервисов работа это значит этим сервисом занимается там сотни команд для примера я вот одна команда хочу вот своими 10 сервисами поиграться с этой штукой то есть мне либо для этого получается какой то я понял из ваших слов нужно взять всех собраться моделью система но мне интересно вот я хочу взять свои сервисы сказать что вот мой ломается остальное все прекрасно отлично работает подключить их int поинтами каким-то даже к живому там стретчингу кому-то и начать играться на конкретно в своем окружении вот вопрос вот как нибудь такое вот это вот можно сделать чтобы вот он прим работала до такое можно сделать и она будет прям работать работа с грубо говоря в данном случае если мы абстрагируемся от задачи да у нас есть одна большая система и мы маленькая часть можем увидеть эту там с другой стороны как у нас есть наша система который мы делаем и у нас есть внешний api и такого рода декомпозиция безусловно нами учитывается что вы можете сказать у нас есть bridge вот в эти api и мы будем их звать поскольку это чистой воды докер сеть все что вы можете сделать в нормальный докер сети и нормальный linux эти вы можете с этой сетью сделать она абсолютно настоящие в этом плане она как раз сильно отличается от того что вы получаете когда берете инструмент моделирования сети который берет самый низкий уровень и пытается томск 0 пойти вверх то есть по сути дела это работа с внешним api или работа с внешней потерю который вы делаете запрос и от себя и смотрите зависимости уже от того например как быстро приходит ответ и что вы делать с этим ответом ставя роутер между to them and поэтом которому вы делаете запрос и вот этой вот под сетью которой вы моделируете и второй момент который хотел бы уточнить это когда мы говорим о пяти миллионах клиентов или 5 миллионах сервисов в первую очередь можем задуматься о том как у нас будут вести себя на шею эти девайсы ножи все больше там на каждом по чистая одеты часы у кого-нибудь еще какой-нибудь браслет для того чтобы мерить его производительность побегу по приседаниям и еще какой-нибудь интернет вещей у вас дома пылесос вас умный ездит и всем им нужно как-то с кем-то взаимодействовать и они еще и слабые на самом деле слабые клиенты и их мир тоже стоит осветить в их мире там есть много вот этих вот маленьких передач много маленьких под сетей они все как-то взаимодействуют между собой их жизнь ничем не слаще чем наша жизнь на уровне микро сервис и тут как раз вопрос о том что 5 миллионов клиентов это довольно важно не только в мире сервисном но и когда мы уходим на уровень взаимодействия либо edge клиентов либо того когда мы делаем сервер с архитектурой пытаемся работать на этом уровне и второй вопрос когда спасибо за ответ на первый вопрос то есть как нагрузку давай то есть получается я пишу там какой-то свой скрипт на индекс танкан же метре всем прочим и он идет и второе интеграция с тем что он там не знаю кибер нэйт сам там схем чар там допустим что то у меня уже есть уже готовы stretching я не хочу там заново все так он конфигурация переписывать я их об оно такое натравила на меня сразу поднялось и сразу бы балансируется масштабируется все это отражается и мы в этой системе сейчас у нас есть хороший tutorial только про то как вам работы стокер кампус но проектов концертный и мы будем рады вашим предложениям и вложениям безусловно по поводу генерации реального трафика с клиентов вы его либо пишите либо делаете синтетику тут к сожалению деваться некуда если у вас нету записанного трафика вы не сможете реальную получить все детали того как клиенты себя вели другое дело что вы не всегда можете его записать или легально использовать и эта проблема номер два что вам нужна такая синтетика которая будет похоже на правду но здесь в основном вопрос не столько в том как наша сеть реагирует на запросы извне а как когда запросы за ней пришел наша сеть взаимодействует внутри себя поэтому это безусловно абсолютно справедливое замечание и это то над чем нам всем предстоит поработать чтобы могли отлаживать наши большие распределенной системы иначе мы с теста янык станка могу натравите покажет и советами скажу вот здесь ломайся и посмотрю как вот мои тоже заготовленные рабочие стресс-тесты поведут в данном случае верно ну попробуйте спасибо мы будем рады седоков планы есть на все что вы сможете предложить нам и на все что вы будете готовы нам помочь сделать мы не самая большая носите команды мы рады предложением сотрудничеству и открыт ко всему меня зовут колпаков алексей федин банк спасибо за доклад очень интересно вот вы показывали ранее как собственно пользоваться инструментом там был такой красивенький web-интерфейс видно было эти узлы все значит можно было двигать трогать а что касается какого-то какой-то автоматизации если какой то и пять чтобы мы могли построить seo и там как-то настроить заранее есть что-то в этом плане презентации с 500 мегабайт ными гифками к сожалению чуть-чуть подтормаживает но про 7 и то есть два вида того как вы работаете с этой системой первый вариант это когда вы занимаетесь разработкой вашего по вам необходим инструмент отладки и возможности вот сейчас в моменте посмотреть я поставил эксперимент она сделала тото и второе это безусловно включение вся и pipeline и про это у нас был слайд но к сожалению у меня пока не очень получается отмотать достаточно сильно назад а собственно славится примером того как вы можете описать довольно примитивный тест дальше включить его в свой вся и поймаем и смотреть как ваш сервис реагируют ну собственно вот пример дамы например выставляем наш некоторые таргет по скорости ответы и дальше смотрим с выставляем некоторых таргет скорости ответы и дальше смотрим на то что если мы параметр дела и какой-то выставляем как будут вести наши сервисы себя безусловно вам потребуется для того чтобы внедрить это ваше и pipeline некоторые фидбэк от вашего сервиса здесь соответственно хиндли нужно на питоне и описать давление проблем которые будут возникать до в данном случае именно это безусловно вы можете записать набор действий который вы производили с экспериментом просто в виде набора запросов и дальше эти запросы пачками кидайте смотреть вот я построил сеть я произвел некоторые ручное моделировании у меня было по ходу этого n 1 запросов и прогони мне это еще раз на той системе который я вот обновил и выпуску красиво здравствуйте спасибо за доклад у меня вот такой вопрос а то есть вы смотрели именно система эмуляции сетей а может быть вы смотрели также или может быть еще не смотрели тогда почему системы имитационного моделирования самих сетей которые как раз бы позволили произвольное количество клиентов ими ну покрайней мере гораздо лучше масштабируется чем докер которой 200 там хостов а мы можем имитационного смоделировать хотите 5 миллионов это почти также полезно как если мы пригласим системного администратора смоделировать нам это на уровне пакетов не нет почему я говорю про те как раз которые абстрактные очень сильно мы спад такого уровня вот сервер реестре какой-нибудь смотрите там были милой системы которые подменяют просто сетевой стек и вместо сити вас так и ставит свою библиотеку которая по сути дела кэширует все результаты всего взаимодействия и ну пропускает 3 класс то туда сюда мы смотрели на примеры того когда моделируется сеть математическими методами грубо говоря когда математики пытаются смоделировать 100 как наша сеть будет себя вести а мы уже смотрим на результаты но это не нереально и тестирование приложений это не то что может сделать ваши разработчик более или менее за обозримый срок и вот эта проблема миллион на самом деле наверное может быть спасибо за доклад меня зовут проха янков кирилла код безопасности такой вопрос что если есть некие сервисы которые работают под винду часть на linux часть на винде страдать отличный вопрос когда вы предложите минимальный патч который по сути дела включит в себя исключительно подключение либо виртуалок либо узлов с windows самка созданные развернутые linux эти к нашему чудесному исходному коду вы сразу же получите возможность этим пользоваться потому что по сути дела индусу восстановится and point девайсом к которому подключено вот это наша виртуальная сеть посередине который роутер зашли в этот роутер то есть вас будет картинка выглядит чуть иначе до вас будет сеть сеть сеть дальше роутер он уходит свои связи куда-то никуда и в неизвестность а именно windows и вот этот роутер вы сможете мучить сколько вам влезет windows будет чувствовать всю ту боль которая на него на его лица и у вас будет вполне себе полноценный но придется чуть чуть чуть чуть допилить напильником нормально спасибо есть над чем подумать мы будем очень рады по чем проведут время к вопросам прошлого консоль и теперь олег у меня для вас задания вы получили несколько вопросов и по традиции мы награждаем человека самым лучшим вопросам ваша задача выбрать этот вопрос мне очень понравились вопросы они были крайне содержательные и хорошо дополняли доклад но приз мы отдадим первому нашему за давшему вопрос но проект про windows да про windows это тоже очень справедливо так ну и соответственно подачи книжка и там то там два этого можно поделить напополам всегда не ваша а теперь олег да я тоже согласен с тем что это важный вопрос windows это честная жизнь а маг кстати тоже маг на mac ответ такой же как на windows даже проще а linux linux уже есть докер образы всех как раз на ну и теперь собственно награда ваша за то что вы пришли и прочитали свою коллекцию свой материал спасибо большое спасибо"
}