{
  "video_id": "XiJVC9nzAW4",
  "channel": "HighLoadChannel",
  "title": "Свой распределённый S3 на базе MinIO — практический опыт наступания на грабли / Алексей Плетнёв",
  "views": 17018,
  "duration": 2334,
  "published": "2023-04-28T06:21:29-07:00",
  "text": "Привет Меня зовут Алексей Плетнев и я ведущий программист компании Базис центр где работы уже 15 лет собственно я тот человек который исследует новые технологии решений и принимает решение о том Будут ли использовать наши команды эти технологии или нет И конечно чтобы рекомендовать какую-то технологию нужно для начала самому в ней разобраться энное количество времени назад нам потребовалось хранилище для наших файлов хранилище распределенные с поддержкой S3 Давайте расскажу немного о компании Сначала мы делаем САПР и erp для мебельных производств Обычно когда речь заходит о значимости весь и со сварной компании на рынке считают количество пользователей которые у нее есть насколько она на слуху у нас все проще Я вам могу стопроцентной вероятностью сказать что больше половины мебели которые есть в этом здании которые есть у вас дома сделаны в нашем софте собственно мы используем классический подход в архитектуре отказа устойчивых распределенных сервисов У нас есть несколько дата-центров и есть в России СНГ мире клиенты которые обращаются к нашим сервисам расположенным в этих дата-центрах собственно есть какие-то методы данные оперативные данные которые хранятся в базе данных и в последнее время для наших сервисов потребовалось хранить тяжелые данные в нашем случае это трехмерные модели и цены Которые из этих моделей составлены Для этого нам понадобилось хранилище собственно мы начали изучать Какие же есть варианты хранения файлов которые Нам требуются и все хранилища делятся на два типа облачные либо локальные которые можно развернуть у себя собственно для нас критичным были во-первых скорость отклика сразу отпали импортные облачные сервисы второе это стоимость хранения сразу отпали облачные сервисы отечественные третье ТО насколько решение отказа устойчиво легко поддерживаемые здесь я думаю всем понятно почему цепь для него нужно целый штат держать чтобы его нормально поддерживать стоимость владения здесь отпало решение не знаю слышали про него нет мы его исследовали называется скалити дальше Вы поймете Почему собственно осталось на выбор два минимум и мы остановились на минимум дальше расскажу почему собственно этот слайд сделал специально совсем недавно чтоб ценники были актуальными здесь указана стоимость хранения одного гигабайта данных при таких водных суммарный объем хранения у нас составляет 100 ТБ объем исходящего трафика в месяц составляет также 100 ТБ количество Гетто запросов по 10 млн тип хранилище холодное в случае если это облака и обычный SAS диски есть локально срок эксплуатации 5 лет я сделал эти водные для того чтобы можно было наглядно сравнить Вот вы видите цены здесь видно что минимум выигрывает и чем больше у вас объем хранения тем соответственно ваш выигрыш становится больше правда есть небольшой но еще один слайд здесь мы видим что стоимость хранения в меню немного выросла Произошло это из-за того что я сюда включил платную поддержку от вендора В некоторых случаях без нее не обойтись и Давайте посмотрим в каких собственно Почему мы остановились на меню во-первых минимум Open Source но и бесплатный что Для нас это означает это означает что не надо собирать никакие внутренние совещания не надо подписывать индей не надо ждать каких-то коммерческих предложений и прочей муки вот это вот бюрократической можно сразу диплом и вперед тестировать второе меню по заявлению разработчиков быстрый это скриншот из документации с тестами здесь мы видим 32 сервера на который суммарно установлено 256 дисков и указано ниже производительность на чтении запись нам этой производительности для нашей задач хватает вполне следующая архитектура умения в распределенная то есть позволяет использовать множество различных нот в том числе распределенных по разным стойкам либо по разным датам это решение легко масштабируется оно поставляется в различных вариантах в контейнерах без контейнеров как угодно то есть расширять можно еще одним важным для нас важной фичей стало возможность сжатия файлов на лету что позволяет более компактно хранить файлы использовать больше дискового пространства которые у нас есть собственно минимум меню заявлено что скорость сжатие достигает одного гигабайта в секунду на лету на одном ядре еще одним примечательным моментом является то что разработчик этого сжатия Клаус пост один из тех кто делает сам меню Ну естественно в наше время не последний не на последнем месте стоит то насколько проект популярен На гитхабе сколько у него звездочек насколько интенсивно у него Ведется разработка вот буквально свежий скрин из пульса на гитхабе для минимум все что было до этого это данные из документации данные из тестов от самого вендора дальше я вам расскажу про те проблемы с которыми столкнулись мы и для начала мы столкнулись с проблемами внедрения первое документация документация сделана таким образом что чтобы понять какой-то параметр что он значит как его настраивать Как тюнить нужно ее перечитать не один раз может быть она специально так написано Я не знаю я дальше вам поясню на примерах что же нам там было непонятно более того когда я готовил этот доклад Я даже когда я готовил этот доклад имея довольно большой опыт эксплуатации внедрения меню Я все равно понимал что у меня есть белые пятна и приходилось лезть в их гитхаб и задавать вопросы Итак первое С чего надо начать изучение меню что нужно понять это и rager код Это технология Стара как мир разработана в 60-х годах и базируется на кодах Рида Соломона то есть минимум Любой объект разбивает на блоке блоки данных и блоки чётности которые используются для восстановления в случае повреждения данных с виду все просто но есть нюансы Вот такая вот табличка у нас приведена где написано ец-двоеточин формула меню И что же здесь означают параметры EC это группа дисков размером от 4 до 16 То есть все диски которые у нас есть в кластеры разбиваются на максимально возможную группу так чтобы число делилось нацело на размер группы М В данном случае у нас всего дисков на всех нодах и N это диски на которые будет писаться информация необходимая для восстановления То есть те самые блоки чётности здесь надо понимать следующий моменты во-первых N количество информации для восстановления не может превышать половины всех дисков второе это число не может быть больше восьми логично потому что максимальный размер группы 16 если мы теряем половину всех дисков нашим кластере то кластер переходит в режим redonly если нам каким-то образом удается потерять на один диск меньше чем половина то кластер должен продолжать работать в режиме в рай мы этого не должны заметить собственно у меню вернемся к документации есть вот такой вот пример абсолютно небольшая инсталляция 16 серверов по 200 дисков на каждый и здесь в документации написано что можно потерять 4 сервера или суммарно 800 дисков согласитесь Далеко не каждый день у нас происходит такая ситуация что мы теряем разом 800 дисков но я вам скажу по другому исследуя меню Я пришел к выводу что нам достаточно потерять 9 дисков из этих сколько там получается 3200 для того чтобы уже потерять информацию как такое может произойти вот формула у нас предыдущего слайда согласно которой число блоков восстановления не может быть больше 8 но по умолчанию из коробки минимум не гарантирует что все восемь блоков с информацией для восстановления и один диск данными не окажутся на одной ноге таким образом летит у вас raid-контроллер вы теряете все и хорошо Если у вас рейд контроллер какой-то свежий вы его быстренько заменили и информация восстановится Если вы эти диски 9 штук 9 повторюсь потеряете безвозвратно Все вы эту информацию уже никак не Каким образом не Восстановите как же этого избежать менее запускается вот таким способом эта строка для запуска обычно из документации здесь указано что 16 серверов по 200 дисков собственно вот тот самый слайд непонятно как сделать так чтобы у нас 8 дисков с данными 8 дисков с информацией для восстановления все оказались на разных нодах собственно для этого эту строку надо модифицировать и строка будет выглядеть вот таким образом я отсюда даже если заметили убрал доты экзамен полкнет для того чтобы хоть как-то вписать эту строку в ширину то есть вот таких вот записей будет 200 в строке запуска для одного кластера и только тогда мы сможем гарантировать что все будет нормально Я задумался но Неужели такой Костыль Как как этого можно избежать обратился к разработчикам и здесь еще один небольшой отступление то Почему Я показывал слайд с платной подпиской сообщество на самом деле у меня большое но все оно сконцентрировано исключительно на гитаре самого минимума и в основном все сводится к тому что все задают вопросы разработчикам разработчики по мере поступления этих запросов пытаются их как-то разгрести кому-то ответить но стоит Вам копнуть чуть глубже и попытаться сделать какой-то большой продакшн кластер они просто какую-то тестовую среду и все ответы разработ сводится к тому что давай денег плати за подписку и мы с тобой вместе будем разбираться как же что сделать больше ответ Если будут какие-то баги конечно их поправят Если будут вопросы касательно эксплуатации никто на них отвечать не будет но зато у них есть на сайте Вот такая замечательная табличка то есть мы разобрались с тем как хранятся блоки и эта табличка позволяет оценить на то сколько эффективно Вы будете использовать место на дисках в зависимости от того нужно ли вам больше отказа устойчивость или больше места для хранения Вы можете получить определенный отказоустойчивость Кроме того уровень отказа устойчивости Вы можете определить для разных файлов есть стандартная есть пониженная единственный момент это Необходимо определить еще на запуске кластеры переопределить в процессе работы кластера вот эти классы хранения уже невозможно и вот знаменитая формула по физике которую все знают для меня выглядит вот так здесь я для себя вывел простое правило чтобы в ней разобраться N это число которое обозначает то количество дисков в кластере в группе которая Мы можем потерять собственно дальше мы переходим к тестовому стенду меню везде для теста оперируют тем самым простым кластером где у нас 16 серверов 200 нот согласитесь на таком классе не каждый разворачивать тестовые среды у меня ушло пару дней на подбор того на какой же конфигурации минимально можно стартануть менее для того чтобы он был действительно распределенным и действительно отказа устойчивым есть вот такие системные требования к оборудованию которые на котором необходимо запускать ноды но нам в принципе хватило вот такого то есть для старта распределенного кластера хватает трех нот с вот таким вот железом у нас горячее хранилище на nvme холодное хранилище на обычных HD и первым делом после запуска Мы решили нагрузить одну из нот чтобы посмотреть как распределяется нагрузка Как видно из этого графика вся нагрузка по расчётам блоков ложится на одну ноду А на другие она просто перетекает то же самое абсолютно касается и памяти Мы видим что при нагрузке ну здесь писалось где-то со скоростью 800 может мегабит в секунду э занимается память одной Давайте поговорим теперь про масштабирование понятно там был в начале слайд красивый в котором были докеры контейнеры все остальное и вычислительные ресурсы минимум скалить вообще не проблема но так как мы говорим о хранилке то в первую очередь для нас важны не вычислительные ресурсы все-таки о место и как же у нас скалить само хранилище собственно меню Никаким образом не позволяет увеличить размер текущего кластера если мы у нас заканчивается место что мы должны сделать мы должны взять создать новый кластер и присоединить его к первому мало того информация со старого кластера в новой перетекать не будет то есть новые данные будут литься на новый кластер до тех пор пока его заполнили не достигнет заполненности первого когда они сравняются данные будут Ну у каждого кластера соответственно своя отказоустойчивость вылетает один кластер все данные которые на нем есть Вы теряете то есть это не единый кластер это набор кластеров документация рекомендует при планировании развертывания рассчитывать место таким образом чтобы вам его хватило хотя бы на два года до того как вам придется этот самый крастер расширять еще один такой жирный На мой взгляд минус при обновлении меню при расширении необходимо перезапускать все узлы сразу то есть даунтай мы вам не избежать никак Тут прям Четко написано что нельзя перезапускать ноду за ноды если у вас нот Как в этом примере 32 представляете насколько ваш кластер остановится я еще раз повторю она становится не только при расширении но еще и при обновлении собственно далее Мы перешли к выбору архитектуры есть два варианта распределенные либо репликация плюс распределенной архитектуры во-первых больший объем то есть при том же самом количестве дисков мы можем получить гораздо больше доступного объема в нашем случае мы получаем 66 процентов от общего объема дисков в случае репликации мы получаем только 50 процентов не нужно включать версионирование для того чтобы в S3 работала репликация в меню Да в любом из 3 чтобы работала репликации нужно версионирование чтобы у вас один и тот же объект не имел при одной и той же версии разного наполнения соответственно версионирование занимает еще место Дальше можно развернуть узлы в большем количестве локаций в идеале меню рекомендуют разворачивать как минимум на четырех нодах в случае репликации можно развернуть только на двух то есть два кластера я документации не нашел чтобы можно было развернуть три или более кластеров территориально распределенных но Я подозреваю что в меню это будет такая боль которую никому не захочется переносить в случае с определенной архитектуры объект доступен сразу после загрузки я его кидаю на любую ноду кластера и мне 200 возвращается только в том случае когда объект полностью везде загружен я могу его с Любой ноты читать для нас Это принципиально этот пункт Я смотрел чейндж-логи в последних версиях стал не актуален но на момент когда мы разворачивали минимум на момент написания этой презентации пользователей политики не растекались в случае репликации по другим кластерам То есть если у меня две реплики на Мастере и на реплике я должен был создавать все пользователи политики вручную сейчас Они вроде как это поправили Но я еще сам руками не пробовал в случае же аппликации выше скорость если каждый кластеров размещен в собственном ДЦ до центре в случае распределенной архитектуры естественно в том числе упираемся в производительность каналов между дата центрами лучше изолированность если один кластер потухнет второй продолжит работать случае распределенной архитектуры Ну у нас сейчас я вам попозже покажу как устроена сеть два дата-центра мы уже потерю не переживем одного переживем Ну и перейдем плавно к проблемам при эксплуатации мы построили вот такую вот территориально распределенную схему 3 дата-центра один У нас в Подмосковье один в Питере и один в Казани здесь приведены некоторые технические параметры так это устроено в этой схеме мы смогли добиться того чтобы вылет одного дата-центра никак не влиял на работоспособность нашего кластера и вот здесь при эксплуатации поперли Что называется критические ошибки вы видите скрин того что вот уместилось на экране из гитхаба вот эти все ишу и все мои их несколько страниц это только то что вылезло в первый год эксплуатации первое Вот который 14 224 после очередного управления У меня просто испарились все пользователи политики я об этом отписался сказали Ну да есть такое поправим Я на тот момент еще не имел должного опыта и поэтому всех пользователей политики никуда в Экселе себе не записывал нам пришлось по-быстрому пробежаться по всем сервисам посмотреть где Чего настроено и все это дело восстановить буквально по памяти а вторая критическая ошибка неверно работала кэширование причем оно в одной версии работало после обновления стала работать таким образом что объекты читаются но писаться не пишутся когда я об этом отписался почесали репу сказали ой оно вообще в такой конфигурации работать не должно мы это отключим я решил эту проблему по-своему в каждом дата-центре у меня стоит минимум перед меню стоит Engine все настроен кэш когда объект набирает определенную популярность он попадает в кэш и затем из конечно раздаётся клиентам это позволило в том числе случайно построить цдм то есть один раз я закидываю Файлик в свою S3 хранилку и этот Файлик автоматом доступен из разных регионов России сам если он популярен он доступен еще и сказать джинсы плюс это позволило нивелировать Ту самую проблему с даунтаймом меню в тот момент когда мини у меня перезапускается все популярные файлы как лежали в конечно так и лежат и раздаёт их инжинкс может кому такая схема пригодится собственно следующий критическая ошибка та самая крутая фича которая нам понравилась сжатие файлов на лету у нас после очередного обновления все сжатые на лету файлы побились просто перестали читаться Я обратился соответственно создал тикет ребята посмотрели посмотрели сказали Ну да наверное побились надо как-то в будущих версиях пофиксить Я спросил что мне делать с текущими файлами мне сказали у тебя платные подписки Нет не до тебя благо я догадался вытащил несколько дисков ставил обратно запустился процесс э-э восстановления и этот процесс восстановления смог мне восстановить эти сжатые файлы это повезло что называется дальше переходим к этому самому восстановлению после сбоев естественно когда ты стоишь отказу устойчивую систему тебе это отказоустойчивый отказу устойчивость необходимо протестировать я подумал все данные у нас первым делом мы туда залили всякие архивы какие-то вложения еще чего-то то что не так страшно прям потерять Я подумал надо из какого-нибудь сервера извлечь диск ставить обратно и посмотреть что будет но один сервер в Казани я определил он один диск потерял без меня Я отправил туда в дату центр диск на замену пока он ехал прошло три дня его там ребята вставили начался процесс восстановления так вот вот так выглядит в консоли процесс восстановления то есть минимум не может мне Конкретно сказать сколько будет займет процесс восстановления и в данном случае заполненность была всего 20 процентов А у меня там 5 тбные диски то есть на каждом диске было записано 1,2 ТБ всего таким образом если заполнить диск полностью то процесс восстановления займет я не знаю как здесь прогнозировать Но если верить этой картинке то 6 месяцев И здесь На первый план выходит та же самая проблема с рейдом пятым Кто знает как работает Raid 5 он работает примерно по той же схеме У нас есть грубо говоря 5 дисков горячий с которыми всё работает один резервный если диск какой-то вылетает то за счет математических операций восстановление можно восстановить данные которые на нём были но при этом нагрузка на остальные диски возрастает настолько что за время восстановления резко возрастает вероятность потерять ещё один диск здесь ситуация абсолютно такая же а еще такая проблема Я вначале показывал что если мы потеряем половину дисков кластер перейдет в регион ли режим перейти там перейдет но он начинает настолько медленно работать что если реально отключить от системы половины дисков даже web-консоль не загружается можно управлять всем кластером исключительно из командной строки плюс умение отсутствует Как таковой мониторинг дисков когда Я обратился с этой проблемой опять же к ребятам разработчикам сказал что вот так вот так они мне сначала посоветовали некоторые параметры для тюнинга я их применил ничего не изменилось они говорят ну ты понимаешь у нас устроена система таким образом чтобы восстановление никак не мешало пользователю Я говорю Хорошо тормозит почему вот начинает у меня один диск просто лагать они говорят Ну у тебя должна быть такая система настроена мониторинга которая не просто определяет Что у тебя с диском проблемы Но отключает его если ты отключаешь диск тогда меню это переваривает и работает нормально если один диск тормозит его не отключаешь тормозит весь кластер заплати и Мы тебе посоветуем хорошую систему мониторинга вот собственно у нас пока устроено Все проще пока кластер небольшой у нас Прометей графана в случае если какие-то проблемы графанов телеграмм шлет сообщения и мы знаем Какой диск надо отключить Ну и пока я готовил этот доклад мне его нагарков это мой куратор Спасибо ему посоветовал использовать zfs Я в документации по меню посмотрел они рекомендуют использовать минимум вообще без всяких прослоек никаких рейдов ничего только вот чуть ли не голые диски он сказал Попробуй за FS Не пожалеешь и действительно после того как я объединил диски на серверах ZF с пулы Моя переписка в телеграме сменила казанским закончилась плюс еще один большой Профит который мы из этого смогли извлечь заключается в том что теперь расширять диски можно без остановки кластера потому что минимум адекватно То есть он не может расширить сам пространство может присоединить новое Но если вставить ему диск большего размера или онлайн расширить диск он это без проблем видит и пережевывает собственно поэтому Мы перешли на zfs это раз и второе для того чтобы мы допустили большую ошибку мы начали строить холодное хранилище Мы решили как холодное хранилище мы лучше будем брать большие диски маленькое количество больших дисков в данном случае Мы набрали 5 ТБ дисков но Выяснилось что лучше вместо одного пяти терабайтного взять пять по одному терабайту все это будет работать гораздо более стабильно поэтому мы отказались от использования дисков серверах и перешли к использованию дисковых полок собственно я вот не просто так Питер прилетел Я позавчера в дата центре новую полку установил ещё и теперь наращивание объема заключается в том что я привожу новую дисковую полку втыкаю подсоединяю делаю расширение пулов zfs мини у меня это все видит и переваривает таким образом на себя для себя Например я на несколько лет смог отсрочить вот этот момент когда мне придется запустить второй кластер и там на 15-20 минут положить целиком и на этом скриншоте я его Случайно у себя нашел Я как раз отскринил тот момент когда я заменил диск и пошел процесс восстановления Как вы видите на всех дисках места Ну цифру может плохо видно но видно что она отличается от остальных и по заполненности диска я могу понять во-первых сколько у меня реально места используется от моего кластера во-вторых когда эта цифра выровняется с остальными я буду точно знать что процесс восстановления завершён такой лайфхак получился собственно что в итоге использовать не использовать миню для себя мы его Конечно продолжим использовать потому что мы по наступали на грабли Мы как-то с ними разобрались но в то же время минимум несмотря на огромное количество звезд на гитхабе огромное количество форков для продакшна еще очень-очень сырое решение с которым надо повозиться многие Я знаю используют его но многие используют его например в пределах одной серверной стойки даже сами разработчики меня не рекомендуют конечно его использовать в режиме территориально распределенного хранилища Но с другой стороны тогда это просто обычная S3 хранилка безо всякого профита какой от нее смысл мы смогли использовать в геодефицированном режиме гео распределенном но при этом каждому обновлению относимся прям с опаской смотрим все эти логики смотрим изменения и в принципе работать не можно но надо быть очень осторожным Спасибо да сейчас человек с микрофоном подойдет спасибо вопросы сейчас я передам свой микрофон Алексей спасибо если бы вам сейчас предложили машину времени вернуться года на три назад вы бы что выбрали все-таки меню честно я об этом думал сидел и в какой-то момент меня вот эти вот какие-то глюки настолько раздражали что я думаю выберу для себя время и сяду потестирую Open и если можно еще вопрос Вы сказали что в этом сей Пинк до клиентов для вас критически важен Да А вы не думали про реализацию реализация направления клиента в ближайший к нему гео ДНС или вариант есть есть собственно мы Для этого используем три дата-центра потому что у нас Да я хотел все-таки переспросить Вы обещали сказать про Open you сравнить все-таки Сравните не Сравните их не обещал Я сказал что мы начали выбирать и выбрали меню то есть глубоко Я настолько не погружался чтобы обол панарел сказать что-то хорошее что-то плохое тогда все-таки другой вопрос можете все-таки рассказать как в итоге выглядела архитектура хранения с учетом Я думаю проще будет если вы после моего доклада подойдете я вам на доске разрисую маркером так будет наглядно здесь просто рисовать негде Алексей Здрасте Спасибо за за доклад Скажите а у вас наверняка в вашей структуре меню используется Ну какие-то баки то вы резервно копируете что вы используете для этого не резервное копирование в минимум мы не делаем мы для резервного копирования используем этот ВИМ и мы его используем просто на диске здесь у нас хранятся именно данные от наших сервисов какие-то почтовые вложения архивы но архивы не бэкапы спасибо а так мы видим используем да Алексей спасибо можете немного прояснить про zfs то есть плюс-минус конфигурацию которую вы используете в zfs и Как изменилась ваша Ну собственно настройка самого минимума под за дефис То есть например вам же уже получается от минимум не нужно сжатие на лету нужно то есть ztfs у нас представляет из себя Rate 0 мы грубо говоря у нас есть дисковые полки в дисковой полке 25 дисков мы их делим на группы по 5 дисков каждой группы из пяти дисков представляет из себя рейд 0 соответственно когда мы добавляем новую полку мы опять делим на группы из пяти дисков и увеличиваем каждую группу на размер этих дисков при этом сжать из этого мы не используем мы используем сжатие от минимум Но есть же проблема в этом случае если вы не используете сжатие то ему Нечего считать контрольные суммы которые есть смысл в том что нам отказа устойчивость мы используем лишь как программный Raid 0 а по скорости сравнивали есть разница вообще не сравнивали для нас скорость сжатия как таковая не важна для нас важно именно вот эта занятое место С какой скоростью он будет жать не принципиально потому что жмем мы не оперативной данные все оперативные данные мы их не жмем всё равно у вас получается одного за диска из Пула получается что теряется весь пул дисков условно нужно пересинхронизировать весь путь Да если синхронизировать весь пол но из-за того что пулов пять А мы можем потерять все пять То есть если в нашей схеме с тремя дата-центрами я сейчас до неё долистаю по факту по факту Мы можем потерять у нас 15 пулов из которых мы можем потерять пять поэтому Для нас это не настолько принципиально вот на схема то есть мы здесь вот этот параметр который я показывал ecn подобрали Именно таким образом чтобы улетал целиком дата-центр То есть если даже все пять групп это дату или одна группа в одном одна в другом 1/3 вылетит мы ничего не потеряем мы только потеряем время на Перри синхронизацию Понятно спасибо Спасибо за доклад а сравнивали ли Извините не вижу куда смотреть решение или чем-то подобного цепь Да сравнивали я на слайдах раньше показывал мы цепь сразу исключили по той причине что у нас мини работает в режиме не обслуживаемого кластера то есть мы его один раз настроили и дальше мы в него не лезем в следующий раз мы планируем у него залезть когда его необходимо будет расширять Я когда смотрел Что такое цепь прочитал множество историй в том числе от ребят из Digital lotion который там не буду говорить как бегали по своему дата-центру и пихали новую память в ноды для того чтобы этот цепь вернуть жизни нам у нас просто нет такого количества людей чтобы этим заниматься Я уже куда-то пощелкиваться А вот цепь Здесь был это вообще не вариант плюс он Как вы живете с буквой где агаполь предполагает что любой сервис использующий там что клиент что сервер обязаны распространяется по рецензии Так мы не распространяем никакой со сварное решение мы его используем внутри своего мы его то есть мы меню не продаем в составе своего какого-то программного пакета мы его просто используем для хранения своих данных ребят еще вопросы поднимаем руки да есть вопросы вот здесь Спасибо большое за интересное ревью меню Как вы думаете все-таки С чем было связано то что меню собирался восстанавливать один диск в течение одного месяца то есть что это какая-то неправильный алгоритм или архитектуру неправильная с чем-то связано да как я рассказывал это именно заложено в него разработчики делали менее таким образом чтобы процесс восстановления вообще Никаким образом не влиял на производительность самой системы у них есть некий параметр который отвечает за то сколько файлов в секунду можно восстановить они мне дали значение этого параметра сказали куда нужно залезть где перекомпилировать чтобы применился я это дело сделал Но тем не менее выше добиться скорости восстановления я не смог Несмотря на то что у меня ресурсы процессора памяти были свободны каналы были свободны Я им показываю Вот вот так они говорят Ну вот так то есть никакого ответа на то как это сделать быстрее В итоге я не получил реально он восстанавливался вот один и два терабайта восстановились Через 9 дней еще раз еще один маленький вопрос значит меню это S3 совместимое хранилище А в S3 целая куча метаданных Да всякие теги там вот где меню их хранить я заглядывал меня под капот И вот я сейчас ходил на предыдущий доклад от Одноклассников меня удивило их реализация потому что реализация минимум мне на самом деле понравилось больше минимум на каждой но один на каждом диске хранит всю информацию о структуре своего кластера то есть там есть просто сделано одна папка внутри которой дерево папок так же как оно представлено у тебя в бакетике единственное что они все пустые А сам файл хранит информацию о том Из каких частей его собирать Таким образом у них нет никакой базы данных всё хранится на каждом диске и каждая нода знает обо всем что происходит вокруг это на мой взгляд очень удобно и это исключает лишнюю точку отказа как вот случай например с базой данных так еще вопросы не стесняемся поднимаем руки есть Можно совсем простой вопрос А как удаляются файлы в меню какие-то старые еще что-то вообще в принципе есть процесс у них есть ретеншен полисе которая позволяет удалять файлы в автоматическом режиме либо руками здесь опять же я вернусь к тому докладу который я прям перед своим посмотрел в Одноклассниках меня очень смутило их реализация когда они делают на разных нодах какие-то шеделлеры которые ходят по другим нодам по сети и пытаются разбивают всё это дело на группы и пытаются из каждой группы из каждой ноды лишний файлы вычистить у меня соответственно один процесс на каждой ноде который знает что ему именно с этой ноты надо это удалить Он в другие ноды не лезет поэтому я там хотел задавать вопросы но не успел убежал на свой доклад конечно блоки физически удаляются если включена версионирование то они помечаются если не включена версионирование да то можно удалить сразу и соответственно место освобождается единственное умение у косяк менее находится в перманентном пересчёте занятого места то есть после того как ты обновил кластеры перезапустил его у тебя сектор ноль на барабане и ты сидишь ждёшь пока он тебе реальные данные отобразит о том сколько же места занято где-то вот здесь Было вот здесь на тот момент залило А и кстати даже вот на этом скриншоте Я вот сейчас только косяк заметил видите написано 11 дисков онлайн 0 офлайн А всего в кластере 12 дисков то есть один диск он как кот Шрёдингера не онлайн ни оффлайн пока его сервер Не достанешь не узнаешь Еще вопросы есть рука Подскажите были у вас проблемы именно с программой частью взаимодействия с меню насколько я помню допустим ис-3 Select там не очень хорошо реализован Нет именно проблемы с производительностью когда все нормально не было он мультипард загрузку поддерживает селекцию Что именно Селект метаданные нормально он их выбирает То есть у нас есть наш софт обращается и читает в том числе исключительно методами для того чтобы понимать о том что там у него языка для Явы и для ещё мы Я не знаю может для кого-то будет важным Мы пытались для минимума То есть когда мы только настроили S3 у нас не все сервисы поддерживали работу с S3 нативно и мы решили Давай будем использовать S3 FS их есть несколько реализаций в частности S3 FS родная амазонская которая через В общем она работала не так как надо она работала более медленно мы нашли реализацию называется Гиза фс от Яндекса мы ее начали настраивать через реверс прокси она не поднялась вообще потому что она дорисовывает туда какие-то свои заголовки потом не может с ними работать когда их фракция меняет и она постоянно заполняет память что нам Пришлось написать шреддер который контролирует это дело и перезапускает в противном случае все процессы упираются в этот самый дезофрест но сейчас мы часть избавились То есть когда у тебя батик маленький у тебя там лежит 5 Гб и три файла это одно здесь вот ещё неполное заполненность вы уже видите почти миллион объектов соответственно если натравить без FS на баки с миллионом объектов он просто подавится Поэтому приходится использовать нативно"
}