{
  "video_id": "0ldvCsBDKlU",
  "channel": "HighLoadChannel",
  "title": "Механизмы мониторинга баз данных: взгляд изнутри / Дмитрий Еманов (Firebird Project)",
  "views": 854,
  "duration": 2862,
  "published": "2018-08-16T04:42:09-07:00",
  "text": "начнем тогда всем привет зовут меня дмитрий мо в последние пятнадцать что лет занимаюсь я водка open source субд firebird в том числе занимался разработкой одной из наших под систем мониторинга о чем собственно и хотел поговорить говорить о буду естественно не столько а насколько мониторинге вообще соответственно о чем не будем говорить да понятно по системным tool зам есть маны есть отдельные специфичные конференции выделены твой овен трать мы не будем есть куча других докладчиков которые любят по сравнивать различные мониторинге друг с другом и об этом рассказать да опять же идем на их доклады и слушаем если там у нас очень много на конференции треков по devops am они любят показывать всякие веселые графики как все было хорошо потом резко все стало плохо как а как с этим жить соответственно тоже сейчас на этом заострять внимание не будем о чем хочется поговорить хочется поговорить раскрыть этот вопрос немножко с другой стороны с на стыке системного программирования и эксплуатации то есть у нас есть спрос со стороны дыба со стороны devops of что они хотят мониторить что мы как разработчики можем как системный разработчики можем им предложить какие решения технически возможны какие есть сложности в реализации как есть подводные камни до как с этим жить нам и пользователям какие метрики бывают еще с ними можно делать и немножко по сравниваем совсем немножко посравнивать так как я занимаюсь open source разработка естесно хочешь сравнивать яблоки с яблоками поэтому возьмем то что наиболее известна домой сиквел пост раз и немножко конечно поделюсь системе шишками которые мы сами по набивали какие проблемы были у нас и как мы их решали что надо мониторить понятно 1 часть доклада водное что собственно нужна людям см а на что нужно смотреть до нужно проверять доступность и целостность серверов баз данных нужно смотреть кто и что в данный момент выполняет то кто что делает получать информацию как в количественном так и в качественном виде количество это метрики счетчики производительности в качестве над как раз там какой запрос на сейчас выполняется и тому подобное кто его там запустил об этом я вам расскажу дальше чуть подробнее написала тысячи пиковый предельной ситуации может быть вместо предельных правильный был написать вы выразить как-то по-другому а смысл какой один из способов мониторинга от когда вы задаете некоторое значение все превышения которого вы хотите мониторить да вот здесь его как бы назвал предельным пороговым правильнее было конечно да и второй вид мониторинга на самом деле когда там выстрелила например там порог для время для медленно полнеба запросов то там 10 секунд например у вас на самом деле все они выполняются за секунду но периодически вала и ст-1 которую пальца 8 секунд это пик но вы его своим пределом не ловится вас предел настроен выше так вот иногда интереснее было такие пики тоже отслеживать именно потому что при росте нагрузки они как раз приведут к тормозам в том месте где вы их вроде бы как не ждали где все было гладко и мониторит ошибки в первую очередь но системно это понятно у каждой собаки есть свой лог ошибок куда и навалят все что там с критической происходит вдруг не дай бог там что-то и не понравилось ядру что там случилось базы сервером и так далее любые ошибки мониторит надо это хорошо но иногда и смысла мониторить еще и прикладные ошибки приложений они нигде по штатному как бы режиме никуда в логине пишутся они возвращаются клиенту он их как-то на них реагирует но тем ни менее узнать откуда она вылезла как часто они вообще лезут по каким-то причинами каким как бык чем их можно привязать что их вызвало да я тоже может представлять некоторый интерес буквально пару слов про доступности целостности сильная на этом заострять внимание не буду немножко разговор о нас других вещах но тем не менее слайд и там посвящу до доступность нам интересно понятно физическая сервака если у вас выделенный сервер там дали виртуальной машины также доступность самой су б.д. то есть можно выполнить какой-то сервисный запрос который не обращайтесь ни к одной базе данных на вернет вам состоянии да и доступность самой базы данных казалось бы набегает все мониторить если у нас база отвалилась нам пользователь сразу прибежали заорали и тут все и так понятно что все стало плохо что и у вас недоступно не чудо мониторить имеет смысл в некоторых случаях например простыми какими темпами скриптами с минимальным легкими операциями все равно замерять время установления например соединениях базе на разных опять же уровнях и потом когда там какие-то проблемы возникнут непонятные можно будет разобраться что у вас именно тормозило сетка сервак именно суп это сетевая подсистема не успевает на пакеты вовремя реагировать до либо внутренности самой су б.д. не может нам подключиться к базе данных слишком долго проходит авторизации или что то еще целостность тут только изнутри то есть это только средствами самой су б.д. но куча естественные средств какими-то все обеспечивается куча проверок в ходе всегда есть не дай бог там особо ты что-то где-то не нравится на сразу там валяться с каким типом чеком сакцентировал error а мы так далее и мониторе that of a журнал ошибок это все хорошо но бывает иногда проблема сел счастью который невозможно увидеть фронтами и потом они всплывают позже когда база уже по сути битая один из стандартных способов отловлен я таких проблем это контрольной суммы которая по умолчанию по моим практически во всех субботы выключенных можно включить либо через конфиг либо перед компиляцией самого исходного кода почему их изначально выключит это потому что они немножко немножко но тормозили сейчас на самом деле тот же crc32 банально отличить считается аппаратно поэтому нет особой большой проблемой это все включить но а это все равно не панацея вы таким способом достаточно легко отмониторить проблемы битой памяти например на вас это нифига не спасет от проблемы с научной прошивкой контроллера диска или сбитой памятью именно контроллера например да когда все уже ушло со страничного каши на диск так что как бы хорошо что она есть но толку от этого далеко не всегда есть и на самом деле во многих продуктах есть внутренние средства валидации физической структуры именно физической структуры когда можно пройти по страничкам на низком уровне по указателям и все у нас всю целостность проверить а нет ли у нас там висячих ссылок и так далее из-за чего такое бывает но всякое в жизни бывает двоят рандомная краше непонятное когда не все там успел у нас где-то сохранится как надо бывает баги банально вставьте до баги бывает и май сиквеле вас грей firebird и где угодно еще но проблема в том что физическая любая валидация она занимает во-первых время и самое главное что то как от низкого рационал учиться всю работу с базой что естественно не приемлем поэтому если собой представляет какие средства онлайн валидации пусть даже с временно установочной блокировки на таблицу что там подвесьте на какие-то операции на время например да ну потом освободит это может быть в некоторых случаях оказаться приемлемым не во всех но в некоторых теперь говорим собственно о мониторинге да кто и что делает какие запросу нас выполняются активность неактивность юзеров до псевдо активность и тут назвал почему потому что в таблицах там ваших мониторинга можно видеть на самом деле что-то про сработает потом и смотрите в топ у вас вроде как с одной стороны работающих запросов 100 естественно процессор кушают там только дай бог 10 да и не потому что вас только 10-я допросу потому что там висят они на блокировках висят на ожидание ввода-вывода и так далее правда активность то есть вроде как запрос выполняется на момент ничего не делает это все та же хорошо когда надо видеть естественно когда мы говорим о том что то где то работает что именно это работает ну мы говорим про база данных происходил база данных поэтому это чаще всего это успел запросам хранимой процедурой так далее хорошо но в какой транзакции она была вызвана пожалуйста дайте нам идентификатор этой транзакции отдайте на пожалуйста время старта этой транзакции в каком коннекте это было вызов какой исида если другими словами соответственно кто ее создал опять же в какое время и так далее это все очень интересно и надо знать идентификации клиентов когда мы говорим про connect и сразу возникает вопрос а откуда это законопатили со из какой программа с какого х 100 и так далее и в принципе так как ресурсы у нас что такое физически дает процесс потоки операционной системы в зависимости от рахите курсу бдв подбросьте только процессы в москве лет только потоки firebird и в зависимости от конфига у нас есть как много процесса много-много процессной так и многопоточный вариант архитектуры соответственно конкретный запрос что именно в рамках кого там процесс или потока по его иди выполняйте иногда тоже интересно видеть и ни в коем случае когда вы пишете мониторинг не прячьте никакие фоновые задачи от пользователей понятно есть вот connect и есть допросы все хорошо в любой суп это у нас есть что отложенная запись грязных страниц на диск сброс кэша фоновый сборщик мусора какой-нибудь до репликатор что-нибудь еще так вот это все нужно честно показывать пользователю тоже они тоже выполняет работу они тоже грузят индийской процессор естественно тоже косвенно влияет на производительность и буквально пару слов про идентификацию клиент я уже об этом сказал здесь чуть поподробнее да с какого х 100 у нас идет обращение к базе максимум возможной информации mac-адрес не все лагер уют но в принципе вот можно было бы но хоста ip-адрес это понятно протокол за семей в виду транспортный как минимум его 4 лице пиво 6 иногда очень даже полезная информация если ваша судьба д поддерживает работу по нескольким транспортным протоколом соответственно по какому именно пришел этот connect дальше с какого приложения вот здесь вот уже интереснее средствами там того , кого спокойно получите клинский порт с какого получили коннект это надёжная информация но только не на самом деле немного вам нужно лезть на клиентскую машину запускает на цитаты примерно там прикидывать что это как откуда до намного четче у нас все идентифицируется под нет у клиента по его там и мини-приложения которое запущена там допустят какой там виндовый экзешник будет и так далее но вся эта информация может быть предоставлена только самим клиентом нескольких физических транспортных средств это получить не может соответственно должен добровольно уровне вашего прикладного сетевого протокола это информация отправлять на сервер если он это делает если ваш протокол это поддержит и на самом деле очень круто очень удобно только нужно помнить что эту информацию кто-то всегда можно скомпрометировать подменить исказить и так далее так что верить ей как бы здесь и серии до джентельмена верят на слово то есть если у вас сетка закрытая enterprise наверное все хорошо если у вас connect хрен пойми откуда да и хоть непонятно какими клиентами то тут все может быть и во всех правилам хорошего тона я бы рекомендовал идентифицировать себя дополнительно некой пеговой строкой как т.к. позвал да что-то юзера читаемое и удобное для чего как бы впоследствии намного легче с этим работать чем с именами там программ спидами и так далее и иногда интересно также через что шел connect на прикладном уровне то есть какая версия клинской либо использовалась и какая версия сетевого протокола именно прикладного за 7 with any транспортного была использована это случай там когда у вас там есть новый сервер поддерживает какую-то новую фичу клиентов и обновить не успели они используют старую версию ленска либо на эту всю например не поддерживает до то есть новой версии stick u100 протокола не используется использует старая тоже будет иногда интересно видеть когда движок работает он может выдавать информацию в двух видах да может собирать то что сейчас происходит здесь я назвал статистик менеджер иногда называется статистика коллектор и где-то там за щекой хранить чтобы потом выдает все клиенту дает называется здесь runtime статистика иногда не называет real-time статистикой мозг проверял time на мой взгляд слишком замыленные уже не имеет отношения к тому к правде скажем так поэтому пусть будет равна им статистика далее явно не буду ссылаться как на snapshot и есть у нас менеджер журналирование который штат там пишет какую-то информацию которую мы устроили на диск виде журнал виде логов вот собственно вокруг этих двух видов статистики и мы поведём наш дальнейший разговор то есть snapshot статистика это текущее состояние дел кто что в данный момент выполняет на момент обращения к этой статистики текущие значения всех активных метрик иногда иногда допускается как допускается если разработчики предусмотрели хранить не только текущее значение счетчиков но и какие-то пиковые то есть максимум до и так далее иногда минимум полезно хранить для некоторых метрик и логе журналы да я их издали буду называть история то есть есть насчет есть истории история то что у нас происходило в прошлом и либо завершилась либо не завершилась на текущий момент мы это не знаем мы можем настроить писать всю историю либо ее часть snapshot мониторинг это периодически опрос текущей информации то есть это всегда заданным интервалом от собираем так как информацию мы получаем с каким-то интервал естественно что-то произошло короткое в промежутке между опросами то потеряем это не увидим на обычно с на мониторе расстраиваться таким образом что ну как бы например вряд ли как если бы там был всплеск по производительности там какой то проблема да в течение менее 1 секунды вряд ли от наколот сильно повлиял пропустили мы ну и хрен бы с ним обычно если проблема имеет место то она длится секунд до 10 секунд минуту 10 минут и так далее поэтому слишком уж как бы тут то что вы что то где то пропустим не имеет большого значения но чем чаще мы опрашиваем под систему мониторинга мин snapshot мониторинга тем сильнее мы грузим базу в зависимости от архитектуры конечно об этом я скажу чуть позже какие могут быть здесь нюанс а история понятно нам нужно много логировать если мы слишком много дофига всего сконфигурируем что надо знать то писать мы будем очень много это будет жуткий диск вы вот вывод это будет раздуваться место на диске и вообще анализировать это потом будет практически не возможно но тем ни менее возможность такая есть главное не злоупотреблять и собственно в качестве итога вводной части что у нас типичная такая сферическая субботы вакууме предоставляет да и то есть наш штатный логе ошибок у нас есть какие-то специальные тузы которые идут в коробке которые позволяют какую дополнительно информацию получить у нас есть и pr публичный до который иногда тоже позволяет вы учить какую-то информацию во всех сейчас есть sql интерфейс к мониторингу то есть какие-то виртуальные там системные таблицы в ухе и так далее и иногда бывает что есть возможность какой-то суббота интегрироваться на низком уровне с плагинами с какими-то который уже дальше что-то куда-то будут писать здесь все будет зависеть от того как эти плагины написано они могут выдавать информацию в тот же zabbix они могут писать логина диск тут пардон все на усмотрение авторов плагинов и теперь переходим к метрикам метрики понятно чаще всего это счетчики счетчики ну скажем так в большинстве случаев это 64 битные интеджер храним и метрикой топки текущее состояние счетчиков они чаще всего инкрементироваться то есть монотонно возвращает возрастающее число до хорошей мониторинг это подробный мониторинг поэтому метрик должно быть много не ленитесь добавлять какие-то новой точки мониторинга к себе в софт серверный софт зачастую именно его окажется что потом вам и не хватало do not какой-то банальный пример из жизни когда там человек запускает запрос select по первичному ключу и потом статистике видят что запрос выполнился с идеальным планом unique index can do статистика нам показываешь он из этой таблицы прочитал одну запись а время выполняет две секунды на ненагруженной базе вот что он да какие выводы он из этого должен сделать но кто-то наверняка догадался это было snapshot транзакция долгоиграющая да и что там было 1000 на самом деле версии этой записи которой пришлось просканировать что получить нужную но не имея счетчик грубо говоря скана бэк версии мы эту информацию получить не можем добавили бы от счетчик это все было бы видно а вот такие неочевидной ситуации который казалось бы ну блин это наш внутренний кухня зачем мы будем выносить на публику так вот именно потом публика вам за скажет спасибо стоит стоит делать детальные счетчики лучше перебдеть чем недобдеть но но вот теперь подходим к самому интересному счетчики они бывают разные и не бывает такого одного большого globe новый счетчик а там счетчик прочитанных записей моего как минимум считаем в разрезе сессии клиентской да кто сколько там прочитал записи но детализация может быть разной в нескольких видах да например архитектурно суп это может поддерживать выполнять несколько транзакций в рамках одной сессии и выполнение нескольких и спел запросы в рамках одной транзакции естественно не одновременно последовательно на их можно переключать друг между другом поэтому а почему бы собственно еще не хранить статистику по транзакции вот мы видим sessions что у нас сейчас выполняется транзакция там 123 это все хорошо но перед этим выполняла 124 из этой же секс и сейчас она не активна но она долгоиграющий snapshot по которой хотелось получить информацию блин а когда на стартовала а что она вообще читала в базе ли меняла в базе а что именно а что принять какие-то действия поэтому детализация иногда имеет смысл при этом считать можно на уровне сессии на уровне транзакций на уровне конкретного запроса то есть до один и тот же счетчик получается у нас размножается теоретически можно пойти еще ниже и детализировать это но на самом деле очень глубоко пока не собственно смотрю тормозить то есть понятно что чем больше мы считаем тем светом медленнее работает и вот об этом как раз хочется немножко сказать что вот ну нельзя вот просто нельзя взять его инкрементировать почему вот пожалуйста псевдокод если у нас один-единственный счетчик рак арт-деко us на уровне сессии моего инкрементируем работает все замечательно теперь мы хотим это посчитать в разрезе отдельно коннекта транзакции запросы и так далее мало того что у нас получается четыре инкремента хрен бы с ним не такие большие затраты а то что мы еще каждый раз вытягиваем нужный нам объект через директор антс да то есть косвенных обращений много и это уже немножко не так быстро работает как хотелось бы но еще возникает ситуация когда собственно объект caught может не быть то есть типичный пример например поток фоновой сборки мусора он не выполняется контексте скилл запроса и в принципе в зависимости от архитектура не может даже не быть контекста транзакции и даже теоретически не знаю в каких субботы такой есть какие нет но теоретически тоже сессии может не быть вам там системный сам по себе живет раз про него знает и так далее и вот эти вот все условной про верочке опять же умноженное на 4 до оказывается совсем медленно работают и получается что считать вот эту детализации очень-очень геморойно ну как совсем тормоза и здесь написал совсем тормоза это 10 15 процентов от времени выполнения времени выполнения когда что за кашированные естественно то есть когда у нас грузится только циpкa это до хрена скажу честно для того чтобы поддерживать счетчики это до хрена когда мы это у себя делали мы немножко выкрутились конечно вот таким хитрым способом кто-то msi понимает может быть поймет но я попытаюсь объяснить то есть до в любой момент времени вы казаки утера есть некий контекст выполнения в котором лежат прямые ссылки да там на базу на точна от отчитайся на транзакции на request и отдельно лежат четыре указателя на статистику они сами по себе объекты сами по себе инициализирует все нулями и пустой статистика обратите внимание не нулевым указателем объектом пустой статистики да когда у нас идет изменение контекста аточа например да указатель меняется и мы берем нужную статистику подставляем куда нам нужно если у нас сбрасывается контекста то чем и опять генерим получаем даме статистику даме статистика просто отдельный счетчик в который просто ни кем не используется так вот смысл в том что избежав косвенной адресации многоуровневой и условий то есть тупо всегда инкрементируем счетчик даже если он нам не нужен и без того что проверять на надо ли это делал работает у побыстрее и водка вот этот последний код который по крайней мере с 15 групп грей процентов до полутора процентов по performance импакта привел к итогам казалось бы ну понятно тоже все не дешево не бесплатно с этим хоть как-то можно жить но это все детализация на самом деле это на в таком виде далеко не у всех есть это же скала редко у кого есть более интересная более типичная ситуация это когда счетчики не глобальные а в разрезе там например таблица то есть понятно кому интересно просто число прочитанных записи если неизвестно откуда это читалось хранить этот счетчик в мой таблица там который лежит в метаданных в памяти мы не можем потому что он общий для всех мы не можем посчитать отдельно обращение к это таблица там отдельной сессии так далее соответственно нужно как-то хранить это отдельно привязывать текущей сессии искать по имени там таблицы пойди таблицы и так далее и тут собственно как там возвращаемся к кадру с боромиром да нельзя просто компрометирующих на ты уже начали найти понятно любой программист уже новая и там хэш-таблицу еще чего-нибудь да для быстрого поиска но это уже не простой инкремент это чуть сложнее если это активно выполняющий кодовый путь то это сильно опять же влияет это все временные затраты их надо оценивать оценивать вас от никто не сможет мог вас не вас понятно да сервер сайт разработчиков нас когда-то у нас выдернул я кусок нашего старого кода но когда моя как раз оценивали там разные варианты алгоритмов до зависимости от директив условной компиляции отдавайте вот так попробуем и давайте вот так попробуем вот так вот в качестве ты прежде было выкинуто был выбран один вариант но в том плане что разные алгоритмы сильно по-разному тоже влияет на производительность и не факт что тот алгоритм который вы всегда считали там самом быстром оптимальном там любите вы используете черно красные деревья для быстрого поиска давай его союзы ни хрена не факт что он будет работать самым быстрым способом наверняка возможно что-то будет другое более эффективно и на это надо пробовать и выбирать и заканчивать разговор про метрики монотонно возрастающая чеки от все хорошо кто что делает это все хорошо на самом деле в реальной жизни когда мы смотрим там flame графа те же по реально нагруженным базам очень редко когда все упирается процессор и где сразу видно что вот там вот тормозит мы именно на процессы на самом деле почти всегда проблемы возникают на ожиданиях что мы можем ждать можем дать диска ввода-вывода может ждать сетевого ввода-вывода мы можем ждать каких-то внутренних блокировок в ядре и так далее ждать мы можем их чуть чуть но часто мы можем ждать их и долго вот это все тоже надо отслеживать что нужно смотреть кто именно какой коннектом то либо когда же детально какой запрос стал на блокировку до кого он ждет кто ему мешает по чему он его ждет на чем она может какой объект собственно выполняет к кооперации дома дисковода вывода там папа операции чтение страницы абсолютно рацию записи странице надежда или в ожидании внутренней какой-то блокировки это надо знать очень интересен контекст вызова в таком случае как бы если продукт open source ный то лучше всего вводить туда там имя файла номер строки потому что одно и то же блокировку можно пытаться учить из разных мест коде и какие-то пути будут более проблемной чем другие вот эту информацию хорошо видеть если от продукт закрытый либо просто если вы не хотите вы носить такую внутреннюю кухню на публику ну сделать какие там идентификатор строки или числовые по которым легко потом по коду найти это место да естественно вводить надо в таком виде что было понятно что если там у нас connect a ждет connect б который ждет conax эту проблему что на самом деле проблема в контакте c то есть совсем не упирается и строить графе ожидания на блокировках надо но в выраженном случае естественно это может замкнуться и мы получим до блог потому что в каких-то случаях до блоки живут в ядре и через какое-то mode автоматически обнаруживается отстреливается но на некоторых операций в блоке все еще возможные и просто как бы их пытаются избежать тупо кодированием алгоритмов но в отличие от других метрик намного интереснее здесь время ожидания и вот это на самом деле очень важно но тут возникает свои приколы во-первых мерить время ожидания не бесплатно опять же мы добавляем какие то процент скидок тормозом во вторых как именно правильно мерить у нас естественно всегда что любая точность любого счетчика она конечно и это далеко не всегда микросекунда это может быть и 20 микросекунд это может быть и 50 микросекунд в некоторых выраженных случаях например на винде это могут быть даже и миллисекунды поэтому точность плавает и с этим как бы нам тоже надо как-то жить что мы меряем берем и какой-то замер время якобы ожидании на блокировки понятно что если у нас идет режим оля ну ладно вейд с нулевым тайм-аутом то мы ничего не мере то что мы его любим захватом либо не захватываем сразу же в режим ожидания мы не уходим если же у нас там стоит такой тайм-аут мы запускаем режим ожидания на блокировки меряем до инструменте ruim точки входа и выхода но захватили блокировку мгновенно не было контента на на ней блин так вот если это блокировка лёгенькая там лач какой-нибудь либо вообще там spylog то время ожидания севда такого ожидания на самом деле успешного захвата блокировки может оказаться запросто меньше чем точность вашего инструментария по времени то есть вы ни хрена не намерен по большому счету вы и сами знаете чего намерен да тут иногда можно выкрутиться типа а давайте вначале попробуем захватить блокировку в режиме новый и только если не получилось тогда запустим мониторинг ожиданий попробую взять ведь мы опять же это вам не дает гарантии что если первая попытка не удалась 2 вполне может получиться и в опять померили хрен знает чё так же интереснее когда мы работаем мониторим ожидании натиском воде выводя и работаем при включенном файлом кеша когда у вас якобы идет вымереть обращение к диску на самом деле вы мерить обращение к файлам укажу у вас ожидании нет вообще ядро не переходит в режим ведь в принципе у вас просто тратится циpкa время в режиме ядра но то что данная мерили но это на самом деле нихрена не дисковый вот вывод да это ожидание вот а вы вот это честно но цифра там будет опять же настолько маленький что практически сравнимы с точностью вычисления поэтому под итоге подобьем до метриках много надо делать но трижды подумайте насколько сильно вам надо детализировать не вам естественно вашим пользователям до что для них реально важно что нет если детализировать надо то очень хорошо продумайте внутреннюю алгоритме к как это лучше сделать не накосячить и с реализацией если накладные расходы на подсчет каких-то там счетчиков велики используйте отложенную записи здесь может быть как совсем банальный случай когда у вас там в цикле в одной процедуре инкрементируем счетчик не надо сразу писать в статистику да за видеть локальную переменную на инкрементировать потом в конце по выходу залить сразу не единичным инкрементом а сразу бам бам до либо вы можете считать какие-то элементы статистики на одном объекте и периодически там по scheduler по какому-то по каким-то точкам кода перебрасываете и выше расположенную статистику агрегировать на ходу с задержкой до просто из за того что это быстрее не нужно делать слишком часто но естественно статистику времени выполнения и иногда какую-то детализированную статистику которая мне удалось хорошо оптимизировать а лучше тогда делать опциональный конфиг runtime опции и так далее как выдавать данные snapshot мониторинга вот тут вот вопрос конечно интересный то есть варианты крайности тут 2 1 мы максимально часто если написал регулярно и это не просто так непостоянно пишем в какой то общее хранилище да то есть это там какая-то разделяемая память временный файл не важно писать можем туда достаточно часто но это из чем чаще вы туда пишем тем семьям мы нагружаем и прославим производительность просто нашей работы это может вообще никто не читать а сколько там процентов вас уже просела но зато если вы часто пишете вот это вот поддерживаете старая статистику актуальны достаточно приходилось высокочастотной мониторинг и читать этот регулярно быстро и эффективно и это хорошо но именно почему написал регулярно после постоянно делать это очень медленно то есть нельзя на каждый чих лезть в общую память и пытался чет там поменять да понятно что из него будет читать это возникает необходимость блокировать чтение и запись любой самый легкий лак он не бесплатный да как бы кто-то может сказать ну как же монотонно возрастающей счетчики и тоже берем и интерлок тэн климента и грязно чтение потом все замечательно без конфликта ни хрена он тоже тормозит меньшей степени но тормозит и вторая крайность не считать практически ничего работаем как работали итог когда приходит запрос по мониторингу а давайте мы тогда сбросим точу надо естественно то что мы не считали мы выдать не можем поэтому чаще всего какие-то метрики считать но все равно приходится просто локально у себя там нет за щекой в каши без каких-то блокировщик просто и считаем и все пришел запрос отдали ещё есть какие-то вопли хранилища тот их прочитал мы спокойно работаем дальше в реальной жизни все сидит где-то между вот этими двумя режимами что у нас есть сейчас в суде в массив или основной инструмент snapshot мониторинга это именно performance схема меня сильно вчера удивило на докладе петра зайцева когда он спросил там из 56 человек в зале а кто использует приходит схема руку поднял один или два человека сразу стал сильный интересно как вообще они мониторинг свой сервер там юзает май топай на топ или там в консольном клиент этом шоу команды выполняют дай потом это все парсит непонятно но начиная чуть вниз 55 версии у них есть такой замечательный инструмент который очень жестко все инструменти рует который можно включить выключить правда через конфиг требуется перезапуск сервера очень много счетчиков он ведет очень много метрик очень много полезной информации уводят работает замечательно настройку что именно мы включаем-выключаем можно производить не только и через конфликт но и на лету гран тайме что именно мы хотим мониторить помимо чисто накопительных счетчиков содержит некоторые значения самаре в разных разрезах что тоже очень удобно и позволяет избежать там вам какого-то прикладного агрегирования для более удобного разбора полетов хороший инструмент вполне нормально работает всегда естесно есть нюансы доу вчера я тут узнал что он на самом деле инструменте рует он все очень жестко как пишут люди даже при включении performa схемы и выключение всех абсолютно опции логирования на все равно на 5 процентов подсаживают производительностью раз она все равно что там считает и поддерживает сама чем больше вы активизируете естественно воспроизводились просаживает сильнее она очень агрессивно все это накапливает это ну понятно тут как бы вопрос цены вопроса да но что удивительно что и как я выше сказал да в принципе когда много мы пишем достаточно легко получить snapshot потом быстро возможном сок частотный мониторинг но как говорится и пока ты это не накосячишь то есть вчера по он тоже был слайд когда там версии master 56 запрос xyz вершин с нагрудного нагруженной базе вполне нормально 30 секунд она москве 8 одну секунду до то есть явно был какой-то 56 косяк на отчет который они до пофиксили и собственно все заработал как и должно работать в позы грызть есть архитектурно заложен такой инструмент как статистика коллектор все запросы со не мгновенно ас в некоторые периоды времени передают ему информацию о том что сейчас происходит и все свои счетчики и статистика коллектор опять же не мгновенно отец каким-то интервалами опишет это в разделяемые временные файлы откуда уже потом все забирается учащихся успел запросами через специальные динамические view happy жест от отдельно там поджи logs есть для анализа блокировок как я рассказал обновляется на периодически с небольшой задержкой получается вы информацию берете но это как бы не суть страшно все равно задержка то меньше секунды например да и такой интересный нюанс что-то транзакция которая обращается первый раз к вот этим вот view hamda пиджи стад она на самом деле делает копию всей информации текущей snapshot а мониторинга и каширу там внутри транзакции то есть получается некий snapshot как будто она работает в режиме изоляции snapshot всегда независимо от того какой он на самом деле то есть все последующее обращение к жестов you have в контексте этой транзакции у нас приведут к чтению целостной информации какой она была раньше не на текущий момент да то есть мы теряем на верность но зато получаем consistency когда у нас не будет ни того чтобы там прочитали одну таблицу потом другую пытались их потом связать а там все уже поплыл на потому что информация в них не согласовано вот они такой подход выбрали сознательно то есть это не какой-то мне бак ничего они посчитали что так удобно у нас архитектур чем ты похожа единственное что метрики мы считаем всегда локально а сбрасываем их в обще хранящем в общую память по запросу непосредственно мониторинга этаже у нас snapshot на время жизни транзакции я не буду заострять внимание на том как именно у нас это сделано во первых то что время сильно ограничена во вторых 29 ноября я об этом буду рассказывать на конференции технологий базы данных у нас есть несколько базовых таблиц то есть мы ведем весь почет всю информацию в разрезе отдельно а там база данных коннекта транзакции запроса помимо этого мы трассируем все вложенные вызовы хранимых процедур триггеров и так далее которые текущий запрос вызвал и отдельно 2 группы таблицы 1 это 100 таблицы которые имеют инкрементируем ее метрики до счетчики и юзать таблицы который показывает текущий использовать каких-то ресурсов это память это размер там временных файлов и тому подобное эти две группы таблиц эти две группы счетчиков считаются для каждого из объектов во всех этих таблицах включая весь стек вызовов и соответственно агрегируется снизу вверх то есть всегда у вас там запрос завершился но вся статистика которое этим запросам сгенерил с вас лежит на уровне транзакции транзакции заметила все и счетчики уехали и доступном уровне сессии вы сделали disconnect у вас есть счетчики остается доступна в глобально то есть сколько уж вообще к базе там было обращение так далее по мониторингу поставщик мониторингу все последняя тема которая хотела сказать это логирование до или журналирование понятно все пишут логи с критическими ошибками об этом даже собственно говорить не интересно это понятно и мы мониторим смотрим и так далее что чаще всего хотят увидеть от логирования первое настроить какие-то alert и то есть мониторит например только клиентские ошибки которые были вас kill запросах мониторит только медленные запросы мониторить там только ну неважно еще какие-то предельные ситуации который вызывается из обычного графика и все вместе назвал как alert то есть нештатная ситуация неплохо иногда иметь возможность интерактивной трассировки то есть просто залезть коннектом к базе сказать а вот я хочу начать трассировать вот это вот это вот это такие события таки это вот таких таких таких цельсий ведь основная есть такие права и вот там куда ты 3z перенаправить не важно куда там файл на консоль syslog там не суть важно и немножко в стороне на самом деле именно вопросов мониторинга но тем не менее архитектурная очень близко находится это вопрос аудита то есть когда мы настраиваем логирование но необязательно полная зависимость от того что мы хотим лагерного что постоянное что она переживает рестарт сервера что она ничего никогда не пропускает отвалились мы не отвалились все у нас пишется и протоколировать чаще всего конечно будет использоваться для вопросы безопасности иногда для каких-то разборов полетов после изменения метаданных но в принципе в принципе его можно использовать и для оценки производительности тоже потом для разбора ситуации когда мы мониторим базу известное просто его нужно более щадящий настроить какие инструменты у нас есть масик или понятно есть у нас error log куда суп это пишите свои ошибки есть general квилаг пал на самом деле называется к verilog но пишет он туда далеко не только запросы пишет туда время установления соединений и дисконект а юзеров да какой юзер там с какого айпи пришел и так далее но пишет и запросы при том что значит из запроса какое время он начал выполняться текст запроса время выполнения то есть достаточно подробная информация что у нас там была отдельно у них можно настроить так называемый слоу крыло кота люди использующие масику чаще всего используют да это самое у них а пока известно в этом плане вещь где вы задаете лимит время не выше которого время вполне запроса вам интересно и он пишет только запросы выполняющиеся дольше этого времени почему он у них и теон отдельно потому что настраиваться в конфиге на разные файлы вывода отдельно вы можете писать основной лак и отдельно слову эпилог поэтому как бы идет сам по себе есть достаточно малоизвестная фишка у них это а пиво вадика появился на достаточно давно в ядре но именно как api то есть много . инструменти рования очень низко уровнем вещи он способен выводить но простых выплевывает в кубрике когда-то изначально единственный плагин который мог это все обрабатывайте куда-то там писать был только в enterprise версий мая сиквела поэтому собственный twitch и бы прошла мимо большинства пользователей а сейчас уже есть open source версии плагина в том числе у перк он и так что теоретически это вопрос это фича тоже можно пробовать заюзать при условии что я можно грамотно с кастомизировать то чтобы он не писал слишком много и не тормозил в подгрести в пользуюсь и сама логирование немножко поинтереснее сделано архитектурно более гибко настраивается можно перенаправлять его в разные места есть полный аналог слову поверила года через определенный параметр statement то есть какие именно запросы там дольше какого времени мало героем до можем настраивать хотим описать все запросы или только до дрель например до нас интересует то есть возможности кастомизации небольшие но есть умеет писать не просто долго получить вопросы о блокировке которые висят дольше положенного времени д блоки возникшие если запросто например сортировкой создал временный файл больше там такого-то размера то есть достаточно интересно в реальной жизни ситуации это как раз то что я называла ли ртами очень удобный все это как бы в рамках и dels единой архитектуры логирования есть тоже такой малоизвестный вещь в основном и юзают сами разработчики но ничто не мешает в принципе использовать это интерфейса где trace у то есть есть возможность динамической трассировки встроенная в само ядро казгаса где он ивенте выплевывает тоже вот в плагин оля который потом может обрабатывать рейс и так далее беда в том что не все это инструмент пользуют даже те кто знает linux это раз во вторых нужно позарез перекомпилировать с ключом и не будет рейс но и в третьих понятно что на винде я там все идет лесом то есть это в принципе нет ну понятно что конкурс не особо-то и любят рекомендовать и минут на продакшене нагружена но тем не менее он поддерживается уже более 10 лет поэтому тут есть некоторые такое условное ограничение мы пошли скажем так не хочет говорить своим путем на самом деле нет мы сделали единый api чем-то идея похожа на подрисовываю но реализовали полностью все всю реализацию нет без оглядки на linux white узы то есть тоже есть некий api который выплевывает событие в плагины которые уже там что-то с этим делает режима работы 2 то есть мы этим быстрому убили сразу как двух зайцев мы сделали возможность аудита то есть когда конфликт что именно монада логировать определяется служит конфиг на сервере сервер сам запускается из лакирование управляет ей переживает все это естественно restart пишут все это блок в журнал на диске ротировать переименовывает и так далее и этот же абсолютный инструмент мы используем для режима пользовательские так называемой трассировки это вот как раз близкий аналог того что есть в возрасте при включенной иной балладе трейси только здесь мы используем не linux white ул за свой собственный инструмент можно любыми подключиться к серверу пользователю который имеет соответствующее право передать конфиг что именно мы хотим трассировать и получает дальше события в плагин штатный плагин выводит просто все в текстовый файл плагин можно естественно подменить написав свой но этот вывод просто потом перенаправляется на консоль клиента и уже дальше может не делать что угодно в штатной поставке у нас идет от плагин который пишет и аудит рейса в текст на she's a porter ii кто профессионально поддерживает базы написали свой плагин который пишет это все в базу которые создали нормальный индексы и потом все можно удобно мониторить наша особенность чем до чем похожи на дет рейс то что все событиям а трассируем в виде начало и конец мы можем отследить когда что у нас начал выполняться когда закончилась для каждого запроса получить плана выполнения статистику выполнения притом не только запроса например статистика время они коммита транзакции можем получить да почему потому что по комету транзакции идет сброс грязных буферов иногда может занимать время больше чем рассчитывали это все можно увидеть всю статистику мы туда пишем не просто как бы время выполнения а какая у нас есть все метрики выводятся туда ты сможешь собрать не только время выполненную конкретно там что конкретно запросто написал изменял в базе в каких таблицах есть точно так же как в по сгрызть через определенные параметры можно эмулировать слог verilog есть возможность создавать кастомные фильтры что именно мы хотим трассировать через рига xp include и эксклю 3gp и самое главное что мы везде лакируем внутренне яичники объектов для чего это нужно еще скажу чуть позже буквально вот пару примеров аналог настройки slow кларе logo и анализ настройки логирования до d или например да то есть все достаточно не сильно сложнее чем москве ли при желании можно сделать смоет все ровно в разрезе каждой конкретной базы настраивается и собственно финальный слайд это то что в реальной жизни тесно приходится сочетать вас есть два инструмента ими нужно пользоваться со в мир сна недостаточно мониторить только snapshot недостаточно мониторе только же логе журналы ну понятно что кому-то иногда везет и проблем у него особых пока нет но ты поэтому они как-то сильно ограничивают своих свое мониторинге но вообще-то нужно делать вместе на самом деле чем агрессивнее вы пишете журналы тем вы можете иметь и как бы право реже запрашивать snapshot информацию и наоборот то есть если вы очень мало чего то настроили только alert и например примитивные дай особо ничего больше не trassir уйти то тогда имеет смысл мониторить почаще чтобы возможность смотреть то что было непосредственно перед проблемой а что вызвало вот эту ситуацию до что делали какие-то транзакции перед тем как у нас возникла такая проблема и так далее самое интересное когда эту возможность можно анализ предоставляемой этими двумя независимыми инструментами можно связать друг с другом интегрировать понятно что самый простой способ какой-то snaps эти мониторинга мы видим выполняющий запрос стартовал тогда-то я вот его текст мы можем ту же информацию вытащить например росла у кори logo или жанре logo томас трассировки неважно как инструмент называется да и какие то выводы из этого сделал посмотреть а какие запросы выполнялись этим же там коннектор перед тем как да что там происходило перед тем как чисто механически намного проще привязывается через зайди вот поэтому мы у себя собственно и вынесли у нас есть среди всех объектов и snapshot мониторинга его трассировки таким образом например с плагином который пишет все объекты трассировки все ивента трассировки в базу и получаем информацию из мониторинга потом обычных банальными joy нами между двумя базами данных это все можно достаточно интересно объединять и смотреть что где у нас как происходило но как бы опять же у нас уже просто нет времени об этом рассказывать деталь но времени смотрю в без десяти поэтому наверное если есть какие то вопросы ещё есть время ответить если нет в коридоре пожалуйста меня ловить и с удовольствием поговорим на любую тему"
}