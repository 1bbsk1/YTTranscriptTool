{
  "video_id": "Hs2txKgnpAk",
  "channel": "HighLoadChannel",
  "title": "Дешевле, надежнее, проще / Александр Христофоров (Одноклассники)",
  "views": 2345,
  "duration": 3209,
  "published": "2018-08-16T03:56:15-07:00",
  "text": "а добрый день меня зовут александр стаффорд я работаю одноклассниках и довольно много времени провел занимаясь внедрением разработка разных хранилищ данных сегодня расскажу как мы храним фотографии видео в одноклассниках я начала немножко истории в 2016 году будет 2011 году мы запустили ппс эта система предназначена для хранения блобов то есть бинарных данных и запустили мы ее в первую очередь для фотографий это высоко доступное хранилища легко масштабируемая все очень супер и в нем потом положили в не иную музыку подарочки и сейчас практически все блога которые есть одноклассниках они лежат в различных кластеров и б.с. в 2013 году мы запустили свои видео платформу и положили видео также vbs и тогда у нас было где-то один петабайт данных я по эту систему рассказывал на холоде в одиннадцатом году если кому то интересно можно даже слайда найти немножко расскажу как она работает для того чтобы двигаться дальше это клиент-сервер на сервер honey данные собственно на дисках а в памяти хранит индекс когда клиент приходит запросом на сервер он придает ключ данные информация записывается на диск и в памяти помещается в яндекс информация о том как по ключу найти эти данные на диске все клиент получает окей все зашибись когда мы читаем мы приходим с ключом идем в яндекс смотрим где же эти данные на диске с диска достаем данные отдаем все клиенту читал почку отлично теперь давайте посмотрим как данные распределены в обоих во-первых они распределены по трем дата-центром как и все наши системы по трем потому что у нас было 3 до центра очень долгое время но и потому что мы очень бережем полицейские данные как происходит то что деление данных мы по ключу берем hash hash а отчисляем партицию некоторую и для каждой партиции у нас есть три реплики по одному диску в каждом до оценки где лежит эта partition все очень просто когда мы пишем без нормальной ситуации мы пишем авторе реплики одновременно и собственно если вдруг какой-то сервер улетел мы продолжаем писать но уже только на 2 реплики естественно если две реплики записали данные на то мы считаем что это запись вполне себе надёжное сервер может потом естественно вернуться он немножко будет отставшие да то есть на нем какой-то части данных не будет и в этот момент включается механизм репликации который будет данный догонять остальных реплик и через время она придет в концертное состояние а когда мы читаем мы читаем с одной реплики по умолчанию мы читаем с той же реплики который находится в текущем дата-центре где и клиент но сервер естественно можно сказать будем считать другой реплики там может отказать диск будем считать трети достаточно одной реплики для того чтобы прочитать данные ты сказать внимательный слушатель и задумываются о том что какая-то же реплика была не консистентная да на ней могли быть не все данные так как это фотографии это данные не мутабельные нам не нужно здесь читать некие кворумы ничего мы просто если прочитали с ноды данные и обнаружили что данных на этой ноте нет мы просто делаем повторную попытку читаем с другой реплики да если мы прочитаем таким образом 2 реплику точно гарантированные и найдем все просто если мы говорим про без то это достаточно немаленькие нагрузки вообще в принципе мы наружу отдаем видео и фото более то рабби то естественно мы отдаем с неких серверов раздачи которые имеют свой кэш и но тем не менее на второй же прилетает 250 тысяч запросов в секунду и это где-то 300 50 гигабит трафика оборудование vbs стояла 2017 году более 500 серверов это 4 у сервера если кто то не знает это вот такие здоровые сервера который занимает 4 место в стойке то есть это 2400 обычных 1 юнит их серверов и там всего стояла где-то 18 тысяч дисков начинали мы с 1 терабайт них дисков в шестнадцатом году уже стояли восьмерки в общем много добра все как бы работает и все хорошо и всем все нравится но возникла одна проблема шестнадцатом году это видео это вот саша который рассказывал на предыдущем докладе прав свои стримы в том числе дело в том что по видео платформу одноклассниках растет достаточно хорошими темпами кроме того разрешение видео растет до 4к сейчас можно пожалуйста стремится телефона можно загрузить видео 4k и собственного клайв за последние годы сильно увеличил скорость роста потому что все эти стримы которые люди телефона фигачат по часу-полтора до их тоже дед нужно хранить что вы получили в шестнадцатом году получили прирост объема данных где-то петабайт в месяц вот представьте себе у нас тринадцатом году было всего петабайт а в шестнадцатом году у нас уже было 15 петабайт и один петабайт каждый месяц добавлялся соответственно прогнозы на 17 год у нас должно было быть уже двадцать пять петабайт если посмотреть на сервера то количество жиров росло не так стремительно более линейно это связано с тем что объем дисков тоже росли но тем не менее на 17 год у нас должно было быть уже где-то 800 серверов если мы продолжали такими темпами да это напомнить 4 june твою сервера ну собственно мы подумали что надо с этим делать потому что есть проблемы со стоимостью оборудования до его нужно покупать все больше и больше это уже не маленькие деньги кроме того когда вы его купили его еще нужно поставить в центр потом за него много лет платить да и стоимость размещения стенд она тоже очень очень весомое но самое интересное что у нас начал заканчиваться место в центрах особенно в одном и если мы продолжали бы ничего не делать и так же ставить оборудование да то возможно на сегодняшний день нам просто некуда было бы поставить сервера для того чтобы там развивать свой сервис да и через три часа будет опек а то доклад от олега анастасия про наши облака это тоже большое тело движение которое сейчас происходит одноклассниках 1 для того чтобы сэкономить место дата центрах и стоимость окей ну понятно нужно разрабатывать новую систему да и цели перед ней в общем очень понятный хранить в первую очередь дешевле хранить нужно еще надежнее почему еще надежные вроде бы три реплики и так достаточно надежно но так как количество дисков растет объем дисков растет восстанавливать их все дольше уже не 0 вероятность что мы можем потерять в принципе все три реплики то есть для нас как бы даже три диска это не очень надежно уже поэтому да кроме того мы хотим продолжать обслуживать польские запросы показывать им видео и фоточки даже тогда когда вылетал дата-центр и какой нибудь еще диск или сервер почему потому что всегда есть какой-нибудь вылетевший диск работает от центре или аварии они тоже случаются ну и последний немаловажный фактор это упрощение эксплуатации чем больше серверов и дисков тем чаще они ломаются тем больше телодвижений нужна для того что все это поддерживать в рабочем состоянии ok первое с чего мы начали естественно решили давайте уменьшим избыточность будь то подешевле посмотрели на разные схемы репликации с точки зрения избыточности с точки зрения допустимые потери дисков то сколько можно потерять ну там понятно 1 простой же приходит голову это зеркало до 100 процентов избыточность но потерять можно один диск можно к ссор до сделать положить в 12 центр половину данных и другой другую половину в 3 курсор между ними избыточный 50 процентов очень заманчиво но потерять все равно можно один диск это не наш вариант давайте смотреть на что-то более сложное есть такая штука коды рида-соломона если это не знает очень известный алгоритм он используется система храни данных протоколов врать 6 и суть его в том что данные разделяются на некоторое количество блоков д каждый блок данных пишется на свой диск дальше к этому добавляется некоторое количество ходов коррекции х да и каждый код коррекции тоже сохраняется на свой диск + этой схемы в том что можно потерять любые к дисков будь то диски с данными будто вклады коррекции не важно если вы потеряли не более коту вы можете установить данные очень заманчивым давайте посмотрим теперь как вот эта схема ложится на ситуацию с тремя до центрами да вот у нас есть d блоков данных как котов коррекция очевидно что в 1 дата-центром мы не можем положить больше чем к потому что эти положим больше то при потере дата-центром мы не сможем служить полицейские запросы значит мы положили кого динка в другой и к 3 очевидно что к не может быть соответственно больше одной трети от общего количества соответственно избыточность минимум вся процентов но все равно 50 процентов согласитесь это очень заманчивая штука мы начали смотреть серьезно загорелись но теперь вот посмотрим что же происходит когда нужно установить данные вот у нас есть 10 блоков данными и 5 блоков с кодами коррекции разложили по 2 центром все отлично вот диск вылетает нужно установить что делаем находим пустой диск какой-то и начинаем восстанавливать данные для этого нам нужно де кодировать данные из других дисков со скольки с 10 оки прочитали восстановили записали в принципе можно себе позволить но есть другая проблема приходит клиент говорит дайте мне фоточку фоточка на сбой нам диски для того чтобы мужчина создать эту фоточку нужно опять же сделать те же самые 10 запросов восстановить данные и отдать и все это сделать на лету но дисков одновременно немного вылетает можно себе позволить но давайте посмотрим что будет если откажет это центр треть трафика в этом случае пойдет по той же самой схеме когда клиентский запрос превращается в 5 запросов в 125 запросов другой до центра очевидно что рост нагрузки весьма немаленькие принципе он линейный да если вы хотите иметь возможность потерять 5 дисков то вам нужно иметь 500 процентов запас прочности это трафик и это и обсе ну очевидно что плюс 500 у нас нету и 400 нету и 300 если был бы он сажал вы все экономические выгоды от этого подхода можно было позволить себе наверное 200 процентов запас мощностью но при этом потерять можно только два диска это тоже не наш вариант начали смотреть что-то другое был вариант такой взять коды рида-соломона использую такую схему когда у тебя три блока с данными и три блока с кодами коррекции разложить по трем на центр все неплохо можно потерять в этой схеме 3 диска это означает что можно работать при потере до центра и одного диска в общем нас это вполне устраивает при отказе до до центра вы получите плюс сто пятьдесят процентов дополнительные нагрузки что в общем то же терпимо и избыточность будут уже сто процентов а не 50 но в целом эта схема неплохая но мы сделали еще немножко по-другому сейчас скажу как мы взяли данные разделили их на 10 блоков половину положили в 12 центр половину другой до центр в третье это центр положили к sour sour понятно этого мало поэтому в каждой на центр мы положили еще коды коррекции по 2 штуки в каждый это центр и в 3 положили к ссор между кодами коррекции получилось что у нас было изначально 10 блоков данных мы добавили к ним 11 разных там годов восстановления и избыточность получилось 110 процентов или кодов коррекции мы используем алгоритм его not заставляет очень простая штука она работает с четырьмя словами это могут быть байты мы используем longhi и суть такая 1 под коррекции это просто к ссор между словами со всех в нашем случае 5 дисков и то есть мы так вот четыре раза считаем к ссор окей а второй код коррекция тоже ксор только посчитаны по диагонали на как мы видим это же захватывает все 5 дисков на по одному слову но но по диагонали и так тоже считается четыре раза и мы видим что второй код коррекции он тоже покрывает абсолютно все данные его плюс что он очень простой и быстрее чем коды рида-соломона аки также как из когда метода сломана можно потерять 2 любых дисков будь то диски с данными или коды коллекции и посмотрим как восстановление происходит вот у нас отказал к это диск мы хотим установить данные у нас есть 2 варианта на самом деле мы можем используем кады коррекции в рамках дата-центра прочитать 5 дисков восстановить а можем сделать курсор прочитав данный из других дата-центров очевидно что с точки зрения и abs of на второй вариант до вас полная раза более предпочтительный отлично но что будет если от каждого центр та же самая треть трафика пойдет по схеме с декодированием но в данном случае нужно будет всего лишь делать курсор между данными из других до центров то есть мы получим на каждый клиентский запрос плюс один запрос в центр в один и в другой и это плюс сто процентов по запасу прочности требуют и это сильно лучше чем 200 или еще какой то теперь давай посмотрим что происходит если отказала много дисков вот та же самая ситуация отказал первый диск хотели бы восстановить данные но отказала еще четыре диска как нам прочитать первого диска очевидно нужно сделать к ссор между шестым диском и диском x1 nokia x1 у нас есть 6 у нас нет но 6 мы можем восстановить с помощью кодов коррекции прочитав 5 дисков в этом же дата-центре правильно принципе все легко и это дает нам возможность потерять любые 5 дисков это вот как раз самый худший вариант изображен если мы потеряли бы например е3 дата 6 диск то мы уже не смогли восстановить данные но 5 любых мы можем потерять теперь давайте посмотрим как писать данные при такой вот сложной схеме и кодирования у нас есть один вариант вот у нас есть 10 блоков данных мы взяли фотографию разбили ее на десять частей и записали в каждый блок по маленькому кусочку второй вариант мы взяли фотку и записали ее целиком в один блок ну очевидно что первый вариант с точки зрения нагрузки в десять раз хуже да потому что на каждый запрос нужно записать в 10 мест потом в 10 мест почитать вообще нагрузка на диске будут здесь раз больше поэтому естественно мы выбираем второй вариант тут все хорошо но есть вот такая вот проблема мы взяли фотографию записали ее в начало первого блока например и диск у нас вылетел можем ли мы установить эти данные нет не можем потому что других блоков данными у нас еще нет у вас их нет мне от котов коррекции да и восстановить ничего не получится эта схема предполагает что данные надежно сохранены только тогда когда вы записали все до конца весь этот большой кусок из 21 блока лет проблему ну очевидно ее можно решить простым способом можно изначально писать данные просто в 3 до центра все 10 блоков да и когда все это будет записан до конца пересчитать получится когда коррекции лишние данные которые были в каждом центре выкинуть все надежно но минус этого подхода очевидно сначала нужно все записать потом все прочитать перекодировать и заново записать не очень эффективны и тут мы подумали но у нас же есть уже vbs который умеет хранить надежно давайте у него писать в начально данные он запишет 3 реплики в каждый до центр все будет надежно потом мы запишем из него в новую систему вот съезда и когда будет сегмент запишем целиком и все коды коррекции будут посчитают тогда мы можем смело и зубы удалять данные все очень нормально и мы так и сделали мы не стали ничего менять в части загрузки данных у нас все по-прежнему льется vbs просто через время мы берём данные перемещаем ввс ok такие результаты мы получили ну если сравнить сразу со старой системой то избыточность место 200 процентов стала 110 процентов и теперь мы можем терять 5 дисков в общем все неплохо теперь посмотрим как это все работает это все на джаве как мы любим ни строчки session авокода вот значит мы взяли блоки размером по 4 гигабайта соответственно эти блоки их 21 штука в одном сегменте до сегмент получается 8 4 гигабайта в нем 40 гигабайтов полезных данных и 44 на кодов коррекции данный нужно где-то хранить этим занимается сервера данных это машина 130 шестью дисками в которой как вы думаете если райт естественно нету и она очень простая тупая она делает очень простые операции она умеет записать блок она умеет прочитать данные из блока по какому-то смещению она умеет установить блок и собственно удалить блок все больше ничего не делать внутри есть некоторых фоновый процесс который занимается мониторинга дисков проверкой целостности и собственно восстановлением данных если нужно в других машин стянуть данные установить у себя данные на диске хранятся предельно просто есть начали таблица блоков которой отмечено какой свободной который занятый и мы их можем переиспользовать благодаря этому и потом дальше просто идут блоки один за одним и блок это фиксированная структура фиксирован и которым есть четыре гигабайта данных и некий заголовок ади блока статусу контрольными суммами блок может находиться в разных статусов у него может сейчас идти запись и он может быть пустой с ней он может быть уже записан до конца из него происходит чтение либо он сейчас устанавливается с другим donod либо в нем была найдена какая-то ошибка целостности дисков серверов но вода и нужно много перелопачивать много отдавать наружу поэтому так просто в жаре с этим не поработаешь приходится прибегать с этим трюком там у нас есть замечательно библиотека ванне у которой мы пользуемся для этого и в частности мы используем send file для того чтобы данные отдавать наружу дать файлов socket клиентам и мы используем много всего в хеппи то есть вся обработка вот этих больших данных там восстановления счет сердце и все делается в теперь и для этого тоже есть ванне у замечательной классики и библиотеку опасность хотите смотрите пальцы 100 много чего полезного номинальная нагрузка сейчас на сервера данных полтора гигабит и соответственно при потере до центра 3g бита и благодаря аккуратному программированию на ja и вот мы получаем что у нас user cp один процент при такой нагрузке что java вполне себе не тормозит и уэйд то 17 процентов но эта работа с дисками тут никуда от этого не денешься все отлично но есть одна проблема диски отказывают причем они отказывают разными способами самое простое это когда вы читали или писались диска и поймали его эксепшен ну окей в этом случае понимаете сдохла и помечаете его как нерабочий шут с ним делаете ну бывает более интересной ситуации при на диск может потеряться просто после рестарта машины оказывает что вас уже не 3 6 дисков 35 до это вполне себе нормальной ситуации или например ядро может и linux может раньше чем приложение заметить что dice вдох он у вас просто на лету пропадет но самой опрашивая ситуация когда диски умирают очень медленно перед тем как сдохнут он просто начинает медленно писать или читать и представьте себе у вас машина в которой триста терабайт данных из одного диска у вас начинаются проблемы со всем серверам ну не очень хотелось бы поэтому мы диски пытаемся максимально изолировать друг от друга и первое что мы делаем и используем диск как единицу кластера то есть мы воспринимаем не целый сервер да как часть кластер а именно диск когда мы вводим у ротацию новый диск ему присваивается иди и о plaster как бы заводится диск и когда диск сдох то же самое мы помечаем как дохлый диск но вся машина стальная продолжает работать и все диски остальные обслуживают запросы у каждого диск соответственно если вы мониторинг статистика проверка целостности данных все такое но сам интересно что у каждого диска есть также свой pull it pull который обрабатывает задачи чтения или записи к этому диску и так раздела для того чтобы если dice то начинает тормозить туда и запросы к этому диску они встанут в очередь долго обслуживаться но это никак не скажется на обработку запросов другими диском ok диск из боятся понятно надо как-то менять ее в bios для этого нам нужно было два человека был админ который от монтировал диск но он передавал задачу инженер инженер заменял диск физически втыкал новый потом админу нужно было что сделать создать раздел за монтировать диск и собственно нажать кнопочку установить данные ну очевидно телодвижение много на что то можно сэкономить и как мы видим основная часть работы здесь связаны с файловой системы и мы подумали на зачем она нужна мы это в принципе храним очень простой структуре все и поэтому мы не отказались мы просто получаем список блочных устройств открываем random access файл мдф с dc и пишем туда master boot record global партии шин тепла для того чтобы утилита системной видели да что это диск кататься с и все такое записываем туда иди уникальный некоторые дав gpt и все и дальше просто там у нас заголовок блоки и мы работаем с диском как с одним большим файлом все очень просто в результате нам сейчас не нужно монтировать форматировать всем этим добром заниматься и когда инженер вставляет новый диск сервер мы можем легко автоматически проверить его размеры если там другие разделы какие-то то есть ночи стали нечистики другие параметры если нас все устраивает то моего автоматом вводим в ротацию там создается раздел как бы и через там секунду на него уже пишутся данные и все и поэтому нам теперь нужен только один человек вместо двух инженер который физически выдергивает диска втыкает новый и надеемся когда ему это было делать робот может быть окей из поменяли теперь нужно данный восстановить раньше как происходило вот в баню диск мы нашли новый диск до нужно не восстановить данные мы восстанавливаемся куча других дисков это было сделано для того чтобы размазать нагрузку до что восстановление дисков не аффект тела живую инфраструктуру но проблема в том что диски стали 10 100 байт на и при нормальной скорости ну например 100 мегабайт секунду когда диск еще может обслуживать какие-то чтения это занимает 27 часов и это как то не интересно поэтому мы сделали сейчас по-другому новой системе когда диск вылетает не дожидаясь нового диска никакого он просто берем 4 гигабайтный блок и восстанавливаем его на другой любой свободный диск который подходит по наши критерии распределения данных да то есть в том же dc но не на том же сервере там всё такое и мы делаем одновременно кучу таких восстановлений то есть мы можем сразу устанавливать на 100 на 1000 дисков столько сколько у нас есть благодаря этому скорость восстановления сейчас два гигабайта в секунду и соответственно диск мы можем восстановить за час все происходит очень быстро есть такая интересная штука со всеми этими системе как не равномерное заполнение дисков и чем она плохого первых ну понятно если у вас на одном диске много данных наргу мало той нагрузка на него на чтение прострелена неравномерно есть еще другая штука если у вас есть заполненные диски часть каких-то очень пустых то естественно новые данные вы будете писать на новые диски да на пустые из за этого происходит такая ситуация что на этих новых дисков лежат очень новые данные она киса старых дисках очень старые новые данные они более востребованы и из-за этого тоже возникает перекос по нагрузке на но почтение поэтому пытаемся как то с этим бороться откуда общей возникает неравномерное заполненность ну понятно сервер там не работал какое-то время вы подняли обратно на нем чуть чуть меньше данных потому что у него не писалось это простая ситуация замена дисков это такая же на самом деле ситуация вы воткнули диск он просто всем незаполненным пустой и но самое интересное то расширение кластеров когда вы воткнули там 2000 дисков 3000 дисков и они все пустые других есть данные но мы не делали никаких специальных механизмов для расширения кластером и все это воспринимаем как просто неравномерная заполнилась дисков и боремся со всеми вот этими кого проблемы с ними задачи все эти задачи решаем с помощью одной простой вещи и балансировка это процесс который происходит постоянно и что он делает он просто берет в рамках до центра находит dice который заполнен более среднего до охота родес который заполнил менее среднего и собственно перемещает блок на менее заполнены dice более заполнен это просто копирование в данном случае и такой процесс происходит постоянно их несколько одновременно происходит пока мы не добьемся одинаково процента заполненности и все не нужно делать никаких там специальных телодвижений для расширения кластера но когда мы начали тестировать мы столкнулись одной такой интересной штукой если восстанавливать на той скорости который мы хотели бы то мы получили трех кратный прирост leithen сида клиентских запросов это ни в какие ворота не лезет да мы попробовали снизить скорость восстановления мусором 30 процентов прирост light нас и не очень хорошо что можно сделать блины msi есть его scheduler да и плюс его в том что цирке у да и плюс его в том что ему можно простить приоритета для процесса или для потока и вот с помощью той же ванне и библиотеки сможет сделать прям очень легко ржевскому потоку можно проставить приоритет что мы сделали мы разделили все запросы на клиентские системы во-первых клиентские те собственно которые приходят польза ли самый высокий приоритет для нас их нужно обслужить максимально быстро а системные то всякие такие менее приоритетные и кроме того мы выделили еще фоновые процессы в да и дали ему тот самый низкий приоритет с точки зрения циклу найс и idol с точки зрения его то есть эти фонды процесс такие к восстановления данных или проверка целостности они получатся пау и диск только тогда когда не нужны никому другому и благодаря и пришлось еще немножко ограничить естественно трафик потому что когда вы устанавливаете одновременно там 30 лишний дисков то ну интерфейс можно очень быстро забить благодаря всем этим мерам когда мы расширяли весной по моему кластер на 2000 дисков надо было переместить семь петабайт это заняло где-то две недели и лотностью при этом было вышить на 7 процентов чем нормальном режиме что очень круто еще раз немножко напомню как у нас хранятся данные то есть у нас есть гигабайт данных который мы разбиваем на 10 блоков да по 4 гигабайта все это называется сегмент вместе с котами к восстановления и как же нам прочитать оттуда данные вот у нас есть фоточка она лежит в каком-то блоки нам нужно в первую очередь естественно знать ее размер нам нужно знать эти сегмента да и нужно знать смещение в сегменте чтобы понять каком блоке и пока мало всю ту это данные лежат но как и видели блоки переезжают между дисками и серверами совершенно свободна для этого нужно еще знать где же блоке расположен на каких дисках неких серверах это вот эта вся часть соответственно называется адресом да она состоит из четырех частей в нашем случае как это происходит клиента идет на некий сервер яндекс до ключом индекс знает как по ключу на эти данные он отдает размер и адрес и с этими данными клиент может пойти к нужному серверу данных и получить фоточки никаких там прокси ничего нет клиента сделает сам получил фото как устроен индекс это java приложение в котором внутри кассандра но да да кассандра потому что мы ее умеем использовать часто всюду используем вот она и плохая хорошая но тем не менее естественно все хранится в трех репликах все распределено по центрам все как и должно быть кроме того кассандра удобно в нашем случае еще тем что можно хранить служебные данные которые нужны для функционирования кластера и в первую очередь кассандре в яндексе мы храним на дисках данные о том как по ключу найти где по ключу в каком сегменте расположены данные да это значительно смещение и размер эта статической информации которая меняется редко практически оно никогда не меняется и поэтому это лежит на дисках но давайте посмотрим как теперь эти данные прочитать как как к клиенту прочитать данные вообще и клиента если знаете как работает кассандра идет показ анапском протоколу какой-то ноги который называется координатор координатор дальше идет другим нодам получает ответы это кормное чтение может результаты дает клиенту ok но в этом случае мы получаем только сегмент смещения размера как я говорил нужна ещё информация о том на каком диске и в каком сервере все лежит соответствует хоть еще в какую-то неведомую систему получить ответ и тогда у вас будет полная картина но мы сделали мышка по-другому мы взяли и yandex вот в нем на диске лежит основная информация которая нужна и серверов данных мы получаем информацию о том какие диски на каких серверах час находятся какие блоки на этих дисках сейчас находятся мы берем всю эту информацию строим индекс памяти таким образом что по сегменту можно было сразу найти все блоки которые в него входят с дисками из хвостами со всеми делами и это позволяет нам за один запрос связать эту информацию и получить целую картину да то есть по ключу с диска достали информацию по сегменту и потом сразу же из памяти достали информацию о блоках и их расположение отлично когда прочитать время в том что клиент не может пойти по каширскому протоколу и прочитать там кроме основных данных которые лежат на диске еще какие-то неведомые данные из памяти ну так как это java приложений и кассандра у нее встроенного внутрь то мы можем наружу выставить другой какой-то более человеческий интерфейс мы с помощью того же ванне вода вставляем ремонт интерфейс наружу и поэтому трофей сам уже отдаем сразу же все данные все классно но я говорил что кассандра это к ровная система дае для надежности нужно прочитать из двух нот но клиент может на самом деле послать сразу же два запроса и точно так же сразу к двум индексным нодом причем он пошлет к тем на которых данные лежат локально он знает топологию кластера кассандра но тут есть еще один трюк на самом деле нам практически никогда не нужно ждать 2 ответов почему потому что как я говорил это данные практически не мутабельные и если вы получили полный ответ о том что найдено сущность найдены все блоки и мы все знаем нам не нужно сдать второй ответ достаточно получить самый быстрый это амортизируется все те паузы gc все остальное временный хитом тормоза на-на-на-на-на но тогда мы можем достать дождаться только самую быструю это дает нам что сейчас у нас один только round trip соответственно среднее время она меньше миллисекунды и 99 процентов запросов укладываются в 2 или секунды что в общем нас вполне устраивает дальше вот еще такая интересная штука клиент получил из яндекс информацию про сегмент про блоки где они расположены отлично отправляя запрос говорит фея вкус буду читать свою фотографию с такого-то диска начал читать половину половины фотографии прочитал не сдох да нужно дочитать как-то фотку до конца все таки так как он знает уже всю картину про все блоки в том числе и коды коррекции ему не нужно никуда отдел дополнитель походов он может сразу же пойти на два других дисках сделать получить ответ сделать к ссор и дочитать фотку и все они как бы больше все просто вот кроме того эти всем большим пластам из теста нужно как-то управлять первую для этого у нас есть вера управление в первую очередь мы говорим о управление серверами данных потому что кассандра сама управлять а вот для того чтобы управлять нужно что нужно получить естественно список дисков блоков с каждого сервера и мастер сервер правление он собственно это и делать но стараюсь а в памяти большую картина мира где что расположено чего хватает не хватает ну и периодически выдает некие команды собственных 2 удалить блок и установить блок это все асинхронной такие процессы вот естественных 2 как минимум есть мастер есть slave до мастера управляет выполнена выполняет команды своих просто подслушивают для того чтобы быть готовым перенять лидерство почему мастер slave хотя мы в принципе очень любим мастер мастер системы ну потому что здесь во-первых очень просто право принимать управляющий решение считаюсь один мастер да если их было бы несколько сложнее во вторых нас есть выбор лидера который за 30 секунд выбери другого мастера если тот потерялся и он основан на кассандра таких лайтовый транзак шанс это кас поверх maxus а ну и кроме того потеря мастера вообще не очень критично в этой системе ну просто не будут приниматься некоторые автоматические действия он может и часто полежать на самом деле аки серверов много дисков еще больше облаков как все посчитать их вообще сотни миллионов и нужно следить за всеми то есть если кто то блок пропал нужно его установить у для этого все процессы кроме того до сложный и практически ни в чем нельзя быть уверенным любой процесс может отказать любая операция даже удаление блока может отказать поэтому для того чтобы принимать решение мы используем просто очень простые правила которые постоянно выполняются и проверяют и если правила срабатывает выполнять какое-то действие первое простое правило если мы не нашли какого-то блока который должен быть у нас системе то мы даём команду самому пустому дискусам у ненагруженном у диска мы горим давай-ка ты восстановили этот блок с других серверов да он уже сам с помощью декодирования может восстановить его во второй там правило если мы нашли куда пустой диск не до заполнены диск то мы говорим этому диску пожалуйста восстановить блок самого заполненного диска это как раз балансировка может появиться лишь не блок да потому что когда мы установили круто блок со старого диска еще не удалили то у нас два одинаковых блока но мы даём команду удалить блок самого переполненного диска и когда возникает сбоя диска мы не делали ки сложных телодвижения мы просто удаляем информации обо всех блоках которые были на том диске и дальше срабатывает первое правило нет блока установить блок и она просто срабатывать много раз только сколько было блоков на этом диске таким образом алгоритм становится очень простой предсказуемый ну и немножко результатах система находится год эксплуатации год и 2 месяца если быть точным сейчас там лежат видео и фотографии пока только видео мы перемещаем после 140 дней и зуба с фотографии по течение полутора лет это связано с тем что мы просто временно устанавливаем миграцию из-за нехватки оборудования и получается что у нас 80 процентов видео лежит в цехе 40 процентов фотографий 60 процентов запросов прилетают соответственно в новую систему по видео и 40 по фото всего уже там 240 серверов при этом мы в принципе за год не купили ни одного серого для этой системы брали сервера которые были предназначены для убээс поставили их как первые некоторые маленький кластер о чик и потом перемещали данные сервера и сестер из кластеров и б.с. вот с точки грира вали данные освобождали место перемещали сервера таким образом и год ничего не покупали вот сейчас там уже вывод 80 тыс дисков все это нужно 12 индексов и вот по информации на август поменяли 150 дисков вот так вот они летят даже новые потому что часть дисков на самом деле было новых все-таки и четыре раза расширяли кластер все проходило гладко ну самый главный результат 19 петабайтов место мы сэкономили это неплохо если верить нашей статистике то доступность 5 девяток то есть она реально получилось очень надежно и самое главное что из тех от 800 по нашему плану должно быть 18 до 700 серверов у нас а по факту начал 625 175 серверов мы сэкономили за счет того что мы переставили их части ров на самом деле даже еще лежит в резерве и настоящий год мы будем экономить 30 процентов на закупки естественно все спасибо готов ответить на вопрос спасибо вопрос пожалуйста росте спасибо за интересный доклад маза вижу вот я смотрю и схема аль вашу самописная да то есть и чем-то напоминает ходу да то есть или как вот предложена гуглом тест тема вот вы когда перед разработкой о смотрели община существующей естественно мы смотрим и не совсем идиоты и я конечно нет не неизвестно смотрели смотрите на ходу посмотрели еще в одиннадцатом году когда мы делали старую систему него есть много проблема первых эта система не заточена под задача не скрыл интенсивности сильно выше точки зрения надежности есть проблема с найм ноды она не масштабируются опять же с точки зрения эксплуатации с ней тоже гора проблем у нас есть ходу кфс в в аналитике и админы вообще не счастливы то есть ну там масса проблем да почему мы ее не взяли даже когда они взяли спасибо очень интересно спасибо за доклад у меня пара вопросов насколько я понял до 5 дисков допускается до потери если какая-то статистика за период эксплуатации что возникала такая ситуация когда больше двух . отказывала для одного блока допустим дата-центр десны другом дата-центры или в рамках одного дата-центра несколько дисков и второй вопрос насколько я понял у вас достаточно много дублирования логике на клиенте правильно для вот если бы была съемка когда вы теряете диск и клиент сам запрашивает данные из других серверов и маленькое дополнение к этому вопросу шифруете ли вы соответственно все вот эти метаданные которые при даёте клиенту который потом запрашивает у вас то есть расположение свержение давайте посеву начнут уже начнет а ну-ка to the paper статистике такой естественно мы по новой системе вообще не ведем вот по старой системе были ситуации когда мы теряли одновременно две реплики из 3 2 диска из трех было в принципе не раз да они не всегда пересекаются во времени там такие бывают ситуации что у тебя один диск вылетел начали устанавливать уже на него писать можно до потом в лицо второй диск но тот еще 1 не восстановился в этом случае писать то можно поэтому отказов никогда не случалось но но две реплики из трех полицейских данных они реально несколько раз в год случается такое что мы теряем соответственно оттуда это родилось смысл мысль что когда диска будет многое восстановления будет долго ну мы можем реально потерять 3 такого не случалось пока что насчет клиентской логике на самом деле может быть вы неправильно поле клиент это наши сервера раздачи это не клиенты какие-то да это наши сервера с кашами вот и они наберут жалкую клиентскую библиотеку так у нас написано джаве у нас проблем с этим нету да поэтому не нужно городить некие прокси до ссор до спасибо большое за доклад меня зовут denis компания байду меня за вопроса первый вы удаляете данные вы отказались от файловой системы соответственно как вы боретесь с аргументацией как она вас насколько сильно на вас эффекте ты насколько вам это большая головная боль то очень вопрос ну смотрите я могу рассказать как это происходило в старой системе для начала там данные хранились файлах но мы не спам и каждую фоточку не хранили свои файлы да все равно это были большие блоки там по 256 мегабайт и гигабайта естественно при удалении там возникает дырки у нас есть процедура compaq шина которая просто берет сегмент начинает смотреть если в нем много дырок да она переписывает его в новый сегмент и на старых системах это происходит постоянно еще нормальный процесс в этой системе есть очень похоже только ну как бы в рамках вот этого большого сегмента мы можем взять прочитать да если вы немного дырок и переписать его в новый понятно что это этот процесс здесь очень тяжелый на он сильно тяжелее чем старой системе но благодаря тому что удаляют в основном новые данные удаление в старой системе и достаточно мала и вот сейчас по истечению года там по факту девяносто девять и восемь процентов или сеем заполненность ну то есть если даже мы почистили федор где мы сэкономили слезы но он есть этот процесс ем периодически происходит я видел 10 сегментов компактен и за год спасибо и второй вопрос про перебалансировку при во время перебалансировки между заполненными незаполненными диском и как-то учитываете старость данных которые вы будете переносить чтобы не получилось как раз той ситуации когда у вас на новом диске который был не до заполнен при я в основном свежие и он начал стал конечно учитываем random random и достаточной вам в принципе варан дают достаточно ровный агаром спасибо маленький вопросик спасибо за доклад очень интересно как я понял сейчас новая система больше использовать для старых данных как бы ps по-прежнему для нового поэтому пользователь по-прежнему свежие данные читаются без а да вот но у вас тенденции приехать полностью на новости нет мы так и оставим у нас будет там вернуть два-три месяца данные буду лежать старой системе на потом но вы вышли глостера сильно становится меньше и эксплуатация до основная причина это была цена эксплуатация до на самом деле цена соответственно уже там экономии такая большая можно его оставить ну эксплуатации отъезд он так был свою маленького размера меньше за 1 поэтому нет смысла а система пришло бы сильно усложнить тогда новую за счет как раз этого первичного сохранение 3 реплик вот и у меня еще вопрос вы показывали там что информация о том где на каком диске блок сохранена хранится в памяти на яндекса серверах вопрос каждый сервер индексирован хранит всю информацию или бы она тоже к борщу это она рассчитана по прикидкам нас топит оба на 1000 петабайт и понятно это без добавления память идущий можно добавить памяти так что дурацкий вопрос сегмент 8 4 гигабайта блоки по 4 giga да почему 4 гида тут просто вопрос баланса бы можно было сделать два можно было восемь но суть такая есть чем больше блок всем соответственно меньше блоков проще ими оперировать до их меньше в памяти занимает место там проще структуры данных не знаю все быстрее работать на другой стороны если очень большой блок то его очень сложно перемещать понятно и про сложно записать потому что чтобы записать эту штуку тебе нужно сначала подготовить 8 4 гигабайта целиком потом ты можешь положить процесс запись поэтому то отдельная очень такая сложная история на самом деле я просто не было время здесь про не рассказать поэтому ну выбирали просто исходя из баланс и тaк 0 и там четыре гигабайта можно было сделать 8 вам говорю не принципиально таки большой захвата мне вопрос про яндекс памяти скажите а как вы поддерживаете консистентной между машинами если вы случается переезд блока на другой диск дата интересна та же тема довольно там на самом деле они теплицы реплицируют состояния двумя способами но даже не самое важное важно то что задержки приказ если она могут возникнуть поэтому когда блок переезжают одного диска на другой просто его сразу не удаляем никогда на старом диски он какое-то время еще там есть и вполне себе доступен для чтения и это дает нам возможность собственно там хоть на час оставь аппликации и ну соответственно репликации она мониторится и все такое там ну если вдруг по какой-то сетевой инцидент до каких привет доклад супер есть вопрос по общение проводите ли вы учить отключаете ли вы когда-нибудь целый дата-центр прям по сети и не возникает ли проблем с другими центрами в это время да вот буквально недели наверно три назад последний раз отключали 2 центр нашли одну может вместить нашла нашли одну маленькую проблемку надо будет починить но в целом там был и просто ошибки в течение 30 секунд было некоторое количество ошибку чтения ну да мы проводим в это время других дата-центрах блоки вот они видят о том же центре блока больше нет они начинают копировать сами себя нет данные между блоки никогда между центрами не переезжают исходя из требований по надежности хранения потому что у тебя вот есть двадцать один блок 7 в одном центре 7 другом сей в третьем они никогда из-за этого до центра не могут приехать в этом а потому что они должны там быть внутри до центра они не будут переезжать потому что собственную если возникла процесса нирования они так считаю что то центре все окей кроме того переезд блоков он неконтролируемый он тоже очень опасен потому что ну представьте вы можете выложить мою новую версию приложения которое решило что у вас там половина ваших дисков побилось например да ну просто из-за баги как-нибудь там какой то ошибка какая-то вы не можете взять и начать все одновременно устанавливать от этого тоже существует естественно защиты которые смотрят что слишком много данных сразу потерялась там туда-сюда в течение дня там да есть куча защит чтобы автоматом автоматом случае отключается спасибо вот можно просто спасибо за доклад меня зовут слава а у вас же файлы они как-то делятся на блоке да наверно не весь файл кладется видео большое до 4 4 уделить иногда мы файлы видео конкретно делим пор 64 мегабайта блок а все остальные ну так как есть так целиком у и с этим вопросом вас дедупликации какая то есть если одно и тоже видео загружается ба-а-а вы сразу пользователь возвращайтесь что это видео загружено или все равно он еще же загружает да но когда видео загружается он падает моники сервера нам видео transform и так называемый те который занимается перекодировкой если не обнаружил что то же самое видео которое загружено то сущность остается ланолин куется со старым контентом который уже был загружен ранее почти все равно за ну загрузить ему надо матче мы не можем определить никак спасибо вашей on google спасибо за доклад а у меня вот такой вопрос система резервного копирования всех этих данных существует вообще потому что ну вы их от даже трех серверов и строили там где там 7 диск сколько и 6 чтобы 6 и не ставить ну это локальная проблема а если у вас например какая то ошибка в приложении которое балансирует ну вдруг появится то есть она может размолоть сразу весь кластер ну нет естественно мой backup не делаем потому что бы капт этих данных это слишком дорогое удовольствие и восстановите давно из бака павшего но практически нереально было вот насчет того что вы говорите все так это теоретически возможно поэтому вот эту часть который управляет до который собственно удаляет блоки исчез это самая ответственная часть и она даже быть просто очень хорошо протестированный коду быть очень простой если все просто и понятно то шансов сломать что-то мало ну и конечно не надо там давать к вам попал туда понятно спасибо ним и например вот то что хранится в индексных ноутах мы это кассандра она как и все наши кассандры они бы капица да там все происходит но здесь это неподъемная добрый день вопрос такой система для внутреннего использования или вы предлагаете коммерческое продвижение продукта пока для внутреннего или коммерческого за может когда-нибудь не заточена только на хранение медиа да ну любых собственно говоря данных до если эти данные нему табели если эти данные мутабельные эта система не очень хорошо подходит просто потому что сделал некоторые допущения что сущности не переписываются это позволяет сильно сэкономить во многих местах да и упростить если они будут частыми news не будут меняться но это будет другая система до спасибо скажете вы уже как-то подсчитали вы в плюсе часть денег потому что один это байда в этом добавляйте в месяц это как бы очень приличные деньги как у вас с этим плюсы по отношению чувак чунья потому что на классники зарабатывают давай достаточно калинину я я принялись ракетная эта штука она сейчас рентабельно или там через пару лет вы хотите выйти на плюсы если не выйдете то все к чертям посмотрите вот раньше у нас было так раньше было серверов у нас больше чем сейчас да и с каждым годом и покупать будем меньше откуда может взяться нерентабельность в принципе нет я имею два для что вы стали делать stream и не про эту его соответственно плюс капает что ну это уж прямо готова сделать то это не техническая часть ну как бы это давайте осени ко мне а вот он сам сбыте выпускать это любимый вопрос многих мы с удовольствием хотя бы у пансионата хорошим видео на хаббла защекотали статья на хабре офисе например статья на хабре будут офисе не будет он source хотели бы но на это требуется много ресурсов может быть когда нибудь спасибо спасибо большое за доклад я возле колонны возле спасибо очень интересно подскажите пожалуйста по поводу нагрузки на сеть между дата центрами изменилось ли если да то как понятно что вы свою задачу по серверам выполнили просто интересно это очень хороший вопрос да когда мы приступали к разработке мы думали по поводу того окупится это не окупятся с точки зрения того что в штур по сети потребуют большего действительно и запаса да и в номинальном режиме вы сейчас не можете ходить локальных свой дата-центр да но там разница оказалась не очень большая на самом деле и цивики сказали что норм но честно к деньгах ними сложно ориентироваться да тут ну смотрите раньше у вас трафик весь ходил практически в нормальном режиме весь трафик входил внутрь и до центра но при этом вам нужно было иметь все равно запас потому что если вы положите за центры просто там знаю выполнять какие то работы то другое центра у вас прилетит плюс 50 процентов то есть сейчас процентов запас он все равно был в каждом центре и это как раз дата-центром трафик сейчас дам и этого трафика можем получить в два раза больше но такая ли это проблема вроде бы нет я говорю ну ну да в два раза больше тратить между центрами конкретно для видео например уважаемые гости к сожалению нас заканчивается время на данный доклад и вы можете задать оставшиеся вопросы уже лично поблагодарим александра за хороший готов спасибо александр"
}