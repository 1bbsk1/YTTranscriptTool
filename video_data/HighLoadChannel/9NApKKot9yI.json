{
  "video_id": "9NApKKot9yI",
  "channel": "HighLoadChannel",
  "title": "Как устроен поиск / Андрей Аксенов (Sphinx)",
  "views": 6678,
  "duration": 3629,
  "published": "2017-07-30T00:47:27-07:00",
  "text": "поиск устроен вот так по большому счету на концептуальном уровне больше говорить не о чем к естественно я не могу сказать что надо ставить этот как его и шпионами и потому что у кого корпорация яички очень мало денег а сколько они списали на это тату нами это вообще страшно подумать как значит аффилированные со сфинксом лицо я должен рекламировать конечно сфинкс для поиска вот ну и что еще и кай конечно postgres для вузов но типа поставил какую-нибудь шнягу там mais quel пожгли там еще что-нибудь люсьен поржал короче потом на нормальные технологии переходишь вот а значит я смотрю что как бы не все уходят то есть не всем еще понятно как все на самом деле устроенные хочется каких-то ненужных деталей их у меня есть значит звук концептуальным ничто не менялось да не изменилось даст начал индексации потом поиск индексация по большому счету по большому счету ничего сложного понятное дело что по малому щеку там где в каждый из трех в кавычках деталей в кавычках небольших спрятал не то что демона целая ну где-то стадо где-то легион не совсем понятно значит но как бы концепция всегда простая все начинается значит с маленького простенького пачек нога search а потом 15 лет этой херней занимаешься он берешь документы разваливаешься их на ключевые слова и как бы просто взять и развалить на ключевые слова документ мама , мыло , раму это ты не далеко ушел от гриппа потому что потом все равно эти ключевые слова перебирать надо строить некую спец структуру полнотекстовый индекс вариантов для его построения человечество придумало в свое время довольно много но слава богу от всех отказалась и в нормальных продакшен системах по большому счету победил на данный момент вариант ровно 1 про него и буду рассказывать все остальные имеют скорее историческое такое назначение что ли и практического интереса не представляют потом когда мы эту спец структуру волшебную под названием индекс построили по ней не только надо сделать поиск к несчастью типичную потому что это надо условно говоря поиска по тексту именно поиска по тексту вам недостаточно ну наверно вообще никогда уже то есть я не могу придумать такой узкий с когда вам достаточно было бы только текстовых данных для того чтобы сделать поиск потому что если вы ищете по какие-нибудь с раны многом то вам надо как бы ключевик найти или маску и наверняка либо отсортировать либо от группировать либо какие нибудь еще дополнительные операции сделать над данными вам недостаточно просто текста для того чтобы найти вам ещё нужна какая-нибудь там сортира хотя бы сортировка по дате или там количество файлов на график выдать и сгруппировать соответственно результаты поиска подними так далее если вы веб-поиск который казалось бы иллюзорно или там ладно уж веб-поиск какой-нибудь поиск по локальные коллекции юридических документов которому казалось бы достаточно значит найти всякое и вот отранжировать и как бы ура ура вот она нихрена подобного тоже недостаточно для этого текста потому что в веб поиски вообще от и холокост ранжирования основывается не только на тексты документов она еще болин десятках и сотнях дополнительных факторов и даже в толковом десктопном поиске по достаточно интересной коллекции опять же кроме текста будет еще масса значит всего значит я потратил много драгоценных секунд на то чтобы объяснить есть дуб обработка и если раньше когда то наверное были идеи что она вообще никуда не нужно то сегодня таких идей нет везде всегда везде и всегда у вас будет какая-то доп обработка обязательно я наверное уже давно не видел в принципе не видел ни одного запроса который бы делал просто полнотекстовый мальчонки просто по какой-то галимой релевантности его сортировал ну как не это конечно несколько некоторое преувеличение понятное дело что когда ты какой-нибудь поиск по какому-нибудь форуму делая что у тебя скорее всего как раз дефолтный режим это та самая сортировка по той самой релевантность если ты особо не заморачиваешься вот но надо понимать что в этом случае у тебя все равно как бы на твои божественные две тысячи постов целых три матча найдется на средний запрос и как-то их не сортирует во все равно по ищешь три матча вот данные решают все собственно понимать какую чтобы понимать как она устроена понятное дело плясать надо от тех данных с которыми мы работаем от того от устройства той самой спец структуры волшебный под названием полнотекстовый индекс полнотекстовый индекс на самом деле структура в первом приближении тупая как палка совершенно вот он вот у меня был ещё хороший пример там фотокарточка с библии но вот как бы интернет там времени нет метро все эти дела и не шучу слайды готовил в метро поэтому так все плохо индекс выглядит вот так на самом деле он выглядит совершенно не так на самом деле все существенно сложнее но ну опять-таки значит как как только вот вы выходите за пределы 1 драфта которые на коленке написали то к словарю еще привязывается всякая мета информация списке не только документов нужны а еще и списке позиции внутри этих список позиции списков позиции можешь этом урологической информация хранится и дали им он много еще всякой радости и атрибутика там сбоку наверняка где-нибудь лежит вот но повторюсь в первом приближении вот она структура только единственный момент поэтому значит визуализация не к этой самой структуры вот введен непонятный оператор стрелочка которая не является двумя третьими оператора сравнение двух элементов для сортировки которые во всякие эзотерические языки пихают а это вот не некая связь что дескать у нас есть отдельно словарь вот он левая колонка тут включаем правую руку возможно это сделать мне любопытно просто даже нука нука и заслонить все не невозможно ладно над был попробовать в общем если мы правую половинку заслоним то у нас останется словари и стрелочка это тоже часть словаря можно считать что это указатель для каждого слова разные он показывает соответственно список документов все ништяк поэтому словарю мы можем как бы найти все документы где встречается абырвалг вот очевидно 123 или все документы где есть петя петров вот очевидно этот момент номер сколько 8 едва вот а вася васичкин короче почему-то нигде не встречается не повезло 8 на самом деле и элемент словаря это вот не просто слово там типично лежит еще дополнительный всякий фарш вот пример фарша в конце например собственному слово во первых смещение в список документов во вторых смещение в список позиции третьих далее количество этого всего чтобы статистику по зачем можно их было бы положить в сами списке в отдельные файлы куда мы показываем и тем самым за более компактный словарь делать но тогда по самому словарь им нихера не понятно какая частота этого самого слова а частота этого самого слова либо частота позиций нужно для того чтобы более оптимальный план запроса построить с одной стороны плюс отдать вам статистику по ключевым словам без дерганья основных здоровых данных яндекса с другой стороны ну и вдобавок может храниться еще какая-нибудь дополнительная фаршированная информация типа вот как мы в спинк сохраним масочку полей которые смочили по этому слову чтобы немножечко архивировать поиск по отдельным полям если это можно сделать ну вот как бы живой пример опять же на самом деле все может оказаться немного не так вот этого придуманный пример если что то есть на самом деле в обоих существующих в мире и доступных простому человеку open собственных системах как бы все не так что в свинки что в лечении словарь на самом деле другой с другими данными там несколько другим формат мы так далее но концептуального отличается не сильно да он не такой там чуть другие поля там у нас вон та позиция носят на самом деле нет на самом деле вот я как бы и все еще один интересный момент который наверное стоит прям здесь упомянуть это то что указатели может храниться не всегда зачастую данные просто проще за инлай нить и собственно словарь конечно после этого всего надо пожать то есть не просто вот такие структуры лежат как бы ну как бы как фиксер значит реализует поиска он вот берет вот такую херню джейсон документы на диск и потом джек мере грузим north работает недостаточно хорошо как будет недостаточно эффективно поэтому надо во первых не джейсон документ со всеми этими данными ну естественно в бинарном формате а во-вторых для того чтобы она была более менее компактных и хорошо работала его еще этот словарик неплохо бы пожать ну то есть для того чтобы был быстрый поиск строим сортированный векторов в идеале и пределе да конечно надо бы для поиска было строить просто тупо хэш ну натурально по всем словам построили хэши обводим мгновенный lookup того или иного слова noise кушон две проблемы мгновенный look up to будет во первых но тогда в словаре необходимо держать совершенно не сжатые элементы он от этого гандона распухает раза в четыре и вдобавок если у тебя хоть что в сша нет никакого диапазона во поиска вообще как бы по физическим ограничением и поэтому пока показ boys капот-строк в принципе конечно мы все равно тебя сразу ненавидели но он вот пользователи постоянно требует зачем то я бы конечно хэш делом нахрена я сказать под строки не понимаю вот основное счастье сжатие в словаре вот натурально основное я не приведу сейчас конкретных цифр типа что ровно 37 процентов всего сжатие там из-за этого остальные 69 из-за другого но основная масса сжатие в словаре после того как то его сортам в человеческом значит нормальном словаре она достигается за счет короче префикс насти языка ну то есть как бы люди довольно тупые ограниченные твари в отличие от роботов и короче поэтому словарь всех человеческих языков одновременно крайне невелик но в самом деле какой вот лекси клон был у пушкина наши все между прочим памятник стоит чуть не в каждом городе и улице есть наверняка олег секунда будет дай бог нам не знаю 30000 лет но сколько из этих 30 тысяч лет можно сгенерировать словоформ news плоть общаться но там максимум тысяч двести насчет возьмем кого-нибудь по академические пушкина возьмем целый значит толковый словарь или не толковые больно круто возьмем классический морфологический словарь значит зализняка и соответственно все что из него выросло постепенно порядок в русском языке сто-двести 1000 м ну таких более-менее ходовых и конечно русский язык еще довольно такой мутабельным с точки зрения программиста можно сказать и флик tewin это называется с точки зрения человека еще не за бывшего филологию если сразу знал и это означает что из каждой конкретной лемы можно зги не из каждого конкретного корня можно сгенерировать ну блин много разных пристава чик там бег бегу бежал бегущие бегущие я и так далее вот таких склонений долбанных в русском языке много поэтому слова форму ног а каждая словоформа ну в пределе индексируются как отдельное слово но даже их короче весь словарь посмотри их там ну блин ну жалкие миллионы это на самом деле очень малым вот и во-первых и они друг на друга похожи как близнецы-братья во вторых соответственно после того как ты такой словарь и сортировал лексикографических тупую у тебя получаются одну еще опечатки конечно есть человеке они как бы тупые ограничены и это проявляется не только в том что словарь довольно ограничена еще и в том что они постоянно норовят как-то по быдлянский писать там такого нибудь албанский придумают wpa2 на чей-то просто тупорылое опечатываются как я например в каждом втором слове и слава богу что автокорректор fixed вот и вот получается вот такое вот там а бы работы с тремя ер абырвалг и так далее нет никакой человеческие возможности хранить на каждый долбанная был лишний восемь байт потому что в юникоде это будет именно 8 байт существенный интереснее сохранить один раз префикс об их а потом для следующего обзора с двойным р сохранить пример и дидкот небольшой в несколько by тиков что мы короче сейчас допишем плюс один символ в конец и символ это сохранить а потом еще два символа допишем в конец а потом три дня не 32 отрежем i walk сохраним но за счет этого нехитрого трюка словарь на удивление словарь человеческого подчеркивая языка сокращается очень существенно к несчастью это нихера не помогает против ботов ботов ненавижу лютой и короче ненавистью потому что именно из-за ботов которые генерят всякие уже лишь мощный там всякие саша найди это мою тем кампейн и вот и вот этот вот весь проще как бы фарш сессии слаги из-за этого словарь когда-то индексирует например у иры ли у тебя распухает какие-то вот просто как бы on адовые дали и особо сделать ничего с этим нельзя ну вот такой вот цешин найди случайный совершенно и разреженные префиксов там никаких нет вот эта гадость она конечно подтирает словарю для нормального языка со словарем все интерес а он пожаловался на жизнь что для всяких автоматически генеральных данных со словарем все плохо но на самом деле плохо но не плохо плохо плохо словарь как бы в каком-нибудь уроком совершенно случае когда у вас настоящего текста нет а есть много-много автоматически и случайно генерировал их таких данных рублёв если вы сгенерируйте 100 миллионов уникальных ключевых слов рандомом в тупую и их проиндексирует и естественно у вас основная масса индекса будет в словаре вас по большому счету вообще тогда наверно весь индекс выродиться в словарь но слава богу в тех коллекциях которые обычно принято индексировать данные как бы более осмысленные чем поэтому кроме словаря есть еще собственно основные данные документы будут позиции а я значит долго рассказывал про префикса и забыл сказать про него не энгин lightning пацаны это значит крайне тупая вещь зачем сохранять 8 битное смещение на документ если у тебя всего один документ и одна позиция вот этим не хитрым трюком мы несколько лет назад в сексе в один удар они 1 нехитрый апгрейт срезали размер индекса по-моему то ли на 30-40 процентов ap а потом увеличению у нас эту идею украли вот ну или придумали независимо что на самом деле вероятнее вот основная часть индекса тем не менее это документы и позиция это просто тупой сортированные списке всегда сортированы иначе никак иначе их эффективно не пересечь в тот момент когда ты делаешь поиск по двум ключевым словам одновременно наверное пытаются их сортировать не сортировать сортировать по какому то значит легко местному ранку и так далее только две категории лиц во первых это значит чуваки котором диссертацию мантис как надо защищать защитить эту военком призовет и вторая категория лиц это чуваки которого диссертацию мандиц как надо защитить потому что это означает продвижение по внутренней карьерной лестнице в яндексе и гугле вот других научных работ на тему я не видел доказательств что как-то вот можно ловко эффективно документы положить в противо естественном порядке не не в естественно с рационом поедишь нику порядке а вот по какому то легко венскому рангу уложить в индекс для того чтобы вынимать топ 1000 поэтому легковесным у рангу для одного слова они все 30 миллионов документов и соответственно топ 1000 для другого слова это работает достаточно хорошо могут люди уже не первый десяток лет пытаются ни хрена не выходит вот позиции позиции позиции позиции нужны в тот момент опять таки когда вы ищете во-первых более одного ключевого слова для того чтобы сделать ранжирование во вторых когда у вас но менее тривиальный поисковый оператор чем просто как бы дай мне все и отварить понятно что если вы ищете фразу например точное совпадение фразы не говоря уж о том поиски поблизости и так далее вам нужно сразу позиции даже если вы потом эти данные ранжировать не будете просто для того чтобы смочить фразу придется блин позиции шмонать и если вам нужно хоть какое-то ранжирование то более-менее толково и ранжирование она на самом деле тоже хочет смотреть в позиции причем делать это очень медленно этих данных много вот их реально много ну сами прикиньте на каждый значит на каждое вхождение каждого слова в каждый документ мы где-то должно сохранить какой-то чисел как какой-то внутренний номер документа на самом деле неважно внешним там номер или внутренней для поисковика мы должны сохранить таких данных как бы их грубо говоря как минимум сравнимы по размеру с оригинальным текстом а то если значит неаккуратно хранить немного оверхедов то и во много раз больше оригинального текста нет никакой человеческие возможности работать с индексом которой второе больше чем размер оригинального произноси равана во текста это медленное нехорошо и имя и вообще памятуют существенно лучше ловко все пожать чтобы она занимала не триста процентов от размера исходного текста а вот и в идеале там процентов 5 и соответственно когда у тебя данных в 60 меньше то естественным образом любая операция над этими данными она как бы работает быстрее сжатии сжатии сжатии сжатии значит решает все внезапно про детали реализации в конкретных значит поисковика к внесению на сегодняшний день насколько я помню данные которых основные данные которые хранятся по полнотекстовом у индексу выглядят вот так отдельно есть поток с блоками и по сжатых и дальше документов отдельно к нему вдобавок грубо говоря в отдельном файле и там по отдельным смещением это я забыл лежат блоки частот но пусть не просто факт что значит вот у нас документ 123 17 а еще факты что в документе номер один у нас было частота три раза слово встретилось в документе номер 2 17 раз наоборот и так далее вот такой вот блок этих самых частот в конкретном документе и естественным образом этот блок частот это определяет длину количество позиций для соответственно 3 мега вектора в котором собственно хранятся конкретные позиции там соответственно вот у нас сколько tf в блоке сколько сколько точнее сумма частоты в блоке но только данных у нас там будет лежать в постели соответственно вот чуваки хранят этот тремя разными стримами скажем так эти данные лежат не в перемешку у нас они лежат в текущей версии в несколько вперемежку ну то есть доводы и tf частоты по документам плюс еще некая мелкая дополнительная мера информация конкретно смещение в список постингов ее по моему там какие-то легкие фокусы про число постингов масочку и так далее которые не всегда есть вот у нас вот такая вот основная кишка лежит для тех кто интересуются файлики индексным с названием спд с расширением спд и отдельно лежит файлик в котором лежат все позиции и опять-таки онлайн хорошо работает если у тебя есть одна позиция надо ее сохранить не надо хранить на неё указатель вот в этом случае если у тебя ровно одна позиция ровно одно вхождение а у вас тоже иваница в общую мега кишку документов вот и как бы как будет устроено в свежей версии которую давно готовим и про которую я недавно начал рассказывать что как бы ждите счастье я пока не знаю предварительно в протопите в прототип чеки довольно клёво работает расклад когда у нас вообще все данные перемешаны то есть эдак и до и постинге лежат примут одной ровной кишкой соответственно это плохо тем что когда тебе нужны только и дает и идентификаторы документов работе грубо говоря поиск по ключевому слову одному тебе насрать совершенно на позиции вхождение этого слова в документ ты больше не имеешь возможности посмотреть в сравнительно маленький в этом случае список идентификаторов документов и выкинуть инете и не смотреть в номера позиций совсем но с другой стороны таким образом общий размер индекса довольно конкретно уменьшается и коду прощается соответственно как бы пока не решился с одной стороны как бы и хочется вроде все таки отселить постинге отдельно как раз для оптимизации вот таких вот либо односложных поисков либо булевых поисках где позиции вообще на хрен не нужны а с другой стороны как бы мой таро сильно раздувает yandex чем еще надо как бы думать бенчмарка тьi так далее вот я считаю будет особенно смешно и гранично если где-нибудь в зале сидит какой-нибудь засланный казачок из гугла и как бы тихо ухмыляется и как бы про себя думает что у нас все не так вот это не единственный метод сделают формат и тем более не единственные верные эксперимента ты эксперименты значит с вот как бы нам половчее сохранить эти данные еще раз списке документов и списке позиций их много их надо хранить как-то эффективно а потом эффективно читать и работать эксперименты значит в принципе как бы наверно не прекратятся никогда я вот смутно помню что когда-то читал какую-то бумажку от гугла гугл вообще как в известной открытой open source на я компания не паче наружу и ни одного документа устаревшего менее чем на 5 лет тоже наружу отдавать нельзя вот ну а вот я читал тем не менее документ который не понятно насколько устарел где вкратце и вскользь в два слова упоминалось что в гугле еще более интересный значит формат хранения этого всего вместо того чтобы хранить какие-то отдельные списки документов pdf прочего этого говна они хранят один гигантский список позиции ну или память или это эксперимент какой-то был гугловый или боевой индекс ну так бы открытая компания ничего нельзя но запомнил значит следующую идею в любом случае интересную хранится гигантский общий список позиций причем плотный типа вот у нас был 1 документ у него в нем было 1000 слов соответственно он занимает номера позиции с 1 по тыщу а вот второй документ в нем было 12 слов он занимает соответственно позиции с 1001 по 1012 а вот третий документ но и так далее вот и как бот вскользь читал что вот клёвый формат типа стало хорошо по моему все таки в гугловской презентации границы документов при этом определяются вот этой самой внешней какой то эта информация ну то есть отдельно лежат отдельно лежит где-то маленькая такая кишка собственно в которой конкретные границы документов прописано что у нас первый начался с позицией 1 и кончился в 1000 2 соответственно начался в позиции 1001 и кончился в 1000 12 включительно и так далее вот то есть как бы вот данные лежат вот так внесение так в сфинксе вот так в сфинкса следующей версии непонятно как как значит у больших дядек ну тоже непонятно как ее наверное постоянно или он постоянно не постоянно не каждый день но регулярно меняется значит а зачем я все это вообще рассказываю всем же насрать но это как бы хорошо когда тебе насрать на внутренности того с чем ты собираешься работать особенно когда ты пришел на доклад про то как это внутри работает но к несчастью это дело довольно не плохо влияет на 2 маловажные сегодня характеристики на скорость как бы работы всего и объем ещё потому что даже просто в тупой механизм кодирование данных которые ты собрался сложить внутрь индекса он меня это объем этих данных блин и соответственно как минимум скорость чтения обработки этих данных в время которое там чуть менее чем бесконечности и насчет чуть менее чем бесконечность это не шутка вот пример условно частотного слова на самом деле слову что менее частотное самое 6 дна и по-моему в этом как его в русском языке это не то что вы подумали предлог и вот но для примера сойдет вот предположим у вас есть вот такой вот список идентификаторов документов 1 3 4 5 6 и так далее он один хрен возрастает и в нем довольно плотные документы зачем нам хранить большие числа которые постепенно вообще до миллиона до растут и на них надо много бит давайте посчитаем дельты между каждой соседней каждыми соседними циферками я от посчитал меня вышло 11 там 2114 и так далее возможно где-то считался это не суть важно важно то что абсолютный порядок циферок во втором векторе который подписан working он ну как бы существенно меньше значит не хитрый ход давайте вот возьмем вот эти маленькие циферки закодируем их переменным количеством байц всеми битное значение восемь байт 14 битное значение в 16 байт ну и так далее некоторые уже догадались как это сделать вот и как бы буквально за четыре часа напишут реализацию на php и выкинут внезапно вместо до 32 бит или не дай бог даже 64 на каждый дачник мы храним спасибо господи 8 в среднем и конечно у нас изредка встречаются какие-то уродские пике когда тут прыжок в основном списки сразу там с 11 до 12 миллионов у нас там будет 12 миллионов -11 тут потребуется 24 бита для этого значения для этой дельты все равно и она один хрен за кодируется ну в четыре байта в три и уже не закодировать потому что у тебя будут оверхеды кодирования вот но в среднем в среднем у тебя блин будет один байт они 4 внезапно данные сход police в 4 раза и эта наука 20-летней давности а значит современная наука то есть всего десятилетней давности это забавные блочные коды которые ну во первых они еще единичку вычитают потому что у тебя все равно обязана быть дельта у тебя набираете начинаются с единички не с не с нуля ну вот вышли единичку там сэкономили питик получается вот такая вот что-то и вот последовательность вот этих нулей и единичек изредка троек их можно закодировать в зависимости от того как подойти к снаряду иногда в 00 в смысле когда у тебя достаточно длинный блок в котором исключительному ли ну то есть у тебя матчах последовательно много документов блок например из 128 документов подряд 1 2 3 сирена часто и частотное слова либо просто ты как бы грабил как бы после одного и того же человека с бложика и очевидно его погремушка она встречается во всех этих документах подряд вот ну как бы и так бывает и понятное дело дельта между соседними и для документы они по единичке вот и все константа той этот факт можно пожать условно говоря в ноль бит на документ плюс небольшой фиксированной overhead мы там пишем один бантик что дескать пацаны следующие 128 дельту нас единичка такое счастье в реальных данных встречается крайне редко на самом деле если я правильно помню свои эксперименты с кода кому я их под забыл уже то вот такое кодирование блоков именно нулем бита но особых значит радости не давала но кодирование достаточно большого блока документа документов в 1 2 или 3 бита по сравнению с восемью опять-таки схлопывается размер индекса еще в несколько раз эффект надеюсь как бы понятен что повторюсь и одно дело если нам надо прочитать с диска или из памяти не важно откуда 100 мегабайт и перелопатить если бы мы не дай бог данные нежели совсем war in там пожаре вон там 25 мегов кодах более приличные значит жмется всем хорошо вот я считаю что все таки как бы важно хотя бы представлять что там внутри происходит и зачем все эти вообще кодеки нужны как настраивать и так далее вот внезапно внезапно вспоминаем противный факт что кроме текста в яндексе есть ещё те самые божественные циферки то есть определенная какая-то мета информация привязанная к документу атрибуте вко в той или иной форме ну то есть данные которые мы индексируем полнотекстовым индикатором но которые тем не менее как бы должны тоже присутствовать и состоять потому что под ним неизбежный дополнительные операции фильтр очевидные значит фильтрация группировка сортировка менее очевидные ранжирование с одной стороны но и возможно просто стороны с другой стороны в тот момент когда вы внезапно мы из поиска делаете требую базу данных несчастью человечество значит придумала много концепций на настоящий момент и миллион разных методов хранения и вот как бы один конкретный еще не победил данные как бы можно сказать что они ранее масках бы реляционные с жестко заданной заранее схемой можно наоборот сказать что мы короче хотим полного динамизма и разврата и поэтому у нас полный эскимо лес после этого тоже можно значит сохранить данные по-разному и реляционные можно криво сохранить с одной стороны но традиционно схема рис конечно при принято хранить особо хорошо ну то есть лучшем случае в какой-нибудь бинарном формате ки меня в этом плане не перестает удивлять история вас грецку или которые сделали поддержку джисона и в первой итерации хранили и тот же сон грубо говоря просто текстом вообще без никаких дополнительных этих как их звать попыток как-то ускорить работу с этим самым жестоким вот слава богу даже люсин таким ужасным образом данные не хранит но он насколько я знаю здесь я могу дико ошибаться под а потому что как бы смотрю в них не каждый день далеко насколько я знаю у них вот как бы внутри та самая значит очень гибкая но соответственно при этом тормозная структура которую я ласково называют флекси тормоз по меньшей мере такая структура данных используется для хранения дополнительной атрибутики умолчать иным образом ну то есть когда вы сохраняете атрибут там не схемы естественно делается оля реляционная база данных с дико быстрым доступом а вот как бы храниться условно говоря jison документ или скорее бы сон документ в некоем бинарном формате где доступ как бы быстрее чем парсинг текста и как бы что все это не так 1 тормозило все обставленным многоуровневую инте шаме чтобы как бы после первого раза доступ был быстро я первый раз доступ был очень медленный в свете решения не сказать чтобы принципиально лучше но местами работает мне кажется все таки батарея у нас значит оригинально наоборот там адский реляционный подход гигантская таблица с фиксированной схемой в памяти что понятное дело неудобно когда ты туда заливает хочешь заливать разреженные данные типа g сончик а вот но а мы от нее отказываться наверное не хоть и потому что я вот верю в то что у человека должен быть выбор как бы то ли как бы застрелиться в голову то ли как бы застрелиться в артерию на ноге и истечь кровью и соответственно должен быть выбор талия как была традиционная схема хранения атрибутов если ты заранее знаешь что тебя в каждом удобном документе есть рука колонка прайс на флот avaya хахаха это собственно уже он не то чтобы в артерию он такой хороший два пальца на ней в принципе тянет плату флотами заценить фото флотами цены хранить вот тем не менее если ты заранее знаешь что у тебя в каждом долбанном документе есть не сразу ее завидев схему она будет клёво эффективно хранится занимать четыре байта на документ и доступ к ней будет мгновенно не надо неджи сончик парсить не по бы сончик вы скачать не через миллион киши внутри люсьена продираться но естественно как бы схемы иногда меняются на этом и на лету и местами всякие исключения возникают поэтому джейсон никто не отменял у нас поддержка есть и некий внутренний формат я пытался как бы сделать из себя хорошего разработчика и украсть чем-нибудь хорошую реализацию вот но выяснилось что я плохой разработчики поэтому все чужие реализации еще хуже чем я могу написать пришлось написать своем кроме того кроме значит собственных хранения атрибутов мало их просто хранить еще желательно их как-то индексировать этими насколько я знаю более-менее прилично пока не занимается никто подчеркиваю здесь значит ключевое слово более-менее прилично вроде бы местами люсин делает прикольные какой то адский совершенно фарш симуляции индексов по блин колонкам по большому счету элементами полнотекстового индекса вот это с одной стороны ну а родных индексов нет и соответственно конечно сфинксу сфинкса есть не менее моментальное решение у нас по отдельно взятым блоком записей строится махонький блочный индекс чтобы если вдруг наступил полный перебор в какой-то момент всех записей уж не каждую совершенно запись перебирать и сначала такой верхний уровень проскакать и какое-то количество блоков откинуть сразу тем не менее значит насколько я знаю в поисковиках никакого креатива сохранением атрибутики пока нет ну то есть когда у тебя есть схема можно креативить хранить не просто тупую по строчную матрицу а всякие там пока ло ночные сжатия делать хотя бы для 1 по колоночный представление по сжатые делать хотя бы для отдельных колонок если у тебя даже схемы нет тоже как бы все интересно можно взять этот схема лес документ сплющит его в тупорылое облака киева илью тегов и потом поэтому тупо облаку как бы линейно искать вот как можно этого и этого не делать но слава богу текстом никто не хранит а можно простить и не просто сделала тупарыло уборка тело велю тегов хотя бы of хотя бы положительному фишек для быстрого доступа а можно его не плющить можно честно иерархию хранить а можно компрессию ключей делать нам как бы там миллион разных трюков но вот дает до них пока как бы еще поиске по меньшей мере open source ные как бы оба целых два не особо доросли мой корень над чем работаем но как бы пока не доработали я считаю немного рановато хвалиться вот в этот момент у меня внезапно наступила метро строгино я короче написал вот такой слайд это не осталось пять минут и еще пол доклада но и мне еще остался обед поэтому как бы это хорошо это нам дает возможность пожертвовать обедом вот внезапно про ранжирование с ранжированием мои текущие значит гипотеза такая что ситуация бывает по большому счету две либо его вообще нет либо она на самом деле в идеале вам нужно тяжелая но вы отдел его и тесь легким но то есть если у вас как бы вообще не стоит задача ни хрена аранжировать то все прекрасно булев matching никакой тяжеловесные обработки документов ну не нужно если у вас в принципе качество поиска хоть что-то да значит если бы вам хотелось чтобы узнать план жирование была более интересная чем но вы не знаете как или не умеете или в принципе 1 3 результатах и так сойдёт то как бы отделывается какой-нибудь легкой шнека и типа встроенного канонического бы им 25 ранжирования придуманного 40 лет назад и везде хорошо описанного вот это вот натуральной они не сложная формула это она только выглядит страшно так что сразу хочется на самом деле в ней две переменные по большому счету интегрируется по всем saw павшим словам thief тером frequency частота слова попавшего в документ и idef эверс документ frequency обратная частота по коллекции это логарифмическая метрика которая грубо говоря равна нулю для документов которые есть везде мент который есть везде в каждом долбаном документе коллекции всей в каждом он ничего не значит с точки зрения ранжирования факт что мы его нашли у ну как бы ни о чем и наоборот документ которые есть в одном документе из коллекции вот на него и дев максимальных единицах и функция там нелинейная там логарифме книги определенное чтобы значит жизнь медом не казалась thief линейный поэтому thief вот сглаживается значит здесь между вместо f кьюай день надо на самом деле написать тех грубо говоря вот но поскольку как бы уже четвертый год руки до photoshop не доходят чтобы формулу старину из википедии подкрасить она выглядит более страшно чем могла бы вот еще здесь есть в этой формуле как бы третий фактор но он уже даже он внезапно даже этот фактор что интересно заметьте уже нетекстовые это средняя длина документа о на в гдр формуле и соответственно в г duckling на слайде вот эти вот легкая математика которая что-то делается с длиной документа это ну соответственно нормирование на для мун документа чем ближе документ к средней длине всем для этой формуле для этой формулы лучше вот оно как бы хорошо изучена 100 лет с ними используется это мыс придают довольно неплохие результаты но чисто статистически никак не учитывая позиции меня вот этот грустный факт что она никак не учитывает позиции и какой-то сраный рыбак который как бы миллион раз повторяет все ключевые слова и мощно спамят базу по точному совпадению фразы а и feel you песню депеш мод одноименную загоняет на позицию примерно сто двенадцать меня этот бонусный факт как бы напрягало еще 12 лет поэтому назад или сколько 14 принц спаси господи такуя старой пойду сама убьюсь поэтому у нас дефолтный ranker сразу свб м25 подмешивает компоненту насчет основанную на позициях про близости фразы которая была степень совпадение фразы запроса и соответственно документ он на самом деле это тоже довольно легко местные в расчетах штука но хотя бы в смотрят в позиции хоть как-то если надо тяжелое качественное ранжирование то на самом деле все весело потому что факторов для того чтобы хорошо все сделать надо учитывать скорее всего много ни разу не 2pm 25 дело не ограничивается как минимум ну точнее bm 2225 никуда не девается но вместо самого родного pn25 надо пользовать всякие модификации подгонять для них весам композите вместо много этих самых б м 25 смотреть на дополнительные интересные факторы посчитанные по тексту а что ещё интереснее смотреть на массу факторов которые вне текстовые которые привязаны к конкретному документу собственно именно в этом основное значит головная боль и машинное обучение у больших поисковиков переменных которые учитываются в этих расчетах их натурально сотни тысяч 8 800 факторов на каждый документ который учитывают в ранжировании ну блин легко бывает это вот как бы вот так вкратце и за две минуты устроена ранжирования понятное дело что как бы про разные аспекты можно отдельный мастер-класс дня на три устроить сначала про общая всякие там пор pn25 поговорить в одном право всякие возможные факторы в машинное обучение болезнь что по делу дальше следующий внезапный ход вот мы поговорили про некие базовые формат что в муке есть как бы слово есть там список документов и так далее как бы поиски иногда делают вид что они real-time а вы и как это устроено внутри вот метро строгино как бы там не то что вай-фая нет там столько связи нет вообще причем очень интересно заходишь двери будешь звонок падает сразу вот просто на колени и потом еще мордой в песок после того как ты прошел паспортный контроль и вот здесь должна была вот эта вот картинка быть которую все знают как этот счет нет никакой ложки не такой ни хрена философ никакого real-time а на самом деле тоже нет в полноценном понимание термина real-time то есть полноценное понимании это было бы как это если бы натуральной индексная структура честно обновлялась в реал тайме но вот я не помню на следующем слайде она есть или нет а мне clicker отключили на всякий случай да нет или это я его сам отключил или короче предыдущий может предыдущий этот оратор как бы исполнил мою хорошую идеи и украл батарейки небу то реки на месте короче clicker сдох и не говорят ничего по поводу того что происходит ну ладно я буду пока рассказывать про этот слайд менее жалко полноценного real-time а значит в природе нет потому что вот этот список документов которой ассоциирован с каждым словом он скотина потенциально большой и он сжатый его обновить это геморрой нечеловеческие к нему можно в лучшем случае что-нибудь дописать в конец вот это да это можно и легко а всунуть туда в середину сунуть туда в середину какой-нибудь документ но практически невозможно поэтому для того чтобы я полнотекстовый индекс сделал вид что он real time ago человечество значит опять таки про исследовала много всяких странных концепций в итоге победила одна вещь real-time эмулируется нехитрым образом когда прилетают новые документы либо новые версии старых документов о попёрла то мы создаем новый маленький на на яндекс с этими самыми новыми документами либо новыми версиями в случае если это риплейс и именно новые версии то в старых существующих индексах на на уж они или мега неважно мы взводим некие флажки что несколько короче если вот этот документ найдешь что-то его не находи его на самом деле больше нет и строим каскад вот таких вот индексов и постоянно чтобы их не становилась тоже 3 миллиона мер жим и соответственно ровно в момент мер же на на индексов физически вытираем если физически вытираем ранее подавленные флажками записи из этого индекса вот каноническая техноло значит слово которое придумали в люси не соответственную весь мир теперь на этом он называется сегменты мод мы не стали изобретатель свою собственную терминологию нас те же самые сегменты только в концептуально одно и то же то есть нет никакой real-time новости на самом деле когда вы льете новые данные в типа real-time систему она создает быстро-быстро по ним дополнительный махонький intex так называемый сегмент старой версии возможно подавляет там kill листом или масочка мире еще как не суть важно и когда-нибудь потом когда два сегмента сольются в экстазе она эти данные физически удалит ну вот как бы вся rialto и вость она вот всегда устроено вот так на данный момент нового клёвого мода на не придумано ну блин сжатые данные ты иди внутрь и zip-архива положить другой zip-архив вот еще про разную физику про разные физические отличия потому что на счёт регулярно задают вопрос там почему не люси ну и как бы почему мы возьму особенно меня конечно радует когда на хабре пишешь статью типа вот мы делаем пуще улучшение 1 камин никому они не нужны в стиле никому они не нужны убить и себе об стену не мы убьем как бы конечно рано или поздно люди все смертные но вот комментарии что несколько никому нахер не надо там ускорение индексации в три раза и там какие-то еще приятные плюшки это вот было весело так что да да иногда не то чтобы каждый день но вопрос задают значит ответ на него на самом деле как бы это такой амбивалентные двоякий концептуально то все внутри одинаково вот это вот структурка со словарем списками или оля 3 рубля секретами для обеспечения real-time асти прочие радости жизни она концептуально везде одна мог блин ну вот как бы попробовали всякие разные подходы лучше всего работает вот такой он везде реализован есть однако как бы такой тонкий момент под названием детали реализации разные форматы и так далее к несчастью эти самые детали реализации разные форматы и так далее местами меняют все местами на порядке вот навскидку я вот придумала и написал на слайд 3 отличия которые все уже наверно прочитали до что вот у нас общий словарь всех слов на все поля всех в которые есть у люси но соответственно на каждое отдельное слово отдельные слова и все хочу когда-нибудь устроить интересный benchmark под названием давайте сравним туда миллион документов just он чиков и чтобы в каждом короче поле с уникальным названием это несложно сделать мне любопытно как она себя поведет после этого при поиске по всей коллекции разуму если одно из двух или я неправильно читают java код что в принципе вы конечно вероятно но не потому что java сложный язык и потому что java провоцирует вас внутри стать 20 разных фабрик декораторов и прочего говна для того что обернуть четыре строчки кода которые занимаются актуальной работой вот как бы это мешает соответственно если я через какой-то декоратор неправильно пробился то возможно не так все плохо вот вроде бы вот так то есть если сделать сильно много полей ту бедняжка загнется навсегда у нас соответственно наоборот не загнется навсегда но если ты ищешь по высокочастотному отставить наоборот низкочастотного на самом деле и высокой селективному полю вот то есть у тебя есть гигантское коллекция там документов по метру каждый и к ним африке махонькие как бы как письму 99 процентов населения но никто не признается тайтл и и ты ищешь поэтому-то я там брин два слова из миллиона ну как бы эффективно иметь отдельные индекс то на эти два слова из миллиона на это отдельное поле а у нас его как бы не реализовано у нас лучшие что реализовано этого масочка в дохли стену это не достаточно хорошо соответственно мы на самом деле в этом случае прошу она им весь индекс они отдельно и конкретное поле это в свою очередь неэффективно ну как бы разные из кейсы разная ломается в разных местах у нас вон там таблице атрибутов памяти про которого я опять таки там всякие тонкости реализации муж долго рассказывает полюсе и соответственно возможность в эту таблицу атрибутов пихать и джисона вский просто в тупую документы в виде отдельных атрибутов волю сено вот реляционные подобное нет у них документы на диске какой-то когда мы последний раз год или два назад меч марка ли удивительно медленный доступ к деле холодный доступ к отдельному полю отдельного документа но это никогда не видно потому что как бы мне политика а ты в херачь на север 64 гигов памяти из них 60 2d подтяжка вот как бы вот физический уровень юн разные значит самые яркие отличия про которые я знаю не вот такие наверняка есть какие-то еще clicker опять сломался нажмите кнопочку следующий слайд там в телеке и пожалуйста и хрен с ним с криком я вижу жест о нет там чё-то чё-то дернулась тому ага нет все таки сменилось а до кроме физики у нас еще значит чтобы открыть и по-быстрому закрыть тему под названием чем сфинкс не люсьена как бы и как нам обустроить россию у меня еще такое общее впечатление skoda складывается что скажем так концептуально разные подходы снаряды там где мы предпочитаем что-то не делать чем делать плохо этом если делаем та концепция давайте все уметь быстро быстро считать а там пользователь сам на своем уровне каком-нибудь каком надо затишье рует таллуле все на все наоборот давайте короче там про стрём кучу памяти посчитаем хоть как-нибудь а потом внутри сервера или соответственно библиотеки там на обоих уровнях и внутри самой библиотеки внутри всех серверов на его построена миллион кишень на разных уровнях их местами даже отключить то невозможно опять таки но как бы чтобы обсирать всех ровным слоем на самом деле плохие оба плоха до подхода в сфинксе хрен ключ включишь какой-нибудь кеш который хотелось бы все-таки включить на уровне сервера влечение соответственно плохо сделать нет ни benchmark ты его сделал там 10 запросов повторил их тысячу раз ну замечательно ты только что померил скорость отдачи от ки из кеша причем потому что киши хрен отключишь ты как бы и даже если здесь не вопрос запросов делаешь целый 20 то ты все равно погоду на марсе измеришь вот и тут внезапно история прокси пьян как вот просто вот значит при этом люсин как бы еще нет они шедевра возводят концепцию в абсолют странная эзотерическая система которая давно сдохла ей пользуется наверное только один человек в мире в продакшене которые написал под названием happy он я ее один раз короче попробовал за бенчмарка значит запустил какой-то тестовый поиск она мне отдала за результат за 0.000 mihiro подумали русские мужики я же окна учу несколько запрос она отдала еще несколько каких-то там ответов это же за 000 000 youtube совсем удивился и было уже как бы начал полировать стену чтобы вот в этом месте самоубийца а потом там выигрывали выгравировали портрет вот но все-таки догадался провести второй тест или там что-то включить то ли сортировку по атрибуту то ли поиск по фразе сделал и так далее внезапная тайное стало явным тупая овца короче при поиске по отдельным ключевым словам вынимала 10 заранее по какой-то магической своей формуле никому не нужный отсортированных документов по каждому слову мер жила вместе эти списки чересчур считала аппроксимацию б м 25 и ахеренно быстро отдавала ахирин нами нужный резалт в сет в нем ножа . количество матчей не было потому что полные списки документов не бебель не перебирались полный матч еще resulted не считался а считалось аппроксимация типа вот у нас есть лиам документов это слово встречается в одном проценте это слово в трех процентов боец перемножим короче с прокси миру и мая так сойдет чего не как только включая что-то значит поинтереснее этого поиска по ключевику а повторюсь он редко когда нужен и редко когда дает хорошие результаты производительность сразу упала где-то в 30 раз ниже сфинкса и на этом я бич маркой окончил и свое знакомство системой тоже кликера по прежнему не работает пожалуйста следующий слайд и конечно уже бы бутербродик ну ладно вот не надо причем недооценивать силу просто тупо багов то есть как бы различие в подходе есть есть еще как бы беды в бенчмарках про это у меня будет еще один слайд чуть дальше но я бы хотел еще значит обратить внимание на то что всегда неизбежны в отдельно взятых случаев просто в тупую жопы в реализациях в конкретной системе вот неизбежный и не чё ты мне вспомнилась история про префикс и потому что она прикольно и показательная я помню это все анекдоты многолетней давности но ты-то вовсем не менее смешные же на самом деле сегодня то все нормально но как бы вспомнить старину тряхнуть этими яйцами смешно помню бенчмарков как-то раз люсин на префикс нам поиски не важно на каком то ли прям префикс нам то ли поезд под строки и удивлялся херли так медленно там тогда декораторов приняты было писать по 15 всего они по 25 поэтому докопаться до проблемы удалось особенно быстро выяснилось что короче при префикс нам поиски реализация на оценку и я в люси не она просто в тупой весь словарь перебирала линейно вообще весь слабому спасибо что не движком request of моему линейный поиск по словарю в 10 миллионов ключевых слов это тоже было очень очень хорошая идея но не надо я как бы порадовался и по моему после этого случилась значит и случился эпизод когда реальность продемонстрировала что не надо радоваться когда сфинкса job to жесты с тем же самым по большому кейсом с поиском подстрок но несколько в другой ситуации то есть некий яндекс по словарю чтобы при поиске подстрок не перебирать блин 10 миллионные словарь и вообще весь ли ей на это безумие полное у нас то сразу был но ситуацию под названием а если какая-нибудь жахнет запрос типа а звёздочка мы не то чтобы не предусмотрели но на наших тестах она вот как то вот вот более менее адекватно себя вела и тут внезапно у клиентов продавца не падает сервер причем не просто падает а его короче кнопкой приходится причем не той кнопкой которая control alt delete кнопка которая дернуть из розетки вскрытие показало что клиент короче немножко ошибся в настройках мы немножко ошиблись в дефолт ахль аль я 3 рубля и запрос журавлев со звездочки в конце боб аудио заменил на пробел потому что charset тайбл после этого остался запрос веса звездочки конце и все бы было хорошо если бы лимит экспансии стоял вменяемые они 175 тысяч слов уж не помню было ли это в конфиге это ошибка конфигурации было или ошибка наших дефолтов не суть важно но в любом случае если бы для этих ста семидесяти пяти тысяч слов из которых 174 тысячи 500 слов встречались по одному разу в одном документе не создавались буфера размером чтение размером 4 мегабайта на каждое слово то серверу тоже было бы немножечко легче вот ну ничего как бы выяснили от чего он падает по чесали репу исправили на теперь значит все у всех все хорошо и и люси он исправился и до них дошло что полный перебор это как бы не клёво и до нас дошло что слово у котов по которому надо прочитать три байта данных не обязательно читать через буфер размером 4 мегабайта мы теперь всего 8 килобайт на него тратим в игре на самом деле моего вообще не читаем через буфер но это уже отдельная сложная история clicker не корни хрена я вообще не ожидала он как paper а он случайно он походу влево нормально а вправо вот на через раз внезапно про бенчмарки значит все знают что можно правильно benchmark неправильно печворк эти вообще все знают что значит как бы надо сравнивать примерно одинаковое смотрите время и как бы потом начинает смысле о том что иногда бывают киши они чего-то сбивают и так далее и в идеале надо смотреть среднее время еще в идеале смотрит гистограмм q смотри считать квантиль кими дианы и так далее выкидывать outlines даже может и так далее к несчастью конкретно в случае с поисковыми системами они сбить марком selecta в базы данных и это подход это подход это подход под названием кликов не работает не работает ни хрена по одной простой причине здесь опять-таки должна быть короче слайдик такая с этим упырем из зомби ленда или как его ну короче с карлом что все плохо одинаковый запрос карл одинаковый именно в поиске к несчастью за счет отсутствия определенной стандартизации что ли или еще чего то это самое понятие одинаково запроса она сильно следующий слайд выходи нажмите там что-нибудь я уже сил нет бороться с ним к несчастью именно в поиске этот самый так называемый одинаковый запрос ни хрена не одинаковы очень по-разному может внутри считаться вот живой пример как собственно считается полнотекстовая часть сфинкс дефолт мы хотим находить все слова а это сравнительно быстро обсчитывается потому что ты берешь самое редкое слово потом как бы под под это дополнительно уменьшаешь все найденные по самому редкому слову об отфильтровывает это дело более часто и более часто и так далее понятное дело что как бы взять слово которое встречается в трех документов и потом по фильтровать воду те достигнут 2 документ условно говоря на выходе яблонь мер жить соответственно три документа плюс миллион документов плюс миллион документов по по второму слову плюс еще два миллиона по третьему сравнительно медленно вот но зато при этом у нас значит сравнительно тяжеловесная по дефолту же ранжирование которое смотрит не только в частоты ключевиков которые там нашлись смотрят еще и в позиции хоть сколько-то считает не самый тяжелый фактор под названием proximity там близость запроса и документа но мы все-таки считает и шманай для этого подчеркиваю позиции а это в грубо говоря минимум вдвое больше работы сразу люсин использует условно медленно реализацию которая слова вроде как орет то есть вот базовые булев мальчик там теоретически медленнее но зато ранжирование сильно более легковесным и вот внезапно вот эта вот история про каса пиа улыбаемся и машем мы дали заранее протестирована и грубо говоря резал сет по каждому ключевому слову которое не значит в бою вообще ни хрена вот вам три одинаковых в кавычках запроса с точки зрения пользователя да они совершенно одинаковые мы просто значит взяли систему и не приходя в сознание сказали шарахнули в каждую запрос текстовые мама мыла раму и это еще в некотором роде цветочки и потому что момент со внутренними кисаме про которые я по моему уже распинался и наверно повторять много неохота ну вот вот вот так как на слайде написано тише везде просто от мы в свое время плюнули пинч харкать определенные штуки просто потому что не смогли отключить все тише ну можно внешний интерфейс некий определенная benchmark отвлечение просто заваливать его запросами в надежде на то что эти киши кончатся вертушку ставить гигабайты памяти и там 10 гигабайтные индексы и удал блевать его запросами никаким другим образом суп и не удается обмерить собственную производительности реального кодали скажем так который интересно было мерить они производительности сша и как бы мой любимый значит пример на счет маркетинга дремин дефолт номер три это вот сниппеты такой отдельный совершенно бокам стоящий момент конечно наверное не всегда и не для всех важной но очень очень показателен неоднократно тут даже уже не то что не каждый день наверное не каждую неделю но тем не менее неоднократно задавали вопрос о чем у вас собственно с ней 5 такие тормозные прямо в ссоре вообще как бы няшка работает как мне не знаю как короче ошпаренный кролик нам витаминах и по у вас тормози щи значит вскрытие показалось той есть значит три основных момента три основных момента что во первых у нас есть подсветка синтаксиса а в самаре оказывается подсветки синтаксиса по дефолту нет и если ты и включаешь он начинает сбор и помню помощью работать в 10 раз медленнее но то есть у нас каждый запрос честно парсится синтаксическое дерево и как бы быстро работает в том числе тупорылые запросы в которых никакого синтаксического дерева не то просто на вареные ключевые слова папочка и все и нато все подсветить вот второй момент как бы то что у них грубо говоря индекс заранее создан некая помогающая структура хотя как ни странно конкретно для подсветки сниппетов там помощью не в 10 раз скажем так и вот мне более всего значит бальзам прямо на сердце это называется оптимизация pro 64 килобайта вы что думаете это опять про буфер который там у нас был 4 мегабайта данных слова читалось нет все значительно проще значит у суворов очень хорошо работает подсветка больших документов потому что он подсвечивает из них все первые 64 килобайта и давай до свидания сфинксы соответственно я честно подсвечивает все тела и пытается во всем теле документа искать опять-таки по дефолту умы по моему сделали все таки в итоге мышку для того чтобы тоже как бы чистить начала подсвечивать вот но короче да вот я случайно нажал это вот все были ламинации на тему под названием как правильно бенчмарка те что такое одинаковый запрос одинаково запросто нет как правильно бенчмарках непонятно уступы разработчику собственно полнотекстовая системы и как бы вам bring беда вот поэтому вот как бы для того чтобы внятно benchmark хоть к несчастью надо хотя бы примерно представлять как все устроено внутри иначе ваш benchmark в лучшем случае окажется несколько значит некорректным с одной стороны в худшем случае покажет в продакшене внезапно характеристики ровно в сто раз хуже чем были на тестах с другой стороны ну типа кишки кончатся hit rate станет из 99 процентов один процент и станет всё не так вот обязательно в любом докладе как бы долг должно быть слово кластер как бы слово джейсон и кошечки ну короче со словом кластером не получается мы сейчас вообще без обеда останемся поэтому давайте ка каллас тираж вардинг и так далее обсуждать за обедом если значит интересным вот итоги подведем здесь тоже должен был слайд быть такой клевый в котором собственно итоге так вся выдержка доклада одним слайдам на его нет а хрен знает какие итоги как бы вот поиск устроены вот так значит я попытался рассказать как он в принципе устроен на физическом уровне словарики документ листы там внезапно сжатие она внезапно важно значит есть ровно 2 + о разные системы и мириадов всяких коммерческих систем на самом деле что те что другие концептуально устроены совершенно одинаково никакого прорыва в коммерческих тоже нет там внутри такие же слова реки списке и списке словарики и так далее все совершенно одинаково меняются тем не менее детали реализации ряд деталей реализации ну как бы тоже попытался подсветить что вот здесь томлюсь он делает так мы делаем немножечко мы и начали вот местами собираемся переделывать и так далее но и собственно из этих деталей реализации общих подходов к снаряду под названием мы хотим честно считать они хотят доблестно кеши еще возник возникают такие странные проблемы которые специфично мне кажется сегодня для поиска под названием хер ты меня за бенчмарка ешь у меня внутри такой кеш такой кеш что ты всегда будешь benchmark ведь только этот кеш вот как бы невероятно обидно но вот факт данной нам в ощущениях вот такой"
}