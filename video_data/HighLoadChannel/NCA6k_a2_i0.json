{
  "video_id": "NCA6k_a2_i0",
  "channel": "HighLoadChannel",
  "title": "Архитектура отказоустойчивого платежного шлюза / Алексей Павлов (Wildberries)",
  "views": 8836,
  "duration": 3258,
  "published": "2021-10-04T02:47:02-07:00",
  "text": "добрый день коллеги сегодня расскажу про архитектуру нашего нового платежного шлюза это как мы обеспечим его отказывалась точность я сказал архитектуру именно новую платежного шлюза потому что где-то год назад мои полностью переписали эту систему я сегодня немножко раскрою в чем были причины такого нашего решения расскажу про те принципы где требование которую мы раз закладывали при разработке нового шлюза и при этом у нас были ограничения потому что они прямым образом повлияли на получившийся архитектуру тесно покажу расскажу про нашу архитектуру акима используем технологии почему именно эти технологии расскажу про используем нами протокол передачи и хранения данных через урал запросов который позволяет нам корректном провод платежи даже в случае пол недоступности база данных а также поделюсь еще несколькими нашими техниками которые позволяют на повышайте слой наших сервисов начала немного ключевой информации компании как вы все наверное знаете в all.biz это интернет магазин marketplace самой крупной в россии в настоящий момент зарегистрировано платформе свыше 6 миллионов пользователей ежедневная аудитория составляет больше 8 миллионов уникальных пользователей помимо того что мы работаем на большей части территории россии мое последнее время также начали экспансии другие страны уже несколько лет успешно работаем таких стороны казахстан беларусь очень по советское пространство в прошлом году мы начали работать в странах европы и к словаки польша германия планах у нас положить дальше европы но впрочем не ограничится только европой мы даже сейчас уже работают этих странах как сша израиль если говорить про нашу нагрузку то это примерно 1 миллион транзакций в день это достаточно большая нагрузка даже когда ли кого рынка систем несмотря на то что мы уже сами по себе достаточно большие мы продолжаем так же активно расти последние два-три года наша составляет 100 процентов год году то есть мы с каждым годом продаем два раза больше товаров и сначала я немножко какую историю расскажу про наш старый шлюз собственно долгие годы вполне себе успешно работал вполне себе устраивал бизнес но я был небольшой нюанс потому что он принимал только два процента плат t5 оплаты которые пользователи через сайт доплачивали товары то время как большинство пользователей выбрали в качестве способа оплаты все-таки система когда они заказывали товар и приходили пункты выдачи смотрели выбирали что мне нравится что не нравится и оплачивали уже факту товары но год назад со всеми с известными событиями это все поменялось были введены ограничения на пришлось полностью перейти на предоплат ную схему и внезапно это сервис стал тут 2000 до бизнеса фактический отказ означал полный пол нарушение всех бизнес-процессов что не мало того что и проводились оплата не формировали заказы не были ни формировались доставки и было выгодно несколько ключевых недостатков старого шлюза вот несколько примеров самокритично это пожалуй пошли проблемы с отказоустойчивостью изошли достаточно сказать что в принципе это система могла работать только в одном дата-центры связано какие-то проблемы в этом-то центры ли какие-то козы приводили к полному downtime у всех бизнеса и возможно сделать быстро и частое обновление с большей архитектурная проблема что она предлагала что для того чтобы обновить шлюз начала надо было полностью погасить собственно загрузить обновления стартануть и там была еще целая процедура безопасности которая включала в себя то что определенные хранителей ключей должны были ходить и ключи безопасности только после этого шлюз мог стартовать это все привело к тому что обновлять эту тему могли не только по ночам понятно дело что мы не могли делать частота обновлений даже несмотря на то что мы обновляли это только по ночам когда у нас на меньше трафик но все равно это аффекта бизнес и как следствие этого мы не могли делать в интеграции фичи то есть у нас критически рост time to market а поскольку это стало критичным для бизнеса компонентом то естественно хотелось его развивать хотелось проводить новые интеграции и так далее также стоит отметить что этот шлюз это очень древняя часть системы она была написана на устаревшем ну для нашей компании стыке это серж апается сервер mssql настоящий момент мы почти полностью отказались от таких legacy систем и как бы не особо было желание в таком важном компоненте для бизнеса где-то держать отдельно этот пить технология и еще как один фактор со времени у нас просто выветрился экспертиза в этом стыке так что было как-то трудно найти ресурсы для поддержания работоспособности и было систем понятно как нам положить развивать эту историю как положите и масштабировать и посему было принято волевое решение год назад начать полностью переписывать с нуля эту систему мы изначально поставили во главу угла отказоустойчивость потому что на наших объемов заказов минут даже простое это огромные потери для бизнеса мы приняли как факт то что отказывать может кто угодно базы данных дата-центры банки мы должны быть к этому готовы мы должны даже в таких ситуациях успешно проводить оплаты мы должны были способны принимать сто процентов ей наши нагрузки более того как я уже говорил мы быстро растём и мы должны были заложить рост в десятки и сотни раз после того чтобы просто ближайшие годы не упереться в какой-то толк масштабируемости ну и мы изначально заложением в архитектуру мультитональность то есть мы изначально подозревали в такое мы разрабатываем такую отказоустойчивой систему то неплохо было бы на ней зарабатывать на продавать другим компаниям ну в принципе мы сейчас этим мы занимаемся в настоящий момент мы используем эту муть единственность в частности для того чтобы разделять наши apathy например по странам нашим дочерним бизнесом ну вот в планах и начать другие компании и стоит поговорить и об ограничении с которыми нам пришлось столкнуться при разработке первая из них это естественно самое главное это писять с с поскольку мы храним карты пользователь на своей стороне нас есть целый большой список предписаний связанных с безопасностью как мы должны храните данные все же связан с безопасностью secure ностью это на самом деле очень сильно отразилось на нашей структуре второй фактор мы должны были как можно быстрее выйти впрок это опять же связано с тем что пока бизнес сидел на старом шлюзе были большие риски связанные с тем что в какой то прекрасный момент может упасть и также были большие потенциальные потери прибыли и второе интересно интересно архитектура начинаешь это в том что у нас в распоряжении было только два центра то есть нам надо было построить мульти dc решение и у нас была расторжение только 2 до центра это начало куча головной боли как нам разруливать предприятий в случае потери связности между до центрами как добиваться концерт со при условии что у нас не работает один по центру в общем мы решили просто отказаться от кого-либо розница решения мы изначально выбрали этот актив архитектуру сейчас я объясню у вас есть два контура альфа и бета коре содержится свои из полностью изолированные костра обернется шей очередей и сидит если что их связывает это синхронная репликация который идет через наши хранилище всех это даже это репликация она делает со сторонним инструментом то есть опять же внутри этого контура нет никого здания о том что есть еще какой-то актив контур более того нам решение дает несколько бонусов в первый из них мы можем как угодно балансировать трафик между этими контурами полностью вводить 1 класс стр например для проведения каких-то технических работ чтобы это не аффект тела боевой трафик либо в случае ну допустим отказа того самого сев а то мы и могли безопасными перевести всех наших пользователей во второй контур и в это время лечить контур в к там есть некие проблемы также дает возможность нам делать изолированные apts ты здесь торт простейший логика вводим весь трафик например на альфу на поставляемую какую-то часть пользователь на которых мы собственно хотим от оси ровать эту фичу и постепенно мы можем переливать допустим пользователя между c как это работает у нас есть два по домена собственно альфа и бета есть глобальный дам нпвп точка ком который по дефолту смотрит дань с вами в один из этих контуров но в там по серединке есть небольшая хитрость маша можно сказать прокси которая прекрасно знает формате передаваемых данных может их парсить понимать из какой страны пришел запрос что это именно за юзера является например это юзеры корпоративно или нет это опять же полезно для bts то что мы запускали новый фичино сам лояльной аудитории наших сотрудниках и в общем у нас достаточно широкий спектр того как мы можем балансировать между этими контурами дальше я немножко поговорю о выданных нами технологиях в качестве хранилища как я сказал мы используем все все это по сути с определенное хранилища которые настраивается под поверх почти что голых тачек оно само по себе является очень надежным спокойно переживает отказы нескольких not репликации эти данные не будут где потеряны и в принципе она является самостоятельным решением не требует особо пошел и администрирования появляется сама страдающий самом менеджеры себя у него есть несколько плюсов первое это легко масштабируется и как по операциям так и диску то есть мы просто добавляем новый новую ноту федерацию и дальше происходит магия что сам все все это разрушит для себя поскольку всех оперируют бабами то есть мы можем себя придумать на стороне приложение к вагонов формат данных в частности мы используем дрю сон у всех есть три интерфейс которым можно взаимодействовать но по правде сказать ис-3 интерфейс там поддержку еще очень слаба и если мы хотим поддержку при помощи уже с листа то это придет это приходится делать полнитель индексы в этом самом севере от чего страдает performance то есть по сути свой стенд интерфейса там есть простейшие кипели операции там положить что-то подключу достаточно для кричу на самом деле я достаточно чуть больше поясню и также один из самых больших плюсов является то что у нас была большая картина в компании мы сиф и используем давно массив и используем удачно в частности вся мидия есть на валладарес картинки видео 3d модели они все все они сервис именно через wi-fi и небольшой чечерский такой плюс это то что у нас есть готовый инструмент для мастер мастер аппликация так называемой сивко в будущем коллеги обещали его закон сарсель побольше рассказать о нем на докладах но я вам расскажу лишь то что он дает нам мастер мастер аппликацию в н кластеров ну данном случае нас костра 2 то есть это был таким решающим пазл нашей системе стоит сказать почему именно выбрал была выбрана sefa никоим традиционно лицо на решение ну причина банальна просто очень долго просто для того чтобы построить грамотные реляционную базу данных мы должны знать формате наших данных в их отношениях если мы говорим о том что у нас очень быстрые сроки разработки то что мы должны как можно скорее выкатывать фичи то есть у нас сможет все поменяться просто щелчку пальцев это означает то что мы просто не сможем построить грамотное решение отношения авиационной базы вот здесь еще добавляется том что мы должны заранее задуматься как мы всю эту сторону штабе ровать позиционирование авторизации как она будет реплицироваться естественно это все бисер это все надо проверять какая там будет доступность какая там будет 40 потери мастера и сел здесь по сути является просто огромным булыжником который позволил нам забить гвоздь но в принципе это мой рады вот и здесь конечно же есть несколько минусов у этого решения во первых да это скудный интерфейс опять же у нас по сути есть де виллерс это положить по значению достать по значению и возникает вопрос а если мы хотим сделать какую-то выборку в сделку юта агрегацию здесь нас спасает то что у нас принципе системе есть дополнительная база в которых хранится информация транзакциях например аналитика мы в нее так также дописываем информацию о транзакции то просто она он является таким критичным для бизнеса компонентом вот так что там мы к ней можем меньше требований по доступности предъявить вот и просто из нее делать все необходимые нам выборки и проще и в парк второй момент что естественно у нас из всего исида остается пожалуй только последние буквы то есть что your ability что наши данная останутся в этой системе ну здесь тоже для нас это по сути не страшно даже это большой плюс потому что мы из-за этого не страдаем полосу мы из изначально закладывали в архитектуре что мы не очень-то полагаемся на данной базы мы изначально осознанно говорим что там могут быть никто стен ци это просто какое-то хранилище информации она может полностью не актуальным и в том случае когда нам нужно действительно актуальная информация мы идем напрямую в банк нашей квартиры было информации там берем актуально информацию например сколько было под раза кгц и возвратов только взошло денег подобная информация в качестве hd мы используем ребятенку ну это просто классика это за что понятно и решение не страшно много экспертизы есть много понятно кейсов но здесь пожалуйста что интересно то что мы его используем никак закончить массаж бас то что это означало что в случае отказа этого самого ребята наши бы транзакции застревали в ком то непонятном статусе он используется только как при тройер если мы не могли выполнить это очень сложной операции ну допустим отправка колбая к мячу там с финальным статусом либо опять же записать эту транзакцию аналитику вот пожалуй разберем небольшой пример мы попытались синхронно дописать транзакцию на всех мы получили там out возможно это какая то не доступность этого хранилища мы хотим все равно чтобы у нас в будущем эта информация по транзакции появилась мы кладем всю необходимую нам информацию в тоску в ребят есть драйвера которые учитывают общее со всеми танками естественно если все таки и мы смогли дописать отсека все выкидываем все успешно если мы все так же получаем неуспех там и на ком и постройке этой очереди она падает а называем очередь ожидания очередь ожидания никто не читает в ней просто тоска союз какое-то определенное время то настраивается через лет rtl и соответственно после того как проходит это время они на куются обратно в рабочую очередь это позволяет нам делать каких-то даст вас какой-то определенной периодичностью но это чревато тем что мы не можем бросить наших партнеров надо как-то размазывает трафике при для этого мы реализовывали треков то есть мы просто постепенно с каждым новым кругом неудачи увеличим этот дтл очереди ожидания и как вы поняли у нас может быть большая некроситет ность данных по времени и здесь возникает вопрос а как не пускать двойные списания мы не можем просто взять и пойти и спросить у базы потому что в базе может просто эту информацию не дописана нас есть нам на помощь приходит из сиди мы просто перец почему оплаты делаем уникальный ключ это индикатор мерчанта никто его транзакция добавляем его в виде проводим платеж с краном дальше прилетает платеж с точно такими фиксаторами мы смотрим видите сиди в туда такая попытка оплаты уже было мы создаем либо и нападать на отрезков те случаи когда мы можем достать информацию на заказ из базы либо в других случаях мы просто даем ошибку что это тугрики транзакции дальше мы накрутили на него некоторые еще другие функции частности на нем у нас стоит элемент почему-то timing не нашей стране к казалось мы вполне удачно справляемся с нагрузкой мы играть или метим именно обращение к нашим банкам как искал в тех случаях когда нам нужна точная информация транзакции мы идем за информацией банки кваритры так получалось что в некоторых ситуациях мы просто вида силе наших актеров именно для этого мы и собственно посолить и метро чтобы их в этом плане не досить также есть один интересный кейс что они из наших банков эквайринга впринципе не поддерживает кит параллельно обращение к одной транзакции то есть мы например не можем узнать статус транзакции не можно по ней провести возврат это приводило к интересам поведению стороне этого и quarro так что нам пришлось сделать еще распределенный мьютекс который посетили завала прощения к этому флаеру ну и что там еще есть это мы хранили сидит наполнителю конфигурацию просто того что ключи есть удобные ревизии можем спицами же там разными версиями эти конфигурации получается просто достаточно удобно также суть еще не авторе другие компоненты но вроде все понятно а просто кишит запросы когда там уже финальные статуса просто чтобы наши партнеры лишний раз не спрашивали по уже готовым транзакции и волк мы сможем того чтобы хранить секретные ключи ключи шифрования для наших карточных данных а теперь я хочу немножко рассказать про наше слово плат в данном случае я привожу пример именно оплата через ей ну в принципе и не суть важно подписанные вызову проходит через те же самые этапы то есть мерфи на своей стороне формирует платежную ссылку перебрасывает на пользователя он приходит на страницу нашего шлюза там он выбирает какой-то метод оплаты там сохраненная карта оплата новой карты после чего его перебрасывает на страницу где уже происходит сама плата мы следом транзакцию своей базе чем call back merchant у что чувак оплатил и собственно проводим самую оплату наших рекламе по после этого если эта плата новой карты либо это под на большую сумму срабатывают наши 3ds срабатывает 3ds и перебрасываем пользователя через redirect и на страницу где происходит у дефекация и после успешно либо неуспешной модификации мы обратно перебрасывает на страницу шлюз и где мы дожидаемся финального статуса по транзакции тему пытаемся финализирован опять же отправить колпаки успешно неуспешным статусом нашему merchant а там дописать в базу финальный статус и после чего моя обратно возвращаем пользователя на страницу мерчанта с информацией дополнительно транзакций успешно неуспешная какой там был тип ошибки и еще там какая-то информация здесь возникает вопрос а как если мы говорим что мы хотим проводить платежи без базы хранить эту информацию безопасно vrm хранить ее в только в фазе не подходит потому что это слишком большая зависимость для базы варианта что поставить кучу дублирую систему но к сожалению тоже не будет работать потому что в случае каких то например сетевых ботаников это дубль система тоже может упасть здесь мы подходим к идее что мы должны где-то на стороне клиента обеспечивать это fall так схему но приходит в колу например это локальное хранилище на клиенте но 10 мы просто я показал это именно на примере нашего я и но мы не всегда контролируем пользователя вот что просто партнеры пользуется нашим шлюзом там нашим api через прокси в общем мы никак сможем сейчас на пользователя cookies тоже плохой вариант потому что нас в это цепочка редиректов если несколько смен доменов собственно здесь тогда возникает такая идея отдавать это все хранить в углах ну очевидно идея что давайте засунем get параметры собственно для этого и созданы к сожалению тоже закончилась файлом потому что банки в call back your luck образуют все get параметры просто по той причине что они хотят туда совать свои и тогда мы придумали что мы будем просто все а кодировать попасть вот пример такой ссылке сгенерированный домен hp.com даже идет опционально лакане в данном случае это поиска вызываем метод прогресс payment это собственно то страница на которой пользователю отрисовываются его карты где ему предлагается провести оплату дальше идут формате кивали key value параметры оплаты есть такие обязательные как например не секатор merchant а потому что именно по этому параметру мои собственные проводим разделение оплат эфире катара юзера на самом деле отжали параметров конечно же больше это в валюта это сумма оплаты и так далее просто я здесь немножко укоротил также могут быть какие-то необязательные параметры например здесь вы сын юстас true то есть меньше чем на своей стороне зафорсил 3ds еще один интересный параметр это expressed то есть мы храним мы ограничиваем время жизни ссылки просто по той причине чтобы не было такой ситуации что пользователь пришел на страницу шлюза там забыла не и они на два дня а потом пришел решил оплатить умер что уже не товара и не совсем понятно как в роли в такие ситуации или того чтобы пользователь не смог например купить iphone и по одному рублю у нас есть удостоверяющие подпись мы подписываем всю часть начинает заметно и всех его параметров в очевидном от него хэш добавляем к этому хочу секретный ключ конкретного мерчанта с вами высчитываем подпись нашего запроса и собственно нам приходит такой запрос мы его просил мы считаем эту подпись с на если все сходится то замечательно мой пускать запросы при самом страницу если если подпись не сходится это понятное дело что есть это пошло не так мы просто говорю что нет мы не будем проводить такие запросы у это протокола есть свои минусы во первых что поисковые движки не будут ранжировать потому что как правило это будет просто слишком длинные url и также есть ограничение старых браузеров сном или 8 и 9 в 2 килобайта опять же у нас просто в очень-очень много параметров там 30-40 штук это все генерируются ссылки которые могут весить там 45 килобайта но и мы принимаем риск то что старые некоторые старые совсем браузера вот этими корректно работать современные браузеры могут десятки килобайт урла успешно обрабатывать принимать и прочим а также есть ограничение сторонних сервисов что они не могут просто все охране такие длины колдакт и урал но в данном случае мы большие мы можем продавливать другие банки лета чтобы они делали своей страны не отработки но если говорить в принципе это как протоколе то конечно то что я показал это была не самая эффективная потому что там у нас есть длинные человека понятные названия это все никак не сжимается и прочее при желании его можно оптимизировать даже у нас были кое-какие идеи но просто до сего момента мы справлялись вполне просто продали продавливая наших партнеров ну и также один больше технический момент это не типичный протокол требует своей реализации если стороны клиента стороны формированию ладно все понятно просто грубо говоря делаем могу проходимся по ней все это канаке теру им высчитываем подпись отправляем запросто страны именно сервер там есть сложности то о том как это все доставать как это просить у нас кнут какой-то момент про просто на каждый параметр было своей регулярно понятно дело что когда у тебя 40 параметров это 40 регулярка твой парсит эти параметры не совсем эффективно мы написали свою библиотеку который позволяет в формате регенерировать все это бани со структурной вот ссылка на нашу библиотеку а на самом деле здесь не совсем в актуальном состоянии есть какие идеи есть какие доработки наверно ближайшее время мы ее обновим но вот если вы хотите попробовать этот протокол то пожалуйста я рассказал о том как мы можем переживать отказывать от центра как можем прислуживать отказа база данных а первый скажу об это как мы можем переживать отказы банков мы выступаем здесь у нас нет своего каринка выступаем здесь такое агрегаторы нескольких и плееров у нас есть свои бизнес-правила как описано в таком джейсон языке который выделяется именно в какие игры будут road запросы из какие банки они проходили именно терминалы там это сделано во первых как для диверсификации чтобы в случае падения одного из банков нас были другие настоящий момент если я не ошибаюсь на схему или восемь различных банков эклеров а вторая причина это потому что просто все эти разные банки все эти они представляют разные тарифы на просто выгоднее кит тарифы в один бак банк какой-то в другой и соответственно для того чтобы нам снять нагрузку вата банка мы можем либо изменить сам роутинг либо там в админке подключать и квикфаер общем есть еще одна интересная особенность что мы при этом не просто роутинг какой-то fall back как правило это учитывай по тарифам самый выгодный fall back и существующих но это снять ручками или лезть в админку что-то включать это требует реакции у нас даже к я и говорил минута там падение может стоить большое количество денег поэтому нам нужен было моментальная реакция до этого мы рисовали патерностер как бейкер для банков то есть если мы видим что у нас какой то банк тайма учит что от него летят пятисотке либо какие-то ошибки которые мы интерпретируем как их системны мы просто его включаем на 15 минут там обычно дежурного за него это разработчика чтобы он посмотрел потому что тоже возможно что это могут какие то мисс срабатывания собственно понимается уже решение начать этот банк не отключать менять роутинг или нет но мы пошли дальше мы не ограничивает церкви бейкером только для банков мы в принципе подержим этот паттерн для всех наших вещей то есть мы видим если в какой-то flow у нас тоже возникают какие-то непонятные ошибки можем просто его трубить например если мы подключим какому стороннюю хранилищу карт видим что это хранилище идет все как неадекватно мы просто его включаем и того чтобы пользователи не могли через этики это как карты оплачивать ну потому что ему просто бы не была успешной оплаты и так же у нас есть церковь бейкер позволяет нам отключать какие компоненты с теми как рейде сайте сиди и даже нашу базу для того чтобы просто не не заставлять ждать по пользователя в этом случае у нас есть несколько резервных механизмов обработки которые дают нам койке меньше гарантии койке больше головной боли но они позволяют нам продолжать оплачивать проводить оплаты пример таких механизмов например очевидной что канала снизу нет доступно наше хранилище мы не можем нас все транзакции мы переправляем все запросы напрямую банк эквайринг и в обычном ситуации мы бы его задать или но опять же у нас есть еще ряд лимита которого это не допускает дальше у нас все всегда под сервисом есть дефолтная конфигурация говорил с у нас есть эти правила бизнес-правила роутинга всегда есть прямо с дюжиной вы нужно какое-то дефолтное правило которое поставляется в случае если возможно и его подгрузить в том случае если нас есть сиди не работает уйдем на рискованных оплат ну что для нас намного легче просто потом эти деньги вернуть чем терять эти оплаты также случае если у нас недоступны наше хранилище карт мы просто форсируем для пользователей оплатой новые карты до получается немножко плохая конверсия получается что мы всех отправляем на 3ds что тоже affected но мы не решаемся полностью оплат у нас есть локальное очередь запросов рабби том случае если будет какая-то под потеря с ним соединение в котором мы можем позже дописать когда он поднимется и у нас есть что дублирование тонизация плат в запросе и статуса такой небольшой fall back 2 даже если вы по какой-то причине не могли пройти все синхронная flow если по какой-то причине мы не смогли там положить в кролик написать через него привести формацию оплаты мы всегда может просто убить эту настройку для того чтобы все пролетающие нас таких тормозок шины проводили эту финализации и напоследок хочу рассказать немножко не про технические моменты а про организационной которые помогают в кушать отказоустойчивость потому что как показывает практика очень очень большой процент именно завязан на человеческом факторе первая из них это то что мы обязательно с серым в чинно вроде от тестируем их на отказ пассируем на то что берет ли их правильно searched бейкер правильно он распознает эти ошибки то есть мы здесь сразу не только то что у нас это новый интеграция правильно работает в нас случае чего может еще и правильно вырубится мы пытаемся приводить учение связаны стимулирование происшествий для того чтобы просто быть готовым знать как реагировать на такие случаи как отказа каких то систем отказ база данных чтобы просто не пока им не паниковать чтобы у нас у всех в голове была инструкция что в этом случае делать все инциденты которого возникают все пока по мы стараемся разбирать там находить причины чтобы находить как который какой-то готова алгоритм который позволял бы на в будущем с этими фактами успешно справляться и мы формируем базу зданий что делать если произошел x который позволяет просто в нее заглянуть если присходит как непонятная ситуация простая случаев просто найти совпадение а и получить готовый решение как с этим бороться вопроса так ваши аплодисменты алексей павлов и в лицезрел перес пока вы придумываете вопросы поднимайте руки наши хелперы вас найдут алексей тебе подарок от организаторов конференции поздравляем можешь поставить возле тумбочке если что уметь пару вопросиков из chateau возле player ace online а ребята кто смотрит нас онлайн тоже можете подключаться нажимать кнопку возле player и выходить в эфир итак у нас первый вопрос из зала давайте первый ряд слева привет вы рассказывали про распределенный мьютекс я немножко не до конца понял можете поподробнее рассказать что это как это работает но это как обычно мьютекс только распределены между несколькими инсцес и посредством и сиди кластера это было связано с ограничение в одном одному из наших банков эклеров который не позволял делать какие-то правельный доступа к на транзакции одним из примеров такого например мы хотим узнать статус транзакции и например полетай куется запрос возврата это могло приводить к ну не совсем адекватному поведению и собственно для того чтобы минимизировать такие случаи вот и был создан по сути такой метод которой ограничил доступ запрос об одной транзакции в этот банк player так хорошо следующий вопрос такой микрофон вставайте здравствуйте спасибо за доклад и я хотел бы спросить как происходило переключение между 100 старым шлюзом на новый то есть я так понял они очень сильно различались по функциональности и выиграл бы говоря просто выключили старые включили сразу новый и там были какие-то промежуточные шаги - нету это достаточно долгая процедура к вышло несколько месяцев первое что мы делали это мы мигрировали данные данный карт и там на самом деле был в двусторонний процесс потому что даже когда мы по большей части пришли на новый шлюз все равно были какие-то факты ситуации общаясь на старой поэтому чтобы просто пользователь всегда были грязные его карта если нам платил через другой шлюз вас было двусторонняя миграция то что касается информация по транзакциям просто на стороне сайта в стране в адрес было информацию шлюз было пройдено оплата через старый через новый то есть они на своей стране разваливали где им надлежит транзакция спасибо так у кого микрофон автора ряд я такой вопрос по поводу диплом это понятно что платежного шлюза есть определенные требования downtime может быть максимально минимальным как вы делаете deployment может быть это blue green deployment потому что у вас есть два контура как вы их называете ну могу рассказать про нашу процедуру дипломанта первое что мы делаем здесь трафик в один наши контуров например только на альфу мы в бета контур накатываем новое обновление и постепенно мы начинаем массировать pride передавая небольшой процент трафика на бету постепенно мы его наращиваем пока полностью не приходим на бету правильно мы смотрим на все наши метрики смотрим все ли в порядке и растут ли у нас флориды все ли в порядке у нас правление оплата с конверсиями если все успешно мы и полностью перекину тогда нагрузку на хорошую обновляем 1 потом снова сплитим трафик филипп спасибо за доклад как обеспечивается беседы с соответствия защиты хранения карт и что кстати вообще выходить и защищенного от конвоиров как понимает что в данных иришечка храните нет потому что это есть если упал центов то в общем все потеки которые на нее были они это если вы северини и распределять но мы не имеем права посреди сохраните villa mare имеете в рандоме вратами затратами да у нас есть верховный запрос оплата устно в рамках это запрос это у нас есть севильи у нас на самом деле и загорелся банками что мы далеко не всегда оплачиваю счета есть буквально это первая оплата так что нас есть договорилась что мы только в случае т.д. соответственно вводим этой серии ну просто вот так вот разрулю понятно дело что оплаты которые требовали севим это пример не можем отправить но это буквально очень очень малый процент всех наших оплат ну и все таки вот как защищать этих хранилище которых хранится карточной информации как понимать к нему хранилище карт прозвучало то есть как обеспечивается защита доступ к вы имеете виду ну в этом хранилище карт ну во-первых дел попить одесса с мы должны скрывать панель с помощью ключа шифрования которого не кого-нибудь чтобы доступа у нас разделен на несколько частей и он хранит у нас болте вал тоже кто хочет зашифрован то у него есть там свои хранители аутистом способом или двумя коробочками или как о и если честно я вот прям в этом плане немножко не подготовился я не знаю точной процедуры надо сделать спрашивать у наших админов точно-точно коробочкой так следующий вопрос пятый ряд так 1 1 у меня два вопроса 1 суточной 2 сырье 1 шуточные вопросы восстану клиент покупал систему крепления только по унитазу можно же такое купить на wildberries что можно купить систему крепления крышек унитаза а я видел этот ним до а второй вопрос у c поесть своей системы синхронного копирования почему они вам не подошли называют себя водомера ли у кого есть у шефа цифр типа но здесь такой вопрос что чтобы эта система как она будет работать студии потери связанности между центрами скорейшей просто нет реплицируют так ну вот наш вот это вот все в наш настройка она по сути все эти значения просто будет описывать в кролик экономики потом когда появится этому связность на их допишут и все данные синхронизируются то есть я еще ничего не рассматривал но у меня в плане нарисовать попку то есть вы не рассматривали стандартные системы цифра в плане асинхронной репликации я кажется это достаточно простая проблема и она срез солнце решаться сравниваешь с одной стороны с другой стороны что не равно таки думаешь ну так не всегда можно делать сравнивать транзакции в ну нет у нас бы так не получилось спасибо так а первый ряд простите пожалуйста такой момент пользователя он может и не вернуться от банка и не передайте сообщение которое закодирована в пасе расскажите пожалуйста как вы обрабатывать эту ситуацию передаете ли вы собственно магазина по альтернативному каналу эту информацию и либо вы можете контролировать эти ситуации на этапе оплате мы на самом деле делаем task в ребят которые опрашивается и квир дожидаясь финального сразу со случае если пользователь так и не вернулся со страницы 3ds то банк там через 10 15 минут просто вылить эту транзакцию приватном что транзакция failed dts но такси тут собственно мы получаем этот статус мы и дальше в бэкграунде просто делаем финализации также шлему талдыкина шомер чан также дописываем в базу файлы причину файла то есть но в этом плане пасут ничего не меняется почему вы решили отказаться от схемы когда через очередь в итоге магазины знают о покупке пользователь видит что него что-то оплаченное в магазине просто через за альтернативный путь но если правильно понял вы предлагаете делать связанность между нашими pattern им шлюзом и магазинам а мы изначально закладывали мульти than то есть у нас будет такая ортогональная система мы не завязываем ся именно на бал перес на взаимодействии с ним мы стараемся делать все таки с тем которая может обернуться любой бизнес все банки не позволяют через в руки и другие сигнальной системы получать обратную связь и платежа вы имеете ввиду что вы решили гипотез вы имеете виду что ловить калмыки от банков финальным статусом да ну здесь вопрос как банком насколько хорошо ну если мы рассматриваем такую вероятность что у нас будет какая-то недоступность неважно что то не факт что залетит call back здесь пошли вопросы о том как как будет вообще эти банки ретро из сколько там будет попыток мы не можем просто прошляпить этот момент так надёжней если мы и сами будем контролировать опрос банков получается так надежно что получается получается так надежно до для скучающих надежнее так следующий вопрос тоже первый ряд доспать говори говорил просто сейчас выведут техника спасибо за доклад алексей компании костес я так понимаю что это самый критичный сервис вашей компании поэтому оправдана наверное все вот те решения которых вы рассказали вопрос следующий как вы оцениваете наличие всех вот этих резервных механизмов организационным действий и всего прочего насколько дает это оверхед к разработке этого сервиса по сравнению там другими какими-то вашими сервисами среднестатистическими в компании спасибо ну говорить про других сервисов компании ну я не совсем могу потому что я не знаю как у них все это устроено я все-таки варюсь в своей кухня да это дает очень самом деле пошла вверх и то что надо в голове держать просто тысячи моментов да это естественно большая сложность в разработке в тестировании но она как показывает практика того стоит могу просто привести пример когда я нам сильно понадобится этим я рад у нас в прошлом году кажется в августе blanc массированное ddos-атака нас была практически полная недоступность то есть у нас не находили запросы в хранилище проезд виде кролика мы по сути один этот сервис сам шлюз он полностью проводил весь цикл оплат и успешно провел в канализацию то есть вот самое самое толкач на ситуации вы можете представить мы с ними вполне успешно справились ну это понятно но все таки вот ну насколько сложнее ваша какая-то субъективная оценка а в два-три раза сложнее теперь ну это просто цифра как бы допустим скажем в четыре с половиной раза аня спасибо так хорошо третий ряд алексей спасибо за доклад очень интересно интересует такой вопрос почему выбрали c просто исторически потому что удачный опыт почему не рассматривали келью базы не знаю кассандру mongo db потому что мы вот например пытались использовать цех тестировали его под нагрузкой и столкнулись что при высокой нагрузке на запись из-за ребаланса происходит проседание например производительностью очень сильная вот второй вопрос если можно сразу как бурить как вообще добиваетесь компактности работаете с этого дублированные платежи и все прочее или как вы считаете это артефакт и понял ну отвечай на первый вопрос там была целая куча на самом деле причина почему именно сев вот вы слайд здесь самое главное причина это пожалуй то что нас есть полста огромный экспертиза компания на самом деле мы и рассматривали другие завершения в том числе манга просто сев за это вот то что он есть очень большая экспертиза и то что это в принципе очень отказоустойчивая система нас вполне устраивал вы говорите рассказали про то что у вас были какие-то проблемы с были какие-то регулировки при большой записи но мы с этим не столкнулись наверно это спасибо админам что у них уже есть какой-то опыт они знают какие-то хорошие рецепты которые позволяют просто с этим не сталкиваться а то что касается вашего второго вопроса да мы стараемся все-таки делать импотентные ответы мы смотрим либо и сиди которые у нас по сути контролирует уникальность транзакции либо мы смотрим базу честно если уже есть такая транзакциях базе всяко мы можем просто достать информацию с базы и отдать ее к клиенту спасибо так ответил на вопрос хорошо а 34 ряд в белой футболке здрасте сергей меня зовут два вопроса наверно по одному задам чтоб не забывалась если альфа кластер у вас разваливает она бета в этот момент идет deploy все у вас в порядке будет ну в этом случае нам придет придется быстро откатывать второй кластер известно него проводить весь трафик но обычно не бывает такого у нас вдруг так неожиданно разваливается кластер все-таки есть какие-то маркеры которые позволяют нам заранее это замечательно при повышении latency там может быть какие-то возросшие тайм-аут и то есть для вас это не будет неожиданностью всякой если у нас будут такие такие непонятные маркер из базы мы в этот день старой все-таки не делать и плэй-оффе так разобраться причинах такого поведения сначала спасибо и второй вопрос и 10 у вас такой отказоустойчивые вместе себя на него полагаете стал быть он кластерный я так понимаю испытывали ли вы проблемы с лиз так называемый с тем что они протекают когда кратко временная недоступность 1 плод то есть когда машина возвращается эксплуатации считает что она синхронизировалась слезами там проблемы не непростительно 1 бистмастер скорее всего сталкивались но правда в том что мы на краске ни на одну из монет всем не закладывается в том числе на и сиди он просто отмирает мы просто будем проводить удары без него после у нас будет меньше гарантий пример вам возможность двойные списания но мы как готовы пойти на такие риски так ряд привязанности выступления у меня тоже 2 вопроса задай порядку первый вопрос может быть я пропустил а новый стек плане языка и бэг-энда какую новую музыку или еще что-то дает того сколько сервисов используется ну то есть тоже прекрасен архитектура вас или монолит и я бы не сказал что микро сервисов у нас просто есть несколько плеяды сервисов сам битвой несколько сервисов которые разбирают из ребят тоски это больше такой такие для сервисной архитектура вот если говорить о количестве то внутри спал по внутри пися dices который непосредственно занята платежами кажется это 8 еще есть несколько сервисов которые снаружи например которые занимаются аналитикой занимаюсь там облигации refan дав холодов понял спасибо и вопрос касательно вот этого протокола который хранит все в пути урала не считаешь ли ты что это с высоким но достаточно рискованная история потому что оно очень сильно менеджер менеджеры клиентским софтом так как запрос проходит множество разных поста си ён браузере происходит потом возможно чувствует a proxy потом кучи ещё каких-то странных свечей роутеров и в любой момент у каких то пользователей может выйти новая версия chrome или я быть браузера который случайно паком отрежет по 100 2000 символов и в итоге все сломается или какая-то прокси за которой сидят пользователя какой-то корпорации тоже пореже поттс и у них тоже все сломается насколько ты оцениваешь риск вот этого потому что это неконтролируемая вами ситуация контролируема и клиентским софтом iacapap ну да такая вероятность кажется всегда есть мы просто это совсем какая-то из теста маловероятная ситуация можно все-таки все браузеры все прокси они идут потому что они краски страус увеличивать эти лимиты ну впринципе принципе такая ситуация возможна у вас на это случай на самом деле есть какие идеи как укорачиваете труп как это сжимать там в пределах двух килобайт ну в общем если это какой-то момент и к ситуации настанет у нас есть идея как с этим в кратчайшие сроки побороться до пасибо а еще вопрос окей давай четвертый ряд итак у вас каждый продублированы про травмированны и еще упомянули что сначала вы проверяете транзакцию в двд потом в базе ну и допустим бы проверки в снг допустим может не пройти а вот эти depraved они крутые должны атомарные происходить может быть такое что транзакцию прилетела на два разных узла вот или они вас всегда вот как-то гитлером стики stations on till определят один и тот же узел понимаете да в нормально ситуация такова его и потому что проще если пользоваться и прибиты к одному до центру но бывают такие ситуации когда мы видим двигаем трафик между центрами например когда у нас идет обновление например канал сяду идут какие-то текст ради ситуации когда мы двигаем тофик до в эту ситуации возможна такая ситуация что мы записали ведь сиди в один кластер вас записали другом кластеры в этом случае у нас все равно как бы будет по яркими потребности что первым что from кластеры ну да я понимаю чем ты такие ситуации могут вполне быть но все-таки пока мы не стал пить такой массовой проблемой мы так и ситуацию разрулил что говорится руками внутри 1 вот это не может такое произойти внутри 1 вот это центр также не может зайти на донат на тварных узнал в пластик этаж транзакции и состоялась лишь интересно вот это центр такой не может произойти имеется ввиду что нет внутри не может призойти потому что три 1-го до центра у них есть единый кластеры cd и на и кластер сифа там не может произойти то есть каждой транзакции она бегает с и d вот это такой когда в kenshi вот такой проверки сейчас находилась плохо слышно каждая транзакция проверяется ведь с этой кластере да какая примерно лет инси миллисекундах у этой проверки весь честно здесь мне надо посмотреть на графики и а так направить не помню но в пределах нескольких миллисекунд это не лезут во многом не является чем при таком при большом количестве раза акции это не будете лес под луной на первый случай если это как-то за 5 тыс иди в то что там будет тайм-ауте так давать некорректно ответа то мы просто избавимся от соседей бой проводить платежи без него просто у нас до снится гарантия будет возможность ванны списание но опять же мы к этому готовы мы будем просто это сами разруливать лишь прошли платежи понятно спасибо"
}