{
  "video_id": "FmZnoQoO5k8",
  "channel": "HighLoadChannel",
  "title": "Борьба с мошенниками / Алексей Григорьев (OLX Group)",
  "views": 808,
  "duration": 2529,
  "published": "2020-04-14T11:12:35-07:00",
  "text": "меня зовут алексей сегодня я вам расскажу про борьбу с мошенниками и про поиск дубликатов слайду меня на английском я надеюсь это не проблема если что то непонятно говорить о расскажу по тропке сначала пару слов обо мне я там видно кто не знает чё там находится я вот до на самом деле вот это край мира мир беджан да все правильно сказали да я вот получилось так что я там вырос мы провел большую часть жизни учился в университете город достаточно интересные там многие надписи на русском и на идиш одновременно ну так душевно есть синагоги еврей по городу расставлены проучился там в школе в университете после университета переехал чуть поближе к центру тут примерно где-то нижний новгород я вот точно не знаю географию нижегородской области но в общем я нижнем новгороде прожил какое-то время работал там java программистом ну несколько лет там поработал и решил поближе европу то в польше там я тоже несколько лет по работал программистом потому что там нет надоело все там курс как раз на курс вышел промышленное учения подумал вот это то чем я хочу заниматься в итоге прошел курсы на курсоре мне оч понравилось ну и в итоге стал работать с аметистом уже в берлине ну да сейчас я работаю до посольством свой знаком с с дубликатами я начал с конкурсы на кадре как его такая платформа с конкурсами по машинному обучению где компании они подготовляют как какой-то конкурс данные и просят участников ну там сделать модельку и только лучше всего это делает получает эти деньги вот компания vita несколько лет назад они сделали конкурс по поиска дубликатов я принял участие меня коллега уговорил потому что я говорю по-русски а там очень много данных на русском у меня говорю попробовать ну я подсел на это ну итоге пятое место заняли но этот доклад будет не про то как пятое место занять на конкурсе про то что делать дальше вот у нас есть система какая-то какая-то модель мы получили на к клинку делать дальше это я сейчас вот тут тут логотипчик видно компания в которой я сейчас работаю если получше сделать компания называется улыбок и ул x группы вообще это такой родительский бренд есть несколько брендов под ним avi ты можешь слышали гладкого компания которая в америке достаточно известным и вот я работаю в компании weeks который посередине ну хотел бы добавить что это не совсем именно та система которую мы используем в al-exe это такая упрощенная версия чтобы понятнее было там нас есть завязка на инфраструктуру про который я тут упущу ну и какие то детали я умышленно опушу чтобы злоумышленники ним не могли воспользоваться этим и причинить нам вред начнем план такой сначала мы поговорим общая проблему как она возникает потом как я решить помощью машинного обучения и в конце поговорим про то как это реализовать как эта проблема возникает ну вот пользователь приходит на какую-то платформу это может быть что угодно какой угодно сайт новости блок социальная сеть какая-то или например сайт для объявлений ну и пользователь что-то публикует ну допустим пуска и объявление о продаже собачки но пользователь может очень сильно увлечься и сгенерировать вот создать несколько таких объявлений или например опубликовать что-то непристойное ну проблем много когда мы даем пользователям возможность публиковать что угодно может быть какой то какой то какие то объявление которые нельзя публиковать не за продавать запрещено и может быть что-то непристойное может быть дубликаты спам и мошенничество вот я хочу рассказать побольше про мошенничество в мошенничестве дубликаты они связаны из одного не всегда следует другое но часто мошенники используют дубликаты именно от для того чтобы обманывать людей как это происходит ну какой-то пользователь приходит на платформу публикует объявление приходит мошенник копирует картинку ждет пока объявление становится неактивным и публикует потом объявления на платформу возможно с описанием получше с более привлекательной ценой не приходит покупатель думаю тогда интересно хочу связаться хочу узнать побольше ну и звоню продавцу продавец мошенник говорит вот вы знаете у нас тут кроме вас еще 10 человек который хотя хотели бы купить собаку но если вы нам если вы нам приведете 100 баксов то есть собаку для вас придержим мы покупатель такой дунул 1 держи вот ну а потом мошенник просто исчезает с деньгами и пользователи очень расстроен как это можно предотвратить ну можно просто не позволять пользователю публиковать все что угодно ввести какую-то систему модерации но она работает простым образом пользователь что-то загружает какой-то контент объявления и она проходит через систему авто модерации система автоматизации может либо принять объявление либо отклонить в случаях когда система не уверена мы просто показывать объявления людям модератором в операционной панели и они уже принимают итоговое решение принять или отклонить объявлению вот эта система автор модерации она может состоять из нескольких компонент поезд непристойного контента пусть запрещенного контента ну и вот в частности дубликаты вот именно про это мы поговорим как вообще подойти к этой проблеме поиска дубликатов вот можно подойти к ней с помощью таких двух шагов простых на первом шаге мы ищем кандидат мы производим поиск кандидатов дубликаты то что не совсем дубликаты но скорее всего а уже на втором шаге мы ищем именно те пару объявлений которые именно дубликаты почему нам это нужно мы не можем вот нам нужны пары пара объявления 1 объявления 2 и мы уже хотим понять дубликат это или нет даже если у нас всего лишь 1000 строк в нашей базе мы не можем каждый сравнить с каждым потому что получается полмиллиона пар а если мы говорим о реальных данных там когда у нас 50 миллионов записей он просто физически не можем сделать поэтому нам нужно как-то отфильтровать кандидатов ну как это делает сам простое просто использовать какие-то эвристики какие-то знания предметной области чтобы по примерно прикинуть что может быть дубликатом что нет вот например нас объявление опубликовано биробиджане телефон iphone ну что мы можем сделать мы можем посмотреть и другие телефоны которые тоже прикованы в биробиджане возможно какие-то из них они дубликаты можем смотреть все объявления которые опубликованы недалеко от прокачана также можем посмотреть все объявления которые опубликованы именно этим продавцом или объявления опубликованные с этого api адреса или объявление опубликовано с этого девайса и все нам даст какой-то какое-то множество публика кандидатов дубликаты не все из них дубликаты но какие-то из них скорее всего дубликата ну и на втором шаге когда у нас уже есть кандидаты мы используем машины обучение чтобы непосредственно идти именно дубликат среди этих кандидатов вот у нас есть какие-то кандидаты которым сперва шага получили мы дальше спрашиваем людей могут быть модератором мы может быть кто угодно испрашивания вот у нас есть пару объявлений что вы думаете дубликат это или нет таким образом мы собираем данные то что нас получается данные to die объявления 1 эти объявления 2 и метко один если это пара дубликат и 0 если это парня дубликат дальше мы извлекаем какие-то признаки ну и потом все это кидаем в модель и модельном в итоге сообщает вероятность что пора дубликат или нет какие признаки мы можем извлечь ну можно начать простого разница в цене абсолютно относительная насколько далеко два объявления друг от друга были я не опубликованные с одной и той же локации или нет с одной тоже api адреса и так далее естественно у нас есть текстово какая-то информация которую лучше использовать заголовки описания и мы можем использовать разные техники чтобы понять насколько 2 описания два заголовка похоже ну вот простой подход называется бага forts это когда мы слова когда мы заголовки в какой-то векторное пространство помещаем и уже в этом векторном пространстве мы можем степени похожести посчитать вот у нас есть допустим 3 заголовка на продаем пиксель продаем iphone iphone xs samsung их дети просто берем все слова которые встречаются заголовках и все в одну кучу складов обучается такое множество то есть это слова которое повторяется как минимум один раз и затем мы вот представляем каждый заголовок точно нас такая табличка получается колонка слова строка заголовок 1 если слово встречается 0 если слова не встречается вот и таким образом мы можем превратить заголовки в виктора вместо того чтобы использовать единички и нолики мы можем использовать какой-то вес придать допустим если слово встречается очень часто возможно но не очень интересно допустим слова продаю продаю используется везде ему лучше меньше издать а слова пикселя на такое встречается реже поэтому лучшим большая стать таким образом мы можем представить заголовки в векторном пространстве и угол между двумя векторами будет показывать нам насколько 2 насколько два документа похоже вся kettler не это делается буквально вот в шесть строк в 5 все очень легко ну и это очень сильный признак для модели у нас все ясно есть картинки особенно для объявления это очень важно потому что продавцы смотрят именно на картинке в первую очередь прежде чем решить стоит ли связываться с продавцом или нет поэтому картинки а содержит себе очень важный сигнал что мы можем сделать картинками из картинок мы можем извлечь хэши есть несколько типов и шеи первый тип это криптографический хэш и такие как md5 они вообще рассчитанные на картинке она просто все файлы и идея в том что если хотя бы один бит изменится хэш в результате будет совершенно другим вот тут на картинке на примере вот мы взяли картинку поменяли чуть-чуть небольшую область их ешь это вот строчка которая внизу она совершенно другая ну в этом вся суть криптографических решений небольшое изменение в сша кардинально другу небольшое изменение в картинке кардинально другой хэш но есть перцептивные хэши они работают по другому там небольшое изменение в картинке она не ведет большим изменением хэши и вот в этом примере вот мы небольшими не в нем немного изменили картинку мы хэш от который napero строчки это дхш он остался прежним вот такие хэши мы можем использовать как они работают но это скорее что потом посмотреть код просто объясню на пальцах вот у нас есть картинка мы берем ее объем на региона какие-то и в каждом регионе в каждой ячейке смотрим на среднюю яркость пикселя ну вот мы видим что только вот ячейка в которой которые картинка была изменена там яркость меняется в остальных остается прежней дальше что мы делаем мы просто берем и старой колонки вынимая отнимаем 1 из 3 вторую и так далее у нас получается такой такая матрица с разностями ну и потом мы просто можем посмотреть на знак если эта разность больше нуля то это у нас труп получается если меньше фолз у нас получается такой массив ulyanov мы можем представить что вот у нас от массивы ли она под каждую строчку у нас в одной строке 88 бульонов можем представить что это биты и что-то нас такое то 8 битное число у нас восемь строк получается 88 элитных чисел им эти числа просто переводим fix соединяем месте и получается хэш простая обрести к работает очень быстро и не имеет таких недостатков как md5 и есть несколько шей не работает примерно одинаково там допустим для пихаешь сначала делается преобразование фурье а потом считается что то похоже ну так как эти хэши обладают свойствами что что небольшое изменение в картинке ведет к небольшими изменениями хэши мы можем просто посмотреть на количество бит разница между нами хэшами и понять насколько разные картинки не можем это использовать для признаков для модели и самое интересное мы этих ишим можем использовать на первом шаге для отбора кандидатов потому что если мы знаем что два объявления использует одну и ту же картинку это очень сильный ну мы практически уверен что возможно этот кандидат это дубликат это стоит на эти картинки вы на эти объявления посмотреть то есть мы можем этих еще использовать на первом шаге чтобы увеличить нашу базу кандидатах мы можем пойти немножко глубже использовать нейронной сети для того чтобы хэши и делать считать вот на этой картинке у нас глубокая нейронная сеть и вот у нее предпоследний слой мы можем использовать что получить векторное представление картинки то есть мы берем картинку это jpeg-файл например на три канала и в итоге у нас получается один массив ну какого то размера там тысячи например эти вектора не обладают такими тематическими свойствами что похожие картинки они находятся в векторном пространстве рядышком то есть если у нас две картинки автомобиля то они будут рядом с друг другом если у нас допустим двое животных они тоже будь рядом а машина от животного далеко ну вот если использовать такую библиотеку которая называется keros это буквально можно 10 строк сделать тут терраса сделает за нас он скачает нейронную сеть которая уже при тренированна то есть нам делать абсолютно ничего не нужно нам нужно просто загрузить этот код выполнить и мы получим векторное представление картинки но это пример с документации взят из кираса и там же написано что вот эта сеть в г.г. она очень тяжелая на пол любая занимает есть сети более легкие вот например мобайл нет она буквально там 10 15 занимает очень легкой и код примерно такой же получается ну вот мы взяли картинку получили векторное представление обычные директор они достаточно большие получаются ну вот если мы используем bms с прошлого слайда виктора получается размерности больше тысячи с таким векторами длинными очень неудобно работать поэтому мы можем использовать стандартные техники для снижения размерности такие как свд сингл singular валиде композиция вот мы получим вектор ab немножко поменьше время размеры 100 дальше эти вектора мы можем превратить хэши и использовать их точно также как прицеп тише как это можно сделать вот есть такая техника называется уосатч это вообще семействах вшей называется localities in teaching и вот один из таких вещей называется рандом projections идея состоит в том что вот у нас допустим есть какие-то вектора мы берем просто случайный вектор генерируем в этом пространстве и проецируем наши виктора на этот вектор и идея состоит в том что если какие-то картинки были похожи в изначально пространстве они будут похожи и в проекции мы тут видим что у нас машины они при проецирования остаются близко друг с другом мы можем таких викторов очень много сгенерировать и таким образом уменьшить размерность и преобразовать это хэш и это делается так с началом и вот буквально в 3 шага на первом шаге мы генерируем вот эти вот случайный виктора и мы сохраняем их откладываем сторону потому что их мы будем использовать для всех картинок чтобы считать хэши для для всего что у нас есть один раз генерировали сохранили используем всегда на втором шаге мы вот делаем это проецирование мы берем картинку пропускаем ее через сеть снижаем размерность и потом проецируем вот эти вектора которые у нас есть и дальше так же опять смотрим на знак больше или меньше и сжимаем это flex получается хэш у нас есть какие-то hashed что мы дальше с ними делаем нужно куда-то их положить какое-то хранилища данных чтобы можно было легко ими воспользоваться ну пластик search очень подходящая хранилище для этого и дальше мы поговорим как раз про имплементацию как можно использовать ластик search чтобы это сделать почему ластик search власти ксир чьи реализует такую удобную структуру данных который называется inverted индекс нам если посмотреть на обычный баз данных такие как масик или другие то у них реализован прямой индекс это значит что по айди мы можем извлечь запись пойди мы можем здесь и документа есть у нас есть допустим картинка сойди один и мы можем извлечь их аж этой картинке обратный индекс работает обратно он наоборот похожу позволяет извлечь эти картинки это именно то что нам нужно нам нужно уметь найти все остальные объявления в которых эти картинка встречаются буквально мы берем хэш elastic срочно возвращает все остальные объявления в которых этот встречается ну и делается это легко мы просто индексируем все этом говорим ластик search верни нам следует объявления в которых этот встречается работать очень быстро но мы знаем что хэши иногда они могут в пару символов отличаться а прошлый способ он работает только для когда совпадений идеально когда хэш совпадает прям один в один нога нам хочется уметь найти хэши которые похожи очень похожи на отличается там возможно в одном-двух символах что мы можем сделать можно взять наш кэш и порубить его на кусочки и дальше так как пластик сторож вообще предназначен для полнотекстового поиска мы можем сказать ластик search вот эта строка на самом деле это текст просто использую это как текст и таким образом мы можем просто все это проиндексировать порубить крыши на кусочки и потом наш запрос будет выглядеть как полнотекстовый запрос в котором мы также запрос любим на кусочки и используем и власти ксир чьи что у нас получается что на первом этапе у нас возвращается фишек которые совпадают один в один все четыре куска совпадают потом дальше идут хэши которая совпадает только в трех кусках из четырех и так далее мы когда у нас есть вот такой результат пластик search мы можем уже перри ранжировать этот список и и посмотреть насколько побитого отличается от запрос ну можно вообще всю работу на elastic серж положить просто проиндексировать все использовать такие запросы как more like this more like из работает таким образом то есть у нас есть какие-то записи мы все проиндексировали мы говорим вот власти search у нас есть такая запись верни пожалуйста все другие записи которые похожи на эту мы просто все бросаем ластик search и используем его для поиска . как это можно реализовать пользователь публикует какой-то контент объявление например с картинкой картинка попадает в хранилище файлов ну например или ps3 как только картинка попадает хранилище хранилища может опубликовать события в очередь что вот такая картинка такой-то файл был опубликован хранилище вот у него такое такое имя находится он там там дальше у нас в сервис есть который слушает на событие в этой очереди и вытягивать картинки он вытягивает картинки считает хэши для почетных и же можно использовать такую библиотеку и мачеха ешь буквально в четыре строки посчитает нам все перспективных же дальше сервис публикует полученные хэши в другого через и уже если это очереди они попадают во власти cosworth ну все это выглядит примерно так из хранилища попадают в сервис для подсчётах шеи сервис для подсчёта хэши и публикует в другую очередь из очереди попадают хокаге власти зачем нам нужно две очереди почему мы сразу волосик search не можем публиковать это нужно для того чтобы на пиковых нагрузках у нас несли club за то есть если набежит куча пользователи начнут загружать картинки у нас вот эта вторая очередь поможет как-то равномерно эту нагрузку распределить и не убить позу вот мы используем вот тут значок лямда означает что это используется и дпр это были лес и дабы с лямбда функции это такая платформа для сердца для свои числе ней и преимущества и и что вообще вот мне когда-то сами тесту не нужно волноваться о масштабировании просто мы ее туда функцию кладем виды бы если она то волшебным образом масштабируется и сворачивается когда нагрузки нет и вот она в пиковых нагрузках доходит до 230 одной картинке секунду вообще без вмешательства с нашей стороны очень удобно вот ну картинки не только вставляют не только создают они еще удаляются с платформы что с ними делать можно опять же воспользоваться хранилищем как только что то удаляется от туда у нас другое событие там другой сервис который эти события слушает и обновляет записи власти к таким образом мы можем понять что на платформе все еще активно и что нет да мы поговорили про нейронной сети как они сюда вписываются мы можем просто сделать ответвление еще одно в одном потоке будут считаться шеф другом потоке будет считаться недавно сеть зачем это нужно потому что нейронной сети и хэши они немножко у них такой как сказать они по-разному работают хэши считается очень быстро нейронной сети считается очень медленно у них требование разные по по железу они съели ци по разному поэтому логичнее их разделить как мы можем вот этот компонент и реализовать если для прошлого для шеи мы использовали лямда то тут все не так просто потому что если использовать нам да там есть определенные ограничения там типа что файл должен быть не больше 50 мегабайт запакованный не больше 205 распакованный если мы говорим о нейронных сетях то это там фреймворке такие как тендер кого они просто гигантские их очень сложно вате лимиты уместить но не невозможно можно там помучиться несколько дней таки ужать вот и в принципе получается достаточно удобно опять же автоматическое масштабирование не нужно волноваться об инфраструктуре и получается цена стоит это где-то 6 6 долларов за миллион картинок то есть если объёмы небольшие это возможно привлекательно но если подумать что у нас 10 миллионов картинок день индексируется то это надо умножить на 10 660 потом еще и для месяц умножить на 30 огромный кучу денег поэтому мы используем кабинете для конкретно нейронных сетей потому что там машина постоянно работают и купер найти с заботиться масштабирование получается намного дешевле ну и то есть если у нас какой то небольшой объем патрулям до удобнее потому что не нужно беспокоиться в инфраструктуре и кубер найтись это машины постоянно простаивать но если объемы большие машины на куб в кубе на эти тени простаивают и получается выгоднее поэтому используем комбинате ну и в общем все это общая картинка выглядит таким образом пользователь загружает какой-то контент потом этот контент индексируется и система автоматической модерации ну получается быть о том что вот пользователь загрузил такое и компонент этой автоматические системы модерации уже идет власти к search и подготавливать документ кандидаты делает скоренко моделью и уже принимает решение принять объявлений ли не в случаях когда система неуверенно опять же мы отправляем людям люди принимают окончательное решение и это окончательное решение мы обратно систему отсылаем чтобы можно было допустить модель ну то есть она вот так вот примерно выглядит но это в принципе основная часть доклада все то есть мы посмотрели про проблему мы посмотрели как ее решить и как это реализовать но и основные какие тезисы с этого доклада что в рот и дубликаты мошенничестве дубликаты часто связаны что можно использовать какие-то эвристики доменные знания чтобы найти кандидатов и потом уже использовать имел чтобы найти реальные дубликаты hаши это очень удобная инструмент для поиска кандидатов картинок нейронной сети могут помочь потому что хэши достаточно просто нейронные сети они могут передать как семантическую информацию добавить elastic search это отличная база данных для того чтобы такую систему реализовать и ws там до очень удобная платформа потому что она заводится автоматически о масштабировании но и она отлично подходит для хэш и потому что это очень просто вечно очень быстро считается но для тяжелых штука типа нейронных сетей лучше уже использовать что-то типа куприна tissot ну на этом все так это контактная информация вот еще тут если вдруг что то не понравилось докладе или наоборот понравилось можете qr-код прочитать и там добавить свои комментарии и там же будут ссылки на слайда вот теперь вопрос чел спасибо за доклад у меня несколько вопросов первый сколько времени вы храните ваши хэши в базе там всегда храним сколько это по объему ну и так по объему сложно сказать но здесь машинная ластик сердца и каждая 700 гигов то есть пока у нас процентов цены действуют хорошо так сколько у вас длина вектора готовы хранить думы виктор а не храним мы вот этого хорошо но 64 бита ну немного и какое расстояние хэмминга вы используйте q и поиски то есть какое количество ошибок и допускайте в битах так еще раз в на расстоянии хэмминга это количество ли да да да которых отличается сколько у вас состояния именно для поиска ну мы просто топ-100 например возвращаем и по лимиту куда и потом вот у нас это расстояние хэмминга она просто как признак для моделей машинного обучения уже модель что смотрите что важно что нет то есть не не такая не жесткая течка уже модель принимать лишь вы сравнивали алгоритмы поиска по тем же ключевым точкам puffy чем именно алгоритмическим они машинному обучению ну вот я участвую в конкурсе и там такие такие методы они показались а чуть хуже поэтому так как у меня уже был какой-то опыт в конкурсе который знал примерно что работает лучше что нет и мы уже вот на основе этого опыта строились и последний вопрос более специализированное решение кроме как elastic писатели находить вот есть такая система как forest это может слышали для поиска ближайших соседей она уже работать сразу на вектор на на викторине нужно делать хэширование вот оно удобно но в наши сайт реал обелить инженер с они не очень любят и и потому что мы постоянно segmentation fault а еще что то то есть если допустим мы используем и ночью приходит там звонок типа что-то упало они просыпаются и смотрят там какой-то код носи упал он думает мне мы лучше на elastix врач то есть творческая система которую все знают и любят его это 11 за плюсов по сравнению с более узкими специализированными решениями то что она были надежно ну зато они более оптимально ну возможно здравствуйте сергей зеленский спасибо за доклад некоторые изображения имеют в себе эту информацию и gzip коды там и так далее вот вы не рассматривали ли такую возможность использовать их к и стив еще ведь они плюс-минус потенциально позволяют ну построить что ли fingerprint устройства и в зависимости от форматов наверное но вот могло вам дать какие-то неплохие фичи вот вопрос смотрели не смотрели помогло бы не помогло бы какие форматы изображений у вас используется ну смотрели немножко помогает но так не то чтобы значительно почему еще не не исследовали это все полностью восстановится используется же птвп формата вот ну какие то используем простые вещи оттуда ну там опять же нет такого стандарта хорошего то есть очень сложно что-то такое осмысленной оттуда извлечь но так сходу не получается нужно достаточно много времени привести чтоб все эти разные форматы привести к этому виду вот пока пока ты еще одна из областей где можно было побольше время провести но что-то мы используем же мне на схожую точную пробку коллеги а потому сильно если как бы вы считаете виктора почему не храните вектора между ними цянь это расстояние то есть по фазанный ластик вам хорошо ранжирует именно похожесть изображение продажам ну не факт что то что то начать на этот символ она у вас должно быть она более похоже чем то что случается на 2 sim да то есть как бы вас там ранжирование наверное все таки этот поиска не совсем ну да поэтому мы берем много кандидатов . сохранить а потом между ними там что-то прочитывать рассчитывать фоне и да конечно можно было бы но вот у нас сейчас власти хранится миллиардах и шеи если мы будем в миллиард гектаров хранить то у нас никаких что него взять милости что-нибудь более рис подсовывая и хранить передачи то есть виктор ратушняк или групп границ сначала сварить хэш ипатову потом покажем уже рассчитывать виктора и поезд будет более релевантный ну да да да да да ну возможно да но пока это достаточно большой выход и то есть вот я как сказал вместо что выбрать допустим топ 5 топ 10 мы просто можем топ-100 взять там какая-то ошибка будет но в итоге там что то что если на 3 бита отличается она может там где-то выдачи все равно будет иметь пмм спасибо за доклад собственно не очень понял на тему до обучения нейросети в процессе принятия ручного решения важен и и россетти собирают фичи что вы именно до учитель я не сам да я в этот момент возможно не приговорил не проговорил прошу прощения нейронная сеть мы используем только для извлечения признаков с картинок то есть у нас модель там у нас деревья мы просто можем при тренировать эту модель и все в одну это как-то откалибровать что вы там наша система не упала но по звона с ним нейронная сеть используется именно для принятия решений отклонить или принять объявление то есть до обучения вы имели ввиду скорее откалибровать нейронную сеть чтобы она соответствовала до учения я имел ввиду что вот у нас допустим появился новый тип дубликатов и как то чтобы вот как-то реагировать на это сигнал то есть мы видим что модераторы начинают допустим принимать какие-то типы объявления чаще чем раньше чтобы упростить работу модераторов мы просто перед тренированные 3 него рого имбирем более свежие данные и более свежим модель тренируем то есть это не онлайн происходит из кореи так оффлайн ваш процесс вот добрый день и спасибо за доклад скажите я так понял было обучение с учителем раз у вас модераторы до использовались граненые люди а какой сколько сэмплов им пришлось заполнить ну когда получились хорошие результаты примерная оценка а насколько много ручной работы пришлось мне сделать ну тысяч 20 30 спасибо деку не пробовали использовать чтобы ускорить и расчетов решению но рыба у нас проблема потому что у нас но не так просто интегрировать нашу существующих в районе склад столько машин а вот ну то есть конечно пробовали но пока у нас и посчитается но в принципе так как у нас все эти легкие тают не проблема спасибо тут просто до предыдущему вопросу хотел узнать какое качество волос ну то есть вот насколько эффективно работает алгоритм прежде чем вмешивается непосредственно ручной режим стану вы оцениваете это качество до оцениваем вот у нас при сезон он где-то 90 процентов вот но вот приколом посложнее оценка потому что нас пара и вот что мы пропустили что не попало в кандидаты это очень сложно оценивается поэтому меня чисел нету но есть какое-то такое ощущение что она так ну да да но с цифрами вот сложнее престижен можно посчитать потому что это по сути все что мы что можем модератором нынешнем но потом можно посмотреть там лагерным а то что мы не увидели это сложно очень оценить поэтому тяжело вопрос у вас там на картинке был нарисован и что в параллели вырабатываются линдой и сетью и там действительно все данные обрабатываются или орды сетью чтобы принять решение то есть и у вас скорость обработки лямды и сети получается одинаково результате все обрабатывает ну да все обрабатывает но для она быстрее работает конечно потому что мыши очень простые сетки потяжелее то есть нам нужно тогда в параллель их обрабатывать вот получается что все равно все о надо обработать а потом позже принимать решения или какие-то вещи простым и можем с хэшами сделать без за нейронных сетей потом как-то и казани быть обработка дней рода как-то спасибо да вот если представить такую воронку как бы то есть есть какие-то простые алгоритмы есть более сложные алгоритмы с привлечением сети и так далее эвристики всякие вот вы оценивали какая доля покрывается каким алгоритм не nikon то или вы просто комплекс на это всем plex надо сегодня спасибо не подскажите что делать с легальными дубликатами ну точно примеру вас там была картинка его на многие же используют промо-изображения то есть что с ними то делать ну например можно запретить использовать такие брожение забрали у вас запрещено не везде а как тогда ну страдать спасибо за доклад у меня вопрос а как вы видете работу спа souls позитив и фолз негатив результатами которые однозначно отправляются в except левый reject вот фосс позитив я уже сказал что это вот престижен мы в принципе можем посчитать перед тренировать сходство этих сложнее потому что мы просто не видим потому что это то что не попало в наши кандидаты вот там уже сложнее оцениваете ну какие то просто можно какие-то там ну иногда люди жалуются вот этот дубликат мы то есть мы смотрим уже так же на флаге пользователей то есть можно там если пользователь приходит на платформу то можно нажать 2 сказать что вот это типа дубликат мы уже смотрим на это то что то спасибо за доклад а скажите пожалуйста как система отличает что все найденные дубликаты являются фродом от некого оригинального не мошеннического объявления или все найденные дубликаты являются умника мошеннической серии объявлений ну там в принципе можно начать с того что просто запретить дубликаты да то есть если даже это один тот же пользователь не мошенник просто хочет как сказать свое объявление то есть можно он просто публикует новые объявления на выдаче появляется раньше потому что новое то есть можно начать того чтобы просто запретить это есть не смотреть какой из них в рот какой из них нет но обычного у нас такая простая листика что если его такая он пользователь один тот же то это вот пользователь просто пытается не заплатить деньги и ну обмануть нас так скажем так просто свое объявление больше посетителей кингу лечь вот если уже аккаунт разные то скорее всего это фрод вот пока так ну и потом у нас есть система такая дополнительная которая говорит типа угнаны это аккаунт или нет там какие аккаунты связаны с этим аккаунтом то есть мы в принципе у нас есть anti-fraud команды которые можно сказать что диск не то фронта это просто просто пользователь который ну пытается там пошел и купил симку в переходе например и пытается свои объявления ещё раз публиковать просто потому что дешевле чем платить за промоушен объявление вот ну где то 50 на 50 вот пользователи просто которые покупать симки и именно мошенники добрый день и спасибо за доклад у меня такой вопрос а вот как-то получалось считает условно говоря от одного бота я не знаю или мошенника вот сколько идет дубликатов в существующих объявлений то есть мне просто интересно кому их целесообразность именно копирование чужих объявлений они ну как бы они сочинения своего то есть условно говоря то есть довольно не сложно взять и придумать типовое объявление продажа iphone а то есть вот это ты уже не объявление не за детектив и считается ли такое например ну допустим когда первый раз эта система картинка попадает нам система но это сложнее у нас конечно есть какие-то базы картинок которых у нас нету мы можем проверить там ну да и сложнее но часто мошенники они люди ленивы и ну и картинок в интернете ограниченное количество они перес пользовать картинки и тогда уже ну иногда они постоянно но у нас есть внешним газа спасибо за цикл от бы тут вопрос у меня задавался все ту же нарезаем про него ли не про него то есть вас параллельно работает лямда и не рамки работают с ним ансамбле работают по нему как бы дают вам фичи или как они просто параллельно работают и какие-то фич мы используем сразу же от тех и других и какие-то потому что вот хэши не более легкие то есть мы можем использовать сразу и подождать пока результаты нейрон ок да и сравнивали как бы как они по отдельности работы потому что я предполагаю что adverse орел всякие атаки повернуть картиночку там чуть исказить хэши может очень сильно прямо до не читабелен для пригадаю и не сравнивали как бы эффективность нейрона который фича выделяет там сын не рамки эффективных не сильно да да то есть хэши лоб а по большому счету если есть возможность и рамки использовать то этого будет более меньше достаточно данных и шик какой-то на первом шаге мы можем сразу же уже ну не знаю там господину этого то есть мне там же , малыш чем-то таким понятно спасибо за и так заклад вопрос такое если мошенник заплатил за провоз своего объявления но у нас ну да приходится больниц но не все счастливы деньги возвращаем да да на политика такая удача деньги и так я понимаю что вопрос у нас кончились алексей какое у нас самый интересный вопрос тут не вот вопрос встаньте пожалуйста мы вручаем вам приза самый лучший вопрос можно поаплодировать и вам алексей огромное спасибо за вклад и все что вы для нас сделали спасибо большое всего"
}