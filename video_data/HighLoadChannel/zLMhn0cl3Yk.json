{
  "video_id": "zLMhn0cl3Yk",
  "channel": "HighLoadChannel",
  "title": "Архитектура надёжной In-Memor-СУБД на примере Tarantool / Владимир Перепелица (Tarantool, VK)",
  "views": 504,
  "duration": 3249,
  "published": "2023-10-06T07:17:07-07:00",
  "text": "Всем привет Так Звук есть отлично Да я гораздо чаще подписываюсь как монс нежели именем и фамилией Сегодня я хочу рассказать про базы данных в целом на примере конкретного решения тарантул о себе немного Я обладаю большим количеством должностей которые я просто подбираю под конференцию я занимался архитектурой я занимался продакт менеджментом я евангелизирую Тарантул довольно давно и это происходит потому что Тарантул я использую более 10 лет причем Я его использую как основное хранилище как базу данных настоящую Я знаю что многие стараются быка 5 мммере решение там сохраним вот то что у нас в настоящую базу данных так вот у нас Тарантул используется как это самое настоящая база данных Сегодня я хочу рассказать о том Почему я не боюсь использовать Тарантул как настоящую базу данных о чем мы поговорим Зачем вообще как памяти превращается почему это надежно поговорим про репликацию и немножко затронем тему с мульти-вершинком корыте Control слайды будут не обязательно пытаться все быстро фоткать в конце я дам ссылку вы сможете открыть на Google слайдах найти все что вам будет интересно Итак Зачем нужны вообще in Memory базы данных Почему нельзя взять какой-то дефолтный сквель какой Вам нравится и напихать ему кэша на терабайт вообще для этого надо откатиться в прошлое и посмотреть откуда вообще взялись Memory BD когда они появились вообще вся проблема в том что изначально все базы данных большинство которые исторические они дисковые диск это медленно то есть работа с диском очень медленная в сравнении с центральным процессором с памятью памяти было очень мало И поэтому память использовалась исключительно как кэш но в начале нулевых сервисы росли пробивали там миллионные отметки по пользователям и появилась потребность в чем-то быстром первое in Memory база данных вот полноценной Memory база данных это таймс Темп он был разработан в 98 году То есть где-то еще в 96 эту потребность осознали за продали потом в 2006 его купил Oracle Мне кажется Он опередил свое время плюс он приедет армии то есть его нельзя просто так схватить попробовать запустить на нем Пэт Проджект но тем не менее это я бы сказал первое полноценное море база данных на сегодняшний день уже не очень популярен возможно проприетарность сказывается а бум in Memory можно сказать начался с моим кэшар в 2003 году Leave Journal Я думаю давайте так кто знает Лев Journal OK Лев Journal это такой сервис блогов для тех кто не знает старый испытывал нагрузку и классические базы данных эту нагрузку не вывозили они сделали себе кэш изначально на перле потом переписали на C максимально простое API клиентская шарнирование но как бы он решил их проблему после мкша появились еще какие-то решения то есть по мере в течение времени появлялись все новые и новые решения которые приближали саму технологию к тому чтобы это было базой данных они писались на C они писались на Джаве то есть в основном То есть когда мы говорили про и Memory Memory это про скорость а скорость это вот либо это интерпрайзный мир где все пишут на Джаве поэтому даже высоко нагруженные решения тоже пишут на Джаве хорошо это или плохо не буду судить В общем сами решайте значит почему не взять Вот этот любимый ваши SQL с каким-либо крышом Казалось бы берем мускуль хорошая база наверное даем ему терабайт кш сейчас сервера большие памяти вообще вагон можно и на 2 терабайта почему это не будет работать также хорошо как какая-нибудь специализированная in Memory потому что изначально базы данных задумывались под диск под работу блоками с диском для них память это кэш кэш нужно проверять на консистентность инвалидировать работа с диском всегда ориентируется на ввод-вывод а не на пропускную способность на количество операций работа с диском требует трейдов требы трейды требуют синхронизации В общем там оно просто наматывается одно на другое и просто кэш во-первых расходуется не так эффективно Как можно было бы то есть соотношение мемори Фуд принтера например Тарантула там где-то один к пяти Для одинаковых данных а ну и собственно что можно с этой информацией сделать почему то есть можно обеспечить монопольный доступ к памяти то есть прочитать память это очень быстро намного быстрее чем диск можно не отдавать чтение из памяти в другой тред можно просто прочитать и выплюнуть это очень эффективно можно сделать оптимизацией конкретно под память можно сделать специализированные локаторы и в целом скажем так ориентироваться на конкретный тип нагрузки то есть дисковые базы Они многофункциональные то есть мемори можно сфокусировать Итак in Memory Нам нужен для того чтобы была производительность большинство решений для in Memory дает сотни тысяч РПС с одного ядра при этом это такие коротенькие запросы LT пышные некоторые решения могут развивать до миллиона РПС с одного ядра например Тарантул за счет чего нет чтения диска собственно все пишется линейно Кстати да транзакции в том числе и пишущие нет необходимости кэшировать мы просто читаем из памяти и применяется некоторые идеология массового обслуживания про это мы позже то есть и того мы взяли классический подход с обычной базой данных которая давала там условные 10 тысяч РПС прошли через историю когда у нас появился кэш система усложнилась там было много Хлопов возрастало латенсе и трансформировали эту систему к потому чтобы вместо того чтобы усложнять систему вернуться к исходной схеме Когда у нас есть база которая по характеристикам не хуже вот того кэша но она упрощает нашу архитектуру целевую приводит ее к архитектуре приложений и база данных но на порядок быстрее вопрос Чем мы за это платим У всего есть своя цена всегда То есть если мы где-то приобретаем то где-то значит мы что-то потеряли цена здесь простая все в памяти все наши данные весь dataset в памяти если мы начнем класть на диск мы уйдем по пути дисковых БД поэтому мы должны быть к этому готовы Итак как пройти путь от кэша к базе данных я расскажу это на примере уже упомянутого Тарантула На текущий момент только в паблике более 10 лет суммарно наверное 12 в разное время он был разным То есть это был и Кэш и база данных и очередь заменитель редиса им конечностероидах в общем его много и по-разному называли Давайте я вам расскажу чем он был изначально Это было просто чуть-чуть улучшенный мем кэш с оптимизациями очень быстро его сделали персистентным кэшом чтобы решить проблему холодного кэша но персистентный кэш это уже почти база данных не хватало только репликации к нему добавили репликацию и вот он уже превращается в такую киевелью базу данных но когда мы храним какие-то данные у нас обычно есть ключик есть данные данные большие и большинство Киеве или решений страдает от отсутствия так называемых вторичных индексов Да если вспомнить может быть кто-то помнит как использовался для высоконагруженных решений там убирали убирали различные ключи убирали все оставляли только праймерики и данные Ну по сути киевелью но в большинстве задач мы хотим иметь множественный доступ к данным у нас память каждый вэлью который там лежит он дорогой поэтому мы хотели иметь возможность нему приходить по разным путям Как прийти от классического Киеве или к мульти индекс вообще довольно просто если у нас есть один ключ указывающий на данные на этот ключ указывает какой-то индекс если у нас есть два ключа то к каждому из ключей можно построить свой индекс но при этом эти Ключи будут указывать на одни данные данные можно собрать вместе с ключами в такую структуру под названием у нас она называется Table cartech каждый называет ее по-разному суть не меняется Это слаботизированный набор полей и к некоторым полям У нас есть индексы с простейшем случае это киевелью в сложенном случае практически реляционная база данных Итак Тарантул продолжает То есть он стал мульти индексной базой данных в нем докрутили Гарантии осид и в целом в тарантуле есть еще возможность писать код на скриптовом языке он продолжает развиваться в него привозят Кооперативный runtime который позволяет вообще писать все что угодно хотите по хттп сходить из базы данных Да пожалуйста это все развивается в некую в некоторую такую платформу которая одно время называлась база данных и сервер приложений или сервер приложений база данных на сегодняшний день мы это называемый платформой потому что вот оно очень хорошо попадает под эти определения а самые важные аспекты которые вот превратили Тарантул из когда-то кэша в полноценное хранилище это персистентность это репликация это наличие множественных индексов без них он бы оставался таким вот кивелью когда все равно давайте рядом поставим базу и хорошие классические привычные гарантии гарантии в решениях достигаются весьма интересно мы можем отказаться от многотрейдовости С отказом от многотрейдовости у нас убегает всякая блокировка мы можем любую транзакцию упаковать до очень короткого Кванта времени когда мы просто из памяти хватаем данные смотрим хватаем еще трансформируем записываем Вот все это короткая транзакция по своим свойствам обладает уровнем изоляции сериал потому что они укладываются одна за другой влог Ну и собственно оно все пишется оно пишется надежно у нас нет параллельности поэтому у нас самая идеальная изоляция в целом этот подход позволил соблюсти Гарантии и дать производительность если говорить про Кооперативный доступ то есть может показаться что Кооперативный доступ это медленно но на самом деле современная память может выдавать миллионы операций в секунду и в целом скажем так время этой одной операции очень небольшое поэтому можем просто скажем так не пытаться параллели для нас параллелизм обойдется дороже чем просто прочитать из памяти и выплюнуть То есть если мы просто выстраиваем все запросы в цепочку запрос пришел становится в очередь пришла его очередь он читает данные из памяти который он хотел и отвечает мы получаем тот самый условный миллион РПС да плюс там будут накладные расходы еще какие-то но в целом транзакционное ядро базы данных вообще что такое база данных база данных это индексы потому что без индексов можно фэйл писать а то есть мы пришли за данными мы сказали что мы хотим нам дали что мы хотим мы ответили то есть вот эта функциональность может развивать производительность от миллиона rps дальше мы её будем расходовать на разные задачи плюс отсутствие нескольких потоков конкурирующих за данные дает нам возможность писать более простой код отсутствие блокировок mutex влог Free то есть Оно конечно есть там есть вспомогательные треды об этом еще будет но в целом ядро базы становится простым и эффективным Как устроен Тарантул В общих чертах все данные лежат в памяти любой доступ к данным осуществляется через индексы доступ к данным осуществляется строго из одного потока при этом любые изменения пишутся через в райп лицируется И периодически для ускорения времени взлета сохраняется консистентность снапшот я сейчас покажу схематично как устроен Тарантул Это довольно неплохо помогает понять происходящее Итак у нас есть главный поток мы его называем транзакционным потоком или TX внутри этого транзакционного потока есть большая выделенная область памяти управляемая специальным собственным локатором называется Арена на арене размещается Арена используется для хранения спейсов Space это такой примитив типа табли скеле который хранит таплы или строки к этим таплом к этим спейсам построены индексы индексы хранятся Там же в памяти под это используется та же самая Арена со всеми этими локаторами для работы внутри для организации работы используется событийный цикл Для более простой работы событийный цикл соединен с механизмом корутин наша корутины называются файберы это альтернативное название Если Вы посмотрите просто синонимы каждая каротина может создаваться при подключении Извне То есть у нас же база данных Мне нужно прийти с ней нужно что-то поделать поэтому для работы с сетью выделен отдельный тред он может принимать входящие запросы как-то парсить их обрабатывать передавать в транзакционный трек и там они исполняются в этих фейдерах строго последовательно один за другим потому что Faber - это Кооперативный примитив Fiber может взаимодействовать со спейсами с индексами что-то читать что-то писать если Fiber модифицирует данные эти данные пишутся во врайта хедлок или xlock писать на диск напрямую нельзя потому что мы помним диск это медленно диск блокируется с диском нужно взаимодействовать строго через отдельный трек У нас есть отдельный тренд задача которого писать на диск то есть транзакционное ядро никогда не касается сети никогда не касается диска Иногда нужно сохранить снапшот всех данных для этого используется отдельный системный файбер который называется snapshot Diamond Он просто в определенный момент берет консистентный снимок Ну скажем так части арены то есть текущих данных и сохраняет их снапшот файл если снапшот файл прочитать можно быстро восстановить состояние базы как я уже сказал писать на диск нельзя и для записи на диск для многих других задач подобного рода есть фаберный пул который хорошо интегрирован с этим событийным циклом То есть с диском с какими-то блокирующими операциями с проприетарными библиотеками которые там блокирующие можно задействовать через fiberloop redpool Значит эта схема вот одного скажем так инстанса Тарантула то есть мы его можем в таком виде запустить он будет работать приблизительно так откуда здесь берется производительность то есть вот я здесь показал то есть мы работаем с диском еще мы там работаем сеть системные вызовы в общем-то дорогие и для достижения производительности используется так называемый батчинг или упаковка многих запросов в Мы возьмем немножко растянем указанную схему возьмем отдельно сетевой тред возьмем отдельно трек который Пишет Вал и транзакционный тред Они между собой скажем так общаются конечно же Итак рассмотрим paypeline У нас есть входящая нагрузка входящая нагрузка принимается сетевым тредом очевидно то есть поднимаются сокеты акцепты все чтение запись то есть там тяжелые силы причем это отдельный тред их можно масштабировать если вдруг мы уперлись в сеть Хотя такой сетевой нагрузки которая бы прогрузила TX мы не испытывали то есть были редкие кейсы Когда нам понадобилось более одного труда для сети для того чтобы передать данные ftx нам нужны некоторые структуры то есть здесь я их обозначу некоторыми буферами То есть у нас есть буфер на стороне сети и входящие запросы в рамках одного это одной итерации событийного цикла укладываются в этот буфер мы пока ничего в TX не передали когда итерация событийного цикла проходит все что Накопилось в буфере передается в транзакционный тред пакетом в целом на передачу txt Red мы тратим некоторые скол Возможно это не всегда но мы его тратим один на некоторую пачку запросов то есть в целом транзакционное ядро потратило меньше одного сезона на обработку нескольких запросов А я напомню что транзакционное ядро это и есть суть базы данных и именно оно отвечает за пропускную способность Итак дальше эти запросы по очереди через файберы обработались поработали как-то с прессами возможно провели изменения Упали в виде изменений в аналогичной буфер на передачу на запись они еще не записаны но транзакционное ядро отработало больше скажем так процессинг этих запросов не будет отъедать у него ресурсов запросы также пачкой перекладываются в вал тред вал тред собирает их в один блок и пишет на диск здесь у нас тоже системные вызовы которые используют много запросов один Таким образом мы можем сказать что здесь у нас используется подход массового обслуживания то есть который используется например в транспортных перевозках У нас есть какая-то штука которая может перевозить много за один раз тратя Малое количество ресурсов то есть мы тратим меньше одного из Кола на обработку одного запроса в транзакционном потоке мы тратим меньше одного из Кола на запись одного запроса тем самым мы можем развивать большую скорость записи запись у нас Линейная линейные диски пишут быстро в общем-то все прекрасно и мы тратим скажем так 0 обращение к диску на то чтобы прочитать данные то есть обычно база данных тормозят когда они начинают читать с диска потому что диск Может начать тормозить здесь для чтения диск не нужен мы читаем все из памяти Итак Давайте попробуем подытожить то есть мы рассмотрели вот paypline данных как это все идет через Тарантул то есть мы поговорили про сохранность сохранность это пришла запись мы ее записали в память мы её записали на диск подождали подтверждение при этом чтение у нас спокойно читают из памяти они от диска не зависят и они работают гарантированно быстро и мне во что упереться они работают только с памятью после перезапуска инстанса процесс читает snapshot читает xlogied восстанавливает все изменения все транзакции строят индексы и в общем-то все готово но этот процесс долгий То есть это вторая половинка цены того что у нас все в памяти эти данные нужно туда поднять при запуске Итак давайте немножко углубимся в тему сохранности данных и про поговорим про персистентность и в sing очень часто Я слышал такое утверждение утверждение Ой у вас не используется Think или там Вы точно его используете для сохранности данных в тарантуле можно включить of Sync но никто никогда его не включает давайте я объясню почему и почему Лично я считаю что f7 в современных распределенных системах не нужен То есть для чего вообще нужен и всем вот у нас есть приложение вот у нас есть диск и приложение записав что-то на диск просит и в Синг что-то посинкать То есть если углубиться То есть у нас есть операционная система есть буфера есть диск у него есть свои буфера в sing мы просто просим сбросить буфера которые лежат в операционной системе на этот самый диск при этом диск может принять их в своей каши тоже не записать если у него нет батареек не защищает от ситуации сломанного диска диск умер все Ну и что помог тебе твой of Sync Но помимо диска У нас есть еще много уровней вокруг У нас есть сервер Ну хорошо допустим Все мы за двое или диски рейд там вот это все у серверов есть такая прекрасная штука Как блок питания блок питания это штука которая может сжечь сразу все диски то есть Лично я сталкивался С отказом блока питания 12 из 14 дисков пошли ушли короче но это не все сервер стоит в какой-то стойке стойка стоит в ряду в общем там есть такой щиток электрический в общем-то тоже Всякое может произойти с этим Я не сталкивался но выше этим этого уровня У нас есть дата-центры то есть Многие считают что дата-центр это что-то такое надежное вообще что никогда не пострадает Да конечно я сталкивался с горящим дата центром и амазон сталкивался и много кто сталкивался В общем как бы отказ можем возможен на любом уровне То есть это центр тоже может сгореть все данные могут погибнуть и что от чего нас Защитила Синг Ну как бы да Он может защитить от случайной перезагрузки машины нам нужно защищаться от кучи различных аварий более высокого уровня и поэтому мы будем защищаться от них и тогда нам станет не нужен и всем и всем от этого может нам помочь репликация у нас был умозрительный эксперимент что на самом деле мы можем взять чисто in Memory систему вообще ничего не писать на диск вот просто только храним в память если достаточное количество раз ее разреплицировать по разным независимым площадкам то это будет намного надежнее чем один Пусть и какой-то очень крутой может быть даже под рейдом диск Об этом можно поспорить этого есть эксплуатационные характеристики но здесь все просто мы рассматриваем вероятность одновременного отказа допустим у нас есть какая-то очень ненадежная штука которая может сломаться с вероятностью 50 процентов либо сломается либо не сломается если мы начнем эти системы дублировать то вероятность одновременного отказа она для сохранности нужно чтобы они не отказали одновременно она у нас падает она у нас падает очень быстро и собственно если мы поставим 10 ненадежных фигн у нас как бы вероятность отказа уйдет практически в ноль кстати именно этим подходом в общем-то и пользуются например в космосе там защититься от отказа нельзя потому что прилетают тяжелые частицы прошивают электронику Электроника глючит биты переключаются поэтому там просто все дублирует по несколько раз так вот для того чтобы защититься от отказа на более высоком уровне нам нужно к нашей сохранности добавить еще один уровень нам нужно добавить репликацию причем в зависимости от режима работы мы можем ждать окончания репликации или не ждать тем самым варьируя между синхронной и асинхронной репликацией а таким образом если Рассмотреть с точки зрения так называемого RP он то есть точки на которую мы можем откатиться если мы храним все данные на одном сервере с очень крутым диском то в случае отказа этого диска Мы возвращаемся в самое начало когда данных У нас не было а в случае если мы реплицировались то мы откатываемся к состоянию когда мы потеряли какие-то данные может быть потеряли может быть не потеряли собственно последние эти данные это будет то что мы не успели реплицировать репликация Кстати может быть довольно быстрой Давайте поговорим про репликацию какие бывают вообще виды репликации Почему их много и откуда вообще она пришла репликация это биологический термин изначально то есть войти он пришел из генетики это функция копирования генома с помощью репликации из одной цепочки ДНК создается две одинако цепочки ДНК в базах данных и вообще в it-системах этот термин довольно близок это механизм создания точной копии в случае базы данных это механизм создания копии одной базы зачем она нужна Ну как Мы уже поговорили то есть резервировать еще репликация нужна для повышения доступности То есть когда у нас что-то ломается То есть у нас была мастер база У нас есть реплика мастер сломался В течение какого времени мы можем восстановиться если рассмотреть однонотовую систему Допустим мы были умные и мы делали бы капы и даже Мы проверяли чтобы восстанавливаются сервер умер наше время восстановление найти бэкап найти новый сервер накатить туда бэкап поднять базу и молиться чтобы Она поднялась в случае наличия реплики мы просто новую реплику смысле реплику промоутим в нового мастера то есть время на восстановление после сбоя сокращается до очень приемлемых величин Ну и репликация может использоваться для повышения совокупной производительности системы из-за некоторых ограничений в случае эти ограничения кстати могут становиться весьма значимыми потому что как я уже говорил in Memory база работает с одним ядром Пусть и очень круто эффективно но у всего есть предел Итак Какая бывает репликация Я думаю большинство из вас знает что бывает репликация синхронный асинхронная Кто юзает синхронную репликацию из присутствующих здесь Я так и думал синхронная репликация репликация которая подтверждается после фиксации на реплике То есть вы пишите в мастера но реплицируется реплика подтверждается мастер такой Да все я записал то есть репликация входит как бы в саму транзакцию она надежная То есть если Вы сохранили то как бы вас надежность сохранность данных очень высокая но она медленная Нужно обязательно в транзакцию включить еще и репликацию и как это ни странно у вас выше шанс отказа сервиса целиком на запись ушла реплика вы не можете писать При этом если что вы восстанавливаетесь очень легко вы ничего не потеряли асинхронное гораздо более привычная репликация на самом деле сегодня если мы говорим репликация большинство подумает именно про асинхронную репликацию подтверждается после записи только на одном узле она может потерять часть не отреплицированную если мы узел потеряем совсем но она быстрая не отвечает почти моментально когда сделал операцию на узле и собственно используется довольно часто в этом случае потеря данных составляет величину репликационного лага Какая бывает еще репликация репликация может варьироваться по направлению то есть есть классический такой вот привычный подход однонаправленной репликации мастер реплика и бывают всякие хитрые схемы типа с двунаправленной с мультимастером они направлены одна на консистентность Потому что если мы пишем через одного мастера мы контролируем консистентность данных а многонаправленные в основном направлены на доступность если их сравнить у них есть вот этот бизнесовый показатель только теперь это время через которое мы можем продолжить работать после аварии в случае мастер систем упал там один мастер мы продолжаем работать у нас еще много равен нулю Но эти системы сложнее эксплуатировать за счет возможностей конфликтов за счет необходимости решать эти конфликты мастер реплика гораздо более привычная вот у кого база данных используется в топологии мастер реплика у кого используется в мастер-мастере остальные вообще базами данных не пользуются собственно мастер сложнее мастер реплика гораздо более привычен значит давайте рассмотрим Как можно сделать репликацию на примере устройства репликации в тарантуле Итак у нас есть вот этот instance про который мы проговорили мы знаем как он устроен он пишет логи если эти логи схватите начать реплицировать у нас получится репликация поднимем рядышком еще один instance в нем поднимем некоторые системный файбер который называется atlar попросим его установить соединение вот с тем узлом который который с данными для него создается специальный отдельный трек который называется relay который будет читать их слоги там чуть Более сложный механизм но верхнего уровня он читает их слоги и отправляет их в applyer applier работает с ареной как обычный клиент то есть по сути вот все что он получает по репликационному Логу он применяет в Арену Арена это пишет через вал на диск Таким образом у нас по сути создается аналогичная копия с теми же данными которые приходят в такое же состояние которое также может писать снапшоты при помощи вот этого механизма каждый такой инстанс каждая реплика может обладать своим relay может обслуживать запросы в целом они могут быть даже равнозначными и вообще Весь вот этот paypeline скажем так что-то пишем это пишется в xlock и через релей куда-то релейции там это применяется может быть соединен в разную сторону то есть в целом эта репликация Может быть как мастер реплика так и мастер так это сделать для того чтобы это сделать нам нужно как-то идентифицировать все что происходит в инстансе и вообще в системе для идентификации транзакций используется довольно распространенный в базах данных подход Lock sequence Number У нас есть монотонная последовательность которая растет на конкретном узле она определяет номер транзакции которую мы сейчас исполняем и вместе с номером узла у нас будет такой глобально уникальный номер транзакции давайте рассмотрим как репликация будет работать вот у нас есть мастер мы его только запустили у него не было никаких изменений его лсн равен нулю мы делаем какое-то изменение вставили строчку Мы за инкрементили лсм и с этими числами сделали эту запись из этого Лога потом можно прочитать еще одно изменение сделали когда мы поднимаем репли и подключаем её по протоколу репликации это всё реплицируется но реплика обладает своим собственным айдишником и своим собственным лсм то есть на реплике изменений не было это были изменения от другого узла в целом они могут инкрементиться Независимо если мы скажем так делаем изменения на одном узле они уже установленному соединению будут реплицироваться если мы соединим узлы в два направления мы получим мастер репликацию мы можем На вот эту бывшую реплику прийти Сделать какое-то изменение она пронумерует его своим лсрном запишет со своим айдишником и успешно триплицирует на другой узел в целом мы можем пользоваться этим одновременно или мы можем пользоваться классическим механизмом зависит в общем-то от нас и от того как мы хотим этим воспользоваться для того чтобы решать конфликты Ну то есть мы там данные поменяли здесь данные поменяли у нас какая-то каша могла получиться как с этой кашей разобраться в тарантуле для этого используется механизм векторных часов Итак что такое векторные часы каждый узел обладает своим собственным вектором где у него инкрементится лсн то есть каждое изменение у нас просто инкрементит его Вектор где его айдишник является этим вектором если у нас есть второй инстанс у него могут быть свои изменения третий у него свои изменения если мы хотим все эти изменения собрать вместе мы просто смотрим на совокупность этих изменений этих векторов эти вектора можно между собой сравнивать то есть кто дальше кто ближе к кому что нужно обновить то есть узел взлетает он видит каких компонент ему не хватает и дотягивает их по репликации Итак репликация в Тарантул основана на вал она использует We Clock для Recon sailing скажем так она может быть как мастер так и мастер реплика в принципе из Тарантула можно собрать любую произвольную топологию То есть можно соединить их все вместе можно соединить звездочкой в кольцо в общем-то фантазии фантазия ограничена максимальным количеством зарегистрированных узлов репликация Может быть как синхронная такая синхронная возможно кто-то уже слушал доклады наши про синхронную репликацию значит и напоследок я хочу рассмотреть еще один момент касательно чтения данных из памяти и multiversion concarency Control в целом когда мы вот говорили про то что у нас кто-то пишет потом он пишет на диск приходит читает из памяти здесь есть один маленький нюанс пишем мы на диск долго то есть мы пришли в памяти Обновили и отправили писать на диск писатель ждет пока запишется но новые читатели уже стоят в очереди и они прочитают эти данные могут это чтение будет ну по-своему грязным это можно тоже схематично изобразить то есть вот у нас есть память вот у нас есть диск некоторые Ну пусть у нас есть ячейка то есть будет символизировать одну строку с каким-то значением исходное значение 0 пришел Райтер записал эти данные они в памяти обновились потому что память является источником Правды и пошел писать это на диск пришел читатель прочитал писатель дописал то есть оно записалось на диск ему все вернулось он такой да все транзакции отличная но здесь как бы вот вроде бы на первый взгляд Все выглядит хорошо то есть мы пишем последовательно мы читаем последовательно то что мы как бы записали мы как бы видим читателями здесь есть нюанс мы видим некоторую запись которая была до попадения вал Если вдруг что-то произойдет вот как бы все идет дальше Мы что-то записали что-то прочитали а потом у нас транзакция не записалась Ну вот эти диски они такие место закончилось И все как бы то есть если с памятью мы можем все узнавать моментально то здесь как бы ну закончилось место на диске вал не пишется Ну вот что делать дальше делать дальше с точки зрения базы данных все правильно она перейдет в режим редонли она перестанет писать транзакции все читатели продолжить читать но мы прочитали уже грязные данные все дальше продолжать работать нормально но вот тот к несчастный клиент который пришел в этот промежуток он такой у меня тут Файлик я хочу его сохранить такой Файлик есть или нет то такой Файлик есть А ну ладно и всё файлика потом нет В общем для того чтобы избежать таких проблем Как вообще можно это сделать Я хочу описать схематичный алгоритм то есть более детально там конечно намного сложнее Но что с этим можно сделать нам просто нужно не показывать данные которые мы пишем читателям то есть мы разделяем области чтения и области записи то есть мы пишем в одну область читаем из другой до тех пор пока она не допишется как только данные дописываются мы все Она уже на диске теперь можно читателям их отдавать читатель успешно их прочитает Если вдруг данные не записались Ну не записались то есть там в области для записи как бы это помаркали то есть читателей Мы даже никак не зафиксили то есть у нас получается такой хороший review который читает только чистые данные которые доехали до диска причем Если эту систему распространять дальше мы можем здесь контролировать не только что данные доехали до диска а и например данные доехали до реплики то есть мы реплицируем эти данные но там их тоже не применяем до тех пор пока они не разъедутся не отреплицируются и не покажет как это все работает собственно про mvc более подробно То есть я показал верхнеуровнево как это можно там не знаю сесть и на коленке сделать про инвестиции будет хороший доклад завтра Саша ляпунов рассказывает как устроенным vcc внутри Тарантула с кучей всякой прикольной теории То есть если вы хотите знать что-то про инвестиции Приходите собственно давайте я подытожу немножко более широко еще раз как устроен Тарантул все данные в памяти доступ к ним из одного треда никаких блокировок максимально дешевый доступ к данным Работаем только через индексами через индексы это дисковая база может себе позволить А давайте для того чтобы найти вот эту одну маленькую штуку прочитаем полтерабайта диска Ну как бы занимать процесс поиском данных памяти нерационально можно но не рационально а любое изменение через Write headlock пишется на диск то есть фиксация есть этот Write hellock реплицируется и мы можем выбирать уровень гарантии который нам нужен Acid достигается На текущий момент уже многими способами там есть однопоточность там есть инвестиции В общем много всего периодически для ускорения времени взлета снимается снапшот и Тарантул можно сконфигурировать очень разным способом то есть его можно сконфигурировать как мастер как мастер реплику синхронные васинхронную репликацию в зависимости от задач и зависимости от требований то есть вот в шкале РТО мы можем занимать любую позицию в зависимости от того как мы этот или этот кластер настроили у меня все спасибо за внимание Если кто-то хочет меня найти вы можете написать мне в Telegram Я убрал все другие контактные способы потому что бесполезно большая просьба оценить доклад Это левый qr-код кому интересно взять эту презентацию это правый qr-код И я готов ответить на вопросы отлично Спасибо большое прекрасный доклад Спасибо большое у нас есть ряд вопросов Давайте Вот пока микрофончик вот молодому человеку несут да вот сюда у меня есть вопросы с чата Давайте ее зачитаю пока передвижение Итак от Владислава вопрос а при обработке запросов батчингом запросы из другого батча не выполняются нет строгая очередь вот в каком порядке они пришли и уложились вот в эту очередь передачи Или точнее правильнее даже сказать в очередь приема в таком порядке они будут обработаны Спасибо большое там деле длинный но по сути дела потому что поток Один в Один работает строго только один Спасибо большое и молодой человек Вопрос вот сюда микрофончик ты говорил что там возможно произвольно топологию соорудить Но это видимо если какой-то либо руками собирать либо какой-то внешней компоненты а штатное средство подразумевают сборку репликасетов в 2-3 n узлов соединенных Full mession То есть все ко всем с автоматическим или ручным фэйловером и одной ролью мастера То есть это такой дефолтный сетап который использует 90 процентов репликасеты дальше Еще могут объединяться в shard группы то есть поверх этого есть еще sharking но я здесь на более низком уровне рассматривал И второй вопрос это Он написан на c++ Ох на чем он только не написан там даже обжегте все в свое время был это си плюс кусочки на си плюс плюс объектив си уже выпилили и солидный кусок работает на лоджите А почему вот именно все они воспользовался отлично Спасибо большое за вопросы Отдайте пожалуйста микрофон больше не зажимаете никто микрофончик задали вопрос сразу Отдайте молодому человеку чтобы можно было сюда вот принести вопрос Почему на одном А почему они на другом э-э Давайте Проси плюс плюс поругаемся отдельно Я не хочу агрить половину зала на другую половину зала Отлично Отлично Так так давайте вопрос и потом вот сюда На первый ряд Вопрос такой а не было желания использовать полусинхронную репликацию никогда на твоих проектах потому что чудесная штука я её в тарантуле уже давно фичери квещу Но вот пока вроде никак а было использовали мы Ее называем всеми синхронной репликация это асинхронная репликация с кастомным подтверждением только части данных то есть одни изменения мне не важны потеряю кусочки просто счетчики Ну как бы не досчитаем чуть-чуть А другие данные это сохранность данных вот для таких мы применяли там в ручном режиме убеждались что оно распространялось на реплике Я не вижу кто задал этот вопрос я Это в самом конце да Ну вопрос на самом деле есть или нет это уже потому что вот очень хочется вот прям из коробки в ядре есть возможность это сделать но прям легкой конфигурации Нет спасибо Хотя я на самом деле добавлю Вы можете просто использовать синхронную репликацию для только избранных таблиц и вот если вам это надо вы выполняете синхронную транзакцию все остальное асинхронно так что будем считать что почти есть Спасибо спасибо если у нас мастер асинхронная репликация как разрешаются конфликты это придется сделать Вам автоматики нет потому что нет хорошего консистентного способа разрешить конфликт у нас есть возможность поставить Триггер Триггер срабатывает на репликации Вы можете увидеть конфликт Вы можете его решить в ту или иную сторону про это у нас были отдельные доклады с примерами там триггеров то есть как например сделать resolved конфликтов основанный на тайм-стемпах или основаны там на каком-то значении То есть это все можно затрогать мы не выбрали никакого дефолтного способа разрешения конфликтов потому что Тарантул Это скорее платформа на которой можно строить разные штуки а выбор одного решения сузил бы применимость Спасибо и в случае за операцией записи если запись на диска синхронная как мы получаем подтверждение операции пишущий файбер пишущая картина она ждёт То есть она меняет данные в памяти после чего она ждет пока она доедет до диска когда оно доезжает до диска Мы видим что оно записалось то есть писатель всегда знает что оно записалось так хорошо спасибо большое за ответ на вопрос и Давайте возьмем вопросик из чата если при ребалансировка Ну такая ребалансировка если ребенок при изменении количества узлов вы клок расширяется на ходу да если мы значит тут про ребалансировку я прям чувствую вопрос про шардинг вопрос шардинга Мы здесь сейчас не затрагивали это более низкий Sharing это более высокий уровень мы рассматривали сейчас устройство более низкого уровня в рамках репликассета каждая реплика держит на себе полный dataset данных у нас реплики могут быть именованные и анонимные реплика обладает собственным идентификатором и может писать Ну то есть валы То есть может от своего имени производить изменения анонимная реплика такими полномочиями не обладает если говорить про We Clock У нас есть 32 инстанса которые могут быть именованы под них есть слоты И если мы присоединяем реплику она просто забирает себе одну из компонентов если мы присоединяем анонимную реплику их можно сколько угодно подключить то она влог не расширяет больше 32 пишущих реплик сделает нельзя Но кажется что топология с 32 пишущими репликами это очень сомнительно этология какая-то тема Спасибо большое И вот пожалуйста на первом ряду у нас вопросик Добрый день спасибо за доклад вот на предпоследнем слайде как раз тот случай когда данные ещё не записались можно на него я получается что я не справлюсь оно достаточно получается что как бы следующая задача корутины которые исполняется она не видит данных от предыдущей получается и соответственно она может сделать какую-то модификацию не зная о том что уже произошло я рассказывал один механизм про то как обеспечить review данных которые скажем так попали на диск гарантированно или на реплике самым vcc Более сложный про это как раз завтра будет доклад У нас есть несколько разных режимов видимости в зависимости от того какая у нас транзакция по умолчанию пишущие транзакции читают вот эти самые грязные данные потому что это снижает конфликты в ноль то есть мы видим самые свежие данные потому что транзакция всё равно зафейлится если она будет после завалившейся транзакции они все укладываются последовательно поэтому писателем мы даём видеть грязные данные они не конфликтят читателем мы даём чистые данные они как бы не отдадут наружу ничего грязного э если транзакция обладает неизвестной видимостью э она определяет по первому стейтменту но есть возможность указать вручную то есть я знаю что я буду писать поэтому я говорю дай мне вези билити э комитет и собственно у нас разделяется коммит и конверт То есть то что мы сказали коммит Мы это можем видеть в последующих транзакциях а то что у нас конфирб оно разъехалось либо на диск либо на реплике Спасибо Спасибо большое последний вопрос из зал и потом вопросы с чата будет Спасибо Давайте очень нравится ваш подход к надёжности но тем не менее подскажите вот есть райт Лог у него на входе есть большой буфер а на выходе есть страничный кэш выключили электричество что у нас со временем восстановление будет а ситуация из реального продакшена машина уходит резко по питанию через некоторое время она взлетает и мы как бы изучаем что там происходило данных попавших на диск было Приблизительно на секунду меньше чем было на реплике репликация авторантуле обладает по сути временем пинга вот какой у вас Пинг Ну точнее РТТ Хотя нет Даже скорее Пинг вот какое у вас время пинга между узлами такое у вас будет лагерепликации и вы потеряете только те данные которые не укладываются в лагерепликации узел который перезагрузился по питанию будет обладать меньшим количеством данных с большой вероятностью он их просто заберет реплики отлично Спасибо большое Давай последний коротенький вопросик Да и мы закончим здесь можно будет попытать тебя там в кулуарах вопросик такой значит из чата тебе легко будет на него отвечать прям Мне кажется супер очень простой в каких реальных проектах высота высоконагруженных систем применяется тарантул можно ответить во всех проекты в которых Я участвовал лично в которых вот скажем так внедрял это облако которое битусичное облаке которые вот сейчас которая ВК Клауд весь Cloud storage построен на тарантуле на тарантуле работает практически вся почта вся авторизация сессии профиля вот Короче у них там за сотню различных планетсов разных ролей для Тарантула Ну и в целом по группе Тарантул довольно обширно юзается Хотя группа компаний очень большая и как бы там есть разные технологии Спасибо большое Давайте поблагодарим докладчика Я предлагаю тебе выбрать один вопрос который так сказать вознаградить на вашем стенде Кто тебе больше понравился прям вот такое что зацепило прям вот зацепила меня 7 асинхронная репликация А вот есть ли у нас человек который задал вопрос про эту репликацию Мне кажется он задал вопрос и убежал и убежал Ну хорошо Окей если что он сможет найти тебя так сказать на стенде тарантула а у нас есть дублирующий вариант дублирующий да Давай дубль вот собственно здесь человека с первого ряда Отлично тогда можно не убегайте после этого сейчас закончится у нас есть у нас есть от организаторов небольшой презент для тебя сейчас соответственно девушка тебе вручит еще раз давайте поблагодарим докладчика Спасибо большое было очень круто Спасибо большое можно попадать докладчика за пределами"
}