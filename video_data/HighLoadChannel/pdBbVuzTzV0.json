{
  "video_id": "pdBbVuzTzV0",
  "channel": "HighLoadChannel",
  "title": "Надёжность высоконагруженных C++-приложений в Яндекс.Маркете / Кирилл Горелов (Яндекс)",
  "views": 826,
  "duration": 2520,
  "published": "2023-04-28T06:10:46-07:00",
  "text": "Я долго думал как в одном предложении сконцентрировать опыт построения высоконагруженных и надежных систем вы знаете получилось примерно так сервисы падают Даже те сервисы которые приносят хорошую прибыль Даже те сервисы которые имеют широкую пользовательскую базу и обычно эти сервисы разрабатываются большими командами очень высоко профессиональных людей и все равно они падают Почему вы Спросите Мне кажется Ответ здесь лежит вот где Дело в том что такие сервисы разрабатываются в очень высококонкурентной среде Где на первую так сказать на первое место выходит скорость разработки скорость которой вы доставляете новый функционал в продаже в Продакшен и здесь это всегда какой-то вопрос поиска баланса между тем насколько быстро Вы можете разрабатываться и тем насколько надежным вы делаете свой сервис этот вопрос также потому что ненадежным сервисом пользователи пользоваться не хотят Это плохой пользовательский опыт и вот это была примерно середина 2019 года июль кажется Когда мы уже израсходовали весь свой годовой инцидентный бюджет и в этот момент мне сообщают что наш поисковый кластер снова лежит И тут я понимаю что у нас нет даже хорошего способа быстро его привести в чувство быстро починить мы потратили часы на то чтобы найти корневую причину и вернуть все в рабочее состояние и тогда я понял что надо что-то менять в этот момент мы начали осознанную работу по улучшению стабильности Сегодня я расскажу вам о том с какими трудностями мы встретились по дороге как мы их решали Но для того чтобы вам было понятнее о чем я говорю я проведу небольшой экскурс в архитектуру яндекс.маркета вообще внутри Яндекс Маркет состоит из большого количества взаимодействующих друг с другом систем такие крутящиеся шестеренки но Яндекс Маркет очень не похож внутри на любой наверное другой типичный маркетплейс Ну как устроен обычный marketplace это как правило какая-то база данных какое-то количество бэкендов фронтенд Яндекс Маркет свое время отпочковался от большого поиска и этот веб-поиск он в своих генах унаследовал внутри он больше поиск чем маркетплейс где большой Поиск ищет веб-страницы в интернете Яндекс Маркет ищет товары здесь на этой схеме можно видеть много разных подсистем Да там внутри есть своя контентная система она написана на Джаве большей частью есть системы которые обернуты в сторону наших партнеров магазинов партнерские интерфейсы биллинг это тоже как правило Java есть например аналитика и какой-то связующий код это много питона А есть к примеру система которая рассчитывает условия доставки для товаров которые вы покупаете и она написана на Гоа но главные системы которые отвечают пользователям и на которые приходится большая нагрузка они написаны на c++ с одной стороны это некая историческое Наследие просто потому что c++ как основной язык разработки в Яндексе был маркетом так сказать принят от того самого большого поиска с другой стороны все-таки Он позволяет писать системы которые способны выдерживать большой РПС поэтому главные вещи в Маркете которые обернуты в сторону пользователей А это поиск это подготовка данных это те программы системы которые строят поисковые индексы они написаны на c++ потому что либо на них приходится большой РПС Либо они имеют дело с большими объемами данных ну и там тогда на первое место выходит время обработки этих данных в общем и там и там есть Sli если говорить про устройство поиска в Маркете да то оно меняется со временем когда-то может быть еще не так давно это была одна большая монолитная программа но мы движемся в сторону микросериализации разделяем ее на части сейчас она представляет себя по большой по большому счету Два куска это базовый поиск собственно тот поиск под которым поисковая база Он выполняет основную работу находя документы и метод поиск это та часть поиска которая умеет агрегировать ответы базовых как-то их переранжировать более релевантные результаты поиска поднимать наверх выполнять какую-то бизнес логику над ними это система географически распределенная на несколько дата-центров Это примерно 130 мини кластеров базового поиска здесь мини кластером я называю группу машин которые целиком вмещает индекс Дело в том что индекс Большой на одну машину не помещается Поэтому вот те машины которые содержат индекс целиком они у нас называется мини кластером и там более 250 Мета поисков информация помимо основного большого индекса также еще обновляется быстрыми пайплайнами часть информации мы возим быстрее чем основная поисковая база но это такие вещи как цены наличие в общем то что влияет на непосредственный пользовательский опыт прямо в моменте а архитектура подготовки данных по большому счету состоит из двух больших частей это то что мы называем Data Camp это наша такая микросервисная большая система которая предназначена для того чтобы получать во-первых данные от наших партнеров от наших магазинов хранить их и обрабатывать То есть это по документная обработка это куча микросервисов связанных между собой персистентными шинами данных а вторая часть она такая более большого приюсная она про то чтобы как раз поисковые индексы строить собственно эта часть готовит поисковые индексы которые помогают поисковой программе быстро отвечать на запросы пользователей там много типов индексов от классического инвертированного и всякие специализированные тоже если говорить про масштаб в числах то сегодня Яндекс.Маркет выглядит примерно так Это около 300 миллионов товарных предложений в индексе хотя на самом деле индекс больше там я по секрету скажу лежат не только предложения яндекс.маркета там и другие сервисы тоже встречаются например яндекс.славка Это примерно 8,4 миллиона активных покупателей и они генерируют порядка 10 кило РПС Так что же такое надежность как мы ее определяем о чем вообще сегодня говорить будем на самом деле у нас есть два главных подхода к определению надежности есть технические метрики классическая Как вы наверное все знаете это доступности да это отношение времени которое сервис потратил делая какую-то полезную работу к общему времени в идеальном мире это доступность равна 100 процентам в реальности она конечно ниже Ну и там есть понятие девяток Да там что такое четыре девятки Там пять девяток это вот количество девяток Включайте что после запятой процентов которые сервис отвечал а не лежал есть другие технические метрики такие как например среднее время между сбоями или бюджет на время простоя в год когда у вас в начале года есть сколько-то минут которые вы готовы полежать и с течением года этот график там как-то убывает Ну и бывают совершенно синтетические интегральные метрики например в томпе о котором я говорил есть одна общая Метрика которая показывает доступность всех его частей и она каким-то образом собирается из низлежащих метрик доступности отдельных компонентов А есть и бизнес ориентированные метрики Ну в случае с яндекс.маркетом главные такие метрики это конечно же заказы и клики да Раньше на первом месте были клики когда Яндекс Маркет больше был рекламные площадкой а не продавцом Да сейчас в большей мере конечно это заказы Но это и другое отличный транслируется в деньги Когда Вы начинаете терять заказы там клики и так далее это тут же видно на ваших основных показателях Поэтому чтобы у вас не сломалось если оно приводит к потерю кликов или заказов это страшно это надо чинить так с какими же проблемами надежности мы сталкивались по дороге начну я с технических вещей поговорим о демонах в классическом так сказать их юниксовом понимании Что такое программа Демона как правило это долго живущая программа которая отвязана там от терминала от сессии и она как правило отвечает на запросы пользователей Да вот все сервисы все серверы в юнисе в Linux они такие они демоны но какие у таких программ бывают проблемы по сравнению с программами которые запустились поработали и завершились самое главное проблема таких программ это исчерпание ресурсов если ваш демон где-то течет и теряет какие-то ресурсы то на длинном промежутке времени это гарантированно приводит вас к сбою Ну и самым частым ресурсом который может утекать является конечно же память вот к примеру фронтенд Яндекс Маркет А хоть написаны И не найти плюс Да это все-таки JavaScript но не так давно у нас был забавный инцидент на выходных где-то там с воскресенья на понедельник вдруг она начинает краситься потому что утекло по памяти а почему вы Спросите этого раньше никто не замечал в течение рабочей недели да Все очень просто в рабочую неделю люди меняют код и катают релизы поэтому штука перезапускается и вы просто не видите этой проблемы а на выходных Все уходят домой у людей там есть личная жизнь перестают катать релизы Ну и все Она утекает настолько что оно начинает падать Кстати если говорить об этом то знать какой мое любимое время это первые 10 дней января Вот кто как А я очень люблю потому что в первые 10 дней января Люди перестают шатать сервис все равно Яндекс Маркет первые 10 января работает как часы люди не катают релизы они там где-то наворачивают Оливье значит стреляют хлопушками им Классно а сервис работает потому что по большому счету баги в релизах приносят больше проблем чем вот такие технические неурядицы как вытекающая память Это куда более редкий кейс А что еще может утекать может и коннекшены А еще бывает файловый дискрипторы Вы знаете что в обычном unix приложении в обычной Linux инсталляции количество файловых дискрипторов которые выделены процессы оно не очень далеко Там вот если вы сейчас возьмёте Посмотрите её лимит будет что-то в районе 1024 так вот оказывается что если ваш демон работает с большим количеством файлов и причём делает это не очень аккуратно то это тоже становится проблемой Яндекс.Маркете был такой Гуру демон название еще от того старого Гуру Я думаю и под него ложилась Большая такая файловая выгрузка с контентом периодически значит подгружал новую и делал это где-то не очень аккуратно он утекал естественно значит оно периодически падало как полечили сначала сделали ему автоматическую перезапускалку а потом просто похоронили на шесть функционал поиска правильно иногда бывают значит кончаются например потоки в пулах Да если у вас какой-то трэдпул и где-то иногда у вас потоки залипают пусть даже редко то если вы остаетесь от системы надолго работать Однажды она у вас станет колом потому что она просто упрется процессор да бывает кончаются Connection и в пулах Короче говоря исчерпание ресурсов это наверное здесь такая основная Техническая проблема а еще бывает другой забавный класс проблем которые видно только под большой нагрузкой Ну и классика это конечно гонки если у вас многопоточная приложения сложное многопоточное приложение то как бы вы его не обмазывали тестами Когда вы его положите под большую нагрузку вы начнете ловить всякие рейс кондишены потому что в тестах это как правило не видно а еще под большой нагрузкой вылезают проблемы масштабирования Что тоже обычно в тестах не видно если у вас где-то нагрузка распределяется неравномерно то как только вы ее нагрузите в самом слабым местом упрётесь вот примеру в той же подготовке данных У нас очень большие бывают объемы которые мы храним в нашей внутренней системе войти Яндекс там вот есть такие динамические таблицы они побиты на Tablet и вот если вы там ключ неправильно выберете и у вас в какой-то Темп начнет падать нагрузки больше чем в другой то все на нагрузки сразу же поймаете эту проблему у вас система перестанет нормально масштабироваться А еще бывает забавная история про внешние системы если ваш сервис входит еще куда-то то вы можете Вот эту проблему под нагрузкой замечать не как плавно какая-то Да деградация а ступенькой то есть ваш какой-то сервис который Вы ходите отвечал нормально нормально а потом вдруг в какой-то момент там на хребет этого верблюда падает последняя Соломинка и у вас Все складывается Ну и конечно же исчерпание ресурсов Здесь тоже с нами Потому что обычно для того чтобы сверлить какие-то пользовательские запросы вам например сколько-то памяти нужно для того чтобы контекст хранить этого запроса и когда их много и они к тому же еще и длинные вдруг под большой нагрузкой вы выясняете что вы съели Просто всю память обработать запросы и бывают вообще забавные случаи даже такой невинное действие как запись в лог может сложить вам весь кластер к примеру у нас в поиске в обработке ошибок была какая-то строчка которую писали влог на ошибку которая случалась где-то в бизнес-логике Ну Казалось бы невинное действие но при определенной фазе Луны в один прекрасный день эта строчка начала выводиться на каждый запрос Как вы думаете что произошло агент который собирает логи программа работающая на той же машине открыл рот попыталась съесть весь этот поток захлебнулась сожрав память дальше пришел ядерный ом посмотрел на все это и убил поезд потому что тот был жирнее таким образом вы просто сложили к чертям весь поисковый кластер а еще бывают эксперименты поскольку это быстрая продуктовая разработка наши продуктовые команды очень любят проверять какие-нибудь гипотезы А как это обычно случается они пишут какой-то новый функционал который по дефолту выключен то есть для того чтобы он сработал нужно какую-нибудь флажочки передать и дальше над этим функционалом запускают abe эксперимент раскатывают на часть пользователей чтобы понять Вообще как пользователям то там Нравится не нравится какой эффект это имеет в принципе на бизнес показатели но проблема в том что этот код как правило плохо покрыт тестами его писали быстро быстро левой ногой в Продакшен потому что это код Экспериментальный он Никогда толком в продакшене не работал и в результате все это в один день приводит к взрыву оказывается что во-первых пользователи куда хитрее тестов пользователи могут ваш код в такие условия поставить которые вы в тестах не придумаете никогда во-вторых пользователи создают нагрузку и на самом деле значит проблемы в экспериментах они очень внезапные А еще есть такая штука Как деградации когда вроде бы у Вас программа в целом функционально работает но не укладывается ни в какие целые Ну например если вы отвечаете какую-нибудь балансеру вы не уложились в тайм-аут все это 500ка если вы примеру отвечаете какой-то другой системе например выхлоп Яндекс Маркет отправляется в большую поисковую выдачу для того чтобы быть замешан в поисковые результаты большого поиска не уложились тайм-аут все Это ошибка и деградации случаются по разным причинам одна из которых это когда какой-то Старый кот встречается с новыми данными Например у меня был забавный случай у меня один сотрудник написал функцию которая сворачивала векторы регионов то есть там Бывает так что доставка происходит в огромное количество регионов и там получается такой длиннющий Вектор чисел Но на самом деле если там подумать то можно как-то схлопнуть до более компактного представления и он туда зафигачил кубический алгоритм который прекрасно работал пару лет замечательно его никто не видел до тех пор пока не подключили почту и там случился вот а почта России есть в каждом вообще во всех деревнях и весях есть почта России там случился большой вход и вдруг эта штука начала работать так что там борода раньше посидеть как правило такие вещи опять же случаются когда делаются mvp проекты то есть нужно сделать быстро и вот такие пилоты они запускаются иногда с Не очень хорошими алгоритмами и на какое-то время притормаживаются люди потом возвращаются к этим проектам запуская уже в полный масштаб а при этом не алгоритмы к этому не готовы бывают опять же чисто технически такие штуки когда что-то перестало влезать в память Ну например в какой-нибудь базе данных индекс перестал помещаться в память или что-то перестало влазить в кэш и тот Запрос который раньше там отрабатывал за 10 миллисекунд теперь работает в 100 раз медленнее Ну и внешняя система опять же с нами Если Вы ходите куда-то еще всегда думайте А что вы будете делать если это внешняя система вам не ответит или ответит медленнее проектируйте свои системы так чтобы всегда был fallback Ну например какие-то частые результаты можно кэшировать и отдавать результат быстро и мое самое любимое это плохие данные это один из худших вообще случаев которые худших сценариев которые Может приключиться с вашим сервисом потому что в этот момент технически сервис работает на графиках у вас все хорошо мониторинг зелененькие горят даже пользователи приходят и что-то заказывают Но что при этом происходит Ну например сценарий более добрый Однажды мы потеряли всю информацию о скидках ну сервис работает только скидок нет пользователи ходят но чуть-чуть недовольные Маркет теряет деньги но не очень смертельно а Был случай и похуже однажды у нас ехали маппинги skyuk Ну skyu это в общем идентификатор товаров Когда вы что-нибудь заказываете у него есть какой-то там товар есть айдишник И вот в какой-то момент эти айдишники Поехали и пользователи делали заказы на сервисе А им приезжала совсем не то что Они заказали и самое стыдно и страшно было то что вместо холодильников мы возили интим товары нам было очень стыдно мы потом перед пользователем извинялись что вместо холодильником такой приехал Вот но сделать этот момент уже было ничего нельзя я об этом ты не узнаешь сразу потому что технически все хорошо и корректно но семантический сервис отвечает чепухой Можно ли с этим бороться Да в принципе можно об этом мы поговорим вот в этом конкретном кейсе доставки не тех товаров если бы мы лучше смотрели на фон ошибок поиск мы наверное заметили потому что там товары падали не в ту категорию поиск ругался Так что можно иметь большие репутационные риски Так что со всем этим делать первый подход постарайтесь этих проблем избежать что мы Для этого делаем нагрузочное тестирование все наши сервисы мы проводим через нагрузочное тестирование на отдельном нагрузочном стенде есть поиск в Яндексе есть специальная штука которая называется Яндекс танк она умеет создавать синтетический трафик в нее заряжаются пользовательские запросы и она так сказать то что у нас Яндексе внутри называется стрельбами она нагружает сервис Какие задачи Это позволяет решать во-первых поиск деградации если у вас с новым релизом сервис тормозит на нагрузочном тестировании или как мы говорим вы это заметите во-вторых это конечно же если ваш сервис растет то вам нужно уметь планировать емкости своего сервиса Ну и конечно ловля багов все те забавные баги про масштабирование которых я говорил большинство из них на стрельбах конечно же видны Но это все равно такой немножко синтетический трафик поэтому мы придумали еще одну забавную штуку она называется Shadow кластер это группа машин на которых установлен поиск и на который мы льем копию настоящего пользовательского трафика То есть все то что пользователи вытворяют в настоящее время оно туда попадает кластер никогда не отвечает пользователям Но на нем очень хорошо проверять новые релизы оценивать как работают новые релизы и индексаторы и поиска на нем же мы проверяем на самом деле все публикуемые данные Каждый раз когда мы варим новую поисковую базу она проходит через этот Shadow кластер и там же находятся те баги которые видны под хитрыми пользователями не все конечно но часто это начинает быть видным то есть с этого кластера мы снимаем метрики в реальном времени и проверяем А как же себя поиск чувствует под настоящий пользовательской нагрузкой А еще Мы научились оценивать сами данные в отрыве от поиска когда ты не знаешь что сломалось Но у тебя вдруг новый набор данных чем-то не похож на предыдущий мы каждая сваренная поколение сравниваем с десятью предыдущими и оцениваем как технические метрики такие как размеры файлов там количество каких-то полей так и количественные характеристики внутри например там заполненность каких-то полей не похожесть их Поэтому если вдруг мы теперь потеряем все скидки или что-то начнет отличаться значительно мы это поймем сработает разладка мы такие данные никогда не опубликуем ну и также мы сравниваем тайминги поиска времена ответов похоже ли они на времена ответов под предыдущими поколениями или не похоже если не похоже что-то идет не так и такие данные публиковать нельзя но наступает день когда приходится принять неизбежное как бы вы ни пытались защитить свой сервис от падения Однажды он упадет Единственное что вы можете сделать в этот момент научиться его быстро поднимать по классике есть Метрика среднего времени на восстановление вот ее и надо минимизировать И для этого мы придумали несколько инструментов первым таким инструментом Яндекс.Маркете стал так называемый ковбойский релот Почему ковбойский потому что быстрый и невежливый обычно обновление данных на сервисе это плавная штука вежливая вы не можете просто так взять и перезагрузить все инстанции своего сервиса он пользователем перестанет отвечать это нужно делать как-то более-менее плавно все время поддерживая минимальная количество машин которые способны отвечать но иногда бывают ситуации когда не до вежливости И вам нужно Резко поменять поведение всего поискового кластера как можно быстрее это либо истории когда поиск и так весь лежит либо история Когда лучше бы он лежал Когда вы продаете что-то не то вот поэтому мы придумали инструмент который позволяет это делать очень быстро очень невежливо и откатывать как сами бинарники поиска так и индексы под ним Но это инструмент он такой очень ручной это была интуза которую было довольно тяжело использовать и при неловком использовании можно было легко выстрелить себе в ногу и начали использовать не только в тех случаях когда поиск совсем лежит Ну и когда хочется быстро исправить какую-то ошибку которая портит пользовательский опыт в этом случае обычно перезагружали не целиком а под это центром и там был специальный ключик типа какой-то центр нужно перезагрузить и Вот ребята я говорю ребят сделайте так чтобы Чтобы перезагрузить Все нужно было явно указать Да я знаю что делаю Хочу перезагрузить все сразу но нет сделали Так что не указание этого параметра перезагружал весь поиск естественно на ближайшем инциденте Когда нужно было что-то починить грохнули весь поиск разом а вторым таким инструментом стал Save Mode поиска знаете как старых версиях Windows когда что-то идет не так нужно урезать функционал редуцированная версия поэтому мы в посте реализовали Такой типа безопасный режим который по большому счету делает два дела во-первых он отключает все эксперименты это нужно потому что иногда бывает очень сложно выяснить какой именно эксперимент портит вам всю жизнь их обычно бывает запущены сразу много а во-вторых он отключает быстрые данные фолбетчицы на данные большого полного поколения это тоже очень хорошо потому что вы знаете что на таком наборе данных поисках стартанул и прошел базовые проверки вот Ну и конечно эта штука не требует никаких релизов выкладок это все включается прямо в продакшне флажком это делается быстро Но дальше мы поняли что люди делают ошибки действительно довольно часто иногда делают еще хуже чем было бы если бы не делали и поэтому мы приняли решение придумать очень простой инструмент который бы автоматизировал базовые сценарии самые базовые и типовые задачи позволял бы выполнять быстро и без ошибок один из моих разработчиков дал ему название саб-зиро видимо Потому что когда вокруг все пылает нужно сохранять Спокойствие и этот инструмент позволяет это делать Он позволяет быстро откатывать ковбойским релогом бинарники поколения Он позволяет включать своих Он позволяет выключать быстро управлять автоматикой и все это из такой вот консольный туза где ты просто выбираешь что тебе нужно сделать Не нужно сочинять длинные сложные команды помогло много раз спасло а потом мы поняли что хоть техника и ненадежна люди еще не надежнее и вот в этот момент нужно заменить человека на автомат Везде где можно даже когда у вас дежурный на месте даже когда они реагируют это все равно время реакции минуты а работу секунды поэтому мы придумали над этим саб-зиро автомат который работает быстрее людей и умеет очень быстро определять недоступность значимой части поискового кластера и вмешиваться быстрее человека в этот момент робот приходит сам включает безопасный режим сам откатывает а потом уведомляет людей о том что там произошла какая-то лютая дичь надо бежать разбираться но у вас сервис продолжает работать это экономит огромное количество времени и денег Ну и напоследок я немножко хочу рассказать про инцидент менеджмент который принят у нас в команде инцидентом мы называем по большому счету поломку которая приводит к убыткам и у нас на это есть инцидентный протокол такие вещи мы умеем чинить очень быстро то есть Вот то о чем я говорил про бизнес метрики это потеря заказов потеря кликов выход за SL и каких-то основных компонентов или репутационные риски это те случаи когда нужно уметь поднять сервис ну быстрее чем за полчаса а лучше значительно быстрее чем за полчаса поэтому на таких инцидентах у нас четко распределенные роли у каждого сервиса внутри Яндекс Маркета есть свои дежурные которые знают как с этим сервисом обращаться знают как его перезапустить откатить починить и есть дежурные которые готовы это сделать почти в любое время дня и ночи есть инцидент менеджер это человек который следит за основными метриками и обеспечивает во-первых выполнение инцидентного протокола а во-вторых коммуникации как в сторону стейкхолдеров так и в сторону там дежурных и командиров инцидента а командир инцидента это тоже очень важная роль это Инженер с очень богатым и широким опытом экспертиза которого выходит за пределы его компонента и здесь Ключевая цель командира это восстановить работу сервиса найти способ быстро восстановить работу сервиса Причем я подчеркиваю не найти проблему корневую а именно вернуть сервис рабочее состояние потому что поиск круткоза Может растянуться у вас на часы А в некоторых тяжелых случаях и над ней А ваша же Задача В этот момент заставить сервис обратно работать найти самую короткую дорогу к этому Поэтому задача командира здесь это во-первых сбор информации от дежурных от экспертов услышать мнение из координировать их работу принять решение о том что делать какой план работ выполнить для того чтобы вернуть сервис рабочее состояние распределить задачи на экспертов и проконтролировать их выполнение Ну и когда вы уже свой сервис наконец подняли вторая не менее важная часть это культура работы с этими инцидентами написание постмортомов проведение встреч по планированию задач на починку очень важно потому что во-первых Вы должны найти Ту самую корневую причину которую вы не стали искать пока поднимали сервис вы ее найдете и опишите в постмортами во-вторых после этого команда собирается на специальную встречу которая у нас называется для того чтобы убедиться что запланированы задачи по исправлению этих багов причем здесь два подхода может быть быстрое решение подставить любой Костыль только бы не уступить на эти грабли второй раз и системное решение которое накрывает целый класс проблем Это уже может быть что-то величиной в проект и в результате вот этих действий мы получили на самом деле очень хорошие результаты Мы за первые три квартала двадцатого года после того как начали этим заниматься количество инцидентов в поиске снизили на 57 процентов А в индексаторе на 77 айчард справа показывает распределение сегодняшнее по причинам инцидентов И как вы видите все равно главной проблемой являются все еще баги в релизе видимо следующий наш шаг будет направлен на улучшение качества кода потому что все равно высокий темп разработки Несмотря на то что в целом количество инцидентов снизилось приводит к тому что мы иногда докатываем проблемы до провода конечно проблемы конфигурации тоже случаются например автомат о котором я рассказывал Да вообще роботы классные ребята Они не спят они готовы все время работать но Иногда случается не так например мы меняли конфигурацию поискового кластера добавляя в него машины и вот этот робот начал три делиться Почем зря и дежурные его выключил всего на денек думает Пока конфигурацию меняем мы сейчас его приглушим что вы думаете на следующий же день мы поймали лютый инцидент где робот проспал никогда не выключайте свою автоматику там вот прям Это должен быть закон который нельзя нарушать Ну конечно Внешние факторы тоже случаются потому что мы пользуемся общей инфраструктурой иногда наши инциденты вызваны тем что не работает что-то больше чем мы классический пример Однажды классно полежал когда кто-то завернул внешний трафик во внутреннюю сеть и это порушило просто все маршрутизаторы по дороге вплоть Единственное что выжило это был какой-то маршрутизатор где-то в Финляндии потому что до туда пакетом лететь недалеко поэтому всю систему обратно размотали оттуда такие вещи тоже случаются но это какой-то очень редкий кейс Так что в целом как-то выглядит вот так что хотел я рассказал теперь я готов ответить на ваши вопросы Кирилл Спасибо тебе за отличный доклад Теперь понимаю как на вас работает но мне не приходили товары память конференции и мы ждем вопросы смело задавайте вижу руку Спасибо за чудесный доклад У меня вопрос немножко шкурно Я знаю что Яндекс выложил очень много своих разработок Open Source ydb и не буду перечислять там больше сотни полезных вещей вот этот ваш чудесный робот который помогает и поддерживает вы не планируете его каким-то образом хотя бы кусочек выложить или он без всего остального Смысла не имеет Спасибо Спасибо большое за вопрос так сразу за первый вопрос президент Передайте пожалуйста удачно пока что планов этого робота выкладывать не было потому что все-таки это такой робот не совсем универсальный Терминатор который значит за ребенком присмотрит кашу сварит он такой достаточно специализированный поэтому без остального большого Смысла не имеет но если вдруг мы его как-то обобщим этот подход распространиться шире то наверное да смысл есть Я вообще очень люблю Open Source Open Source это классно сам пользуюсь Мне кажется маленько дойдет и вместо нас на сцену буду спать е роботы им будем уже не нужны Следующий вопрос Спасибо за доклад красиками смотрите у вас написано что вы снизили количество инцидентов поиска индексатора на столько-то процентов но к сожалению цифры без контекста непонятные то есть смотрите у Вас например вот снизилось количество ингредиентов А может вы стали реже катить релизы может наоборот чаще непонятно плюс непонятно как увеличилась или уменьшилось пользовательская база на Маркете плюс непонятно как увеличились продавцы на Маркете за этот же период то есть инциденты поиска и индексатора то что они снизились это хорошо но без контекста непонятно насколько это хорошо И вообще хорошо ли Может быть инцидентов как раз должно было стать больше потому что пользователи стало больше пользовательского опыта функционала больше И это должно было привести к количеству инцидентов и это нормально было бы а вот у вас снизилось Спасибо большое за вопрос прям Я знаю что в следующий раз сделать добавить больше информации вопрос очень релевантный на самом деле отвечаю скажу так количество релизов только увеличивалось потому что все вот эти последние годы мы же были заняты тем что мы меняли бизнес модель становясь продавцом и наращивали как пользовательскую базу так и количество партнеров с которыми с которыми Мы работаем и строили склады там огромная работа происходит поэтому количество релизов наоборот только увеличивалось всем Хочется постоянно наращивать Темп разработки так же как и пользовательская база Именно поэтому задачей сложная в условиях роста в условиях снижения вот этого вот цикла разработки когда каждая бизнес команда требует чтобы их фичи как можно быстрее доставлялись в продакшн нужно уметь поддерживать надежность Так что Отвечая на этот вопрос скажу что наоборот все это было в условиях роста Кирилл Спасибо за доклад вот возник вопрос по поводу Вашей тунзы которая автоматически все перезагружает по сути это же инструмент фактически неограниченными правами на всю систему там на все кластеры и у него должно быть очень много различных доступа в сертификатов а как вот вы следите чтобы это все было в актуальном состоянии чтобы там не было такого момента когда сертификат для определённого сервиса протух например и туда Просто не смогла его вернуть нормальное состояние из-за этого Спасибо за вопрос Ну актуальность за актуальностью сертификатов в принципе следит нашей группой сре во вторых Мы периодически проводим учения Хотя учение именно по применению этой тузы отдельно мы не делали Вот но на скажем так мы живем в мире где она периодически срабатывает и спасает нам жизнь вот так что даже в рамках там больших каких-то учениях или в рамках просто ежедневной жизни где она периодически срабатывает спасая нас Мы поддерживаем это в актуальном состоянии У нас есть отдельно выделенная группа эксплуатации которая следит за тем чтобы доступа не зарастали Кирилл я пока не микрофон прошел второй вопрос она имеет очень много доступа не получили посторонний то что ваши роботы он может и очень быстро Хороший вопрос на самом деле система которая контролирует доступы она является частью общей инфраструктуры Яндекса то есть все роботы все сервисы и все пользователи так или иначе через эту систему работают так называемый TV токены они их получают и используют поэтому люди которые имеют доступ к этой системе они в явном виде должны запросить доступа и чтобы ее использовать они должны иметь доступ в это окружение а все спасибо за ответы у нас ну и плюс К чему я должен сказать что опять же этот доступ возможно только изнутри наружу эта штука не видно Спасибо И у нас есть Следующий вопрос наверное последний а потом мы сможем продолжить все общение в кулуарах Добрый день Кирил У меня есть вопрос по поводу нагрузочного тестирования я увидел это как определенный защиту так сказать у меня возник вопрос по поводу профиля нагрузочного тестирования Как вы его составляете это первый момент и второй момент по поводу вида тестов потому что я видел регрессионный NT и соответственно с учетом того что трафик пользователей постоянно меняется то вопрос Вы составляете на каждый соответственно релиз новый профиль или же есть какая-то цикличность Вопрос хороший есть цикличность не на каждый конечно не на каждый релиз новый профиль потому что нужно уметь сравнивать релизы между собой поэтому вы должны несколько релизов протестировать одним и тем же набором данных чтобы они были сравнимые Вот но периодически собирается новый профиль из настоящих запросов пользователя набирается новое вот это вот Лента запросов которые используются в тестировании это нужно делать безусловно на какой-то периодической основе потому что во-первых функционал меняется профиль запросов несколько меняется нужно идти В ногу со временем Поэтому да это происходит может не очень часто но периодически обновляется"
}