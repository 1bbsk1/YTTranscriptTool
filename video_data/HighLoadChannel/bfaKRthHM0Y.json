{
  "video_id": "bfaKRthHM0Y",
  "channel": "HighLoadChannel",
  "title": "Как перейти от batch к streaming на примере рекламной контент-системы / Егор Хайруллин (Яндекс)",
  "views": 1742,
  "duration": 2917,
  "published": "2023-10-06T07:21:34-07:00",
  "text": "Всем привет Меня зовут Егор и я работаю в Яндекс рекламе и сегодняшний мой доклад будет посвящен событийной архитектуре например рекламной контент-системы доклад соответственно будет интересен всем кто занимается рекомендательными системами и также будет интересен разработчикам баз данных так как база данных это Ключевая компонента любой контент системы любой рекомендательной системы немножко Поменяй Я уже почти 10 лет работаю в Яндексе все это время я занимаюсь разными частями контент системы рекламы занимаюсь ее ускорением с часов до минут до секунд и за все это время я прошел путь от стажёра разработчика до руководителя отдела также преподаю в шаге когда давным-давно его Закончил Сейчас веду семинары по курсу алгоритмов люблю плюсы не люблю первым Окей допустим у нас есть некоторые рекомендательная система и С чего начинается внезапно с некоего партнера который хочет принести нам данные чтобы мы что-нибудь с ними делали например хочет чтобы мы рекламировали его товары или например продавали его товары если например какой-то marketplace что он ожидает в целом в первую очередь чтобы она начала работать то есть мы должны его данные как-то учесть дальше в целом он готов принять что мы можем его данные готовить к показу там несколько часов особенно когда он только пришел в принципе нормально но потом появляется чёрная пятница скидки товар закончился цена изменилась и он уже не очень хочет часов Он уже хочет меньше с другой стороны у нас есть пользователь который приходит начинает что-то делать он например хочет купить себе телефон начинает этот телефон искать что он вначале ожидает Наверное он ожидает что рекомендательная система им предлагает какой-нибудь другой телефон который возможно Ему больше подойдет чем тот который он сейчас смотрит другими характеристиками и так далее в какой-то момент он этот телефон все-таки покупает И тут он уже ожидает на самом деле что ему не будут продолжать показывать тот же самый телефон который только что купил а он на самом деле предпочёл бы чтобы мы показывали например теперь какие-нибудь велосипеды какие-нибудь холодильники которые ему тоже наверняка интересные или хотя бы там говорят чехол в итоге что мы хотим мы хотим чтобы данные были учтены Мы хотим что данные были на самом деле учтены как можно быстрее и мы на самом деле хотим чтобы данные были учтены за меньшее количество ресурсов все-таки железо это дорого и соответственно эти все задачи по идее должна решать контент-система которая вот все эти куски связывают воедино Какой самый простой способ сделать контент систему вот как говорил Мой коллега да взять мы превьюс все туда запихнуть подождать несколько часов и вот у вас готовый индексы вот у вас готовые Таблицы с факторами Ну а дальше подложили это по вашей рекомендательный движок вот он начинает показывать рекламу или продавать товары предлагать музыку все что угодно но в чем проблема такой вот батч подготовки данных проблема Проблема в том что этом апрелью с операция обычно работает достаточно долгое время это могут быть единицы часов зависимости того сколько у вас данных и железо при этом если например захотите сделать так чтобы она работала два раза быстрее то вы или количество данных два раза уменьшите Либо вы количество железа в два раза увеличите очень часто и тот и другой вариант но он какой-то не очень прикольный и если вы хотите ваша система работает 6 часов задержки А вы хотите час как-то X6 ресурсов ой Ну не прикольно А но в чем весь прикол Что на самом деле Вот в этих вот данных которые наш Mac операция перемалывает на каждой итерации на самом деле мало что меняется Наши партнеры не меняют всю свою рекламу каждую минуту они не меняют не меняет все цены каждые там 5 секунд и так далее с другой стороны пользователи тоже они вот сейчас может быть что-то покликали но история которую про них есть она никак не менялась поэтому получается что мы на самом деле огромное количество данных перекладываем просто в холостую и за это платим ресурс не надо что-то с этим делать чтобы с этим как-то бороться нужно система которая бы в которой мы бы платили не за размер наших данных а скорее За изменения которые действительно происходят и такой вот подход проще всего реализовывать на базе каких-то стриминг процессинговых парадигм Поэтому вот куда-то туда стоит смотреть Окей давайте теперь вернемся к нашей рекламной контент-системе И вообще поймем что же с ней происходило что же она из себя представляет в целом Да она выглядит примерно так как то что я показал на одном из первых слайдов есть партнеры есть пользователи есть рекламный движок все примерно такой же из такой вот особенности что это настоящая высоко нагруженная система сейчас у нас под контент системой порядка сотни терабайт данных это с учётом всех наших обогащений и так далее Это данных без учёта репликации без учёта э-э там не знаю нескольких копий это просто 100 ТБ данных при этом входные потоки это миллионы событий внутренних органов и десятки тысяч событий которые приходят нам от наших партнеров с каким-то изменениями система также достаточно открытая в том смысле что она активно взаимодействует с другими сервисами Яндекса для обогащения данными пойти и сходить в сервис которым лежат эмаль модельки чтобы что-нибудь посчитать какой-нибудь нейросетевой Вектор или пойти сходить в хранилище изображений система которая может объяснить что на этой картинке Котик а не собачка в принципе тоже полезно как вот она была устроена наша контент-система несколько лет назад ну скажем что здесь примерно 2017 на 16 год здесь выделяется два блока первый блок этому принюс который был сфокусирован на обработке данных от партнера Ну можно его назвать баннерный стейк на самом деле там не только баннера там и какие-то другие объекты рекламные иерархии Но вот называем это баннерным стейтом и вторая система которая была более такая микробачевая была сфокусирована на польском профиле на юзер профайле соответственно подготавливает данные для продакшна в виде некоторых бинарных дампов которые потом опились в память фронтайме пользовательская часть она представляла данные в виде некоторого киевеллию который в которой через КТП рантайм уже ходил в момент запроса чтобы что-нибудь достать интеллектуальная и так далее ну и начнем мы наше погружение с юзер профайла что с ним было что же мы с ним делали во-первых это уже была стриминг система в которой в принципе даже была событийная архитектура нам прилетал Micro buch размеры там примерно секунд на 30 он постепенно применялся над профилем этот профиль предварительно загружался из Киеве или storage в котором он хранился в виде протобуфа и после в какой-то момент в рамках некоторого такого комита он соответственно сохранялся в Киеве в сторону нас будут довольно большой там порядка 10 терабайда их было 10 ключей и так далее агрегация вот это вот микробача над профилем была достаточно нетривиальная история в рамках которой запускали какие-то эмаль модельки считались какие-то сложные агрегаты над событиями в целом такой профиль мог там занимать десятки а то и сотни килобайт Но что же тут было не так-то что же мы сюда пошли смотреть но это была плохая система Вот плохая значит для начала это сторож был Киеве ли он был чисто in Memory без какого-либо сохранения данных на диск поэтому вручную по шарнируем между всеми нашими хостами поэтому какие-то вещи связанные с дивопсом хостов у нас было 256 поэтому всякие вещи связанные с дивопсом были довольно трудоёмки а вещь вида А давайте засыпем туда ещё железо обычно сводилось Ну давайте класть удвоем Так мы прошли путь 32 64 256 А вот дальше как-то уже было Хмм 256 хостов ещё купить как-то уже не так прикольно А задача закинуть туда ещё 10 хостов железа ну 10 хостов под эту базу короче нет и Давайте не будем это делать да чуть другое придумаем нет вот этот перекладывать сейчас все вот перешарнировать вручную не прикольно Если говорить о процессинге он был написан у нас на переле Вот почему не люблю первых при этом 70 процентов данных здесь подчеркну конечно что это не по важности важные данные мы обрабатывали но вот если смотреть в объёмах вот в строчках то 70% данных в итоге улетали вновь потому что просто упиралось во всё это железо а вообще-то 256 хвостов то есть вообще это не мало А вот то что всё-таки мы обрабатывали оно могло лежать на этом хдд минуту 10 минут час а потом удалиться например или не удалиться в итоге это а еще тут интересный факт да что А еще эта система была поднята в 3dc Независимо в каждом из DC и поэтому мы на самом деле обрабатывали все данные три раза в общем в итоге продуктово плохо мы теряем какие-то важные события мы теряем какие-то флажочки у нас нет гарантии на то что вот этот вот важный флажочек мы обязательно обработаем и так далее продукт недоволен постоянно приходит что-то говорит что-то опять сломалась надо что-то сделать инфраструктура тоже недовольна это сложно поддерживать сложно масштабировать Сложно чинить В общем мы решили что надо бы всё переписать Конечно я скажу что мне решили что надо переписать вот удалить и написать заново мы там придумали более Классный план Это я могу как-нибудь отдельно рассказать но в общем мы поняли что нам нужен какой-то классный стриминговый юзер профайл надо что-то делать делать Мы это решили с того что надо понять на базе Чего это все делать Мы все-таки взрослые не надо просто идти бежать писать какие-то велосипеды первое На чём мы сфокусировались это storage Нам нужен хороший сторож вот никакой был раньше вот хороший во-первых должен был быть действительно распределённый которая была бы автор шарнирная чтобы мы больше никогда не думали про то что где живёт конкретный ключ сегодня чтобы я просто засыпал туда ещё 10 хостов и оно живёт и не думал про то что какой-то хост выпал или какой-то дата-центр выпал Оно просто живёт не хватает железа я приношу железо добавляю команду remount всё работает вот хочу вот так я хочу транзакционность потому что я хочу чтобы изменения как-то гарантированно все прилетали или не прилетали и не было так что мы часть записи записались а часть записи не записались когда мы говорим о каких-то гарантиях вида экзамен все-таки хочется что-то таком духе хочется аналитику иметь возможность запускать надо это ими данными например вот я рассказываю что на самом деле байданных и бывают ситуации когда эти 10 превратились вдруг в 11 всё началось и задаётся вопрос так что она у нас там друг появилась такого что вдруг опухло и самое простое посадите аналитика сказать Вот тебе табличка Давай напишем запрос и посмотрим что же там в профилях лежит что же там сколько места занимает Ну и немножко чистил мы хотели в этот момент Примерно смочь это сторож натянуть на 1 Гб в секунду записи и примерно 1 миллион у капов на чтение в секунду соответственно у Капа В смысле строк в секунду в итоге мы выбрали внутреннее решение один таблицы это наш проверенный временем на Prius принципе она ходу по Если кто-то вдруг еще не слышал про или войти соответственно реализация внутри войти полностью с ним при интегрированная она Они полностью реализуют все вот эти свойства требования с предыдущего сайта Но что было отдельное интересное и приятно что так как это было частью этой системы то была полноценная поддержка мы определились операции Да уходим как-то в стриминг так далее и соответственно если возникает вопрос Давайте поймем что там с табличкой Происходит что в ней лежит что там за данные и так далее что же так давайте не сразу что касается процессинга на чем же мы будем код-то писать сторож это классно Надо же еще код писать и кажется что не хочется писать что-то такое супер персональное хочется разделить бизнесовую логику от инфраструктурной хочет чтобы бизнес свой купили как-то просто легко а вот как там инфраструктурная логика что куда кормиться было как-то скрыто Как там профиль подгрузить как этот профиль сохранить поняли что нам нужен какой-то фриланс по нему Мы понимали следующее что во-первых когда есть классная сторож как вот я ранее рассказал реализовать экзотики вас достаточно Простая история Ну О'КЕЙ что-то на обрабатывал берёшь транзакцию в рамках этой транзакции пишешь Все изменения простые ты которые поменялись А ещё в рамках этой же транзакции пишет какую-то мета-информацию про то А что же было обработано и Например если мы считаем что входной поток - это некоторые очередь например как в кафке где есть понятие то давайте просто предположи что мы данные обрабатываем строго последовательно и будем томить например офсет До которого мы данного чернос обработали также Мы понимали что хотим писать на c++ и потому что наша система достаточно себе у баунт также Мы понимали что у нас А у нас было довольно много уже написанного кода на плюсах и Мы также понимали что хотим иногда спускаться в какие-то низкоуровневая оптимизации банально поиграться например с авокатором потому что иногда это может вам принести процентов 15 от перфора вашего приложения и странно от этого отказываться в итоге мы насчет посмотрели и решили что будем писать свой собственный freem Work под названием Big garten и вот чтобы нам это сделать чтобы это все заработало Мы у себя в голове держали примерно такую схему этого самого процесса как это все должно выглядеть ну во-первых идет вот этот поток логов можно считать для простоты что это что-то подобное очереди из той же самой Кафки оно попадает в компоненту под названием shard задача которой с гарантиями закливаться просто перешагировать весь этот поток по некоторым ключу намного множества маленьких шардов или партиций некоторые другую очередь в нашем случае это очередь была часть уже а потом эти очереди вот эти кусочки очень эти шарды распределяются между облачными воркерами процессорами и каждый процессор обрабатывает свой кусочек свой свой шарф и по необходимости подгружая данные State обновляет их записывает назад такая вот достаточно простая схема это все под транзакции нытья звучит просто но реализовать конечно не так просто оказалось уже еще много всякого кода написать уже придумать как балансировать шарды между горками что если вдруг отвалился чтобы все как-то достаточно быстро переехала как вообще шахта обрабатывать параллельно в несколько рядов как вот это вот всё организовать так чтобы человек который пишет бизнес свой Q не думал что там вообще какая-то Параллельная обработка чтобы он писал строго на поточный код Это же просто какие-то вещи связанные с каким-то оптимизациями типа А давайте не подгружать профиль каждый на каждый чих если мы его только что записали Мы вроде бы знаем что там лежит в памяти есть инструменты для того чтобы этим управлять мониторить Как быстро банально Когда нам придется из-за того что что-то там не так Ну пришел покрутилось он на 5 часов работает ну сейчас мы потеряли Ничего страшного а тут возникает Так у нас всё должно быстро работать Значит надо и быстро уметь это поднять быстро покрутить быстро то есть отдельно даже пришлось писать какие-то специальные инструменты для этого Ну при этом на самом деле самое важное проблема тем не менее не вот эти перечисленные а на самом деле поток на запись оказалось что если просто взять наш тогдашний User profire перенести на эту схему как есть примерно ничего не переделывая то поток на запись получается три гигабайта в секунду что примерно в 3 раза больше чем наши желаемые 1 ГБ 1 ГБ Был немножко определён в том числе тем сколько у нас есть железо сколько Мы готовы за это заплатить В общем сколько мы вообще готовы поставить гигабайт был немножко спущен немножко так определенно Окей надо чет делать начал думать первая идея была следующая что отдельные события очень часто меняют весь профиль весь протобуф не целиком и на самом деле разумно Давайте писать только то что поменялось Давайте разберем наш профиль на какие-то логические колонки и будем писать только то что реально поменялось это нам уменьшило поток в полтора раза что уже прикольно но недостаточно Следующая история была такая что в принципе то все равно отдельные события очень часто меняет даже колонку не целиком например Мы в профиле храним список из тысячи объектов и события просто добавляют туда еще один объект писать заново все 1000 объектов звучит как-то поэтому мы нашли что оказывается все давно за нас Придумано есть вещь как бинарная Дельта кодирование есть алгоритмы И на самом деле можно просто взять от обувь как он сейчас лежит стороже есть протобуф то как он должен выглядеть в нашем понимании Посчитайте эту Дельту и записать только эту Дельту с помощью некоторых евристик такого рода чтобы вот этим вот мерджам занимался не стоит а скорее клиент мы смогли поток на запись уменьшить практически до одного гигабайта в секунду и что было крайне приятным эффектом Если первые две чиселке были все также скамейц периодом комита примерно секунд 30 то вот этот вот один и один гигабайт в секунду уже можно было держать даже прикормить Раз в 5 секунд и так далее просто потому что внезапно размер коммита стал больше зависит не от того сколько у нас данных в профиле какого он размера а скорее от того сколько мы туда записать нового хотим Ну и последний вишенкой на торте был что есть такая штука такой алгоритм сжатия данных экз т.д А еще у него есть возможность предотвучать его со словарем и Оказывается это предотвращение со своим Очень круто работает берешь словарик обучаешь его на своих данных и 40 на выходе там килобайт 10 20 примерно и получаешь Профит процентов 5-10 от и потока на запись потому что данные становится меньше и на самом деле даже от размера стороже В итоге так на проблему с потоком на запись победили и после вот всего что я рассказал мы запустили юзер профайл система выглядит достаточно похоже на ту самую типовую бигр т-процессинг в принципе недостаточно прям Один в один и что наверное самое важное тут надо подчеркнуть что да система стала теперь гарантированно обрабатывать данные экзак ливанс причем экзак ливанс сразу в 3dc если dc-1 выпадает то мы обрабатываем данные в двух оставшихся датах и общий лак на доставку данных от момента их попадания влоги до того как они стали доступны нашему runtime с учётом всех промежуточных там каких-то перекладываний тупников репликаций Чего угодно стал скорее умещаться в 10 секунд что стало очень достаточно круто для нас И после этого мы решили перейти к нашей части у нас теперь есть вроде как у нас есть Давайте посмотрим что там происходит там наверное все может быть даже посложнее может быть пострашнее а там действительно было сложно и долго весь этот поток событий изологов из директа нашей базы с данными партнера попадал довольно нетривиальную паутинку различных на превью с операцией где это некоторое время все перекладывать перекладывалось и потом вот только вдруг попадала в рантай например такой простой базовый ресурс как тайтл баннера доезжал до продакшна где-то часов два вот каким-то таким путем вначале это агрегирую все в Мега стоит потом это все как-то готовилось потом попадала в джабу задачи которые сварить непосредственно эти вот бинарные дампы и потом уезжают А вот например помните рассказывал про сервис моделей А вот деньги Например ехали 6 часов так как они попадали в более длинный цикл где еще часа 4 вся эта база перемалывается с целью посчитать нейросетки всех всех всех объектов и оно еще там через еще на 4 часа позже попадал в рам-тайм В итоге получается так что когда рекламодатель меняет свой тайтл то баннер то конечно доедет быстро там ну как условно быстро два часа а вектор для него Да я не тот часов через 6 а Вектор это при этом часть самого одного из топовых факторов и получается что на самом деле хоть и баннер доехал до продакшна показываться он нормально не будет пока не доедет этот Вектор как-то не прикольно и в силу того что с одной стороны система сложная с другой стороны имбидинг выглядит как там болящее место мы решили что мы не будем пытаться съесть пирог одним прикусом Мы возьмем что-нибудь маленькое попробуем нанести пользу маленьком месте за быстрый срок чтобы вообще проверить что наша идея мы вообще можем такие задачи решать и поэтому мы стали делать контент систем Real Times мы посадили его на поток из директа иогов собрали в нем самый минимальный профиль баннера минимальный в том смысле что только то что действительно необходимо для отчетов мбидингов и каждый раз когда в этот самый Цезарь приезжал событие что баннер теперь выглядит вот так то есть когда баннеры менялся каким-то значимым образом мы в этот же момент тут же шли и запускали расчет соответствующих нейросетей чисто про этот баннер или там несколько баннеров которые вот поменялись примерно в одну секунду и в итоге у нас на нашем и те появилась табличка собственно вот стоит вот этот один таблица в которой был минимальный профиль баннеры и рядышком лежащие имбидинг для этого баннера и по сути уак этой таблицы был в районе 30 секунд что было очень хорошо после этого так как Дин таблицы являются нативной частью тебя и дружат на превью сам без каких-либо дополнительных телодвижений Ну кроме как подправить изменить там то что формат баннера теперь выключить по-другому там не стал убить этом протобу Ну что-то таком духе что в принципе за пару дней можно подправить то мы просто взяли вот эту джабу которая занимается построением бинарных дампов и просто на травили на этом лицу собянингами в итоге стали доезжать ну 30 секунд чтобы они появились на таблицы и потом все остальное чтобы ехали ран-тайма в итоге общий лак доставки стал час чтобы было быстрее чем тайцев внезапно нашим быстрее чем тайм что звучит много обещающих и действительно После этого мы решили что нужно срочно всё начать запихивать в Цезарь и на это переезжать в частности он это уже у нас лежал собственно в этом самом минимальном банере мы Начали его возить в пруд таким образом Ну и постепенно начали перетаскивать все остальное в итоге схема у нас стала выглядеть вот таким вот образом мы весь поток логов и изменение от рекламодателей отправили в Цезарь перестали это писать в Цезарь оно обогащалось с помощью каких-то внешних источников по мере необходимости по мере наличия какого-то факта об изменении и на эти мы имели вот эту самую набор табличек в которых лежали профили всех наших объектов в виде протобуфов со всеми нужными обогащениями с минимальными задержками в районе единиц там десятка или там секунд и дальше работает вот этот построение собственно бинарных дам В итоге время доставки мы сократили примерно в два раза вот здесь вот видно что график начали затухает это вот по мере перекладывания данных в Цезарь у нас система начинала Работать быстрее вы что ресурсы освобождались потом идет резкий скачок вниз это когда мы полностью перенесли эти данные в непосредственно в Цезарь В итоге лак стал меньше часа Ну где-то примерно час но сейчас конечно хорошо это лучше чем 2 часа это лучше чем 3 часа в Пике но всё равно это час не прикольно Откуда же час берется он берется Как раз вот из этого построения бинарных дампов которые с 30 минут что-то делают и потом еще везут эти большие базы через тупир до продакшн Там они пока заматятся в память Пока не на диск запишутся по сети доеду точку это время требует что-то Надо с этим тоже делать о чем Здесь проблема Здесь проблема абсолютно такая же как я говорю в переменных первые две минуты доклада что мы на каждой итерации берем и перекладываем все данные у нас база не меняется мы строим на самом деле вот эти дампы вообще по всей базе и мы начали думать как бы нам сделать так чтобы мы на самом деле возили в Продакшен не всю базу целиком а по возможности только то что реально изменилось не Можно иногда привозить и то что не менялось Но вот как-то изменить это соотношение и мы начали строить такую систему она у нас в данный момент находится в активной разработки идея Да простая Давайте при кантай маленький файлики только с изменениями и немножечко то что уже было ранее отправлено как вот оно есть и вот сейчас эта штука у нас доставляет данные про наш любимый бединги про самые важные факторы на котором мы любим экспериментировать за 15 минут поэтому мы понимаем что 15 минут это на самом деле не финализированная точка И можно делать лучше но это просто вот mvp буквально прямо сейчас внедряется и вот это одна из первых версий вот что же в итоге хочется сказать вот по результату вот этой всей нашей работы мы для себя поняли что событийная архитектура Это очень хорошо даже если ваш контент система большая и даже для большой контент системы можно достигать минутных и даже секундных лагов не нужно делать какие-то компромиссы между скоростью объемом или полнотой легко масштабировать горизонтально просто досыпая железобируется и внезапно упрощение разработки продукта Потому что если раньше например в рамках нужно было понять в какое же место нужно воткнуть эту джабу которая обогатит профиль еще чем-нибудь банер чтобы не замедлить весь контур то здесь просто приходишь и добавляешь Вот тебе весь профиль баннеры Добавь нужный пунктов которые это обогатит не на какие влаги времена задержки это влияет практически никак не будет Поэтому даже осталось тоже прикольно так как у нас появилось понятие профиль баннера который им доступен в полном виде они могут просто прийти и внедрить свои модельки имея все знания о банере это вот нам правду скоро время докатывания фичей до Production Однако Мы также поняли что есть и куча подводных камней которые мы потыкались за это время первая проблема это смена парадигмы все-таки нам придется действительно простой понятный а такая вот событийная эти потоковые процессинги это нужно изучать и этому нужно учиться это сложно также сложно что и безусловку нужно теперь писать немножко по-другому и это приводит вот этот переезд приводит к тому что все это безуслов и которая уже написано нужно переписывать нет в целом переписываюсь сама по себе полезно можно выкинуть кучу ненужного кода не актуальных фичей это всегда полезно но тем не менее здесь это и необходимость трансформация соответственно занимает время у нас это за него годы мы начали Первые шаги в 2017 Если не ошибаюсь до 2017 выбирали мы как раз выбирали и сейчас двадцать второй год мы в целом довольно много сделали при этом знаем что там ещё есть много что можно сделать ещё и всякие системы которые можно ускорить этим занимаемся и соответственно ускоряйте довольно сложно так как упирается в разработчиков и не так легко взять их просто сделать 100 а не 10 Потому что вот этот процесс обучения строить нужно очень серьезно подходить к выбору базовой инфраструктуры Какая база данных что cycd как быстро вообще систему управлять и так далее и Самый наверное важный пункт который конечно попадает наверно в первый но хочется отдельно вы подчеркнуть это переосмысление системы явление когда у вас что-то ломается в том большом апрелюсе вы просто фиксите его и запускаете заново и через Там шесть часов у вас всё починено полный стейт с нормальными данными Итак в принципе любая проблема очень часто решается Ну подчинение эту строчку и перезапусти всё работает а здесь уже так не работает задача взять и всё пересчитать уже вот непонятно как это сделать мы же реагируем только на дельточки и вот как вот теперь это делать Это сложно В итоге событийная архитектура нужна продукту и нужна инфраструктура и событийная архитектура это наша с вами будущее Всем спасибо здесь на слайде есть почта есть qr-код есть мой Telegram Пожалуйста пишите Если есть какие-то вопросы есть какие-то пожелания про что хочется чтобы еще рассказали вообще начали вот рассказывать про рекламу хотим еще больше рассказывать рекламу поэтому Интересно что интересно аудитории Если у вас есть какие-то вопросы с удовольствием на них отвечу Ура Спасибо большое Давайте поблагодарим уже вижу поднятые руки вот молодой человек Давайте вот туда микрофончик дадим Привет Спасибо за доклад У меня два маленьких вопроса Первый такой вывод перечисляете какие-то критерии по которым вы выбираете базу данных у меня в голове сразу возникает идея это у айди би должна быть а тут почему-то какой-то другое решение я правильно понимаю что там есть один скрытый критерий который связан с тем что у вас контур мап retus он был уже выте и поэтому вам было удобно использовать что-то связанное с этим да И второй вопрос он такой вот вы говорите про то что вам Пришлось написать собственный фреймворк бигр Т но Неужели в Яндексе в 2017 году не было чего-то подобного при том что в поиске точно есть какой-то быстрый контур еще в каких-то сервисах есть такие похожие контуры Почему вы изобретаете что-то новое Окей давай тебе отвечу на вопросы значит касаемо edib Да один из пунктов был то что системы в тот момент были достаточно похожи и находились вот в стадии такого развития и войти был найти на проинтегрированность на продюсер Это было очень на самом деле как оказалось в дальнейшем очень важным пунктом что мы Потом могли спокойно запускать придётся спокойно интегрируются с контент-системой касаемо части про велосипедизм так Ну действительно Да в Яндексе были разные решения но они обладали некоторым родом недостатков Поэтому в итоге решили писать бегерте во-первых чтобы тот опыт который у нас был с этими всеми системами которые были в поиске его собрать весь положительный опыт учесть весь негативный опыт учесть те вещи которые у нас также появились какие-то требования более там детализированные и Поэтому решили что вот кажется написать етона за систему и вот даже сейчас я расскажу что в том же самом поиске вот какие-то части тогда работали как раз вот на предыдущих версиях на других решениях но сейчас оно все мигрирует как раз на этот самый bigreta так хорошо спасибо большое Вот давайте вот сюда молодому человеку передадим микрофончик и поднимайте руки чтобы я видел заранее куда можно Спасибо чертовски интересно доклад просто нам тоже актуально из позволите два вопроса Первый вы упомянули про сап шардин Я исхожу из предположения что вас стримы сделано очереди примерно как в Кафки или Амазонки Где клиент сам запоминает позицию откуда он будет читать если просто перезагрузки так понимаю это на один шард У нас идёт несколько клиентов которые сами запоминают что такое что мы вот входной поток Или например на тысячу партий это 1000 партиций У нас конечно партии дальше каждый воркер берет несколько партийцы за этой тысячи как-то не распределяются дальше возникает такая история что партиции на хвост Например у нас там получается там шесть партий а хвост например двадцати ядерный вот там или шестнадцати ядерный как шесть партиций обрабатывать на 16 Шахтах так чтобы пользователь думал что у него всё одна поточная ну и соответственно мы это решили для себя так мы берём Вот эту вот шарт и в рамках процессинга его ещё раз Режем по тому же самому ключу нас обширды каждый сам шахт обрабатывается достаточно Независимо но в момент когда мы хотим делать коммит мы синхронизируем всю вот эту параллельную обработку и датчик звучит просто но вот год надо написать дьявол всегда в деталях но я так понимаю что вот когда идёт обработка это такой микробетч да то есть мы читаем пачкой из очереди кажется тема для доклада вообще на самом деле прекрасно да давайте второй вопрос и потом вот э-э человеку также по по ряду для следующего а-а Когда у вас идёт процессоры Хмм которые обрабатывают И что вы делаете Если вдруг обнаружились ошибка в процессоре Как вы перерабатываете то что уже прошло Через Enter Вот это чудеснейший вопрос как раз вот достойно ещё вот по больному А у нас здесь какие есть истории зависит на самом деле от того в чём суть ошибки в некоторых вещах можно переправить событие ещё раз в некоторых местах Можно иногда сказать Ладно Жизнь боль просто принять тот факт что что-то побилось где-то мы сделали систему которая Например если в процессе кино исключения не смог эту строчку распасть и так далее откладывать её в сторону то что потом восстановить Ну где-то мы прям думаем о штуках о том чтобы например мочь взять кусочек из вот этого вот инкрементального стейта нашего занулить и потом перезалить все события за это время назад в систему чтобы вот оно пересчиталось здесь вот такой и пока у нас идет индивидуальный подход можно сделать отдельный доклад рассказываем про то как это все делается и какие у нас были варианты и так далее но вот как-то так что спасибо и пожалуйста Вопрос Добрый день смотрите перейти отчета стоит достаточно ну долго да и дорого продать бизнес идею что нужно 50 человек часов и будет хорошо вопрос человека Я и мой тогда руководитель раз сели придумали надо делать И тогда это было очень очевидно почему это надо делать так как это удешевляло нам систему и Мы кстати ее даже удешевели примерно раза в полтора по количеству ядер которые там использовались тупо в деньгах да И при этом еще решали там пачку продуктовых вещей от связанных вот с этой полнотой скоростью обработку и так далее тогда было прям очень понятно Зачем это делать Да кстати на самом деле про двух человек мало сказал помимо меня кто разрабатывал нам еще нужно было довольно активная помощь со стороны команды там тоже было два человека Вот так вчетвером можно сказать там впятером примерно Если взять всех людей как-то там помогал там будет человек 10 мы собственно придумывали как же нам все это делать после того как у нас получилось первая версия на самом деле начали появляться некоторые отпочкования Ребята в других командах находили что Ого нам это нравится Мы тоже хотим попробовать и в какой-то момент получилось так что есть критическая масса людей разных по компании которые уже сделали несколько внедрений понимаешь что это хорошо это круто И вот в тот момент Мы перешли вот на переход на приюса И там нам да Надо бы уже десятки человек что-то так делать Ну и там уже Было очевидно что Нам без этого никуда у нас появилась довольно много продуктовых запросов вот банально есть система под названием автобюджет Кто вот рекламу заводил наверное знает штука которая должна умным образом определять Какие ставки надо ставить байрам в зависимости того как они показываются чтобы выполнять какую-нибудь цель и ей очень на самом деле полезно когда она работает в таком окне это выглядит как очень хороший продуктовый развитие Ну сложно всегда рассказывать когда у тебя много лет опыта сразу в один доклад спрессовано Да вот как бы да так хорошо давайте тогда Следующий вопрос вот молодой человек вот в серединке зала пожалуйста будьте добры микрофончик если у вас такие ситуации если есть как вы их решаете когда Бачи зависимые и одно событие вас инициировала какой-то батч вы как-то пытаетесь по цепочке инициировать пересчёт следующих если для них событий Второй вопрос Ну В некоторых случаях если переходить по чек онлайн обработке либо чем может быть важная ситуация когда не было событий то есть не было активности нужно что-то сделать как в этом случае действуете Спасибочки здесь не до конца понял Возможно мы сказали себе что у нас такого просто не бывает события приехала оно должно применить над профилем дальше Если в рамках этой обработки возникает новые события мы их снова пишем в очередь это очередь попадает на других профилях здесь немножко похоже на вот акторную модель когда один актер что-то сделал отправил команду что-то сделал фактор это вот эти вот Big garte worker и вот обобщенные а между ними для передачи сообщений используется очередь в этом случае получается у Вас как бы актеры знают О следующих Хотя они как бы не связаны он пишет в универсальную очень то есть он должен знать что он должен записать потому что получается они содержат сакральные знания цепочки будущие Ну да то есть смотрите то есть да если мы обработке какого-то события понимаем что кто-то на это Должен реагировать мы пишем это в специальную очередь а потом те кто хотят на это реагировать это Вот как-то так это устроено значит касаемо второго вопроса Это по поводу какого-то регулярных каких-то кронов У нас есть штука под названием сервис Ок такой наш подход идея следующая Давайте с периодом например в сутки или час генерировать Ключи из нашей вот этого нашего стейта в процессе просто их регулярно туда подсовывать и с каким-то периодом Так мы например можем всю базу Например у нас если говорить про бани стоит например вся база гоняется там примерно с периодом в сутки а какой-то подножство объектов которые нам более интересны мы их умеем поддерживать сбоку и их гонять чуть чаще например раз в три часа или раз час есть какие-то которые раз в 15 минут затащили вот Ну как-то так делаем есть истории мы их прорабатываем про реализацию каких-то вот таймеров какой-то отложенный очередь что когда события это применяется на профилем такое Давайте этот профиль Через 15 минут потрогаем что-нибудь сделаем с ним вот и такое у нас там есть несколько реализаций на самом деле в Яндексе вот этой вот идеи мы пока не сделали General но думаю скоро всё-таки напишем хорошо так спасибо большое за вопрос так Кстати у меня вопрос есть Слушая вот Бывает же такое что есть два последовательных события над одним профилем Но одно уже приехало в обработку а второе как бы доезжает и Ну когда не последовательно все хорошо Ну смысле В зависимости от времени да бывает так что Например второе событие которое более позднее доехала быстрее до обработки Ну потому что распределенная система у нас все едет в разные стороны то есть соответственно Как вы с этим работать с этим тут примерно такой ответ во-первых с этим ничего сделать на самом деле нельзя потому что когда ты живешь когда у тебя данные пишется в произвольную партиции Кафка Кафки или что-то похожее на самом деле гарантирует что два события будут записаны куда-либо вот в нужном порядке очень тяжело и что они будут вычитываться правильном порядке тоже тяжело если делать какое-то локальное порядочение это во-первых А сложно код надо писать ужас Вот а второе нужно еще это еще даст какую-то задержку потому что надо подождать какое-то время и увидеть самый точно все обработали а при том что еще бывает какие-то кратковременные и недоступности чего-нибудь это вообще сложная задача поэтому мы для себя решили что А давайте-ка мы просто скажем правило что Вот обработка профиля вот это вот главная функция должна быть так написано чтобы порядок применения событий был неважно То есть если например приходит событие удалить и давать вот два такая события то есть мы например в профиле специально сохраняем э-э как бы события о том что вот что-то был удалено и его там будем хранить какое-то время мы считаем например для себя что первый порядок очень там на интервалах минут или часов или например какого-то инцидента Вполне себе возможные надо на них рассчитывать а например передачи на периоде недели это уже что-то совсем странное поэтому можно считать через неделю этот делит можно как бы удалить в принципе некоторых базах данных Так что он так делают когда вот им удалили они хранят что удаление было Вот есть мы удалили эту строчку надо помнить хорошо И еще у нас есть какие-то вещи видно что я специально в какой-то момент заменял обычные std сорт на СТД stable Soft чтобы профиль будет терминирован чтобы вот в тестах было проще потому что в тестах у нас правда данные начинают иногда переупорядочиваться и отдельная история в тестах была как бы сделать так чтобы вот от запуска к запуску всё было детей минировано потому что находить то что флот немножечко изменился там на не знаю в пятом знаке после запятой не очень прикольно Хорошо спасибо большое за ответ Давайте поблагодарим докладчика если остались вопросы можно будет задать в экспертной зоне она сразу на выходе и пока не расходитесь я попросил Егор Выбери какой-нибудь вопрос который тебе понравился больше всего вот просто да Ну один какой-нибудь да давай я понимаю что сложно но особенно в распределённой системе очень много хороших вопросов я бы наверное так можешь мне дать списочек мне прям так Параллельная обработку с обширдированием У нас есть небольшой подарочек э за лучший вопрос который только один Ну вот всегда так и у нас кстати от программного Комитета и организатора конференции есть небольшой презент для докладчика Спасибо тебе большое До скорых встреч Спасибо большое докладчику спасибо С нами и прощаемся до новых встреч в этом же зале"
}