{
  "video_id": "vx3H6BpyOng",
  "channel": "HighLoadChannel",
  "title": "Переосмысление Picodata как cluster-first-СУБД / Ярослав Дынников (Picodata)",
  "views": 246,
  "duration": 2990,
  "published": "2025-01-17T02:25:24-08:00",
  "text": "Всем привет Спасибо значит смотрите план такой я сначала запи наш продукт потом буду рассказывать про алгоритмы и архитектуру продукта подробно остановимся на двух аспектах это управление кластером и распределённый SQL но обо всём по порядку А пока поели с точки зрения польва вот это вот ВС SQL естественно распределенный мы с вами Наде вс-таки пользователь взаимодействует с кластером как с единым ресурсом а под капотом спрятаны шардирование и репликация отличительных особенностей У нас две первая заключается в том что все данные лежат в оперативной памяти этим мы Режем косты за экономии на доступе к диску этот пользуемся ло пишется на жёсткий диск Но вот чтение всегда выполняются из оперативной памяти вторая особенность состоит в том что доступ к локальному хранилищу реализован однопоточный а потому без блокировок это снова идёт на руку производительности масштабируется мы исключительно горизонтальным образом И даже если речь идёт о современных многоядерных системах то утилизация ресурсов достигается за счёт развёртывания нескольких инстан сов На одном сервере а не за счёт многопоточности Итак распределённый SQL оперативная память килан Что получится если всё это сложить вместе Пика дата маленькие быстрые данные нуно Я пришёл сегодня выступить в роли экскурсовода и хочу вас познакомить с продуктом изнутри со стороны разработки и начну я с исторической справки надо понимать что такие крупные продукты как распределённые СУБД сложные продукты в вакууме не разрабатываются И вообще исторические аспекты часто играют решающую роль в вопросах выбора архитектуры Почему тот или иной кусок кода получился так а не иначе ну так исторически сложилось и вот так исторически сложилось что я и мои коллеги в прошлом причастны к разработке Тарантула Тарантул - это база данных и Application сервер в одном флаконе идея заключается в следующем вы держите данные в оперативной памяти вычисление проводите рядом с данными и наслаждаетесь скоростью так гласит лозунг Тарантула хороший лозунг и благодаря своему определению про Application Севе Тарантул со временем оброс богатой экосистемой следующая технология которая я упомяну - это VP Тарантул сам по себе умеет реплицировать а шардирование в нём реализовано в виде отдельного модуля чтобы всем этим управлять был е карт изначально он создавался как фрево для разработки изначально он создавался как инструмент управления кластером шарда Но со временем разросся до целого фреймворка и он закрывал такие вопросы связанные с и с разработкой и с тестированием он же был сердцем всей системы на нём всё работало Ну и для эксплуатации тоже он Служил Вот такая богатая экосистема но были у не особенности которые нам никак не удавалось решить вопервых быст сервер приложений которые я упоминал на это не что иное как интерпретатор компиляции и колектором это в принципе достаточно сложная штука и обычно всё идёт хорошо Но если нет то удача сладкой во-вторых во-вторых огорчало чисто функциональное ограничение логическим выше и получается что SQL ничего не знает про то что данных на инстанс лежит только часть А не все это странно разработка в принципе тоже была очень увлекательной но сложной вся экосистема Тарантула получилась достаточно специфичной многие вещи были изобретены внутри экосистемы и в других областях были в новинку поэтому вопервых более трудом Да и в принципе разработка распределенных приложений - это наука не из простых вот в качестве иллюстрации могу привести пример кода который мне достался на ревю значит смотрите функция локально открывает транзакцию потом делает несколько сетевых вызовов потом локально коммитить транзакцию Ах если бы распределённые транзакции было так просто реализовать Ну и возвращаясь к таран про эксплуатацию теперь тут возникает такой диссонанс с одной стороны картридж позволял управлять каждым репликам в отдельности но с другой стороны типичный кластер обычно выглядел как 100 роутеров 200 сторожей по реплицировать ими по отдельности но никак не хотелось поэтому картриджем управлял абл а конфиги для абла в вопросах управления конфигурацией кластера картридж использовал не кворум подход а требовал чтобы конфигурация была на всех инстанса до одного одинакова конфигурация - это грубо говоря описание Какие инстансы в кластере есть кто с кем реплицируемый а сначала клал туда этот конфиг убеждался что все готовы его принять после чего ещё раз пробегал и говорил всё нормально применяйте а это приводило к тому что любой затупил проблемы на всём кластере и делал управление затруднительным Итак у нас получился следующий план вот беря в рассмотрение эти пункты которые я перечислил мы а собираемся заменить двухфазный комит из картриджа на рафт про него я в следующем блоке более подробно расскажу параллельно реализуем распределённый SQL ничего сложного и всё это переписываем на раз чтобы рж коллектор нас не донимал наслаждаемся результатом но Забегай вперёд сразу скажу что у нас всё получилось но обо всём по порядку и посмотрим наите и так рафт рафт - это алгоритм решения задач консенсуса в сети ненадёжных вычислений так в Википедии написано а я попробую более с простыми словами объяснить что это значит вот смотрите здесь на картинке стопка жёлтых карточек это узлы кластера их много они одинаковые и они хотят согласованно хранить некоторое состояние состояние - это Маши зелёный прямоугольник Справа сверху просто так передать это состояние между узлами мы не можем мы уже убедились в этом на примере картриджа а вместо этого рафт нам предлагает отслеживать изменения в этом состоянии записывать их в некоторый журнал Ну и объясняет Как правильно этот журнал реплицировать чтобы на всех инстанса он получился одинаковый если на всех инстанса мы получим одинаковое продвижение по журналу то и состояние индуктивно будет тоже одинако нас это полностью устраивает что именно представляет из себя журнал и состояние алгоритм Вот который описан в статье он не специфики разработчика но если мы посмотрим как это мачи на нашу экосистему то получится следующая картинка состояние у нас хранится в тарантуле ру собственно те данные которые там лежат журнал Мы тоже пишем в Тарантул и И те и другие представляют собой отдельные спейсы Я здесь хочу провести аналогию с файловыми системами Возможно это чуть-чуть свет прольёт значит смотрите вот возьмём какой-нибудь X4 состоянием е является совокупность файлов которые она хранит но где-то там рядышком под капотом есть журнал файловой системы про который не все даже догадываются тем не менее Он критически важен для корректной работы файловой системы Тарантул в свою очередь на файловую систему пишет два файлика снапшоты снимки состояния и Файлик ило это Log обычный в котором он записывает изменения которые в состоянии происходили но пользователю пользователь тара про хло ничего не знает у него есть набор спайсов который можно что-то записать вот аналогичная история получается с Пика датой пиката в спейсах Тарантула хранит данные состояния в другом спейсе хранит журнал а пользователю выше предоставляет некую более высокоуровневые суус модуль я про него ещё ничего не сказал это как раз тот кусок логики который реплицируемый делает он это по запросу клиентов но здесь тоже стоит упомянуть что клиентом в первую очередь является сама Пика дата а пользователь запросы оформляет Ну не напрямую в raft а посредством других интерфейсов Итак имея рафт У нас есть механизм который может обеспечить нам консистентность кластера а мы продолжаем нашу экскурсию и взгляните вот на эту картинку она иллюстрирует идею о том как пиката собирает и объединяет несколько разрозненных инстан в кластер У каждого инстанса есть своё локальное хранилище А кластер менеджер позволяет всем этим управлять централизованно каждый инстанс как я уже сказал это Тарантул И пиката в чём заключается управление например координирует кто с кем должен реплицировать у нас маленький взрыв мозга обычно происходит смотрите у нас есть два уровня репликации уровень репликации Тарантула внутри реплика Сета и уровень репликации на уровне кластера Это какие-то другие данные причём на этом сложности не заканчиваются Я здесь пропустил ещё один слой шардирование на этом уровне пиката должна ещё настроить где роутеры где Раджа и если у нас добавляется новый стод то об этом надо оповестить все остальные роутеры причём делать это важно до того как на Stage польются данные иначе получится Так что какой-то роутер просто не знает что часть данных уехала и пользователь получит неполную выборку Ну или что-то плохое другое произойдёт ещё одна картинка которая иллюстрирует Ту же самую мысль но с другого ракурса значит смотрите у нас инстансы объединяются в реплика кассеты объединяются в тиры на тирах Я сегодня подробно задерживаться не буду но отмечу лишь что это некие логические тоже логические группы которые позволяют нам отличать роутеры от сторожа Ну и вот такие проблемы которые в картридже не были закрыты на этом слое решить а на Верхнем уровне управляет всем этим кластер Пика даты едем дальше то что я сейчас рассказывал было посвящено одному фиксированному состоянию кластера но Давайте теперь обсудим как это состояние изменяется во времени у нас инстансы могут появляться уходить и так далее Давайте определимся Что такое состояние кластера вообще определение достаточно логично состояние кластера - это совокупность состояний всех егов состояние каждого инстанса - это один из двух вариантов онлайн или оффлайн и вот что важно слово состояние вообще с английского можно перевести в двумя вариантами А статус и State вот нам важно что мы используем именно слово State и это подчёркивает что конгу состояние инстанса - это не столько его статус онлайн олай а то как остальные инстансы сконфигурированы по отношению к нему приведу пример если на инстанс написано что он онлайн Это значит что он под нагрузкой роутеры шлют к нему запросы И если в какой-то момент он вдруг перестал пинговать то это значит что его надо Поскорее бы уводить какие-то действия предпринимать кто же всё это в кластере делает и здесь я вернусь к предыдущей теме про рафт и расскажу ещ один кусочек рафт протокол централизованный в нём выбирается один единственный Лидер который управляет всеми остальными инстанса а инстансы сидят фолловерами и просто слушают и подчиняются ничего сами не делают за одним исключением Если вдруг какой-то фолор замечает что Лидер пропал долго от него не получат ни данных р битов то он переходит в специальное состояние кандидат и начинает новые выборы чтобы стать лидером инсу нужно собрать кворум рафт - это кворум протокол и все решения которые в кластере принимаются должны подтвердить N пополам плю о инстан сов большинство А так вот и здесь становится ещё одна интересная Инженерная Задачка связанная с отказоустойчивость у нас Мы предполагаем что кластера будут достаточно большие сотни инстан сов и более и обеспечивать кворум в виде там 51 инс из 100 не слишком рационально потому что вероятность отказа в такой ситуации она только повышается с ростом количества инстан Ну понятно что ра создавался для лучше всего себя показывая там в случае с тремя пятью инстанса которые распределены по нескольким дата центрам А если мы всю сотню заставим голосовать то это во-первых накладные расходы увеличивает Ну и во-вторых отказоустойчивость от этого никак не прибавляется чтобы этого избежать в пикате система построена следующим образом в кластере у нас может быть сотни Инста изу даже не пытаются переходить в состояние кандидата не пытаются начинать выборы записи не подтверждают ничего такого вот а всю нагрузку по обеспечению алгоритма Раф реализуют вот эти пять Воров из которых один выбирается лидером а остальные сидят фолловерами на этом закончу про кластер и Давайте перейдём к про хранение данных как я уже сказал пиката база данных SQ ная а у нас есть таблицы и таблицы бывают двух видов глобальные и шардирование И те и другие в принципе реализованы поверх тарантуле Спей сов но есть масса отличий глобальные таблиц реплицируемый глобальных таблиц одинаковое на всех инстанса в кластере Там же в глобальных таблицах пиката хранит состояние кластера которые я только что показывал а в шардирование тарантулом схема данных у нас является общей на весь кластер а вот здесь для устрашения приведена картинка которая иллюстрирует многообразие фи кото пожи это один только Ну я попробую объяснить какта работает с какда реализует распределённый на примере одного запроса значит за отправную точку мы возьмём следующий незамысловатый вопро запрос где а ра еде колонки первичный ключ А по нему же выполнено шардирование и вторая колонка б которую мы селекти значит чтобы этот запрос выполнить Ну первый этап вопросов не возникает нужно построить а абстрактное синтаксическое Дерево у дерево выглядит следующим образом на Верхнем уровне у нас есть один запрос Select который состоит из трёх частей projection это Какие данные мы селекти колоно B второй - это сканирование мы сканируем всю таблицу т и условии селек Мы сравниваем один ключ после того как построен из него надо построить план запроса в и это достаточно сложный процесс построения Я в деталях его рассказывать не буду функция которая это реализует больше тысячи строк но у нас есть статья на хаб в которой это всё подробно описано гуглите по слову я расскажу лишь основные этапы которые пония вот эти вот три основные узла шли в ряд то здесь они уже перегруппировка потом мы фильтруем элементы отбрасывая теплы которые нам те строки которые нам нужны и в конце из полученного набора данных Мы выбираем те колонки которые пользователь просил отдаём ему вторым пунктом на при построении иара АСТ обогащается и обогащается информацией о том какие вообще Колонки есть форматы таблицы и следующее немало на Значит на этом трансформация не заканчивается и ещё один шаг идёт нам нужно определить как распределены данные вот здесь жёлтым выделены специальные узлы которые описывают распределение в данном случае всё просто у нас всего одна табличка и данные перемещать не требуется Но это информация понадобится нам позже в более сложных запросах вот пробежимся ещ раз по этой части исполнения запроса роутер берёт SQL парсит его синтаксически строит АСТ на основе АСТ трансформирует получается план запроса И после этого его можно передавать в Эктор экю - это отдельная эпопея оно также выполняется в несколько этапов т во-первых это выполнить поправлять на сторожа вот этот получившийся план собрать результат и вернуть пользователю но так как у нас данные порро мы знаем как и лишний лишних действий предпринимать не хочется то на этом этапе выполняется такая оптимизация экзекутор может определить кае РД сходить делает это см обм опять и смот накладывает ли данный узел какие-то ограничения на на данные вот сканирование например не накладывает сканирование просто сканирование там надо просканировать все все шарды все Раджа А вот ул 1 он поня эе на одном единственном инстанс на котором хранится сегмент с условным номером 10 Пользуясь этой информацией экзекутор может сделать следующий шаг и отправить разослать запрос по сторожам на Раджа происходит ещ кусочек магии силу начинает работать уже таранный SQL который я говорил которого нам было недостаточно но он нам всё-таки пригодился полученный план запроса трансформируется в инструкции ВД роутер этот вбе исполняет и возвращает пользователю готовый кусочек данных вот опять проем что дее разбивает ER на под деревья Сейчас я расскажу как для каждого проводит bucket Discovery потом делает Map rce мы получили данные и в предыдущем примере Select B From T там всё было просто там одна таблица но если мы представим себе какой-нибудь Join то может случиться Так что таблиц по которым нам надо Джони по шардирование за один проход это ВС дело собрать В таких случаях в добавляются на этапе построения специальные мо узлы и Эктор когда этот план выполняет он делает следующим образом он также снизу вверх проходит по всем узлам и если встречает Motion то он отдельно конкретный этот треугольник исполняет собирает в табличку и подставляет результат выполнение в ER уже в виде просто п данных и переходит к выполнению следующего узла второй исполнил и потом когда это всё дело соединяется в третьем там уже можно провести какую-то дополнительную обработку полученных данных например это может быть агрегаты сумма какая-нибудь что-то такое тот же Join в конце концов что-то я быстро всё Проговорил и готов переходить к выводам значит смотрите у нас какая экскурсия получилась мы поговорили про историю Откуда всё это взялось я рассказал как реализован алгоритм ф как кластер вообще устроен Как устроена иерархия топология кластера про истан реплика подчеркнул что важно использовать й а не статус и как рафт централизованно выбирает лидера лидера чтобы всем этим управлять стейта про динамическое переключение про динамическое переключение голосующих узлов мы обсудили проп Воров на весь р в конце концов Как работает распределённый SQ А у меня на этом всё спасибо за внимание Спасибо большое за доклад Ярослав а вижу сразу достаточно живой интерес Пойдёмте с первых рядов вот Владимир задавай вопрос Добрый день Владимир гу зенцов компания теньков Ярослав подскажи пожалуйста ты сказал что таблица бывают глобальные и шардирование и если могу то будет ли в момент пересчёта шардов кластер работать как на чтение А так и на запись смена ключа шардирование только через пере наливку через создание новой таблицы засунули в неё старые данные начали пользоваться новой таблицей такого Что пр живой таблице меняется такого не предусмотрено и если можно ещё один вопросик Ты рассказал что среди всех инсо нот у кластера голосует всего не более пяти машин если соответственно вдруг умирает одна из пяти и у Вас остаётся их четыре кием голосую на какой-то из не голосующих То есть ситуация что у вас В голосовании ничья невозможно Ну мы стараемся максимально отодвинуть то есть кластер останется без кворума только когда у вас из сотни инстан сов останется меньше трёх всё понятно спасибо за вопрос А так давайте вот поближе к центру зала переместимся вопрос Когда получается можно в микрофон пожалуйста У меня вопрос Когда получается добавляется ещё одна шарда ну шардирование распределение данных между ними если вы добавляете новую шардирование ни не мешает перевести кусочек данных из одного места в другой то есть там между как бы ключами шардирование и непосредственно слоем хранения данных У нас есть ещё один lel of в виде который нам позволяет всё это делать достаточно просто Спасибо Давайте опять к первому ряду вем и потом вот тоже в цент зала к камере Добрый день спасибо за доклад Меня зовут Максим я из ВК подскажи пожалуйста а как шард жил без рафта и глобальных таблиц там же тоже нужно вот эту мап Батов к конкретным реплика сетам как-то держать консистентность эно закрывал в своё время картридж картридж делал Что значит шарт говорил чтобы у вас кластер работал пожалуйста сконфигурированный по времени во-вторых надо убедиться что ничего не забыли вот картридж это делал двухфазный коммитом он Раскаты конфигурацию на все инстансы потом говорил ОК там было небольшое окно с некон систентки вот образом оно работало А с этим действительно была проблема в том что на каком-то какой-то инстанс мог уйти оффлайн между а там подготовкой и коммитом вот этого двухфазного комита и тогда кластер оставался на некоторое время в неопределённом состоянии надо было прийти и ткнуть э чтобы он до применял эту конфигурацию а сама переливка Батов вот их миграция из одного реплика Сета в другой вот я так понимаю что Рже может работать и без картриджа вот этот момент сама переливка самой перелив кой занимался тоже централизовано один из инстан сов шарда Ну один из инстан сов Тарантула решению рда и в принципе это Ну могло в спокойно могло вызвать какие-то проблемы Но по опыту не вызывало То есть просто чисто везение спасибо спасибо Следующий вопрос из центра зала так Сергей X5 тех у меня пара коротеньких вопросов первый как политика вот кластера создания в пикате происходит как в Грин ПМЕ например что это жёстко как бы у нас один возможный кластер Да столько-то шардов всегда и больше никак Или например как Хаусе возможно что на одном кластере на одной парке машин Существует несколько как бы вариантов кластеров что там тебе и с одним фактором репликации и максимальное количество шардов и так далее я вот возможно это пропустил Потому что чуть позже подошёл доклад я вот эту вот картинку покажу сейчас вот эту Вот Угу У нас всё максимально гибко у нас инстансы объединяются в репли касеты репли касеты объединяются в тиры с точки зрения р - это отдельная группа хранение данных но не обязательно хранение это могут быть просто роутеры просто там или вот другой пример голосующие узлы про которые я говорил вот их целесообразно вынести в отдельную как бы группу чтобы они не рутин ничем не занимались А вот рафт обслуживали и всё было хорошо и на любом уровне мы можем динамически управлять добавлять новые инстансы кластер сам их сконфигурирован 10 шардов а в другом 20 один инс Нет один инстанс принадлежит ровно одному репли касету и один реплика принадлежит ровно одному тиру То есть у нас здесь строгая вложенность я понял спасибо быстрый вопрос касательно построения запроса учитывает ли как быта то что данные например распределены по ключу шардирование Да и как бы группируются по этому ключу значит можно как бы группировку выполнять локально на внутри одной машины не нужно перераспределением заниматься Вот и если учитывать то это автоматически происходит или необходимо например как бы вручную инструкцию как бы прописывать что сделай операцию локально учитывает давайте я тоже слайд открою подходящий вот этот да пиката при исполнении запроса умеет определять во-первых Какие данные надо отдать во-вторых где они лежат и при необходимости она то есть в общем случае исполнение запроса требует на все Раджа отправили к запроса со всех собрали Но если у нас например есть ра е который является ключом шардирование а является ключом шардирование то без проблем определить что отправлять такой запрос имеет смысл всего на один СТО на него сходит и вернёт данные исключительно с него поделюсь Спасибо большое привет У меня два вопроса один очень короткий а Тарантул ванильный у вас или вы как-то его чуть-чуть может быть дорабатывается перед использованием А мы его дорабатываем У нас есть РК А и В продукте мы пользуемся исключительно форком но всё что пригодно для отдачи в upstream мы стараемся отдавать в upstream А если не секрет то что именно дорабатывается а-а значит если говорить про апстрим темы то много было патчей связанных с экспортом символов с опиш Тарантула которые торчат наружу просто самого Тарантула не всегда об этом задумывались и когда мы какие-то вещи делали нам банально Не хватало то есть там функция в тарату есть но она приватная символ Нару не торт вот такого Много сейчас обширная работа идёт параллельно над ку синком в тарантуле синхронной репликации Мы в этой части пытаемся тоже контрибуций которые только у вас вот что именно А это по большей части всякая специфика которая нужна нам Ну вот например Тарантул он же представляет из себя отдельный Бинар А нам как-то надо а я говорю что у насра капотом вот один из первых таких патчей который в астрим не пошёл - это C сборка которая Пика фу которая Тарантул собирает в статическую библиотеку do а не в Бинар Ну и дальше это подливы к нам О'кей понятно слушай ещё второй вопрос вот была картинка где был ра headlock да и там State А вопрос Там было два значка Тарантула это два инстанса или просто схематическое изображение что здесь таран И здесь тоже это схематическое изображение зона ответственности Я бы так это сказал ну то есть и и он в одном Ира это два ССА в одном инстанс Тарантула Спасибо за вопрос а Давайте следующий вот с третьего ряда Здравствуйте спасибо за доклад если в кда медовые транзакции Если нет то когда будут будут пока нету Мы работаем над этим А И второй вопрос вот вроде бы в третьей версии Тарантул отказался от картриджа да да вы с этим тоже Что что-то будете делать Или у вас пока будет версия с картриджем мы значит смотрите то что таран отказался от картриджа это как раз про кластер менеджер про управление продуктом целиком про его конфигурацию и в этой части мы ничего делать не собираемся у нас кластер менеджер свой нам от Тарантула важен движок к хранения локальный там SQL в dbe вот это вот всё А поэтому вот так вот спасибо Так давайте э обратно на первый и потом на третий ряд Мне кажется мы все вопросы на самом деле успеем обсудить потому что времени у нас с запасом отлично Спасибо ещё раз Такой вопрос э уточняющий рафт используется не для данных он только для состояния кластера и настройки да для данных состояние кластера - это тоже данные хорошо для клиентских данных для для клиентских данных тоже да то есть значит я просто тогда уточню вопрос Спасибо тогда получается что у нас происходит с записью является ли она Ну мы можем писать только в одну точку получается Здесь Да в мастера А запись в глобальной таблице действительно проходит через одного мастера лидера рафт лидера сейчас значит не с этого надо начинать смотрите а в глобальной таблице можно писать в том числе пользовательские данные для чего это нужно Ну представим себе какой-нибудь справочник достаточно популярный кейс Когда у нас шардирование вот справочники имеет смысл реализовывать именно на глобальных таблицах чтобы как бы содержимое есть везде и любой Реж на какой бы запрос не прилетел он себе этот э там для джоина э из этого справочника информацию подтаскивание идёт только через рафт лидера но вы не обязаны ходить на рафт лидера Напрямую это всё происходит под капотом весь роутинг естественно а тогда вот в этом один ука А про шарды мы храним только состояние куда ходить дальше на роутеры Да образно Ну это всё на уровне топологии конфигурации кластера известно то есть вы приходите на любой инстанс там в глобальных таблицах в конфигурации написано как вообще кластер устроен Из чего состоит Там же в статусе написано кто сейчас Лидер понял И тогда наверно последний такой немножко вопрос В некоторых местах встречался что у рафта есть проблема с Ну при исходим во-первых могут возникать задержки да Когда происходит перевыборы и сталкивался с информацией что когда рафт растянут кластер у него тоже могут начаться проблемы определённые вот есть ли какая-то какой-то от вас опыт в этом плане А ну вот с тем что слишком это же вопрос про количество голосующих узлов Я так понимаю с размерами кластера да Да и нет Как это влияет влияет это негативно то есть слишком большие кластера делать не стоит Вот как нам тцд завещал 1 3 57 357 максимум Да так оно и есть а то есть количество голосующих узлов стоит выбирать исходя из м доступных ресурсов условно говоря это там два цода по 2 и оди в третьем цоде какой-нибудь координатор либо 3 ДЦ если три если у вас 3 ДЦ Ну я понял то есть есть ограничение по растягиванию самого рафта Да рекомендуемые А с точки зрения вот когда мы там удовлетворяем условием что Раф растянут не более чем на се например нот Да ну сталкивались ли с какими-то проблемами в этом кейсе то есть какие-то задержки дополнительные перевыборы которых не должно было быть и так далее м в таком в такой ну интересен просто опыт промышленный в такой прикладной постановке ничего не могу подсказать Спасибо Угу Так ну и Давайте да Вопрос Последний из третьего ряда Спасибо Ярослав очень классный доклад У меня есть серия небольших но мне кажется сложных вопросов по поводу вот в реплика сеете есть много от learners Да у знающих реплик так сказать которые Ну только читающие Да по сути Они они не голосующие они не участвуют в выборе Да и пять реплик которые каждый из которых может стать мастером и ты говоришь что если одна из этих пяти недоступна то мы там другую из этих там 95 может сделать голосующие Кто принимает решение о том какую реплику выводить из состава голосующих какую вводить сам ли консенсус из этих машин или какие-то отдельные там пинго может быть это ВС строится на основе Ну делает это ВС Лидер а на нём он достаточно много э логики на себя берёт во-первых э перевод состояний инв онлайн офлайн то есть instance перестал пинговать Лидер один из компонентов там failover мониторинг на рафт лидере это замечает а отправляет в рафт команду что Ребята у нас Таргет состояние поменялось Вот это инстанса надо его убрать а значит кворум подтверждает изменения сначала целевого состояния потом вступает другой компонент который уже говорит Ага У нас нам надо Давайте начинает обходить все инстансы по одному говорит так ребята применяем новую конфигурацию вот с этим не общаемся всех обошёл говорит я всё проверил всё больше никто на него не ходит текущий обновил тоже Ола Вот как-то так оно развивается то есть какой-то отдельный ещё один модуль консенсуса такого Мета консенсуса по поводу состава этого кластера не те же самые машины выбирают Кто из не голосующие Да тот же самый просто это просто у нас консенсус модуль такой жирненький получился тогда Правда ли что если из этих пяти машин внезапно три будут недоступны тогда Лидер не сможет кворум выбрать новых голосующих Ир станет и по мы делаем ставку на то что такого не произойдет потому что вы правильно распределили эти пять голосующих потому что пиката правильно распределила этих пять голосующих она я про это ничего не говорил но там среди прочего есть указание зон доступности соответственно пиката Принимая решение может смотреть в каком до оно у вас разп и она будет статься тащить их так чтобы они были максимально далеко друг от друга а вот хотите такую иллюстрацию приведу А значит кейс с двумя дата-центра а там 50 в одном 50 в другом и чтобы отказоустойчивость этого дела обеспечить мы берём где-нибудь виртуалку а которая ничего не делает только работает вот этим вот raft для файлове висом таким Да значит если в какой-то момент у вас один ДЦ отказал там связанность потеряна то пиката на вот этих двух оставшихся сможет перевести голосующие узел То есть у нас останется три но два из них в одном дата-центре И вот тогда наступает щекотливый момент если ещё значит если двум из этим Если два из этих окажутся в одной стойки им не повезёт тоже погаснуть одновременно то всё мы остались без кворума Вот Но бы все инструменты у нас в руках чтобы такого не допустить но фактически отказ трх реплик уже может приостановить кластер в неудачном случае Ну тут ничего не поделаешь спасибо Так у нас есть время ещё на один вопрос Давайте последний и нужно будет подумать какой больше всего тебе понравился такой е вопрос появился А вот смотри ты рассказе условно чтобы запустить Пика дату надо значит сформировать реплика сеты их объединить в тиры там будет много инстан сов нужно выбрать пять которые будут собственно участвовать в рафте э как будто бы очень много асов Как вы вообще плоте А это всё происходит автоматически под капотом вам нужно только запустить PK dat Run с нужными параметрами грубо говоря стартовать Значит я стартую мне надо lon такой-то порт у меня такой этот самый failover до географическое расположение указываешь Пир к кому коннектится в кластере уже существующему а он туда идёт с Jo запросом говорит я тут новенький значит что мне делать хоп его приони вот тот вот консенсус тот Лидер рафта который там уже существовал он его добавляет в таблицы И постепенно вводит в эксплуатацию это делает не пользователь это делает пиката сама то есть условно один демон поднять с нужными параметрами на группе серверов и всё хорошо будет да а плоте наме плом на Ну да наме как основной вариант как запасной р и доке смотрим вперёд под это тоже готовится тамр есть А в какой не я понимаю какое-то сетевое хранилище не отвечу Нет локальные диски Ага всё понял спасибо Хотя тоже не уверено Спасибо можно будет более подробно пообщаться покопать ещё в дискуссионной зоне Я думаю если на конференции есть люди которые знают ответ на вопро ком приди Да твоя очередь выбирать лучший вопрос прежде чем мы лучший вопрос выберем У нас есть ещё отдельная рубрика А значит смотрите пиката это не только компании но и продукт наш опенсорс най и мы маленькие но быстро развиваемся и у нас среди прочего есть один контрибутор который у нас не трудоустроен Я бы хотел его а есть он у нас в зале Роман Роман а подойди ко мне после А мы тебя давай сго держи ручай Роман Проходи пожалуйста на сцену торжественное вручение Спасибо за то что вносишь вклад в развитие сообщества Пика даты и Давай теперь к вопросу теперь к вопросу да А Вы если что можете стать вторым контрибутор матрёшку собирай коллекцию ходи по конференции мотика они все разные и будет целый набор будешь потом всем показывать хвалиться Спасибо большое Ты тоже не убегай Спасибо тебе за доклад подарочек от онтика от нашей конференции Спасибо что делаешь it сообщество лучше прогрессив нее Успехов тебе в этом деле"
}