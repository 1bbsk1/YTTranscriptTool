{
  "video_id": "Nz4750cHqFk",
  "channel": "HighLoadChannel",
  "title": "Безболезненная подгрузка миллионов товаров с сотен интернет-магазинов на PHP / Иван Красников",
  "views": 242,
  "duration": 2395,
  "published": "2023-10-06T07:23:20-07:00",
  "text": "Всем привет очень рад что вы пришли честно меня зовут Иван Я технический директор компании Search Booster я разработчик в том или ином виде пишу код уже на последние 25 лет плюс Ну начало Это был всякий бейсик Паскаль сейчас это больше PHP питон его и вот те части в моем докладе которые будут связаны с кодом они в основном будут про PHP Я бы хотел поделиться опытом который мы получили в ходе реализации одного из задач одного блока задач моего текущего проекта а в общем перед нами стояла такая задача есть набор интернет-магазинов достаточно большое количество они не против поделиться с нами Своими товарами и ценами потому что они хотят организовать поиск непосредственно по этим товарам и ценам соответственно тут задача парсинга у нас не стояла у нас стояла задача периодически каждый час это все выгружать и класть в наш elasticsearch Или opensearch дальше над этим есть некие части которые связаны с поиском Но сегодня в рамках моего доклада я буду больше говорить про вот этот первую часть про подкачивание вот эти файлов и про непосредственно вставку в базу данных то есть ну на конференции были довольно хорошие интересные доклады про поиск если чуть углубиться задачу ну можно в таком виде представить То есть у нас есть набор xml файликов это в рунете это в основном 95 процентов файлики формате Яндекс Маркета то есть свое время там не знаю сколько лет назад Яндекс Маркет придумал такой стандарт такой xml личку в которой есть товары цены описание и наверное во всех движках интернет-магазинов и есть такой формат поэтому мы с ним интегрируемся в зарубежке Это скорее всякие Facebook merchan Fit Google мерчен Фит и ну смысл примерно похоже Вот такая задача Она не только у нас возникает Лично у меня даже допустим на предыдущем проекте тоже похожий вид задач появляется мы только мы выгружали не для поиска генерировали интернет-рекламу то есть объявление в Яндекс Директ и все такое И даже если посмотреть чуть шире то это могут быть файлы не обязательно интернет-магазинов это могут быть какие-нибудь это база данных минкомсвязи логи всякие данные адресов кладр Ну то есть такой паттерн достаточно популярный и если вы будете это делать то скорее всего вас получится примерно такая система то есть вот этот некий внешний контур с набором всяких разных xml фидов Ну либо там каких-то других файликов от разных хозяев появляется планировщик который смотрит Какие непосредственно фиды в данный момент Нужно обновить То есть те которые давно не обновлялись появляется очередь задач задача которой брать задачи выстраивать их в очередь передавать загрузчику загрузчик выгружает получает задачу выгружает ее непосредственно с сайта и вставляет власть к сердцу схема такая достаточно простая нормальная работающая тут Главное чтобы все выполняли свои функции то есть чтобы планировщик брал именно те задачи которые нужно которые не нужно обновлять чтобы он не брал очередь чтобы она работала стабильно желательно уникализировала задачи потому что бывают всякий сбой вдруг мы там не успели что-то вовремя обновить чтобы задачи не копились чтобы если нам тот же фид нужно обновить второй раз подряд чтобы мы это две задачи склеили Ну и загрузчик соответственно должно быть достаточное количество загрузчиков чтобы не успевали разбирать эту очередь быстрее чем она Пополняется Казалось бы на этом все то есть проект реализованным VP готов можно устраивать корпоративы с алкоголем безалкоголем тут зависит уже от политики компании Но к сожалению в реальной жизни разработки mvp это лишь какой-то первый этап и все самые интересные неприятные вещи происходят уже когда проект переходит в продакшн Этот проект не исключение Так как вот это количество вот этих внешних фидов достаточно большой и приходят Все время какие-то Новые магазины которые добавляют и начинает расти нагрузка на Саппорт то есть что-то не грузится и изначально когда нету какой-то удобной системы где владелец этого файлика видел бы причину почему не загрузился соответственно Саппорт нервничает напрягается и так как у нас Саппорт умный он к нам пришел говорит А давайте мы будем части вот этих логов неудачных показывать прямо в интерфейсе неделю и он соответственно сможет реже нас нагружать потому что он уже будет видеть что он там бурле Ошибся там файл 404 отдает Ну что-нибудь такое фича изначально показалась хорошей правильной Ну примерно как-то так реализовали то есть там Вот появилась строчка видно что человек там бурле ошибся или там хост не рисовалится все классно Можно опять устроить корпоратив если предыдущий там провалился либо был Достаточно давно и но вполне возможно на этом корпоративе Как нам подходит Саппорт и говорит клиент обратился И видит такую странную строчку что это вообще означает тут некая водная То есть у нас Мы при индексации используем кликхаус для сбора всяких статистических метрик и библиотека которую мы использовали она в своем эксепшене указывает помимо урла непосредственно логин пароль и вот когда кликхаус мигнул в Облаке все фиды которые в этот момент загружались получили такую замечательную ошибку и мы сказали вот клиент вот логин пароль хаоса может сам ходить там смотреть любые чужие данные тут немножко нас спасло то что контур закрытый То есть у него еще нужно было попасть но была прям неприятная такая грустный момент если что-то подобное случится в вашей жизни никому никогда об этом не рассказывать Какой вывод можно сделать Ну показывать статус загрузки вот этих фидов это фича хорошая но нужно это делать обдуманно И пока и не показывать явно тебе Exception который прилетают нужно немножко какой-то слой рендера приделать хотя бы в таком паттерне когда мы что-то вот загружаем внешнее как я уже говорил большинство проблем связано что что-то не грузится И периодически Вот вы как разработчик ведете Этот проект вам приходят разные люди говорят у меня они грузятся у меня не грузятся у меня не грузятся и Вот соответственно подходит PM и говорит ну не грузится к этому моменту возможно уже выработался некий такой алгоритм разбора проблем и так как большая часть проблема что файл просто физически недоступен пьем Может достаточно умный и проверил открывается ли этот файл у него в браузеере да файл открывается всё корректно Ну давайте смотреть находим где-то примерно такой код то есть у нас есть курул который выгружает https Файлик кладет его локально и выводит ошибку если она есть и всё Окей если соответственно ошибки нет запускаем видим что что-то у нас с эсселем то есть не получается получить локального сертификат подписанта Ну надо все-таки перепроверить открываем в браузере видим что хром у нас горит все окей сертификат действительно никаких проблем с ним Нет ну надо Дальше копать добавляем строчку то есть мы хотим побольше данных получить что там происходит момент курла Как курул работает смотрим видим примерно то же самое но появляется какая-то загадочная строчка с cr.pm файликом мы Разумеется не знаем что это за Файлик начинаем гуглить ночь не открываем его локально смотрим что в нем находится и это какой-то список ssl сертификатов точнее подписи ssl но интересно что же такое-то немножко исследуем эту тему В общем в интернете два узла могут установить ssl защищённое соединение и без каких-то центров сертификации то есть они могут просто договориться обменяться ключами но тут возникает проблема что нет гарантии что тот веб-сервер с которым мы общаемся Это действительно он а не там какой-то злоумышленник который встроился между нами и можно было бы вариант что мы бы у себя локально хранили подписи публичные всех всех узлов в интернете и просто сверяли что сделали это сайт не тот но это очень нереальный подход в условиях современного интернета соответственно появились центры сертификации то есть мы доверяем вот этим вот определенным центром а они уже подписывают проверяют что вот этот владелец веб-сайта действительно в какой-то момент этим доменом владел выдают им сертификат и он этот сертификат соответственно нам отдает Вот вот браузер и устройство настолько доверяет вот этим центрам сертификации что соответственно этот Файлик лежит локально и встроен в устройство в сетевые библиотеки и но у нас что-то не грузится может в браузере открывается то есть Может быть у нас Файлик протух каким-то образом то есть надо его как-то как его вообще обновлять Ну можно нагуглить вот такую штуку чуваки из курла и разработали экстракт называется В общем у них такая гипотеза не гипотеза в том что если Firefox кому-то доверяет то значит и куру должен доверять и они соответственно парсят репозиторий Firefox там новые билды они вытаскивают оттуда с цель сертификаты и делают вот такой пьем Файлик Давайте мы его скачаем подключим то есть URL позволяет можно вот этот Файлик явно указать Ну все крестим пальцы запускаем видим что файл начал использоваться но к сожалению ошибка та же самая интересно то есть ну вроде как мы все Обновили то есть мы через PHP файл загрузить не можем мы скачали себе список сертификатов как у РФ Фокса и все равно ничего не работает а в браузере в Хроме у нас все открывается а может быть что-то Firefox пробуем открыть сайт соответственно Firefox и видим что Firefox тоже не очень этот домен любит Ну тут можно дать обратную связь типа чуваки у вас сайт не открывается Firefox и наверное в какой-то момент Они это починят но нам как-то надо все-таки файл загрузить можно просто прибить куру сказать игнорируй вообще весь ssl доверяя всем подряд и мне кажется Многие так делают а сам так делал Ну хочется все-таки правильно сделать чуть-чуть Копаем дополнительно тему В общем Mozilla у нее есть свой реестр сертификатов как я уже говорил о Хрома ситуация интереснее То есть у них изначально они брали с устройства ssl сертификата То есть у вас там Windows у вас там один набор у вас Mac У вас другой набор и хром позавидовал Firefox и они тоже стали делать свой реестр но этот начал выкатываться только в сентябре вот этого года и соответственно там в каком-то режиме ab-теста То есть у вас может быть Chrome берет сертификаты с устройством может быть у вас сборка которая уже сшитыми сертификатами а получается нам Ну чтобы загрузить Файлик нужно как-то прокачать вот этот наш пьем из Firefox и добавив ему что-то чего не хватает чтобы мы могли валидировать успешно сертификат идем просто в лоб идем в Chrome открываем вот этот Альфа ssl там которые отсутствовал смотрим серийный номер сертификата гуглим то есть не самый безопасный путь потому что угодно нагуглить но идем на вот этот Global sign видим что это настоящий alfacel сертификат что сосать проверенный берем вот этот кусочек четыре выгружаем собственно сертификат прикручиваем его запускаем и всё работает то есть мы соответственно прокачали наш пьем добавив в него отсутствующий сертификат и у нас как же так же как у Хрома всё работает тут хочется сказать что какие выводы можно сделать что ssl достаточно сложная штука И сейчас на самом деле вот эта тема она немножко набирает обороты потому что все большего количества российских компаний международные вот эти подписанты центр сертификации сертификата не выдают там допустим со сбером такое подобное случилось и Вполне возможно что вот такую штучку с прокачиванием файлика у нас Придется делать достаточно регулярно добавлять всякие российские центры сертификации которых по умолчанию нигде не будет и соответственно возможно уже пора задумываться об этом и как-то даже может быть Вот этот Файлик класть прямо в репозитории чтобы вас везде было одинаковое окружение а проблемы не Единственная проблема с загрузкой и наверное в жизни У каждого из нас Возникала когда мы пытаемся что-то загрузить А это что-то слишком большое настолько большое что даже браузер не может скачать но хотя бы пытается и Ну приходится дебажить смотрим влоги находим такой интересный кусок что у нас процесс был убит кем-то Ну системой вероятно Смотрим как вообще xml та парсим видим вначале Мы в некую текстовую переменную строку переменную загружаем 510 мегабайтный Файлик потом simplexml его парсим смотрим расход памяти видим что потребление порядка там 487 мегабайт Ну в целом немного файл большой может быть еще как-то округляется что-то потому что файл 510 мегабайт 487 вспоминаем что у Memory Get usage есть еще аргумент который называется Real usage ему Передаем True чтобы он точно прям настоящий расход памяти показал а не какой-то неточный который он показывает изначально видим расход 487 какой-то добавился а приходит гениальная идея что можно немножко еще оптимизировать и вот этот Файлик не через строковую переменную в simplexml засунуть а прямо его заставить симпл xml прям с файла его считать делаем смотрим расход памяти 2 мегабайта то есть вообще шикарно то есть мы память практически не потребляем ну прям Здорово то simplexml такое классное решение что позволяет так эффективно расходовать память но у нас же тем не менее падает у мкиллер не просто так же работает Давайте проверим наш мемори лимит То есть получается на если мы же потребляем все-таки больше чем 1 гигабайт Значит мы здесь пойдем добавляем мемори лимит смотрим потребление по-прежнему 2 мегабайта никто не упал Ну то есть вроде как честно то есть Memory usage неплохо работает Давайте проверим все-таки мемори лимит зададим там 1 мегабайт чтобы у нас точно упало что мы убедимся что мемори лимит работает запускаем смотрим по-прежнему ничего не упало Хотя явно выводится что потребление 2 мегабайта Memory лимит у нас 1 мегабайт идем в документацию оказывается мемори лимит минимальное значение 2 МБ То есть он просто округляет этот расход и Ну если мы задаем меньше чем 2 он считает что memor или limit 2 и здесь потребление 2 соответственно все окей по памяти мы не вышли но тем не менее Киллер почему-то работает Давайте его прям силой заставим умереть про нашпичный добавляем возвращаем все как было добавляем Вот это чудо переменную контент Который всю память в памятью загружает и видим что Fatal Error наконец-то произошел и вот два мегабайта Мы превысили пытались еще 510 скушать вроде работает а и что получается то есть simplexml такой эффективный что он может Файлик практически Ну то есть без дополнительных расходов памяти парсить большие файлы и но тем не менее мы падаем То есть почему simplexml может быть таким эффективным Наверное потому что он в других вариантов нет кроме того что он с диска считывает непосредственно от xml Fit и не тянет его в память А бегает по диску когда мы парсим соответственно если мы Файлик удалим то получается Simple xml сломается потому что ему негде будет бегать делаем удаление файла следующей строчкой специально пробегаемся по xml и видим что расход по-прежнему 2 мегабайта все нормально работать никто не упал только файлика уже нет то есть вообще какая-то мистика происходит идем в Memory Get usage документацию PHP мы находим такой замечательный пост человека под именем адера татарком который 18 лет назад замерял память у своего какого-то PHP скрипта и он измерил расход памяти через утилиту PS и через Memory Get usage и он обнаружил разницу в 6 мегабайт между этими показаниями и он предположил что наверное это вот код интерпретатора ПХП какой-то встроенный код то есть некие вот эти накладные расходы на PHP 4.3 Если не ошибаюсь который был тогда то есть 43 PHP всего 6 мегабайт у него съел неплохо Давайте померим наш расход тоже через PS запускаем Наши консольную утилиту в этот момент запускаем PS смотрим что он выводит видим что у нас аж 29 процентов памяти съедено из 16 Гб Если не ошибаюсь Вот и видим такие вот интересные столбики в СЗ и rss которые содержат достаточно большие числа идем смотреть что это такое видим что это Virtual Memory size и Resident Set size грубо говоря разница между ними это То есть если процесс сделал выделения памяти то есть молог Memory laked но память еще не съел то rss rss эту память не покажет а в СССР покажет то есть вот эта разница между как бы какой-то потенциальная сколько процесс может потребить и фактической Но от ротатор ком использовал rss давайте тоже будем использовать rss Residence а Итак смотрим переводим так как там у нас килобайты оказывается что у нас потребление памяти 4,8 Гб а мемори Get User показывает 2 мегабайта и получается вот этот современный php8 настолько в кавычках эффективность что вот он сжирает гигабайты памяти к старому хорошему PHP нужно было всего там 6 мегабайт Ну какой ужас Давайте чуть побольше всего померим то есть добавляем в наш проект такую функцию которая идет в консольку и берет показатель потребления памяти rss у текущего процесса из которого PHP запущен а смотрим Наш замечательный расход и видим что Memory Get uses показывает по-прежнему 2 мегабайта а PS очка показывает 4,2 ГБ Теперь нужно понять это PHP не торт или simplexml не торт замеряем по потребление памяти до simplexml и после Simple xml видим что расход стал то есть до у нас всего 25 Мегабайт потребилось А после 4,2 ГБ то есть не торт все-таки simplexml А если нам не нравится simplexml что еще в PHP такого популярного есть которая Ну наверное будет получше Ну кажется что это xml Reader берем его смотрим сколько он памяти ППС употребляет и видим что да потребление есть но всего 25 Мегабайт по сравнению с четырьмя с личными гигабайтами у Simple xml Ну мы уже настолько Как бы ни к чему не доверяем что нужно как-то дальше какие-то тесты провести А вдруг xml Reader от нас обманывает и пока он Файлик парсит он это все в память грузит все перегружает а в конце когда мы из него выходим он такой Все у меня руки чистые там память не потреблял то есть Нам нужно что-то типа Memory Get usage но который нам не врет Ну тут такой делаем некий Костыль измерительный то есть в момент когда мы считываем каждое там какое-то количество элементов фиде слишком часто это делать не стоит потому что вот измерение через вот этот вызов консольного процесса достаточно тяжелая процедура сравнительно и соответственно периодически пока мы парсим делаем такие отсечки в рамках которых мы замеряем расход памяти смотрим оказывается что все неплохо потребление памяти растет совсем незначительно и в Пике это также те же 25 лишним мегабайт а все прикольно мы разобрались Мы хотим уже устали от этого проекта хотим уехать в отпуск Но мы его не можем оставить то есть до Сегодня мы simplexml починили Но вдруг какой-нибудь другой разработчик придет какой-нибудь библиотеку засунет которая будет также память потреблять и Memory Get usage это будет не видеть и что же делать то есть хотелось бы вот это потребление памяти засунуть как-то в тесты но причем засунуть честным образом можно весь тестовый контейнер ему ограничить его потребление по памяти Ну просто там либо в кубернете Если вы запускаете либо Докера тоже можно ли имеет память и задать если он упал значит кто-то в проекте что-то плохое сотворил Но это будет как бы не на уровне Юнита это будет на уровень повыше и не так легко это встроить во все вся CD и ошибка будет не очень явно отображаться хотелось бы что-то типа такого как вот то есть сердцем потребление памяти но хочется в рамках чтобы это работало не вралоном а делаем какой-то примерно такой класс то есть условно Он каждый раз когда мы замеряем память он запоминает максимум измерений которые были и также в момент загрузки в какие-то случайные моменты в скрипте Мы замеряем наш расход и далее уже в тесте мы можем вместо Memory Get pqcy сделать наш Memory Get US который нам уже не врет вывод доверять нельзя PHP не видит всю память которую используют почему же он не видит Ну потому что вот допустим simplexml он использует либо xml библиотеку и она как бы отдельно от PHP процесса же Ну точнее она живет в рамках php-процесса но не в рамках той памяти которую контролирует PHP и получается либо xml себе это все скушал в память все эти 4 ГБ и процесс раздулся а PHP это не видит А наши снам то без разницы у нас сервер есть и это все-таки настоящая потребленная память Поэтому если вы хотите прям хорошо измерять то нужно как-то продумать все это дело например использовать PS хорошо разобрались со всеми проблемами загрузки то есть рано или поздно Мы уже классифицировали все что может не загружаться и в один прекрасный день приходится иксо горит все вроде грузится но в поиске данной старые Ну приходится опять откладывать прогулки чайные потребления всякие прочие надо что-то делать дебажем Ну так оно есть пробка на вставку данных выластик Search приходится устраивать хакатон вся команда собирается изучение всех возможных документации по власти к сердцу можно как бы железом это искупить эту проблему Но это надо деньги закупать где-то это все брать и хочется как-то в разы все ускорить но не приступая Они покупают железо то есть как-то настройками или кодом все это порешать и вот результатом этого хакатона является три уникальных в кавычках секрета работы с эластик searcher первый секрет батчинг на Самый наверное Мне кажется вообще все базы данных в мире вот батчинг им полезен и кажется что это такая очевидная вещь которую все используют но я вот периодически к разным там проектам подключаюсь и даже достаточно крупные там получившие Там миллион долларов финансирования проекты могут жить без батченко то есть они в какой-нибудь свой пост представляют данная строчка за строчкой и просто добавление упаковки вот этих всех данных пачку И тем самым сокращение накладных расходов БД может всё в десятки сотни раз сократить расход соответственно внедряем нас все начинает работать примерно в два раза быстрее а ну тут замерили еще в батчинге интересный момент это подбор пачки То есть когда мы вставляем данные построчно Ну понятно что у нас вот эти накладные расходы на вставку максимальные и это самая плохая Точка Но увеличивая размеры пачки данных которую мы вставляем мы как бы до какого-то времени получаем прирост скорости но начиная уже дальше это может быть даже падение потому что нас оперативка кончилась либо Ну какая-то такая ровная кривая которая уже эффективность не дает нужно подобрать правильный размер пачки данных которые мы вставляем Второй секрет он уже более такой эластик Search ориентированный и он связан с такой постройкой как Refresh интервал То есть как вообще эластик Search индексирует все то есть мы вставляем и уже научились данные вставлять через батчинг вставляем пачку документов изначальный ластик все же берет эти данные decoded и вставляет в немо-ре-буфер по нему поиск еще не работает поэтому Memory buffer у есть отдельная процедура Flash которая запускается каждую секунду по умолчанию и он берет все это содержимое буфера создает у него там есть индекс который мы создаем индекс состоит из сегментов и вот каждый эта процедура Flash она создает один сегмент власти кёрчи и получается допустим 60 10 минут вставляем данные какой-нибудь Там миллион товаров и соответственно создастся получается 600 сегментов то есть на каждую секунду будет создан сегмент по сегменту поиск уже работает но Проблема в том что когда сегментов много ластик всё же тяжело по ним всем пробегаться искать и поэтому власти ксёрча есть отдельная процедура которая ведически запускается она сжимает эти сегменты в один но не в один как-то оптимизирует их И помимо этого Проблема в том что собственно есть накладные расходы на создание вот этого сегмента Итак у нас получается мы эти накладные расходы каждую секунду платим и вот мы можем Refresh интервал сделать либо большим либо вообще выключить и делать Ну как бы накидом накидываем данные в мм ребуфер и потом все одной пачкой заставляем ластик все записать вот у нас получилось ускорение в 1.86 раза что для нас было достаточно интересный благо не супер Сложно это сделать но тут может возникнуть вопрос а не упадет ли у нас эластик Search Memory buffer он как бы оперативной память и Обычно мало И что если мы ставим больше документов чем этот инемо-ре-буфер может обработать да и у нас ещё Ну так как мы Refresh Internal убрали Ну тут проблемы нет ластик Search когда у него память кончается он насильно делает сегмент и соответственно здесь проблемы нет третий секрет он уже такой то есть еще предыдущий в принципе документации по эластик всё ещё более-менее ну нормально гуглится то есть вот этим немножко сложнее есть понятие маппинг типов данных то есть При динамическом магинге мы заранее не говорим Какие данные мы будем вставлять в эластик мы их просто вставляем и ластик всё же сам догадывается это у нас строка это число и неплохо всё работает и есть Явный маппинг когда мы заранее говорим вот у нас будет Возраст это интеджер также у нас будет фамилия это стринг и мы в явно маппинге заранее все это указываю и мы попробовали сделать mapping Явный то есть до этого у нас был динамический и мы увидели ускорение аж 6,8 раз то есть прям существенно ускорение и понять его причину было Ну не сразу Короче мы догадались но в общем смысл что если у вас в кластере достаточно большое количество индексов И когда вы вот пачками это все индексируете Каждый раз когда появляется новое Поле ластик Search идет и обновляя идет в кластер и обновляет настройки индекса типа у нас в этом индексе теперь появилось поле возраста но интеджер все-таки О'Кей О'Кей О'Кей и вот в нагруженной среде Вот это Ну достаточно большие задержки добавляет и соответственно если мы переходим на Явный mapping то мы можем от этих издержек уйти и в разы ускорить вот эту вставку но возникает проблема так как мы интегрируемся с какими-то внешними случайными хаотичными файликами мы не знаем точно какие там наборы полей то есть да знаем что есть цена названия там но описание товара но мы не знаем параметры мы не знаем диагона если там диагональ экрана нету Если там вес товара нету Мы не знаем то есть Нам нужно как-то на PHP понимать Ману узнавать вот эти маппинги тут вот такой алгоритм пришел на ум То есть как он работает вот мы начинаем делаем перед непосредственно прогоном со вставкой данных мы делаем предварительный прогон для того чтобы понять Какие данные у нас вообще есть смотрим товар один видим что 1000 вес это так как 1000 это число мы пишем в табличку что это число видим диагональ 22 это тоже число смотрим второй товар видим тоже что 32 число Окей И как только у нас какое-то значение хотя бы в одном поле является строкой мы значит костим все этот весь этот столбец в строку тут могут быть когда третий товар Несмотря на то что он опять 33 является числом но мы помнили что у нас диагонали хотя бы одна строка встретилась и мы весь этот столбец костим в строку тут могут быть разные тактики То есть может быть если вот это одно единственное значение может быть проще его выкинуть и все-таки продолжать работать с этим полем как с числом Ну тут уже на усмотрение бизнес логики должно исходить и второй момент тут получается мы усложняем загрузку то есть Нам нужно дважды распарить xml первый раз чтобы Ну соответственно Вот это понять вот эти маппинги создать индексы потом делать непосредственно вставку Но вот пробег xml Reader достаточно нормально работает особенность у него там SSD диск нормальный CPU и вот эти накладные расходы на Личный прогон они прям очень небольшие по сравнению с ростом производительности за счет перехода на Явный маппинг Ну выводы типа батчинг рулит Практически везде если у вас в проекте Нет батченко нужно делать бачинг и второй вывод такой Хочется какой-то более универсальный сделать То есть вы живете с какой-то базой данных и вас все начинает тормозить и Вполне может быть что есть какая-то небольшая специализированная настроечка которую поменяв вы решите все свои проблемы и в разы все ускорить то есть волшебные какие-то настройки реально существуют и не надо отчаиваться нужно там стараться до того как масштабировать за счет железа нужно стараться оптимально конфигурировать систему на этом У меня все вопросы похлопаем Ивану тут за лучший вопрос дают подарки да правильно это должен был я сказать за лучший вопрос подарки если вопрос один получается за любой вопрос можно получить подарок Спасибо за доклад Как вы решаете вопрос Если с новыми полями понятно да То есть вы дополняете власть их а при изменении структуры в источниках когда полярные оборота не убирают Ну конкретно у нас такой частный подвид задачи То есть у нас настолько часто вот эта структура у каждого меняется что мы нам проще Не сравнивать что было что стало А на Мы каждый раз создаем абсолютно новый индекс То есть у нас вот есть поисковый индекс которым работает клиент Мы решили допустим заново обновиться мы создаем новый индекс И как только он полностью создан полностью с мержен проиндексирован мы как бы сервера переключаем на новый индекс А старый Выключаем то есть вот подход с какими-то атомарными изменениями он в любом случае когда пригодится Но тут еще момент что на рынке как бы есть вот Файлик который есть у всех его можно скачать но на рынке вроде как нет общепризнанного стандарта как изменения подгружать Ну вот в нашей тематике интернет-магазинах и то есть мы конкретно не как но задача действительно интересная и полезно Спасибо за доклад приходилось решать аналогичную задачу понятно что у вас не один единственный worker обрабатывает Да это по очереди вы параллелите конечно же обработку я сталкивался с такой ситуацией что Надо мерить когда параллельно становится более вредной чем полезной если worker запустить Очень много они будут долбиться в одну единственную очередь И тем самым замедлять самих себя плюс ресурсы не бесконечные измеряете ли вы этот параметр то есть оптимальное количество параллельных процессов но в рамках этого проекта нет то есть нам Ну мы смотрим Что у нас есть некий солей по времени обновления фидах и мы достаточно легко в него укладываться Ну просто почему-то узкие места чуть-чуть в другом количестве и у нас есть вот заведенное количество воркеров и ну там эластик Search не является узким местом пока что именно на индексацию и мы просто увеличиваем количество воркеров пока хватает Но рано или поздно мы действительно разопремся и тут Ну разные могут быть подходы то есть вот предыдущем проекте мы где с интернет-рекламой работали там вообще просто ограничение Яндекс не дает больше чем там три потока одновременного Ну там работы с API и соответственно приходится на своей стороне делать некую штуку которая будет следить чтобы мы вот Ну вот этот расшаренный ресурс не использовали больше чем X потоков и Ну тут Вполне возможно что тоже придется когда вот это место все-таки нагрузится настолько что нужно будет ограничивать либо заводить второй кластер третье кластер Ну то есть уже шардингом решать вот этот вопрос на ставку Спасибо за доклад хотелось бы спросить вот вы рассказали про то что вы сами ходите соответственно за данными Вот а если у вас API которые могут воспользоваться клиенты и загрузить самостоятельно данные своей системы но у нас всегда то какие практики соответственно там применяли у нас API есть вот именно с другой стороны есть API который говорит мы Обновили Файлик скачайте его приоритетно но вот именно на вход мы данные только Ну как бы по получается пул модели тянем то есть потому что интернет-магазин на своей стороне когда Ему удобно обновляет этот прайс-лист мы его к себе подгружаем то есть в принципе ну как бы были идеи тоже пропишки но почему-то её никто так и не спросил то есть потому что давайте мы будем по API пушить в данный момент изменений то есть есть часто Ну периодически возникает что э просьба разделить э данные непосредственно знанием товаров названием там категорийным деревом от цены наличия Потому что часто это в разных системах у кого-то цены наличие Ну там в одной системе а описание товаров в другой системе лежит идут соответственно Приходится работать с несколькими файликами один может обновляться на стороне клиента допустим раз в день или даже раз в неделю А вот эти цены наличие может быть нужно раз в 10 минут обновлять то есть вот такой подход тоже бывает но вот опишки нет Так вопросы еще в чате Пока нет вопросов спрошу Я а если вы работаете по пол-модели и у вас значит много магазинов из которых вы тянете вы как-то распределяете эти крон задачи чтобы не все ровно в 12:00 в 13:00 запускались но у нас некрон у нас сразу как бы вот очередь то есть принципе если вот ну как-то для себя это делать каком-то небольшом проекте то с кроном более-менее но там и проектов немного когда проектов много в какой-то момент возникает потребность делать очередь и уже соответственно эту очередь разгребать там же можно сделать приоритизацию То есть какие-то VIP клиенты могут грузиться быстрее в рано или поздно в такой модели приходится делать очередь иначе никак а подскажите А что используете для очередей но тут в этом или в этом проекте очередь не самая нагруженное место потому что количество интернет магазинов оно там ну не ну в докладе Яндекса было про полтора миллиона Я немножко удивился честно говоря Как можно полтора миллиона интернет-магазинов в России но это скорее всего речь шла про сайт То есть у нас количество сильно меньше и у нас там работает ларовая очереди поверх radisso Спасибо Ну что наверное пришло время подвести итог какой вопрос показался самым интересным Но вот самый первый мужчина задавал самый первый вопрос вам приз Ну и нашему докладчику приз от организаторов Спасибо за выступление Еще раз попробуем Спасибо большое следующий тех толк у нас в 14:30 практики для ускорения разработки от мира платформ смотрим в онлайне в 14:30"
}