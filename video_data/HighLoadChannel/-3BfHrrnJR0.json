{
  "video_id": "-3BfHrrnJR0",
  "channel": "HighLoadChannel",
  "title": "Как подключить к Apache Spark проприетарный источник данных / Александра Белоусова (Яндекс.Go)",
  "views": 1652,
  "duration": 2682,
  "published": "2021-10-04T02:45:36-07:00",
  "text": "приглашается докладчица александра белоусова из яндекс гол расскажет нам про apache spark различные внутренние кишки этого продукта добро пожаловать приходи привет дает всем привет я надеюсь что меня слышно вроде меня слышно меня зовут саша я работаю в яндексе я в яндексе занимаюсь интеграцией а подчас парка со всякой нашей внутренней инфраструктурой и сегодня я буду рассказывать о том что я делаю почему как у меня получилось сделать какое-то быстрое и простое решение как и почему я с него мигрировала на более сложные но более производительные ну и конечно какие грабли и костыли я по дороге собрала и начнём мы с того что вообще надо было сделать в яндексе есть своя система для хранения и обработки больших данных которые называются войти ну или по-русски все произносят как ведь можно представлять себе быть краткое некоторое такое подобие ходу по то есть у него есть часть которая отвечает за хранение данных это чем-то похоже на gdfs но есть важные отличия и the scheduler то есть планировщик ресурсов которому операции приходят и просят там какой-нибудь количеству циpкa или поймите он решает на какой но это все будет бежать и есть своя реализация mapreduce поверх этой реализации мы придется есть еще пара штук который позволяет вам не писать отдельно мапперы иридий серая написать кусочек иску или или кусочек кода на питания и она разложится нам по перу you ready sears войти это классная штука в самый большой кластеру тиару чтобы вы понимали влазят экзабайт данных в миллион ядерно которых могут запускаться операции и этим пользуются около 10000 пользователей вот прям каждый день а еще у них самый няшный вообще на свете логотип с у нас хвостом я не знаю чем у него редко показывают и вот поверх всей этой штуки нам хотелось запускать spark что значит запускать парк во первых это значит что мы хотели научиться запускаться внутри яндекса вова контура и спускаться на ресурсах кластер и тя во вторых мы хотели уметь читать данные которые быть я уже лежат и писать результаты запросов выть и финальному хотели показать для интересующих нас типов запросов all time its по твоим лучше чем у тех решений которые уже были в яндексе соответственно как вы понимаете первый пункт вот из этих трех он мне очень сильно влиял на производительность просто надо было сделать просто научиться как-то жить а вот вторые два пункта они влияли очень критично поэтому первое время существования этого проекта я в основном этим и занималась за то есть я занималась интеграцией спарка с этим как с хранилищем данных и для того чтобы понять что я делала во первых нужно знать что войти это файловая система вот если вы пользовались хдп сам надумал что почти все пользовались вы знаете что ходи фасту файловая система там есть дерево директории под директории в этом всем лежат файлики вот войти это тоже файловая система там тоже есть директории под директории файлики но есть еще дополнительный тип сущности это таблица 3 таблица в те с точки зрения пользователя это какой-то вот такой набор структурированных данных прям буквально табличка и какой-то набор метаданных которые лежат рядом которые можно посмотреть это схема фиксированный какие то типа колонок это например количество строчек в табличке ну и так далее с точки зрения хранения таблица тоже какой-то вот набор или метаданных данные они естественные лежат одним большим куском они разбиты на кусочки на каких диапазоны строк которые хранятся отдельно это называется чанки и так вот в этом состояла задача состояла в том чтобы вот такие таблички научиться читать и писать и сначала я решила сделать максимально такое простое решение на коле ночная посмотреть как она будет работать тут небольшой дисклеймер сейчас дальше будет много разного право spork-и править я буду использовать термины и пиа и испей я думаю что все привыкли к термину ой ну вот и пиа это те методы и там и классики которые мы используемую систему и вызываем методы а s.pio это набор всяких классов интерфейсов которые нам нужно реализовать или расширить чтобы все заработало соответственно все что у меня будет со стороны спарка я буду называть s.pio и все что будет страны и тебя я буду называть ее 5 что мини-фильм дорабатывая ему просто пользуюсь и начну для того чтобы понять как будет устроена чтение давайте немножко вспомним как устроен spark главная абстракция в спарке который мы все время пользуемся это dataframe dataframe по сути состоит из двух частей это его схема эта строка type и это данные которые в нем лежат и трдд жко рдд это позиционирование данные то есть пдд мы должны уметь получать набор партиций кусочков из которых он состоит а по партиции должны уметь из проецироваться по ее строчку вот такая простая штука и для того чтобы такую простую штуку читать spark предлагает нам реализовать интерфейс который называется relations и в котором по сути нужно будет определить два метода для получения схемы и для получения рдд жки и одышка у нас тоже будет состоять как бы с двух методов это получение и партиции и получения строчек по партиции и того чтения будет состоять из трех шагов это получение схемы отдельно получение партиции рдд и по конкретный партиции рдд получения строчек из которых она состоит и отдельно я сейчас еще покажу все тоже самое но на физической схеме на физическое у нас получается съесть парковое приложение оно состоит из драйверы экзекуторов рядом она стоит как от система увидеть в ней лежит табличка у нее есть данные метаданные и за первый шаг за получение схема у нас встречает драйвер он идет и смотрит чё там за второй шаг у нас тоже отвечает драйверами но он должен понять каким образом табличка разделяется на кусочки из каких частей состоит рдд а дальше он эту информацию отдает экзекьютора мы конкретно читать данные будут уже экзекьютора это важно потому что мы в спарке вообще в принципе важно понимать за что у вас отвечает драйвер за что отвечает экзекуторы это часто всегда много на что влияет это вот вот так это выполняется физически а вот так это выглядит в коде как вот такие три шага а что у нас со стороны тебя со стороны и тела у нас есть java api есть какой-то объекте который называется выйти клайн который нам предоставляет какие-то методы для работы с табличками и сейчас мы будем смотреть на то как это соединяется между собой и сначала мы посмотрим на первый шаг на получение схемы в те я таблица схематизировать и вы можете уйти попросить схему данных которые лежат таблички он вам отдаст какой-то вот такой jison и тот же сон для спарка нужно будет превратить в struck type казалось бы да легко вот есть поле а это будет страх fields типом in the jar вот есть поле б это будет строка фил с типом string дальше мы приезжаем к следующему полю и у этого поля какой то такой вот типа int64 вот есть выйти такой тип который ни на что не напиться в спаек спарте даже ни на что не нравится в джаве и вообще что это но люди почему-то пользуются ну и соответственно здесь я делаю что я перекладываю подобные типов какие-нибудь тип из парка которые мне кажется адекватным переложить мы строчку например но самое главное что я сохраняю о метаданных информацию о том что тип на самом деле он был другой вот не это потом понадобится когда я буду данные парсить который мне открыть а придут или вот например в те есть такой тип и не ну и типа лежит а то это означает что в табличке на самом деле же лежит какая-то произвольной g сонина но обычно jison не совсем произвольно это на самом деле какой-то сложный тип просто в те до некоторого времени сложные типа не поддерживались и вот такие legacy данные остались для такого я решила сделать для пользователей для подобных случаев возможность протянуть через параметры запроса дополнительные подсказки что там на самом деле тип другой и я это тоже использую при парсинге схемы ну или вот последний пример быть я нормально когда в колоночки ей название колонке есть точки вы им все равно spork-ов такое не очень нравится поэтому я опять таки переименовываю и в метаданные засовываю какую-то информацию о том что это на самом деле было и того вот так у нас устроено чтение схем и посходили в эти вы клиенте получили схему которая лежит в те превратили в structor практически напрямую ну вот бывают иногда когда какие-то шероховатости да и все следующие про что мы будем говорить это получение партиций я когда показывала что такая табличка вы те я говорила что она хранится ни единым большим куском а как ты разбит в начинке но к сожалению с точки зрения пользователя нам не видно что табличка разбито начинки для нас это просто один большой набор данных ну и в принципе ладно для простого решения я решила что я знаю точное количество строк в таблице зная точное количество чанков я одно на другое поделю ну и все вот такими кусочками буду считать умею диапазона запрашиваете хорошо да у такого подхода бывают всякие разные проблемы но в целом как бы для простого решения нормально сойдет и так реализовали метод get партий шанс он использует пару методов языка для получения набора строчек не делит одно на другое и осталось написать метод компьютер и его мы реализуем тоже через готовый метод вы те которые читают и светя строчки единственное что конечно же эти новые api он дает строчки в каком-то своем формате нам нужно все переложить парковый формат ну и по дороге если мы делали какие-то страшные преобразования со схемой тут нас просто сделать все то же самое с и надо учесть этого вот такие три шага вот так они реализуются через методы и тя это на самом деле все прочтение вот так оно устроено теперь давать также простенькая реализуем запись что он для этого нужно быть для того чтобы записать табличку выйти и нужно создать а для этого нужно знать какая у него будет схема и в готовую табличку нужно написать какие-то строчки это же строчки нужно привести к этому формату а в спорте у нас опять есть dataframe уже готовый у него есть схема в стратегии мы можем мотивироваться по его строчку и для того чтобы реализовать запись spark нам опять таки предлагает взять все тот же самый relation и реализовать в нем один метод insert enter ту на вход будет приходить dataframe он должен что-то с ним сделать и так сейчас мы будем это опять сшивать вместе spark не накладывает никаких ограничений на то как должен быть реализованы in certain вообще все равно но мне было удобно чтобы он состоял как бы из двух частей на первой части мы подготавливаем табличку в которую сейчас будем писать и берем для этого тракта и поезда то фрейма а на второй части мы в готовую табличку заливаем данные перемать и рации застройщику из dataframe а и пишем быть с помощью этого метода на физической схеме за подготовку таблицы за ее создание будет отвечать драйвер ну логично описать данные будут экзекьютора не будут брать те партии которые на них уже лежат параллельно и независимо записывать их выть вот так выполняется вот так это выглядит в коде и в целом тут я даже не буду подробно останавливаться потому что но мы делаем все то же самое что в чтении только в обратную сторону у нас теперь есть строка type a нужно получить сетевую съемку у нас теперь есть строчки в spore кому формате а нужно переложить и хоть вой ну и самое главное что нужно учесть это что вот method right роуз он вызывается с экзекуторов то есть он будет вызываться параллельно он будет вызываться независимо в те мне для этого нужен был буквально там один параметр который показывал что я сейчас буду в табличку писать из разных мест этого вот такое простое решение реализуем буквально несколько методов у нас заработала чтение заработала запись важно здесь что с чтение у нас состояла из трех шагов эта схема партиции чтение конкретные партиции а запись по сути состояло из двух шагов создали табличку и записали в нее сагзи тьюторов данные если вам интересно как именно выглядит код вот для такой простой схемы то есть подробное выступление ятса колосков kia очень советую его посмотреть очень классно и так мы очень простой подход мы сделали но почему-то я решила переезжать с ним она другой подход и давайте поймем что вот не так 100 реализации про который я только что рассказала ну во первых в этой реализации нельзя читать быть чайным что это значит я говорила что вот есть табличка выйти вот она лежит чанг и мифа хранилище на самом деле все даже еще чуть-чуть сложнее возле каждого чонко у нас мало того что данные там нем лежат еще возле каждого чанка лежат его мета-данные то есть какая-то полезная информация починку например сколько в нем строчек или какой-то минимум максимум по ключевому столбцу в общем всякие полезные штуки и spark вообще-то мог бы этим пользоваться spark если у вас есть например вот такой dataframe и выходите по нему сделать каунт если вы вычитаете построчно он будет делать это так типо вот первая строчка а те один вторая строчка 2 3 строчка три и так далее пока до 9 не до считает а если вы его научили и тренироваться по матчам то он не будет смотреть на конкретные строчки а будет смотреть просто тупо на размер бача хочешь стать так 369 классно быстро и того когда мы читаем построчно они пока лодочном и во первых не даем спарку использовать вот эти метаданные бачей для того чтобы что-то про оптимизировать он мог бы а во-вторых так как у нас данные быть я на самом деле уже лежали в по колоннам формате уже даже метаданные по ним были мы потратили на то чтобы скормить pour que pase построчно мы потратили на эту цыпу на вот эту перекладку из одного формата в другой соответственно мы дважды проиграли сделали что то не то во вторых в простом подходе нельзя прочитать директории целиком что я имею в виду я вот говорила что их dfs омывайте это файловые системы и находи fs мы можем сделать так мы можем вот директорию темпа экзампла взять и прочитать целиком все паркетный файлик который мне и лежат и в принципе на войти это тоже бы имело смысл нам чуть такого тоже хочется на самом деле нам хочется даже более классные штуки находились и если у нас директории названы специальным способом вот с равно the spark умеет добавлять то что написано в названии директории как отдельную колоночку и уметь это использовать например при фильтрации и в принципе войти ну чего то подобного бы тоже хотелось может быть не с названиями табличек но как-то этого на самом деле чего мы хотим от нового подхода мы хотим научиться читать пока логичными патчами и мы хотим переиспользовать всякие фичи которые на самом деле уже написаны для gdfs а и для паркета это вот портишь не discovery это обход директории на чтение еще всякие прикольные штуки и я стал на это смотреть и поняла что этого нельзя добиться какими-то локальными изменениями в простом подходе про который рассказывал о нужно вот именно глобально паре factory и сейчас я буду рассказывать как я и тори факториала и для того чтобы понять как это устроено тоже как устроена чтение давайте сначала посмотрим как спарк читает паркет сходи fs а в этом подходе будет принципиально важно что вот раньше у нас была одна единая абстракция которая говорила как читать и писать вот с войти полностью описывала а теперь она логически разделяется на две части на знание о том что данные у нас лежат в какой-то файловой системе тип вот х dfs и на то что данные лежат в каком-то определенном формате паркет или каком другом и для того чтобы описывать первую часть то что дано лежат файловой системе спарки есть интерфейс который называется file system это по сути ну вот прям абстракция над файловой системой который прям точно такие же методы как бы либо файловой системы например если у вас есть файлик вы можете спросить какой то его статусу значит это файлик узнать его размер или если у вас есть директория вы можете понять что это директория или если файлика нет вы можете понять что его нет или если есть директория то можно и и поместить узнать что там в нем внутри лежит тут важно что file system это именно абстракция на уровне кода то есть в принципе и даже если ваша система это не файловая система можно написать какой-то file system над ней можно как-то эмулировать поведение похожего на файловую систему следующая абстракция это вторая часть это файл формата это абстракция которое рассказывает что у нас значит данные лежат в каком-то определенном формате она работает с валиками и должна по массиву файликов например умеете понять какая схема того что в них лежит или по конкретному фалик уметь прочитать все строчки которые в нем написано и того вот у нас есть такие вот две штуки и мы сейчас с их помощью будем читать datasette который называется data set 1 и опять она с чтением будет разбиваться на шаги на первом шаге спаркс ходят в эту в эту директорию посмотрит что-то вообще лежит вот там лежат паркетные файлики дальше он отдаст эти паркетные файлики файл формата и спросит какая у них схему формам на самом деле в этот момент входит в метаданные паркета посмотрим какая схема превратит то в structor и на третьем шаге отдельно будет прочитан каждый паркетный файлик и превратится в итератор по строчку этого вот такие у нас значит три шага и на самом деле если мы вспомним что там было в простом подходит а на самом деле они довольно сильно похоже просто раньше у нас было получение списка портится для рдд а теперь у нас есть листинг директории и раньше мы читали конкретную партицию а теперь мы читаем конкретный файлик то есть так в принципе у нас портится рдд будет соответствовать паркетном у файлик у это более-менее почти всегда так и выполняться это будет в общем точно так же как в простом подходит переставить местами картинки но опять у нас драйвер отвечает зависнем коза схему а экзекьютора будут читать конкретные файлики теперь давайте смотреть как это все описано войти войти эта файловая система в ней есть директория да щас мы будем реализовывать войти file system наверное логично если нас учета спросили про директорию сказать что да это директория и если нас попросили ее по листику давайте по листьям ново войти бывает не только директорией файлики а бывать чаще таблички и когда она спрашивают что такое табличка я решила на уровне и file system as паркового отвечать что это директория в том что то выбор да как был в системе слива файл либо директория я решила сказать что это директория для чего для того чтобы дальше ее можно было поместить и этот листинг мог бы вернуть мне диапазоны строчек таблицы в каком-то удобно мне формате есть по сути чанки эти границы чанков и сделала я это для того чтобы дальше spork-ах начал у меня читать табличку и тогда он на первом шаге по листик мою таблицу поймет что она состоит из вот такого набора диапазона в строчек дальше он будет выяснять схему таблички это будет устроена точно так же как в простом подходе просто сходим в метаданные таблицы превратим это в structor и на третьем шаге spark самостоятельно вот эти отдельные диапазоны строк отдельные чанки раскидает по экзекьютора me и они будут независимо вызывать метод рест который будет их читать и так на самом деле это вот все что нужно для чтения в более сложном подходе нужно реализовать вот эти интерфейсы file system и file format и за счет того что я их реализовала я по сути встроилось в весь этот код который был уже сделан для хдс и и паркета и все вот эти фичи про которые я говорила портишь на discovery у меня заработали просто из коробки потому что я рассказала спарку как устроена моя файловая система но кроме вот этого покормил паркетных fitch нам ещё хотелось уметь читать по матчам а в том что я сейчас показывал везде было построчное чтение везде вот этот метод ритуал формате у меня все еще возвращал iterator по строчкам ну на самом деле в формате можно реализовать еще один метод который рассказывает о том что мы хотим читать не строчками апачами и если он возвращает true то просто местре должен возвращать начать возвращать каумин арбате около manor бочче это вот такая штука это количество строчек которых них лежит и колоночки а каждая колоночка это тип данных какие-то мета-данные по этой колонке типа количеством улов там или есть ли там новый вообще и методы для получения уже конкретных значений из конкретных строчек колонки будет boolean get string ну это в зависимости от типа данных которые там лежит и так на самом деле все вот реализуем вот такой column вектор начинаем отдавать его из методы рид реализуем саппорт бочче так что он возвращает true и все у нас заработал аполлон ночные чтения и это в общем то все прочтение здесь мы реализовали пар интерфейсов или реализовали вот этот column арбат теперь давайте смотреть на записи точно так же посмотрим сначала как экспорт пишет паркет в ходе fs и вот у нас есть как отпускать директория в ходе сессии вот у нас есть уже готовы dataframe который мы хотим в ней писать spark первым делом пойдет создаст ту директорию в котором мы хотим описать и создаст в ней временную директорию который называется temporary дальше он будет в это там переписать все паркетные файлики все напишет тот момент когда он все до пишет он мол нет все паркетные файлики is temporary в целевую директорию a temporary удалит и если мы посмотрим на то как это выглядит на физической схеме то завод первый шаг у нас будет отвечать драйвер потом у нас экзекуторы независимо напишет паркетные файлике и потом опять таки драйвер когда все закончится магнит все файлики в целевую директорию за второй шаг на этой схеме отвечает знакомые уже нам file format он описывает как строчки спарка превращаются в файлике определенного формата то есть там буквально надо реализовать один метод а за первый и третий шаг на этой схеме отвечает еще одна абстракция который называется примет протокол вообще вся вот эта вот машинерии с созданием временного файлика ему вам в конце она нужна для того чтобы в хадисе в системе который на самом деле нет транзакции сделать вид что они есть то есть чтобы у вас с дата сайт не появлялся частями по мере того как пишется фрики а чтобы сделать вид что он появился такой разом весь и на самом деле вот эта палками протокол он устроен чуть-чуть сложнее то есть чуть больше методов и помимо подготовки записей успешного ее завершения бывает еще случай когда что-то пошло не так нам нужно все попортить ну просто удалить директорию и набор аналогичных методов но на уровне экзекьютора на уровне конкретного тоска давайте теперь смотреть как все это ложится на войти она очень хорошо ложится на войти потому что войти из коробки есть транзакции и вообще не надо вот этого всего делать просто берем и честно в камин протоколе у нас этапе создаем транзакцию на комете к medium транзакцию на аборте аборте на транзакции вообще ничего делать дополнительного сложного не нужно ну а method right который нам нужно реализовать файл формате он прям точно такой же как и в простом подходе просто нас есть набор строчек нужно их записать его готовую табличку выйти и того мы научились читать по колон ночными патчами мы перри использовали код который написан для gdfs а и паркета и на самом деле этот подход принес еще несколько дополнительных преимуществ я вот говорила что мы реализуем файл чистом да по верху идти и вот например сесть директория то мы можем ее политик посмотреть что в ней лежит но войти бывают не только таблички и директории бывать в файлике можно дописать еще один метод который будет создавать файлик и писать в него что-нибудь и нет который будет открывать уже существующей файлик и читайте из него данные зачем это нужно во первых это нужно для того чтобы спаркс admit умело скачивать файлике и сыть а теперь можно положить jorney кувейте запускать парк сабмит и он будет скачивать и запускать и появилась возможность сохранять иван флаги в быть и читать их spark history сервером вот буквально также как это работает в хадисе но поверх теперь другого хранилища и осталось рассказать все что я рассказывала сейчас выглядело как но вот прямо раз-раз и готово берем у нас есть буквально там три интерфейса чуть реализовали полетела здесь еще два интерфейса реализовали полетела на самом деле она чуть-чуть сложнее и кое-где бывают грабли и бывают неприятные касты который приходится вставлять и наверное 1 грабля про которую я расскажу она связана с вот этим первым шагом чтения с листингом таблицы которые делаются при чтении я рассказывала вот буквально пару минут назад что это делает драйвер я всегда верила что действительно делает драйвер но он за это отвечает и действительно если вы будете вычитать какую-то директорию из откуда угодно там мы за тебя исходя из вас это обычно выглядит так драйвер идет к файловой системе выясняется из чего состоит директория она состоит из чего-то от вложенного дальше он берет первую заложенную штук выясняет из чего состоит она получил дальше берёт вторую изложенных директорию и и листья и того он все polystyl выяснил что из чего состоит ну как бы все нормально кроме того что все это делает драйверы все он делает в один поток поэтому создателям спарка ведь мы-то не очень понравилось не зашили в код такую логику чтоб когда у вас становится много вложенных директории вот от какого-то момента начинает работать немножко другая логика драйвер по-прежнему базовую директорию лестницам дальше он видит что вот нет 2 вложенных директории дума учат много думает как бы мне это распараллелить и параллели со своим любимым способом заворачивает это в тоски и раскидывает их на экзекьютора поместите пожалуйста ребят они идут и листья вот эти вложены под директории а потом отправляет это все обратно на драйвер ну и кажется ну ну тоже ну нормально ну вот такой способ парализации написали в спарке ну почему бы нет а потому бы и нет что вот я когда писала свою реализацию file system я в методе для листинга директорию решилась что нам нем удобно возвращать не обычный файл статус а какой то немножко такой свой от наследования там еще всякого дополнительного положу мне будет удобно я считал что это всегда вызывается на драйвере она еще нигде кьютер и оказывается бывает и экзекуторы это получается мой кастомный файловый статус дальше он все реализуется отправляется на драйвер драйвер от одессе реализуется драйвер уверен что там обычный файл статус и он бодро теряет все вот это вот дополнительные при этом кафе la статусом накрутила и говорит мне что это обычный файл статус ну что с этим делать ну во первых нужно всегда внимательно следить типа что у вас вызывается действительно драйвере а что вызывается на экзекьютора если что-то вызывается на языке турах это иногда вот прикольно аукается но во вторых не просто не пытаться обмануть эспио если он сказал возвращать файл статус возвращать павел статус не нужно вот это придумывать следующие прошли буду рассказывать это pro file system я до работы в яндекс много работала с ходу пумы сходив и сам я много пользовалась выступал системам и я как привыкла что он ведет себя как синглтон вот его нужно создавать через метод эстетический get этот эстетическими за все время возвращает одно и то же и в общем то у вас все время один instance этого file system на всю программу как бы все хорошо синглтон in настолько я к этому привыкла что это даже как-то никогда не перепроверяла и в своей реализации video file system я в какой-то момент начала использовать ведь новый клиент который был прям самому настоящему сингл тоном вот прям одним объектом на все и когда у меня все это началось взрываться что могу ты уже закрыла путевый клиента все еще пытающийся из него значит достать я послал почитала наконец-то код как устроен falsies там вот это кусочек кода из метода который получает falsies там из кэша что здесь происходит мы здесь берем блок проверяем что files фрэнсис там уже есть в кэше если он там есть ну возвращаем все успех а если его там нет то отпуска я мог и начинаем создавать систему создаем создаем снова берем лак проверяем к божок отпускали возможно нас кто-то опередил теперь если нас действительно кто-то опередил то то что мы создали она уже никому не нужна закрываем выкидываем и просто начинаем пользоваться тем что уже есть в кэше не ничего особенного но нужно помнить что реально file system это не синглтон он вполне законно может быть создан ни один и лишние так будет закрыт примерно сразу же главное чтобы это не где-нибудь не выстрелила они выставили это если вы не будете использовать настоящий символ тонны в коде файл систему и последняя история веселая история про кастомные параметры в спарке spark вообще довольно удобно прокидывать все всякие кастомные параметры и пишите минус минус конов начинаете название параметра нас парк . оно все само прорастает значит парковый конфиг можно дефолты в спорах дефолт прописать очень можно все использовать что в спарке сделано для их конфигов на эти конфиге видны в логах видны на вкладке in varmints parkway и для всяких из секретных параметров от не очень хорошо но вот в спарке есть такой специальный конфиг для секретных параметров который называется spark reduction риггс ты мол все параметры названия которых подходят подругу я ручку будут отовсюду выпиленные и красивыми звездочками закрыты ну как я здесь стою и про это рассказываю наверно это не так и я действительно попробовал прокинуть значит секретный параметр таким образом прописать середе ксп что он секретный значит зашла в spore кьюай у меня действительно все закрылось везде звездочки классно я зашла блоге драйвера и увидела там свой секретный параметр в открытом виде потому что почему бы и нет и хорошая новость состоит в том что в sparkly 30 это починили они теперь прежде чем что-то писать влоги редакции оттуда параметры и там все таки будут звездочки не параметру открытом виде ну окей вот из залогов драйверов сидела выпилено вы понимаете что если вы будете пробрасывать действительно так параметр через минус - клонов то вас все будет видно в списке процессов тут можно как-то продраться через вызов спаркса вмето но в итоге все равно упрётесь в то что spark сабмит там внутри вызывает java и вызывает там свою специальной классика ему точно все в открытом виде прокидывает и не заставившего делать иначе поэтому наш параметр виден в списке процессов это тоже печальненько и я как-то покрутилась это печален клей у меня уже был for кнут spark я по-новому глину одно строчечку же дописать но не страшно за правда но учетом sports агнец одну строчку допишу буду из январе мента доставать пропихивать как бы нормально и год я такая довольно значит у меня все опрокинулась до тех пор пока не пришел ко мне пользователей которые по с парком юпитере пользовался и не сказал что у меня все равно там в какой-то джаве опять этот океан светится я полезла выясняете выяснилось что когда вы запускаете что я в коде который работает в паспорте написал прокиды вани этого параметров при создании сессии что с этим не так а то с этим не так что когда это будет такой код пишите именно в паспорте то этот кузов где торг рейд он запускает g в ямку и запускает там наш любимый spark сабмит и в ему в открытом виде прокидывает все что вы написали в конфиге в питоне ну у меня уже был костыль одну нормально ладно все через этот же костыль все прокину все хорошо все будет работать ничего нигде не светится все отлично проходит несколько месяцев и ко мне приходит админы говорит что вот у вас там значит кластер на нем worker там стоит я значит глеб но у все списке процессов и случайно увидел что там где-то тоже есть токен в открытом виде и вот так я выяснила что еще и оказывается там spark standalone когда драйвер запускает он ему тоже все в открытом виде прокидывает я к этому времени очень устала бодаться вот со всем этим поэтому я подобен а добавляла костылей и сказала пусть живет но вообще уже даже не очень важно каких я вот тут к склепе на добавляла она в итоге заработала но главный вывод из этой истории он такой не надо прокидывать секреты через конфиге sparco никто это не дизайнер под то что вы будете прокидывать через это секреты это может показаться что это прикольно вот берешь два параметра как бы она заработала нет оно так не работает он взрывается везде где может взорваться для охранения секретов нужно использовать специальные сервисы и ходить через них итоге на самом деле итоге какие вот наш мы писали писали и написали и простое решение вот то о котором я рассказывал в первой части доклада она реально была сделана за три дня причем первые пол дня я скачиваю идею там открывалась парк еще что то такое идеал а вот здесь она и оно реально прям простое можно посмотреть я такой в на коленочки сделать легко рефакторинг на более сложная он пошел посложнее там несколько недель условно то что вы все перепилить разобраться как она должна работать и плюс еще мы потом потратили ещё несколько недель на то чтобы сделать чтением отчаяно но чтение обычаями того стоило потому что у нас реально запросы на чтение стали работать в два-три раза быстрее но тут сильно зависит от структуры данных которые этими бочками летают впереди еще куча всякой интересные работы еще нужно сделать хорошее push дал фильтров до уровня яйца нужно закопаться в оптимизацию записи потому что я больше копалась в оптимизации чтения просто потому что аналитические запрос так устроена что вы обычно много читаете мало пишите хотелось бы что-то сделать для использования информации про то что данные в войти уже лежат сортированные нишах этих например лишний раз и я вообще в этом докладе не касалась того как мы интегрировали scheduler am на самом деле там тоже хочется сделать но классного интересного для более такой гибкой и удобной интеграции и я надеюсь что это все получится и когда нить я проекта расскажу и на этом у меня все можно задавать вопрос александра спасибо у вас замечательный доклад вот и давайте перейдем к соси и вопросов кто хочет задать вопрос и я на всяк случай напомню что за лучший вопрос у нас есть подарок поэтому александр запоминаете вопрос при записи вбить записи которые собственно отправляются они как-то подготовьте возможность в принципе выйти подготовить данные как допустим плеч бейяза ведь файл и и прям засунуть его туда или это все обрабатывается через сервер и в принципе там только такая реализация ну нет смотри во первых data api который есть у меня он сейчас он такой типа ты ты шлешь строчечки а ведь там разберется типа что с ними сделать он на самом деле там тоже ну по хитрее устроен он не сразу начинает писать их ноги склонах там собирать в какой-то банчок потом думает значит как их получат пожать но как бы вот это скорее вопрос немножко такой на будущее в то что я говорила про оптимизацию записи на самом деле и патчами раньше никому не надо было языка читать то есть все вполне устраивало чтение строчками и мы его до перевале вот конкретно для интеграции со spark поэтому я думаю что в принципе скорее всего мы что-то допилим чтобы ведь можно было писать более крупными кусками как-то более при подготовлена спасибо спасибо за ваш вопрос и был еще один вопрос а здравствуй пасибо большое за доклад вопрос может быть глупой по поводу вот запись и выпить там два два основных момента это создание и собственно запись и я как понимаю это создание это именно вот определенной структуры ну грубо говоря на диске да а вот если механизм регистрации самой таблице в метаданных сервера или где-то интернет создание она была как раз про создание таблички уйти от по по сути ты говоришь вот у меня будет такая табличка в этот момент пишутся и и метаданные данных не пока нет вот вот вот это и есть по сути регистрация дальше ты будешь вот фото готовой олесь что то новое вот а если здесь у нас потом будет ставка он не буди пересоздавать им просто поверни конечно не будет ее пересоздавать мы просто будем мы на самом деле из разных мест из разных экзекуторов одновременно пойдем в это уже готова и будет мы сюда пишем мы типа договорились что мы пишем все одновременно войти нам скажи туда все хорошо спасибо так если ещё вопросы у нас зале есть есть два вопрос еще здравствуйте пасибо за доклад было интересно скажите пожалуйста вот что произойдет и точнее проверяли ли вы что случится если сложилось например сам слой хранения на стороне идти при записи с парком и ищет точнее мы считаем так что экзекутор вроде бы все записал но при этом сама окончания финализации записи данных анна пакуин причин буханку не приземлилась то есть будет если например spark решил перетравить эту тоску что произойдет с записью нет смотрите дальше что произойдет записью если я правильно понимаю вопрос внутри у нас есть транзакция на запись который открыл драйвер дальше у нас есть тосканы экзекьютора экзекутор на самом деле вот он тоже открывает транзакцию он говорит что не есть родительская транзакция то есть быть еще вот так можно есть родительская транзакция вот это открытая драйвером диффузор тут у нас открыл новую транзакцию он начал в нее писать и вот до тех пор пока войти ему не подтвердит что до данные записаны он свою транзакцию коммитить не будет если вот в этот момент у него что-то пойдет не так он свое транзакцию зао портит spark об этом узнает перезапустит вам task если же что-то пошло не так уже после вот этого комента то ну то что если что-то пошло не так на соседнем тоски то мы попросим просто транзакцию который на драйвере если я правильно понимаю что вопрос про это тот ну вот так так и следующий вопрос спасибо за доклад у меня области вопрос по поводу опрокидывания секретов через парковый property естественно какие сервисы для хания секрета вы используете такой topcon source или во что-то написано внутренняя спасибо за ваш вопрос александра спасибо за твой доклад"
}