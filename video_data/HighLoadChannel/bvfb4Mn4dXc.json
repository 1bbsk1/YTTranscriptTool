{
  "video_id": "bvfb4Mn4dXc",
  "channel": "HighLoadChannel",
  "title": "Эволюция акторной системы / Алексей Станкевичус (Яндекс)",
  "views": 1096,
  "duration": 1982,
  "published": "2023-01-19T05:55:16-08:00",
  "text": "последние 7 лет я занимаюсь разработкой войди by bodybeat базы данных предназначенное для хранения огромных объемов данных это могут быть петабайты информации она написана на си плюс плюс и в основе войди by лежит активная система о истории которой я сейчас хочу рассказать сначала расскажу почему мы вообще делаем войди by поверх факторной системы расскажу возникающий проблеме фоновые нагрузки и от разных способах ее решения при помощи delta им потоков watch догов таймеров обмена потоками и закончим мы расскажем о том какое ждет ее будущее с половинами ядра ядер и вытрезвитель им итак какие есть вообще подходы к создание многопоточных программ ну в основном есть два подхода первый это использование разделяемой памяти когда к общей памяти вы из разных потоков от склеить и доступ и вам потребуется мед x i can товары фьюче promise и все остальное для того чтобы обеспечить этот доступ сделать его чтобы сделать доступ к данным не нарушающим коэффициенты самих данных второй подход заключается в том чтобы между потоками или запущенными в этих потоков сущностями передавать сообщения соответственно вы передать сообщение с потоков поток и у вас не могут возникать все те проблемы которые связаны с многопоточной классической разработкой с разделяемой памятью у вас не будет до блоков у вас система значит на проще построена ну и конечно же есть гибридный способ когда вы на самом деле делаете нечистую передачу сообщений а допускаете передачи например указатели на общие данные которые вы будете читать из разных потоков и за счет того что вы их не откуда не пишите вы на самом деле почти не ломайте абстракцию передачи сообщений а предыдущая наша система мы делали как системы с разделяемой памятью и получалось сложно на слайде показана реальная схема одной из систем которым мы нарисовали для того чтобы в какой-то момент понять как она вообще работает и как дальше можно ее развивать стрелки разного вида здесь обозначают прямые вызовы вызовы отложенных процедур передачу callback а вызов это вокал beck а при этом на самом деле все очень сложно потому что важно вызвать callback из нужного потока который владеет данными они из другого потока когда нами не владеет потому что в этом случае вы должны нарушить и войди by мы решили делать поверх факторов и специально для виде by разработали свою собственную систему на си плюс плюс соответственно суть вкрадчиво в том что все является акторами акторы это по сути однопоточный стоит машины которые могут получать сообщение обрабатывать и вы создавать новый сообщение для других актеров в процессе и либо порождать новые акторы преимущество акторов в том что они очень легко переходят из распределенного из многопоточного правильного программе это нераспределенная например если у вас есть actor который должен записать что-то надежно на диске он может послать сообщение actor надежной записи который передаст сообщение трём актёрам разных дисков в принципе ему все равно куда ушли сообщение на эту же машину или на другие машины соответственно кто-то дисков выполнят запись ответит этому автору он посчитает до 3 получит необходимо качество ответов и сообщить что запись выполнена успешно факторная система работает сейчас например на 10 тысячах серверов исходный код войди by факторной системы доступен под лицензией apache 2 0 в принципе actor ная система выполнена как отчуждаемое библиотека и на самом деле она используется не только в воде вино и в других сервисах и теперь видим проблемам проблема фон вы нагрузки нагрузку можно разделить условно на две части на самом деле у нас их порядка пяти видов но для простоты давайте врачи с двумя интерактивная нагрузка и фоновая интерактивно нагрузка это сообщение для акторов которые быстро обрабатывают их отвечают характерное время это десятки микросекунд фоновой нагрузка может обрабатываться дольше там характерное время это сотни миллисекунд может быть даже секунды если мы допустим возьмем четыре потока как на слайде показано слева направо идет время и красные прямоугольники это обработка сообщений акторами в этих потоках то видно что пока у нас система не перегружена акторы получают сообщение обрабатывают их и у нас даже сайт свободно процессор но если в какой то момент времени у нас поступает на обработку достаточно длинных фоновых задач чёрным показаны то может так получиться что у нас не будет свободного процессора для того чтобы запустить дар активную нагрузку и время как на интерактивную нагрузку будет отвратительным для решения проблемы очевидна и простое решение это использовать пулы потоков мы разделяем наши потоки на два пула скажем два потока для интерактивной нагрузки два потока для фоновой нагрузки в этом случае у нас всегда остается выделенный процессор для обработки того или иного вида нагрузки но возникает проблема нет утилизации справа видно что когда у нас кончилась фоновая нагрузка мы просто не утилизируем два ядра не про простаивает очевидное решение организовать 5 подписку можно добавить еще один поток у нас 4 аппаратного выполняющихся потока но мы добавляем 5 который будет конкурировать с помощью операционной системы и мы надеемся что когда у нас освободиться поток где обрабатывал фон в нагрузка вместо этого его вытеснит другой поток в котором сможет обрабатываться хотел там в нагрузку к сожалению реальность не такая как ожидание если мы так сделаем то у нас начнут очень долго обрабатываться отдельные интерактивные запросы связано это с тем что cfs планировщик будет вытеснять наши потоки как ему вздумается и времена проводки драгоценных запросов превратятся в те самые единицы миллисекунд порядка десяти двадцати могут быть если дефолта настройки scheduler можно пытаться крутить настойки шедевра linux более менее нормально можно получить времена порядка трех миллисекунд но дальше в общем работает плохо какое первое решение мы попробовали сделать мы решили сделать новый активную систему активную систему версии 2 0 в которой мы решим эту проблему сделав вытесняющей многозадачности визир space соответственно идея в том что мы длительно выполняющийся задачи будем вытеснять и продолжать выполнять когда у нас появится свободное ядро по сути мы будем не создавать пулы потоков а выбирать из какого пола задач выполнять задачу в каждый момент времени в каждом из потоков для этого мы организуем несколько real time of потоков на каждом ядре основной dell тайм поток со средним приоритетом выполняет scheduler который выбирает задачи из какого пола задач брать в каждый момент времени более приоритетный real-time в поток watch долго периодически просыпается и если нужно вытесняет этот all time вы поток для того чтобы запустить другой килтом в поток в обрабатывающей другие задачи и плохо ведущие себя задачей мы вытесняем в nedele time приоритет и там уже блока ведущей задачей заканчивают обработку под управлением cfs 6 равен x то есть работает и все примерно так у нас выполняется задача из первого пула в какой-то момент просыпается watch dogs и вытесняет весь поток вместе с задачей весь поток становится потоком снять алтаем приоритетом после этого watch dogs будет другой the time вы поток который продолжает обработку на этом же ядре периодически первый поток должен спать для того чтобы дать в принципе возможность по выполняться они был тайным потоком и какой-то другой нагрузки запущенные на машине после этого он просыпается и продолжает выбирать задачи с помощью свою шага мы организовали два на самом деле дедлайна жесткий и мягкий после выступлений рост кого дедлайна мы пытаемся вытеснить поток с помощью watch долго на самом деле это дорогая операция требующие 2 переключения контекста и поэтому мы стараемся по возможности вытеснять задачей мягкая соответственно съесть меньше по длительности софт лимит после наступления которого но до наступления каплями то есть и заканчивается обработка какого-либо сообщения котором происходит вытеснение активности с из этого пула и выбор активности из другого пола дублером соответственно мы выполняем задачи из разных плов по очереди по кругу так чтобы каждый полк получил немножко процесс этого времени каждую целевое значение нашего времени отклика порядка 1 миллисекунду мы пытаемся все полы по кругу прокручивать их в принципе это работоспособная система но поток watch бога на самом деле имеет смысл запускать только один на всю систему и поток watch долго создает дополнительно нагрузку это дополнительная нагрузка порядка трех четырех процентов и получается что одной из гайдар в нашей симметричной системе работает хуже чем остальные не все ядра одинаковы это неприятно для того чтобы снизить нагрузку от ручного мы размазываем его выполнение по разным потоком мы будем его по очереди на разных кадрах так чтобы нагрузка стал равномерный но 3 или 4 процента это все равно очень много поэтому мы используем не просто периодически просыпающийся поток а таймер и которые мы перри заводим на более позднее время если у нас все потоки проявляли активность и выбирали новую задачу за последний небольшой период времени соответственно лишний раз мы не будем watch dog мы передвигаем таймер вперед и в результате мы до поста снижаем вверх от и кран того можно не просто разбудить watch dog чтобы он запустил еще один поток на самом деле в нем все достаточно real-time приоритетов для того чтобы более приоритетный поток который был до этого watch догом сам и продолжил выполнять у себя scheduler после того как он проснулся он становится основным толкаем потоком а следующий будет приоритетный поток можно использовать как следующий watchdog так мы можем пройти по кругу и вместо двух переключения контекста делать только одно при попытке опытной эксплуатации этой системы мы столкнулись с рядом неприятных проблем самые серьезные из них заключались в том что на машине переставали нормально работать драйверы операционной системы и другие программы из-за того чтобы мы работаем в телкам приоритете оказалось что мы не даем процесс снова времени для работы драйверов мы очень быстро обрабатываем свои сообщения посылаемых посети а сети не работает потому что драйвер city of low deep space в ней delta им поток свою нагрузку и не получает процессорного времени им дополнительный а проблема в том что при запуске двух и более таких приложений на одной машине они дерутся между собой и нормально не работает ни одной из них и хорошего решения мы тут не придумали матч расстроились и решили перепрыгнуть пропасть в несколько прыжков мы решили сделать сначала более простую версию факторной системы ответ на систему полтора которая будет просто обмениваться потоками она будет перекидывать поток из одного пола в другой полк в принципе идея простая мы берем поток и если в пуле на загрузку скажем порядка 40 процентов этого потока ему говорим но наверное если мы отдадим один поток вполне будет при выраженной при этом этот поток можем отдать для более загруженного пула и там станет лучше с производительностью но он на самом деле нельзя просто так взять и перекинуть поток и сплав по ул если у вас загрузка было порядка 40 процентов на 10 секундам интервале это может означать что четыре секунды из десяти все два потока работают на полную мощность а потом 6 секунд вообще нет нагрузки они ничего не делают и если вы такого пула заберете один поток the late in se задача в этом пуле ухудшится на 4 секунды поэтому на самом деле необходимо сначала это сколько все уже плохо именно слейтон стив пули прежде чем пытаться отдавать оттуда поток мы измеряем время активации то есть то время которое проходит между тем как у автора появляется сообщение другого автора и тем как автор получает процесса чтобы это сообщение обработать и мы измеряем время на которые ухудшится время обработки но оценку сверху делаем если мы отдадим один из наших потоков из пула для этого мы строим циклический буфер в которой с разрешением порядка 10 микросекунд мы записываем by текке про каждый поток соответственно единичный бить и к мы выставляем если в этом потоке обрабатывалась какое-то actor на и сообщение в этот интервал и получаем вот такой замечательный массив мы делаем его постобработку мы получили работаем с ним вытрите алгоритмами мы не где здесь ничего не блокируем и при постобработке мы пытаемся освободить один из потоков мы пытаемся переместить единичные биты в другие строки где было свободное время или если нету свободного времени также мы перемещаем их проверив будущее смотря насколько далеко в будущее перемещается нагрузка соответственно нас интересует оценка сверху поэтому самое худшее время на которой у нас может увеличиться время обработки сообщений и есть нужно на величина теперь мы имеем знание на то насколько у нас все плохо и знание о том насколько станет хуже когда мы отдадим один поток и это позволяет нам понять ее превышает ли сумма этих двух значений целевое время откликов apple если она не превышает и мы можем сделать предположение что вот как раньше в предыдущий период работы системе хватало процессоры чтобы хорошо работать так и следующий период будет хватать и можем отдать ядро в пул который перегружен и предложено пул тоже определять следует на самом деле не потому что у него высокая загрузка цп у а скорее всего потому что там превышена и времена отклика который фактически измерено в результате мы получаем заметное улучшение производительности на том же железе порядка 20-25 процентов на масштабе в десятки тысяч машин это на самом деле очень много ядер но на самом деле хотелось бы гораздо больше получить и почему получили так мало мы получили так мало из за того что а к данная система полтора не отдает последнее ядро из пула последний трудно оставляет в пуле для того чтобы задачи в принципе имели возможность выполнить и мы могли заметить насколько им плохо какие следующие шаги мы хотим сделать мы хотим повысить гранулят ность передачи лидер с помощью не вытесняющей многозадачности было бы очень здорово если бы мы могли передавать is full of pool ним целый поток а половинку понятно что половинку точно передать сложно потому что мы хотим делать это с не вытесняющей многозадачностью то есть масс будет только софт лимит из предыдущих слайдах мы в результате сможем оставлять пол ведра для полов занимающихся batch обработкой и если к ним придет быстро delta им в нагрузку и немножко использовать их процессор скорее всего она очень быстро завершится и если вдруг появится задачей фонового типа они смогут получить свой ядро и при этом мы можем не последнее но все остальные ядра поделить между двумя полами так чтобы фоновая нагрузка тоже имела возможность обрабатываться на одном из я der разделенном фон в нагрузкой и интерактивной огрузкой при этом система становится значительно более отзывчивой при изменении нагрузки в плохо и кроме передачи половина потока на самом деле мы долго думали как же можно вернуться к правильно красиво модели с вытеснением и при этом не страдать от всех возникших проблем и мы придумали ядро вытрезвитель идея здесь следующее у нас будут ядра для работы всех обычных наших потоков мы будем запускать там нашивок родную систему и единственное что мы одно ядро выделим как вытрезвитель для потоков хулиганов и будем прибивать туда по осени те те потоки в которых обрабатываются долгие задачи то есть мы оставим всю ту же самую схему swatch долгом который будет приходить и переносите потоки которые плохо себя ведут прибивая впо affinity кедров вытрезвители после этого мы сможем на освободившемся ядре запускать этот пул задача которого мы хотим выполнять на процессоре в текущий момент и при этом мы не будем страдать от того что у нас высоко приоритетные потоки не дают выполняться всему остальному на нашей машине очень важно здесь после того как освободится хотя бы на одном из ядер процессор после выполнения очередного сообщения не брать новую задачу взять вместо текущего потока вернуть affinity маску одному из вытесненных потоков из вытрезвителя а самому поток уснуть для того чтобы не возникало нарушения очередности я опять же длительных ожиданий как припяти подписки а вот так я все рассказал спасибо за внимание алексей спасибо переходим по вопросам здравствуйте спасибо за доклад вопрос такой то что вы рассказали про векторную систему уже давным-давно реализовано в лонге наверняка вы слышали такую вещь там интересно сделано тоже на каждом ядре процессора запускается scheduler который управляет задачами там микро процессе которые не процесс и система микро процессы внутри самой ярла новый машину виртуальный вот соответственно меня вопрос такой как у вас и реализовано или передачи сообщений между распределёнными распределенными копиями программ и вашей инстансами на разных кадрах и на разных машинах сити например то есть вот есть кластер там 100 машин и чтобы с одной машины из одного потока и и снова актара там в другой на рандомные другой машине передать сообщение чтобы оно обработалось там с какой задержкой сделан оси то досмотрите у нас акторы могут передавать сообщения и адресатом сообщений является actor идентификатором актара является на самом деле номер машины в кластере и уникальный номер actor на этой машине соответственно когда мы посылаем сообщения факторной системы видит что сообщение послано например рак тару с другой машины и отравляет это сообщение в interconnect это тоже actor на той же самой машине которая стерилизует это сообщение и передает по сети на целевую машину на машине сообщение передается уже нужно маг трон одессе реализует и обрабатывает да очень похожи на r long в таком случае 2 просека продолжение вы его рассматривали в принципе или может быть там что-то подсмотрели я нет вообще ребята смотрели на самом деле ерлан к очень интересной но мы писали систему на си плюс плюс и поэтому есть ряд сложностей основное отличие эрланга на мой взгляд она в том что на эрланге можно вытеснить любую задачу прямо посреди выполнения в юзер спейси потому что там язык позволяет там компьютер просто разметить код правильным образом он сам аспард самостоятельно выйти снится и все это будет ну естественно происходить а на си плюс плюс так не происходит и вот отсюда начинаются все сложности которые с которым на героический боремся ничего просим здравствуйте меня зовут андрей мне вопросы вот про вашей ядро вытрезвитель металле такой опасности что очередь задачки трубы 3 зритель будет постоянно и бесконтрольно прости ведь на это ядро и так попадают очень долго играющая задачей раньше для них было сколько ты кадир а теперь строго одно ну и время выполнения этих задач в очевидно он увеличивается и предсказать количество их плюю появлений невозможно вычислить еретические то очередь бесконечным растущие все верно смотрите ядро вытрезвитель получает задачи которые мы в предыдущей модели вообще вытесняли и говорит что их не надо выполнять и мы стараемся забирать задачи с hydra вытрезвителя как только у нас появляется возможность выполнить новую задачу на любом из я der отданных этом уже полу то есть вот здесь три черных задачи у нас но вот и зритель вы теснились в какой-то момент и как только 4 или 5 задача закончить выполняться мы не возьмём новую задачу мы выйти с ним поток обрабатывающие старую задачу и вместо него вернем affinity маску поток увезли зрителя при этом в вытрезвителе потоки не совсем стоят они под управления cfs все таки как то что доля ции там порядка 20 миллисекунд у них может быть время ожидания процессора это очень хорошо потому что еще одна проблема когда не рассказал заключается в том что если у нас в коде есть ему такси вообще использовать мед x и когда у вас actor очень плохо но мы иногда используем и если вас коде есть минут x это вам очень важно не вытеснять поток который только что его взял и в данном случае мы можем позволить себе иногда вытеснять поток взявший mitex потому что он таки его отпустит но мы конечно повлияет на производительность системы но если мы ведем где-то медокс наверное мы готовы к тому что это может быть долгой операции спасибо за доклад у меня такой вопрос вот если я как пользователь вашей библиотеке вы сказали что эта библиотека библиотека тоже дома стоял пан собственно да я захочу использовать и вот что является фоновой задачи это я как разработчик сам могу определять да как разработчик вы можете просто указать сколько пулов задач у вас есть какое количество потоков вы выделяете для каждого из плавок то в системе 10 вы просто говорите сколько потоков в каждом из полов и когда вы запускаете актара вы указываете в какую тип какой тип активности красят для этого кто то соответственно в каком пули он должен обрабатываться вакт с теми полтора настройки расширяются там вы можете указать не только желаемое количество потоков пули но и минимально допустимая максимально допустимое соответственно когда пол перегружен он может пытаться увеличивать количество потоков в дом максимума и когда он не загружен он может отдавать потоки до минимума там на самом деле все еще сложнее есть система приоритетов плов есть в разные приоритеты и считается что при прочих равных поток нужно передавать измене и приоритетного полов более приоритетной например у нас подпункт обрабатывающие сетевые соединения внутри он более приоритетный чем пул обрабатывающие и допустим дисковой активность поэтому если они оба будут голодать то вероятно поток будет передан на время из пола обрабатывающую диск во кинуть на кассете понял спасибо еще такой вопрос о наверняка ок тарные системы на плюсах уже были до вашего как-то смотрели на них если так сходу чем не устроили может быть напомним посмотри это было давно это было 7 или 8 лет назад но основная проблема это производительность и конечно же любима проблема разработчиков то что это сделали не мы на самом деле если бы мы взяли какую-то чужую оккультную систему то при том что у нас очень специфичные требования к ней мы бы практически все ее переписали скорее всего понял спасибо чего просит здравствуй спасибо за доклад у меня вопрос а учитывать или вы специфику платформы гипертрейдинг на включенный на процессоре или если платформа мойте процессорные но мы но мы но ты и так далее и есть великое рекомендации по настройке платформы под ваш фреймворк либо библиотеку и так далее спасибо самок ударная система этого не учитывает то есть на in the love например она замечательно работает между на монадами и мы не заметили как из-за серьезных спецэффектов от того что платформа с двумя юными ногами на md чуть медленнее связь между но монадами и там по возможности стоит прибивать по affinity все приложения сахарной системе к одной из дому нот и в принципе запустить два приложения на 2 нам а годах может быть лучше чем растянуть его между но мы годами где учитываем то что у нас еще есть пользуется нового кода мы мы это учитываем а локаторе до локатор он в курсе про это для него важно а про hyperthreading трейдинг нормально с этим работает у вас до достаточно хорошо то есть есть ли у вас нет кода который очень плохо относятся к соседям по ядру например вы занимаете какой он тяжелый криптографией то у вас все будет хорошо если вы используете дома в x 512 операции то может быть плохо спасибо так еще вопросы давайте последний и потом уже продолжим урок общаться алексей здрасте спасибо за доклад скажите пожалуйста вы проверяли свою систему на логическую корректно с помощью инструментов типа джексон или тому подобное да у нас есть свои инструменты проверяют еще и систему на логическую корректность и но разные свойства интересны среди прочих у нас есть инструмент который рассматривает в процессе тестирования системы все возможные комбинации порядка выполнения обработки сообщений несколькими факторами соответственно мы проверяем что нет такой комбинации при которой система ведёт себя не так как мы ожидаем у нас есть инструменты проверяющие способность системы при различных отказах но вот самим джексоном и обсуждение точно не помню как правильно называется право рк да так мы не правление хорошо спасибо"
}