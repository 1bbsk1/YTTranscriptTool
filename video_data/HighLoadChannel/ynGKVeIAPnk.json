{
  "video_id": "ynGKVeIAPnk",
  "channel": "HighLoadChannel",
  "title": "Тюним память и сетевой стек в Linux / Дмитрий Самсонов (Одноклассники)",
  "views": 2347,
  "duration": 2158,
  "published": "2017-04-07T15:37:34-07:00",
  "text": "я надеюсь обед прошел успешно мы сейчас вам для разгона такую зубодробительные тему приготовили про то как тюнить памяти сетевой stacks в линуксе от компании одноклассники на укладчик дмитрий самсонов поприветствуем пожалуйста здравствуйте друзья привет админ и надеюсь все отдохнули покушали обсудили услышанные доклады и теперь настала моя очередь я буду рассказывать о оптимизации linux меня зовут дмитрий самсонов я работаю в ведущем системным администратором в компании одноклассники и в прошлом году мы решили поменять дистрибутив но это решение было не спонтанным до этого на протяжении многих лет мы использовали старый дистрибутив об иисусе 10 2 поддержка которого запой закончилось очень давно мы провели довольно подробное изучение других дистрибутивов которые сейчас представлен на рынке в результате длительного выбора остановились на centos и начали в общем то переводить сервера мы провели довольно детальную подготовку благодаря чему переход с одного дистрибутива на другой заключался в нажимание кнопочек никакие инженеры не нужны были все было автоматизировано и раскатка и настройка и все остальное однако когда мы дошли до серверов видео раздача мы столкнулись с некоторыми трудностями что такое сервера ведь сервера видео раздача здесь вы видите их конфигурацию таких серверов у нас больше 40 они пропускают сотни гигабит трафика и какие же у нас были проблема такова будет состоять из пяти частей две части его памяти 3 просить так поехали первый доклад условно называется м киллер для начала небольшое отступление о том как поделен на память в линуксе у многих администраторов чаще всего такое что у начинающих есть неверное представление у меня но тоже в общем то было о том что память в линуксе это какой-то один большой кусок из которого кто угодно что угодно берет возвращает и так далее особенно свободной памяти такое представление почему то есть но на самом деле все не так просто память поделен между процессорами память по зелено дополнительно на первом процессоре на несколько зон каждая зона поделена на множество различных кусочков размером а4 килобайт до одного мегабайта и в процессе использования она может перераспределяться из больших кусочков маленькие итак первая проблема с которой мы столкнулись на графике вы видите нагрузку на процессор начиная с 6 часов вечера до 12 часов дня следующих суток сначала within час пик после этого нагрузка начинает снижаться но появляются какие-то скачки system time циpкa после этого скачков нету нагрузка по system time просто очень высокая сервер работает очень плохо пропускает мало трафика и периодически даже запускается воин киллер как вы думаете что происходит еще раз фрагментация памяти совершенно верно как я уже сказал в начале после загрузки операционной системы память поделена на большие куски по одному мегабайту но в процессе того как приложение запрашивают нужны им объем эти куски разбиваются на более мелкие части еще как через какое-то время еще более мелкие и а через какое-то время вся память порезаны на очень маленькие кусочки само по себе это не является проблемой это нормальное положение вещей так работает linux этим ничего нельзя сделать и в принципе это не является большой проблемой до определенного момента проблема это может стать если у вас на сервере мало памяти либо если у вас большой мимо репрессор то есть интенсивное использование памяти в нашем случае у нас было довольно много свободной памяти поэтому сначала фрагментации мы даже и не подумали но было интенсивное использование что же можно сделать с этой проблемой в линуксе есть всего один параметр называется мин free кубойд на основе которого выставляются верхней нижней и минимальный уровень в зависимости от которых операционная система высвобождает память либо дефрагментирует от процесса в linux называется compaq шинам файлики про джон инфо можно посмотреть текущий уровень мин low и high параметры тот по умолчанию при загрузке linux ядром высчитывается на основе того сколько у вас памяти установлено в сервере в нашем случае мы увеличили его то 1 гигабайта после того как мы увеличили данного гигабайта все проблемы были решены благополучно текущую фрагментацию можно посмотреть в файлике просьба12 здесь видна распределение памяти как процессором по зонам и полянкам значит вы видите несколько колонок первая колонка это куски память количество кусочков памяти размером 4 килобайта дальше 816 и так далее до одного мегабайта как вы видите больших кусков очень мало или их нету вообще зато маленьких очень много соответственно память фрагментирован а у этого параметра есть некоторые некоторые проблемы потому что ту память которую вы укажите как мин фрики боится на не может быть никем использовано в буквальном смысле то есть ни одно приложение не сможет его получить эту память ядро не сможет даже эту память использовать вплоть до того что даже поищешь не сможет туда не сложить никакие страниц то есть вы фактически берете кусок памяти и выкидываете и но таково положение вещей следующая память следующая тема по память это звук ситуация была следующее мы разобрались во фрагмент с фрагментацией сервер работает все хорошо но через какое-то время появляется своп при том что эти сервера сконфигурированы свм свой пин с 0 то есть сервер должен уходить звук только в том случае если у него ну вообще не эту память а у нас 40 гигабайт что происходит ваши идеи прежде чем я отвечу на этот вопрос покажу ещё раз слайд обратите внимание что вся физическая память поделена на между процессорами на ноты это технология если можно сказать называется numa то есть когда операционная система знает какая память находится ближе к какому процессу или какие устройства находятся ближе к какому процессору если приложение здесь вот видно ситуация которая сложилась у нас на серверах видео раздачи используется один большой темп fs файл который занимает практически весь объем оперативной памяти и когда приложение стартует она создает этот файл создавать файл можно только в один поток соответственно этот поток работает на каком-то ядре процессора и использует память операционной система видит что приложение хочет памятью наберет память ту которая ближе к этому ядру процессор соответственно сначала забирается вся память с этого процессора а второй процессор практически память не использует когда на первом заканчивается тогда он начинает заполнять память на втором процессоре и если создание этого темп с файла это процесс довольно медленный то вот его использование это уже интенсивный процесс поэтому когда приложение начинает отдавать собственно видео пользователям и у нас появляется тот самый мимо репрессор то в какой то момент времени в некоторых ситуациях ядро решает что все плохо и надо как-то подстраховаться и она начинает из первой из из 0 ноды с 0 процессора память о пить и это может происходить даже если у вас на втором процессы гигабайта и десятки гигабайт свободной памятью есть утилита прекрасно называется но most от которой можно посмотреть текущие распределения между нодами конкретного приложения и в целом по операционной системе значит данном случае видно что свободной памяти на одной из нот в два раза больше чем на другой то есть при определенных условиях мы опять же можем столкнуться с тем что сервер начнет сводится что делать есть два направления в которых можно работать первая и конечно же самое лучшее то оптимизировать приложение для этого нужно соблюдать многопоточность чтобы у вас не было какого-то слабого места бутылочного горлышка из-за которого от но мы не будет никакого толку при этом есть еще одна рекомендация но до finiti то есть старайтесь привязывать отдельные процессы и отдельные 3d процессов к отдельным ядрам чтобы у вас распределение на уровне самого приложения тоже был равномерным между отдельными процессорами второе направление которое можно выбрать это выключить numa совсем выключить его можно либо целиком для операционной системы через параметры ядра либо просто запустив приложение через утилиту но moze tl второй вариант очень простой если вам нужно быстро что то сделать но moze tail and early fall ваше приложение старт и поехали мы на данный момент как раз используем последний вариант пока что мы не переделывали архитектуру приложения но мы обязательно это сделаем далее поговорим немного про сеть сразу хочу сказать о чем я не буду рассказывать я не буду рассказывать о том как конфигурировать сеть от начала и до конца все возможные параметры их слишком много поэтому здесь есть небольшой список у вас обязательно должны быть сконфигурированы ринг буферы у вас должно быть сконфигурированы принимающая и отдающая очередь и также параметры сокетов и все возможные флот и первая проблема называется бруклин pipe на этом графике вы видите количество ошибок broken pipe на одном сервере в секунду это картинка появилась у нас не с переходом на centos она была и до этого но до перехода на новый дистрибутив мы и не занимались сам переход во время перехода мы решили что неплохо бы все-таки разобраться что же на самом деле происходит в тисе пи дампе было видно так называемый half duplex клаус sequence то есть быстрое закрытие соединение как вы думаете что же это было out of ортер пакеты или пике 3 ordering эта ситуация в которой получатель то есть клиент получает пакет и не в том порядке в котором они были отосланы это может происходить в разных ситуациях как с этим можно бороться для того чтобы это не происходило пакеты одного соединения должны двигаться на различных уровнях одним и тем же маршрутом в процессе того как пакет создается и движется клиенту он может проходить через множество разных слоев и соответственно везде должен делаться однозначный выбор если это одно соединение если у вас какая-то агрегация значит выбор должен делаться на layer 3 + r 4 также неплохо привязывать 3d и отдельные очереди сетевой карты к отдельным и драм и самое наверное полезное в этом всем списке это рфс технология которой мы поговорим немного позже на этом графике видно что же произошло по 100 как мы настроили все все возможные варианты то есть привязали все где только можно было к кадрам к очередям и настроили тот же самый рфс внизу если кто видит синяя линия это количество ошибок которые мы наблюдаем на сегодняшний день как видите разница драматическая в чем минус минус заключается в том что если у вас небольшое количество соединений и одни соединения работают быстро другие работают медленно по одному соединению идет 1 мегабит трафика по другому 100 мегабит то естественно то тот процессор то ядро процессора на который приходят эти 100 мегабит он будет занят больше чем ядро на которой приходит пора мегабит следующая тема называются распределение сетевой нагрузки по ядрам с этой проблемой мы столкнулись конечно не с переходом на centos эта проблема актуальна для всех мы ее решали различными способами но после перехода мы увидели что нагрузка распределяется не самым оптимальным образом если набрать в какой поисковым пояском поисковой службе запрос повышенная нагрузка на сибиу 0 вы увидите там десятки тысяч различных проблем не все они кстати будут связаны сетью но как вы думаете сколько результатов если допустим пасе пью один существует такая проблема не существует один миллион результатов и и вообще все не по теме и так как проблема выглядят чаще всего если она связана сетью первое ядро процессора загружен на сто процентов остальные загружены существенно меньше или вообще практически ничего не делают в первую очередь что вам нужно сделать это проверить из количества очередей на сетевой карте сейчас эта проблема может быть не столь актуальна как ранее тем не менее обязательно проверьте или опции ядра или через детей стул чтобы у вас были включены все очереди вместо одной после того как вы включить в си очереди этого будет недостаточно потому что опять же linux по умолчанию свали так все на первое ядро и картинку вас никак абсолютно не изменится важно распределить эти очереди прерывания от этих очередей между различными ядрами процессора это можно сделать двумя способами первое это распределить динамически запустить демон какой-нибудь сарки баланс ргдб арки выбор богатый плюс этого варианта в том что эти демоны используют уже известную нам numa соответственно сохраняется локальность данных и повышает к шкид за счет этого приложение может работать быстрее второй вариант это статическое распределение так называемый раз с recipe сайт skylink мы используем как раз таки и rss потому что при rss распределение более равномерное что такое rss значит внизу на сетевая карта наверху процессор каждая очередь сетевой карты которую мы создали имеет свое личное прерывание и это прерывание зама плена на свое ядро процессора допустим мы это сделали что мы видим все равно какая-то ерунда 8 ядер на сто процентов нагрузка у нас конечно повысилась мы пропускаем больше трафика но остальные я все равно ничего не делают что делать если у вас демон то ситуация может несколько отличаться но в любом случае все ядра не будут загружены первое что приходит в голову первое что приходит в голову это нам нужно больше очередей пошли магазин купили карточку и дать 10 гигабит ную 128 очередей настроили rss сейчас у нас все заработает ничего подобного 16 ядер работают остальные все равно они отдыхают проблема в том что rss использует только последние четыре бита сша которой он рассчитывает по заголовкам пакета четыре бита это значит что максимум максимальное количество вариантов максимальное количество вариантов я der которую он может выдать это 16 с этим сделать ничего к сожалению нельзя но есть другие варианты есть такой прекрасный документ от разработчиков ядра называется скиллинг txt внизу есть адрес и в этом документе описано то же самое то же самое rss после этого идет рпс рпс это saw тварный аналог rss и у него его плюс заключается в том что у него нету такого ограничения какой раз с он может распределить на значительно большее количество ядер у нас на видео раздачи сервера имеют 40 others соответственно нагрузка вставляется по всем 40 я дам следующая технология в этом в этом же документе называется xps это transmit пакет steering ps3 сиф пакет selling значит transmit значит что это аналог рпц но для исходящих пакетов и последняя самая важная самая интересная рфс recipe flown steering плюс этой надстройки это именно настройка над рпс он заключается в том что рфс учитывает на каком ядре находится приложение для которого предназначен пакет если рпс рассчитывает просто хэш он не знает кому предназначен этот пакет рассчитал хеш кинул его на ядро приложение там нету рфс знает где работает приложение и пакет будет сразу же помещен туда у нас на серверах видео раздачи на сегодняшний день настроены все эти технологии каждый из них служить своей цели и вместе получается хороший результат в чем минус есть две проблемы первая то что нагрузка опять же может распределяться неравномерно как я уже говорил разные небольшое количество соединений от не более нагруженные другие менее нагружены при этом они у нас жестко привязаны к определенным ядрам в результате нагрузка неравномерно и вторая проблема это оверхед на процессор все перечисленные технологии работают за счет центрального процессора он делает всю работу сетевые карты практически не участвуют за счет этого нагрузка повышается но несущественно это не все есть еще и accelerated рфс и ускоряется он за счет сетевой карты как раз таки поскольку ускоряется он за счет сетевой карты то сетевая карта должна поддерживать от технологию к сожалению поддерживается она не всеми сетевыми картами есть поддержка умела нокс и у solar flare у нас в продакшене есть карточки милан огс мы конечно же захотели попробовать какой же будет результат пойдет ли нагрузку на процессор нагрузка на процессор в результате не снизилась снизилась она не из-за того что помогла эта технология из-за того что на карточках мило нокс она работает плохо 10 гигабит на интерфейс и выдали максимум пятерочку и все что же intel почему intel не поддерживает такую замечательную технологию они не поддерживают потому что они сделали свой аналог аналог работает на стороне сетевой карты он вообще никак не использует центральный процессор называется сильный чур фильтр или от r в документации бывают разные названия разные даже расшифровки тем не менее по сути это рпс плюс рфс у этой технологии есть 2 минус 1 минус это то что она распределяет между ядрами менее равномерно во вторых то что она допускает бэкки 3 ordering то есть у вас опять же будут ошибки broken pipe возвращаясь broken pipe почему это плохо проблема в том что когда клиент получает пакет и не в том порядке он закрывает соединение выкидывает данные и отправитель должен их еще раз послать таким образом у вас есть постоянно или transmit и и возрастает вот этот паразитный трафик следующая тема одна из самых мутных самых сложных которые на данный момент потратил больше всего времени называется softer кью что это такое видео раздача хоть это и раздача и исходящих пакетов там конечно больше тем не менее с after few нагрузки по полученным пакетом именно больше почему это так мы поговорим несколько позже сейчас рассмотрим что происходит когда сервер получает пакет когда он получает пакет он попадает сетевую карту сетевая карта генерирует прерывание которое привязана к этой очереди то есть пакет попадает в очередь очередь генерирует прерывание дальше по rss прерывание попадает на какое-то ядро процессора процессор генерирует softer кутайсов тварное прерывание софт варна и прерывание определенного типа нет rx на конкретном ядре процессора важно что в этот момент обработка хардвар нова прерывания прекращается это значит что процессор может продолжить свою работу в обычном режиме может переключаться между различными задачами и следом запускается процедура на people процедур анапе пол замечательно тем что прежде чем начать работу она выключает вообще хардварные прерывания чтобы процессор уже больше не отвлекался и пока прерывание отключены она сама идёт в сетевую карту и выгребает оттуда все пакеты которые туда приходят так она делает на определенное количество пакетов после чего прерывания включаются заново и все повторяется по кругу первая проблема точнее не первая основная которая есть софта ирку с которой мы столкнулись на наших серверах это с тем что нагрузка softer пью на процессор она была очень высокой тут не так важны цифры важно что мы не смогли весь трафик отдать пользователям потому что уперлись процессор процесс работал на сто процентов из after few из них занимал больше половины что делать модерация прерываний волшебный параметр настраиваем модерацию прерываний нагрузка на процессор падает что это такое модерация прерывание может работать на основе двух признаков первое это когда сетевая карта получает пакет она не сразу генерирует прерывание она ждет какое-то время и мы указываем какое второй вариант это если мы указываем не время количество пакетов то есть сетевая карта будет генерировать прерывания и запускать обработку только когда получено определенное количество пакетов таким образом обработка само по себе происходит большими пачками и за счет этого нагрузка ниже нам не нужна в отдельности каждый пакетик обработать здесь видно схематично насколько упала нагрузка если говорить о цифрах то она примерно на треть снизилась но это не все а минус в чем же минус минус в том что когда вы увеличиваете время которое пакет проводит в сетевой карте вы увеличиваете latency соответственно вам приходится выбирать между пропускной способностью или ли tense если у вас приложение требует низкого лет insead значит вы не можете вообще использовать модерацию просто нужно настраивать ее более аккуратно есть адаптивные режимы на некоторых сетевых картах которые в первую очередь заботится о том чтобы была низкая лет инси но и позволяет все-таки повысить пропускную способность тоже это не все к сожалению проблема с которой мы не разобрались это не линейный рост softer кью нагрузки то есть тренд который демонстрирует softer кроссовки softer кю он не совпадает с трендом рост пакетов не совпадает с трендом роста трафика на наших серверах в тестах последние 20 процентов нагрузки на процессор не дают нам практически никакого прироста трафика то есть сервер работает процессор занят тем не менее трафиком и больше не отдаем с это проблемы не разобрались но продолжаем наши изыскания следующий важный слайд не относится напрямую к тем проблемам с которым мы столкнулись многие системные администраторы когда сходят на какието доклады почитают какие-то лекции какие-то статьи бегут сразу же на свои сервера все это сразу сконфигурируем все теперь я а безопасен от всего на самом деле ничего подобного все изменения нужно обязательно применять поэтапно и если вам они не помогли никакого эффекта не оказали обязательно откатывайте сожалению некоторые это не делают и в результате через какое-то время все что они на configure ли начинает работать против них потому что в разных нагрузках разные параметры могут действовать на систему по разному поэтому обязательно делайте это итеративно благодаря тем работам которые мы провели у нас углубились наши знания о линуксе мы получили очень богатый опыт у нас есть политики сев engine которые автоматически генерирует все наработки которые мы получили в рамках этой работы самое главное мы получили бесценный опыт после доклада мне хотелось бы встретиться с теми из вас у кого есть интересные наработки в этих областях с теми кому удается обработать еще больше трафика приходите после доклада ко мне я буду ждать вас снаружи поговорим обсудим поделимся опытом спасибо вопросы да пожалуйста не слышно к сожалению можно микрофон и извините туда микрофон передали да пожалуйста сейчас здравствуйте меня зовут андрей у меня простой довольно вопрос может быть я пропустил или прослушал а сколько в итоге трафик это через сервер прокачивается и какая конфигурация понял 4 10 гигабитных сетевых адаптора сколько прокачиваете реально данных 40 то есть вот прям 40 отца есть той что есть то есть до 40 мегабит отдаем на внутрь на внутреннем интерфейсе где 20 гигабитный bonding там из хранилища подтягивается еще порядка десяти-пятнадцати хоббит спасибо пожалуйста можно вот туда микрофончик здрасьте еще раз спасибо за доклад меня станиславом зовут два коротких вопроса первые вы сказали что отключили шума до для выделения памяти сразу вопроса верха инвалида ции каша у вас пошел то что явно кэш начал его лидера ваться именно на процессорах при такой опции какой эффект это понесло in an overhead на самом деле мы не измеряли эффект потому что предыдущая операционная система об инсульте 10 2 она не поддерживала numa и сравнить нам на сегодняшний день ищем мы еще не занимались плотной этой проблемой это только находится у нас планах хорошо спасибо и еще коротенький вопрос вы сказали про модерацию прерывание это все хорошо нового сетевые карты имеют кольцевой буфер которые чем реже мы забираем тем больше пакетов мы теряем наш мы их перетираем сетевой карте что с этим у вас мы теряем пакеты в сетевой карте только если мы их не успеваем обрабатывать у нас нету никаких потерь мы мониторим эта система мониторинга и на стороне свеча я на стороне адаптера но это зависит конечно от того какие у вас приложения успевают они или нет тут важен мониторинг и тонкий тюнинг не подскажите по мониторингу я просто на самом деле себе плохо представляю как можно увидеть из системы до что мы перетерли то есть по сути с точки зрения клиента и тори transmit да мы не ответили на пакет его он попробует его отправить заново но увеличится у нас ри transmit и но это не повод подозревать что мы перетерли кольцевой буфер но у вас ошибки будут на самом интерфейсе то есть как минимум и в конфе там можно посмотреть и и идея что ул минус с там есть он заносит именно перетирание я просто не в курсе именно про то все все пакеты которые система не успевает обработать они обязательно заносится в счетчике сетевой карты ясно хорошо спасибо большое пожалуйста еще такой вопрос ну так красиво все сказано про то что в 40 гигабит выдали на сервер всегда ли на самом деле при этом сама система себя вела стабильно и ядро не падала и в драйверах ошибок не было и драйверы подбирать не приходил сетевых карт мы делали тест который длился какой-то промежуток времени тут надо понимать что естественно у нас эти сервера постоянно не работают на полную мощность потому что есть такое понятие как отказоустойчивость если у нас сервер будет работать на сто процентов и он упадет остальные тоже сложится у нас есть резерв поэтому в пике сервера загруженные примерно на 60 процентов но когда мы делали тест и естественно мы доводили до максимума более того у нас еще оставалось оставался определенный запас по процессору порядка пятнадцати двадцати процентов таким образом ничто не мешает системе продолжать нормально функционировать и отвечая на вопрос про керном паника и другие проблемы нет такого у нас не было система вполне стабильно спасибо дмитрий дмитрий вижу там спасибо вам большое за хороший доклад вот я хотел немножко в прошлой окунуться и сказать что вот в эпоху гигабитных сетей когда это еще считалось так скажем арендам холодам считалось что то соло и прочие всякие во вклады на во всех картах в большинстве карт реализовано плохо их рекомендуется отключать подскажите как с этим обстоит вот сейчас вот вы упомянули про проблему с ртс колонок а как это обстоит у более так скажем интимных производителей спасибо умела про mila max я говорил только в контексте accelerated рфс но как я уже сказала теле rated рфс работает сама поддержка есть только умела нокса и у solar flare и если говорить о нашей компании то из этих двух производителей есть только mellanox но смело ноксом как я уже сказал проблемы есть что касается флот со входом у нас никаких проблем не замечено по крайней мере на этих серверах все включено все работает все в порядке мы тестировали кстати говоря сетевые адаптеры 10 гигабит на и более новые от интела которые работают на драйвере ай 40 и сетевые адаптеры и к-700 действий и там есть проблема стс они рекомендуют отключать в противном случае адаптер постоянно fl апается но это не проблема потому что есть весу то есть то на что они рекомендуют переходить с о более устаревшие технологии спасибо еще вопросам"
}