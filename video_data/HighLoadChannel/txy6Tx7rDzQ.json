{
  "video_id": "txy6Tx7rDzQ",
  "channel": "HighLoadChannel",
  "title": "Как Badoo модерирует 1 миллиард фотографий в год / Андрей Гоменюк (Badoo)",
  "views": 955,
  "duration": 1933,
  "published": "2017-04-22T14:45:52-07:00",
  "text": "Миллиард фотографий в год или 3 млн в день или 30 фотографий в секунду. Как BДУ модерирует такое количество изображений? Как раскладывает на пару сотен серверов на разных континентах. Об этом расскажет Андрей Гоменюк. Всем привет. А я работаю в компании Баду, и мы модерируем модерируем фотографии. Пару слов о компании. BAДУ - это социальная сеть для поиска новых знакомств. Сайт находится в топ-2 Alexa с 2007 года. Сейчас мы наге в первую сотню. Зарегистрировано больше 160 млн пользователей, которые ежедневно загружают более 3 млн новых фотографий и видео. А к бэкэндам приходит более 40.000 запросов в секунду. Сайт написан на ПХП, данные лежат в МSQэле. Самые узкие места переписаны на CC++. Всё это лежит на Линуксе. В качестве веб-сервера используется Engine X, PHP FPM и для кэширования используется Memcash. Зачем вообще мы это делаем? Специфика специфика проекта такова, что мы хотим видеть живых людей в поиске. Это очень повышает активность пользователей. При этом мы хотим минимизировать жалобы от пользователей и запросы от полиции об удалении незаконного контента. В основном это порно, расчленёнка и прочая фигня. А ещё одна проблема, то, что пользователи предвзято относятся к сайтам знакомств и стесняются загружать свои фотографии, потому что боятся, что их там узнают, найдут и так далее. И вот так вот мог бы выглядеть Баду, если бы мы фотографии не проверяли. А здесь представлены примеры практически всего, с чем мы боремся. Эротика, дети. И внимательный зритель даже может обнаружить человека, занимающегося сексом с козой. А вот так вот выглядит сайт. На самом деле вполне себе живые люди, всё красиво и аккуратно. А я буду рассказывать о том, как мы это делаем. Вот есть какая-то железяка, мы в неё загружаем фотографии, и на выходе она нам говорит: \"Хорошая фотография, плохая, сколько здесь людей, где они и так далее\". Но это где-то фотография из будущего, потому что всё делают люди. А люди в офисе не сидят. Это фотография из интернета. А люди сидят по домам. И, наверное, многим из вас станет неинтересно. Скажите: \"Фу, всё, люди делают\". Но чтобы как-то заинтересовать, приведу немного статистики. А у нас два датацентра. Один в Европе, один в Северной Америке. На каждом из датацентров более чем по 100 серверов с фотографиями и более чем по 100 серверов с пользовательскими данными. А в пике каждую секунду загружается более 150 новых фотографий, и всего это более 3 млн в сутки. Сейчас мы приближаемся к четырём. В пике онлайн бывает более 120 модераторов, которые всё это делают. И среднее время модерации одной фотографии, это 1-2 минуты. Чего мы хотим вообще от модерации? Мы хотим, чтобы всё происходило быстро, потому что чем быстрее фотография отмодерируется, тем быстрее она попадёт в поиск. Мы движемся к цифре не более одной минуты для первой фотографии. И, в принципе, мы близки к этому. Во-вторых, нам нужно качество. Мы за каждую фотографию платим, и мы хотим, чтобы качество было хорошим. И вот третий пункт про стоимость. Он вполне себе очевидный. Не хотим делать лишнюю работу. Никто не хочет, но при этом в рамках большого проекта начинаем сталкиваться с такими проблемами, как пользователь удалил фотографию, и нам нужно очень быстро где-то пометить, что она удалена. Вот это тоже проблема. Собственно, модерация состоит из трёх этапов. Первый этап - это сортировка, когда мы каждой фотографии присваиваем некоторую категорию в зависимости от содержимого фотографии. Это эротика, дети, тут разные. Всего шесть категорий. Через этап сортировки проходят абсолютно все фотографии и в том числе видео. Про видео я скажу лишь в двух словах. А то, что оно проходит тоже через этот этап. Мы просто берём ролик, нарезаем девять случайных кадров, собираем в одну картинку и показываем модератору. То есть абсолютно ничем не отличается от фотографии. Следующий этап - это кадрирование. Это, собственно, выделение лица из фотографии. А, в принципе, эта задача решаемая, она решается довольно давно, разными сервисами. Кто-то позволяет пользователям самим вырезать, кто не хочет париться. Пользователи вырезают всё по-разному. Плюс это тратит их время. Им не так просто, когда они начинают импортировать фотографии из фейсбуков и ВКонтактов. Там импортируют сотню фотографий, и не будут же они бегать, каждую вырезать. Вот. Но, к сожалению, даже кадрирование бывает ошибается, особенно если на картинке какой-то плакат сзади или красивая собачка и страшная девочка, и он выделяет собачку и так далее. И третий этап - это, собственно, проверка качества. У нас есть два варианта, как автоматическая, так и ручная. И о них я тоже расскажу. Схематично это выглядит так. Фотография попадает в кластер. Из кластера мы отправляем события в модерацию. Там модератор принимает решение, и дальше это решение уходит обратно в кластер. И начнём с первого этапа - это как мы доставляем фотографии в модерацию. А если бы у нас был один сервер, там была бы одна база данных, одна табличка с фотографиями, просто выбирали оттуда новые записи со статусом new, отдавали их модераторам и тут же применяли прямо. Но у нас на каждой на в каждом детацентре больше 100 серверов, и каждый раз все оббегать в поисках новых фотографий - это пустая трата времени и ресурсов. Вот. А поэтому логично, что при добавлении новой фотографии мы отправляем некоторое событие куда-то. Но у этого решения есть две проблемы. Во-первых, мы не можем это сделать в транзакции. И, во-вторых, мы очень сильно зависим от этого ресурса. Не очень понятно, что делать, если пользователь грузит фотографию, а этот третий ресурс недоступен. В качестве решения мы реализовали свою очередь, которая лежит физически на каждом на каждом сервере с базой данных. Туда же мы пишем в одной транзакции события. А после этого мы собираем все данные новые в несколько потоков в один центральный сервер с базой данных в каждом дата-центре. При этом не нарушается целостность данных, так как всё происходит в рамках одного сервера. А уж потом сливать мы можем сколько угодно долго. Но есть ещё одна проблема, что у нас два датацентра. Два датацентра означают, что одни пользователи сидят на одном, другие на другом. И вполне логично предположить, что модераторов можно сделать так же. Одни сидят там, другие там. Но тогда возникает проблема масштабирования. И здесь она решается довольно тяжело. Проблема такая, что вам нужно, во-первых, запариваться насчёт найма людей, насчёт управления с ними. Это значит, что они будут там говорить на разных языках. Вот. И плюс, что делать, если на одном дата-центре скопилось много фотографий, но там сейчас никого нет. При этом на другом сидит куча модераторов, и им нечего делать. Поэтому вполне логично, что модерация происходит вся на одном дата-центре. Тем более мы умеем, наша очередь умеет доставлять события между датацентрами. А после обработки мы просто посылаем события обратно в нужный дата-центр, где уже лежит фотография. Но есть один минус. А это некоторая задержка для фотографий, которые грузятся из Америки, северной и Южной, то есть из другого дата-центра. Задержка там может достигать 30 секунд, может быть больше. Некоторое время назад у нас появился быстрый канал. Это позволило для некоторых сервисов внедрить мастер-мастер липликацию. в том числе и для модерации. Это означает, что мы практически мгновенно получаем события о новых фотографиях и практически мгновенно доставляем события обратно. И дальше уже на каждой площадке соответствующие скрипты разбирают, делают всё, что нужно с этими фотографиями. При использовании мастерма репликации есть, конечно, некоторые нюансы с запросами, но мы их все поправили. И итоговое время на доставку одного одного события, то есть одной фотографии - это порядка 20 секунд. Вот. Теперь двигаемся дальше. Собственно, сортировка и кадрирование происходит по одному принципу, поэтому я расскажу на примере сортировки. Так как проект большой, серверов много, нам нужно каждый раз, чтобы показать фотографию, нам нужно о ней знать какие-то данные. Например, на каком фотосервере она находится. А так как за фотосерверами там ещё есть кэши, кто с какого кэша взять и так далее, чтобы каждый раз эту информацию не дёргать с шардов. Тем более, если мы находимся в одном дата-центре, а информация о пользователей в другом. Чтобы туда-сюда не ходить, мы всю эту информацию кэшируем у себя при получении нового события. Ну, есть проблема, что делать, если пользователь во время модерации взял, удалил фотографию. Поэтому мы подписаны на десяток с лишним событий, которые реагируют на удаление, перемещение, смену приватности и так далее. В результате получаем очередную проблему, то, что записей больше, чем чтений, потому что пользователи довольно активные, периодически загружают, удаляют, перемещают. И если бы мы хранили все записи в одной таблице, мы бы столкнулись с тормозами при записи, удалении и прочих операциях. Из-за того, что на эту таблицу пришлось бы навешать кучу индексов для практически всех случаев жизни. Например, выбирать файлы по приоритетам, по датам, по модераторам и так далее. Мы нашли для себя решение. Это один источник записи, одна таблица. Правило очень простое. При этом нет абсолютно никаких проблем со вставками и нет никаких проблем с дедлоками. Например, про дедлоки расскажу историю. Все, наверное, используют он duplicate ke update. Он всем хорош до тех пор, пока вы не пытаетесь апдейтить одну и ту же запись с разных источников. В этот в это время он сваливается с дедлоком, поэтому многие переписывают его на select плюс insert или update. Но когда у вас репликация, вы так делать не можете, потому что в одном датацентре сделали select, записи нет, сделали inert. В другом датацентре сделали select, записи нет, сделали inт. И репликация умерла. Вот, соответственно, использовать надо. Для этого мы статистику делим на две таблицы, пишем в обе, а после этого отдельным скриптом сливаем в одну. Всё быстро и хорошо. В простом случае фотография выпадает во входящую очередь. Когда от интерфейса приходит запрос на новые фотографии, мы перемещаем запись во взятую, э, отдаём в интерфейс. Интерфейс возвращает события, перемещаем запись, ээ, возвращает решение. фотография перемещается в исходящую очередь, из которой уже дальше идёт событие в нужный датацентр. Пишется статистика, фотография перемещается в архив и так далее. Но есть ещё одна проблема. Всё же что делать, если пришла фотография, пользователь её в этот момент удалил, а событие ещё не успело дойти. В этот момент фотографию загрузить не можем. Или какие-то проблемы с сетью, или проблемы с каким-то фотосервером или с кэш-сервером. В этом случае решение очень простое. Если в интерфейс если интерфейс модерации получает статус отличный от 200го, он применяет к фотографии специальный статус автоскип и отправляет её в таблицу с пропусками. После этого мы раз какое-то время оттуда такие записи достаём и перемещаем обратно во входящую. И каждый раз инкрементим счётчик. Если фотография слишком прошла несколько итераций и так мы её не смогли показать, мы её так и оставляем там. Вот. Э пропуски позволяют довольно наглядно видеть, где у нас какие проблемы на данный момент, с каким сервером, с фотосервером, есть ли какие-то проблемы с каналом между Америкой и Европой и так далее. Следующая проблема, например, разом пользователи взяли и импортировали все свои фотографии из Фейсбука или модераторы разом взяли все и уволились или ушли пить, к примеру. Вот. Или требования к модерации изменились. Э юристы сказали, что вот это вот больше не порно это эротика. А вот это не эротика, а порно. И нужно 10 млн старых фотографий взять и обратно отправить на модерацию, на перепроверку. Для этого у нас есть специальная таблица отложенных, в которую попадают такие записи. В ней может лежать сколько угодно записей, и они нас не очень волнуют, потому что как только во входящее количество опускается ниже какого-то предела, там, по-моему, сейчас это 300.000 фотографий, мы начинаем потихоньку подмешивать туда записи из отложенных. Таким образом, мы можем в фоновом режиме перемодерить сколько угодно фотографий за буквально несколько недель. Что делать с качеством? Есть читеры, которые сидят, тыкают кнопки, и в статистику им пишется, что они модерируют. А чтобы проверить, как они это делают, у нас есть специальные несколько человек, которым мы доверяем. Они заранее проверяют некоторые фотографии. После этого этот массив фотографий мы складываем в отдельную таблицу и 1% от запрошенных фотографий подмешиваем. Таким образом, на каждые 100 фотографий модератор одну получает проверочную. После этого мы сравниваем результат модерации. Если он отличается, мы считаем, записываем ему ошибку и по достижении определённого количества ошибок мы с ним расстаёмся. Но, к сожалению, это не позволяет держать общее качество на высоте. Для одной из конференций один наш сотрудник сделал такую клёвую штуку. Вращающаяся земля, на которой из мест регистрации всплывали в онлайне фотографии, которые пользователи, которые там только что зарегистрировались и которые прошли процедуру ээ кадрирования. И вот когда, если долго смотреть на эту штуку, вроде всё замечательно, но довольно много фотографий, которые то обрезаны не так, то там вообще какая-то фигня. И, естественно, для этого была добавлена ручная проверка. Если фотография промодерирована правильно как хорошая, то она отправляется сразу на сервер, но при этом мы откладываем её на перепроверку. Если же она плохая, то нам нет смысла торопиться её откуда-то удалять. И мы для начала отправляем её на перепроверку. Уже если пере если супермодератор сказал, что действительно плохая, только тогда удаляем её и делаем с ней всё, что нужно. Работает это так. На экран мы выводим по 50 фотографий с одним статусом, э, группируя по полу. модератор, буквально за несколько секунд накинув их взглядом, может найти те, которые отличаются от статуса. Там смотрят, например, хорошие фотографии, там дети. А одним нажатием он отправляет такие фотографии на перемодерацию. Их обычно немного, на 50 - это там две-три штуки. И за один час он может так перепроверить до 10.000, в некоторых случаях больше. И вот финальная схема с фотографиями для проверки. И идём дальше. Остался из этой схемы интерфейс. Интерфейс выглядит вот так. Здесь проведён интерфейс для кадрирования. Отличается он от интерфейса сортировки только тем, что здесь выделено лицо и показаны примеры того, как будет выглядеть обрезанная фотография. А всё остальное делается кнопками. Основных кнопок всего штук пять шесть, да? А и мы всячески пытаемся модератору помочь проверить фотографию. Например, в кадрировании мы сразу выделяем лицо. Мы рамкой рисуем, показываем пол анкеты. И если модератор видит, что рамка синяя, а на картинке девочка, значит это явно не владелец аккаунта. Весь интерфейс написан на флеше или на флеше. Мы из него собираем дополнительную статистику обо всяких таймингах, о скорости загрузки каждой фотографии, о временах, пока фотография лежала, где-то ждала, ээ, прежде чем показаться модератору, сколько он на неё смотрел и так далее. А, и чтобы не создавать лишнюю нагрузку, мы все фотографии отправляем небольшими пачками, но не сильно большими, чтобы они по долгу не залёживались у модератора. Остался последний этап - это, собственно, применение решения после того, как фотография промодрилась. А зачастую для того, чтобы применить решение, нам нужен физический доступ к фотографии. Это её обрезать как-то или переместить в другой альбом. А удалить. И поэтому логично, что каждое событие модерации должен обрабатывать тот фотосервер, на котором она находится. После получения события модерации мы достаём информацию о фотосервере, где лежит фотография, и складываем их в отдельную таблицу для уже для применения решения. Весь этот процесс вместе со временем на нарезку и прочее занимает не больше 30 секунд. И общая схема как-то вот так выглядит. Так как у нас модерация ручная, то картинки тоже все вручную сделаны. А в баду есть такой принцип: ты не не контролируешь до тех пор, пока не измеряешь. И создавая любой ээ любой сервис, мы начинаем с того, что начинаем его как-то мерить. Э измеряем мы скорость доставки различных событий, скорость модерации различных её этапов. размеры каждой очереди, а также с помощью пинба собираем стандартные показатели. Тайминги мы меряем в основном с крайбом. Теперь о человеческом факторе. Модераторы люди, поэтому они начинают работать только тогда, когда им удобно. Они делают то, за что больше платят, а платим мы больше за кадрирование. Так почему-то исторически сложилось. И если записей мало, они начинают просто сидеть ждать. заниматься своими делами. Потом, когда в интерфейс придёт достаточное количество фотографий, они их быстренько прощёлкают, проверят и дальше занимаются своими делами. Но это напрямую влияет на скорость, которая для нас очень важна. Чтобы они работали ночью, мы увеличиваем тариф за ночную работу. А мы можем закрывать очередь кадрирования автоматически, если в очереди на сортировку очень много фотографий. Очередь кадрирования не так приоритетна, потому что, в принципе, лица определяются зачастую довольно точно, поэтому можем иногда пропустить этот этап. А если модератор не проявляет активности, мы его выкидываем через некоторое время, и когда в очереди достаточное количество фотографий, мы подаём звуковой сигнал, что, мол, иди обратно. А вводятся различные приоритеты для важных фотографий, которые в результате отдаются самым активным. И в итоге средняя скорость принятия решения у нас порядка 10 секунд на приоритетную фотографию, порядка 30 секунд для обычной. Принятие решения, подразумеваю это время с момента попадания фотографии в модерацию до момента её ухода оттуда. Ну и напоследок такой вопрос: не могли бы мы всё автоматизировать? В данный момент мы сотрудничаем с одним европейским вузом, который разрабатывает систему, основанную на искусственном интеллекте, которая позволяет определять содержимое фотографий. А в ближайшее время мы собираемся получить от них первые результаты, но, к сожалению, те проблемы, которые мы видим уже сейчас, у нас довольно сложные правила модерации. Например, это разница между эротика и порно. Это разница между лёгкими наркотиками и тяжёлыми, которые даже модератор иногда не понимает, что это порошок или сахар. А в том числе правила могут меняться, и каждый раз, если правила меняются, нам нужно заново надрессировывать этот искусственный интеллект. И заявленная скорость анализа нас не очень устраивает, потому что сейчас это порядка одной секунды на каждую фотографию. Правда, я не хочу соврать, я не знаю точно, это на на одно ядро или на целый какой-то абстрактный процессор. И в любом случае после этого нужна ручная перепроверка. Так что пока что это всё где-то в будущем. Но вот одна из вещей, которую мы сейчас активно автоматизируем - это определение знаменитостей. Дело в том, что пользова модераторы не всегда знают всех знаменитостей в лицо и не могут понять, то ли это Джулия Робертс, то ли это просто у девочки род большой. И, например, вот эти вот, вот этот вот парень и три пингвина - это самые часто чаще всего загружаемые знаменитости к нам на сайт. А подразумевается не именно эта фотография, потому что у него их много. Э, и здесь просто его лицо. Вот. А я не помню его имя, не так важно. Ну, ты снимался. А, в общем, проблема как раз такая, что модераторы знаменитости не знают, и обучить их всем мы тоже не можем, потому что они приходят, уходят. Вот. И мы научились это делать, научились определять знаменитостей, но пока это в процессе разработки, и мы рассказать об этом не можем. И возможно это будет тема следующего доклада на следующем хайлоде. Так, ну, пожалуй, всё. Теперь я отвечу на ваши вопросы. А подскажите, пожалуйста, вот с модерацией там ходит только метон информации или фотография сама тоже бродит в события? Нет, приходит приходит только запись о фотографии, а сама фотография загружается уже со своего кэш-сервера. Спасибо. Я непра Скажите, пожалуйста, у вас очередь на МсQэле, если оттуда сделать два селекто, два потока, то вы получите одну и ту же выборку. Вот как вы за этим боретесь? У вас там ктейбл или что? А у нас есть заранее определённые для каждого события, и поэтому каждая каждое событие расфасовывается по своим подписчикам, а дальше уже подписчик сам решает, сколько ему селектов делать. То есть вы, когда добавляете фотографию в очередь, вы сразу пишете, какому модератору это дать. Нет, тогда я не понял. Когда мы добавляем фотографию в очередь, она ни к кому не привязана, она просто лежит себе и всё. А когда он А когда она перемещается во взятую очередь, там она, да, уже помечена, кому за кем она закоплена. О'кей. День добрый, Виктор Вайдвагел. У меня несколько вопросов. Вопросы раз. Вы говорили, что из каждого ролика выбирается девять рандомных карт. Это означает, что теоретически, как я понимаю, я могу долго и упорно грузить плохой ролик, пока он там не пройдёт. Или нет? Теоретически, конечно, да. Угу. А следующий вопрос на самом деле по цифрам возник. Вы озвучили цифру в 120 модераторов примерно и 10 секунд на принятие решения. То есть это означает, что на если все модераторы работают круглосуточно, то каждый модератор э обрабатывает примерно 9.000 фотографий в сутки. Ну то есть круглосуточно без отрыва от станка. А общая цифра 3 млн. Вот ещё 2 млн. Они на каком этапе отсекаются? Я не очень понял. А я, когда озвучивал цифру в 10 секунд, я поправился, что эта цифра подразумевается от момента прихода в модерацию до момента ухода её оттуда. Собственно, модератор принимает решение обычно за 1-д секунды. Угу, понял. И последний вопрос. Вы ещё упоминали про супермодераторов, насколько я понял, записание, но большая часть отмодерированных фотографий потом через них проходит. А этих героических людей сколько по порядку примеров? Я не могу сказать точно. Потому что мы только месяца два назад это сделали. Их порядка четырёх-пяти человек. А, и они всё обрабатывают сразу. Нет, нет, всё они не успевают обрабатывать. У нас там есть время, некоторое задержки. Если фотография так и не успела верифицироваться за час, по-моему, она уходит дальше. Понял. Но вообще мы стремимся к тому, чтобы покрыть весь этот процесс людьми. Угу. Понял, спасибо. А скажите, пожалуйста, а вот вы говорили о том, что вы кадрируете фотографии, вы их руками кадрируете или как-то автоматически? У нас есть система автоматическая, которая определяет лица, она их показывает. И в больше в большинстве случаев от модератора требуется только нажать Enter, что всё хорошо. Но если плохо, то он мышкой двигает там квадратик. Я понял. А вы её сами писали или это что-то стороннее? Это что-то очень старое, где-то взятое, насколько я знаю. О'кей, понятно. Ну тоже там Open CV, наверное, да? А скажите Open CV, наверное, да, какой-нибудь. Ээ, не знаю, честно. А скажите, пожалуйста, каким образом вы поддерживаете актуальность вот пула модераторов? То есть, грубо говоря, там, не знаю, под Новый год много людей там уходят отдыхать, а нагрузка, наоборот, возрастает. Как вообще вот это вот управляется? А у нас просто очень много модераторов. А скорее мы поддерживаем не их наличие, потому что с наличием проблем не бывает, а мы поддерживаем то, чтобы они вовремя отсекались, чтобы не создавать лишнюю задержку для модерации. А находите вы их, ну, то есть просто на сайте, э, есть там реклама. Хотите быть модератором, становитесь или как? На сайте на сайте нет, где-то есть, по-моему, на каких-то на каком-то форуме. А, понятно. Ну, вообще с ними проблем нет. А ещё тогда вопрос. По какому принципу выбираете супермодераторов? Это люди, которые довольно давно работают, которые хорошо справляются. Спасибо. Добрый день. А можно такое немного корпоративный вопрос, если он не является тайной? Сколько времени в человека часах было потрачено на разработку данной системы? И, собственно, сколько человек занимается поддержкой данной системы? Сколько времени было потрачено, я не скажу, потому что её начали писать довольно давно, а поддерживали разные люди. А сейчас мы, скажем так, у нас в компании нет такого, что есть продукт и его поддерживает один человек и занимается полностью им. Это всегда люди занимаются поддержкой одного, двух, трёх проектов и так далее. Но вообще, я думаю, не сильно ошибусь, если на поддержку тратится в зависимости от загрузки несколько часов в неделю в среднем. А я же правильно понимаю, судя из вашей нарисованной схемы, там достаточно много сервисов и, соответственно, легко предположить, что они имеют достаточно большую вероятность сбоя, а, ну, что показывает, собственно, ваши графики мониторинга. А, поэтому я, собственно, ну, спрашиваю, но просто интересно. То есть если вы говорите, что несколько часов в неделю, то мне кажется, это не совсем правильно. Я говорю о среднем времени, соответственно, когда если всё хорошо, то никто никто не занимается. Если всё плохо, этим занимаются. Если появилась какая-то задача, например, недавно вот мы делали ручную верификацию, естественно, там больше уходит времени. Но это какой-то конкретный штат, там, допустим, два-три человека отвечают или там Да, да, дада, это два-три человека примерно. Спасибо. Включая флешера. День добрый. У меня вопрос, Баян. Ну просто, может быть, у вас есть более длинный ответ. Почему вы не используете собственных пользователей для модерации? Меня это волнует. Одна альтернативная социальная сеть просто это явно использует, а вы как-то прямо сказали: \"Нет\". Я думаю, если бы мы использовали собственных пользователей, с ними было бы сложнее. А зато дёшево, да? Но пока вы пока вы ни за что не платите, вы не можете чего-то требовать. Как только вы, если вам нужны какие-то конкретные условия, вы вам приходится за это платить. Ну, ну вы не не пробовали, то есть вы попробовали, у вас получилось, что это плохо и отказались или просто сразу нет? Насколько я знаю, не пробовали. Спасибо. Здравствуйте. А как долго у вас в среднем работают люди и как вы их потом лечите? А у меня нет такой статистики. Я знаю, что у нас есть люди, которые работают годами, в том числе это вот те, которые супермодераторы. Средний срок жизни мы не замеряли. Возможно, такая статистика есть у менеджера, который занимается наёмом их наймом. Всё, спасибо. Угу."
}