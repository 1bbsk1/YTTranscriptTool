{
  "video_id": "bA-Y_BfTZZY",
  "channel": "HighLoadChannel",
  "title": "Почти без магии, или Как просто раздать терабит видеопотока / Михаил Райченко (Вконтакте)",
  "views": 1500,
  "duration": 2627,
  "published": "2019-05-15T05:15:08-07:00",
  "text": "давайте нас к экскурса в принципе я расскажу вам про людей restream от нам про сами им про там с каких платформ мы принимаем видеопоток на какие раздаём и в конце расскажу про текущую архитектуру лайвов про то какие ограничения у нее есть какие возможности и про то как текущая архитектура пережила такой эффект как клевер в принципе это и есть план моего рассказа и вот у нас заработал clicker отличным первое сначала я расскажу немного о самих прямых трансляциях и про скримеров то есть людей которые присылают нам видео контент и который мы показываем другим зрителям после этого немножко сравниться видео звонками и из самим видео зачем я это делаю ну если вы задумаетесь о том чтобы сделать свой сервис трансляции то наверно вы хотите каким-то образом возможно схитрить возможно где-то сэкономить где-то оптимизировать и тут важно понимать где мы можем сэкономить по сравнению с видео где мы проигрываем где мы выигрываем проведу небольшой экскурс в историю расскажу как было устроено самая первая архитектура видеотрансляции то есть это можно даже назвать прототипом тем не менее она работала и если вы делаете свои трансляции опять же вы можете на ней даже и остановиться после этого расскажу как формулировались требования к архитектуре откуда они взялись и собственно говоря расскажу про то как мы их реализовывали и что у нас в итоге получилось могу сказать что версия архитектуры которые сейчас есть было наверное сделано примерно в четырнадцатом может пятнадцатом году она над ней был приведен достаточно большой тюнинг но тем не менее все основы остались такими же после этого мы говорим поговорим про масштабирование балансировку это достаточно важно так как хорошая архитектура должна выдерживать горизонтальное масштабирование и должна достаточно устойчиво себе ввести при большом количестве зрителей и большом количестве стримов расскажу какие протокола мы используем у нас достаточно небольшой набор протоколов как для приема видео так и для доставки и расскажу почему мы их выбрали соответственно обсудим метрики качества и как мы мониторим трансляции это важно для эксплуатации любой системы или опять же как уже сказал расскажу про как текущий архитектура я пережила рост клевера и насколько она справилась собственного это был план моего доклада давайте начнем ну это простая картинка я думаю все сервисы трансляции выглядит примерно так у нас есть некоторые стримеру присылают поток rtmp к нам и мы показываем зрителям то есть ничего удивительного ничего сверхестественного откуда приходит видеопоток основным источником но не основным достаточно большим источником трафика для нас является наше мобильное приложение vk life что в нем хорошо в нашем приложении мы можем полностью контролировать то как мы кодируем видео мы можем произвести много оптимизации на стороне клиента для того чтобы потом с минимальными задержками показывать его нашим зрителям из минусов мобильные приложения работают поверх сетей это может быть 3g но в любом случае почти всегда это мобильные сети соответственно это вносит некоторые лаги необходимо дополнительно буфере zero вать данные на стороне и приложения для того чтобы поток шел как можно более ровным вторым источником является стримеры которые стремятся либо а без либо в кастом либо каких-то еще десктопных приложений это достаточно большая аудитория иногда это семинары иногда это игровые стримы игровых стримов особенно много с таких приложений из положительного таких приложений немного мы можем достаточно хорошо рекомендовать их настраивать нашим стримером но при этом там много настроек и зачастую шлют совсем не тот поток который мы хотим ну и третья категория это rtmp под rtmp поток от медиа-сервер офф это может быть какие-то совсем маленькие медиасервера то есть домашнего формата человек стримит там вид на улицу или что-нибудь еще либо достаточно серьезные трансляции от наших партнеров там может быть все что угодно таких стримов не очень много но в основном они весьма важны для нас кто зрители ну опять же это мобильное приложение тут все понятно самая большая проблема это сетевые задержки из плюсов опять же мы можем кастомизировать достаточной степени player нам это удобно хорошо но не везде скажем так это получается на 100 процентов в плеер навеки . com тут тоже все просто это обычный в плеер который вы можете открыть посмотреть большая аудитория достаточно vk.com много зрителей трансляции некоторые трансляции висят у нас в разделе видео там может быть десятки тысяч без всякого пиара иногда из особенно если это интересный контент соответственно каналы достаточно большие у зрителей в этот которые сидят на в плеере поэтому трафика достаточно много в том числе на одну трансляцию ну и третье это в плеер вконтакте на каком-либо 100 на стороннем сайте вы можете начать стремитесь все что хотите и в себя на сайте повесить в плеер вконтакте вы можете выступить нашим партнерам если у вас интересный контент можете повесить у себя можете повесить у нас то есть как хотите можете организовать трансляции свои таким образом мы тоже все будет работать давайте сравним с видео звонками видеозвонках нам простят небольшие искажения изображения то есть это видео звонках проще мы можем существенно ухудшить изображение но при этом мы обязаны выдерживать очень хороший лет инси то есть при большой задержке сервис будет абсолютно невозможно использовать в трансляциях в этом смысле немножко наоборот мы должны поддерживать достаточно хорошее качество изображения но при этом можем увеличить light нэнси в силу множества факторов например в любом случае плеер и так или иначе буферизируется себя поток это может быть секунда 2 для того чтобы пережить деградацию сети например поэтому-то мне нужно скажем так милисекундные 10 милисекундные задержки в большинстве случаев можно к этому стремится но продуктового выглядит что это не не является обязательным условием с обычным видео тут ситуация ровно наоборот нам нужно очень хорошее качество при этом нам желательно минимизировать размер видео то есть соотношение битрейта и качества для того чтобы с минимальным потоком отдавать наилучшее изображение из плюсов мы не ограничены по времени в момент загрузки видео у нас достаточно много времени для того чтобы оптимизировать видео посмотреть как мы можем его сжать наилучшим образом что-то сделать утащить его на крыше если надо в общем все достаточно хорошо лайках ситуации опять же обратно и у нас очень мало времени на транскодирование при этом возможности по времени мало но нет никаких ожиданий по трансляции то есть зрители нам простят если у нас будет задержка либо качество будет не очень самая первая версия она вполне ожидаемая на самом деле она немножко другая на следующем сайте слайде покажу этот стриммер медиасервер и зрители то есть реальная версия была вот такой вот стример медиасервера уровень каширования и зрителям в принципе эта версия позволяет масштабироваться достаточно сильно я бы сказал там десятки тысяч зрителей она уже должна выдерживать у нее есть другие недостатки например если посмотреть на схему на эту видно что она не отказоустойчивая и мы должны угадывать с медиа сервером для того чтобы хорошо блонди сбалансировать зрители мы не можем на каждый медиасервер повесить много кощей это просто дорого поэтому мы посмотрели на это поняли что это конечно просто и понятно есть некоторые возможности для масштабирования но явно чет не хватает мы начали формулировать требования что важно количество трансляций мы должны держать тысячи и возможно даже десятки тысяч и трансляций при этом количество зрителей на одну трансляцию может быть достаточно велико но до миллиона вполне должны держать на данный момент и при этом суммарное количество зрителей тоже достаточно велико то есть там десятки сотни тысяч отказоустойчивость тут отказоустойчивость с двух сторон во первых отказ принимающей стороны то есть . где мы принимаем видеопоток не должен сильно влиять на трансляции и отказ со стороны раздающий то есть отказ одного из серверов со стороны где мы раздаём видеопоток тоже не должен влиять на наши трансляции доставка до регионов тоже важный момент глупо тащить весь видео контент например из петербурга или москвы до какого-нибудь там новосибирска екатеринбурга или даже из питера до москвы это не очень хорошо потому что кивая задержка будет большая будут лаги все будет лагать не хорошо поэтому наша инфраструктура должна учитывать что мы доставляем контент до регионов и опять же важное свойство это удобство эксплуатации мониторинг так как система достаточно большая зрителей много важно вовремя отсылать alert и администратором в случае каких то проблем и в том числе мониторинг продуктовые технические метрики в итоге мы пришли к достаточно простой но при этом эффективной инфраструктуре выглядит она так это стример ему выдается ссылка на точку входа это приемники их несколько стример начинается rtmp поток он попадает на приемник на приемнике происходит некоторое валидация и пересылка дальше на медиа-сервером приемник выбирает на какой из медиа-сервер офф отправить поток балансирует по входящему трафику зачастую по нагрузке по доступности сервера он выбрал меня сервер и отправил туда поток задача медиасервера с одной стороны просто и понятно с другой стороны она сложная нужно во-первых за транс ходить то есть преобразовать видеопоток в несколько разрешений для доставки и во вторых нужно сформировать в нужный формат с медиа-сервер офф мы попадаем на уровень каширования это первый уровень каширования он не очень большой но он очень важный мы на этом уровне мы каширу им фактически все видеопотоки которые у нас есть и раздаём их дальше это не очень большое количество собирать серверов это большое количество оперативной памяти либо дисков быстрых на самом деле и того и другого этот уровень защищает медиасервера от перегрузки фактически в любом случае вне зависимости от количества зрителей или серверов раздающих уже конечных речи серверов медиасервер они будут перегружены ник не соединениями не собственно говоря раздачей они защищены достаточно надежным ну и последний уровень кэширования это с сервера который уже стоит непосредственно в регионах в точках доставки тут все просто они не очень сильно кэширует трафик и задача просто отдать его зрителю то есть что интересно балансировка в этой схеме мы выбираем балансировку то есть мы стараемся на каждый сервер прислать зрители которые смотрят один и тот же поток здесь очень важно локальность каша потому что есть серверов может быть достаточно много если мы не будем соблюдать и временную и локальность каша с точки зрения стрима там и перегрузим внутренний слой этого нам тоже не хотелось бы поэтому балансируем и следующим образом мы выбираем некий сервер edge для региона на которой мы отсылаем зрителей и отсылаемых до тех пор пока они не начинаем понимать что произошло некоторое заполнения и стоит отсылать на другой сервер схема работает очень надежная и очень простая естественно для разных стримов мы выбираем разную последовательность с серверов как и последовали за который мы рассылаем зрителей соответственно балансировка работают достаточно просто так же мы отдаем клиентам ссылку на доступный и сервер зачем это сделано это сделано для того чтобы в случае отказа от сервера мы могли перенаправить зрителя на другой то есть зритель пока смотрит трансляцию ему там вырос несколько секунд приходят ссылка на нужный сервер если он понимает что ссылка должна быть другое он переключается то есть переключается плеер и продолжает смотреть трансляцию уже с другого сервера еще одна достаточно важная роль серверов это защита контента то есть вся защита контента посуде происходит там у нас свой модуль денег сам для этого до пелин соответственно ну не знаю он чем-то похож на security link следующее немножко пройдёмся масштабирование по количеству стримов понятно что в этой архитектуре мы очень хорошо от масштабированы мы можем легко добавить медиа-сервер офф приемников и у нас не будет проблем при этом с точки зрения раздачи контента мы можем легко отмасштабировать с сервера пока не упремся в пропускную способность слое кэширования внутреннего но это достаточно тяжело сделать масштабирование по суммарному количеству зрителей тут тоже все понятно нам нас не пугает что зрители стримов много мы соблюдаем локальность кэширования поэтому 123 зрительно на стрим легко 100 зрителей на стрим легко там тысячи десятки сотни тысяч тоже легко если что добавляем сервер для стрима и соответственно начинаем масштабировать дальше самый интересный момент масштабирование по количеству зрителей на 1 трансляции тут сложнее потому что если мы начинаем отдавать один и тот же сервер а потом пускаем трансляцию может произойти неплоха не очень хорошая вещь сервер будет некоторое время перегружен ну для этого мы иногда включаем режим случае популярной трансляции выдавать ни один сервер а сразу несколько либо случайным образом либо по трафику либо в случае перегрузки сервиса придет в ссылке новый сервис и зритель переключиться на другой сервер ну после того как мы отмасштабируем из можно заниматься качеством чем мы и занимаемся последнее время так масштабирование работает хорошо какие протоколы мы использовали основной протокол но один из основных было rtmp не только на вход но и на раздачу контента основным преимуществом это низкая задержка то есть это может быть там полсекунды секунда 2 секунды на самом деле преимущество на этом заканчиваются это потоковый протокол его сложно мониторить он закрытый в некотором смысле несмотря на то что есть спецификация flash player уже не работает то есть по сути он уже все нужна поддержка на уровне седин то есть нужны специальные модули в ingenico или свой софт для того чтобы нормальная передать поток или съесть некоторые сложности в мобильных клиентов из коробки в мобильных клиентах поддерживается очень плохо нужны специальные доработки и все это все все очень сложно 2 протокол который мы использовали это и через выглядит он достаточно просто это текстовый файлик индекс файл так называемый в нем содержатся ссылки на индексные файл и с различными разрешениями а в самих файлах содержатся ссылки на миде и сегменты чем он хорош он очень простой несмотря на то что он староват он позволяет использовать седин то есть вам нужен всего лишь gemix для того чтобы раздать hls протокол и он понятен с точки зрения мониторинга ну вот собственно его плюсы простота эксплуатации ingenico как просим сервер легко мониторить и снимать метрики то есть вам достаточно мониторить примерно то же самое что вы мониторите на своем веб-сайте сейчас это у нас основной протокол доставки контента из существенных минусов это высокая задержка высокая задержка вышел с а она вложено фактически в сам протокол так как необходимо достаточно большое время буферизации плеер должен ждать как минимум пока загрузится один чанг а по-хорошему должен дать пока загрузится два чанка но 2 медиа сегмента иначе просто случае лога у клиента появится про груз player это не очень хорошо влияет на юзер экспириенс второй момент который дает задержка вышел с это каширование то есть плейлист кэшируется на внутреннем слое и кэшируется на серверах даже если мы которым образно говоря на секунду на полсекунды это плюс примерно две три секунды задержки суммарно получается 12 до 18 секунд от не очень приятно явно можно лучше с этими мыслями мы начали улучшать hls улучшили моего достаточно простым путем давайте отдавать последний даже еще не записанный с медиа сегмент в плейлисте немножко раньше то есть как только мы начали писать последними 10 мент мы сразу анонсом его в плейлисте что в этот момент происходит уменьшается время буферизации в плеерах то есть плеер считает что он уже все загрузил и спокойно начинает качать нужные сегменты мы немножко обманываем таким образом player но если мы хорошо мониторим стал и так называемый то есть про грузы в плеере нас от не пугают мы можем опять же перестать отдавать сегмент заранее и все вернется в норму второй момент выигрываем суммарно примерно 58 секунд откуда они берется это время сегмента то есть от 2 до 4 секунд на один сегмент плюс время на каширование плейлиста то есть это еще две-три секунды то есть задержка у нас уменьшается причем достаточно существенно задержку уменьшается с 12 15 секунд до пяти-семи что еще хорошего в таким таком подходе это фактически бесплатно нам нужно всего лишь проверить что плеер и совместимы те которые несовместимы с таким подходом мы отправляем на старую роллы на новой орлы мы отправляем соответственно совместимые плееры нам не нужна апгрейдить старые клиенты что тоже важно который это поддерживают нам фактически не нужно дорабатывать плееры в мобильных клиентах то есть религий дорабатывать и нам не нужно зарабатывать в player это выглядит достаточно хорошо из минусов нам необходимо контролировать входящий видеопоток ну то есть случай мобильного клиента мы можем это делать достаточно легко когда стрим идет с мобильного клиента либо транс ходить в обязательном порядке почему потому что плеер должен знать сколько занимает один медиа сегмент времени а поскольку мы анонсом его до того как он реально записан мы должны знать когда какое время он займет когда мы его запишем и так в принципе таким образом мы улучшили hls теперь хочу рассказать как мы мониторим и соответственно какие метрики качества мы снимаем одна из основных метре качество это время старта идеально когда вы листаете например мобильном клиенте до трансляции нажимаете кнопку и она сразу начинается еще идеально было бы чтобы она начиналась до того как вы нажали кнопку но к сожалению только когда нажали второй момент эта задержка сигнала мы считаем что несколько секунд это очень хорошо 10 секунд терпимо там 20 30 секунд это уже совсем плохо почему это важно например для концертов и для массовых мероприятий каких то это абсолютно не важная метрика там нет обратной связи мы просто показываем стрим лучше пусть не лагает чем будет небольшая задержка а например для стрима где идет какая-то конференция или человек что-то рассказывает ему задают вопросы здесь от начинает быть важным потому что вопросы в комментариях дают достаточно много и хочется чтобы зрители получали фидбэк как можно раньше еще одна из важных метрик это буферизация и лаги на самом деле это метрика важно даже не . не только с точки зрения клиентов и качество а с точки зрения того насколько мы можем про тюнить доставку hls насколько мы можем выжить данных из наших серверов поэтому мы мониторим как и среднее время буфера в плеерах так и столы ну и выбор качества в плеерах тоже понятно так неожиданные изменения они всегда напрягают соответственно это тоже важная метрика мониторинг у нас есть много метрик мониторинга но здесь я выбрал эти метрики которые срабатывают всегда если что-то пошло не так во первых это количество стримов онлайн как только что-то идет не так это количество падает сразу то есть иногда сильно иногда не сильно но в случае любой ошибке или проблем на серверах мы замечаем это моментально но как мы миндаля там десятки секунд минуты то есть достаточно быстро входящий и исходящий трафик тоже понятно и метрика причем входящий пожалуй более стабильная так как исходящий зависит от популярности трансляций а тут далеко не всегда можно предсказать насколько какое-то мероприятие будет популярным поэтому в основном входящий трафик ну и время ответа из серверов это понятная метрика причем она скорее важно для администрирования эксплуатации так как проблемы на серверах на из серверах сразу я проявляют случае любых проблем теперь расскажу как мы справились с таким приложением как клевер когда мы использовали нашу инфраструктуру для того чтобы транслировать видеопоток с вопросами и ответами все знают что такое клевер ну так примерно матчах окей это онлайн викторина ведущий что-то говорит спрашивает выпадают вопросы вы отвечаете 12 вопросов 10 минут игры там в конце как какой-то приз что здесь сложного это рост вот справа это график конкретно вот этот пик это нагрузка на сервера в части api в момент старта клевера все остальное время это обычное течение трансляции это нельзя со приравнять количеству зрителей это пожалуй количество запросов именно нагрузка это тяжело то есть пять минут к нам приходят иногда в пике было миллион зрителей они начинают смотреть трансляцию они регистрируются они выполняют какие-то действия они запрашивают видео то есть казалось бы что очень простая игра но то что это происходит очень сжато по времени все действия то есть том числе и финал игры дает достаточно большую нагрузку и так какие вопросы и челленджи перед нами были это быстрый рост иногда это было плюс 50 процентов в неделю то есть если например в среду у вас 200 тысяч людей то в субботу или воскресенье у вас могло быть уже 300 это много начинают всплывать проблемы которых раньше не было видно и при этом он проходит два раза в день у вас очень мало времени на решение этих проблем в утренние и трансляции обычно поменьше людей что понятно это утро вечером побольше то есть принципе у вас есть утренняя трансляция чтобы одни божиться понять что может пойти не так или что начинает идти не так и к вечерней трансляции вы должны уже иметь решение проблемы иначе будет очень неприятным примерно у нас уходило 12 часов на решение устранения проблем это время кажется что много но на самом деле нет потому что нужно протестировать нужно выкатить нужно попробовать на какой-то другой трансляции подходы и нужно быть уверенным что вы решили эту проблему потому что если вы не справляетесь за это время скорее всего в следующий раз вы упадете все проблемы начинают себя проявлять на примерно 200 на 300 тысячах это при приводит к очень неприятным последствиям там на 400 пятки 100 в чем еще сложность из за того что это трансляция 1 а зритель немного случае например лагов на сервере трафик моментального раза тает в 35 раз как это происходит у плеера происходит стал он получает новый плейлист которым есть сегменты которых он еще не видел они скачал и он начинает их качать качает их в 3 раза быстрее то есть он хочет скачать в 3 раза больше иди на момент на для того чтобы за буфере zero вать немножечко и при этом еще может через некоторое время переключить качество и произойдет еще рост примерно в 35 раз как с этим бороться ну первое это деградация качества это понятно и второе обязательно на клиенте нужно делать exponential быков иначе вы просто не подниметесь случае проблем экспаншен быков понятно то есть попробовали сломалась следующий раз должны попробовать через две секунды потом вам через четыре через восемь тогда вы достаточно быстро поднимитесь случае проблемы и не перегрузить свои бэкон сервера собственно что показал нам клевером во-первых масштабирование балансировка работают очень хорошо в такой схеме если будете делать большую систему трансляции в принципе рекомендую такой подход он очень хорошо работает мы сделали тестирование фоне то есть каждый раз когда пользователь открывает любую трансляцию мы формируем например некоторую фейковую которую шлём те же запросы которые слали бы в клевер в этом случае мы можем удостовериться что наше решение во первых сработали случае проблем а во вторых что не возникли новые проблемы сделали метрику которую мы обычно не отслеживаем и не смотрим это количество пользователей у которых не было стала в течение 5 минут это интересная метрика при чем она интересна именно для клевера то есть если у вас обычная трансляция то кажется что неважно сколько пользователей испытывают проблемы то есть в среднем работает то есть 10 процентов столов нормально там 10 процентов столов на 100 стримов тоже нормально для клевер это важно потому что видеопотоки идет идут вопросы и даже один стал или один правок вызывает большой негатив чего не происходит в обычной трансляции то есть там нет не такой негатив вызывается из одного лага вот если там 100 лагов у пользователя да это неприятно один лагов в течение 15 минут но нет такого негатива сильно улучшилась взаимодействия команд то есть в клевере поучаствовали команды сетевой инфраструктуры администрирования бэг-энда продуктовые команды я бы сказал что очень-очень взаимодействия стала более продуктивным как не странно это звучит лучше ли инструментарий для диагностики фактически в плане видеотрансляции мы научились детектировать проблемы сразу после возникновения и даже предвосхищать некотором смысле любые проблемы которые у нас возникали в на финальном этапе мы даже наверное не деградировали не по качеству не пугайтесь и не проходим характеристикам но и прокачали навыки нагрузочного тестирования спасибо нашим тестировщиком не достаточно много сделали для того чтобы помочь нам разобраться с любыми вопросами соответственно пик клевера на одну трансляцию нас приходился один миллион зрителей и соответственно отдавали мы в одну трансляцию около терабит и видеопотока это достаточно много домашними ли он может чуть меньше это достаточно много для одной трансляции там возникали проблемы не проблемы скорее скажем так интересные ситуации с балансировкой и интересные ситуации с балансировкой других стримов тем не менее архитектура справилась достаточно хорошо что в итоге архитектура нас полностью устраивает и могу ее смело рекомендовать hls останется у нас как минимум основным протоколом для веб-сайта и как минимум запасным протоколом для всего основного возможно мы перейдем на им пик дэш отказались от rtmp отказались это темп и несмотря на то что он дает мир меньшую задержку мы решили что лучше будем тюнить hls и возможно рассмотрим другие средства доставки в том числе дэш как альтернативу ну будем улучшать входящую балансировку здесь понятно хотим сделать бесшовный fell over для входящей балансировки то есть случае проблем на одном из медиа-сервер офф для клиента переключения на другой происходило незаметно совсем ну и возможно разработаем средство доставки от с серверов до клиента ну скорее всего это будет какой-то ю т п какой именно мы сейчас думаем и находимся в стадии исследования собственно всем всем спасибо опросы спасибо часть доклад маленький только что вы за столько-то выступал спикер от одноклассников и они рассказывали что они не пришлось переписать стример пришлось переписать encrypt пришлось энкодер переписать делаете ли вы такое решение или вы используете к и стоковые решение которых на рынке типа гармоника и так далее хороший вопрос на самом деле сейчас у нас крутятся сторонние решение из решений которые мы использовали open source у нас был инженер с модулем rtmp долгое время скажи он нас не от с одной стороны радовал потому что он работал и он достаточно простой мы очень много с ним бились чтобы он достаточно стабильно работал у нас сейчас есть завершается переезд gemix rtmp на собственное решение с транс кодером сейчас думаем соответственно приемник именно приемная часть тоже уже переписана снг нектар rtmp на свои решения добрый день и спасибо за доклад если позвольте я задам несколько вопросов я хотел задать про нарезку rtmp начал с но я так понимаю вы это уже ответили то что сын дженкс rtmp пересели на свои решения расскажу о свои решения этого спонсорстве мы рассматриваем возможность выпуска в понторез причем рассматриваем скорее мы хотим его выпустить скорее вопрос во времени и подготовки к выпуску в open source понятно что просто выложить в интернет исходники этого мало нужно подумать сделать примеры некоторые примеры deployment а а вы спрашиваете с какой вы хотите использовать этой целью да потому что я тоже сталкивался с enhance rtmp он очень мягко говоря не поддерживаться очень давно автор вроде особо даже не отвечает на вопрос она еще поэтому если хотите можете написать на почту не совсем ну скажем так отдать для своего использования думаю мы можем договориться потому что мы действительно планируем его использовать но слизал танцор сети нет уже вопрос по поводу вы говорили что можете сочилась наташ переехать в чем-то началась не устраивает или как это такой вопрос что можем мой может можем может нет на самом деле очень сильно зависит от того к чему мы придем в плане research альтернативных способов доставки то есть но у depay'a то есть если мы найдем что-то хорошее совсем хорошая то наверное не будем трогать hls если покажется что им пик дашь на соответственно более устраивает можете переедем кажется что это не очень сложно мы не уверена нужно или нет то есть синхронизация между стримами вы там явно лучше то есть между качествами и так далее то есть есть плюсы так что вот и следующий вопрос по поводу allure то вы говорили о том что если допустим стримы и стримы перестаньте стремиться то ну типа сразу это некий alert вы не ловили такое допустим что-то не зависите от вас там ними затем какой-то провайдера упал там ни цента какой-то мегафон упал и люди перестали стримить когда-нибудь скажем так что ты не зависит от нас это в основном всякие праздники и прочее вот да мы ловили но ну поймали до администраторы посмотрели что сегодня праздник что в остальные характеристики все в порядке и успокоились и про масштабирование то есть на каком уровне масштабируется то что вот я допустим с телефона запросил стрим мне сразу придет некая ссылка с правильным age сервера или да а сразу сразу придет ссылка и если надо вас переключит но если будут проблемы на конкретном сервере моего выглядит глючит кто мобильный player либо в плеер то есть запрос не стримили как ему придет новая ссылка куда он должен пойти за livestream он соответственно туда пойдет и перри запросит stream то есть и прости что так долго на каком уровне у вас каши и плейлисты и и чанки или такую или только плейлисты и чанки по-разному немножко кэшируются в плане времени в плане отдачи и так далее но к шуруем и то и другое и последний вопрос по поводу создания и шаров то есть у вас был такой то что вы наблюдаете допустим 2 миллиона зрителей на одну трансляцию in the mind что-то не успеваете и быстро к там не знаю поднимаете какой-то меж сервер пожалуй такого не было либо все заранее поднимаете запасом ну во-первых запас есть всегда небольшой или большой не тут неважным а во-вторых так не бывает чтобы трансляция внезапно стала сверхпопулярной нотами на орган начал там транслирует там я не знаю мы умеем неплохо предсказывать количество зрителей то есть в основном для того чтобы пришло совсем много людей мы должны приложить усилия соответственно мы можем регулировать количество зрителей на трансляции как мы эти усилия прикладываем спасибо большое за ответы я тут другой стране нет да вроде у меня два вопроса первое вы сказали что мерить инструментально стороны player задержки как бы очень просто у нас в чан как то есть медиа сегментах есть таймс темп в имеем менее сегмента в плеере мы просто его возвращаем то есть он там не в явном виде совсем возвращается в моей душе расспрашивали птс это раз нет второй вопрос помнится пробовали запускать пиринг выбор тисе почему отказались кого пробовали запретной выбор тисе в процессе что смысле опере протокол я понял доставка я вам не могу ответить на этот вопрос это происходило без меня то есть не могу сказать почему пробовали и к сожалению даже не могу сказать почему отказались большое спасибо за лекцию у меня есть два вопроса первый по поводу приемника и медиасервера насколько я понимаю раньше у вас там был engine ксор типа модулем сейчас и там и там у вас самописные решения то есть по факту это один медиасервер который проектирует другие медиа сильвера который уже там в кэш energy правильно ну так не совсем тогда как это сам офисное решение но оно разное в плане и медиасервера и приемника то есть инженер темпа это был некоторый комбайн который умелый ту это у нас сильно отличаются внутренности приемника и внутренней стиме медиасервера и по коду и по всему единственное что их объединяет это обработка rtmp и второй вопрос по поводу балансировки между ядрами каким образом это происходит мы имеем трафик мы отсылаем на каждый сервер немножко не понял вопрос сейчас объясню пользователь запрашивает через плеер плейлист ему возвращает относительные пути манифесты much on cam или абсолютные пути например там по и печеньку или там по домену который относительные пути относительно то есть не происходит балансировки в процессе просмотре стрима одним пользователем происходит достаточно хитро во первых мы можем использовать 301 redirect при перегрузке сервера то есть если мы видим что там все совсем плохо затем коми мы можем его ну за сегмента мы можем отправить его на другой сервер но это должно быть зашита владику player куда в логику player нет вот как раз вот эта часть не должна быть защита 301 redirect просто плеер должен уметь выходить по 301 ссылки то есть мы можем со стороны сервера в момент перегрузки отправить его на другой сервер смысле он спрашивает у серверы сервере модель это не очень хорошо поэтому в логику player зашиты перри получения ссылки на как раз случай отказа одного из серверов вот это уже в логике player не пробовали работать по не относительным абсолютным путям то есть при запросе к плееру производить какую-то магию выяснять где есть ресурсы где нет ресурсов и уже отдавать плейлисты на ну с указанием конкретно были лишь с указанием конкретного сервиса на самом деле это выглядит в том числе и рабочим решением то есть возможно как бы если бы вы пришли тогда мы бы взвесили это изобрели другое но текущие решение тоже рабочая поэтому так перескакивает с одного на другого не очень хочется и да это выглядит тоже рабочим решением боровы то есть именно до тоже может быть такой хорошо спасибо за ответы спасибо за доклад скажите с молча кастом вот как-то это дружат и через и так понимаю совсем не как в текущей реализации у нас в системе наверное ничего с multicast там в лайвах не дружит то есть у нас там не включается понятие мультикаста возможно где то в глубине админской магии внутри сети что то есть но это скрыто вообще от всего а значит и никому об этом не известно спасибо"
}