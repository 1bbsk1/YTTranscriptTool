{
  "video_id": "d5LSXOrHoKc",
  "channel": "HighLoadChannel",
  "title": "Эволюция репликации в Tarantool / Сергей Петренко (Tarantool, VK)",
  "views": 352,
  "duration": 2639,
  "published": "2024-04-17T01:10:25-07:00",
  "text": "на эту сцену выходит Сергей Петренко прошу аплодисменты Да всем привет Меня зовут Сергей Петренко последние пять лет Я работаю в таранту занимаюсь всем что связано с репликацией в частности участвовал в разработке нашего алгоритма синхронной репликации выборов лидера на основе рафт что вообще такое Тарантул это база данных с возможностью хранить данные в оперативной памяти но не беспокойтесь все данные все равно попадают на диск журнал упреждающие записи в частности он у нас используется для репликации когда я готовил этот доклад мне сказали что нужно объяснить что такое репликация я полез гуглить Выяснилось что это термин из биологии и вот мне очень понравилось цитата со статьей на Википедии про биологическую репликацию что это Ключевое событие в ходе деления клетки вот как ну для вас данных репликация тоже важна по сути это процесс создания точной копии данных но нам для сегодняшнего доклада важно знать что подходов к тому как сделать репликацию несколько достаточно сказать что есть синхронное есть асинхронная репликация и нету какого-то универсального подхода каждый подходит для своих задач О чем я сегодня вообще хочу вам рассказать Я хочу рассказать какой путь Прошли мы в Тарантул с репликацией с чего Мы начинали Где мы сейчас находимся Наш путь не был частью какого-то грандиозного плана который бы нас привел из точки А в точку б где мы захватываем мир скорее время от времени менялся фокус в зависимости от того где Тарантул хотели использовать в очередной момент времени и приходилось новые решения базировать на том что было сделано до сих пор иногда это получалось хорошо например вот как раз когда мы реализовывали синхронную репликацию поверх асинхронной Здесь был пару лет назад про это доклад Владислава шпилевого там как раз все прошло достаточно гладко и протокол хорошо подошёл и всё и опять же здесь же мы рассказывали про адаптацию нашего журнала к тому что от него хочет трафт И вот это уже было наоборот сделать достаточно сложно но мы начнем с того сегодня что я В общих чертах расскажу как репликация в Тарантул работает а затем мы погрузимся в историю начнем примерно 8-10 лет назад Точно не скажу с версии Тарантул 1 5 и дойдем до сегодняшних дней А под конец Я немножко приоткрою завесу над чем мы работаем сейчас что хотим сделать дальше Но сперва чуть-чуть про архитектуру важно заметить что в тарантуле репликации шардировании реализованы Независимо репликация это механизм встроенный в ядро а шардирование работает с помощью внешнего модуля который называется кластер обычно состоит из множества шардов их могут быть десятки или сотни и в каждом шарде есть свой мастер своей реплики все узлы одного шарда мы называем реплика сет То есть если вот на картинке посмотреть это выглядит примерно так извините 100 шардов я не нарисовал только два они в кружочках справа и клиент общаясь с кластером обращается к роутеру А роутер уже знает на Какой конкретно Sharp нужно пойти чтобы получить или записать необходимые данные Ну и вот в каждом шарди есть мастер какие-то реплики Вот так это все работает мы с вами будем сегодня говорить про то что происходит в ночь 3 этого маленького кружочка про Один шар или репликация начнем с настройки репликации во-первых в конфигурации каждого узла можно указать Какие ноды для него будут источниками данных Можно даже самого себя указать это важно при начальной настройке ну самое простое Что можно сделать это сделать один Мастер и несколько реплик на каждой из реплик указать мастера как источник данных на Мастере либо самого себя указать либо никого не указывать но можно несколько источников данных указать самая популярная и то что мы всегда рекомендуем это топология Full mesh Когда у вас каждая реплика видит всех остальных участников репликасета может от всех получать данные вообще узло для того чтобы начать работать необходим начальный снапшот его можно либо сгенерировать самостоятельно заполнить какими-то данными по умолчанию либо получить от кого-то у кого этот снапшот уже есть по сети у нас снапшот содержит первый снапшот содержит различные предзаполненные системные таблицы список всех пользователей список их прав список системных таблиц и так далее и так далее процедура Вот это получение или создания начального снапшота называется bootstrap и когда реплика сети ни у кого еще нету начального снапшота узлы между собой собираются выбирают каким-то образом лидеры будут страпа которые для себя снапшот генерирует самостоятельно а всем остальным его раздает но выбирают они не так чтобы очень надежны они не общаются друг с другом когда это делают каждый Просто смотрит на свой список источников репликации и кого-то из этого списка выбирает по определенному алгоритму тот кто выбрал себя загружается на счет сам все остальные подключаются к тем кого выбрали Но несмотря на то что такое не очень надежный алгоритм используется нам важно чтобы Лидер бутсапа был только один потому что он присваивает всем участникам кластера уникальной идентификаторы и должен это делать не противоречивым вот чтобы Лидер страпа получался только один мы используем следующий алгоритм во-первых каждый узел себе генерирует на старте а затем проходится по всем источникам репликации в списке и каждому присваивает очки Ну и выбирает мастера с наибольшим количеством очков очки даются за то что мастер может писать за то что мастер сам уже прошел bootstrap например ну и за другие вещи которые по нашему мнению важны для лидера буцрапа Если вдруг получится что два узла набрали одинаковое количество узлов очков реплика не знает что делать то она становится на том чей ID меньше Таким образом мы добиваемся что вот Даже несмотря на то что реплики друг с другом не общаются и выбирают Независимо у них больше шансов сойтись на том что они выбрали кого-то одного я уже упоминал что мы используем журнал Он нам нужен для восстановления после сбоев сбоев во-первых а во-вторых для репликации и журнал использует тот же бинарный формат что и клиентские запросы и поток репликации тоже использует тот же самый бинарный формат то есть с точки зрения реплики поток репликации выглядит точно так же как поток запросов от клиента Ну или можно сказать как восстановление из журнала другой ноды и в целях репликации мастер хранит журнал до тех пор пока все реплики не подтвердят его получение но теперь углубимся в историю Я хочу начать с версии 1.5 как я уже сказал это было порядка 8 лет назад и на тот момент Я если честно в Тарантул еще не работал Я учился в школе и думал что буду делать игры но что-то пошло не так и вот мы с вами теперь собрались здесь кое-что я про Тарантул 1 5 тем не менее узнал и сейчас с вами поделюсь во-первых там можно было назначить единственного мастера в конфигурации или источник данных соответственно топологию можно было собрать только звезда или каскад реплик можно было сделать сколько угодно и если вдруг необходим файловер то нужно было зайти на каждую реплику и сообщить ей адрес нового мастера что довольно таки неудобно поскольку у нас бай-дизайн один единственный мастер то и Прогресс репликации в то время можно было отслеживать просто количеством выполненных операций за все время существования репликассета это число называется лсн или локсикландр но тем не менее можно было по ошибке сделать себе двух мастеров в таком случае консистентность была бы навсегда потеряна каждый из мастеров бы занял один и тот же диапазон ЛСР своими данными и затем при синхронизации Они бы просто посмотрели на то что их ЛСР совпадают и счастливо пошли дальше Следующий большой вехой стало появление мастер-мастера репликации случилось это версия 1.6 и вот дальнейшим развитием этого алгоритма мастер мы занимались примерно до версии 24 это там ну то что началось 7 лет назад и закончилось года три назад вплоть до 2020 года зачем вообще вдруг Понадобился мастер где-то во времена 1-5 разработка пришла к эксплуатации чтобы узнать что же с тарантулом который естественно идеален не так может быть можно что-то улучшить и Выяснилось что эксплуатации не хватает удобного файловера но говорили они об этом довольно странными словами по крайней мере услышали их так что вот если мастер упал Мы хотим писать на реплику Ну пожалуйста Бойся своих желаний можете теперь вообще во все реплики писать поскольку теперь у нас бай-дизайн несколько мастеров уже Прогресс репликации нужно отслеживать с помощью vaclock а не просто лсена влог это у нас массив ЛС фактически предпочтительной топологии становится фулмишь То есть когда все подключаются ко всем и теперь уже нужно чтобы каждый узел получил свой уникальный ID то есть вместо влоги фактически и появляется процедура bootstrap для того чтобы раздать эти уникальные айтишники узла Ну да как я уже сказал здесь Мы готовы к тому что два узла будут писать одновременно изменения каждого из них со временем попадут на другой узел поскольку мы теперь раздаем какой-то ограниченный ресурс место логично Что появилось ограничение на то сколько этих мест Локи мы можем раздать появилось ограничение на то что всего репликасете может быть 31 узел конкретно то что их может быть 31 связано с тем что там используется оптимизация с битовыми масками Но даже если бы этой оптимизации не было сильно больше 30 узлов добавить бы не получилось Дело в том что у нас довольно часто эти вот клоки пересылаются и если вам вот по какой-то причине нужно 30 или больше пишущих нот то фактически довольно часто нужно пересылать 30 чисел из вока что уже становится дорого Ну а для тех кому не нужно писать на 30 нодах нужно только читать появляются так называемые анонимные реплики у них нет идентификатора они не могут ничего писать Зато с них можно читать их можно сделать по-прежнему сколько угодно еще одно нововведение того времени это локальные таблицы то есть таблицы изменения с которых не попадают на другие реплики в каждой реплики своя Копия этой таблицы со своими данными и реализованы они так теперь у нас каждая реплика владеет не одним лсн а двумя одним Она считает глобальные операции глобальных таблицах а другим изменения в локальных таблицах в vaclock естественно реплика показывает только свой глобальный лсн но также чисто чтобы технически нам упростить задачу локальные свой лсн она тоже складывает в воглок в нулевую компоненту и это нулевая компонента We Clock она носит чисто технический характер на каждой реплике Она своя То есть если вот мы посмотрим на три реплики Они полностью синхронизированы Но несмотря на это у них в нулевой компоненте у каждой какое-то свое количество выполненных операций над локальными таблицами ну человек который начинает использовать Мастер и мастер репликацию должен быть готов к появлению конфликтов что вообще это такое если у нас два мастера одновременно пишут по одному ключу то при синхронизации мы либо получим конфликт если это были инферты либо если это были например реплейсы мы получим тихую перезапись данных без каких-либо ошибок Вот например первая нода Сказала Фу равна 1 вторая сказала фуравно 2 и после репликации эти операции в их журнале окажутся в разном порядке ну и соответственно первая нода теперь будет думать что фуравно 2 А вторая что фу равно 1 но при этом с точки зрения веклока Они полностью синхронизированы обе выполнили две операции вот а тем не менее данные на них разные и если вот такое уже произошло то починить это можно только еще одной вставкой например первая нода может сказать фура внутри со временем по репликации эта запись попадет на всех и кластер придет в консистентное состояние но лучше конечно не решать такие проблемы постфактум а предупреждать их для того чтобы избежать конфликтов Можно например применять коммутативные операции если мы не говорим фуравно один фу равно 2 говорим фуравно фу плюс 1 то понятно что после двух операций В каком бы порядке они не произошли фу будет равно 2 Чего нам хочется добиться Но если коммутативные операции не получается применять то на выручку приходит стратегия разрешения конфликтов такая например как lastraight wins когда из двух конфликтующих версий данных мы оставляем ту у которой более свежий Times Temp но тут можно дать фантазии волю и придумать сколько угодно таких стратегий в зависимости от вашей задачи основная идея в том что мы дописываем какую-то специальную этой информацию в данные так чтобы узел при получении конфликта на основании этой одной информации смог консистентно между каждый узел смог консистентно выбрать одну и ту же версию данных Ну это я так говорил про общие последствия мастер-мастера теперь стоит рассказать Какие проблемы мастер принес конкретных тарантуле основная такая и достоинство и недостаток в одном лице это фул-местопология с одной стороны хорошо что записи распространяются от всех ко всем даже если реплика вдруг перестала видеть мастера она получит изменения не напрямую А через какую-то другую реплику Но с другой стороны мы получаем дублирующую пересылку данных например вот если у нас есть три узла даже всего один из них Мастер и все соединены сперва свою операцию мастер разошлёт обеим репликам а затем каждый из репликает уже операцию пошлёт другой потому что не может быть уверены что вторая реплика эту операцию Уже получила Ну и чем больше узлов мы Соединяем Full меж тем больше эта проблема усугубляется но у всех наших пользователей типичные размер репликассета это не больше пяти реплик поэтому особой проблемы Для нас это пока что не доставляет Следующий большой вехой здесь я наконец уже работал в тарантуле а то я вам какие-то выдержки из истории читаю стало появление рафт мы снова обратились к эксплуатации еще раз спросили чего же они хотят и на этот раз Либо они стали лучше формулировать свои мысли Либо мы лучше их понимать но мы услышали что им все-таки нужен автоматический фейловер и какие-то повышенные гарантии консистентности по сравнению с тем что Тарантул давал до сих пор и вот упор был сделан именно на рафт про это уже очень много было рассказано в наших прошлых докладах вот некоторые из них можно будет в конце презентацию скачать и на эти ссылки покликать если интересно А сейчас я подробнее на рафте синхронной репликации уже Останавливаться не буду Ну вот несмотря на такую смену парадигмы на появление рафта предпочтительно этапология у нас все еще фул-меж все еще доступно асинхронная репликация При желании все еще доступен мастер то есть мы вообще старались изменения касающиеся репликации всегда делать так чтобы то что уже работает продолжала работать а у пользователя просто появлялись какие-то новые возможности и вот последнее что мы на данный момент сделали такой вишенку на торт консистентности положили это добавили уровень изоляции линеарайзебу его еще называют линия резуемый чтения это когда вы не только Пишите по кворуму но и читаете тоже по кворуму это вам позволяет всегда читать самую актуальную версию данных последнюю подтвержденную Ну и такой бонус что читать самые актуальные данные Можно не только с мастера но и реплики она там пообщается с курумом реплика Они между собой договорятся что вам отдавать и отдадут После этого у нас освободились силы для того чтобы уже поработать над удобством страпа и реконфигурации Но прежде чем я расскажу что мы там улучшили надо понимать что же там было улучшать очень долгое время Тарантул славился я не назову это наверное проблемами скорее правильно сказать особенностями для которых не было никаких удобных ручек чтобы эти особенности преодолевать первое то что при восстановлении после длительного простоя нельзя сразу начинать писать Нужно сперва синхронизироваться с остальными иначе вот допустим у вас есть какой-то Старый мастер в какой-то момент он погибает срабатывает фейловер еще два дня работает новый мастер тут вдруг старый мастер вернулся у него в конфиге все еще написано что он мастер Ну вот почему ты так случилось и он не глядя на то что произошло вокруг вообще игнорирует последние два дня изменений начинает принимать запросы на записи от клиента с этим нужно что-то делать и другая проблема которая касается вот этого не очень надежного алгоритма что прямо гарантировать что все выберут одного и того же лидера мы можем только если топологии и только если убедимся что каждая реплика увидела всех остальных перед тем как принимать решение иначе там кто-то один не увидел предпочтительного лидера выбрал себя а все остальные выбрали этого предпочтительного лидера и кластер уже не забудь трапецию нужно будет пытаться заново в свое время обе эти проблемы решались с помощью такой настройки которая называется replication Connect corum и по удобству напоминает вот такое вот приспособление то есть работает но не очень удобно идея Там здравая абсолютно Первое это то что во время бутстрапа мы выходим с ошибкой если не набрали кворум подключений даже не пытаемся никого выбирать если мы не видим всех и второе во время нормальной работы независимо от того что написано в конфигурации мы обязательно остаемся в ритуале до тех пор пока не синхронизируемся с форумом и значение по умолчанию здесь продиктовано нуждами boostrap это все в списке мастеров то есть N что же с этой настройкой не так плохое значение по умолчанию во-первых то есть для бутстрапа нам нужно увидеть всех но уже При нормальной работе Если мы поднимаем новый узел нет никаких гарантий что все прикасети работают если вам вдруг срочно понадобилось либо реконфигурировать либо ввести новые тачку когда кто-то еще реплика сети лежит Вы вынуждены понижать Коннект иначе до тех пор пока вот Этот второй лежит ваша нода будет то есть Ну и вторая проблема то что конфигурации используется одно и то же значение Хотя нужды совершенно разные это приводит к тому что пользователи особо не парились просто выкручивали это значение в ноль или в единицу просто чтобы оно им больше никогда не мешало и никогда не становилось источником их проблем но при такой настройке это ручка абсолютно ни от чего не защищает то есть мы там допустим увидели сами себя или увидели 0 синхронизированных подключений и радостно пошли работать в вот наконец мы занялись этой опцией Мы решили версии 2.11 её за диприкатить и вместо неё придумали новое поведение при конфигурации и три новых режима будут страпа но сначала про поведение конфигурации Теперь мы вместо какого-либо заранее заданного кворума синхронизируемся со всеми кому смогли подключиться и пытаемся набрать максимум подключений в течение определенного таймаута только после этого начинаем синхронизироваться то есть с одной стороны у нас нет риска застрять застрять в рингле на бесконечно долгий промежуток времени потому что вот мы подолбились многие которые они доступны какое-то время перестали синхронизируемся с остальными Но с другой стороны у пользователей больше нет этой ручки которую можно опустить до нуля и мы в любом случае пытаемся синхронизироваться со всеми для кого смогли Достучаться такой Мне кажется довольно удачный баланс теперь про три режима bootstrap какой же выбрать Но не переживайте все не так сложно как у этого молодого человека на картинке можно выбрать любой первый называется авто это режим по умолчанию и он наиболее похож на то что у нас было до сих пор единственные чем этот режим отличается от старого это то что на узлах появились дополнительные проверки во-первых тот кто сам себя номинировал на то чтобы быть лидером теперь все-таки спрашивает остальных они с ним согласны они тоже его номинировали или нет и при добавлении новой реплики уже существующие кластер это реплика тоже отвалится если по какой-то причине у нее в конфигурации указаны не все в этом классе то есть мы должны убедиться что она точно попыталась подключиться ко всем точно выбрала лидера который ее добавит в кластерные лучшим способом из возможных особенности этого режима как и старого режима в том что при бутсапе узлы нужно запускать плюс-минус одновременно то есть с момента когда вы запустили первый узел до момента когда запустили последнее должно пройти Не больше чем определенный тайм-аут который можно сконфигурировать если этого правила не придерживаться то там уже как повезет может быть все хорошо все Увидели какую-то одну общую точку и будут стопятся с неё А может все быть не очень хорошо и получится то же самое что со старым режимом и когда мы разрабатывали этот режим Мы думали про случаи когда пользователь просто вот запускает несколько узлов ему нужно чтобы все само работало он не хочет лезть ни в какие настройки он хочет чтобы оно как-то запустилось и поехал второй режим называется config здесь Люди работают с трапом явно указываем в конфигурации каждого узла никаких дополнительных проверок теперь нам не нужно Full mession Нам тоже не нужен достаточно увидеть одного единственного лидера bootstrap и поехали это наиболее удобно когда мы запускаем большое количество узлов уже нет никакой необходимости делать это одновременно то есть главное чтобы лидеры будут стропа запустили А все остальные когда к нему подключится уже никакого значения не имеет хоть через день хоть через час как угодно и когда мы разрабатывали этот режим Мы думали про системы deployment типа ansible ну и наконец последний режим супервайст здесь уже никто не выбирает лидера в конфиге мы лидеры не указываем вместо этого лидером становится тот кого назначат оператор это удобно когда вы например запускаете какое-то количество узлов но у вас там не все с первого раза запускаются по каким-то независящим от вас причинам нам хочется чтобы просто тот кто выжил и был лидером то есть заранее в конфигурации это прописать не можете это все решается по факту Ну еще одно достоинство этого режима что узел который будет регистрировать новые реплики уже работающем кластере можно менять тоже не залезая в конфигурацию То есть если у вас есть какой-то фейловер либо наш файловер либо какой-то Ваш самописный Вы просто лишним шагом в момент назначения нового мастера назначайте этот же узел лидером для всех будет приходить в кластер Ну и здесь мы думали про cubernetis когда дизайн или этот режим Но то что здесь мы думали про губернатиса в режиме конфиг пранси был не значит что эти режимы нельзя использовать как угодно еще то есть хоть вручную хоть с помощью каких-то ваших тулов которые у вас написаны все три режима можно использовать Они прекрасно работают Ну в таком образе истории Мы дошли до сегодняшних дней и теперь давайте посмотрим о чем же мы думаем что мы хотим сделать дальше Я уже упоминал что типичный кластер Тарантула это десятки или сотни шардов Они же репликассеты в нашей терминологии и все о чем мы говорили в этом докладе работает в пределах одного единственного шарда то есть между кассетами уже общение происходит с помощью внешнего модуля но часто при шортировании есть все-таки какая-то небольшая часть данных которую мы хотим распространить прямо скопировав на все шарды такие данные обычно называют словарями и вот у нас есть идея что эти словари можно реализовать с помощью репликации опять же нам никто в принципе не запрещает заставить две реплики из разных шардов с коннектиться и получать друг от друга какие-то данные ровно по тому же протоколу по которому Они получают репликацию Можно даже переиспользовать этот подход с двумя л сценами которые у нас появился для локальных таблиц только теперь уже расширить его Клоков каждый репликация трекает Прогресс репликации своим локальным клоком Там они договариваются с помощью этого клока кто Какие данные должен получить глобально во всем кластере Существует еще общего клок с помощью которого реализована репликация словарей проблема только с топологией я уже говорил что в репликасете у нас не больше пяти реплик типичная и соединить их фул-мечник какой проблемы не составляет А вот в кластере сотни репликасетов и даже если мы не станем все узлы соединять со всеми а просто какого-то одного представителя каждого репликассета попытаемся Соединить с другими представителями других сетов это все равно меньше уже нерешаемая задача и собственно сейчас мы думаем какой же стратегию подключения между фибрикосетами можно было бы использовать Итак что мы с вами успели посмотреть началась все с того что порядка 8 лет назад у нас можно было назначить только одного лидера делалось это через конфигурацию топология была только звезда если Лидер умер надо назначить нового нужно было зайти в конфиг каждого из узлов и переназначить там этого лидера дальше мы в попытках упростить фэйловер пришли к тому что теперь у нас все могут писать не обязательно им этого делать они могут и все соединены со всеми дальнейшее развитие нас привело к тому что теперь у нас Лидер назначается автоматически и при этом используется синхронная репликация Ну и конечно до сих пор можно использовать любой из этих трех подходов При желании и сейчас как бы уже закончились подходы которые бы мы не реализовали и поэтому нам ничего не остается кроме как работать над удобством того что мы придумали а на этом У меня все спасибо за внимание слайды можно посмотреть по ссылке При желании Спасибо тебе огромное очень интересно ну что друзья у нас практически полный зал Я предполагаю что будет много вопросов У меня к вам большая просьба Давайте задавать по одному вопросу но самому интересному хорошо Итак если вы смотрите онлайн вопросы можно задавать в чате я их буду зачитывать если вы здесь в зале поднимать руки Helper будут давать микрофоны Итак поднимайте руки Кто хочет задать вопросы вижу две руки пожалуйста Добрый день компания Билайн Расскажите как можно повысить отказоустойчивость в случае с несколькими цодами Можно ли размещать мастер реплики на разных цодах как при этом будет синхронно или асинхронные репликации лучше делать ваше мнение повысить отказоустойчивость то есть мы мы за что боремся за доступность или за консистентность можно в обе стороны копать вы не до этого хотели А это и то хотелось бы ну наверное рафт для этого вполне подходит то есть у вас сколько два цода всего да три Но если 30 то точно надо брать рафт если с одним из содов что-то случится два оставшихся спокойно там смогут выбрать другого лидера синхронная репликация вам гарантирует что никогда не случится такого Что старый Лидер что-то закоммитил А другие этого не видят у вас будет одна Линейная история всех изменений там от начала времен и рафт это то что вам нужно в трёх содах единственное придется платить повышенными задержками на запись нужно дождаться пока из главного сода хотя бы в один из содов данное доедут супер спасибо Следующий вопрос пожалуйста Да спасибо за презентацию Я хотел спросить про алгоритмы консенсуса Это насколько я понимаю трейдов по скорости как раз вот сейчас ты упоминал то что это будет есть какие-то примерные оценки насколько это трудозатратнее сколько времени на это уходит и прочее Спасибо но получается мы платим лейтонси на запись при синхронной репликации то есть Отличие в том что сосинхронные репликации сразу после попадания на диск мастера мы отвечаем клиенту А здесь сначала Все Попало на диск мастера затем произошла репликация до форума реплик Ну в самом простом случае до одной реплики затем реплика эта вернула ответ только после этого мы ответили клиенту То есть у нас добавляется Лагер репликации между мастером репликой если там он какой-то есть то есть если реплика вообще отстаёт сильно то мастер очень долго не сможет коммитить синхронные транзакции Ну а так добавляется просто пинга от мастера реплики в лучшем случае Можно пожалуйста попросить говорить Микрофон чтобы в трансляции тоже слышно было прям временных каких-то оценок не проводили Ну посмотреть что-нибудь плохо жить со всем отставать будет Ну если реплика будет совсем отставать то мастер будет просто откатывать транзакции потому что он в течение какого-то тайм-аута по умолчанию он у нас 5 секунд так и не дождался чтобы она ему подтвердила эту последнюю запись вот если она не отстает ну не настолько сильно по крайней мере то просто он будет ждать пока она не ответит и будет подтверждать транзакцию после этого конкретных оценок нет потому что смотря Где ваша реплика и где мастер То есть если они в одном соде то наверное это ну прям минимальный какой-то имеет Спасибо Итак если у нас еще вопросы вижу руку пожалуйста Добрый день спасибо за доклад подскажите пожалуйста не планируйте использовать информацию о географическом расположении при расчёте форума чтобы допустим Не было такого что у нас все инстанции кубер зашатулил в одинцот или в одну стойку и она не считалось как троекратное я понял чтобы можно было кворум назначить грубо говоря по локации узла они просто по количеству узлов пока что про это не думали супервижу еще одну руку пока хелпер несет микрофон напоминаю обязательно переходите по ссылочке оценивайте доклад qr-код здесь на слайде Не забывайте это правда важно Добрый день меня зовут Максим Сбер МегаМаркет у вас в официальной документации на сайте написано что для добавления инстанса в кластер нужен ребут всех нот кластера но планируете ли горячие Ну расширение кластера как власти например Спасибо для добавления инстансов кластер нужен ребут всех нот правильно я услышал надо проверить документацию я тут уже довольно долго никогда такого не было но за последние пять лет я имею ввиду надо смотреть что документации написано это точно не так супер спасибо огромное есть Спасибо большое за доклад поправьте меня если я неправильно понял в ранних версиях у нас участники кластера определяли статический конфиге и есть ли причины использования этого способа вместо какого-нибудь сервис Discovery Ну по большому счету у нас до сих пор кто будет участником репликасета определяется в конфиге И даже если Один узел видит соседей он об этих соседях не скажет другим своим пером то есть Каждый должен сам прийти к тем кто у него сконфигурирован какой-то Discovery других узлов у нас часто реализуют уже поверх мы еще и Application сервер на луа помимо базы данных и вот налого у нас есть модуль Swim который позволяет обнаруживать другие узлы там которые которых кто-то видит Вот надеюсь ответил Так ну что остались еще остались вопросы отлично Мне так нравится когда аудитория живая интересующийся спрашивают классные конструктивные вопросы греет душу Спасибо большое за доклад Сергея Алимов Триколор наверняка для соблюдения персистентности в тарантуле используется снапшоты стандартные вот в качестве во время выполнения этих снапшотов каким-то образом механизм изменяется механизмы репликации Может быть там замирает что-то или меняется какая-то мастернода нет ничего не меняется во-первых снапшот выполняется отдельным трудом который получает консистентный review на какой-то момент на определенный wacklock и дальше просто из этого review сейчас сбрасывает на диск репликация при этом никак не аффектится потому что мы в любом случае храним файлы журнала до тех пор пока реплики не подтвердят их получение и фактически У нас есть один журнал до момента снапшота и в момент снапшота мы начинаем писать уже новый журнал после снапшота реплики просто дочитывают старые журналы переключается на новые для них это прозрачно еще небольшой вопросик может быть знаете вот качественная очень количественное Сколько длится примерно снапшот условно там одного терабайта данных если реплика занимает нет тут не подскажу Но со мной здесь есть коллеги мы можем все вместе пообщаться после доклада уже в кулуарах Спасибо Да у нас есть вопросы с чата Михаил Васюков продолжение Про Сервис Discovery Когда будет реализован Discovery для мониторинга настраивать руками десятки метрик для каждого узла кластера например трудоемко вот тут К сожалению ничего не могу сказать валят Спасибо супер еще рука пожалуйста скажите Сейчас используется режим конфликтов Почему был выбран именно он и вроде бы используются всякие Локи То есть можно было бы разрушения конфликтов и добавить вот Да хороший вопрос Спасибо нет вас тратим по умолчанию не используется по умолчанию не используется Ничего если у вас мастер то два реплейса приведут вот к той ситуации что два конкурирующих реплейса приведут к ситуации что на одной ноге будет фу равно 1 на другой фу равно 2 а все вот эти механизмы разрешения конфликта они отданы на откуп пользователя есть триггеры которые выполняются при В общем каждой вставке и эти триггеры могут модифицировать вставляемые данные с помощью этих триггеров любую стратегию можно реализовать То есть вы во-первых самостоятельно пишете Триггер который дописывает таймс Темп Если вы хотите реализовать реализовать А во-вторых Вы самостоятельно пишете Триггер который затем пытаемся темпом из двух записей выбирает ту которую оставить например ну или вы можете использовать конечно же или любую другую стратегию разрешения конфликтов которая больше подойдет конкретно вот в вашем случае спасибо Итак у нас был вопрос из той части зала прошу Здравствуйте а можно рассказать как будет работать консенсус на чтение как раз в момент фойловера когда идет на определение концепцию для мастернода для новой то есть там везде написано было презентации что надо доступно на Red Only естественно получаем ситуацию когда по идее нам нужно быть читать консенсусом а это надо соответственно ещё не Мастер и сама по себе концепцию не обрела как это будет всё в целом работать ну то что надо Не Мастер и консенсус не обрела это не проблема у нас же вообще в момент переключения лидера новая запись прекращается и мы видим такой фиксированное состояние всех но да первое надо например видела 5 операций вторая четыре операции Давайте их лепестки будет всего три ноды третье оно не знаю видела две операции тогда мы точно знаем что последние данные имели шанс закамиться это вот четвертая операция потому что хотя бы по 4 операции есть на двух узлах из трех и для чтения мы можем использовать эту же информацию Мы знаем что чтобы вернуть правильный ответ пользователю консистентно прочитать мы должны у себя локально дочитать все вплоть до четвертой операции она должна в нашем журнале присутствовать если стало понятно то хорошо Если нет Спросите еще пожалуйста Да мы можем продолжить Потом взять в куаро есть вопросы чата Дмитрий ангелович если оценки скорости репликации по сравнению с аналогичным решениями например Манго тебе лучше тоже в кулуарах кулуара должны быть жаркими Ну что есть еще какие-то вопросы которые хотелось бы задать здесь нет отлично Тогда вопрос к тебе и наверное самый сложный все вопросы были классные очень многие сложные какой выберем Давайте два мне во-первых понравился вопрос про географическое определение форума вот в той части молодой человек уже ушел который задавал вопрос На первом ряду а говорят могут передать тогда вас супер Спасибо огромное вам за помощь и тебе огромное спасибо за очень интересное выступление было много классных конструктивных вопросов а небольшой подарок от он типа уже тебе как спикер Спасибо огромное за подготовку Такого замечательного доклада Аплодисменты спикеру пожалуйста"
}