{
  "video_id": "MTp2Z7RXLMY",
  "channel": "HighLoadChannel",
  "title": "Оптимизация стоимости хранения данных в объектном хранилище / Виталий Исаев (МойОфис)",
  "views": 757,
  "duration": 2935,
  "published": "2023-04-28T06:10:46-07:00",
  "text": "друзья Всем привет Меня зовут Виталий я работаю в конт разработчиком в компании мой офис и сегодня мы поговорим с вами о конфигурации и о деньгах Я думаю что каждый из нас сталкивался с необходимостью настройки какого-то сложного софта который интенсивно потребляет аппаратные ресурсы компьютера как правило у такого софта сложный конфиг и бывает достаточно трудно найти ту комбинацию параметров при которой этот софт демонстрирует хорошую производительность и при этом потребляет минимальное количество железа одно из наиболее ресурсоемких категорий софта на сегодняшний день являются системы хранения данных сюда я отношу как классические СУБД так и хранилища различного назначения Сегодня я расскажу о том как мы искали оптимальный конфиг для нашего внутреннего объектного хранилища и какие уроки Мы извлекли из этого поиска для того чтобы развернуть исходя нам нужны естественно диски различных типов нам нужна оперативная память центральные процессор какие-то другие И если мы просуммируем стоимость каждого из этих ресурсов мы получим итоговую стоимость хранения данных ее я в этом докладе буду обозначать с помощью буквы C естественно у каждого ресурса свой рынок своя цена своя динамика цены например диски постоянно дешевеют но SSD диски делают это быстрее чем HDD диски и в результате ожидается что примерно через четыре года стоимость быстрых и медленных дисков сравняется что конечно изменит ландшафт для систем хранения данных то же самое можно сказать в отношении оперативной памяти она постоянно дешевеет но в последние 15 лет скорость изменения цены значительно снизилась как считается из-за тех же проблем что вызывают нарушение закона мура на рынке центральных процессоров тем не менее как минимум до 2016 года стоимость вычислений снижалась практически монотонно мы можем констатировать что рынок железа очень динамичен где-то цены падают где-то они стагнируют но в целом ситуация постоянно изменяется и нам определенно стоит это учитывать при проектировании своих систем для хранения данных если опросить экспертов по skd то окажется что на сегодняшний день доминируют два подхода к проектированию софта такого класса Первый заключается в экономии индийского пространства любой ценой то есть мы хотим запихнуть как можно больше клиентских данных в диске наименьшего объема второй и возможно более популярный подход заключается в том что Центральный процессор объявляется главной полностью объявляется самым дорогим ресурсом а оперативная память и диски приносятся ему в жертву вот такие два подхода но как мне кажется ни один из них не обладает достаточной гибкостью для реалий сегодняшнего дня ведь в самом деле нам нужно не просто сэкономить центральные процессор или диски нам нужно прийти к оптимальной стоимости эксплуатации софта а достигается она только в том случае если мы нашли оптимальный баланс аппаратных ресурсов в какой-то ситуации можем сэкономить больше CPU в какой-то мы экономим диски Но в любом случае наши действия направлены на то чтобы найти ту комбинацию аппаратных ресурсов которая минимально по стоимости чтобы это случилось нам нужны два слагаемых во-первых мы должны писать софт который умеет конвертировать на программном уровне за счет определенных алгоритмов потребление одних ресурсов потребление других ресурсов Ну и конечно нам нужно уметь искать оптимальную конфигурацию то есть тот набор нашего софта при которой достигается оптимальный баланс аппаратных ресурсов сейчас мы сосредоточимся на этих двух пунктах вот этот подход который мы условно называем баланс сориентированным мы применяем при разработке корпоративной почтовой системы maillion она рассчитана на одновременную работу до 1 миллиона пользователей и это накладывает существенные требования на ее производительность и экономическую эффективность еще на этапе дизайна Мы заметили что данные которые обычно циркулируют в электронной почте они характеризуются тремя важными свойствами во-первых большинство писем состоят из данных текстовых форматов благодаря чему они хорошо компрессируются во-вторых в корпоративной среде часто встречаются дубликаты писем либо полные либо частичные ну и наконец письма и мутабельны то есть письмо которое Однажды было отправлено или получено оно уже никогда не изменится и мы поняли что оптимизации нацелены именно на эти свойства наших данных позволят нам сэкономить существенные объемы аппаратных ресурсов и написали свое собственное распределенный объектное хранилище которое назвали диспёрстобжек Store там на уровне ядра мы поддержали алгоритмы компрессии И дедупликации сейчас мы эти алгоритмы рассмотрим Чуть более подробно именно в контексте поиска оптимального баланса между аппаратными ресурсами начнем с компрессии мы написали свои бенчмарки чтобы разобраться Какие алгоритмы компрессии подходят лучше всего для наших данных Ну как Мы все знаем качество алгоритмов компрессии измеряется с помощью коэффициента сжатия и скорость сжатия то есть пропускной способности алгоритма и на этих графиках а пространство координат сформулирована именно этими двумя метриками А каждая точка Это результат работы какого-то конкретного алгоритма также Мы специально разбили сжимаемые данные на 6 размерных групп и вот что получилось на этих графиках видны две очень важные особенности которые характерны для всех алгоритмов компрессии Ну во-первых они очень плохо сжимают маленькие данные Это хорошо видно на первых двухразмерных группах где почти все точки попали в Красную зону то есть туда где коэффициент компрессии меньше единицы где алгоритмы компрессии работают себе в убыток А во-вторых чем крупнее становится сжимаемые данные тем теснее выражено обратная пропорциональность между скоростью сжатия и коэффициентом сжатия то есть алгоритм сжатия не может быть одновременно хорошо сжимающим и быстро работающим что в точности соответствует трейдов между затратами на дисковое пространство и затратами на цпу если мы хотим сэкономить больше диска мы больше CPU и наоборот ключевыми настройками которые определяют эффективность компрессии являются естественно тип алгоритма уровень сжатия у тех алгоритмов где он может быть задан и предельный размер объекта начиная с которого мы начинаем выполнять сжатие то что самые маленькие объекты сжимать неэффективно Как мы увидели регулируя вот эти настройки мы можем балансировать затраты на дисковое пространство и затраты на CPU а где дупликации можно подойти с двух сторон представим что у нас есть какой-то клиент он хочет сохранить у нас объект потом приходит еще один клиент со своим объектом еще один а вот четвертый клиент приносит тот Объект который уже когда-то встречался в нашей системе из пятым клиентом происходит то же самое ну как я уже сказал наличие дубликатов корпоративной переписке это абсолютно нормальное явление как же должно повести себя объектное хранилище рассчитанное на такой паттерн применение на самом деле все очень просто оно должно Просто уникализировать каждый из объектов и сохранить данные каждого объекта в одном единственном экземпляре так же каждому объекту нужно при совокупить маленькую структуру данных которая называется счетчик копий она будет инкрементироваться каждый раз когда выполняется повторное сохранение объекта который уже когда-то встречался в нашей системе это очень простой но эффективный способ экономии дискового пространства но за него приходится заплатить появлением слоя метаданных которые приходится поднимать диска при каждом обращении К объекту но можно пойти еще дальше и попытаться дедуплицировать пересечение между объектами мы делим объекты на чанке Здесь нам помогает алгоритм который называется контент define chunking и Он позволяет установить границы между чанками в соответствии содержимым объекта затем чанки по определенным законам делятся на сегменты который в свою очередь являются элементарными единицами хранения данных в нашем объектам хранилище реальные данные привязываются именно к сегментам тогда как объекты эйчанки просто содержат ссылки на них также эти ссылки размещаются в метаданных самих сегментов это нужно для реализации сборщика мусора по сегментов и по сути На этой картинке нам осталось только перекрасить объекты и чанки в красный цвет и констатировать что нашим объектом хранилище появилось очень много взаимосвязи и очень много метаданных эти метаданные часто изменяются Они часто нужны поэтому они хранятся на быстрых дисках Но именно эта архитектура позволяет нам реализовать дедупликацию сразу на двух уровнях это дедупликация объектов и дедупликация сегментов оценивать эффективность дедупликации мы будем с помощью все того же коэффициента сжатия так вот на Верхнем уровне на уровне объектов на наших стендах Мы наблюдаем более чем десятикратное сжатие И это не предел потому что здесь нет никаких сложных настроек все зависит исключительно От количества дубликатов во входящем потоке данных а вот с дедупликацией сегментов все интереснее и сложнее если все хорошо то коэффициент дедупликации сегментов зависит от количества объектов то есть наблюдается такое интересный эффект чем больше данных мы Запишем в объектное хранилище тем эффективнее будет работать дедупликация но происходит это только в том случае если чанкинг был правильно настроен и мы во всем диапазоне размеров наблюдаем аккуратные 70-батные чанки Если же он был настроен неправильно как на правой картинке то размер чанка возрастает до нескольких килобайт при этом дисперсия размеров чанков становится просто чудовищной в результате коэффициент дедупликации еле-еле отрывается от единицы поэтому первый главный вывод который можно сделать это то что чтобы дедупликация работала ее требуется настроить под конкретный поток входящих данных чем еще хороша дедупликация сегментов Дело в том что популярные сегменты нужны при чтении самых разных объектов поэтому эффективность стратегией было бы размещение этих сегментов в кэше тем самым мы сэкономим один из самых дефицитных типов аппаратных ресурсов для систем хранения данных это операции ввода вывода на медленных дисках на этом графике мы видим как эффективность работы с телами сегментов зависит от коэффициента дедупликации чем лучше работает дедупликация тем эффективнее кэш тем выше его хитрейт и получается в результате что кэш объем которого составляет всего лишь 10 процентов от исходных данных демонстрирует хитрейт более чем 60 процентов это означает что мы можем более чем в два раза снизить его на наших медленных дисках К сожалению мы не можем напрямую управлять коэффициентом дедупликации есть лишь настройки которые косвенно влияют на статистическое распределение чанков по размерам это верхние и нижние границы размеров это параметр avirage Beats который отвечает за чистоту выделения чанков Ну и все тот же пороговые размеры объекта начиная с которого Мы выполняем чанкинг Однако средний размер чанка играет ключевую роль в балансе между данными и метаданными в нашей системе если чанке большие то дедупликация работает плохо метаданных мало мы экономим быстрые диски но увеличиваем затраты на медленные диски Если же чанки маленькие дедупликация работает хорошо метаданных много у нас увеличивается рост затрат на SSD но при этом мы экономим HDD Итак мы с вами рассмотрели два алгоритма которые объединены в конвейер и которые обрабатывают входящий поток объектов нашей объектное хранилище каждый из них регулирует какой-то свой трейдов между аппаратными ресурсами каждый из них обладает собственными настройками и ситуация осложняется тем что настройки одного из алгоритмов опосредованно влияют на результативность другого алгоритма потому что данные которые выходят из чанкинга они на самом деле отправляются на вход в компрессию и получается что в нашей системе появляются какие-то сложные неявные взаимосвязи которые проанализировать и спрогнозировать как нам найти тот конфиг который соответствует оптимальному балансу аппаратных ресурсов есть два пути первый путь интуитивный то есть мы основываясь опираясь на свою экспертизу выдвигаем какую-то гипотезу как правило не проверяем ее Идем вместе с ней в продакшн но я бы например сказал что объектное хранилище с такой архитектурой будет максимально эффективно если максимальную дедупликацию помножить на максимальную компрессию для всех объектов кроме самых маленьких Почему Да потому что именно эти настройки позволяют максимально устранить всю избыточность во входящих данных и поэтому данных будет мало второй путь Экспериментальный мы должны провести какие-то опыты извлечь новые знания о нашей системе и уже на основании этих знаний сделать решение о оптимальном конфиге этот путь точнее и дороже и конечно мы пойдем именно этим путем мы будем рассматривать нашу систему как черный ящик с определенным количеством входов и выходов вход номер один это конкретные значения настроек тех алгоритмов которые мы только что рассмотрели вход номер два - это нагрузка которую мы подаем на нашу систему а на выходе мы будем снимать статистику которая будет свидетельствовать о потреблении тех или иных видов аппаратных ресурсов для дисков Это количество данных и опции для центрального процессора это утилизация ядер для оперативной памяти это просто объем затем мы конвертируем Каждую из метрик потребления в стоимость да то есть каждый ресурс что-то стоит ему вычисляем частную стоимость каждого ресурса и затем суммируем их и получаем финальную стоимость хранения данных в нашем объектном хранилище по сути только что мы с вами определили стоимостью или целевую функцию как ее еще принято называть что мы о ней знаем она определена многомерном пространстве которое задается параметрами конфигурации нагрузки мы не знаем аналитический вид этой функции и даже если мы захотели его узнать скорее всего у нас бы ничего не получилось но мы точно знаем что эта функция ограничена снизу потому что Из своего опыта мы понимаем что у любой системы есть какая-то минимальная конфигурация на которой она выдержит какую-то нагрузку и чтобы символизировать это мы рисуем здесь такую параболу ветвями вверх Наша задача заключается в том чтобы найти точку оптимума то есть тот набор параметров целевой функции при которой достигается ее минимальное значение на ее области определения очень важный момент заключается в том что вызов целевой функции стоит дорого в нашем случае чтобы посчитать значение целевой функции нужно развернуть чистый стенд загрузить данные подать нагрузку снять метрики что-то посчитать и получается что весь наш стенд превращается в такой большой программно-аппаратный вычислитель целевых функций и одно вычисление может занять минуты часы в худшем случае дни и конечно нельзя говорить о том чтобы можно было найти точку оптимума путем грубого перебора всех комбинаций параметров нам нужно сделать это за минимальное количество шагов и здесь нам помогут методы математической оптимизации к настоящему моменту придумана огромное количество методов на все случаи жизни есть например безусловно оптимизация которая ищет Оптимум на всей области определения условное ищет только в какой-то ограниченной части локальные методы ищут локальную Оптимум тогда как другие пытаются дать гарантии на глобального оптимума многое зависит от параметров целевой функции они могут быть непрерывными дискретными и даже категориальными и за каждым из этих методов скрывается какая-то своя математика свой набор свой набор методов важное значение имеют свойство целевой функции если Она известна если известен ее аналитический вид Значит мы можем ее продифференцировать и воспользоваться таким популярным методом математической оптимизации как градиентный спуск Например если функция дешевая значит можно безболезненно вызывать ее много раз и найти Оптимум с высокой точностью Если же она дорогая то а высокой точности скорее всего придется забыть Наша задача Это задача поиска оптимального конфига является примером условной глобальной оптимизации неизвестной и очень дорогой функции и чтобы решить эту задачу мы воспользовались оптимизатором rbf опт это достаточно новая библиотека Она появилась несколько лет назад она заточена на применение с целевыми функциями из реального мира она пытается найти приближенное значение глобального оптимума за ограниченное количество шагов важное свойство этого оптимизатора заключается в том что он не требует указания аналитического вида целевой функции что крайне важно для нас единственная трудность которую мы столкнулись заключалась в том что этот оптимизатор написан на питоне тогда как мы используем Go поэтому мы написали свой враппер в котором реализовали API для оптимизации а также несколько новых полезных функций Некоторые из тех графиков которые вы дальше увидите они реализованы в этой библиотеке Итак у нас появился инструмент для исследования И сейчас мы будем пытаться найти Оптимум у пяти разных целевых функций мы будем двигаться от простого к сложному и наблюдать за тем как изменяются параметры оптимума в зависимости от формулировки целевой функции сначала пару слов об окружении все это происходило на обычном десктопном компьютере с восьмиядерным процессором 32 гигами оперативки данные лежали в оперативной памяти потому что у нас не было задачи протестировать производительность диска на этом компьютере Однако на таком стенде удалось развернуть отказы устойчивую инсталляцию объектного хранилища это трех нодовый кластер с репликацией метаданных в количестве 3 экземпляра и с кодами Рида Соломона для данных в конфигурации 2 + 2 То есть это двойная избыточность в такой конфигурации такой кластер может пережить потерю любого из узлов и продолжить обрабатывать запросы как на чтение так и на запись в качестве исходных данных использовались объекты которые были извлечены с одного из наших тестовых стендов там где письма друг другу отправляют тестировщики медианная размеры объекта Как вы видите совсем небольшой около полутора килобайт Но вот дисперсия просто гигантская и связано Это с тем что некоторые объекты отличались друг от друга по размеру более чем на 7 порядков это очень много и понятно что в условиях такого разброса подобрать универсальную оптимальную конфигурацию хранения будет довольно трудно каждый оптимизационная сессия проходила с разными наборами исходных данных Но в любом случае их было не очень много и для того чтобы смоделировать с помощью небольшого набора данных нагрузку из реального мира приходилось экстраполировать результаты с помощью масштабных коэффициентов это достаточно скользкий путь который может привести к ошибкам в силу того что мы можем не доучесть какие-то масштабные эффекты которые как правило присущи тем системам с которыми Мы работаем но другого пути особенно нет и еще несколько важных нюансов фактически в ходе работы оптимизатора мы фиксировали нагрузку то есть мы искали оптимальную конфигурации системы под конкретную нагрузку каждый в каждый оптимизационной сессии оптимизатор выполнял по 100 вычислений целевой функции при этом мы защищались от не оптимальных конфигураций которые мог предложить оптимизатор с помощью тайм-аутов то есть мы установили правило что любая запись и чтения любого объекта объектное хранилище должны уложиться в 10 секунд если это не происходило то мы отметали эту конфигурацию как заведомо невалидную на данной таблице мы видим диапазоны вариации параметров здесь наиболее интересные Single chunclimite compression limite они изменялись в пределах от одного байта до 1 мегабайта и по сути Именно они определяли для какой доли объектов выполнялись чанкинг и компрессия также для простоты мы зафиксировали один алгоритм компрессии это было Z std и варьировали только его уровнем сжатия Итак давайте перейдем к первой целевой функции она устроена очень просто здесь только одно слагаемое это слагаемое стоимость HDD дисков то есть мы как будто на время забываем о том что у нас есть метаданные и хотим сконцентрироваться только на оптимизации количество медленных дисков вообще каждая целевая функция должна быть сформулирована так чтобы отвечать на какой-то вопрос для первой целевой функции он звучит так Каковы минимальные затраты на HDD диски для хранения одного терабайта клиентских данных подчеркнул что здесь имеется ввиду не наше внутреннее представление данных а именно те данные которые нам передал клиент снаружи ответ минимальное значение этой функции в 18 долларов достигается при максимальной компрессии Мы видим что оптимизатор выбрал уровень компрессии в 20 ну здесь нет ничего удивительного потому что все мы прекрасно понимаем что компрессия нужна для того чтобы сэкономить дисковое пространство но здесь получились достаточно интересные результаты у дедупликации она совсем невелика коэффициент дедупликации лишь немногим больше единицы и Судя по настройкам дедуплицироваться будут только самые крупные объекты Но это еще не все оптимизатор позволяет прикинуть нам сколько будет стоить не оптимальная конфигурация в данном случае это почти 38 долларов То есть если мы неправильно настроим компрессию и дедупликацию Ну чанкинг то мы переплатим только за наши жесткие диски уже в два раза ещё один интересный момент заключается в том что оптимизатор позволяет немножко посмотреть на внешний вид целевой функции поскольку она многомерная посмотреть на нее можно только спроецировав ее значение на плоскости сформированы парами параметров и те из вас Кто будет использовать оптимизатор для решения своих задач найдут в этих графиках много интересного Но я же просто хочу акцентировать Ваше внимание на том как много здесь красного цвета то есть как велика вероятность не угадать с конфигом подобрать неправильный конфиг и получить завышенную стоимость эксплуатации нашей системы а во второй целевой функции мы вспоминаем о том что у нас есть метаданные и отвечаем на вопрос каковы минимальные затраты и на медленные и на быстрые диски необходимые для хранения одного терабайта клиентских данных ответ 68 долларов достигается примерно при таких же параметрах это максимальная компрессия и примерно такая же дедупликация она стала еще хуже здесь фактически дедупликация равна единице в третьей целевой функции мы вспоминаем о том что для того чтобы обработать наши данные нам требуется ресурсы центрального процессора компрессия это же вычислительно сложный алгоритм Поэтому нам надо прибавить стоимость вычисления для простоты Будем считать что мы записываем данные только один раз и поэтому мы арендуем в каком-то облачном сервисе облачные CPU по цене 2 цента за одно ядро за один час и прибавим эту стоимость к стоимости дисков и будем отвечать на вопрос каковы минимальные затраты на диски и аренду облачного цу необходимые для однократной записи и последующего хранения одного терабайта клиентских данных а значит стоимость возросла уже до 153 долларов настройки дедупликации примерно такие же как и были раньше а вот компрессию оптимизатор снизил до единицы то есть оптимизатор увидел что те затраты которые мы понесли на работу CPU они не компенсируются то экономией которую мы получили на дисках то есть оптимизатор помог нам выбрать сторону в балансе между затратами на дисковое пространство и затратами на CPU в четвертой целевой функции мы попробуем разобраться с таким важным фактором который определяет стоимость хранения данных в исходя как операции ввода/вывода сейчас мы ответим на вопрос каковы минимальные затраты на железо которые нужны не только для хранения одного терабайта данных но и для постоянного приема и отдачи этих данных на высокой скорости на скорости 1 гигабит/с подобная постановка вопросов заставит нас пересмотреть подход конструированию целевой функции во-первых Теперь мы говорим не об аренде облачного цпу а постоянной работе соответственно мы должны купить нужное количество процессоров которые будут работать непрерывно во-вторых при выборе дисков нам надо учитывать не только их объем но и их пропускную способность в предыдущих трех целевых функциях мы исключительно отталкивались от средней стоимости одного терабайта хранения но сейчас мы должны выбрать тот набор дисков который будет удовлетворять нас по объему по пропускной способ и при этом будет дешевле любого другого набора дисков которые удовлетворят первым двум пунктам третий такой методологический момент заключается в том что как я уже говорил мы выполняли сессии оптимизации на маленьких данных это необходимо потому что иначе они бы никогда не завершились и с маленькими данными Есть риск что они будут закашированы на различных уровнях поэтому мы можем не увидеть тот реальный объем и о который генерирует наши системы поэтому мы полностью отключаем все кыши которые есть в объектом хранилище и поскольку не буферизированный вот вывод через Директ мы так и не осилили Мы периодически в начале каждой нагрузки сбрасываем Page От чего зависит стоимость HDD дисков и дисков вообще ну с HD дисками Все просто чем больше емкость диска Чем выше скорость вращения тем выше стоимость а вот с SSD дисками все немножко посложнее здесь помимо емкости и опсов добавляется еще один параметр это параметр надежности этого диска dvpd как же нам выбрать ту модель диска которая подойдет для нашей инсталляции лучше всего которая сделает эксплуатацию наименее затратный может быть нам купить топовые октаны или за те же деньги купить Микрон 8 раз более объемный но в четыре раза более Медленный или же мы можем за те же деньги купить целую кучу десктопных ssd-шников от hikvision Ну насчет последнего я конечно шучу потому что они не подходят для использования в СУБД именно по параметру надёжности решение этой задачи кроется в интеграции модуля подбора дисков и шире модуля подбора железа с оптимизатором модуль подбора дисков итерируется по этой тепловой карте и для каждого из дисков которые удовлетворяет нас по своим потребностям рассчитывает число n и T это то количество дисков данной модели которое удовлетворяет нас как по объему пространства так и по пропускной способности затем элиты мы умножаем на стоимость одного диска и получаем стоимость набора дисков данной модели получаем массив стоимости и наборов и выбираем там минимальную стоимость запоминаем для какой модели диска была получена минимальная стоимость и на финальном шаге мы суммируем стоимость набора HDD дисков стоимость набора SSD дисков и стоимость CPU результат оптимизации этой целевой функции перед вами цифры получились достаточно внушительные но они не должны вас смущать потому что мы специально сконструировали такую конфигурацию системы при которой мы фактически отказались от использования оперативной памяти то есть мы сделали так чтобы оптимизатор увидел реальное количество IO которое генерирует с нашей системой и смог его оптимизировать и мы видим что параметры Оптима в принципе не отличаются от предыдущей целевой функции в качестве побочного эффекта оптимизатор выдал нам рекомендации по выбору дисков Как вы видите он предлагает нам купить большое количество маленьких самых маленьких дисков со средней скоростью вращения то есть произошла классическая ситуация для схd Мы уперлись в его и будем разруливать ее путем покупки большого количества шпинделей вот ну с SSD дисками здесь все просто мы в них даже не уперлись просто покупаем три штуки по числу нот и наконец Пятая целевая функция в которой Мы попытаемся учесть эффект от покупки оперативной памяти понятно что в такой системе у оперативной памяти будет много разных потребителей Но для простоты мы скажем что мы просто заложим стоимость кэшей одна только объектного хранилища в объеме 20 процентов от исходных данных и добавим стоимость этой оперативной памяти нашу целевую функцию и мы видим что покупка более дорогого ресурса покупка оперативной памяти неожиданно привело к снижению стоимости эксплуатации на 8% а по сравнению с предыдущей целевой функцией И что еще более интересно оптимизатор увидел что мы используем оперативную память и сумел вывести более оптимальную конфигурацию для дедупликации в результате эмпирически определенный коэффициент дедупликации возрос до 1,8 то есть ну хотя бы восемь процентов мы экономим на этом слое Ну и в качестве Приятного бонуса мы теперь можем купить не 227 дисков А всего лишь 153 подойдем к итогам Давайте вспомним С чего Мы начинали Мы говорили о том что объектное хранилище построенное по рассмотренной архитектуре Скорее всего будет эффективно если помножить максимальную дедупликацию на максимальную компрессию для всех объектов кроме самых маленьких Потому что эти настройки устранят всю избыточность данных Однако оптимизатор численным способом показал нам что это не так И что мы должны ограничиться дедупликацией только самых крупных объектов и при этом включить легкую компрессию Почему так получилось Я думаю что проблема в том что мы использовали единые настройки чанкинга и компрессии для объектов всех размеров иными словами и для однокилобайтных для 1 мегабайтных объектов Мы выбираем нарезку на чанке примерно одинакового размера и в результате если мы выбираем маленькие чанки то у нас очень хорошо работает дедупликация но при этом начинаются тайм-ауты на крупных объектах Если же мы выбираем крупные чанки то тайм-ауты не наблюдаются слайдансе все хорошо но и дедупликация не работает поэтому очевидно что нам надо переходить к более тонким по диапазонным настройкам этих алгоритмов для наших объектов вводить классы размеров Ну и в целом делать что-то подобное TC малоку и аллокатору памяти в языке а ну я надеюсь что в ходе своего доклада я показал преимущество баланса ориентированного подхода к проектированию софта и преимущество применения методов оптимизации с помощью оптимизации мы можем найти не только тот конфиг для нашей системы который соответствует минимальной стоимости хранения данных но и В некоторых случаях мы можем от оптимизатора получить рекомендации по выбору железа и даже подсветить какие-то архитектурные проблемы как это получилось и с нами Большое спасибо у меня все а сейчас пока спикеру некуда убежать со сцены мы начнем с ним общаться фронтен центр прошу Спасибо большое за доклад у меня будет два вопроса Я их задам последовательно вопрос первый Правильно ли я понимаю что здесь оптимизация производится один раз и дальше не меняется Может ли случиться такая ситуация когда паттерн использования как-то сменится и придется адаптироваться очень хороший вопрос есть идея как сделать само подстраивающийся хранилище то есть то хранилище которое будет адаптироваться к потребностям клиентов Но это большая разработка но в принципе вот из этого строительного блока Можно отправиться и сделать вот эту фичу Давайте обсудим а вопрос второй та библиотека которую вы упоминали она сразу делает весь цикл или возможно управлять ее итерациями То есть сейчас сделает рацию один потом сделает рацию 2 сделай операции 3 в чем Point то есть для чего нам надо ее останавливать Ну например чтобы строить в цикл эксплуатации а но в этом плане но я думаю что поскольку это Open Source ответ да вот но нужно будет что-то допилить немножко Ну то есть не поддерживает но там есть Так возможность заложить туда исходные данные которые были получены от предыдущей оптимизации в этом смысле каждый следующий шаг является как бы суперпозицией над всеми предыдущими шагами и в принципе можно попробовать это сделать да Виталий коллеги рассказали мне что тебя будет подарок за лучший вопрос тогда тебе нужно будет выбрать и запомнить лучший вопрос это сложно поэтому сосредоточьтесь пожалуйста Следующий вопрос фронтен справа Спасибо за доклад У меня тоже на самом деле целая группа вопросов некоторые по хитрее некоторые Попроще давайте по одному потому что желающих много потом вернемся Да спрошу наверное самое интересное Я так понимаю что это система которая работает на горячую которая постоянно есть какой-то доступ и вы ничего не упомянули про редактонс и про дупликацию данных про БКБ То есть у вас там что-то 220 чем-то дисков которые если один выходит из строя система падает Как у вас это устроено есть какие-то покапы просто система горячие как они консистентны поддерживают как так я отвечу на этот вопрос очень коротко вообще у нас есть серия статей на хаббре именно про наш объектное хранилище где это все описывается я пришлю ссылку вот если в двух словах То отказов Значит у нас как вы поняли есть два слоя хранения информации в нашем хранилище слой метаданных и слой данных значит отказоустойчивость метаданных обеспечивается с помощью простой репликации вот при этом на этом уровне баз данных работает как eventual consystems и multimaster база данных вот там используются crdt типы поэтому даже если что-то разъезжается то в итоге Все равно все съедется на уровне данных Мы тоже ничего нового не придумали используем классические коды риды Соломона то есть на уровне данных это что-то типа софтверного рейда но очень кастомного и заказчик может в соответствии со своими потребностями выбирать то количество дисков которые он хочет до получить То есть он сам балансирует нашим хранилище отказоустойчи финансовые затраты Вот То есть можно выбрать допустим конфигурацию 10 плюс 3 Да вот и терять три любых диска вот эти расчеты получены для конфигурации 2 + 2 это минимальная как бы избыточность которая при этом дает хорошую отказа устойчивость То есть можно даже писать если потерял одно ноду Вот это если так в двух словах но в целом я пришлю потом материала Следующий вопрос фронтен справа Привет В целом очень круто эмоции прям типа всю задачу забрали но у меня как всегда вопрос начальника сколько вы реально бабла сэкономили как вы это померили зарплата окупился или нет какой размер и размер кластера Да много вопросов значит Смотрите по поводу размера данных мы являемся вендорами софта и мы не эксплуатируем мы не храним данные наших клиентов поэтому база может быть Какая угодно по размеру там физических ограничений нет используется sha-256 это могут быть там бесконечные множество хранимых объектов в качестве экономии которая реально достигнута на тех стендах которые мы все-таки имеем доступ Вот это 20 процентов вот может быть это немного Вот но смотря на каких масштабах вот что еще сказать какой вопрос Ну считали ли вы финансы сколько денег вы сэкономили А если считали то по какой методике типа методика представлена то есть вот мы складываем все возможные типы аппаратных ресурсов которые затрачены на разворачивание инсталляции и смотрим Какая стоимость при том конфиге который Мы считали оптимальным до этого исследования и значит итоговую стоимость Получается примерно так Следующий вопрос контент справа на самом деле два вопроса один из другого вытекающий первое При таком массиве дисков у нас влияющим является реконфигурация их в случае выхода из строя учитывалось ли это при расчете параметра CPU И второй вопрос вытекающий из этого Почему именно кода Рида Соломона потому что они очень тяжелые для конфигурации а под реконфигурации Рида соломоновой имеется ввиду восстановление данных а какие есть альтернативы ксор Мне кажется он еще медленнее Почему у нас кроме ксора есть raid1 Raid 10 При учете такого массива они и При учете того что у нас более затратным является CPU чем диски на это будет более простым решением уйти в простую копирование простое Да спасибо я к Вам подойду отлично Следующий вопрос Back and по центру Спасибо большое за доклад вообще очень интересно у меня вопрос по третьей целевой функции Вы можете открыть Да вот здесь значит смотрите здесь как я понял вас такая гипотеза что CPU используется все время хранения данных на диске что в общем-то жизнь это не соответствует я обычно храню там когда мне нужно хранить терабайт я диск покупаю на год А CPU трачу только на то время когда я пишу Вот Совершенно верно Может быть я плохо объяснил это но расчеты были именно такие вот как вы объяснили то есть мы берем CPU на короткое время для однократной заливки данных вот а диски считаем Так как если бы они нам нужны были навсегда а есть какой-то профиль скважности то есть процент использования CPU что вот в среднем за год CPU используется для файла например интересная Метрика но нет сейчас нет просто это же основная Метрика которая вот делает по сути вот эту функцию То есть вы говорите вот я терабайт буду хранить А значит я допустим писать его буду минуту значит я стоимость CPU должен буду умножить на минуту а стоимость диска на год и поскольку у вас выводы такие что CPU очень дорогой а диск очень дешевый вот коэффициент между временем использования CPU и временем использование диска на самом деле он будет целевой функции А вот это все будет побочка Понятно Ну отчасти согласен да спасибо так вот теперь мне нужно будет побегать а нет еще не нужно Middle справа Спасибо за доклад у меня вот какой вопрос учитывались ли затраты на Zip Ну имеется ввиду на то оборудование которое должно лежать на складе у того кто эксплуатирует чтобы немедленно заменить случаи отказа и не ждать когда его привезут по гарантии смотрите Да спасибо на самом деле нет предела совершенства и факторы которые можно учесть мы понимаем что на самом деле они бесконечны и вот если учесть все что угодно то тогда затраты на разработку уже могут превысить эффект вот от применения этой целевой функции но в целом Я думаю что я писал просто некий фреймворк Вот который К тому же отчасти выложен в опенсорсе мы можем попробовать как-то его по развивать вот чтобы ну решать вот эти общие задачи затраты на электричество вот затраты на дата-центр Вот это тоже все Надо как-то разбираться как-то считать Вот и в итоге приходить к какому-то итогу Так что да мы не считали Но это нужно сделать Следующий вопрос Back and слева Спасибо за доклад очень интересно было но вообще такой фундаментальный работа проведена У меня вопрос про дедупликацию про чанкинг но как известно существует два варианта онлайн и Оффлайн В первом случае момент записи сразу производится chinking и дедупликация во втором пишется как-то Вот все как есть а потом когда система находится под меньшей нагрузкой простаивает выполняются все вот эти тяжеловесные работы там по расчетам по дедупликации вот у вас какой вариант используется и вообще ну выбор на основе чего этот был сделан Когда вы его принимали я понял то есть выбор в том делать ли дедупликацию на лету или делать асинхронно как это на винде например делать да а сразу скажу что вопрос с асинхронной дедупликации даже не стоял потому что мы в целом относимся с очень большой осторожностью к фоновым задачам потому что мы понимаем что в тех случаях где сложность алгоритма Линейная грубо говоря мы можем прийти в ситуацию когда данных стало больше чем это то количество которое мы можем переработать то есть в итоге это дедупликация может уйти на бесконечность Да и в итоге как бы мы останемся ни с чем поэтому нет мы все делаем синхронно прямо на лету Следующий вопрос фронтен центр Спасибо за доклад было интересно Можем немножко вернуться к слайду Где у вас появлялись там чанки так еще Да вот здесь вот эти вот Чан сегменты они как-то вместе лежат или нет компрессия выполняется каждого сегмента в отдельности либо не группируются в один какой-то большой сегмент если группируется в один то одно письмо из там разбилось на 5 сегментов они как фрагментировано хранятся Сейчас расскажу да Значит также хочу как бы отослать к серии нашей статей я пришлю потом ссылку если коротко то ответ такой чанке разделяются на сегменты как раз по годам Рида Соломона то есть сегменты это просто разделенные чанки и дополнительно дегенерированные по ирейжу кодингу сегменты значит к тем сегментам которые являются исходными данными затем сегменты по определенному закону размещаются в кластере вот есть различные гарантии хранения Ну как минимум один каждый сегмент хранится на своем диске то есть не бывает такого что все данные легли на один диск или на одну ноду например вот поэтому сегменты хранятся По отдельности по-разному в разных местах нашего кластера Тогда вопрос продолжения вы получается оценивали компрессию каждого сегмента в отдельности Да забыть если все вместе хранить их рядышком и там их компрессировать получается и затраты на CPU они будут несколько меньше достали Но ведь мы не можем охранять рядышком потому что иначе это будет не отказоустойчиво Нет они могут храниться распределенно так Но все равно Где где-то рядом В каком плане на одной машине и на одной машине на одном диске Но если это диск выйдет из строя Да его копия там хранится где-то в другом месте а вот в этом главный Поинт что сегмент это хранятся в единственном экземпляре в хранилище то есть мы не делаем полную репликацию сегментов вот иначе это было бы неэффективно мы бы не отличались там от Я не знаю классической базы данных с репликацией все сегменты хранятся в одном единственном экземпляре и Отказ устойчивости их достигается с помощью кодирования Рида Соломона а можно еще в продолжение немножко текстов файлы они очень хорошо жмутся когда они большие когда они маленькие они хуже жмутся Да вот и возможно если мы объединили бы сегменты сжали и эти сжатые копии там хранили на разных дисках то возможно мы не сильно будет просели по необходимости я понял то есть коэффициент сжатия будет настолько высок Что понятно интересная мысль Да над посчитать Спасибо Ну что ж еще один вопрос фронтен справа Здравствуйте не рассматривали выглядит какое-нибудь применение хардварных решений отдельных То есть я так понимаю что у вас какие-то General Proper сервера вы собираете просто CPU диски на совали их там кластер поехали а может быть какие-то более кастомные специализированные решения карточки компрессии Хард Варны отдельные системы хранения данных понятно что там контент defind chunking на них не сделать но вот после этого например использовать специализированную как раз именно просто для хранения оптимальную компрессии и так далее и там поддержание всяких репликаций и прочего вообще такие идеи были но немножко в другой плоскости то есть мы хотели одно время использовать fpg от Ой нет ни в Ну да Нет персистентную память Intel для хранения как этих метаданных то что там достаточно большой оп сгенерируется и есть возможность немножко переконфигурировать нашего mbd-базу данных которая хранит метаданные чтобы она лучше работала и было дешевле вот Ну потом эта тема сама с собой свернулась и мне кажется что все таки важнее уметь работать на комоде Вот чем покупать что-то топовое крутое Ну мне кажется во всяком случае профиль наших клиентов он в основном такой Ну да это интересно там многое можно достичь"
}