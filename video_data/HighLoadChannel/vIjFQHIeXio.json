{
  "video_id": "vIjFQHIeXio",
  "channel": "HighLoadChannel",
  "title": "Бесфайловая система хранения. Почему YDB работает с дисками напрямую / Владислав Кузнецов",
  "views": 5891,
  "duration": 2603,
  "published": "2022-03-21T13:43:21-07:00",
  "text": "всем привет меня зовут влад сегодня мы поговорим про яндекс это bass и про конкретные и самый низкий слой наверное про слой хранения что такое индекс это bass это база данных мы стараемся сделать примерно такой как все привыкли то есть у нее есть язык запросов над например как es que el можно писать нормальные человеческие запросы у нее есть транзакции строгой ассистент не все перечисленные на слайде но с какими-то бонусами то есть база автоматически восстанавливается после сбоя в там разных стоек серверов дисков это не требует вообще никого вмешательство не людей не эксплуатации не даже инженеров дата-центре зачастую базу может горизонтально масштабироваться как по ресурсам железным то есть если не хватает хранения можно добавить досыпать дисков все расширится автоматически все за использовать так и по компьютере sur son если не хватает для обработки данных можно закинуть динамических not чтобы справлялась части про транзакция также базы ориентируется на надежное хранение мы в первую очередь храним данные и мы храним стране сохранить их максимально надежно также при этом нужно стараться делать это быстро потому что надежно хранить данные котором доступ какие-то секунды но это никому не нужно говорить мы будем сегодня про самый базовый слой файловой системы все к ним привыкли все используют файловые системы где-то там наличных компьютерах на серверах но так ли они хороши у них есть большие бонусы во первых это удобства можно красиво организовать структуру хранения можно расположить все в папке можно бэкапить свою базу с помощью там копать банально ворсинка если вас кончается место можно там удалить файл какой-то ненужный и все заработает также профайла система знает все это очень удобно можно нанимать новых людей и не учить их своим велосипедом а просто показать что вот-вот из файлик вот удаляй здесь здесь гасанович профи все будет работать также довольно большой бонус у них есть готовую еду вверх он позволяет делить ресурсы устройство между разными пользователями между разными писателями но у вас есть какие-то недостатки в первую очередь для нас это готовое решение казалось бы готовые значит классно но в готовое решение практически невозможно вносить изменения и точно невозможно вносить изменения которые портят жизнь другим пользователям вы конечно можете в up stream что то сделать но это должно улучшить жизнь всем они только вам также базы также файловой системы пишутся имеют круто выход они пишут метаданные например они меняют если вы меняете размер файла нужно сделать запись в минуту если вы что то записываете меняется к составим это можно убрать но если вы создаете файл это снова записи на диск лишь не если вы удаляете файлы это снова записи на диск файловой системы также вы также тратитесь на лишний слой абстракции все запросы проходят через код файловой системы там могут быть ошибки могут быть баги мы даже с этим сталкивались однажды файловой системы не бесплатный по паспорту они тратят какие-то проценты там ну для x4 я посмотрел с дефолтным и настройками это порядка 16 процента в принципе не так чтобы много но с другой стороны если это умножить там на какие-то десятки тысяч дисков как у нас это превращается в это довольно заметные деньги и файлы система фрагментируется это может быть может не быть проблемой с нашим нагрузкой мы не тестировались но кажется с нашей нагрузкой это может превратить где градациям обычное приложение работают следующим образом они создают файловой ну использовать файловой системы и обращаются к linux с какими-то обычными рисковыми там на открытие файлов на запись на создание файлов на локацию и при этом все запросы идут через весь стек в и деби работает с дисками напрямую общается сразу за слоем блочных устройств и соответственно весь код который из ядра нам пришлось перенести в users дать точнее мы написали свой вариант общий тренд в высокопроизводительных приложениях идет в сторону скорее отказа от ядра все переносится в юзер space есть такая библиотека из пяти kay inc овская они в основном in toscana open source там вообще нет никакой файловой системы и там и десна режима работы с вами устройствами это опись по адресу на пися шине мы тестировали спиде кей получаются очень классные результаты какие-то микросекунды на ответы на егэ вообще сказочно придумывать свои велосипеды нужно если вы защищаете диплом или хотите сделать что-то максимально просто диплом и мы защитили поэтому пришлось сделать что-то очень простое просто трус структуру на диске у нас она в целом похоже на файловой системы ну если вы хранить штат на диске вы будете сделал что-то похоже у нас есть очень маленький супер блок который пишется один раз в жизни там какая-то минимально информация типа дата форматирования размер диска размер сектора которые мы используем и она пишется только один раз мы и никогда больше не читали есть очень маленький syslog в котором есть системная информация мы его пишем редко там раз низко секунд там он нужен для нахождения текущего 1 чанка logo и следующая структура это собственно up and only лог как любой базе данных у нас есть up and lock он пишется в чанки чанки мы просто развели диск начинки по 128 мегабайт адрес чанка находится просто офсет умножить на 128 мегабайт чанки общего значения то есть в них может писаться как данные писатель писатель решает что он хочет нам хранить либо данные либо лог и процесс записи в лог выглядит максимально просто мы пишем в chunk когда он заканчивается мы пишем ссылку на следующий чанг дальше пишем в chunk когда нам нужно обрезать лог мы делаем записи syslog и соответственно новая голова теперь находится прост для сравнения в в процессе работы довольно типичная ситуация когда писатели хотят компакте какие-то данные у них есть там 1020 старых чанков там еще десяток новых которые не данные переложили и они хотят за один раз за одну транзакцию удалить старые файлы и как закомитить новые если делать это файловой системе напрямую то это будет порядка 20 порядка 200 псов на обновление метаданных на создание директорий там и так далее у нас все это смешивается в одну запись на диски и мы получаем что вместо одной секунды можно делать такие операции за там порядка в 1 сиг на ходу 8 миллисекунд мы пришли к тому что мы всегда выключаем в рейд кэш устройств и у хдд его семье в каком-то виде у них не всегда он заключаемый почему так причина 2 во-первых надежность не всех и т.д. правда могут записать все что у них есть в буфере на диск мы такое видели пару раз а вторая причина это производительность в райт кашу хдд дисков достаточно интересный и у некоторых дисков он реализован таким образом что все что пишется на диск она аккумулирует в какой-то небольшой памяти и когда контроллер когда сам контроллер решит может быть в не очень удачный момент это сбросить он начинает это сбрасывать балкам в на диск и соответственно диска не отвечает какие-то там сотни может быть даже сильно сотню миллисекунд можно же секунду мы это отключаем и весами контролируем когда нам нужно писать обычное приложение используют парадигму такой разработки когда пишется все маленькими записями в буфер в операционной системе и потом изредка делает flash и сбрасывает это все на диск мы немножко ушли от этого мы это все реализовали на своем уровне и поэтому мы всегда файлы открываем в синхронном режиме не используем буферов ну потому что у нас уже свои буфера зачем тратиться 2 раза и все запросы у нас идут синхронный ответ на них мы получаем только когда правда данные записаны на диск при этом очевидно такое приложение работать будет очень долго если писательство напрямую в синхронном коде поэт мы используем довольно распространенным библиотеку linux у и либо его для синхронной работы с диском на все запросы пользователей старриджа мы отвечаем только когда данные уже записаны на диск то есть если кто то пишет транзакцию в базу ответ придет только в том случае если все реплик все данные во всех репликах уже точно записаны на диск и так же у нас есть собственный планировщик нагрузки так как мы не используем файлы систему у нас вообще открыт 1 файловый дескриптор на одно устройство нам нужно как-то балансировать полосу и abs и между разными писателями и это добавил нам гибкости когда есть собственно планировщик можно делать какие-то довольно интересные приоритеты например можно выставить какой максимальный приоритет когда запрос выполняется единственно на диске не ничего другого на диск не будет подана пока запрос не выполнится можно сделать супер низко приоритетные запросы например для кого-нибудь фоновой обработки фонового чтения для аналитики можно использовать такие запросы могут использовать диск только когда ник никто другой вы не использует на диске нас хранится самая обычная сам дерево в общем-то достаточно распространенное решение у него есть два типа операции она использовать диск если посмотреть на срез работы с диском на но использует в двух вариантах первый этап яндон вилок это какие-то достаточно мелкие записи обычного размера там порядка сектора диска и они происходят часто и в общем то здесь закрыты все скрыты все light насти который видит пользователь и фоне происходит компактным и the bulk чтения чанков и балка запись мы в первую очередь ориентируемся мы оптимизируем запись в лог потому что ее видит пользователь и здесь такой финт ушами который мы сделали если всем нужно писать в лог то можно взять несколько пользователей в минска писателей и все их записи в лог объединить в один ясный лоб получается если 6 рисунке 6 виртуальных дисков пишет в лог это все схлопывается в одну запись и тратится один собственных это в общем сторож выглядит так что что есть клиенты которые в него пишут они обычно удаленные на каких-то других серверах запущены они пишут через прокси и пишут на виртуальные диски физический диск он менеджер один компонент это называется по диск если посмотреть чуть за другого ракурса сверху весь сторож это совокупность дисков совокупность серверов и к там какие-то сотни тысяч и в каждом из диски и все эти диски разделены на группы в группу обычно входят 8 дисков сам сторож предоставляет очень простой api он используется только на нашими же компонентами в нашей системой он хранит небольшие бла бы от мегабайта до 10 а то байта до 10 мегабайт и хранит огромное количество у нас на самом деле типичных ранений типичное количество бобов это какие-то там миллионы миллиарды мы их пишу на любой чих там транзакция комитет какое-то изменение начала транзакции пишет потом комитет изменения пишет потом отвечает пользователю пишет сторож имеет две типичной конфигурации у нас были разные варианты мы остановились на двух первое это красота центровая в которой три одинаково реплики пишутся в 3d то центра она позволяет переживать отключение дата центра и еще несколько стоек в оставшихся там не любых стоек но одну как минимум всегда переживает и более эффективная по месту 1 центровая конфигурация она используется реже кодирование в которой 4 части с данными дополняются двумя блоками отчетности это все пишется на 6 дисков у неё в избыточность полтора запись данных происходит проходит с следующим образом данные режется на шесть частей с помощью реже кодирования и пишутся на 6 дисков по хорошо определяется на какие именно запись идет и данные отправляются на эти виртуальные диски но здесь интересный момент то есть у нас было один кусок данных которые мы могли записать за один iops мы его разрезали на 6 и записали за 6 tips of то есть получается у нас был один диск мы на него писали тратили один abs взяли 8 дисков распилили все это it и получаем 6 tips of и в итоге мы по цене 8 дисков получили 8 шестых диска там 1 и 3 это не очень выгодная математика получается поэтому здесь включается включает входит в игру наше решение что есть несколько виртуальных дисков живут на одном диске и они склеивают свои запись ну и получается мы как сами создали себе проблему сами решили теперь их рассказам в этом на конференции и один блок пишется в итоге превращается в примерно 1 на самом деле здесь интересная ситуация что если кластер загружен слабо то диски пишут довольно нура синхронно они пишут редко и поэтому каждый его диск в принципе идет со своей записи свой сектор и загруженность кластера выглядит достаточно большой потому что диск принципе используются если повышать нагрузку за загруженность классно загруженность дисков не растет потому что просто все операции начинают склеивать и так можно там смотришь на загруженность кластера я на девяносто процентов пишешь мне ответ пять раз больше у меня загруженность 90 процентов пещер хоть раз больше у меня загруженность 85 процентов потому что там что то как то начинает мочиться и идти в на диск в одной записью довольно забавная история которая мне показалась смешной если он сам дерево оно довольно древние она придумана уже там в девяносто шестом году и она сделана исключительно для оптимизации работы с хода до дисками у нее все все сделано так чтобы был об андорре лог который писался параллельно на дорожку или какие-то большие записи и чтения при этом берем суперсовременные н вами ssd любые ssd смотрим на их структуру оказывается что они устроены следующим образом у них есть рис блоки так называемые и данные хранятся а не было с лешим свойствам у них в итамаре из блоки менять beats нолика на единичку условно это бесплатная операция практике бесплатно и так происходит любая запись от сбросить единички на ноль это очень дорого это требует больших ресурсов и поэтому н в н е сделать таким образом когда в нее пишите она записывает а довольно быстро но как только нужно что-то перезаписать она поднимает он представьте у вас есть там мегабайт ный блок есть в нем какие-то грязные данные допустим пол мегабайта что-то дописываете сверху он поднимает все данные в память ну в свою внутреннюю меняет данные и записывают в новый блок и получается про этом политики что может быть каким-то колоссальную в принципе ничем не ограничен то может быть какие-то сотни и если использовать его сам дерево у нее up and запись идет всегда на чистую зону а балка записи in bulk чтения и записи они всегда пишут ну тоже в чистую зону и получается очень низкий рейтинг или внешние посмотри у нас на правах на разных дисках у нас примерно 1 и 3 смарте можно посмотреть при этом в интернете нашел данные многие пишут что у них там порядка 3 5 некоторые оптимизируются с помощью там зонт блок девайс с есть такое видение позволяет писать в отдельные блоки явно у них порядка единицы какие проблемы с каким проблемам железо мы сталкивались вот мои как разработчик самый любимый проблемы это явный отказы дисков вас просто в коде возвращается ошибка вы просто возвращаете ошибку автоматика отстреливает диск инженеры вы меняет все хорошо вам никто не звонит не надо ни с чем разбираться с очень быстро и просто 2 вид проблем это когда если на хдд долго не читать данные и не перезаписывать их они начинают портиться и про ту хоть просить это третье 2 которой мы видели это потеря с тех данных которые были в полете на момент пропаже питания это достаточно коварная ошибка с ней очень сложно разбираться особенно в породе невозможно узнать были ли вообще записаны данные отвечали ему мы могли на них положиться на устройство получить ок на запись ответить пользователю а потом оказалось что данных нет мы такое в праге наверное не видели мы тестируем железо активно потери данных мы сохраняем специально пришлось написать новую систему для тестирования который сохраняет стоит в в отдельный процесс и грохать основной процесс периодически там с отваривания прямо по шине и такой мы видели на устройствах также существует проблема с человеческими ошибками какая бы ни была совершенная система менеджмента дисков их замены все равно люди будут ошибаться и да если у вас тысячи десятки тысяч дисков то люди будут ошибаться уже какой-то заметное количество раз это будет уже заметно не просто раз в год а как то чаще с хода дисками мы столкнулись с такой проблемой что через два-три года они начинают терять данные вот яндекс облаку недавно исполнилось 3 года и мы начали замечать редкие ошибки угадайте какой кайф гарантию дает производитель нах это за диски как раз примерно три года и есть данные в интернете от крупных провайдеров которые показывают что через три года начинается очень большой рейд выпадение хода дисков вот мы начали через три года видеть редкие ошибки на чтение на хдд и как-то беспокоились то есть мы единичные ошибки можем корректировать и одну и две ошибки из чтения мы можем восстановить но страшно что в какой-то момент у нас окажется такая ситуация что есть бог который записан в 6 копьях у него сломалась протухла одна часть протухла 2 части и потом когда то подвох не третье мы об этом не знаем пока не придет пользователи не начнет читать как только прочитает мы поймем что нас данные потерянном изжили и не знали поэтому мы сделали в принципе достаточно известное решение там в зад офис есть крапинку в уроке это же я видел они про это пишут процесс просто периодического перечитывания всех данных на диске если мы читаем данные и куска нет мы восстанавливаем это из оставшихся реплик и перезаписываем со скорпионом есть забавная история мы когда вы включили мы поставили небольшой inflight дисков параллельно карабин к порядка мтс а штук и так как мы запустили это первый раз он начал активно находить ошибки у нас начали выпадать диски у нас выпадает 1 2 3 4 5 напишет инженеры вынесли из дата-центра вы и ничего там не делали у вас ничего нового у нас тут диски кончаются мы такие вот запустили процесс проходит еще полчаса не говорят остановите пожалуйста нас кончились диски но к сожалению индексов ки и приносит в дата-центре поэтому у нас побежал инженер покупать новые диски приостановить пожалуй стоит процесс ну и после этого мы уменьшили inflight и уже там несколько дисков в день у нас выходило и их оперативно меняли у нас произошел довольно заметный переход к репликации когда вы перед активным переходим на январе если на хдд полная перезапись если вы просто по специфике смотреть там 200 мегабайт секунды они пишут объемы у них порядка там десятков таро байт они полностью перезаписываться порядка например за сутки при этом одни загружены у нас очень сильно обычно практически в полку и если диск выпадает и нужно его реплицировать то это очень долгий и очень болезненный процесс он иногда мы сделаем это так чтобы он не касался пользователи чтобы не было нагрузки но есть как небольшое проседание но если просто небольшой the replication сходится вообще за несколько суток поэтому когда диск выходит из строя у нас выбора не было мы просто сидим и ждем пока хост не вернется с н в н е все изменилось все на внж современные пишут порядка там 3 4 5 гигабайт в секунду редкие образцы и при этом имеет объемную там четыре то байт восемь терабайт в принципе сейчас пока еще нет таких больших январь веках хдд у него полная перезапись составляет порядка 10 минут при этом загружены они не так сильно как хдд там на 50 процентов у нас наверное максимум они загружены в продакшен ах поэтому репликация есть где развернуться и она сходится быстрее и поэтому мы теперь когда диск выходит из строя мы не дожидаемся когда он вернётся даже не в том случае если диск выходит из строя если например сервер вводят на обслуживание там заменить какой планку памяти это может длиться там два часа три часа иногда занимать сутки если раньше мы ждали пока здесь вернется то теперь можно просто этот диск перевести виртуальный диск перевезти на новое место все данные восстановятся там примерно за час и мы уже совершенно не беспокоимся о том что у нас могут выпасть какие-то еще диски у нас полная полная репликация и новый диск уже на новом месте продолжает работать чтобы как-то оптимизировать процесс мы используем сделали такую концепцию дисков доноров если мы заранее знаем что сервера уйдет в обслуживании то диски на нем все еще остаются они еще какое-то время работают мы можем данные с этого диска перевезти на новое место напрямую просто это в нашем мы сами это контролируем в нашем коде и после того как все данные переедут диск уже в новом месте продолжает работать еще одна забавная история на уже не с нашим проектом случился с коллегами которые тоже базами занимается в яндексе он искал такую смешную историю он как однажды дежурил у него выпал диск он первый раз дежурил в сервисе мне выпал диск он пишет инженеру замените пожалуйста диск с таким-то силе никому такого хорошо проходит там 10 15 минут он видит еще одна ошибка еще один диск сломался на такое пишет инженер о замене пожалуйста теперь еще один диск итак за за день у него сломалась 16 дисков он за годом из страны какие-то диски ненадежны потом проходит какое-то время он дежурит следующий раз уже в другом центр меняется диск он пишется за меч пожалуй диск вот такой цели не говорят мы так не делаем мы не меняем диски по сирене куда вы подождите но мне меняли алгоритм у нас вообще нет такого протокола это невозможно такой так секундочку пишет тому инженеру спрашивает а как вы меняли смогли так легко я достаю полку там 16 дисков я достаю диск смотрю серийник не тот вставляем обратно доставить диск не тот вставляем обратно и так он в итоге за один за один день поменял ему всю полку диск вот чтобы такого не было мы идем в сторону явного менеджмента дисков по серийником у нас есть единая точка в системе единый реестр в которой хранятся все серийники всех дисков мы их блогеру нам историю когда они были вставлены в кластер когда мы их вынули и все компоненты на старте проверяют по диск на старте проверяет соответствие серийника текущего тому который должен быть в базе и мы запрещаем доставай диски если они нам нужны и в инженеры могут и добавлять диски вынимать диски прям просто указывая серийник в планах мы хотим это все прокачать до того уровня что можно диск просто вставить в абсолютно любой сервер и он автоматически войдет в кластер и все все данные сможет отдать это очень удобно например если в теории произойдет когда-то в будущем такая ситуация выйдет из строя один сервер например из-за памяти и данные на нем есть на другом сервере сломается диск на третьем сервере внезапно тоже сломается диск и у нас ситуация потери данных минус мы потеряли три части из 6 этот диск этот сервер в котором данные все еще есть он на недоступен у если там даже он не в обслуживании может носить сломалась или он в другом дата-центры можно эти диски достать и вставить соседнюю стойку который работает и данные автоматически переключаются и мы не их не потеряем на этом все спасибо за внимание большое за доклад и давайте мы зададим каким вопросом для начала ну конечно безумная история с дисками это конечно прекрасно коллеги у кого микрофончик так вот вижу вижу вижу вижу вижу давайте начнем с конца вот тут молодой человек у прямо в белой кофте прям вот дать на чем оттуда здравствуйте спасибо за доклад вы упомянули липою на одном из слайдов и собственно вопрос в том хочется узнать использовали ли вы posix а его или возможно попробовали уже за использовать и а у ринг место ли полива после салона мы изначально не хотели использовать она мне нравится потому что он внутри он эмулирует асинхронность с помощью того что он создает потоки под каждый запрос если я правильно помню и там какие-то очень какие космические накладные расходы вот поэтому от него отказались сейчас мы смотрим на два направления наверное будущий будущий работает у ринг мы его еще не пробовали в тестах и из пяти кей вот его мы пробовали он правда выглядит классно все в юзер спейси все устройства мотаться прямо напрямую в юзер space в очереди к минимальной там задержки там на постановку операции на запись тратится меньше микросекунды ну его в полностью операция там с чтением проходит за микросекунды вас спасибо так и вот здесь вот за колонной девушка грядки спасибо большое за доклад интересуют скорость репликации то есть сколько по времени данные синхронизируются и второй вопрос у меня насколько понимаю я на xdd сейчас нельзя использовать real-time системах нужен окружена где чтения-записи вот это вот все происходит то есть какую нагрузку выдерживает вовсе репликация на когда дисках она составляет порядка трех дней ну зависит от нагрузки но мы ориентируемся на до 3 дней обычно это обычно это наверное сутки на нём е это десятки минут ну типичное время там какие-то 5050 и пирсинг или это десятки минут максимально наверное сейчас но мы не особо следим за этим потому что она всегда сходится очень быстро и мы статистику не собираем так если навскидку говорить примерно час привел то ему это очень заря так говорить это пятна потому что наша база данных у лтп и в принципе мы создавали и для обработав высоконагруженных в real time приложениях какую нагрузку держим очень сложно сказать у нас огромное количество вариантов работы то есть есть вариант когда она использую как база данных она использует ее можно вот в neobux облаке потрогай например есть варианты используя когда над нею ставится очередь мониторинге поэтому вот патрон нагрузки слишком сильно зависит ну я не скажу на какие рпс можно получать нельзя нельзя к сожалению есть данные где-нибудь об открытых источниках где когда тестировали и можно их посмотреть все польские данные по работе с например с виртуальными машинами в яндекс облаке внутри яндекс за тобой сохраняться и работать там автоматическим здесь вопрос пожалуйста да такой вопрос то есть выглядит что решение но такую generic вообще для всех сервисов по работе 100 мин то есть вот есть например обжиг старый туда есть яndex зато боясь а вот учим такая специфика как бы вот то есть почему бы им тоже например не напрямую не работы с диском то есть пример наш про одно и то же то что я сейчас рассказывал это просто речь у нас вся система разделена на несколько слоев и у нее есть слое бы сам ну наверное самый большой по объему как вы усилий вложенных и кода это слой транзакций он наверное он сильно сложнее и больше там есть таблицы есть за язык запросов есть консистентной транзакции то что я сейчас рассказывал это просторы сторож он правда очень простой он просто хранит данные по ключу он никогда не используется напрямую не каким сервисом он всегда используется только для наших же систем которые выше строится это может быть виртуальные диски в яндекс облаке эта система мониторинга ну или вот как я сказал в база данных то есть грубо говоря вот вот тот же самый подход можно использовать для построения объектных не щели нет ну и конечно вот есть доступ к базе данных вы в принципе можете в ней сделать табличку в нее писать данные и она внутри если это грамотно спроектировать то фактически запись там в строку она будет превращаться в одну запись в сторож и также будет идти на диске насколько я знаю есть проекты которые делают обжиг срочно базовой тебе и третья попытка то есть яndex обжиг старик он сделан вот на этом механизме или нет да на этом все спасибо спасибо за вопрос и вот молодой человек пожалуйста сейчас вопросик а потом будете вы обычно распределенная система с 1-го типа ходу по сначала строят файловую систему вина распределенную систему и там внутри наверняка использует похожие наверное пока не мир гугля возможность пользу похожая идея то есть сразу и убрать файла систему системы на системную оставить новых знаний создать распределенную файла систему вот у вас есть вообще файловой системы но и такая похожая вещь или вы решили от нее избавиться если решили избавиться почему мы не шли в эту сторону потому что когда система создавалась у нее изначально было два основных две основные части первые то таблетки которые части таблиц и у них есть строго понятные паттерн использования они хотят писать очень много достаточно маленьких записей и при этом им не нужно какое-то пространство там им не нужна никакая иерархия не нужные деревья им просто нужен ключ и значение поэтому сторож создавался исключительно под них чтобы сделать базу данных и сторож оптимизирован под хранение вот огромного количества таких маленьких записей поэтому сторону файловой системы надо этим мы не смотрели но никто не мешает сделать сверху с использованием вот того что того api который сейчас есть ну как базы данных можно сделать их хранение файлов dll файлы систему распределенные спасибо молодой человек отсюда пожалуйста и дальше потом будете вы добрый день спасибо за доклад у меня такой вопрос вы не думали вот свой подход своих слой работы с файлами но и по сути распределенную базу данных применить для нормальных баз данных потому что у вас получается очень дешевый диски очень высокая скорость и почему это не использовать она на обычных реляционных базах данных там на том же ластики где но в реляционных до запись конец типа не будет но на эластики то можно запись конец сделать в принципе можно использовать это если в яндекса блокировать виртуалку то у нее диск виртуальный он будет работать на в и т.п. и в каком-то смысле это вот то то о чем вы говорите на сколько я понимаю она будет писать выдели напрямую интегрировать в какие-то другие системы мы не пробовали даже не думали об этом но мы можем поговорить об этом это интересно это хорошее интересно интересная тема для дискуссий так и давайте вот чем лодочек пожалуйста спасибо за доклад очень интересный до диски у вас стали собственно дешевыми запись стала дешевый а что с целостностью данных мы же не можем полагаться всегда на то что данный у нас всегда правильные всегда корректны и как нам их восстановить то есть вы рассматривали этот вопрос у нас усыновление целостность восстановление целостности у нас при что всегда шесть копий и любые 2 6 частей и при потере любых двух частей мы можем восстановить целостность в случае одна дата центровые установки при этом целостность отдельных кусков мы проверяем с помощью crc сумму нас мы сами пишем сердце на диске и проверяем уже по секторам что данные не побились просто хранения выгодно спасает на ходули например хорошо но вот на слайде был представлен вариант когда у нас данные разбросаны по нескольким дискам какой-то из дисков у нас совпадает данные на нем через виртуальный диск соответственно они теряются как нам в данном случае восстановить данные не нужна ли дополнительная докторская степень для этого или все существующие инструменты восстановления данных нам помогут у нас то все автоматически восстанавливается ну я подам в другом докладе рассказывал если диск ломается то например все операции чтения которые идут они идут сразу на 8 дисков и с группами и он читает просто любые 4 если он видит что один диск нет либо отвечает ошибки либо вообще не отвечает ну там надо например выключена то он просто вы становитесь любых четырех частей автоматически этот пользователь не видит вообще выпадение дисков и пользователями виду система выше понял спасибо а если диск вставляют пустой то он автоматически идет на соседей опрашивают у них информацию про то что у него должно быть ну то что потерялась и восстанавливает это тоже автоматически спасибо и пожалуйста вот ваш хват скажите пожалуйста есть ли какое-то взаимодействие с другими базами данных с какими и какие были сложности есть есть взаимодействие другими базами данных допустим там не знают те же сам cliff house или с реляционными и на узкий и состоянии могу ничего про это рассказать я не знаю про эту часть микрофончик я и скажем так нижней части системы из слоя хранения а этим занимаются люди которые уже ближе к базе есть она есть внутренние адаптеры для внутренних каких-то басма там пока пим в другую базу вы ведь тоже тоже яндекса вскинул публичность облаке есть транспорт есть трансфер менеджер я не свободна ну опять же это лучше обращаться к тем кто проект знает я далёк от этой от этого часть хорошо это в кулуарах можно обсудить тему трансляции данных пожалуйста вот вопрос и потом сразу девушка меня вопрос добрый день получается используйте только те годы то есть никаких рядов как на уровне контроллера никакого копирование нет нет резерве 2 просто планировщик с помощью которого приоритизировать и запись на диск как внутри устроена получается оперативной памяти собираете какую информацию и решаете что лошадь или чуть поподробнее про планировщик который решает что в данный момент более прилететь не записать на диск про планировщик можно делать отдельный доклад это этим занимались на несколько месяцев с кучей чтение скучать статей он в памяти своей держит несколько очередей то есть у писателей у каждого писателя их 8 штук у него есть несколько очереди по приоритетам разбиты и планировщик вон из этих очередей выгребает задачи в таком порядке каким как он считает нужным там есть real-time приоритет когда он пытается соблюдать дедлайны виртуальные дедлайны запросов есть цевка участию которое completely fair хэнк в анапу в линуксе такой scheduler есть вот позаимствован оттуда идеи он позволяет надолго сроки получить честно и разделение ну какой то чтобы она стремилась к честному разделению полосы между писателями там как такового каша нет то есть все что приходит она помещать очередей но не специфицированы просто пользователь пишет он может писать например если это запись в лог но она типичная наверное какие то сотни может быть килобайты если это запись в это флашей состоит таблиц то это порядка там 100 мегабайт так хорошо спасибо девушка и поднимите руки кто еще хочет задать вопросы так хорошо потом молодой человек здрасте насколько я знаю яндексе есть похожие продукты индекс зато bass индекс тейлз также известный как видите подскажите пожалуйста сильно они похожи какие-то основные отличия можете подсветить и связанный вопрос для каких задач и что лучше подходит для каких задач лучше подходит поиск проектов очень сильно ли коварный вопрос да он очень простой у них очень сильно разделены ниши ведь это про большую обработку данных он очень медленно там средние время ответа это порядка минут и в принципе динамические таблицы близкий к нам да они позволяют быстро работать но это как бы ответвление если говорить про основной ведь то это какие-то минуты про динамический я не очень много знаю если честно так и вот молодой человек добрый день у меня такой вопрос если я правильно понимаю у вас идет работа железом на низком уровне правильно да вот соответственно интересно как тестируются какие-то негативный сценарий когда диск отказала либо электричество выключилась мне кажется довольно редко исполняться и его сложно будет правительстве тестов нет как тестируется но это тестируется в большим количеством разных подходов в первую очередь мы просто мокко им устройство создам какой-то сохраним в памяти данные и выключаем это по команде и звание например мы в тесте создаем 8 виртуальных устройств и периодически выключаем их и в разных комбинациях с потерями данных это первый подход но это пока самый базовый самые банальные какие-то ошибки искать второй подход у нас есть такая шатал к которая берешь кластеры подаешь на него нагрузку и параллельно пускай оша толку и она считает кластер то есть наш от этого во всех возможных комбинациях это рестарт из серверов это удаление данных с дисков это отрывание дискос шины noble nook симона послать запрос чтобы устройство прямо на уровне шин оторвалось вот это тоже автоматический случай и ну кажется все хорошо спасибо и последний вопросик давайте сделаем сейчас добрый день младшая спасибо за доклад вопрос на самом деле довольно простой с учетом того на каком низком уровне у вас работает вся система в целом насколько вы завязаны на конкретного вендора конкретного оборудования то есть занимаетесь ли вы перепрошивкой контроллеров this и так далее вот себя до занимаемся потому что вендоры пишут очень странные иногда присылают новые устройства ну мы на вендоры не завязаны у нас огромное количество метров и х д д н ы м е в принципе ну как любая большая компания она хочет диверсифицироваться чтобы не было блока на какого-то одного производителя в поэтому в принципе любой in вами любых и т.д. можно заменить на другого производителя аналогично там объема и система будет продолжать работать мы нередко видели как особенно это наверное с новыми дисками или иногда прошивки новые когда приходит диск у него прошивка например один из жизненных примеров производители любят делать trim secu нам без предупреждения то есть у нас обычно trim есть фоновый когда он отрабатывает там за кит микросекунды фигурный когда он точно загуляет тот секторах на котором были даны записано на самом деле черный он вообще там может по десять раз перезаписывать это еще какими-то паттернами вот это работает там может и миллисекунды работаю с моим диск реально зависает перестает отвечать потому что мы на нее три послали вот такой мы от отслеживаем мы посылаем производителям они меняют под нас прошивку сложно сказать этим занимается рэнди отдел и не связан с ними спасибо большое давайте поблагодарим лат за ответ на вопросы и у нас от организации конференций небольшой презент спасибо самом деле было очень много очень классных вопросов выбери какой-нибудь один вот который тип прям вот понравился и запал коварный вопрос нужно вспомнить вспомнить тут было всякое разное про вендоров не понравилось про vendor вот отлично вот добыть вот тогда вязальный презент книга и я всех вас попрошу если у вас настоящего вопроса хоть и что в кулуарах обсудить у нас есть специальная зона виртуальной дискуссии куда переместиться здесь сразу при выходе на право и там же можно будет задать онлайн вопросы задавайте их прямо там в дискуссионные зоне"
}