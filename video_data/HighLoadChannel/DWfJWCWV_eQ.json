{
  "video_id": "DWfJWCWV_eQ",
  "channel": "HighLoadChannel",
  "title": "Eventual consistency при производстве пиццы / Евгений Пешков (Додо Пицца)",
  "views": 3805,
  "duration": 3160,
  "published": "2019-06-03T09:05:56-07:00",
  "text": "меня зовут пешком жжения я работаю в компании да да c до d и пиццерий на данный момент 333 пиццерии хотя может быть уже открылось еще парочка мы представлены 10 странах и у нас порядка 185 заказов в минуту в пике на данный момент немного о себе я разработчик big and разработчик мы разрабатываем да да из dota из это некоторой степени сердце нашей компании это система позволяет работать под нашим партнерам с нашей информационной системой не о чем особо не думаем мы очень любим цифры и выводим например такие краны у нас публичный вот наша выручка скриншот с выходных то есть часть если информация все цифры доступны ну что-то в паблике что-то нашим менеджерам но у нас нет сложности доступа к информации немного про наш тег мы пишем надут нити сишарпа из подобных дохнет core 80 из используем майский или редис и rabbit рынке в качестве брокера сообщений и все это мы хотим в облаке вейдера немного как мы начинали обычной архитектурой есть какая-то база есть какие-то приложения приложения используют шар от базы для обмена информации все что должно быть транзакции тран работает в транзакции никаких проблем одно приложение меняет систему трансакции же меняется другое и все работает замечательным мы растем мы постоянно растем вот если взять график с окончанием шестнадцатом году то у нас была уже порядка 150 все это генерала 50 заказов в минуту 14 февраля 2017 года мы проводили очередную акцию пиццы сердечками и было очень много заказов какой-то момент мы достигали 50 заказов в минуту и это наша система после этого подала мы ее прибудь или восстанавливали как могли как только порядок заказов опять доходил до 55 система опять подала соответственно туда из вот наша замечательная она не могла выполнять эту функцию которую должна выполнять люди переходили на бумажке в пиццериях и что самую плохую как бы мы посчитали примерно прогресса открытие новых пиццерий и поняли что вот это цифры 55 заказывают просто утром через несколько месяцев и всю нашу работу можно выкинуть просто в урну если ничего не поменять как я уже сказал у нас было shared база которую и было собственно той точкой отказа нашей системы естественно как могли масштабировать вертикальное и мы выделяли 3 applique то есть какие-то известные вещи мы сделали но все равно это не помогало вина записи базой иногда работал очень плохо соответственно было принято решение распиливать наш монолит отделять какие-то сервисы ну в первую очередь как бы отделять надо было то что грузит базу то есть мы нас не было задачи явно там type the market например как у коллег и зла моды то есть мы решали чистую нашу проблему соответственно делали достаточно быстрая по максимуму перри использовали все что было и было принято идеологическое решение что сервис должен быть понятным бизнесу то есть это какая-то бизнес все-таки подсистема соответственно выбрали ну как один из сервисов которые выбрали для отделения этот рейтинг tracking эта система которая позволяет пицца мейкером на кухне делать пиццу для вас во время ничего не путаю не путай размеры не путай толщину у теста на каждой станции выводится ну вешается планшет на котором выводится маленькие подсказки что пиццы maker должен делать прямо сейчас соответственно патимейкер берет плюшку раскатывает топает по планшету информацию летает к нам соответственно мы эту информацию в дальнейшем перерабатываю мы показываем например на сайте что там пицца готовится или готова мы показываем ее в системе отправки курьеров что пиццу вот готова ее можно с курьером отправлять и так далее соответственно вкратце к что получилось ну получился tracker осталось legacy часть которую ну как бы часть монолита который не распиленный добавился rabbit чтобы обмениваться сообщениями проследим кратце путь то есть пицца maker толкнул по трекеру tracker что-то записал свой старик за к метил дальше отправляет сообщение дальше отправляет сообщение в ребят из ребята консьюмер и вытаскивают сообщение от переправляет в in point монолита ну и монолит это все пишет свою майский базу и вроде бы все хорошо мы как бы на самом деле систему чуть сложнее есть обратные связи и в целом 2007 году показал что это решение вполне себе рабочие мы преодолели планку стол заказов минуты легко у нас уже под конец года было более 250 пиццерий и ну в целом все было неплохо но давайте посмотрим на схему еще раз так если посмотреть на схему более внимательно то мы увидим много потенциальных точек отказы то есть у нас могут быть проблемы сетью могут перезагружаться виртуалке мы находимся в облаке иногда мы не контролируем даже перезагрузку виртуалок могут быть какие-то баги в коде у нас не было и дам патент на стена тот момент то есть мы не могли или троится ничего и не было коммутативности необходимо было соблюдать порядок и это накладывал свои проблемы то есть вот на схеме tracker заметил данный в сторону что могут произойти как искал может rabbit быть недоступен либо rabbit перезагружается либо сети что-нибудь может консилер быть недоступен там отвалилась jobo который должна консью мить или какие-то сетевые в неполадки соответственно консилер может не достучаться до in пойнта монолита монолит может внутри что-то сломаться какие-то баги или еще что-то произошло и также в монолит может не достучаться до своего из тораджи соответственно мы получаем ситуацию что тораджи трекера что-то поменялось а в майские ли базе монолита не поменялось ну как бы соответствии с этим не ничего и это очень неприятная ситуация на бизнес это соответственно сказывалось как например новый заказ мог быть принят на треке мог быть принят то есть вы на сайте заказали пиццу но такой заказ не попадает на tracker соответственно его просто никто не начинает готовить как я сказал пиццы мейкера смотрят только на tracking и не знают ничего о других системах может быть обратная ситуация заказ приготовлено то есть пиццы maker отметил как приготовленный но заказ не доступен к отправке в системе отправки курьеров просто нет этого заказа так как он висит еще в статусе готовится и вот такие заказы зависали и нельзя было например закрыть смену то есть maker просто не мой менеджер смены просто не мог уйти домой он звонил нам то есть в саппорт саппорт находил программистов и мы что-то там пытались сделать какие наши пути решения проблем были мы начали с и на патент насти retrieve когда у вас сеть моргает когда сервисы могут быть там в перезагрузке еще что-нибудь но это такой достаточно легкий способ что-то повысить надежность ваша система это ретро ить то есть в ребят если один раз не ушло но повторите еще там несколько раз возможно это поможет соответственно такие ретро и повысили все равно надежность но так все решение естественно когда к нам приходили люди саппорта просили сделать что-то заказами ну мы просто готовились кейт скрипты которые наши админы на продакшен базе исполняли сделали дашборд в дальнейшем то есть скрипты сделали более там такой рутинной кнопочкой сделали sing по кнопке часть работает силы но все равно недовольство накапливалась количество пиццерии увеличивалась соответственно количество проблем тоже росло и ну в какой-то маленьким поняли что нельзя так дальше жить нам нужно консистентной ну консервных это когда у вас есть две ноты и про один объект реального мира они знают но одинаковые данные в какой-то момент различают строму consistency это когда по сути дела и транзакционных такая привычная в базах данных когда-либо объект меняется в обеих системах одновременно либо ни в одной из них это понятно и безопасно для бизнеса то есть если вы разговариваете с каким-то продукт оунер am в принципе он понимает что вот так и должно работать проблем в том что это достаточно сложно реализовать особенно в гетерогенной среде среди микро сервисов когда у вас нельзя поднять какой-то менеджер транзакции ну вот коллеги но при разломали у них есть там какой-то бизнес откат то есть они сначала что-то делают потом откатывают и так далее ну и в целом меньше надежность так как нам надо чтоб отработали все подсистемы иначе транзакция не состоится и второй вариант и вошел consistent это в пользу иван шоу его что такой ивент шёл это когда система но до 1 меняется и отправляет какое-то сообщение в dota 2 и через какое-то время предсказуемые в принципе но до 2 меняется тоже в соответствии с этим изменениям то есть в конкретный момент времени две ноты всегда почти 1 согласовано но в конечном итоге данные сходится если прекращает поступать новые события это в пользу и вышел consistent говорит его относительная простота в реализации но не всегда допустим по бизнес лайки например этом банке наверное не очень будут за если переводы со счета на счет будут проходить в такой системе ну и возможно у вас есть другие какие-то вещи которые не позволяют делать в шоу как я уже сказал и вошел consist носил neolock может быть в принципе достаточно большой в каких то вы можете показывать статистику говорить какие-то ссылают пирсинг или но в целом иногда лак будет большой ну собственно возвращаясь к нашей схеме мы выбрали вышел консистенцию же тогда по факту ну какой то такой вышел consist of не знаю больше консистенции нам повезет если все повезет если звезды сложится то будет все будет если нет то данные будут расходиться соответственно мы начали думать где мы можем сделать гарантированы гарантированную доставку и собственно вот что мы придумали как мы как попадает сообщение на tracker и то есть обычный это превращается в дтп запрос ну предположим это какой-то мув обычно мы отсылаем айди агрегата и какие то данные которые надо будет в мир жить в этот агрегат соответственно мы получаем агрегат применяем полученные данные причем когда мы применяем полученные данные мы поднимаем версию агрегата агрегаты есть version ность и это нам позволяет в дальнейшем нормально работать с этим формируем события и событий и агрегации сохраняем в им транзакционный в базу данных тут очень важно сделать это транзакционный то есть либо вы меняете состояние системы и прикапывать и события либо у вас вся транзакция откатывается состояние системы не меняется никаких новых событий не попадает system that транзакционных это очень важно если тут и я не соблюдать то в принципе ну как будет вот как у нас раньше было дальше как мы отправляем событий в ребят ну либо мы сразу после сохранения транзакции в отдельном потоке отправляем это событие либо если сразу не вышло по каким то причинам то потом по крону то есть мы сделали некий иван publisher который вот те прикопанные события до tale кивает да ребята но и в любом случае мы удаляем из б.д. события только после успешной отправки что у нас получилось даже если rabbit как в прежней схеме отвалился по каким-то причинам был недоступен сеть моргнула у нас есть иван publisher наш который гарантированно рано или поздно до толкает события да ребята сообщение да ребята то есть это может какое-то время пройти но в целом мы контролируем это и можем реагировать но в целом от нас реакция уже ручной не требуется таким образом мы решили проблему гарантированного павличенко событий в ребят в наши парадигме ребята это некая такая очень надежная система сказ надежность которой мы ну как у боремся инфраструктур на но в целом мы ее считаем мы в целом считаю что если сообщение попала в рыбе то оттуда она только ну консьюмер а может забраться возможно это не так но это дальше итак еще раз мы сохраняем транзакционные заказ и событие агрегаты есть версия то есть мы в принципе знаем про какую версию мы отправляем события в ребят если состояние менялась версии не поднимается в базу ничего не пишем и событий удаляем после успешной отправки так эту проблему мы решили теперь у нас появилась вместо этого другая то есть раньше мы отправляли события сразу как оно произошло в реальном мире и там уже либо да либо нет теперь в принципе может случиться так что мы отправляем события ну вот тем способом как раньше напрямую при этом он начинает работать наш крон который тоже отправляет события таким образом у нас может появиться два одинаковых события в ребятя и второй момент то что так как мы отправляем что-то покрова ну что-то напрямую нас может произойти ordering то есть если раньше порядок в ревите соответствовал реальному как бы происходящему в реальном мире то теперь это вовсе не правда соответственно две проблемы новые рекорды рынка дедупликации где делать ну наш ответ то что этим должна заниматься бизнес-логика вашего приложения то есть не мы решили не перевешать эти задачи на messenger систем то есть мысль насильственно с предельной тупой что вошло то ушло и без всякой там попытки вычислить лишние события или или перестроить их соответственно можно выделить блок консью минга событий то есть то что из ребята вычитывает события и в итоге догоняет дома и ские база монолита какие ожидания у нас от консьюмер а ну в первую очередь эдем по ценности коммутативности ретро и везде где можно и возврат необработанных сообщений в очередь давайте поподробнее об этом и дам патент ность ну вообще этом патенте от свойства обработчика такое что если два одинаковых события пришло ему на вход то состояние после обработки 2 события уже не меняется как мы это решаем мы решаем это собственно за счет version насти мы берем сообщение из ребята получаем агрегат который соответствует потому что надо например это если мы говорим про заказ то мы доставим заказ из нашей legacy монолитной базы и привет сравним просто версии событий а если события новее то мы его обрабатываем если пришло какое-то старое события дошло просто то мы его просто игнорируем большинстве случаев ну иногда мы там пишем какую-то статистику что какие-то движения были но в целом на сам агрегат уже мы не сам агрегат мы не меняем коммутативной скому то тивность это свойство при котором порядок сообщений не влияет на конечный результат то есть если нам сначала приходит событие 1 а потом событие 2 или наоборот результат будет одинаковым мы это решили опять же за счет versio нирования и бизнес-логики то есть нас достаточно мы можем бизнес лойко разрулить недостающие события если какие-то у нас есть дырки то приход события через с дырками не нам не мешает соответственно вся коммутативностью осудил решается обработчиком и проверкой нового события ретро ну везде где это можно везде где можно сделать безопасно если у вас хаттаб и вызовы и ну там вам приходит что-то типа 500 вы можете ретро из скорее всего ну опять же все зависит от вашей бизнес какие у нас это можно что можно тут усилить ну можно внедрять различные паттерны типа circuit breaker то есть если у вас и так все лежит не надо добивать там литрами по десять раз на 10 кратными литрами можно просто какой-то уйти вт тайм-аут и через какое-то время опять начать ретро it и еще одну одну вещь которую мы сделали это возврат событие в очередь так как у нас все теперь этом патент на и коммутативное то если мы наш консьюмер и вот не смог обработать это событие по каким-либо причинам таллин point отвалил стали база данных только к это ошибка в коде мы просто возвращаем это в конец очереди ребятам какие проблемы но из-за из-за чего мы можем возвращать в очередь ну может быть какая-то кратковременное недоступной сервисов то есть сеть моргнула сервиса ушел в ребут окей мы вернули назад рано или поздно нам опять дойдет мы обработаем и все будет хорошо ну может быть какая длительная недоступной сервиса не знаю по каким то причинам у нас спала база данных и мы не можем до нее не как достучаться соответственно событие в этот момент гуляют по кругу возможно вся очередь уже почти красная но в момент когда мы все-таки поднимем нашу базу данных все события пройдут и опять же консистентных будет достигнута бывают ошибки в коде мы выложили какой-то новой версии сервисы приходит обработчик приходит какое-то событие падает с малой фиренц сам соответственно так как мы возвращаем события в очередь у нас есть шанс например перри выложить ну или откатить с исправлением ошибки и опять же когда сообщение придет к нам повторно мы его сможем корректно отработать и все будет хорошо бывает кривые данные по каким-то причинам сообщения содержит в себе данные которые ну никак не могут быть в принципе консистентной сохранены в базу данных такое в принципе может случаться соответственно ну такое сообщение будет вечно гулять по кругу с этим особо ничего не сделаешь ну и еще важная причина у нас почему мы иногда возвращаем в очередь от нарушения порядка то есть предположим что у вас есть событие создания какого-то объекта и его изменения если изменение пришло до создания то ну бывает сложно что ты либо сделать у нас даже нет вообще никакой информации про агрегат соответственно мы такие мы такую ситуацию разрулил просто возвращаем в очередь ожидаю что тем самым мы выстроим в нормальный порядок то есть где то там в очереди фраза затерялась создании объекта нам в итоге приходит создании объекта потом повторно приходит его там удаление например и все хорошо с ответ так соответственно что важно здесь это мониторинг то есть вы должны мониторить насколько часто у вас события гуляют ну обратно в очередь сколько раз они там уже побывали если у вас одно событие вала сотни и более раз ну что ты с ним не так скорее всего возможно его надо удалить просто уже руками либо подправить что-то в базе данных потому что ну что-то здесь совсем плохо и опять же если у вас очередь там количество повторных событий стало слишком большим то возможно у вас недоступно какая-то часть вашей системы и это наглядно вам покажет ну и либо какой-то давайте посмотрим на canceling что еще интересного может там происходить это предложен такая схематично очередь у нас консьюмер берет сообщение что-то с ним делает и перекладывает результат в сторону как я сказал если случае ошибки он возвращает назад масштабирование бывает сообщение ну много мы растем вылетом пике а вы не хотите чтобы обработчик отработал события там через 20 минут то есть вы хотите чтобы вас ну была близка к real-time у соответственно что вы можете сделать так какое событие едим патентные и коммутативное вы можете в принципе сделать сколько кодом консилеров которые будут лопатить ваше сообщение достаточно быстро и очередь опять станет ну какой то такой предсказуемый но бывает обратная ситуация как я говорил о база данных у нас было тонким местом и бывает что консьюмер и вот это множество can серверов начинают грузить базу соль сильно что ну как бы она начинает уже там подлагивать соответственно можно сделать обратную ситуацию включить тротлинг выключить на север и убавить скорость как бы обработки сообщений да пусть они копятся там какое-то время в в очереди в условный там надежный и ничего где с ними ничего не произойдет но при этом сторож наш продолжает работать и мы его не как бы не кладем чрезмерной нагрузкой когда нагрузка спадет очередь рассосется в итоге самое важное что целостность данных будет соблюдена согласованность что в итоге мы получили вот сделал это все данные не теряются мы можем масштабировать и включать тротлинг и еще мы такое получили приятный бонус мы можем теперь переигрывать событие когда я говорил что мы удаляем события из базы у нас есть все равно архивная таблицы и мы можем переложить данные из архивной таблицы опять в нормальную таблицы ивентов там она подходит иван publisher им все это улетит опять выйдет и заканчивается в обычном режиме тем самым например ну как один раз мы допустили достаточно серьезную ошибку у нас все отработала но данные учета были нарушены соответственно мы исправили перри выложили сервис вернули вернули события в базу и тем самым восстановили учет ну таким каким он должен быть давайте еще раз пробежимся потому что мы сделали во первых важно сохранять агрегаты события в транзакции важно сделать i want publisher который будет допиливать зависшие ивенты из вашего 100 roger rabbit ну или в другую какую-то шину важно возвращается события в очередь чтобы они имели папы ну как бы еще попытке чтобы от ну правильно обработается и добавляйте ретро мы добавили ретро и где это можно где-то безопасно и тем самым мы как бы решили нашу проблему теперь данные у нас согласованы в двух системах мы теперь не волнуемся что утром нам опять там разгребать всякие зависшие заказы проталкивается заказы с трекера в кассу доставки и так далее соответственно задача нашим было решено что важно при когда вы решаете вот делать что-то похожее возможно у вас такие же проблемы важно понять насколько часто у вас такие проблемы бывает что системы у них нету согласованности но в целом это не влияют ни на бизнес процессы не на то как крепко может спать программист то есть возможно там 1 месяц 1 неделю каким-то скриптом вы это все устанавливаете и все довольны и второй момент подходит ли вышел консистенции с точки зрения вашего бизнеса с ваших бизнес-процессов то есть доступны ли в задержке в изменениях систем пять-десять минут и более то есть здесь нет какого-то такого жесткого предела не все спасибо на спасибо очень интересный доклад у меня было был вопрос вы начали показали более сложную схему где движение не только в одну сторону например заказа но еще в разные другие мне интересно при такой схеме если у вас какой-то контроль версий то есть не может быть такое произойти что ну грубо говоря порядок у них разный а версии никто я понял выстроили контроль версии у нас решается пока что ну грубо говоря выкладками если мы вынуждены нам поменять что-то ну как бы в работе нет я не про версию кода а я про версию объекта при войну время ну то есть версия нам дает полную уверенность как что меняется у нас не настолько уж запутанная система чтобы ну то есть изменения обычно порождается обычно в одной системе и консью меца в другой ну то есть это такая типовая просто она может быть например 1 ну то есть он создается объект например в монолите с первой версии попадает в трекеры помог треть или меняется со второй версии выпадает уже обратно то есть ну такого я об этом не сказал но мы используем некий механизм ну грубо говоря optimistic конкор насти то есть когда мы вычитываем агрегаты с базы вот здесь она имеете ввиду с разных систем нет не может такого получится то есть у нас такой может получиться но там еще дополнительный из бездны слой как с терминальными статусами то есть у нас есть совсем типа терминальный статус если он прилетает то мы можем там мы даже на версии не смотреть ну какой то финальный конечный статус из которого жизни нет уже но в целом можно сказать что нет таких добрый день спасибо большое за доклад мне такой вопрос вы механизм до 2 раз рассматривали и если да почему один из оказались наверное не рассматривали добрый день спасибо было интересно такой вопрос случай когда сообщение постоянно снова попадает в очередь ну там мог быть кривые данные еще что то есть атаки безнадежное уже сообщения каким образом вы их мониторить и отлавливаете и убиваете или что вы с ними делаете ну у нас есть alert и по количество возвратов соответственно если мы едем какое-то сообщение ну смотрим обычные либо каких-то данных не хватает ну по каким-то опять же скорее все ошибки в коде они не записались в одной из баз ну либо если совсем сообщение какое-то безумно кривой этом не знаю свой какой-то был ну просто его можно удалить там еще что то видел такого не было но сам для будет и счетчик грубо говорят сколько раз события есть информация какой раз он уже здесь спасибо оон вопрос может чуть чуть не совсем по теме используется ли какой-то дашборд которые смотрят люди не айтишники если да то какие у них там может быть полномочия по знать перед исправлению чего то что пошла не так спасибо у нас были даже борды для бизнес-пользователей соответственно там синкай заказы они могли там самостоятельно но это не очень часто практикуют то есть это либо какой-то бизнес процесс ну тогда это нормальный интерфейс это не дашборд а просто ну какой то там личном кабинете выведенная кнопка либо ну через infrastructure из админов через саппорт спасибо за доклад здесь скажите а вы lifetime используйте на сообщение может ли у вас получится что сообщение допустим заказ созданный позавчера будет там двое суток крутится в системе вот через ваш механизм retrieve и создаться когда он уже не нужен получателя ну к сожалению может такое получится это вот собственно один из моментов на который обращал внимание что бизнес должен понимать что в принципе риск есть мы конечно опять же отмечаем время не только количество впадание как долго по времени она живет то есть если событие про то что пицца готова там два часа живет в репите но что-то пошло не так тоже окон реагируете на уровне тогда может бизнеса или поддержки но такие сообщения ну какого то налаженного процесса пока что нету то есть это вот относительно свежие все ну подразумевается что мы можем хотел сообщить менеджеру пиццерию и все всегда есть полу ручной режим работы то есть на бумажках они могут там сказать что вот давай готовь такую пиццу просто без трекера но это надо увидеть и соответственно alert и нам могут позволить это увидеть хотя бы также вас зависшие здравствуйте спасибо за доклад у меня такой вопрос если я правильно понял у вас изначально была проблема с тем что 0 из пропускную способность mais quel а вот и собственно дальше последующее действие были связаны с тем что вы сняли унесли часть нагрузки оттуда амортизировать и по сути нагрузку родов у него поступает плюс достроили систему так чтобы события как бы уменьшили доезжали вот но тем ни менее похоже что маску и остался узким местом этой системы единой точкой отказа ну то есть грубо говоря если у вас там на входе вступает их события и рано или поздно происходит должен там переварить такая x там действий запросов к нему соответственно если у вас план что делать с тем когда мы искали 5 кончится соответственно дальнейшем мы хотим двигаться в сторону и к сервиса в том числе чтобы не было каких то систем с тяжелыми базами данных то есть ну то есть вот тот монолит который сейчас есть он по сути дела может стать некой report базой которые просто аккумулирует всю информацию в себе соответственно cry for базовым совершенно другие требования по работоспособности то есть можно там и при при рассчитывать какие-то отчёты ну то есть совсем другой бизнес процесс соответственно и останутся только маленькие базы быстрые в принципе надеемся что таких проблем дальнейшем не будет этим здравствуйте спасибо за доклад такой вопрос и у вас конечные пользователи которые пользуются этой системой да там это пицца maker и и без остальные бизнеса люди в пиццерии они как-то видит что у них проблемы начались ну то что событие пошло по долгому пути пошел там через cron догонялка что она там сто пятьдесят раз уже попала в очередь что-то такое у них есть ну на данный момент нет ну и на сайте электрон догонялка она достаточно быстро срабатывает нормальном случае если у нас не лежит инфраструктуры то завершающих заказов почти не бывает кроме богов каких то соответственно про баги и про зависающей нашу структуру мы стараемся оповещать наших партнеров есть каналы но это как бы вообще не только в контексте вин шел consists но просто с любым про рыбы и проблем мы стараемся как можно раньше сообщить понятно еще небольшой вопрос насчет опять же retrieve да сколько у вас есть какое-то понимание там сколько раз событие должно попасть в очередь ну я просто хочу это объяснить этой позиции что если у нас какая-то пошла ошибка и постоянно перекладываем событие то мы натыкаемся на такой водопада то есть как как лавину вот мы скатываемся то есть ошибки в логе засираются там метрики все это нарастает вы как-то это отсюда давайте я чекаете все так мы нет мы как раз приняли решение не отсекать ни в коем случае всегда мы крутим если пошла вот такая что что-то прям совсем плохо там и ну просто реагируем то есть это значит что эта проблема высшего уровня как бы то надо брать сей что-то делать хорошо нет последний вопрос вы говорили у вас 150 пицца в минуту до вот это максимальная ваши ну на данный момент по моим рекорд 8 марта этого года 185 см 185 отлично сколько это событие в минуту не скажу ну немного мы не мы не нагружаем рыбе то есть ребят у нас в принципе такой достаточно лайтовый просто обработчики они могут быть достаточно тяжелыми сами по себе по бизнес слой гейдж там в куче таблицы то есть вы вроде двинули пиццу а там запишется статистика право пиццы мейкера и открывает ему смену например параллельно если первое его движение там ну то есть там очень много бизнес новых данных которые хорошему надо тоже разруливать но пока не когда я понял спасибо большое спасибо за доклад и спасибо за пиццу может быть такой простой easy вопрос если читаешь как документацию ребята там для разных языков есть продюсер есть консьюмер на каждой стороне перед тем как что-то сделать с очередью она и инициализируется то есть но это может быть разные проекты продюсеры консьюмер то есть там она директории чатик сечение что такое там все это описывается если что-то там не так то ошибка отлетает либо не создается очередь не подключается да мне вот эта участь не очень нравится потому что разных проектах какой то одна и та же часть копируется и например для разработки т.к. начинаешь у тя rabbit чистый ничего нет в нем такого и ну на продакшн и вряд ли может быть rabbit ляжет все там пропадет и нужно восстановить все эти очереди да ну вот как на разработке вот в этот вопрос решаете чтобы наполнять rabbit конфигурация ну у нас есть обертка на трепетом то есть нас есть такой бас api она как wrapper и в том числе ответственна за создание очередей при ну то есть у нас когда консьюмер из там стартуют они через вот это по совету создают очереди которые нужны у нас очереди предсказуемо называется то есть у нас нету каких-то неожиданных очередей то есть мы можем то есть не консьюмер их создают да ну как бы это стороны консилера все-таки идет создание хорошо еще такой вопрос вот я услышал но у величину измерения 150 заказов в минуту или пицц минута а вот если вот ну так побыстрому примерно такие нормальные величины там количество запросов в базе там запросов к веб-серверу ну по-моему что-то порядка 2 с половиной к серы к серверу в секунду но это я не не к базе по моему такие цифры но опять же и вот я говорил что там очень запутанный бизнес логика там достаточно тяжелые транзакции то есть создание объект создании заказа требовало очень много манипуляции и зачастую это вызывало локи ну может быть это там неудачное решение программистские какие-то подсказываются но в целом это сейчас так что принятии заказа достаточно тяжелые для нас и ну это для нас метрика поэтому здрасте здесь слева спасибо за доклад я правильно понял что когда событие приходят в tracker сначала сам tracker пытается поместить его в очередь и только если у него это не получается он кладет его в базу и после этого кран догонялка еще раз и троится как бы помещение в очередь да все так tracker ну как бы делать свою часть логики для себя и сразу же кидает у меня вот вопрос такой а работала бы это лучше хуже если бы tracker не пытался сразу поместить в очередь он бы всегда клала в базу и сообщение помещались бы в очередь крон догонял кай нож таким образом например у вас был бы всегда только один способ попадания сообщения в очередь только кронам то есть там отсутствие дублирования кода возможно пришлось можно было бы избавиться от помещения заново сообщений в очередь к концу миром потому что сохранился бы порядок вот что можете сказать ну иногда хочется быстрее поэтому ну мы не знаем крон когда отработает а тут вроде вот уже а он работает там раз в минуту рассекла руку гарри 1 минута с каким-то отложенным лагом там спасибо ну в целом можно и так жить то есть я знаю что многие делают просто только иван publisher то есть без ответственности системы published что-либо то есть многие даже это выделяют отдельным микро сервисам которые ходят тебе за ивентами просто то есть ты сохранил агрегат и ивент в себе и все дальше твои ответственностью этом кончилось может быть чисто идеологические тогда же было бы правильнее доклад а у меня такой вопрос я выгляжу на это все событие агрегаты ничего не напоминает ну то есть например а не рассматривали ли вы взять например тот же самый secures и пример в вашем случае мне даже кажется взять его можно без где нормализации то есть просто сохраняем цепочки событий зачитываем получаем учитываем событий получаем последнее состояние заказа не очень много потому что будет у вас ивентов на сам заказ поэтому типа покрутить их каждый раз будет довольно просто в этом случае вам не придется вообще хранить агрегат в в таком случае вы еще более легко можете добавлять ведь порядок событий потому что можно таймс там заранее записать можно переиграть событий опять же еще более легко вообще ничего удалять просто добавляете новое событие какой-то в конец которая как те мыльным способа меняет состояние заказа и все это очень здорово было бы но это дольше делать так как все было уже то есть у нас не было бы очень много времени чтобы кардинально переписывать тем более soccer с сам но нельзя сказать что какой-то глобальный опыт все-таки чуть-чуть другая парадигмы ну и вон sources in canada у нас так или иначе получился то есть мы мы можем их вот этот поток события переигрывать и в целом жить этим secures ну возможно в каких-то новых системах мы будем именно так делать здравствуйте спасибо за доклад такой вопрос о вас нет ивентов которые должны быть обработаны последовательно то есть например был заказ пиццы в будущем он четыре раза изменился у вас очередь обработки пришла не в том порядке в котором пользователь менял заказ и в итоге он получил не то что хотел ну мы либо это можем разделить на уровне бизнес-логики через version ность либо если мы не можем какие приводил пример с отменой не созданного заказа мы такой заказ не создаем на отмену мы просто отмену 500м грубо говоря она улетает обратно в очередь потом доходит сообщение про создания он создается и потом доход доходит сообщение про отмену то есть он на уровне бизнес лайки минут разруливать сын не слышит то есть получается клиента не повезло почему он все будет нехорошо просто чуть дольше ну пределах в душ там допустим заказал 4 пиццы изменил размер 2 еще одно отменил и потом еще одну добавил мне но мы это все сделаем как бы не то есть у вас нет момент получается то есть отмененные пицца с изменение не настолько атомарные что вот типа вот плюс там чё-то то есть событие может содержать себе достаточно много информации то есть можно например ну вот вашем примере предположим что у нас была такая возможность мы бы просто в событий присылали вот всю информацию о заказе и получили бы заказ версии 1 вот содержит 2 pets таких две таких заказ версии 2 содержит две пиццы таких плюс одну отмененный + 1 измененную и так далее соответственно накатывая эти версии на то что есть в монолитной базе мы бы это все свели как надо рано или поздно ну когда все 4 события бы про игрались мы бы события содержит информацию для восстановления но не не всегда тупой но большое спасибо понимаю то что у вас заказ он идет по каким-то этапом да и на каждом этапе он должен выполнить только один раз на то есть это как бы изданную система такая не опускай либо pipeline либо какой-то цикличный граф ну и она на какие-то этапы может вернуться несколько раз может вернуть может вернуть ну то есть это вот в каких случаях то есть почему нельзя сделайте дупликацию то есть не знаю там то что пиццы должна приготовиться один раз это не знаю там понятно да то что пицца должна там доставить один раз это тоже понятно вот в каких случаях возможно что на один и тот же как бы этап в рамках одного заказа возвращается это событие ну вообще например у нас заказ может быть готов о том пицца maker понял что что-то там не так или коробку уронили вашу то есть он вроде бы готов ну вот и и роняет он не готов они от меня этого опять прогоняет по трекеру там недостающую пиццу опять готов но как бы то есть вот по кругу видите то что система не дупликации как бы на том объеме которые есть она как бы не такая сложная и большая то есть в любом случае есть какой-то идентификатор этого заказа до есть этапы то есть эти публикацию можно делать по ключу до отменять и я какой-то этап да можно как бы уже конкретным запросам то есть почему вот именно они не ну у нас так или иначе детали catia происходит мы не консью мем второй раз одно и то же сообщение просто мы его получаем из ребята и не перекладываем на массе jing систем эту ответственность где дупликация то есть а так-то мы особо не тратим всё я понял как и второй вопрос то есть исходно договорилась то что узкое место это база данных то есть разделив базу данных на две части и мы в лучшем случае сократили на 50 процентов нагрузку это в лучшем случае да то есть мы все равно в нее упремся вы не смотрели на то чтобы не знаю там либо киты распределенные базы данных как бы по использовать либо еще что то потому что выигрыш на 50 процентов это грубо говоря не знаю там по темпам развития ну на год до скажем ну мы постоянно смотрим конечно же что можно сделать как я отвечал уже мы продолжаем выделение сервисов из общего монолита тем самым разгружая нашу базу данных плюс ну там ведётся какая-то работа по reporting у то есть очень много нагрузки гинер reporting и все время например условный менеджер саратове мог положить нам всю систему запросив какой-то отчет там за полгода например и падала и производства подала все на свете соответственно моего reporting если останется может быть немного измененном виде или это будет какой-то do the lake или еще что-нибудь на то есть мы превентивные меры какие-то делаем чтобы так сильно не убрать не упираться спасибо большое к сожалению у нас не осталось больше времени на вопросы евгений скажите пожалуйста вам запомнился какой-то вопрос какой то вопрос вам понравился больше всего даст спасибо спасибо вот следующий сервисом"
}