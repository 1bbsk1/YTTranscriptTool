{
  "video_id": "8HUpveKdsgs",
  "channel": "HighLoadChannel",
  "title": "Человек-вектор и подъем конверсии: как мы сделали шаг к Federated Learning /А.Просветов, А.Семенова",
  "views": 277,
  "duration": 2686,
  "published": "2021-10-04T02:44:46-07:00",
  "text": "сначала пара слов о нас меня зовут артем просветов я работаю в ланит виду лекции высшей школе экономики и с нами коллега анастасия которая является нашим экспертом в компании клевер дейта и прекрасно разбирается во всех тонкостях данных которыми нам приходится заниматься также она преподает высшей школе экономики начать я хочу с контекста и рассказать пару слов про нашу компанию потому что как только будет понятно чем мы занимаемся станет очевидно с какими трудностями мы сталкиваемся наша компания занимается разработкой и развитием платформы game pe эта платформа позволяет строить сегменты объединить данные из различных источников и затем удобно оперировать этими данными например запускать на них модели машинного обучения или рекламные кампании средней секции в пирамидке представлена dense это платформа которая позволяет покупать продавать и обмениваться внешними данными и рамках этой платформы нам приходится работать с данными клик стримов наших партнеров мы помогаем им достичь достаточной степени и персонализации в данных а также оператора фискальных данных также подключены этой части поэтому в некоторых маркетинговых активностях можно использовать информацию о покупках пользователей в бирже данных dense присутствует достаточно большое количество информации количество поставщиков ежедневно растет аудитория этого сервиса больше 85 миллионов уникальных человек в день и мы естественно не можем предоставить нашим покупателям данных сыр и сырую информацию потому что она будет в персонализированном виде нести как может нести какой-то ущерб и в нашем докладе будет как раз рассказ о том как мы преодолели этот барьер и смогли достичь такого уровня и персонализации которые не стирает информацию о человеке но позволяет строить модели машинного обучения с высоким уровнем качества на этих де персонализированных данных и я передаю слово коллеги спасибо большое артем есть несколько видов задач но я сейчас расскажу про самые типичные это look-alike и вероятность совершения целевого действия на сайте наши клиенты фукава установлен темпе вот почему я буду про это дело рассказывать там вот чуть чуть пораньше у нас в оглавлении были указаны проблемы проблемы в наших задачах но не то чтобы это прямо сильно проблемы но это вещи которые вызвали у нас у самих вопрос а вот сейчас я про это расскажу поподробнее стандартное представление пользователя с помощью нашей таксономии выглядят приблизительно таким образом то есть чек представляется в виде вектора длины количество атрибутов нашей таксономии порядка 30 тысяч вот такой на каждого человека вектор длиной 30 тысяч и на каждом конкретном на период заданном месте стоит соответственно бинарная информация о том присутствует ли интерес или какой-то факт у человека мы этим пользуемся это в общем то работает хорошо это бывает полезно но тем ни менее вектор длины 30 тысяч ну этого человека почему именно 30000 потому что маркетологам очень важно сформировать быстро сегмент сегмент они формируют по каком-то интересу и если они могут задать простой интерес например аквариумным рыбкам тогда они выполняют простую операцию на всем сегменте этой бирже данных это интерпретируем а я информация и в этом ее плюс но она очень разрозненно и и и разреженная тасс но нужно понимать что строить ну что профиль исполняются весьма ребенка и у нас чересчур разреженные вектор получаются и строить на них модельки ну это вызывает определенные проблемы вот высокая размерность пространства представлений длинный вектор профиля это все про то же самое и мы хотим перейти к фидере тот людях в чем концепция этой это этого слова предположим есть две компании у каждой из компании есть свой набор данных о пользователях и какие-либо свои метки например метки о том что человек совершил целевое действие конверсию тем не менее компании ограничены по возможностям распространению информации об этих людях простой пример это больнице они не могут выносить за рамки своей больнице информацию о своих пациентах но понятно что машинное обучение если располагая достаточным количеством информации и если работает со всеми данными из всех больниц может достичь высокого уровня качества в своих моделях предсказатель ных и это принесет пользу всем как же преодолеть этот барьер для его преодолевания преодоление строится небольшой небольшая модель на стороне закрытого контура и синхронизирует информацию с общим сервером при том информация по сути не передается за рамки закрытого контура но передаются градиенты это изменения в весах модели градиенты уже не представляют из себя персонализированную информацию с ними можно работать и благодаря этому удается прийти к винвин результатом так а я все еще вернусь к нашим типичным задачам как мы раньше строили look like но при предыдущем представление пользователя мы на самом деле не на представление пользователей она клик стример то есть на последовательности посещений url страниц строили при помощи т.ф. эти фактора и серое логистическая регрессия некоторые вы зла и луковой к которым пользовались в общем-то это работало но если нужно было устроили поля сложная какие-то модели не только не логистическим регрессиями едины скажем так вот на каких данных мы это делали есть целевой класс предположим какое-то мы хотим найти людей похожих на этот целевой класс мы собираем коллега стрим по целевому классу и также ну в зависимости от требований баланс насти и мнение аналитика во сколько-то раз больше чем целевой сегмент мы собирались men случайных пользователей рунета и их клик stream то есть учились отличать целевых от случайных вот-тв и df означает подход в котором используется информация о частотности частотности интересом в рамках группы и насколько этот интерес часто встречается в рамках всего до pассвета да это очень ценно на самом деле замечание да и вторая задача это предсказание осуществления целевого действия как пример ну например осуществления подачи кредитные заявки на сайте банка если человек уже является клиентом банка но еще например не захотел взять кредит тогда строится задачу бинарной классификации где модель учиться отделять тех людей которые совершат целевое действие от тех людей которые не спешат целевое действие вот собственно на этих двух задачах мы и планировали проверять им ботинки потому что ну embedding это векторное представление с него никаких метрик не снимешь пока поверх него не построишь какую-либо модель модельку у которой уже можно снять метрики и оценить ее качество вот вернемся к перечислению но в наших так сомнительных мест собственно почему мы захотели им бединге потому что деньги решают все вот эти вот 4 вопросика которая перед нами стояли и про то как уже звучит задача построения maiden гав мы хотим построить некоторые некоторую модель вот здесь она мячиком обозначена которая по пользователю и точнее по его к экстриму про пересмотренным им страничкам строит векторное представление на период заданном пространстве ну точнее в пространстве на период заданные размерности вот то есть перед нами стояла такая задача и мы начали ее решать на каких данных ну у нас в костер джорни хранятся вот такие вот артефакты это ссылка с которой перешел пользователь время события наварро дентифик отаров ну некоторые заголовки там юзер агенты и тому подобное мы представляем пользователя в виде последовательности посещение орлов и дальше перед нами стоит вопрос какую парадигмы выбрать то есть ну как он в один где-то строить и расскажем вам несколько историй несколько наших эпох первый подход первое парадигма это было человек текст то есть давайте последовательность улов это будет предложение которое характеризует человека и далее в этой парадигме уже можно подбирать алгоритм но мир достаточно динамичен интернет так еще динамичнее не спешит вернуться ссылочка меняется поэтому нам нужен какой-то такой алгоритм который сможет предсказывать вектор даже для тех узлов которых еще нет словаре потому что наш предыдущий подход стояв айдиас но классический такой подход бы и celine очень хорошие уверенный в и celine one был основан на жестком словаре и у нас это не устраивает потому что чуть чуть там появится буковка m мобильная какая-нибудь версия и урал уже перестает быть викторе зуи моему положим узком условию вот поэтому выбрали но в качестве текстового повезла в текстовый пришла и модели фас текст с буквенными н граммами вот до решили то что наш пользователь это будет документ и каждая ссылка соответственно будет таким до для вас текста есть два вида скажем так обучения это бака ford escape from собственно это достаточно известная информация я немножко напомню про то что backwards он предсказывает контекст по слова с кипром предсказывает слова по контексту и для нас конечно с кипром выгоднее просто по нашей мотивации мы хотим предсказывать какое-то какой-то вектор по его окружению поэтому мы и выбрали обучение skip грамм ну представим параметры размер на 128 почему размерность 28 потому что так хорошо работает мы проверяли это следствие экспериментов эмпирические подобрали то что размерность 128 она во первых не огромные получается вектор вот достаточно такой компактный во вторых на нем хорошо показывают себя классификатора вот ну и количество н собственно м грамм то есть скольки буквенные сочетание мы смотрим равна 5 ну это уже действительно экспериментально было проведено несколько раз мы пробовали разные параметры вот и взяли повислой нам реализацию уже готовую коробочную от дженсен в общем-то нас вполне устроило значит как проходит векторизация после обучения уже как применяем модельку мы каждый url декорируем отдельно и далее уср дня вот без каких-то особенных сложностей у нас был построен вот такой вот бы и celine векторизации вернемся к тому что можно предсказывать какие-то url и которых не было в нашем словарики предположим у нас есть урал бла бла вот и его в словаре нет то есть модели его не видела однако урал бла модель видео и благодаря инграм можно викторе заводь даже тот уровень которого в словаре не было ну нам это очень очень выгодно потому что ну как подсказывает опыт если с точки зрения букв ура или похоже то есть надежда на то что их контент тоже будет похож да какие мы заметили плюсе ну реализация джон симм очень удобно очень у нее много есть каких-то коробочных опций которые нам подходят и нам нравятся вот и у нас повысились метрики качества как понимать фразу метрики качества выше мы просто строили классификаторы поверх df и дев преобразования и строили классификаторы поверх фастекс преобразования и строили ну и сравнивали их метрики вот прошу обратить внимание на то как поднялся recall для целевого класса это вот прям очень заметно то что мы можем предсказывать те ну то есть мы работаем не на жестком словаре на жестком словаре у нас вот в 4 раза ниже было качество чем на таком гибком грамма вам представление и получается что мы в состоянии передать уже не вектор размера 30000 бинарном представлении до намного более компактный вектор размерности сто двадцать восемь цифр которые характеризуют человека практически в той же самой полной мере как и сырые данные на этом сжатом представление можно натренировать модель которая по метрикам качество весьма близко к модели на основе сырых данных это нас уже воодушевило да то есть вот здесь мы видим казалось в rokovoko f1 мере не такой вот прямо у алла эффекта от текстовых им берингов ну во-первых это борис line mb денги во вторых дело в том что наша цель была построить алгоритм который работает не хуже как минимум вот мы видим что не хуже и нас это очень радует значит мы отметили себе то что человек вполне себе текст вот да ну помимо этого нам хотелось бы проэкспериментировать еще как-нибудь возможно мы получим какие-то результаты и получше на каких-нибудь других парадигмах и подумали а что если человек не текста что если человек граф вот и сейчас я расскажу про вторую историю про граф его векторизации до что если важна структура переходов у нас наш граф составляет около 160 тысяч доменных узлов и эта цифра постоянно растет число простить вот постоянно растет из-за того что у нас подключается новые поставщики и это просто такая важная информация был выбран в качестве повислой на алгоритм ноту век собственно как и любое построение байден гав его цели отобразить соседние углы соседние узлы в графе таким образом чтобы в пространстве представление векторы объектов находились рядышком то есть берутся случайное блуждание на графе составляются последовательности и дальше обучаем skip грамм модель которая уже представляет нам mb денги так дальше как мы собственно говоря факторизуем да мы берем попарно переходы и взвешивая ой не взвешивая считаем среднее и возвращаем уже вектор пользователю вот таким вот образом что мы получили мы получили замечательную точность для уже известных доменов но и метрики полноты выше чем у классического подхода однако было и немало минусов который мы заметили в нашей реализации опять же повторюсь это такой bass line графа вы нельзя было рассчитать вектор для неизвестного от о кино то есть если у человека не было ни одного известного taken at a приходилось каким-то образом колдовать то есть ну тут мы уже все знаем тут выбирается липовым дули либо среднее либо ещё что-нибудь вот и показатель метрик классификатор в построенных поверх этих им в деньгах они были несколько ниже чем в нашем классическом toyfa idea of a + но грег подходе вот-вот можем увидеть конкретно как ниже в общем-то ну вот прикол для целевого класса поднялся например при сижу на полу ну вот такие результат видно что несмотря на то что метрик качество просели но они были получены и независимым образом совсем другим алгоритм совсем другим подходом значит часть информации конечно было потеряно однако можно не останавливаться на достигнутом и дело дальнейшие шаги да все верно итак человек текст или человек gras похоже практически на чек пароход вот но так мы и решили сделать обратиться к всеми известной любимой хорошо знакомые теореме кондорсе которую можно по простому сказать с конкатенировать как мы нашим бединге вот и перейдём к мы к формату текст сканка tener ну embedding текстовые с конкатенировать и с графом имбилдингом и получим этим размерности 256 вот ну у нас были определенные правила как мы поступаем когда там не находится в красивой модели каких-то токенов мы там брали среднее будет текстовая модель она нас вполне способно было предсказать хотя бы что-нибудь на основе клик стрима по человеку вот и получили результаты которые нас уже порадовались а всем это вот последняя строчка в первой строчке мы видим наш tf и дев плюс лаврик подход а в последней строчке тот же самый классификатор но поверх уже объединением рейтингов и мы видим в целом крайнюю положительный тренд то есть действительно нам удалось сохранить информацию пользуясь текстовыми и графам ими б деньгами у нас размерность уже всего 256 и выигрыш прям существенный с нашей точки зрения да как мы собственно говоря это проверили в жизни мы решали проблему холодного старта но не для рекомендательной модели онлайн кинотеатра а для модели которые подбирают способ подписки вот но тем ни менее нашим партнерам ну хотелось знать что-то о пользователе прежде чем его предлагайте какой-то тип подписки поверх наших mp1 гав была построена партнерами неглубокая полна связанная сеть и она себя показала в 20 раз лучше чем случайный предиктор то есть это был прямо успех это на объединением этим к в текстовых и графах вот здесь вот еще представлены графики того как себя ведут модели на ботинках на там вот где указаны на сырых данных это имеется в виду tf и дев плюс но грег подход вот и на mv этим как мы видим то что результаты прямо лучше и лучше или как минимум не хуже что нас тоже очень-очень радует кроме того хочется еще отметить что естественно градиентный busting на сырых данных работает лучше чем лог регрессии ну как и следовало бы ожидать но даже сила градиентного бустинга не способна достичь того результата который достигают с помощью embedding нас это уже вдохновила для дальнейших действий до для дальнейших действий и мы плавно переходим к третьей истории когда мы решили начать сначала то есть вот мы представляли человека только его последовательностью улов по которым он пришел вот и подумали что же еще можно использовать какую еще можно добавить информацию для того чтобы усилить наши результаты но напомню что у нас есть и вот прямо просится в руки информация о времени перехода ну значит мы поэтому решили проэкспериментировать с временными дельтами внутри сессии пользователя и построили третью историю это тайминг кадр так мы ее назвали вот мы и построили сект усик модель на основе декодером выставим слоев и энкодера и использовали к информации о ссылках добавили информацию о временную составляющей ну там не в чистом виде временная дельта несколько преобразована но она несет информацию о времени перехода внутри сессии вот немножечко про параметры модели но они все представлены на экране но главное в в рамках разговоры о бом песенках это то что наши размерность сократилось до 100 нам так стало удобно и это работает хорошо до как училась училась в общем-то хорошо училась и все у нас сошлось вот мы видим как ну очевидно то что у нас и красным обозначено тренировочная эволюция ну тренировочной выборки синим обозначены являются на тестовой выборке видно что они совместно растут или падают и очень интересный скачок который в принципе произошел на самом деле полон технической случайности из того что сервер отрубили такое то забываю но тем не менее видно что переобучение нет кривая обучения падает и это радует да как собственно производится векторизация при помощи домен кадра это используется только кодирующая часть и далее из выставим своя мы забираем соединенный вектор как мы тестировали далее я как раз представлю результаты сравнения теста наших имбилдингом на тайминг озере с уже mb деньгами объединенными графами плюс текстовыми здесь классический подходит мы уже сравнивать не будем вот будем смотреть только на наши три истории до 1 это как мы тестировались навык лайки мы взяли несколько сегментов целевых построили на них лука лайки на разных представлениях пользователей и сравнили вот на рекламной кампании автомобили x не могу рассказать каких именно мы получили вот такой вот невероятной прирост тайминг одера над граф текст подходом и аналогично на лак лайки для другой марки автомобилей также получили очень существенные скачки качества и вот f1 меры да я должна рассказать что означает f1 score calle экстрим и ав1 score реппер simple clicks trim это имеется ввиду целевой класс rio pro simple это наша случайная выборка но это и здесь для таргитая не а таргета предоставлен представлена отдельная f1 мера также построили look like для апартаментов высокого класса и тоже получили очень существенный прирост вот и look-alike для авиабилетов ну здесь когда мы видим целых 2 красных строчки хочется сказать что вы к like эта задача не четко и вообще вот но мы видим то что падение в качестве f1 скоро не очень сильная и то что мы сохранили в общем-то ориентировочную тот же уровень информации это во-первых ну во вторых задача look-alike для авиабилетов но даже на моем опыте могу сказать то что ну оно далеко не всегда выстреливает вот то есть в среднем по лук лайком не по границе опалу к лайком мы получили очень существенные приросты и для рака у к и для f 1 скоро pda каждого класса мы в отдельности но и вместе очевидно тоже это просто была победа теперь я расскажу конкретно про задачку на которые мы тестировали для предсказания осуществления целевого действия на сайте у одного нашего клиента банка есть ну была задача мы строили модель осуществления подачи кредитной заявки на сайте среди уже клиентов этого банка вот и получается у нас и раньше модель работала хорошо и нас это устраивало и банка ты устраивала на новых им митингах на основе то mencoder а ну просто модель выстрелила невероятно насколько поднялись метрики я просто сначала не поверила конечно сразу появилась гипотеза что появил у нас переобучение поэтому есть необходимость конечно проверять все эти эксперименты на практике на конкретных рекламных кампаниях и проводить аб-тестирование это точно вот ну мы пробовали наши граф текст как минимум в реальную задачах и про это уже рассказывали вводят в дальнейшем мы собираемся в нашем тайминг о даре поработать ну поэкспериментировать над возможно другими представлениями временной составляющие void ну и мы не могли себе задать вопрос работает ли в принципе подходим в деньгами на других данных потому что я то рассказывала про клик stream а у нас еще есть это оператора фискальных данных напомним что такое оператора фискальных данных и как собственно говоря выглядят их данные они выглядят таким образом это чеки онлайн-покупок вот ну и нам конечно не хочется передавать в сыром виде ну и не то чтобы нам не хочется это нельзя передавать сыром виде такие данные поэтому их нужно где персонализировать и поэтому в том числе для нас очень важно было построить mb денги на о ft у нас есть уже текущей pipeline которые надо сказать работает хорошо и вот здесь есть ссылочки на статью и на наше предыдущее выступление на холоде когда мы рассказывали как мы построили систему которая о названивай атрибута строить модель бинарной классификации которая определяет интерес или факт владения или какой-то факт у покупателя вот но напомним такой вектор имеет длину 30 тысяч он разрежет ну и в общем начнем сначала да какие у нас были проблемой но и мы не хотим так много информации персональное передавать точнее мы хотим иметь еще опцию передавать некоторые mb ding основанные на данных f d до менее разреженное представление и еще менее интерпретируем мы провели эксперименты на этот счет и даже привели пилот с другим нашим партнерам с другим онлайн телевидением и тоже решали задачу холодного старта рекомендательные системы но уже не найти подписки на собственно говоря на контент вот и наш до успех собственно он мы теперь ждем пресс-релиз мы пока особенные детали не будем рассказывать про то какие именно им байден где мы построили на данных в той но провели весьма разнообразные эксперименты с алгоритмами вот собственно результаты этого пилота в общем виде вот можно свести в такую табличку когда мы видим то что несмотря на то что точность модели упала ну неточность 40 ук метрика упала на 4 процента метрики рекомендательной модели am возросли очень существенно вот и это для нас очень ценно напомним и саму рекомендую не строили мы только передали им в тенге которые являлись ранее базы до падение метрики на бинарной классификации естественно были связаны с тем что были добавлены новые признаки это наша им бединге в то же время не проводилось тщательного отбора признаков а так как признаков стало существенно больше конечно качество чуть чуть просело то есть этого можно было ожидать радует то что качество именно рекомендательной модели n 10 in 10 лет 50 существенно выросли до собственно таким образом каждый из идентификаторов которые имеют какую-то информацию с собой попавшие в нашу биржу может быть представлен в некоторым пространстве имбой зинков и персональная его информация будет полностью затерто да поэтому плавно переходим к выводам очень важно серьезно подходить к процессу тренировки моделей и понятно что чем лучше будет предоставлены данные для моделей тем лучше качеством у дает удастся достигнуть а лучше данные удается передать модели за счет м берингов поэтому хорошо подготовленные бединге это залог успеха во многих задачах и важно в процессе формированием викингов учитывать максимум информации и добавлять ее например из даже не особо ожидаемых предсказуемых источников например информация во времени между событиями естественно не стоит останавливаться на достигнутом и всегда можно сделать что-то еще лучше чем то что есть на текущий момент ну да а я от себя посоветую то что нет какого-то общего рецепта нужно уметь забывать то что делал начиная с чистого листа или вспоминается то что делал комбинировались все подходы позволяет каким-то алгоритмом сливаться в общем я вам посоветую экспериментировать подходить творчески к тому что вы делаете вот успехов вам будьте открыты давайте раз по планируем громко это был первый доклад про машин лёнинг сегодня все жэки это было очень круто я считаю большое спасибо нашим спикером давайте пожалуйста зададим 3 вопросы из зала потому что все таки здесь освещались серьезные вопросы и вот в таком формате со сцены к вам наверно это будет не самый эффективный способ поэтому лучше всего будет в кулуарах поэтому давайте ваши три вопроса и зала добрый день спасибо за доклад у меня один вопрос поделены на две части а первый каким образом вы не фиксируете пользователю вот вы говорите что вы сопоставляйте орлы с пользователем дальше проводите некие операции и вот каким образом вы едите фиксируете пользователи и самое главное в этом говорить о что есть cookies и как быть через полгода там купили sarl и так далее вот и второй вопрос как вы проверяли качество look-alike а то есть вы посчитали вы нам показали цифры это ваши данные ваша модель и вы получили ваши цифры а насколько это действительно качественный look-alike как можно оценить спасибо спасибо за вопрос явно чувствуется что человек подкован в теме мы все ожидаем с нетерпением когда отключится механизм с at party cookies потому что именно благодаря этому механизму дается идентифицировать людей в клик стриме действительно когда этот механизм будет отключен придется перейти на альтернативы которые уже разработаны и со стороны google естественно это будет нет то же самое будет пространство для маневра и будет возможность создать что то новое но для данных life д у нас твердо идентификатор на основе и телефонов до электронных почти до электронных почт а вот то то чтобы разработали его можно будет как то быстро при перенести на вот новые технологии google одну из там например те же блоки до технология google например flash да она снова использует аналогично парадигму фидере ты пленник она на стороне клиента строит не интерпретируемый вектор который характеризует интересы этого человека и уже на основе этого не интерпретируем его вектора будут запускаться рекламной кампании признаться специалисты нас уверяют что может идентифицировать человека и без эти cookies потому что в параметрах браузера достаточно лакун и таких закоулков в которых можно спрятать идентификаторы и обойти это ограничение вы имеете ввиду fingerprint он запрещал блокирует в те же приложение за использование fingerprint и удаляет из appstore тем не менее некоторые маневры все равно возможен до уже в этих ограниченных условиях с этим согласен ситуация несколько действительно напряженной и люди в ожидании того что как поменяется мир с новой эпохой мы проводили исследования того что было сделано со стороны угла и очень удивились тому что они шли путями похожими на нас путь то есть они точно также сначала строили tf и дефлектор азер и на основе него тренировали аналог mb деньгах то есть там был вектор пользователя который характеризует его интересы мне кажется что мы прошли дальше потому что наши модели более тяжелые и нет возможности такие модели и гонять на стороне пользователя однако хочется верить что качество у нас будет выше теперь касательно вопрос того что да это метрики который мы приводили это оффлайн метрики которые мы замерили у нас естественно что по этим результатам были запущены и рекламной кампании но у нас есть такая особенность бизнеса что есть некоторый разрыв коммуникации между специалистами которые занимаются данными и маркетологами которые смотрят на результаты рекламной кампании мы знаем скорее окраску с которой маркетологи к нам приходят по результату проведенных компанией и если они довольны то не приходит еще раз если они не довольны то они уходят и дверь закрывается то есть в данном случае мы видели что все остались довольными спасибо да ну еще я также хотела добавить то что на самом деле для того чтобы что-то утверждать по поводу им ботинков мы даже подготовили дизайн и эксперимента у нас ну все документировано и потому что чисто навык лайки на самом деле делать какие-то выводы но очень тяжело потому что сама по себе задача нечеткая ну что такое отделе красное от всех цветов да там что же есть красный в конце то концов но большинство метрик которые мы приводили это метрики бинарных классификаторов просто на отложенной выборки так спасибо чем добрый день спасибо за доклад хотел спросить вопрос по поводу того как сформулировано была задача вы несколько раз приводили метрики которые вы посчитали критериями успеха это метрики бинарной классификации при этом выучили на данных использовать super white подходы вот вас текст во рту век все заперлась не пробовали вы вы делать свой контент madeline который бы сразу учился на торги который бы вы достигнуть хотели ну в таком виде бы бединге получились скорее всего точно лучше чем вонсо 1 ст miner штуки да спасибо за вопрос и тогда дизельное бединге буду были бы идеальными но нам бы нужно было в таком случае их строить каждый раз для каждого клиента вот каждую задачу а нам хотелось бы сделать такой вектор универсальный который был бы некоторым полуфабрикатам который мы могли бы передать на сторону заказчика и это сантис ты быстро бы подставили этим бединге в свои pipeline и получили бы результаты вам прирост качества их моделях ну и замерили бы прирост качества новых моделях то есть у нас на самом деле нашем поединке они годятся не только для лука лайков и целевые и осуществления целевых действий они также могут решать проблему холодного старта рекомендательных модели на совершенно других данных то есть мы не можем указать конкретную конечную задачу чтобы взять и единый pipeline да сейчас я как раз на формулирую вопрос с чего начался с того как стояла задача и задача у нас действительно стояла таким образом что мы должны найти де персонализированное представление человека в таком виде которые можно было бы использовать в широком спектре прикладных задач понятно спасибо спасибо"
}