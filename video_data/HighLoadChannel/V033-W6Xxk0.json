{
  "video_id": "V033-W6Xxk0",
  "channel": "HighLoadChannel",
  "title": "BARSiC — асинхронная репликация и консенсус для 70 баз данных / Григорий Бутейко (VK, ВКонтакте)",
  "views": 318,
  "duration": 2580,
  "published": "2024-10-29T03:08:40-07:00",
  "text": "друзья под Ваши аплодисменты Григорий Бутейко ВК Да риша привет Да давай давай чек звук меня слышно так пока нет Раз 2Т Барсик Барсик Барсик Барсик Барсик Почему Почему решили назвать Барсик вот расшифровка будет в середине это просто сокращение всё спойлер а в конце вы получите суперприз всё Гриш пожалуйста начинай так Здравствуйте меня зовут Гриша Бутейко Я работаю в команде инфраструктуры ВКонтакте и расскажу про Барсик - это система репликации для как здесь написано в заголовке семидесяти баз данных значит расскажу о том как в контакте встроена репликация дальше Зачем Понадобился Барсик как он работает как он устроен и вторую половину доклада посвящу Рони распределенных систем и каким образом мы Тестируем Барсик с помощью п модели то есть код на императивно языке тестируется с помощью п модели Ну вот ядро в контакте нарисовано это приложен без состояния и базы данных которые хранят состояние база данных поделены на кластеры и вот они специализированый больше или меньше потому что часть забросили часть новых написали специализация Что делает она помогает выбрать структуру данных в базе данных прямо под задачу то есть база данных лайков она совершенно не так устроена как база данных юзеров или чатов и там есть общие блоки естественно но в целом это именно код который прямо жёстко оптимизирован некоторые годами оптимизировали под конкретную структуру данных и даже с учётом статистики То есть если там мы знаем какое-то распределение Как там лайки ставятся то мы можем прямо структуру данных подобрать вот прямо под эту задачу и по оценкам Это позволяет работать то есть если бы мы взяли просто какую-то одну базу данных то нам бы понадобилось в 5 10 20 то есть там в разных кластерах нам потребовалось значительно больше железа ну кластеры шардирование и вот собственно он Ну шар состоит из реплик фиолето это типа разные машины вот такие границы машин и модель единственный мастер соответственно приложени отправляет пишущие запросы на Мастера значит откуда они потом реплицируемый просто прописаны статически в конфигурации соответственно система очень простая база данных работают в модели rsm То есть просто есть журнал событий Все изменения пишутся в виде этого журнала они пишутся прямо в файл То есть тут как бы очень лоу тех э и соответственно на Мастере они пишутся файл А на реплике они как бы читаются с конца файла скажем так вот и попадают изменения с мастера на реплика с помощью сервиса репликации то есть есть отдельный сервис Он следит за кончиком файла берёт оттуда журнал события журнала и копирует на реплики э и здесь интересное свойство - это оптимистичное исполнение То есть у нас если репликатор скажем тормозит или отказывает это никак не затрагивает мастер он продолжает как бы бежать вперёд продолжает писать события также если мастеру нужно сделать какую-то долгую работу вот здесь снимок рассмотрен то есть ему нужно свою состояние которое накопилось какой-то файл записать чтобы не сначала истории потом применять Да события журнала А как бы с какого-то места начать то он просто форка ется он как бы копирует своё состояние и РК процесс ребёнок он на спокойненько что-то делает А мастер опять ничего не ждёт мастер как бы продолжает бежать вперёд теперь Вот рассмотрим отказ мастера для начала собственно репликатор всё равно что мастер отказал просто в конец файла перестали события писаться Ну мало или может просто их нет а приложени они видят что мастер отказал но приложени много то есть они там раскиданы по инфраструктуре их там десятки сотни тысяч поэтому для мониторинга есть вот собственно сервис мониторинга специальный отдельный который следит за тем Работают ли мастера и когда сервис мониторинга видит что мастер отказал соответственно сервис конфигурации раскидывает просто новые конфиги и вот синенькие стрелочки - это ти пароли а синенькие точечки это вот как бы ну роль мастера Да так показано условно то есть приложен должна знать кто мастер сама база данных должна знать кто мастер и ещё репликатор должен знать кто мастер соответственно при раскидывают гонки тупо то есть вот сейчас проснулся данных мастером а она сама себя уже не считает и вот вторая проблема Это что там какая-то ну наоборот да два мастера есть это тоже проблема и проблема она ещё в том что потом когда всё это придёт в норму у нас окажется что вот вторая машина мастера она записала какие-то события которые придётся откатить то есть потому что просто Конфликт это как бы форк истории и репликатор просто отрежет этот хвост соответственно эти события пропадут то есть у нас нет точки комита у нас нет такой таких команд которые мы уверены что не откат и соответственно в таких базах данных мы не можем хранить деньги Например Ну и вообще всё что надёжное там какие-то индексы обновлять это проблема значит Ну вот теперь подытожим плюсы системы репликации что она крайне простая очень надёжная здесь никто никого не ждт то есть время ответа крайне предсказуемо то есть ну тако типа можно считать В общем что-то вот такого типа да то есть как бы простое и надёжное но как бы у ней есть минусы которые мы хотим как раз исправить во-первых нам не нравится система мониторинга Ну во-первых это отдельная компонента которая сама может отказывать плюс там есть всякие моменты что система мониторинга она как бы тоже должна быть распределённая в общем это добавляет сложность затем переключение то есть переключает традиционно у нас условно админ То есть он там должен посмотреть на алер и принять решение Или это может делать скрипт Ну в любом случае здесь время реакции большое И если отказывает много шардов Например целый дата-центр там не знаю 10.000 шардов то это проблема вот и Значит мы хотим без ошибок при переключении ролей обойтись без гонок Да и мы хотим точку комита чтобы в таких базах хранить деньги и всякие как бы другие надёжные штуки вот ну для В общем реплицировать и соответственно консенсус много их там полно реализации полно самих алгоритмов хорошо область изучена вот Поэтому собственно для начала мы пытаемся собственно коробочное решение Да какое-то выбрать но коробочное решение нам не подходит по нескольким причинам вот самая Основная это оптимистичное исполнение то есть классический консенсус он как бы как работает туда накидывает он их упорядочивает комитет и присылает какие-то события что вот команда закона но в таком режиме мы работать не можем То есть у нас Мы обязаны делать именно оптимистично исполнени ничего не ждать ина как будет слишком медленно вторая прина у на очень многое с двумя репликами а не с тремя и добавлять третью реплику скажем так непрактично да то есть это просто дорого и у нас есть дополнительные Протоколы которые тоже из коробки нам никто не даст это во-первых нужно старые старую систему репликации перевести на новую безшовна И ещё нам нужно делать сплиты тоже безшовна то есть Сплит - это когда вот было 100 шардов нагрузка растёт и нам нужно эти 100 шардов поделить То есть каждый шарт поделить на два и соответственно удвоить мощность Ну то есть из коробки мы не можем взять Поэтому нам приходится что-то как бы самим делать Вот собственно Барсик нарисован с его расшифровкой ну это просто собственно сокращение от синхронная система репликации журнала событий с простым интерфейсом к базе данных то есть простой интерфейс Нам нужен что у нас много баз данных они пожи но тем неменее интегрироваться и чтобы сами базы данных все 70 не нужно было там радикально менять Вот значит как бы нарисую старую картинку типа как было до Барсика и соответственно первое что мы делаем при проектировании это рисуем котика Естественно да вот Ну на самом деле мы вот так вот переставляем чуть-чуть наши компоненты То есть раньше база данных писала файл А репликатор оттуда читал Теперь мы делаем полноценный протокол между базами данных и уже барсиком скажем так и файлы журналы пишут только барсики соответственно роли теперь поскольку барсики там каким-то образом имеют консенсус они соответственно выбираются басиками и дальше по опять же командному протоколу доставляются до банных банных став приложен для роле поэтому он может надолго там отказывать выключаться и ничего страшного потому что собственно именно конфигурации не так часто меняем и у нас появляется возможность если мы сделаем в консенсус комит также по командному интерфейсу доставлять их до баз данных и от баз данных до приложения значит там вот нарисован тоже что Барсик по-прежнему другой процесс Ну основной плюс это вот база данных c+ основном поэтому как бы повреждение памяти - это реальность возможность Да его и хочется от этого защищаться плюс Ну там я сильно не буду останавливаться в общем из минусов мы получаем тоже некоторые гонки ролей Ну В каком смысле у нас роль Барсика и роль собственно базы данных Они немного могут отставать друг от друга тоже но э проблема локальна мы е чиним специальным образом теперь вот собственно мы поняли что нам нужен консенсус и у нас три варианта как бы как его Где его взять можем внешне сервисом воспользоваться это в принципе популярное решение взять там поставить зупер и хранить там что-то либо можем взять какую-то библиотеку с ней с интегрироваться Ну и последнее как бы решение самое такое хардкорное Да и печальное последнее Да в нашем списке это сам уже писать Сам кон вот катинка как сервисом консенсуса пользуются То есть все данные вот синенькие - это данные жёлтенькие - это метаданные данные Мы через сервис консенсуса не можем прокачать он просто ну не выдержит таких нагрузок поэтому мы вот для каждого кусочка данных пишем какие-то метаданные в консенсус и другая реплика соответственно она замечает что вот в зупе в во внешнем сервисе есть эти метаданные и просто идёт в тот сер в тот в ту реплику где есть сами данные и соответственно их скачивает но проблема с внешним сервисом такая что очень сложно обеспечить синхронизацию метаданных и данных вот в примере вот что что здесь произошло здесь реплика которая эти данные как бы породила она там Крэш нулась или у ней диск сгорел и данные пропали И теперь у нас в метаданные есть а данные взять не откуда то есть у нас как бы происходит расхождение данных и метаданных когда мы пытаемся внешним сервисом пользоваться Ну и Собственно как решать можем сделать локально прежде чем в сервис класть спат от перезагрузок но не спат при если машина там полностью сгорит исчезнет поэтому Ну можно что сделать можно на кво Да на 2/3 машинах данные сохранить и потом уже записать в сервис метаданные но если мы заметим что у нас уже есть птуп чтобы эти данными обмениваться у нас уже есть кворум то как бы собственно не жирно ли нам ещё какой-то внешний сервис Да иметь Тем более что в общем-то этот сервис он ещё и денег стоит то есть нам придётся на какое-то количество машин там на сотню там наших машин ещё поставить Ну хорошо если 10 Да а то и 50 Да этих внешних сервисов Поэтому вот внешним сервисом он нам не подходит теперь библиотеки библиотеки собственно они нам не подходят по той же причине который я вот это я просто повторил слайд по которому нам коробочное решение не подходило то есть у нас вот из-за вот этой специфики Да именно работы баз данных необычной нам как бы и библиотеки не так просто найти и ещё если вот пошарить по всяким орным реализация то можно заметить что тестирование там ну как бы часто оставляет желать лучшего и мы боимся их использовать если можно так выразиться значит вторая половина будет про тестирование как раз вот собственно поговорим Ну и в итоге собственно приходим к тому что придётся нам и консенсус тоже написать И для этого надо алгоритм выбрать Ну у некоторых есть ассоциация консенсус равно raft Значит мы хотим проще чем raft потому что мы знаем есть как бы есть такие варианты вот em replication мы выбрали это алгоритм похожий на raft но проще так я о нём особо ничего не буду говорить так на свойствах остановлюсь чуть-чуть потому что конкретно Вот его прям детали нам не сильно сейчас интересны там тоже есть раунды есть фаза выборов если они успешны будет фаза коммитов в этом раунде Но в отличие от рафта у нас как бы тот кто будет мастером в раунде он просто предопределённость если мы хотим какую-то реплику сделать мастером по скажем так внешним причинам допустим она просто сейчас менее Занята то есть какая-то реплика там пишет например какой-нибудь снимок и она занята Мы хотим в этот момент мастера передать определённое реплики и мы просто организуем выборы под Да всё очень просто ну и критерием комита у нас является тоже то что данные сохранились надёжно на двух машинах из трёх Значит теперь собственно вопрос Если мы это написали то как мы будем это тестировать Откуда мы знаем что это работает и вот как мы можем вообще распределённые системы тестировать ну во-первых нужно компоненты протестировать всякие типа там диск сеть да ниче ничего особо сложного нет более сложные компоненты со сложными состояниями мы можем поф и ещё мы можем потом готовую систему можем интеграционное тестировать например псом Но мы вот джепсон не используем у нас там свой инструмент Ну условно говоря идея понятна то есть мы развернёт как-то закрывать там виртуальный сетевый адаптеры между этими компонентами шить их в общем будем тестировать но в барке мы используем вот подход то есть мы не отказываемся от первых трх мы их все используем но мы ещ используем ла модель сейчас собственно поговорю Зачем Ну вот распределённая система вот снизу наблюдатель и если наблюдатель хочет её тестировать ему нужно сформулировать варианты каким-то образом как ему их сформулировать То есть пока собственно состояние до него долетит оно уже поменялось и прилетает оно не одновременно есть сва варианты и вторая проблема вот ну это фактически интеграционное тестирование Да нарисовано вторая проблема - Это то что у нас есть не детерминизм событий То есть у нас очень много порядков вот даже здесь на рисунке у нас машины общаются с сетевыми пакетами и есть часы и Например если мы в тестах Тестируем у нас всё классно все тесты проходят Но на самом деле у нас просто сетевой пакет успевает всегда прилететь до таймера Ну или наоборот А в проде Когда мы это выкатили то выясняется что события пришли чуть в другом порядке и всё у нас как бы кши соответственно Вот как нам вообще действовать Ну тоже да как бы ничего сильно нового наверное не открою нужно распределён ную систему представить как конечный автомат с событиями соответственно события будут Ну как бы очевидно всем Какие сеть таймер там всякие дисковые команды когда исполняются Ну также вот можно разрушать машину разрушать дата-центры В общем разрушать там вселенную неважно В общем мы всё это хотим тестировать тестировать Мы хотим по компонентной ды от руки тут дальше пойдут немножко не успел э значит вот это типа Барсик у него есть консенсус модуль консенсуса у него есть сеть диск и таймер и вот вот эти компоненты для консенсуса они все являются они синхронные и в них может произойти событие неожиданно соответственно чтобы тестировать мы делаем диспетчер то есть диспетчер - это окружение для той компоненты которую мы тестировать собираемся вот для консенсуса он сам код консенсуса абсолютно Такой же А вот код диспетчера он заменяет полностью все остальные компоненты то есть сети нет есть просто буферы в памяти диска Нет есть буферы в памяти и часов нет но диспетчер в любой момент может сгенерировать событие таймаута когда он захочет на самом деле диспетчер выглядит вот так да для консенсуса То есть он содержит всю распределенную систему все как бы три консенсуса для П И вот если там они друг другу какие-то события сетевые например отправили то они лежат у диспетчера в буфере и диспетчер Может взять их и по своему выбору в каком-то порядке доставить причём он может это делать предсказуемо сколько угодно раз этот тест прогони всегда будет строго одинаковый то в Диспетчере нет ничего асинхронного и возникает вопрос хорошо это Мы научились делать А как тамм теперь выбрать событие Как выбрать порядок Ну в при понятно НГ применить вы придумать сначала случайные события применить их посмотреть в какие куски кода Мы заходим дальше модифицировать эти события и пытаться зайти во все куски кода для некоторых систем типа парсеров это вообще замечательно работает то есть зашли во все Закоулки парсера типа протестировали парсер замечательно А в консенсус другое дело то есть в консенсус кода мало а состояние очень много поэтому Фазер очень быстро зайдёт во все кусочки кода и после этого он просто не будет понимать инно чтобы с этим бороться применяют вот покрытие состояния То есть можно какие-то значения переменных не только фаеру сказать что он там в какие-то куски кода заходит Ну и сказать вот Фазер смотри тут у нас там какая-то переменная например номер раунда Вот она поменялась значит ты молодец нашл новое события это всё можно делать но вот на практике когда мы соответственно реализовывали консенсус Барсика то постоянный вопрос Какую часть возможного фаер нашл то есть вот у нас например файзер очень долго не мог зайти в кусок кода вот просто не мог и всё То есть он нам работает сидит на мощной тачке он там пыхтит неделю работает кусок кода не заходит а этот кусок кода нужен Мы точно знаем что он нужен и наоборот вот он Сидел сидел и вдруг оп он нашёл что-то интересное То есть фанг он хорош но он очень слабо предсказуем и возникает вопрос А может быть мы просто все состояния обойдём Ну вот это Идея хорошая просто вопрос сразу а как узнать что такое все состояния То есть у нас нужна нам какая-то модель причём модель нужна именно нашего кода значит вот сразу как бы мы читали статью какую-то в статье какой-то протокол описан и мы его реализовали ну естественно мы сделали изменение как практически вот скажем в статье всё состояние журнала оно доставляло одним событием то есть так оп типа пакетик прилетел и весь журнал там все 100 теб досталось А в реализации нет в реализации журнал приходит по куска и поэтому в реализации есть новые состояния которых не было в статье соответственно э это другой протокол строго говоря И если мы делаем какие-то изменения или просто два раза реализуем то мы каждый раз делаем новый протокол Поэтому чтобы вообще в принципе утверждать что мы реализовали какую-то статью мы реализовали какой-то протокол мы вот ориентируемся на инварианты То есть если структуры данных условно говоря похоже максимально высокоуровневые и инварианты тоже одинаковые или похожи то Это один и тот же протокол и Значит мы делаем таким образом ла модель очень близко к коду как только можем То есть например вот состояние мы слизу просто одинаково то есть в ла модели есть журнал это просто в памяти Да структура данных ла А например в коде У нас есть журнал в виде какой-то структуры там она там хранит это файле она там бьёт какие-то кусочки неважно Пока Мы это можем стерилизовать в один вид можем сравнить эти состояния и дальше что мы делаем значит Да эта модель она реализует статью потому что в этой модели такие же и варианты соответственно мы можем симулятор Вла велеть эту модель просили то есть симулятор пройдёт найдёт все состояния Ну он как бы долго может быть будет работать там день неделю и так далее но тем не менее он пройдёт и построит весь Граф всего что возможно в этой модели после этого что нам нужно нам нужно сгенерировать тесты То есть как у нас модель ла выходит с пустого состояния также у нас и код начинает в пустом состоянии и дальше из пустого состояния мы проходим по всему этому графу так чтобы посетить все переходы посетить все состояния и при каждом переходе мы сравниваем что сериализация состояния модели ла и сериализация состояния кода она просто ну обязано быть одинакова соответственно Граф состояния кода он просто стопроцентно совпадает с графом состояния модели ла за исключением небольшого как бы нюанса что если у нас в коде есть дополнительные переходы то соответственно модель нам никак не поможет их найти То есть например переход в бак да м и поэтому мы продолжаем и фази тоже наш консенсус чтобы вот находить вот такие вот как бы переходы в новые в неправильные состояние поскольку у нас всё равно там модель диспетчер У нас есть Фазер то мы это для нас бесплатно и ещё вот я говорил про все состояния но все у нас есть как бы неограниченные переменные То есть у нас есть номер раунда это вообще натуральное число какое угодно может быть поэтому на практике мы когда симулятор ла запускаем мы лимитирует есть мы говорим ты там дальше определённого раунда не ходи и если он там п там сем раундов исследовал ну чем больше раундов тем дольше ждать соответственно и он не нашл никаких проблем и в нашем коде не нашлось никаких пром то мы используя там некоторую интуицию можем сказать Ну раз там за 7 раундов никаких проблем не нашлось А мы перебрали просто вс возможное Что может быть то значит наверное и за де Раундов не найдётся и дальше мы можем это на каком-то облаке там поставить на 512 ядрах это дело на месяц считаться для оди там условно говоря чего-то и просто там заняться своей работой Если что мы через месяц узнаем что и для О как бы у нас проблем Нет скорее всего вотт вот результат применения моделе для консенсуса для всего лишь трёх реплик и там трёх раундов у нас уже 10 миллионов состояний э вот Фер он буквально обходит 1% То есть он нам проработав месяц он посетил всего 1% состояния и по если использовать не только покрытие кода но покрытие данных то там больше но у меня вот цифры здесь нету ну всё равно там будет несколько процентов э Значит так что-то у меня написано да тесты работают несколько минут то есть тесты быстро прогоняют можно прямо при каждом при каждом как бы запуске при каждом коммите проверять И ещё если вот такая такое мощное тестирование как тестирование ла моделью то можно заниматься оптимизация переписывания как только код проходит вот эти вот тесты соответственно больше с ним не возникает никаких проблем чаще всего Ну и вот собственно заключение то есть мы начали с того что взяли вот традиционную систему репликации хотели улучшить свойства сделать там автопереключение мастера а в результате мы получили Барсик То есть это компонента которая любую базу данных делает отказоустойчивый если эта база работает в модели соответственно интен рованием и тестировать вот таким подходом Это довольно просто и недорого то есть мы по оценкам потратили условно 10% времени команды на это всё получив очень большую уверенность в компонентах Барсика в консенсус и мы ещё там другие тоже некоторые конечные автоматы тоже Тестируем таким подходом Ну они менее интересны потому что консе хотел сегодня рассказать э у нас там есть стенд на котором кластер Барсика развёрнуты не знаю там ребята допит его то можно будет подойти и посмотреть как он работает по нажимать кнопочки отказов покша что-нибудь ээ я значит был Гриша Бутейко из команды инфраструктуры ВКонтакте Спасибо всё можно позировать Спасибо Гриш а очень классно у кого вопросы есть поднимайте руки сейчас у нас есть в принципе минут 10 то что пообсуждали есть чат зала Да QR код вот появился туда тоже можно задавать вопросы я их буду зачитывать в режиме интервью Гриш у тебя будет супер задача выбирать лучший вопрос что прям реально Можно прийти и сломать кластер на стенде ВК Ну на маках он работает первый день поэтому там некоторые ещё проблемы не связаны Да конкретно с барсиком уже поэтому задава микрофон Дава с кон нанм Вставайте Пожалуйста задавайте тогда добрый день это универсальное решение или для какой-то определённой су Если бы это было каким-то образом другим реализовано на основе там cdc или ещ чего-то на основе журналов это у каждой базы данных своё смн у тех Но вот у нас есть интерфейс баз данных это как бы сотны интерфейс и там условно говоря есть пять команд Барсик когда выбирает роль для базы Он присылает команду CH соответственно Смени роль дальше если у базы роль мастер Она имеет право генерировать события журнала и присылать в Барсик если Барсик Да Барсик будет присылать коты периодически То есть он асинхронный как бы там база данных ну это обсуждали она бежит вперёд она пишет как бы события журнала Барсик всё это комитет И периодически ей говорит что вот до этого ничего не откатится соответственно если произойдёт откат то Барсик скажет сделай реверд до определённой позиции и соответственно если база потеряет если Барсик потеряет роль то он скажет базе ты теперь не мастер больше пожалуйста Не генерируем события журнала то есть мы это скажем так у нас не прям совсем гомогенные база они там действительно немножко разные Ну вот если удаётся вот эти пять команд реализовать в базе то всё она будет работать с барсиком соответственно у нас там есть на основе слай решение то есть когда sqlite становится как бы кластерным лайтом Как пример ES прям прям SQ Да просто А есть база данных на файлах просто вот конечно да то есть большинство именно работают то есть вот э вот оптимизация где там я рисовал всяких дельфинов жирафи ков вот чтобы как бы срывать листья так же ловко как жираф там придётся именно вот залезть прямо в биты и Так развёрнуто давайте у кого микрофон Давайте микрофон берите Вставайте в чат Пишите кто смотрит онлан тоже пишите в чат буду тоже каким способом вы логи парте Ну то есть простом мы не парсим логи Зачем нам парсить логи данных вы как доста вот когда база данных генерирует события она посылает барсику команду в этой команде написаны байты которые являтся собственно событием пакети ет надёжно хранит вычисляет хэши доставляет до реплик и на репликах уже в свою очередь базе данных говорит примени вот эти байты как как команду база их парсит и соответственно применяет то есть база она знает про команды а для Барсика все команды это просто байты он их никак не интерпретирует он их только пакети ет Ну то есть какой-нибудь для барси можно применить вероятно прямо нельзя применить которая изначально не является кластерной то есть вот понятно да то есть это ну как бы в существующее когда там есть свой журнал у базы то нужно будет что-то модифицировать отключать кластеризации Да и подключать заново всё это ну типа да то есть если там есть Ну не знаю можно вот тоже какой-нибудь редис там вспомнить который Примерно вот если у вас был был просто Stage и вы хотите какой-то кластеризации Вот примерно можно условно говоря было взять Барсик и подключить То есть если вот есть именно сингл Маер решение которая изначально не кластерная Вот это прям идеально для Барсика кандидат Окей кого микрофон Ага давайте Добрый день несколько вопросов но они короткие первое Как у вас работает консенсус на двух шарда вы сказали что у вас много двух Шарх этих и наверное консенсус как-то странно второй вопрос На какое максимальное количество шарт рачи ели ша много то если у Вас например разваливаются два дата-центра то не происходит ли такого что у вас два мастера образуются и как из этой ситуации выходит Например если они развалились Ну там например на сутки да если забуду вопросы то Напомните пожалуйста Да с конца сначала чтобы не было двух мастеров собственно вот в протоколе типа там Илим фаза выборов то есть на первой фазе Мы сначала собираем условно говоря команда я начал выборы в этом раунде с консенсуса машин после этого мы знаем что консенсус машин он начал новый раунд и поэтому в старом раунде этот консенсус уже ничего не может закоммитить то есть мы сначала убеждаемся что не может быть комита в существующем старом раунде и только потом мы уже проводим выборы мастера в новом раунде поэтому двух мастеров тут вот не может быть Никак понял то есть а сколько у вас процент консенсуса процент Ну да Сколько процент машин должен проголосовать чтобы получился консенсус Ну на практике у нас очень популярны двух и трёх машинные кластера то есть есть кластера где там 17 машин Но это как бы всякая специфика То есть это там какие-то деньги распределённые Локи Но в основном это две или три реплики соответственно вот две из трёх условно говоря Ну то есть если у вас будет выборы мастера вот с двумя посложнее ээ когда мы работаем Когда у нас всего две реплики полноценных то мы делаем третью реплику винес так называемый он описан в статье в em replication можно там в принципе посмотреть Ну то есть идея такая что м у нас есть третья машина но она не хранит журнал она не хранит состояние То есть она используется просто как такой арбитр чтобы каждая из э из двух машин не могла пойти и начать писать свою историю то есть этот арбитр он помогает как бы выступает третьим который говорит Кто из них мастер Но дальше они как бы уже бегут вдвоём и в принципе мы в таких кластерах Мы даже коммите просто одной машиной Ну мотивация здесь какая то есть вот вся вот эта вот теория отказов она строится на том что мы начинаем с одного любого отказа То есть у нас система должна при любом одном отказе вообще работать как бы как ни в чём не бывало если у нас вылетела одна Реплика из двух У нас есть уже один отказ поэтому мы позволяем второй машине типа коммитить вместе с вине славей репликой Но если скажем вторая машина тоже отлет это уже двойной отказ Ну при двойных отказах гораздо сложнее то есть делать систему которая все двойные отказы Переживает это как бы следующая ступень значительно более сложная Да спасибо Ну да чтобы два отказа случилось Это уже вероятно сильно меньше Да там модель отказов вообще штука сложная то есть мы так подбираем к тому чтобы сформулировать модель отказов Ну мы знаем что мы любой единичный отказ переживаем включая там всякие ошиб контролеров контролеры обожают сказать что я вснк а потом если выдернуть питание перезагрузиться то обнаружится что эти данные пропали то есть вот это вот единичные такие случаи Мы тоже Переживаем а двойные какие-то Переживаем а какие-то нет то есть если нам повезёт будет двойно часть двойных отказов Мы тоже переживаем но часть как бы нет часть мы умрём А если все базы данных исчезли это Одиночный отказ или то есть есть модель атаки когда хакер пробирается на инфраструктуру и просто делает All просто ну условно говоря получив какие-то креды там главного админа и чтобы этого не было это отдельная система просто про неё заговорили есть эп серверы которые отделены они туда принципиально другой вход другие креды и туда есть права только как бы постить вот события журнал то есть дописывать Соответственно в этом случае мы Ну через некоторое время восстановим из бэкапов вот так всё круто так Дава во Спасибо Денис Маркет Гуру у меня простые вопросы относительно в презентации было 70 баз данных что это за базы данных это просто 70 инстан сов или это 70 популярных всем известных типов баз данных это 70 популярных никому неизвестных баз данных скажем так то есть вот у меня там в примере был кластер чатов лайков и юзеров ВКонтакте эти кластеры у них вот она специфическая и оптимизированная под задачу то есть там какие-то плю деревья там всякие БМ фильтры какие-то трюки с битами Да они делаются вот под задачу потому что у нас очень много серверов если у нас кластер из 4.000 серверов если мы можем что-то там подманивания на лайте Да тоже популярны Потому что если нам нужно много индексов иметь то самим там этими манипулировать плю с деревьями Ну это как бы очень муторно и мы можем применить слай просто чтобы он нам индексы строил то есть соответственно мы события журнала Барсик их реплицируемый из общеизвестных которые Ну пользуются люди не только ВКонтакте где-то внутри себя Какие ещё поддерживаются вот сейчас из известных поддерживается Site и вся специфика ВКонтакте То есть если вы придёте в нашу команду движков то вы сможете как бы восторгаться или там наоборот А если не приходя в команду то есть общая полезность этой технологии Она в опенсорс кто-то ей может пользоваться или это просто вещь сама в себе и она доступна только ВКонтакте ну Эта система интересна потому что условно говоря мы её писали в процессе мы там пытались напы пере но мы прямо вот ничего такого не нашли опять же потому что нам Мы бы хотели просто взять да существующее решение поэтому Ну можно сказать что это условно говоря любопытная технология Я не знаю Можете ли вы её применить вот прям в своей повседневной деятельности Но если вы будете дизайни базу данных то это как бы вариант Тем более что мы в принципе ну собираемся это дело может быть даже как-то орси по крайней мере планы этого есть и последний Маленький вопрос иде было в докладе это Если я правильно понимаю просто про тестирование чтобы это стабильно работало потом в продакшене или это как-то прорастает в ранта м Значит тут основная идея что мы Тестируем именно тот код который работает в проде то есть вот консенсус Он написан на императивно языке и это ровно тот код который мы фази ровно тот который мы прогоняем на модели То есть все эти 10 млн переходов именно того который работает в проде то есть в отличие от условно говоря Ну я много был на докладах по Ла п в основном люди просто говорят вот у нас есть модель мы там типа какие-то е свойства доказали но они никак не привязывают это к коду разве что посмотрели на одном экране код на другом модель типа такие ну вроде одинаковые спасибо Так давайте я вопрос из чата задам и Наверное мы потом в кулар пойдём Ну посмотрим значит ча Валентин спрашивает Есть ли примеры багов барсики которые нашло ла тестирование есть да то есть собственно м Каждый раз когда происходит какая-то крупная оптимизация переписывания или новая фича допустим витнас да то есть вот эти лавей реплики для кластеров из двух реплик когда их делаешь то Ну приходится изменять Код да То есть были какие-то условия одни Теперь они другие и каждый раз после этого естественно Ну то есть если бы у нас не было модели то э мы даже стали как бы так ну Глядя на это утверждать что вот если вы не применяете Ну хотя бы фанг вот не формулируйте свою распределён систему как конечный автомат то наверное там совсем всё плохо да будет потому что вот 10 млн переходов - это ну реально очень много невозможно в голове их удержать Угу Так О'кей ладно да последний вопрос тогда Гриша ты выбирай лучшие или Два лучших сколько там у тебя Спасибо за доклад в общем-то У меня тоже несколько вопросов но первые из них у вас чистая репликация или там возможно etl процессы Ну то есть трансформация данных и э потом уже как бы заливка на несколько таргетный источников Ну у нас шардирование оно как бы вот оно снаружи да то есть каждый шарт в него Данные попадают исключительно для него единственное где мы делаем что-то подобное это вот я говорил про протокол Сплит То есть когда мы когда нагрузка на шарт возросла то Вот его бесшовное деление оно там происходит было три машины сначала состояние копируется на шесть машин а потом в какой-то момент они вместе приходят к решению что вот с этой позиции истории Разойдутся то есть мы вот такое делаем но это вот всё что мы делаем с шардирование в барсики Спасибо и в общем-то ещё один такой вопрос а сама репликация она может накладывать нагрузку на источник на базы данных вот как вы с этим боретесь э Значит у меня там на слайде было что вот одна из проблем двух разных процессов это как раз то что у нас допустим есть диск И если бы это был один процесс полностью один код то мы бы могли там Файн тюнить типа тут барсику надо что-то там записать и кнуть база пока не пишет или она там может что-то ну то есть можно было бы сложную логику реализовывать сейчас мы этого не можем то есть мы там частично что-то крутим просто в ядре линуса Да чтобы приоритеты установить но в целом вот приоритизация доступа к диску - это проблема потому что F - это дорогая тяжёлая операция поэтому Ну мы как бы эту проблему осознаём и она нам мешает мы е стараемся решить работаем над этим Спасибо большое а Гриш лучшие вопросы у тебя есть свой какой-то подарок да я так понимаю да противная собака вот Я даже знаю кого зовут э да Ну вот мне кажется самый лучший вопрос Это про то какие практические базы данных вот белым чек Да задавал какие именно вот то есть как это применить в реальном мире а не где-то в уютном уголке ВКонтакте Мне кажется это самый классный вопрос так ты получаешь диги и Давай тогда ещё один вопрос у нас от спонсоров есть пакетик классный э и второй Мне кажется что ну мне понравился вопрос собственно про две реплики то есть вот человек человек заметил что там как две реплики приходят к консенсусу Да Андрей О'кей всё А давайте по аплодирую Гриша друзья а очень классно для тебя тоже есть подарок от онтика вот он где-то возле сцены мы тебе сейчас обязательно вручим всё пожалуйста проходи в кулуары друзья L"
}