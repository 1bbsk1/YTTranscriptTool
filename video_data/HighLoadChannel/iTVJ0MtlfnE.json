{
  "video_id": "iTVJ0MtlfnE",
  "channel": "HighLoadChannel",
  "title": "\"Облако\" в Badoo год спустя: работа над ошибками / Юрий Насретдинов (Badoo)",
  "views": 149,
  "duration": 2741,
  "published": "2017-04-22T14:47:42-07:00",
  "text": "всем привет меня зовут эрин из рединов и я пишу на печке так хорошо отдавать начнем я расскажу про нашего блока в году и для начала что из себя представляет баду если вдруг вы не в курсе мы являемся социальным социальной сетью для знакомства мы людьми которые является крупнейшей в мире и мы представляем себя вроде как вполне настоящих и лот у нас 200 миллионов пользователей 40 fps вот поэтому хотел рассказать о том что мы сделали для одной задачи которой нас было зачем мы сделали облака и так для начала я рассказывал хотел бы сказать то что в прошлом году на хэллоуин я уже делал доклад про наше облако но тогда она была только в зачаточном состоянии моего только запустили на продакшн соответственно я еще раз расскажу о том что облако из себя представляют более кратко чем прошлый раз и о том какие новые фичи мы внедрили и собственно с какими проблемами столкнулись когда мы его разрабатывали и так на чем работает баду году из себя представляет так и очень стандартный linux индекс . fpm стек также мы используем сервисы который написан на си си плюс плюс также были на джаве и в качестве базы данных мы используем мы искали тарантул секу light использовали наши демоны которые тоже больше не используются и в качестве memcache в качестве каши вышел свою мы используем естесно моим cached но пан неестественно но memcached начала я бы хотел русской какую задачу мы решали самого начала когда буду появился у нас были задачи которые требуют обработки не в вебе отложено то есть например это обычно какие-то очереди например человек залив фотографию на наш сайт мы должны от модерировать и естественно это происходит не в режиме реального времени а где-то у нас на серверах проходит через много стадий и потом только видит пользователь уже видит конечный результат то что фотографии от модерировал селенит соответственно для того чтобы обрабатывать очереди и иногда некоторые другие вещи и ноги и делать какие другие вещи мы используем коли скрипты которые по крону мы использовали christ ag ли скрипты которые запускались по крону и работали под управлением какой-то централизованная система которая следила за тем чтобы скрипты исполняли составлена что мне надо в нужное количество экземпляров и так далее это система называл своим крон представляла но если следующий есть единый конфиг в котором записаны в соответствие между именами серверов или же именами групп и строчки в кроне которое должно быть прописано на этих машинах соответственно с помощью вызова утилита там м крон мы раскладываем новый crontab в котором грубы для каждого сервера появляются соответствующий строчки в кроне про скрипты которые должны на этих машинах работать проблема которое возникает такой архитектурой в том то что если какая-нибудь машина выходит из строя то нужно переносить строчки в кроне на другие сервера и дело в том то что это не может делаться автоматически потому что все скрипты иногда предполагает то что они работают на конкретной машине и так или иначе они хотят этого или не завязывать на то что на этой машине есть могут не работать на других поэтому мы не делаем автоматический перенос где он пал ручной перенос то есть разработчики включается в процесс переноса и смотрят за тем что и скрипты работать это не очень удобно вот что мы решили сделать вместо этого вместо этого мы решили сделать какую-то систему которая будет вместо того чтобы вручную без того чтобы заставляет разработчиков прописывать скрипты на конкретных машинах она будет сама выбирать где запускать скрипты и в случае падения машины нам будет сама перезапускать их на машинах которые все еще живы этот подход называется современных терминах этот подход называется в общем то облако также поскольку мы собираюсь разрабатывать эту систему мы думали делать свою или же попробовать какой-нибудь готова не буду вдаваться подробности почему мы решили писать свою систему но помимо запуска по расписанию мы решили что неплохо бы иметь возможность скрипты добавлять и добавлять задания для запуска скриптов и через api и так как выглядит как вы или скрипты которых я рассказываю не выглядит примерно следующим образом мы пишем какой-то класс который либо наследует который реализует какие то ли вы какие-либо интерфейса или же extended от какого-то класса который мы с вами задаём и например в на слайде в качестве базового класса сделан выбран обработчик очереди соответственно разработчик говорит как называя свою очередь говорит с какого сервера я забирать это опущена в данном примере и объявляют функции которые обрабатывают соответствующие записи и возвращает результат ну и веб интерфейсе пользователя добавляет свою классную систему и говорит сколько максимум экземпляров может быть запущенную сколько максимум размер пачки о чем я дальше скажу и о том сколько времени на обработку 1 запись к и максимальное время обработки 1 запись после того как пользователь эрни разработчик скрипта добавил свой скрипт в интерфейс он начинать магическим образом либо там разгребать очередь или запускаться и что-то делать и соответственно запускается потому расписанию которое указало разработчик или же есть это обработчик очереди то начинают работать просто по факту наличия задачу в очереди то есть как бы вроде бы все понятно как это реализовано внутри здесь есть три больших компонента которые я постарался показать разными цветами есть москаль сервер который имеет реплику потом два управляющих сервера каждый из которых в свою очередь общаются как с базы данных так и с машинами на которых непосредственно исполняется задание и соответственно осуществляет запуск приседаний и непосредственно машины которые входят в облаков на которые запускаются скрипты скрипты в данном случае так ли скрипты и для того чтобы запускать скрипты на конкретных машинах у нас стоит демон который мы тоже написали сами и немножечко изменили для задач его блока который называется печки прокси д все что он умеет делать это просто запускать какой то скрипт написан на печке и передавать ему те аргументы который сказал пользователям и сообщать о том как он работает как он сделал запущен упал или еще что нибудь хотелось бы еще немножко привести цифры что из себя представляет облака на данный момент в облака у нас входит около тысячи машин но при этом большая часть скриптов сосредоточено эти тысячи машин и разбиты по группам большая часть скриптов которые запускаются на самом деле входит в небольшую группу около 50 машин и поэтому 1000 запуска в секунду на 50 машин учитывая то что скрипт и как правило работает не моментально они занимают 10 секунд это кажется достаточно хорошая нагрузка также систем сама система которая осуществляет запуск и контроль за исполнением написано на печке это фрагмент который осуществляет непосредственно запуск скриптов и слежение за их исполнением планировщик который выбирает машина на которых нужно исполняться и генерируй задание по расписанию он написан на go на самом деле дата-центра у нас два ни один и цифры на самом деле нужно умножить на 2 но на один мастер приходится например 15 тысяч запросов в секунду примерно 50 на 50 чтение и запись совсем до недавнего времени где-то пару недель назад мы наш планировщик был написан тоже на печке поэтому я сказал то что я пишу на печке и мы в аду на самом деле почти весь софт сначала стараемся написать на печке москве только если с ним возникают какие-то проблемы мы переписываем это на какие другие языки здесь на графике показано задержка планировщика то есть сколько времени проходит с момента когда задание добавлено в систему до момента когда на нем ему выбраны машина на которой он должен работать так получается то что планировщику нужно иметь состоянии то есть он должен знать обо всех машинах и знать какие задания он на какие машины добавил для того чтобы не перед не перегружать их поэтому поскольку печки не поддержит на год радость расправляете его на печке представляются не такой простой задачей кое-что эксперимента мы решили попробовать написать его не напичкан огонь и как можно видеть это дало достаточно неплохо результат мы смогли распараллелить наши наш планировщик и он все еще живет на одном забыл ядре то есть он многоразовый но процессора потребляют примерно столько же работает 10 раз быстрее как мы осуществляем балансировка нагрузки балансировка нагрузки тела вашего автомобиля на самом деле не так много способов лично мне известно и самый простой из них это так называемый взвешенно roun рода когда вес которым вес означает вероятность с которой конкретное задание попадет на конкретную машину с чем выше вес машины тем больше шансов что какое-то задание попадет именно эта машина не говорю на другую при этом в качестве офисов машин мы используем оценку производительности которые мы запускаем 1 минуту поскольку машины бывает не только физическое на виртуальные их производительность может меняться со временем и в качестве веса машины мы берем производительность машины умноженные на процент свободного пространства буду циpкa на картинке скорее всего не очень хорошо видно что желтенькая это вес настоящая belle абела это оценка производительности машины и вот что получается это график загрузки процессора на 3 самых загруженных машины и 3 самых незагруженных машинах которое входит в группу на которой мы запускаем большую часть наших скриптов как можно видеть учитывая то что в этой группе около 50 машин распределение с помощью такого алгоритма получаются весьма весьма равномерным хотелось бы отметить то что машины которая входит в эту группу они совершенно различные конфигурации поскольку она же облака нам добавляют машины которые как бы мы не требуем ничего для от машины от машин которые мы добавляем мы работаем и на машины в гипер трейдингом без гипер трейдинга динамический подсчет весов позволяет выровнять нагрузку на любом типе железа и так потом я говорил о том что одна из самых важных вещей которые нам не нравилось это то что когда сервера выходит из строя нам нужно вручную как-то переносить скрипты и но что-то делать в случае с москве ль сервером существующие механизмы автоматического переключения и если они не используются синхронную репликацию не очень надежны вы можете получить должность срабатывания если у вас моргнула сеть или же затормозил сервер на какое-то время поэтому мы решили все таки не использовать автоматически well over для sql сервера и используем ручное переключение на слой что касается управляющего логике мы соответственно делаем следующую вещь вся управляющая логика разбита на маленькие цикл и перед тем как начать цикл мы берем блок в базе с помощью selected лак и после того как цикл отработал блок отпускается при этом лоб берется в том же соединение моя скелет в которой мы отправляем дальнейшее запросы поэтому если запрос поэтому если соединение закрылась и лоб был отпущен то мы не сможем дальнейшей транзакций в эту соединение послать случае падения машины через настройку которая создается в москве под названием white тайм-аут москаль сервер сам закроет соединение и таким образом отпустит лог вот в этой мог мы выставили в 60 секунд и есть еще некоторые настройки которые тоже задают тайм-аута на сервере которые принудительно закрывает соединение поскольку лак и запросы берутся в одном и том же соединение то это позволяет нам обеспечивать отказоустойчивость даже при выходе из строя управляющие машины и так есть еще один момент сами скрипты которые запускаются если скрипт работала на машине которое упало то мы должны каким-то образом надежно определить то что эта машина упала на ней больше не выполнять никакие скрипты в общем случае не известно механизма который позволяет делать эту надежно но мы думали о том чтобы например делать следующее если машина нам не отвечает в течение допустим 5 минут там и через айпи и мы посылаем ей сигнал перезагрузки и соответственно мы можем быть уверены что ничего на этой машине больше не работает это плохо потому что представьте себе что машины вам не ответили потому что например на ней умер крон машина живо вы ее просто перезагрузить или же вы ваша система немножко сошла с ума и возьмет начнет перезагружать случайно машины это плохой вариант поэтому мы придумали следующее каждый разработчик нашей системе всегда задает максимальное время работы скрипта после того как максимальное время работы превышена к нему посылается сикти ram если же мы не можем послать фиксируем например мы потеряли связь с этой машиной но она продолжает работать есть системный вызов alarm в unix-like вирусную система он говорит в системе просят система посылает сигнал который называется говорим через то время которое вы укажете соответственно мы говорим перед началом работы скрипта чтобы ему был послан сигнал alarm через максимальное время работы плюс небольшой запас чтобы мы все-таки убили сами по-своему сигналом они про сигнала лад и для того чтобы это работало на самом деле требуется синхронизация часов потому что вы не можете делать alarm на время работы вы должны делать alarm на таймс темп когда ему нужно завершится он зависит от того как идут часы часы на данной машине я могу объяснить подробнее но в клара и так я вкратце я постарался описать какая у нас архитектура облака теперь давайте я расскажу о том какие вещи мы добавили после этого во первых это сбор налогов сделано он очень простым образом каждый запуск поскольку мы используем москаль то это просто на самом деле арт инкрементный счетчик при вставке в таблицу с запуском она получает каждый запуска получает уникальный модификатор и вывод скрипта перенаправляется в два файла а у ты иди задания out her лак и мы пишем все логе в одной директории и с помощью найти фай следим за изменениями в этой директории и отсылаем изменившейся верни не изменишся новые строчки которые написали скрипт и в фейсбук овский демон скраб скраб это что-то вроде си слога его единственная цель это отправлять строчки которые в него попали и доставлять принимать строчки от пользователей и отправлять их на какую-то центральную машину или же на распределяете их по нескольким машинам и с помощью skype мы доставляем в течение нескольких секунд логин на центральную машину и обрабатываемых расовым их по файлам уже которое относится к каждому скрипту в отдельности соответственно логе обычно занимает довольно много места поэтому мы их периодически растираем с помощью лугано тэйт и также поскольку у каждого запуска есть уникальный модификатор мы можем проиндексировать логе и куда-то записать информацию о том где в логе содержится налоги конкретного запуска и соответственно сбой в интерфейс и наши разработчики могут читать логик которые относятся к конкретному запуску и соответственно посмотреть например почему вчера в 10:00 скрипты почему-то упал по памяти и летом по максимально во время исполнения посмотреть что происходило без необходимости гриппа типа возможно гигабайтом или даже тра байтом файлов так же как я говорил основная задача которая решает большинство наших скриптов это разговора очереди вы наверное спросите юрий скажете зачем вы используете москве для того чтобы хранить очереди или не спросите а я все равно отвечу возможно москаль используется ровно для нэп вещи это транзакционных посылки событий если вы делаете грубо говоря апдейт в базе и посылайте какое-то событие которое связано с этим апдейтом то достаточно легко получить ситуацию когда например апдейт пройдет о событии не будет поздно на или же наоборот события будет послана а ведь не прошел для того чтобы с этим бороться можно использовать разные подходы и мы просто вставляем в отдельную таблицу дополнительную запись и потом из этой таблицы разбираем записи которые у нее есть в порядке прихода событию дело в том то что если вы разбираете очереди в москве много потоков то вы можете получить некоторые проблемы с производительности связаны они в основном с тем то что довольно сложно придумать как синхронизировать между собой исполнение мы например некоторые задачи разбираем очередь 400 потоков довольно сложно придумать с к или запросы которые можно посылать в очередь чтобы выбирать оттуда в 400 потоков я привел несколько типичных подходов каждый из которых мы используем в своей работе но они не очень хорошо масштабируются или же если вы например заранее поделили очень там скажем на 400 кусков то может легко так получится то что на одном шарди вы уже давно имейте пустую очередь а на другом шарди вы имеете скопление из большого количества элементов мы должны следить за тем чтобы вы равномерно разбивали ваша очередь поскольку этот вопрос нас беспокоил весьма существенно весьма существенно то мы решили написать с использованием нашего облака с использованием 5 разбор щеку очереди как он работает вместо того чтобы пытаться из каждого worker а делать какие-то запросы которые равноправны и пытаются залочить только свои записи мы имеем два скрипта один скрипт является грубо говоря мастером и делает просто постоянно select из очереди и выбирать новые строчки все эти строчки он группирует папочкам и через и 5 добавляет в облако и соответственно сами worker и не конкурируют за записи в этой очереди этим занимаются наши системы и соответственно мастер естественно помнит все диапазоны все и диски которые он уже выбрал и выбирает новые с помощью запроса select и not in после того как задание отработали они удаляют записи из таблицы соответственно таким образом мы осуществляем именно разбора очереди что что это так что дает такой подход вы не имеете конкурентности вы не иметь большой конкурентности между маркерами за одной и той же таблицы при этом вы можете создавать такое количество worker of которые вам нужны в данный момент и соответственно достичь более равномерного более равномерные загрузки worker of и соответственно такой способ если вы вдруг используйте очереди на москве можно сделать без какого-либо и пьянь сделать его с помощью вызвав орг соответственно мастер когда выбрал очередную пачку for каяться и отдает эту пачку ребенку итак поскольку наша система уже не знаю на ногах наверно еще не стоит но в год работает продакшене почему что случалось с нашей системой этот год во первых хотя пускать то что всего за год мы имели три существенных downtime а интервалом где-то по одному часу если посчитать аптайм то есть получается аптайм 99 97 процентов это как мне кажется достаточно неплохо для системы которая активно разрабатывается в данный момент на раннем этапе разработки нашего облака мы столкнулись с проблемой то что мы умудрялись запускать несколько раз задания с одним и тем же иди и когда мы попытались обставить информацию о том что мы запустили это задание со конкретном о еде второй раз мы получали ошибка да прикидки проблема в том то что наша управляющая логика если скрипт не смог ставить информацию в лог сам то мы делаем это за него если не получается то мы выходим с ошибкой и говорим то что ничего не получилось раскрыли ошибка мы не можем продолжать поскольку как я сказала это произошел на раннем этапе разработки то мы не рассказали мониторингу достаточно информации о том что делать если нашей системы лежит и и что это на что это влияет поэтому они не знали что делать несмотря то что удалить запись из москве это делать 10 секунд прошел где-то час до того момента когда стало понятно что происходит и самое главное кому звонить естественно это произошло где-то в три часа ночи и тоже что накладывает некоторые ограничения на время за которой можно отреагировать второй раз мы падали из за того что мы просто не протестировали мир который был с конфликтами мы забыли про greatest и перед тем как выложить это на продакшн и долго пытались понять что происходит и где-то в течение получаса давно time нашей системы был из-за кроны мы обновили кром и он перестал запускать задание которое записано в и тисе crontab и поскольку hard with от каждой машины посылается через кром то наша система перестала видеть машиной она считала то что больше машин в группе нет и запускать соответственно ничего нельзя хотя бы сказать то что петр зайцев вчера рассказ или сегодня говорил о том что как правило downtime системы в хавел убил эти системах вызывается нее за железо и не из-за каких-то технических проблем он возникает из-за человеческих ошибок и не смотря на то что мы ни разу не падали из за того что у нас вышел из строя какой-то сервер не выходили из строя и довольно много основной downtime у нас приходился на человеческие ошибки также в процессе эксплуатации мы столкнулись с некоторыми особенностями работы моя сквер одна из этих проблем и который я уже рассказал коллегам из москве ли это медленный дроп тайбл в москве начиная с какой-то версии содержится патче который устраняет эту проблему но у стороне ты не до конца когда вы делаете дробь большой таблицы берется какой берется лог не не очень понятно откуда он берется но тем ни менее перед тем как сделать андаинг берется окна метаданные видимо на список имен таблиц и выполняется blink из таблицы удаляется из learning идет долго например получим полчаса не может идти например 30 секунд то все новые запросы которые хотят открыть какие-либо таблице они ждут этого лака перед тем как выполнится соответственно у вас просто начинают накапливаться висящие транзакции в списке процессов которые хотят открыть просто как бы то произвольную таблицу она не имеет никакого отношения к той таблице которые удаляйте но как бы вот потом есть такая вещь в москве как purge thread возможно скажите кто не знает про то что такое портрет в москве ты сталкивался с этим хорошо я тогда постараюсь кратко объяснить москве в и на dp есть поддержка и мисс сеси что означает то что каждая транзакция может видеть свой snapshot данных это значит то что если вы удалили данные в какой-то транзакции то остальные транзакции все еще могут видеть эти записи и для того чтобы осуществить непосредственно удаление строк есть отдельный трек который следит за тем какие записи уже больше никем не видится и удаляет их физически из таблицы поэтому возможно ситуация когда вы удалили много строк из таблицы потом например сделали select из нее и не смотря на то что там немного данных москве все равно не на тебе все равно выбирает все записи которые там есть том числе те которые еще физически не удалены из таблицы и пропускает их виде то что в этой транзакции не вижу эти записи и соответственно например можно получить то что select кого звездочка искусство таблицы в и на тебе будет занимать минуты и репликация репликация в москве есть асинхронные и синхронные да как и рассказывали царев и много кто ещё мы используем а синхронную репликацию асинхронной репликации 1 поточная домой осколе 56 но и в москве 56 она обладает некоторыми ограничениями по сути не стоит считать то что она вообще работает как мне кажется поэтому на самом деле все изменения которые вы производите с базой должны успевать выполняться в один поток иначе ваша репликация будет отставать так же если у вас много машин то естественно то что вы скорее всего будете с каждой машины хотя держать постоянное соединение с базой потому что если вы грубо говоря с каждой машины по 10 раз в секунду открываете и закрываете соединение то это не очень хорошо сказывается на производительности самого москве все игры вот так делать нельзя потому что если вы откроете больше несколько тысяч соединений в москве и при этом настроек вы крутите настройки которые позволяют держать много подключений то с большой вероятностью когда вы начнете посылать запросы в эти соединения мозг или пойдет для того решения проблемы есть так называемый вещь как пол коннектор но она есть в москве 56 и в мария тебе так же наверное я бы опустил проблемы ядра linux с которыми столкнулись и хотим рассказать о том что мы собираемся делать дальше как я уже сказал мы попробовали написать планировщик нога бой был выбран потому что это относительно новый язык с хорошей поддержкой со стороны разработчиков и со встроенной многопоточность и поэтому мы смогли малой кровью распараллелить наш планировщик и при этом получить хорошую производительность мы можем также сделать для кода которые занимаются запуском скриптов мы можем иметь по грубо говоря одному зеленому цвету на машину на который нужно осуществлять запуск и соответственно если какое то есть какая-то машина тупит или же просто сеть плохо работает до этой машины то это никак не сказывается на запуске задание на других машинах и так же как я уже говорил нас есть демон . и прокси д который осуществляет запуск интерпретатора php на каждый запуск задания и это достаточно медленно поэтому как ни странно мы управляющий логику хотим перевести на go демон носи переписать на печке вот и поскольку мы большую часть управляющий логике хотим написать нога который не зависит от нашего фреймворка мы в принципе можем открыть исходные тексты это системы если она будет кому не нужно и как я уже сказал гу отличный выбор для того чтобы писать такие вещи как управляющая логика как ну такие какие-то системы которые осуществляют очень много соединений и который содержит в себе какую-то достаточно нетривиальная логику в качестве базы данных я бы не рекомендовала использовать москве для этой задачи потому что вещи вроде весь еси и из-за которых возникает довольно много проблем в масштабируемости на тебе они отсутствуют в например в таранто ли и как правило вам не нужно сквер для такой задачи потому что все логе вы можете писать все еще в москве ли индексировать информация но хранить часто меняющиеся состоянии это плохая идея из-за того что записи могут не успевать уржаться и вы будете иметь каскадное увеличение спою за маской сервере но и также наверно не стоило пытаться приспособить существующие демоны на печке на оси которые у нас были и сразу написать все на печке спасибо добрый вечер спасибо за доклад вопрос 1 по поводу многопоточности в печке пробовали вы такой такую библиотеку как петр с смотрите насколько я понимаю очки устроен таким образом то что у него нету никаких лаков для структур данных которые он использует соответственно если даже вы будете использовать лишь нам библиотеку которая позволяет запускать 3d бы все она должна это делать очень аккуратно поэтому я бы не стал так делать возможно это а что вы имеете ввиду под локоть не очень понял смотрите если у вас есть какой-то массив например которые вы хотите модифицировать из разных тредов то для того чтобы не было повреждение памяти вы должны использовать какую-то синхронизацию при доступе к мьютекс и на это есть или вот на печке я не видел даже минуте и стресса и зовут просто вопрос вы смотрели в ту сторону не смотрели как это выглядит и мне показалось что это нельзя использовать руками не трогали скажем так нет и второй вопрос почему у вас я так понимаю что планировщик запускает скрипты каждый раз на удаленных машинах да нет планировщик просто пишет в базу данных о том что вот этому задание присвоены конкретной машины и записывать очередь на исполнение заданий на запрос заданий в отдельную таблицу из нее уже читают отдельные горки которые осуществляют этот осуществляет запуск просто как бы вас они реализованы все таки как скрипты или как демоны и это скажем так они поскольку работают короткими циклами то не так важно они работают постоянно или их постоянно перезапускает в нашем случае они работают во внешнем while true игру говорит делает какое-то количество итераций выходит спасибо спасибо за доклад у меня такой вопрос вы говорите у вас есть проблема спорт штырит связанная с транзакциями это значит что вы используете транзакции для какой-то работы с очередью или это просто внутреннее одиночный транзакциями эскель которые которые в любом случае будут вы на д.б. мы делаем меж табличные транзакции ну так получилось мы использовали просто моя сколь мы сделали как бы такую архитектуру как к нам показалось логичный мы используем меж табличной транзакции и так же и на тебе используется потому что мы храним там достаточно важные данные мы храним состоянии облака мейсом в данном случае не подошел бы из-за того чтобы краше моя сквер сервер или же при сбое питание мы могли бы эти данные потерять или получить некое системное состояние и в ну в общем то почему бы не переписать эту на какие-то более простые транзакции на базе того же в москве ли который ну уже с большей вероятностью не будут вызывать проблем у porsche tried проблема с просто рот следующее если у вас есть таблица которая стоит очередь то когда вы из нее удаляете она все записи которые удаляются на самом деле помечаются моя скорее лим как удаленные и когда вы делаете следующий раз select мой стиль выбирает эти записи и выбрасывает их соответственно если у вас есть много worker of которые дело select из этой таблицы то вы получаете очень грустную картину для того чтобы выбрать следующие 100 строк ему должен сначала пропустить миллиона удаленных а потом выбрать следующие 100 строк и это как правило заканчивается тем то что на москалю заканчивается циpкa он уходит в полку по процессору и портрет начинает работать еще медленнее потому что он уже конкурирует с телег to me поскольку он один работать в один поток хотя это тоже не совсем правда но увеличение пульс количество портретов не оказало влияние на производительность на на наших условиях я не совсем это имел ввиду я говорил о том чтобы сократить объем транзакций для того чтобы уменьшить количество к болезней которые да ну вот этих вот записей которые как раз лишнее на самом деле нужно не делать длинных транзакций таком случае будет все хорошо ну для того что вот почему бы ну это кажется таким достаточно не делаем 11 загсы к самом деле по этой причине то есть я говорил о том что такая проблема есть мы с него успешно боремся но она не приятно потому что на нее трудно отлаживаем мы с келли не говорит о том что в вашем запросе было 1 миллион удаленный строк поэтому москаль тормозит спасибо спасибо за доклад небольшой вопрос вы не только в этой системе используйте маску и campo москаль используется как база данных для bada вообще да и такой у вас везде переключения на резерв вручную осуществляется до почти везде за исключением возможно paribas которые переключаются через то что называется сервис айпи но я не уверен в этом честно говоря насколько я знаю почти все базы переключаются вручную спасибо спасибо за доклад такой вопрос есть а почему вы не использовали для распределения задач между ворами специализированные решения для очередей типа того же бита про rabbit я слушал много негативных отзывов но это на самом деле не является причиной по которой мы используем москве маской мы используем просто как другая базу данных по умолчанию мы храним очередь мы храним все очереди в москве по крайне мере persistent на очереди в москве и язык программирования мы выбирали под тем же причинам почему нет нужно сначала попробовать написать на печке большинство вещей которые у нас работают они успешно работают на печке и москве главным образом поэтому дайте за поддержки да конечно спасибо скажите пожалуйста подетальнее каким образом происходит ручное переключение падает сервер zabbix и мониторинг который у нас работает круглосуточно виде то что смазка или сервером что не так он звонит дежурному администратору или же если это дневное время то звонит просто либо и говорит о том что вот случилась такая проблема и нужно переключить лес на наш мастер сервер который является маску сериям облака настоев это мигрирует айпи адрес или в коде везде меняется адрес новые нет она просто обновлять записи dns прямо говоря вот и соответственно перезагружает am bound который вас использовали каширования гиннесса но сервер на при кишечных соответственно там каширу ющие dns и наверное он рилот делают даже руками и тишина спасибо вот к предыдущему вопросу о вас в итоге в таком случае вас downtime получается или как в случае выхода из строя мастера да и нас это устраивает у нас устраивал у нас вполне устраивает аптайм дело с 997 потому что это не фронт это просто кли скрипты который работает на фоне а понятно да она для конечных для пользователей да у вас там по-другому сделаны для пользователей у нас используются железная штука названием ltm около traffic manager это не хочу ни не помню к сожалению деталей вот но это просто как бы роутер который умеет раскидывает нагрузку по группе машин с какими-то весами и это как бы это железное решение которое резервируем а я тоже таким же способом но она хорошо она не вручную опять включается насколько я знаю по зиму"
}