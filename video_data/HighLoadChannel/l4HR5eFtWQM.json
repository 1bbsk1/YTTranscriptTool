{
  "video_id": "l4HR5eFtWQM",
  "channel": "HighLoadChannel",
  "title": "DNS в Facebook / Олег Облеухов (Facebook)",
  "views": 1208,
  "duration": 2661,
  "published": "2019-05-15T04:08:25-07:00",
  "text": "всем привет спасибо что пришли и не знает 10 утра на второй день это прям испытание целая самом деле пришел на все хорошо надеюсь все попили кофе все счастливы и довольны ok можем начинать всем привет меня зовут олег я работаю production инженером фейсбук занимаюсь я многими проектами один из которых dns о котором я вам сегодня расскажу презентация будет довольно техническая так что я надеюсь все присутствующие имеют хоть какое-то представление о dns ok если вы станцию вопрос или вы просто заходите со мной связаться пообщаться там пожалуйста свяжитесь со мной через linkedin мой лагерь логин указано на слайде все знают пути слайды будут на английском но вы не удивляйтесь я буду все равно все объяснять и показывать если вас станут вопрос пожалуйста хранить их до конца презентации спросите там окей я думаю с приветствием закончили можем начинать фейсбук мы оперируем в огромном масштабе миллиарды пользователей и терабит и трафика заставляют нас постоянно оптимизировать систему и искать пути чтобы уменьшить нагрузку и улучшить качество сервиса и даже казалось бы такой простой понятный древний сервис как dns становится большим интересным проектом сегодня мы поговорим о трех основных использованиях dns facebook как мы балансируем нагрузку и причем здесь dns как ресурсные записи попадают в нашу глобальную инфраструктуру и наконец что такое да кв-1 и как мы используем его как мы используем dns в его организация просто чтобы напомнить в самом простом случае клиентский запрос выглядит следующим образом клиент делает dns запросы получает api вы 6 запись на самом деле мы активно продвигаем айпи вы 6 и у нас даже есть регионы где нету а пиво 4 вообще как вы знаете обычно в dns используется в dp хотя в некоторых случаях может использоваться в себе следующим шагом является установка ищите без соединения и отправка того самого get запроса выглядит все просто и привычно все мы видели это много раз в далеком 2011 году команда трафик к решила измерить качество работы в facebook во всем мире основной метрикой выбрали сетевую задержку соединение с серверами facebook в то время у нас было только 3 дата центра и все они находились в сша так вы видите у пользователей в северной америке блины лучшее качество сервиса однако чем дальше пользователи находились а дата-центров тем хуже становился опыт использования facebook мы не могли так просто взять и построить дата центра по всему миру поэтому команда стала думать как уменьшить сетевые задержки и улучшить качество пользования сервисом на текущий момент men and sea роли 12 дата-центров ситуация разумеется улучшилась однако как вы все еще можете видеть большинство из них находятся в штатах давайте посмотрим на простой пример где пользователи скорее подключается к одному из наших датацентров в сша сетевая задержка в одну сторону через океан то конечно примерная задержка но более-менее правда правильная составляет 75 миллисекунд что не так долго казалось бы но мы с вами скоро увидим почему это плохо давайте разобьем один простейший ищите ps запрос на составляющие и посмотрим с точки зрения протоколов мы все знаем что установка тисе пи соединения потребует от нас рукопожатия состоящего из трех шагов сян сян ak и aq для этого нам понадобится как минимум 1 round trip что займет 150 миллисекунд затем нам нужно установить тела и соединения это 4 этап на рукопожатия которая потребует а нас два дополнительных round trip ну и наконец мы можем править тот самый еще типе запрос ради которого все и затевалось для этого нам понадобится еще один раунд rip и так чтобы просто сделать простой базовый запрос нам понадобится ужасающей 600 миллисекунд решение так называемый point of presence точки присутствия или просто поп по факту несколько серверов роутеров которые терменируйте себе и tls и прокси рует оригинальный запроса дата-центры они разбросаны по всему миру и мы постоянно добавляем новые мы стараемся устанавливать их максимально близко к конечным пользователям давайте теперь посмотрим как улучшится сетевая задержка для все того же пользователей скорее если те цепи и tls будут терменировали серверами в том же регионе они зато центром у нас есть паб в азии круговая задержка для которого составляет 30 миллисекунд установка тисе пи и tls теперь займет всего лишь 90 миллисекунд вместо 450 и теперь мы можем отправить тот самый тебе запрос и прокси ровать его через океан общее время составит всего лишь 240 миллисекунд просто из-за того что мы добавили несколько серверов поближе к пользователям мы уменьшили задержки более чем в два раза а как же устроен поп внутри на самом деле примерно так же как и дата-центр только в гораздо меньшем масштабе клиентский запрос попадает на один из наших роутеров и по и семьи на балансировщик и нагрузки основная разница между поп и дата-центром здесь в том что у нас нет веб-серверов и пользовательский запрос просто прокси руется в основной дата-центр по заранее зашифрованному туннелю все это звучит здорово да но как же мы решаем на какой поп отправить пользователя очевидно нам нужно какой-то дополнительный метод балансировки нагрузки на которой мы можем активно влиять нам на помощь приходит dns и наша система картографа которая постоянно изменяет и измеряет наше введение карта интернета как же мы решаем на какой по подправить пользователя вот несколько метрик и критерия в которыми мы руководствуемся близость к пользователя если вы когда-либо пробовали делать trace рот-то могли заметить что маршруты в интернете не всегда выглядит так как вы себе их представляли провайдеры зачастую оптимизируйте под свои нужды не сильно заботясь о пользователях конкретного сайта опять же очень важно понимать что физическая близость к поп не значит что он самый близкий по сети загруженность как мы уже знаем об состоит из серверов роутеров ресурсы которых конечно один поп физически не сможет обслуживать всех пользователей в регионе поэтому мы как минимум должны обратить внимание на утилизацию поп состоянии помимо очевидного выхода из строя сам поп может быть в отличном состоянии а вот провайдер испытывать трудности наиболее частые причины отказа бывает обрыв кабеля или даже отсутствие электричества особенно в проблемных регионах в некоторых локациях даже электричества периодически выключает на ночь если у нас недостаточно данных мы все еще можем из учитывать физическую удаленность так как же мы верим ту самую близость к пользователю при помощи нашей системы которая называется sonar примерно каждые 40 1000 запрос мы отправляем на ней оптимальный поп например вместо ближайшего пап в азии ваш запрос на аватар например может быть отправлен на и ведь даже на другой континент несмотря на то что это происходит не так часто и пользователи не замечают этого мы все еще получаем очень много данных и очень неплохой сетевую карту интернета и так картографа эта система которая получает все эти данные создает карту и решает какой айпи адрес вернуть пользователю на запрос на dns запрос вся карта обновляется примерно раз в минуту и мы можем активно реагировать на любые изменения в сети dns карта еще известно как сплит х райзен dns где зависимости от айпи адреса клиента мы можем возвращать разные результаты это открытый стандарт каждый может его внедрить у себя на этом слайде вы видите еще одно представление процесса где пользователи приходит через разных провайдеров и получают айпи адрес наиболее оптимального для них по процесс доставки dns-записи и карты на сервера выглядит следующим образом картографа генерирует dns карту формате для тайне днс dns publisher компилирует этот файл в базу данных сидибе запускать целую кучу тестов и начинает раздачу через нашу торрент инфраструктуру все это оркестре руется через apache за кипер на самом сервере все еще раз тестируются и базы данных подменяется на этом слайде показан картографов действия зеленая линия подписано как capacity лимит указывает на лимит производительности до появления картограф ира особенно пиво по вечерам нагрузка на некоторых поп здесь оно отмечено красной линией подписано как юзер demand была значительно выше чем они могли обеспечить однако другие поп были не dogru жены в это же время в некоторых случаях это вызывало отказ в работе или плохом качестве сервиса однако как только мы смогли использовать данные стоп мы смогли лучше балансировать нагрузку и использовать оборудование максимально эффективно синяя волнистая линия прямо на границе с лимитом производительности демонстрирует успешность работы картография так выглядит утилизация попав в европе вы можете видеть как в разных временных зонах просыпается все больше людей и какой-то момент мы достигаем лимита производительности картографа отлично реагирует и балансирует трафик между регионами сдвигая пользователи на альтернативный поп а это как выглядит утилизация на глобальном уровне где слева на графике видно успешная утилизация по в сша а справа в европе остальных стях света надо все же упомянуть о проблеме с публичными dns резольвер ами такими как google dns справедливости ради конкретно с google dns дела обстоят не так плохо как с другими риэлторами как вы помните наш корейский пользователь подключался через пап в азии однако если он использует публичный dns резольвер то ситуация становится гораздо хуже совсем не факт что публичные dns-сервера находится близко к пользователю вернее очень часто это совсем не так давайте представим себе что пользователь попал на сервер публичной dns инфраструктуры где-нибудь в канаде этот сервер начал рекурсии и разумеется попал на ближайший к нему поп из-за того что клиентам для нас является сервер публичной dns инфраструктуры мы вернем адрес папа не более оптимального для него они для оригинального пользователя ну и теперь пользователь обречен ходить через океан для каждого запроса теперь давайте поговорим о том как устроена внутренняя dns инфраструктура сразу отвечу на главный вопрос зачем отдельную инфраструктура все просто безопасность и производительность например мы не хотим делать доступными имена серверов извне ну и конечно же dns-сервера сервис dns должен быть максимально близко к сервисам в дата-центре в то же время до сна внешне dns инфраструктуру не должно влиять на работу внутренних сервисов хоть с другой стороны конечно внутренне и внешне инфраструктуры организованы довольно схожим образом давайте рассмотрим типичные dns-сервер по и не каст запрос выпадает на open source найти широкий и рекурсивный inbound который пытается получить ответ от автор это тивных dns-серверов те в свою очередь разделены на шарды и таких шар дав у нас много для быстро ты распространения и дублированы для отказоустойчивости каждый шар это артрита тивной тайне dns мы используем стаб зон чтобы балансировать трафик и распределять его sn bow на по ним вы могли заметить что он bound local рекурсии только на внутренний dns-сервера это сделано сознательно для безопасности в то же время у нас есть отдельный резольвер который только отдельные проекты могут использовать и который уже умеет рекурсии интернет вот так выглядит типичный остап зон конфигурация для n bounce правда для этого sharding пришлось организовать по dns зонам что вылилось в довольно большой конфиг для on баллона около двух мегабайт если хоп на релат занимает довольно продолжительное время хотя в целом от использования нба у нас только положительные впечатления он идет в ногу со временем и даже поддерживает и dns о котором мы еще с вами поговорим таких типичных dns-серверов у нас очень много и чтобы балансировать трафик между ними мы используем экзо би джи пи демон который через бюджет и сессию стар или top of rack switch анонсируют одинаковый и vip с каждого сервера работать заказа бюджет и очень просто нужно всего лишь создать файл на диске и обновлять его каждые несколько секунд контента два файла тривиальный вам просто нужно указать айпи адрес который вы хотите анонсировать и адрес бюджет и устройство которое умеет принимать сессии из нескольких источников если обновление файла не произошло в соси устаревает и switch удаляет маршрут до хоста или же вы можете удалить маршрут сами просто заменив анонс новых дроу файле кто же делает изменение в dns записях ну во-первых сами инженеры делаю записи изменение записях покупка новых компаний валидации доменов и так далее все это требует возможность изменять записи вручную однако люди могут допускать ошибки наиболее действенным методом поимки которых является кадре view и тут даже тут мы автоматизировали процесс у нас есть ли интер который проверяет на самый раз раненые ошибки такие как дубликаты неправильный формат данных или циклы после того как изменение попали в конфигуратор в нашу внутреннюю правлению систем управления конфигурациями оттуда они попадают уже на dns сервера вот пример совершенно в валютных с точки зрения dns записей которые могут ладить быть логически неверными и если первая запись совершенно верно и то две другие представляют собой не очевидным для человека ошибки которые могут теоретически положить весь сайт например мы не хотим отправлять трафик на айпи адрес который нам не принадлежит однако люди могут сделать такую простую ошибку но в основном конечно же сами сервера изменяют данные в dns каждый новый сервер контейнер попадает в нашу систему конечно же мы должны убедиться что этот процесс максимально быстрый безопасный и надежный для того чтобы запись из серверов попадали в наш dns у нас существует специальный фреймворк написанным на питоне у меня с отдельно отношения капитану но мы его используем просто потому что это очень популярный язык и все команды могут легко его освоите использовать отдельной команды могут создать свой уникальный task который будет экспортировать данные из абсолютно любого источника данных к нам в систему хорошим примером здесь будет апекс данный task следит за микро сайтами расположенными на внешнем облачные провайдеры после получения айпи адреса он добавляет запись в наш dns существует множество других to suck так и как а по discovery например который добавляет адрес а сетевых карт и устройств подобного рода тоски через наш open source рпц 3вт пишет в день с митей top service написаны на гол который агрегирует его лидирует данные если новые данные не прошли проверку utair оставлять старую версию и сообщает об ошибке дежурному если данные прошли проверку они записываются в зуке пир нужно упомянуть что в зуке пираньи все еще хранятся независимо друг от друга и даже хранятся в разных диодах после того как данные попали в места хранения мы переходим к следующему шагу объединению и логической валидации presenter сервис написаны на go который множество независимых источников генерирует один файл в тайне dns формате заодно он проверит что между записями нету дубликатов конфликтов если они все же есть презенты расставить приоритеты где например записи созданные вручную имеют наибольший вес вот так выглядит полная версия dns pipeline как и в случае с картограф тором publisher превращает результаты работы презентер в базу данных сидибе запускает тесты и начинает раздачи черри нашу торрент в его структуру то невинность поддерживать sea cup no reload что еще больше ускоряет процесс еще пара презентер publisher продублирована для разных швартов что тоже ускоряет процесс несмотря на то что pipeline выглядит довольно сложно с большим количеством проверок и компонентов большинство данных проходит через нее и распространяется на сотни dns-серверов папе торрент всего лишь за пару минут как вы можете видеть как минимум в пяти местах у нас производится проверка данных что помогает предотвратить серьезные инциденты в какой-то момент записи даже распространялись настолько быстро что мы с ужасающей регулярностью начали убивать ssd-диски чтобы избежать этого мы переместили базу данных сидибе в нпф с тем самым сэкономили кучу ресурсов и еще больше ускорили работа dns прелесть этой papillon еще в том что нам совершенно не нужно ничего менять если мы добавляем или удаляем dns-сервера каждый клиент каждый сервер имеет полностью независимо представления и чтобы добавить новый сервер нам просто нужно установить нужный софт на него все же надо сказать пару слов про сидибе про которые я уже несколько раз упомянул сидибе расшифровывается как константа the bass и не случайно ее невозможно изменить можно только пересоздать написанная даниелем бэушный нам автором тайне dns это безумно быстрая база данных за счет неизменяемости и отсутствие log off однако с ней не очень просто работать код сильно запутан где например есть множество 1 буквенных переменных ну и конечно же такая чистота обновления в основном репозитории не может не смущать однако несмотря на это так никто и не сумел ее хакнуть и получить свою обещанную награду которая станешь было 1000 долларов сама себе получается из текстового файла записано в определенном формате у меня лично этот формат вызывается ужас и недоумение если немного разобрать формат для пива 4 записи то первый символ тут он указан как + означает эта запись а типа летел вместе с time stamp могут выполнять разные функции если указать и тел в ноль the time stamp будет как раз тем временем когда запись появится онлайн довольно удобно л.о. как раз является тем самым механизмом для сплит horizon dns который так эффективно используется картографом в места л.а. по факту вы можете указать клиентскую потеть и только для нее вот эта запись будет возвращаться теперь немного статистики по нашей dns большинство запросов и это измерено с клиентом выполняются менее чем за десять миллисекунд суммарно у нас несколько гигабайт dns записей с таким количеством проверок ошибки очень сложно попасть на прод и честно говоря я не помню ни одного серьезного инцидента за долгое время прохождении через pipeline занимает до пяти минут обычно около 1 минуты это значит что с момента как вы как инженер создали запись в своей базе данных в своем приложении мы будем отдавать айпи адрес в течение 1 минуты за счет шарди рования достигается еще большая скорость распространения данных нельзя конечно же не упомянуть и о надежности нашей системы на каждом сервере facebook у нас установлена специальная проверка которая призывает тестовые записи причем эти записи рандомизированы чтобы избежать получения ответа из кэша on bound все эти данные отправляются в нашу систему мониторинга и мы получаем alert и если качество сервиса ухудшается причем поскольку сервис все еще написан все еще используют в основном ю т п потери могут быть связаны как сетью так и с загруженностью самого клиента например несмотря на это в общем случае запрос клиента не приходят только на один из десяти миллионов запросов что является довольно неплохим результатам док ходим это очень интересный термин который широко используется войти кругах по факту это означает что если бы мы производили собачий корм то были бы настолько уверены в его качестве что ели бы его сами так и с нашими сервисами мы настолько уверены в них что используем альфа версии чтобы тестировать последний vici и баги так причем с dns спросите вы как вы помните ли как вы помните пользователи делают запрос к нашему внешнему dns и получают айпи адрес наиболее оптимального для них для них расположения во внутреннем же dns оперативный сервер спрятан за on bound который обеспечивает рекурсию это значит что все запросы атон bound будут приходить к тайне dns и тайне dns не будет знать кто оригинальный клиент на самом деле многие могли заметить что это та же самая проблема что из публичными dns резольвер его совсем недавно еще эта проблема решалась оригинальным методом у нас был dns идти мы его называли состоял он еще с одной пары unbound этой не dns on bound boston фигурировал таким образом что если на него приходили запросы от для facebook.com instagram.com или других бог ходим доменов мы возвращали ответ с локального тайне dns который возвращал докладе gps в остальных же случаях он рекурсия впрок эта хитрость работал довольно стабильно но добавляла сложной системе вообще вопрос с рекурсивными серверами между клиентом и авторе пассивным сервером волнует не только нас google amazon другие компании все сталкивались с этим ограничением и также как вы помните это периодически влияло на работу картографов к счастью не так давно и постепенно набирает все большую популярность расширенный стандарт для dns так называемый и dns суть проста клиент или артрит активный сервер извиняюсь клиент или рекурсивный сервер добавляют оригинальную клиентскую потеть прямо в день с пакет который через сколь угодно рекурсивных серверов доходит до автор это тивного сервера если вам сложно это представить можете подумать о export for hdp заголовки из веб мира вот пример тисе пи дампа без и dns и с и dns где в dns пакете отчетливо видно сеть которую мы передали теперь мы можем с легкостью избавиться от dns сайте и уже знакомая нам схема будет выглядеть значительно проще где рекурсивный inbound просто добавляет и dns информацию о под-сети в оригинальной подсети и передает ее авторе то тивно му серверу и теперь мы можем с легкостью вернуть доков уделяет и нашим сотрудникам и продакшен айпи внешних заключение бы хотела сказать что несмотря на то что протокола dns уже более тридцати лет на его основе можно делать очень серьезное улучшение которые помогут экономить много времени и денег мы смогли уменьшить сетевые задержки для внешних пользователей и обеспечить док пудинг для внутренних пользователей и поскольку потакал не стоит на месте очень скоро вы можете увидеть интересное решение такие как dns о грехе tps практически все эти решения вы можете собрать самостоятельно на базе окон сложных компонентов большинство про большинство я уже рассказал некоторые упустил например прокси джейн этого заговора у него балансировщик нагрузки вы можете сравнить его с яндекс одной из функций эти чем это наш runtime для нашего собственного диалекта печки которая называется шаг айпи вес мы используем для низкоуровневого балансировки ну про стальной я более-менее рассказал на этом все спасибо за внимание привет я такой вопрос вот кстати рассказал схему доставки изменения dns-записи до ваших серверов там получалось примерно две минуты но для литре балансировки для третьего или для серверов физически все более менее понятно ти тайминги достаточно но вас наверняка внутри есть контейнер систем управления контейнерами и и каждый контейнер как правило и всегда своей пи адрес и сервиса скорее всего часто сделала discovery на нужен sbs discovery и хочется понять как вы в этом с такой интеграции живете именно в контейнер нам мире потому что кажется что две минуты плюс кэширование которые еще один assisted элем происходит она уже дают большие задержки по сравнению с то скорость в которой контейнеры по системе доплыть до хороший вопрос на самом деле то есть во первых сами dns сервера у нас не использу контейнер мы используем только бормотал железки вот действительно нас постоянно появляется исчезают новые сервера им нам нужно контейнеры и нам нужно добавлять для них активно записи вот на текущий момент это действительно происходит там в течение 1 минуты то есть мы использовали разные шарды как я упомянул и вот именно контейнеры они используют отдельный shard и нас насколько насколько я помню то один из меньших сортов то есть на самом деле про повешен для конкретно вот этого шарда для the power это право это наша система контейнеризации занимает около 1 минуты вот то есть это пока что минимум который мы можем обеспечить но у нас в планах есть большое улучшение которое например поскольку у нас используется такая система которая называет у нас две системы на самом деле есть одна называется айла про которую есть много разговоров публичных и вторая называется api пор task то есть у нас есть контейнер и каждому добавляется айпи вот и те и пи мы можем предсказывать на основе каких-то параметров и у нас сейчас есть в планах чтобы мы эти api прямо отдавали из самого dns а генерируя по тому же алгоритму то есть у нас не будет про повешена вообще то есть это приходит ds мы даже лука в базу данных не вам делать все тебе мы будем напрямую это отдавать то есть вот это есть план но пока что да у нас есть ограничение в одну минуту то есть контейнер стартует и да но справедливости ради контейнеры мы подключаемся не по dns в основном у нас есть специальные утилиты которые с кейджу лера это тех самых контейнеров обеспечивают и тебе предоставляют то есть ты делаешь ти w как топор в ссср дальше контейнер и он уже попадает потому что он ходит с кейджу он знает айпи адрес то есть мы ходим без dns в контейнер и то есть это не то что прямо острая супер проблема но до хорошего хорошего про сего здравствуйте здрасте у меня вопрос спасибо 1 чит за доклад у меня вопрос про составную часть dns конкретно про тела с я видел на 1 с кем у вас менее 100 миллисекунд на их in шейк и хотелось бы может быть если вы знаете приоткрыли завесу как вы достигли результата 90 миллисекундах и джейк сейчас я промотаю то-то по-быстренькому то не то мы сейчас говорим о про этот слайд да за раз какая да то есть 4 этап на рукопожатие с точки зрения сетевых задержек этом мы рассматриваем да то есть про то что там генерация занимает некоторое дополнительное время мы здесь не упоминаем но это за это приходится платить в любом случае но как еще есть происходит оптимизация и в принципе мы используем это насколько я помню что в некоторых случаях можно обойтись даже 2 этапным хан шейком если этот сервер когда-то недавно уже был использован то есть вот это вот время которое может добавиться на генерацию ключей мы используем на мы отсекаем на том что это повторное подключение и мы спали только двойных от шейк да но я очень много про это рассказать не смогу сожалению я не в этой команде работаю вот я больше именно по dns у специализируюсь но да вот все все все что я могу ответить на это вопрос спасибо большое за год за доклад меня зовут артём и из компаний одноклассники хотел бы спросить последнее время dns стал тем местом куда больше всего через которое хакеры проникают сети и атакуют клиенты за счет подмены и так далее инвалид ации кэша и поэтому есть прям тенденция и на самом деле она уже довольно-таки старая защищать лес появляется dns скрипты подписи с днс зоны так далее можете вкратце рассказать про это не дрянной или нет у нас было довольно много x3 до иначе каждая я понял да у нас было довольно много экспериментов с этим мы пробовали и dns скрипта dns cd и на самом деле мы активно к интервью чем прямо сейчас вот почему я упомянул денисович дпс вот то есть вот эти все методы защиты мы действительно пробуем и пользу каком-то в мире их тестируем проблема в том что мы еще пока не выбрали потому что если мы делаем это мы делаем это хорошо и dns всех все еще в довольно плохом состоянии то есть например тайне dns из коробки его не поддерживает нам приходится его сами самим имплементировать вот и если мы сами имплементировать мы можем это сделать не очень хорошо из-за этого это может быть какая-то опасность еще больше например да когда все доверяют а мы плохой сервис проводим вот из-за этого мы все то и не анонсируем это и мы еще не говорим что это готово но мы активно работаем дествительно в этих направлениях но пока что денисович дпс выглядит наилучшим образом мы активно участвовали в rfc когда у стакане вали что конкретно мы хотим чтобы что мы видим в будущем вот то есть в будущем из теперь скорее всего я понял спасибо большое привет спасибо за доклад вопрос такой вы говорили что на записи настолько много что базы получаются в районе нескольких гигабайт их перезагрузка занимает довольно долгое время не приводит ли это к тому ну вернее монету ем перед как вы решаете проблему с тем что dns-сервера вынуждена постоянно перезагружать большие базы и заниматься только этим да то есть тот тут немножко мы перемешали парочку терминов то есть рилот долгое время занимает он бандана его конфигурацию потому что 2 мегабайта конфликт большой конфеткой он вам просто некогда по факту не используется с таким задаром коллегам с сидибе конкретно с базой данных для тайне dns у нас таких проблем нет потому что она поддерживает сикх об на релат и по факту поскольку мы еще храним это втм pfs то этот сетап занимает миллисекунды то есть тайный dns он действительно тупой и он действительно однопоточный и он действительно подвисает на миллисекунды потому что мы делаем уму сикхов но по факту этот всех бьет только м м ap то есть он просто перематывает файлик который уже лежит мпф с в памяти то есть это происходит реально моментально и мы действительно это это вообще не проблема для нас то есть обрело занимает миллисекунды вот привет спасибо за доклад алексей чупин киршин вот сказал что с помощью 1с вы можете получать айпи адрес исходного клиента да то есть но учили 3 курса попадают либо например как осуществляется где а балансировка то есть вы используете какой-то своих партнеров допустим или вы сами вычисляете гиа расположению ну то есть вот близость клиента к попу сейчас я тоже на самом деле это так ну ладно неважно на самом на самом деле то есть у нас много пользователей пользуются фейсбуком и в вашем регионе ну предположим вот прямо из москвы возьмем пример да москве много людей пользоваться фейсбуком и мы знаем примерно из какой точки москвы сколько задержка происходит потому что мы постоянно каждый 40 1000 запрос как я упомянул вы постоянно отправляем на различные поп и пробуем их и у нас есть базы данных в нашей внутренней системе которую мы да это наша система который мы храним эти записи и мы говорим о кей вот из этого региона от этого провайдера более-менее вот до этого места такая вот задержка и мы постоянно это пробуем и например если поп какой-нибудь очень сильно загружен мы говорим о кей пользователь ты можешь двинуться на соседний потому что там загрузка тоже довольно более-менее приемлемое спасибо то что вы сделаете сами это какая-то необходимость ну так просто потому что допустим ваши партнеры не за тот же команд занимается тем же самым вы это делаете сами почему-то или вот так не знаю так исторически сложилось то есть 2 разных есть тоже вещи akamai это седин которую не только ну мы за ним и мы его используем как как сидел это очень мало на самом деле там меньше чем какой-то малюсенький процент сейчас логическая чуть лишние но не важна суть в том что вот картограф фирмы используем да так называемых динамических запросов когда пользователи приходят получить там основную веб-страницу когда им нужно статика и например вот ваш news feed ваша лента новостей когда вы на нее смотрите мы уже заранее inject им вам туда потому что мы знаем где вы находитесь более-менее мы туда inject им адреса поп который более-менее для вас оптимален урок в районе да то есть там плюс минус полмира вот и это 2 то есть это отдельный разговор про это можно вести как на шоссе дейэн работает но поскольку это всё сконцентрировано на поп то по факту 1 динамический запрос мы балансируем просто выдав вам правильный адрес а потом ваша news feed которая она формируется абсолютными юрий ламин на ближайший к вам поп окей вот все просто спасибо за доклад вопрос такое правильно я понимаю что их за бюджет используется для связи n баунти тайны dns то есть тех забить жди это отдельный демон который запущен на сервере и интерфейс не взаимодействие один из интерфейсов это просто файл мы в этот файл пишем вот этот айпишник который может быть в россии 1917 если правильно помню до внутренней api адреса приватные пи адреса мы просто ему говорим пожалуйста анонсирую вот этот айпишник с меня на switch мы и экзо бюджеты и и вот на этом мои печеньки может быть запущена нба und может быть запущен тайне ты не сможешь запущен ваш собственный сервис апачи что хотите неважных просто айпи адрес любой рандомный вы просто говорите пожалуйста switch который наверху за меня анонсирует что я здесь нахожусь вот и много серверов так делают и switch уже просто балансирует нагрузку между айпишник ами вот этими посвящен показывается минут нет нет я то есть это физическое устройство мы называем top of rack switch то есть у нас есть стойка в ней куча серверов на самом верху у нас действительно самом верху стоит свечь который принимает gps сын bound как вот на сервере которая запущен это просто сервис это можно можно травмироваться от него представишь это apache или что это тайне и что это он bound что это что нибудь другой не важно он должен работа на каком-то пи адрес и правильно что мы делаем это мы анонсируем этот ай-пи-адрес к свечу и говорим я здесь нахожусь а до меня такая то метрика хлопов сколько там до меня прыгать вот и вторая часть второй вопрос про бюджет и рассматривали ли вы вариант следующий когда клиенту возвращается некий айпи адрес вот и этот ай-пи-адрес через бит gdp соответственно наносится там со мной а с другой так далее же провайдер выбирают кратчайший маршрут рассмотрели вариант вместо карт ургов карту грифера и вместе с ним ну то есть это было дока картограф мира по факту то есть мы изначально пользовались так это да это самый простой метод на самом деле когда вы анонсируете одинаковое peace кучи разных мест и провайдер уже сами решают куда вам идти но во первых провайдере делают плохо и это очень долго контроль очень сложно контролируется и долго обновляется то есть если вы захотите выключить этот паб или на нем огромная нагрузка вы начинаете там меньше анонсировать пока это все располагаться по интернету ваш папа уже дымится будет вот мы его используем вот этот метод для того чтобы изначально вот там был слай когда в внешне как когда публичный dns-сервер пытался зари курсить и вот этот вот самый публичный dns-сервер он попал на ближайший паб по dns то есть он у он узнал наш автор это тивной dns у него а . н.с. facebook.com вот этот api адрес который он действительно работает вот именно по такой вот системе как вы описали то есть вот этот вот первая попасть на любой ближайший dns работает именно так ну дальше мы используем карта гроттер просто потому что это а быстрее bb mmorpg более предсказуемо и у нас лучший контроль override спасибо за вопрос алексей мтс сказать просто как вы определяете собственным ближайшего клиенты вообще адреса если dns не поддерживается поблёкли долларом white так что не присылает простых много нигде не конца понял вопрос в плане если если ко мне приходит и единая 100 ну то есть мы по умолчанию работаем без и dns то есть к нам приходит то есть обычно клиенты ходят через своего провайдера то есть вы сидите дома у вас есть какое-то там провайдер этот провайдер приходит к нам как вот recourse рекурсивный dns-сервер да у них есть как правило провайдеров они приходят к нам и говорим о кейс этого провайдера мы знаем что примерно у такой талей концы и мы просто записываем это всем наша база данных и поскольку клиентом является именно провайдер не конечный подать на конечный пользователь скорей сидит за натом у него там куча фаерволов и прочего к нам приходит именно рекурсивный сервер от провайдера и мы уже знаем что вот это согнет который нам провайдера слэш не знаю по пиво 4 там слэш пусть будет на 4 неважно приходит на мы знаем что для слоев 24 вот это вот сеть мы позже мы вот вернем вот этот адрес паб это вы говорите провайдер sky игры про публичные да про публичной dns если они и нам не передают и dns то эта проблема действительно и чего делаете вот вопрос собственных в этом ничего не делаем то есть мы ожидаем либо единиц и честно говоря то есть я упомянул что у гугла не так все с этим плохо по двум на самом деле причина первая google передает эти и dns то есть когда мы приходим к когда клиент приходит к dns они записывают ваша вашу под-сеть оригинальную и передают ее то есть если вы поддерживаете вы уже можете этим пользоваться и второе это то что что google тоже делает они выход из язык инфраструктур и публичный на сервер ты тоже не один единственный сервер вниз восьмерок нам приходят вот что они делают они на самом деле понимают где мы находимся и они присылают нам пакет с айпи адреса который максимально близко к нам то есть они эту часть тоже оптимизируют но это только google так делает то есть там если вы пользуетесь какими-то странными dns резольвер ами то этого конечно не бойся вы будете мучиться потому что мы не можем определить другая ситуация если пользователь ну понятно да да спасибо за вопрос секунду скажите неужели вот такой оптимизациях и джейка действительно стоит свеч да очень стоит во первых это гораздо лучший опыт использования сервисами то есть когда пользователь не сидит и не ждёт загрузку и полсекунды 600 миллисекунд чувствуется действительно для пользователя и запросов мы делаем очень много естественно то есть каждая как каждое обновление то есть это это много не сильно запросов это 10 чувство разным а записям к разным да то есть множество запросов к различным на нас есть дополнительные сервисы естественно то есть есть веб есть дополнительные какие то есть мобильное приложение она ходит не дать на к одной точки только то есть она может ходить к разным дополнительным . спасибо"
}