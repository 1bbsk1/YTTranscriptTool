{
  "video_id": "n6BvAi9skgg",
  "channel": "HighLoadChannel",
  "title": "Event-based self-healing monitoring / Кирилл Сотников (Cognician)",
  "views": 194,
  "duration": 2187,
  "published": "2017-04-22T14:47:52-07:00",
  "text": "меня зовут кирилл я приехал из самары работаю системным инженером больше пяти лет сейчас работаю на южно-африканскую компанию которая находится в кейптауне удаленно соответственно это очень далеко на самом деле в компанию нас маленькой поэтому мы предпочитаем называть как мишин поэтому предпочитаем использовать очень разные сервисы so as a и все прочее и поэтому сегодня слал amazon будет звучать достаточно часто но я хочу обратить внимание что amazon здесь просто как пример и я хочу рассказать не про решение конкретно прибиты гвоздями к амазона а про концепцию и какие-то примеры как это реализовано на базе готовых кирпичиков вымазали так мониторинга он совершенно бывает разный и уже сегодня звучали разные варианты как это все оформлено у кого как это сделано у кого-то разные тулы у кого-то целое отделаю который следят за всякими метриками и прочими и но сводится к одной картинке который вот сейчас вы можете увидеть то есть это наша инфраструктура это сам мониторинг и какая то реакция на alert и который этот мониторинг производит я не буду сейчас углубляться во все сферы мониторинга который может быть остановлюсь именно на али ртах и на реакциях сами как бы каждый alert он по сути требует чтобы построить достаточно хороший мониторинг каждый allure требует реакции то есть обычно это человеческая реакция и если alert не вызывает реакции то в результате мы получаем просто шум и каждый allure даже если он бесполезен и он должен быть него тоже должны быть реакцией то есть это либо он должен быть удален либо он должен быть изменен так чтобы следующий раз он был полезен это все приводит к тому что обычно по ночам когда все плохо вот такая вот картинка что человек просто просыпается плюс люди бывает не спят и едят ходят в отпуск они уезжают куда-нибудь где особо нет интернета всякий теплые моря и так далее поэтому это как бы слабое место именно во всем этой системе и и я как бы поэзии немножко изменил эту самую схемку конвейеры и хотел бы рассказать как именно пришло в голову вся концепция и рассказать это на примере который был в жизни то есть такая схемка просто обычная достаточно стандартная инфраструктура которая состоит из просто node bin sehr front and back and и за брендом есть некий сервис неважно какой сервис в нашем случае это было две ноты active rest and buy ну соответственно только одна в один момент было рабочий остальная просто ждала свои своего как моментам из один прекрасный момент что цепь ему становилось плохо в результате стендбай подхватывал на себя его роль и остановился с тем двоим и на бумаге в документациях и все выглядело красиво но в реальной жизни получалось вот так то есть у нас все переключалось на стендбай пропадал не работал и никогда больше не возвращался соответственно следующий failover наш стенд бай наш процессор лавера он просто был запилен и как нужно было как-то решить чтобы следующий чтобы вернуть наш стенд бай как-то найти решение того чтобы его починить и чтобы не было такого что приложение просто не работает и пользователи становятся несчастными потому что они не могут пользоваться нашим сервисом был очень потрачено много умственных усилий как это реализовать началось все в лоб с того чтобы настроен alert который естественно нужно было реагировать и нужно было просто было решение того что единственного я просто был выключен и соответственно в тоске лин групп поднимала просто новый приложение заново deploy лодзь и все хорошо работал продолжал ну естественно требовал человеческого усилий и поэтому не она не нравился я еще плюс ленивый поэтому не люблю руками работать и я долго думала и подумал почему бы не использовать amazon адскую линду это относительно свежий сервис у которого есть ряд особенностей одна из основных особенностей индезит то есть она реагирует на какой-то определенный event-ы может быть допустим плюс она не висит постоянно включенный она включается только именно в тот момент когда она необходима соответственно получается дешевле потому что amazon берет деньги только за то время из-за те ресурсы которые конкретно отработали если у нас лямда отработала раз в месяц и взяла там 100 мегабайт памяти использовал то мы заплатим только за это все остальное искусство что она выключена она будет бесплатно + у неё достаточно большая поддержка различных языков программирования и можно написать все что душе угодно на одном из так сказать экстремумах языков я взял про это не буду рассказывать просто я пишу их на коже то есть целиком поддерживается все работает никаких проблем нет одном докладе я говорил про вчера это было про сложность и когда вот и на предыдущем докладе pro bass bassoon как это правильно произносится говорили про то что очень тяжело какие-то патче применить какие-то добиться от разработчиков каких-то изменений ли чего чего то здесь в этом решении этого нету потому что у нас свой собственный код он небольшой простой и мы можем его что переписать что поддерживать что сделать всё что угодно плюс еще наверное последний важный это надежно когда мы что-то запускаем для того чтобы нам что-то починить это должно запускаться откуда-то извне потому что если мы запускаем что-то для того чтобы починить свои собственные то у нас получается что мы можем потерять этот самый сервис который чинит или если мы его будем располагать ну например как манит их которые можно использовать тоже для каких-то действий на инфраструктуре но дело в том что он хостится на тех же самых серверах которые мы с мониторим так сказать и он может отвалиться то есть лямды такого не будет она достаточно надежно и поддерживается третьими лицами как который который не имеют отношение к нашей инфраструктуры так вот отвлекся и решил применить для моду и конечно решение выглядит так то есть у нас есть актив стендбай но да и у каждой ноты есть своих орбит в один прекрасный момент случается процесс файла вера их орбит у нас один отваливается в этот момент у нас срабатывает наш alr который у нас на alert который у нас настроена к удаче и мы его отправляем в sns топик с топик является триггером для лямды это лямда уже запускает наш кусочек кода который мы сделали наш кусочек кода который мы сделали и она не цели зиру и ту же термины и тенденциях который до этого я делала руками и он получается просто убивается of those келин группа его перезапускать заново все никаких проблем все работает наши инстанции в если у нас пропадает стендбай пропадает пропадает hard бит и лямда его автоматически сама чиниться то есть это может произойти в любой момент я сначала следил конечно за этим потом перестал на него вообще обращать внимание просто иногда вижу что у меня приходят там письма о том что топик и получил сообщение и там либо инст вас был запущен новый вот после того как я это все реализовал я не показал что а почему я не могу использовать это для других случаев это в принципе достаточно хорошее решение и мы по идее можем применить это любой формализуем эй случай формализуем и я имею ввиду что мы можем четко определить что мы у нас есть какие-то какие-то инциденты на инфраструктуре какие-то которые мы можем описать есть четкие действия которые мы можем найти инцидент сделать от отреагировать то есть у нас получается что мы можем формализовать так называемую инструкцию что очень четко понятно что делать скажем или друг в одном или ином случае здесь важно отметить что еще отвлекусь есть такое понятие alerts фотик это пришло из медицины у них практически аналогичной проблемы были очень много alert of на системах жизнеобеспечения и только единственное отличие что у нас это железки а у них живые люди которые как бы являются именно источниками этих данных метрик и allure тов и у них была проблема то что вешались alert и практически на любой показатель и в результате это также игнорировались alert и как и у нас там есть как бы золотые правила которые вышли из этого из-за частей софта есть одно из них что alert и должны вот постоянным быть улучшаемое и их должно быть мало они должны быть только вот важными иначе что мы просто перестаем на них реагировать отсюда появилась вот такая вот схемка то есть добавился просто один квадратик который подразумевается себя так называемый хендлер то есть мы берем все наши alert и которые у нас бывают и мы их просто пропускаем через еще один одну стадия через один хоп и в результате в зависимости с того что это за alert мы просто на него реагируем по другому то есть это либо идет реакция на самую инфраструктуру либо это идет alert на уже девелоперам operations почему alert не идет сразу с мониторинга есть такая вещь как виктор abs это формализовал то есть называется транс modifier что это такое это дополнительный контекст для каждый как бы инцидент и то есть alert не просто какое-то слово что чувак хотя там сломалась базы данных иди сходи посмотри он добавляет какой то контексте этот самый alert и например добавляет график предыдущего времени какой-то промежуток который возможно может помочь решить именно эту проблему то есть это можно тоже реализовать помощью лямды и слать alert они напрямую с мониторинга а пропускать их через хендлер идти в наш мониторинг за графиками вы выдергиваете оттуда метрики и уже после этого отсылать alert и то есть это примерно как это все работает сейчас хочу несколько сценариев продемонстрировать что очень различны и применению всего этого мы можем как бы проявить фантазию и добавить разных кейсов простой самый сценарий кончилось место на диске думаю много у кого бывала в юности когда начинали работать то есть куча решения которые уже готовы как бы зачем изобретать велосипед как говорится но мы можем здесь посмотреть как кое-какие кейсы которые нам помогут в другому немножко взглянуть на проблемы та же самая общая схема красненьким отмечено что проблемный хост зелененький все дни проблемные хасты и первое решение на первом решение мы просто запускаем так как в предыдущем случае мы также запускаем ленту запускаем такой же самые и прикол и просто выключаем и the instance of those келин группы его создает и как бы новый инст вас уже с пустым диском все хорошо никаких проблем нормальное решение да да да такой это ближе к имеют ability сервером идет что мы их не меняем мы их просто пересоздать можем подумать что у нас например железные сервера мы не можем их грохоте тогда мы можем запустить лямда и просто в этой ямке формализовать и сечь команды которые у нас все почистит какое-то место зависимости от того какое приложение какой сервер и так далее все это можно формализовать и в результате это просто кремом или там чем-либо как угодно эхо стрелочка так далее просто за транг эти сети файлы там логик кэше и так далее что что-либо бывает часто вылетает отжирает место как был примерно того как такой простой кейс обработать еще час и сценарий просто сервис недоступен сервис не доступный сервис может вызвать кучу причин если наоборот кучу причин может вызвать не доступный сервис и здесь важно как бы понять что нужно именно найти что именно вызвало не доступный сервис мы не можем просто так пойти все по грохоте заново создать то есть опять наша съемка допустим у нас фото 1 backend он кончился больше не работает но это скорее всего не знаем пока спим пока пока у нас пользователи не могут достучатся что мы делаем мы точно также запускаем лямбду по триггеру с cloudwatch и или с любого другого мониторинга на самом деле который позволяет сделать alert то есть на любой вкус и также запускаемые с команды которые пробегается по нашли инфраструктуре и находится в чем проблема то есть они могут также по как во всем хастам в инфраструктуре пробежаться и например найти что у нас кончилась память палец процесс не прослушивается порт и уже в зависимости от этого мы можем тоже придумать варианты то есть мы либо можем грохнуть просто backend и создать новый и ну в это время конечно у нас у нас равно бы кант лежит поэтому мы можем грохнуть создать новый немножко времени потребует второй способ мы можем просто запустить также прогнать сыч команды запустить чеки найти что-то и плюс вызвать опять другую amazon лямбду которая совсем с другими аргументами что-то сделает совсем другое допустим она может поднять backend instance и запустить там он символ который нам как-то перри подготовит наш installs который ну и закачать какие-то данные может быть все что угодно уже насколько фантазии хватит тот илья рассказывал про вчера про он символ пас который там был мини пас то есть это можно в принципе применить и именно триггером уже от запускать лямда который будет работать с ansi блам и готовить какие-то вещи буквально можно даже сделать какой-то дашборд колен можно на кнопку нажимать и будет по кнопке запускаться лямда который будет готовить нам надо кластера все это можно применить в результате мы получаем просто выключены и incense и новый включены instance уже подготовленный рабочий и обратите внимание что никто не просыпался бэкон сам починился но возможно там 15 минут пользователя не смогли залогиниться пока болезнь была переподготовка всех один способ переключения deployment и и так далее нужно время downtime у нас выходит только на это вместо того чтобы мы ищем людей которые ты че нет мы их нашли они потом эти люди начинают искать интернет который у них заработает потом они начинают искать проблему находят проблему решает ее и плюс вот это время которое у нас было потрачено на deployment и и так далее то есть мы экономим время downtime у нашего и соответственно нашу репутацию а еще один интересный сценарий это возросла нагрузка но тоже пользуется amazon и может возразить есть поддержкой при выросший нагрузки amazon сам может запустить наши новые инстансы и зачем как бы нам здесь вообще велосипед вы наверняка опять съемках детства забыли зачем здесь может понадобиться наш велосипед наше решение то есть допустим мы не можем просто так взять и запустить incense нам нужно как-то хитро подготовить наши другие сервера допустим если мы не там не пользуемся консулом хотим прописать новый backend в конфиге других серверов то есть мы можем сделать это также с лямды мы можем также как я уже говорил запустить enfeeble который подготовит наш instance мы можем в принципе мы можем сделать всё что угодно потому что мы работаем с низкими абстракциями мы работаем с цепями языком программирования в этой линде и нам не нужно вообще нет никого ждать нам не нужен ни как нам не нужно ждать когда кто-то поддержит какой-то хипстерской этом базу данных или еще что-то какой-то сервис мы просто через опять можем причем какие-то хитрые сложные вещи это буквально каждой функции выглядят 50 строк это очень редко когда больше 50 строк если там что-то совсем хитро вот уже второе решение вот раньше был я говорил можно запустить его можно там можно шеф соло потому что нравится и как-то наши сервера перри подготовить они просто что мы подняли ресурс а на этот ресурс еще с чем-то должен быть вот я поторопился транс малгрейв фаером вот так примерно выглядит waffle доки графики который здесь конечно очень такой ну дурной пример как вакууме то есть показывает именно сразу приходит alert сразу приходит с графиком и говорит примерно что куд-куда именно нужно копать так же мы можем применить это решение не только какому-то починки но мы можем еще помочь де бо жить можем помочь улучшать приложение можем работать с исключениями то есть возвращаемся к нашей съемки добавляем что у нас выскочил exception кто не знает клад watch позволяет еще и логе держать куда отправлять поэтому у меня все все в одном диалоге и и метрики то есть наш также exception убегает в cloudwatch на 5 запускается лямда и уже лямда идет в cloudwatch обратно который берет от туда какие-то метрики зависимости от того что за эксепшен выскочил и уже формируют alert и вместе с этими метриками отправляет уже зависимости того кому это интересно настроена может быть программисты админы operations и кто угодно то есть там даже есть у нас какой то не критично эксепшен мы можем с ним работать мы можем его просто как-то благородно оформите помочь людям с с этим получить больше информации добавить контекста немножко сложнее то есть мы добавляем сюда не только alert еще так же реакцию на этот exception то есть мы также ну например какие бывают xl база недоступна хвост какой-то там любой мы также можем запустить опять проверки на наш инфраструктуре также можем опять запустить реакцию на это инфраструктуре и что-то по там зависимости от того что за exception не дожидаясь пока у нас упадут метрики или еще что-то уже принимать какие-то шаги самый наверное страшный сценарий регион я имею виду она зоновский регион то есть бывает такое что у нас целиком отваливается все и вообще ничего не работает мы не можем ни до чего достучаться и не понимаем что делать есть два варианта решения самый простой вариант решения это держать копию инфраструктуры в соседнем регионе либо вообще в другом дата-центре и уже случае каких-то ошибок уже на дальность с этим работать просто переключаем dns и мы можем также для лвс лямды можем руками там то есть здесь просто пример того что это возможно второе решение более интересно в случае если у нас все упало не работает наш регион то мы берем просто и запускаем опять наш лямда и запускаем клауд формейшн инси был там читала что у грунтом паппет кому что нравиться и по нашим всяким рецептом мы поднимаем точную копию нашей инфраструктуры накатываем базу с бэкапа и но возможно мы потеряем какую-то часть данных которые мы не успели там если бы капа у нас там не каждый там 10 секунд прилетают новые данные топ возможно что-то потеряем но это не так катастрофично как например мы будем лежать четыре дня и переносить все это руками или так далее также еще можно работать с любыми пей как я говорил и например в случае если у нас все вообще упал и и мы вообще ничего не можем сделать мы можем запустить твит и сказать ребята временные проблемы и все войдите в положение то есть это нормально практика я вот сейчас наблюдаю что у многих компаний появляются twitter и именно аккаунты со статусами немногие компании прям тут хоп и остальные там очень много кто кто просто периодически мастит что у нас там какой деградации производительности у нас частично не не работаем потом починили . сейчас грамм в порядке вещей тоже такое клауд флаер по моему тоже позволяет настройках сделать отправку если сервис слег через них есть трафик проходит и отправить твит аккаунты корпоративной спасибо за внимание если есть вопросы этом мои контактные данные можете писать вами и так далее спасибо за доклад а скажите пожалуйста вот вы сказали что это event базовая система кто посылает туда овен для того чтобы вот это все произошла ивенты шлет именно 100 систему мониторинга и я забыл добавить как раз спасибо за вопрос то есть вся эта система это не замена мониторинга это надстройкой над мониторингом это расширение его функциональность то есть мы можем взять любой систему которая у нас позволяет делать alert и и позволяет нам сигнализировать о том что какое то какая то метрика вышла за свои пороговые значения и мы можем настроить это чтобы это ну то есть естественно что некоторые системы придется как-то дописывать если это самое главное что эта система должна уметь отправить в sns топик если этот снс топик мы можем с ним работать например вот мы работаем с это догом очередной счас и он умеет прямо из коробки все alert и дублировать любые сны стоппи который подключен и с этого ужаса нас топика она разлетается в разные системы и там можно прям дует и может смс-кой уйти может ну и в любую в принципе там очень много сейчас кантемиров появилась которые могут слушать это все нас топик и получать данные от туда то есть и получается любая система мониторинга является генератором всех этих ивентов которые являются триггерами для amazon лямды то есть ли смысл в том что мы просто расширили наш мониторинг ад это приветствуем меня зовут андрей я тоже слышал волшебное слово про клауд фармишь а уже удавалось встретиться с ограничениями клад фармишь на на количество ресурсов уже как-то эта проблема уже возникала решали или нет меж виду что клад фармишь ограничено может количество самих ресурс описывать ресурсы в одном джейсоне да да там есть ограничение на размер тимплей то насколько я знаю я сталкивался но у меня вложено тимплей ты и с таким наследуемые я понял спасибо спасибо большое за доклад очень интересно такой вопрос если в максимально упростить структуру получается у вас есть какие-то метрики по которым случае возникновения этому критических событий и запускается сторонний некий сервис который делает какую-то магию вот это вот магия она пишется людьми то есть просто запускается какой то скрипт который там по ssh сходит на хост счет игрок нет а вы не боитесь человеческого фактора что вот скрипте который будет чинить тот или иной сервис будет допущен например ошибкой он грохнет там все под корень а вы не боитесь что довело передает ошибку и грохнет своим кодом в приложений что-то очень боюсь я дивился точно такой же вопрос что вы доверяете девелопером писать код приложения почему вы не можете доверить девелопером например писать те же те же самые чеки alert исправления или об сам то есть такие же люди тоже могут сделать ошибку поэтому мы всегда должны резервировать всегда учитывать что это возможно и здесь нет никакой магии в чем еще плюс в том что эти со сам же код эти функции для морских можно хранить в том же самом репозитории их можно точно также review ить их можно точно также прогонять на тестовом окружение и то есть с ним с ними поступаем так же как с нашим собственным приложением и я не вижу никаких проблем что мы там что-то безрезультатно сломаем спасибо добрый день вопрос такой очень интересный сроки были озвучены да там 15-минутный простой недели 4 дня какова ваша сила вашего сервиса да то есть и вообще реально ли использовать вот эту а в таком виде где-то на нормальных боевых систем и там где down down time в пять минут стоит там пару лямов баксов то я понял вопрос опять же мы отталкиваемся от того что все зависит от того какой у нас мы строим сервис и допустим я работаю в стартапе который бит убит проводим сервис у нас как говорится 5 9 8 9 людей силы да если вы работаете с такими вещами которые там простой пять минут и очень много денег то естественно что вы должны обложиться там соломкой со всех сторон у вас должны быть очень много резервированию и то есть скорее всего вам это может быть даже не понадобится единственно как расширения не именно что исправление ошибок на инфраструктуре а как какое-то расширение чтобы помочь разработчикам leap сам просто быстрее понимать что идет не так и так далее но это опять же это не золотая какая то пуля которая решает все проблемы это инструмент который просто расширяет вашу возможность можно образно вы не боитесь попасть вот на такую ситуацию можно назвать ее дребезг мониторинга когда у вас одни и те же события поехал падут в очередь и ваша система начнет там что-то убивать и поднимать и допустим тайминги вот эти они не совпадут и получится что у вас постоянно будет какой-то бесконечно хаотичный процесс да кстати это возможно но опять же мы люди должны думать над тем что мы делаем мы естественно что ситуация возможна но нужна система как бы от контроля не знаю дополнительного что ли или человеческий факт но если что-то у вас сломалась у вас запустил систему которой должна это починить ваш сервис лежит и она перестала сам ученица то здесь появляется человек который просто останавливает эти лямды находят почему она перестала работать если нашел чинит если не нашел и понимать что его лямды там вот как вы говорите треск случился то он просто вместо них работает руками сам то есть никто не мешает самому все это починить потом после если все у вас равно все равно сервис уже лежит если вас что-то такое сломал что что можно еще сделать хуже а как определить у вас сервис находится в состоянии ремонта ямада функций то есть там сейчас происходит там фирменный шин и старт инстансов либо у вас лямда функция что там не смогла сделать либо сама лямбда не работает и вам нужно уже руками это все очень логе лямда сама пишет все свои все принты которые мы в ней укажем она пишет их сразу в клаву дочь и естественно какой признак того что лямда не работает то что у нас в перестал работать сервис если она не смогла если мда не работает она соответственно могла починить что-то что сломалось и если это что-то сломанный лежит то в результате мы всё равно увидим что сервис у нас на мне лежит не работает а саму лиам дуче недостаточно просто это она элементарно лакируется плюс она очень простая потому что это по сути одна две функции кода и что еще принципе там нету сложенных каких-то частей в самой лимбе еще вопрос у меня вот еще вспомнился еще один вопрос есть а не боитесь ли он до блока амазонов ского вообще ну то есть вот как бы бывает а бывает так что amazon не работает я много раз видел меня много раз было желание куда-нибудь переехать как вы со своими все миллиардами то будете валите если чё а а я могу ответить здесь еще человек рассказывал про чат-ботов врача tops то есть весь функционал лямды по сути легко переносится в чат-бота единственное что это должно быть самописный потратить нужно на это время случае лямда мы не тратим времени на реализацию самого функционал и мы тратим время на конкретное применение плюс не знаю wenn du lac мы везде кругом винды ложи на если что-то идёт не так . если думаете об этой проблеме о том что можно там два облака иметь время артом и как половин функционал в одном уроке держать половину в другом ноги как ты их так вариантом делать то есть вам от неважно если amazon скажет что мы больше с вами не хотим работать то мы будем искать решение конкретной проблемы в тот момент когда на возникнет если этого не случается то я не вижу смысла это в любом случае само приложение сам сервис его достаточно просто перенести но без каких-то вот таких плюшек который потом просто позже добавится но эти плюшки тоже не сразу появились мы когда ты без них тоже жили окей спасибо я ещё немножко дополнена счет линда если боятся в интерлока то есть такие штуки как хронос на месте потом де крону бистро и внук то есть всякие стригут кромка то делать такие раздачи почти очень похож на лямбду похоже я просто за vendor ложным озоном игру про другие не знаю спасибо кириллу наступление вам спасибо"
}