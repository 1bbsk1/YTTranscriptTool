{
  "video_id": "Y2TuN3IqAuc",
  "channel": "HighLoadChannel",
  "title": "Как мы управляем трафиком тысяч подов в мультикластерной среде Kubernetes / Максим Чудновский",
  "views": 1369,
  "duration": 2264,
  "published": "2024-10-29T02:37:33-07:00",
  "text": "Встречайте Максим чудновский верте Всем привет Сегодня мы будем с вами говорить про кубернетес про сеш про поды Я предлагаю не терять времени сразу начинать соответственно кто я такой я вте отвечаю за это достаточно зрелый продукт мы его делаем почти 5 лет уже соответственно им пользуются сотни команд это сотни кластеров продакшене которые используют сеш и понятное дело это десятки тысяч подов вам об этом рассказывали соответственно Я делаю много чего ещё дополнительного вокруг если вам интересно заходите на там все ждут милости пром уютное место соотвественно Почему мы говорим вообще пробе Вот скажите поте больше чем один лаер кубернетес класс да в компании соответственно есть такое достаточно капитани мнение что если у вас больше чем один кластер то у вас уже есть кубер мультик логично Если вы думаете что это сказал какой-то человек который разбирается в кубернетес Возможно это так и есть потому что это сказал я только что возм мы снижаем риски взаимного влияния мы берм приложение Селим его в отдельный кластер оно жит там горе не знает все довольны во-вторых это отказоустойчивость сделали несколько кластеров разложили их по дата центрам и тоже всё стало сильно лучше когда разложились по дата центрам сразу поняли что можем ещё так масштабироваться потому что приложение в один кластер не влезает давайте сделаем несколько кластеров и тут же мы можем обеспечить локальность трафика Давайте создадим кластер поближе к нашему пользователю у него быстрее откроется приложение билки к нему быстрее приедет Пицца все довольны Однако этот подход Ну как и все другие имеет свои недостатки соответственно в чём проблема Ну первое - это консистентность кластеров когда у нас один кластер мы можем особо не думать сделать куб сети илип или там м upgrade и всё полетело когда этих кластеров десятки всё это дело нужно как-то синхронизировать если мы живём в Гито обсе то нам нужен какой-то Каскад из arg CD или флакса в общем теперь об этом нужно думать Следующая история - это это nor sou Traffic это тот трафик который порождается вне кластера кубернетес например на мобильном приложении пользователя и заходит в кубернетес через Игрес один кластер один Игрес много кластеров а нам нужно делать какой-то межклассовая Второе приложение которое является соседом в этом кластере и вот было бы неплохо так как у нас есть резерв из пяти кластеров если в одном кластере что-то пошло не так этот трафик автоматически ушёл в какой-нибудь удалённый кластер и пользователь не заметил беды Ну и конечно обозревает - это тоже большая проблема вот на лоде вообще почти каждую конференцию есть доклады Как обрабатывать миллион рейсов в секунду и прокачивать терабайт логов в секунду соответственно Когда у вас есть много кластеров это ещё нужно как-то сводить в какой-то системе оберти единой И это прямо задача задача Но сегодня мы с вами будем говорить историю мы поговорим про то как обеспечить нормальную работу С ивест трафиком в мультиклассер среде соответственно когда я начал это решать я подумал наверняка я не такой уж особенный и кто-то уже столкнулся с этой проблемой и наверняка есть ребята которые это решили и выложили в Open sce поэтому мы сейчас это возьмём сделаем три слайда на конференцию и всё будет готово и в общем это multer Services API это спецификация у неё есть референсная реализация которая находится в Альфе Ну года так три и выходить оттуда не собирается и есть несколько реализаций А ну сторонних живая одна из которых живая и это реализация в Google Cloud вот в Google Cloud это Ja всё остальное так или иначе не работает но сама спецификация хорошая в ней есть три основных концепта есть понятие терсе - Это множество кластеров которые должны работать как один соответственно дальше есть ресурс который называется серс экспорт Мы через него публикуем наш сервис в мультик как обычным кубовый сервисом но только для одного кластера и дальше создаём импорт и начинаем его с ним взаимодействовать и на этом конструкция заканчивается очень хорошая спецификация жалка что никто её не сделал в плане реализации Но чем хорошо Cloud native Community так это тем что всегда есть множество альтернатив почти всегда и первое из них - это CN действительно если cni плагин уже делает оверлей ную сеть в кубе для того чтобы работали обычные кубовые сервисы то почему бы там и не решить проблему мультик подумали ребята из силиум и так и сделали У них есть сиу ш и силиум мультик и это его архитектура здесь есть такая неоднозначная проблема что вам нужно иметь уникальные адреса для подсетей для подсетей подов и сервисов в каждом кластере чтобы они не пересекались иначе мультиклассер сили уми не работает но Окей это можно разрешить а основная проблема здесь в том что как только вы делаете мультик на базе cni этот cni у Вас остаётся навечно потому что стоимость миграции на другое решение будет колоссально дорогой и это очень хорошо для ребят из сиу но не очень хорошо для нас с вами об этом подумали другие ребята и сделали submariner Они подумали если нам плохо делать это в обычном оверлее Куба давайте сделаем второй оверлей И там решим эту проблему собственно и сделали здесь история в том что есть некоторый Gateway в каждом ла между которыми строятся тоннели потому что они будут сообщаться через дикий интернет А дальше уже в конкретном кластере мы набором операторов разрулил маршрутизацию всё будет хорошо здесь есть та же самая проблема с перекрытием сетевых зон а она имеет решение в субмарине но не очень хорошее и в любом случае мы ограничены Ну достаточно низкоуровневое работой сетью А если мы хотим что-нибудь более совершенное там какой-нибудь кластерный качный релиз на пользователей которые являются сотрудниками нашей организации то тогда возникает вопрос А как это делать и соответственно ведь внутри кластеров уже есть решение как это делать Это сеш сеш забирает трафик с приложения и настраивает маршрутизацию балансировку как угодно соответственно Почему бы не добавить эту логику в сеш и не делать тоже самое но на межк уровне и это можно действительно сделать но сервис сама по себе штука не очень простая когда мы туда мультик добавляем совсем станови значно поэтому не каждый это любит Итого получаем вот такое сари У нас есть много решений проблемы ивест трафика и соответственно часть из них просто не работает часть из них имеют проблемы которые не подходят к нашему Продакшен и вот что-то вроде бы подошло но делать это очень сложно а может быть и нет Давайте посмотрим соответственно самый популярный Service ISO у нас Open Source ядро это тоже и поговорим про и и имеет варианты мультик деплой и Первый из них я называю гаш идея очень простая У вас есть два кластера в каждом из них есть сервис меш они объединяются в один гигантский ш соответственно вы просто открываете доступ к API серверам кластеров э каждый isod каждая контрольная панель начинает следить за всеми кластерами которые входят в стр сет следить за всеми подами которые там находятся и как-то делать мультик ещё прямая видимость между подами нужна но там можно гевея это разрулить И тут вот какая штука ИО М это вообще Ну на самом деле не про объём Потому что если у вас есть сервис smh и в нём там 10.000 подов этот сервис СШ будет работать сильно так себе А если мы добавим сюда ещё десяток кластеров лучше от этого выла не станет И поэтому есть альтернативная реализация это когда у нас есть Независимый сервис соответственно есть два кластера есть два сеш Вот они теперь эти сеш ни за чем не следят но как-то синхронизируются проблем с масштабированием тут нет Есть один только вопрос в слове как-то синхронизируется потому что это нужно както Итого получается что и у нас есть два подхода один Ну не очень масштабируется подходит для малых инсталляций не строгой безопасности а второй классный но он сложный и требует реализации я на самом деле уже рассказывал как это делать с точки зрения конфига ИО кому будет интересно можете посмотреть доклад кому не интересно сразу скажу Это примерно девять яму файлов на каждый сервис в каждом кластере каждый раз соответственно это очень ну такая сложная история и поэтому а становится понятно что нужно что-то делать Давайте что-нибудь с этим сделаем Итак что мы хотим мы хотим полно функционально управление потому что конструкция явно будет сложная и соответственно Надо как-то с этим работать и спроектировать так чтобы ничего не отъехала раньше времени и дальше Мы хотим совместимость с любыми версиями кубернетес ISO Ну просто потому чтобы adoption был максимально широкий это выглядит логично с точки зрения API спецификация от кубе Community прекрасная собственно я бы на неё ориентировался и стал на неё ориентироваться но AP группу выбрал всё-таки свою потому что в процессе развития этого решения выявилось очень много и в настраивать нужно самому но суть остатся прежней вы делаете серр и ваш сервис теперь доступен на межк уровне как обычный внутрисекреторная сделать так чтобы приложение а стало работать в домене CL Local то есть оно выходит в новый домен и при этом ничего не поменялось то есть трафик как был локализован в этом кластере так и остаётся но появляется некоторый API который позволяет сделать вот такую штуку при необходимости мы можем уйти в любой удалённый кластер где опубликован сервис B и начать с ним работать соответственно Мы это можем сделать в случае если локальная копия не работает либо мы делаем межклассовый шаг для того чтобы это заработало - это DNS DNS в кубе вообще вещь такая достаточно специфичная в случае если сервис smh то дважды специфичная давайте я поясню О чём идёт речь А вот у нас есть приложение а которое ходит приложению б по cler Local всё хорошо это приложение а подключено в serv smh у него есть Сайка сетевым прокси этот S Car получает маршрутную конфигурацию от контрольной панели и опять-таки всё стандартно контрольная панель эту конфигурацию забирает из Куба на основании сервисов и подов которые там созданы Логично но ещё информация о сервисах Я имею в виду кубовых сервисах используется для того чтобы настроить кубовый DNS чтобы работала стандартная балансировка которая есть в кубе из коробки это абсолютно Типовая схема как работает и в ЧМ идея смотрите приложение а сначала самостоятельно делает запрос кубовом DNS и спрашивает А скажи-ка мне пожалуйста кто такой cler Local и это проходит вообще вне ИО там прямо отдельный маршрут для этого трафика есть ИО про этот запрос ничего не знает это сделано для того чтобы подключался сервис smh не инвазивно соответственно kuber отвечает кластер IP дальше приложение делает вызов уже по Клар IP на кубовый сервис трафик попадает в Сайка и уходит уже на IP адрес пода цели назначения В общем всё сработало и сделал свою балансировку И теперь мы хотим чтобы это сработало в домене рсе и понимаем что никто рсе вать больше не будет и нам нужно решать эту проблему и Поэтому придётся работать с дэнсом соответственно мы просто добавляем сюда ДНС какой-то программ программно управляемый для того чтобы он резол эти кластер сеты причём абсолютно неважно какой IP адрес он отдаст потому что мы дальше на Иво уже сделаем всю необходимую маршрутизацию отправим трафик в удалённый кластер и этот IP адре он имеет чисто косметическую функцию чтобы приложение не сломалось до того как как се smh попало с точки зрения деплоймент в кубе обычно делают так что у вас на ворке есть кэширует обычный кор NS там несколько подов он уже не нагруженный за счёт кэша вот мы просто правим его файл говорим что теперь стримом для clet Lo является дополнительный DNS там живёт точно такой же кор N S и вся эта иерархия прекрасно работает при этом можно даже особо не думать что такое Federation DNS потому что он будет не нагруженный Итого мы резол терсе если мы не хотим делать DNS мы можем прямо на деплоймент прописывать эти хост лисы Но это понятно Дорого потому что придётся править каждый деплоймент чтобы работал мультик и в актуальных версиях est можно обойтись без этого Но об этом я расскажу в дискуссионное вторая часть решения - это раун мы решили первый момент мы вышли в Service smh и теперь в этом serv smh нам нужно как-то отмар Зро в удалённый кластер вернёмся к нашей схеме опять-таки ВС просто есть есть и есть только теперь помимо сервисов появляются ещё новые сидиш сервис экспорт и сервис импорт чтобы публиковать в мультик и чтобы потреблять из мультик и если появляются новые шки то логично предположить что появится какой-то оператор который будет работать с этими сишками Это стандартны паттерн для кубано Давайте его и добавим буде много думать у нас появляется новый оператор который просто работает с экспорта и импорта что-то Рит в кластере и куда-то это отправляет Ну попутно Было бы неплохо чтобы он ещё маршрутизацию для сервис мыша сделал чтобы трафик всё-таки уходил в удалённые кластера и получается что это простой оператор Он на вход берёт один ял и на выход генерирует три Яла правильной структуры для того чтобы сработала исти простая бизнес логика Она легко тестируется и проверяется с точки зрения деплой Это стандартный оператор под кубернетес он работает как и многие из них по модели сингл тона соответственно через механизм ler election который встроен сразу в cuber То есть это максимально стандартизированное решение и важный момент здесь про этот оператор в том что для Discovery он не следит за подами единицей Discovery он считает сервис в кубернетес и это важный момент потому что все остальные решения которые я про которые я вам рассказывал за подами следят и соответственно страдают от этого А почему мы не следим Давайте расскажу соответственно первая часть - это вот тут это Сай Car приложение клиент Откуда будет выходить первоначальный запрос и здесь нам не нужно знать про удалённый кластер нам нужно знать про копию приложения локальную и про игры с Gateway через которое мы пойдём в удалённый кластер это локальные поды и за ними прекрасно присмотрит локальная копия ISO через механизм workload entry Либо мы можем создать headless серс и подписную работает соответственно Здесь нам поды из удалённого кластера не нужны идём дальше попадаем на Gateway работает только с исами в удалённых кластерах соответственно ему нужно знать Просто про адрес балансировщика который стоит перед кластером ему информация о подах тоже ни к чему ну и соответственно и Gateway в принимающем кластере тоже эта информация не нужна таким образом мы получаем что нам достаточно следить просто за кубомирное работает и вот когда Мы открываем для себя этот момент мы понимаем что можно сократить вычислительную нагрузку на мульти кластер в 15 раз просто потому что по шкодам кубернетес максимальное количество подов в одном кластере превышает максимальное количество сервисов в 15 раз соответственно это официальные данные которые взяты из Community kubernetes А собственно идём дальше и появляется третий компонент какой-то Discovery очевидно что это компонент с котором общается контроллер с первой части соответственно так и есть у нас есть контроллер который зал в Лапо кубовые сервисы и отправляет это на компонент который уже является межклассовая и это состояние можно хранить сразу в кубернетес соответственно мы просто делаем набор технических сирк и наш начинает писать в Куба ВС своё состояние кластера информации там очень мало по факту это имена кубовых сервисов поэтому мы API не нагружая при этом он достаточно надёжен и мы как правило не ломаемся А если пи не доступен то соответственно и нам можно не работать Всё мы пошли отдыхать до свидания чините кластер эта конструкция прекрасно работает пока не появляется второй кластер где есть вторая копия Discovery и теперь их как-то нужно синхронизировать И на самом деле опять очевидное решение А давайте мы просто все контроллеры подключим к одному Discovery и скажем что это мастер кластер который теперь отвечает за всех собственно это самое простое решение и мы так и сделали в нашей первой версии потому что в целом Ну Discovery компонент он реплицируемый проблема если погибнет дата-центр с нашим мастер кластером то соответственно вся конструкция продолжит в каком-то виде работать но нам это не нравится и тогда мы немного подумали Ну и просто добавили и чей сторч туда у него отдельная архитектура и возможно я отдельно о нём расскажу таким образом и работает наш решение для диринга Что хорошо так как оно всё равно даже в случае с внешней базой данных хранит копию всей информации сразу в Куба то администратор через Cub ctl может посмотреть состояние всей системы посмотреть кто кого видит что пошло не так Или как это всё работает в общем ему это прозрачно Но вот здесь когда Мы перешли на общую базу данных появилась такая провокационная мысль Мы же делаем решение для высокой доступности А почему бы нам не сделать так чтобы наш Discovery фро сам себя и сам себя сделал высоко доступным Ну Казалось бы это провокационная идея Мы немножко больше подумали об этом и поняли что это возможно смотрите как вот у нас есть три кластера в каждом из них есть Discovery и есть контроллер который хочет получиться подключиться к своему Discovery в кластере 3 контроллер сначала подключается к некому Boot модулю это обычный Discovery выть любой из Хв чтобы ково после того как контроллер получил от бутстрап всю необходимую информацию он строит всю маршрутизацию для самого себя в домене РС и это начинает работать с локальным Discovery при этом вся эта маршрутизация автоматически работает и для удалённого Discovery если локальный вышел из строя таким образом решение защищает само себя и дополнительные схемы обеспечения надёжности нам больше здесь не нужны собственно вот и всё эти три шага позволяют комплексно решить проблему ивест и это работает и уже достаточно долго а для тех кто хочет копнуть вглубь есть вот такая схема и это детальная схема в которой описано Как где меняются IP адреса что где случается какая маршрутизация я её добавил Для того чтобы Вы могли скачать презентацию она в хорошем разрешении сделать Zoom и посмотреть Если вам будет интересно Ну ещё эта схема имеет историческую ценность Это самый первый технический дизайн решения про которое я сейчас рассказываю поэтому оно такое достаточно дорого для меня время идёт к концу пора закругляться соответственно в качестве итогов Я бы хотел вместе с вами ответить на несколько простых вопросов почему такие вопросы Ну просто потому что я уже говорил решение сложное и при его внедрении явно можно что-то сломать вот очевидно что что-то может пойти не так и мне кажется здесь очень важный момент об этом подумать заранее и Давайте подумаем А что у нас меняется в плане производительности для ответа Я возьму старую схему которую мы с вами видели и После перехода в терс ничего не меняется Мы работаем по-прежнему локально не растёт не йн не повышается пропускная способность всё работало как было мы бай дизайн не можем ничего сломать потому что на самом деле конфигурация на эво она выглядит практически точно так же как и нативная для ИО соответственно мы просто получаем механизм увода трафик в другие кластера но Бади ничего не ломаем и никак не изменяем эту конструкцию Что хорошо потому что мы надж с точки зрения отказа устойчивости смотрите Какая история если ломается Discovery нам не страшно он Федерико кластер если умрут они все то мы потеряем возможность деплоя новых сервисов в федерацию но так как все маршруты статические Вы помните мы не следим за подами мы следим за сервисами они тоже меняются только в момент деплоя чаще всего это не страшно пользовательский трафик не пропадёт даже если умт полностью весь во всех кластерах собственно тоже самое касается раун вость публикации сервисов но все маршруты продолжат работать и вс будет отказоустойчивый ДНС запрос но это решается во-первых кшм потому что DNS штука достаточно инертная И как я говорил в современных версиях и это уже не нужно потому что там DNS встроен прямо в Сайка и автоматически настраивается соответственно в таком случае погибает вместе с приложением и нам больше нужно переживать за то что когда-нибудь погибнет ДНС соответственно отказоустойчивость не меняется мы никак не ухудшая эту систему не добавляем новых точек отказа Это тоже хорошо Ну и касательно безопасности Я думаю вы уже догадались какой у меня посыл тоже ничего не меняется схема безопасности я вам детально рассказывать не буду но в дискуссионное управление ивест трафиком прямо как сес MH у нас У нас есть межк Virtual Service и мы можем прямо там по предикату сказать Вот этот трафик в один кластер вот этот другой такая-то стратегия балансировки такой-то файвер и так далее Всё это настраивается очень просто и и главное прозрачно это всего один ямо файл которым мы настраиваем эту с ним кластер э кубернетес ванильный кубернетес Ну или какой-то другой кубернетес и хотим сервисы перенести из одного кластера во второй Потому что первый больше не поддерживается соответственно объединив оба этих кластера в федерацию мы можем посер висновку они будут думать что работают с локальный копии на самом деле трафик у идёт в удалённый кластер уже целевой Понятно Что latency растёт и для тех приложений где latency важно эта схема Ну не самая подходящая Но если у вас такие приложения там вообще Стоит подумать А нужен ли вам сес smh Какие преимущества он вам даёт мы всё-таки на High мы здесь любим масштабирование поэтому Давайте пару слов я об этом скажу нам абсолютно неважно Какое количество кластеров подключается в кластер сеет в отличие от других решений просто потому что каждый кластер у нас обслуживает сам себя там есть локальный набор контроллеров и они самодостаточные они рабо следят за этим кластером и нам не важно сколько их будет эта конструкция масштабируется by Диза во-вторых нам не важен ещё и размер этих кластеров потому что мы не следим за подами и слава Богу эту информацию не транслируем от кластера к кластеру мы следим за локальными сервисами внутри локального кластера их может быть весьма ограниченное количество Спасибо трешхолд кубернетес И мы этот трешхолд прекрасно перевариваем но нам интересно количество опубликованных сервисов мультик которые должны быть доуп меж кострами Вот это важный параметр но хранится настолько мало информации что по большому счёту мы можем держать Держать практически любую нагрузку с точки зрения планов на развитие этой конструкции можно отметить то что нам нужно сделать Соф мутен и я вот о чём я вам всегда говорил что мы на меж кластере работаем в домене терс то есть Команда А выберет какое-то имя для своего сервиса например бананы и оно будет в кластер сеете придёт команда выберет название ны И оно тоже будет в кластер сеете А вот если они обе команды захотят выбрать название бананы то у них произойдёт конфликт и кто успел тот и молодец Ну в общем-то честный подход но было бы неплохо дать им возможность завести межклассовая они друг друга видеть не будут вот чтобы видели то это Soft mcy HB Это для того что работал фик L4 Потому что сейчас это только L7 вот для того чтобы сделать L4 нужно всё-таки делать О мы о не любим Но мы любим HB Это новая фишка которая приходит к нам в serv MH соответственно это может быть полезно Ну и последняя часть - это меж кластерный и я говорил об этом в начале всё-таки это полезная штука чтобы Ир трафиком уметь управлять на мультик но здесь такая скользкая дорожка потому это сделать нужно интегрироваться с внешней инфраструктурой конкретного КД провайдера соответственно иметь набор необходимых инфраструктурных драйверов Это долгая история и так далее но делать всё-таки нужно на этом У меня всё большое спасибо вам за внимание и я отвечу на вопросы Спасибо большое Макс и прежде чем мы перейдём к вопросам коллеги дом Леон какво давайте давайте сделайте приятные спикеру он для вас доклад готовил вы посидели послушали и вот на этот QR код Пожалуйста оставьте свою оценочные нам как программному комитету позволяет собирать самые лучшие программы А так теперь у нас пошли вопросы где мои прекрасные помощники Вот давайте вот здесь первая рука была потом вот там да привет Спасибо за интересный доклад действительно увидел что соединяются между собой кластера что при этом ничего не ломается единственно из доклада так и не понял соединять их Для чего соединяют их для того чтобы у вас Ну у вас есть два кластера у вас в кластере один приложение в кластере два приложения вот если вот это приложение выходит из строя вы запрос потеряете и Максимум что вы можете делать - это снова отре и вернуть ошибку пользователю в нашем случае Вы этот запрос не потеряете он автоматически может уйти в удалённый кластер более того Теперь вы може самостоятельно сказать Что окей вот у меня Моё приложение разп на 10 кластеров я хочу обновить три из них Да я просто политико говорю что три сейчас будут обновляться увожу трафик на рабочие кластера обновляю возвращаю трафик назад или у меня есть Greenfield кластер Ну какой-то тестовый куда я выкатываю новое приложение Я говорю что пожалуйста со всей вот инсталляции кластеров там их может быть там 10 штук Да вот но трафик лояльных клиентов пожалуйста на один мы там сейчас будем проверять И вам не нужно-то и при этом один балансировщик не пострадал вам не нужно настраивать ничего кроме того чтобы создать один Ямал так получается всё-таки прои тогда во-первых ты сказал не изменяется отказ устойчивость получается она увеличивается правильно Ну да Нет я имею в виду когда я говорю что она не меняется в том что вот как только вы взяли это решение поставили у вас гарантированно by дизай ничего не сломается Вы можете поставить неработающий мультик он работать не будет да и вы не потеряете свой трафик но потом когда вы начинаете использовать это решение да у вас Понятно повышается отказоустойчивость вы начинаете управлять трафиком у вас появляется межк обби всё это ссытся куда-нибудь в Викторию и так далее и я всё равно из этого ответа не понял зачем связь между кластерами стоит один балансировщик который мониторит что здесь работает здесь не работает выкидываем задеплоить отдельно на конкретный кластер что-то никто не мешает Для чего с трафика потому что мы можем от Ну холст чекать всё это дело на балансировщик который стоит перед кластерами или перед кластерной инсталляции Да вот и соответственно заходим сюда сломались вернулись сюда Но если трафик инициируется вот отсюда прямо из кластера Да по каким-то причинам это либо пятый микросервис в Каскаде микросервисов либо вообще у вас просто какой-то Джоб сработал Да вот здесь появился какой-то запрос он идёт сюда на своего соседа и этот сосед по каким-то причинам недоступен Вы этот запрос гарантированно теряете Т обживать это за тогда вам прид все сервисы которые должны быть доступны меж кластерами опубликовать на этом балансировщика и это как бы не очень секюр Спасибо большое за очень такую прям жёсткую конструктивную дискуссию с требовательными ответами любим такое ж чем сдуй вопрос за наконец-то появился вопрос изн некто с шестнадцатеричным именем спрашивает Сколько времени требуется на переключение трафика при отказе первого кластера Сколько времени требуется на переключение трафика при отказе первого кластера А как настроите вы можете отстреливаться вообще сразу можете отстреливаться через какое-то время потому что это реализовано через механизм shet Breaker который есть в да и это настраивается на самом деле точно так же как и вы прямо можете указать параметры будете ли вы раться Если да то с каким шагом поэтому можно очень быстро делать Спасибо Макс дополнительный нюанс к этому вопросу на аватарке 404 написано парень знает о ЧМ говорит вот где-то здесь у нас был вопрос Вот давайте да рука поднята Спасибо за доклад у меня немного провокационный вопрос Если конечно это можно в рамках идей А насколько это внедрено в Сбербанке это это работает это работает давно и это работает на Ну всех этапах производства то есть на тестовых средах на продакшене да это как бы работает используется это боевой функционал спасибо я здесь делаю маленькую ремарок которые не работают в продакшене это наша обязательная принципиальная позиция может быть где-то вот там вот вот вот рука хотел бы верну к первому вопросу для по что те задачи которые вы героически решили кажется что они уже решены внутри одного кластера один большой кластер он имеет достаточно большие метрики по масштабированию там решены проблемы с репликами с плом все проблемы С трафиком которые вы сейчас обсудили они уже решены и исходя из того что вы рассказали Я так понял что часть решений вы повторили которые есть в кластере но при этом назвали это не кластер а мультик а часть решений подошли и реализовали как-то по-своему хороший вопрос Кстати это пря кандидат на лучший вопрос да смотрите Сейчас отвечу Ну во-первых есть такие приложения которые вс-таки в один кластер не влезают их очень мало но они есть Вот Но это вырожденный кейс Другой вопрос Почему создаются много кластеров это первое локальность мы хотим делать поближе к пользователям вовторых Решаем проблему шумного соседа вот знаете в в начале кубернетес пути была очень популярная проблема с двой когда она не видела ресурсы которые выделены на под она думала что без дополнительной стройки помоему восьмая jav была она думала что ей доступно всё что есть на кере кубернетес и Если ваше приложение текло по памяти то оно сначала убивало свой вокер ске дались на другом воке ворке убивала его и весь кластер у вас складывался вот и команда которая там например пишет Наго очень сильно недоумевал при этом А почему кластер сложился вот из-за какой-то там джавы Поэтому вот эта вот изоляция приложений в кубернетес она вот она достаточно условна Да она больше логическая то есть М спейсы - это инструменты логического нонни кластеров а но не физического и эту концепцию можно доктир одного кластера и пожёстче сделать но в концептуально не про это и соответственно мы сразу ну мы подходим к тому что нам всё равно нужно тонти по кластерам чтобы вот этой проблемы шумного соседа не было и тогда как бы мы приходим в мультикласс в границ масштабирования Куба ещё не упёрлись Да и было бы удобно просто что Несмотря на то что приложения живут в разных кластерах Они могли прозрачно общаться друг с другом и эту проблему и решали Ну кейс шумного соседа Я так понимаю решается закрытием одного конкретного Бага в кубернетес который Судя по всему был закрыт Достаточно давно и по фиксим есть лимиты это Это был баг джавы Но на самом деле таких замечательных историй э я могу привести там Ну достаточно много поэтому Ну это прямо повод подискутировать И по обсуждать реквесты шедулер умеет управлять всем этим и если в конкретном сценарии есть какой-то баг Но это баг Ну конечно вы тогда подходите к тому что вы понимаете что кубернетес не даёт вам жёстко изоляции между приложениями И вам этот кубернетес Нужно настраивать и вы начинаете тюнить стандартный скеле потом в какой-то момент вы понимаете что его способности недостаточно вы там может Выделите какой-то пол машин под ва ваше приложение и туда скели никто не будет да потом эти машины по какой-то причине например заполняться ну вы отмашка вдруг Да и выясняется что в кластере были свободные ресурсы А вы на них их занять не могли потому что себе маленькую зону изолированную выделили которую заполнили и я к я к чему что закрыться та Можно но вот кейсов того как вы можете сломаться их практически бесконечно количество и от каждого кейса Ну вы гарантированно не закроете вы будете просто Ну вот здесь сломались о перенастроить эту проблему вот тут какой смысл и вам не нужно вот так вот итерационного то кейс Друзья давайте мы зададим вам возможность задать другие вопросы да дискуссию Я никуда не ухожу Да мы продолжим обязательно спасибо большое за вопрос два вопроса онлайн Они не смогут потом задать вопрос там А в кулуарах насколько возрастает сетевая задержка при переключении Ну само переключение происходит мгновенно потому что оно происходит прямо сразу на клиенте То есть это механизм аналогичен тому как вы сходили на одно На один под в кубернетес прямо вот в своём ном спейсе Вот и если он сбоит то вы идёте на другой механизм стио срабатывает это также и Механизм там тот же самый Анси самого запросто Ну понятно возрастает Да в зависимости от того насколько быстрая сеть у вас внутри кластеров и между кластерами Можно ли выбрать приоритет кластера к примеру Я хочу выбрать чтобы всегда работал второй кластер да И это прямо очень гибко настраивается и можно управлять локализацией трафика как угодно то есть можно локализовать её в своём кластере чтобы поть н что можно уводить это в другой кластер можно настроить любую стратегию балансировки это прямо из коробки by дизай делается друзья Спасибо большое у нас плавно подходит время к концу я предложу дальше продолжить ку в зоне вопросов и ответов Макс Выбери пожалуйста самый лучший вопрос мой Фаворит сохранился Это вопрос про рование ро который дальше тебя продол Спасибо большое друзья Аплодисменты Максим мы тебя от лица компании онтика от лица программного комитета Благодарим за то что ты к нам прил за то что рассказал очень интересную ю и на высокой нагрузке от нас небольшие памятные подарки приходи пожалуйста к нам ещё рассказывай нам интересные вещи Спасибо большое спасибо"
}