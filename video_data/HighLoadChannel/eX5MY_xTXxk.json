{
  "video_id": "eX5MY_xTXxk",
  "channel": "HighLoadChannel",
  "title": "Надежность поставки данных при потоковой обработке / Евгений Равнушкин (Яндекс Маркет)",
  "views": 262,
  "duration": 2470,
  "published": "2024-04-17T01:10:17-07:00",
  "text": "Давайте вам представлю нашего первого спикера сегодня это Евгений равнушкин Он расскажет нам про надежность поставки данных в микросервисной архитектуре Евгений прошу поддержим Евгения Всем привет И сейчас будем говорить про потоковую обработку и общение микросервисов через очереди я когда-то давно пришел в Яндекс Маркет когда он еще помещался на одном этаже офиса и начал заниматься продуктовыми проектами но как-то каждый раз получались большие инфраструктурные и вот последние четыре года Мы строили развивали сервис который называется офферное хранилище Маркета товарные предложения в Маркете называются офферами и сразу говоришь это не аналитическое хранилище а именно операционное хранилище Вот и большой сервис важный когда-то давно в Маркете Просто когда он был агрегатором товарных предложений партнеры передавали прайс-листы приходил робот скачивался эти прайс-листы парсел складывал временные таблицы и относил их под поиск Маркет выдачи Маркета для покупателей построена через поиск и после этого время офферы из этих временных таблиц сбывались тут немаловажно сказать что еще при этом парсек одновременно ходил С каким ручки и обогащала эти офферы дополнительными знаниями потом когда стали строить marketplace стало понятно что не все данные одинаковы Поэтому нужно цены доставлять быстрее и тоже товар раскрывается выдачи быстрее поэтому взяли просто пристроили сбоку сервис быстрых данных которые уже мог прямо из партера получать цены и через очереди относить их под поиск при этом этот сервис уже был с настоящей базой данных хранилище поэтому там можно было сделать для партнеров которые могли передавать но пока только цены но оставались проблемы Что на самом деле у нас теперь две цены одна приезжает прайс-листе а другая лежит в сервисе быстрых данных которая доезжает в другое время и Кроме того партнеры все равно не видят что происходит с их данными и поэтому мы взяли и все переделали мы сделали большое базу данных и рядом построили группу микросервисов которые назвали оферное хранилище и теперь уже партнеры могли полностью получать оттуда свои данные и даже при этом приходить и редактировать товары все другие маркетные источники которые например передавали характеристики товаров Мы подсоединили через очереди А также через те же самые очереди сообщений стали поставлять данные из офферного хранилища событийной модели теперь поиск получал их через очереди А еще маркетные потребители это внутренние потребители сервисы которые например проверяют офферы на стоп слова или например карточки и даже скачивают картинки в том числе Мы тоже подсоединили через очереди Ну и тут важно что на самом деле иногда потребитель является источником потому что скачали картинку и результат об этом принесли обратно в офферное хранилище В итоге получается что у нас сервис офферов стал как такой большой сервис событий партнеры передают офферы как-то этот товарное предложение меняется и в этот момент запускается целая цепочка обновлений по всему Маркета по B candom запускается генерация карточек которые Ну уже более подробную информацию со всеми характеристиками показывают потом дополнительно специальным образом подготавливается он к выдаче валидируется и например информация со складов тоже туда же добавляется и потом уже публикуется для покупателей доклад из двух частей сначала немножко погрузимся внутрь в техническую часть архитектуру и потом уже собственно поговорим про то какие С какими проблемами встретились в этом случае и как потом решали поскольку оферное хранилище это мастер система товарных предложений партнеров то как бы добавив к ним дополнительную информацию можно уже значит непосредственно раздавать ее над как внутри Маркета так и публиковать покупателям Вот и все это делается на стриминге так называемом или Real Time обработке Но тут еще получилось такая очень хорошая продуктовая функциональность как каталог партнера потому что мы через очереди опять же от других пациентов можем сразу обновлять статус это вот товара и показывать партнеру что еще надо с оффером сделать чтобы он стал Там классно размещаться на Маркете Кроме того еда и лавка например когда размещаются на Маркете они тоже приносят свои данные товарных предложениях именно в офферное хранилище потому что оно знает каким образом их раздать внутримаркета по нагрузке Ну мы обновляем несколько сот тысяч этих офферов в секунду и это получается сотни мегабайт данных именно на запись в нашу базу ну и храним сейчас несколько миллиардов этих товарных предложений при этом нам надо на самом деле часть данных те же самые стоки доставлять до выдачи меньше чем за 20 секунд внутри Это несколько микросервисов которые все соединены очереди и построены вокруг масштабируемого Киеве или сторожа при этом Ну как я уже сказал источник данных и потребитель данных иногда Этот один и тот же сервис и наши на самом деле как команда мы поддерживаем офферное хранилище вот рядом другие команды со своими сервисами но внутри нашей команды Мы свои микросервисы тоже Соединяем через сервис обновлений который непосредственно контролирует обновление офферов чтобы раздавать события мы значит свои сервисы Соединяем тоже с ним через очереди и таким образом получается некоторые шина данных для того чтобы именно информацию а каталог партнера Ну кабинет в личном кабинете партнера он строится отдельно на синхронных интерфейсах и правда поиск для него мы опять же собираем на такой технологии Яндекса как все еще за сервис тоже через очереди Вот Но при этом из него выдача уже строится синхронно Но это отдельная часть на которой Ну сейчас мы не будем сосредоточиться внутри хранилище у нас динамический сортированные таблицы внутри Яндекса или после вот ссылки в конце будут уже презентации её можно было скачать поэтому не обязательно записывать Вот Но что это даёт нам нам это даёт транзакции на киеверию сторож и даже несколько таблиц и Мы практически не беспокоимся надёжности масштабированности инфраструктурного уровня потому что там и автор шарнирование есть и Ну в этом в этом случае за нас решает Кроме того поскольку у нас сохранились ещё старые поставщики которые получают данные через таблицы и выгрузки то нам здесь еще важно что часть реплик у нас в таких кластерах вычислительные расширительным ресурсам и там можно делать me previews даже по тем же самым нашим киевелю сторожем плюс поскольку Там не только сортированные таблицы в качестве Киеве или есть еще и упорядочные таблицы найти можно собирать очереди Ну и таким образом вы можете даже получить транзакцию и как на хранилище так и на очередь которую вы собрали по сути в том же в той же инфраструктуре Ну и в нашем случае было поскольку много продуктовых проектов и постоянно происходят какое-то работа с данными нам важно было еще их уметь доставать прямо непосредственно руками часто и Яндекс говорил который есть нам в этом случае помогал он по сути похож на SQL но при этом с одной стороны такие были с другой стороны мы храним там частью нашей документов вот мы берем протопов с описанием товара это вверх пример верхнего уровня описания когда отдельно лежит значит описанный многоуровневый протобафом описание товара тайтл цены отдельно стоки которые со складов пришли и статус для партнера и про добавив нам на самом деле даёт с одной стороны то что мы э можем Разные группы товаров описать и расширять его э при этом Даже те команды которым нужно делать продуктовые проекты они могут добавлять туда своё свои необходимые части и с другой стороны поскольку Он у него есть экстренный как бы аннотация по рефлексии мы можем туда прямо бизнес-логику завязывать на какие-то части полей ну здесь можно заметить что здесь уже выделяются какие-то верхние уровни вы такие группы по сути полей которые Ну по-разному немного обновляются с разной скоростью и имеют разные гарантии что касается надежности тут стоит поговорить о трех больших пунктах Это непосредственно восстановление сервиса потом на самом деле как ни странно восстановление подписчиков подписчики Но это непосредственно потребители которые задают какие-то правила для того чтобы получать какие-то обновления офферов подгруппы полей Вот и в конце еще поговорим в третьей части правонагрузку потому что там тоже есть свои особенности связанные именно С потоковой обработкой в чем вообще вот Real Time Processing именно здесь стоит Fall система какие его особенности но мы берем сообщение из очереди на входе и сравниваем с тем что в базе и складываем изменения в базу после этого те изменения которые мы обнаружили по сравнению с базой то мы пишем в очередь поставки у нас версия армирование на прикладном уровне то есть мы не сильно завязываемся на порядочность сообщения на входе но при этом сравнивая версия которые лежат в базе и Например если мы применили Изменения к базе в транзакции но при этом после этого по каким-то причинам потеряли запись для поставщика то получается что мы даже воспроизвести это не можем потому что база уже в актуальном состоянии и в этом случае получается что как бы даже здесь журнал не помогает И что касается инцидентов именно то здесь такая особенность что по сути в потоковой обработке ваши данные понемножку изменяются понемножку портятся например становится не валидными или вы их понемножку теряете вы и обнаруживаете и это сильно позже чем на самом деле инцидент начался но при этом Вы можете позволить себе остановить сервис что-то сделать поскольку у вас там ну есть некоторые в очереди И после этого вернулся к обработке на самом деле тут стоит говорить что мы в основном решаем проблемы в прикладном коде потому что ну инфраструктура в настоящее время достаточно надежно Ну и там это решает отдельные команды но при этом поскольку Time to Market очень важен для бизнеса Мы катаемся там несколько релизов в день и поэтому как это бывает не бывает идеальных программ бывает не до тестированные поэтому всё равно так или иначе какие-то ошибки в ходе приезжают и поэтому здесь они так или иначе будут и э ладно если вы например начали терять сообщения вот ну как бы вы просто не актуализируете их но если вы например стали ещё их и превращать какие-то инвалидные и э в случае маркетплейс - это как бы ну фатальная влияет на репутацию потому что у пользователей внезапно потихонечку товары в корзине превращаются не в том что они на самом деле туда клали Вот и поэтому Ну с этими ошибками нужно как-то работать Кроме того в случае очереди на самом деле у вас есть некоторый период хранения в этой очереди да то есть она не резиновая и поэтому хранить данные Ну например сутки поэтому вам нужно тоже в этом случае об этом помнить В итоге получается что часть данных с одной стороны потеряется с другой стороны как бы что касается цен например или скрыть и терять очень дорого потому что ну прямо вас в рантами деньги значит из-за этого теряются потому что у вас отменяются заказы как по классике можно было бы но мы делаем бэкапы Ну и мы действительно их делаем вот и какие-то логи из всех систем которые у нас есть мы тоже накапливаем но на самом деле При потоковой обработке что получается вы допустим час спустя обнаружили что у вас на самом деле час назад произошел произошла как некоторые проблемы Вот ну вы берете останавливаете сервис в зависимости от того как вы быстро умеете из бэкапов останавливаться Ну в случае тебя это там просто копирование таблиц это почти мгновенная операция только там надо пересобрать метаданные и потом начинаете накатывать журналы и пусть Вас например есть какое-то онлайн резервирование по железу даже допустим вы держите значит в два раза больше железа чем на самом деле вам надо хотя в случае обработке это дорого вот ну вы начинаете восстанавливаться из журналов бы вы просто восстанавливались эти полчаса но допустим вы не можете себе этого позволить и сразу одновременно с этим поскольку у вас все данные версионированные в журналах и в очереди обновлений Вы можете Просто сразу их одновременно начать накатывать но тем не менее у вас значит эти Полчаса вы потратили за это время у вас данные там были в каком-то не очень актуальном состоянии Но на самом деле проблема шире потому что в это время вы наливаете заново всем своим десяткам потребителей те события которые вот вы из журнала накатываете на бэкап и получается что запас по мощности нужен не только у вас как вы там типа большого мощный сервис а всем вашим потребителям которые значит получают от вас события потому что пока вы восстанавливались вы им в очередь вот те данные и поэтому мы на самом деле пошли таким путем В итоге что пришли к тому чтобы частично частично отключать поставки То есть например если какие-то источники нам передает невалидные данные то мы просто их отключаем Ну и дальше начинается какой-то разбор и из бэкапа мы на самом деле тоже восстанавливаемся не полностью определив то подмножество данных которые нужно восстанавливать мы просто есть специальный инструмент который по фильтрам может быстро его из бэкапа по сути как источник подсоединив накатить на именно стоит Ну и Кроме того те данные которые мы сами вычисляем у нас там есть микросервисы которые готовят некоторые части оффера мы просто можем переобайти базу отдельным сервисом тоже и таким образом восстановить их Ну и отсюда вытекает еще один способ что она на самом деле надо сразу делить данные на класса для того чтобы потом во время инцидента применять к ним разные стратегии но основные данные такие класса получаются такие что есть мастер данные которые вообще Нельзя терять есть только у вас приоритетные данные они интересны тем что вот Ну вот например случай цены истоков что с одной стороны они важны бизнеса с другой стороны они как правило чаще обновляются то есть на них и поток идет больше Поэтому если вы их каким-то образом изолируете в обработке то вы в этом случае получаете сразу и дополнительные масштабирования но есть воспроизводимые данные которые Можете просто перечислить и тогда потоковой обработке при восстановлении вы Вы можете в этом месте какой-то дополнительное время потратить Итого по первой части получается что мы восстанавливаемся по частям и сервис в этом случае не останавливаем чтобы просто не терять актуальность данных на выдаче Маркета и Кроме того надо стараться изолировать хранение поставку по классам данных для того чтобы потом в случае восстановления действовать по-разному но при этом у вас есть всегда потребители которые в данном случае ну просто наши соседние команды которые тоже завязаны на обновление офферов и у них своя какая-то инфраструктура свои процессы поэтому периодически Ну или например в самом начале когда они подключаются они приходят с некоторыми запросами на то чтобы получить всё состояние базы или какое-то подмножество полей поэтому эти данные по сути в данном случае это не чистые события Это непосредственно последние данные которые у нас лежат и они их интересуют именно вот то потому что полей на которое они завязаны в своих бизнесом логике и такой запрос они могут приходить в нашем случае сначала приходили лабораторным Вот Но при этом здесь он постоянно случается и это на самом деле требует дополнительных каких-то операций то есть Нам нужно посканировать базу для того чтобы выгрузить какое-то часть и это может оказаться например внезапно дополнительной нагрузкой на вашу базу и надо при этом еще обеспечивать надежность какие-то ретраи может быть параллельную обработку чтобы это быстрее происходило поэтому это просто отдельная отдельный сервис который требует внимания Но можно было бы вообще этого не делать если альтернативный способ Если вы заранее договариваетесь потребителем что независимо от событийной поставки как бы офферы не менялись раз например в сутки в неделю вы просто поставляете все данные которые нужны и таким образом они просто подождут Вот но часто в случае маркетплейса ждать Просто нельзя Как можно формировать запрос на самом деле со стороны потребителя если Ну часто это стоит Full потребителя и он какое-то подножится для обработки все равно хранит поэтому он может сравнить просто с помощью какой-то выгрузки с нашей стороны те данные которые он получил с теми которые у нас лежат Либо мы можем сами это обеспечить тоже например через тот же самый превью с и ту разница которая получилась выразить например в виде идентификаторов Ну и выглядит так что самым эффективным способом с точки зрения именно экономики является и разработки является просто запросы идентификаторов сущностей когда на самом деле они просто выгружаются в очередь со стороны потребителя и поставщик их в том режиме в котором он может просто вычитывает из своей базы И в этом случае взаимо действовать не требуется и сервис вот этот вот поставки Он становится очень похож на сервис обновлений В итоге по второй части что в любом случае при потоковой обработке Придется делать переправку для потребителей потому что ну мир не идеален в этом случае нужно это как-то исправлять И можно если вы с самого начала заложитесь на то что такой процесс будет то потом просто вы можете сделать один раз его сделал по протоколу разработав потом просто Можете уже в этом месте Не дополнительно не тратятся про нагрузку Возможно самое интересная часть но здесь на самом деле для потоковой обработки ничего такого нового Просто ну в случае маркетплейса можно нормализовать данные там разные кардинальность между например описанием товара и наличием на складах которых там Dark Store может быть тысячи Вот и даже в случае ну это просто снижает нагрузку и точно также можно дезинфицировать данные на входе Потому что часто передаются данные одинаковые Ну и даже с той же самой версии поэтому сравнив их просто поток на запись можно уменьшить и писать на самом деле Иногда просто дельты шарнирование использовать в нашем случае это автор шарнирование можно прикладном уровне писать в отдельное колонки чтобы снижать опять же поток на запись но мы еще используем все реплики для нагрузки на чтение потому что ну она на несколько порядков больше просто и понятно что нужно в случае потоковой обработки не останавливать разбор очереди на плохих сообщениях Ну и дальше контролировать сколько вы будете что касается параллельности на самом деле вот если взять очереди сообщений Но там в Яндексе это Лог брокер можно рассматривать кафку внутри очереди она как-то протекционирована и но по сути единица и обработки у вас является вычислительной у вас является читатель это партийцы вот и Ну внутри очереди внутри топика у вас партийцы балансируются за счет инфраструктуры у вас все равномерно но в случае когда у вас большой сервис и много таких топиков у вас получается что у вас есть топики с разной мощностью Ну там сообщение разного размера и один читатель особо жадный Может просто взять и начать читать сразу много топиков внутри топик партийцы сбалансированная между топиками нет и отдельно получается вам нужно Здесь либо балансировать писать что-то чтобы балансировать между собой эти топики в зависимости от весов например либо просто начать перекладывать например встретилась режиме эти сообщения в какую-то одну большую очередь если у вас там потребитель разные Да при этом внутри сущности примерно одинаковая и поэтому вы их Можете просто переложить в другую очередь еще на самом деле в этом случае получить дополнительные бонус в случае как такой как приоритетная очередь потому что на самом деле гарантии поставки на данные очень разные Ну и в случае вот нашим например стоки нужно поставлять просто за секунды и изоляция в этом случае обработки очередей она помогает при этом не обязательно делить вычислительные ресурсы да то есть ну как только вы поделили вычислительные ресурсы вот отдельно там первую очередь вторая третья у вас получается что в каждом случае запас Да он складывается И вам нужно больше железа можно вычислительный ресурс оставить общее и но при этом вы сначала разбираете по приоритетам очереди Вот и Ну в зависимости от вашего фрайверка который вы можете написать и тогда у вас разбор одинаковый но при этом они изолированы по данным и если предметной области еще получается разделить данные так чтобы вы хранили их отдельно Вот то тогда и поставка вы сможете организовать надежную что по нагрузке в итоге мы снижали поток на запись Ну какими-то обычными способами но при этом отдельно надо продумывать параллельность обработки И как вы будете масштабировать и Если с самого начала разделить данные на классы и потом изолировать их обработку в вашем сервисе то вы просто сможете больше времени тратить на разработку и в инцидентах быстрее восстанавливаться как мы за вот эти вот года последовательно решали эти проблемы когда подняли поток в обработке выкатились в провод сначала перед отправку потребителям все равно Пришлось сделать и изоляцию по классам данных тоже потому что от этого зависит гарантия одновременно с этим когда стали расти активно и уже требовалось много железа снижали нагрузку оптимизировали при этом поскольку во время инцидентов восстанавливаться надо было все быстрее то использовались восстановление и в итоге вот сейчас продолжаем работать над параллельностью обработки но при этом потоковая обработка она классная с точки зрения того что вы по сути тратите железо только на именно факт обновления данных То есть вы можете их быстро актуализировать по всем вашим бэкендам сервисом и поэтому за счет этого эффективно масштабировать ее то есть Вы тратите железо только на то что у вас меняется и везде данная актуальное но при этом приоритетность данных надо предусматривать потому что они не все одинаковые как сущности так и внутри сущности данные тоже и надо самого начала проектировать как вы будете перегружать данные и устранять эти потери или не валидные данные и на самом деле как бы получается что с одной стороны потоковая обработка она дает именно возможность параллелить но при этом нужно думать о балансировке еще про потоковую обработку в Яндексе доклады можно посмотреть вот вчера был доклад именно про оптимизацию тонкую на базу и Егор читал в прошлом году доклад тоже про стриминг У меня на самом деле все что я хотел важным поделиться с вами Давайте обсуждать Большое спасибо классная архитектурный доклад вижу что уже много рук Вот давайте начнем с первого сюда сюда Здравствуйте у меня Спасибо два вопроса Первый состав сообщения сущности по офферу То есть вы кладете весь оффер только Дельту который поменялось Но по сути в очередях приходят дельты В некоторых случаях это целиком оффер от партнера по некоторым протоколам но внутри Маркета это приходит именно дельты какая-то под ножество из тех сотен полей которому написан Вот и версионировано на самом деле каждое поле вот и это позволяет Дельту применять раз когда останавливает поставщикам как они понимают что эти изменения нужны а у вас данные были некорректные все так поэтому это версионирование оно на самом деле общее во всех наших очередях как на входе так и на выходе вот мы на прикладном уровне версионируем эти поля и поставщик тоже опираясь на эту версию То есть получается некоторый такой универсальный язык внутри Маркета Вот который позволяет нашим потребителям тоже из этого же протобафа использовать версию я понял еще успеваем до 6 время да здравствуйте вы рассказали вас предпо такой обратки данных ошибка не всегда очевидна и может накапливаться Как происходит Табу шутника их самых ошибок или это отдано на клиента то есть там клиент побежал Саппорт что-то не так ну в некоторых момент просто срабатывают разладки на больших снапшотах на бочах потому что ну мы сбоку все-таки по всему стоит и проходимся и когда ошибка уже накапливается Так что она становится видна в каких-то представителях то тогда в этот момент запускается автоматика но к сожалению нередко это просто люди которые видят что что-то здесь не так тестирование на клиентах какая-то синхронизации Ну это не тестирование на клиентах на самом деле как бы автоматическое тестирование и релизов и просто есть вот просто так получается что когда вы постоянно ведете разработку активную именно продуктовых проектов то все равно какие-то ошибки вы видите в итоге уже где-то ближе к потребителю спасибо Вот еще на первом ряду есть Здравствуйте спасибо за доклад уточняющий вопрос по докладу Я вот слайда про войти так понял что платформа войти поддерживает реализацию очередей Вот на этом моменте Я так понял что вы на этой технологии своей очереди строите а потом дальше по тексту вы упомянули собственно брокера более классической технологией аналог Кафки То есть тут немножко запутался то есть начнём всё-таки очереди реализована да да здесь стоило пояснить очереди у нас на кавка подобная просто да они там кросс-доцы вот и всё остальное и ну как бы они как-то свои базы данных обеспечивают просто высокую пропускную способность на войти просто их тоже можно организовать потому что там есть упорядочные таблицы и мы сейчас в процессе перехода на самом деле в некоторых местах именно на это потому что даёт Когда у вас в одной инфраструктуре стоит очередь Вы можете по сути транзакцию организовать сразу на все эти таблицы очередь тоже в виде таблицы в некотором роде представлено И вам одновременно и транзакция дает гарантии на то что вы причитали и применили Изменения к базе либо не применили Изменения к базе и не прочитали тогда А если у вас еще и очередь поставки здесь выражена то у вас вообще транзакция сразу получается на все три точки обработки в потоковой системе вот Ну в этом смысле это классно просто это как бы немножко дороже разработки но например вот э-э реалтаем процессор у рекламы Да Яндекса Вот они пошли этим путём понял то есть войти всё-таки есть очереди и вы хотите на это перейти но пока что сидите Ну скажем так мы как сервис мы мы внутри можем это перекладывать Но на самом деле это как бы поскольку марки большой Там типа сервисов и получается что мы не можем значит прийти и всем значит поменять их разработка на то чтобы они использовали очереди войти Ну вот очень удобно супер спасибо Ну что ещё на последний вопросик да давайте также Спасибо Жень классная Влад Спасибо два вопросика первый ты говоришь что нужно делить данные на классы но данные на классы делятся внутри одного оффера правильно как обеспечить консистентность этих данных если Одни из них мы успели доставить А другие не успели доставить И второй вопрос Ты говоришь что войти использовать этизавренность используется для ЛТП обработки запросов и нам вчера заявляли что это в основном придет движок то есть для ЛТП не очень предназначен соответственно чем мы платим какое там типичное время обработки запросов сколько он там транзакции в секунду на единицу времени выдает вот если можешь поделиться такими цифрами Было бы очень полезно ну как вчера Олег на самом деле говорил сейчас базы то становится такими которые умеют все ить как раз такая Такой что с одной стороны он все аналитически может быть запросы на мой предъявился делать и там ну вот есть именно вот отдельные кластера но мы используем не аналитическую часть мы используем именно транзакционную часть на таблицах и эти транзакции на самом деле нам обеспечивают консистентность то есть ее можно открыть там на таблицу на часть и поэтому когда мы обновляем одну часть оффера да то ну у нас есть компресс здесь вот поэтому это нам обеспечивается вторая часть вопроса была про вторая когда мы одну половину доставили нет собственно что касается цифр Но на самом деле можно просто смотреть их на сайте в нашем случае там мы можем за миллисекунды ну скажем так за сотни Милли Ну да вот отдавать данные в каталоге партнера Да что касается синхронных интерфейсов Ну сами транзакции у нас зависит просто от того количества бизнес логики которые внутри нее попало но она может быть длительной Вот Но она для пор для пользователя незаметно потому что ну как бы в каталоге он видит выдачу Но обычно как вот любые фронты Ну то есть Колы за сотни миллисекунд транзакции сотни миллисекундная транзакция как бы тут в чём дело там внутри наш код прикладной который из этого оффера там что-то мержет э-э складывает преобразует вот отдает францужено и так далее вот поэтому это не миллисекунды идти вот по изаурусу его характеристики правильно просто пойти и посмотреть их замеры потому что ну они лишены без услуги которые вот специфичны для нас поэтому мы это себе транзакцию можем в случае потоковой обработки делать сколько угодно длинную Ну приемлема вот потому что мы под ней можем там прочитать записать сравнить поставить вот это вот в чём дело Спасибо про консистентность Мы в дискуссионной зоне обсудим Спасибо за ваши вопросы в зале нам осталось еще два мероприятия первое Евгений надо выбрать лучший вопрос который сейчас был из присутствующих но мне понравилось понравился вопрос Где я рассказывал про версионирование на прикладном уровне кажется вот у вас было до кого Да хорошо потому что это важно и еще раз давайте скажем Спасибо Евгению за классный доклад Спасибо подарка один подарок и один подарок от Группы компаний ВК Маруся будет еще один собеседником можно сказать потренироваться Всё спасибо"
}