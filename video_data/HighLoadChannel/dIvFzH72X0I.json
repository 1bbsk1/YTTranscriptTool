{
  "video_id": "dIvFzH72X0I",
  "channel": "HighLoadChannel",
  "title": "Концентрируемся на бизнес-модели данных: от ETL к ELT / Иван Зерин",
  "views": 3327,
  "duration": 3087,
  "published": "2022-03-21T14:12:31-07:00",
  "text": "о чем сегодня пойдет мой рассказ я попробую сегодня это не задумка организаторов наверно так получилось я попробую сегодня немножко закрыть боль которая вчера возникла у те кто пожалел что это не спонсорский work и расскажу про open-source фреймворк которым можно некоторые штуки закрыть ребята из яндекса сделали прям классную работу у них получилось несколько интересных штук которые на самом деле мне кажется были бы очень интересные у подзор си и в подборке про которой сегодня буду рассказывать немножко про меня меня зовут иван я работал в компании сендер мой опыт разработки ну где-то почти 12 лет большая часть этого опыта это backend разработка по большей части я java developer и последние 4 года я строй аналитику в компании сендер наверное все про меня теперь немножко про компанию что мы делаем мы песочный бизнес который доставляет parfum мы работаем с сша с недавних пор мы работаем еще и в канаде отправляемый parfum вот в таких вот кейсах которые вы видите на фоновой картинке там 8 миллилитров стоит это 16 долларов подписка оплатил штанцы долларов 1 месяц получил кейс с выбранным парфюмом у нас сейчас 400000 пользователей мы существуем с 14 года и наши нагрузки они примерно не очень велики на самом деле будем честными это где-то 100 тысяч визитов в сутки и 600 700 тысяч ивентов в сутки происходят наших системах по данным по данным мы где-то получаем 50 60 гигабайт сутки вот у нас техническая команда примерно 50 человек все работают удаленно это не дань к видны эпохи мы изначально были ремонт командой кстати харим если кому будет интересно подходите расскажу подробнее нам нужны backend разработчики fund and разработчики вот и нашу инфраструктуру полностью хостится на амазоне специфика бизнеса у нас есть специфика как у любого бизнеса она следующая подписанный бизнес он на самом деле после одной покупки он практически всегда убыточен я не знаю ни одно подписчикам бизнеса который с одной покупки дело бы прибыль как правило прорыв к и tefal клиента себе в убыток и потом надеетесь что он с вами проживет ну достаточно долгую жизнь покупая регулярно какие-то продукты и таким образом вы отобьете его привлечение и затраты свои на инфраструктуру но в реальном мире все это работает чуть сложнее потому что больше из клиентов даже со скидкой не хотят к метиться в долгу деньгами они готовы платить помесячно чтобы иметь возможность любой момент уйти отписаться от вашего сервиса и таким образом не делать ставку на ваш бизнес видит какой-то крупной суммы например виде годовой подписки из этого следует что нам нужно накапливать данные в сильно большем объеме чем обычному река мир сервису потому что нам нужно знать про пользователя больше чтобы ему больше предлагать чтобы уметь удержать его как можно дольше в нашем сервисе и здесь есть еще один интересный момент то что по подписываем бизнесом очень мало готовых решений коробочных и они как правило не удовлетворяют ну покормили нашим потребностям точно поэтому много чего мы пишем сами некоторые вещи даже делаем порой впервые и после вот рассказа про то какая специфика немножко про то какая проблема у нас есть основная проблема это то что у нас клиент останутся прибыльными через пять или шесть циклов оплаты то есть у нас клиент должен пробыть с нами пять шесть месяцев минимум чтобы мы вышли на окупаемость бизнес для этого делает все и даже больше потому что мы пока что живем и успешны для того чтобы бизнесу предоставить максимальное количество данных мы как я уже сказал накапливаем большое количество данных и вращаем их с разных сторон представляя для бизнеса различные разрезы репорты выгрузки и прочее прочее прочее это я так плавно подвел к тому что мы делаем в команде аналитики для того чтобы помочь нашему бизнесу жить в предыдущих сериях два года назад я точно также выступал на highload в питере и рассказывал про то как эволюционировал наша система как мы строим систему аналитики как мы с чего мы начинали и что мы в итоге получили кому интересно будет посмотреть немножечко про нашу внутреннюю кухню можете вот по qr кода себе сфотографировать перейти посылочки поставить лайки все такое вот теперь непосредственно что мы сегодня будем обсуждать обсуждать мы сегодня будем следующие вещи сначала немножко теории про различия между концепциями и тел и и lte потом я немножко расскажу про то какие у нас были проблемы с нашим идеалам и потом мы перейдём к инструменту тебе титул и чем он нам помог ну и потом в конце небольшое заключение какие у нас результат из 9 кстати вопрос зал в одиннадцатом году людей которые знают что такое тебя типом в зале не было сейчас кто-то знает что такое 9 поднимите руки три человека на зал ладно хорошо есть повод рассказывать дальше значит немножечко протона чем остаемся в девятнадцатом году примерно вот так выглядит наша архитектура таки выглядела сейчас уже немножко по-другому мы ушли с shift она snowflake у нас там произошли какие-то изменения дополнительные увеличилось количество источников данных в общем много чего поменялось на самом деле архитектура достаточно классическая у нас есть сайт который пишет данный пост гаррос мы через реплику читаем эти данные с помощью а в с glu раньше читали закидывали их в redshift и раньше в ту же эти данные использовал в сторону локера для бизнес-пользователей внизу вот компонент пристроен 9 введен при угу прямоугольником и вот как раз на прошлом докладе было очень много вопросов что это и зачем она нужна ну и исходя из этих вопросов у меня возникла идея о том что как раз наверное стоит сделать доклад отдельный посвящена этому инструменту что больше рассказать не по верхам а как-то чуть чуть подробнее углубиться вот теперь немножко кто не знает что такое теле я надеялся но пропустим этот слой так этель быстро википедия говорит нам следующее у нас есть процесс который перевозит данные из одного источника в наше хранилище ну или в другой какой-то до источник и каждая буква это аббревиатура означает действие которое происходит в рамках этого pipeline а с началом и данные извлекаем из источника потом мы с ними что-то делаем преобразуем и потом их загружаем целевое хранилище вызовов на горя этель это обозначение такого pipeline экстракт transform и лот это ну такая базовая штука в даты инжиниринге есть такая книга ральф кем был написал я думаю что это но если не самая популярная точно одна из самых популярных книг в дата инжиниринге она про то как строить хранилище данных первое издание был один шестом году последний пункт 2013 но это так статистика по большому счету что важно в этой книге то что ральф кем был в этой книге задал две интересные концепции 1 это одиночная модель а вторая концепция я здесь правда не уверен задал ли он или он здесь просто детально все описал вторая концепция эта концепция этель там он подробно описал что это зачем какие шаги где выполняются и там же написал что этель возможен с использованием стейджинг ире и это такая опциональная штука которая дается на логику по команде разработки тем не менее вот и тельце стать джим керри суть все та же это такой же этель который у нас до этого был только мы сырых данные прежде чем преобразовывать загружаем какое-то промежуточное хранилище это может быть база данных это может быть с 3 какой-то жёсткий диск что-то подобное не суть самое главное что мы туда можем выгрузить данные и это стать джим керри она находится под нашим контролем чем такой подход хорош по сравнению с подходом когда мы просто читаем данные в памяти пытаемся их на лету обрабатывать аральским был написал что в этом случае намного дешевле установить павший процесс логично у нас данные все уже есть мы не должны идти в источник данных и по новых перри затягивать второй момент что намного проще делать backup или делать snapshot этих данных точно так же данные у нас есть под нашим контролем быстрее делать бэкап с ним шок понятно это что проще восстановить исходные связи данным от ваших преобразованном связей например у вас после преобразования получилась какая-то ошибка вы пытаетесь понять откуда эта ошибка пришла если вы пойдете в сами источники данных возможно вы даже не сможете на данный момент посмотреть данные в том виде в котором они были когда вы и затягивали ct джим керри это возможно минус идешь в том году это стоило дороже нужно было или отдельная база или дисковое пространство так или иначе диска и пространство нужно было да это стоило дороже в двадцать первом году это ну сложно сказать что это минус это скорее просто такая тема вот данность надо платить за диск да и все но не очень большие деньги что дальше дальше моё небольшое наверное заключение о том что концепция и lte это некое развитие концепции этель состоит джон керри и длс и это этель состоял джим керри я так обозначил кто знаком с концепции и lt поднимите руки остальные не знакомы ok это все то же самое что и теперь только буквы последние местами мы переставили и и соответственно получилось что mas шаги лоты transform они поменялись местами они происходят теперь в другом порядке как это выглядит на практике вот смотрите у нас есть хранилище данных data warehouse обозначен да и мы в отдельную схему под названием ро да это мы завозим сырые данные там храним после чего прямо в этом же дата хаус происходим какие-то преобразования с данными и грузим в другую схему которую называем transform дейта потом к этой схеме цепляется биой tool и мы читаем данные уже оттуда готовые для бизнеса в чем преимущество в том что у вас данные сразу находятся в целевом источники вам не надо делать каких-то дополнительных движений если необходимо что-то посмотреть все у вас под боком если нужно делать snapshot восстановить откатить так далее все находится соседней схеме прямо рядом небольшая подбивка того чем этель отличается от и lte здесь поста на самом деле все достаточно просто этель по своей сути он представляет из себя концепцию преобразования данных в памяти там есть как правило какой-то отдельный класс стр допустим на спарке до который выполняет преобразование с данными в оперативной памяти соответственно нужные дополнительной мощности нужен один кластер при концепции и теле намного проще прям на входе отсекать какие-то ненужные данные или маскировать данные чувствительные и когда-то это было действительно дешево но было дешевле не то что деж был дешевле чем концепция и lte концепции уйти с другой стороны эта концепция в том что данные преобразуются уже in place то есть мы уже данные привезли в их целевое хранилище там выполняем преобразования какой минус нам нужно больше диско диско нужно больше ну скажем в два раза притом каноничных случаях на хранении сырых данных и на хранения обработанных данных мы правда в этом случае можем не иметь отдельного класса для обработки данных сегодняшний мощности баз данных позволяют делать многие преобразования средствами самой базы данных средствами и и кластера у нас сюда подтягиваются плюсы которые мы который аральским был прописал в и тельца station каире это восстановление отладкой бэкап и иногда у нас так бывает что сырые данные мы можем использовать напрямую то есть мы можем сразу бить тулу подключить к схеме сырыми данными и в итоге у нас получается меньше операции ввода-вывода ну возможно это кому-то будет важно вот теперь немножко про то какие сейчас тренды у нас данными происходит прежде всего нужно отметить что диски стали стоить дешево пространство стоит дешево и все это привело к такому большому буму в сфере работы с данными прежде всего количества этих данных стало расти причем расти достаточно быстро и для того чтобы обрабатывать новые объемы и массивы данных нам нужно работать с уже более сложными моделями сложные модели требуют документации требует тестов требуют отладки и с обычными подходами уже здесь достаточно сложно вести разработку что еще у нас благодаря этому появился такой бум инструментов по работе с данными вот я внизу привел небольшой скриншотик взял обзорную статью по современным инструментом работы с данными там был достаточно большой постер где были расписаны все самые модные современные решения по работе с данными я сделал небольшой вырезал небольшой кусочек из этого постера и вставил сюда это вот примерно инструменты которые как-то касаются сегодняшнего доклада то есть на самом деле это очень небольшой эпизод из этого постера но тем не менее даже здесь инструментов очень много пожалуй за последние пять лет количество инструментов прям сильно увеличилась есть еще один интересный тренд это то что современные хранилища данных они сами могут выступать источниками данных для других каких-то сервисов это так называемый обратный тел когда вы делаете загрузку в обратную сторону вот и исходя из этих трендов можно сделать такой вывод что в современном мире и террелл по большому счету умирает все движется в сторону и lte ну и собственно говоря на этом наверное моя небольшая теоретическая часть она заканчивается перед уже больше к практической вроде как все теперь понимаю что такое террелл что такой и lte я надеюсь по крайней мере к нашей схеме вернемся здесь есть один интересный момент прежде всего у нас здесь есть каноничный и they'll процесс это взгл он выкачивает данные из редон для реплики преобразует их с помощью кластера апачи spark загружает преобразованные данные на с3 и после этого раньше эти данные выкачивается с 3 в свои схемы и там раскладывает абсолютно прям стандартный классический этель процесс на этой же схеме есть и lte процесс а это вот непосредственно дебюте инструмент который нас есть он работает с уже загруженными данными и вынимает x1 схемы преобразует и кладет другую зачем нам нужно в нашей инфраструктуре и этель и эл-ти ответ прост эту схему было сделано в момент когда мы переходили сity дельное и lte и нам нужно было держать 22 решений одновременно поддерживать вот что у нас было почему мы переходили со взгл у нади 5 что у нас не устраивало ввс gloom а в с глуп для тех кто не знает это амазонов ска и решение в котором вы пишите скрипты на питоне подключаете библиотек apache и spork-и делайте преобразование с помощью питонов ского кода через либо apache spark в чем проблема проблема в том что это решение крайне плохо встраивается в какие-то series эти процессы вы все держите на стороне амазона готовый хлеб для того чтобы поддерживать жабы в актуальном состоянии через pipeline и вся и сиди процесса нет мы писали собственно обертку на питоне чтобы загружать новых жабы существующей изменять старый удалять тесты мы так и не поняли как мы должны писать на этом инструменте достаточно высокий порог входа для с этим инструментом нужно понимать хотя бы как-то базового питон spark нужно понимать как выполняются трансформации нужно понимать как работает с хранилищами данных и все это надо стать таком low-level уровня и понятное дело что у нас было много драбов начале мы где-то штук с 5 закончили по-моему мы на 50 на 50 мы уже тратили очень много времени на то чтобы поддерживать больше самую инфраструктуру и сам код нежели с этими джебами что-то делать осязаемые понятно и бизнесу там была возможность выносить общий код в какие-то общие библиотеки но она была такая не очень хорошо не очень хорошо ложилась на теорию разработки там параллельную когда немного разработчиков что-то делают это не работает и еще один момент который здесь очень важен это то что в памяти этого процесса невозможно выполнять возможно может быть возможным просто мы не знаем но сложно делать преобразование если вы хотите не только взять данные из источника данных но еще из вашего целевого хранилище что то что уже раньше загрузили чтобы это все вместе загрузить в питоновский скрипт в глубь нам надо сильно постараться какая проблема у нас возникло что бизнес начал нам накидывать все больше и больше задач раньше когда ничего не было бизнес говорю давайте сделаем хоть что-нибудь мы посмотрим там на данные мы сделали первые пять модели бизнесу понравилось полетели задачи мы перестали успевать это все делать потому что мы занимались технической обертка и нежели занимались решением и в этот момент мы стали смотри по сторонам что предлагает нам экосистему что мы можем найти чтобы все это упростить и мы наткнулись на 9 и так что такое 9 9 это инструмент это фреймворк написанной на питоне который в концепции этель или и lt реализует как раз вот эту букву ти то есть реализует дает он возможность делать удобно преобразования ваших данных с помощью sql никакого питона sql немножечко макросов на джинджа написанном порог входа очень небольшой любой аналитик берет открывает sql все отлично знают практически все даже некоторые продукт оунер открывают смотрят что происходит все понятно если очень грубо сказать то 9 это такой некий регистрирующий фреймворк который берет ваши написанные sql модели выстраивает их в pipeline зависим в зависимости от того как эти модели зависят друг от друга и выполняет их на движке базы данных теперь снова вернемся вот картинки которая уже была немножко про другую тему но как бы я поленился рисовать отдельную картинку и перри использовал ту которая была здесь вот непосредственно место ти5 во всей в pipeline и и lte а ну как раз показано на этой картинке здесь у нас так сейчас попробую с помощью вот собственно работу на 9 его место это выгружать данные из с одной схемы преобразовывать загружать в другую не очень работает вот здесь важный момент смотрите там где написано data warehouse из отображены 44 логотипа это big вере snowflake от shift и позже раз как бы тем самым показано что 9 умеет работать со всеми четырьмя этими базами данных но на самом деле количество коннекторов или адаптеров она не ограничится только этими решениями по моему сейчас около полутора десятков адаптеров среди адаптеров есть пресс то есть spork-и вот эта большая четверка в общем даже для crack house а есть адаптер который поддерживает с помощью комьюнити по моему ребята и сами из детей они к этому компоненту не имеют отношения но тем не менее все open source вот теперь немножко посмотрим на то как модели данных непосредственно пишутся в дебюте как это как это физически выглядит в виде кода здесь есть одна концепция которую следует наверно устроить что 9 говорит о том что любая модель данных это лишь select фактически с помощью selecta вы делаете выборку данных с которыми вы хотите работать а 9 дальше берет на себя задачу по тому как эти данные записи сеть базу как правильно их правда итить если они правда телись как правильно их удалить если они удалились измените далее так далее вам только нужно написать select и указать какого типа будет модель на выходе это может быть таблица в ухо или сети команд и был expression таблицы на уровне 9 разделяются по нескольким по типам но тем не менее физически на уровне база данных есть просто обычная таблица и что еще очень важно то что модели могут ссылаться друг на друга они могут переиспользовать друг друга и могут зависеть друг от друга таким образом 9 знает при запуске какой модели какие модели уже к этому моменту должны быть готовы должны быть сбил длины и и должны быть запущены на базе данных пример такой модели пример достаточно там простой он реальные на самом деле представьте что у вас есть корзина товаров вот у нас есть таблица под названием a rock art и в этой таблице вы имеете ссылку на пользователей usa ради все просто казалось бы проблем никаких есть один момент вы знаете что среди ваших пользователей есть пользователей тестовые и вам нужно их отфильтровать соответственно по большому счету как это делается в дебюте вы пишете одну модель вот эта модель в правом верхнем углу и школьника и и выглядеть примерно вот так то есть вы выбираете всех пользователю которых email вот похож на что-то такое тест точка ком и на выход вы даете просто айдишники из этой модели таким образом вас получилось модель где вы сосредоточили всех тестовых пользователей в другом файле создайте модель уже непосредственно связанную с корзиной данных с корзиной товаров извините и в этой это вот как раз слева у нас основной шквальный код и в этой модели вы делаете лавджой на вот эту выборку неправильных пользователей ну скорее тестовых пользователей да и соответственно через of joint фильтруете вашей целевой модели тестовых пользователь их просто отфильтровывается для того чтобы указать какой тип материализации вашей итоговой модели вы в конфиге модели перри приписывайте в разделе matter lives приписывайте тайбл собственно говоря это говорит 9 о том что вы результат выполнения этого selecta хотите за причастить базу виде таблице указан тоже с правом нижнем углу схематично как выглядит работа 9 с точки зрения взаимодействия моделей скомпилированный с интерпретированы кода который 9 в итоге исполнит на движке база будет вылет как то вот так то есть по факту модели фильтрации пользователей ненужных она будет встроено в виде сети в ту модель которую вы написали для того чтобы выбрать данные из корзины товаров то есть это будет проходить абсолютно незаметно для вас все будет проходить под капотом сети и будет встроено и по факту вы получите такой вот код который уйдет сторон в сторону базы данных дальше есть встроенное решение в дебюте для работы с таблицами в которых вы хотите обрабатывать только дельту только вновь пришедшие данные они называются incremental и эта модель выглядит практически так же как и предыдущие модели вот пример такой модели все точно также мы пишем select какие-то join и ссылаемся только в конфиге вытерлась указываем что это инка ментальная модель incremental и указываем по какому ключу нам нужно отсеивать дубликаты для того чтобы понимать как определить дельту в виде макроса есть специальная команда и из инструментов где мы указываем непосредственно условия с помощью которого 9 может понять что сырых данных есть новые данные которые нужно обработать отдельно в данном случае мы просто проверяем таймс темп максимальные который хранится в нашей модели нам нас на текущую модель мы ссылаемся с помощью слова адрес ну и собственно говоря все все что больше максимального там с темпа в нашей текущей модели мы загружаем и добавляем к тому что у нас хранится все это происходит под капотом то есть 9 полученный результат этого selecta добавит в таблицу в которой уже есть данные до этого при openteck их и будет дальше работать с ними дальше есть еще одна модель это snapshot модель канонично она называется по моему солнеч engine dimension и по факту оно из себя представляет накопление исторических логов изменения строк вашей исходной таблицы здесь на примере видно что у нас есть статус и которые изменяются спиннинг на shift и как-то выглядит вы просто работаете с таблицей в которой лежит pending как только он меняется спиннинг на shift и вы запускаете 9 после этого изменения 9 сам определяет что значение поля статус изменилось делает новую строчку куда прописывает в технические поля 9 велит фромы детей в элиту новые значения для того чтобы вы могли точно понять с какого по какое какой статус его сохранился и какой был валиден логику сны что-то можно построить как надо так например на поле last апдейт или на изменение каких-то конкретных значений в колонках пример такого с ним что то все тоже достаточно просто вы просто в селе бти указываете таблицу из которой вы хотите выбирать новые данные для вашего snapshot а а в конфиге указывайте стратегию с помощью которой вы хотите сделать добавление новых данных ваш накапливаемый snapshot за на случай это стратегия чек то есть мы проверяем что значение колонки хэш изменилась или не изменилось если изменилось то у нас появляется на записи если нет то новые записи не появляется все достаточно просто ещё один важный момент это крутая фича 9 это тесты и документация при этом с каждой модели вы можете положить ним файл в котором прописать тесты и документацию 9 будет сам запускать эти тесты ну че за определенную команду понятно что не совсем уж сам вот и 9 предоставляет определенный набор тестов готовых которые уже есть к этим тестом вы можете написать свои тесты константные а на базе документация которую вы запишите виам файлы вы получите в версию вашей документации которую можно загрузить шарики так далее использовать свои ежедневные разработки пример такого файла это опять же работа с моделью который мы рассмотрели раньше таблицы бекарт здесь я просто указываем название модели потом указываем какой-то дискрипшн который нужен для документации и потом идет описание колонок каждая колонка это имя description опционально да можно добавить тесты в данном случае вот для колонки а где мы добавили два теста 1 проверяет что у нас нет значений с что нет колонки извиняюсь где нет что нет строки где айдишник будет пустой и что в сердечнике внутри колонки 1 они уникальны нет повторяющихся ключей таким образом вы контролируете что вы не за дублировали данные не загрузили что-то лишнего как выглядит константин константин это такой же select вы просто делаете отдельный sql файл в папке тест куда кладете школьник который будет что-то проверять на данном примере мы проверяем что пользователь была лишь одна успешная сессия это считается пройденным если у нас школьник не вернул никакого результата и упавшим если он вернул хотя бы хотя бы один результат сейчас по моему в одной из последних версий в дебюте добавили фишку что есть еще промежуточном состоянии варнинг например у вас если кастом тест вернул 5 результатов это будет wording если вернул 10 то это будет опавшей тест а вот так выглядит версия документация который генерит 9 по итогу вашего заполнением файла среди прочего здесь есть прикольная штука очень интересная и талайна из граф ну так вот получилось я максимально и увеличил и сделал скриншот что было видно было опасение что экрана будут не настолько большие и будет плохо видно поэтому вот как то так в общем суть очень простая вы можете выбрать какую-то модель и 9 вам нарисует в документации от каких моделей зависит ваша целевая модель и источником для каких моделей ваша целевая модель является еще одна прикольная штука где 5 это макросы макросы пишет она джинджа есть уже готовые макросы например встроенные тесты это готовые макросы на джинджер которые написаны в 9 добавлены в 9 для чего мы их используем мы их используем для того чтобы убирать повторяешься куски кода и для того чтобы автоматизировать какие-то действия которые мы можем сделать с помощью 9 например мы клонируем наши схемы из production of test с помощью скрипта написано на 9 пример таких скриптов вот это скрипт который мы используем для того чтобы уйти от достаточно раскидистого способа определения минимальной и максимальной данной даты чтобы не повторять это каждый раз мы по факту написали до таких макроса их просто вставляем в нашей школе коды и и бить и дальше от все разворачивает уже готовый школьник заменяет а вот так выглядит наш макрос на клонирование с кем мы живем нас на флейте там есть такая штука как клонирование схем и собственно говоря вот мы написали такой макрос и вызываемый о через командную строку через дебютировал на первой шин ну и там передаем дальше все параметры теперь немножко о том что по итогу у нас получилось мы уже больше двух лет эксплуатируем 9 и как сейчас это у нас выглядит сейчас это выглядит у нас так что у нас есть достаточно встроенный вся и процесс у нас после каждого коми то происходит запуск построения всех моделей потом происходит тестирование всех моделей на стоит же а когда приезжают новые данные для prado запускаются последний успешный build с последней ревизии с последней успешной revise до запускается на проданных сейчас у нас вот 270 моделей в 9 и больше 1200 а тестов каждый прогон занимает у нас 70 80 минут на самом деле там можно сделать более интеллектуальные способ прогона именно делать бил только на том что изменилось но мы пока что ты ты мне добрались это новая фича которая добавила с 9 недавно вот и соответственно мы данные завозим 1 сутки после успешного билда на проди рот копируем клонируем себе на тестовый момент после сборки еще интересный момент мы используем три слоя моделей это такое условное разделение мы договорились что у нас будет как бы три типа преобразования в рамках 9 чтобы наши модели не усложнять первый слой базовый это вот как я показывал примерно ситуация с очисткой тестовых пользователей то есть мы там делаем какую-то очистку не валютных данных преобразование типов и перемывания колонок форме ручная она слой не очень обязательный он больше для того чтобы снизить сложность конечной модели бывает важно ну и конечно я модель это то что уже экспозиция в сторону локера в сторону bio инструмента такие результаты у нас по итогу опять же этих двух с половиной лет эксплуатации 9 ну выглядит от все примерно так что самый главный плюс что бизнес намного быстрее начал получать доступ к данным мы меньше стали писать кода про то что нам делать вокруг данных и больше остались на том как с этими данными действительно что то сделать чтобы помочь бизнесу у нас стало наверное можно так назвать внутри компании больше к контрибьютором наш репозиторий потому что с помощью скверне к появилось больше людей которые могут туда заглянуть что-то понять что-то поправить очень важный момент у нас появились такие достаточно условны и но тем не менее абстракции то есть мы например вот на примере фильтрации тестовых пользователей мы получили такую возможность один раз написать код который фильтрует тестовых пользователей и больше к этому не возвращаться ну просто ссылаемся на этот код если нам нужно и не нужно помнить о том что ага я пишу новые скальник здесь надо не забыть отфильтровать тестовых пользователей дальше у нас есть появилось опять же с помощью 9 это выстраивания зависимости между моделями 9 понимает какую модель нужно в каком порядке запустить и запускают все это параллельно в несколько потоков но это там для ускорения призвана но тем не менее есть такой момент очень важный момент у нас появились тесты мы действительно нашли очень много косяков и проблем понятное дело когда там 200 с лишним модели очень сложно писать их правильно опять же тестирование получить помогает очень сильно и все это отличное легло в наш классические соседи процесс ну и наверное вывод по итогу моего доклада это то что 9 как инструмент он позволяет вам снизить сложность ваших моделей они будут реально сложены вы сможете с помощью 9 разложить ваши дата модели низкого уровней абстракции и тем самым не только убрать опять же сложность в этих моделях но еще и позволить переиспользовать ваш код кстати так в дебюте поступают в комьюнити есть прям целые пакеты которые вы можете выкачивать там написанные модели для каких-то популярных вещей например там для данных из google analytics или данных и shape фай то есть вы можете выключить себе этот пакет и переиспользовать то что уже было когда-то написано чтобы из данных закачан их через записку получить нормальный хороший удобоваримый результат в виде данных который можно тут же цеплять в луки и показывать бизнес-пользователю у меня все спасибо за внимание вопросы спасибо за доклад вопрос по инструменту 9 есть ли у него под капотом возможность работы с продолжающейся эволюции структуры данных источников то есть если он еще продолжает развиваться данные в нем наполняются вот им и вот через и lte через делаем какие-то трансформации но нужно учитывать то что сама структура меняется вот вчера она была такая сегодня стала такая то и хотелось бы чтобы написаны все вот эти процессы трансформационные они ну скажем так не переписывать их там вручную чтобы это может быть как то гибко менялась ну как вот ликви бэйзел джавер например я не очень понимаю тогда вопрос то есть вы хотите за ложится как-то в коде написанном в дебюте на то что завтра как-то в вашем источники изменится схема и будут или там как-то джейсон будет в или по-другому и привет новые данные или как выяснил пример вот например у нас есть какая-то табличка в ней там было н полей там назывались они так то но сама структура сам источник он продолжает развиваться вот хотя и вп ради там появляются новые колонки в них новые данные содержатся а где то не дай бог там тип поменялся вот но так как вот я видел то что здесь предполагается написание скриптов там тепло и сквер там возьми из одного положил другое вот мы соответственно все поедет в какой-то момент если не предусмотреть вот это изменение которое ну от разработчиков этого источника ну в каком то виде должно до вас дойти до безусловно то есть вам нужно понимать что как бы так или иначе это все равно и сколь ник да вы завезли данные вашу базу они скорее всего если у вас изменила схема скорее всего так или иначе тут какие-то проблемы на этапе загрузки ваше хранилище то есть у вас сам процесс выкачки и загрузки вашу базу он упадет возможно упадет скорее всего еще там если он все же заедет например там типа поменялись да а у вас схема вашей базе куда вы постоянно грузили ваши данные она имеет другие типы то вы просто их не сможете загрузить если вы грузите какой-то джейсон то понятное дело вы загрузите его но до вам придется здесь тогда поправить код потому что джейсон изменился чтоб его развернуть нужно делать какие другие операции то есть что-то предусмотреть заранее но мне сейчас нас сложно сейчас подсказать что можно использовать вот в рамках 9 то есть вам придется да этот ход поправить и предусмотреть но за исключением разве того что вы напишете везде select звездочка до этим самым вы себя обезопасите от того чтобы вписать туда именно колонок но стоит ли она того не знаю иван есть из интернета вопрос внимания на экран алексей приезд спасибо на самом деле первые пять опрос озвучили мне тогда интересно по поводу туринга как можно работать в какой-нибудь стадии разработки как подсвечивалась вот эти вот краз поля и либо таблицы но либо видно связанность модели вопрос классный хороший к сожалению на данный момент в плане туринга ребята из девяти запилили свой онлайн редактор но мне кажется лучше бы они инвестировали свое время возможно в какой-то там плагин для идеи или для visual studio в общем для оффлайн инструментов я пока что не знаю какого то нормального туринга который бы понимал вот эти хитросплетения макросов на джинджа и иску элей я пользуюсь в судьба им текстом то есть как то это так происходит то есть на данный момент я знаю что комьюнити работает над тем чтобы написать плагин для идеи но пока что не готов сказать насколько это близко шению хорошо понял спасибо спасибо спасибо за доклад вопрос по поводу этого инструмента он выполняет какой-то запуск по расписанию или он все-таки генерирует скрипта кладет его в какой-то базовой дальше вы цепляете и запускаете смотрите запуска вообще этого инструмента он зависит от вас от того как вы его строите да и где вы его положите то есть для того чтобы если вы хотите запускать по крону окей настройки крон который будет в командной строке дергать этот инструмент и запускать его в базу хотите сделать это пауками ту точно также встройте и вода то есть он по факту не предусматривает встроенных решений которые решают вопрос момента запуска это инструмент который по большому счету берет на себя ответ на только за трансформацию данных а где его запустить как их запустить в какой пас и насти это уже вопрос уже к вам то есть это не совсем комплексное решение этель это только вот кусочек который внутри те или будет и вы уже сами решаете когда вы запускаете в каком плане мы например это решаем с помощью фреймворка арго то есть у нас не знаю сталкивались нет это оркестре дующий freemont который запускает по разным условиям запускает контейнеры и соответственно выполняет их может выстроить между ними цепочки связи есть один контейнер исполнился другой упал так далее но там это не связано никак с 9 это просто вы можете вот контейнер позже все что вам хочется любой процесс который вам захочется вы туда упаковали перевозку наших данных и в следом за перевозкой упаковали отдельным контейнером 9 и собственно запускаем друг за другом в связях понял спасибо еще вопрос пол потоковую обработку он умеет триггеры создавать или что-то такое триггеры создавать что вы имеете ввиду в триггер и после inserto слой исходных данных чтобы их потом сразу как-то обработать на самом деле вы можете опять же это триггеры можно создать с помощью scoin если я правильно понимаю таких триггеров вы можете написать без проблем обертку в виде макроса куда положить искали код который создаст ригер и потом этот макрос просто вызвать из вашего генетического кода как вам нужно то есть без проблем здравствуйте спасибо большое за доклад очень классный инструмент наверное мы его попробуем и у меня с этим вопрос поскольку он работать чести маски или мне так понимаю что он использует синтаксис и склеили и все возможности соответственно к определенной базы над которым настроен правильно понимаю да все так и соответственно вопрос заключается в том насколько вы ушли в принципе от этель процесса в сторону и уйти и конкретно в контексте сбора сырых данных то есть у нас получается сырые данные это только аналогичная таблица в этой же базе данных и мы не берем например в расчет какие-нибудь хранилище другого рода ну типа допустим data storage же там какие-то другие допустим файловые хранилища и так далее и мы отсюда соответственно этот инструмент может работать только внутри самой базы и если у нас данные сырые сохраняются каком-то другом формате нам все равно нужны теле процесс до выше правильно понимаете получается у нас настроен процесс который загружает все наши данные из разных источников и то там с 3 другие базы по азгора с какие-то описки сторонние все это запускается собирается завозится мы используем сингер для нашего теле процесса завозим все это в нашу схему в с на флейте и потом запускается 9 который все это перемалывает и перекручивают и да мы фактически ушли от этель процесс полностью в ялте мы ничего не делаем на стороне перевозки данных мы их просто забираем и кладем все остальное делает 9 пока что нам этого хватает здравствуйте спасибо за доклад хотел спросить пробовали например более дженерик инструменты для построения дагов ну там типа того же airflow просто кажется что писать чистый сквере они запускать абстрактную задачу то бывает иногда сложно особенно если приехали jison и а нам хочется их разложить улице он q оба за это не всякая умеет вот прямо из коробки смотрите здесь есть немножко разные зоны решают эти задачи в общем 9 она может быть как составной кусочек air flow на самом деле то есть вы там можете его туда положить и сделать как одним из типов я не очень силён в airfield опомнись как разные типы контейнеров до или разные типы задач и вы их можете сцеплять друг с другом вот 9 может быть одно из этих контейнеров или одной из этих задач на самом деле вместо р float используем орга о котором я говорил там тоже вот он оперирует также абстрактными задачами фактически ему все равно что мы запускаем вот мы запускаем что что из трактор дагов да по факту факту да то есть так вот и работаем просто 9 он не решает из коробки эту проблему и он не занимается этим здрасте спасибо за доклад смотрите 9 хороший инструмент жаль что нет никаких идей или плагинов к идее но может быть как-то решено проблемы с хождением к существующему проекту 9 например там фреймворка там роботов фреймворка есть сама документирование там различных rest сервисов есть сама документирования что-нибудь у дивизий есть чтобы я как новый разработчик подключился посмотрел какие модели есть что им что они из себя представляют и мог их переиспользовать при создании новых моделей или как редактировать существующие я правильно понимаю вы про то что вот у вас есть какой-то код legacy то что что у вас уже существует и вы хотите это подключить и где 5 каким-то простым способом нет нет нет есть 9 из там порядка сотни моделей 9 есть ли какая-то сама документацией всего скопом модели ты здесь может быть какой-то html никотин генерируется на лету где они списком всем выведены с кратким описанием что-то в общем как бы документация которую я показывал она генерится а я вам файлы можно по всем моделям сгенерить есть специальный пакет в дебюте которые вы можете запустить и указать модели и он по ним сделает какую-то базовую донатим ними документацию то есть ям файлы он сам на генерит а вы потом мозги не реф этот html вы сможете через него по навигации походить посмотреть какие модели вас есть какими связи да там не будет прописано понятно комментариях потом что кажется что каждое поле делает но как бы в целом вы база вы можете это сделать без проблем спасибо здравствуйте спасибо за доклад у меня тоже вопрос про тибете есть исходные модели у нас в базе данных и нам нужно получить результирующие 9 он имеет ли какие-то случаи как у вас тоже был пример там промежуточно есть базовая модель промежуточно если у него какие-то ограничения которые не позволят получить любую модель которую мы захотим и нам к примеру какой нибудь промежуточный слой для преобразования моделей придется там писать собственный сервис и помимо ти5 ей 9 может преобразовать есть вот у нас в одной базе данных лежат модели то он может преобразовать любым образом как нам надо по большому счету вы ограничены в рамках преобразований только тем что может сделать движок базы данных котором вы подключены то есть вы можете использовать ну например да там нас в полисе есть какая-то операция в рот шахте ее нет или наоборот или например shift очень плохо работать джейсоном развернуть джейсон например с помощью redshift а используя эти 5 порой бывает невозможно я с таким сталкивался snowflake например как движок о базы данных он позволяет намного удобнее работать джейсоном и мы переехав с redshift она snowflake то что мы делали каким-то сторонним питон процессором по разворачиванию джейсона внутри базы мы стали делать это с помощью 9 то есть по факту в ограничены с индексом базы которые вы подключены а дальше вы можете уже использовать все что там есть если конечно база данных не может преобразовать то к сожалению здесь 9 ничего не поможет спасибо друзья мы уже 1 минуты как должны выйти в цифровые кулуары это неплохо давай выберем кому подарим книжку за лучший вопрос ой вопросы были интересные хорошее мне понравился вопрос дмитрий да по-моему у дмитрий уже библиотека ладно хорошо вот девушка которая сказала что будет будет ряд 9 у нее тоже библиотека я ела ким пауки так так так так так так так вот молодой человек у него нет до 5 эти вот я уже забыл какой был вопрос напомню что вопрос был хороших давай сюда тебе тоже памятные призы и тебя ждут в целом цифровых кулуарах пожалуйста спасибо пожалуйста накиньте на спикера и до выясните все что он практического не сказал"
}