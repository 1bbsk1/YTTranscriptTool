{
  "video_id": "yRYYR13ymik",
  "channel": "HighLoadChannel",
  "title": "Распределенные системы в Одноклассниках / Олег Анастасьев (Одноклассники)",
  "views": 2212,
  "duration": 3416,
  "published": "2017-05-14T22:53:06-07:00",
  "text": "меня зовут олег анастасьев я работаю в одноклассниках с 2007 года в команде платформы и с тех пор мы сделали достаточно очень много улучшений и сейчас у нас абсолютно надежная сеть она никогда не ломается потерь пакетов не бывает она очень быстро и мы даже не учитываем сетевые задержки и построена на на новейшие технологии широкополосного оптического infini бандой теперь нам даже не нужно учитывать размер данных одно и то же железо пакеты в одно и то же время достигают всех серверов в дата-центре у нас отлично спроектирована сеть так что ее никогда не нужно менять она у нас полностью защищена у нас есть своя система на deeply линги для защиты сети все это управляется одним единственным администратором гениальный человек зовут елена и мы наконец пришли к тому что стоимость передачи данных у нас ничего не стоит как вы думаете где здесь набрал а наврал я здесь практически везде на на самом деле все это называется заблуждение разработчиков распределенных систем и если сходить по ссылке то вы узнаете почему это в общем вранье и заблуждение абсолютно во всех операций распределенных системах они были сформулированы пить ерундой чем и джеймсом гослингом давным давно еще в прошлом веке а что же на самом деле есть у нас в одноклассниках а в одноклассниках у нас есть четыре географически распределенных дата-центра понятно что если у вас есть несколько дата-центров и они географические распределены то уже надежную сети практически невозможно сделать у нас были случаи когда у нас оптику урвал пьяный экскаваторщик были случаи когда в коллекторах бомжик провода полили не задержка естественно тоже нельзя пренебречь если у вас есть географическое распределение скорость света никто не отменял и когда у вас внутри дата центра работают сто сто пятьдесят более 150 типов разных сервисов на 8000 ах-ха рваных серов естественно что нельзя это все выключить поменять что-то в сети и потом все включить обратно так не бывает это слишком сложно поэтому естественно что топология сети и сеть перри конфигурируется на ходу ну может быть у нас хотя бы админ нормальный есть хороший один давайте посмотрим кто на самом деле работает с продакшном в одноклассниках с продакшном работают инженеры это те люди которые больше обслуживают оборудования работают сетевики те кто настраивают сеть и администраторы тоже у нас есть но их ни один человек а 15 до и в конечном итоге еще есть разработчики здесь наша модель больше к относится к дворцу то есть сами разработчики могут эксплуатировать собственный продукт на продакшн и я сам являюсь собственно разработчикам поэтому мой сегодняшний рассказ будет больше вот именно с этой перспективы ну кроме вот эти всех людей еще у нас есть наши пользователи которых одновременно на портал заходит более семи миллионов и для них наш портал выглядит где-то так здесь у нас страница друзей да и вот у нас оказываются друзья я выбрал здесь фильтр по моим коллегам и давайте подумаем что нужно сделать для того чтобы от рендерить показать вот эту картинку вот эту страничку во первых нам нужно получить список моих друзей во вторых нам нужно применить выбранный фильтр по коллегам потом нам нужно подавить черный список то есть тех людей которых я внес черный список или меня внесли вчера список их показывать не надо затем нам нужно получить данные профиля и тех пользователей для того чтобы их отсортировать сортировка может быть разные она может зависеть как от того день рождения сегодня у человека он онлайн насколько мы блестит и для этого нужно получать информацию о пользователях затем нам нужно получить наклейки вот например девушка очень любит наклейки всякие подарочки для того чтобы их потом нарисовать на профиле потом нужно посчитать всякие разные счетчики их тут очень много здесь вот друзья а всякие много много много цифр их нужно посчитать и тогда мы можем показать эту страницу как бы мы могли решить это действительно если бы мы могли поместить все данные на если бы у нас был вот такой один большой большой серый в которой мы бы мы все данные могли бы поместить мы бы действительно сделали что-то подобное но у нас все сложнее и такой простой способ не работает потому что допустим у нас только данных о дружку 12 миллиардов связи и только вот эти данные занимают 300 гигабайт но это не является в общем-то большой проблемой а проблема является то что запрос на запросов на этот кластер в секунду около полумиллиона приходит профили пользователей еще большая проблема потому что и хотя их и меньше всего 350 миллионов но на этот кластер приходит три с половиной миллиона запросов в секунду и просто чтобы отдать этих пользователей по сети и вот трафик с этого класса идет 50 гигабит на сложно даже найти в принципе железку которая просто может тупо этот трафик отдавать поэтому поскольку нагрузка такая большая поэтому мы вынуждены делить все наши данные между машинами и в общем в итоге у нас естественно получилось распределенная система и которая работает на самом деле как то так запросы пользователя поступают на веб frontend если он работает с версии портала или на api frontend если он работает нативного приложения эти front-end и держат в себе сисю с пользователем а для того чтобы получать данные они вызывают applications сервер applications сервер работает в терминах бизнес-операции то есть у него где-то там есть запрограммированная бизнес операция которая называется получить все данные необходимые для того чтобы отрисовать страницу моих друзей и там есть логика которая в контексте выполнения этой бизнес-операции вызывает соответствующие кластера сервисов микро сервисов у нас есть отдельный кластер который хранит данные о дружба у нас есть отдельный кластер который хранит данные профилей пользователей и у нас есть отдельный кластер который служит для хранения будет листов почти все эти сервисы устроены одинаково чаще всего у этого сервиса есть локальное хранилище это может быть какой-то диск или обычный платы или это ssd который этот сервис использует для хранения своих данных логика обработки этих данных и необходимые каши программируется на java с помощью языка java они находятся в одной и той же java virtual машине и логика и крыши то есть они к лоцируется вместе с бизнес логикой каши понятно для чего нужны диски тоже не резиновая и часто не могут обслужить то количество запросов которые мы хотим все это управляется с помощью такой штуки как который называется ремонтный интерфейс ремонтный интерфейс представляет собой бизнес интерфейс этого сервиса для микро сервиса дружку он мог бы выглядеть как-то так то есть это мы видим что здесь у нас есть ремонтный method to get free and buy фильтр который получает по указанному айди пользователя и фильтру список они шик моих друзей и точно такой же ремонтный интерфейс есть у кэша профилей это тоже вот он ремонтный метод который называется get user болиде который по айди пользователя получает данные профиль этого пользователя вот это все работает с помощью нашей библиотеке ванне а она отличается тем что там в отличие от классической схеме схемы когда у вас micro series the rest час он какой-то вас никакого резона нет у нас есть библиотека ванне а она open source вы можете найти ее на детками она значительно быстрее работает и и значительно более компактная сериализация чему чосон и тогда имея вот такие животные интерфейсе мы могли бы написать вот такой код для нашего об летишь на сервере то здесь мы получаем список айди друзей нашего моих друзей дальше мы подготавливаем массив и в цикле для каждого из моих друзей проверяем если он в черном списке добавляем запрашиваем данные профиля друга и добавляем в результирующий массив и готова вернули в список моих друзей давайте посмотрим внимательнее на вот этот параметр по вот этим параметрам мы портится неру им воду на который будет отправлен запрос у каждого микро сервиса есть своя стратегия по которой он профи санирует данные то есть стратегия это каталоге к которая отражает айди которым вот этот айди в аде партийцы и затем в конкретные луны стратегии могут быть разные в зависимости от того какую что использует микро сервис для своих данных если он использует кассандру это стратегия будет что-то похожее на кассандра link если он использует вальдемар значит там будут вольдеморт про тишины а если кассандры ли вольдеморт не используется на самом деле то это достаточно часто происходит то тогда мы используем стратегию которая называется взвешенный квадрат все сервера кластера делятся на партиции и зайди по стратегии вычисляются портится и в рамках одной партиции существует несколько реплик данные реплик в пределах одной партиции гарантируются полностью одинаковыми наборы реплик всех партиций называется сет и на этот сет можно назначать вес для клиентов вес может быть назначен разным например для разных клиентов например для того чтобы локализовать трафик между клиентами 1 дата центра и тогда клиент выбирает ноду по алгоритму имея всю эту информацию клиент выбирает но до по алгоритму завешено важно утробина посылает ей запрос теперь когда мы знаем как это устроено подкапотном давайте еще раз посмотрим на вот этот самый кусок кода и попробуем найти здесь проблему проблема здесь мы в цикле опрашиваем ремонтный сервис да много раз для каждого и зайди мы вызываем ремонтный микро сервис и здесь начинает играть цена сетевого запроса у нас в инфраструктуре внутри если вы производится вызов пределах одного дата-центра то один запрос может произойти за 103 100 микросекунд если же нам нужно вызвать другой дата-центр то 700 или до вплоть до одной миллисекунды может произойти давайте посчитаем какая могла бы быть задержка допустим для меня я человек не очень дружелюбный у меня всего 200 друзей да и тогда для каждого мы должны сделать минимум два запроса на black list in a cage профилей на 200 друзей по одной миллисекунды 400 миллисекунд на получили просто на вот этот цикл много это или мало много для меня может быть еще не так очевидно но если мы возьмем например какую-нибудь девочку популярную которую 10000 друзей на то мы получим что на этот цикл мы бы потратили целых 20 секунд что же делать передавать список пакетные запросы да тогда мы немножко перья делаем наш ремонтный интерфейс таким способом чтобы он принимал не ключ и не по одному а списком да то есть вот здесь мы get- юзер боится переписали на входе у нас массив иди шик и на выходе коллекция профилей и тогда код нашего applications сервера будет выглядеть как-то так мы получаем список моих друзей тут же для всех этих друзей вызываем черный список передавая им список айдишник для фильтра и этот черный список возвращает только тех которых можно показывать и дальше одним запросом получаем все профиле пользователя вот такие пакетные запросы будут выполнены по алгоритму сплит мерч в этом случае сначала массив а дешак взбивается попортится и получается несколько отдельных массивов для каждого из этих массивов определяется по взвешенному round robin у нужная реплика вызывается получается от них ответ и в конечном итоге они объединяются в один ответ и возвращается клиенту вся эта логика сплит мяч отрабатывает полностью на клиенте это мы сейчас посмотрели как это работает если это все работает как нужно но первая же запись распределенных системах какая надежной сети не бывает давайте посмотрим рассмотрим простейшую распределенную систему из двух коммуницируют между собой серверов клиент посылает запрос сервер делает какую-то его обработку и присылает ответ как вы думаете вот такой простейший распределенной системы сколько может быть сценариев отказов сколько угодно неправильный ответ не 3-4 как минимум кто больше 6 кто больше давайте рассмотрим что может пойти не так что же может произойти ну во-первых может произойти пропажа клиента или пропажа серверы пропажа это внезапно это когда участник внезапно пропадает то есть это какая-то установка приложения аварийно на крыш reboot сервера все что угодно обрыв сити что еще может произойти может произойти потеря как исходящего сообщения так и входящего сообщения это может быть проблема опять же носите перри конфигурация сети переполнения входного буфера на одном из или другом конце 4 было поехали дальше может произойти тайм-аут отличает вне сетевых оглянись в вне временных ограничений дальше самое интересное что еще может быть неправильный ответ то есть один из участников отвечает данными который другой участник не в состоянии понять вот по вот этой классической проблеме в сентябре прошлого года упал skype тогда старые версии клиентов skype вских не понимали фару новый формат сообщений которым присылал сервы ну и мой любимый произвольный отказ участник он ещё называется византийским отказом участник генерируется общение не на основании запросов пользователей любое количество просто это могут быть какие-то dos атаки это может быть умышленные действия взлом может быть может быть какая-то логика retrieve вышит 6 под контролем сервера съезжает крыши он начинает 3 трать запрос все время да не на основании запросов от пользователя это 7 сценариев которые на распределенной системе из двух участников а если этих участников 3 сколько может быть сценариев отказа оси 4 а если это еще и взаимосвязано операция тогда сколько может быть идея здесь в том что вариантов или сценариев а вот таких отказы отказов растет значительно быстрее количество серверов дата центры и поэтому самая сложная часть в разработке 1 видео распределенных систем это предусмотреть все варианты отказов и реакции на них поэтому давайте остановимся на этом поподробнее что же делал с отказом в отказами можно в принципе ничего не делать это одна из стратегий простая и дешевая но здесь есть ключевой вопрос как эти отказы влияют на состояние системы и кто их может заметить если отказ никак не влияет на состояние системы и никто этого не видит то мы можем его в принципе игнорировать но часто очень сложно сказать увидит кто нибудь отказ или не увидит и как это повлияет поэтому часто игнорировать не получается еще важная часть что отказы невозможно предотвратить отказы случаются сами по себе в то время когда мы их не ожидаем и не ждем мы можем только скрыть отказ замаскировать его поскольку отказ невозможно предотвратить никакими способами то при проектировании системы неправильно исходить из постулата что отказов никогда не бывает нужно наоборот исходить из постулата того что отказ который возможен произойдет обязательно что же с ними делать да как скрывать отказы ключ скрыть и отказов эта избыточность мы можем внести избыточность по информации то есть это какие-то коды защиты от ошибок могут быть всякие интересные кодирования это может быть избыточность по железу то есть это здесь относится к этому относятся всякие резервирования реплики какие-то дублирующие схемы ну и самый простой популярный вариант это избыточность по времени то есть это какие-то транзакции центральные поскольку последний вариант самой популярной давайте посмотрим что какие могут быть сложности с этим вот допустим у нас та же самая распределенная система клиент посылает серверу запрос добавить друга сервер делает необходимые изменения в трансакционного хранилище и пропадает из сети давайте посмотрим вторую распределенную систему второй случай та же самая распределенная система клиент посылает запрос и сервер neues пропадает не успев внести никаких изменений в первом случае клиент должен повторить запрос извините 2 клиент не должен повторять запрос потому что измене изменение уже сделаны во втором случае клиент обязан повторить запрос потому что изменений не было сделано но как клиент может определить какой из этих случай провязав случаев произошел правильный ответ никак на самом деле пропажа сервера с точки зрения клиента никак не неотличимо даже со случаем потери сообщения потери ответа от серы поэтому со стороны у клиента что может ли он сделать у клинта возможны три стратегии первый вариант клиент может не давать никаких гарантий совершенно операция или нет это не очень юзабельны стратегия наверное да но тем не менее это наиболее частая в тех системах которые не проектировались учетом отказом лет может никогда не повторять запрос то есть максимум он доставляет запрос один раз и the most lanes тоже такая под вопросом до стратегия но я позже покажу где она может быть использована ну и клиент может всегда повторять запросто есть по меньшей мере один раз мы доставим запрос серверу какой-либо из реплик сервер но какую стратегию когда выбирать давайте посмотрим это на предмет на примере операции добавить друга операция добавить друга состоит из двух больших частей во первых это транзакция в и сид хранилища то есть мы добавляем друга в какую-то таблицу и к метим транзакцию здесь есть какой-то мастер который управляет транзакции здесь однозначен успех то есть или commit прошел или нет мы точно знаем также мы можем если хотим сделать rollback откатить изменения будто вы их никогда не есть вторая часть которая обновляет информацией в кашах многочисленных о том что у этого человека появился новый друг здесь совершенно другая ситуация здесь очень много реплик здесь нет мастера и часть из них может отказать давайте сначала рассмотрим собственно транзакцию а для этого введем понятие и дам поверхности эдем патент ность это свойство операций которая проявляется в том повторное применение операции к объекту дает абсолютно тот же самый результат что и одинарное вне зависимости от ос один или больше раз вы можете эту операцию применять без и получить в результате тот же результат что это может быть например это может быть самый простой это чтение можно читать сколь угодно раз одно и то же вы получите на выходе второе это добавление в сет какой то вы можете добавлять одни и те же данные вся сколько угодно раз получите тот же самый сад на выходе вычисление максимума или минимума тоже и а все больше ничего нет не очень много но на основании вот этих конкретных операций была сделана такая распределенная система как кассандра там есть добавление в сет то есть в калом фамилии новых данных там есть вычисления максимума там штампа между двумя конкурирующими изменениями и получается очень хорошая система с хорошими распределенными свойствами но в реальности не все бизнес-операции можно уложить вот в тут же к санту и здесь нам на помощь приходит такой небольшой трюк мы можем считать что в пределах тайм-аута и дым патентным является упорядоченное атомарная изменение с контролем дубликатов и тогда естественно что только для эдем патентных операций можно применять стратегию всегда повторять запрос давайте посмотрим как это на примере клиент посылает запрос подружиться на одну из реплика предтеч не сильно applications сервер проверяет являемся мы ли мы уже друзьями нет нет нет не являемся мы делаем изменение вас из хранилища и тут сервер падает клиент ждет тайм-аут повторяет запрос на другую реплику другая реплика проверяет являемся ли мы уже друзьями до являемся то есть это дублирующая операция и здесь сервер ничего не делает и просто возвращает ответ такой как будто бы именно эта реплика произвела операцию то есть заметьте что здесь 2 реплика не возвращает никакого рода ошибки типа вы уже друзья не не вам нельзя дружиться она отвечает так это нормальная ситуация это летрай и она отвечает так как будто бы именно она что сделала эту операцию теми же самыми данными это требует достаточно простой практики в программировании но зато даёт вам и дым патент ность но добавления друга допустим это такой очень специфический случай да и и он очень легко проверяется на дубль это на дубликат то что в принципе это проверка в сети да по большому счету но любую а то мариную операцию можно сделать и дым патентной с помощью sequence инга тогда клиент перед тем как отправить запрос выписывает какой-то идентификатор операции посылает эту операцию серверу с указанием этого идентификатора идентификатор сервер проверяет выполнял ли он уже этот эту операцию по идее если не выполнял то делает изменения и готовы подружить давайте посмотрим что может быть служить вот таким идентификатором операции ну во первых это может быть простой монотонный счетчик тут есть одна сложность да то есть если у вас несколько клиентов не один а несколько дата становится очень сложно поддержать чтобы эти идеи этих операций не совпадали между собой также сложно если клиент внезапно обучиться да с каким счетчиком дальше му стартовать хотя это не очень хороший алгоритм он тем не менее для этих целей используется например в тисе пейте чтобы отслеживать номера пакет второй вариант это может быть текущее время в миллисекундах например время у нас всегда натурально монотонно возрастает вне зависимости от того сервере будь это время уже кого это прошло за эту проблему мы решили но опять же если у вас есть несколько клиентов которые одновременно работают то время жду ними несколько может отличаться какие бы вы протоколы синхронизации времени не использовали вас все равно время на серверах будет разным это может быть проблемой мы такой операций найди используем например в чатах но он не таком чистом виде а мы используем есть а дичь а то айди участника чье то но и в качестве но есть и время когда участник chateau это сообщение написал да вот это является первичной де для операции добавить сообщение можно использовать например или айди а мы используем time you ready здесь уже значительно большей гарантии уникальности натурально могут быть но все-таки даже для эдем патентных операций таких как добавление друга можно не захотеть применять стратегию всегда повторять попытку и давайте это рассмотрим на примере второй части этой операции обновление информации сша для того чтобы когда происходит операция добавления друга клиент должен notify целовать все каши о том что появился новый друг чтобы они обновились информацию которая у них есть в памяти для этого он должен вызвать все реплики по одной но никто же не гарантирует того что одна из этих реплик внезапно пропадет она может крашнутся ее просто могли остановить админ или еще что то здесь не могли могло произойти в такой ситуации бессмысленно повторять попытку потому что никто не знает когда же это реплика вернется назад это частичный отказ да и что же делать что повторять мы не можем это бессмысленно но без повтора данные между репликами будут раз синхронизованы потому что это реплика когда ее запустят она пропустит все нотификации которые не были она запустится с устаревшими данных что за делать как же быть а тогда мы синхронизируем к с через базу данных то есть в каждом из инстансов в каждом из нот каша существует фоновый процесс который раз в 2 секунды где-то выполняет вот такой простой запрос на получает все изменения из таблички и смотрят какие пользователи изменились и если они были изменены то обновляет информацию в памяти сервера то же самое он делает когда он стартует то есть он берет время которое последнее было известно и время работы это этого не стал вся каша и запрашивать все изменения и перепрыгивает их памяти в такой ситуации дополнительный повтор не нужен и достаточно единственной попытке обновить кеш на лету если она не прошла все больше мы не пытаемся ну давайте может посмотрим что еще что же еще может пойти не так вот у нас есть клиент он посылает запрос на фронт что я хочу подружиться фронт посылает требовать бизнес операцию подружиться на сервер и тут сервер и начинает немножко тормозить он пошел своп или диск у него начал отказывать или сам дурак настроил сервер так что он начал притормаживать до клиент ждет тайм-аут как мы и договаривались но пользователи ждать не могут и они посылают еще больше запросов на фронт фронт отсылает эти запросы снова на тот же самый сервере они тоже останавливаются на тайм-ауте фронт тоже умирает у него заканчивается пул потоков или руди о заканчиваются количество открытых соединений или у него заканчивается память конец более менее предсказуемой и весь вопрос во времени когда же он сдохнет обычно происходит очень быстро буквально десять секунд что же делать ну например мы могли бы выбрать тайм-аут таким маленьким чтобы запросу успевали отваливать на допустим если у нас пул на фронте а тысячи потоков или 1000 там соединений свободных мы могли бы поставить тайм-аут в одну секунду и тогда тысячу в принципе операций за секунду мы с этим таймауту можем служить до для нас тысячи операций в секунду вашим не вариант и тогда мы применяем вот такую штуку она называется вывод из ротаций фронт перестает обращаться к серверу когда он видит за последнюю секунду x непрерывных отказов x это число конфигурируется для каждого сироп налаживать свою дальше как и дальше клиент не обращается вообще к этому серого он обращается только к другим репликам этого сервиса да и запускает фоновый процесс который проверяет а тот сервер который вывалился из ротации он ещё жив или нет если он это фоновый процесс он происходит сразу нему минуту он пробует какую-нибудь операцию с ним сделать если тот сервер отвечает то клиент обратно возвращает возвращает этот сервер то он снова начинает посылать туда запросе сервер начинает их обслуживать простая стратегия понятная она работала некоторое время но с ростом нагрузки стали появляться другие более интересные эффекты вот у нас допустим те же самые клиенты они где-то 10000 операций на фронт отправляют фронт отправляет это на сервер серверов хороший backend мощный хороший поставили 24 коры в среднем он способен обслуживает служит каждую операцию за полторы миллисекунды тайм-аут мы поставили ну полторы секунды потому что там бывают долгие операции какие-то всего 24 core i7 и 23000 операции он тянет большой запас 10 поступает 24 он может два раза запас все хорошо все прекрасно до тех пор пока сервер не начинает немножко притормаживать чуть-чуть почему это может быть он начинает немножко уходить своп до или диски у него начинает немножко отказывать и поэтому ретро и какие-то возникают или памяти не начинает немножко отваливать и там возникают лиц с эти паузы могут быть какие-то баги в java виртуальной машине привет ребята допустим у нас были случаи когда виртуальная машина внезапно де оптимизировала метод какой-то которую надо этого оптимизировать этот метод стал работать очень медленно и и тогда у нас допустим среднее время чуть-чуть повышается до 24 миллисекунд но при этом способность сервера abs обслуживать запросы падает до тысячи операций в секунду и соответственно он больше не может ты 10000 операций секундах служить и клиент умирает точнее фронт умирает еще до того как он успел понять что что-то пошло не так вот что делать в этом случае капитан очевидность предлагает поставить тайм аут тайм аут на 2 и 4 миллисекунды все так очевидно не получается некоторые операции занимают больше мы могли бы допустим считать какой-то средняя и выводить его из ротации осветления повышается выше критического уровня в 24 миллисекунды но вот этот способ мы пробовали на самом деле он оказался очень ненадежным и поэтому последнее время мы применяем спекулятивный по тв повтор приста коллективном повторе фронт посылает запрос на реплику не дожидаясь того когда тайм-аут закончится то есть мы сразу посылаем запрос и на реплику первичную и сразу же на вторичную как только одна из них отвечает мы продолжаем продолжаем выполнять обработку запросов клиента да то есть мы ждем как когда нам ответит самая быстрая из 2 реплик то лучше стала вот такой спекулятивный запрос спекулятивный повтор драматически уменьшает задержки и и средний и 99 persantine вот здесь вот реальные графики сэра классов до этого 99 причинитель задержки на секундах желтенькое это да спекулятивного ретро я до а костенька и это после того как я включил спекулятивной ретро заметьте что здесь по ночам происходят всякие интересные бака пышма копы и пике достигают вплоть до 1 секунды здесь они даже там были бы где-то там на пятом этаже этого здания до их обрезал здесь такого во первых не наблюдается 5 значительно ниже и во-вторых вот эта волатильность да то есть стабильность системы стабильность системы значительно выше потому что мы всегда работаем по самой быстрой реплики и убираем спорадические фильтр маски реплики но понятно что спекулятивное летроп не всегда применим да вы у вас должны быть и дым патентной операции они генерируют дополнительную нагрузку до почему онда в кавычках потому что дополнительная нагрузка на самом деле оно не является дополнительной на репликах сервиса потому что реплики сервиса всегда должны быть готовы принять на себя дополнительную нагрузку и они либо принимают дополнительную нагрузку либо просто полет электричество с пустым циpкa очевидный минус это дополнительный трафик на клиенте будет больше в два раза трафика чем до ввода спекулятивного если эта проблема значит он тоже не применим но спекуляцию можно балансировать мы сейчас всегда отправляем запрос на две реплики сразу но в принципе можно отправлять дополнительный запрос если время ожидания больше чем 99 пирсинг или или больше там чем 50 представители допустим мы сейчас думаем о по 50 ампер сентилья но пока еще не придумали как то сделать надежду ну что как обрабатывать отказы индивидуальных реплик мы разобрались давайте больше что еще одна реплика может отказать в реплике могут отказать почему чрезмерная нагрузка перегрузили кластер и все ноды свалить чрезмерная паранойя какой-нибудь хитрый скрипт отрубил все сетевые доступы на ваш рабочий класс готова баги все мы делаем баги кто-то запрограммировал какой то лог все реплики уперлись в этот лог кластер не работает люди естественно могут дать не ту команду и все пошло прахом могут быть как-то масштабной аварии ураганы потопы и все что угодно и у вас вываливается обрушивается потолок настойки все чтобы на вас может не работать что же ещё может быть а может произойти вот это все сразу вместе в каких-то совершенно непонятных комбинациях это самое интересное и что же в этом случае делать все реплики недоступны по какой-то причине что мы будем делать а мы должны в принципе все что можем сделать это деградировать деградировать можно по консистентной и это как раз тот самый случай допустим у нас есть кэш и есть авто это тивная база данных если мы всегда в нормальной ситуации читали из авторитете мной базы данных и она сейчас недоступна то мы можем читать информацию из кэша допустим потому что там хоть и устаревший теоретические данные но вся но хоть какие то есть да если нам в этом месте к системных неважно то ok мы можем использовать неполные данные вот тот самый если вы помните алгоритм сплит in which a то мы запросили в несколько реплик и некоторые из них полностью не доступны в данный момент по какой-то причине полностью все реплики партиции недоступны мы получили на клиент не полный набор не все не все ответы от всех реплик и тогда мы показываем вот такую красивую заглушечку типа извините не все данные доступны и какие то функции на фронте тогда выключаются автоматически ну или мы можем отключить функцию полностью вот здесь вот как чьи-то шаловливые ручки положили игры на портале и и игры мы отключили извините но все остальное фотки видео музыка все что угодно продолжает работать портал давайте посмотрим как бы мы могли это сделать в коде например вот тот самый ремонтной интерфейс users кого каша на мы вместо тогда коллекции профилей пользователей возвращаем вот такой дистрибьютор wrapper который в свою очередь содержит кроме самих данных еще и признак их can системности точнее не консистентной и тогда если не все реплики отвечают то клиенту возвращается признак не консистентной sti и на фронте а кот его может проанализировать что-то нарисовать какие-то функции отключить и так далее также интересной частью является клиентский стаб нашего user каша вот этот кусок кода отрабатывает на клиенте и он вызываться начинает тогда когда мы понимаем что все реплики данного сервиса полностью недоступны нет живых серверов они все вывалились уже тогда вызывается вот этот стаб метод единственный ее задачей вернуть те данные которые фронт поймет что что то не так но тем не менее не свалиться при этом в данном случае дна get user сбоить и он просто вернет пустые данные с признаком не консистентной sti если бы здесь был не тот который меняет какие-то данные то здесь был бы выброс какого-то эксепшен а допустим или если эти данные полностью не важны то здесь был бы но ничего бы не было сейчас мы рассмотрели как это все должно в принципе работать но это все должно так работать в теории но проверить это мы должны потому что чтобы быть уверенным что действительно так работает даже проверить что система работы с отказами правильно а чтобы быть увереными мы должны это протестировать что же можем протестировать ну во первых конечно мы должны тестировать свой собственный продукт здесь сложность потому что мы выкатываем продукт на продакшн приблизительно раз в неделю и мы должны проверить все сценарий откатов что очень довольно сложно стандартные продукты ну стали тестировать стандартный продукт кто-нибудь вашу любимую отказа устойчивую базу данных ластик search кассандр мангу вода не нужна вьетнам девушка а особенно стандартные продукты нужно тестировать а почему расскажу пример как то мы тестировали кассандру ту самую перед тем как ее в продакшн ввести в эксплуатацию вот взяли все распределили по дата-центром реплики выключили дата-центр все хорошо летит работает разливаем шампанское включаем дата-центр и весь кластер ложится выплескиваешь почему так произошло потому что внутри использовалась раньше очень не эффективная структура для хранения hand of то есть тех мутаций которые должны были быть доставлены на реплики которые лежали внутри кассандр она была очень быстрой на запись но очень медленно и на чтение соответственно когда дата-центра упал туда очень много данных пошло в эти hand и но она быстро жена запись работает поэтому она успела их записать а когда дата-центр поднялся то возникла дополнительная нагрузка на чтение которое дала столько нагрузки что кластер перестал справляться с той нагрузки в термальной про наши нагрузки и все надо свалить здесь в чем соль соль в том что стандартные продукты вы не сами не разрабатывали как они будут реагировать в каких ситуациях что там запрограммирована вы не знаете поэтому все сценария отказов вы должны проверить и убедиться что то реакция который запрограммирован стандартных продуктах подходит для вас ну и понятно нужно тестировать всякие админские маневры особенно если они происходят в первый раз желательно все все это прогнать до того как вы будете по живому резать для того чтобы тестировать свой продукт мы используем такую систему внутренние она называется горилла что она делает она работает на нашем тестовом стайлинговые сервере она определяет для каждого сервиса какие какие соединения по каким пактом открыты на каждый applications и равана каждый из микро сердца отрубает их запускает авто тесты и проверяет что ничего не валяться те заглушки которые должны показываться показываются а также она проверяет то что сервер стартует в той случае если его база данных где доступна это очень часто вот это пункт мы ввели после дня 404 когда мы года три назад 4 апреля упали из-за того что абсолютно все сервера выключаюсь все 8 тыс вот и при попытках запустить applications are a они все у тыкались там некоторые из них они не все они утыкаюсь в ожидании соединения с базы данных у всех же такой код есть в инициализации надо открыть соединение дождаться пока оно установилось потом что-то там может быть прочитать а может быть и не прочитать да и пойти дальше поэтому с тех пор мы проверяем что даже если любые сервисы недоступны все равно стартует с недоступностью этого как тестирует стандартное решение есть какой-то любимым манга или там мой скил ли еще что то стандартная как вот и стирать обычно тестирует вот так то сделают тестовый стенд синтетической нагрузки до но здесь есть две проблемы первая это то что топология сети вообще сеть тестового стенда она не никоим образом не совпадает с тем что у вас в реальности есть в продаже а во вторых очень сложно воспроизвести профиль нагрузки то люди диссертации пишут как воспроизвести тот или иной профи синтетически а у нас столько времени нет поэтому мы тестируем все это как настоящий джигит и на продакшн но так чтобы этого никто не заметил а проверяем мы при этом от сценарий отказов и восстановления системы как это могло бы быть сделан для нашего каша пользователей тогда вместо одного каша пользователи у нас есть прокси и у этого пункте есть два реальных для реальных систем и первичная которая работает правильно хорошо стабильная версия и мира тот который мы используем не стабильная версия и тогда мы сначала каждый запрос пользователя или каждый запрос клиента но сначала вызываем первичную систему и возвращаем результат а потом вызываемся тем же самым запросам вторичную систему и сравниваем совпадают результаты первичной и вторичной системы и здесь также очень важно иметь динамическую конфигурацию на основании которой мы включаем нагрузку на испытываемую систему увеличиваем или уменьшаем и тогда переключаем и так далее таким способом вы можете тестировать под реальные нагрузки на реальном продакшене реальные сервера статья которые потом включиться в продакшн вы можем их протестировать и потом после того как все хорошо переключить местами незаметно ни для кого не заметно праймари и мира и тогда испытуемая система станет первичный будет обслуживать запросы ну вот с тестами разобрались но то что тесты проходят совершенно не означает что наша реальная система действительно работает а вот диагностика production она может показывать реальную картину реальной распределенной системы зачем оно нужно быстро определение того что авария в принципе существует когда то давным давно на заре мы у нас один из серверов две недели был в аварии а мы смотрели влоги оттуда не заглядывали и потом только через две недели обнаружили шона free space локализация к эта проблема что же сломалось конкретно своевременное предупреждение об авариях тоже важно что мы собираем zabbix и когти здесь он не помогут ни помогут а собираем и операционные биометрики которые прошли по вот этому ремонте нгму протокол собираются имена вызванных операций количество вызовов успешности длительность каждого из вызовов потом это все отправляется в систему dv х она там агрегируется и на основании чего показываются всякие разные графики агрегированные чувство вызовов и ошибок за определённый промежуток времени агрегации задержек с из которых считаются среднее максимальная превентивные задержки и так далее и тогда получаются вот такие всякие интересные графики вот эти интересные графики на которых видно цена сетевого запроса это за замерена с клиента это сервер а вот такие разные графики на которых уже заметен инцидент какой-то на 99 тыс интеллект инцидент значительно интереснее задержка очень выросла сильно и мы начали составлять вот такие всякие дашборде очень много много много много и специальные люди которые называются монитор щеками сидели и целый день смотрели в эти графики и для того чтобы определить не возникло ли какая-нибудь anomaly который нужно которое предвещает какой-то инцидент часть из этих людей ослепла другая часть поумнела и они написали себе систему автоматического поиска на валей она сама просматривает кучу статистике каждую пятиминутку и сравнивать с историей и на основании этого там есть такой хитрый алгоритм да и на основании этого она рисует вот такой граф проблемой что здесь нарисовано у нас нарисовано что немножко притормозил видеосервис из-за которого притормозили фронты в этом дата-центре в этом и в этом дата-центре то ну тут еще есть какие-то накладные наведенные проблемы тоже но в принципе очевидно что задержка на фронтах было вызвано именно вот этим конкретным сервисом также у этого сервиса конкретный человек который вы эксплуатируют он мотивации руси решает проблему в принципе это все что я вам хотел сказать краткое содержание всех предыдущих слайдов выйти на экране возможности отказов распределенных системах безграничны отказывай маскируются за счет информации времени или железо при не маскируем их отказах мы деградируем и отказывай нужно тестировать точно также как вы тестируете функционал и отказывай нужно диагностировать и предупреждать на реальном продакшене спасибо вопросы у нас есть время на вопросы последняя система автоматического мониторинга вернее предсказания на выпутался будет когда-нибудь проблема в том что почти у всех практически у всех больших ребят в типа гугла и netflix и twitter и нас яндекса нету яндекс и нет а пока есть такие системы но боб и source fx вряд ли увидите просто потому что они очень сильно завязаны на всю инфраструктуру что есть в в у нас допустим чтобы вы могли бы внедрить такую систему у нас вы должны сначала использовать наш ванне а потом вы должны использовать нашу систему статистики потом вы должны использовать нашу систему 2-х и все то есть это все тогда нужно открыть то есть это очень это опять же системы такие довольно инфраструктуры и в сложно все открыть они очень большие спасибо за доклад вопрос такой вот когда говорят об разбиение на партиции то там был утверждение что в рамках партиции реплики гарантированно одинаковы это как гарантирую только что была про отказы тут у нас эти гарантии есть гарантия до что не одинаковы в пределах какого-то времени то есть нет того что они будут две секунды а еще вот смотрите есть весь дизлайки нас такой сценарий что нам нужно сбегать на один сервис потом на второй сирс потом на 3 1 3 сервиса в среднем они отвечают нам по доме или секунде у нас time all стоит по 5 миллисекунд предположим тусу максимализм получается 15 миллисекунд на время ответ внутри вот вы такие расчеты делаете то есть не по меня не а вообще примерно прикидывайте сколько там получится в худшем случае время ответа это прикидывается сейчас в пределах одного кластера но как раз про если мы говорим о сплету мертв и таких вот то там учитывается просто время таймаута но на самом деле не на каждом соединении считается она всем класть или когда целиком да то есть при входе из клиент обратился кластер то он должен вытираешь там а если это но и у самого фронта и свой тайм-аут тоже обрывает запрос заброс если он слишком вылетел про стратегию retrieve там был показан что мы на 2 сервер бегаем и навес на первый получил делю на 2 а почему перед это число 2 это эмпирическое число или там вот положено панику машину протокол ну смотри значит на самом деле у нас есть несколько систем 1 до не делает 22 другая 3 да это зависит от того какая там к ассистент ность идет весит или корм на до сих пор на эту 3 на самом деле идея достаточно простая то есть если мы первый запрос сделали и он отвалился то возможно проблема в конкретной реплики если мы сделали второй запрос тот же самый и он тоже на другую реплику он тоже отворился то проблема уже не в реплике проблема уже где-то в другом месте и просто ретро и мы и не решить когда или пан или backend какой-то там вывалился которую все эти реплики используют идеи в этом понятно а вот еще просек ментальностью про последовательные весь год до пресекать вот а то ты где распределенность то есть у нас там в 11 раз рвб ну там клин серны взаимодействие простой получается что у нас если вот этот сервер умер то вторые сервера в нашем в нашем парке шине они не никак не узнают про задачи или мы в базу кладем все таки request здесь имеется ввиду что изменения для того чтобы применить sequence sing изменение должно быть атомарном атомарные изменение может быть например и сип транзакция да то есть если транзакция очевидно во внешнем хранилище не то есть она тянется все понял и вот там по такой было утверждение что мы если получая ответить то мы говорим что раз отключаем запрос но при distance of time out many укладывать помните сценарий а тут вопрос такой и выпускаем тренд который отмониторить наши booking.com бегали вопрос в том что мониторе том что он мониторит то что сервер доступен соб но жив нет не значит не тормозит у нас же там причина отключение было в увлечении задержки ответа а мониторе мы просто доступность это как бы два разных показатель в принципе да два разных показателям но чаще всего вот эти тормоза вот именно такого плана возникают из того что есть проблемы на сети и тогда серую полностью не доступен или 2 еще вариант что если все worker треды нас на на микро сервисе вывалились то есть они все занято не на чем-то висят там то сам запрос доступности тоже не проходит по что он обслуживается теми же сам автор тоже и последний там замечаем что у вас получалось нам еще полторы миллисекунды а суммарно было 24 тысячи запросов оно не бьется у нас 4 коры на полторы миллисекунды до 20 не получается спасибо за доклад вот мне показалось что значит ipad and asti опираться там несколько спекулятивная ситуация может отвалиться сервис потому что долго транзакция в базе данных ответственности мы сделаем повторы транзакции она все еще не закрепилась и мы не знаем ее выполнение тогда следуй за просто же подвисает то есть как мы тогда можем обеспечить эту ситуацию до в ситуацию здесь с внешним aisin хранилищем единым да так такой не ну вот такой случай не сработает у нас это используется в ситуации когда у нас я и кого ровная система есть точнее стоит кассандра да и каждый из реплик имеет свою локальную копию данных ну свою реплику да и в этом случае вот у нас работать до в этом случае вот когда долго транзакция как еще хотелось ответить первому за вашим вопрос по поводу мониторинга на самом деле есть аналогичной системы которые внедряются в либо виртуальную машину java либо селлар и собирают аналогичные метрики то есть вы не пробовали что-то такое спасибо добрый день спасибо за доклад хотел вас спросить не пробыли вы в своих проектах использовать мозге контейнере чтобы к примеру если что-то неправильно работа быстро перещелкиваю версии нет на самом ловким и откинули практически сразу по по причинам того что он просто слишком тяжеловесные сложные в разработке и не дает никаких гарантий то есть мы больше здесь вот эту проблему решаем тем что у нас есть несколько реплик да и мы раскатку новой версии введём не сразу на все реплики а постепенно да то есть сначала она раскатывается на на один сервер проявляется как он работает потом на тестовую группу раскатывать со проявляется как она работает и после этого постепенно эта версия попадает на все реплики я насколько понимаю вас набокин декабрь и конечно сервисы джи би контейнером да нет я а что если можно это это java standard edition приложения которое работает вот как сервер bonnell спасибо большое-большое спасибо олегу очень интересный доклад спасибо за вопросы"
}