{
  "video_id": "x2XYOOiyhSc",
  "channel": "HighLoadChannel",
  "title": "Переизобретаем файловую систему: OpenZFS / Георгий Меликов (VK Cloud, VK)",
  "views": 748,
  "duration": 2627,
  "published": "2024-04-17T01:10:18-07:00",
  "text": "а со следующим докладом для нас выступит Георгий Меликов из VK клауда Георгий расскажет нам про файловую систему Open zfs Аплодисменты спикеру Всем привет давайте начинать сегодня мы поговорим про файловые системы и мы будем говорить про них не просто в каком-то контексте говорим мы говорить мы будем про такую не совсем дефолтную как Open zfs Я работаю ВК Cloud являюсь руководителем мы делаем не только сторожа но и не только решение но так как у нас вроде еще zfs нет то Здесь представлен в качестве контрибьютора проект в него с 2016 года все потому что я очень люблю свои данные прям очень ноутбуке только ZF Linux Сегодня мы говорить будем с вами не просто профайл из темы в контексте того Какие задачи наши сторожи должны решать Они конечно же должны как-то масштабироваться После этого мы должны как-то переживать отказы оборудования оборудование на самом деле даже если не отказало оно может очень сильно врать нам например на запись давай что-то не то после этого решив эти проблемы нам надо как-то жить с нормальной производительностью при этом еще функционал какой-то давать ну и в конечном итоге за любым проектом стоит Community Потому что если кот есть люди нет но он в общем-то очень быстро сдохнет сразу предупрежу что в конце будет ссылка на слайды если хотите можете сразу их получить начнём Мы немножко с истории очень долго на самом деле история ему уже больше 22 лет э-э пережил он за это время на самом деле немало и весьма сложно началось всё с компанией сан э в 2001 году в 2005 был успешно Извиняюсь за tfs был успешно реализм в качестве части операционной системы Open Solaris И после этого очень быстро появилось очень много орков не только в общем-то в линусе но и других операционных системах даже некоторых местах за счёт совместимых лицензий оно попало в общем-то в дефолты но к сожалению или к счастью в 2010 году сам Майкл Системс был поглощен компанией Oracle и может быть это и было хорошо с точки зрения развития бизнеса но с точки зрения развития Open Source проекта все было не очень хорошо потому что ZF закрыли как его пансолярис и Исходный код был в общем-то отворкан и он жил в виде отдельных орков на разных операционных системах после этого в 2013 году была создана отдельная организация чтобы эти форки как-то развивались всё-таки одну и ту же файловую систему а не различные фортики со своим функционалом но здесь мы прервёмся с историей и всё-таки продолжим Ближе к делу а наша система наш вопрос начинается с того а достаточно нам вообще одной файловой системы что строить какой-либо сторож потому что в общем-то он многокомпонентный и начнем мы с рассмотрения вопроса А какие типы сторожей бывают обычные То есть как в общем-то любые системы у нас есть два типа это локальная система у которого железо под собой всё это очень быстро и легко и распределённое да когда у нас система нарезана намного железок всё это ходит по сети и наши клиенты в общем-то тоже к нам ходят по сети это Обычно другие отдельные железки и отличие этих подходов два это задержки локальном оборудовании конечно же всё очень быстро но это всё очень быстро распределённых системах задержки у нас будет очень значительные потому что мы ходим по сети и эту стоимость очень высока но все меняется наоборот когда мы говорим про масштабируемость потому что локальную систему вы общем-то нет масштабируете да вы можете залить это железо деньгами купить какой-нибудь Мейн фрейм но к сожалению он один поток все равно не особо отмасштабируется это будет стоить вагоны это всё равно ограничено при этом распределенные решения они в общем-то при наличии нужной архитектуры весьма хорошо масштабируется может быть даже не ограничено в этом докладе мы будем говорить про локальное решение э Я думаю что их век никогда не закончится они всегда будут нужны мы будем сравнивать текст четвёртый XF с NTFS sts начнем с примера баз данных Наверняка у каждого из нас вроде есть какая-то очень важная база данных которая от которой зависит Наш бизнес и когда Наш бизнес растет наш баз данных тоже растет конечно же одного жесткого диска нам то или какого-либо сторожа не хватит по этой причине Мы в какой-то момент захотим подставить туда но все-таки больше чем один диск Да конечно же Но мы вспоминаем про ответственности то есть разработчики баз данных не хотят решать Итак большой Ну они решают и так большое количество проблем и брать ответственность за файловую систему либо сторожа это ещё больше нагрузка соответственно здесь обычно нужно применять какое-то решение которое позволит вам получить единый блочный э-э волям которые уже операционная система сможет использовать Обычно это так называемый еды какой-то Независимый объём объём диск набор дисков который объединены в один виртуальный Пул а это дает нам масштабирование Мы можем подставить еще Сколько нам нужно дисков и получить больше пространства но также мы должны решать задачу избыточности то есть мы не хотим потерять весь пул при вылете одного из дисков соответственно одним из первых одной из первых мыслей может быть что давайте просто купим нужную железку нам Да она решает эти задачи все вроде хорошо в общем-то Все Вопрос решен Ну к сожалению Как говорится холодный берда тут очень много нюансов это железка она стоит не в каждом ПК нашего мира она выпускается в ограниченном масштабе по этой причине и прошивка для неё пишется с учётом этого и в конечном итоге у вас есть проприетарная прошивка И к сожалению у неё очень много проблем бывает по этой причине у вас есть железка И вам надо ещё подобрать конкретную версию прошивки которая не будет иметь проблемы при работе дальше мы можем поговорить про производительность потому что мы не хотим чтобы эта железка стоила Как сервер или дороже соответственно оборудование в ней будет стоять Ну более дешевое какой-нибудь ARM или что что-либо еще мы можем это скомпенсировать наличием какой-нибудь батареечки которая позволит нам кашировать синхронную запись например Но самая большая беда это в общем-то наш худший случай это отказ и в случае отказа этой железки а железо когда-либо всё равно сдохнет по-другому просто не бывает в случае отказа вам придётся не просто найти какой-то аналог вам придётся найти именно Ту самую железку с той самой прошивкой и по-другому просто никак иначе ну вы купили другую железку Она имеет другие алгоритмы и соответственно вас просто массив не сойдёт она не знает как его собрать наравне с этим есть Другое Другая проблема что та самая батарейка которую мы вставляли для увеличения нашей производительности в случае ее отказа мы получим дикую дикую деградацию производительности это не простая батарейка это конкретная специфично это не пальчик батарейки по этой причине классическим решением в настоящее время всё-таки являются сотворной ряды и будем рассматривать этот вопрос на примере стека линуксового в нём есть подсистема mmd которую мы обычно конфигурируем через так называемый МД Адам утилиту Мы в общем-то этим наименованием мы будем оперировать дальше Вот это да там позволил нам собрать единый в Единый виртуальное устройство очень-очень много дисков Но конечно же мы не хотим это это единое устройство использовать потом на всё то есть мы хотим всё-таки как-то нарезать что отдельные файловая система для рута нашего Да в системе отдельное для Home directory отдельная для нашей базы данных непосредственно по этой причине у нас появляется логическое разделение таймов делаем мы это обычно через LM и вот только сейчас мы получили возможность создать файловую систему поверх этого сделайте мкФ как-то её настроить да потом проверить и так далее и основными до этого подхода в том что здесь уже в общем-то немало слоёв и они друг другу по факту ничего не знают они должны друг другу предоставлять похожие интерфейс в конечном итоге из лфм у вас Выходит то же самое блочное устройство но проблема усложняется и Давайте вспомним что мы все-таки хотим как-то это менеджер эти администрировать то есть Нам нужно настроить как-то сначала диски поработать с ними нам нужно потом из этого собрать Мда вам что-то похожее для наших нужд потом после этого нам нужно еще ворхом утилит ввме и нарезать это всё-таки аналогические воли мы потом у нас все равно будет на выходе блочного устройства в котором Нам тоже как диском Иногда надо управлять и только после этого мы сможем создать файловую систему на ней почекать и так далее а теперь Давайте сравним это с тем Э что нам даёт фс первое основное отличие дтф - Это не просто файловая система это и блочный и менеджер там в том числе То есть это единое решение которое позволяет делать вам Всё в рамках так называемого единого ZF сплайла и вот конечно же этот пул э точнее конечно же ZF сам э по себе биологически устроен из-под компонентов Например у нас есть отдельная компонент которая отвечает за э логическую нарезку на сущности то есть вот получение Тех самых файловых систем итоговых э в парадигме ZF с это называется до тестами Ну тут есть Очень интересно нюанс что на самом деле за тест это объектное хранилище и только уже паутих эти объекты мы начинаем оперировать этот сетами и собирать какие-то файловые системы Ну и у нас должна быть Как должен быть какой-то под компонент который будет отвечать за избыточность то есть мы хотим получить те самые аналоги зеркалирования пятого или более какие-то хитрые кейсы например дистрибьюторы говоря менеджменте сильно лучше вам нужно всего лишь две утилиты и то одна из них нужна очень редко задпул используется для работы с дисками То есть вы создаете пул добавить диск удалять диск и так далее Это дальше вам нужно управлять теми самыми детоссетами и это логическая сущность вы используете для этого команду zfs что-то настроить их создать удалить все есть еще на самом деле третья команда она называется zdb она используется для дебага Но если вы с ней знакомы на самом деле это будет хорошая тема поговорить в кулуарах потому что она не должна понадобиться никогда просто так и Давайте немножко остановимся на дата сетках поподробнее вся их фишка в том что по легкости использования и по легковесности создания их они аналогичные работе с обычной директорией вот как вы делаете м-кодирова вы также делаете zfs Create какой-то датасет и на выходе вы получаете на самом деле сущность которая является для операционной системы той самой файловой системы отдельной и всё это даёт вам возможность настраивать эту файловую систему конкретные датасет очень гибко Например у вас есть данные вашей базы данных вы э это обычно дикий Рандом с маленьким блоком То есть вы выставляете блок поменьше Используйте какой-то компрессию полегче например лз-4 и в общем Всё хорошо дальше у вас у большинства баз данных есть валлоги в райхеологии они пишутся последовательно соответственно их более эффективно писать с большим размером блоком это позволит нам их сжимать эффективнее как-то оперировать метода данных будет в конечном итоге меньше или и поставить например какой-то более эффективное компрессия она будет помедленнее но посильнее сожмём Зато Никто вам не мешает в общем-то на этот же пол писать файлы и для них мы можем э сделать ещё больше размер блока сделать какие-то более хитрые настройки по типу отключить от тайм либо вообще в общем-то если нам не нужно мы можем отключить э ну включить игнорирование синхронные записи то есть мы в худшем случае потеряем последние x/s записи Зато синхронная записи зато это будет работать сильно быстрее чем обычно Ну теперь мы в общем-то затронули вопрос отказов Давайте про это и поговорим и в классическом варианте наш дефолтный случай это не просто штатная работа нашего массива наш дефолтный случай должен быть это работа при деградации Например у нас ушел диск и в этой ситуации классические решения со вторник они как будут работать вот например у вас вылетел 20 ТБ жёсткий диск вы его вытащили Встали Встали на него место новый вспоминаем что это отдельные слои Да ничего не знает про данные Соответственно что будет все 20 ТБ вам нужно полностью синхронизировать и на время этого ребилда массива пока вы синхронизировать все эти 20 ТБ например А у вас получается деградация ваша э вашего перфоманса на ту же самую базу данных если она поверх вас работает да на этой же самое время вы получаете увеличенный риск отказа всего массива потому что одновременно может вылететь то Ну ещё один диск И всё в общем-то данные его не соберёте а теперь посмотрим на ZF и вспоминаем про его основное отличие то что он всё знает про данные соответственно мы знаем что конкретно лежало на этих двадцати ТБ если он был вообще пустой то нам ничего себе синхронизировать не надо если там лежал отрабатывать их в 20 раз уменьшим время деградации производителей и 20 раз низкие снимем время э не простое В общем снизим риски того что весь пул развалится и залитый ещё одного диска Ну давайте посмотрим еще вглубь в классическом варианте мы можем использовать так называемые зеркалирование данных То есть у нас есть два или более диска и что мы делаем на них просто дублируем данные да то есть вроде все все копия есть на разных местах один диск вылетел но и Бог с ним У нас есть ещё одна копия Да но на самом деле это тоже не самый худший самый худший случай - это когда у нас произошло Привет ещё потере питания ребят в чём что э мы параллельно записать ровно те же самые данные на два диска не можем то есть в любом случае где-то будет какой-то рейс то есть эта операция по факту не атомарно А что томарностью здесь я подразумеваю да это операция которая либо прошла успешно либо не прошла вообще и у вас нету промежуточного состояния в принципе то есть вы хотите записать там не знаю мегабайт на диск И к сожалению у большинства дисков размер сектора 4 КБ у вас этот мегабайт никогда не записывается целостной то есть у вас будет промежуточное состояние вот всё-таки записали и вот у вас получилось где-то тыква в общем-то из этих 500 КБ когда я говорю про данные Я говорю и про метаданные в том числе а не только про данные непосредственно файлов потому что нам нужно хранить информацию А где же этот файл у нас на файловой системе лежит да то есть это путь до него нам нужно хранить информацию какой у него размер какие даты создания там изменения Какие в конечном итоге у него права потому что мы работаем в рамках файл операционных систем э Где у нас очень много пользователей Мы хотим честную изоляцию Да ну и в конечном итоге мы должны хранить А где же у нас данные на диске лежат потому что надеются на то что они будут всегда расположены последовательно Да нет это мир Мы хотим что чтобы Наш бизнес рос соответственно и базы данных наши Вырастит соответственно фрагментация Для нас это дефолтный случай просто другого не будет и нам нужно решать вопрос обеспечения целостности этих методах классический вариант который часто используется это использование журналирования либо тех же самых валогов то есть мы последовательно пишем наши изменения когда у нас этот кусок кончается он просто Начинаем писать сначала соответственно у нас всегда есть предыдущее состояние какое-то более целое и мы можем на него откатиться либо применить конечном итоге но этого подхода есть основной минус то что по факту всё что мы журналируем мы это дублируем и в общем-то это очень неприятно дополнительно к этому у вас может произойти очень интересная ситуация мы вспоминаем что наш худший случай это на самом деле потери питания и при потере питания что мы делаем после этого Вот наш сот начал работать обратно Да вот наш хост нашей основной базой данных начал загружаться вот мы начинаем применять эту этот журнал на файловую систему параллельно мы должны на самом деле провести так называемую операцию файл систем чека и эта операция в случае большого объёма данных может затянуться на самом деле на часы вопрос администраторов который отключают автоматические файл систем человека при загрузке это отдельный вопрос здесь обсуждать не будем Давайте посмотрим теперь на ZF ztfs журнал для целостности вообще не нужен ZF используется более хитрый подход он называется копию то есть сокращённо и в таком случае у нас всегда есть какая-то иерархия метаданных и в конечном итоге снизу блоки данных Наших мы на них ссылаемся и особенность копим райтов в переводе в общем-то копирование при записи в том что мы никогда не перезапишем данные поверх мы всегда будем записывать их в новое место если мы изменили один байт Да мы скопируем предыдущий блок и запишем его в новое место и вот в настоящий момент вот у нас какие-то блоки где-то лежат Мы про них ещё ничего не знаем у нас эта дата нету дальше мы в общем-то строим всю иерархию метода даты То есть мы какие-то ноды метаданных э-э со шлёмся на данный и потом получим какой-то так называемый уберлок в терминологии zfs это самая главный блок который ссылается в общем-то на состояние файловой системы Вот она вот здесь и на самом деле на этом слайде мы видим уже два состояния файловой системы у нас есть предыдущие берблок который представляет берлог один и следующий уберлок два то есть в такой ситуации когда мы уберлок два успешно записали А даже если точнее ситуация наоборот мы успешно ещё не дописали А мы всегда можем откатиться на предыдущее состояние и таким образом и в общем-то обеспечиваем здесь абсолютно атамарность То есть у нас либо берлог второй записался либо нет независимо от того сколько у нас данных под ним было вообще всё это происходит абсолютно один раз и без записи как я уже сказал тут есть ещё очень большой плюс Хмм все старые данные в какой-то момент в будущем Мы сможем перезатереть когда мы решим этим нужен посчитаем это нужным Ну от этого мы можем получить здесь абсолютно бесплатный снапшоты которые здесь будут являться просто банально ссылкой на предыдущее состояние то есть вот эти данные мне пожалуйста не трогай они мне нужны в будущем все это вся стоимость снапшотов как я упоминал ранее кейс С отказом диска это на самом деле ещё относительно простой случай наиболее неприятный случай когда ваше оборудование начинает врать и не говоря мы вам об этом то есть вы например Пишите а получаете на стене что-то другое и тут давайте рассмотрим более интересный кейс э-э есть более эффективный вариант по полезному пространству записи избыточности это так называемый Рэй пятый его аналоги то есть кейс Когда у нас на разных дисках есть уникальные данные она примерно на третьем как на картинке лежит э лежит так называемая данные чётности это данные какой-то операции над данными вот ну вы можете видеть в общем-то что картинка Три она очень похожа на Ольга картинки и в общем-то на самом деле она является честной операцией Sorry над этими картинками то есть технический при отказе одного из дисков на самом деле можете с помощью диска два диска три получить содержимое дико1 но тут есть очень неприятная ситуация потери питания про которую мы говорили ранее и при потере питания у нас происходит ситуация что данный отчёт и основные данные могут отличаться соответственно в худшем случае Вы вместо данных с первого диска можете получить прямую фотографию ну всякое бывает да и у этой проблемы есть целая отдельное название оно называется Rate Right Hall К сожалению большинство решений рейдов даже софтварных подвержены этой проблеме Также нельзя забывать про наших любимых друзей частицы из космоса которые могут прилететь Очень удачно в конкретную ячейку памяти и произвести вам в ней так называемый то есть инвертирование значения вашего Бита и в общем-то Вот вы два реальных примера вы проходите спидран какой-то игры вы подпрыгнули не на метр на 10 всё у вас никто никогда не обгонит Да и второй кейс более худший реалистичный проходит выборы bitflip изменение чисел и плюс миллион голосов Да ну это Никуда не годится и в классическом ну эти с этим тоже Конечно же можно бороться но если мы будем говорить про классическое и не к своей где у нас всё есть слои мы добавляем ещё один слой который начинает добавлять куда-то дальше свои Ну примерно считает их отданных да но он должен обеспечивать те же самые гарантии что и все остальные То есть вы хотите омарность вы хотите синхронность что-то ещё это вот он должен это поддерживать а теперь посмотрим на ztfs Да ztfs тоже считает хэш суммы в общем-то Тут ничего удивительного потому что это один из наиболее простых вариантов посчитать проверить целостность Да мы просто посчитаем обратно но дальше идет хитрая штука мы считаем хешей и в конечном итоге у нас есть самые главный Хеш который пишется рядом с суперблоком который в общем-то и представляет собой состояние всего Пула то есть мы имеем одну строчку всегда знаем состояние вообще всего в системе и при наличии той самой избыточности о которой мы говорили ранее у нас получается ситуация когда мы всегда знаем какая из копии данных верна Всё ли хорошо и там я не знаю Если у вас что-то пошло с данными не так мы можем провести так называемые салфилинг то есть мы сами полечим данные То есть вы как администратор даже не узнаете об этом там в 3:00 ночи Вы просто потом статистики узнаете что да что-то пошло не так но мы всё восстановили всё хорошо Никто вам не мешает сделать это автоматически ой вручную э с помощью краба он в общем-то вычислит все данные проверить его хэш сумму и опять же что-то будет не так Да это я поспорил сорян ну всё-таки попробуем а кто знает про мир 3 взяли поднимите руку пожалуйста кто слышал про такую штуку как Меркель 3 вот это вот дерево оно на самом деле называется Меркель 3 не слышали А Кто пользовался китом отлично спойлер никто не увидел Значит да Круто Очень смотрите Меркель 3 на самом деле используется и в гите То есть вы все пользовались Меркель 3 и оно вообще Вокруг нас то есть хэш в гите Ну хэш любого коммита в гите он на самом деле является подобием Меркель 3 он всегда использует предыдущий хэш предыдущего кометы для своего вычисления И тут я считаю что за tfs - это вот хороший пример файлы - это гид в мире файловых систем также на самом деле такой же подход использует часто в блокчейнах если кто-то слышал Вот и здесь на самом деле аналогии не заканчиваются первое интересная аналогия то что есть аналог каких-то То есть это он называется транзакционная группа это тех же сокращённо и что мы в нём можем сделать Мы также как Где те группируем какие-то записи в одной единые То есть это тамарная запись у нас получается технически это тоже транзакция то есть мы также можем сделать образной реверт как И те и да нам опять же не нужно журнал Где в общем-то тоже его нету на этом налоги не заканчиваются У нас есть аналог то есть операции которые вы можете дополнительно добавить при записи То есть например Вы можете сжать данные в общем такие за вас это сделает Как из ffs тоже может вы можете посчитать контрольную сумму вы можете сделать шифрование нативное данных Вы можете для дуплицировать их да на самом деле что вы захотите тут лэлкам это открытый продукт Тут есть один большой плюс в данной операции что мы данные группируем по сути у нас происходит батчинг записи то есть мы любой рандомную запись Превращаем в последовательно мы просто неважно что где изменяли мы их последовательно диск сбросим но в этом есть и минус при создании последовательной записи мы на самом деле чтение Превращаем всегда рандомное то есть Любое любое чтение в ztfs оно примерно рандомное к сожалению и здесь в общем-то давайте рассмотрим вопрос А как же нам выдерживать При всём этом функционале производительность нам нужно как-то её всё-таки поддерживать нужном уровне Да копия он райт - это дорого это весьма Весьма дорого у нас появляется очень много метода данных Но с ними можно бороться во-первых в отличие от других файловых систем в которых размер блока 4 или 8 КБ то есть весьма маленький у нас размер блока дефолтный целых 128 КБ и с одной стороны это может показаться весьма большим но по задержкам на самом деле это не так плохо Плюс вы можете сэкономить на размере методы данных в конечном итоге Вы можете эффективнее сжимать эти данные Ну и сами методы данные при этом очень жгутся они потрясающие жмутся А по дефолту в zfs они жмутся с помощью Z4 Также нельзя забывать про проблему с фрагментацией в классических файловых системах мы решаем это двумя путями либо при локации данных например мы выделили заранее вот Что вот этот гигабайт лежит именно там и перезаписываем их но это те самые проблемы с атомарностью и так далее плюс мы же опять же растем у нас базы данных увеличивается соответственно этот подход он не будет работать вечно Ну и подход который например доступен microses Windows mtfs Да мы можем дефрагментировать данные но проблема с дефрагментацией следующая что если у вас есть фоновый процесс на который вы завязаны по производительности то когда ты этот фоновый процесс он может быть очень больно выстрелить и ваше дефрагментация просто никогда не закончится вот и всё и тут не только ZF но и другие ну большинство других файловых систем в том же линуксе они не занимаются дефрагментацией то есть фрагментация это Это жизнь да мы можем её как-то обходить но всё равно от этого мы никуда не денемся и в случае с все у нас у нас всегда все плохо у нас копия он райт и по-другому мы не работаем принципиально но честно говоря Зебра должна быть полосатой на тонна Зебра потому что наш худший случай это наш дефолтный случай и в таком случае мы всегда учитываем наши проблемы на запись на самом деле мы уже поговорили что мы можем оптимизироваться мы группируем рамках транзакционной группы весь Рандом может происходить только в оперативной памяти но самая большая проблема здесь возникает на стене как я уже говорил ранее то есть Нам нужно как-то решать вопрос фрагмента э рандомного чтения И не только в ztfs Но вообще во всех файловых системах и операционных системах У нас есть ещё один слой и Memory cash через который у нас пройдёт вообще все чтение Да это еще один слой в линуксе у нас используется так называемый page cash он использует лист рисунки лист подход То есть у нас есть списочек во главе которого наиболее последнее данные по доступу но проблема в том что когда вы например будете делать бэкап а он будет размером например больше чем ваш кэш вы эти данные в общем-то искажа можете эффективно вымыть А у вас была Рабочая база вы хотели горячие данные вроде они были горячими и с этой проблемой надо Бороться и с ней можно бороться но принципиальная проблема рукаша она существует всё равно в ztfs проблема рандомная чтение всегда есть по этой причине нам нужно что-то более хитрое и на самом деле вот FS своя реализация кш и там есть не только регистрируют подход там ещё используется и мост freekentle подход это отдельный списочек которые мы сортируем по частоте то есть самые часто используемые данные у нас всегда будут самыми главными самыми горячими и можете мне не верить здесь эта статистика с моего ноутбука за несколько недель который в общем-то показывает что эффективность мост freekentleys кэша сильно-сильно выше нежели мост лист больше 94 процентов попаданий в кэш были мост фриквант лист кэш то есть да Он может быть реализация zfs Ну конечно может быть в какой-то мере медленнее но за счёт большего количества попаданий мы всё равно здесь выигрываем также часто когда говорят про растения забывают вопрос префетинга данных То есть как мы обычно работаем Да с данными мы прочитали какой-то бантик потом прочитали следующее и так далее вроде чтение последовательно И вообще о чём разговор но к сожалению любая дополнительная операция доступа на самом деле для нас э что-то стоит например те же самые силы и чтобы каждый не реализовывал какую-то предвыборку данных большинство операционных систем как ZF да Ну и файлы как система делает так называемые префечь данных то есть они за вас заранее выгружайте это эти данные в тот самый конечно и для ДТП очень важно но ну просто статистика если бы этого работать вы работаете в Ровно в 1.000 раз медленнее чем по килобайта также zfs позволяет выгружать ваш кэш не и на nvme диски То есть у вас кончились слоты оперативной памяти Окей доставляйте на вымешка можете пользоваться ими плюс если ваши данные сжимаемые Никто вам не мешает из это поддерживает сжимать их и взушке то есть они X3 сжимаются отлично У вас виртуальный X3 памяти на кэш и это прекрасно тут вы можете заметить что погоди оперативной память и зажать Ну как это вообще совместимо это по идее Ну одно быстрое другое медленное и тут нам прямо помощь приходит очень эффективно и быстрые алгоритмы сжатия современные мы рассмотрим лс4 к примеру он на один поток на э сжатие работает на скорости больше 800 мб/с в среднем А на нажатие больше чем 4.5 Гб в секунду это всего лишь несколько раз медленнее чем базовая операция мцпи Да вы можете использовать э ЗСД то уже автора Он будет более медленный Но более эффективный тут есть ещё один интересный кейс Хорошо если наши данные нажимаем OK мы заплатим эту стоимость это будет выгодно А если она не сжимаемая что нам прыгать постоянно переключать Включить выключить И тут я спешу вас обрадовать за 4 содержит эвристику на этот случай он будет такие данные просто пропускать соответственно стоимость lz4 а она примерно равнам себя здесь это очень классно даст тоже пытается здесь оптимизироваться но не настолько выигрышно Так что тут я немножко Соболезную автором Ну не автором точнее пользователем вот РФС они внедрились за std И посчитали что этого хватит То есть как дефолт за std он всё-таки медленнее а-а Мы конечно же можем реализовать это в обычном стеке Но это будет Ещё один тот же самый слой и честно говоря когда я считаю здесь количество слоёв И что мне надо будет сделать чтобы это администрировать либо просто себя поднять локально тут уже очень много получается действий Да и плюс на самом деле это потеря производительности в конечном итоге потому что у вас очень много чего повторяется мы можем говорить еще очень долго про функционал но к сожалению время презентации конечно же ограничено так что тут Давайте немножко Вернемся от дел к истории остановились мы на том что люди это наше всё да я остаюсь мы на том что у нас есть организация и его послушном мире побеждает более активное комьюнити и так вышло что этот оппонент zfs он преобразовался в zfs on Linux и мы объединились под одной единой кодовой базой ztfs он Винкс в итоге форка вплоть до того что мы из этой же кодовой базы сейчас можем собирать фрибсдэшный ветку zfs и мы получаем абсолютно тот же функционал уже на двух операционных системах и это очень классно на самом деле на очереди и другие форки они тоже ждут в лите в Стрим То есть это очень просто платформенные файлы получается с эстетичным функционалом но организация Open ZF в первую очередь создавалась для объединения разных форков И для этого ну у нас есть проблема у нас есть э ZF с какой-то версии да Э мы там можем конечно добавлять функционал множить эту циферку там не пять а шесть например стало Да но когда у нас очень много орков Мы хотим как-то поддерживать их не ну жить одной файловой системы поэтому причине был придуман и были придуманы так называемые фичи флаги а они позволяют вот в разработали какой-то фишку везде на весь вечер флаг если он удерживается в другом форке вы просто создаёте пол без этого флага и вы совместимости абсолютно То есть это очень классная кость платформенность получается также Да к сожалению в линуксе оно не из коробки потому что у нас не совсем совместимые лицензия с ядром но честно говоря я считаю что это может быть даже Плюс потому что мы не завязаны на цикл разработки люксового ядра мы можем жить отдельно и решать наши проблемы и придумать что-то новое плюс тут есть очень интересные сайд эффекты например мы можем поддерживать весьма старые ядра то есть Ну вам не нужно иметь бензина конечно ядро и как-то здесь гнаться за какую-то за каким-то функционалом вы просто загружаете нужные модуль Да и получаете весь Новый функционал она уже работает Ну и на самом деле то что она не в дефолте не значит что оно далеко в большинстве дистрибутивов Вы можете Просто набрать стало или аналог и в общем-то дтф у вас будет плюс во многих дистрибутивах оно уже позволяет э ставит на ротовое раздел Ну и в том же FreeBSD - это одна из дефолтных файловых систем тут есть ещё интересная фишечка что есть уже форка для Windows И когда-то я очень мечтаю что если мне понадобится Microsoft Windows я поставлю его именно на ZF вот начинал я прокомментировать и прокомментировать хочу как раз поговорить на самом деле тут есть не только компании которые те бизнес это ZF здесь есть просто инициативные люди И эти люди на самом деле состоят не только в этом комьюнити там вы можете встретить этих же людей там и в линуксовом комьюнити и в системе Да и так далее да Эта операционная система она написана на фишечке Нам нужен перформанс Да мы модулей драфт всё-таки Но на самом деле вокруг э вокруг сеточного ядра всегда есть очень много обвязки те же самые тесты они написаны на чём-то другом какие-либо фреймворки утилиты в конечном итоге есть документация которая конечно же не содержит в себе фишечки И тут я как ментейнер документации с удовольствием вам по сапорчу Если вы хотите как-то в этом поучаствовать это тоже очень интересно надеюсь мне удалось показать вам что можно делать на недефолтных вещах очень много интересных вещей и при этом это получится сильно проще Да к сожалению серебряные пули нет но это в общем-то интересный экспериментировать пожалуйста экспериментируйте на этом всё буду рад вашим отзывам пожалуйста оставляйте их связаться со мной всегда можете по телеге также приложу наши камни Юнити это не только zfs но ещё общее SDS начать такие плюс и Ну я всегда буду рад ответить на ваши вопросы здесь и в конце мы можем ещё обсудить тоже я ряд вопросов оставил Да георги спасибо большое за доклад а потрясающая низкоуровневая подробности И мемчики вообще все в тему А у нас вопросы Из зала вот здесь вот человек поднимает руку Всегда пожалуйста микрофон ссылка на слайде там кстати Извините Спасибо большое за доклад давно пользуюсь ZF И на работе и дома и у меня соответственно вопрос Почему вы ваши редиректу нравится это же совершенно разные механизмы отлично знаете что сделал У меня еще три слайда про это сейчас Как вы считаете что это одно и то же давайте так Это кто вообще понимает этот слайд поднимите то не понимает их будет Вот отлично большая часть зала не понимает Фишка в чем сейчас когда мы говорим про копию Мы обычно на самом деле имеем в виду редиректор разница на самом деле технически в компьютер ставится Вы правы она есть в том плане что копия он райт он в другой момент должен как бы копировать но Фишка в том что сейчас везде где встречаете ко мне он райт имеется в виду именно редирект вы говорите там я не знаю у виртуальной памяти в линуксе нравится это на самом деле Директ он райт и так далее и ещё есть один забавный слайд Если вы поистите директором рейд на Википедии его несколько лет назад выпилили То есть вы не встретите уже упоминание редирект он райт и везде где он имеется ввиду это будет копен райт я не согласен что везде потому что очень много работал именно с железными схd и у разных вендоров соответственно у одних копия на эту других редирект он райт они работают очевидно по определению и Это явно видно на нагрузку тестах то есть не согласен что Смотрите тут я просто в общепринятой терминологии То есть когда я вижу копию он райт с хд там тоже стоит уточнять А это точно копия но это не редирект нравится И вот это Путаница она есть к сожалению и вот вот интересный слайд который показывает что когда мы говорим вот на комьюнити общее Да не только Сторожевая то обычно это примерно одно и то же спасибо микрофон вас тоже Вижу ваш следующий будет Давайте Да спасибо за очень интересный доклад интересно Меня интересует следующее Можно ли свои алгоритмы сжатия подкидывать в ztfs да то есть прям есть как бы готовая опишка которая его можно посмотреть я вам так скажу вызов там интерфейс очень просто там буквально вызов то что сожми мне это разожми мне это я обвязка с точки зрения интеграции то что эти варианты есть то есть вы просто берете например тот же патч на за СТД Он внедрялся всего лишь пару лет назад по-моему и просто по нему по шаблону прямо внедряете спасибо если вы имели в виду что давайте просто там не знаю какую-нибудь книжку там в неё выгрузим поток и потом из них получим Но это менее производительность нам нужно модулей дрожить не я имел в виду как ну как просто поддержку Да как модуль была рука у вас тоже вижу там коллега был первый следующий ваш тогда меня вопрос я же правильно понял С него С базе много однотипных данных то после запуска после завершения дедупликации производительность должна ощутима увеличится смотрите там звездочка дупликация была не просто так чтобы рекомендую сразу что вам имеет смысл использовать компрессию для этого То есть те же самые солнечные базы они очень классно жмутся то есть там если посмотреть э как ребята жмут тот же там позже и так далее они сразу говорят то что у нас от x-3 до X100 бывает просто нажатие э-э с точки зрения этой дупликации Прямо сейчас Да оно может вам помочь если у вас прямо идентичные блоки и вы попали Да по смещениям в блоке но прямо сейчас реализация во-первых нюанс что деду публикация синхронная сейчас то есть мы прямо в процессе записи должны проверить что у нас такие блоки есть и так далее и по реализации это может деградировать если у вас в оперативной памяти таблицы публикации не помещаются больше то есть оно будет работать пока у вас есть память ну вы знаете все следующие эффекты дальше Оно просто деградирует по перформансу потому что синхронная дедупликация и реализация без логирования без использования каких-либо логов для этого может быть вопрос покажется странным мы можем снапшоты использовать качестве бэкапов является ли snapshot Backup А кто считает что является snapshot Backup поднимите руку они являются Ну очень много да Круто блин Хороший вопрос в том где этот снапшот находится если он находится на том же хранилке и он ссылается на те же блоки Конечно вот идея в том что мы делаем снапшоты может сильно помочь у него есть Вы естественно снапшоты можете найти FS выгружать на какой-то другой ход там же делать то же самое с данными То есть вы можете их проверить на скраб да то есть что они целые а потом самая магия начинается потом на самом деле вот Ну первое инициализирующий поток он такой немножко скучный он идентичен а дальше вам нужно экспериментаторы когда только изменений здесь так как мы работаем только с объектами мы можем им комментарии сделать очень дёшево То есть у вас там 100 ГБ набежал Вы только эти 100 Гб в общем-то и прочитаете как есть вам не нужно перебирать разницу вообще совсем это очень классно работает да спасибо Вот здесь Всем добрый день спасибо за доклад У меня на самом деле несколько вопросов вопрос первый Почему если система такая магическая в пройдене где не используется вопрос классный на самом деле если бы я не сказал наверное спросили А где фронте есть да Смотрите тут есть интересный момент то что никто не мешает на самом деле Вот почему база данных Не используют чем больше больше чем один диск И почему они используют файловые системы например да Можно я продолжу хорошо Не потому что ли во-первых zfs очень требователен к ресурсам а к примеру очень прожорлив до оперативной памяти у него есть проблема ресиверинга при замене дисков либо добавлений новых и третье при снижении доступного Места там 85% загрузки и выше начинается падение производительности Нет не по этим причинам не по этим поясните пожалуйста про причины мы можем в кулуарах поговорить что с многие там мы можем ссылаться на конкретные Баги и так далее да и вообще там немножко холивар начнется по поводу прожорливости вот по поводу основных причин вот опять же на примере баз данных и почему они там не в себя не встраивают файловую систему там начинается очень много вопросов вопрос удобства вопрос На каком уровне тебе нужен функционал Да мы его как Клауд мы соответственно инфраструктурный провайдер да и у нас многие вещи во-первых они будут дублироваться в самой виртуалке машины Да во-вторых Нам нужно решать очень много других вопросов миграция данных она должна быть Реал таймовая Да соответственно вот там с бэкапами тоже Мы обычно отдельным слоем их делаем так далее То есть в облаках у ZF есть на самом деле кейсы использования один из интересных которые кстати вот может быть и у нас тоже появится когда-то Надеюсь э недавно амазон его имплементировал для так называемых файловых шар То есть когда у вас есть центральное место как раз локальная система которая вам позволяет там получить там образом миллионов там и офис как они заявляют по крайней мере коллеги а они недавно делали презентацию на эту тему Кстати на ФССП Вот то есть применение на самом деле есть просто вот кейсы опять же то есть мы на уровень для нас вот вчера был классный доклад на самом деле это только от Мистер Кирсанова Он рассказал про то что мы строим велосипеды у нас Монолит и так далее на объемах часто нужно строить что-то чуть-чуть другое потому что тебе проще реализовать что-то либо с нуля либо более там точечное сделать И тут По мне так Такая же причина То есть когда у тебя определённые масштабы тебе там не нужен много да за tfs очень классными местами подходит на местах где тебе нужно огромная стороже и так далее это не значит что мы их не будем использовать да Опять же но там есть какие-то советы эффекты и вот Ну как с этим жить надо при выборе Да хорошо спасибо так смотри у нас есть ещё вопросы с чата Я хотел бы его зачитать а Михаил жучков спрашивает Судя по слайду ZF с э в Линукс работает мимо механизма VPS Если это так то будет ли она быстрее работать в psd Нет он в любом случае через effs должен работать там есть Хаки чтобы обходить поискать на самом деле Чтобы не дублировать два раза Да и не э квалифицировать память но в конечном итоге у Linux Ну у него всё через effs проходит соответственно Мы тоже через него работает с точки зрения бсд или Линукс я честно говоря не видел в сравнении где э прямо кто-то из них сильно вырывается потому что в конечном итоге прямо сейчас это одна и та же кодовая база вообще одна и та же то есть собирается и всё то же самое Окей хорошо спасибо если он еще вопросы Из зала так вопрос задала видимо больше нет Если у кого-то какие-то а там еще Вижу вижу Извините Спасибо за доклад можете сказать ноты в zfs это прям то же самое что в ext и наверное вопрос про динамическое или выделение Да динамическая Это как в Axe то есть они лимит zfs наиболее большой из популярных файловых систем вы его не считая никогда не достигните даже в XF если он сильно меньше именно по структуре это напоминает динамический это больше X Да вы в состоянии статические они динамические Спасибо тут извиняюсь что дополню может по ну наоборот интересная штука расскажу Вы когда например то же самое цитируйте Вот вы создаете МДМ массив он на самом деле у него есть режим там Линии его ленивый инициализации Когда вы заверяете его что там точно Ну либо Поверь мне да А вот это всё наоборот он когда создаётся он по умолчанию что Ну у него нет но что-то ссылается Да он у него нет ссылок у него всё пустое то есть у него неважно что под ним здесь Да и то же самое при локации Вот и ноты он тоже динамические и тут соответственно абсолютно всё динамическое То есть у нас нет пределах акции в большинстве мест Понятно спасибо еще кто-нибудь так Ну смотри если на этом все нам нужно разыграть два приза среди вопросов первый приз будет от онтика а второй приз это будет настольная игра разработанная коллегами из VK клауда по мотивам архитектурной секции интервью кому что так онтика вот кто там про Почему не у нас используется да да вот тому колени вопрос про редиректор мы делаем разработали очень интересную архитектурную игру вот она вам пойдет да да настольную игру пожалуйста вот сюда и тебе также при соточка Аплодисменты спикеры Ещё раз спасибо"
}