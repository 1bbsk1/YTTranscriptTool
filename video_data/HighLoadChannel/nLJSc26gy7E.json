{
  "video_id": "nLJSc26gy7E",
  "channel": "HighLoadChannel",
  "title": "Как забыть про проблемы с производительностью? Tarantool в качестве СУБД / Александр Харченко",
  "views": 1462,
  "duration": 2306,
  "published": "2023-04-28T06:10:46-07:00",
  "text": "Добрый день дамы и господа Меня зовут Харченко Александр Я занимаюсь разработкой уже 20 лет И сегодня я расскажу как мы решали и решили проблему с производительностью в самой нагруженной части нашего проекта в каталоге товаров в Ситилинк немного о нас Mary Tail начали в 2008 году начались продавать и компьютерные электроники потом начали продавать все что покупают наши покупатели Разумеется мы росли в 2008 году у нас было Legacy нас было пять человек потом Мы перешли на симфонии в 2019 году мы начали переходить на микросервисы пишем их на гол в качестве транспорта для общения используем gpc и В качестве базы данных мы используем Тарантул Также сейчас добавляем разбираться транспорт Кафка что мы сейчас делаем нашей главной задачей сейчас распилить наши монолитные приложения на и замороженное мы уже написали 130 микросервисов все они на голэнг а базой данных для этих микросервисов является Тарантул этих баз данных уже 80 или больше Какая задача перед нами стоит и стояла во-первых номенклатура из двух миллионов товаров из которых в продаже находится 150 тысяч Почему я их выделяю потому что 2 миллиона товаров номенклатуры они просто существуют А 150 тысяч товаров продаются и по ним постоянно меняются цены и остатки остатки распределены по 800 складам и Вполне может быть такая ситуация когда продажи одного чайника изменит 200 срок остатков а продаем много для того чтобы обеспечить нашу систему данными нам нужно обслужить 40 обслужить чтение 40 тысяч точности в секунду это меньше чем это больше чем РПС Но это количество сущностей и то что стало для нас удивлением в Пике может прийти до 50 тысяч изменений в секунду Ну и нагрузка постоянно растет на 30-50 процентов В год это были условия Какие требования нам нужно уложиться во-первых это 30 миллисекунд на ответ А во-вторых нельзя кэшировать Почему нельзя акашировать нельзя Не просто так написано в кавычках потому что кэшировать конечно можно и конечно мы это делали в начале мы сделали простой кэш который инвалидирует данные по ttl и все собственно говоря было хорошо Пока в один определенный день не случилось аварии монкаш не перезагрузился на источник данных опросилась обрушилась нагрузка он не выдержал мы начали думать что с этим сделать добавили персистентное хранилище которое не боится перезапуска все собственно говоря хорошо работало но мы росли и мы перегрузили эту сетку которая у нас была на тот момент мы начали думать над решением как не попадать больше в такую ситуацию и сделали хранилище ВПЦ самых востребованных данных с этим собственно говоря все было хорошо но напомню Это было был кэштин валидации по ttl и в определенный момент мы столкнулись с ситуацией что происходит слишком большое обращение к источнику данных а слишком большое количество обращений происходит потому что инвалидация произошла в один момент а чтение в этот момент осуществляло множество участков кода мы придумали чтобы решить эту проблему блокировку с помощью распределенного ютекса то есть сделали реализацию такой что одновременно обновляет только один источник то есть один обновляет А все остальные ждут собственно говоря все это отлично работало но в определенный момент мы опять же уперлись то что мы росли и источник данных опять перестал справляться с задачей потому что инвалидация происходит в один и тот же момент Мы решили эту проблему и перешли к версионированному крышу то есть в источнике данных есть некое число которое описывает состояние коллекции данных И в хранилищем тоже есть Ну или президентом хранилище есть число которое описывает состояние коллекции который находится в этом каше то есть мы инвалидируемся уже только тогда когда данные на самом деле стали не валидными это решение тоже работало но оно опять же уперлась проблема роста так как мы так как мы начали обновлять данные в слишком большом количестве потому что количество запросов увеличилось Мы решили решить проблему кардинально То есть то что собственно и было проблемой то что источник данных не справляется с нагрузкой которая на нем находится то есть мы ограничили количество запросов которые отправляем в источник начали обновлять коллекции С задержкой но зато не перегружать источник это решение в итоге сработало Но что же у нас получилось во-первых Все работает но все работает Не так быстро как хочет бизнес данные не успевают обновляться когда бизнес что-то не нравится он звонит в поддержку и как следствие поддержка начинает очищать кэш вручную Разумеется по частям не сваливая систему но это не нравится разработке также остается проблема в том что если все-таки полностью очистить хранилище источник данных не справится то есть в случае какой-то аварии с потерей данных при системном хранилище нам придется поднимать разогревать кэш частями и одно из главных проблем Это отличие цен в разных частях сайта Потому что если например название отличается в карточке товара и В корзине пользователя это не будет никоим образом волновать Но если будет отличаться цена то пользователь будет считать что его где-то обманывают это будет создавать для нас тикеты которые мы должны будем сразу же решать решение было достаточно сложным И поддерживать его было тоже очень сложно и ключевой проблемы было в том что мы переходили на микросервисы и про проблематику кошак микросервисах я скажу немного позже В итоге Почему нам не подошел кэш он периодически все равно создавал высокую нагрузку нагрузку на источник данных а это привозил приводило к даунтайму а решение было очень сложным То есть если брать изначальную версию с кэшированием по ttl и финальную версию которая Имела в районе 36 реализаций под разные виды хранилище под разные стратегии инвалидации это было сложно поддерживать мы начали уже К тому моменту использовать микросервисы А если в монолитном приложении мы можем использовать один и тот же кэш разных участках то Каждый микросервис должен владеть полностью своим грешом то есть мы никак уже обойти дизайн не можем справиться с проблемой разного каша в разных микросервисах И также у каждого микросервиса кэш свой что с ней снижает показатель хитмиз как результат мы не смогли выполнить требования бизнеса с помощью кэша но очень пытались посмотрев на решение которое у нас есть мы поняли что мы создали по факту базу данных вокруг кэша и логичным было бы заменить это решение на настоящую базу данных мы начали с того что нужно было выбрать базу данных который справится с этими задачами и начали формулировать требования во-первых Это должно быть мемори решение потому что с такой нагрузкой справится только in Memory в идеальном мире нет аварий Мы живем в реальном мире аварии бывают и чтобы они не были проблемы мы должны рассчитывать на них сразу же для инмаморе баз данных Существует такой процесс когда состояние базы данных сохраняется на жесткий диск снятие снапшота в некоторых OBD это является проблемой или являлась на тот момент это не должно быть проблемой потому что мы не можем допустить ситуацию когда хранилище раз в 10 минут начинает фризить нам нужно было масштабирование потому что мы росли растем и будем расти а соответственно нам нужно решать эту проблему живем мы в экосистеме кубера Да и разработчикам гораздо проще когда они могут просто поднять контейнер с базой данных эффективность микросервиса что мы вкладываем вкладывали в эти слова микросервисов много баз данных много а накладных расходов не должно быть много поэтому решение должно быть эффективным и нам очень хотелось решение мы начали искать варианты первым вариантом Был конечно жередец Но мы его не выбрали потому что тогда доклад бы назывался как мы решили проблему с помощью редис главная Проблема была в том что тогда это была версия 3 и у нее были проблемы например при выборе нового мастера могут быть простой до минуты большие задержки на снятие снимков про которые я говорил и репликация в случае аварии была достаточно длительной мы начали искать дальше и выбрали тарантул почему же мы собственно говоря выбрали тарантул большинство операций которые нам нужно от база данных они крайне простые вот первичный ключ Получи по нему данные или Сто первичных ключей получив записи Тарантул был эффективным в микротранзакциях у тарантула прогнозируемая производительность он использует один поток для доступа к данным за счет чего конечно же можно уложить все что угодно но в случае с тарантулом нам показалось что решение будет прогнозируемым Мы всегда сможем легко определить Почему началось торможение потому что мы испытывали такую проблему с той базой данных которую мы использовали до этого мультимастер репликация позволила нам легко сделать фейловер и рассчитывать на то что мы будем использовать мультимастер для того чтобы масштабировать не только запись не только чтение но и запись как показал уже опыт мы переучиваем офера точнее обучаем гоферы для работы слова в Тарантул где-то за две недели спокойно успешно начинает писать базу данных а языком в тарантуле является вот еще одной возможностью которая была для нас очень значимой это возможность использования Тарантул как сервера приложений мы не были ограничены только хранилищем А можем поднять gpc или Graf Server внутри Тарантула обеспечив близость вычисления и данных и за счет чего получить высокие performance и это было проект А нам этого очень хотелось собственно говоря мы выбрали базу данных Но нам нужно теперь было интегрировать ее в нашу инфраструктуру а напомню на тот момент Это около 100 разработчиков и одним из вопросов нужно было сделать для них все легко и просто С чего мы начали если дать разработчику пустой репозиторий он напишет решение но напишет его сильно дольше чем если дать ему какое-то решение в начале не нужно было выбрать дать ему шаблон скелет приложения Или дать ему фреймвок определенный мы начали анализировать наши начали анализировать эти решения и вот к чему Мы пришли с шаблонами у нас уже был успешный опыт Так мы делали микросервисы шаблон оставляет максимальную гибкость никто не мешает разработчику полностью Выкинуть все решение и написать полностью свое с помощью шаблона мы даем разработчику полностью работающее решение он умеет решение с которым может поиграться И даже если он еще не знает как работает Тарантул он это увидит в том репторе который для себя создаст одна из проблем работы с шаблоном это то что нужно в определенный момент обновить все например 82 Тарантул если раздать разработчикам то каждая такая Задача будет минимум один рабочий день соответственно мы потеряем 80 человек дней Но такая же проблема была у нас с микросервисами и у нас К тому моменту было работающее решение называлось оно и есть до сих пор реплинтер это приложение которое позволяет сделать автоматизированные массивные обновления всех репозиториев мы добавили в него возможности исправлять тарантулы и собственно говоря это решение подошло но и фреймвок мы тоже рассматривали главным плюсом фреймвок как раз является простота обновления увеличили версию библиотеки получили обновление минусом фреймвока и плюсом то что он устанавливает рамки и разработчик будет в них работать иногда это хорошо иногда это плохо Ну и при использовании фреймвок будет гораздо меньше кода в репозитории выбор был сделан в пользу шаблона У нас есть уже с чем работать но теперь база данных надо как-то создавать мы сделаем с помощью telegram-бота разработчик заходит отвечает на ряд вопросов после чего для него создается репозиторий прописываются записи в Консул попозже расскажу зачем они там нужны и создаются например четкий вольте все это для разработчика полностью прозрачно потом наступает этап разработки мы даем разработчику шаблон и небольшой туллинг для того чтобы помочь ему в работе собственно говоря это тесты юниты проверка покрытия линтеры возможность запустить эту базу с помощью одной простой команды и маленькая команда для того чтобы подключиться к консоли собственно говоря следующим этапом является Билд разработчик создает пол request и что же умеет наш Билд во-первых он умеет прогонять тесты практика показывает что все что не проверяется в билде когда-нибудь будет каким-нибудь разработчиков пропущено поэтому и билдею надо проверить обязательно надо также проверить покрытие так как база данных по определению является stateful приложением мы установили требования покрытия минимальные 90 процентов а для критических баз данных сто процентов проверяются линтеры отправляется уведомление в telegram-бот это позволяет отреагировать если кто-то забыл про то что сейчас действует код Фриз и успеть его поймать ставится метка в графане при теплое Это позволяет нам очень легко расследовать инциденты открыв графа но сразу видишь произошел разлив увеличилось потребление памяти и собственно говоря позволяет сделать деплой Сейчас я расскажу про deploy Но на самом деле я не могу рассказать про тепло не рассказав как у нас работает архитектура поэтому сначала архитектура в центре всей архитектуры находится балансировщик с помощью консула он знает Какие существуют базы данных какие-то рантулы существуют и Какие из них сейчас работают нормально есть сами тарантулы весь трафик мы подаем только на одну из баз данных вторая находится без запросов но она в случае аварии трафик будет переключен на неё И эти базы данных между собой общаются по каналу репликации с помощью мастер репликации для приложения которые пишут разработчик для микросервиса все максимально прозрачно для него есть только один and Point и ему не нужно знать что находится за ним это принципиальная схема в реальности Конечно все выглядит чуть-чуть посложнее потому что мы не можем позволить себе единую точку отказа и всего Разумеется много а теперь собственно говоря про deploy конечно же нам нужно было обеспечить бесшовное тепло и потому что мы не можем позволить себе даунтайм а для большой базы данных теплой может занимать до часа например начинается все с того что разработчик устанавливает тег будь ТРЦ так для стрижа или продавый тег и начинается сам deploy первый instance который существует для этой базы данных останавливается после чего размещается новый код и начинает запускаться читая дам памяти если он проходит radings пробу и все хорошо тогда аналогичные действия проводятся со вторым инстансом Если же что-то не запустилось Билд прекращается автоматический откат не производится и разработчик вместе с инженером должен разобраться с этой проблемой Потому что если ты автоматизировано попытаться откатить результаты могут быть не прогнозируемым такой риск не пошли дальше нам нужно было решить вопрос Как нам хранить конфигурации ведь для того чтобы разместить базу данных конфигурации нужно много нужно сконфигурировать непосредственно сервер где будет размещаться база данных нужно сконфигурировать балансировщик нужно сконфигурировать бэкапа и доступы все это много конфигов Но мы не хотели делать много конфиков и сделали инфраструктуру как кот то есть репозиторий в котором хранится База данных содержит себе полностью все конфиги которые с ним связаны и настройки доступа и настройки репликации и настройки бэкапов и все необходимые настройки собственно говоря бекапы есть люди которые делают бэкапы есть люди которые теперь делают Backup Мы решили быть в первой категории поэтому сразу начали делать бэкапы для этого мы используем велерон и задаем в конфиге все того же репозитория на стройке для стыджа для провода отдельно да иногда нам требуется бэкапы также на 3G и расписание с которым будет сниматься бэкапы с этой базы данных а для того чтобы например раскатать бокап отдельно мы используем Дрон Вот и промоутек Итак что у нас есть у нас есть чем работать у нас есть Как создавать базу данных У нас есть как базу данных тепло ить как конфигурировать и возможность снимать бэкапа самое время перейти к собственно говоря разработке базы данных Тарантул это не привычные многим разработчикам база данных и это привело к тому что нам потребовалось написать ряд правил какие-то из них были написаны сразу но большая часть была создана в результате опыта Итак что же мы придумали для разработчиков хоть мы работаем всегда с одной базой данных но разработчик всегда должен учитывать что это мультимастер репликация потому что он никогда не знает в какой момент произойдет переключение переключение может произойти потому что какая-то база данных по какой-то причине упала Или например Потому что начала тормозить и балансировщик переключил трафик на другую поэтому он должен учитывать то что это мультимастер репликация так как это не просто мультимастер но еще и синхронная мультимастер репликации никаких финансовых операций делать нельзя поэтому это стоит запрет в случае если нужно сделать финансовые операции Мы выбираем другую с БД нельзя делать большие БД тут Важно пояснить что большие здесь не про размер Если у вас есть база данных который 300-400 гигабайт или даже терабайт Но в ней у вас есть тысячи rps это не будет проблемой тут как раз именно про rps важно помнить Тарантул не утилизирует под обработку данных больше одного потока и поэтому он не сможет использовать все CPU и поэтому нужно база данных делать небольшими небольшими по количеству операций следующим сюрпризом для нас стало что оперативная память не бесконечная и нам нужно помещаться в нее всегда а для того чтобы знать что мы помещаемся нам нужно рассчитывать объем поэтому мы с вами написали небольшой калькулятор который позволяет зная схему количество записей в каждом спейсе и РПС рассчитать объем который будет потреблять в Пике база данных тогда мы стали мы избавились от проблемы с потреблением памяти Работаем только через хранимые процедуры почему же мы так делаем в самом начале Мы работали через прямые методы доступа к данным и все было хорошо до того момента пока это не начало работать в продакшене нам потребовались хранимые процедуры для того чтобы сохранить обратную совместимость То есть когда мы развиваемся разлить базу данных и приложения одновременно невозможно Ни При каком подходе более того всегда может быть какой-то сюрприз и база данных просто или приложение не разольются и не будут развиваться еще несколько часов соответственно Мы работаем через хранимки сохраняем обратную совместимость То есть когда нам нужно изменить сигнатуру хроники мы меняем добавляем новую версию старую поддерживаем пока не выпилим предыдущую методы которые мы используем должны быть только и доплатантны То есть replace можно insort нельзя Почему Потому что это мультимастер и запрос всегда может прилететь на обамастера и он должен выполнить то что ожидает Разработчики если он отправит два инсорта то у него ставится две записи если он отправит два реплейса с одинаковым первичным ключом у него будет одна запись при использовании методов обновления данных нужно учитывать особенности репликации это лучше всего рассказать на примере например разработчик отправил запрос два разных запроса один попал на сервер а второй попал на сервер б один запрос изменил на сервере apoli 1 Второй запрос изменил на сервере б Поля 2 и в начале все было так как разработчиков одной записи и все начали было так как разработчик рассчитывал но потом эти сервера через канал репликация обменялись реабилитационными журналами и в итоге на сервере а изменилось поле 2 а на сервере P изменилась поле один для того чтобы такого не было разработчик должен менять только то поле которое ему нужно они все сущность целиком Тогда через репликационный журналы пролетит изменения на сервере А поле 1 а изменение на сервере поле 2 и когда они меняются репликационными журналами на обоих серверах будут базы данных 1 и 2 это небольшое правило но с такой проблемой мы сталкивались когда мы меняем схему нужно указывать параметры Note exist.ru потому что миграция Может выполниться на двух серверах и тогда пойдет ошибка потому что схема на одном из серверов уже была изменена автоэнкрементирование Ну и создание первичных ключей с помощью сигнала запрещено это асинхронная репликация и сервера будут всегда когда-нибудь состоянии что они не знают как изменила другая реплика сиквенс соответственно у нас запрещено не просто инкрементирование но и запрещено генерация первичного ключа в базе данных только в приложении почему это сделано а рассмотрим ситуацию когда разработчик приложения отправила запрос на сервер а на создание сущности при этом база данных сделано таким образом что сущность будет создаваться первичный ключ будет генерится в базе данных запрос прилетел на сервер на сервер а там сгенерировался какой-то записался в базу но обратно приложение пришла ошибка и приложение отправило сообщение запрос в сервер B и там создалась новая запись как итог две записи Если же делать как делаем мы то есть создаем первичный ключ в приложении запись будет всего лишь одна как я и говорил Тарантул это база данных у которой всего лишь один поток на обработку данных Иногда нужно сделать тяжелые операции например пробежаться по коллекции из двух тысяч из двух миллионов элементов в это время все остальные запросы будут ждать в очереди Но если передавать управление другим потоком то они будут стоять в очереди соответственно если мы делаем что-то большое обязательно нужно вставлять команду eld для передачи управления другим потоком еще один вопрос который для нас был тяжелым это изменение схемы Разумеется в процессе работы схемы данных меняются и возникает ситуация когда нужно добавить новое Поле в Space при этом Pass это в принципе аналог таблицы соответственно все записи картриджи которые находятся в этой таблице необходим необходимо обновить то есть добавить в них вручную Поля с дефолтным состоянием типа Иначе мы получим ситуацию когда часть картофеля например имеет пять полей а часть картофель имеют шесть полей это будет неудобно в работе придется писать для того чтобы понять как эту запись декодировать правило правило было не очень много но для разработки Не хватало еще тестирования так как это стоит вам приложение ошибка в нем очень-очень дорого может стоить соответственно в обязательном порядке еще до выпуска правил работы мы определили как мы будем тестировать собственно говоря вот что мы используем то есть для тестирования мы используем тест для проверки покрытия лаков Ну или интер-дуочек это закрывает большую часть проблем которые разработчики могут допустить чего мы добились собственно говоря два с половиной тысячи РПС до 60 тысяч запись на чтение в секунду до 50 тысяч изменений и 30 миллисекунд это очень похоже на требования которые были собственно говоря мы справились задачей которая перед нами стояла но не без проблем Какие проблемы мы испытываем при работе с тарантулом нормальная ситуация как я и говорил базы данных находящихся в мультимастер режиме реплицируются между собой но это не всегда так иногда разрывает репликацию Почему эта проблема потому что для починки Нужно одну из баз данных очистить и она потом заполнится через канал репликации И это тоже не проблема Проблема в том чтобы гарантированно определить в какой базе данных все данные Поэтому нужно смотреть на состояние каналов репликации и выбирать это происходит не просто ещё одной проблемой что на данный момент мы работаем без шортирования Ну почти без шортирования на самом деле мы разделяем большие базы данных по доменам на маленькие базы данных что позволяет получить еще такой Профит как возможность при обслуживании сложного запроса сделать параллельное обращение к разным тарантулам То есть например для каталога про который я рассказывал это 6 баз данных если мы выбираем максимально полный агрегат товара 6 запросов выполняются параллельно также мы используем шортирование на уровне приложения то есть приложение выбирает на какую из баз данных ему обращаться это решение мы только-только начали использовать но оно нормально работает собственно говоря какие наработки у нас сейчас есть первое про что я говорил мы используем только процедуры для того чтобы легко и просто проверить что разработчик не начал использовать методы прямого обращения к данным мы ограничили интерфейс второе что Мы заметили разработчики retride свои запросы это абсолютно нормально но смотреть постоянно на код review о том что кто-то пишет ретро оболочку нам не понравилось мы написали просто враппер который позволяет все запросы То есть если разработчику нужно ретровить запросы он создает подключение которое обернута также мы добавили метрики в прометел у тарантула есть замечательный модуль который позволяет экспортировать метрики в прометеоз но мы хотели увидеть две вещи которые в нем Нет во-первых это статистика по процедурам хранимым которые мы вызывали то есть разделенная по хранимым процедурам а второе это время за которое приложение получает данные из Тарантула а не за который Тарантул отвечает поэтому мы сделали обёртку свою для прометеус Какая работа сейчас у нас выполняется в начале когда мы начали интегрировать Тарантул оператора для губернетаса еще не было и поэтому сейчас когда он появился мы добавляем картридж нашей экосистему А когда у нас появится картридж мы будем использовать шарнирование в шарт то есть без разработки в приложении собственно на этом У меня все большое спасибо что послушали а теперь мы переходим к самой интересной части я Напоминаю что конференция это в первую очередь про поговорить мы здесь собрались не чтобы поучиться не чтобы послушать новости не чтобы развлечься конечно чуть-чуть развлечь Но это отдельная история А чтобы обсудить подгорающие насущные актуальные темы Анатолий прошу первый вопрос второго дня Меня зовут Анатолий А вот вы рассказывали о куче проблем с мастер репликацией Я когда в проде Тарантул использовал мы просто две реплики ставили в лидонли и на них запись не приходила и соответственно дальше видео lection вы настраиваете и всё работает как в Классике Почему вы так не стали делать почему мы начали использовать мультимастер собственно говоря для того чтобы обеспечить фейловер То есть как только один из инстансов падает балансир переключит трафик на второй который сразу же готов к работе произойдет выбор нового лидера как в классических базах данных и у вас старая нода которая была мастером перестанет принимать запросы на запись а новые надо начнет принимать в результате у вас Но есть конечно вероятность что часть данных потеряется не доедет потому что репликация синхронно Но она очень низкая И большинство проблем с апдейтами и мультимастер аппликации вроде бы так решались Нет нужно детально и разобраться в вопросе чтобы ответить вопрос Я предлагаю уже в кларах за кофе ноутбучики открыть и посмотреть Так А тут мы обсуждаем те вопросы которые мы можем за несколько минут задать из зала Middle справа Привет меня зовут Максим Спасибо за доклад А можно вот чуть подробнее про то что он используется как Application Server то есть часть бизнес-логики вы получаете увезли в Тарантул то есть ну или это было просто Ну вот как это шортированием связалась То есть у нас получается что вот какие-то правила опять же Ну вот пройденпотентность если мы говорим какие-то правила бизнес логики там типа с одного мастера Одно поля Обновили с другого другое у нас бизнес логика разъехалась в РМ нам не помогла потому что шарнирование на уровне горного приложения если я в двух словах сформулирую как делили бизнес-логику между Application сервером и горшными микросервис всё очень просто Тарантул не досталось без логики совсем Всё досталось микросервиса полностью то есть наш подход что работать при работе с тарантулом что-то с подгрессом это для нас хранилище мы используем их только в качестве хранилища А тогда вот эта история что одно поле пришло с одного мастера одно с другой и все разъехалось оно пришло из microset с одного микросервис с одного пришло одно поле в один шард нет другого внезапно пришла в другой шар это не в шарт это в разные мастера мы рефардировании вот они про фердирование тут я еще не говорил разные мастера приложения отправила один и тот же запрос Как так произошло оно от первого сервера получила ошибку как будто бы запрос не выполнился А вот поэтому запрос на второй сервер а можно сразу тогда еще один вопрос а с ришардингом уже сталкивались ну вот прямо на Живую А мы еще не использовали вышарт то есть мы пока что шортирование у нас так сказать в больших кавычках шортирование на уровне доменов и шортирования на уровне приложения в Sharp настолько ждет потому что мы сейчас перенесли все в кубер и следующим этапом будет подправить базы данных чтобы можно было использовать картридж А их 80 это совсем не быстро Ну то есть ну просто как шорты тарантулы которые управляются микросервисом с этим тоже еще три шардингом пока мы еще не работали но только игрались на локалках всё так прежде чем Следующий вопрос мне тут нашептали помощники что у тебя есть подарки за лучшие вопросы да три штучки тебе сейчас будет очень тяжело тебе их придётся как-то запоминать я стараюсь вот Старайся это будет очень тяжело прошу Да спасибо за доклад А вы как-то мониторите консистентность данных ну то есть допустим если не разные Поля обновлялись из одного из другого мастера а обновления прилетело на одно и то же поле для того чтобы мониторить консистентность мы опираемся на состояние лака то есть Какой лак между двумя репликами Нет ну вот произошло переключение там улетел апдейт На это поле с одним значением один мастер и параллельно во второй на это же поле с другим и они по репликации синхронизировались и получилось что разные значения в мастерах при этом сама репликация осталась рабочей нет такое мы не мониторим мы это мониторим на уровне тех ревью код review так тут потом еще куларов будут отвечать на вопросы набежали Тарантула воды обсуждают ведение тарантулы Довольно интересно на самом деле в тарантуле луашечка и много всего полезного Следующий вопрос мидовая по центру Меня зовут Виталий хотел спросить такой вопрос Если у вас проблемы с рассинхронным временем то есть время на приложение или авторан то ли если там нужно текущее время тайм чтобы проверять записи и обновлять Соответственно по поводу времени Да у нас была такая проблема когда мы обеспечивались синхронизацию данных пришли мы к такому же решению как с генерации первичного ключа время мы берем из микросервиса то есть микросервис передаст в качестве аргумента метода время которое нужно Тарантул записать на микросерверах может отличаться но это уже вопрос к микросервисам и снова я Здравствуйте а не пробовали раз для хранимых что еще раз даст для хранимых там вот Господа там Осипов Константин и Пика дата написали адаптер можно теперь хранимки нарасти писать они Нет не использовали Объясню почему Потому что в моем понимании это будет гораздо сложнее В плане бординга для разработчиков то есть ложечка действительно за неделю за две разработчик начинает на ней писать а вот с раз там я боюсь будет подольше Но это возможность мы сможем использовать если будем выдавливать максимальные перфоманс от себя добавлю я несколько лет назад делал конференцию про тарантулы К сожалению знаю о нем чуть больше чем хотелось бы там используется ЛуАЗ джит поэтому именно по скорости оно будет не сильно отличаться от роста и если мы хотим писать хроники для с помощью Rust то главные ценность будет это система типа frasta которая защищает от написания плохого хода и так далее но Цена которая за это платится она чудовищная то есть обучить разработчиков Вот хорошо использовать раз это прям очень-очень-очень дорого так что такое так слева где-то был нет слева Не было вопроса у меня ложные Воспоминания так фронтенд справа Большое спасибо не могли бы немножко уточнить вот ситуацию с первичными ключами То есть вы не хотите полагаться на механизм генерации ключей который встроен в базе генерируйте Их на стороне приложения а вот как обеспечивается на стороне приложения уникализации этих ключей То есть как распределенно сгенерировать уникальные ключи или предложения только одно приложение конечно же не одно например каталог крутится в 40 инстанциях для того чтобы сгенерировать уникальные ключи мы используем Привет коллеги кто еще хочет поговорить со спикером пока он стоит на сцене и мы Некуда бежать от наших вопросов и него плюс 20 красноречию формулировании мыслей Добрый день У меня немножко вопрос про производительность проводили ли какие-то тесты на больших данных терабайтной базы И где примерно находили деградацию производительности деградация производительность будет по нашему опыту не от объема точнее мы не дошли до тех пределов у нас не хватило оперативки чтобы дойти до таких объемов То есть у нас как раз терабайта оперативки и было на тот момент когда мы выбирали на самом большом сервере а деградация будет от количества запросов То есть это по нашему опыту но это не означает что он допустим на двух терабайтах Такой проблемы не будет и снова я а Правильно ли я понимаю То есть вы винил не используете только менты X только MTX Да винил на нашем опыте когда это еще была версия 2.2 показал себя крайне медленным поэтому от него отказались Понятно спасибо большое"
}