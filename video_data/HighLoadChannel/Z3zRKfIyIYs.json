{
  "video_id": "Z3zRKfIyIYs",
  "channel": "HighLoadChannel",
  "title": "Новый граф Одноклассников / Антон Иванов (Одноклассники)",
  "views": 1001,
  "duration": 2932,
  "published": "2020-04-14T10:46:33-07:00",
  "text": "меня зовут антон я действительно работы в одноклассниках я работаю в команде платформы наша команда занимается тем что делает инфраструктура одноклассников стабильный эффективный сегодня я расскажу про то как мы улучшали наш граф друзей расскажу что это такое где он используется как он был устроен и какие с этим были проблемы что мы придумали чтобы эти проблемы исправить и немножко о том как мы переносили данные из старого решения в новое решение заранее хочу предупредить что это доклад не про графа и баз данных скорее было рассказывали про то как мы пожалели кассандру и наш кастомный in memory индекс кассандру будет немного про наш индекс будет побольше ну поехали что такое игра в друзей социальные сети это пользователя много много пользователей они могут между собой устанавливать связи типа дружбы причем мы можем пометить что не просто два пользователя друга но они являются братом сестрой одноклассниками сослуживцы мята и так далее и мы можем даже указать несколько тегов нам такой связи с точки зрения сайт это вот такое окошко где можно можете подробно отметить кем конкретно является вам тот или иной пользователи ну и у нас получается логический неориентированный граф друзей не ориентирован и потому что в связи симметричные если антона андрей друзья атлант он андрею друг андрей антона тоже друг и у нас есть отдельный граф запросов на дружбу нас тоже есть пользователя и они стучат друг другу и говорят добавив меня как коллегу добавьте меня как друг или даже может опять можно опять же указать несколько тегов и здесь у нас появляются ориентированный граф это совершенно отдельный графон на сайте три часа с точки зрения сайта совершенно не так как граф установленных дружбу если вы только постучались друзья человеку не смотрите просто не сможете просматривать его приватные фотографии но и так далее где это используется ну самый яркий пример это формирование ленты чтобы вас лента было интересно именно вам но еще и для просмотра для проверки прав доступа чтобы вы не могли увидеть фотографии которые пользователи выкладывают приватно конкретным пользователем там только своим коллегам только своим самым близким друзьям и вот это один из самых нагруженных запросов на наш граф порядка четырехсот двадцати пяти тысяч запросов в секунду наши клиенты делают еще это очевидный запрос дай мне список моих друзей порядка 125 тысяч запросов в секунду входящие заявки друзья да то есть мы должны понимать очень быстро не только кому вы отправили заявки в друзья но и кто к вам постучался друзья чтоб могли быстро принять заявки исходящие заявки в друзья естественно очень тяжелый клад множество различных запросов они достаточно тяжелый для того чтобы подобрать вам рекомендации друзей очень часто и нагруженные запросы это просто данное количество друзей это пользователя но чтобы подписать так под ним сколько сколько у него пользователи многие пользователи очень тщательно следят за тем чтобы это количество ни в коем случае демедж авось ну еще есть класс запросы в которой должен быстро отрабатывать покажи мне общих друзей с этим пользователям иногда на сайте пишутся просто количество общих друзей с конкретным пользователем чтобы могли пойти и наверное его знаю добавили к себе друзья да это показывается прям конкретно что это за друзья все это создают нешуточную нагрузку на серверную инфраструктуру на север и приходится почти 2 миллиона запросов в секунду в пике на север примерно но ещё до того как мы принесли это в наше собственное обычное решение было пятьдесят тысяч запросов в секунду на сервер и мы смотрели что у нас примерно 300 миллиардов связей хранится нашим графе как это было устроено и какие недостатки были в этого решения ну давайте вначале разберемся с базами основная таблица уже установлена грушку нас сохранилась в и склеили это было таблицы для того чтобы отразить например вот самую простую связь с антон и натали являются друг другу лучшими друзьями и коллегами это отражалось уже двумя строчками в такой таблице был праймари кей было де пользователя а ее друга с которым у меня связь и битовая маска типы в каждый взведенный бить их означал лучший друг это коллега сослуживец и так далее одно ребро мы хранили два раза исходящие связь от антона к анатолию это анатолия кантона вот и таблицы был эффективный праймер окей он был исторический нужен но в какой-то момент он нам стал доставлять проблем и потому что если что-то может разъехаться уникальность идет по эффективному первичному ключу то это обязательно разъярится рано или поздно и у нас получалось надо такое что одно ребро имела разные маски типов и мы во время миграции не всегда понимали что из этого является правдой забегая вперед могу сказать что на самом деле нет ничего из этого могло не является правдой но об этом потом естественно все 300 миллиардов связей у нас не умещались в одну таблицу поэтому мы сортировали и и для того чтобы сортировать мы брали последний байт и кишки пользователя получали 256 партиций группировались jardin по 16 штук и каждый из этих шарда вклады на какой-то конкретный выделены и железные искали сервера ну и таких серверов у нас было 16 штук и они находились 3 дата центрах было проблемам и у нас не было никакой никого файла вера никакой там репликации действительно при выходе из строя одного железного сервера у нас эффективно по крайней мере 16 использовать или не могли менять свои дружбы на самом деле еще больше потому что каждую связь мы помощи говорил отражаем двумя ребрами и поэтому даже те пользователи и друзья которых лежат физическим другом сериале все равно не могли создать связи с теми пользователями которые попали на в год неработающую ноду при выходе из строя это центра треть наших пользователей может быть даже еще больше не могла изменять свои дружбы и кроме вот этой проблемы чувствительности к отказам было еще достаточно очевидная проблема если вы используете сортирование просто по последнему байту это вам тяжело масштабировать такое решение нам приходилось если мы хотели добавить сервера как то вручную переносить между ними данные да еще и соблюдать чтобы это было там консистентные так далее все это требовало очень много работы отдельно у нас было и сколь таблиц и запросов на дружбы она была устроена проще там не было таких битвах масок там не было первичных эффективных ключей был просто три колонки создатель получателей тип если несколько типов это было несколько строк сортирование у нее была устроена абсолютно таким же образом по последнему болтом 16 сколько серверов с абсолютно такими-же последствиями а как хранились входящий запрос нам нужно быстро понимать кто к вам постучался в друзья на исторически опять же так получили что мы хранили эти данные в отдельные системе которые вообще-то занималась хранением нотификаций это на самом деле так было логично да потому что вы обычно вам нужно нужно эти данные показать пользователю всплывающем окне когда он заходит на сайт а почему бы не хранить эти данные собственных сервисе найти фикации но это было еще один атом . зоопарк а теперь о том как вот над всеми этими таблицами и базами были устроены каши был кэш дружку же установленных он тоже использовал сортирование по последнему байту объединял их нет 16 штук партиций в 64 и каждый из этих сортов находилась в трех дата-центрах и это было хорошо потому что при выходе устроят например одного сервера у нас оставалось в том что этот центр еще один кэш который будет обслуживать запросы а даже если вышел из строя видит это центр у нас были другие лета центры чтобы можно было обслуживать запросы то есть отказывай устойчивостью здесь в принципе все было нормально и масштабировались это решение простом добавлением серверов в каждый шар единственное что к чему можно было придраться это что при таком масштабировании мы увеличиваем reflection фактор но у нас и так был достаточно большой 7 серверов в каждом шарди вот а если мы будем добавлять еще он будет увеличиваться что в принципе не очень эффективно когда у вас много данных не хочет сохранить их семь раз тут надо сделать небольшую оговорку и подумать был ли был ли это решение кажом в классическом понимании потому что вообще-то мы хранили все данные в памяти они никогда не вытеснялись как то мы рассуждаем о кашах мы все-таки рассуждаем в том числе о том как мы вытесняем данные да здесь у нас данные не вытеснялись мы должны были хранившийся данные в памяти для того чтобы обеспечить performance периодически эти данные допились на диск и после рестарта прочитывались диск и как был сразу готов к работе отеле чтобы обеспечить тот факт что данная всегда должны быть в памяти и все это скорее правильно называть индексом в памяти чем к шуму и вот в это понимание потом позволит нам лучше перепроектировать систему чтобы сделать ее более эффективный надежный как было устроено обновление каша дружб вот у нас есть изменение дружба который мы хотим как-то отразить в нашем нашей базе и наших кашах мы записываем это в какую-то базу входит в моду базы и одновременно пишем у в отдельную базу который хранит только обновления которые происходят дружбаны и потом уже каждые две секунды каши и наши перечитывали данные из этой базы обновлений почему так сложно на самом деле этого решение весь хорошая плюшка у вас нет проблемы на валидации устаревшего значения если у вас крышка это время лежит то он поднимается смотрит на каком месте он в базе данных обновления застрял дочитывает все новые обновления и и вуаля он консистенция готов к работе в этом решении есть недостатки например на со сказанным с той частью вы добавили еще одно звено вашу систему виде баз данных обновлений который может падать вы не можете обновлять дружбы без этой базы данных обновлений потому что они попадут тогда в каше и отказоустойчивость вашей системы купола кроме того была проблема 2-х секунд нова запаздывания надо пользователя обращали на это внимание и создавали дружба потому и почему она в течение двух секунд еще не создалось это было достаточно редкий специфичными пользователями и баз данных друг дружку и базу данных обновления они разные у нас исторически получилось так что это была не одна я базу и между записями в эти две базы не было транзакций и в принципе теоретически они могли разъезжаться и просто вот на 303 100 миллиардов связи которые мы хранили это иногда происходило как был устроен обновление уже каша запросов на дружбы почти так же то есть мы писали в базу данных но здесь мы решили не делать отдельный базы данных обновления писали сразу в кэш но это его решение абсолютно противоположные плюсы и недостатки у вас нет дополнительной точки отказа у вас не запаздывания но у вас есть проблемы на валидации если у вас какое-то время лежал попробуйте потом его установить на консистентные состоянии если к этому моменту в окончательно потерялись в том как был устроен наш граф дружбу общем я не могу вас в этом винить потому чувствительность систем было очень много были всякие каши базы и я ещё не рассказал про отдельно кэш который обслуживал нас аналитический запросы тяжелые рекомендательные специально выделили отдельные конечно по не мешал быстрым сайт вам запросам в общем это все было тяжело поддерживать и тяжело осознавать поэтому мы стали это перепроектировать прежде чем перепроектировать что-то надо еще раз понять проблемы давайте отдельно посмотрим на проблемы с базы и проблемы с кашами ну с базы у нас была проблема отказоустойчивости потому что нас были скоро сервера без там репликации так далее тяжело-тяжело и масштабируя разъезжая баз и собственно количество этих база парк с кэшем у нас была проблема задержки обновления опять же некоторые разъедания и опере зоопарка этих кощей зоопарк механизмов обновления этих кашей давайте вначале посмотрим на базу мы это сделаем очень быстро потому что там все очень просто в на класс никах накоплен большой опыт работы с кассандрой и это как раз база данных который обеспечивает отказоустойчивость в ней нет единой точки отказа и обеспечивает простое масштабирование я сейчас подробно про кассандра рассказывать не буду приведу примеры докладов которые мы уже делали как мы построили на кассандре наша собственная система которая обеспечивает эти транзакции который есть полноценные индексы и вот последняя ссылка о том как мы вообще используем кассандру в 1 classic в том числе в хранилище классов тут только про базу я расскажу что мы взяли все наши данные которые мы хранили в базе как положили в одну табличку у нас получилась такая табличка где есть ади пользователя эти друга и все все все маски которые нам нужно хранить для того чтобы отражать consistent на запросы пользователей и уже установленной дружба таким образом у нас логическое разделение между разными базами мы устранили мы уже не можем создавать исходящий запрос а потом принимать дружбу и не удалив и входящий запрос потому что все данные в одном ярде и в одной базе с базы данных понятно давайте думать что делать с кашами какие требования скажу я на этом останавливалась еще раз про говорю что все данные должны быть в памяти почему потому что у нас есть много разнообразных тяжелых бачок запросов в том числе на рекомендацию друзей при распознавании лиц это используется оранже не поисковых запросов так далее которые ходят по холодным пользователям то есть реально многие из них делают такой тяжелый запрос как друзья друзей когда вам нужно пересечь друзей пользователей фактически запрос он перелопачивает вам когда вы делаете рекомендации друзей он прилла почует вам всю базу и эти запросы были реально чувствительны ко времени ответа когда мы начали приводить на новый граф мы заметили у нас небольшую performance ную проблему и некоторые запросы выдавались за 1 миллисекунду к нам тут же пришли проектировщики вот рекомендательные системы друзей и сказали мы теперь не успеваем за день перестроить все эти рекомендации как которые мы хотим перестроить вам нужно укладываться меньше чем в одном лю секунду ну вот а из этого требования общем вытекает все остальное если вы храните все данные в памяти у вас много данных вы должны хранить компактно у нас получал где-то 100 гигабайт на ногу нам требуется и вы должны уходить в хип а если вы положите эти данные в хит java мы используем джаву то вы можете но напороться на проблему с garbage collection нам с паузами garbage collection а тогда когда мы перед проектировали систему еще не было модных garbage collector в типа шенандоа и записи и и мы сознательно ушли в хип какие у нас есть варианты решение этих проблем и чтоб они соответствовали этим требованиям может попробовать выехать на кашах кассандра можно попробовать доработать существующую таблицу обновлений чтобы там улучшить ее характеристики потом отказоустойчивости консистентной sti можно прикрутить какой-нибудь модный архитектурный паттерн типы when sorting и таким образом выехать давайте посмотрим на первый вариант ну кассандры есть пока мире по крайней мере 2 каша первое что приходит в голову это кий кэш если вы посмотрим на процедуру на весь алгоритм чтения данных из кассандры это он достаточно сложно сейчас не буду подробно на нем останавливаться ну-ка кыш находится вот здесь и он как раз помогает вам некоторые некоторые стадии обработки запросов на чтение обойти но все равно это остается достаточно сложный алгоритм там блум фильтры там вам нужно прочитать данные с дисков если у вас данные в портишь на кришне оказались и мы даже в лучшем случае если все это работает идеально выжимали из этого порядка 30 33 тысяч запросов в секунду с одного ведра что в принципе неплохо но кассандры есть еще руках руках эта структура данных которые хранят в памяти ужасно джину из разных осях таблиц строку изначально ваша строка может храниться в разных с таблицах руках хранит в одной хранит уже все вместе снаружи на он хранит и духовке пену тут есть одна проблема каждый раз когда вы читаете и совки прессе данные кассандры их реализует в хип и из этого решения мы вытягивали порядка сорока пяти тысяч запросов в секунду русский drum правда есть еще одна неприятная особенность которая бьет по нашим требованиям что данные счета должны быть в памяти это этот кэш инвалиде руется при изменении как только вы изменяете дружбу он вас из кэша пропадает если еще раз когда вы прочитаете дружбу вам необходимы из базы данных обратно читать если данные в кэш что не круто поэтому мы решили да ну вас был еще б и зла и нас был прежний кэш но нам позволял выжимать 11 миллионов запросов в секунду игра казалось бы зачем вам столько вам столько не нужен у вас нет такого количества запросов на но с другой стороны это позволяет экономить цепью сейчас у нас одна нота графа загружает 10 лидер всего на 30 процентов если у вас сервер имеет обычно сейчас сервера имеет большее количество еды на остальные игры вы можете положить другие системы и таким образом улучшить утилизацию вашего оборудования поэтому мы решили все-таки использовать уже существующий кэш он был отказывай устойчивы у него не было проблем с перформансом единственного было как-то вкорячить в кассандру давайте попробуем в кряке с помощью таблицы очереди вот у нас есть клиент еще раз повторюсь что у нас пишет в основную таблицу данные еще он пишет какую-то таблицы очередь неважно как она устроена из которой потом кэш дочитывает данные что с этим можно было сделать ну например положить таблицу очередь в ту же базу данных что основная таблица сделать между ними транзакцию у нас не вырастает количество подвижных частей частей системы у нас сохраняется консистентной это вполне рабочее решение логическим продолжением этого решения можно считать вэнс очень когда вы все данные пишите в лог а потом система будь то база данных будто каши и так далее просто учитывались данные злоба и восстанавливают те структур данных которые нужны для того чтобы быстро обслуживать запросы но все эти лаги вам нужно еще одну как подвижная часть системы делать разбираться как это все работает давайте еще раз посмотрим на процедуру записи в кассандра у вас там на самом деле уже есть лог вот он кассандра вначале пишет все данные в к митлаг чтобы обеспечить чтобы обеспечить отказываем чтобы обеспечивать при системных данных и вы вполне можете в этот процесс залезть и писать данные не только ним таблицы но и ваш кастомный индекс памяти или ваш кастомный кэш который вы рядом сделаете ровно так кассандр пишет например в свой руках данные просто он нам не подходит под фонд нам соображение но мы вполне можем заменить на свой собственный кэш что мы и сделали и у этого решение много преимуществ во первых это просто как ни странно да просто потому что у вас нет никаких новых систем которые нужно обслуживать которые могут падать и так далее это надежно и эффективно у вас нет никаких меж процесса взаимодействия которые могут пойти не так воды могут падать и так далее у вас вся взаимодействия с ковшом происходит внутри процесса кассандра единственное что вам нужно в листьев кассандру как-то это сделать я сейчас подробно про то как это сделать рассказывать не буду потому что об этом подробно расскажет олег анастасьев вот он сегодня в 17:00 в зале сингапур доклад называется эффективная надежная микро сервисы и больше здесь будет не столько принтера сервиса сколько именно правда паттерн встраивание базы данных их к шее в ваше приложение для того чтобы улучшить характеристики этого приложения а я расскажу про то как собственно устроен наш индекс памяти который нам обеспечивает необходимый performance ну мы берем юзера кишку она у нас имеет тип long 64 бита с помощью хишама пим ее на какой-то уже готовый яндекс и этот индекс у нас указывает на ячейку большого-большого длинного массива если мы хотим положить туда данные а данные там уже лежат ну просто потому что это хорошо могут быть коллизии то мы попробуем следующую ячейку и дальше например если там данные тоже лежат на можем поскольку попробовать еще и следующую ячейку и таки положить туда данные и это кто знает называется механизм разрешения коллизий линия probing него есть проблема он подвержен образование так называемых костров вот вашему длинном массиве образуются прям целый длинные цепочки занятых ячеек в этом месте ваш конечно предстоит быть констант нам по по скорости чтения и по скорость добавления и становится фактически линейным и вы либо следите за тем чтобы он скажешь был достаточно разряжен ли придумаете какой-то другой механизм разрешения коллизий ну мы остановились на следующему простоте наверное механизме разрешение коллизий этого просто каждый раз увеличиваем шаг то есть произошла коллизия увеличили на один шаг произошла произошла еще одна коллизия на два шага и результате как alem данные уже подальше от изначального от того и чик куда мы должны были по его положить дано изначально наши тесты показали что даже если каир заполнен на 80 процентов деградации не какой-то фокус ну нету в таком механизме единственное что единстве вам нужно обязательно предусматривать ограничение на максимальное количество шагов на тот случай если вы все-таки заполнили ваш хэш больше чем на 80 процентов пропустили это тот случай если у вас какая-то атака просто об этом надо не забывать такой механизм построения построения кашей называется открытой адресация в отличие например от механизма спам построения к 6 помощью разрешения коллизий с помощью цепочек например в стандартных java skype шмапы используется именно цепочки изначально определяете baked куда кладется ваше значение а потом у вас есть связанный список в котором хранятся конкретные значения почему открытой адресация ну у нее лучше локальность данных царь чему цепочек для того чтобы посмотреть где ваши лежать данные в цепочках вам необходимо определить bucket и потом по цепочке еще перейти на нужную ячейку это может быть в разных участках памятью совершенно а здесь у вас один большой длинный массив если вы определили изначальную ячейку то вам не составит труда быстро ваши данные окажется либо прямо в этой ячейке либо где-то рядом увеличение шага это некоторый компромисс между локально стьюи образованием вот этих кластеров ну окей мы с вами придумали как положить юзера и диску в конкретную ячейку массива открыто адресации где же список собственно хранить друзей для этого у нас есть еще один массив он абсолютно такой же по длине и каждый его чек соответствует соответственно имеет такой же яндекс как ячейка юзера диски и там уже хранятся заголовки списков и эти заголовки указывают на конкретные списке в которых хранятся друзья как устроен этот заголовок ну это адрес начало списка и это фактическая длина этого списка потому что мы выделяем на заранее чуть больше памяти чем требуется под список и постепенно заполняем закончилась память абсолютно как в раю листе ржевском копируем это в другое в другой участок памяти опять накапливаем друзей тут интересно что длину мы храним не в собственно в списке а в заголовке почему ну теоретически нам требуется под адрес списка 64 бита это позволяет адресовать с чудовищными объемы помечены нам столько не нужно вот и собственно ну по длину можно выделить 32 бита и за все про все у вас уйдёт 96 бит для того чтобы хранить адрес ее длину но еще раз повторюсь нам столько не нужно практически нам достаточно 48 бит на указатель потому что мы собственно собираемся нарисовать такие большие объемы памяти нам там кусок 56 и терабайт вполне достаточно и в биты которые не используются из этих 6 4 бит мы можем положить собственно длину списка друзей нас получится ограничение 65 536 друзей опять же нам это некритично потому что реально в бизнес логики и используются меньшие ограничения этого у нас получается 64 бита как мы это покоем с помощью таких битвах операций мы раздвигаем фактическая длина 48 бит и логической или с адресом делаем получаем собственных и дыры почему это прикольно во первых у нас есть существенной экономии определение массиву это реальная длина массива нашего на одной ноте 500 миллионов на экономи примерно 2 гигабайта оперативной памяти не супер много но приятно и другое другое преимущество это то что вам для того чтобы узнать количество друзей у конкретного пользователя это еще раз повторюсь это достаточно частый запрос вам не нужно делать еще один ход по памяти лезть в список и узнавать как конкретное количество друзей он вас есть прямо в заголовке поэтому отсюда запрос отрабатывает очень быстро теперь давайте поговорим о том собственно как у нас устроено аллокация памяти под список друзей ну мы для этого используем open source библиотеку нашим собственно выложено в открытом доступе здесь если захотите посмотреть там есть класс называется мало conti краз простейший релиза и реализация локатора памяти пригодна для использования в многопоточной среде ну хорошо память выделили теперь давайте эту память используем для того чтобы хранить наконец-то собственно друзей ну для этого мы просто выделяем массив внутрь этой памяти и каждая ячейка этого массива это int указатель на фрэнд индекс маска типов и таймс темп который используется для того чтоб разрешать конфликты также как кассандр разрешают конфликты с помощью time stamp of так и мы в каше таким же образом от решаем конфликты тут можно быть внимательным и увидеть что вообще-то мы используем начале у нас юзера идеи имеет тип long 64 бита но здесь мы храним что-то другое на мы храним какой-то it как мы пакуем изначальный long в какой-то интер на самом деле мы здесь используем абсолютно тут же тоже ту же таблицу адресации мы просто ссылаемся на заранее зарезервировано ячейку который мы и таким же образом резервировали для пользователя для того чтобы положить его друзей опять же зачем мы так заморачиваемся почему не хранить long фрэн дойди полноценный прямо в списке друзей ну опять же здесь мы пока нам гораздо больше чем в предыдущем случаем на 300 миллиардов к связи мы экономим порядка 1 терабайт памяти когда пакуем лонга винт это уже очень приятно а второе это у нас есть эскиз к нам нужно быстро заблокировать какого-то пользователя спамера и вместо того чтоб ходить по всем списком всех друзей и выдергивать его оттуда вычищать нам достаточно всего лишь в таблице открытой адресации пометить чего каким-то образом например мы вводим просто знаковый бит у этого пользователя пометить что он не валидный что вы убери его из всех результатов запросов которые то больше давать клиенту или таким образом получается заблокировать его один раз ну а дальше мы там можно вычищать не вычищать это уже как как performance как происходит удаление из друзей прежде чем рассказать как происходит удаление списка друзей надо сделать поправку на то что это не такая частая операция и мы просто копируем весь список на самом деле то есть мы нас есть большой список друзей и мы хотим удалить какого-то друга из середины мы просто копируем начала этого массива и конец этого массива без этого друга это не очень быстро операция на линейное по времени от объема массива но с другой стороны она не такая часто им можно не заморачиваться как происходит удаление из таблицы адресации для того чтобы удалить пользователю которого не осталось друзей не как мы мы никогда его не удаляем z таблицы адресации на самом деле это очень редкая ситуация когда у вас пользователя нам не совсем не остается друзей его нужно удалять из этого массива кроме того мы на самом деле ссылаемся помните в списке друзей на элемент в таблице адресации и поэтому нам просто нельзя удалять оттуда в этом нет ничего страшного абсолютно мы по этому поводу не переживаем зато у нас пропадает целый класс проблем о том как удалить какого-то из таблицы адресации немножко прям многопоточность для того чтобы добавить нового пользователя в ячейку открытый адресации вам достаточно сделать conference fap вам не требует никаких log off и проставить и свободную ячейку вы пытаетесь положительно данные если у вас получилось значит никакой другой поток не сольется вместе с вами классе туда данные если не получился просто пробует следующую чек по этому алгоритму которую я описал ранее как работает многопоточном многопоточность списков друзей там нам нужны локи м потому что мы будем там писать активно читать данные из этого списка друзей и мы просто заранее выделяем 256 у нас такая такая константа данном больше меньше не требуется 256 трейдера и клоков по озеро кишки друзья которого мы хотим хранить мы опять же с помощью последнего байта определяем какой лук нам конкретно нужно захватить берем этот предлог при чтении write лог при изменении списка и здесь как всегда ритлоке совместимы между собой ряды в райт локи не совместимы между собой теперь последнее его и важное требование как нам обеспечить тот факт что у нас данные сюда в памяти даже после рестарта кассандры приятная новость заключается в том что несмотря на то что чтобы благодаря тому что мы положили все данные собственно в один процесс с кассандрой нам не требуется никакого механизма и механизма snapshot of который мы использовали в предыдущем решение в кашах помните я говорил о том что мы там пим данные на диск периодически вот на этого нам не требуется потому что у нас все данные так есть на диске в кассандре вот и единственное что нам требуется при запуске кассандра это проиграть все данные из базы прямо в кэш единственное что этой и операция действительно не очень быстрая то есть она у нас занимает порядка часа каждый раз когда вы остроту диск сандру там пререлизе ли там при аварии ждать час не очень хочется вот поэтому мы используем такое решение мы просто эмма пим область памяти в которой у нас хранится кэш файлик файловой системе которые существуют только в памяти nude в линуксе она например дело с если чем и этот файлик он выживает restart процесса кассандра kassandra или стартует этот файлик остается и она вполне себе может abra обратно зима пиццы и готово к работе нокии мы с вами все проблемы которые нас беспокоили решили давайте посмотрим как мы будем переносить наши данные из старого графа в новый план у нас был такой вот мы читаем из старого графа какое вначале в какой-то момент начинаем писать и старый и в новый граф но что им до сих пор только отдаем клиенту данные только из старого графа далее мы постепенно мигрируем данные старого графа в новой начинаем отдавать данные из нового граф в том числе сравнивает что нам отдал старый граф что нам отдал новый граф если расхождение нету значит мы данные перенесли корректно и можно переключаться после чего мы переключаемся естественном и ровно на этом пункте увидели что расхождение расхождение есть помочь я вам рассказывал про то что в некоторых частях нашей системы отсутствует на традиционность иногда баз данных скажу может разъехаться действительно таком количестве связи и мы это увидели вот и нам надо было с этим что-то делать вот у нас есть старый кэш и это данный который пользователь видит на сайте потому что он всегда читают данные нас из каша и не с базы базу нужно только для того чтобы запер чести данные и у нас есть базы мне есть некоторые другие некоторые некоторые другие данные некоторые такая параллельная вселенная если мы будем переносить данные и старые базу что мы вначале и сделали а потом переключимся на новый граф вы пользователи покажем другую вселенную которую о котором ничего не знал они там были данные старой базе он их никогда не видел потому что сюда видел данные которая приходила из старого каша решение здесь была очень простой мы просто стали забелин нато на то что лежит в базе и мы стали переносит это из кэша и после этого мы как раз перенесли все хорошо расхождений у нас после этого не было мы успешно переключились на новый граф что в итоге у нас получился один кластер сервисы вместо всего этого зоопарка решение которые в котором я рассказывал начале он обеспечим отказоустойчивые в нем нет единой точки отказывай он достаточно просто масштабируется с помощью кассандры у нас мы встроили в кассандру in-memory индекс которой консистентными с базой кстати мы это решение уже перенесли в нашей собственной ван клауд облака сейчас там двадцать восемь пенсов в каждом из трех ходов и центров я уже сказал о том что каждый instance это примерно это 10-я der и сорок четыре гигабайта оперативной памяти сейчас на него приходится порядка на каждый инсцес порядка 30 тысяч запросов в секунду что выливается в через четыре 100 мегабит в секунду по сети а каждый запрос мы обслуживаем достаточно быстро это меньше 3 миллисекунд на запрос над на стороне сервера при этом типе он загружен всего на 30 процентов если вы хотите больше узнать про то как автоматизировано наше облако то приходите завтра на доклад леонид талалаева в 1000 скриптом а у меня на этом все спасибо за внимание давайте вопросы в микрофон да кто с микрофоном подойдет привет у меня два вопроса во-первых наверное эта структура каша не выросла за один день то есть вы этапами и делали оптимизировали интересен процесс как вы мерили профит потенциальной какой share какую сторону пойти и как тестировали если там fast тестирования пир фестивале для многопоточности и второй вопрос про окна системность через дерму при краше как обеспечивается консистентной или если они мне не смешивается как вы понимаете что пора пересобрать кэш 0 ага первый вопрос я наверное смогу ответить хорошо потому что я не участвовал непосредственно в разработку и я рассказал им докладе что у нас этот конечно же был к тому моменту как мы разрабатывали наш граф на новый я наверное не очень компетентен в том чтобы рассказать про то как именно мы разрабатывали этот кэш а по поводу можно потом обратиться и я скажу кто может рассказать об этом лучше вот по поводу persistent насти где в если чем мы все очень просто у нас там есть контрольные суммы во-первых какие-то которые зависит от собственно от структур данных каша если оно меняется в процессе тепло и предложения дата мы понимаем что этот конец больше не водителей выбросить и данные все прочитать заново из базы то есть если мы замечаем что нам что кэш пустой или если он не соответствует тем параметрам который нам нужны вы просто выбрасываете все данные заново читаем из базы наполняем это корр заново блоке у нас только в память и память у нас локи берется у нас никто не берёт локи со стороны клиентов с локи берет только сервера покупать может только процесс сервером состоянии лола к все wizard of users пей считаете не понимаете кто-то взял лук начал что то менять в можно не в компетентном состоянии хэш-таблица и так не бывает там достаточно простые запросы которые не затрагивают одновременно много данных для того чтобы об этом не волноваться буду там специально проектирование tf чтобы это было именно так привет спасибо за доклад под в основном докладе было про простые запросы эти банки и вилы и то есть узнать друга есть список друзей для одного человека я хотя бы задать пару вопросов про более сложные запросы типа друзья друзей и возможно больше то есть какое-то больше аналитическую нагрузку не сп то есть используется или при этом тоже самое решение какие наткнулись какой глубины на практике запросы возникают потому что с каждым новым уровнем там скорее всего как квадратичный объем данных нужно вытаскивать вот и для таких запросов не пытались вы по-другому ли как-то организовать но хранение данных то есть как-то кстати рисовать чтобы все попадало в 1 ну скажем данные для таких запросам лежали как ты рядом possibly понял вопрос действительно это интересный класс запросов друзья друзей друзей например или еще глубже на практике у нас таких практически нету и мы под них не оптимизировали наш граф а запрос собственно друзья друзей работают там никакой магии нету мы берем друзей и одного пользователя мы подберем на друзей другого пыль ну мы берем друзей пользователя потом каждого для каждого этого друга запрашиваем его друзей это действительно требует несколько hop off по разным на дом графа это действительно не там не так быстро как просто взять и получить список 1 друзей но сейчас мы в это не убираем сейчас это работает то как это прям вот так ли описал все вопросы у башни но сразу появились здравствуйте спасибо за доклад очень интересно знать вот вы говорили что в какой-то момент вы сравнивали что возвращается из старого крыша из нового кэша вы это делали в процессе когда вы запрашиваете данные или или как-то более аналитичным в оффлайне да спасибо за вопрос думали об этом делали нам что-то сложное для того чтобы анализировать это мы сошлись на том что мы будем просто в момент выполнения запросов сравнивать данные в момент выполнения запроса писать в этом блоге если мы не видим в блогах большое количество событий а значит точно все плохо нам не нужно это анализировать особенно да нам не нужны для этой муки систем придумывать мы будем с этим разбираться как только мы увидим что в логе ничего не пишется вот все-таки значит все в порядке мы просто пришли к выводу что мне какая аналитика по этому поводу не нужно вот и и выехали так это был какой-то отдельный элемент сравнение то есть идеальный сервис который сравнивал нет это происходило на стороне клиента собственно когда он выполнял запрос в самом клиенте был инкапсулированы знание о том в какой граф нужно ходить мы могли с помощью нашей системы управления порталом переключать в какую граф конкретно нужно ходить нужно ли сравнивать нужно ли читать из старого графа из нового и так далее но прямо на стороне клиентов происходило сравнение запись в логе здравствуй спасибо за доклад вы сказали это что у вас есть ограничения на 65 тысяч друзей вот я честно говоря одна классика я не пользуюсь нет ли у вас там пользовательским то огромным числом подписчиков обрабатывать или вы их как-то отдельно там и так далее или это все в рамках того же механизма такие хорошие вопросы вот сейчас есть пользователю которых много друзей они действительно упираются в некоторую константу которая задана бизнесом она меньше чем 65 536 поэтому мы ничего с этим не придумывали вот мы живем с этим ограничениям и пока ничего с этим не делаем вот я добавлю ты наверное еще про звезд у них нет друзей у них есть подписчики дай там миллионы могут быть подписчиков но это в другой системе делается это здесь да да это хорош замечание до спасибо за доклад меня пара маленьких вопросов в какой-то момент а был массив и вам нужно было что-то из него удалить и вы как бы его полностью копировали вот почему просто например несчастность последним элементом и ну просто мешать вам вас инвалиде руется каждый вопрос больше нравится с каждым вопросом это действительно интересная штука нам важен порядок нам бизнес продиктовал то то что в этом массиве друзья должны лежать в определенном порядке в порядке добавления на самом деле поэтому не можем просто удалить середины и свапнуть каким-то элементам мы нарушим от этот порядок нам нужно его сохранить поэтому нам приходится копировать еще в хэш-таблиц вас в самом конце вы говорили что вы делаете просто касс и теперь с не получилось делать и как дальше ну там в порядке в увеличении вот и это хэш-таблицы ну не растет в размере получается она растет есть какие-то такие 13 надо понимать что количество добавления друзей по сравнению с там со чтением не такое большое он действительно добавляются и и это таблицы пасти постепенно растет но мы сейчас посмотрели что столько памяти сколько мы выделили нам еще хватит на на там на год или даже на 2 вперед просто мы заранее выделили больше памяти чем в больше элементов в этой таблице открытой адресации чем нам сейчас требуется давай здравствуйте вопрос следующее если какой-то пользователь и решил удалиться из сети соответственно как вы обрабатываете это событие в отношении всех друзей которые у него есть у ну мы удаляем рано или поздно тех друзей которые есть я сейчас наверное не смогу это воспроизвести лучше потом задать нет вопросов я могу скинуть проконсультирует и знает лучше но рано или поздно удаляем естественно я не уверен что я понял вопрос а то есть когда мы удаляем пользователя он должен удалиться из друзей других пользователей да да и счетчик у них должен уменьшиться правильно все верно так происходит но это происходит на лету или к нет нет это тяжелая оффлайн операция который происходит не мгновенно когда вы удаляетесь соц сети некоторое время наши сервера пыхтят на том чтобы у вас от отовсюду вычистить а просто удаление связи удаление связи друзей это длительная операция или это можно быстро выполнить в этой схеме ну вот в этой схеме я рассказал что из-за того что нам требуется вот это фактически середина массива удалять кого то то это достаточно тяжелая операция особенно если вы удаляете пользователя из всех всех всех всех всех всех друзей до других людей чтобы отложить на чуть попозже нам не бесплатная danny константное так время отведенные на вопросы кончилось но вы можете расспросить антона в дискуссионных зоне после небольшого подарка вы выбрали лучший вопрос но уже о было выбрать вопрос ночь все очень понравились мне понравилась особенно вопрос с косым и вопрос с удалением с сохранением порядка почему мы не используем переброс встаньте пожалуйста создалась сдавать вот вам небольшой приз можно покупать спасибо а теперь мы вручаем свою благодарность за ваш труд и вклад спасибо большое все прошу вас пройти в discussion азанов"
}