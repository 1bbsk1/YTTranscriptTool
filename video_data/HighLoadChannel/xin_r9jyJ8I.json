{
  "video_id": "xin_r9jyJ8I",
  "channel": "HighLoadChannel",
  "title": "Как мы защищаем при перегрузках миллионы клиентов / Антон Колесов (Nexign)",
  "views": 1058,
  "duration": 2911,
  "published": "2023-04-28T06:10:46-07:00",
  "text": "Всем привет меня зовут колис Антон я занимаюсь внедрением эксплуатацией продуктов интеграционного слоя в компании nexign и Наша компания занимается предоставлением биллинговых решений для операторов сотовой связи мы разрабатываем внедряем эксплуатируем наше решение у операторов масштаба тир 3 до тир-1 и для контекста на слайде приведены цифры Core слоя нашего решения в ландшафте клиента Мегафон и на примере на примере этого Контр приложений Сегодня я расскажу о том как боролись с проблемы влияния перегрузок сервисов на глобальную доступность Начнем с того что мы будем вкладывать понятие перегрузка перегрузка это когда конечный сервис перестает справляться с потоком поступающих запросов и соответственно время их выполнения начинает увеличиваться И основная проблема здесь это в том что исходное влияние может выходить за пределы конечного сервиса и выходить на более глобальный уровень есть такой проблемы мы действительно столкнулись на этапе федерализации нашего решения то есть это был период такой достаточно активный архитектурной трансформации когда мы распиливали Монолит и мы столкнулись очередной инцидентов которые имели достаточно серьезное влияние на бизнес и получили прям значительную порцию негатива как от абонентов так и от внутренних наших пользователей и расследование показало что причиной просадок доступности были в связаны с перегрузкой ряда конечных сервисов то есть перегрузки эти в конечном итоге вылились в то что было исчерпан лимит доступных соединений причем соединение исчерпались некоторых случаях там стране конечных приложений сервисов и каких-то инфраструктурных логических лимитов так и в некоторых ситуациях лимиты исчерпались даже на уровне оборудования непосредственно что же конкретно произошло предлагаю посмотреть на упрощенной схеме контраприложений здесь у нас есть некие клиенты генератор запросов условный балансировщик ngx и ряд конечных сервисов при этом конечный сервис они могут взаимодействуют друг с другом посредством API и условно делить общие источники данных там находясь на единой базе используя там каждый свою схему например вот и как наша проблема выглядела на примере этой схемы например сервис балансов условно проседает производительности у пользователей соответственно возникает какие-то проблемы с отображением балансов в плане их актуальности или скорости получения ответов и так как я говорил что сервисы могут быть как-то связаны друг с другом в принципе увязаны какими-то разными связями на разных уровнях и так далее То развитие ситуации может быть разные например соседний сервис продуктовых предложений начинает копить Коннект в ожидании вслед за сервисом балансов связано с тем что перед подключением например услуги он синхронно ходит в сервис балансов и запрашивает там доступные средства абонента или например там изменившийся план запросов на не оптимальный вот у балансов он создает избыточную нагрузку на базу и влияние перекидывается на соседний сервис уже через источник данных и с высокой долей вероятности события даже будут происходить как-то вместе параллельно и так далее так как все-таки связь сервисов непосредственная и все эти взаимные ожидания и блокировки в конечном итоге приведут к тому что произойдет то самое исчерпание доступных выше соединений и соответственно влияние распространиц уже на глобальный уровень даже на те сервисы которые не затронуты как бы исходные проблемы потому что все доступные соединения будут висеть вот на на тех на кого собственно непосредственное влияние оказывается Вот и практика показывает что в таком случае произойдет Каскадный рост нагрузки Иисуса который вероятностью потому что конечная пользователи начинают получать какие-то первые ошибки и реагирует на них обновлением страниц пери логинами перевоставлениями заказов и так далее за пользователями при этом не отстают и какие-то наши внутренние механизмы при повторов которые но призваны все-таки довести там выставленные заказы выполненные состояния Вот и в конечном итоге нагрузка которая без того сейчас уже достаточно большая начинает увеличиваться увеличиваться увеличиваться и глобальная доступность начинает складываться из-за этого и все это превращается в аварии Достаточно серьезный такой обращаемостью Вишенка на торте здесь будет то что такой такие проблемы и развития таких проблем связано со сложностью локализации источника с чего все началось потому что система работает достаточно высокими рейтами и соответственно вот моменты когда влияние начинается на уровне там какого-то одного сервиса тех же балансов до выхода это уже на глобальном какой-то масштаб проходит буквально считанные минуты и соответственно пользователей все это время наблюдается как бы ухудшение и увеличение времени перебоев доступности интерфейсов обслуживания разных либо интерфейсов для абонентов там личный кабинеты либо это интерфейсы обслуживания которые там операторов например колл-центр и так далее Вот и соответственно инженеры которых подключают к решению инцидента в аварийном режиме они на старте видят примерно такую картину То есть все запросы где-то что-то кого-то ждут вся графа на оранжево-красная за что хвататься куда копать сходу непонятно и собственно этом есть проблема вот и от такого влияния безусловно нужна защита то есть защита виде какого-то троттлин механизма тротлинга аварийного запросов к конечным приложениям вот в нашем случае помимо того что мы поняли что такая защита нужна это как бы очевидно стало это еще и подкреплялось неким явным таким желанием от бизнеса до получить такое решение как можно быстрее И тем самым исключить возможный рецидивы В таких ситуациях в общем поэтому изначально установки у нас были такие И вот приведенные цитаты она полностью как бы отражает то к чему в идеале необходимо стремиться В таких случаях логично что если сервис начинает деградировать в плане производительности и время запросов время обработки запросов увеличивается то хотя бы временно необходимо его разгрузить и поток входящий запросов к нему ограничить и какие здесь могут быть подходы к решению Можно попробовать решить эту проблему на уровне конечных сервисов то есть для этих сервисов необходимо будет выдать какие-то явные лимиты либо количество соединений либо Время ожидания ответа от этих соединений идеальным вариантом будет Возможно даже сочетание этих факторов вот при этом какие-то базовые лимиты уже реализованы на уровне библиотек которые обеспечивают интерфейс взаимодействия с этим приложением или на уровне коннекторов до источников данных но скорее всего это какие-то такие глобальные лимиты которые выставляют сразу Возможно они вообще как оставлены в дефолтных значениях и не вынесены в какие-то параметры приложения вот мы же хотим добавить определенной гибкости всему решению чтобы вот выставляем лимиты были более приближены к реальным требованиям Поэтому с высокой долей вероятности мы столкнемся рядом но и первым таким но будет то что если у нас уже имеется сотни реализованных приложений сервисов конечных и так далее у этих сервисов уже наверняка есть свои особенности в их реализации свои какие-то требования к времени ответов Ну в общем свои какие-то договоренности эксплуатационного характера и поэтому придумать для всех имеющихся сервисов какой-то новый универсальный механизм их лимитирования Это скорее всего невозможно Вот соответственно необходимо будет условно реализовывать эти лимиты каким-то образом отталкиваясь от особенностей реализации конечного сервиса или делите как-то по группам там зависимости от того на какой технологии Они сделаны из каких-то условиях они работают какие у них требования так далее в общем это будет такое придется поработать индивидуально там с каждым либо с конкретной группой вот и это уже не выглядит невозможно но с высокой долей вероятности это выглядит как то что это достаточно трудозатратно в плане людских ресурсов в плане там привлекаемых команд и так далее В общем это будет дорого и Кроме того если мы придумываем многие вот разнообразные варианты этих лимитов то дальше стоит вопрос А как их поддерживать Как устанавливать Как хранить это потенциальная большая проблема и причина перегрузки может возникнуть например на уровне выше самого сервиса например до конечного приложения где-то там сетевые задержки возникают периодически и собственно вариант с решением на уровне сервиса он в принципе не гарантирует результат не гарантирует защиту и того как бы мы внимание и того мы получаем что данный подход он во-первых потенциально выглядит дорогим что для нас было недопустимо так как решение нам нужно было прям быстро Вот и второе это то что он не гарантирует результат поэтому мы решили посмотреть как бы дальше и зацепить немножко уровень выше то есть попробовать реализовать решение на стыке конечного сервиса инфраструктурного слоя Для этого нам подходит в принципе механизм Breaker первым приближении хотя бы да и для его внедрения нам потребовалось бы научить конечные сервисы вести себя более менее предсказуемо то есть они должны самостоятельно понимать что проблема появилась и переходить в режим отдачи определенного ответа формата ответа вот этот ответ замечал бы узел в котором есть поддержка паттернбрейкер и соответственно временно размыкал цепь и изолировал проблемное приложение как бы на момент пока с ним не проведут какие-то работы либо он там сам него становится В общем Но в этой реализации собственно кроется те же самые недостатки в предыдущем варианте если у нас уже имеется флот целое приложение готовых дата необходимо вот каждый будет обучить вот понимать что у него проблемы переходить в режим от датчик года ответы и так далее В общем это выглядит долго опять же и Кроме того скорее всего снова не гарантирует защиту если проблемы где-то до сервиса возникают Вот кроме того можно попробовать это отреагировать масштабированием то есть отмасштабировать Можем в случае необходимости например конечный инстанции конечных приложений или соединений доступных этим приложением но с высокой вероятностью обусловлено такой подход будет только применим Да только в том случае если причина этих перегрузок связана с возрастающей входящей нагрузкой на приложение вот если причины связаны именно с какой-то деградацией связанной с ошибками внутренней логики или еще чем-то таким то есть например там перебоис доступностью к источнику данных или источник данных перегружен В общем какие-то такие проблемы то скалирование здесь не особо подойдет скорее всего вот поэтому пойдем дальше поднимемся уже полноценный на инфраструктурный уровень то есть на уровень балансировщика который стоит перед приложениями и попробуем залимитировать исходящие соединения в сторону приложений с балансировщика и как минимум это выглядит универсальным универсальным таким вариантом реализации вот этого лимитирования По крайней мере она будет унифицирована не нужно будет подбирать там определенные способы реализации под каждую платформу под каждой особенностью конечные приложения и так далее В общем с точки зрения реализации это будет единообразно Это уже хорошо Это значит что как минимум это будет гораздо менее трудозатратно Вот и мы изначально пошли Именно таким путем потому что первоочередной посыл у нас был что не допустить рецидивы как можно быстрее проблему решить значит для того чтобы это сделать у нас было уже все необходимое нам оставалось только привести ряд подготовительных мероприятий как-то снять статистику по утилизации текущей утилизации коннекта в сторону приложений дальше нам необходимо было посчитать лимиты которые мы планируем выставить на основании вот этой статистики здесь нужно быть аккуратным в том плане что усреднение в лоб может не подойти для некоторых приложений например существует какие-то значительные спады ночной нагрузки и соответственно не слишком сильно усредняют наши лимиты каких-то случаях приложений бывает особенность в том что происходит пивные резкие всплески нагрузки В общем это тоже необходимо учитывать поэтому здесь надо максимально как бы потратить время и учесть все особенности мы это сделали на всякий случай провели естественно все итерации там нагрузочного тестирования проверили что это все работает на всякий случай 25-процентный запас и внедрили подсчитанный лимиты И что же мы увидели что этот подход действительно решил нашу исходную проблему то есть принципе уже Победа он исключил ситуацию когда проблема с одним конкретным вот конечным приложением могла привести к недоступности всего контуру то что все коннекты замотаются на это приложение и как бы все остальное отваливается в плане доступности для потребителей вот этого больше не происходило то есть контролируемость аварийных ситуаций заметно улучшилось это раз и второе это то что мониторинг начал показывать уже в принципе источник проблемы это тоже большое дело потому что глобально красным уже ничего не закрашивалось показывалось либо конкретный сервис который страдает либо группы этих сервисов и по ним уже найти виновника собственно было гораздо проще чем это раньше когда вот ты приходишь у тебя всё всё красное Вот Но пожив с этим решением какое-то время мы все-таки начали замечать его недостатки первым таким недостатком для нас стало то что он работает не избирательно сервис конечный как правило реализует несколько функций при этом проблемы с производительностью могут затрагивать определенные и что происходило в случае проблемы с этим API то есть пул выделенный на все приложения забивался вот этим вот проблемным апе и Кроме того он составлял например бы какой-то незначительный процент от общего трафика приложения и что мы имеем на выходе что вот основной поток запросов который идет в приложение Он точно также попадает под ограничения потому что весь пул забивается вот это вот не самой популярной но страдающие функции это есть хорошо и вторым второй проблемой с которой мы столкнулись это сложность собственно в поддержке не то чтобы это было что-то новое Ну не по крайней мере мы какой-то отчасти это этого ожидали потому что все-таки вариативность наших сервисов и вот эти вот количество лимитов которые мы рассчитывали она достаточно большое было Вот Но тем не менее после того как мы согласовались рассчитались внедрились в этот же день буквально мы поняли что в Контры постоянно происходят какие-то работы то есть архитектурная трансформация одних сервисов Я не знаю масштабирование других сервисов и все это приводит к постоянному и достаточно бодрому изменению профиля утилизации соединений которые мы так долго Считали и так вот выверяли Вот и оказалось что нам его нужно постоянно теперь корректировать мы постоянно вынуждены были мониторить насколько актуально мы все сейчас работает посчитано и так далее Нужно ли какие-то носить правки и мы тут попросту увязли и поняли что Мы оказались в таком цикле и в этом цикле мы всегда были догоняющими мы как бы считали текущие текущей утилизацию проверяли насколько она валидна как-то ее постоянно правили и так далее В общем опыт который мы вынесли На этом этапе такой что все что мы пытались применить в плане лимитов это все были попытки применить статический лимиты к живой системе и это как бы возможность сделать Но дальше это будет достаточно сложно все поддерживать и содержать и на этом этапе мы поняли что нам нужен свой механизм лимитирования который позволит нам выйти из этого порочного Круга и мы собственно реализовали механизм динамического тропинка требования к работе механизмы ставили достаточно простые первый должен работать полностью автоматически от расчета лимитов до их корректировки и второе он должен работать с точностью до функции а не приложения и далее расскажу о том как мы достигли этих требований на примере нашей реализации но собственно зная подход это можно будет повторить уже в своих решениях наше решение представляет собой модуль для X его Задача в реальном времени считать лимиты доступных соединений при необходимости завязать поток входящих запросов работает он при этом по ключам нашем случае ключом выступает метод плюс функция но он может произвольным образом настраиваться ключ в зависимости от каких-то потребностей условно работу модуля можно разделить на три фазы это фаза сбора статистики фазовое явление отклонений по данным полученной статистики И фаза реакции на выявленное отклонение начнем с фазы статистики в качестве статистических данных у нас используется Скользящие средние скользящей средней это такой популярный и широко применяемый инструмент технического анализа и по-простому это результат усреднения данных за определенный интервал вот мы их используем для определения тренда изменений значений и для примера на слайде приведен график времени выполнения запросов условный и добавляемая на график кривая это как раз Скользящая по 5 интервалам Скользящие позволяет нам усреднить исходные данные за счет чего более наглядно показать тренд изменения этих значений и сглаженность при построении скользящей зависит от количества интервалов которые берутся ее расчет чем больше интервалов Мы берем в расчет скользящий тем более сглажено получается результирующий кривая вот для примера Скользящие по 15 интервалы Мы видим что она гораздо более сглажена относительно скользящей по 5 тем более относительно сырых данных теории мы разобрались возвращаемся к работе собственно механизм непосредственно накопление статистики у нас ведется по двум типам данных это летенси собственно это время выполнения функции или ключа функции в нашем случае поэтому буду говорить функции и собственно connections Это количество соединений которые были задействованы в для обработки этой функции по каждому типу данных у нас строится по две скользящих средних быстрые Скользящие средние строятся за Малое количество интервалов и Малое количество интервалов делает их более чувствительными к изменениям И поэтому они отражает текущие состоянии функции условно Да и медленно и Скользящие средние они строятся по большему набору интервалов и больше набор интервалов делает их менее чувствительными к изменениям и поэтому мы берем в качестве некого референсного состояния функции Вот и собственно в нашем случае длина интервала она настраивается мы взяли минутный интервал и дальше интересный момент с тем как подобрать количество интервалов для быстрой медленной скользящей это такой вопрос как бы интересный В общем количество интервалов входящих в расчет быстрой и медленных скользящих напрямую зависит от того какой их надо подбирать смысл здесь в том что нужно соблюсти баланс быстрой сработки механизма с одной стороны быстрой реакции на то что система начинает ухудшаться и вот здесь срабатывает количество интервалов которые мы берем расчет быстро и скользящий то есть Нам нужно подобрать такой набор интервалов чтобы усреднение не было слишком слишком сильным мы должны видеть изменения текущие и при этом мы должны взять такое количество интервалов для медленной скользящей чтобы мы могли получить валидные референсные состояния какое-то да то есть чтобы текущие изменения сильно не влияли на медленную скользящую И тем самым то есть усреднение должно быть не слишком сильным не слишком как бы незначительно наоборот В общем нашем случае мы подобрали Вот это рацион что оптимальным вариантом будет Вот минутный интервал и соотношение количества интервалов которые берется в расчет быстрые медленно должно быть 1 к 6 То есть мы берем В итоге три интервала для быстрой скользящей которая показывает текущее состояние и 18 интервалов для медленно скользящей которая показывает референсное состояние пока Скользящие не сформируется соответственно дальнейшая логика работы механизма по данному ключу никак не ведется и для примера вот на сайте приведен правый ключ по которому как бы полностью сформированные Скользящие по ним уже может отрабатывать дальнейшие фазы там выявление отклонений и реакции по наоборот по правому плечу соответственно Мы видим что быстрый Скользящие заполнены но медленно и Скользящие заполнены только на 4 интервала из 18 и соответственно ждать наполнение статистики нам еще 14 минут нужно будет Вот Скользящие собрались Давайте попробуем визуализировать то что у нас получилось но визуализировать будем Не сами Скользящие а их отношения Вот и отношения к скользящим по коннектам это по сути характеристик производительности запросов То есть можно к этому относиться так вот соответственно Фаст нашем случае будет отношением вас лет connections и будет символизировать характеристику производительности запросов на сейчас и слову это характеристика производительности запросов некая референсной в прошлом и вот эти значения мы как бы будем в дальнейшем уже между собой сравнивать и понимать насколько у нас отклоняется текущая производительность от нормального ее состояния вот на графике это будет выглядеть так и мы видим что значение фастты слова расположен достаточно близко друг от друга В некоторых случаях фаз должна пересекает слов Что является нормой так как фаст он меньше усредняется соответственно какие-то периодические выбросы в нём Ну могут находиться даже когда проблемы Как таковые нету Вот и сравнить их вот в таком виде будет трудно Поэтому нам нужно придумать какой-то допустимый рукав вот этих отклонений текущего состояния от номинального и чтобы получить к слову мы применим некий повышающий коэффициент условно Берс и вот этот повышающий коэффициент даст нам действительно реальный сигнальный порог превышение которого уже будет говорить о том что проблема появилась вот на графике это будет выглядеть следующим образом то есть технически мы просто слово подняли назначение этого коэффициента и что нам это дает это нам дает то что вот текущие незначительные флуктуации вот Фаста Они уже не пересекают этот сигнальный порог и произойти это может как я говорил только в случае какого-то серьезного отклонения Вот операционно мы пришли к тому что значение берста мы для себя выставляем в районе 200 процентов это дает оптимальный баланс между скоростью срабатывания и точностью срабатывания с одной стороны нужно будет подумать чтобы мы не пропускали те моменты деградации которые являются уже критичными с другой стороны нам нужно исключить какие-то ложные срабатывания Поэтому вот этот параметр он очень важный так же как и количество периодов которые мы берем скользящую Вот и здесь нужно будет посмотреть в плане м-м тестирование как какой параметр подобрать какой подходит И как ведет себя система экономического троттлинга в этом случае вот далее мы переходим к фазе выявления отклонений фаза это достаточно короткая В общем ее задача сводить к тому что постоянно как бы проверяется как бы нормально ли текущее состояние относительно предыдущих каких-то периодов относительно нормального состояния и мы видим что на слайде фаст начинает расти и в конечном итоге пересекает сигнальный порог это соответственно потенциально первый Триггер того что проблема началась и далее мы переходим к фазе реакции фаза реакции значит начинается того что нас слайд у нас добавляется график по соединениям с верхним все понятно это текущее и номинальное состояние функции то есть характеристика производительности по Нижнему графику это детализация по сетевым соединениям соответственно фиолетовая линия Это количество текущих активных соединений которые используются для обработки функции и красная это некая расчетное значение которое модуль получает и который он считает неким предельным значением допустимым для данного ключа вот как именно считается посмотрим чуть попозже Вот но контроль доступных соединений это собственно первый шаг фаза реакции по порядку пройдемся по вот хронологии работы по ключу этому то есть это фаза когда все хорошо Мы видим что Фаст находится под сигнальным порогом То есть как бы текущее состояние в принципе укладывается в допустимые рукав относительно более раннего состояния этого ключа и по Нижнему графику Мы видим что активное соединение находится под расчётными то есть тоже все хорошо вот момент когда возникает проблема у нас Фаст текущее состояние Начинает отклоняться она начинает расти в конечном итоге пересекает такие сигнальные порог Это первый Триггер и далее мы проверяем что у нас соединениями происходит активное соединение начинает расти потому что первые запросы которые проскочили в приложению они собственно и он в этот момент начинает деградировать по производительности они зависают в ожиданиях ответов эти соединения И вот собственно они дают Этот рост при этом мы видим что график доступ соединений которые рассчитывает модули он реагирует обратным образом он уменьшается почему это происходит потому что активное соединение считается как отношения нормального фона соединений к отклонению скорости выполнения запроса если говорить более простым языком то формула это реализует принцип Во сколько раз запрос замедлился на столько раз необходимо сократить доступное ему количество соединений Вот и собственно урезание вот этих вот активных соединений до уровня расчетных Это следующий шаг фазы реакции которая называется троттлинг и что на нем происходит Как работает Если верхний график текущее состояние отклонилась от номинального первый маркер второе если текущие соединения активные превышает расчетные соединения второй маркер то необходимо уже непосредственно вмешиваться в порядок обработки запросов и запросы перестают пропускаться непосредственно к приложению они начинают предварительно ставиться в очередь и у этой очереди два параметра есть то есть это Время ожидания которое предельное Время ожидания в этой очереди и количество слотов которые доступны в этой очереди вот выход из очереди возможен только в случае если приложение конечные отвечает пользователю на предыдущий какой-то запрос в этом случае Коннект освобождается в него собственно достается какой-то условный запрос из очереди Если до наступления тайм-аута нахождение в очереди коннектов таких не появляется то соответственно запрос из очереди отбивается и также он будет отбиваться если на входе все доступные слоты в этой очереди уже заняты и Соответственно в этом случае будут происходить моментальном графике Мы видим что спустя минуту график активных соединений начинает падать он заходит под расчетные соединения и собственно это и есть результат работы троттлинга то есть количество приложению В итоге запросы значительной степени сокращаются вот также на период действия троттлинга мы замораживаем расчет и актуализацию статистики по этому ключу потому что Даже те запросы которые в Back and попадает с высокой долей вероятности мы по крайней мере так считаем что высокой долей вероятности они имеют нецелевое время выполнения и соответственно для того чтобы не портить быструю статистику мы я замораживает момент когда производительность работы приложения восстанавливается Фаст в этом случае нырнет опять под сигнальный порог троттлинг выключается активный расчетные соединения возрастают соответственно активные также становятся под расчетными и статистика размораживается начинает пересчитываться в штатном режиме отдельно нужно сказать что в расчет статистики Мы берем коды ответов до 300 и 504 остальные куда ответов мы считаем что по ним достаточно сложно сказать целевая сейчас производительность ответы или нет Поэтому вот эти запросы по которым принципе такой прогноз можно как бы сформулировать какой-то вот значит здесь приведена информация по ключу которой мы храним эта информация мы можем получить через Apple для каких-то диагностических наших целей Либо мы их используем для забора и формирования витрингов последующем из этих данных то есть мы строим витрин мониторинга по в разных разрезах и соответственно изначально мы их используют для того чтобы оценивать насколько корректно работает механизм динамической троттлинга Все ли правильно там с точки зрения вот этих пересечений там Фаст Слоу какой Берс нужно и так далее но как бы убедившись что все работает штатно и хорошо дальнейшем мы начали использовать его качестве источника данных для поиска каких-то системных проблем с определенными функциями то есть мы смотрим как бы если какая-то какой-то паттерн там Поведение сработки троттлинга по определенным функциям от чего это зависит В общем достаточно интересные данные и среди них у нас есть Собственно сам ключ флаг активности работы механизмом троттлинга по этому ключу количество отклонённых запросов количество запросов поставленных в очередь это всё за последние интервал за текущий который расчётный и статическое значение размеры это очереди Кроме того здесь есть ряд из значит рассчитанных текущих состояний по ключу и состояние по ключу и Кроме того исходных данных на которых эти значения строились То есть это сами четыре Скользящие то есть две быстрых по лэттенсии connections и 2 медленные по соответственно Lightning вот дальнейшем далее предлагаю сравнить результат работы динамического троттлинга и статических порогов которые действовали то их внедрение и сделаем это на примере реальной системной проблемы то есть на слайде приведены функции условного приложения которые Ну и их распределение по частоте вызовов Вот одна из этих функций периодически проседает в обслуживании в производительности и причины этих просадок не так важны для рассказа но на примере этой функции мы сможем посмотреть как бы результирующие от работы того и старого и актуального подхода к организации этой защиты начнем со статических лимитов вот та самая функция составляет 15 процентов от всех обращений но мы видим что момент когда с ней возникает проблема то есть она просаживается по производительности это приводило к 83 процентной недоступности конечного приложения то есть 502 ответа отдавался практически по всем входящим вызовам к приложению так как вот лимит выделены лимит коннектов был утилизирован полностью Вот это 15-процентной функции Вот это уже после того как мы попытались искусственно завысить эти пороги на инженексе в сторону приложения вот при достаточно значительных рейтах этих запросов такое расширение оно как бы не имеет особого смысла и вот собственно результат после переключения на динамический троттлинг То есть это та же функция тот же тот же бэкен та же функция и мы видим что доступность в этом случае кардинально отличается из условно двух тысяч вызовов по проблемной функции затролилась из них около двух тысяч запросов при этом остальные запросы вообще никак не затрагивались и тут мы видим что При динамическом подходе с изоляцией на уровне конечные функции нам удалось снизить недоступность конечных сервисов 83 до 14 процентов относительно изоляции на уровне приложений и как бы безусловно нужно сказать что это достаточно такой удачный подобрал кейс для демонстрации все-таки обычная функции страдают не по одной как-то группами но тем не менее но Факт есть факт что вот глобальные отказы в обслуживании вызваны вот исходной проблемы когда одно приложение наматывало на себя все коннекты остальным просто не хватало места мы решили это уже внедрением статических лимитов но внедрение динамического подхода оно как бы значительной степени снизило процент недоступности конечных приложений это тоже прям бекзил вот и промежуточным для себя сделали Вывод что в принципе вот это вот работа с изоляцией на уровне приложения это уже сейчас скорее такой антипатан и вот эти цифры это подтверждает то есть Работать нужно с изоляцией конечных функций далее Предлагаю перейти к выводам и первое как бы закрепим то что прозвучало сегодня первая собственно оно же главное что конкретные проблемы с вот конечными сервисами какими маленькими они ни были могут и будут влиять на глобальную доступность Поэтому вот организовать какие-то механизмы защиты необходимо заранее и продумать этот вопрос при этом статический лимиты на любом уровне не важно уровень приложения это инфраструктурный уровень какой угодно Неважно где это в любом случае лучше чем ничего но если мы хотим поддерживать эти лимиты Далее в актуальном состоянии то есть не просто выставить и забыть поддерживать их в актуальном состоянии как-то учитывать изменяющиеся профиль нагрузки и так далее Это скорее всего будет такая так уже задача которая ну либо потребует больших ресурсов на сопровождение Либо это будет утопии как вот это было в нашем случае потому что это слишком большое трудозатратный достаточно скажем так процесс при этом динамический подход он позволил нам снизить критичность аварий с одной стороны и с другой стороны он полностью автономно работает он сам подстраивается под текущие изменения в Промышленной среде он не требует никаких кроме первичных настроек он не требует больше никаких действий от инженеры эксплуатации Вот и соответственно он срабатывает более чувствительной относительно завышенных порогов статических потому что у него пороги всегда актуальные и он срабатывает более точечно за счет того что работает приложение по функции Ну и Кроме того мы получили новый такой Пласт статистических данных которые мы применяем для поиска системных проблем то есть нам подсвечивается конкретная функция подсвечивается моменты когда она начинает деградировать производительность и мы можем по всяким там разным паттерном временным или еще каким-то или там привязанность к определенным релизом понять что причиной подобных просадок вот ну от меня наверное в завершении что не допускайте глобальных отказов в обслуживании Надеюсь что наш опыт был полезен Спасибо за внимание Антон Спасибо тебе за доклад Я даже знаю чем займу сегодня сегодня вечером Это тебе подарок от организаторов как конференции у нас уже куча вопросов Давайте порядке очереди мы динамически а статически вы выбирать Да давать Добрый день спасибо за доклад не могли бы снять маску просто ничего не слышно Добрый день спасибо за доклад Ну я так понял там по моему не говорили что у вас источник данных То есть это там Логин джинсы грубо говоря да нет мы в контексте обработки запроса То есть это инженекс и в контексте обработки запросов при обработке запросов это фаза там ну неважно то есть источник Это не приложение вопрос собственно вы там сказали что у вас есть Q то есть создается впечатление что вы часть проблему унесли Ну вот в конфигурацию этого Бакета в котором у вас копятся запросы то есть вот там было сказано что есть кеса есть тайм-аут в этом месте как управляется Но это конфигурационное значение То есть у нас можно задать размер этой очереди и соответственно предельно время нахождения в ней и соответственно вот задача этой очереди это как раз исключение прямого проектирования приложения то есть мы его разгружаем настоящий момент Вот соответственно все запросы попадают в некий буфер предварительный и Вот выходит из него только в случае если приложение начинает как бы адекватное время отвечать либо просто отвечать И вот запросы напрямую в него не проксируются тем самым мы по сути контролируем пропускную способность к нему Вот и очередь это ну как бы с точки зрения реализации вопрос или в чем в коллеги если выступаем текущую я Предлагаю перейти в зону Key просто без микрофона участник записи нас не услышат буду смотреть странную говорящую голову Как говорит сама собой а у нас есть вопрос да спасибо за доклад можете отсчет пропустил а вот про настройку соответственно всего этого то есть там фигурировали ключи было три икса просто это достаточно частная проблема при настройке динамических путей соответственно Каким образом то есть нужно самому прописать соответственно ключ или он как-то автоматически вычисляется на основании чего тогда он автоматически вычисляется на основании условно некого регулярных выражения по которым вот эти вот уникальные там идентификаторы всякие штуки из запросов закусываются и у нас джинсы есть реестр Ну скажем так у нас это называется мактуре вот то есть реестрри которые являются уникальными которые можно группировать вот они у нас используются для мониторинга и безопасности используем как ключи Да спасибо за доклад Вопрос такой вот начали поднялся такой вопрос то есть я так понимаю это временное решение для того чтобы прикрыть какой-то высокой нагруженный сервис То есть это а потом уже когда разбираетесь именно с приложениями все решайте этот сервис убирается или это он на постоянке Работает Он работает постоянно но отчасти как бы это верно то есть это временно это возможность дать время инженерам подключиться решить проблему с конечным сервисом если они это делают то производительность восстанавливается и как бы он дальше нормально работает троттлинг работает при этом постоянно то есть вот он будет гасить запросы к этому сервису до тех пор пока его производительность нет Спасибо И у нас следующий вопрос Спасибо за доклад вся эта тема очень похожа на анализ временных рядов и хотелось бы спросить А будет ли возможность расширять вычислимые метрики то здесь это просто две Скользящие средние которые там пробитие не пробитие но очевидно это там для 99 может быть работает для одного процента можно было бы расширить каким-то другими сторонними самописными плагинами они бы считали Это для конкретных ручек но безусловно можно было бы Но в нашем случае наш реализация не подразумевает этого у нас технически сравнивать не сами Скользящие их отношения Вот то есть мы взяли отношение Light and connections как для себя определить как характеристику производительности вызова Ну и собственно и сравниваю то есть мы сравниваем текущее состояние с референсным здесь не ставил задачи сделать это вот для того чтобы еще для одного процента как-то это более усложнить эту формулу либо какие-то дополнительные условия вести в общем в нашем случае надо было сделать быстро поэтому мы сделали вот так покрывает 99% кейсов и вот сейчас не возникло потребности вот в том что ещё один процент дополнительно покрытие как-то но мне кажется это отличная тема чтобы развить это в отдельном продукты поделиться сообществом основу нам сегодня рассказал и у нас следующий вопрос Добрый день спасибо за доклад а Вопрос немножко в сторону а Как ваша эксплуатация реагирует на включение троттлинга Какие алерты валятся В какой момент Они подключаются Ну то есть хорошо вопрос Это сейчас реагирует в принципе как то что возникла какая-то проблема с конечным приложением до этого у нас Была договоренность что Когда мы это дело все обкатывали вроде они реагировали на то что приклеился вот они реагировали на то что ну собственно нужно было верифицировать корректные сработки когда мы это прошли сейчас по сути у нас на есть определенный код ответа мы отдаем специальный какой-то ответы 509 он говорит о том что это конкретно работать динамический троттлинг если количество таких кодов ответа относительно общего потока запросов какое-то значительно это создается автоавария и к решению собственно подключается уже который видит функцию по функции определяет конкретное приложение и подключает инженеров можно еще небольшой вопрос даже если включается часто Ну на чуть-чуть он не генерит автоаварию является ли это как в каком-то основанием заведения проблемы на команду это делается Вот не в таком не в аварийном режиме когда прямо дежурная смена видит это все на больших мониторах это команда эксплуатации которая отвечает уже за этот самый инженекс она в принципе там на регулярной основе смотрит вот что троттлинг да например как-то вот включается кратковременно это тоже потенциальная проблема И мы разбираемся сначала на своей стороне вот возможно это неправильно выставленные бёрст и у нас эти сработки например ложные вот это как раз был на начальном этапе когда мы подкручивали вот бёрст чтобы исключить ложные срабатывает Ну мне кажется это отличная тема для кулуаров и когда мы сможем через это обсудить и у нас есть время на ещё один вопрос вот там на задних рядах Да спасибо я правильно понимаю что вот это некий модуль который в инженец написан это некое приложение которое в памяти хранит еще вот эти очереди они никуда не перситься и нет для того чтобы это сделать вот я не знаю как-то распределенно так чтобы инстансы могли делить Это между собой или держать ассистентно такой задачи тоже не стояла потому что период Вот это накопления статистики у нас составляет 18 минут нашем случае в нашем конфигурации мы посчитали что реализация Какого президента хранилища или распределенного хранилища чтобы все ноды между собой делились это особо нам не требуется поэтому такой реализация Спасибо В таком случае вопрос как раз надежности туллинга Да вы сказали что во время разбора инцидента вы смотрите оцениваете действительно или же сам какой-то вот Наверняка у вас есть какие-то наработки да как вообще вы оцениваете Да вот где где произошел сбой то есть сама Тула неправильно посчитала метрики выставила порог да Или же сам бы кэнс будет да У нас вот Собирает все данные вот которые по динамическую тропингу по ключу в принципе это исчерпывающие данные для того чтобы понять По какой причине он принял решение например включить троттливый Да по запросу то есть вот эти все Скользящие расчетные значения они все у нас есть все они выводятся в мониторинг и изначально мы когда проверяли корректность работники мы как раз выводили данные вот сырые данные в мониторинг не просто смотрели что функция и количество 1500 например затратных вызовов А мы смотрели Почему они затролились То есть как бы выводили значение смотрели в какой момент Они пересекаются в общем это все изменить"
}