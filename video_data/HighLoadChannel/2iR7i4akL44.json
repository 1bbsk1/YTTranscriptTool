{
  "video_id": "2iR7i4akL44",
  "channel": "HighLoadChannel",
  "title": "Обфускация баз данных / Алексей Миловидов (Яндекс)",
  "views": 3350,
  "duration": 2938,
  "published": "2020-04-27T12:05:29-07:00",
  "text": "и так что мы знаем про с какао снята вот у меня там футболки написано работает быстро всем известно что работает быстро все вы это и так знаю а как мы можем иметь наглость об этом вам так рассказывать и постоянно об этом говорить как мы можем быть в этом уверен и разумеется нам для этого нужны тесты производительности take a slight и включит кто-нибудь папа яркое уст-во так могу рассказывать но люди должны тоже видеть итак нам нужны тесты производительности и конечно тесты производительности у нас есть и они были у нас еще с 2013 года задолго до того как ли house был о комфорте тогда клик house в основном использовался для нашего сервиса вам аналитики яндекс метрика и за основу для тестов производительности мы тоже взяли данные яндекс метрики просто взяли из наших таблиц выгрузили уа наконец-то и про мы взяли нашу produces данные случайный набор из 1 миллиарда просмотров страниц пользователя яндекс метрики на основе этих данных также потестирую до другие системы потому что тогда 2013 году знаете мы были еще не уверена в том что при halls лучший сейчас вы в этом уверены вот и на основе тех же самых данных взяли небольшую сумку и некоторые функциональные тесты тоже ставил а не гонять вот примерно как это выглядит это все видео на нашем сайте все нормально заострять внимание не будем но проблема в том что эти тесты на закрытых данных и шестнадцатом году кликов стал окна source системой и стало как-то неудобно что мы все время говорил что мы значит лучше и быстрее быстрее обрабатываем запросы но эти тесты никто снаружи не может воспроизвести значит что с этим делать ну и несколько других проблем в том что мы хотим не только какой-то фиксированный набор запросов сюда этих тестов использовать но еще например добавить векторы изолированные тесты на регрессии определенных деталей системы и мы хотим чтобы их добавляем не только мы но и внешние contributors что в любой внешней человек мог взять некоторые данные желает вас продакшена и проверить именно свою часть функциональности на них и что в принципе могли делать один вариант это просто взять и выкинуть все эти старые тесты которые у нас были ну это конечно можно но обидно как-то было до можно было бы место них взять известные бенчмарки типа теперь сидеть в беседе с взять открытые набора данных как например данные по перелетам соединенных штатах открыты данные по поездка на такси в нью-йорке тоже замечательное я могу сказать что на основе этих данных то что у нас тоже есть а не указано прямо в документации если что заходите можете посмотреть но проблема в том что мы хотим в том числе проверять как наша система работает на нашей production нагрузки и следить за регрессиями именно на тех сценах которые нас интересуют то есть нам для тестов нужны настоящие данные из яндекс метрики и конечно не хотелось бы все эти накопившиеся тесты переписывать а почему вообще важно использовать данные с продакшена ну скажем один пример позвать ее данных если взять какой-нибудь сгенерирован этот asset наверняка там поданы сгенерированы просто равномерно случайно в этом случае данные сжиматься не будут совсем и такой фактор как знать о данных полностью выходит из рассмотрения этими дать парками производительности но для аналитических sbd звать а данных это одно из наверно одно из основных свойств которые влияют на производительность и более того сжатие данных нет очевидного решения непонятно как лучше потому что это всегда компромисс между использованием процессора и объемом данных на диске ясно интересно право знать о данных есть мой доклад с хайло цубер в прошлом году можно будет посмотреть то есть если мы хотим сделать искусственное тестовые данные мы как-то должны добиться чтобы они сжимались примерно также как настоящие второй пример скажем нас интересует производительность на вот таком простом запросе запрос грубое мы делаем грубой по регионам считаем количество уникальных посетителей приближённо типичный запрос в яндекс метрике хотя на самом деле там все запросы по сложнее но это такая выжимка что нужно от чего зависит производительность этого запроса и во первых конечно зависит от того как считается количество уникальных посетителей какая то внутри структура данных во-вторых зависит от того сколько разных включая агрегации сколько разных регионов зависит от того сколько оперативки мы будем тратить на каждое состояние агрегатных функций uniq и более того для регионов нам также важно иметь в виду то что данные по ним распределены неравномерно то есть тестом москва санкт-петербург наверху а потом еще много-много других регионов для которых может быть в результате будет там один посетитель два посетителя то есть это самая агрегатная функция виник должна адаптироваться зависимости от количества данных во многих системах просто берут и используется до приближенного расчета которого клок структуру вот все знают про копиру лак лак или не все поднимите пожалуйста руки ну вот если просто взять от структуру данных она будет использовать фиксированное количество памяти ну например там 10 кило 512 килобайт например для всех регионов даже для самых маленьких и так мы получим гарантированно низкую производительность потому что тут важно что у маленькие состояния там счет идет уже на единицы байт так что если мы хотим взять тестовые данные хорошее в них должны быть те же самые свойства данных то же самое количество уникальных значений и то же самое взаимное распределение количества уникальных значений может быть не очень удобно говорить но я называю свойствам cardinal ность а когда у нас там зависимость координально sti количество посетите от региона это будет называться взаимную cardinal ность конечно если мы получим какие-то данные точно такими же распределениями как настоящие то это несколько противоречит анонимизации данных ну например мы да мы изменили изменили все идентификаторы посетителей все регионы изменили и как бы там настоящих данных не осталось но полностью сохранились все количество в этом случае я могу из этих искаженных данных восстановить исходное если у меня есть какая-то информация но прекрасно то что мой сосед за прошлый год посетил 7 разных стран и дел просмотр страниц из 7 разных стран я найду всех посетителей которые тоже в искаженных данных преобразованных посетили 7 разных стран их будет не так много и может быть я там легко обнаружить кто из них мой сосед так что получается противоречивые требования и постановка задачи непонятно я еще пример почему важно важно тестировать на настоящих продакшном данных ну например мы тестируем не систем управления базами данных остановить попроще а скажем структура данных хэш-таблица что может быть проще все знают хэш-таблицы все разбираются многие даже писали своих вот но есть проблема и проще всего проиллюстрирует на примере и описать человек говорит он попробовал кажется библиотеку хоп скотч кошмар или просто хоп скотч пап сюда просто fox кого-то такая структура данных попробовал и говорит на простейшем тесте она на 60 процентов быстрее чем хэш-таблиц клик хауса я могу руб постой то что абсолютно не имеет никакого смысла давайте до настоящие данные дал ему настоящие данные ну не те которые из яндекс метрики а овцы руины о том как я их получил скажу потом и он не говорит на должна оказывается действительно в хаосе хэш-таблица от 2 до 5 раз быстрее почему потому что в том тесте он просто вставлял в таблицу идущий подряд числа от одного до n абсолютно бесполезный тест ну почти абсолютно чуть-чуть полезный итак у нас есть задача получите данные для тестирование производительности которые обладают такой же структуры как настоящие продакшен данной и такими же свойствами для тестирования производительности таки же коэффициент сжатия такие же кардинально sti значение такие же относительные кардинально sti всякие свойства вероятностных распределений но при этом анонимизирован и и постановка задач неформальная но формально никто давайте не собирался на доказывать и так уметь с этим работать и есть задача максимум если мы сможем вдруг сделать инструмент такой для анонимизации данных которым сможем пользоваться кто угодно снаружи ну например человек из какой компании говорит на моих данных коль house медленно работает мы ему говорим ну дай нам свои данные мы протестируем и оптимизируем его он говорит нет я не могу дать свои данные коммерческая тайна а моего горла смотрите есть замечательный инструмент запускаете получать такие же данные даете их нам мы тестируем оптимизируя производительность и он на ваших продавцов данных будет работать быстрее вот я со так сделать вообще круто будет так что посмотрим что мы пробовали и на всякий случай замечу что не стоит переоценивать важность этой задача для нас на самом деле этой задачи не было в планах и никто ее делать не собирался единственное главное дело что просто решение как-то вот вдруг придумывается само собой какое-то интересное и в один какой хороший день у меня будет хорошее настроение и много делал который я смогу отложить на потом вот и первая попытка весьма естественно это использовать явные вероятностной модели что это такое для каждого столбца нашей таблице будем примерно угадывать какое семейства вероятностных распределений для него подходит ну например для каких-нибудь идентификаторов посетителями пусть это будет равномерное распределение для чисел уингз а4 а для даты посещения пусть там день недели будет распределен неравномерно и коэффициенты понедельника вторник и так далее будет выписан и вручную значит выбираем семейства распределения по статистике считаем какие у него параметры то есть танк для нормального распределения считаем и от ожидания среднеквадратичного отклонения и после этого до генерация новых данных просто будем разыгрывать случайные величины из этих распределений и так я начал писать такой код и у меня получался получился такой так скажем скрыв на си плюс плюс что я в программе явным образом выписываю этот столбец из такого распределения этот столбец с такого распределения и проблема в том что у наших данных о посещениях веб-сайтов еще тогда в 2013 году было более ста столбцов я вот стал выписывать выписал где-то 10 столбцов и после этого задача вдруг резко перестала быть интересно для меня и я ее бросил и проблема в том что тут не просто выписывая вероятность распределение тут еще нужны всякие эвристики которую читают зависимость между данными скажем один посетитель обычно приходят с небольшого количества api адресов это что значит генерируем идентификатор посетителя а для генерации api адреса использовал хаша а то день в кадр и посетителя и совсем немного случайности чтобы получалось немного и к адресов вот ну короче надоело и больше этой задачей никто доделывать и и не собирался может быть если бы я был более старательным я бы взял и за один день 10 столбцов сделал за другой день еще и в конце концов написал до такой способ но проблема в том что это решая она еще не обобщаем а я нужно самостоятельно исследовать эти данные понимать эстонии и себя представляют в принципе обобщить тоже можно скажем иметь в программе какой-то каталог распределение и выбирать из них автоматически для каждого столбца да и зависимости между столбцами тоже можно посчитать есть статистика относительная intropia с помощью них можно понимать например что домен от орла является однозначной функция цао урла но все равно слишком трудоемко и надоело вот следующий подход который был к этой задаче это использовать нейронные сети извините лучше картинку я не подобрал но наверное от картинка показывает что в принципе нейронной сети могут подойти для опускаться и данных единственная проблема я абсолютно не разбираюсь не в нейронных сетях не в машинном обучении так что я просто дал эту задачу случайному человеку с улицы и тогда на глаза попалась замечательно статья андре горбатого под названием необъяснимая эффективность рекуррентных нейронных сетей кто из вас читал эту статью известно статья андрей карпат во известный между прочим человек ну ладно вот и я дал этот человек задача попробовать человеку которому надо было делать диплом естественно он подписал соглашение о том что он может работать с этими данными получил эти данные и начал пробовать что-то с ними делать вот из особенности применения первая особенность это то что мы должны получать структурированные данные не просто текст хотя текст генерировать намного проще есть уже готовые способы а если кинуть структурируем данные мы просто возьмем таблицу и пусть наш нейронная сеть будет только отдельное значение мне заполнять вот но естественно мы попробовали сначала более простой способ когда мы берем дам скажем формате все асв тренируем на нем нейронную сеть а потом грн такой же дамб и некоторые строчки в нем могут оказаться не валютными просто не загружается в базу такие строчки будем просто пропускать и может быть там 10 процентов например строк загрузится и нас это устроит вот попробовали ну и на первый взгляд качество отличная url и выглядит как настоящее но при этом совсем не настоящие в общем обнадеживающе но есть правда проблемы первая проблема это то что эти нейронные сети тормозят но я это ненавижу 100 строк в секунду правда на машине сцепу на джипе у наверное было бы быстрее ну и к тому же у меня есть подозрение что может быть сам алгоритм в этом не виноват а просто с ним неправильно работаем но так как я сам в этой теме не разбираемся то и определить это не могу из такой скоростью 100 строк секунду человек должны успеть сгенерировать datasette из одного миллиарда строк до защиты диплома и это будет провалена вторая проблема это то что модель получается слишком большая мы попробовали использовать для данных несколько гигабайт размер моделью получился примерно 1 гигабайт или полгигабайта в суд типа того и у меня возник вопрос а вдруг эта модель внутри себя содержит все исходные данные ну или какие-то исходные данные там номера кредитных карт еще там какие-то mail и api адреса а нам это не надо у скорее всего это не так но как я могу быть в этом уверен конечно можно применять всякие интересные техники типа сжатие нейронных сетей но это никто не сделал короче непрактично но не беспокойтесь человек защитил диплом на отлично а результаты просто не использовались и были выкинуты пришлось пробовать чтобы быть дальше еще один очень интересный подход мутация сжатых данных вот рассмотрим нашу задачу нам нужно генерирует данные которые обладают таким же коэффициентом сжатия в той исходные данные как это сделать и есть удивительное решение надо просто взять сжатый файл и редактировать его напрямую без заражать и я тогда размер сжатых данных не поменяется и данные будут развиваться примерно с такой за скоростью да еще и сама преобразование будет работать быстро потому что она будет обрабатывать меньше данных и вот эта удивительная идея как только она появилась я сразу захотел ее реализовать правда проблема в том что она не решает полностью нашу задачу решает какую-то другую задачу но я все равно захотел это реализовать так всегда бывает и так пусть нас интересует алгоритм сжатия или за 4 будем вспоминать как он работает это алгоритм сжатия из семейства elysee 377 сжатый файл представляет собой набор команд одного из двух видов первая команда это скопировать дрожат и файл некоторое количество байт как есть называется литерал а вторая команда это повторить те данные которые были раза ты некоторое время назад на некотором расстоянии например скопировать 15 байт которые были раза ты здесь ну так тоже можно там десять назад или там 20 байт назад и в сжатом файле можно просто команда оставлять как есть но редактировать байты из литералов тогда в развратом файле будет по сути ни одного байта не скопирована из исходных данных очень интересная идея я вот не уверена что все поняли что я имею в виду ну давайте посмотрим что получается получается как-то так вот это вот преобразованной url и видно что повторения сохранились и я еще добавил эвристику чтобы сохранять фрагмент http просто мне так захотелось а еще чтобы сохранить некоторые виды пунктуации но тем ни менее результат выглядит как-то по-дурацки это во первых во вторых из-за того что сохраняются все длины строк и вся пунктуация меня кажется что данные все-таки недостаточно анонимизирован например был у нас варела yandex.ru а в искаженных данных там будет какие-то сколько 6 букв а потом . еще две буквы ну и люди легко пойму что это индекс это утрированно конечно примерно до других данных что-то похожее вот так что я это задачу реализовал посмотрел как работает работает очень интересно но использовать мы это не можем и решение приходится выкинуть вот такая печаль дальше вот что получается мы попробовали три разных метода ни один из них не работает только настроение испортилось а когда настроение испортил с чего остается делается остается просто идти и читать случайной странице в интернете и вот я стал читать случайной странице на одно из них обнаружил вот такое эта игра wolfenstein 3d многие играли многие знают анимация сцена гибели их главного героя экран заполняется кровью путем закрашивания пикселей что это такое на самом деле это случайная перестановка поясню почему и так надо закрасить все пиксели экранах в случайном порядке красным цветом можно было бы это делать так генерирует случайную координату и заполнять его красным цветом герой рот следующее тоже заполнять его так вот тыкать на экран но когда многие пикселем и по закрашивали повторно по второму разу и когда на экране расстался по 1 незакрашенный пиксель мы бы его очень долго закрашивали потому что нам патриот бы много время что в него попасть а здесь мы закрашиваем каждый пиксель ровно по одному разу как это делается очень просто надо просто взять случайную перестановку всех пикселей и в порядке это случайно перестановки закрашивать и есть куча способов это сделать скажем в оригинальной игре в стоит 3d использовался алгоритм под названием сдвигаю регист регистр короче с линейной обратной связью от алгоритма часто используется для генерации случайных чисел в железе но обычно не непосредственно берут несколько таких миллионер фидбэк shift регистров и как-то их комбинирует что это такое как он работает берем какое-нибудь число скажем 64 бита на каждом шаге сдвига миллиона 15 вправо the beat который образовался слева мы его заполняем к ссорам из бит на нескольких фиксированных позициях берем вот так вот несколько позиции которым переносим в начало и сдвигаем и есть утверждение что некоторые виды этих быстро факторы конфигурации даются случайную перестановку правда они число ноль и всегда оставляют как число ноль на месте но это неважно есть и другие варианты скажем если мы число какой-нибудь знак воды множим на нечётное число будет взаимно однозначное преобразования а лучше взять какое-то большое число и лучше не не чётное простое и все будет прекрасно или скажем операция к 40 вт сдвинули куда то потом покорили с исходным числом хорошо перемешивает биты есть вот я стал читать всякие случайной странице в интернете и пока не понял что из этого можно сделать я прочитал не только оригинальную статью про то как происходит за окраска в игре важность этой 3d но прочитал комментарий на hackers news и в комментариях голосуй к на другую статью некоторого сальватора san felipe а а вы знаете с этого сальватора как кого кто такой сова торы да абсолютно верно создатели редиса вот он написал в своем блоге статью в кости от это абсолютно не про редис там слова редис не встречаются ни одного роза он говорит что для того чтобы геймеру случайной перстов kia замечательно обобщенное решение под названием сеть всей сталь а вот тут картинка из википедии абсолютно непонятная поэтому я покажу свою что такое сети стали она позволяет взять любую функцию не обязательно взаимно однозначно скажем можно взять хэш-функцию вообще какую угодно функция и с помощью нехитрого преобразования на ее основе получить другую функцию которая будет взаимно однозначно то есть это просто какое-то чудо берем функцию которая генерирует на основе данных случайный мусор засовывание все тестера и на выходе получаем функцию которая тоже на основе данных query же случайный мусор ну а теперь взаимно однозначно то есть этот случай на мусор можно обратить как она работает берем аргумент разделяем его на две половинки по битам скажем 32 бита левая половинка 32 бита правой а потом на место левой половинке кладем правую а на место правой половинке кладем к сор из левой половинке и функции от правой половинке может быть ничего не понятно но это преобразование один раунд с этих эстер и можно обратить снова берем нашу функцию от левой половинке которым я вас правая половинка и получаем исходное и эти самые сети fest или они являются основой для некоторых алгоритмов шифрования есть утверждение что если мы берем какую-нибудь криптографическую стойкую псевдо случайную функцию и применяем несколько раундов сети fenster окажется 4 то мы получим как фотографического стойку взаимно однозначно функцию то есть шифрование данных и по идее шифрования данных можно было бы использовать в качестве случайной перестановки правда тут есть некоторые технические проблемы связаны в том что чтобы сохранить кардинально наших данных мы должны шифровать их одинаковые данные одинаковым образом то есть был там на право пользователь вася и мы его будем шифровать на про там масса но каждый раз мы будем это делать одинаково образом и у нас была в исходных данных одинаковой арестовать в разутюжку данных будет точно такое же количество маш то есть цифровыми мы как бы используем один и тот же вектора инициализации повторно что является недопустимым недопустимым для задачи шифрования но вполне вас просто в каком-то смысле допустил для нашей задачи вот тут стоит заметить что я не очень хорошо разбираюсь не только машину в обучении но и всех рование данных тоже нужно то остается пока вера отвага так что будем работать и так если брать эти самые случайные перестановки то у нас полностью сохраняются кардинально sti исходных данных полностью сохранится взаимные кардинально sti одни и те же данные если они повторялись в исходных данных они продолжат повторяться на тех же расстояниях то есть данные будут еще и сжиматься скорее всего примерно так же и в принципе я хотел причал сохранить порядок величин то есть тесто были величины величины где-то от 1 до 10 то пусть тоже в результате будет величина от 1 до 10 как это сделать я просто разбиваю данные по классам размеров например по степени 2 от 64 до 127 и дело в случайную перестановку только в пределах этого класса вот и это все подходит для числовых полей а что делать для строковых полей для строковых полей значит нейронные сети рекурентные нам не подошли мы такие капризные может быть подойдет что из более проще это марковские модели ну все знают марковские модели для генерации текста поднимите пожалуйста руки кто знает ну чего там мало а между прочим эти модели печально известны для задача генерации спама вот а если вы ещё не видели как они работы заходите на яндекс и рефераты это старейший сервис даже не cyrus а так прикол какой-то на 1 апреля сделано потом осталось вот что такое марковская модель для задача генерации текста марковская модель порядке я считаю для текста статистику условная вероятность встретить следующей буквы такую-то при условии нескольких н предыдущих букв вот тут записан пример марковской модели порядка 3 она считают вероятности всех букв a b при условии всех возможных комбинаций предыдущих трех букв и такие модели аннигилируют несколько правдоподобное конечно тексты но примитивные смешные бредовые зато с ними всегда весело работать вот пример я преобразовал заголовки страниц и вот получилось если это не читать то выглядят как настоящие а я сочетать то ну короче сами почитаете вот но как использовать эти марковские модели для их использоваться генератор случайных чисел который выбирает каждый следующий символ а я хочу чтобы повторяющиеся значения то есть она прервала точка yandex.ru преобразовалось какое-то другое значение но тоже для яндекса всегда было каким-то фиксированным для этого я просто вместо генератор случайных чисел беру хэш-функцию фишеру исходное значение и вместо генератор случайных чисел для генерации нового то есть одинаковое значение будет преобразоваться в одинаковые а ещё я хочу чтобы значения которые начинались на один и тот же префикс в исходных данных в результате шаг данных тоже начинались на один и тот же префикс например были url и с одинаковыми доменами а потом после дамиана идут параметры они разные но домена одинаковые пусть преобразованных данных тоже будут одинаковые домены и вот тут вот необычный трюк который очень естественный но в то же время весьма красивый мы будем вместо генератора случайных чисел для выбора каждого следующего ула использовать хэш-функцию от плавающего окна из восьми байт в исходных данных здесь это показано на картинке например мы генерируем новый url астат об этом ftp google коза там вот и хотим выбрать следующую букву для этого смотрим в исходный url в исходных данных и над той же позиции берем следующий восемь байт к шуруем их теперь пусть у нас есть вероятностное распределение я выписал это вероятностное распределение наверное видите в нем указано что там буквы а чаще постараются буквой b меня часто я беру остаток отделение хэш-функции но общее количество статистики и как бы sampler ую это вероятностное распределение я определяю что следующая буква будет буквой же посмотрим что получилось вот получилось как то так заголовки страниц там был какой-то сайт типа наш интернет магазин купить бесплатно низкая цена там одежда обувь вот и получилось что там были заголовке страницы начинающейся одинаково и здесь тоже и некоторые вещи даже очень интересно я помню бесплатно в большом ассортименте но это случайно так получилось генератор случайных чисел все честно вот и так что же я получил в итоге вот когда я попробовал это четвертое решение задача надоело мне окончательно и поэтому я стал и и постепенно превращать в результат которого можно пользоваться я взял эти случайные перестановки с помощью сетей все стали с учетом со из классов взял марковские модели которые хэшируются там исходные данные все это собрал вместе его получилось утилиту под названием кликал совхоз катар это утилита доступна в пакете сколько а с клиентом если вы установили краха вас клиент на вашем сервере уже присутствует это волшебная программа и вы можете ее использовать работы то надо termini равана для этого вы указываете какой-то сид случайный ключ который вы должны выбрать а потом можете просто его забыть но ясно его как вы кому не сдадите то в принципе из него можно восстановить большую часть исходных данных подходит эта утилита не только для cliff house а для любых табличных данных то есть вы можете выгрузить на продам ps пост грессов из майскую обработать его с помощью к house of phase кадр и у вас получится анонимизирован ее данные и самое главное работает быстро не тормозит ну быстро относительно данной задачи как примерно пользоваться просто в командной строке задаем сид задаем формат входной и выходной мощности as we see as with jason на каждой строке вообще любой который поддерживается коли хаусом задаем структуру таблицы на вход и дамб и на выходе будет ассоциированный дамп есть документация есть помощь прямо в командной строке можно легко пользоваться в результате нам одобрили руководство одобрила публикацию этих of a струнных данных ну их загрузили теперь вы можете скачать и использовать их для тестирования производительности там на самом деле есть некоторые особенности который ставит вообще этого задача под вопрос но я эти особенности говорить не буду думайте сами результат такой что уже используется в том числе внешними людьми для того чтобы активизировать лучшим образом алгоритмы которые есть у нас и не только у нас и так все спасибо алексей спасибо большое пять минут там сэкономил отжала от себя друзья есть вопросы поднимайте ручки смотрите вот раз пожалуйста пока алексей получают памятные подарки призы и обнимашки волонтеров да слышу меня вот сейчас они обнимать своего лучше вставайте пожалуйста посвятили алексей сейчас нужно как всегда женщина кавычек заняться вопрос такое меня зовут андрей можно ли эти тан и используют для опускаться для раздельной опускаться числовых данных и текстовых например я хочу у меня есть предположим номера банковских карт я хочу оставить их как числа но при этом они будут изменены и есть фио я хочу их оставить как текст это возможно с помощью вашего телета в принципе может нам правда примерах будет чуть меня не удобно нужно получить сначала дамп числовых данных в том же порядке в котором таблица была в каком фиксированном порядке фокусировать их и потом каким-то образом поставить на место исходных данных то есть это два приема будет сначала яффу сырую числовые данные потом мифу сырую текстовые текст воевать вообще не хотели of a стирать почему хочу то есть числа останутся числами секс останется текстом чтобы избавиться от зависимости между ними чтобы невозможно было прочитать исходную информацию но при этом цифры все-таки остались цифрами а текст текстов цифры оставив цифра текст текст она в принципе а то дает это утилита из коробки она сохраняет тексты остается текстами цифры цифрами или вам нужно чтоб какие-то данные вообще не были обфусцированы нет все вкусы равана но не если вы горите цифр останутся цифрами текст exam именно так это утилита и работает она полностью понимает типы данных вы задаете эти типы данных и в рамках их она работает понял спасибо спасибо алексей а тебе удобно поближе подойти да пожалуйста вставайте предложите скажите пожалуйста а можно ли с помощью этой утилиты обфусцированный не под клик house да разумеется она не зависит от база данных вы можете ассоциировать просто любой текстовый танк спасибо на первом предыдущий запрос спасибо вопрос по все-таки по результату вот вы с не четкая постановка сделали какой-то четкий результат вы не давали его скажем crypt аналитиком чтобы они подтвердили его четкость и конкретность что нельзя из него вытащить что-то прямо полива какой-нибудь именно так мы и сделали только мы дали их ненастоящим крипто аналитиком они которому человеку который носит доступ теста я не ошибаюсь директор по данным и вот он посмотрел на это данной и говорит все нормально вот второй ряд раз здрасте подскажите пожалуйста а вопрос изменения данных на лету вставал вопрос избиения данных на лету например в рамках данного работаете с реальные базой данных но при этом когда делается запрос данные на лету изменяется и выдача происходит уже опусти ровно как раз очень интересный вариант и есть несколько похожих направление для применения этой задачи одно из похожих направлений вот как раз просто выговориться чтобы отдавать пользоваться вне что не настоящие хотя это для этого задача меньше спроса чем просто не давать пользователям данной или давать канули скажем 0 и вместо и к адресов а то зачем пользователи запутывать идея в том что в вашем база данных изменяется уж не хотите ампутировать каждый день , если вы хотите дать возможность вашему потенциальному клиенту посмотреть на выдачу реальных данных но при этом но это будут реальные данные но клиенту with a fast и раваны какие-то имена и не увидит реальных людей то есть например там за год назад вся эта адреса превратить вас и руины именно именно на лету во время выдачи результатов данных до очень интересно тут есть несколько соображаем первое соображение это то что в принципе алгоритм работает быстро для задача преобразования данных но для выполнения запросов на ли тут может быть он будет не достаточно быстрым ну растратили быстром но не так как я хотел но я всегда хотел большего спасибо большое спасибо вам большое за доклад вы говорили что для любой из свд может использоваться скажет есть опыт использования 40 вам с тем же никто не пробовал интересно было бы узнать я не в курсе чтобы кто-нибудь пробовал но я не вижу с этим проблем потому что для этой утилиты она просто надо ничего не знает просто берёт текстовый дамп преобразует и готова вы только указываете типы данных типы данных и указывайте такие которые понимают клика us но тут ничего особенного нету то есть там and words are можно написать увлекался хочешь принято писать string а по скорости сколько занимает среднему там нигде не вид допустим там ну примерно там пятьдесят гигов данных вот миллиард просмотров страниц изъян такс метрики там кажется просмотр страниц это где-то килобайт сжатом виде то есть это получается терабайт данных тут заняла где-то полтора дня понятно спасибо далее был сказать что немножко оперативки там требуется для и когда minor тонах для марковских моделей несмотря на то что они на самом деле внутри заглубляются и сглаживаются для подходящий анонимизации здравствуйте и спасибо большое за так вот интересно вот у меня первого сорта перехватили потому что у меня первое впечатление было человек первично не было что вот этот алгоритм которого рассказать что обратим ты что данные можно обратно преобразовать вот и наверное это надо строго проверять а второй вопрос более практически а можно ли с помощью вашего алгоритма задавать какие-то constraint и правило на результат потому что эти данные будут помещаться в таблице которые будут соответствовать например существующему структуру на которых мы уже могут быть наложены конструкты и можно ли задавать зависимость между там соседними ну колонками так скажу насчет первой части по поводу обратимости данных преобразование числовых данных полностью побратима но параметризовано некоторым ключом но этот ключ держится в секрете и алгоритм он с одной стороны как бы крипто стойки на с другой стороны как бы применяется неверно применяется неверно потому что аналогично как бы знаете наверное электроника кого звук молота operational то есть одинаковые данные одинаковым образом пассеруется то есть в принципе не очень хороший применения сейчас я отвечу и к вашему преобразование текстовых полей необратимо но на самом деле почти обратимо там просто некоторые коллеги будут про генерация хэш-функций и в принципе есть и дать кому-то некий ключ которую используется для генерации то подавляющую часть данных можно будет обратить это так вот второй вопрос по поводу constrain дав нам это тоже пришлось учитывать участка артритов уже реализовано скажем те базовые вещи что строки которые валидные в кодировке utf-8 преобразуется в такие же но мы этого добились того что просто когда со старою нашу морковку модель составляемые по кодовым . мне по байтам но с более серьезными к острота их придется явно вписывать в эту утилиту и из коробки они не доступны тем не менее такие вещи там также присутствуют эвристики для сохранения непрерывности временных рядов для этого мы ассоциируем не сами значения дельта между ними и сохранимся из класса то есть с и дельты были маленькими то и результаты тоже будут маленькими что конечно вызывает еще больше вопросов вот теперь ваши друзья ещё два вопроса вот раз и видимо 2 остальную в кулуарах это к вопрос про обратимость тоже а как по вашему мнению по расчетам по тем алгоритма которые есть сколько времени требуется чтобы подобрать хэш ключ с новость например на трёх строках и правдоподобностью вывода по подбору на трёх строках выбранных выдает и например на выходе яндекс или какие-то хаотичные данные и перебирать примерно времени это займет в принципе алгоритм называется крипто стойким если сложности задачи такая же как сложность полного перебора и в основе использоваться некоторые примитивы которые такими свойствами обладают но за счет того что они используются неправильно эти свойства быть нарушено но в основном это достигается только если у вас есть некоторые еще дополнительно и априорные знания данных как например тот пример который я приводил что какой-то пользователь там посетил ровно семь регионов и вы находите всех таких пользователей потом испрашивается теперь я могу посмотреть на какие сайты они заходили но это не так потому что сайты тоже преобразованы каким-то образом это же ассоциированы но теперь вы например знаете что самый популярный сайт в интернете это yandex.ru а второй там допустим google берете самый популярный преобразованный сайт он будет там например губкин . народ в . и рф и знаете о это индекс и теперь вы знаете что человек заходил на яндекс но не знаете в какое время потому что время тоже августе рано вот так по цепочке применяются все правила в принципе вы что-нибудь можете из этих данных понять если ключа получится подобрать толкните может быть все понять место подобрать ключ то в принципе да можно все памяти и конечно эти знания позволяют вам подобрать возможно подобрать ключ там на самом деле гораздо больше проблем у меня есть одно маленькое оправдание оправдание то что во-первых эти данные за 2013 год бог говорит маленький datasette пока что 8 миллионов строк и он выбран никто вопросов случайно как что-то типа 1 50 тысячная из каких то там хитов спасибо спасибо и последний вопрос или вопрос рассосался там уже до рассосался но прекрасно алексей осталось поклониться спасибо большое"
}