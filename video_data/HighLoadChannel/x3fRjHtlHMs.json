{
  "video_id": "x3fRjHtlHMs",
  "channel": "HighLoadChannel",
  "title": "Чем мы смотрим на прод Яндекса, и как это вам поможет / Иван Карев (Яндекс)",
  "views": 726,
  "duration": 2854,
  "published": "2023-01-19T06:56:46-08:00",
  "text": "всем привет да мы сегодня поговорим о том как мы смотрим на продакшен как вам это поможет молекула в целом будет наверное даже больше идейной чем технически там будет и то и другое в целом не хочется в итоге вас заинтересовать и сделать так что вы захотели подобную систему реализовать у себя и в целом мой рассказ и мои слайды использовать в качестве мотивации шпаргалки просто потому что посмотреть какие-то диеты подчеркнуть danish меня зовут иван хорев я на самом деле я не xi работаю кучу лет но к сожалению для тех пор с тех пор как мы пару раз приносили кафу его уже покинул сейчас работы в ока но мысли лишь тогда будет про мой прошло собственный мой опыт я довольно давно пришел в дело главную страницу это был frontend в одну страницу много аудитории нужно было поймать как она работает чтобы через сломался потом через сколько-то лет появился джин я тоже читала самого начала делаете это переулок а это уже другая история с бесконечной ленты тоже куча данных учил victory тогда он был еще маленьким довольно потом был эфир это видео сервис там уже был много ведь рассмотрению параллельно все это время то мне кажется 78 лет я занимался скорости фронтэнда собственным измерением того что как работает где как не тормозит в среднем общин free который мы в итоге сделали себя а потом на весь я на фраскати ли это же был такую довольно большой кусок и там соответствует много данных ну и соответственно последних года три-четыре была история про видео платформу виде усмотрение это и заливка видео хранение раздачи player в общем тоже куча данных и очень много чего непонятного собственно во всех этих вещах нужно было понимать вообще как все работает как его титанах ориентироваться собственно почему рассказ и вообще что я понимаю подработать продакшна а сам не такие довольно очень довольно простые вещи хочется быть уверенным что ничего не сломалось если что-то сломалось во время находите это ну из довольно большого объема данных при этом не жалобы надо что что-то работает медленно ряд тоже нужно уметь находить и и желательно не по звонку пользователя как-то самостоятельно и даже без его привлечения надо если какие то проблемы то опишите проблему хочет иметь находить автоматически и соответственно в полном объеме но на самом деле как во всей этой истории мне всегда было интересно понимать а что вообще работает как это работает что происходит и потому что сервисы большие очень много всего может случиться и вот это вот уверенность чтобы мозги что не сломался на самом деле всегда 20 седан нужно а ну цыц сначала мы перенесемся какой то стоит сколько-то лет назад немного историям расскажу с чем мы жили довольно давно ну тут за какую-то отправную точку для выбрал 16-ый год собственно must во время уже понятно у нас был мониторинг ошибок нас был акебоно собственно я работала она довольно сносно вылетает проблемы в части команд соседей был центре он тоже как ты работал но не то чтобы очень крутые ошибки в наших сервисах монитора с помощью метрики но очень какой-то такой зоопарк и не то что всем было хорошо при этом мы также занимались измерением скорости приборы какие-то были скорее они были такие полов лайнами когда нам в команду статистика делали от по отчеты мы там могли через там задержки в день понять что как работает на самом деле под четкого понимания что все хорошо и уверенности что прям весь а стоит ли стучится про эту измену узнали у нас прям не было конечно облака и ты железной инфора и там все мониторе лась с точки зрения мытарями клиентов было все немножко не так хорошо собственно основные проблемы с чем мы сталкивались и ну никак не кажется проблем такие довольно общее первое что данный было довольно много ну прям очень много и понятно что все наши серы с которым у тебя разворачивали даже довольно большие инсталляции имели этого не выдерживали либо на у нас не тащила соответственно мы делали какие-то срезы данных там запускали там на часть потока хранили данный не за очень много времени нужно было много довольно ограничений при этом если были какие-то необходимость что-то по исследовать по узнавать то ты запросу делаешь а потом можете начать здесь ли 15 ты получишь результат ну на самом деле не крут как путь проблему debarge хочется быстрее делать понятно что у нас это все было командам раскиданы там часть жила в нашей команде там условно у админов часть живет вообще соседних команда статистике где-то быстрее это медленный ночью свернуть какие-то заказы к снежникам не так чтобы все ошибка было удобно с точки зрения появления новых вещей удобством при этом конечно же была проблема со скоростью появления данных но там в то время нас не кажется минут 15 балок в принципе мы не могли по крайней для вот своего со среза данных и быстрее получать ну не очень хочется жить до когда все сломал что-то поэтому значит с 15 минут ну и понятно что если были какие-то сложные вещи можно было решать каким кастомным запросами но в то время у нас там уже было понятным большая-большая напротив системы можно было то что to provide по фильтровать но это опять сложный запрос надо было уметь писать надо было ждать их короче не круто было ну понятно мы хотели все эти проблемы решать это был так узнать нас было даже несколько итераций давайте попробуем сделать какую систему мониторинга ну хотя бы для ошибок но в общем виде это казалось просто невозможным то есть все медленно на маленький объем там не супер круто давайте сделаем быстро на всех и супер удобно то есть прям очень разное состояние ну и вообще целом не было такого понимания что же мы хотим получить как и должна работать то есть какого то вот образа системы четкого не было поэтому чтобы что-то начать делать начать делать довольно простую вещь мы просто начали делать инструменты которые помогали нам в нашей работе там под конкретные действия вот например у нас были дежурстве мы разбирали ошибках фронтэнда тогда давайте сделаем так чтобы этот кейс работал хорошо для него какие-то я вам прям инструмент при этом до было довольно много данных от всего чего угодно не хотелось как-то удобно навигировать то мне не только а чё ты ментам как ты еще в общем сделали сначала какие то смотрел key для того чтобы понимать что в этих данных есть мы соответственно были какие-то кейс когда нужно было найти какую-то одну сессию проблемное и по профилировать но вот решаю эту проблему тоже делали купить инструмент собственно это путь который мы прошли он за нос довольно много это не то что вот сначала ничего не было только через пять лет все стало работать конечно у нас в промежуточные этапы были уже вполне понятные шаги и и система работала просто условным хочу рассказать про то что было в конце и как мы это сделали и чему только пришли в общем в чем наши рецепты и система контактировали бить и мы поняли такую вещь давайте и мы сделаем так чтоб все наши данный быстро появлялись как а уж при этом интерфейс над хаосом тогда нормальных не было и мы поняли что мы хотим сделать удобный интерфейс для того чтобы написать надо над этими данными навигировать такие простые вещи там графиков таблиц чтобы в удобном виде работали этом было пучит сценариев для которых эти простых вещей не хватало поэтому поняли что докажи такого сценария мы сделаем отдельный свой средств свою смотрел q так что было удобно именно его лишать при этом внутри яндекса есть конечно очень фри и мы сделали так чтобы со всей ит-инфраструктуры который нам помогал ему прошел работы наш сервис был очень хорошо интегрированы ну и самого начала мы начали интеграцию со смежными командами то есть на самом деле бог получалось такое что мы какие-то свои кейс решили то пришли к снежникам взяли их кейсы их данные подключили к себе сделал решение для них и так по кругу и не знаю так 10 20 30 раз собственность схема в итоге получилось довольно простая идея так значит у нас есть данные из разных источников мы начали с браузеров понятно что все что было в браузеров и к себе собрали нас но мне там много разных и специфичных данных потом мы поняли что она тоже самое хочется видеть для всех брендов нас много разных сервисов мы почему башев из бэг-энда не собрать . какаши клиент ума потом понял что ничто ничего не мешает делать это и для приложений в принципе очень понятная схема эти данные мы должны прочитать эти данные мы должны раз парсов профильтровать или сложить подобные в виде включался и томск они живут с другой стороны мы сделаем для них и смотрю кнут обычную сервис backend и надя и сколько-то клиентов почему клиентов довольно много они отличались опять же там в зависимости от данных зависимости от кейсов и понятное тут бэкон где-то внутри работу интегрировался хорошо с нашей внутренней форме такая высоко урановой схему брать теперь вот тут часть которой про заливку собственно что для нас на самом деле было всегда камни преткновения пока мы честно сделали все остальное не получилось давайте сначала тут небольшое отступление собственно какие у нас были данные у нас были ошибки все возможные все что можно было собрать любыми способами за браузеров плюс любые кастомные ошибки когда мы хотим сказать что вот было какое-то событие с такими параметрами вот просто его трафик был цсп или и за пять назад его на больше внедряли и соответственно там много данных много специфика много сервисов тоже все эти данные чтобы поднимать ошибки где на фанит хорошо было бы иметь и у в базе с ними удобно разбираться у нас были все данные про скорость их было довольно много довольно разных специфичных и все-все-все мы тоже в этот крутых расположили при этом понятно что у нас только как были веб-сервисы нам хотелось все что касается в файле кпсс ок картинок то есть акции слоги по сути статике сидена туда же положить то есть видит там проблемы с их размером там может volk volk не быть при этом нас в нашей при опять же есть там несколько уровнем балансиров и потому что так получается что какую-то какая-то часть инфы хорошо монитор с 1 приобщим сбалансировано всегда было сложно и дам нашим где во всем всегда там будет с ним работать неудобно но мы решили но давайте молоке балансиров торг себя положен ну а потом я все слуги самого сервиса и все клиентские логе все клики показы с к роду все что можно клиенты дэвид тоже туда положим ну и потом соответственно когда у нас было все это и работа с видео платформой все проведи усмотрение мы туда тоже решили положить ну потому что это удобно собственно сколько таких данных была вот картинку примерно на полгода назад порядка такие больше свой ген real player ну видео смотреть немного плюс там есть периодически события там длинные сессию там пару сотен миллиардов денег подробно это причем были и сами событий player xsl он и плейлисты и чанки были и какие-то логе папа скорость player вот это был такой же самый большой кусок понятно что довольно большая была статикой у нас по сути это вся стать к индексу там все картинки и скрипт и вообще со всего чего угодно мы тоже видели цсп довольно большой неприятный кусок потуги тоже мы туда положили какую-то часть какую-то часть нативных приложений завезли в этом месте понятно что приложение живут там с каким-нибудь там треккинга мы типа от метрики в этом месте это будет такое некоторое дублирование то что отметок она немножко для другого какие-то события было удобно нам иметь у себя соответственно ошибки вообще всего для чего мы могли дотянуться вот у нас примерно 100 миллионов денег случается это все клиенты довольно много брендов часть native ну довольно много штук мы соответственно какое-то количество access of event log off сервисов ну те которые были в нашем вот в нашей окрестности и затянули просто для удобства ну и соответственно там зависимости от сервисов закаленных получалось собственно чтение логов здесь я не буду супер рассказывать подробно условно у нас есть читалка в нашем случае в яндексе этого брокеры но по сути аналогом можно считать как у мы читали сколько сотен гигабит даных разбитых по топиком соответственно в зависимости от типа событий больше всего было в плеере довольно много было вашей к скоростью и довольно много было стать при этом можно будет потом отдельно посмотреть внимательно собственно вот та вещь которая нас который мы сделали после этого все наши ночной работает эта пастора по сути нам нужно уметь world а не перекладывать данные из того места где мы считаем вплеталась так чтобы это было хорошо удобно понятно в то время было не понята как это делать вот мы взяли решение которое называется лапша тары у наших ребят из маркета по сути вся аналитика маркета в то время ты сейчас по моему на этой штуке работает это под написано на имена джаве соответственно пар сервис мы писали мы нажали на него же мы написали парсер для fi наши статике при этом параллельно другая команда писала такой же парсер для видео события он был написан на было мне кажется там чуть попозже но при этом все круто работала ну вот тут можно примерно по оценивать нагрузки и там сколько нужно зависимости от языка при этом команда дзена в то же самое время то совать свой пастор тоже на джаве он тоже довольно давно но при этом работал то тоже этих проблем не было ну и соответственно если говорить про то как сделать бы сейчас надо не нужно это писать самим есть уже этой штуки в облаке вот дата трассы и подобные штуки они как раз помогают после эту головную боль себя снять и вот эти штуки на самом деле хорошо на что наконец-то уже сколько лет назад появились как сервис и придумать не надо поэтому если будет такое делать перестал его что решение а здесь есть параметр наших баз нато время то тоже можно будет паника про них потом внимательно посмотреть понятно что на сам большая база была под вот события player я не подчас если честно не помню какое временных хранили но в итоге мы их по сути хоронили какие-то месяце ошибки мы ходили вообще за все время их не вычищали ни разу данные про скорости какие-то оперативные данные супер подробные нас один это ж там за единицу месяцев то есть зависимость от типа 2 просто вычищаем x заразный время ну да и соответственно часть машины была на железе это как бы вечно эпопеи как там и сколько усы железную пивку это обычно это задача которую мы много раз себя решать вот на что если говорить про запись сколько мы себе записывали вот здесь вроде здесь видны эти цифры они вроде сейчас кажутся ну вроде бы норм и сейчас сейчас этим цифрами конечно уже никого не удивишь там в 125 миллионов строчка секунду в то время нам казалось как оказалось просто какое-то чудо и мы там 50100 тысяч казалось вообще что-то вот когда ток так начинали было бы круто сейчас конечно на все разрослось оптимизировала на сколько-то раз но в итоге с таким объемом и все эти парсеры справляются и базу справляются в принципе здесь проблем нет она довольно хорошо масштабируется собственно на этом на конечно такая техническая часть все и теперь перейдем к кейсом вообще к день где и как это можно использовать собственно правой части ей кит пример интерфейс интерфейс вот тут я специально по скриншотам чтобы было какое-то понимание чтобы это не выдумывать ну довольно довольно простой есть календарь где можно выбрать время чтобы сделать пришли запросы верху строка поиска в котором можно взять любое выражение логическая над разными колонками этого фрика usb который вас лежат внутри в этом в этой строке поиска есть авто со jest значение в этой колонке очень сильно помогает слева есть панель навигации где по сути каждый кейс это один the big ну и скат рабочая область там есть графики в таблицу и как-то мы их выводим вот здесь показана показано 1 правка какого-то релиза половины была одна версия релизов актер другой версии risen понятно что как в чем прелесть и силы клика us можно смотреть на данный целиком дамское белье в виде агрегата можно смотреть на каждую строчку и это очень сильно помогает то есть можно прямо найти конкретно каждую строчку понять какие были поля это ничего особо не стоит можно их профильтровать и посмотреть там текла которые нужны очень прикольно при этом мы довольно поздно до этого дошли но это тоже нас анри супер прикольно когда ты в этих полях на самом деле тоже берешь и фильтруешь внутренних по какому значению вот это самый простой кейс да не те данным которых например время первого байта больше секунды то есть они все самые медленные запросы и дальше идешь разбираешься почему так сколько их там какие-то срезы можно сегмента понять в общем внутри по метрика внутри этих данных тоже можно их фильтровать понятно что все прошивку нас была сделана довольно хорошо все стектрейсы мини фиксированные для разных языков в мире притягивать все работало очень прикольный вещь которую мы сделали на самом деле она была удобна в том что или дежурств условного нас есть это ошибки мы там за текущий день берем ну группируем потом количество цветов справа было про кнопочка в один клик завести тикет на эту ошибку кажется такая мелочевка на самом деле очень сильно это помогало когда очередной дежурный видит новую ошибку понимаю тахара вот мы и разобрали вот ticket можно пойти посмотреть что с ним либо это катанова ошибка надо пройти его завести и даже разобраться при этом соответственно в кете была обратная ссылка на интерфейс что можно было прям в один клик попасть в то место в то время и в том в ту ошибку которая возникла то есть можно было в обе стороны прям очень быстро навигировать и в этом месте конечно либо такой вот как раз сделали для сюда чтобы удобно очень крутая вещь который мы сделаем не сразу хотели всегда думали что это невозможно потом зале попробовали вроде завелось это тренды это когда у вас есть огромный фон ошибок а потом как анти одна маленькая начинает там появляться а вы наслали на на на на на на этом большом фоне их не видите ну сделали довольно просто взяли просто два промежутка времени взяли все ошибки в в них и вышли одно из другого и увидели вы словно те ошибки которые появились например за последний день или там появились новые летом выросли или упали тоже очень удобно того чтобы понимать что вообще нового прямо сейчас происходит еще дальше который тоже сильно помогает и экономит время это когда вот вас есть какие то данные и видно там какие-то всплески или просто поток событий а потом что-то случается и хочется понять вообще что делаю сделали другие люди в этот момент что могло какой-то ошибки напря привести одет могли все что угодно могли ли закатить могли там дата-центр киньте его отключать были учения могли включить эксперимент или заключить могли киньте еще configure скатить общем как то в общем когда у вас есть какая-то система которая в себе аккумулирует все вот такие события у она либо есть либо очень прикольно сделать и и эту же систему просто накладывать на график ошибок и показывать друг под другом всегда очень прям быстро находить а значит это событие случилось после того как вы видели какой-то эксперимент или или после чего то еще чтобы не бегать по часикам не спрашивай кучу людей а кто что делал открывая страничку и сразу очень полезно и нам была необходимая вещь это понимание что происходит с нашими экспериментами экспериментов много в яндексе системы экспериментов довольно навороченные и летает как бы с одной стороны и сила другой стороны ее слабость ну потому что например там какую-нибудь простую метрику в нем тоже надо заносить реализовывать это как-то ну сложно это какой-то долгий путь а тут просто вот у нас есть ошибки и они размечены одни в бутылку каждой ошибки есть список экспериментов в которой она попала и соответственно если мы из своей эксперимент систем экспериментов находим пару эксперимент контроль вы просто сравним по эксперименту и контроля группируем их и получаем div и это очень удобная штука и просто необходимы для того чтобы быстро понимать что с экспериментом то есть если вы хотите потом делать какие-то выводы считать то вам нужно сделать так сам начал убедиться что у вас данные не расходятся что вас условную группу контроля экспериментальная одинаковых словно по размеру а следующий шаг что в одной нет ошибок которые есть другое если так получается что в экспериментальной группы есть ошибки вы можете по сразу понимаешь я так сказать не валидный и вообще даже потому ничего не думать и эту штуку мы на самом деле могли понимать очень быстро буквально там за 10 минут они так что ты включаешь экспериментом день-два ждешь только потом появляешься я там уже блин ошибки были давать его перри заведем в общем так на старте мы очень много всякого личного фекальных вод на самом деле нос срезов то было многое можно было сравнивать вообще по любому из них например там по ну по названию или интернет провайдера и так можно было понимать вообще что у одного до другого провайдера и сбывается специфичные ошибки и такой кейс немножко придуманный но ты таки из вполне себе работающим словно бывает так что ты разбираешь на примерке медленные запросы и понимаю что у тебя там высокой процентиль вроде у меня сначала каких ошибок нету но вот она к это очень высокая и не и и высокое там 205 эта цифра очень большая не поешь почему а потом делаешь средств срез по провайдерам и понимаю что просто какой-то провайдер в этом регионе не очень хорошо работает и сразу понятно типа ну да ладно вы такие так и будет спишем что вот в этом месте ошибка потому что зависит не от нас то есть хотя бы это какое-то понимание почему это происходит тоже добавляет соответственно из просто на картинке нарисована не только установки с соединением двух разных операторов и есть и гистограммы процентили посчитаны вот по любой из этих цифр можно понять что один хуже другого и дальше уже удачу уже с этим что-то делать есть данный просидел можно просто понять какие бывают фары сколько их файл в какой-то шкет какой протокол ну в общем вот просто какая-то общая картинка что вообще происходит с нашей стати ты при этом если нам нужно к деталь информация можно там еще других фокус описать их тоже заливки house и например понять что у вас на странице иногда бывает такое что вас там загружается мегабайт же снова года а вы вообще можно ли говоря про это и не знаете потому что ну может быть какой-нибудь там браузерный плагин какой-то под добавляет в общем этот продукт про то что можно разным образом эти данные прозы статику по профильтровать и в том числе глупые и понять что а вот да у нас вот в этом месте к этой проблемы даже уже думаю что с этим делать понятно что у нас очень много завязано hydrogen мы его по всем сервере есть прям библиотечка которая может вытащить название персоной системы телефона все что угодно их можно профильтровать вот здесь там версия мобильного сафари начинается с какой-то версии операционной системы в общем любые такие срезы они очень хорошо работает и ну и в том числе какое-то количество проблем как раз локализуется по срезам именно связанным с браузерами вот еще одна из которой мы сделали и оказалось как раз она очень полезны для того чтобы понимать что происходит сессии пользователя с одной стороны нужно было довольно большая подготовительная работа по разметке всех логов каким-то единым request ольги и и как только они размечены можно сделать запрос с этим какой стадии во все наши источники тут вот здесь увеличенном окошки на самом деле там семь или восемь таблицы это вообще разные данные там одни просторы с другие прошивки третье по что-то еще и вот мы делаем запрос во все эти источники их потом собираем и показываю на одном таймлайне с мы сразу можно сказать там какие были проблемы что вообще в целом у пользователя происходило и как и какой-то сделать превью этой сессии мы потом очень подробное описание каждого события например ней и что круто что это работает над всем объеме данных то есть здесь можно взять любой рукой стадии любого из нашего сервиса frontend это будет работать еще парочка те которые назывались назвал продвинутыми а вот я говорил про эксперименты и говорил что если эксперимент плохое то можно сразу выключать ну логичный шаг был ну давайте сделать эту автоматику мы завели эксперимент у нас есть тикет по экспериментальной в котором понятно вот эксперт под контроль дальше мы просто робота можем пойти проверить насколько us а что там с данными если мы понимаем что там есть резкий рост ошибок именно экспериментальной группы и тот же робот пост идет этот эксперимент автоматически выключает то есть разработчик на самом деле вообще не курсу происходит он просто какой-то момент видит в кете но его там призывают типа извини но мы твой эксперимент выключили потому что был плохой ноте буду знать пойду потом разберусь вот еще один кейс был связано с тем что часть ошибок проведи усмотрением нам смогу нам удалось локализовать например только скопировал их по дел координатам такой работает не всегда пу что они есть там в основном только на мобилках но блокатором большая проблема что у нас было много ошибок и мы не понимали в чем это связано а потом когда мы их взяли они были различные координатами просто их на карту наложили сгруппировали поняли что они были ровно в одном месте в каком-то таком же зале в котором было конференции просто плохой вай фай эти люди которые сидели и смотрели конференцию вай-фая в зале естественно получаем много ошибок ну вот вот такое тоже был так еще один маленький сайт про технические детали чтобы примерно прикинуть сколько это стоит и как это вообще сделать вот столько данных описали в своей базе впк us вот с точки зрения ошибок и скорость это была одна база общем понятно что мы там стремились тому что когда она была меньше ключе лежать и даже получить профит и вот под при ипре переходим дальше нас собственно сами данный приказ подали с задержкой меньше минуты это был просто какой-то окно настраиваемая в зависимости того там ну сколько ты хочешь в в этом внутренними данных ну и для того чтобы иметь всякую красивой статистику по сети мы очень активно использоваться словари который матча лео хищник в разные другие названия там названия провайдеры на снова название автономной системы или города это очень сильно помогает ну просто так как как какая-то дополнительная информация по запросу что в итоге мы получили мы получили систем в которой мы покрыли все свои кейси мы могли понимать что с ошибками что со скоростью сделать дежурство мы туда за суд затащили очень много данных очень много логов соответственно при этом это был один интерфейс один движок просто чуть-чуть разные базы чуть-чуть разные настройки может быть там пару разных the back of для того чтобы какие-то специфические кейсы делать ну и логов мы подключили много десятков или сотен даже то есть это была такая довольно простая процедура что там можно было за час и подключить себе новые ловко условно там новый сервис или новая команда а потом на самом деле мы поняли что это эта штука эта схема она на самом деле применимы еще для довольно большой части задачи например вот у нас была такая проблема что у нас есть видео плеера он гибнет много событий событий одиночные и по этой событием одиночным какой-то как отличить часть проблем нет не видно у нас под у нас внутри эти события переваривали строились по ним агрегаты по ним строились отчеты нас получилось кого-нибудь ежедневный отчет что вот например там суммарное потребление трафика на таком-то сервисом таком бы срезе там столько то петабайт и это было понятно что там но какая цифра большая еще у нас стоит не понятно то есть у нас получалось что есть события в самом начале сыры в самом конце уже посчитаны и тогда он поняли а почему бы нам агрегаты то есть те сессии которые мы рассчитываем по этим событиям а вот взяли все события группировались по request ольги и за один день например вот прежде чем даже в тот же сам метка когда мы читаем отчета так давайте ему это событие в сыром виде привлекался положим точно такая же табличка их там сколько там миллиардов записи в день немного относительно всего остального будет мы кажется удобно ok вторая штука которая была очень полезная она была про такое подавать ему базу контента кто на самом деле тоже может быть довольно большой тоже палата vklucaut при этом это было очень важный и такая прям супер необходимая вещь например когда мы из эфира переели в.д. там в одной базе был много контента миллиона штук из другой базе было много другого контента там десятки сотни миллионов штук и некоторые из них эти и вот так что выход ибо именно про видео вот эти записи были и там и там и вот на самом деле чтобы понять а какие они какие в чем они отличаются как у них поля это нужно было вот одни лежали где-то в манге другие лежали где-то забыл где еще не важно они еще были по таблице в общем для того чтобы ответить на любой вопрос нужно было написать довольно большой и скулы ленчик запустить его его мог написать там типа 23 человека в разработке и соответственно какую-то над на все вопросы а что там нас лежит как на соотносится нужно было довольно долго ждать еще там была куча шик ничего не понятно так вот если эту эти базы контента вот предварительно с джо не собрать сгруппировать и положить опять же vklucaut то дальше с ним можно работать точно также как все со всеми остальными данными при этом их можно очень хорошо обогащать и эту таблицу делать довольно широкой и в том числе даже данными в онлайн то есть у нас есть контент про видео вот есть просто условно этим видео и мы приводим счетчика сколько этот видео получило показав на текущий момент довольно удобно врач с покажу пару примеров то собственно сама схема очень похожи но только с левой части у нас есть другие источники у нас были какие-то нашей сессии которые мы какими-то сложными вычислениями считаем что он даже раз в день либо например какие-то базы вот это был под gres сколько ты мы этим скопив сколько бы ни было таблиц мы их все с джонни ли выстроили в в одной большой запись у нас есть какие то данные в онлайне мы каким-то регулярным процессом всё это собираем и точно также приклады отвлекался ну и соответственно очень схожим способом делаем для это для них я и для этих кейсов точно также используем это же сам интерфейс все получается но смотрите когда мы либо жили почему у нас сессии занимает видео мы никак не которые много трафика сделаю простую вещь давайте возьмем сессии я сортируем их по суммарному трафика в этой сессии ага вот у нас есть сессии которая занимает 40 гигабайт казалось быть такого быть не может но у нас не бывает такого размера файлов но покрыли при этом отчет по говорит что вот эта сессия есть можно посмотреть на уазе шнек может это что-то еще собственно ты нажимаешь одну кнопочку вонять о сортировались ты получаешь то проблем и дальше идешь с ними разбираешься здорово с контентом получилось пример то же самое у нас есть очень много контента непонятно где как живущего и только разработчиков на поднимают а тут мы взяли это все переложили speakout is реально его смотрим вот у нас есть тайтл заголовок у нас есть ссылка на это видео у нас есть какие-то технической информации про эту информацию все что угодно там вот там было там сотни полей все чтобы это результат контент ты с ним не навигировать вообще абсолютно даже не знаю где и как он страниц это все про модерацию все про длительность и все что угодно 5 можно посмотреть что с ним происходит и в том случае когда у вас есть такая база получается что вопросы вида а давайте попробуем запустить новый продукт с короткими видео а сколько у нас таких видео вот такие вопросы решаются очень просто раньше нужно было прийти к разработчикам к аналитикам посмотреть что это за видео сколько таких короче это занимало дни на то чтобы эти данные собрать это просто вбиваешь вот на данном случае дай мне квадратные видео длиной больше двух минут вот тебе эти видео можно посмотреть разбивку по баки там по длительности можно посмотреть превьюшки может быть все что угодно то есть все такие запросы от менеджеров от аналитиков разработчиков что-то у нас есть контенте кокаин бывает вообще чуть не происходит они решаются просто мгновенный была еще одна проблема опять же связаны с места на ужин с трафиком с местом на дисках бывало такое что к нам приходили люди из использовали настолько хвостик то ли как не знаю что вы просто заливали много таро байт видео но при этом это видео не смотрели и вот условно с помощью такой системы можно было это видео очень просто находить вот у нас есть видео мы группируем по названием каналов канал дальше у нас есть при джоне на онлайновые показы этого видео соответственно съесть онлайновая свои новые данные сгруппированный ассорти ровно и получалось такое вот те видео которые лежат и не смотрятся вот столько места не занимаются видно сколько таких каналов или что это за канала можно дальше идти разбираться почему кто это люди можно дальше идти условность срезать им высокое разрешение потому что все равно не смотрят на то есть мы с ними право удалить но их хранить их во всех возможных разрешениях на как бы тоже не очень хочется поэтому соответствие что-то с ними делать дальше можно было и там сколько-то сотен терабайтов пород и раз и мы прямо из них очень быстро связали вот опять же копаем был прям был какой-то специфический кейсы мы под него сделали для себя какую-то удобный смотрел мы в итоге что получала что например кейсы когда у вас есть вот прям в runtime сейчас какая-то ошибка в android плеере вот вы знаете что вот есть видео с таким-то айтишниками вас только этих ошибок и то там то что происходит в ран тамино клиентов вы потом берете этот гадюшник вбиваете в другую смотрю по-другому углу находите эту информацию этого видео техническую это же ваша контентная база дальше идет с ним разбирается и может так получиться что дело было именно в этом видео но она например там неправильно расходилась и в этом месте получается такое очень быстрая бесшовный переход одни данные собраны одним способом там под какой-то бешено нагрузкой другие данные которые лежат и обновляются раз в день они все провязаны адис никами там прям в интерфейсе можно благах из одного в другой по прокликивать и очень быстро между нивелироваться соответственно таким образом можно было довольно быстро эти проблем находить что в итоге получилось у нас мы сделали там несколько несколько десятков интерфейсов для разных наших топ 30 или 40 мы завезли в нашей системы много десятков сервисов всех те кто к нам приходил мы им рассказывали показывали и спокойно к себе мигрировали не знаю сколько людей не считал но многое сколько людей из разработки всеми нашими с теми пользуюсь ну просто потому что это было удобно они как это как рабочий инструмент использовали это опять в релизах в де баги проблем по всем во всем чем угодно ну да я 1 из того что у нас есть рассказал может треть потому что это было гораздо больше специфичный кейсов каких-то других еще деталей я при просто тут не стал рассказывать и основной прав в том что это действительно экономит кучу времени то есть вам не нужно ждать и вам не нужно думать как это найти вот просто есть все данные в полном объеме удобно и это происходит потому что большинство проблем в таком случае вы находите буквально в один клик в открыли интерфейс вы нажали нужный вам the big и нажали сортировку или просто в били какую-нибудь фильтрацию по какому-нибудь поле и вы просто видите топ проблем по эти проблемы и пошли разбирать и собственно вот таким образом можно итеративного улучшать свой сервис собственно рецепт сбора такую систему как нас это получилось у нас была очень компактно команда по сути у нас был один супер мотивировано человек все это время еще парочка разное время ему помогала мы ничего не стоим согласовали просто брали и делали при этом важно и мне кажется прямо принципиально важно делать такую систему внутри команды которые собственно шапки задачи то есть не нужно отдавать это какой-то внешней команде видит с как-то не что-то объясняет в общем до тех пор пока вы руками не прочувствуете руками свои проблемы для себя не решите это будет как вот лишний коммуникации сделают нет они так и в общем лучше это сделать в рамках работы в своем сервисе соответственно мы решали конкретная задача и их на самом деле получилось не так много там 20 30 50 неважно сколько но и в там не тысячи я тоже очень помогало у нас не было абстрактной штуки которые решает что-то там были прям конкретные кейсы соответственно мы довольно на на родим этапе приходили к внешним командам и поэтому им было удобно пользоваться они наши семена на 6-м переходили и соответственно низкий порог входа это всегда такой критерий успеха того что ваш тем будет востребован правда что нам нужно было довольно тесно интегрируется с внутренней 2 и это как раз важная часть что не нужно писать их то общих адаптеров непонятно чему конкретно идти то есть оба система сделали с не этикеты сделали для нее у нас было инфра с общими событиями сделали решение для нее и так далее ну и соответственно 100 для себя было там много в интерфейсе в ходе вещей которые можно было собирать чуть ли не из конфигов и там способ отображения там данные которыми мы пар смысле cows у той чтобы такие штуки можно было довольно быстро что уповать собственно тут вот этот слайд для двух вещей чтобы папе рассказать витале спасибо он сделал больше сейчас работала немножко стесняется поэтому не очень хотел рассказывать но правда муж сказать ему спасибо есть 2 вещи это про то что вообще в для того чтобы такие вещи случались мне кажется важно чтобы было две вещи первая если ты разработчик это хочешь сделать что-то то надо просто брать и делать не боятся даже если что непонятно вот ты понимаешь что тебе что-то хочется сделать пожалуйста не нет не стесняйся и пробуй а вторую если ты руководитель и видишь то есть такая проблема если такой человек то нужно дать ему время возможности это сделать сам солнца собственно вот у меня было какое-то идейная часть организационная я сделал так чтобы у metallica было на это время соответственно виталик сделал так что она все заработало важно иметь всегда возможность такие штуки реализовывать мы тут еще несколько слов благодарности команде маркета залог shatter за то что мы чуть завели команды разработки моды за то что мы все это в первый раз подняли это много много ошибок разобрали и это все у нас завелось у нас в команде поиска есть часть пирс контента и мы там многое заработались метриками и эти данные мы тоже новых анализировали там был большой оффлайн контр это было его онлайновая часть соответственно эфир видео платформа было много значить усмотрение player to the всему хранение видосиков много было прикольного ну и соответственно цену там тоже данные про рекомендацию как эту систему решаете проблему мы тоже много полезных дел да собственно все я рассказал то как я понимаю задачу понимание работает продакшна если у вас есть что обсудить давайте обсудим спасибо спасибо большое прекрасная доклад и еще это от организаторов да давайте начать задавать вопросы вот я вижу уже там зари по центру здравствуйте вопрос по ресурсам какие-то тысяче ядер вычислительных не мне кажется хотя если честно ресурсы это очень правильный вопрос эта штука ну в целом довольно дорогая но все равно я стоимость по сравнению например там со стоимостью того же трафика для видеоплеера это была какая-то там мало этом меньше там 5 процентов условно и и понятно как будет у каждой части есть свои ресурсы есть ресурсы на запись есть ресурсы на хранение логов соответствие если не хватает там то можно логе чуть меньше хранить ну а так в нашем случае это были какие-то десятки машин портьеры и десятки или сотни машин налоги но это было много блогов я вас нет таких объемов но в любом случае любой облачное решение ле вентр вам будет стоить мне кажется на порядок дороже ответом допрошу по возможности вставать что вас мало легче увезли докладчик здравствуйте спасибо за доклад хотел уточнить какой нюанс проводилась проводился ли какой-нибудь анализ как это все помогло пользователь ну то есть например да стало меньше обращение в службу поддержки после того когда вы начали сами обрабатывать свои ошибки то есть сами их искать сами видеть но соответственно нужно было уменьшить количество обращений там я не знаю время реагирования на ошибку то есть тоже должно было сократится потому что вы не тратите время на поиск этого всего то есть какой профит получил пользователь ну правда тут на самом деле моя позиция политика такая если мы это узнали от пользователей значит мы уже сделать поздно собственно поэтому всегда старались сделать так чтобы на мусора помогала находить такие проблемы превентивно и уметь их решать самостоятельно и вне зависимости о того сколько людей страдают а для себя и весь ответ на вопрос а какой объем проблемы чтобы мы это не узнавали скалы спартак центра сколько там жало было за час а сами с вами очень четко подарком понимали что столько-то людей страдают и соответственно уже и исходя из этих данных сами бы понимали надо уточнить прямо сейчас или там может чуть-чуть отложить для вас правил по центру li-ion спасибо за доклад у меня такой вопрос как вы сделали контентную систему а именно как обновляли данные допустим на просмотрах 3 хаусе он же апдейт не поддерживает а это вот у нас полно ребята рассказывали нет там не было бы это там просто даппи созданы все время я могу потом кого-нибудь привести кто-то про это прям расскажут хорошо но в целом мы считали описывает события потом агрегатом высчитывали сколько но при этом то эти значки даже такое было что мы же не смотрели в этом месте здесь вот не было той часть которые про просмотрят между другой приказ и другие смысле смысле логикой заливки вы нас андре могли брать данные с каким-то вопросом условно там за день вот мы в статистикой любой там мы принесем откликался посчитали сложили в систему отчетов и дальше где-то надеюсь мы просто при джоне вали эти данные к контенту обновляли в том разделе ли там просто 4 часа или тут и joy него и или раз там несколько часов клика уж заливали новые версии ну да ну потому что не до конца понял вопрос но вот те данные которые у нас были про кант эту систему да мы попросту перетирали все нас просто балок эту версию последнюю словно спасибо давайте это вот последний вопрос уже в будем будем отправляться сырое кулуары здравствуйте меня зовут андрей востоке получилось довольно большая сложная система мониторинга в нем наверняка тоже возникали по ошибке как они в свою очередь мониторе лись но они монитор с на самом деле через себя то есть только krause который treehouse то все тело шатра это их ошибка метались сами себе в том числе как раз один из этих треков привел к тому что инфра для самого cliff house of от большого для мониторинга было сделано нами потому что им нужно было мониторе что увлекалась хорошо работает замечательно и в решении а она будет глупо не приводило например я пытаюсь отправить сообщение 1 пункт exception-ы препаратов его звонка и толпа enter exit и , подойдя к естественно до дали понять и сейчас таких вещей не помню но помню что бывает правда что стреляла что прям много прилетала ошибок их нибудь там больших крупных релизах on free вроде выдерживала отлично спасибо давайте еще раз поблагодарим его на"
}