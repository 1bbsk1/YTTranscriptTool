{
  "video_id": "4tIS58sQ7Mc",
  "channel": "HighLoadChannel",
  "title": "Архитектура Мессенджера Авито – путь одного сообщения / Александр Емелин (Авито)",
  "views": 19070,
  "duration": 3000,
  "published": "2020-02-26T12:03:45-08:00",
  "text": "собственно меня зовут александра эвелин я из компании авито и сегодня буду рассказывать вам про messenger авито про его архитектуру это проект над которым я работаю о чем будет рассказ собственно рассказ будет про особенности бэг-энда массажа лета мы еще закроем затронем ограниченную область клиент-серверный протокол да и в целом попробуем составить общую картину архитектуры мессенджера на примере отправки одного сообщения от одного пользователя другому пользователю в контексте авито до 1 пользователь и то покупатель второй пользователи это продавец в чем мне кажется ценность данного рассказа в том что мы посмотрим верха уровню она все наши основные концепты а не какой-то оторванный кусок да то есть вы сможете составить представление о том как все у нас работает и часть наших решений может быть очевидно часть была выставлена в результате деградации мессенджера собственно даже если вы и свой мысли уже не пишите надеюсь какие-то решения вы перейдете да и сможете в будущем применить как вообще посмотреть messenger в действие очень просто он доступен в любом приложении авито мобильному или на сайте в разделе сообщения в общем-то messenger как messenger до похож на лучших представителей жанра я думаю что ничего особенного здесь нет еще раз покупатель пишет продавцу они общаются по поводу объявления по поводу товара договариваются сделки возможно голове у кого то из вас сидит вопрос да почему авито делает свой messenger почему не дали возможность указывать в профиле ссылки на какие-то существующую популярный мессенджер вроде в отца по телеграма и форсить переписку туда попробую ответить на этот вопрос несколькими пунктами но во первых это единая точка входа в переписку пользователям не нужно иметь пересечений messengers установленных на девайсах до которых достаточно много что начать общение следующая эта тесная интеграция с процессами авито интеграция с объявлениями интеграция с профилями продавцов покупателей кажется логично без уже реализованы некоторые интересные механики вроде авито доставки до это сработает как будет платформу возможность не светить свой номер телефона классная фича когда вы подаете объявления на авито сейчас есть возможность указать галочку что вы будете общаться только через messenger и соответственно ваш номер не будет доступен никому не утечет какие-то сторонние базы данных и последние данные остаются во vita мы можем их изучать и улучшать и улучшать функционал messenger защищать наших пользователя от каких-то известных мошеннических а так производить арбитраж какой-то ну например мы считаем на основе данных среднее время ответа продавца в чате потом эту информацию показываем наша статистика статистика бэг-энда мессенджера на сентябрь 2019 года у нас два миллиона уникальных пользователей в день полтора миллиона запросов в минуту к бренду vetter psy запросы и 500 тысяч постоянных соединения онлайн в пике да то есть это как мы в дальнейшем увидим websocket соединения и 7000 пользу сообщений в минуту генерят клиента как мы к этому пришли коротко о том какой путь прошел messenger когда-то он был плоским и ставил на 2 слонах край в роли слона выступал печки монолит а сервис по 1 точка ком выступал для того чтобы держать постоянное соединение от пользователей и через него мы рассылали мгновенный real-time модификации например о том что пришло новое сообщение через некоторое время пришло понимание что messenger полезен пользуется им нравится и мы переписали поздно не переписали поздно по заменили по кнопкам на самописный сервис на но джесс бизнес-логика по-прежнему осталась спички монолите но джесс lavita не прижился по ряду причин и в определённый момент все это была переписана на язык в собственно при этом переписана было на и загоне только та часть которая держит соединение от пользователей в том числе и бизнес-логика бизнес логика была выйди вынесены в отдельный микро сервисы то есть у нас сейчас микро сервисная архитектура все наши сервисы крутится внутри губернатор да то есть более справедливой картинка будет вот такая и в общем то помимо год в нашем стеке есть манга деби редис ребятенку и подробнее их роли мы коснемся походов сегодняшнего рассказа пойдемте дальше в общем то сейчас мы готовы смотреть на актуален архитектуру мессенджера вот так она выглядит это то о чем мы сегодня будем говорить по ходу рассказа я буду подсвечивать элементы цветом до о которых буду рассказывать ну и вот собственно первые элементы вот эти если мы провалимся внутрь первый сервис и нашей цепочки который работает уже внутри губернатор это сервис socket так называемый этот сервис наружу не смотрит он прикрыт внешним engine с балансиром и собственно каждый пользователь который пользуется мессенджером устанавливает websocket соединения с сервером вы можете видеть здесь что у нас не только websocket используется протокол для общения с клиентами но еще и т.п. мы об этом тоже будем говорить напомним да что такое websocket вообще это полна дуплексное соединение между клиентом и сервером используется тогда когда нужно общаться сообщений обмениваться сообщениями в реальном времени очень быстро в двустороннем режиме собственно начинается vip соки соединения с степи запроса на сервер это так называемый апгрейт соединения сервер и клиент обмениваются специализированными с головками и после успешного обмен и у нас получается теперь сессия чистая между клиентом и сервером по которой гуляют фреймы websocket протокола до 1000 кит протокол это фрейм новый протокол и мы как программисты можем пересылать по и всяких протоколу как бинарные так и текстовые данные вообще websocket достаточно эффективный транспорт и у него небольшой вверх от над чистый css себя за счет своего флеминга он добавляет всего два восемь байт в зависимости от типа фрейма к вашим данным и часто решающим фактором почему используют websocket является то что он нативно работает из браузеров то очень удобно высокий собственно кроссплатформенное доступен на всех для всех языков библиотеке есть это наш график за 3 июльских дня до websocket подключения собственно большая часть приходит весь к мелко ну как есть расскажу большая часть приходят из десктопных браузеров потом мобильная android приложение дальше ios приложение давайте попробуем как-то уже начать суммировать функция 1 сервиса на наши на этом этапе это authentication пользователь да здесь вот мы от инфицируем пользователя только зарегистрированные пользователи могут пользоваться мессенджером авито для этого мы ходим сторонний сервис и по идентификатор сессия проверяем что это за пользователь к нам пришел как я уже сказал он держит websocket соединения и принимает терпеть и запросы от пользователей рпц запросы у нас используется джейсон рпц протокол между клиентом и сервером и в общем-то это например получение каких-то данных для отображения список сообщений список чатов и также сервис socket ответственен за то чтобы подписываться на нашу шину real-time нотификаций и слушать входящие асинхронное уведомления пользователям да ну например кто-то написал сообщение вот она приходит сервис соки через шину real-time модификации отправляется клиенту который сейчас подключен в общем то прошин уриал the medication мы будем подробно говорить в конце однако до в 2019 году они все пользователи могут соединиться с websocket им по-прежнему почему на самом деле мобильные нативные клиенты проблем не имеются подключение основном это касается браузеров мало того что пользователи по-прежнему использует internet explorer 8 9 есть корпоративный пользователю которых работодатель ставит на машину доверенный рутовый сертификат и таким образом имеет возможность перед шифровать на уровне своих прокси даже тело с трафик собственное так делают так делают многие компании почему они так делают либо там старое программное обеспечение на прокси который режет хлеб соки трафик ли бы они делают это намеренно не знаю но такое встречается плюс есть браузер и браузерное расширения которые также блокируют все таки трафик на определенную домена собственно как мы решили проблему мы используем fall back если te pido в случае с мессенджером это просто еще тебе полинг периодические запросы раз десять секунд за теми модификациями которые пользователь пропускает у нас один процент пользователей на этапе фабрики поэтому такой компромисс для нас плюс минус оправдан в общем-то да сервис socket обработки писи запросы вытащите fall back вообще на таком количестве рпц как выяснилось очень сложно заметить невооруженным взглядом какие-то аномалии в поведении пользователей мы долгое время тоже не замечали но как то дошли руки мы померили сколько запросов приходят от каких то пользователей и поняли что у нас масса паразитного трафика откуда этот паразитный трафик берется ну во первых это предприимчивые люди которые пытаются использовать messenger не по назначению до для своей отличной выгоды а во вторых это баги в наших собственных клиентских приложениях которые иногда приводят к пиковым рекламу количество запросов к каким-то нашим ручкам это приводит к пиковой нагрузки на базу данных на какие-то ручки какие-то ручки приводят к пиковые нагрузки на другие сервисы наши от нашей зависимости зависит от типа ручки в общем-то на этом уровне мы еще внедрили тротлинг в общем всем советую на самом деле посмотреть на очень крутой статью отстраивать точка ком как они делали свои тротлинг мы используем точно такой же подход слегка модифицированную версию их алгоритма собственно talked бакен bucket до этого алгоритм который чаще всего используется для троттлинга он переживает всплески какие-то временные в трафике и в общем-то мы используем тротлинг у нас используется редис шарди раваной там выполняется процедура вот по вот этим ссылочкам потом презентацию вы доступно вы сможете найти как статью так имплементацию практически целиком и работает как часы на самом деле так что еще одна функция тротлинг торопись и ну и собственно этот сервис является таким такой тонкой оберткой прокси дают сервисом который терменируют websocket соединения принимает рпц и дальше он прокси ru это тир песен следующему сервису в цепочке уже почти типе протоколом следующий сервис в нашей цепочки называется сервис агрегатор он собственно содержит бизнес-логика вода делает все необходимые запросы сторонние сервисы так называемые пей композиция выполняет формирует ответы клиентам согласно то так контрактам с клиентскими приложениями ну и наверное что самое интересное на этом уровне у нас grace ал degradation реализован то есть если какие-то внешне зависимости messenger например сервис объявлений или сервис пользователя авито недоступны the messenger продолжает работать сервис агрегат are handled эту ситуацию и временный заглушки форме тут на самом деле это классная тема потому что когда большое это лежит на messenger продолжает работать и люди по-прежнему могут переписываться в случае отправки сообщения от одного пользователя другому все что сервис агрегатор делает это идет следующий сервис который называется сервис db5 этот сервис уже являются оберткой над нашей основной базой данных и собственно сохраняет туда данные давайте откроем еще один элемент на этой схеме это mongo db собственно если говорить о манга то у нас используется 8 отдельных реплика советов которые ничего друг о друге не знают да то есть мы используем клиентское сортирование и виртуальные bucket и для того чтобы распределять данные по пользователям пожар дам шарден по юзер айди у нас криштиану роналду еще на вид они торгуют поэтому sharding по юзера иди в принципе работает хорошо какого-то перекоса в сторону одного пользователя нету и собственно вполне себе хорошая схема тут появляется любопытно особенность нашей системы это то что мы для каждого пользователя покупателя продавца храним копию всех его данных до по переписке данные это список читов список сообщений которые мы доступны это значит что одно сообщение которое отправляется на самом деле копируется в два шара до в 1 шард который подлежит покупателей 2 шард который принадлежит продавцу мы будем смотреть что нам это дает но первое наверное самое прозрачное преимущество это то что благодаря такой схеме при запросе при чтении данных из базы нам не нужно собирать данные из разных сортов мы идем в 1 шард и отдаем пользователю все данной которые есть плюс это достаточно гибко мы можем сохранить пользователю одному пользователю шар данные другому слегка модифицированные данные и собственно что нам это дает и как мы это используем мы еще тоже сегодня посмотрим понятное дело что это приводит к избыточности данных ну об избыточности мы еще поговорим после в общем то сервис сохраняет в базы данных но на данном этапе на этапе синхронной обработки росписи запроса да вот этой цепочки мы данные сохраняем только в шарф отправителя сообщения вот собственно почему мы сохранимся от отправителя потому что нам нужно добиться consistent насти отправителя отправил сообщение он следом может запросить историю сообщений и ожидает что вот это сообщение уже в среди истории его есть а вот уж арт получателям и сообщения сохранять не спешим что мы делаем это публикуем пару событий в ребятенку одно принадлежит одно событие говорит о том что вот есть новое сообщение первому юзеру второе событие говорит о том есть новое сообщение втором юзерам публикуется в бутылку успешно опубликовали отдали синхронный ответ окей отправителю сообщения он получает ток не рисует галочка прессуется галочка в интерфейсе сообщение отправлено в этой в этот момент у нас появляется еще rabbit in куда собственно грабит руку развёрнут как отказывай устойчивый кластер из двух машин у нас настроенные политики отказоустойчивость для . обмен экстензии для очередей которые там хранятся еще один сервис сервис baby star вычитывает событие из рыбы темку и благодаря этому может асинхронно обрабатывать события собственно асинхронно он обрабатывает событие те что мы опубликовали до новых сообщениях мы на самом деле любые другие а события которые мы не хотим обрабатывать на синхронной стадии когда пользователь ждет успешного ответа мы можем через rabbit переслать в этот сервис в данном случае он получает те события сообщениях в этот момент сохраняются данные в sharp получателя и после этого сообщения публикуются в нашу шину real-time модификаций как отправителя так и получатель и вот в этот момент собственно как вы можете видеть действие у нас здесь могут отвалиться в любой момент причем мы не можем уже вернуть ошибку это асинхронная обработка дамы юзеру сказали что твое сообщение отправлено значит мы его должны доставить поэтому если что-то пойдет не так нам нужно довести работу до конца мы делаем ретро и собственно ретро и у нас организованы на основе того же самого ребенку в ребятенку есть фича у очередей который называется dlx это дед лет алексеевич и это цель это время через которое сообщение если его не вычитал консьюмер будет сваливаться в указанный дед лет дед лет алексеич собственно мы делаем систему каскадных очередей каждый из которых экспоненциально увеличу каждая из которых из пицца экспоненциально увеличивает 100 т.р. и если что-то не идет не так при обработке события мы кладем сообщения в ретро и очередь первое увеличиваем при этом контр в теле события до чтобы если второй раз что то пошло не так мы потом пока он true поняли что нам надо следующую очередь класть и вот так вот по кругу идёт идут ретро и пока в сервисе операция успешно не завершится но эти операции не транзакционные да может быть мы сохранили данные в шорт получателя но при этом отправка в шину real-time модификацией не отработала тут мы пользуемся дым патент насти операции стараемся вообще в мессенджеры как можно больше операций делателем патент на чтобы в случае retrieve нас ничего не разваливалась и юзер экспириенс не портил сюда чтобы дубликаты сообщений не прилетали на экран пользователя например следующая операция нас седым понтент на это сохранение сообщения в мангале беда мы по идентификатору смотрим и если идентификатор совпадает да там и просто реплей сим запись и на клиентских приложениях мы также до патент на обрабатываем входящие нотификации также смотрим на айдишники если пришла пришло две одинаковых нотификации клиенты их мер сжатые дубликатов сообщений не показывается благодаря этому user experience не страдает и у нас есть ретро и которые вынесены из процессов приложения до в отдельную ребятенку систему собственном и внутри процессов памяти не не ждем не откладываем обработку следующих событий consuming следующих событий которые есть в основной очень любит mq которая просто случится через какое-то время по тому же самому сценарию антиспам у нас есть тротлинг да как я говорил на тротлинг это не защита от спамеров конечно в любом мессенджеры должен быть антиспам и у нас мы не исключение и антиспам на самом деле очень интересная тема которая мы в последнее время стали уделять большое внимание наша система антиспама основана на обученные модели машин обучения дата санс и так далее но не эта тема ну хоть это интересно и это все таки не тема сегодняшнего рассказа я хочу рассказать о том как антиспам вписан в архитектуру messenger да если вы внимательно смотрели вы могли заметить что мы пропустили один сервис на вот этой цепочке это так называемый сервис middleware на самом деле сервис middleware от еще одна стадия pipeline она обратно этапе обработки сообщения через этапе про года сообщения через рыбе темку да то есть сервис middleware встревает в этот процесс и до того как сообщение свалиться в сервис baby star вычитывает сообщение из ребята при этом напомню что он вычитывать эти сообщение и данные еще не были сохранены в sharp получателя и нотификации real-time еще не были отправлены в этот момент мы имеем возможность сходить в антиспам и проверить сообщение собственно мы это делаем мы идём сервис антиспама и случае если сообщение является спамом помечаем данные флажком специальном на который уже сервис дпс top опирается в общем-то интересная особенность здесь то что у сервиса middleware есть ограниченное время на обработку сообщения это достигается той же самой схемой с один вектор excel-g метатель в очереди до только там 1 очередь если сервис антиспама недоступен то в этом месте мы тоже делаем грейс пул degradation и сообщение на самом деле достигаются достигают сервис baby star и отправляют с пользователем вот собственно функция сервис middleware а то что то что я рассказал это первая вторая то что мы постфактум также можем проверять сообщения на спам но наверное это уже вне сегодняшнего рассказа тема если кому то интересно потом расскажу после а давайте перейдем наверное к самое интересное для меня лично теме atashi no real time модификации наши это еще один квадратик вот он красный появляется по большому счету он замыкает круг да вот круговорот сообщения в нашей архитектуре сервис baby star публикует rialto я модификация сервис socket как я говорил подписан на них и перенаправляет их уже пользователю человек получает сообщение через 200 соединения но отрисовывается на экране соответственно у нас эта схема основана на редис и называть и называемые внутри сок stream давайте вообще посмотрим какие требования у нас есть кое-то real time she не первое это мгновенная notification событиях да мы messenger мы хотим доставлять клей сообщение пользователю как можно быстрее второе это хранение горячего каша сообщений в течение ретенция периода в чем здесь смысл зачем нам внутри шины хранить кэш сообщений на самом деле две причины первое это для реконнекта у нас 500 тысяч пользователей онлайн и у нас websocket приложения и bcex приложение стоит full то есть когда пользуется reconnective они могут это делать массово до в результате чего например внешней джинкс балансиры ладят свой конфиг или мы переросла дэва им наш сервис socket соединение рвутся 500 тысяч соединений лавины идут на наш backend в общем-то а что делать доверить эта пиковая нагрузка на нашу базу данных клиента хотят проверить а не потерял ли а какие то сообщения пока был оффлайн собственно чтобы эту нагрузку убрать мы используем шину на алтай модификаций как горячий кэш сообщений до которые из который мы можем использовать чтобы при река ногти юзера восстановить ему состоянием также кэш сообщение внутри age in real-time ситуации используется нами для очки типе fall back а то о чем я рассказывал дамы раз десять секунд забираем пропущенные сообщения как раз из вот этого горячего каша собственно ретенция период как в течение которого этот конечно должно храниться должен удивлять как удовлетворять каким-то нашем бизнес требованиям у нас сейчас выставлен ретенция период 5-минутном это достаточно мы хотим это лист vans доставку сообщений в этом месте в пределах ритмичным периода мы не хотим в этом месте терять сообщение и наша шин или алтай модификации должна поддерживать огромное количество топиков подписок вот пользователь собственно каждый пользователь который приходит в messenger подписывается в shiriri алтай модификации на свой собственный канал свой собственный топик пользователи у нас там сейчас полумиллиона до будет наверное больше собственно этих топиков масса плюс эти топике должны быть эфемерными да потому что пользователи приходят пользователи уходит это не какой-то один раз сконфигурированной а система где конфигурация этих топиков постоянно нет они эфемерны они постоянно динамичные меняются как было когда я пришёл в авито в юнит мессенджер для в качестве шины real-time дефекации использовался ребятенку и не сказать что это плохое решение вот все гарантии были соблюдены и каждый пользователь когда приходил устанавливал создавал в ребятя отдельную очередь свою и все работало однако это плохо масштабируется на 100 тысячах пользователей в messenger авито уровень федерации ребятенку да забыл сказать что насквозь не просто ребята уровень федерации ребенку то есть большому счёту это слой инстансов rabbit отщепление had вот того основного ребята которые я показывал до этого и вот на отдельном слое федерации rabbits создавались очереди принадлежащие пользователям на 100 тысячах пользователях мы видели следующую картину брокер ну вот уровне федерации ребята потребляли 70 сфу лидер и 80 гигабайт ram оперативной памяти это очень много и на самом деле все это разваливалась без видимой причины ребятенку написанный or long я не знаю long компании никто не знает р long в логах ничего нет как расследовать не знаю поэтому только restart на спасал но так вышло что последние шесть-семь лет я являюсь автором open source сервера центрифуга все это единственный мой вопрос к вам , он будет а кто слышал про центрифугой можете руку поднять я плохо вижу попрошу моих коллег потом озвучить нет количество рук что такое центрифуга этого panzer сервер ltm сообщений по сути это отдельно стоящей сервер ой так по сути это отдельно стоящей сервер на который приходят пользовательские connect и и этот сервер предоставляет вашему приложению и пей для того чтобы какие-то реал тайме дефекации пользователям публиковать это очень крутая схема для проектов у которых нет встроенного конкор инси да там на питоне вы пишете на джангл вы не можете использовать конкор инси она не поддерживается в языке джанга не умеют async раньше ну если только вы не за качественными в неё специализированный форум warframe вар который за снежан гатчину лс но это по большому счет костыли вот такая схема хорошо вписывается в миг расти русскую архитектуру вы возлагаете connect и на отдельностоящий сервер но и в общем то что можно сказать и центрифуги они ели в socket протоколы свойств для общения с клиентом есть fall back сок джесс популярный fall back для алекса кита в качестве протокол из пояса джейсон или про табу можно выбрать это попс observer через него через центрифугу можно вызывать описи вызова делал терпите вызова есть произнес информация это информация о том находится ли сейчас пользователя онлайн или нет и доли дуться не магазины для нас пункты это кэш сообщение в каналах даты центрифуга умеет хранить тот кэша котором я рассказывал и она скейлится до миллионов соединений благодаря рельсу и консистентными шарди рование радиусов в общем то если кто-то был на моем докладе на galant . кон конференция месяц назад то я там рассказывал чего можно добиться с центрифугой приводил результаты бенчмарков внутри кубер нету с миллионом соединений потом можете найти эту информацию либо меня найти после доклада я вас подробно расскажу о цифрах и собственно моя первая мысль когда я пришел во vita как круто было бы затащить вместо тяжеловесного ребята легковесную центрифугу 3 fox стать на go написано но по ряду причин потому что клиент-серверный протокол у messenger был уже другим какие то есть особенности затащит целиком центрифугу не получилось на потому что это полный рефакторинг всех наших клиентов по большому счету но получилось затащить regis engine центрифуги то есть движок из центрифуги который отвечает за работу с редисом и мы затащили это внутрь messenger авито назвали полученную систему сок stream и сейчас сок стрим у нас стало так да все необходимые гарантии шины которых я говорил соблюдены вместо 70 циpкa на стол 0,3 циpкa то есть это x210 раз прирост по сравнению со схемой с ребятам полтора гигабайта рам на 100 тысяч соединений это тоже там 50 большой прирост после 10 80 гигабайтами а сейчас у нас стало 500 тысяч соединений есть возможность масштабироваться как минимум на порядок дамы делать benchmark я думаю что мы без проблем достигнем цифры 5 миллионов пользователей 10 миллионов пользователей вот дальше уже скорее всего будут проблемы то есть потому что это начинается неизведанной области что там за десятью миллионами пользователей я пока не знаю но кажется что десять миллионов пользователей это такая далекая цель и большому количество интернет-проектов хватит этой цифры это все безболезненны сортирует сюда мы добавляем редис который на самом деле от устойчивый ряде zdes sentinel и сантин элем то есть х и выберите там поддерживается добавляем новый такой instance редиса и так как это используется для горячего каша стрима сообщений ничего не ломается нам не нужны ришар де данные в этом месте с 1 и инстанции релиз на другой но клиент просто получит ошибку при попытке восстановить сообщение из горячего каша он пойдет на в нашу основную базу данных и догонят состоянии из нее но это только в момент ришар 1 га который очень редко происходит время публикации одна миллисекунда в девяносто девятым при свинтили за счет того что ридус чертовски быстр 2 с половиной тысяч в секунду мы сейчас публикуем различных модификаций и мы получаем reconnect всех наших пользователей за пару секунд совершенно безболезненной благодаря тому что редис имеет очень эффективный протокол мы используем pipe лайнинг чтобы общаться с редисом и за один запрос делаем на самом деле как можно больше запросов кризису и в том числе мы делаем подписки бача мида то есть перри подписываемся на подписке клиентов многих клиентов за 1 round trip кризису то есть в этом месте очень эффективный слой и реконнекта очень быстрый в общем то плохо возможно видно мне по крайней мере так было срыве темку и это пример раскладки на сервис сокета удачный я выбрал удачные на то что стыдно показывать неудачный там времена доходили до 15 секунд и так далее да это сделать пирсинг а вот так вот сейчас выглядят раскладка сокета когда у нас 500 тысяч коннектов совершенно бесшовно для клиентов они ничего не замечают чуть-чуть расскажу про баб сад и мы с сашкой аж до котором я говорю про кэш сообщений как это работает достаточно запутанная схема на первый взгляд попробую пояснить то есть вот у нас есть два пользователя до они приходят на сервис socket нас интересуют нижней пользователь юзер 43 оба пользователя подписываются на канал и в одессе да через pops up и собственно все это работать через одно соединение с редисом происходит какое-то событие мы публикуем модификацию вот в правой части там звездочка в этот момент мы за 1 round trip в этот момент мы дергаем в lua процедуры в одессе которые нам за один раунд 3 позволяет сделать две вещи первое это сохранить нотификацию в горячих ожидает и структура данных лист у каждой нотификации есть инкрементальный номер который увеличивается в десятом арно мы делаем лист ограниченного размера выставляем expiration так что память не утекала и после этого мы публикуем сообщение в pops up если пользователь активен ему приходит через pops up notification и в общем то происходит моментально редис попса по очень эффективной окей тут возможно у кого-то возник диссонанс да вот вам нужно от лист вонсо вы ребят используйте попса придется то есть то мы знаем что pops up этот мост vans доставка сообщения поцапались а почему это этот мост vans потому что во первых сообщения в любое соединение в любой момент может порваться даиш и когда соединение среди став самым перри устанавливается все сообщения будут пропущены во вторых во-вторых в том же редис попса бия буферы ограниченного размера да там на самом деле много 16 мегабайт но какие то сообщения могут даже незаметно для вашего приложения дропаться вы о них не узнаете собственно что мы делаем на самом деле мы внутри сервис соки для каждой и для каждого коннекта для каждой структуры данных принадлежащих коннекта храним asset каждого asset клиента в стриме то есть мы этот офсет получаем при в тот момент когда пользователь приходит из того же ряде получаем каждый раз когда прилетает новой модификации мы проверяем что номер новой модификации из попс оба валиден да то есть он + 1 от того что есть сейчас у клиента также новые нотификации могут долго не прилетать поэтому мы периодически синхронизируем состоянии клиентского соединения самостоятельно хотя в редис и и спрашивают а вот такая вот позиция правильного клиента если правильно и оставляем если неправильно и в случае любого несовпадения на этих на этом этапе мы и соединения закрываем вернемся вот к этой съемки соединение закрыли да но клиенты наши каждый раз когда получают входящую нотификацию сохраняет ее инкрементальный номеру себя и при коннекте они восстанавливают сообщения 100 с того номера который мы реально дошел физически и собственно вот теперь да когда мы закрываем соединение в этот момент клиент reconnective и восстанавливает все пропущенные сообщения из сша пара недостатков нашей архитектуры они есть до до совершенства далеко во первых это избыточность хранимых данных мы копируем сообщение нас x2 избыточность это не очень круто для текста наверное это терпимо и это терпимо до тех пор пока у нас переписка в мессенджера we то есть между двумя пользователя двумя пользователями если вдруг по бизнес требованиям мастер джавита потребуется хранить переписку tomcat супергруппы супер чат и да где 100 тысяч пользователей в одном чате такая схема уже не очень подойдёт скорее всего такие чат и мы будем хранить отдельно и будем работать с ними отдельно как то это будет новая сущность и также порядок асинхронной обработке событий не гарантирован да мы видели что у нас есть система ретро ешь на стадии асинхронный обработки это может привести к тому что события будут обрабатываться в таком неизвестном вам порядке для нас это большой больших более не вызывает юзер экспириенсу нас не страдает возможно нам повезло возможно благодаря этим патентным операциям которые мы повсеместно используем но для вашего и из кейса до вам следует задуматься а нужен ли вам тут порядок железно до его на самом деле можно добиться то есть мы могли бы заменить кафку гладковка вера живет на кафку собственно в кафки есть парте церовани и и мы можем события принадлежащей одному use ради железно обрабатывать в правильном порядке мы так не делаем для нас текущая схема работает неплохо выводы и дым fontaine ность наше все собственно старайтесь делать как можно больше операций для патентными до в условиях микро сервисов несовершенной сети временно недоступности ваших сторонних зависимостей вы будете делать ретро и собственно это единственный способ добиться к стабильности надежности до высокого силой вашей системы гм pantene ность ребятенку позволяет сделать некоторые крутые паттерны и обработки сообщений да благодаря fi чем где для трек сенча ttr почитайте посмотрите собственная рассказал где у нас это используется добиться порядочной обработки можно событий в распределенной системе но собственно это сложно это всегда компромисс какой-то протокол и логика движения сообщений системе важнее языка программирования до я почти ничего не говорил про гаи на самом деле не важно какой у вас язык вы пишете на питоне micros there is ok замените все на питон все будет работать а вот протокол и порядок передачи сообщений вот что важно и в socket отличный протокол для отличный транспорт для общения с клиентом но даже сейчас 2015 году вам по-прежнему нужен fall back обеспечьте вашим пользователям доступ к основного функционала вашего приложения это то о чем я говорил это grace ал degradation да вот у нас она есть нескольких местах возможны в некоторых местах ее еще нет иному я стоит добавить это важно если у вас похожей задаче посмотрите на центрифуга доступна на гитхабе google ads легко мне кажется она может вам помочь собственно все это так так можно со мной связаться если у кого-то есть вопросы самое время их задать а тебе можно написать хотел изучать к vita хороший вопрос на самом деле у нас сейчас в авито происходит общение между покупателем и продавцом по объявлению но в будущем возможно появится юзер the user переписки да и есть такие планы вообще вообще круто было бы лайфхак то из ты постишь на авито товар который ты продаешь просто хайлов 2019 и тогда все приходят и пишут тебе в чатик до вопросы начинаем с москвы потом смотрим что есть петербурге новосибирске пожалуйста поехали раз у меня три у мелких вопросов 1 каким образом производится шарди рование вот по use ради какая формула остаток от деления дает нам на количество шар номер виртуального баки то который принадлежит одному из восьми реплика сетов которые у нас манги есть а делаете вы балансировании данных между черными если sharp новые добавляется про mongo db разговор мы делаем ришар ding раз сколько ты лет с запасом и собственно в момент ришар лингамы подключаем еще один реплика сет который вытягивает данные существующего полностью и затем перри раскладываем код с новым распределяем виртуальных bucket of по шагам окей спасибо вы сказали что у вас полная дым патент ность и она достигается за счет ноги идентификатора сообщения нотификации всего остального если пользователь вам прислал два сообщения одним и тем же модификатором ну естественно там риплейс как вы сказали как генерируется идиш ник сообщения на самом деле у нас 2 единственный к один гель с клиентами другой делится на сервере клиенты генерят рандом айди при отправке и на сервере мы также при первой при первом получение получаем не режим еще один айдишник из нас два единственных один из клиентов второй сервер и вот мы их используем как ключи спасибо следующий дальше в графе спасибо за доклад я возможно пропустил но я не очень понял как у вас происходит механизм замечания того что сообщение доставлено получателю ведь у вас там две галочки в интерфейсе 2 рисуется когда она реально доставленное получается что должна быть должен быть точно такой же обратный путь там через рабби thank you с таким же попса об там и так далее да работает все точно также спасибо за вопрос о белых очень хороший подчеркнул что это вот я рассказывал на примере просто от изначально отправки сообщения да конечно у нас еще масса функционала которой осталось за рамками этой пуше мобильные когда человек не of дни онлайн это или статус и доставки и прочитано sti сообщения это все есть это работает на примерно также то человек прочитал сообщение с клиентского девайса улетает рпц ну и дальше примерно такая же цепочка со своей логикой происходит и нотификации по такому же пути достигают другого пользоваться ребята в москве вы сейчас спикеров дискуссионной зоне здесь прямо на выходе поймайте и все ему все все у него спросите вы видите тем более что шепотом можно даже индии подсаливать теперь петербург и новосибирск внимание на экран питер задайте вопросы пожалуйста доброе утро москва добрый день новосибирск у нас первый вопрос от дениса а я прям вижу кому достаточно красиво вопрос у вас не было проблем с websocket ами когда вы их сервера завернули чистку бернетт усы видимо там еще в докер это акула сканер там есть свои нюансы на самом деле то есть не в нашем случае был нюанс такой интересный у нас cabernet из до снаружи кубер не то чтобы попасть внутрь cabernet с обычным уходим через ingresso ingress его вид а в кубе рн эта сосна использовать in джинкс и собственно каждый раз когда состояние сервисов внутри авито меняется появляется новый сервис впереди плова its a young конфиге engine ксари ладится это происходит очень часто это происходит постоянно да у нас миль миллионы сервису хотел сказать много сервисов и собственно если ходить в кубер нетто с через такой ingress и использовать при этом websocket а к соединения будут постоянно рваться мы входим внутрь cabernet из через not part ii это нам позволяет в обход конгресса попадать на сервис socket ну вот такой нюанс и и могу рассказать что-то еще наверное на ум не приходит denis есть второй вопрос да то есть еще один вопрос уже от николая прошу спасибо спасибо за доклад вопрос такой если у вас какое-то время хранения сообщений соответственно подразумевается к но и если нет то насколько высот рассчитана какого вас величинах манги соответственно и что будете делать когда он закончится или вот вы как а за добавляете там новый мощности ришар держитесь у несколько лет спасибо за вопрос сообщение сейчас хранится ведь хранятся вечную да то есть навсегда мы его сохраняем данных у нас не так много сейчас ну что за текст пять терабайт ну хотя для кого-то это может быть много для нас не как мне кажется это не так много пять терабайт данных собственно когда место заканчивается мы делаем ришар ding добавляем новые манги так и живем спасибо петербург новосибирск задайте пожалуйста ваши вопросы там да привет у нас есть вопрос алексей спасибо за доклад у меня такой вопрос я так понимаю у вас статический контент загружается отдельный хранится где-то на сервере и такой вопрос связи с этим у вас получается формируется просто уникальная ссылка потом удается пользователю и происходит ли какой-то какая электризация для получения контента по этой ссылке и вы ходите туда получается весь этот контент вечно все видео и фото смотрите у нас начался конкурс кто удобнее всех сидит задавай вопрос поэтому в москве ребята тоже когда задаете вопрос принимать куни классную позу пожалуйста если я правильно понял вопрос речь про картинки наверное да например да мы загружаем картинки в отдельные хранилища у хранилища в которой мы загружаем есть конфиг есть время жизни этих картинок но я не могу сейчас вспомнить какой у нас выставлен для картинок мессенджера картинки хранятся там есть идентификатор картинки которые нам отдает хранилище мы сохраняем в база данных мессенджера идентификатор собственно потом есть идентификатор и секретный ключ трудного годовыми когда пользователь запрашивает картинки мы преобразовываем идентификатор и секретный ключ через сервис хранение картинок в реальные url-адреса да вот так вот ну и теперь спасибо за вопрос петербург спасибо москва спасибо спасибо тебе большое за доклад отлично аплодисменты все твои"
}