{
  "video_id": "jZc3UfRNtXg",
  "channel": "HighLoadChannel",
  "title": "Как мы сделали собственный Software-Defined Storage для публичного облака Cloud.ru / Сергей Лысанов",
  "views": 723,
  "duration": 2673,
  "published": "2025-01-17T02:35:14-08:00",
  "text": "Всем привет Спасибо что пришли послушать мой доклад и меня зовут Сергей лосанов и я технический Лидер в компании clr и я руковожу разработкой собственного софт для публичного облака компания clr разрабатывает своё собственное пуб Облако которое называется и вле мы поговорим из чего вообще состоит публичное облако дальше мы разберём требования которые мы поставили к СДС для публичного облака перед разработкой и потом посмотрим на архитектуру нашего решения посмотрим на путь а от диска виртуальной машины до физического диска о том как вообще диск который предоставляет Stage подключается к виртуальной машине посмотрим на проблемы с которыми мы столкнулись при разработке и в кон я пока производительность нашего решения любое публичное облако строится на трх компонентах первая компонента - это компьют Это виртуальные машины которые предоставляются пользователю виртуальные машины запускаются на обычных x86 северах с помощью гипервизора км на лисовой операционной системе и почему вообще облако называется облаком потому что оно как бы резиновое если нам надо запустить больше виртуальных машин то мы просто добавляем новые компью в стойку и запускаем новые виртуальные машины далее чтобы виртуальные машины взаимодействовали между собой ей необходимо предоставить Network и виртуальная машина также может иметь публичный IP адрес или ходить наружу в интернет и причём сеть для виртуальных машин надо конфигурировать динамически делать это на литу и за это отвечает софт который называется Soft Def Network и чтобы виртуальную машину вообще запустить ей необходимо предоставить storage то есть диск в простом варианте мы можем предоставить обычные локальные физический диск на компьют Хосте Но тогда мы теряем отказоустойчивость и в случае потери Хоста или диска мы теряем целиком виртуальную машину и более того Мы хотим делать ач детач диска от одной виртуальной машины к другой и делать это на литу То есть тут возникает необходимость предоставлять диски для виртуальных машин по сети причём эти диски должны быть отказ устойчивыми и делат это наш собственный Software define storage о котором не пойдёт речь далее и в начале разработки мы сформулировали Основные требования к нашему SDS для публичного облака storage должен предоставлять три интерфейса первый - это блочный storage то есть диски для виртуальных машин второй Интерфейс - это объектный редж то есть по протоколу S3 который стал уже стандартом дефакто в области и третий Интерфейс - это файловый Сегодня мы не будем говорить про объектный и файловый storage объектный и файловый storage работают поверх основного блочного Я буду рассказывать именно про самый нижний слой про блочный сторедж далее так как публичное облако постоянно растёт к нам приходят новые пользователи то и Stage тоже должен расти вместе с публичным облаком мы должны легко там на лету вставлять новый диск или хост в кластер И тем самым масштабировать как ёмкость нашего решения кластера так и его производительность ж должен быть отказу устойчивым да мы должны переживать выход из строя Хоста или диска таким образом чтобы клиент этого даже не замечал И для этого надо хранить избыточные данные причём избыточные данные надо хранить эффективно мы сразу выбрали способ хранения данных в Реж кодах а не в репликах так как это экономит нам физическое место на дисках и экономит нам железо это по сути такой софтвар Rate 6 мы используем схему 4 +2 для сетевых дисков виртуальных машин и 9 +3 Для более холодного Режа для объектного Режа Что означает эта схема мы дальше посмотрим и также диски должны быть тонкими Fin provision что это означает пользователи лоцируются туда 50 ГБ остальные 50 ГБ Нам бы тоже хотелось как-то переиспользовать перепродавать их и вот вот в нашем продакшене коэффициент пере подписки составляет 2 с по то есть реально пользователи аллоцировать место всё взаимодействие с нашим сторожем происходит через библиотеку lip Client это библиотека написанная на плюсах Она имеет достаточно простой Интерфейс это открыть виртуальный блочный девайс И пописать данные виртуальный блочный девайс по какому-то асету и почитать данные по какому-то асету в нашей терминологии виртуальный блочный девайс - это во ВОМ состоит из чанков ачан - это последовательный кусок данных размером 1 ГБ Чан - это также единица отказа устойчивости и локации в нашем реже чанки разбиваются на чанк леты и вот для схемы 4 +2 чанк разбивается на 4 чанта По 256 Мб и к нему высчитываются е два чанта с чек суммами с помощью Рей кодов чанк леты всегда расположены в разных фейр доменах что это означает то есть чанты одного чанка должны быть расположены в разных хостах если мы хотим переживать выход из строя Хоста сами данные хранятся на чанк сервере Чан Сервер - это сервис который написан на плюсах Он написан на двадцатом стандарте с помощью модных рутин с помощью фреймворка систа это однопоточный сервис и мы запускаем один процесс ча сервера на HD диск или четыре процесса ча сервера на о диск так как один процесс не может утилизировать производительность одного диска и таким образом у нас в кластере получается тысячи Чан серверов ча сервер Он обрабатывает от клиента записывает данные читает дае диска ит лиже устанавливает данные метаданные хранятся в metadata сервере метаданные - это информация о том где расположен чанк и где расположены чанк лета конкретного чанка сам МДС написан на гун это реплицировать машина классическая которая реплицируемый МДС также хранит конфигурацию кластера занимается мониторингом здоровья кластера если что-то пошло не так он планирует задачи на восстановление данных и он также планирует задачи на балансировку данных если мы добавили какой-то новый диск или хост в кластер Давайте посмотрим как вообще виртуальная машина подключает диск сам сетевой диск диск который предоставляет редж подключается к уму с помощью стандартного драйвера который называется V Host User blk этот драйвер использует протокол vhost как Control Plane для настройки шарино памяти между виртуальной машиной и нашим vhost сервером эта виртуальная память используется для virto очереди А verto очередь используется уже как Date через Vero очередь блочные команды от диска виртуальной машины попадают в наш vost сервер и дальше уже сервер эти блочные команды исполняет в библиотеке lip Client lip Client исполняет эти блочные команды взаимодействие с нашим сторожем по протоколу по нашему rpc через сеть и после того как IO попало в библиотеку Li Client клиент вначале должен получить информацию о том где расположен чанк например виртуальная машина записала данные АЦ в нулевой чанк клиент вначале обращается к лидеру МД и спрашивает его Дай мне информацию о нулевом чанке МДС отвечает списком чанк Летов этого чанка и списком чанк адресов чанк серверов и их адресов и далее запись идёт на мастер Чан сервер мастер Чан Сервер - это первый КСМ Чан сервер в списке То есть это Чан сервер который хранит чек Сумы ре кодов а не сами данные и Мар CS он кодирует данные с помощью реже кодов когда на него пришла запись и транзакционная с помощью двухфазного комита и все записи у нас на диск идут с флажками о Direct и о Sync то есть мы клиенту отвечаем Окей на запись только после того как все его данные персистентность на диске также все данные мы покрываем crc32 как на диске так и в Сити мы не доверяем железу и таким образом мы защищаемся от сайнт корап чтение происходит через дата CH сервера клиент также в начале должен получить информацию о расположение чанка если эта информация ещё не закоротил через ма CH сервер ма CH сервер восстанавливает данные с помощью ре кодов и отвечает клиенту и для отказа устойчивости избыточные данные можно хранить как в репликах так и в режу кодах каждый способ имеет свои преимущества и недостатки преимущество реплик заключается в том что они простые в реализации и они производительные так как у нас нет никаких накладных расходов на расчёт избыточных кодов А их недостатки точнее один большой недостаток - это в том что у реплик большой Space overhead если нам надо хранить для трёх реплик два дополнительных блока у нас получается Вех по спейс 200% в режу кодах для трёх оригинальных блоков ABC мы высчитывания м блоками или ти блоками ещё иногда и вместе с оригинальными данными ABC они составляют В итоге для схемы 3+ 2 у нас получается по спейс 6% то есть два блока КСМ только для схемы 4 + 2 у нас орх по Спейс 50% и так далее но недостатки Реж кодов заключаются в том что есть накладные расходы на расчёт Реж кодов на расчёт избыточных данных и также у них есть ряд проблем в реализации о которых мы и поговорим далее первая проблема - это частичная запись страй в ЧМ она заключается допустим клиент записал один блок данных и запись идёт на мастер Чан сервер блок данных мастер пытается посчитать ры коды А чтобы посчитать ре коды ему необходим целый Страйп ему необходимы блоки А и и вначале он читают эти блоки по нее и после того он уже кодирует поку с помощью реже кодов и транзакционных пишет на своих соседей другие Чан сервера Таким образом у нас получается read before Write на каждую запись и это очень сильно ухудшает нси вй рекв по сравнению с теми же репликами и эта проблема очень актуальна для виртуальных машин виртуальные машины очень любят писать блоками по 4К рандомно А база данных любят писать рандомно по 8 КБ блок и тем не менее нам бы хотелось использовать коды экономить место на диске и при этом иметь производительность Ну хотя бы близкую к производительности реплик и мы пришли к компромиссного гибридном решению Случайный записи у нас сохраняются в репликах так называемом код уже хранит данные в кодах и попадают либо большие последовательные запи н либо остывшие данные из h h у нас всегда расположен на nme расположен либо на том же nme если это All FL storage либо на отдельном HDD диске и фризер по по мере остывания данных перекладывает данные из H в из репли коды и таким образом у Надин происходит вун что не влияет на lency клиентских Кстов Ну я бы сказал почти не влияет и получается что Hot storage работает как персистентный кэш в репликах и он занимает 5% от общего объёма nvme диском и вообще вот по разным исследованиям активный working Set в радже то есть количество горячих данных - это 5 тире 15% от общего объёма Режа и также нам помогает накапливать целый Страйп с течением времени что немного минимизирует проблему частичной записи страй он может работать как поверх голого блочного девайса поверх голого диска так и поверх файловой системы он представляет из себя дерево экн в памяти плюс wrl для метаданных и плюс блочный в НМ реализована схема записи под записи всегда лоцируются новые блоки и запись идёт туда а старые блоки освобождаются кож хранит чанты как файлы на xfs и он занимается как раз кодированием ре кодов и транзакционный записью данных и ре кодов и данные обновляются как inplace в файле но обязательно через wr headlock и текущая имплементация коджа имеет недостаток у неё есть Wi amplification и так как данные у нас в начале попадают в Wi headlock а только потом файл но текущая схема относительно простая так как она работает поверх xfs это решает часть задач Ну xfs решает сть часть задач за нас и в целом нам это позволило зариться в ограниченные сроки в будущем Мы сделаем новую версию C над которой уже идёт работа эта версия будет хранить данные поверх голого блочного девайса в ней также будет реализована схема Red как вход и эта версия будет уметь также дедупликация проблема ре кодов - это это вообще распространённая проблема с которой встречаются даже локальные файловые системы когда пытаются сделать рейд например zfs и в ЧМ заключается эта проблема представим что мы хотим блок шт то есть блок изход переместить в ма считает PQ шри для кодов и пишет по сети шшш по какой-то причине пятый ча сервер не смог записать данные может сеть моргнула может питание моргнула а данные штрих и штрих записались и в итоге мы Поли данные за есем востановить блок а или C с помощью реже кодов то мы получим мусор и Чтобы избежать этой проблемы Чтобы избежать корапшн Мы во-первых версионирование режи кодов проводим через в headlock через двухфазный комит и координатором двухфазного Коми выступает Мар CH серве как раз он в начале выполняет фазу prep записывает данные и к суммы журнал после того как все prepare были разложены он делает комит данных также пишет комит журнал и если хоть один prep не прошёл то транзакция откатывается и после то после того как мы накрыли наши Коми сервер имеет право обновить уже данные ре кодов и делать это in файле просто и таким образом мы Амар обновляем данные и коды с помощью двухфазного комита ило в общем-то такая стандартная схема достаточно и третья проблема ре кодов - это большие схемы ре кодов например 100 +2 в этой схеме у нас по спейс всего 2% в общем-то заманчивая схема но любая запись почти любая запись в такую схему у нас превращается в частичную запись Страйп и чтобы прочитать там Страйп состоящий из 100 блоков на нам нам надо выполнить десятки чтений по сети и эту проблему можно решить с помощью так так называемых Дельт вот чтобы обновить ре коды нам не обязательно считать блоки PQ нам достаточно посчитать дельты для P и дельты для Q а Для этого нам надо просчитать только предыдущую версию блока которую мы хотим записать в ре коды и после того как дельты и данные были разложены также через двухфазный комит журнал э Мы также их inb файле уже обновляем и вопрос да выгони Али Мы кажется что мы сэкономили одно чтение по сети вместо того чтобы читать недостающие блоки А и C мы сделали только одно чтение и прочитали предыдущую версию блока Б И вот ответ нет не всегда выгоднее использовать дельты так как появляются ещё Дополнительные расходы на применение этих Дельт вот чтобы применить Дельту Чан сервер должен вначале прочитать предыдущую версию чек суммы применить к ней Дельту записать результат в и только изло уже записать данные файл обновить их данные и вот таким образом для схемы 3+ 2 применение де оно вообще не оправдано но в целом мы как бы экономим Да Одно сетевое чтение но получаем сверху ещё несколько локальных чтений и несколько локальных записей ивв н на котором мы тестировали производительность состоит из сти ж хостов и шести нагрузочных хостов чтобы проводить бенчмарки мы запускали - это такая известная туза для тестов файловой системы и блочных девайсов чтобы использовать F мы написали к ней собственный планчик который использу бите также как и все остальные Лин в пускали 256 клиентов То есть fio у каждого было был один ВОМ 50 ГБ и схема кодирование 4 +2 и рандомное чтение блоком по 4К в целом показывает хорошие результаты это почти 5 млн операций в секунду так как чтение и причём Неважно где лежат данные в хоте или в холде так как чтение у нас по сути это просто чтение с файловой системы и ответ клиенту А запись вход отличается в начале теста Когда у нас hge пустой мы по сути пишем только в репликах и получаем где-то 1,4 млн операций в секунду по мере заполнения Хот сторедж у нас начинает фоне работать фризер и мы получаем 900.000 операций в секунду и вот когда ход Да полностью заполнен это по сути и есть производительность коджа то есть производительность коджа это вот как раз и 900.000 операций в секунду тест на lenc на рандомное чтение и запись с глубиной очереди оди для одного клиента показывает где-то 250 микросекунд на чтение рандомное и чуть больше 300 микросекунд на рандомную запись блоком 4К и также надо отметить что мы здесь использовали rdma а не tcp directory Access это когда пишет сразу в память другого приложения и это происходит если использовать tcp то tcp тратит много пу ресурсов в ядре на mcp и в заключении Мы за 2 года написали с нуля наш собственный Соф в из ЭКО выбрали путь хранения данных в режу кодах а не в репликах так как это экономит нам железо это экономит нам на наших масштабах там десятки петабайт сотни петабайт миллионы долларов и мы дальше Идём по пути снижения стоимости нашего решения мы сейчас работаем над технологиями де дупликации и компрессией данных как я уже говорил проблемы с производительностью Реж кодов мы решили с помощью Ходжа с помощью того что горячие данные Мы храним в репликах и вы можете сами попробовать наши виртуальные машины в платформе Evolution У нас сейчас есть тир Вы можете бесплатно сделать себе виртуальную машину и S3 Бат и протестировать это и обязательно делитесь обратной связью у меня на этом всё спасибо СБО за Если вы смотрите нас в трансляции кликайте по кнопке в плеере переходите в чат задавайте вопросы в чате Если вы смотрите нас в зале вы также можете по QR коду перейти в чат трансляции у этих вопросов будет небольшой приоритет пока что начнём с вопросов из зала Давайте молодой человек В первом ряду Добрый день спасибо за доклад не удержусь это уточнения при Raid H вы упоминали zfs это как раз пример где проблема с хо решена просто вот тощ на се решена Да спасибо спасибо за упоминание Вопрос номер один вы показывали схему проход и Cold storage она была изображена как H Stage стоит перед Ко редм и соответственно вопрос напрашивается кажется это не так это на одних и тех же хостах происходит Да и соответственно проблема похода за Ну когда вход сторожа ничего нету в колта она решается автоматически Да да это более того это в одном процессе это в одном процессе Чан сервера и Чан сервер знает где лежат данные вход или код если в коже пусто он проваливается в Call ST понял спасибо и Последний вопрос вы упоминали что при чтении идёт доступ на дата чанки в случае аварии и недоступности одного из Дато чанков видимо будет какая-то деградация Да производительности Да конечно ну потому что чтение пойдёт на марн сервер там да Надо будет также прочитать данные соседних Чан серверов относительная деградация Какая по производительности получается не измеряли измеряли ну тут Смотря в чём мерить ВХ или ВН если в иох то у нас есть вообще такой Таргет что если один хост выходит из строя то просадка производительности должна быть не больше 30% от общего Ну от обх всего А если по то вы можете в принципе сами прикинуть вот я там цифру приводил Да 200 микросекунд это просто чтение с одного Чара е добавляется сверху п 200 микросекунд чтобы прочи кодов и отдать данные Клин Ну то есть 500 микросекунд так на скидку тут скорее вопрос как понимаем что дата сервера 11 недоступен Хороший вопрос потому что если мы просто на него идём у нас будет таймаут соответственно это будет уже не 30% Да всё верно мы можем в курах если что это обсудить чтобы не держать аудиторию У меня просто ещё один вопрос наверно извинись уж последний вы говорили про чанки и так далее размер чанка размер чале какой у Чан габа зависить от схемы кодирования для 4 +2 чанк лет 256 Мб Ну то есть чанк вьётся на четыре части Спасибо пока пожалуйста с пре предпоследнего ряда вопрос Спасибо большое за доклад у меня такие два вопроса Первый рассматривался ли вариант когда данные ещё не доехали до ко ST но они уже почитают есть попытка их прочитать они будут читаться схо пока яня во смотрите вот пока данные не уехали полностью в Stage они есть в хоте у блочного девайса есть такая операция там ну то есть пробить дырку и пробивания дырок занимается фризер как раз у нас который перекладывает данные из хата в Stage когда он переложил данные в Stage получил о от он пробивает дырку вте только тогда то есть тво ситуация такого что данные уехали а ещё не доехали они точно будут либо там либо там может быть там и там даже в какой-то момент времени А ну то есть данные могут быть прочитаны до того как ещё фризер обработал да да Окей И второй вопрос Когда вы уйдёте от фса какой прирост в опса Вы ожидаете на самом деле не очень большой мы прирост ожидаем я думаю это будет 10-20 про xfs даёт не очень большие накладные расходы на самом деле Вот вы если запустите там просто ну по беч Маркете ми диск и поч Маркете xfs у вас xfs будет на 10 20% хуже Поэтому такие числа мы и ожидаем примерно Следующий вопрос у нас из чата э Стасу архитектура напомнила ceph rbd Действительно ли это так ещё раз Какою А фбд я ждал вопрос про ф Странно что он не первый прозвучал даже не второй Нет это вообще кстати Ну цеф другая архитектура если тут есть цефавора объекты а не чанки там объекты всё-таки более мелкие да и вот у них хранение за хранение метаданных у них там так называемые есть Груп и вот Гру групп есть об сервера и эти об сервера как-то между собой договариваются у кого там актуальные данные у кого неактуальные данные У нас вот в этом плане совершенно другая архитектура у нас metadata Север знает На каком Чан сервере лежат актуальные данные То есть у нас единая точка принятия решений кто Up кто это это больше извини это больше похоже на Google File System это первая статья такая про распределённая система которая вышла там в начале дся Вот это оттуда больше архитектура похоже Спасибо вопрос из первого ряда Ага привет Спасибо за доклад у меня такой вопрос кажется Когда ты рассказывал про схему для вот поддержания каждого там условного диска требуется Некоторое количество потоков на этот Host User blk и вот интересно какую часть ресурсов нады это подключение там съедает эти потоки И как-то вообще влияет может лимитируется Да тоже хороший вопрос Вот я упоминал что мы используем frw cstar cstar это фреймворк который делает с ADB может слышали нет такой Кан который написан на плюсах и они замоченные на фосе и они по кандиру SH архитектуру то есть всё работает в одном потоке но этих потоков может быть несколько но они независимые и эти потоки называются шарды и в сервере мы контролируем количество шардов на скидку Один шар даёт где-то 100 киов и мы можем добавлять туда больше потоков И тем самым Один шар занимает Вот как раз о цпу Ну понятно да один поток и где-то гигабайт памяти мы на него лору если мы хотим больше получать псов с гипервизора так как выхо стоит на гипервизоры где крутятся виртуальные машины то мы просто добавляем туда больше шардов И вот в проде у нас там разные конфиги есть либо там по четыре шарда то есть потока в Хосте либо по вос шардов Хосте то есть смотря какая производительность у гипервизора Дава вопрос пго р у коло СБО зам менее прикладной Вопрос вот Хотят ли у вас пользователи делать снапшоте вашего блочного устройства вы там в какой-то момент сказали что у вас появляется вал во время транзакции транзакция комитет и он блок модифицирует ипс вот Ели у Вас возможность создавать snapshot если есть как это устроено Если нет то планируете ли это сделать Это первый вопрос и второй вопрос по поводу бенчмарком если много клиентов будут писать Ну примерно в одни и те же шарды то Возможно они будут друг другу мешать но в бенчмарках у вас очень хорошие цифры получились и возникает вопрос есть ли у вас какой-то механизм троттлинга квотирования и всё такое прочее первый вопрос про снапшоты У нас они есть а и вход стороже они реализуются легко так как там Red on Wi и там дерево тентов и новый сшт - это просто новое дерево этен сбоку в корже там снапшоты чуть посложнее реализованы там пришлось по приседать потому что мы используем xfs там реализована схема Copy on Write для снапшота То есть когда мы делаем спш и после того как приходит запись э в блок данных этот блок данных копируется в новое место старый блок данных копируется в новое место а уже новые данные обновляются inplace вот вот Это если вкратце про снапшоты а но снапшоты ещё не доехали в в пользовательскую панель для ну для виртуальных машин собственно в нашем Чике в веб Чике вы это не увидите они доедут вместе с бэкапа и вопрос про бенчмарк а нет Ну на самом деле клиенты не мешают друг другу и наоборот если мы сделаем меньше у нас будет лучше перфоманс там будет лучше шли мы там будем попадать в одни и те же деревья тентов может быть даже в один тот же этен и там наоборот будет производительность хуже и Ну как я сказал нет клиенты не мешают друг другу Спасибо Следующий вопрос Здравствуйте спасибо большое за доклад вот у меня такой вопро при ках ситуациях может возникнуть допустим марве ут такое реально смотри тут схема очень похожая навсю схему сейчас буду словами хово говорить Есть такое понятие как вот есть реплика фактор Это ад то есть мы можем продолжать писать даже Коре то есть мы можем потерять один Севе и продолжать писать и при этом Север выберет новый мавер это будет просто следующий сервер Да спасибо и ну и возможно следующий Это вопрос Вот получается весь этот сторедж он крутится для Вик в МХ обычно Linux ильва запись в Лие синхронная через ш если не сказано иначе То есть если реально возникает аварийная ситуация то правда ли что пользователи это не заметят у них данные пока рапт или как-то там хитро это обыгрывается ну правда пользователь это вообще никак не заметит Ну мы про какую сейчас аварию говорим в плане МКА считает что у не вше всё норм там по факту опять же неп смыс не завершается успехом но клиент который этого не замечает не если не завершается успехом то фай система сразу перейдёт как только мыра система линуксом перейдёт поэтому мы никогда не должны возвращать ошибку наверх в виртуальной машине Мы можем внутри себя её бесконечно ретра внутри Хоста то есть мы её можем спокойно минуту ретра если там и больше если со лежит прямо но на ошибку наверх Мы никогда не вернём и мы будем ждать пока эта операция там выполнится и Понятно спасибо большое Есть ли ещё вопросы Давайте вот в проходе второй ряд Сергей добрый день Юрий зовут много вопросов на самом деле но наверное сформулирую кратко Вот почему 4 п 2 вот может быть очень просто но всё равно можете объяснить степень двойки и если делать больше то возникают опять дополнительные накладные расходы на кодирование ре кодов то есть мы ВС равно будем Чава запить на более больших схемах будет ниже и это такое компромиссное решение делать меньше там 3 ПД то уже большая избыточных данных мы уже не экономим место на дисках Ну в общем такое просто компромиссное решение А вот не услышал в сторону каких-то герас вещей идёте то есть растягивать кластер пытались ВХ три дата-центра он э но в каждом дата-центре у нас свой отдельный блочный редж пока в ближайших планах этого нет растягивать а в целом масштабируемость вот непонятно то есть насколько вообще кластер один можно э развернуть Угу смотрите Ну мы э Не масштабируемые ну так на скидку это там по-моему чуть меньше 10 пиб Ну если для фш если для HDD там сильно больше конечно и почему так э по двум причинам неохота класть все яйца в одну корзину в один кластер мы лучше сделаем ещё сбоку один кластер а Вторая причина чтобы сторожова сетка была плоская у нас свечи сорока восьмиквартирная Я скорее примкнула Мне это напомнило Тектоник поэтому такой вопрос насколько ну насколько много можно вместить ваш сторедж теоретически то есть есть ли там узкое место например в Мета дате сколько много Ну вот как я сказал там кластер 10 пиб это вообще типа Нет дебата нет что вот не позволит принципиально Окей metadata Севе как раз да вы правильно сказали у нас metadata сервер он Несмотря на то что он хранит все свои метаданные на диске он весь свой стейт всё равно Ну он этот стейт читает с диска и поднимает в память и в итоге у нас получается где-то 10 гибра на 1 данных так на скидку и в итоге мы просто мся ВТА серве а никак его прошар нельзя а нет такой у нас необходимости мы не хотим до нас пока это устраивает Спасибо Есть ли ещё вопросы у нас целых три приза за лучший вопрос Сегодня И вот сразу появился ещё один кандидат Да Большое спасибо за доклад немножко может быть пропустил а каким образом понимает как где сейчас находится Мастер chang сервер то есть вот этот Discovery АДС отдаёт список чанк Северов Да конкретного чанка и в этом списке Он помечает кто мастер кто мастер решает metadata сервер и Обычно он всегда выбирает Вот чек сум Чан сервер который хранит чек Сума Давайте второй ряд ещё один вопрос Добрый день Спасибо доклад Я может прослушал сколько у вас дата центров сейчас у нас ну сторож всегда развёрнуты блочный в одном ДЦ вообще у нас кластера двух ДЦ и скоро подъедет третий ДЦ но и кроме наших ДЦ это ещ есть то мы в общем используем на самом не только наша компания вот Где гарантия удт из строя что данные останутся доступными если ут Ну с виртуальными машинами Вам никто таких гарантий не даёт ни одно публичное облако для этого специально разрабатывают сервисы уже пове которые называются в обект который уже растянутся на несколько дата-центров и в случае аварий ВМ устанавливается случае аварии этом устанавливается и снапшота Только таким образом делается recover с вопрос с предпоследнего ряда СБО закладкой вопро что использовать cval и двухфазный comit чтобы перекладывать данные с H ST на C А как долго вы делаете тра если вдруг с кол сторожем что-то случилось и у вас все транзакции фейля и как об этом узнаёт metadata сервер вообще о том что у него Ну начинаются проблемы с перекладывание с H Stage на Call storage Угу Хороший вопрос недолго он даже не пытается трать как только storage Точнее там даже fre Ну да он нарывается на ошибку что там другой недоступен Может там диск на другом отвалился и он получает ошибку отдаёт е наверх фризе а фризер сразу жалуется ВТА Север север уже решает Что делать с тем на кого пожаловались он может его исключить он может его попробовать понго если это какая ворт ипра Он решает что Да он на самом деле недоступен то он его исключает из чанка и говорит Чак серверам что типа у вас новые участники вашего чанка вот Работайте с ними и транзакция продолжается Ну то есть это там занимает пару миллисекунд на всё это пожаловаться и получить новых участников раз раз а хотел спросить что кое вот ушли в этот уже H Stage они где-то каширу вверху или нет то есть мы их по сути потеряем которые уже положили или нет Или они там останутся и мы можем их как бы трекать и возвращать они всегда остаются вхо сторе пока они пока мы не получили Окей от Режа пока они точно не ушли туда во заданы пришло время распределять подарки о О'кей А мне понравился вопрос про перформанс хвост сервера про потоки Хоста и про его шарды когда поднимите пожалуйста руку кто его задал сюда первый подарок вопрос про снапшоты пятый ряд у колонны И матрёшка кому она достанется и первый вопрос был про чтение ИС хота и колда да когда не можем ли мы прочитать первый ряд молодой человек В жёлтой футболке Большое спасибо вам за вопросы для спикера у нас также есть подарок на сегодня наша программа закончено но можно пообщаться со спикером в дискуссионной зоне СБО Y"
}