{
  "video_id": "XtbOcUVVDrY",
  "channel": "HighLoadChannel",
  "title": "Сменить 4 системы мониторинга за 4 года и остаться в живых / Александр Качмашев (Точка)",
  "views": 278,
  "duration": 2500,
  "published": "2024-10-29T03:01:19-07:00",
  "text": "а да Всем большое спасибо что вы так рано присоединились к докладу Сегодня я вам буду рассказывать про мониторинг Как ни странно и давайте мы с вами немножечко поиграем в игру а поднимите пожалуйста руки кто 4 года сопровождает систему мониторинга А кто за эти 4 года сменил систему мониторинга хотя бы один раз отлично А меня зовут Саша я занимаюсь тем что вращают в точке Я являюсь инженером и архитектором и Ледо команды которая занимается А тем что называется инфраструктура для разработки и сегодня мы будем с вами разговаривать про мониторинг что мы с ним делали и как вообще в целом выживали 4 года пока меняли четыре системы смотреть на мониторинг мы будем с трёх разных сторон сегодня это будет эксплуатация пользователей и ресурсы соответственно если мы говорим про ресурсы то в основном это конечно какие-то там вычислительные мощности потребляемые дисковые пространства и в целом отказоустойчивость Ну то есть мы платим за то что мы реплицируемый стоимости владения этими ресурсами если поговорить про мониторинг с точки зрения эксплуатации то тут Конечно надо упомянуть то что большинство из нас вращает либо самостоятельно системы мониторинга либо берёт Облачное решение но Облачное решение как мы знаем это чей-то чужой компьютер соответственно там точно так же есть Self hosted решение для тех кто это решение Облачное предоставляет В итоге это всё обыкновенный отдел эксплуатации который занимается сопровождением и поддержкой и если проговорить про пользователей то тут всё очень просто есть прямые потребители этой системы мониторинга Ну а также отдел эксплуатации который это система мониторинга предоставляет зачастую а является сам потребителем своей собственной системы Ну в целом будем говорить о них как о клиентах каких-то систем мониторинга если поговорить про точку и её систему мониторинга то если обобщить наших пользователей то у нас около 600 разработчиков тестировщиков инженеров которые пользуются мониторингом и больше 900 сервисов и здесь сервисы - это вот типа микросервисы нано сервисы Пика сервиса а может быть и типа Монолит который тоже является сервисом и проговорить про мониторинг ни в коем случае нельзя без каких-то цифр которые все так сильно любят прямо сейчас у нас 270.000 активных точек сбора данных с таге для Прометея около 3 С5 мл метрик в час влетает в наш систему мониторинга если в месяц то мы храним примерно 5 трино точек и прямо сейчас в графана у нас там больше 15000 правил Анга для того чтобы люди могли это всё дело смотреть если так влом прикинуть нельзя сза что какойто грандиоз мониторинг где-то около средня может даже Чуть поменьше и весь мониторинг который у нас сейчас есть потребляет Примерно там пол траб оперативной памяти 100 каких-то там вычислительных ядер все в кубернетес привыкли к этому Ну и 4 терабайта дискового пространства здесь написал Ром потому что где-то там в S3 где-то там на дисках и тому подобное в целом 4 ТБ если поговорить про точку и всё что у нас есть с инфраструктуры то мы живём по холо тии А у нас есть Великолепный подкаст в подлодке где Андрей рассказывал про то как разработчики те самые пользователи мониторинга живут в точке в системе холакратия и какие у них есть плюшки и разные преимущества и если поговорить про инфраструктуру для разработчиков Круг которым я работаю мы занимаемся поддержкой железа здесь мы подразумеваем как сеть так и серверы Ну фактически классический беме как у большинства понятно что там есть гипервизоры и прочее подобное а также мы поддерживаем саму платформу для разработки это всякие разные базы как сервис очереди как сервис кубернетес всякие разные логи и как ни странно система мониторинга и мы фактически не являемся отделом сопровождения то есть люди сами катают свои приложения в прот сами за ними следят Всё полностью на их отку если поговорить для разработчиков про инфраструктуру то здесь всё очень интересно получается у наших разработчиков по по сути есть возможность свободно выбрать решение которое их устраивает для того чтобы мониторить в данном случае свои приложения и собственно Круг др предоставляет одно из таких решений и из-за этого нам нужна какая-то конкурентоспособность понятно что большую часть конкурентоспособности мы закрываем за счёт того что у нас есть плотная интеграция со всякими разными нашими системами соответственно требуется как можно меньше телодвижений для того чтобы получить как можно больше результат и е парочку лирических отступлений для того чтобы мы дальше перешли к тому ЧМ будем говорить про мониторинг мы стараемся использовать либо осные продукты либо если опенсорс продуктов нет то это какие-то Community Edition то есть ну в жёсткий энтерпрайз не лезем Ender Log стараемся Ну минимизировать и ещё один пунктик на то чтобы можно говорить Что ой так вы там Не обновлялись ну мы стараемся обновляться достаточно активно но иногда мы отстаём конечно же на полгода потому что всякое в жизни бывает про мониторинг мы с вами сегодня будем разговаривать со стороны удовлетворённости Тех самых трёх категорий которые у нас были объявлены а не про какие-то там внутренние Пети как мы выбирали одну систему А почему вот Конкретно она А почему вы не взяли Вот это этого сегодня не будет здесь просто доклад про то как мы это меняли Почему мы это меняли и почему нам за это сильно ничего не было и про удовлетворенность Конечно же будем говорить трёх наших точек зрения те самые ресурсы про которые мы говорили про стоимость хотим как можно меньше Простите как можно меньше денег платить хотим получить как можно больше производительности за эти деньги Мы очень хотим отказоустойчивого решение и Самое важно из-за того что мы живём на метале Нам очень важно чтобы это решение самостоятельно восстанавливалась Ну то есть все кто живут на метале должны привыкнуть к тому что диски смертные память смертная сервер сдох и Ну пошли поменяли новый поставили оно само накатило восстановилась и ничего не требует соответственно выполнив все пункти получим максимальную урно с от ресурсов все счастливы рады довольны если про удовлетворённость поговорить с точки зрения эксплуатации то получается очень важно получить минимум Мороки в сопровождении этой системы очень важно получить минимум ручных манипуляций Все любят ямалиев очень хочется получать понятную документацию Потому что если нет документации то приходится там либо в кодик ени соответственно появляются и ручные манипуляции Маро и вот это ВС Ну и очень хочется стабильный мониторинг Все хотят очень сильно спать по ночам Кто не хочет всё равно хочет стабильный мониторинг и в итоге собственно выполнив все требования получим максималь Уно с точки зрения эксплуатации А у пользователе вообще всё просто с мониторингом они хотят его стабильный они хотят его достоверный Очень хотят простой интерфейс для того чтобы было всё понятно просто там нагу Лили что-нибудь получили Готовое решение применили всё работает и очень хотят доступность здесь мы подразумеваем как availability то есть с точки зрения пользователей так и то что оно там доступно с любой точки подключения из любого места и в итоге собственно картина мира которую хотят видеть все все группы удовлетворены на 100% все счастливы Давайте будем смотреть что есть про мониторинг нельзя поговорить Если не упомянуть закс поднимите пожалуйста руки у кого был закс а поднимите пожалуйста руки у кого закс остался вот я здесь на слайде не зря написал шарпик и два крестика потому что закс у нас фиг его знает С каких лет До сегодняшнего дня версии мы соответственно обновляли все тоже дотех стабильных иго поговорить схема Здесь всё я думаю те кто хоть раз сталкивался с системой мониторинга ВС понимают Просто клиент Просто куда-то смотрит просто получает свои метрики откуда-то Если посмотрим на интерфейс закса с точки зрения пользователя тут что-то на админское вот прям совсем по админки это всё не просто добавление Метрика и новых триггеров - это какая-то боль прям 100% всё не очень просто вс не очень легко гайды в интернете ну для админов если посмотреть на забег с точки зрения эксплуатации то получается очень интересная штука У нас есть возможность использовать базовый мониторинг мы настроили какие-то шаблоны мы сказали что вот так оно будет для всех тригери соответственно там типа 5% места а пишем Арт о том что чуваки у вас там осталось пять свободного места и в итоге здесь плюс и минус потому что для кого-то 5% свободного место это всё алярм надо подниматься там среди ночи и что-то чинить а для кого-то да Нас ещё там 10 теб типа ещё пару месяцев поживёт ничего страшного и в итоге это всё приводит к тому что появляются какие-то заявки на кастомизацию того самого базового мониторинга надо что-то исправить кому-то 5% превратить там в полноценное число кому-то там проценты срезать и тому подобное Ну Марокко не удобно ручная работа Ну и конечно у закса есть большое преимущество у него есть активный закс Агент соответственно мы его настраиваем он нам позволяет сразу понимать что что-то с системой происходит Она там сама регистрируется очень но вещей сразу готовых тут однозначный плюс если поговорить про забе с точки зрения ресурсов то здесь тоже всё прекрасно у него очень простая архитектура как мы видели в самом начале минимум каких-то движущих частей они все понятны там база данных там сервер здесь агенты и в итоге это всё приводит к тому что в целом у закса очень адекватное потребление ресурсов для того чтобы предоставлять систему мониторингов если он сам пропал то вы соответственно об этом всм узнали сразу понимаете что надо что-то пойти сделать там починить сервер или Агента Если он вдруг сломался И если посмотреть на график удовлетворённости заксом по всем плюсам минусам то получается довольно интересная картинка у нас не особо удовлетворены пользователи Ну потому что там что-то над Минском и в целом сопровождать такую систему с точки зрения простых людей Ну сложновато но суммарное значение Оно такое ну пойдёт пользователям то не нравится поэтому надо что-то менять у нас собственно закс остаётся как низкоуровневый мониторинг там вот прям чисто по админский смотрим то что можно смотреть заксом Потому что многолетняя практика очень легко накатывает Очень удобно Ну и в целом он остаётся утилитарный для тех пользователей которые я не знаю там в лохматых десятых раньше годах настроили себе мониторинг каких-то решений которые сейчас затянуть достаточно сложно и в итоге заксом пользуется и вот в шестнадцатом году мы решили сделать пользователям поудобней мы взяли inf версии О как ни странно потому что версия 2 вышла только в девятнадцатом году поднимите пожалуйста руки у кого был илакс А кто ещё использует до сих пор inf вам очень сильно повезло потому что у него очень простая схема ещё проще чем у закса если посмотреть на систему с точки зрения эксплуатации ТО получается самый громадный плюс который есть у inf - это телеграф Вот кто использует телеграф поднимите пожалуйста руки ой у вас так мало оказывается система очень удобная позволяет использовать кучу плагинов можно настроить всё что угодно очень гибен можно отправлять метрики хоть в Прометей хоть в илакс хоть куда угодно плагинами обшивается по самое не балуйся плюсом и минусом у него есть соответственно вы в Граф метрике как-то складывает и куда-то их отправляет самостоятельно и вот из-за этой пуш модельки у икса здесь плюс-минус Потому что телеграф стоит где-то на Хосте вы его не контролируете люди могут просто начать писать вам метрики каждую секундочку А то и чаще Ну хотим М О божечки если посмотреть на систему с точки зрения пользователей то здесь однозначно большой-большой ПС это интерфейс все кто работает с базами данных а я думаю это большинство разработчиков м просто пишут такой же очень похожий запрос на то как они это пишут в базу очень удобно и Граф вы берёте кастомизированные пишите свои и монитори всё что угодно и пушить хотите пушить каждую секунду пушить каждую секунду для вас это однозначный плюс как бы игнорируем то что это всё будет заваливать систему мониторинга если посмотреть на inf версии о с точки зрения ресурсов то здесь один громадный минус как я уже сказал мы использовали опенсорс ную версию и у него всё очень плохо с катастроф устойчивостью он находится в одной реплике если с ним что-то происходит то он ничего не переживает и если посмотреть на удовлетворённость системой с точки зрения всех наших групп то получается занятная ситуация всё не очень кво по ресурсам потому что он достаточно отказоустойчивый но пользователи эксплуатации в целом довольны Ну и в итоге получаем что удовлетворение от икса Чуть побольше чем от закса Но однажды у инфла версия 1 была обнаружена Великолепная вещь называется она утечки памяти илакс просто вытекал вот он ты в него пушишки он постепенно сжирает всю память и вытекает мы можем противопоставить предварительные ребут соответственно пишем крон джобулда и соответственно из-за того что это всё реута ется из-за того что это всё происходит удовлетворённый системой конечно же падает то есть оно там накопительным эффектом стреляет всё сильнее и сильнее больнее и больнее все от этого устают там даже иногда ребут ежемесячные еженедельные уже не спасают и Надо реута чаще всё больше печаль беда соответственно люди которые на это ВС смотрят такие хаха у вас илюс Мы возм Прометей им свою систему мониторинга они уходят строить свою систему мониторинга у них начинает возрастать конечно удовлетворение потому что они строят что-то своё но по ресурсам мы начинаем проигрывать потому что как ни странно все работают внутри одной инфраструктуры тратят деньги на то чтобы запустить ещё новых виртуалок с Прометея себе там фанчи поднять и прочее удобство ну и соответственно эксплуатация тоже падает потому что у команд разработки которые поддерживают свой мониторинг появ эза в мониторинге соответственно надо там нанимать людей и тому подобное Ну не особо клёво когда есть что-то рядом в итоге это всё сводится к тому что у нас тут появляется несколько разных систем мониторинга и из-за этого оно всё такое шатко валко Но конечно же у инфла всё ещё очень удобный сиквел но он совершенно ненадёжный это всё оказалось первой версией но во второй не сильно ВС поменялось бесплат и вот в 2019 го началась Эра кубернетес мы начали всех затягивать в платформу появилось удобство Мы подумали что надо что-то сделать и такие А давайте возьмём графит вы Спросите Саша почему графит А вообще за что А я напомню всё просто у нас есть бе Metal Прометей он не очень так-то хорошо переживает падение отказы и вот это всё у него нет никакой репликации поэтому хотелось сделать что-то более отказ устойчивое и в итоге мы выбрали схему графита через кликхаус потому что с точки зрения ресурсов у нас появляется большая катастрофа устойчивость кликхаус стоит его можно шортить несколько Дат центров его можно также реплицировать несколько Дат центров очень удобно всё красиво великолепно но из-за того что мы используем осорно решение которое стоит перед кли хаусом оно пишет специальную табличку graffit туда складываются все теги которые у вас есть в Метрика и она бесконечно растёт в 2019 году мы в кучу разных способов испробовали просто написали очистку для неё А вот буквально в этом году на гитхабе для этого решения появилось Иу которая говорит что у вас табличка вытекает Ну и кликхаус не тормозит тут 100% минус Как ни странно если вы используете кликхаус по назначению то он не тормозит А если вы используете кликхаус не по назначению то он внезапно начинает тормозить и не давать того удовлетворения которое есть его приходится там закидывать ресурсами И очень сильно за ним смотреть с точки зрения эксплуатации опять же у нас есть кликхаус у него очень понятная документация его очень легко настраивать за ним очень легко смотреть всё хорошо у нас был опыт сопровождения у коллеги из-за этого мы графит и выбрали потому что всё выглядело хорошо Однажды я проверял скрипт который удалят кластера курса и вме запустил всё на боевом кластер удалился один мониторинг тоже вместе с ним погас это конечно же шутка что коллега уволилась ровно из-за этого но тем не менее у нас пропал опыт сопровождения такой системы соответственно мы начали растить компетенции сами очень много Мороки очень много чтения документации которое Ну как бы очень не хотелось Ну и в итоге изза того что кликхаус тормозит у появляется из-за этих всех проблем то есть графики медленно появляются алёрты медленно работают с точки зрения пользователей мы их попросили всё переписать Они же писали всё на сиквеле мы такие типа А Возьмите графит о удобно Вообще класс вам всем очень понравится Нет были самые отважные наверное полгода эксплуатации мы предлагали всем кто хочет попробовать новую систему мониторинга которая более они были готовы это всё переписать на нас смотрели Конечно же очень косо но в итоге у них что-то доработано очень медленно очень кликхаус всё ещё не был приспособлен к тому чтобы обрабатывать метрики вот так вот В тупую и если посмотреть на удовлетворённость всем этим решениям то получается очень странная ситуация эксплуатация довольна пользователе вроде как Нет ну и по ресурсам там вроде плюс-минус всё Терпимо и как бы суммарное значение мы всё ещё вроде растаем по сравнению с люксом всё неплохо и собственно выводы которые мы себе сделали из использования графита Ну очень медленно э в режиме именно кликхаус с карбон кликхаус и графит Клик хаусом Ну и алёрты Ну скорее всего нет вы не хотите использовать алёрты которые врут потому что либо очень долго выполняются либо просто не выполняются потому что таймаут пришёл и мы посмотрели на всю эту схему которая у нас есть такие Ну возможно ВС же прометеус и в 2020 году мы решили что-то поменять и случилось такое Волшебная штука как мы нашли приколюхи прямо вместе с хаусом и ещё ите очень удобно Как видите это был целый год у графит кликхаус который ходит в кликхаус и собирает трики в какой-то момент времени выпустили новый кого Вы можете в graffit кха прийти не с графит запросом А с promql запросом ну и соответственно для того чтобы хоть как-то оптимизировать то что кликхаус всё же ещё тормозит мы поставили пром который нам нужен для того чтобы несколько дот центров ходить и собирать информацию с нескольких прометеев и соответственно поставили Прометей Федерации Ну чтобы хоть как-то спасаться от того что Прометей дохнет и диски вместе с ним тоже И если посмотреть на всё с точки зрения ресурсов то схема с Клик хаусом У нас осталась у нас есть катастрофа устойчивость У нас есть проблемы с табличка и ха всё ещё тормозит Ну мы ещё добавили себе минусов потому что добавили ещё кучу точек отказа проте пром и ха который про интерфейсе даёт с точки зрения пользователей Ну мы как бы оставили им возможность писать все запросы на граффити и сказали х там есть ещё новый вариант давайте вы возьмёте и напишете всё на пром э ну попробуйте написать всё на пром вдруг Зайд Ну и как бы продавать надо было удобно потому что мы такие ну пром общий мировой стандарт дефакто берёте любую репу на гитхабе смотрите какие там есть графики и всё для Прометея готовая все счастливы все довольны Ну как Point вообще отлично Ну Арто делать всё равно нельзя в кликхаус потому что он тормозит потому что всё будет лагать и опять же для себя как Для отдела эксплуатации мы смотрим на то что prq очень удобно на него легко настраиваются любые метрики которые сообщество уже подготовила У нас есть прокси для тех кто не знает это штука которая позволяет агрегировать запросы из нескольких разных дата Сорсо Прометея мы используем несколько дата-центров дата-центры сильные и независимые соответственно прометеи в каждом стоят свои и пром Нам нужен для того чтобы агрегировать данные в одном единственном месте Он опенсорс у него практически нет документации нормальной и он прям очень прожорливый ну и ха плюс минус у нас всё ещё остались он есть с ним всё удобно но у нас мало достаточно компетенций которые мы постепенно наращиваем и плюсом он тормозит Ну блин и графит кха это однозначный минус был потому что там попытка реализовать пром ql там есть несколько багов Всем надо было писать обязательно скобочки потому что вот такая вот штука И тут конечно Всем надо было к этому сильно привыкать Но если посмотреть на всю эту схему с точки зрение удовлетворённости то получается такое прям средня ково решение пойдёт как бы вот в целом неплохо в этот момент времени те кто уходил делать свои прометеи начали чесать репы мы соответственно из-за того что у нас появился пром Q начали писать разные шаблон для того чтобы людям было удобно переезжать на систему мониторинга они такие хй Так у вас тоже есть мониторинг он тоже на промитей так же как у нас Возможно мы подружимся и в итоге собственно а почему мы пошли вниз а не вверх если начали возвращаться люди а а потому что Хаус тормозит и нам надо было что-то сделать мы это всё дело поменяли поменяли мы это на таноса это случилось двадцать втором году там были Конечно большие муки выбора Как видите версия 021 была в то время и у таноса мы использовали подход с Танос сайд карами и Танос Ром То есть просто запросы бегают куда-то в танов И получаю данные ЕС посмотреть на Танос с точки зрения ресурсов то у него есть большой и плюс и минус он очень прожорливый он сам по себе кушает достаточно много плюсом отправляет кучу запросов во все Прометей которые вы ему подключили Сай карами и соответственно общее потребление ресурсов у него увеличивается Но типа за сч этого вы вроде как получаете достаточно быстрые ответы Кто первый тот и вот это всё и у него есть Ну очень удобно же классно вы метрике храните где-то как все любят S3 - это бесконечное хранилище но здесь плюс-минус Потому что на самом-то деле таносу требуются локальные диски он не чисто в S3 живёт поэтому там пару терабайт просто для того чтобы он обрабатывал данные на себе как-то их компреси и отправлял уже в ЕС потре на с проем с точки зрения эксплуатации сталь и это конечно несомненно плюс никакие запросы переписать не пришлось но тас всё ещё бета и это конечно же минус у него всё не очень клёво с документацией Хотя пытаются у него есть куча багов плюсом из-за того что это бета они Вполне себе любят сделать Break внутри минорного изменения Ну типа мы же бета нам и если посмотреть на та с точки зрения пользователей то у пользователей всё хорошо у них есть стабильная а они не переписали ни один свой запрос который уже написали на пром куэ смотрим на общую удовлетворённость пользователи счастливы для них вообще ничего не поменялось понятно что внедрение новой системы было достаточно плавно мы просто переключили дата сорс у людей как работали графики так и продолжили работать с точки зрения эксплуатации опять же всё неплохо Ну конечно есть пару нюанси с документацией тем что приходится сильно там в НМ ковыряться Ну ресурс он прожорливый со ему накидывать эти ресурсы периодически прямо вот по его какому-то Нони руки Ну и общее удовлетворение у нас вообще на достойном уровне было пока Однажды прожорливый Титан из плюса минуса прожорливой не превратился в полностью прожорлива он начал выедания оперативные памяти там в прометеи в разные Тано скры и прочее И вот однажды с точки зрения пользователей эксплуатации у нас случилась вещь что Титан разбушевался и мы получили вот такую штуку примерно в 00 ночи Ну в разных часовых поясах Мы живём в Екатеринбурге это был 00 ночи Екб мы получили звонки о том что No дата все кто не перевёл No дата ещё с ин флакса Те кто не Переезжал получили кучу звонков о том что всё жизни нет Пора вставать десятки нных разработчиков которые смотрят во всё это дело ну индекс удовлетворённости конечно же падает просто в тартарары и мы такие ну надо что-то поменять Давайте возьмём Викторию matx Ну стильно модно молодёжно у Виктории Рик тоже достаточно простая архитектура всё красиво куда-то пушится где-то там читается с точки зрения пользователей ВС е ничего не поменяли ВС е теже самые запрос изменение для них дата сорса прошло Вообще полностью не замечено ну перестали же появляться но из-за того что мы используем несколько дата-центров они сильные и независимые нам опять надо было вернуть Пром это однозначно и плюс и минус А ещё из-за того что мы живём на метале и мы привыкли к тому что диски дохнут нам надо делать бка А это соответственно ку места ку разных изменени фав В общем закидываем всем что только можно смотрим постоянно что с ними всё хорошо с точки зрения ресурсов опять же есть хранилища отдавали это дисковые из-за того что мел Это плюс-минус потому что ну просто так подкинуть диск достаточно непросто А если вы живёте там во всяких разных облаках то для вас конечно это просто плюс и всё хорошо в жизни А ещё у Виктори нет даулинга то что есть в решениях советах оно оно сохраняет просто первую точку из какого-то интервала это не похоже на правду ну и соответственно из-за того что всё это хранится где-то на метале появляется Конечно вопросики к отказоустойчивости то есть там нельзя потерять больше дисков чем у васк вот за этим всем следить обязательно восстановить данные в общем однозначно минусы но как бы итоговое это значение у нас всё ещ прекрасное пользователи ВС ещ рады у них всё прекрасно Ну эксплуатация ресурсов Ну потерпим что уж тут и общий индекс ультр конечно после таноса громад неше А все заметили Да подвох в 2023 году Мы возвращаемся на таноса потому что спустя полгода разработки они выкатили новую версию 032 там пофиксили кучку багов мы немножечко поменяли схему поставили Танос ресивера потому что очень удобно у людей э кроме пром куэ в плюсах появился глубина запросов понятно что из-за того что есть нормальный дау семплинг мы можем хранить практически бесконечное себе время эти запросы ну и соответственно с точки зрения эксплуатации этот самый дау семплинг реализуется Вообще полностью прозрачно просто пишете в config Как часто вам там что-то сжимать и Танос всем этим сам занимается ничего не надо придумывать он там умеет сам строить графич сам выбирает лучший показатель сум AVG максимум красота ну и соответственно у нас никуда не делись плюсы минусы которые были и раньше Танос всё ещё прожорливый Да мы Понятно чуть-чуть себе скостил Танос ресивера и соответственно пушим в него метрики вместо того чтобы заваливать прометеи запросами ну и в итоге собственно общая схема выглядит достаточно неплохо потому что мы убрали себе с эксплуатации лишнюю мороку потому что оно всё в оно практически Т то есть выкладывает туда и радуемся жизни спим по ночам а и общий индекс удовлетворённости в целом по системе тоже достаточно прозрачный Он всё ещё растёт Что приводит нас к общим выводам которые есть И эти выводы всё ещё про удовлетворённость и они достаточно простые если посмотреть на весь путь развития систем мониторинга то видно что там мы меняли систему мониторинга в целом когда что-то происходило прямо ужасное Ну и что-то с этим надо было сделать и закидывать ресурсами зачастую очень не хочется Ну миллион денег - это одно а бесконечность денег - это другое но если посмотреть на все системы мониторинга которые мы использовали на момент внедрения без вот этого ухудшения в зависимости от того что больше метрик стало прибывать большее количество людей стало пользоваться сервисом большее количество сервисов что начало заезжать то в целом всё ещё видно тренд на то что у нас достаточно стабильно растворение системой мониторинга и из всего этого мы вынесли себе пару весомых уроков которыми я очень хотел поделиться с вами первый мы вернули себе пользователей в систему мониторинга как помните они уходили с инфла потому что всё плохо строили своё что-то там делали но за счёт того что у нас есть опять интеграции за сч того что мы делаем всё легко и просто для пользователей плюсом всегда найти какие-то компромиссы для того чтобы пользователям было удобно но нам тоже А мы получаем возможность как-то реагировать на поведение наших клиентов и соответственно подстраивать под эту систему мониторинга чтобы все были довольны самый большой э минус который есть и самый большой фейл наверное который с нами случился мы приучили пользователей к проблемам это те самые Nod То есть это реб инфла это там смерти таноса из-за того что он передал это тормоза у кликхаус это всё приучает пользователям тому что типа ну система мониторинга Ну она такая себе И вот с этой проблемой нам ещё разбираться ближайшие годы то есть восстанавливать Доверие к системе мониторинга - это не сею минутная задача понятно что она растёт достаточно экспоненциально но тем не менее с этим придётся жить ещё какое-то время Ну и основной момент изза того что есть изза того что мы используем металы классическая проблема закидать железом не всегда получается соответственно есть большое желание выжать максимум соков из того что есть сейчас Найти возможность использовать там максимум оперативной памяти использовать максимум дискового пространства для того чтобы получить желаемый результат и при этом типа не переплатить непонятно куда денег ну и собственно из всех трендов которые есть мы стараемся менять не просто мы такие шаут Отчаянные а потому что нам надо принести общую пользу и общая польза это в том что система мониторинга достоверная система мониторинга можно пользоваться легко просто и удобно на этом У меня всё Всем большое спасибо Буду рад если Вы проголосуйте за мой доклад Саша спасибо тебе большое за доклад я чно многого интересного маленький пода от организаторов меня на следующую систему мониторинга что планируете Не знаю пока надо посмотреть хотя бы полгодика выждать А то мы вот только что таноса поменяли назад Понятно ждём тебя в следующем году закладом как вы сменили се сист мониторинга за 5 лет и у нас вопросы Из зала пока несут микрофон я напомню коллеги Не забывайте голосовать про доклад это полезно организаторам и спикером это реально очень важно а Спасибо за доклад Я здесь У меня похожая проблема у меня появляются пользователи которые должны пользоваться мониторингом и вопрос следующим у вас в начале было большое количество оперативки и цпу на ресурсы А в каком проценте тратится на энд мониторинга и что уходит на для пользователей тот та же графана и прочее Ну то что я показывал в самом начале на слайде про там по раба оперативной памяти и прочее это фактически текущее значение мониторинга во время переезда с викторие то есть там оно чуть-чуть увеличено это чисто про то есть нафа там она стоит в паре реплик и соответственно гигов во оперативки надо всего-то навсего сказал у нас достаточно утилитарный его потребление ресурсов относительно того же там таноса или Виктории но оно вообще минимально у него основной потребитель ресурсов - это база данных соответственно Ну как бы в неё чуть больше вкинутый оперативки там условно 60 Гб на одну Базюк их две Спасибо И у нас следующий вопрос здравствуйте Дмитрий Сбербанк Я могу вам предложить куда раз переть штука такая называется коллеги я предлагаю обменяться опытом в кулуарах мы сможем всё это обсудить и решить что лучше вопрос насколько ваша система мониторинга выросла и позволяет сколько метрик мониторить Если вас устраивает тас сейчас сложный вопрос потому что я негу точку найти от которой вам нужно посчитать значение количество метрик насколько оно выросло Оно из-за того что мы фактически предоставляем Ну Облачное решение по мониторингу какое-то там сегодня нам могут написать миллиард метрик А завтра нам могут написать 200 млрд метрик и соответственно к этому надо пытаться как-то там готовиться Ну хорошо а в качестве если у вас облако то про фану мимир вы пока не занимались А как я уже сказал Я не брал здесь муки выбора потому что фановый мир Мы конечно же рассматривали но у него есть куча нюансов о которых я готов в кула поговорить Я тоже спасибо спасибо за вопрос и у нас следующий Спасибо за доклад А подскажите пожалуйста почему вы про используете а не про как я уже сказа у прок есть возможность э использовать его в качестве обработчика из нескольких дата-центров А дата-центр у нас сильные и независимые ээ соответственно метрики в одном дата-центре не покидают этот дата-центр ну типа нету жизни в этом дата-центре зачем нам метрики знать из другого дата-центра они всё равно новые не дойдут Ага спасибо ещё вопрос У вас там 4 ТБ метрик насколько я помню это за какой срок хранения это 3 месяца понял спасибо и у нас увеличивать время на один вопрос а Александр Спасибо за доклад хотел уточнить вот с точки зрения пользователей вот эти метрики - это больше технические метрики или всё-таки есть возможность например какой-то там не знаю бизнес-процесс да там э метрики сделать чтобы ну некую бизнес логику тоже там отслеживать мы не ограничиваем никак людей То есть в принципе возможность использовать нашу система мониторинга для там отслеживания Бизнес метрик есть понятно что есть всякие дата хаусы которые больше для для этого приспособлены но там условно посмотреть какой-нибудь SL там сло Вот это Великолепное у людей есть возможность ну то есть если мы относим это к бизнес Метрика и ещё один коротенький вопрос оди Добрый день Александро Спасибо за доклад кудра Никита Сбербанк Скажите пожалуйста вы рассматривали кликхаус но я очень ждал что вы бы ещё рассмотрели а Dr вы смотрели в эту сторону или не интересен или как-то просто обошли его стороной и почему как я уже говорил мы выбирали кликхаус потому что у коллеги были компетенции и соответственно его было легко просто начать использовать и там интегрировать нашу схему то есть муки выбора тогда всё ещё были но приоритет был отдан кликхаус с графитом потому что были компетенции интересно хорошо Спасибо и традиционно уже Вопрос Выбери Два лучших вопроса У нас два приза один приз от спикера другой принц от нашего подарочного спонсора Газпрома А я наверное выберу первым первый вопрос самый он достаточно важный был Прошу выйти на сцену к нам с поздравляю и второй а второй Вот это конечно муки выбора как с мониторингом в принципе наверно ну тут вариантов меньше Да нет так-то На самом деле систем мониторинга не так много а а давайте я последнему потому что система мониторинга выбрать очень важно ждём вас к нам сейчас выйдут Саша к нам сейчас выйдут Я прошу вас обойти Моле во давайте а пока человек выходит А как вы теперь передаёте знания когда испытали муки перехода человека с компетенциями которой не хватало всё так же как обычно из уст в уста документацию никто не пишет всё ещё Мы же очень любим документацию использовать чужую свою писать ну ждём тебя в следующем году как вы изменили процесс документирования И сколько систем вы поменяли за год да всем Спасибо Оставайтесь с нами"
}