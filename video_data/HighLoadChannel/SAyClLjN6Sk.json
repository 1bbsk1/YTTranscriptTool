{
  "video_id": "SAyClLjN6Sk",
  "channel": "HighLoadChannel",
  "title": "Cassandra для хранения метаданных: успехи и провалы / Андрей Смирнов (Virtustream)",
  "views": 13274,
  "duration": 3389,
  "published": "2018-07-19T05:00:52-07:00",
  "text": "итак сегодня я буду говорить о кассандре и прежде чем я начну мне интересно сколько из вас уже используют кассандру или знакомы с ней с тех кто пришел супер я не думаю что у нас в россии так много людей использовать кассандру но наверное разговор у нас будет более интересно за счет этого и так меня зовут андрей я работаю в компании archos 304 занимается тем что делает облака для enterprise то есть идея состоит в том что словно amazon делает облику для всех over the stream делает специфические вещи которые необходимо больших компаний мы работаем в полностью удаленной команде у нас командой небольшая и мы занимаемся одним из продуктов ярче стрим одним из облачных решений это облако хранения данных если говорить очень просто то это с 3 то есть три совместимо имею ввиду и wss 3 из 3 совместимое пей в котором можно хранить объекты то есть для тех кто не знает что такое из 3 это просто что-то pipe с помощью которых я могу куда-то в облака загружать объекты получать их обратно удалять там списке и так далее дальше уже более сложные фичи на основе этой простой вещь у нас есть некоторые возможности которые нету amazon и которые нас отличают от одной из них являются так называемые гиа регионы то есть если в обычной ситуации вы создаете кореша будете хранить и объекты в облаке вы должны выбрать регион регион это по сути дата-центр и ваши объекты никогда не покинут этот до центра если с ним что-то случится то ваши объекты больше недоступны мы предлагаем гиа регионы в которых данные находятся в одновременно в нескольких дата-центров но как минимум в двух как на картинке на слайде и соответственно клиент может обращаться к любому до центра для него прозрачно данные между ними реплицируется как минимум то есть мы работаем в режиме active актив и работаем в этом режиме постоянно можно это использовать для разных вещей 1 то что я сказал это большая надежность хранения очевидно да это доступность данных даже при отказе одного из дата-центров и из там интересных возможностей если эти дата-центра далеко друг от друга географически то какой то из них может быть ближе к клиенту в разные моменты времени и вам просто быстрее к данному обращаться в ближайшие к нему дата-центр для того чтобы эту конструкцию которых мы будем говорить разделить на части я представлю те объекты которые хранятся в облаке как два таких больших куска один простой кусок объекта это данные данные неизменные их один раз загрузили потом ясно что с ним может случиться мы их можем удалить если они больше не нужны предыдущий проект у нас был связан с хранением за байты данных поэтому у нас проблем с хранения данных не было это уже было для нас решенная задача и вся бизнес-логика все самое интересное все вещи связанные с конкурентностью обращения записи и перезаписи они в районе мета-данных метаданные оба об объекте забирают в себя ну наибольшую сложность этого проекта и они указывают на те самые данные которые хранятся с точки зрения пользователя это объект с точки зрения нас можем разделить на 2 части и сегодня я буду говорить только о метаданных несколько цифр цифры сегодняшние того что у нас есть если вы на них посмотрите внимательно то первое что бросается в глаза это очень маленький средний размер объекта который мы храним то есть у нас очень много метаданных на тот объем данных которые у нас есть для нас это была не меньшим сюрпризом чем для вас возможность сегодня глядя на этот слайд то есть мы планировали что у нас данных будет как минимум на порядок если они на два порядка больше то есть каждый объект будет значительно больше и объем метаданных будет меньше потому что данный хранить дешевле с ними меньше операция метаданных гораздо до уже и в смысле железной смысле обслуживание смысле различных операций с ними этими этими эти данные довольно быстро изменяются недостаточно высокая скорость изменения это какое-то пиковое значение которые я привёл не пиковой не сильно меньше но тем ни менее можем получать довольно большую нагрузку в какие-то конкретные моменты времени теперь от матовая назад dota цифры это уже работающей системы когда перед нами встала задача мы хотим такую штуку сделать мы хотим иметь гиа регионам хотя уметь актив актив и нам надо где-то хранить с метаданные что это может быть очевидно что это хранилище это базы данных должна поддерживать актив актив очевидного должна масштабироваться мы заранее не знаем мы очень бы хотели чтобы нас продукт пользовался бешеной популярностью как он будет расти мы не знаем эта система должна масштабироваться само собой метаданным должны хранить надежно потому что если мы их потеряем в них была ссылка на данные мы потеряли общем-то весь объект в силу того что мы работаем в нескольких дата центрах и мы допускаем возможность того что дата центр может быть недоступен более того дата-центра находятся далеко друг от друга мы не можем во время большинства операций которые приходят через ebay требовать чтобы эта операция выполнялась одновременно в двух дата-центрах это будет просто слишком медленно ну и как минимум недоступны если второй год центр недоступен поэтому часть операции должна работать локально в одном дата-центре но очевидно когда-то должно происходить некая конвергенция и данные должны быть видны с точки зрения уже разрешения всех конфликтов в обоих дата-центров поэтому консистентной операции должна настраиваться но вот на самом деле под эти требования с моей точки зрения я очень был бы рад если бы нам не пришлось использовать кассандру потому что для нас это был некий новый опыт но ничего другого не подходит да и это на самом деле не кажется печальная ситуация на рынке подобных систем хранения потому что ничего другого нет что такое кассандры это распределенный key will you базы данных с точки зрения архитектуры с точки зрения идеи кота в нее заложены мне кажется все классно ну если бы я делал я бы делал то же самое потому что перед нами стоял выбор когда мы это делали мы могли вроде как написать свою систему хранения метаданных но мы больше и больше понимали что вам придется сделать что-то очень похожее на кассандру и те усилия которым это потратим наверно того не стоит на всю разработку у нас было всего полтора месяца и конечно было бы странно тратить их на написание база данных своей если кассандру вот для меня разделить такие по слоям как слоёный пирог я бы сказал что это всего три вещи то есть это кластеры из узлов каждый узел умеет хранить kill you данные локально то это одна часть кассандра 1 и и слой во вторых кассандра в целом умеет эти данные распределять по узлам кластеров включая репликацию и умеет их распределять так что кластеры может расти или уменьшаться в размерах и данные будут перераспределяться и третье не менее важная часть что когда мы обращаемся из нашего приложения к нашим данным каким-то запросам кассандра умеет наш запрос распределить по узлам так чтобы мы получили те данные которые мы хотим ее стой с тем уровнем консистентной sti которые мы хотим до хотим его прочитать просто кворум или хотим кворум с учетом там 2 дата центров и так далее так далее для нас вот два года с кассандрой представляют из себя вот то что вы видите на графике американские горки русские горки хотите это называйте начиналось все глубоко внизу у нас был нулевой опыт работы с кассандрой нам было страшно мы запустились все было хорошо а дальше начинается такие постоянные падения и взлеты какая-то проблема все плохо мы не знаем что делать у нас какие-то сыпятся ошибки потом мы ее решаем выходим новые новые новые так далее и вот эти американские горки они в принципе не заканчиваются по сей день первый и последний слайд в котором я скажу что кассандр классное носитель на классные это отличная система но если я буду оставшиеся там тридцать пять минут говорит что она хорошая я дома никому не будет интересно поэтому я буду говорить о том что плохо но оно действительно хорошее и одна из тех систем которые позволяет нам иметь там время отклика в миллисекундах то есть там меньше 10 миллисекунд заведомо и это хорошо для нас потому что для нас важно время отклика в целом и операции с метаданными для нас является лишь частью любой операции связанные с хранением объекта будь то получение или запись нам надо еще сами данные записать там пользователь авторизовать и так далее так далее с точки зрения записи офигенное масштабируемость туда можно писать сумасшедшей скоростью если это необходимо но в некоторых ситуациях это необходимо когда мы перемещаем большие объемы данных между записями она действительно отказоустойчивого да то есть действительно падение одного узла в ту же секунду не приводит потому что что то какие то проблем проблемы начнутся рано или поздно кассандра декларирует что в ней нет единой точки отказа но по сути как бы эти точки отказа есть везде и на самом деле те кто работал да наверно знает что даже падение узла и не то что обычно терпит до утра до надо эту ситуацию починить быстрее ну и на самом деле просто то все-таки это проще по сравнению с другими там есть возьмем стандартную реляционную базу данных кассандра проще в том плане что если мы хотим понять что происходит очень часто что-то идет не так и нам нужно понять что происходит у нас больше шансов разобраться дай дойти до малейшего винтика наверно с кассандрой чем с чем то еще после этого у меня 5 история о плохом но я еще раз говорю кассандра хорошая она работает у нас продукт который работает на 5 история плохом это то ради чего я надеюсь вы пришли меня послушать и я пришел вам рассказать 1 историю они в таком хронологическом порядке они не очень друг с другом связаны но первая история была самая грустная для нас так как мы храним данные пользователей самое страшное из возможного это их потерять в чем потерять без возврат на как случилось в этой ситуации у нас были предусмотрены способы как восстановить данные если мы их потеряем в кассандре но мы их потеряли так что действительно не могли восстановить для того чтобы объяснить как это происходит мне немножко придется рассказать о том как у нас все внутри устроены с точки зрения ис-3 есть несколько таких базовых вещей есть вещь это называется baked ее можно представить какой-то огромный каталог в которой пользователь заливает объект у каждого объекта есть имя ключ и связанный с ним там метаданные размер content type и собственно данные объекта при этом размер баки то ничем не ограничен то есть это может быть 10 ключей может быть 100 миллиардов ключей разницы никакой нет и любые возможные конкурентные операции то есть может быть несколько current ных заливок в один и тот же ключ может быть конкурентно и удаление так далее так далее в нашей ситуации когда у нас актив актив операции могут происходить в том числе конкурентно в разных дата-центрах не только в одном поэтому нам нужна какая-то схема сохранения которое позволит такие вещи разруливают да и в конечном итоге мы выбрали ну простую политику что побеждает последние по времени записанная версия то есть если происходит несколько конкурентных операций это не обязательно то что наши клиенты специально это делают это может быть просто запрос который начался но клиент не дождался ответ что то еще произошло попытался снова вы и так далее поэтому у нас есть две таблицы базовые первые таблицы очень простая таблица объектов в ней пара имя баки то имя ключа связывается с его текущей версии если объект удалён то там в этой версии ничего нет если объект существует там есть его текущая версия то есть в эту таблицу ума по сути только изменяя вот это поле текущей версии и таблица версии объектов эту таблицу мы только вставляем новые версии то есть каждый раз когда происходит загрузка нового объектом мы вставляем новую версию в таблицу версии даем ей некий уникальный номер сохраняем там они и всю информацию и в конце только обновляем на нее ссылку в таблице объектов то есть так у нас вот работой но вот например как они связаны и на этом слайде есть например объект у которого две версии 1 текущая одна старая есть объект который уже удален а его версия все еще есть то есть нам надо время от времени заниматься очисткой ненужных версий да то есть нам надо пройтись и удалить то что уже по сути на что никто не ссылается причем удалять мы можем не обязательно сразу же можем это делать в отложенном режиме это наш такая внутренняя очистка мы просто удаляем те вещи которые больше не нужны и вот тут возникла проблема проблема была в следующем и так у нас есть актив актив 2 дата центра в каждом дата-центре метаданные хранятся в 3 копиях то есть у нас реплики три плюс три всего 6 реплик когда к нам обращаются клиенты мы операции выполняем с консистентную с точки зрения кассандра называется локальный кворум то есть гарантироваться что записи и чтения произошло в две реплики в локальном дата-центре это гарантия до иначе операция не выполнится но с точки зрения кассандра всегда пишет во все 6 реплик будет пытаться да то есть 99 процентов времени все будет хорошо на самом деле все 6 реплик будут одинаковые но гарантированно нам 2 у нас была ситуация сложная на самом деле это был даже не гиа регион а мы хранили даже для обычных регионов которые в одном дата-центре мы все хранили вторую копию метаданных в другом дата-центре в общем история длинная и даже не буду все детали приводить но в конечном итоге у нас был процесс очистки и вот тут есть та самая проблема который удалял ненужные версии он работал тоже скасси стэнд ностью локального кворума в одном дата-центре может в двух смысла запускать его нет не буду друг с другом бороться и все был хорошо пока не оказалось что наши пользователи еще иногда пишут вот другой дата цена чем мы не подозревали так у нас все было настроено на всякий случай для файлового но оказалось что они уже пользуются этим мы и больше часть времени все было хорошо но рано или поздно возникла вот ситуация которая есть на этом слайде в одном do that обод это центра от реплицировать запись в таблицу версий но запись в таблицу объектов оказалось только в одном дата-центре во второй не попал соответственно процедура чистки запись запущенная в 1 верхнем дата-центре что она сделала она увидела есть версия которое на которую никто не ссылается и и удалила причем ударил не только версию но и само собой данные ну все полностью уж это просто ненужный объект да и это удаление безвозвратные ну и конечно дальше происходит бум потому что у нас таблица объектов осталась запись который ссылается на версию которой больше нет так мы первый раз потеряли данные потеряли их действительно безвозвратно ну благо немного соответствует что делать в нашей ситуации все просто так как у нас данные хранятся в двух дата-центрах вот этот процесс очистки является процессом некой конвергенции синхронизации мы должны читать данные с обоих дата-центров и этот процесс будет работать только тогда когда оба дата-центра доступны но так я говорю он отложенный процесс он не происходит в процессе обработки и пи это не страшно у эту консистентной это такая особенность кассандры 2 в 3 немножко есть лучше есть экспертность это называется к вору в каждом дата-центре но в любом случае есть проблема того что это медленно потому что на во-первых приходится обращаться к удаленному дата центру а в случае consistent насти все то есть это шесть узлов это означает что он работает со скоростью худшего из этих 6 узлов то есть настолько медленно сколько у нас есть куча узел в кластере в конечном итоге до ну одновременно мы тем самым происходит процесс так называемого редре п р то что в кассандре называется когда не все реплики и синхронные то то есть где эта запись не прошла этот процесс одновременно их чинят а так устроена кассандра но когда вот это случилось нам поступила жалоба от клиента что вот объект недоступен мы разобрались поняли почему мы конечно же первое что мы захотели сделать это узнать сколько у нас еще таких объектов и мы запустили скрипт который пытался найти конструкцию но вот похоже на вот эту дату когда есть запись в одной таблице на запись в другой нет и вдруг вы обнаружили что с таких 10 процентов но ничего хуже наверное не могло бы быть если мы не догадались что дело не в этом проблема на самом деле в трубу в нашу базу данных прокрались зомби и такое полу официальное название для этой проблемы для того чтобы понять что это такое надо поговорить о том как работает удаление в кассандре вот например у нас есть какой-то кусок данных x он за записан идеально реплицировать на все наши 6 реплик если мы хотим его удалить как и любая операция в кассандре она может быть выполнен не на всех узлах ну например мы хотели там гарантировать консистентной там что там два из трех в одном data center то она выполнилась на 5 и на одном запись осталось может быть узел в этот момент лежал если мы вот так вот удалим и потом попытаемся прочитать с такой же консистентную хочу 2 из 3 то кассандра увидев значение и его отсутствие интерпретируют это как данные да то есть почтения обратно расскажет о данные есть хотя мы их удалили поэтому удалять таким образом нельзя поэтому кассандра удаляет по-другому удаление на самом деле является записью когда мы удаляем данные кассандра записывает некий маркер который называется тумстоун соответственно надгробная плита он небольшого размера и он помечает что данные удалены таким образом при чтении если мы читаем одновременно маркер удаление и данные кассандра всегда предпочитает маркер удаления в этой ситуации это говорит что данных на самом деле нет да то есть это то что нужно замечательная система tombstone это маленький маркер но понятно что если мы удаляем удаляем удаляем данные когда-то надо идти маркеры удалять они будут копиться бесконечно поэтому тут словно есть некое время жизни который конфигурируется то есть рано или поздно тумстоун и тоже удаляются уже данные удалились сдались маркеры и когда маркера нет в ситуация эквивалентной ситуации когда данных нет что может произойти есть такой процесс кассандре кто называется repair починка задачи его сделать так чтобы все реплики были синхронны потому что у нас возможно разные операции в кластере может быть не на всех узлах выполнились может быть мы меняли размер кластера добавляли реплики убирали может быть вас какой-то узел когда-то упал там жесткие диски и так далее но у вас мог быть реплики не консистентной repair делает так чтобы они были consistent вот смотрите мы удалили данные у нас есть где-то маркера удаления где-то остались сами данные но мы repair не сделали вот она в таком состоянии прошел период и маркер удаления исчезли ну просто вышел их срок жизни да и вместо них осталось пустое место как будто данных нет если после этого запустить репер который должен привести реплики в консистентной что сделает репер он увидит что на одних узлах есть данная других их нету что надо сделать надо их восстановить соответственно все шесть узлов будут снова сдан и вот эти самые зомби это данные которые мы удалили но которые вернулись plaster обычно мы их не видим но если мы к ним не обращаемся какие-то ключи возможно там у них ключ к случайной если ничто не ссылается мы его не увидим но если мы попробуем просканировать всю базу данных попытки там что-то найти ну как вот мы тогда искали а сколько же у нас таких вот записи где мы дали или объекты эти зомби очень мешают решение очень простое но довольно важная repair в кластере нужно делать в любом случае но бывают разные ситуации когда либо мы не успеваем сделать repair то есть он идет очень долго потому что это одна из самых тяжелых операций для кластера она связана с этим сравнением данных на узлах но в любом случае что мы должны сделать чтобы вот период через который удаляется тумстоун и был больше чем интервал репер интервал репер это то что нам удается делать то с чем мы вот мы знаем что мы успеваем там кластера 3 перед 10 дней в 20 дней в неделю в три дня но период удаление тумстоун должен быть выше этого значения которые постигается только из практики потому что если мы слишком будем агрессивно и парить мы окажется так что кластер плохо отвечает на frontend запрос следующие еще 1 класс андре классическая проблема для кассандры и натана что часто на пару с часто с этим сложно бороться на самом деле итак у нас в с3 есть bucket как я сказал может быть произвольного размера дав может быть 10 ключей может быть 100 миллиардов и одно из этой который мы должны поддерживать это отдать список ключей в баке причем список должен быть отсортированным он отдается в само собою постранично его можно так сказать листать и он должен быть всегда консистенция с текущими операциями если я там записал объект удалил объект беру список ключей он тот же самый как и после моей операции не могу в этом отложены строить как мне это и 5 реализовать у меня есть таблица объектов которую я показывал bucket ключ там текущая версия но в общем то вроде та самая таблиц которая мне была бы нужна чтобы построить этот самый список ключей но есть небольшая проблема я в совершенно правильно выбрал для этой таблицы в качестве первичного ключа пару bucket и ключ первичный ключ определяет где эта строка будет находиться на какой узле по сути да это та самая почему хэшируется объект когда он будет храниться в кассандре но это одновременно означает что ключи одного пакета хранятся на разных узлах ножки говоря на всех если их достаточное количество потому что они все равномерно размазаны с точки зрения хранения этой таблице это классно потому что я не знаю меня makita мог быть разного размера совершенно я не могу заранее угадать какой большой какой маленький себе я данные одного пакета хранил бы на одном узле то у меня была проблема с масштабированием вот так они у меня разум размазана равномерно по всему кластеру с точки зрения этого хорошо но с другой стороны понять шли я не могу никаким образом построить список объектов в кластере из такой таблицы значит мне нужно что-то еще не нужен какой-то еще способ с помощью которого я мог бы этот самый список объектов получить нокий кассандра говорит что у нее есть некие более сложные конструкции я говорю я могу завести еще одну таблицу специально для списка ключей в боккетти я в нее положу ровно ту информацию которая мне нужна мне нужен bucket мне нужен ключ и мне нужно некое минимальное количество метаданных объекте чтобы построить ответ на запрос я использую так называемый то что называется в кассандре композит ки и если я построю к этой таблице запрос который мне нужен я хочу выбрать данные из баки то начиная с какого-то ключа и чтобы они были отсортированы запрос работает он делает ровно то что мне нужно рад я да я конечно рад у меня все получилось но кто только что слушал внимательно предыдущий слайд я сказал что если у меня первичный ключ и с баки то это означает что все данные кладутся на один и тот же узел но проблема на самом деле хуже в кассандре есть некое шизофрении потому что разные слои кассандры по сути разговаривают на разном языке тот слой с которым мы сегодня взаимодействием чаще всего представляет кассандру как нечто отдаленно похожее на реляционную базу данных с таблицами с запросами похожими на из квелли так далее и вроде как все хорошо но есть внутренний слой данные как на самом деле кассандрой хранит и исторически он был первичный к нему был свой и пей который был совершенно другой и вот такая конструкция как я здесь написал внутри на самом деле хранится как длинная строка в которой каждый ключ на данной ситуации ключ в баке является отдельной колонкой и чем больше у меня размер баки то тем больше колонок в этой таблице когда я делаю запрос и я этого не вижу и об этом никак не могу узнать если я попытаюсь залезть там на уровень или почитать посмотреть как оно устроено да я могу это узнать но что это означает что ширина от этой строки количество колонок оно равно в моей ситуации размеру мы его баки то почему она кстати работает хорошо потому что колонки на физическом уровне хранятся отсортированные по имени поэтому в моей ситуации любой отсортированный выборка вроде как классно но в кассандре есть куча операции когда она оперирует целиком значением колонки даже если я спрашиваю дай мне сто ключей а там хранится миллион ей опять зависимости от версии они поменяли но ей приходилось буквально прочитать всю строку в миллион оттуда выбрать 100 все остальное выбросить для того чтобы построить ответ на мой вопрос но если вы представите что эти еще данные распределены по нескольким узлам да ведь это же нас несколько реплик и любой запрос на самом деле тоже не запрос конкретные реплики это по сути запрос который пытается построить консистентные представление по нескольким узлам одновременно и если у меня есть там миллион колонок в одном узле миллион в другом миллион в третьем формально для того чтобы построить такой ответ мнений не я не могу просто как бы что-то сделать простой я прошу дай мне пожалуйста 100 ключей которые больше такого то значения если все узлы идеально совпадают это да а если не совсем то этот запрос такой ну с точки зрения squee летом с лимитом свр и так далее становится всем нетривиально короче кассандр пытается вот такую широкую широкую строчку протянуть в памяти к она тянет это в памяти она еще написано на джаве ей становится очень плохо вот эта конструкция называется лаш парте шин она как бы возникает незаметному что пока к данных немного десятки сотни тысячи десятки тысяч ключей даже сотни тысяч все хорошо но потом начинается экспоненциальное падение с точки зрения производительности узлы начинают падать garbage collection не справляется и так далее так далее м получается такой просто каскадный эффект плюс к тому это большая широкая строка репрессирована и не только один узел падает опадает их сразу много потому что у них у всех одинаковая проблемы мы конечно же когда начали делать мы об этой проблеме знали вы подумали что надо что-то сделать поэтому нашу таблицу которую мы используем для листинга объектов мы заранее предусмотрели в ней возможность разбросать данные одного баки то по нескольким колонкам мы и буду называть partition условно позиционировать и и так чтобы оказалась на разных узлах и чтобы у нас не было large портишь вас к нему есть 2 требования мы хотим чтобы каждая такая partition была ограничена в размере то есть не больше чем сколько-то ключей и несмотря на то что мы данное разбросаем мы хотим чтобы могли получить этот сортированный список ключей быстром что это ваши был изначально задачам ради этого все иначе когда мы запустились мы так и не придумали как распределять данные и мы в качестве вот этой колоночки кипиш везде использовали 0 и как всегда бывает фичи они идут впереди любых улучшений которые не приносят непосредственного профита точки зрения продукта поэтому конечно же пропустили тот момент когда эти самые партиции стали большими и у нас было несколько очень веселых месяцев когда мы прикручивали решение уже к системе которая находится состоянии агонии практически но давайте обсудим как это можно было сделать вот о чем мы сразу подумали но ok хорошо у нас есть баки то у него есть куча ключей давайте все ключи прогоним через камни хэш-функцию и распределим так по партийцы первая проблема хорошо какого размера должна быть это это вот хэширование до какого кокаином н выбрать если мы выберем слишком маленькой у нас будут ваш портишь слишком большое тоже как слишком много моих распределим не понятно при этом мы ничего не знаем размере багета заранее да то есть он может расти а может изменяться может уменьшаться ну а самое главное если это просто хэширование то получается что свойства отсортировано sti пропала и в каждой партиции лежат какие-то ключи чтобы получить от сортированный список мне надо делать запрос ко всем партийцы им объединять результаты от них крайне неудобный крайне неэффективно особенность этих мортис и станет много 2 ну достаточно очевидный раз мы хотели чтобы свойства отсортировано сохранялась давайте использовать какой-то префикс ключа если мы возьмем сколько-то символов слева и ключи как-то хорошо распределены то мы можем распределить по парте цем каждая будет префиксом каждым будут ключи причем они отсортированы если мы знаем какие ключи нам нужны мы знаем в какую партицию обратиться и так далее но тут опять возникает та же самая проблема как угадать каким образом именуется ключ именуем ключи ним и именуют наши клиенты как они их называют кто-то их называет условно тому как md5 хэш он идеально подходит под эту схему у кого-то 1 там 30 символов это константа у всех ключей да и или что то еще да то есть мы не можем угадать эта схема работала бы хорошо только если мы знали как выглядит ключ в конечном итоге мы пришли к схеме когда мы будем динамически угадывать как распределены угадывать анализировать как распределены и ключи и это распределение со временем может измениться baked может расти может меняться структуры ключей мы будем адаптироваться под этом и динамически с помощью таблицы их распределять то есть таблицы будет указано что вот ключи с такого-то по такое-то лежат в такой-то партиций с такого-то по такое-то в такой вот такое условное решение с префиксом только префикс сложный и динамически это и не совсем это префикс в конечном итоге мы это сделали нам пришлось немного повозиться потому что здесь много интересных задач и таких вполне себе научных когда мы имеем какое-то состояние bakida сейчас каким-то образом он разбит на партиции потом мы понимаем что какие-то партиции слишком большие слишком маленькие нам надо найти новое разбиение которые с одной стороны будет оптимальным то есть она размер каждой партиций будет меньше там какого-то нашего предела и они будут более-менее равномерный и при этом переход от текущего состояния к новому требуют минимального количества действий ну понятно любой переход требует перемещения ключей между партициями но чем меньше мы их перемещаем тем лучше у нас это получилось это наверное самая сложный кусок всего вот сервис если говорить о работе метаданным в целом да который занимается вот этим подбором распределения и мы его переписывали переделывали и делаем до сих пор потому что всегда обнаруживаются какие-то так сказать клиенты или некие паттерны создание ключей которые бьют слабое место этой схемы ну скажем и предполагает что пакет будет расти более менее равномерно какой-то распределение подобрали и дальше все партиции растут но у нас нашелся клиент который ну грубо пишет всегда в конец но в том смысле что у него ключи всегда отсортированном порядке и он все время бьет в самый-самый конец в самую последнюю партицию которая растет там с такой скорости что там за минуту это может быть 100 тысяч ключей свободно а сто тысяч это примерно то значение которое в одну партицию влезает мы просто не успевали бы отрабатывать этот алгоритм и нам пришлось для этого клиента ввести специальное предварительное распределение так как мы знаем что как выглядит его ключ из мы видим что это он что все похоже мы просто начинаем ему заранее создавать пустые партиции в конце дабы чтобы он мог туда спокойно писать а мы пока немножко отдохнули до следующей итерации когда нам снова придется перераспределить ну и все это происходит в онлайне с точки зрения того что мы не останавливаем операции могут быть операции чтения записи можно в любой момент запросить список ключей он всегда будет консистентной даже если мы находимся в процессе перри разбиения это довольно интересно но это получается с кассандр и здесь можно играть такими трюками связанными с тем что в кассандра умеет разрешать конфликты да то есть если мы в одну и ту же строку записали два разных значения то выигрывает то у кого таймс темп выше обычно таймс темп это текущий time stamp но его можно передать вручную то есть например если мы хотим строку записать значение которое в любом случае может быть перетертый если клиент сам что-то запишет мы копируем какие-то данные но хотим чтобы их клиент если вдруг вам одновременно с нами пишет мог перезаписать сверху мы просто копируем наши данные с time с темпом чуть-чуть из прошлого тогда любая запись текущая она будет заведомо перетирать что мы делаем неважно в каком порядке мы их записали мы раньше или такие вот трюки позволяют сделать это в онлайне но если кратко если вас схеме данных намечается что-то похожее на наш партий шин надо сразу попытаться что-то с этим сделать надо придумать как вы будете разбивать как вы будете от него уходить рано или поздно это возникает потому что любое такой инвертированный яндекс но как его то о чем я рассказывал нас есть bucket ключ в объект и нам нужно из bakida список ключей по сути это индекс ну рано или поздно он возникает практически в любой задаче причем partition может быть большой не только от данных она может быть большой от еще тумстоун of которых я говорил до этого от маркеров удаления маркер удаление точно так же с точки зрения внутренностей кассандра мы никогда не вижу вас на иру снаружи но внутри они являются данными и partition может быть большой если в ней много что чего-то полно удалять но что удаление является записью тоже об этом не стоит забывать ещё одна история она на самом деле такая постоянная история от начала до конца что-то идет не так да например вы видите что время отклика от кассандра вы время отклика выросла до отличает медленно как понять как разобраться и в чем проблем потому что никогда не бывает такого внешнего какого-то сигнала окей проблема вот та вот я вот просто для примера график это время отклика кластера в целом усредненная да на этом графике видно что у нас проблема у нас максимальное время отклика уперлась 12 секунд 12 секунд внутренней тайм-аут кассандра это означает что она тайма учиться сам а потому что если будет тайм-аут выше 12 секунд означает что скорее всего работает garbage collector и она не успевает даже ответить и в нужное время она отвечает сама по таймауту но и и время отклика как я говорил должно быть в пределах там в среднем до на большинство запросов там 10 миллисекунд у нас здесь средняя выскочила уже в сотне миллисекунд есть ответы по тайм аут что-то не так глядя на эту картинку невозможно понять в чем причина но если ту же самую статистику развернуть по узлам кассандры я могу увидеть что в принципе все узлы более-менее ничего но один узел у него время отклика отличается на порядки скорее всего что-то с ним какая-то проблема то есть просто к статистика по узлам изменяет картинку полностью эта статистика со стороны приложения но и она является на самом деле очень часто бывает сложно понять в чем проблема что когда приложение обращается к сандре она обращается к кому-то узлу используя его как координатор то есть она дает и запрос и координаторы его перенаправляет к репликам с данными те уже отвечают и координатор формируют конкретный к конечный ответ обратно но почему координатор отвечает медленно может быть проблема с ним как с таковым до то есть он тормозит и отвечает медленно а может быть он тормозит медленно потому что у медленно реплики отвечают то есть если реплики отвечают медленным с точки зрения приложений это будет выглядеть как медленный ответ координатора хотя он здесь не причём здесь счастливые ситуации видно что узел отвечает медленно 1 и скорее всего проблема именно в нем что-то в нем происходит такое что приводит к проблеме всегда сложно понять что не так статистики нужно много нужно много мониторинга нужен график во-первых со стороны как приложение так и стороны самой кассандры потому что если кассандре совсем плохо со стороны кассандра ничего не видно можно смотреть и на уровне отдельных запросов на уровне каждые конкретные таблицы на каждом конкретном узле может быть например ситуация когда у нас у одной таблице то что называется в кассандре со стаей был отдельные файлы и для чтения кассандре приходится грубо говоря перебрать все и состоит из них слишком много то просто процесс этого перебора занимаясь слишком большое время и чтение начинает проседать там решением является комплексом который уменьшает количество этих is as they love но надо заметить что может быть всего на одном узле для одной конкретной таблицы может быть у вас так как кассандра написано к сожалению на джаве и работает над живые может быть она в garbage collector ушел в такую паузу что просто не успевает ответить когда garbage collector уходит в паузу не только ваши запросы тормозят но и взаимодействие внутри кластера кассандра между узлами начинает тормозить и узлы друг друга начинают считать ушедшими в доу ну то есть упавшими мертвыми и начинается еще более веселая ситуация потому что когда узел считает что торговый зал в дауне он во первых к нему запросы не направляет во-вторых он начинает пытаться сохранять данные которые над ему нужно было бы реплицировать на другой узел себя локально начинает себя потихоньку убивать и так далее бывают ситуации когда это можно проблему решить просто с помощью тоска тюнинга так кассандре есть много внутри thread пул фиксированного размера может у нас достаточно ресурсов все хорошо все замечательно но просто тратил надо увеличить она может больше обработать она просто не может потому что мы слишком много в нее отправили ну и наконец может быть нам надо со стороны драйвер ограничить конкурентность иногда бывает ситуация что мы просто слишком много конкурентных запросов отправили и как любая баз данных кассандра справиться не может уходит в такой таскать клич когда у нее время отклика растет экспоненциально а мы пытаемся еще и больше и больше работы дать всегда есть какое-то контексту проблемы до что происходит кластере работает ли щас repair на каком узле и в каком ки спейси с какой таблицей у нас например были проблемы с железом до смешного что у нас была бы лишь часть узлов ну как то вот работает медленно не справляется в чем дело обнаружено что для того что в bios у них процессор ставил в энергосберегающем режиме ну по какой-то причине время начального providing a железо вот так остался у него примерно 50 процентов было проснулся процессор по сравнению с другими узлами вплоть до того очень тяжело на самом деле разобраться может это выглядит как сложно такой симптом например вроде как узел делает комп action делать как ты его медленно другие быстрее его делают иногда это связано с железом иногда не связано с или знает просто баг кассандра очередной поэтому мониторинга нужно много и и обязательно и чем сложнее фича в кассандре чем дальше она отстоит от простой записи и чтения чем больше с ней проблемы тем быстрее она может кассандра убить но между при достаточном количестве запросов поэтому если есть возможность не надо смотреть на какие-то вкусные фишки и пытаться их использовать лучше их избегать по мере того как это возможно ну не всегда возможно конечно рано или поздно приходится последняя история про то как кассандра испортила данные в этой ситуации это произошло внутри кассандра это было интересно мы видели что примерно раз в неделю у нас в базе данных появляется несколько десятков испорченных строк то есть они буквально забиты мусором причем кассандра во лидирует данные которые поступают к ней на вход скажем если строка то должно быть utf-8 а там лежит такой мусор что она не utf-8 и кассандра ничего делать не дает потому что при попытке ее даже удалить или что то еще так как все что поступает на вход utf-8 я не могу удалить значение которое не является utf-8 потому что я не могу vr вписать никак потому что любой ключ который я даю он должен быть utf-8 но они появляются появляются там примерно как вот такая вспышка в какой-то момент и дальше их нет течения опять нескольких дней или недель и мы начали искать проблему мы думали что у нас там был проблемный узел с которым мы возились потому что то с данными делали состоит ли копиров или может быть там может быть все таки есть какой-то у этих данных можно посмотреть их реплики может быть этих реплик of есть общий узел такой наименьший общий делитель может быть какой-то узел дает сбой нет ни фига может быть что-то с диском на диске данные испортились нет ни фига может быть память нет разбросаны по кластеру может быть это какая-то проблема репликации один узел все попортил дальше реплицировать плохое значение нет ну наконец может быть проблема приложения у нас это начал возникать в кайф 2 кластерах кассандра один работал на версии 2 1 2 там на 3 что то то есть вроде и кассандры разные а проблема одна и та же может быть у нас сервис отправляет плохие данные ну верилось с трудом к центры во лидирует данные на входе а не могла записать мусор но вдруг ничего не подходит мы бились долго и упорно пока не обнаружили маленькую проблему а почему это у нас на узлах есть какие-то граждан фиджи в которых вы как-то особо внимание не обращались и как-то в стык трейси подозрительно выглядит garbage collector и почему-то некоторые стектрейсы тоже как-то мусором забита вот пока мы не поняли о ну почему то джем используемые с 2015 года какую-то старую версию ну и это было единственным общим что объединяло кластера кассандры на разных версиях кассандра я не до сих пор не знаю в чем была проблема потому что в официальных release snow софтом живым про это ничего не написано но после обновления все исчезло проблем больше не возникло пчелы возникало ни с первого дня она возникала в кластере начиная с какого-то момента но вот оно началось он работал на том же самом живыми довольно долго какой урок из этого backup бесполезен данные мы выяснили а не портились в ту же секунду когда были записаны ну то есть вот в момент когда не входили в координатор они были уже испорчены то есть там бэкапить бесполезно какие-то колонки были не повреждены вы могли эти данные прочитать частично восстановить но в конечном итоге приходилось восстанавливать из разных источников у нас был backup метаданных в объекте в самих данных что восстановить связь с объектом мы использовали логии так далее то есть мы смогли восстановить некоторые были испорчены но в конечном итоге очень тяжело доверять базы данных если она ваши данные теряют без даже без какой-либо с вашей стороны какого-то действия поэтому в качестве совета старайтесь иметь какую-то независимую копию данных из которой вы можете восстановиться если это необходимо это может быть каким-то там решением последнего том уровне пусть на это потребуется куча времени там ресурсов но чтобы что-то было какой-то вариант который позволит вам вернуть данные ну и последнее про кассандру когда вы с ней начинаете работать возникают постоянное ощущение особенность в этом переходите ну например там словно газ хороших баз данных спасбросок ребята а как вообще тестируете релиза всех тестируете когда иметь потому что возникает ощущение что если в этом релизе исправили баг предыдущего то обязательно добавили нового причем бакыт они там не какая-то фигня это обычно там испорченный данные или что-нибудь ещё или что то еще какая то еще некорректное поведение чем сложнее фичи тем больше с ней проблем багов и так далее да там знаменитый рэппер о котором я рассказывал который чинит консистентных данных стандартном режиме когда он опрашивает все узлы он работает хорошо но в режиме туза ming ree ментальном когда он пропускает те данные которые не изменились со времени предыдущего рипе что логично вполне он был заявлен давно он формально как пища существует мы все говорят не в версии 21 не используйте его не когда он обязательно что-нибудь пропустит в 3-ем и исправил когда вышла 3 несколько из нас канет в 3 и справа использовать его нельзя есть вот там список из пятнадцати багов вот поэтому не в камне с 4 мы сделаем лучше и я им не верю вот а это большая проблема потому что с ростом размера кластера поэтому надо следить постоянно за их бак трекером и смотреть что происходит без этого с не к сожалению жить невозможно если такой наброс если разбросать все базы данных по такому спектру предсказуемости для меня кассандра она вот где-то слева в красные области чаще на плохая надо быть просто готовым что она непредсказуема в любом смысле этого слова и в том как она работает и в том как что-то может случиться вот чего я вам желаю я вам желаю найти другие грабли и на них наступать потому что с моей точки зрения несмотря ни на что кассандра это хорошо и возвращаюсь того слайду которого я начал и это несомненно не скучно то есть вы не заскучаете с ней спасибо у нас есть немножко времени ответить на вопросы простите что я так долго есть кто-то не успеет я готов в коридоре 1 раз работает пардон спасибо за доклад за кассандра беда хотел задать пару вопросов 1 как бы много проблем которые были здесь озвучены они на самом деле не совсем кассандра вский а просто вы когда начинали делать вы не знали какие пользоваться потому что статические про мире ключи и так как бы черный болт шрифтом выделены во всех гайд лайнах проблему в самой кассандре даже если мид и на там собран 22 миллиона два миллиарда записей на на парте шиндо то есть поэтому выбирают статически ключ это антипатра если вы не знаете что вас там реально будет очень мало данных и вы реально хотите хотите их хранить на одной ноте вот поэтому на таких жутких все учатся вот тоже сама касательно джема но я не совсем уверен что это проблема кассандр если это старый живым нет проблем кассандры в том плане что она написана но в этом плане давайте это называется не пишите базы данных донатом это бессмысленно как запускать ее на сервере у котором там сдвоенная память это например ну вроде никто в разум и все не память уже со сбоями и делает но java еще до сих пор не подросла до уровня но у меня два вопроса в частности это как бы так просто впечатления 1 это касательно там стонов пробовали ли вы настроить комп action чтобы он удалял их быстрее потому что комплект шин в процессе компонентом штаны они тоже состоит из удаляется и это мог помогла бы вам унизить понизить риск таких проблем которого возникали меня так как раз если я в этом проблеме рассказывал если мы там стал он удаляем слишком рано комплексом красиво удаляют да я просто старался не входить детали вать to repair торжествен восстанавливает данные если мы repairs не запустили до того как вы имеете виду ваши плеер или кассандра kassandra вский но когда комп action проходит он же потом штампу тоже заменяет данные если два там сторону с большим темпом он удаляет в единении то есть я эту картинку рисовал то есть у нас есть два узла на одном остались данные на втором тумстоун ну я случае как будто центров окей да не в не полная запись соответственно если я не сделал repair и том что он ушел то у меня остается данные пусто данные пуст во время реперы это будут данные плюс данные о чем говорил что repair должен быть чаще чем да да но это вместе игра из камбоджи естественно не решает это надо как вы правильно сказали но по поводу формулы этой что он должен быть чаще чем чаще да да а repair это то что происходит с практикой ласкал кластере там лежит 5 мегабайт там repair занимает 10 секунд до но когда у вас пластыри лежит 20 гигабайт to repair может занимать 10 дней 14 дней там 15 дней и очень сильно зависит и от не консистентной sti данных то и как это ни странно звучит чем чем больше различий на узлах тем дольше работает repair да это такая динамическая вещь и маленький вопросик еще что вы используете для мониторинга ндс центр или что торгуя для мониторинга самой кассандрой у родной мастера и и родную статистику которую она экспортирует обертку и я у нее уже есть репортёр внутренней но он такой ролл и амиду они полнее но мы мониторим очень много со стороны приложения да то есть по сути стороны драйвера спасибо там вот есть вопрос зайд я не вижу у кого микрофон раз-раз-раз-раз павел звук кстати вот вопрос про грабли мы на самом деле нашли это уже на логичный граблей они называются сцилла вот это пор ты для аналог тоже кассандр нарисованный на баржах вот вы и правда как мы очень а каждый раз когда вот на этом графике общения с кассандрой мы уходили в провал очень хотелось поверить что силы это то самое тот серебряная пуля которая нам поможет мои и активно тестировали но мы ещё ни разу на ней не пробовали в продакшене да зовем так у вас какой то есть негативный ссылается что это кассандра переписанная носи на си плюс плюс там людьми которые умеют программировать но в отличие носком деле начался такой небольшой холивар по вот того что лучше использовать будущем проекте кассандру либо стелу мы там проводили аналоги аналитику по гитхаба смотрели количество комментов комиссаров сколько релизов там детского проекта развиваются ну пока что думаю будем использовать пиццы лу но если у вас новый проект как минимум опробуйте она очень становится гораздо более зрелый со временем очевидно это был довольно молодой проект и им там буквально два года каталка они начали да и новое это очень разумно выглядит то что они по крайней мере обещают все спасибо последний вопрос окей последний вопрос не будет а всё спасибо если какие-то вопросы я буду коридоре"
}