{
  "video_id": "QaUbVgZ3ehM",
  "channel": "HighLoadChannel",
  "title": "Эволюция инструментов и сервисов в Badoo / Антон Турецкий (Badoo)",
  "views": 1389,
  "duration": 2487,
  "published": "2019-01-14T00:09:38-08:00",
  "text": "меня зовут антон и я раба ну так напомню про подует забеге за и think a по версии издание задней pendant и в bodum и держим по-настоящему высокие нагрузки давайте только посмотрим на эти цифры 350 миллионов сообщений наших пользователей мы обрабатываем ежедневно на нашем ресурсе зарегистрировано 390 миллионов пользователей на минуточку это почти пять процентов населения всего земного шара 60 миллионов пользователей ежемесячно производит какие-то активные действия на нашем ресурсе и каждый день мы получаем порядка 300 новых регистраций в буду я работаю системным инженером уже чуть больше чем 7 лет и давайте посмотрим как же выглядит наш отдел системных инженеров в цифрах и так нас 10 человек в нашей команде сидим мы 2 офисах часть нашей команды находится в москве часть нашей команды находится в лондоне на нашем попечении находится больше 3000 машин и мы обслуживаем только две с половиной тысячи только внутренних инстансов различных сервисов наша команда ежемесячно успешно закрывает около 350 различных сложности задач и так наши основные задачи это чтобы всегда все везде работала и никогда ничего не ломалось но если немножко конкретизировать основные наши роли то мы занимаемся тем что мы вводим новые машины и поддерживаем про структуру занимаемся capacity планинга и сопровождение инфраструктурой также мы занимаемся диплом внутренних сервисов но к этому диплом внутренних сервисов не относится тепло и кличке кода для этого нас есть специальная команда релиз инженеров которые занимаются выходкой кода для production также мы занимаемся тем что мы принимаем на обслуживание какие-то новые сервисы делаем ним сопроводительную документацию и такое одно из основных умений сотрудник нашего отдела это 24 на 7 быть готовым к тому что придется решать что-то очень сложное и необычные это всё замечательно я вот сказал про людей сказала про нашу ответственность но человек такое создание которого в принципе свойственно думать и поэтому работы обслуживая решая задачу своем отделе мы пришли к определенным выводам и я собственно сразу эти выводы вам покажу и пройдемся по ним подробней и так мы пришли к первому выводу которая говорит о том что сервисы не любит перфекционизм немало важный момент заключается в том что сервис вам нужен творческий подход иногда тебе кажется что сервис то перестал устраивать но возможно стоит посмотреть на него под каким-то другим углом и какое-то решение придет очень важным моментом является то что инструмент нужно выбирать для решения задачи они придумывают задача под конкретный инструмент и последнее но не не наименее важно это то что нужно уметь продать свой сервис свой инструмент который ты делаешь и так погнали сервиса не любит перфекционизм что вы это значит давайте вернемся на какие-нибудь 57 лет назад посмотрим на нашу отдел что от нас вообще хотели что мы должны были делать у нас было две основных задачи первой задачей это взять баре металл железо выкатить на него операционную систему и 2 под задача тоже довольно важную серьезно эта поддержка конфигурации как это выглядело наверное в большом количестве вариантов но в частности у нас это выглядело так что у нас для такого голову тепло и всегда присутствовал стандартного доха цппк и о из профиля vidiq самойлик или kickstart и не важно для сервис конфигурации использовались стандартные инструменты виде arsenka scp и вот такой magic это виде там ваш скриптов каких-то перу пару скриптов какой-то такой различные ерунды которые могла взять определенные директории положить куда нужно и все работало в принципе на тот момент наверное это было неплохо и я думаю что данная конфигурация вполне себе же жизнеспособно на данный момент если вы занимаетесь диплом для знаю 10 машин в год или 10 машин на месяц такая конфигурация будет работать но время шло и как бы мы понимали что этот инструмент не идеален поэтому мы все таки решили уйти от каких-либо самописных скриптов мы обозначили себе 2 2 области которые мы будем заниматься для сетапа оборудования вы начали использовать искать для поддержки конфигурации вы начали использовать паппет паппет мы выбрали на самом деле потому что у нас у некоторых ребят в отделе уже на тот момент был опыт общение с по пятам чьих тогда только набирал обороты в общем таким вот решением посидели посмотрели решили выбрали и на решение с по пятам и собственно говоря остановились в принципе нормально да мы сделали одну систему сбоку поставили вторую все это запустили поехали но время на месте не стоит и идеального инструмента мы не могли не могли сделать хотя бы потому что завтра когда у тебя появятся новые требования инструменту скорее всего тебе начинает казаться что инструмент уже не настолько виде аллен поэтому кратенько пройдемся по тому что мы делали вот с первым и со вторым инструментом извините как я уже сказал про xcode он нам нужен для вот такого головы сетапа оборудование мы поняли что два основных направлениях которые надо улучшать это то что касается различной шаблонизация профилей для операционных систем и вторая часть консольной серая поэтому для шаблонизация мы взяли и просто в тимплей тиков нарисовали там какое мы хотим делать парте церовани и если у нас машина с большим datasette а мы не хотим доделать were формат диска то мы можем сохранить его при инсталляции ли какой-то определенный раздел и собственно добавили и пози торе и с аддонами и базовой репозитории мы раскидали по версии мажорными норм операционных систем которых мы используем что же такое консоль консоль это собственно тот монитор который вы видите и чуть ранее когда мы только защита пили наш кластер секс к там консоли мы продолжали пользоваться либо либо мордочку либо это было встреч с консолька и на данный момент мы централизовано все это дело перекинули через api mantle и это помогает нам очень часто увидеть если у нас например получилось какой дубак trace ядра когда уже на системе не не работает ничего и кроме как консоль китай вряд ли где-то это увидишь бывают такие ситуации когда ну что-то у тебя ночью возможность сервер может произойти у тебя нету доступа в основном у интерфейсу и если у тебя есть возможность попасть туда хотя бы через консольный интерфейс не выполняя каких-то дополнительных набор набора действий там вломиться ps а еще куда-то далеко и у тебя есть свой шар konce это круто ну и бывают такие ситуации когда у тебя считаются опять же операционную систему что-то где-то там какой-то затык случился и неплохо было бы взять и посмотреть что у нас происходило дальше с по пятам спать там все достаточно просто и достаточно обычно у нас перестал справляться один мастер с нагрузкой и стала несколько как только у нас появилось несколько мастеров очевидно что нам понадобился балансировщик на котором мы смогли тир минировать с цель частично отдавать какую-то статику и различные дополнительные извращенные методы там появились и маги сертификатов творить и потому что не очень удобно менеджер эти сертификаты по питон разных машинах как этом синкай подписывать и так далее когда есть 1 года это просто удобней и так напомню что мы тепло им свои сервисы и для diplo из сервисов мы в качестве основного инструмента мы рассматриваем паппет и так что же такое сервис понятие эти abs вот как он выглядит для нас сервис это обычно какой-то бинарный собранный файл не важно будь это гол будет вести или что нибудь еще такое и конфиг конфиг или конфиге почему это является проблемой почему я вообще об этом говорю сразу возникает несколько вопросов как хранить бинарные файлы где хранить бинарные файлы как их потом раздавать решение которые могут прийти сразу в голову мы берем схему с мульти мастером в первом случае мы берем каждому каждому мастеру поднимание key file stor и вот они так отдельно живут во втором случае мы берем на multimaster растягиваем нфс неважно что в обоих случаях возникают определенные сложности который говорит о том что если мы поднимаем несколько file storage но надо изобретать инструмент для синко случае сетевой шары это больше вопрос на любители то есть кому то нравится кому то нет мне не сервера им нравится вариант хан эфесом например но важным моментом здесь является еще и то что я вот нарисовал такого человечка видео инженеры нашего отдела которому нужно всегда при тепло и подать выполнять два действия первым действием он меняет код в китовом репозитории кот кот пакета и вторым действием он должен положить кого-то файлик как показала практика такая работа оказалась неудобной и вот это вот не удобно она была вполне измеримы измерялось она в количестве когда кто-то правит код он соответственно уже уезжает там в продакшен манифесты файлик не успели за sing отели если этот multimaster он не успел везде доехать на ноги мы получаем ошибку файл мы не получили все делать да вот возьмем закинем файлов гид бинарные файлы в git тянет как минимум каждый раз когда мы делаем push у нас готовый реп весь код разъезжается попадет мастером да идея закинуть binary в git ну такая себе идея наверное мы осознаем что размер репозитории истории будет безусловно расти но нам это удобно мы делаем один раз это мы делаем один раз commit мы это дело пушин круто каждый день и репозиторием и клонировать не собираемся сделали поехали с этой схемы мы на самом деле прожили приличное количество лет и следующий вывод о котором я хотел бы сказать что сервисом нужен творческий подход иногда кажется что дело в инструменте но на самом деле нет даже в нашем случае это было не так и так почему же описанная ситуация являлось проблемой появляется следующая задача мы не хотим больше вносить наши изменение сразу в мастер ветку которая разряжается по всем машинам мы хотим такую некую deep мы хотим тестировать стороне но что что этому помешала просто взять и сделать из из готовых решений приходит на ум поддержка по пятам динамических окружений в детей есть этих детей есть бранча вроде казалось бы это это хорошо но как я уже сказал ранее когда мы накидали бинарных файлов в репозиторий история росла наша репозитории дорос порядка наверное 218 гигабайт соответственно вытягивать каждый такой branch достаточно дорого потому как когда мы тянем branch с нашим кодом мы получаем фактически такой клон нашего мастера только там это нет не просто переключение ветки что приходит в голову то что нужно что то делать очевидное решение репозиторий 18 гигабайт надо с ним что то делать давайте делать что мы с ним можем делать мы соответственно удалили что нам не нужно было мы переписали историю о удаленных файлов и решили попробовать решение виде лардж git lfs large files top собственно то как работает lfs я думаю что многие знакомые но визуально на это можно посмотреть на начальном этапе у нас есть наш бинарный файл в нашем виде объектов нашем beat'em репозитории что мы с ним делаем мы берем бинарный файл на самом деле отправляем этот сторож файла в репозитории фактически нет если мы еще дополнительно перепишем историю но на месте файликов готовым репозитории у нас появится такой текстовый файлик текстовый файлик с метаданными которые с помощью которых мы всегда можем стянуть что-то из lfs а по итогам наш репозиторий стал вот меньше 1 гигабайта с тех 18 которые были изначально и казалось бы вроде как уже все решили и вот таким образом как изображено на этом слайде выглядит наш такой назовем его до назовем просто попить сервер то есть это несколько контейнеров с разделенным внешним ворьем собственно в котором находится непосредственно сам код в одном из контейнеров находится исключительно бинар бинарник попить сервера сбоку есть помогать иные контейнер который мы называем гид puller который собственно всякий раз следить за тем что в этом происходит в репозитории если изменение произошло он делает пул и немаловажный момент что он ещё делает detail of a switch git lfs ведь как раз таки занимается тем что он берет и вот эти файлики с метаданными которые вот были обозначены как бы тусим линк и вместо этого simulink он берет и подтягивает файлы это нужно потому что пакет должен знать о контрольной суммы все остальное чтобы файлик отдать дальше клиента и получается так что проблема по факту остается то есть наш гид пул офис lfs ведь занимает вот такой вот размер это опять же 13 гигабайт если мы говорим дальше на ветках и a branch of the докажет я пришел на работу захотел сделать им какой-то тестовый ticket сделано выбрали запушил его и в итоге я получаю что я даже для этого теста водить это должен выкачивать те же 30 гигабайт что можно сделать можно пойти простым путем можно взять и договориться внутрь из ребят внутри отдела что у нас будет условно tree branch и у нас будет прот у нас будет 100 edjing будет testing мы наверно смогли бы жить в этом случае с этими 13 мегабайтами но давайте посмотрим что мы можем сделать мы точно знаем что когда мы делаем какой-то тестовый branch мы обычно занимаемся заменой кода то есть там кидать binary какие-то новые свежие в тестовый в раньше никакого смысла нет давайте посмотрим как же попьет агент обращается накопит сервер и что мы можем сделать с этим дед запросам если посмотреть чуть более внимательно то мы видим что у нас в запросе всегда присутствует ими файлы которые мы хотим скачать и у нас есть январь это собственно тот самый то самое динамическое окружение которых чистом виде лежит приходит от запроса как я говорил ранее между всеми нашими мастерами мы используем балансер виден джон xa поэтому мы возьмем и просто ну ложки в определенном лакей шуни мы возьмем и подменим любое значение аргумента января mind на продакшен тем самым наш пакет сервер примет следующую структуру если мы так сделаем у нас точно так же остаются пакет серверы гид puller но в dip puller помещается еще один дополнительный сервис под названием 10r тонкий который собственно занимается тем что следит за нашими тестом и ветками при появлении на сервере он поднимает новое окружение при исчезновении при удалении ветки сервера соответственно на сервере он чистит этот код и дело в том что в этом случае мы для уже пула наших тестовых branch и мы не делаем никакого у нас вечно поэтому но у нас по факту там вот новый брони заводится вот так а сразу все это хозяйство работает итак о том почему стоит выбирать инструмент под задачу сделали вот эту клевую штуку сделали branch и как на это смотреть то есть идея же было не в том что мы можем просто сделать какие-то бранча там уменьшить нашу репозитории мы же хотим узнать как наше изменение отразится на наш инфраструктуре мы хотим видеть сколько машин применили изменения сколько машин перезапустит какой-то сервис у кого файлик изменился у нас был и есть elastic search поэтому почему бы не использовать elastic search для сбора логов с папито гента и потом что-то как-то рисовать поэтому схема работы получилась такой мы делаем новый branch вносим туда ходить свои изменения закидываю branch на сервер скриптом либо еще каким-то инструментом какой нравится мы запускаем либо на всех квартирах либо на определенном срезе кластеров побед один в режиме noob он называется в памяти но собственно драй ран причем флажок комок мы можем регулировать как на стороне сервера для своего бранча так собственными агенте после этого отчеты со всех нот и уезжают в ластик search мы формируем красивые ссылку на акебоно дашборд собственно вот отличная демонстрация того что elasticache подходит вполне себе здесь для такого сбора логов он решает решает у задач которая была поставлена и решение это получилось за мне кажется может 1 2 рабочих дня я думаю все знают прокате ссылки внимание я не буду рисовать еще одна задача которое опять же подтверждает вывод о котором я говорю о том что надо брать инструмент под задачу сбор логов докер-контейнер of очевидная штука есть куча шиберов виде все слог внд реально огромное количество но для начала давайте посмотрим какие контейнеры есть в буду который мы обслуживаем и в чем разница есть контейнер и наши есть контейнер и не нашим собственно наши контейнеры это те контейнеры или образы которые полностью нам подвластны где мы можем рулить число гамме и чем чем угодно и все остальные этого тамаля docker hub или что-нибудь еще и замечательный момент присутствует еще в том что количество наших контейнеров она достаточно измеримо то есть вот есть там у нас условно там тысячи сервисов их останется 1000 ну может напишите еще там сервисов 200 нормально а вот все остальные за счет того что вот контейнеризации она проникает в различные отделы все что хотят все переезжают на контейнер вот это все остальное нарастет и так как же устроена схема сборки логов в тех контейнерах которые нам подвластны которые мы называем наши у нас есть вот система на ход системе всегда присутствует syslog который слушать видели порт есть какой-то абстрактный контейнер в котором работает демон и внутри контейнера есть сервис си слогом как же может писать демон при желании демон может взять и написать в diff lock демон может взять отправить на хвостовой машину сразу на юге report ростовом и суслову соответственно как бы создатель сервиса не писал это как бы его выбор вообще не проблема в конечном итоге должны сообщений которые отправлялись diff lock по правилу по умолчанию из такого контейнера поедут на чистовой syslog после чего соответственно не будут уже отправлено по правилам кастовой тачки куда-то там центральное место в нашем случае это будет власть и search что делать со все остальные мы не очень хотим заниматься вот этой вот поддержкой настройки версий слога в каждом каком то притянутым за уши контейнере надо как-то решать за тут надо попытаться решить задачи в общем плане у него должно быть решением но так как многие люди при работе с докером очень любят смотреть налоги и использую докер лакс заходя на машину то соответственно это стало одним из основных условий для решения задачи и что у нас получается у нас получается что из всего того набора который мы можем использовать остается вариант с джейсоном не остается вариант журнал by jason не интересно давайте попробуем журнал d и как же получилось как устроена сборка с использованием же наладим у нас есть докер демон который по умолчанию пишет журнал не по умолчанию мы его просим написать журнал d соответственно надо иметь надо не забывать в случае использовании такой схемы о том что там есть различного рода ride in шины и просто про это стоит не забыть потому что может получиться такая ситуация ты точно знаешь что твой контейнер тебе установилась что-то там плюет постоянно ты запускаешь logs это видит что он ничего не происходит возможно что сработал как раз таки ограничитель на прием файликов журнал дим дальше у нас в стороне станут как бы сбоку стоит то есть не журнал де пишет в syslog syslog который берет и читает файлики журнал deep проверяет по определенному сценарию если там присутствует имя контейнера выполняет какие-то свои набор фильтров и после этого кладет файлики на локальную систему в чем такая сложность как бы быть почему так сложно потому что захотелось довольно кастомных фильтров именно на стороне как же мы читаем из журнал де в каком формате куда мы будем писать потому что контейнер и там тоже могут быть разных дед нам нужны дебаг логине нужны где то мы пишем дайсона выступал где-то нет где-то у нас там django trace со странным multiline им то есть вот это все должно быть это можно менять и дальше у нас есть файл бит который собственно читаете в локальной файлике и отправляет их либо напрямую власти ксеркс либо прогоняет их через луг стоишь в данном случае файл бит возможностью тех настроек которые ему можно указать и вот такой контрпример который говорит о том что не нужно делать задачи под инструмент и здесь такая холивар на я достаточно тема на самом деле по поводу выбора регистратора и если мы говорили про scheduler регистратор года два или три назад наверное можно было подискутировать на тему таких минусов но модов и так далее чем дальше мне кажется что купер найти с уже становится таким стандартам наверно индустрии все дальше к этому идет всем нравится все им пользуются но когда мы очередной раз поставили себе вопрос что мы хотим от оркестра так как у реально какую задачу мы хотим решить мы хотим изучить кабир найти с или мы мы что-то сделаем дальше немаловажный момент который говорит о том что баду достаточно взрослая компания и у нас присутствует достаточно большой запас сервисов которые писались и пять лет назад и семь лет назад и я думаю наш больше и насколько наши сервисы готовы вот к этому новому тренду состоит ли со всем остальным мы знаем о том что мы скорее всего получим играх и потому чтобы плотно не говорит ничего мы готовы его принять и как бы что мы получим если мы даже его примем и до тех пор пока я думаю что вот эти вот весы с этим человечком пока одна из чаш не перевесит примут в пользу что нам нужно что мы хотим я думаю что мы так же продолжим дальше заниматься выбором того самого регистратора и такая последние наверно смешная поучительная история которая говорит о том что свой сервис надо уметь продавать я понимаю что я не менеджер не продавец и продавать здесь поэтому взято в кавычки и так приблизительно тогда же когда вот у нас начали появляться все вот эти контейнеры docker и все остальное очень было постоянно на слуху идеи сервис discovery мы на самом деле эту идею мне кажется еще да да гири зации вынашивали и думали что это клёво короче но как-то сделать и вроде инструменты были готовы и на тот момент было два на рынке наверное они остались на до сих пор 2 готовых сервиса которые позволяют это делать мы таки посмотрели типа чё нет то давайте вот эти сиди клево звучит берем пробуем ставим мы его притащили настроили этому все эти репликации кластер все круто настроили отдали сказали всем привет давайте пользуетесь и все то есть как бы вот мы его настроили поставили он никого не нужен почему все я думаю что причиной почему все и были приблизительно вот такими что не готова была инфраструктура с позиции не то что на молоко это неправильно и просто не было общего понимания того зачем соответственно вытекающей или наоборот пункт который является может быть даже причиной который говорит о том что люди не готовы потому что получили мы этот серый discovery мы получили этот к в что дальше не понятно то есть мы как бы мы были не готовы скорее всего со своей стороны если мы вот готовы были предложить этот сервис как такой новый сервис который мы поддерживаем типа ребят пользуйтесь круто как жить не зашел ребята забили короче на самом деле нет ну отчасти нет мы любили и любим до ты собственно наш dns-сервер площадочный он всегда отвечает однозначно по какому имени живет тот или иной сервис поэтому ну как бы хорошо не будет нас такой динамики мы не так часто куда-то перетаскиваем если перетаскиваем то можно всегда пойти и поменять но опять же время на месте не стояла и появлялись какие-то внутренние проекты внутренние такого формата которые собственно к основному году имеют отношения постольку поскольку это там какие-нибудь например selenium ферма еще что то то есть те узлы которые можно постоянно горизонтально поднимать главное чтобы оно у тебя это надо где-то регистрировалось а дальше что вы не можешь либо council тимплей там либо как угодно ты можешь просто забирать себе ops 3 мая с ними работать решили мы посмотреть на консул как бы почему понравился наверное одна из первых причин это наличие такой сущности как сервис они просто такие вылью из коробки безусловно чехол все к она у климова ученого есть работает сделанный писал хочет скрип хочешь там connect очень нужный нам мульти дата-центр из коробки который позволяет опять же особо не прикладывает дополнительных усилий спросить что же у нас находится там в америке они в европе и там бомбическая штука которая есть сырое это dns интерфейс вы к нему и так привыкли тут как бы и тут он есть классно же и даже в качестве бонуса есть в apple что мы со своей стороны как я уже сказал что мы умеем и любим dns моим регулярно пользуемся и так же у нас есть описание всех наших сервисов в попить и то есть мы всегда знаем однозначно что вот этот сервис такого типа должен быть на этой машине на следующей машине или еще где то соответственно взять нарисовать джейсон с таким описанием для любой машины вообще него под вообще просто не вопрос поэтому мы взяли и за короткое время просто взяли и выложили вот это вот описание на все наши ноды сделали рилот консул агенту и в конечном итоге мы получили в консоли карту всех наших сервисов классно и при этом мы ее сделали но мы никого не заставляли братья переходить почему так получилось сделать вот образец того как работает dns client у нас на каждом как каждом стоящим сервере у нас всегда есть локальный кеш верующий он был он демон которой есть определенный набор правил и в таком в простом приближение выглядит он так что мы для всего остального используем наши стандартные dns-сервера а для зоны . консул мы берем и идем пока к локальному агенту испрашиваем у него сделали показали рассказали а дальше началось такое сарафанное радио потому что другие уже команды начали задавать вопросы силе начал на вас еще есть консул то ваш можете заюзать там для чего нибудь может заведем у нас появился один из первых примеров так называемый of the config когда надо было разложить что-то на все все все машины и за короткое время и сложность заключалась в том что если это был какой-нибудь там я не знаю scp are sent неважно что когда у тебя там одной машины сдохла потом она поднялась на него как на дасин катя здесь тебя однозначную версия база твоего файла всегда на машине присутствует для задачи с selenium фермы для нашего календарь эта штука тоже отлично справляется там машиной падают регулярно поднимаются контейнеры регистрируются круто для общего примера взяли и запихнули все наши но ты вообще все-все-все на которых есть запущенный engine x и просто он там что-то отдает или он отдает по своему какому-то сервисному пути 200кг выговоришь . все хорошо то же самое касается и вп м.а. и вот заключение которое я бы сделал по вот этому пункту как мне кажется звучит так что продавать то что тебе нравится за это больше платья потому что конкретно с консулом мы поверили показали рассказали а дальше она уже пошло и поехало потакают лавинообразно скажем так на этом практически все давайте попробую ещё раз повторить немножко такие перефразированный выводы которые говорят то что все можно сделать лучше чем делалось до сих пор с этим спорить тяжело сидевшый с этим тоже не поспоришь потому что как я кажется наша история с приключением с китом достаточно показательны в любом случае никогда не бойтесь экспериментировать это немножко противоречат конечно тому что я вот он говорил не ищите нить и нить не ищите не придумывайте задача под инструмент но без экспериментов не получится новых знаний и вот как то так ну и всегда помните главный технический вопрос а нафига спасибо итак начнем вопросы у кого есть поднимаем руки здравствуйте и спасибо большое за доплату вопрос достаточно простой почему вы не рассмотрели использование какого-либо репозитория серии артефакты и либо любого да гладь я главный вопрос я понял в нем неоднократно спрашивали почему не рпн почему еще что то я думаю что здесь нет ответ именно почему не смотрели то есть не приходила в голову то есть мы у нас был заход достаточно давно попробовать сделать что-то с рпн коми но почему-то это не понравилось разработчикам они сказали что вот мы вам будем отдавать бинали вы там уж как-нибудь сами то есть эта тема неоднократно проскакивала на вот историческая игра получилась так что начали вот мы туда пихать этот бит потом он вырос а потом начали уже это с ветряными мельницами бороться понятно спасибо большое у кого еще вопросы давай сначала спереди потом вам спасибо за доклад очень интересно а подскажите как вы храните и как вы работаете с секретами с чувствительной информация там с паролями но слизь волк тоже от х шакур по вот и в памяти соответственно есть сетевая шара которые монтируют что все мастера в которой хранится таким по которым он может сходить волк то есть это не в том ну короче где там репозитории не кто-то не хранится большое спасибо антон за доклад скажите пожалуйста на сколько я понял вы использовали контейнеры но не использовали никакой регистратор правильно да такая ситуация допустим мы знаем что контейнер и приведут приложение они живут память они падают что вы делали таких случаях как вы восстанавливает работоспособность как это связано с регистратором но обычно этим занимается регистратор он следить за контейнеры михаил с чайками и прочими вещами и потом занимается как бы восстановлением у нас есть отдел мониторинг который следит за контейнерами в том числе это люди из люди и всякие там интерфейсы ведется индексов графон и прочее прочее руны то есть мне не очень понятен вопрос в плане того что я не до конца понимаю в чем разница отдельно стоящего сервисы и сервисов контейнере ну допустим тоже cabernet сон занимать тем что следит по сути вместе с докером за состоянием контейнеров приложений да там . потребление памяти ресурсов цп и про чуток новому никто не мешает на самом деле рулить потребления my и лимитами через систему двери через стандартный инструментарий бы использовать систем для да нет мы хорошо не систем да и скорее всего мы используем ся групп через инструментарии докером радости лимитирование такое если вы проект спрашивали ну в принципе там просто у нас большое количество сервисов мне кажется что вас вопрос касался именно того что типа вот он сам должен следить где такой пир поднимать и так далее ну допустим он упал ну вот у нас очень часто сложный сервиса заключается в том что сервисы могут быть довольны жаркими по памяти и по всему остальному то есть прям вот сильно жаркие сервисы и часто этот сервис может жить на какой-то конкретной машине то есть нельзя взять его перевести потому что очень часто такой сервис может из себя представлять некий binary с какой-то встроенной базы данных который собственно работает сам по себе вот просто он есть и все поэтому для него случай когда ты последи за него подними его где-нибудь еще просто не сработает но хорошо если он допустим он работает а начал жрать дико памяти перестал отвечать на запросы что вы делаете есть так произойдет я думал что мы его 300 артем просто вручную да хорошо спасибо ещё у кого-нибудь вопросы здравствует оклад у меня вопрос с вашей текущей вид системы вы автотесты как вы запускаете еще раз автотесты у вас есть нет то что имеет все то что вы с текущим гид систем как понимаю что в это льете в битве на рынке льете а кто запускает вот авто тесты и так вообще выполняет если они авто авто тесты вы имеете ввиду то есть тестируется ли бинар перед тем как он выкладывается на площадку куда или что после выкладывания допустим у нас схема по-моему не по моему работает следующим образом что у нас этим сити занимается непосредственно сборки бинго если это хищный там башни какие-то демона после этого как я уже сказал что у нас есть наши не мясо контейнер и собственно если ты наш контейнер то образ то под него уже написаны все докер файла который точно знает куда сходить в тем сити куда положить по какому пути этот binary закинуть туда конфиге после этого собирается такой имидж с определенным шагом или в определенный namespace отправляется в registry после чего запускается либо автоматикой либо тикет на отдел тестирования на то что типа ребята давайте прогоним потом соответственно происходит кучок dry bag о том что там о как там или не мог обратно и если все эти тесты проходят такой образ по-моему не по моему такой образ маркируется новым тегам или он перезжает другой namespace и уже найти operations то ли это задача о том что вот давайте ребята катин теперь наделила на площадку то есть типа вот такой automatic если вы это имели ввиду да еще кто-нибудь с опросами трасса спасибо за должен плат очень интересно по поводу логов вот если можно отмотай слайд по поводу си слога и файл бита вот именно тот слайд и непонятно зачем число в принципе наш пояснить но сейчас вот тут не понятно зачем syslog игре файл битый файл bin чуть дольше на до непонятно зачем но смотреть то есть по факту можно еще сделать можно просто за маппить файлик syslog до в докер контейнер двое проходили была первой операции в socket имеются ввиду нет нескольких слов вообще не нужен то есть прям файлик ты можешь потом файл битом свою мантию нормализации же logo и потом и ware factory ти то есть он будет доставляться выложу еще раз вот по этой схеме я выкидываю syslog да есть просто файлик syslog в который можно складывать все события например декор контейнере правильно но сама питер прям такой легкий блок диалог варлок syslog файлик то есть я правильно понимаю что warlocks услуг это может быть любой файлик это может быть варлок - ну да да окей ну формате формате слов формовать услуг радость и получается так что в принципе именно в сетевом стыке syslog мне нужен то есть из пусть там будет socket например число года в которой будет улетать там все события к процессов но грубо говоря как они будут удовлетворять использовать логирование сразу syslog блогерам имеется ввиду этом сложность заключается в том что по моему текущей реализации если логировать выбирать lager докера syslog то там он не пишет насколько я знаю он не пишет именно контейнеров пишет только айдишники причем вот эти вот длинные здоровы и второе в этом случае мы обозначали условие которое говорило о том что нам нужно сохранить локальную работа докер лакс со слогом это тоже не работает в случае журнал где как раз таки есть поддержка того что он всегда пишут он контейнер name он там присутствует и мы его можем вычитывать как бы уже формировать то что нам нравиться идея с прокидываем сокета системы которые те флаг где флаг все это закончится на моменте когда произойдет рилот или ресторации слуга кастовые машине нет все таки еще раз с если я правильно понимаю то логе доставляется все слог именно по сетевому стыку нет ни вообще нет есть есть докер демон у него есть вот систем грешной jour de он пишет туда левую часть картинки можно закрывать вот сейчас мы считаем что у нас просто в этом журнале есть какие-то файлики syslog он стоит в данном случае в стороне вот ну прямо в стороне и не в него пишут а я попытался изобразить что он делает именно рид из журнала то есть он оттуда вычитывает он читают события в журнал де принимает к себе и дальше уже выполняя какие-то там свою круто жесткую воду ещё вопросы есть у кого-нибудь хорошо тогда также после завершения сейчас можно будет антон задать оставшиеся вопросы на дискуссионной зоне а сейчас мы благодарим его за доклад и сюда"
}