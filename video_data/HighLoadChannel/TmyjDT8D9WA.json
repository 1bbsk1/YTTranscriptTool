{
  "video_id": "TmyjDT8D9WA",
  "channel": "HighLoadChannel",
  "title": "Применение микросервисов в высоконагруженном биллинге / Роман Гирш, Александр Деулин (Мегафон)",
  "views": 2211,
  "duration": 3017,
  "published": "2019-12-05T12:57:15-08:00",
  "text": "еще раз здравствуйте меня зовут роман гиш я работаю архитектором цифровых сервисов в компании мегафон и сегодня вместе с александром мы вам расскажем про то как мы про наш опыт использование микро сервисов о том как мы вообще о нашем пути как мы готовим почему мы их используем в общем надеюсь будет очень интересно ну что начнем для начала небольшой ликбез да что же такое биллинг я уверен ну наверняка большинство знать что такое биллинг вот если там перевести с английского дата это просто как выставление счета за какие-то услуги вот на биллинг в мегафоне это понятие немного шире туда очень много процесса включается если представим ну обычного человека который зашел например на сайт мегафона выбрал себе тарифный план выбрал номер телефона внес какие-то денежные средства для того чтобы активировался телефон то есть дальше происходит какая-то магия подключается услуги на оборудование активируется и дальше вы уже можете делать звонки пользоваться интернетом в общем пользу связи более того вы даже можете увидеть как у вас меняется объем и минут если у вас пакетный тарифный план либо каким образом вас меняется баланс в режиме реального времени ну и в конце месяца вы можете получить либо счет ли быть или детализации либо и то и другое вот принципе достаточно with your ним какие какими процессами занимается биллинг на самом деле задач у телеком оператора и в частности биллинговых систем достаточно много и вот например есть такая схема она на самом деле доступна в интернете эту схему рисовали консорциум операторов целиком пиратов и вендоров это ну такая бизнес архитектура третьего уровня решает фактически все задачи телеком оператора ну вы спросите как как какая вообще проблема была мегафоне о чем вообще я буду рассказывать вот дело в том что исторически мегафон образовался как объединение 8 крупных филиалов вот филиалы эти были достаточно независимые имели свой маркетинговый отдел своя была какая-то корпоративная культура и каждый имел свою бен гу систему такая организован структура она достаточно удобная для запуска локальных инициатив внутри филиалы можно достаточно быстро ну что там тариф поменять услугу запустить но крайне неэффективно для запуска федеральных инициатив то есть тех инициатив которые запускаются по всей стране и со временем в таких инициатив стала запускаться достаточно много вот и очень стало критичным вопрос о скорости запуска но приведу пример например чтобы запустить простейшие там изменение стоимости тарифного плана такая задача могла занимать именно ну изменение стоимости тарифов по всей стране могла занимать до полугода вот и естественно ну а зная причину вот почему так так много времени занимала дело в том что конфигурации от биллинга квиллингу очень сильно различались но в силу того что ранее как запускались локальные сетевые конфигурации в общем сильно разошлись вот естественно одним из решений на поверхности это было объединить уменьшить количество bing биллингов что мы сделали уменьшили их с восьми до семи штук вот но тогда стала другая проблема это вертикальное масштабирование дело в том что максимальное оборудование на тот момент извините супер дом производства хилит по паккарда просто ну не хватало для того чтобы поместить туда более там 3 больших филиалов вот и тогда в 2004 м году к нам пришла идея запуске проекта под названием единый биллинг перед которым стоят были поставлены следующие цели ну во первых как я говорил это обеспечить максимальную скорость для запуска федеральных инициатив улучшить клиентский опыт вот и предоставить возможность эффективно управлять затратами на оборудование на персонал и соответственно при разработке новой архитектуре перед нами стояла достаточно такие серьезные вызовы каким образом для абонентов со всей страны там от калининграда до владивостока обеспечить функционирование но единой конфигурации при этом чтобы сервисы работали стабильно вот ну понятно что когда филиал был сосредоточен максимум двух трех часовых поясах у нас было ну начну окно когда мы могли проводить работы и перерыв связи был не так критичен случае же когда у нас работать единый конфигурации на всю россию технологическое окно у нас там уменьшается фактически ну до 1 часа когда калининграде только ложатся спать а востоке только просыпается от время очень в общем очень незначительная для того чтобы делать с плановой работы вот и таким образом мы при проектировании новой архитектуры суляс сформулировали скажем так четыре основных принципа архитектура должна быть доменной печи на следующих слайдах поясню что имеется ввиду что понимается под доменами распределенный то есть должны быть и резервы для того чтобы для того чтобы у нас система работали стабильно в режиме active актив идти в стендбай вот и ну использовать современный технологический стек и интеграционный чтобы мы могли легко домены менять использовать доменное производство roott других вендоров вот и соответственно вот скажем так упрощенная картина как у нас выглядела архитектуры до трансформации в филиалах это была единая база данных на которые крутились все процессы телеком оператора тарификации обслуживания так далее и это ну как мы называем к одной банки обращаясь очень всей внешней системы для того чтобы там получить информацию либо внести какие-то изменения соответственно по после яркин трансформацию мы выделили несколько доменов домены выделяли как по принципу того что домен приносит там максимальную бизнес функциональность своем на своем уровне то есть приведу пример был выделен например домен terrific тарификации который занимался конкретно расчетом стоимости за услуги там связи дополнительно как какие-то услуги интернет отдельно домена обслуживание каждый домен имел имеет свой api чтобы к нему можно было обращаться напрямую за за бизнес функциональностью имела свою имеет свою базу данных вот и например уже внешней системы мы могли рисовать какие-то сквозные сценарии через api когда там используется несколько доменов ну и плюс домен еще могут общаться друг с другом через шину сообщений на базе кафки или рабита таким образом у нас ну очень скажем так такая доменная архитектура очень близко к метро сервисная да потому что используется в те же самые принципы ну а чем александра чуть позже расскажет на текущий момент наш проект единый биллинг он подходит концу фактически мы туда с мигрировали более 80 процентов клиентская база в конце года планируем уже завершить наш проект и все старые системы лига системы перевести в режим чтения и теперь немного в общем занимательные статистики на какие цифры рассчитан наш единый биллинг ну понятно это вся наша абонентская база 80 миллионов абонентов и больше это то что скажем так на вершине айсберга вот то что внизу и дайте парком это мы обрабатываем 300 300 миллионов клиентских профилей в режиме реального времени но вы спросите почему обманет 8 10 клиентов 300 миллионов вот дело в том что у нас мы работаем как с потенциальными клиентами так и с клиентами которые но у нас от нас по каким-либо причинам ушли вот и остался там либо дебиторская либо кредиторская задолженность также мы ежедневно обрабатываем до двух миллиардов событий это событие тарификации то есть все звонки с весь интернет мы считаем считаем режиме реальном времени это около 2 миллиардов событий при этом используется 250 терабайтов активных данных ну и 5 петабайтов архивных бань архивных данных все это расположен на 5 тысяч серверов и 14 сотов 14 так куда там уже ушли минутку вот проект уже подходит концу вот и мы уже скажем так это проект дал достаточно сильный толчок для развития цифровой к системе мегафоне и мы уже решаем задачи другого порядка запускаем партнерские продукты используем те же самые принципы которые мы использовать при проектировании единого биллинга основная задача ускорить запуск уже не федеральных инициатив а уже продуктов с привлечением партнеров то есть мы можем разрабатывать как собственные микро сервисы вот так и подключать микро сервисы партнеров и уже нашим клиентам предоставлять комбинированные продукты вот дальше я передаю слово александр у вас скажем так погрузит более сделал более глубокое погружение вот расскажет чем мы столкнулись в общем телефон любит сюрпризы поэтому ребята переодеваются такое шоу а вы знаете что по разному бывает обучение в школе учишься лучше течет такое вот так операция делается вот так там только в конце и получать звуки в конце года дошло и собственно происходит другой способ это когда сначала увидел весь курс как он устроен а потом уже начинаешь углубляться в подробности и вот сейчас доклад по 2 ну ровно таким образом что вы увидели сверху как и зачем что происходит и подробности у александра раз-два-три наконец-то разобрался с техникой отлично рад вас видеть всех меня зовут александр юдин я тимлид команда разработки микро сервисов мегафоне сегодня я вам расскажу про появление микро сервисов в ландшафте целиком оператора вот и поделись опытом построения небольшого микро сервиса спешим на 10 миллиардов записи микро сервис steam мы начали заниматься совсем недавно полтора года назад совсем небольшой командой и когда мы приступили к этому упражнении мы поняли что в общем-то под понятие микро сервис каждый понимает немножко свое вот и для того чтобы там не делать там какое-нибудь там чуда-юда да и не делают все не расходиться в реализациях мы для себя определили что что такое micro series будет у нас в мегафоне в нашем в нашем и рэнди вот конечно же мартин фаулер нам очень помог в этом вот но и мы немножко дополнили его определение своими особенностями вот ну извините за банальность конечно повторюсь микро сервис это в общем то атомарная функция который рисует конечно бизнес ценность он имеет как правило собственное хранилище данных и у него есть строго регламентированный интерфейс ну и мы дополнили но его он легко легко заменяется обозримо для небольшой команды имеет слабую связаны с другими другими микро сервисами но и мы дополнили для себя это определение тем что нам обязательно нужны инструменты почему мы пошли в эту историю с микро сервисами нам нужно было скорость скорость которая появилась внедрением единого биллинга уже не устраивало бизнес и нам нужно было как-то отвечать на эти вызовы ну когда мы определились с тем что мы делаем соответственно у нас нам стало гораздо легче определиться с тем как как как мы это делаем очень быстро у нас появился появилась такая ему платформа наш технологический стек где в общем то нам у нас появилась партнерская программа партнерская командовать и инфраструктуре которая развернула нам инструменты соседи и мы им пользуемся как как сервисом вас есть есть все необходимые полномочия вот но мы не тратим время на их сопровождение интеграции и на обновление мы любим java spring будет не брезгуем питона мы обязательно под вторым вторым стеком у каждого разработчика есть поскольку мы используем так уж сложилось что мы используем тарантул не только как базу данных но и как сервер приложений у нас получается достаточно быстро на нем реализовывать микро сервисы как правило нам не нужен даже никакой дополнительный промежуточный апликэйшен собственно все все в одном флаконе наверное стек вам тоже хорошо знаком если будут вопросы готов ответить в секции вопросов-ответов либо перерыве мы достаточно серьезно подошли к нашему с нашим себя и процессом поскольку подходов к разработке мегафон делал несколько они все зале заканчивались seas когда мы поладили огромный слой каких-то собственных кастомных доработок про который все забывали они не были покрыты тестами и в общем то потом эксплуатировать эту всю историю оказывалось очень сложное и приходилось сносить и в общем-то единый биллинг это в общем то история про то как мы переходили вот так этих вот кастомных биллингов до которые на пелены были как каждого филиала мтс самостоятельно до разошлись в решениях мы переходили в единый пилинг в какое-то единое продуктивное решение но жизнь в общем-то показал то что разработка нужно и мы уделяем особое внимание безопасности кода его качества у нас работают как системы системы обнаружения уязвимостей dt детектор уязвимости и кода так и многочисленные линды обязательным шагом является юнит-тесты ну и на этапе тепло и у нас всегда проходит интеграционные и функциональные тесты мы достаточно ну мы достаточно много занимаемся методологии поскольку мы не хотим хаоса мы понимаем что много на микро сервисы но за микро сервисные будущее это достаточно хайпа и технологии мы быстро растём и в общем-то мы определили для себя такое ну сделали внутренней манифест который основан на принципах 12 факторного приложения это позволяет нам готовить продакшена клауд-рэп приложения которые в общем то мы и пропагандируем внутри для всех команд разработки мегафоне которые у нас есть еще из подходов да то что мы используем паттерн define a beast то есть мы сначала делаем контракт на обитой отдаем его в канал и соответственно мы делаем бэкки фронты делают свою работу и выкатывают свой функционал и мы часто очень часто по времени сходимся это позволяет избежать waterfall а и в общем то используем документацию с их язык вот у нас обязательно в наших наших apple песенок всегда есть функция которая отвечает актуальным актуальным фан форматом а5 можно их можете посмотреть собственно swagger спецификации у нас генерится в самом самом приложении и плюс выкладывается на портал ну и понятно понятно что мы не любим работать руками не любим таскать в провод руками приложения инфраструктура как-то код и как следствие телеметрии как как кот кубер notice и прометей позволяет нам в общем-то достаточно комфортно и тепло это наше приложение в продакшн ну я вам рассказал сейчас как мы делаем микро сервисы какие подходы но самое главное до для чего мы это делаем целиком очень очень конкурентная среда и нам нужно быстро доставлять наши новый бизнес функционал до продакшена это опять же к тому что у нас но единый биллинг да это все-таки такая такая система которая там в основном о релизах там есть зависимости и даже то ускорение которые мы получили срединным пилингам его не хватает для того чтобы сейчас конкурировать на рынке для того чтобы быстро выводить функционал но и в общем то мы решили что мы будем строить вот этот вот средний средний слой цифровое нашей цифровые экосистемы в котором мы будем развивать наши микро сервисы подход полностью оправдал себя у нас в эклоги порядка 50 реализованных микро сервисов мы делаем их быстро они в основном имеют конечно целиком целиком специфику но я расскажу про на мой взгляд пару мой взгляд на пару интересных проектов которых которые не потребуют глубокого погружения на наш контекст давным-давно в далекой-далекой галактике на вход big logo рэнди поступило задача подружить двух девушек с характером одна из них была импульсивно не часто не дождалась ответа вот вторая была напротив задумчиво и иногда перри повторяла вопросы они были в общем две девушки с особенностями нам пришлось погрузиться в бота нейропсихология и в общем-то мы пришли к определенным выводам скажите поднимите руку кто кто вообще верит в женскую дружбу немного немного вот у нас примерно до был примерно такой же процент поэтому мы мы решили что наверно лучше использовать другие паттерны и в общем-то у нас в результате появился в серединке вот такой молодой человек и каждая девушка считает что она общается к таким замечательным микро сервисом который фактически изолирует 2 ботов и сглаживает их особенности к тому же он проводит авторизацию он сохраняет согласие поскольку мы передаем персональные данные в аква лесу да ну и наверное вы узнали всеми всеми любимую алису яндекса и наш голосовой помощник елена вот то есть это был один из наших первых микро сервисов когда мы в общем то пас построили интеграционный слой добавили туда достаточно функционала чтобы 22 бота могли комфортно общаться да и собственно для теперь у нас есть волеси навык мегафон он работает и мы его постоянно улучшаем но если мы начинали вот с таких но небольших скажем так маргинальных сервис труда которые затрагивают небольшую небольшую абонентскую базу то на сегодняшний день мы вышли в общем-то на флагманский продукт и наши микро сервисы есть везде и вот одна из истории стенда это то что мы запустили тарифный план нашел флагманскую линейка включайся выбирай на микро сервисах в четыре раза быстрее чем если бы мы делали с текущими релизами циклами суть микро сервиса состоит в том что вы можете выбрать для себя объем и этот объем нужно заполнить потом с ним работать в личном кабинете и в общем то эти бегунки под капотом дает это запоминание работы в личном кабинете это как раз микро сервис друзья не подойти ничего страшно но второй второй задачей 2 наши задачи кроме кроме то кроме тотем являлось повышение надежности повышение надежности и качества услуг какие какие вообще у нас вызову стояли до то есть роман рассказал про то что наш единый пилинг тоже использовал такие микро сервисные подходы да это не совсем микро сервисы да потому что там ключевое отличие единого биллинга что там нельзя вытащить какую-то функцию да и заменить своей вот в этом в этом основное отличие все остальные очень похоже то есть полное покрытие opi а им домена и деление свой бизнес контекст своя база данных у каждой подсистемы но не все но и в общем это получилось такой файл highload система каждый из под систем которых обслуживает 80 миллионов абонентов если раньше это было распределено на 8 филиалов сейчас это в общем то восьмикратное восьмикратный рост 8-ми кратный рост нагрузки на каждую под систему и проблема в том что не все под системы единого биллинга имеют каши мы не можем изолировать слой работал бы написать допустим базой данных и поэтому повышенная любая повышенная нагрузка сразу дает нам какую-то деградацию как ну день деградацию работы с базой сокращенное технологическое окно получили два часа всего у нас есть времени на проведение работ у нас появились новые услуги которые требуют доступности наших сервисов выше чем принятая доступность биллинговой системах банковская карта мегафон тела real-time компонент да эти все продукты ра должны работать 24 на 7 практически без перерывов вы не должны замечать никаких сбоев процессинге вот поскольку они но эти продукты интегрированы очень часто и очень глубоко интегрированы в то собственно нам нужно в общем то повышать надежность и увеличивать доступность наших наших систем также у нас идет естественный рост нагрузки растут абоненты новые системы появляются и появляются еще новые высоконагруженные потребители которые требуют например там 55 corps одну операцию 20 до 50 секунд latency в 95 перцентиле но 99 и 99 доступность какой ответ на это конечно же это построить ну нужно строить свой слой к шее нужно поднимать данные из master system строить каши и нагрузку на чтение как минимум переводить на на каши почему почему на чтение почему это нельзя встроить ну во первых не все не везде vendor встраивают у себя каши да и мы не можем со своими корешами строится в биллинговые решения как я говорил уже такое ну по монолит да мы не можем туда зайти и поэтому возникает вот эта красивая картинка да за ней возникают достаточно большие сложности потому что наш кэш должен стать сбоку он не должен влиять на мастер систему при этом он должен быстро получать данные и в общем-то быть достаточно производительным ну и мы начали рассуждать как же мы можем рядом закрытой системой построить кэш но мы очень любим парк подход или древен дизайн когда у нас данные получается из любой шины вот так же у нас есть у раков days как основная как основная база основное хранилище в нашей биллинговой системы ну и есть несколько способов получения данных и 40-го ну давайте подробнее немножко рассмотрим все все все наши кейсы которые мы прошли на которых набили шишки прекрасный способ получения данных из shinee мы подписываемся на там и кончина топик на что угодно получаем нашем случае у нас больше ребята сейчас используется появляется также кафка вот мы сделали один пилот прекрасно прекрасно он себя показал система была говорящие вот но на втором пилоте в общем-то все закончилось система оказалась не ратифицировали своими событиями в кролик вот и там ждать ждать появление модификации достаточно долго но мы пошли дальше в общем-то поскольку у нас есть был план б было раков мы посмотрели в сторону триггеров и сразу сразу отвернулись потому что триггеры во первых это внесение уже изменения в логику самого приложения во вторых это дополнительной . отказано высоконагруженных приложениях нам не хотелось идти в эту историю мы провели просто теоретический но как бы экскурс в историю посмотрели все аварии которые у нас были с триггерами и сказали что нет вот но поскольку мы любим open source мы не любим лицензии мы стали смотреть что же у оракла осталось из механизмов перелом из механизмов переноса данных которые не требуют лицензирования остался единственный механизм континиус к вере notification все остальное ушло в deprecated к сожалению мы остались с одним механизмом ну и нам ничего не оставалось как начать его пилотировать подскажите кто нибудь сталкивался с этим механизмом кто-нибудь его использовал в работе ну вот мы тоже мы тоже не использовали документация по нему есть расскажу по наши грабли чтобы вам два раза не ходить вначале нам очень понравился этот механизм он не требует дополнительных инстанции farecla не требует какого-то дополнительного обслуживания это механизм подписки на внутренние там д д д д м л события которые мы можем настроить но зарегистрировавшийся подписавшись на модификации там есть 2 2 2 возможности oracle может модифицировать событиями во внешнюю очередь через advanced либо можно использовать пельчик вот файл когда мы все события складываем в том же урок ли в например в отдельную схему в отдельную табличку с очередями ну вот в эту вниз очередями но просто отдельную табличку с изменениями которую уже потом вычитываем и чистим это ну как бы в целом механизм работает но есть нюансы поскольку мы но для нагруженных систем кота для нагруженных систем на запись вот этот от персиков теперь считал стал создает дополнительную традиционная нагрузкам и фактически перекладывание вот в эту промежуточный таблиц созданию создаем двойную нагрузку на запись на oracle вот плюс под капотом там используется все-таки advanced q и мы также создаём нагрузку на внутренние очереди oracle и но и на нагруженных базах мы видели конкуренцию в принципе с этим как-то можно было было бы жить если бы не одна проблема на парте цианирование таблицах этот механизм не ловит изменения не ловит изменений если вы одним коми там закрываете более 100 изменений то есть 100 первое изменение все все ваши труды по репликации превращаются в тыкву вы сразу теряете транзакционные целостности мы открывали кейс вор клин нам с этим не помогли в общем то мы ушли от этой технологии и перешли старому доброму golden gate у все минусы вам хорошо знакомы это прежде всего его но необходимость дополнительных инстансов и необходимые но и необходимость лицензирования лицензии у нас были ebay у нас были в общем то мы поняли что вот для нашей как конкретно для для для нашего кейса подходят именно этот случай и golden gate позволяет нам гарантировать это единственное решение которое позволяет нам гарантировать транзакционному целостность он также имеет низкое влияние на мастер систему вот и связка со сцены xit ну нам фактически позволяет контролировать какой точке репликации мы находимся в текущем моменте ну и если вот там завершить этот экскурс да и у вас есть ну допустим у кого-то есть закрытые системы да то для вас в общем то подойдет наверно любой способ даже + 1 наверное lidl а если там есть какие то еще особенности вот все зависит от вашего из кейса а для нас подошел в golden gate как самое универсальное решение поскольку нам нужно много кашей и у нас много оракла но сейчас я расскажу про наш проект который мы как раз реализовали с использованием данного подхода api им это продуктовая витрина это небольшой микро сервис там всего два метода но и 10 миллиардов записи в кэше вот значит что из себя представляет с точки зрения стоит с точки зрения продукта пим это новая продуктовая модель когда все ваши тариф ну в общем все все услуги все тарифы да теперь называются продукты они отличаются от офферов то есть offer это одна сущность да когда вы подключили что тогда вы подключили уже продукт у вас есть например тарифный план этот продукт в категории тарифный план и все что у вас подключено теперь это продукты и вот эта витрина хранит актуальный список подключенных ваших всех продуктов как технологических так и коммерческих которые есть у вас на данный момент ну и самая большая проблема стала да в том что нам нужно загрузить в хранилище два с половиной терабайта сырых данных из оракла или но порядка 10 миллиардов записей но поскольку в кэш мы поднимаем не все данные а только те которые ну как бы тот только горячие нам нужно выбрать именно ну а то тебя очень отсечь все всю историю и убрать и убрать неактуальные статус и понятно что мы должны ворог ли сделать select с какими-то условиями применить в и когда мы применяем в на табличке с одним миллиардом записей оракул становится плохо мы получаем скорость прогрева кэш порядка двух и более недель вот и плюс идет нагрев идет в общем то пропадет увеличении загрузки процессора мы получаем полку под сто процентов мы долго бились с этой проблемой оптимизировали запросы пробовали разные варианты и в общем-то сработал у нас работал на больших объемах только один вариант мы убрали в и oracle полетел у нас собственно получилось ну в общем мы поняли что oracle это прекрасная база да она имеет отличную производительность на чтение главное не ставить ей условия да посмотрите за собственно вот все на самом деле все оказалось просто как автомат калашникова мы берем в горный драйвер ставим пресечь то есть мы берем пачка пачку 10 трендов и селекция собственно но каждый трек натравливает но идет нас каждую каждую просто партицию никаких в причем на в этом селе gti мы уже сразу выбираем нужные нам метаданные и складываем но мы можем можем отфильтровать уже часть часть данных вот как себя ведет oracle при этой нагрузки то есть степенью степью ничего не происходит и утилизируется но понятно что утилизируются сети вот вывода здесь нужно иметь некоторый запас в пике мы получили 400 мегабит по сети но в общем то у нас сеть построена с запасом вот вывод тоже и главное что сепию не просила не было влиянием нет влияния на мастер системы ключевое до ключевое ну ключевая мысль до что мы в общем то и 40 la выжимали и 666 600 тысяч и даже до миллиона записи в секунду из простых таблиц мальчики учитывались до миллиона записи секунды подскажите кто-нибудь видел такие скорости на уроке коллеги можете повторить это в общем то но тут тут нет никакого секрета вот собираем автомат калашникова и в общем то если вам нужно нужно быстро быстро читать все 40 вы просто берем и переносим весь секрет в том что мы переносим вот всю логику select a sea логику фильтрации на приложение и пишем быстрое приложение которое молотят нашу нагрузку на на лету фактически мы ну скажем так прибиваем планы запросы уже нам а стороне приложения и это работает быстро то есть мы получили в результате мы получили прогресс каша за 19 часов из которых 5 часов у нас занял импорт из oracal в csv дальше мы 8 часов грузили это все в тарантул и проигрывали приду накопившиеся ри диалоги в течение в течение шести часов вставка тарантул у нас шла со скоростью 200 тысяч записей в секунду это кластер и объем данных которые мы поднимали в кэш 1 1 терабайт изначально ворог лет с половиной терабайта ну и влиянием влияние на уроках вы видели ну и немножко вот machinery наши механики про то как мы греем кэш но прежде чем приступить к упражнению вычитки и 40-го мы делаем но мы начнем и мы начинаем копить изменения мы их складываем в репликатор это in stare at all с определенной ролью но не прогружается в промышленный кластер и после мы запускаем процесс уже по процесс импорта и 40 лет складываем csv на тарантул с ролью импортер после этого после того как импорт закончен мы складываем мы мы начинаем про грузка в тарантул мы добавили немножко логики в импортеров для того чтобы не грузить роутера не нагружать на нашу систему мы функционал ротора перри переместили в этот импортер и у нас нужный sharp вычисляется вот в этом в этом инстансе всего их десять раз по 1 проливаем загрузку вот и после того как мы завершили всю ставку мы проигрываем мы мы начинаем проигрывание ряду лагов и по достижению необходимых как бы после того как мы их проиграли да и наше отставание репликации уже там приемлемо в там в районе там одной-двух минут мы уже на аперитив переключаем нагрузку для внешних потребителей на чтение мы уже отдаем это мы даем раба работающие api наружу вот в нашем нашим нагруженным потребителем как я уже сказал всего 2 методы и 10 миллиардов записей вот поскольку но такой такой объемный кэш получается такое объемное приложение то в общем то мы это все мы это мы мы конечно шар героем для того чтобы данные быстрее поднимались кэш при старте у нас на к у нас используется два дата центра для дела резерва и на каждом из дата-центров есть 3 х 100 где в сумме работает 27 сортов ну как to reply ну в общем вот 27 сортов это 100 раджи да и 66 роутеров ну немножко про особенности построения модели схемы в начале мы хотели смешивать несколько таблиц вам казалось схема схема хранения данных ворог ли не очень оптимальной были дублирования но в конце концов мы поняли что для того чтобы вот строить кэш и контролировать нашу целостность данных нам необходимо повторять ddr1 в один и сохранять связку сцены csi домом для каждой таблички в tarantul а то есть мы параллельно еще но как бы ведем себя технологический space где храним эту связку для каждой из таблицы таблицы ворог ли соответствуют по таблицам в tarantul и с прессом в таранто me ну и для того чтобы увеличить производительность на чтение мы собрали данные по одному абоненту в одном шарди у нас используется общие библиотеки которые пришлось затаскивать в 1 шард вот мы но тем самым мы избежали map рядился нам не нужно обход не нужно обход по кластеру если мы приходим за данными по одному абоненту мы сразу забираем их снова шарда это несколько усложняет запись но дает прекрасное производительность на чтение й итоге еще раз да то есть холодный прогрев кэша у нас занимает 19 часов мы научились делать большие большие кэш еда для систем которые в общем-то являются для нас черным ящиком вот наверное все друзья вопросы спасибо огромный александр спасибо роман возьмите пожалуйста такие официальные сертификаты благодарности от оргкомитета там два каждому спасибо и и и есть еще подарки ну то есть вы как связисты я думаю сможете оценить жидкость находится сидят у нас есть время на 3 вопроса и я вижу первую руку здравствуйте спасибо за ваш доклад у меня такой вопрос вот вы описали о том что вас прогрев каша длится 19:00 и вы там описали поэтапно что это сначала вас эскель потом вы загружаете тов тарантул а потом читаете лак и у меня сразу такой вопрос просто исходя из презумпции ума было почему-то так сделано и мне вот интересно почему не было сделано какого-нибудь стрим решения то есть напрямую сразу оракла тащите скилов slim режиме в процессе это еще не сделано правильно но вот поскольку но мы набираемся опыта мы ее доим зубы вот мы еще пам мы мы все мы всему учимся да и разработкой занимаемся недавно для нас сначала но мы делаем простое решение потом начинаем его оптимизировать и сейчас в работе прямая загрузка тарантул бездействий ну конечно лад новый и в общем-то или 2 2 моментом и trail и сейчас в xml складываем да это же но фактически когда мы то есть мы говорим про холодную погрузку да вот процесс вида есть еще 2 в то вторая ветка когда мы забираем с golden gate а в троил файл и вот мы их складываем сейчас в xml их также обрабатываем грузов тарантул то есть накатываем изменение вот процесс изменений мы тоже хотим сделать онлайн и получать их сразу подписавшись на там java юзера бит то есть у вас конечно не обновляется динамически нет он обновляется динамически мы читаем мы постоянно читаем эти троил и мы забираем их ну как бы xmr пин xml файл и вот мы подписаны на рено лаги это не охай а вот то есть мы фактически в онлайне получаем поток поток логов и у нас в общем-то ну кыш постоянно обновляется golden gate дети шлет постоянное изменение которое мы постоянно вычитываем и и накатываем на тарантул александр спасибо мы сейчас ребят по одному вопрос только можем каждый задать за давайте сразу сама важных уже есть микрофон у следующего человека здравствуйте подскажите пожалуйста а как идет взаимодействие между микро сервисами например мы отправляем на сервис елена о увеличим не количество на ну трафика на 10 гигабайт то есть это же второй микро сервис получается на елена да елена это уже реализованный бот да это наш голосовой помощник со своими сценариями обслуживания и поэтому все что заложено в елене да вы получаете сразу из коробки мы не писали нового бота мы просто по дружили двух девушек с характерами но да да да я понял но смотрите сейчас у нас 3 ст вот мы мы думаем в сторону джер писи либо другого любого протокола взаимодействия между сервисами извините я повторю ещё раз микрофон то есть смотрите у вас есть два микро сервиса один должен отправить команду на второй микро сервис ну то есть инициировать операцию у того микро сервис как бы они становятся связанными но в микро сервисной архитектуре логика о том что микро сервиса должны быть изолированы вот как вы решаете эту задачу вы понимаете да архитектура и там концепции да это одна жизнь это другое невозможно представить себе чтобы с микро сервисы не взаимодействовали проблему решаем просто мы открываем api контракт и если нам нужно построить какой-то композит например один микро сервис вызывает два других мы идем за это контрактом от этих двух этих двух микро сервисов и в общем-то реализуем композитную функцию но также смотрим в сторону графа ребята жизни сюда справедливо один короткий вопрос остался и следующий вопрос пожалуйста у дискуссионной зоне вот человек с первого ряда и он мне просто ближе здравствуйте спасибо за доклад у меня такой небольшой короткий практически вопрос почему остановились на таран туле проводили ли какие-то измерения но производительность и кому словно до проводили до у нас смотрите все все очень просто в свое время наш вендор биллинговой системы очень много экспериментировал с разными базами данных он привозил нам решение скучный зам с кассандрой рома с чем еще много-много с чем луна и да еще игнайт еще да еще игнайт болта и у нас там появился такой такой зоопарк я не знаю манги наверно мне привозили там с венгерскими решениями что мы покупая с венгерским решением ну как покупая windows к и решение потом еще покупали покупали в техподдержку на базу данных отдельно вот ну и поскольку мы там активно экспериментировали но начали эти 5 в микро сервиса да и в общем то нашли там такую такую технологию да ну такой не знаю даже sdk наверно да это тарантул но и мы попробовали нам понравилось вот соответственно просто эволюционно взяли и сделали плюс мы проводили еще сравнение но по нагрузке решение от вендора на игнатик оч базе и на tarantul и все они сравнимы вот игнайт выигрывал за то что за счет того что он отключил валы ну то есть он мне не подтверждал запись на диск вот решение с тарантулом получилось более экономичным по сравнению с коучем по размеру занимаемой памяти все остальное все остальные параметры в пределах погрешности но тарантул немножко лучше писал на запись но там это опять же может быть мы недостаточно кейсов покрыли но в общем-то для нас вопрос сесил был по большому счету ну я добавлю что на самом деле мы можем даже поделиться результатами наших замеров вот можем обменяться там контактами высшим даны расскажем подробнее друзья пожалуйста будет экскурсионная зона напротив зала с хлеб чар там вы сможете свои вопросы дальше задавать сейчас александр или роман выберут лучший вопрос скажите что он вышел пару ну я думаю что вопрос про трандл очень хороший потому что у нас очень много сервисов разорваться на таран туле и мы считаем что это годитесь даром кожи как тебя звать где работаешь почему-то спрашивал почему ты задавал вопрос здравствуйте раз раз раз меня зовут антон я работаю в тхт занимаюсь эксплуатации у нас сейчас как раз ну тоже много оракла и есть задачи по выносу часть информации для более быстрой обработки в какие-то внешние базы данных собственно поэтому и поинтересовался спасибо спасибо огромное"
}