{
  "video_id": "_4HdQqR4kZQ",
  "channel": "HighLoadChannel",
  "title": "Сегодня я помержил в ClickHouse 30 пул-реквестов, а он всё ещё не тормозит / Александр Кузьменков",
  "views": 1042,
  "duration": 2599,
  "published": "2021-10-04T02:42:05-07:00",
  "text": "всем привет слушать нормально да рад вас всех видеть в такое тяжелое время 10 утра сегодня я вам расскажу про то как мы тестируем производитель ские хауса в автоматическом режиме в процессе разработки вообще что за клик house кто-нибудь в зале пользовал всех ли хаусом о супер сколько у нас вам будет интересно послушать остальным будет еще интереснее но вкратце для тех кто не знает crack house это колоночный субботы для аналитики и скверная то есть там таблички можно писать и скальные запросы на почти стандартном скай и основной основной сценарий это построение отчетов по большим сырым данным в реальном времени как мы говорим дважды да это значит что и запросы выполняются в реальном времени и новые данные тоже добавляются к результату в реальном времени и клика us изначально разрабатывался как базовая технология яндекс метрики это одна из крупнейших в мире систем веб-аналитики ну и соответственно это основной для нее такой кейс и вы наверное слышали что как ли house не тормозит а кстати пока не забыл скачайте слайды потому что там у меня есть ссылочки и можно будет почитать или посмотреть там какие-то еще статьи вот поэтому qr коду вот но почему не тормозит ну к тому есть много причин основном конечно архитектурные как секрет производительности простой да бери больше кидай дальше но применительно к субд это во-первых требует эффективного ввода-вывода да надо значит эффективно хранить данные для этого они хранятся по колонкам чтобы читать только то что нужно они сжимаются конечно же вот вчера может быть вы были на докладе лишь умела вида он рассказывал что иногда даже для чтения из памяти оптимально сжимать данные это приводит к росту производительности ну а уж для чтения диска тем более есть пар с индексы различные которые тоже позволяют снизить вот вывод и затем когда мы данные эти прочитали нужно эффективно их обработать в первую очередь это векторизованное алгоритма до данные обрабатываются не по одной строчки по много сразу чтобы можно было использовать там синглом страшно les paul дейтон алгоритмы вся специализированный например для одного грубая у нас есть порядка 40 различных алгоритмов каждый из которых оптимален в определенном случае он выбирается автоматически и конечно это все масштабируется как горизонтально так и вертикально до в пределах одной машины и в пределах кластер машин ну откуда мы вообще знаем что это все сработало мы бенчмарка ли у нас есть очень много тестов для этого какие вообще бывают тесты производительности но самая базовая вещь наверное это то что мы используем в процессе разработки микро benchmark синтетический там возможно даже не весь код клик хаоса а какая не в программка на си плюс плюс да и мы тестируем например хэш-таблицу вот например опять есть интересный у нас доклад про это оказалось что в одной базе данных функций сумма работает быстрее чем у нас но мы конечно не сможем такого допустить поэтому мы проверили 27 различных реализаций функции суммы с помощью таких микро бенчмарков выбрали одну и померзли в house это самая базовая штука немного более высокого уровня это уже and and test и когда мы выполняем какой-то запрос какие-то простые вещи тестируются на синтетике ну например если мы ту же сумму тестируем да там имеет смысл этого синтетических данных тестировать но важно также обязательно тестировать на реальных данных даже когда вы оцениваете например насколько быстро вашим сценария будут работать запросы важно проверять именно своих данных а не на какой-то синтетики потому что у них бывают особенности различные у нас довольно много кода который учитывает особенности реальных данных например то что там одинаковые ключи идут подряд и последнее время у нас появился еще такое вообще отдельный жанр это бенчмарки оборудования с помощью при хауса вот например есть там несколько облачных провайдеров и мы хотим сравнить машины которые они предоставляют как по производительности отличаются и у нас есть пакет специальные запросов для cliff house и запускаете там это и он вам выдает чисел и что одна там лучше чем другая это все есть на сайте я даже бенчмарков свой телефон с помощью этого у меня пиксель 3 а там arm6 четырех битной нормально crack house работает почти все выполнилась но некоторые запросы не прошли потому что ли гигабайт памяти оперативки все таки маловато тоже почитайте это все про розовые такие мероприятия да я сейчас рассказываю то что мы запускаем руками вообще у нас очень высокая скорость разработки например вот за апрель нам прислали 401 по request да от сорока четырех уникальных авторов у нас разработка надев хабиб a request это одно как бы осмысленное изменения то есть это как эта фича там или исправление тому подобное я это кстати узнал потому что у нас есть база всех событий гитхаба тоже в клик хаусе можно написать простой запросик и вот посмотреть и конечно при таком потоке изменений уже ничего вручную от смотреть не получается нужна какая-то автоматическая система для того чтобы скринить изменения но у нас конечно из системы seat там много разного рода проверок и вот в частности автоматическое тестирование производительности там тоже есть что она из себя представляет это просто какие-то из куриные запросы до объединенные логические в группы тестов на данный момент у нас есть 215 тестов которые содержат 2640 запросов и вот это делал все запускается во первых для каждого комментов мастер или в релизную ветку а во-вторых для каждого комментов парик месяца чтобы можно было это использовать как инструмент лодки вот как это выглядит на гитхабе вот сверху вот это вот галочка это если видели использовать git хабом по указке не работает если пользователь хабом там есть вот такая галочка вы на нее нажимаете сейчас я запутался в слайдах вот нажимаете на эту штуку и там список проверок но вот тут видно проверка performance написано 3 запроса ускорились 72 нестабильных это хорошо давайте нажмем на details там написано сравнение производительности какие-то вот ками ты тут значит старый комит такой сравнивается с новым кометам таким-то вот мой коллега максим который сидит нас на первом ряду привет максим что-то ускорил как это у него тут оптимизация для вычисления ногу логических функций и действительно мы видим если мы смотрим вниз действительно некоторые за просеки стали работать аж в два с лишним раза быстрее очень хорошо сработал вообще что там сравниваются за числа да вот написано в два раза быстрее а что это значит но если это запрос понятным и меряем время выполнения запроса надо мерить его много раз потому что если мы померили там один раз получили 2 1 2 1 2 и 2 до непонятно как сравнить может быть это шум вообще надо мерить скотт сколько-то раз потом как-то мы эти измерения должны усреднить очевидный вариант это средняя но средний не очень среднем может не заметить если у вас какие-то там странности распределения у вас например сильный выброс какой-то произошел еще вариант есть минимум максимум до максимум вообще не годится потому что максим может быть произвольной там чем затормозила сеть затормозила диска тормозил все умирало до не годится минимум теоретически на него есть границы снизу до минимум достаточно хорошая штука но тоже достигает сама не всегда и будет сильный шум вообще как с точки зрения предметной области нас интересует скорее максимальное время выполнения потому что мы хотим гарантировать что при house не тормозит что запрос не выполняется долго но вот на практике с этим есть сложность поэтому мы имеем медиану такой консервативный вариант это стабильная достаточно статистика ну вот померили мы медиа ну да а дальше чего как нам понять чего вообще изменилось не изменилось чего происходит надо с чем-то сравнить но такой базовый вариант это какая-то модель да но у нас модели никаких нету потому что смоделировать что там происходит довольно сложно мы это используем только для приблизительной оценки по порядку величины типы вот мы знаем что там можно вот этот алгоритм может работать со скоростью 10 гигабайт в секунду и мы примерно понимаем что такое производительности надо ожидать но это не точная штука по порядку величины есть вариант сравнивать с историческими данными мы тесты производительности гоняем давно все это пишем конечно же результаты всех клик house и можем сделать запрос как какая была производительность у этого запроса раньше мы так делали раньше там возникают сложности в основном связаны с тем что изменение окружения вносятся систематическую погрешность у нас разное это все происходит внутри яндекса вы и системы запуска задач на каких там серверах там их несколько сотен и там немного разные железо например да вот мы находили у нас было какое-то необъяснимое изменения производительности оказалось что там один сервер перегревается и когда на него попадает эта задача она почему-то начинает тормозить или разное окружением и тоже увидели какую-то очень страшную регрессию производительности у нас все запросы замедлились оказалась там обновили о топ на какую-то уличную версию и сервер стал тормозить целиком чтобы избежать таких проблем мы сейчас делаем такую вещь мы запускаем 2 сервера одновременно на одной и той же машине и выполняем запросы на них просто попеременно это позволяет всю систематическое вот эту погрешность связано с окружением убрать ну и дальше еще встает вопрос как сравнивать до получили мы эти два набора чисел непонятно частями делать надо применить какой-то статистический критерий но у меня была в университете тройка по теории вероятности поэтому я скачал книжку и оказалась очень хорошая книга там как раз про статистически эксперимента для любых целей оказалось что там есть алгоритм который нам идеально подходит который разобран на примере сравнение удобрений для томатов почитаете там подробно описано не буду рассказывать о том что достаточно сложно это непараметрический bootstrap ный метод там производит численный эксперимент и строится специальное такое рандомизации он на и распределение и какой смысл этого рандомизации онного распределения главный смысл в том что оно позволяет нам ответить на вопрос какую мы можем пронаблюдать разницу производительности даже когда мы сравниваем сервер просто сам с собой потому что за счет статистических эффектов всегда там будет какой-то шум до всегда будет значение немножко разные и нам надо понимать вот то что мы пронаблюдали на нашем запуске это что-то удивительное или это шум и вот эта штука она дает нам число что у тебя будет твой запрос отличаться на 0 2 секунды это нормально и мы соответственно можем игнорировать все такие вопросы запросу тут надо задать конечно какой-то порог фолз позитива в да потому что потенциально в каких-то очень редких случаях она может отличаться но неограниченно грубо говоря да поэтому надо как задать порог до api вылью какую вероятность мы считаем удивительный но мы задаем 1 или 0 01 это значит что мы будем смотреть только на случаи которые могут произойти не чаще чем раз в сто прогонов то есть вот если мы видим какую то ситуацию которая наблюдается только раз в сто раз мы считаем что произошло что-то удивительное смотрим на этот запрос ну тут конечно еще надо понимать что при такой формулировке сразу получается что у нас будет на 100 прогонов 11 фолз позитив мы просто из определения этого критерия соответственно 20 фолз позитив на прогон прогон если у нас и есть две тысячи запросов всего и к счастью мы можем отфильтровать еще по значению изменения в общем получается такое такая схема все что меньше пяти процентов изменения мы просто выбрасываем это нам не интересно все что меньше вот этого числа с который мы посчитали это статистически незначимы считаем что это шум просто если разница больше этого с это вот то что нас интересует как раз это значит производительность реально изменилась и еще есть такой специфический кейс это когда вот это ожидаемая разница она больше пяти процентов это означает что у нас не очень качественно запрос да он не позволяет нам замечать маленькие изменения он всегда будет показывать эти пять процентов и мы то что меньше не поймаем это мы тоже разберем еще с ними делать но давайте пока рассмотрим самый такой случай основное да вот happy пас который у нас есть это когда мы поменяли код и у нас из-за этого изменилось производительность статистически значимо это случае не не единственный на самом деле во первых может быть еще статистический ложноположительный просто результатом роста случаев сколько the falls позитив мы ожидаем еще бывает очень интересная вещь которая нам сильно мешала мы не сразу еще поняли что она происходит это изменение производительности реальные статистически значимые воспроизводимые но которые не вызваны нашими изменениями кода это связано с тем что компилятор вообще эта штука очень сложная и код kliks это та штука очень сложная и вот компилятор он применяет разные оптимизации там как это у него для этого есть мега конвейер и там много эвристик и у этих эвристик часто есть пороговое значение но типичный пример и тайну лайнинг функций например и когда как бы количество например за inline их функция она достигает определенного значения они перестают так что это такое они перестают и наладится и может так получиться что мы поменяли код вообще в каком-то другом месте какая-то функция другая за инлай не лазь асад наша функция которая нас интересует перестал войну они ц'ада там у нас перестал из этого векторизации цикл и вообще изменилось производительность 10 раз совершенно реально при том что мы ничего не трогали никак не связана с этим это один момент а еще про линковки тоже есть похожий момент да когда мы собираем эти все скомпилированный функций в один бинарник то расположение функции в бинарники она определена плохо к каким-то случайным факторами задается и взаимное расположение функции она тоже может сильно влиять достаточно на производительность вот у крупных контор таких как google фейсбук у них есть по специальному даже инструменту который позволяет нам получать более предсказуем расположения функции свинарнике и таким образом более предсказуемой производительности но мы пока до такого еще не дошли просветление ну допустим все таки у нас поменялось производительность действительно чего нам делать как понять чего происходит почему она применялась для этого у нас есть масса инструментов интроспекции наверное самое интересное тут это встроенный в house профайлер запросов вот если вы используете treehouse то это уже все включено у вас в продакшене скорее всего если вы не выключили это обычный в общем-то сэмплер у ющий профайлер который раз в какое-то время шлет просто всем потоком сигнал и записывает их stacks stack trace и кладет в специальную системную таблицу system tray свалок вот этим за просекам мы задаем что у нас там раз в десять миллисекунд будет центрироваться стектрейсы всех процессов и дальше таким простеньким за просекам делаем эти стектрейсы в формате нужном pipe им их прямо вот в этот грибовский скрипт венгров пыль и все у нас все работает у нас получается отличный стандартный flame граф как кто не кстати умеет пользоваться flame графами интересно о супер даже кто-то знает что такое ну вот в общем в один запрос и вот тут видно что действительно это дифференциальный оффлайн граф тут красненькое то то что стало медленнее осинкой то что быстрее и видно что действительно какая функция тут уже на скриншоте не видно но так он интерактивный там можно нажать и посмотреть что действительно это та самая функция максим все ускорил все супер что еще у нас есть еще для тяжелых таких случаев у нас опять же есть встроенный в клик house сбор аппаратных метрик процессора но может быть вы знаете в таблице к verilog там есть колоночка profile овец и там вообще довольно много метрик в основном там как сказать domain специфика специфичные для клих русски это штуки сколько он прочитал например данных там или сколько он отправил select of наши орды и такого плана метрики и там же в специальной настройкой можно включить чтобы туда записывались аппаратной метрики процессора у ядра linux для этого предоставляет специальный интерфейс 1 when the open есть инструмент перф стад консольные до of krakow все это можно делать изнутри и дальше мы можем строить такие интересные картинки вот тут например так что это у нас по значит по горизонтали у нас время выполнения запроса а по вертикали значение метрики первым ranch он страх шанс да сколько было ветвлений в этом коде относительно деленное на у в секунду вот и видно что на новом сервере это значение существенно меньше чем на старом и мы можем таким образом ну как это говорит в пользу нашей гипотезы что действительно мы убрали этот branch у нас все стал работать быстрее и там таких метрик много например разного рода к смеси и можно тоже для платки каких-то тяжелых случаев их все применять это все генерируется конкретно эта картинка нет но в целом вот flame графа например они все генерируются всей в процессе запуска этих тестов то есть так как пользователь тестов просто скачиваешь как файл и там все у тебя есть ничего руками дел книга это мы рассмотрели самый такой простой случай да но вообще эта система сложная достаточно но только полноценный intent с там запускается два сервера там настраиваться окружение для них какое-то данные генерируется запускаются запросы много раз и ошибок ошибок может быть масса там разные ошибки среды и до изменения сборки и тому подобное самое неприятное то конечно нестабильная сейчас быстренько расскажу про in my стабильные запросы это запросы которые дают непредсказуемое время выполнения да вот например запрос может выдавать там не знаю числа от 1 до 10 секунд и мы не можем ничего сказать про производительность по такому запросу и вот эта вещь и она в отладке сложнее всего потому что сложно установить причину когда есть как бы запрос быстрый медленно и да тогда понятно понятно можно сравнивать там какие-то метрики да вот эти флангов и дифференциальной а тут это гораздо сложнее так но рассказать я наверно не успею уже про разные виды нестабильности но это такая самая интересная часть дальше немного про потребительские свойства но эта штука и как бы люди пользуются в процессе разработки и поэтому к ним предъявляются определенные требования до в первую очередь это должно заканчиваться за какой-то обозримое время сейчас мы укладываемся в три часа для паре квестов хочется иметь минимум воспользуетесь да потому что когда постоянно вот эта красная галка и надо красный крест надо осматривать поставим эти запросы это очень бесит хочется чтобы даже если у нас будет фолз негатив это не страшно мы там все равно потом найдем когда-нибудь главное чтобы поменьше было фолз позитив но для этого у нас есть много разных техник там игнор маленьких изменений статистическая значимость разметка тестов про которым узнаем что они плохие мы их размечаем вручную и тому подобное должна быть возможность интерпретировать результат да вот красный красный крест и что это значит что произошло надо чтобы было понятно объяснил ночью не так и нужны инструменты интроспекции какие то то что я рассказывал платформеров как пример и нужна подробная достаточно инструкция у нас из отчета можно там нажать вопросик перейти на инструкцию которая описывает что это за часть отчета и что с ней делать но и самая такая сложная организационная проблема про которой хочется побольше сказать это вообще как бы тестирование разработка они находятся в некотором таком антагонизме да и когда ты делаешь хочу тебе не хочется исправлять какие-то непонятные тесты вроде ты же написал куда он работает все хорошо но давай померяем это я даже написал тест производительности ну и что что он показывает от 1 до 10 секунд мы работаем же тест есть вот такие такие вот вещи такие ощущения бывают и разбираться с этими нестабильными запросами это тяжело но к сожалению в перспективе приходится это делать потому что до этого у нас была система которая очень и как сказать позволяла много да можно было писать практически чего угодно и она просто гоняла эти запросы до позеленения там очень долго часами перезапуска во всю задачу и потом ты просто смотришь на этот отчет галка зеленые все хорошо да производительность это изменилось ну не знаю может изменилось может нет такая вот конвергенция вот к этой вот user story общему произошла так опять а не ту кнопку произошла постепенно вот и уровень уже бессмысленности этих действий он уже начал приближаться криптовалюта есть просто выполняется очень долго какие-то вычисления и по результату просто генерируется зеленая галка майнинг зеленый галки такой вот это достаточно сложно как бы избежать скатывание но я надеюсь что у нас получится так что еще это мы говорили про сравнение двух серверов которые запущены рядом друг с другом какие еще есть варианты с кем можно сравнить наши результаты ну конечно же это геометрии средний геометрически изменений по всем тестам такая очень полезная метрика она очень стабильная потому что там две с лишним тысячи запросов оно никогда не меняется если она поменялась это значит там произошло что-то прям удивительной совсем можно сравнивать со старыми релизами это мы делаем обычно вручную тоже такой же сайт ту сайт сравнение прогоняем как ни странно и даже ну ни как не странно ожидаемо у нас нет регрессии практически на тех запросах которые у нас есть и даже есть ускорение вот например с 28 это предыдущий lts версия мы получили там среднем 10 процентов ускорения в основном это за счет перехода на новый компилятор clang 11 и конечно еще полезно сравнивать с историческими данными мы все эти результаты пишем в cliff house всегда у нас уже накопилось там за 15 месяцев семь лишним тысяч хамитов занимает это все делала 6 гигабайт в сжатом виде вот и по этим историческим данным можно написать такой вот тривиальной запросик например если мы хотим понять чего у нас изменилась производительности когда да вот мы пишем такой запросик простенький на оконных функциях которые кстати уже есть начиная даже с 23 говорю с 20 13 вот тут например вот этот запрос он ищет в исторических данных ступеньку и проверяет что водка мид на котором произошла ступенька там их сайт сайту сайт сравнение тоже была такая же ступенька такой вот гибридный фильтр одновременно используется сайту сайт сравнения и исторические данные и вот если мы применим это на примере который мы разбирали про ускорение функций действительно видно что да вот такой commit он нам ускорил это на 20 процентов вычисления какой-то логической функции и по историческим данным это тоже видно потому что вот например мы посчитали что для предыдущих запусков было 260 милисекунд а для следующих запусков стала стабильно сто девяносто секунд и когда мы сайта сайт сравнивали два эти сервера мы тоже такую же разницу увидели мы можем быть четко уверены что действительно этот коммент повлиял на производительности то запрос если посмотреть наверх то видно что эта строчка 3 а в предыдущих двух строчках оказываются какие-то левые вещи вот про которые я вам говорил изменение связанные с граничными эффектами в компиляторе когда просто что-то поменялось непонятно почему так что от них все-таки не избавиться еще немного интересных фактов мы как нормальный человек делал бы такие вещи на наверное кроме орда там или может сахаб я не знаю на чем этот сайт на чем это принято делать мы как любители treehouse и делаем конечно же все обработку накликал сокол все статистические критерии то мы все генерацию этих таблиц для отчетов все накликал сокол если вы не пользуетесь михаил сокол он обязательно начните это очень крутой инструмент который позволяет вам выполнять все ваши обычные клика устные запросы но без сервера да просто по csv-файлы обвязка там вообще очень отход какие-то скрипты на баше используем что интересно еще сторонний драйвер питоновский клик house драйвер так что если вам интересно какой брать питоновский драйвер берите этот он есть у нас все мы даже ловили там баги у нас в хаусе связанные с поддержкой в этого драйвера идея все делать как ли хаусе оказалась очень полезной потому что это получается такой нормальный тяжелый интеграционный тест помимо своей основной функции тестирования производительности он еще заодно тестирует и сам playhouse тоже мы сделали очень много улучшений в собственном юзабилити клих аусла около уловили баги в профайле вычисления джонов и так далее и это пожалуй сейчас самая тяжелая проверка которой у нас есть ну конечно после сборок так все на этом я рассказал все что хотел подпишитесь обязательно наш блог я туда пишу статьи про очередные как-нибудь вещи интересные который мы делаем спасибо супер спасибо тебе сад и у нас есть для тебя благодарности а именно именно я рам кота ему почти сеть если вдруг у тебя еще иконостас мне собрался-то вот уже можно начинать и замечательных вершины худи плюс значок х и лодках мне вчера поправили поднимайте руки о первый ряд 2 ряд и дальше пойдем еще двое привет вы тестируете на кластере это первый вопрос автор а второй вы выбирать те медиана сколько раз вы гоняете тесты на первый вопрос вот эта система не тестировать накласть и это все отдельные отдельные машины там она даст довольно жирная но нет на кластере вручную до если надо на второй вопрос медиана для паре квеста семь раз мы гоняем это минимального сколько можно ну потому что у нас есть очень сильное давление чтобы быстрее да потому что когда мы там гоняли даже 13 раз это уже 10 часов и но никто не в состоянии всем дождаться и держит просто с не прошедшими тестами то есть это не годится как система для разработки поэтому нам надо даже потери точности но кстати что интересно для большей части запросов это дает очень хорошую точность даже проценты вот если статистические критерии то построить то даже 7 раз вполне достаточно оказывается дорогие друзья в онлайне я надеюсь что вы тоже проснулись вы себе похлопали также как мы себе здесь с утра чтобы ободрить вы там нажимаете кнопочку выйти в эфир и можете задавать вопросы у нас сейчас вопрос отсюда и было две руки по центру вот туда ребят пожалуйста приносить спасибо за доклад доклад у меня здесь у меня такой вопрос разные алгоритмы на разных данных дают разные результаты допустим на маленьких данных на больших данных сортированы не сортированы и допустим на одних данных показал изменение производительности положительная она других данных отрицательно вот какой из этих случаев будет приоритетным мой решать это сравнивается мы решаем по месту то есть ну тут надо смотреть на конкретную ситуацию действительно это происходит такое да это стандартная ситуация когда какая то оптимизация она на одних запросов дает прирост она других регрессии и тут мы уже смотрим на неё и думаем дома что я может сделать чтобы она всегда давал прирост или может ее можно включить только для тех данных где она хорошо работает или просто смириться с тем что регрессия будет и такой мы тоже делаем иногда мы считаем что он прирост важнее чем регрессии то есть тут по конкретному случаю же надо выбирать и получается уже не автоматически а после автоматически тесто вручную но удар и интерпретировать нужно и сколько таких про какой процент после автоматического теста таких кейс тут как сказать эта система она просто производит измерение до принятия это просто уже принятие решения по результатам теста фактически дальше там уже надо думать ну довольно часто то есть если если мы говорим именно про паре квесты которые хотят оптимизировать производительность да там это стандартная ситуация что мы что-то сделали но результат оказался не такой как мы ожидали и это это вот важная функция эта система донато benchmark от на разных там вариантах и покажет нам регрессии про которыми может быть и не думали изначально супер ребята представляетесь во-первых потом поднимите руку кто еще по центру был самый первый а вот добрый день алексей соколов мтс вопрос такой а на каких данных вы тестируете то есть это какая-то у вас прям тестовая своя база или там например тоже гид хабаровская потому с одной стороны если тестировать на каких-то старых статичных данных то тесто вроде как не совсем корректный могут быть для каких-то новых доработок а если на новых то соответственно старый результату могут быть не совсем галеоне у нас наверно наши так такие основные наши данные которые мы больше всего любим это данные яндекс метрики да потому что это изначальный кейс они у нас кстати опубликованы в обфусцированный виде на сайте и мы их используем для вот бенчмарков оборудования с помощью клик хауса про который я говорил но помимо этого у нас есть еще несколько реальных наборов данных тоже не вспомню сейчас каких и очень много синтетики вот в этих тестах там часто мы используем синтетику потому что там именно да посмотреть на какой-то конкретный кейс где мы например видели регрессию раньше и мы хотим его именно протестировать наш напоминает что тебе предстоит выбрать лучший вопрос пожалуйста задавайте свежий кухни привет здесь меня зовут сергей спасибо за доклад у меня несколько вопросов вы храните где данные по утилизации ресурсов процессе тестов возможно как-то сравниваете один вопрос второй вопрос возможно стабильные какие-то версии проверяйте на стабильность то есть длительный какой-то тест утечку может быть какой-то найти и понял на первый вопрос да мы храним их конечно взгляд хаусе у нас эти данные собираются автоматически в процессе выполнения запроса там как не по машине да вот чтобы а топ встроенный в клик house мы ещё не сделали хотя у нас бывает такое желание от лешего же хочешь тут сказать в процессе уже скоро будет опоп встроенный в клик house но по крайней мере пока для запросов мы записываем все эти метрики и у нас даже есть мы применим такой же статистический критерий ко всем остальным метрикам например к изменению память к потреблению памяти и можно точно также ответить на вопрос изменилась ли значимо потребление памяти после наших изменений такого плана так а еще второй вопрос был про нагрузочное такое больше тестирования у нас есть для этого инструмент специальный называется коллег house benchmark он умеет просто выполнять очень много запросов параллельно например у нас как часть приемочного тестирования в яндекс метрике мы берем с его помощью зеркале ruim продакшен низар коллируем нагрузка но запросы которые яндекс метрика делать продакшене мы их большом количестве выполняем на тестом классе такое тоже делаем но стресс-тест это немного специфическая вещь она она не про днепра производительность но да у нас есть такой режим просто все функциональные тесты тест производительности запускаем 20 потоков на одном несчастном сервере и смотрим как он в 32 даже но вообще это бесчеловечно смотрим как он падает он не падает тут же 1 ряда подсказывают просто есть суфлера из яндекс эти вопросы у нас в правой части зала второй ряд дальше будет первый ряд привет спасибо за доклад у меня такой вопрос ответ 2000 тестов которые на которых и проверяйте свои комменты они вас закат кожаные правильно понял да да это все руками написанную есть такой момент знаешь что нельзя взять просто произвольный запрос и чтобы из этого получился тест производительности к сожалению то есть приходится думать и приходится писать руками за это но вы пробовали там использовать это инструменты типа сиквел смит для генерации там запросто которое то есть статья наши ноги да у нас есть специальный fazer но это немного не по теме но да мы используем fazer и конечно это не связано с производительностью почитай в бок боги у нас есть это хорошо спасибо чего нет готового кисть готова была да конечно россиян скажите если нет регресса после тестирования от релиза к релизу он работает быстрее едешь смысл постоянно обновляться лески это рекомендации мы всегда рекомендуем обновляться на последнюю стоял версию а если обновляться в кластере в разные версии роль инвестирования и rolling будет влиять на работу с разными версиями могу работать вот это я не помню лишь подсказывали ну у меня сейчас есть отличие в релизах на двух серверах да можно в процессе обновления запускать на разных частях кластера разные релизы но при этом рекомендуется делать не оставлять это надолго то есть именно во время обновления чтобы было обновление без downtime а почему потому что вот в некоторых случаях например меняется протокола репликации и в случае если работают разные версии репликация будет приостановлено пока вы не обновить кластер до конца речь идет о единстве таблицах a distributed таблицы будут работать прекрасно у нас два доклада в одном он учился даже если даже просто все проклято я надеюсь вы сможете продолжить в онлайн кулуарах у нас известен кто следующий вопрос задает и напомните все кто хотел задать вопросы чтобы мы ориентировались добрый день я михаил вопрос вы говорили что тестируете на каждый полу request два сервера одновременно предыдущий и соответственно весь следующий именно тестируются какие-то конкретные версии то есть например devil apple master plus a request и про отдельные commit вы еще говорили что они тоже тестируется это как происходит в качестве референсной мы используем обычном ближайшую версию из мастер это определяется по github просто мертв bass и для мастера тоже самое ну для мастера это просто предыдущий коммент буквально вот мы запускаем сервер от предыдущего кометой от текущего и сравним то есть по request идет какой один готовые ценниками да и мастер и отдельно от колита сравнивается по запросу бури квест вот мы берем просто эту ветку да вот куда она сейчас указывает собираем это а потом берем имидж bass с мастером вот место где она отошла от мастера это я понял вот как референс используем соответственно эту сборку ну хорошо и у нас последний вопрос не потому что у вас нет больше рук а потому что просто время уже выходит привет скажи пожалуйста как часто мимо вашей системы тестов проскакивают лаги в руки в релиз который просаживают performance и может быть у вас есть какое-то тестирование на проводе пост-релиз на следите за этим готовить и делайте выводы у нас конкретно как у разработчиков клик хауса нет про до своего наши основные друзья с продам эта война метрика в яндексе еще баннерная система да у них есть припрут у них есть свои очень обширные тесты которые они применяют перед выходкой и да действительно регрессии бывают во первых не все покрыто этими тестами да это немного специфический такой жанр например там нет у распределенных запросов хотя это казалось бы базовый такой есть ну это подсказки залу нам не слышны они такие даешь говорит что на самом деле есть на одном сервере но это немного не то добывают находится кстати руками то есть вот когда я потом сравниваю руками запускаю сравнение lts версии и при хауса через полгода до оказывается что регрессии все которые даже были мы их уже исправили и их нет то есть в целом все работает достаточно эффективно наша система тестирования саша саша вот скажи вот вот ощущение здесь на сцене они вот насколько на скалолаза нет похоже но это дольше чем скалолазы скалолазание там обычный рывок и упал долго есть еще занятие в ангар тоже сюда хочется подольше сейчас не будем об этом говорить с кулуарах в кулуарах хорошо какой запросу больше всего тебе понравился это так сложно они все перепутал перепутались у меня мы должны вручить сейчас приз до должно вручить первому вопрошающему у него был хороший вопрос поздравим друзья победителя на свою маленькую розыгрыш а я напоминаю что есть цифровые кулуары вы можете вместе пройти с докладчиком докладчик там сейчас будет те кто в онлайне могут подключиться взлом и задать свои вопросы обсудить какие-то моменты спасибо вам огромное сейчас прошу освободить зал и организована чтобы мы провели дезинфекцию благодарю"
}