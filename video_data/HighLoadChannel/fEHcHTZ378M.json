{
  "video_id": "fEHcHTZ378M",
  "channel": "HighLoadChannel",
  "title": "Зачем и как мы написали MapReduce-движок над Clickhouse / Андрей Кузнецов (ВКонтакте, VK)_en subs",
  "views": 3397,
  "duration": 2028,
  "published": "2023-01-09T06:42:23-08:00",
  "text": "Здравствуйте меня зовут Андрей Кузнецов и сегодня я вам расскажу зачем и как мы написали mapress движок кликхаус Прежде чем идти далее расскажу пару слов о себе Яру команда инфраструктуру аналитики ВКонтакте на протяжении уже более чем двух с половиной лет команда делает сервисы для работы с данными и поддерживает большой кластер кликала сейчас у нас 96 хостов до этого я работал Яндекс и 404 Давайте начинать Дайте представим себе что у нас есть какой-то страшный запрос на Клик Хаусе например Выглядишь так и попробуем его запустить здесь можно обратить внимание лишь на две вещи первые вещи то что здесь есть Joint а второе то что здесь есть конструкция и пытаемся что-то посчитать что у нас получается Если мы запустим просто в кликхаусе то мы получим Memory limit ошибку потому что наш запрос использует почти 227 гигабайт оперативной памяти у нас ВКонтакте есть некоторые синтаксический сахар который мы добавляем этот запрос и запускаем и Он отрабатывает без каких-либо проблем и вот тому как мы сделали такую обертку и будет посвящен мой доклад а какой у нас план на сегодня поговорим об исторических предпосылках аналитики ВКонтакте поговорим о сборке агрегатов витрин настоящее время как у нас устроено сейчас и посмотрим как сборка агрегатов и три привела к появлению нашего посмотрим что он умеет и в конце посмотрим как мы подводим еще эксперименты с помощью нашего Итак кликхаус продуктовой аналитики ВКонтакте появился у нас в 2018 году и тогда типичная таблица у нас была устроена вот таким образом данные хранились в базе Local и это были собственно хостовые таблицы семейства мирштрий вот каждая такая таблица имела префикс Local чтобы ее было проще опознавать и Каждая таблица в обязательном порядке имела следующие дополнительные вещи Это был Partition Buy обычно это какое-то поле по временной метке допустим день или месяц и в обязательном порядке обычно это User ID но могут быть и другие варианты Например комьюнити если это таблица например по сообществу далее на каждом Кости для этой таблицы есть буфер который сбрасывает данные в эту таблицу расположены на базе буфер и имеет соответственно префикс и этот слой используется для вставки данных мы используем буферные таблицы для того чтобы формировать более крупные Бачи И тем самым менее нагружать файловую систему Хоста и экономить ресурсы и поверх всего этого существует кластерная таблица дистрибутив из которой собственно происходит чтение данных она у нас лежит в базе дефолт Вот соответственно это очень важный сайт здесь нужно запомнить вот три вещи есть база локонов которые собственно хранить данные из база буфер который данные пишется есть база дефолт которая есть база состоящая из таблицы с которой мы читаем данные вот это было в 2018 году и действительности это устройство живо до сих пор с небольшими изменениями иногда но в целом оно соответствует тому что есть сейчас какие у нас есть первый тип таблиц и самое важное это логи логи они пишутся спрода прямо с событий происходящего против это большие таблицы как правило от 100 миллионов до 10 15 миллиардов и обычно мы используем движки репрессия номер 3 для этих таблиц второй тип таблицы это агрегаты они наполняются путем запуска обработки логов и как правило гораздо меньше потому что не по факту является сжатыми логами это 10-100 миллионов строк и здесь разнообразие движков чуть побольше мы иногда используем лишь 3 для специфических кейсов Вот теперь что касается как аналитика была устроена в прошлом 2019 году она приобрела вот такой вот вид данные напрямую писались из в кпхп через kitton House в clic House то есть Прямо вставляется Стрим в среднем задержка в те времена и сейчас была приблизительно пять минут и до сих пор на такой остается основными клиентами были супер сет дата грипп и клехаус клиент суперсети обитали в основном аналитики тогда немногочисленные они также любили делать забросики из Data гриппа и крихаус клиент Это был скорее админский интерфейс для тех у кого был доступ логи писались напрямую с провода а чтение и сжатие данных в агрегаты происходило с помощью самописанных агрегатора на Java который буквально делал следующее типичный сборка агрегатов это запросы Select и с каким-то грубаем Ну вот здесь на стадии представлена грубаи в дату и соответственно свертка Лога в ДТ и юзер теперь как это происходило Как устроена сборка агрегатов было тогда давайте посмотрим на правую часть слайда значит запрос разбивался на подмножеством подмножество определяются сэмплом который как вы помните мы задавали при создании таблицы то есть сэмпл действительность это часть ключа таблицы по которому можно читать данные соответственно сэмпл разбивает таблицу на какое-то количество не пересекающихся множество которые можно прочитать отдельно обработать и Сложить это результаты обработки этого множества данных уже в таблицу агрегат Вот почему схема называется диагональной потому что мы считаем из дистрибутика то есть мы запрашиваем дистрибью таблицы Первый кусок данных если будет таблица говорит OK посылает запросы каждый номер три получает ответы дагрегирует их и потом в конкретный уже хост то есть чтение у нас происходит со всего кластера то есть диагонально а вставка происходит у нас в конкретный хост в чем минус такой схемы Ну очевидный минус это большое количество сетевого взаимодействия потому что каждый кусок данных берется со всех серверов до агрегируется в дистрибутив на конкретном Хосте а потом вставляется уже тоже в конкретный хвост то есть здесь основная проблема Это именно сетевое взаимодействие вот мы до сих пор используем эту схему но мы не стоим на месте и достаточно сильно развиваем наш агрегатор прежде чем переходить к новой схеме Греции Давайте пройдемся потому как был алгоритм девятнадцатом году Итак разбиваем весь набор не пересекающиеся множество сэмплбай или через модулы вариант гораздо хуже потому что он требует скана а Давай это все-таки чтение по индексу но тем не менее тоже возможный вариант и он иногда используется далее на каждом из N кусков Мы выполняем агрегацию складываем результат в один из хостов Ну или шардов далее если мы вставляем буферную таблицу то Мы обязаны проверить сброс буфера и если мы вставляем реплицированную таблицу То есть если мы должны дожидаться окончания репликации последние два пункта очень важны Потому что если у нас одна сборка зависит от другой и мы не дождались просто буфера или мы не дожались репликации мы можем прочитать некорректное подмножество данных что чего нужно любыми силами поэтому уже были в 2019 году встроены такие проверки в 2020 году мы решили переписать агрегатор на питон это нам сильно простила поддержку Мы очень сильно поработали над отказоустойчивость то есть Мы научились переживать отказ одной реплики в шарде и даже кратковременные выпадение шарда в рантами То есть если по какой-то причине полностью ушел там например 120 секунд то мы можем просто его пропустить и работать с другими шардами если схема сборки нам это позволяет делать также Мы научились восстанавливать упавшие агрегации с последней успешной вставки То есть если у нас какая-то длинная сборка бежала например в течение часа и на середине упала а то раньше мы как делали мы зачищали таблицу и заливали заново сейчас этого делать не нужно можно просто передать соответствующий флаг агрегатор он поймет какую последнюю часть данных он ставил и начнет собирать дальше с неё и самое главное что она позволяет делать новый агрегатор это локальные сборки кроме диагональных теперь вкратце посмотрим что такое локальная сборка локальная сборка отличается от диагональной тем что мы у каждого Хоста запрашиваем все под множество данных и обрабатываем на каждом Хосте эти количество данных независимо от других остов то есть мы запросили энный кусок у допустим первого Хоста обработали его и положили вот сразу же в тот же самый Host но в другую таблицу Итого у нас Хосты обрабатывают данные полностью независимым и никакого сетевого взаимодействия передача данных по сети не происходит важный момент в том что при этом чтение происходит не из таблицы дистрибутив до кластерная происходит именно из самого мир штрихи мы это видим секции From где у нас запрос идет базу Local и соответственно в таблицу с префиксом То есть это локальный мир 3 и без него они всего кластера это нам сильно экономит ресурсы и когда это возможно мы всегда стараемся использовать локальную сборку и это вот основная фича нового агрегатора которая позволяет делать всё гораздо быстрее и более аккуратно по отношению к нашему железу типичная среда обитания агрегатора это вообще etly plain pipe-line это консольная утилита которая дергается каким-то шевелером например азкабаном или airflow и она имеет следующий аргументы первый аргумент это название агрегата второй аргумент Это количество не пересекающихся кусков Тут нужно пояснить что здесь количество кусков один в действительности мы домножаем это фоне на 32 потому что нас 32 шарда далее количество потоков То есть если мы указываем 32 потока У нас 32 шарда и при этом собираем локально то мы параллель грузим все 32 шарда нашего кластера и они работают Независимо и вот последний флаг это флаг как раз изучающий локальный сборка то здесь мы распиливаем все множество на 32 куска и собираем его локально 32 потока мы пошли дальше и задумались А что если дать http интерфейс агрегатору вот у нас были следующие соображения какие будут плюсы первый плюс очевидный аналитики смогут самостоятельно готовить себе агрегаты без лишней бюрократии джира долгого код review gitlab и так далее То есть без нашей помощи станет проще тестировать сложность написали запрос котором 1000 строк Вы можете его запустить и посмотреть то ли он собирает то что нужно вообще работает ли он и так далее и наконец-то появится потребованию у нас был сильно встроен основные Pay планы и его было невозможно запустить на своих основных метриках не добавив Поэтому если дать возможность людям самим собирать агрегаты то появится возможность читать обтесты минусы минусы были такого характера Мы подумали что аналитики Будут еще сильнее упоровать кликхаус аналитики будут вынуждены разбираться в http то есть чтобы ходить по можете пить дёргать агрегатор и придется писать много кода но действительности получилось гораздо лучше чем мы себе представляли получилось у нас следующее у нас получилось удобное защита парня у нас появилась теперь между аналитиками и агрегаторами Клин хаосом есть который защищает перегрузки сетевой слой мы спрятали в удобный клиент который выглядит очень активно и забавный случай один из аналитиков работал с клиентом как будто бы он работает дата сетом локально То есть он не понимал что в действительности его запрос его данные обрабатываются на бэкенде а не у него в юпитер-тиратский аналитики используют API как сервис для дата инженерного самообслуживания то есть они знают escale могут использовать его для сборки своих витрин самостоятельно не ставим задачи И тем самым Маркет достаточно быстро падает значит Кроме того важный интересный момент это то что наш инструмент он конкурируется с парком для запуска от hook запросов на больших данных часто аналитики выбирают где будем запустить запросы чаще они предпочитают на самом деле наш инструмент они Спарк естественно мы изначально планировали у нас это получилось сделать интерфейс для обмена больших данных и дополнительно мы автоматизировали рутину по добавлению колонок и удалению всяких кривых данных ошибочно аналитиками нами вот теперь прежде чем переходить к внутренним устройству Давайте остановимся на основных терминах которые нам будут важны для дальнейшего понимания первый термин это операция это набор логически связанных действий которые требуются сделать для получения Конечный результат Какой может быть Конечный результат это может быть сборка витрина то есть операция greation это может быть Select который отдает данные аналитику в тетрадку это может быть подсчета B или alter конечно же понятно это добавление или удаление дальше джоба это составная часть операции по факту это запрос над ним куском данных из множества от одного до n кусков pipeline это последовательно выполняемый набор операции которым как может быть связан по данным так Нет вы можете независимой операции винить paypline для какой-то своей цели выполнить даже если они не связаны по данным такая возможность имеется теперь вкратце поговорим о внутреннем устройстве внутреннее устройство сервиса очень простое архитектура максимального упрощенная для того чтобы можно было легче разрабатывать и быстрее внедрять новые фичи Значит мы используем в качестве внутреннего хранилища статусов и логов операций в качестве очередей Мы берем рейтис и дальше у нас стоят norker они все общего назначения то есть может как выполнять агрегации так и выполнять все лекты так и считать и эти воркеры уже непосредственно работают с данными на Клик Хаусе в качестве входной точки перед ними стоит который обрабатывают входящие запросы из бетон клиента потом клиент это обычный обертка на до 6 безобрасываем для того чтобы аналитикам было удобней обычно питон клиент вызывается либо из Юпитера либо например с airflow Если кто-то что-то хочет автоматизировать поставить на расписание и так далее важный момент в устройстве всего нашего бэкенда это устройство шеделлинга это наверное самое интересная часть о которой можно чуть подробнее У нас сейчас есть предопределенный классы очереди есть класс коротких Чародей куда операция попадает если секунд средняя то есть от 121 до 900 секунд и длинные это 91 секунды до 6 часов Это сейчас определяется аналитиком когда он описывается операцию питоновским кодом он задает тайм-аут для своей операции будущем Мы хотим отказаться от этого решения перейти на eml в определении таймингов для каждой операции но сейчас мы используем схему с мультиваркерами А до этого мы использовали схему с одного очередь один worker как это работа приходила операция классы очереди определялся по проставленному тайм-ауту А дальше внутри класса выбиралась самая короткая очередь вот сейчас у нас для одного класса Чародей существует только одна очередь которая уже читает несколько оргеров Как легко понять это сильно снижает Время ожидания в очереди и в принципе работает более прозрачно и Понятно чем схема с одного очередь один worker и мы с конца августа полностью пришли на схему скажу еще пару слов очень простой в качестве веб-сервера мы используем фастафе потому что на синхронные современные принципе довольно удобно обрабатывать и уже был опыт работы с ласком поэтому выглядит хорошим вариантом в качестве бэкенда воркеров мы используем workers наверное самая простая библиотека для синхронных людей Нам пока хватает возможности мы довольно неплохо научились ее готовить поэтому используем ее То есть она у нас используется не только в проекте API Ну например в проекте Аномалия То есть у нас такой довольно стандартный стек который нам позволяет создавать сервисы для работы с данными довольно быстро логика воркеров это кастомная библиотеке То есть каждый из операций действительности это кастомная библиотека например для агрегации и для секретов используется greation Tool То есть это тот агрегатор который описывал на первых слайдах для подведения об тестов нас используется библиотечка достаточно короткие порядка полторы тысячи строк всего поэтому объем кода на самом деле очень-очень небольшой вычислительная часть у нас довольно таки стандартная это прежде всего нам пай пандус и для подведения результатов мы используем еще с Models линейной моделью об этом чуть позже Теперь давайте посмотрим на устройство клиента наверное самое интересная часть и будет более понятным Какие возможности у нас сейчас есть И что появится в дальнейшем для начала схема классов клиенте выглядит вот так есть две основные ветки наследования от базовой операции это ветка админских операций типа альтеров и пользовательские операции которые наследуется вот потомка Operation plain Operation plation операция может быть использована и вот здесь мы видим что есть операция bregation есть Наследник Селект агрешена есть еще Наследники бакинском кастом в баке с кастом символ это шаблон для AB дальше тоже подробно об этом расскажу Ну и в принципе система построена таким образом чтобы аналитики и те кому требуется что-то писать могли легко отслеживается базовый класс реализовать себе нужны Итак первая операция самая основная tagrecation по факту это сборка витрины То есть это выполнение на сердце Давайте посмотрим более подробно слева мы видим код запроса код витрины который собирает у нас что-то из таблицы слогами видим прокинутый placeholder в sample of Set Они будут заменяться агрегатором на соответствующие куски также видим поиск холдер ТТ для даты дальше это все добавляет свою отвертку regation прокидывается клиенту соединение и указывается тайм-аут в данном случае мы указали 120 секунд это значит мы попадем быстро очередь и указывается гранулярность гранулярность Как вы помните Это количество кусков на которые надо разбивать в нашем случае здесь это будет равно 32 куска дальше операции стандартный интерфейс И в нем этом интерфейсе есть метод который согните эту операцию и дальше с этой операцией работает какие есть еще аргументы в Операции самое главное аргумент как я говорю это локальная сборка и называется Документ и издан шар цемен включен то соответственно сборка происходит локально можно указывать конкретный шар на котором вы хотите собрать можно указывать даты можно указывать даже когда запустить То есть вы можете создать операцию гибко зашедлить и она бы к этому запустится в то время в котором вы сказали вот Ну понятно Я уже понимал что есть гормональности есть количество оркеров которые надо передать в целом это очень конфигурируемая операция не по факту больше 20 флагов и можно довольно гибко настраивать то что вы хотите следующая операция Select Select это самая популярная операция наверное самое важное Давайте посмотрим подробно представим что у нас есть запрос вот мы что-то агрегируем в некоторых существенно ради по таблице контент тут Важно отметить что таблица контент сэмплирована по User ID то есть мы вычитываем подножить ради агрегируем вовне то есть каждом из N кусков на котором разберем dtset может быть представлен ключом ради эн раз вот поэтому мы действительности делаем в этом селекте предогрегацию а потом уселикты есть специальный метод DF который можно передать свой кастомный запрос который уже выполнит до агрегации что мы делаем Сначала мы на кусках собираем агрегацию а потом эти N кусков поверх них запускаем агрегацию вида селектор сначала разби эскиз их агрегировали а потом до агрегировали эти куски если посмотреть на Pay планки преобразовались данные то на стадии предогрегации Мы из таблицы контент Юз получили временную генерируемую таблицу в таблице condonalds при этом было 11 миллиардов строк а на выходе мы привели 163 миллиона строк а дальше настать до агрегации Мы из временной таблицы генерируемой 163 миллиона строк взяли и на выходе получили 29 на каждой стадии мы последовательно уменьшали объем нашей таблицы и наверное логично предположить что операция Select это некоторые pipeline что происходит по запросу происходит инференс схема промежуточной таблицы то есть название колонок из вашего и типы дальше происходит запуск промежуточной таблицы потом происходит запросы с промежуточной таблицы и до агрегации если требуется дальше ваши данные попадают в Юпитер значит сборщик У нас есть сборщик мусора временные таблицы удаляются по какому-то заданности в действительности если мы посмотрим на это чуть по другим углом то запуск операции это стадия стадия запроса из промежуточной таблицы навигации это стадии редис Итого Селект агрегации у нас реализует то есть мы разбили набор данных на куски их Независимо отработали а потом по какому-то набору ключей нам нужно сделали и собственно Select это и есть некоторые мапредился интерфейс данным creehouse который нам позволяет обрабатывать очень большие объемы данных которые не влазят в память одной машины обрабатывать дикие селекции с джойнами и всякие такие вещи которые часто пишут аналитики Ни в чем их не ограничи ваем Давайте теперь перейдем к paypline который реализуют функциональность которую мы описали выше Давайте представим что мы хотим использовать нашему предел Стратегию и результат до агрегации потом мы хотим еще куда-то вставить мы считаем что это большое догрегируем и поэтому допустим рисуем какую-то картинку в суперсети любой системе для решения такой задачи можно использовать что для этого нужно сделать первое самое важное это зададим название временной таблицы заранее То есть вы видите мы написали придумали какое-то название указали номер реплики в которой нужно уложить дальше мы это название используем в аргументе был то есть таблицы будут сгенерированы именно с этим названием а следующее задание операция агрегации У нас считает из этой таблице подставляем знания таблицы создаем объект добавляем туда две операции через метод и запуска и в действительности вот эта вот схема реализует My preduce стратегию в виде такого вот пайплайна чем это может быть полезно но как я уже говорил Допустим мы хотим нарисовать график по очень большому набору данных Сначала мы должны Map сделать потом уже после этого рисовать вот ну параллельно где-то если это требуется но это уже все должно происходить на уровне биосистемы то есть здесь самое важное часть это последовательность которую мы реализуем вот таким вот достаточно простым и я бы сказал шаблонным кодом отмечу здесь что в таблице временные которые создает операция Select мы еще указали что мы хотим иметь ключ сэмплирование еще мы это сделали для того чтобы наша операция агрегации в случае если у нас очень много данных запишется промежуточную таблицу могла тоже отработать по частям и это нам позволит даже в случае большого набора и промежуточных данных не упасть и обработать корректно в первой таблице мы читаем а потом делать финальную агрегацию это все достаточно просто и код шаблоны можно импортировать и второй важный пример использования пайплайна это подсчета B Для начала я расскажу как вообще у нас устроена AB Значит у нас а б считается не на юзерном распределении она по бакетам Почему Потому что у нас очень большое количество экспериментов если я все правильно помню то это порядка 130 экспериментов в день в действительности можем обрабатывать и значительно большее количество экспериментов каждого эксперименте порядка там 500 может быть 600 метрик и поэтому мы переходим по банкетному распределению распределение получается следующим образом это берется какой-то Хеш от юзреть И делится на количество пакетов и этом бакету применяется какая-то грядирующая функция например сумма соответственно нас выборка уменьшается достаточно сильно и мы работаем с маленькими распределениями которые можно быстро легко обрабатывать Давайте себе представим что у нас есть аддитивная Метрика То есть это Метрика которую можно за весь период эксперимента сложить допустим просмотр постов в умной ленте чтобы каждый раз не запрашивать весь диапазон данных давайте сделаем следующую штуку Давайте будем хранить кумулятивную сумму по этой метрике в этом конкретном эксперименте и в этом пакете и каждый день будем подливать небольшой кусочек данных И тем самым у нас диапазон чтения будет всегда один и тот же то есть мы будем всегда читать набор ключей и скорость чтения не будет зависеть от количества дней которые идет эксперимента будет зависеть только от количества ключей в этом эксперименте Мы сначала получаем по бакетное распределение а потом распределение для аддитивных метрик уже агрегируем в кумулятивную таблицу которую мы храним данные за весь период и после того как мы все необходимые агрегации сделали мы вызываем уже собственно подводилку AB на полученных данных как это выглядит подсчет А Б как я говорю это же paype line он состоит из следующих частей первая часть это получение по бакетного распределения для этого есть уже упоминавшийся мной шаблоном баке с кастом это шаблон по факту генерирующие запрос вида Join Логана Join Экспериментальный размер экспериментальной разметка это просто логирование вызовов где-то Брукс которым из которого мы понимаем в какую группу попал конкретный юзер конкретном эксперименте мы видим в шаблоне перечисления метрик и потом перечисление формул того как они считаются и указание вот ну и какие-то вы conditions дальше это добавляется в Starting Jobs класса B5 Наследник базового класса paypeline специфический который внутри себя генерит необходимую нам последовательность сборки кому сборки багетного распределения дальше сборки кумулятивной суммы и вызовов уже Starting Jobs мы можем распечатать посмотреть и вот код который указан на данном сайте он генерирует следующий это получение побакину распределения вот тут для 27 октября дальше идет тоже агрегация но эта агрегация в кумулятивную таблицу то есть тоже для 7 октября А дальше идет два вызова операция B первая операция B Это для неадитивных метрик а вторая для детей то есть одна из них ходит в бакетное распределение а другая в кумулятивный байке на распределение Вот то есть бyплайн это шаблон для б который уже под капотом генерирует все нужные операции в последовательности корректной и соответственно аналитиком обычным людям об этом думать не нужно И это все уже делается них автоматически Вот теперь перейдем к самому интересно это план на будущее не всегда легко приятно говорить как я говорю уже распределение по очередям было бы на самом деле сделать очень хорошо потому что люди часто ошибаются с тем тайм-аутом который не указывают и попадают в некорректную очередь мы уже достаточно хорошо научились предсказывать время выполнения операции и сейчас эта задача находится уже в активной разработки дальше Мы хотим автоматически развивать на куски то есть не указывать система должна сама понять что ставить Как нужно сыграть а кроме того Мы целимся в очень амбициозную задачу эта интеграция с биосистемами что мы хотим делать Мы хотим тяжелые запросы исполнять через кодогенерацию Pay планов в нашей связке selectation если аналитик написал очень тяжелый запрос например подсчет не знаю просмотров постов умной ленте за год по каждому юзеру то такой код запускать на кликхаусе в прямую нельзя он будет сгенерирован соответствующий питон код для pipeline и он будет запущен уже и человек получит свой график несколько попозже но и самое наверное амбициозная Цель этого консоль сейчас у нас кот относительно небольшой маленькой команды и наверное будущем Мы бы хотели чтобы поделиться сообществом нашими наработками И наверное Возможно кому-то это будет интересно Вы уже собственных компаниях и теперь несколько забавных цифр у нас примерно 9 воркеров в кликаусапи выполнять задачи два человека выполняют задачи по разработке то есть команда действительно очень маленькая еженедельно запускается примерно 10000 агрегации Select и активная аудитория сервиса это 70 человек в неделю это большинство из них аналитика но есть разработчики мэрии инженеры и иногда даже продукты за прошлый год больше всего запустил человек операции это 39 тысяч операций А по рекорд это две с половиной тысячи операций за год спасибо за внимание Буду рад ответить на ваши вопросы Спасибо"
}