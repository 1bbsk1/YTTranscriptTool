{
  "video_id": "8LfjqZKHVII",
  "channel": "HighLoadChannel",
  "title": "Поиск в HeadHunter: как подружить machine learning и production / Данил Смирнов (HeadHunter)",
  "views": 455,
  "duration": 2048,
  "published": "2023-01-19T05:55:17-08:00",
  "text": "меня зовут данил и кантер я занимаюсь разработкой поиска когда деюсь многие из вас слышали есть такой сайта хару где можно разместить вакансию резюме и самое главное найти интересующую вас вакансию и резюме но наш поиск это не просто поиск это основное средство рекомендаций в котором применяется большая часть машинного обучения используемого в компании и к жилью для задача машинного обучения все еще нет какого-то стандартного решения которое позволяло бы внедрить это с учетом нашей нагрузки и количество данных а там где есть что-то нестандартный открывается огромный простор для ошибок костылей и велосипедах о которых хочет сегодня поговорить ведь на чем еще лучше учиться чем мы на чужих ошибках поэтому поговорим что с работу нас разработки такого сервиса что не сработало какие оптимизации можно применить а что было изначально обречена на провал и не стоило даже пробовать чтобы все были на одной волне хочется вначале немного поговорить про что такое inference то есть предсказание моделей что конкретно нам нужно реализовать на продакшене у нас от дантистов есть некая обычная модель по сути это математическая конструкция которая на входе подается некий набор векторов фич а на выходе мы получаем число в нашем случае мы интерпретируем это число как скоро конкретной вакансии чем он выше тем выше вакансия будет в поиске у нас применяется так называемый он сам лишающий деревьев в качестве модели дайте затем сложена за ним стоит на самом деле очень простая идея у нас есть некий набор деревья в каждом узле которого мы проверяем значение соответствующие фичи из массивов фич а на выходе получая мы в это число в конце после прохода всего дерева и результат модели это сумма значение всех деревьев остается вопрос что такое фильтр фич на самом деле он получается довольно сложный математикой из наших исходных данных это конкретно резюме и конкретное вакансия этот процесс можно назвать векторизации и идея в том что мы преобразуем текстовые данные например название вакансии и желаемую позицию либо числовые данные то желаемую зарплату либо вовнутрь и фиксации может быть даже нейронная сеть которая сравнивает насколько хорошо подходит данный опыт разуме к требованиям вакансии еда выходе мы получаем собственно интересующий нас вектор twitch проблема в том что когда вы ищете в поиском что либо вы ожидаете получить ответ за приемлемое время посчитает такой вектор и викторе заводь все вакансии которые прошли фильтры поиска для вашего резюме в онлайн практически нереально до из призрак считать все эти пары очень сложно потому что нас миллионы и резюме и сотни тысяч вакансий и на самом деле вопрос как решать эту проблему скорее вопрос к dots on this там одной из вариантов решения на котором мы остановились это разделение это вопрос электризации на несколько этапов в первом этапе мы считаем виктора отдельно для всех вакансий и отдельно для всех резюме это можно делать асинхронно и я такой широкий зультаты а потом уже во время поиска мы по определенным правилам заполняем этот мастер fitch например в этом правильно писано что нужно на позицию номер два поставить результат скалярного произведения вектора 1 и 2 мы ее вектора 2 из вакансии этот тип орла по сути становится частью модели которые интерес которой мы хочет мы хотим делать в продакшене итак мы понимаем что нужно сделать самое время думать как наша первая идея была вполне стандартный идея архитектуры использование машины и подошли когда мы делаем reference в том же по сути сервисе что и отвечал за обучение то есть делаем отдельности роста питоне который получает на вход и диски из поиска и уже внутри себя оранжевых проблема в том что кэш векторов который нам необходим для расчета модели зачем на большой и нам в любом случае придется делать шарди рования внутри этого сервиса делать какой такой который обновляет кашин викторов при изменении вакансии и резюме и еще дополнительно к этому скорее всего разбивайте дядюшки на baci чтобы время ответа нашего сервисов была в пределах разумного а в это время у нас уже есть готовые структур инфраструктуры на джаве в поиска которая уже есть яндекс уже решена проблема формирования и обновление его поэтому мы решили сэкономить и упростив подвижкой в натуры делать inference прямо в джаве каким образом мы это сделали в первом варианте виктора вычисляются асинхронно при индексации и сразу складываются вместе с документом в яндекс используя дакфейс если финны модель это просто дерево в памяти которые мы можем легко обойти самым стандартным годом и результат этой модели то есть score это просто тот же скоро что используется для сортировки результатов в плюсе не заработала ли в таком виде даже не близка наша даже первая самая консервативная модель потреблять столько ресурсов что нашего кластера в то время из 20 мощных серверов которые обеспечивали все весь поиск вакансий на сайте не было ни единого шанса в чем были проблемы это увеличение индекса сразу в два раза увеличение потребления памяти в сервисом поиска в полтора раза и это не считая той памяти что использовать для каша файловой системы для индекса и самая проблема на это увеличении загрузки процессора до 10 раз для пленок запросов модель тогда точно не оправдала бы десятикратное увеличение количества серверов что делать нам точно не хотелось пытаться урезать модели потому что чем меньше fitch модели тем меньше у нее шансов выучи какие-то детали пользователи оценивают качество поиска по первым результатам поэтому мы решили пойти немного другим путем добавить фильтры так чтобы модель работала только на тех вакансиях которые скорее всего подойдут пользователю и отсекать то и точно на то что они подойдет на первом уровне у нас используется эвристики это вас новые листики по региону чтобы отсечь те вакансии которые заведомо далеко и пользователь на них точно не откликнется потом идут два фильтра линейный фильтр из очень обожал количество фич и фильтр на решающих деревьев из уже гораздо большего количества фич и в конце концов то те вакансии что прошли эти все эвристики и фильтры уже выпадают в оранже ли нашей модели на самом деле внутри это немного сложнее мы не просто отсекаем вакансий потому что пользователи тоже иметь возможность найти их когда ищет что-то мы просто ранжируем их группами и вакансий которые не прошли фильтры находятся в после вакансий которые прошли при этом они сортируются скором который назначил каждый фильтр в таком виде это пошло на продакшен и затем не секрет что она в таком виде работала мы можем провести никакие привычные итоге сработало ли такая модель на самом деле ответ неоднозначный да в таком подходе есть свои плюсы это что используя петров позволил выделить заветное усложнение моделей в начале у нас было всего лишь 200 фич сейчас как было видно предыдущем слайде уже в районе 750 и их число продолжает расти плюс исходная идея все еще валидная нам достаточно просто поддерживают инфраструктуру заботе столько в обновлении одного индекса но есть и минусы во первых это усложнение масштабирования сейчас поиск это довольно требовательна к ресурсам сервис потому что он одновременно делает и поиск и inference разделении на отдельные сервисов было позволило бы проще сделать грозит горизонтальном масштабировании наши архитектуры и еще один минус те оптимизации которые существуют в питоне в каких либо библиотеках и для ускорения inference а они практически отсутствуют в джаве и как раз об этих более низкого у них как оптимизации хочется говорить дальше дело в том что после релиза поиска после релиза первой модели креста fitch росло и росло и нам очень не хотелось пытаться заливать поиск железом поэтому нам пришлось сделать некие оптимизации того как мы вы уже вычисляем этот inference само собой изначально у нас был на этой сукой оставит занимает сколько вакансии прошло каждый фильтр в среднем но для понимания где узкое место больше всего полезен график по статьям infernis для каждой из фильтров и самой модели что было удивительно после первого исследования то что основе время занимал вовсе не 10 реализации векторов или расчет фич сам inference то есть обход дерева и мы стали установить какие существуют готовые решения для данной проблемы как ожидалось конкретно для джавы готовых решений практически не нашлось но в питоне есть целые библиотеки проприетарные open source не которые заточены именно на ускорение из решающих деревьев и к этому есть множество подходов например это бочче и вычисления на видеокарте это использование векторных инструкций процессора представление деревьев в виде которые более кэш френдли для процессора и на самом деле множество других нужно упомянуть как мы в этой реализовали в изначально нашем поиске этот inference наверное всем на собеседовании встречали задачу про обход графов ширину в глубину или поиск значения в дереве и скорее всего для такой задачи самой простой структуры данных было использование класса not со значением и ссылкой на левой и правой по дереву именно так мы и то и реализовали да а синтетические это оптимальное решение но в реальности работает это очень плохо и вот исследование существующих решений загнул нас на такую мысль ведь процессоры и jit компиляторы как раз и оптимизируется для того чтобы выполнять код на нужен намного быстрее так почему бы не дать им такую возможность и попробуйте очень простой вариант ускорения фото генерацию и тут нам повезло мы нашли готовый в которое в том числе позволяла генерировать java код из решающих деревьев and усилена но java начала вставлять палки в колеса и первая проблема это ограничение на размер прихода метода библиотек успешно справилась с этим разбиением на отдельные методы но для наших особенно больших моделей высоту из еще с одним ограничением это ограничение на пол констант 55000 на класс поэтому пришлось немного доработать и вы носите методы в отдельные классы в таком виде это дало сразу ускорение в 24 раза опять же в зависимости от моделей но хоть ryze и заметный он показался все еще недостаточно выяснилось что у кода генерации есть еще проблемы в джаве первое это то что большие методы в принципе не компилируется g там да это можно настроить но есть и связанная проблема из-за того что методов много вызов каждого это он не бесплатен и из-за большого их количество это занимает очень заметное время в поиске и что самое неприятное при тестируем выяснилось что для того чтобы подобрать оптимальные значения для производительности нашей модели из этих параметров они не линейно зависят друг от друга и этот значно сложно причем оптимально че не меняются от модели к модели для нас работал вариант когда мы просто отключаем все лимиты на компиляцию g там но к сожалению это далеко не универсальное решение и в вашем коде может не сработать в итоге мы получили ускорение примерно в десять раз и после выпуска это на продакшен мы увидим другую картину в том что у нас потребляет память в основном теперь это стало расчет в чьей и оказалось что расчет фичей это например на 8 процентов вычисления скалярного произведения векторов а перед дальнейшей оптимизации тогда понять как же мы можем вычислять собственно эти скалярное произведение векторов во первых это самый стандартный вариант из определения попарно и произведение компонентов этих векторов в коде простейший вариант а лизации будет примерно таким мы можем провернуть некий математический трюк и переписать исходное выражение в таком вот виде где за один сезон в сути константы которые можно прочитать для каждого вектор отдельно кажется что это важно не больше но здесь используется больше 2 дорогие операции умножения вместо четырех в исходном коде но за место них да мы используем больше операции сложения и умножения которые по идее должны быть более дешевы следующий вариант как мы уже написать это это с появлением функции масса эвэй в джаве 9 на самом деле код получать именно таким же просто мы заменяем умножение сложения на вызов одной функции и на самом деле вопрос чтобы быстрее написать умножения и сложения или же воспользоваться этой функцией и последний вариант который появился там недавно это вектор и 5 по сути и 5 для использование векторных инструкций процессора да он еще в инкубаторе но мы тоже хотели его протестить как возможный способ оптимизации решение скалярного произведения на нем будет выглядит она следующим образом мы бьем нашей виктора на патче длина которых равна размеру поддерживаю мы на нашем процессором векторных инструкций и делаем уже известны нам операцию ф.м. для каждого такого подчас храня результат в бать часам потом суммируем этот матч и да считываем хвост те значения которые не влезли в наш исходный вектор ко всему прочему хотелось еще проверить такой вариант поможет ли ручной он rolling этих операций в джаве масса фэй и вектор и пей и как узнать какой способ быстрее это джабба микро benchmark harness мы решили попробовать начали для случайно длины векторов когда мы сделаем мы тестируем наборы виктор викторов длины от единицы до соответствующего тестового примера например сам большой это 260 в нашем случае математическая хак некий математических как показал что на самом деле она дает ускорение но совершенно небольшой и только на очень больших векторах масса форме показал себя хуже чем любые другие варианты но его n roll вариант на удивление оказался таки быстрее и удивительно но победителем стал вектор и пять из инкубатора он почти на 30 процентов быстрее простого варианта но вот его он rolling вариант на самом деле практически бессмысленных судя по тестам и как еще можно увеличить скорость этой операции на самом деле использовалась довольно старый трюк сделать длину векторов кратной нашему нашему векторному инструкции например на нашем случае это было восемь тогда для больших викторов ускорение получается почти в два раза счет того что нам в принципе не нужно высчитывать этот хвост для вектор и 5 и в таком виде мы внедрили на продакшен какие выводы можно сделать из этих оптимизаций во-первых плюсы в том что опыт генерация достаточно простой метод который может заметно ускорить вам inference и то что вектор api уже вполне можно использовать в продакшене уже больше полугода мы успешно его используем и не столкнулись с не какими проблемами из минусов это уже упомянутой проблемы с jit компиляции и необходимость аккуратной ручной настройки и еще одна проблема из-за того что оптимизация залито викторов мы использовали не сразу а через какое то время до scientist и обучали модели с практически рандомными длинами векторов и мы до сих пор еще не перри обучили все модели на то чтобы получить все плюсы от данной оптимизации следующим большим испытанием нашего поиска стали и поведенческие признаки что такое хочется иметь для большинства документов в яндексе некий набор докторов и обновлять его одновременно и достаточно часто от 5 до 30 минут что это может быть например это city or a по сути популярность вакансии и мы хотим эти данные использовать прямо в векторе фич для нашей модели например при поиске для вакансии иди 2322 мы знаем что то значение будет 15 подставляем в фич вектор и отдаем модели либо это могут быть сложнее признаки например ls по сути кола бартерная фильтрация когда в вектор fitch попадает скалярное произведение двух векторов по вакансии иди и по user айди что нужно чтобы нормально на их использовать продакшене во первых нужно решить как их сервировать в питоне и как-то передавать в джаве возможно сохраняя локально для того чтобы не забывать их снова но мы решили первый вариант сделать как можно более простой руководствуясь принципом дешево и сердито это реализуем в порталов кладем в с-300 речь в нашем случае это менее и загружаем при старте никогда не сохраняя локально потому что мы всегда можем закрыть их снова сработаны это совсем нет дело в том что матрицы получаются очень большими не такими большими чтобы не поместится в массив но точно не поместятся в прорабов и даже в документации про добыв написано что он не подходит для нашего случая но мы настолько привыкли использовать готовый инструмент что даже не подумали об этой проблемой какие другие варианты мы рассмотрели мы начали с hd five это по сути комбайн он может практически все а если что-то он не может то почистился плагин который позволяет это сделать в минус его в том что в джарвис нормальная реализация это все-таки обертка над си библиотекой да сейчас вроде бы появилась чисто java реализация но непонятно сколько на готовы к использую продакшене из-за того что нам хотелось что-то простое и просто работающие то мы решили пойти и сделать свою простейшую реализацию нам повезло потому что у нас все машины были x86 архитектуре и что в питоне что в джаве числа с плавающей запятой представляется в одном формате поэтому единственное что нам нужно было это какой-то механизм быстрой превращение набора байтов в слот или интерьер и раньше скатова вас возникала подобная задача ясна вариантом был использование on сейф но его начинают deprecated и более того начинают удалять из новых версий джавы по частям к счастью к этому времени уже появилась альтернатива в our hands и оказалось что ей очень просто пользоваться да на некоторых задачах она показала себя чуть чуть медленнее чем он сейф но для нас так как мы делаем загрузку асинхронно это было не критично заработала ли в таком виде даже лучше чем мы рассчитывали это пример нашей абэ метрик для всего лишь одно 1 поведенческого признака наш порог значимости превышен несколько раз эти политические признаки очень эффективны что порождает новую проблему над садист хотят все больше и больше и через несколько месяцев после первого выделения из за того что представьте мы выкачиваем все данные поиск стал стартовать и больше десяти минут весь релиз занимал больше часа при этом нельзя было просто так стартануть поиск я синхронно загружать данные из своей эффективности по выдача пользователю менялась просто кардинально если он попадал на essence поиска без этих данных мы решили что мы можем все сделать просто и в лоб как мы привыкли просто на и начали писать синхронно на диск и тут как всегда мы не продумали все возможные проблемы и получили то что следовали ожидать наряду с очень сильным увеличением загрузки диска мы получили огромные стоп зе ворлд паузы когда у вас сервис ждет более 400 миллисекунд танеко нормально времени ответа и речь уже быть не может почему такое происходит дело в том что в стандартных гарбич коллекторах в джаве если какой-то тренд дело системный вызов и он оказывается долгим то все остальные трейды ждут его завершения на самом деле мы в компании очень следим за тем чтобы нас стоп завод был как можно меньше и как можно меньше было долго системных вызовов мы и пишем блоге через syslog пау dp и статистику gods and по которым мы с собственной строим эти графики мы пишем выдавшим по сути зама пленную на оперативную память файловую систему и точно не будут долгих системных вызовов и еще одна проблема который мы довольно долго пытались разобраться оказывается что и джалла временами при остановке трендов пишет в темпе некую внутреннюю информацию и если в этот момент диска доказывать сильно загружен то у вас возникает огромный чтоб заворот пауза и вырос параметр как раз отключает такое поведение но вернёмся к нашему случаю на самом деле самое забавное что там вовсе не нужно сохранять эти данные поведенческий на диск нам всего лишь нужно чтобы они переживали restart приложения или в нашем случае restart докера какие варианты как это можно сделать во первых это сохранит уже в упомянутый давшим на хвосте тут проблема в том что в этом случае игнорируется ограничений на память докера совершенно мы можем занять практически всю доступную память или упереться в ограничении уже бывшим на хвосте при этом других даже некоторые могут его использовать пострадают еще вариант был поступить как делает русин мэтт быть буфер к сожалению во первых у него есть ограничение 2 гигабайт на буфер и нам бы пришлось писать свою реализацию разделение больших файлов на эти танки по 2 гигабайта потому что тот и 5 который особенно лисин он не поддерживает рандомно и получение векторов по офз эту как нам было нужно всегда в джаве была доступна реализация через g&g lina и млын сейф и последних версиях нам доступно еще один конкретный вариант это форекса скайпе ай да он в инкубаторе но часть этого этой поддерживает memory maps файлы на самом деле когда мы почистили это в начале еще когда коренном лекция 5 только появилась on self реализация была быстрее примерно 30 процентов и мы решили использовать ее у неё оказалось огромное количество проблема в которых нужно думать во первых вам нельзя полагаться на garbage collection вы должны сами заботиться о западной памяти сборка был сборка мусора так как знает только happy может не вызваться в принципе в тот момент когда вы уже займете всю доступную память мы решили это использование счетчика ссылок на томик когда больше на данную памяти ссылок не у кого нет мы можем и из а спокойно освободить и это нужно делать очень аккуратно потому что при попытке чтения записи не в ту область и получи просто красивым это он сейф вас тут никто не ничего не защитит еще одна проблема это то что нужно аккуратно побить лимиты для докера эмп как и файловый кеш учитываются в лимитах на оперативную память если вы выглядит неправильно то ваше приложение докер просто убьет когда у вас при появится новый файл и который вы хотите завопить в оперативную память еще одно более g нервная проблема с которой мы столкнулись это надо это нужно ограничить и с и группами процесса на хвосте потому что если на холсте процесс будет активно работа с диском то опасности может решить что для него кэш важнее чем для вашего приложения и вы потеряете всю производительности как linux будет внутри поднимать файлы в память а потом снова освобождать ее еда linux все же будет сказать на диск down это будет отдельными делать отдельными тратами etro да это можно в каком-то виде настроить но все равно нужно быть готовым что у вас будет великое увеличение нагрузки на диск и последнее и наверное самое важное когда-нибудь и с таким подходом не получится но ведь джаву на самом деле к счастью к этому повторяется тест на мосты джаве картина изменилась совершенно и memory api уже быстрее чем он сейф поэтому мы планируем скоро переписать всю работу за map лимфома файлами именно на использованием амори api и кроме ухода танцев есть еще не с оптимизацией который надеюсь получится сделать в относительно скором будущем сейчас для того чтобы делать вычисления с нашими авторами приходится поднимать их из архип а в хип то есть копировать в массив байтов я надеюсь если а скорее даже когда люси начнет внутри использовать memory и теперь можно будет напрямую работать с данными в of happy и еще одно желание оптимизации кота у нас есть это поддержка флот 16 дело в том что для уменьшили микса electric тароф мы используем флот 16 и если вариант сейчас с ним работать это в онлайне конвертить их из 16 в java вский флот до аппаратная поддержка инструкции для работы слышишь нас появилась только в самом последнем расширение стандарта в их и еще нет процессоров которые могут с ней работать но виктор ресурсы для быстро конвертации сложность в fat32 уже достаточно долго поддерживается всеми процессорами поэтому если бы вы вектор ab и внедрили такую конвертацию то это позволено заметно ускорить операции с этакими векторами что хочется в плане итогов какие можно выводить сделать из нашей эпопеи с внедрением машинного обучения в поиск первое это то что привычный поляне инструменты далеко не всегда работают это и наша попытка использования обычного графа для представления модели и парта буфа для сериализации более кропотливый анализ и знание границ применимости нашей инструмент позволит нам сэкономить огромное кучу времени на переписывание кодов и наверное самое важное что уже сейчас в джаве доступно много возможностей для и использование ска уровнях оптимизации и хаков хорошем смысле этого слова о которых раньше java программист и могли только мечтать я очень надеюсь что это развитие продолжится и в будущем мы получим ещё больше возможностей выжать всё из доступного железа спасибо спасибо за доклад давайте перейдём к вопросам из зала кава появились интересные вопросы давайте микрофончик дам спасибо за доклад у меня такой вопрос поиск это в том числе поисковые запросы тут виктора в основном показывались резюме на вакансию как происходит решении следующей проблемы у нас есть поисковый запрос совершенно нерелевантный нашим резюме но мы взяли чужой аккаунт чтобы сделать запрос у нас вакансии java разработки а мы ищем уборщицы юном рекомендуется вакансии java разработки как учитывается контекст поискового запроса еще в этой модели на самом деле поисковый запрос перед попаданием в поезд отправляется в отдельный сервис на питоне который преобразует его в отдельный набор векторов и уже эти добавить или набор векторов передаются в а теперь модель которая обучена именно на запросы с те каким-либо текстом или какими-то особыми параметрами тот пример приводил это в основном до более менее зачем рекомендаций когда вы просто нажимаете допустим фронтире кнопку найти ничего не заполняя никакие параметры не указывая надеюсь ответил следующего просто приветствую такой вопрос использовали были какие-нибудь методы снижения размерности вектора это понимаю довольно большие да спасибо на самом деле конечно сложный вопрос мы пробовали использование размерности но это не дало ощутимого результата плюс например для л.с. они эти матрицы и так получаются методом неким медным похоже на снижение размерности поэтому там мы ее стяжать значно бессмысленно мы пошли по более такому пути в лоб мы просто используем флот 16 там где нам можно снизить размерность да сейчас как я говорила это немного бьет по признательности но мы очень аккуратно этим пользуемся ну и надеюсь когда-нибудь мы сможем это использовать без уменьшения производительности"
}