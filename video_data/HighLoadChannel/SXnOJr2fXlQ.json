{
  "video_id": "SXnOJr2fXlQ",
  "channel": "HighLoadChannel",
  "title": "Репликация в Tarantool: конфигурация и использование / Георгий Кириченко (Mail.ru)",
  "views": 404,
  "duration": 3064,
  "published": "2019-05-14T14:50:26-07:00",
  "text": "всем привет меня зовут георгий кириченко я занимался абсолютно разнообразными вещами связанными в данный момент времени я член тарантул картин в этой команде я занимаюсь частности всякими внутренними вещами в tarantul и касаясь пешка баз данных внутренними коммуникациями и в том числе репликации собственно говоря репликация сегодня вам хочу рассказать о том как она устроена в нашем tarantul итак что такое репликация репликации то процесс создания копий данных и заново хранилище в другом хранилище соответствует а копия данных и называется репликой для чего это может использоваться но абсолютно очевидно и применение вы допустим хотите иметь резервной копии ход стенд бай вы хотите иметь горизонтальное масштабирование разносить вычисления по всему кластеру и для этого вам необходимо иметь возможность использовать одни и те же данные на разных узлах вычислительной сети кластер и а какая бывает репликация репликации классифицировать по остальным двум признакам первое это направление мастер мастер репликации либо мастер slave репликация masters лев replicate это самый простой вариант у вас есть один узел на котором вы производите изменения данных и эти звене данных вы транслируете на остальные узлы где они применяются мастер мастер репликация чуть более сложный вариант когда изменения вы вносите ни на одном узле сразу на нескольких в этом случае у вас каждый узел как а активирует некие изменения к своим данным так и применяет к себе изменение выполнена других узлах и репликацию по цвету и ровать по режим ее работы это а синхронная репликация синхронная репликация синхронная репликация подразумевает что данные не будут зафиксированы и пользователей не будет отвечено ok до тех пор пока изменения не будут распространены хотя бы по какому-то минимальному количеству членов кластера асинхронной репликации подразумевает то что процесс фиксации транзакций и и комменты взаимодействие с пользователями процесс репликации это два абсолютно независимых процесса то есть вы данные за комете они попали в какой-то журнал потом it is меня каким-то образом переползли на другие члены очевидной это идет к тому что а синхронная репликация имеет ряд побочных эффектов и так как же устроены репликация в тарантул в tarantul и репликация строится из бог никаких таких базовых кирпичиков этими кирпичиками вы можете построить кластер любой топологии какой у вас интересуют и так это базовый элемент конфигурации является до направлено то есть всегда вас есть master и slave мастер выполнять некоторые действия формирует лоб операции и этот лог операций выполняется на реплики второй аспект это репликация синхронны то есть тарантул говорить вам накопить окей независимо от того сколько реплик этот дельту видели сколько реплика люк себе применили вообще получился это сделать третий момент репликации в тарантул rock bass таран ту валюту внутри себя репутационный журнал в принципе журнал операций называемый вал операция попадает туда построчно то есть если вы изменяете какую-то строчку спейси то эта операция записывается как один элементы как строка журнал после этого фоновый процесс вычитают эту строку и отправляет ее реплики сколько у мастера реплик столько на фонах процессов то есть даже репликации происходит на разные члены кластера асинхронно друг от друга это независимые процессов а ну и третий элемент каждый член plaster а во первых как в обязательном порядке иметь свой уникальный идентификатор его собственность это тот дети секатор который генерирует рисуем создание и и идентификатор в кластер либо номер этот-то численно константа который присваивается реплики в момент подключения в кластере она остается вместе с репликой во все время существования данной реплики в кластере и также что мы можем сказать о репликации в тарантул во первых то что все данные на reply как в силу асинхронности репликации сюда попадает запаздыванием он очевидно вы сделали какое-то изменение сказали коми система он сказал ok на мастере уже операция применилась вы видите ее эффект на replica h не применится с некоторым запаздыванием которое определяется скоростью с которой фоновый процесс репликации то операцию вычитывать отправляю на реплику и реплика себе применять и из этого также следует возможность синхронизации данных представим себе что у нас есть несколько мастеров эти мастера изменяют взаимосвязаны между собой данные может так получится то что операции которые используют не коммутативной относится к одним и тем же строчкам тогда у вас получится так что две разных инстанций кластера будут иметь разные версию данных данные разъедутся вас возник некий конфликт и этот вопрос нам чуть позже рассмотрим так на сказали то что репликации в таран пылью однонаправленная мастер slave каким же образом нам сделать мастер мастер а если мы имеем если мы хотим это и сделать очень просто мы возьмем и подключим slave к мастеру другому допрос назначим обе роли каждому члену кластера при этом надо понимать что мастер мастер репликации это не какой нибудь там особый тип это на самом деле всего лишь комбинация двух независимых друг от друга потоков данных все очень просто а то есть мы сами сочиняем какие-то особые режимы мы просто используем то что мы есть просто комбинируем при этом нужно понимать что эти операции оттуда не синхронны они не взаимосвязаны вы скажем ты-то репликацией может работать быстро это медленно ну скажем у нас ведь так настроена а теперь мы можем использовать тот же самый принцип отучить к нашему кластеру 3 мастера точно также мы строим фалмер где каждая реплика является мастером для каждого двух других сливов да и при этом он сам по себе является словом этих этих же самых мастеров тут надо сказать вот что то что реплицируется не только те операции которые выполнены локально на данном мастер до который там были ниц и раваны у также те операции которые он получил извне по репутационным протоколом то есть в данном случае скажем так на реплику номер три изменение созданные на реплики номер один придут дважды по этому каналу а также по каналу через вторую реплику то есть сначала не придут сюда попадут в его журнал после этого не бутан транслированы сюда дважды это свойство позволяет нам строить более сложной топологией не используя full меньше скажем вот такую на секунду здесь мы каждому мастеру до который троём составляет full место составлять рядок ведро кластер и прицепили по индивидуальной реплики исходя из того что у нас происходит проксирование дельт на каждом из мастеров все три вот этих вот слева чистом виде будут иметь у себя все операции которые были выполнены любому членов кластеров на самом деле очень удобно да и такую конфигурацию можно применять для в суд на разных вещей то есть мы не создаем паразитные связи между всеми членами кластера до при этом мы имеем точную копию мастера с каким-то минимальным запасом немыслимых разместили скажем рядом и мы имеем возможность скажем испустив как вот стенд бай скажем место этот мастер вышел из строя да мы просто подменим айтишники и наш кластер продолжит работать в режиме с испытывают 3 мастеров очень удобно и просто да и это опять-таки все делается просто используя базовые элементы репликации ты-то тут маленький ли ментик мастер slave который у нас есть теперь возникает вопрос если у нас операции проектируется между всеми мастерами до операции приходит каждый реплику по несколько раз да как нам понять как к операции нужно выполнять какую нет нам необходим некий механизм фильтрации для этого каждой операции вычитанные из блога приписываться два атрибута 1 атрибут это идентификатор сервера на котором данная операция была впервые инициировано то есть этот не тот кто является нашим пиром а где-то на операцию вызвал пользователя и второе это порядковый номер это операции на том сервере который является инициатором то есть смотрите каждый сервер когда выполнять операцию каждый полученной строке сгенерированы представит увеличу sequins 1 2 3 4 5 6 7 8 9 10 да и таким образом если мы знаем то что для серверов таким идентификатором мы применили операцию слс этом скажем 10 то операции которые пришли нам по репликации с его сыном 987 10 нам применять не надо надо принять следующую 1112 и так далее очень легко и просто и отсюда возникает у нас возникает другой вопрос как тарантулу себя хранить состояние тех операций которые он уже применил для этого существуют векторные часы backlog что представляет собой в клуб вы clock это вектор с последних максимальных lsn примененных относительно каждого члена кластер а то есть если мы находимся на некой реплики toefl sn1 это операция применённая с мастером под номером цветы диффе котором 1 2 3 и так далее также можно вы клок обозвать как некий известный данные реплики снимок состоянии кластер а в целом да то есть данная реплика зная то что три члена и и кластер иметь такие лысины дошли до такого состояния и используя данный вы клок да и знаю операцию сервер айди пришедшие операцию lsn мы банально вычленяем нужны нам компоненту локального вы плотно сравним полученная из транса в основном операции принимаем решение принимать применяемый данную операцию или нет такая схема позволяет нам обеспечить так то свойство что операции при нецивилизованные инициированные каким-то конкретным мастером будут применяться строго последовательно есть мастер их шлет последовательно мы применяем в последовательно но при этом операции про инициированы потоки операции созданный на разных мастерах могут собой перемешиваться ну силу того чтобы привлекать у нас асинхронное пошли дальше после того как мы немножечко популярности теории давайте посмотрим то как мы с вами создаём кластер да то есть я говорил что вот у нас есть кластер но на самом деле сначала нужно создать предположим у нас есть платья встречи с двух элементов мастер мастер слова в двух и мы хотим прицепить к нему 3 instance у него есть уникальный идентификатор юид но при этом идентификатора у него еще нет если некий тарантул ученицы лизе раваны желают присоединиться кластеру он обязан послать специальный за операцию joy на один из мастеров который может операцию выполнить ну то есть находится в risd врать режиме в ответ на join мастер отправляет подключающийся реплики snapshot свой локальный снимок реплика на канву исполняет о себе этот snapshot при этом идентификатору не все еще нет она все еще не член кластер после того как реплика wanna катет к себе snapshot она в принципе находится в неком состоянии какого-то синхронности но с небольшим отставанием от в принципе всего кластер а и после этого мастер на котором происходили join назначает данные реплики идентификатор после назначения дентифик а то регистрация в классе реплика становится его палач полноценным членом и после этого она они цитируют с мастеров которые известны потоки репликации свою сторону атаки репликации это в принципе это журнал то есть это журнал тех операций которые произошли на данных мастерах которые им известны начиная с не то состояние этой реплики которой она получила в процессе join а если данный replicate каким-то причинам отвалилась выключилась и либо порвались канал связи до то при подключении к кластеру она джон уже не выполняет потому что у нее есть некий локальный снимок да она просто запрашивать все операции которые произошли с ее последнего состояние в этом кластере а как происходит регистрация реплики в кластере вот это для того чтобы хранить в себе состояние структуре кластер я специально space называется он кластер то неудивительно и по факту на хранит в себе отображение идентификатора сервера в кластере до его как и какой-то порядковый номер и его уникальный идентификатор ну вот это реальная слепок с примера до который я принес который я готовил к своему выступлению да вот есть по из кластер мы видим в нем есть идентификатор и 123 и виды он здесь нужно сказать то что не фиксаторы в общем-то не обязана идти по порядку да то есть может так получиться что такие члены выбывают кластер какие-то добавляются так далее и здесь расположена 1 мин а который несет в себе тарантул до для нас в наших пользователей как правило по одному кластер не собирают да вы запускаете некое приложение дано разворочу сразу целый кластер и у вас квартир собирать сразу из многих членов но при этом мы должны знать то что само по себе репликация в tarantul и асинхронная до представим себе что если два мастер одновременно подключает новый новых членов да и назначать нам одинаковы идентификаторы чему это приведет к тому что у нас будет конфликт да то есть скажем на одном мастере юит будет такой строчкой дано другому быть другим вот скажем пример неправильно joy надо мы видим нас есть 2 мастера и 2 реплика которые желательно подключиться они делают joy на разных мастерах предположим они получают одинаковые до индификатор и это придет к тому что репликации между этими двумя мастерами и теми кто успеет с них про реплицировать их дельта развалится разделение кластер а на части и в чистом виде и мы ничего с этим сделать не можем чтобы это побороть мы должны с вами использовать механизм который позволяет нам инициировать реплики инициализировать строго на одном мастер и в один момент времени до в этом случае наша проблема с конфликтом при джоне будет решена потому что мастер как бы один он у себя все эти операции могут пункт консистентной не пересекаясь для того чтобы это реализовать в тарантас такое понятие как лидер инициализации и есть алгоритм выбора этого лидера происходит он очень просто реплика желающие подключиться кластеру сначала подключается просто понравится соединение со всеми известными ей мастерами из конфига переданного после этого она выбирает те из них которые инициированы мы помним с вами то что скажем и кластер поднимаем и не все еще члены его успели полноценно заработать до поэтому должны был только те которые сейчас работают в рабочем состоянии после этого мы выбираем из них те которые доступны на запись тарантул бывает 3 2 1 и будет readonly регистрироваться на редон ли ноги мы не можем потому что она нас не может зарегистрировать она не может инициировать локальной операции она не может взять и занести в кластер и после этого из того списка отфильтрованных реплик членов пластика который мы получили мы выбирали тот из них который иметь наименьшую it что нам дает данный алгоритм данный алгоритм дает то что если мы на подключающихся кластеру инстансах нецивилизованных используем один одну и ту же конфигурацию один и тот же список серверов то они уберут 1 1 того же мастера и джона спою успешно отсюда мы делаем первый рецепт то придется лизации при подключении реплик кластеру если мы хотим это делать каким-то образом параллельно мы должны обеспечить чтобы у всех этих реплик были одинаковые конфигурационный параметр по репликации если макета что-то за опустим то есть доля вероятности то что у тех отличающихся инстансов отличающимся конфигурации будут инициировать на разных мастерах и мы с вами не сможем стартовать банально но все же предположим что это случилось мы ошиблись у нас прошел артефакт админ забыл поправить конфиг у нас сломался какой-нибудь ansi был да и что-то в таком духе произошло и ситуация с разделением кластером части произошла что может говорить нам об этом во первых то что и подключаем ее реплики не смогут создать свой сад снапшоты мы это увидим то что реплика не стартовала она нам сказал точно случилась ошибка второе на самих мастерах влогах мы уже увидим конкретные ошибки связанные с тем что возникли конфликты спаси кластер это как раз говорит о том что произошел джон подошел неверно неправильно теперь иди к это вопрос как там так легко эту ситуацию разрешить на самом деле проблем у нас нет есть стандартный алгоритм первым делом мы должны разумеется право лидировать ту конфигурацию которому надо задавали подключаемся репликам потому что если мы это не починим да то что бы мы ни делали мы джон с вами не сделаем после того как мы убедились и поправили конфигурацию мы приступаем ко второму этапу очевидно спасти катионов уже есть конфликты и нам их нужно зачистить репликаций устроено таким образом space а кластер что операции дэвид пройдут успешно в отличие от операции регистрации которые сделаны через операции которые могут конфликтовать при пересекаясь с ключах мы очищаем space кластер и после этого мы в принципе вполне можно попытаться детализирует нашей реплики заново и практика показывает то что с очень большой долей вероятностью но наша проблема исчезнет вас plaster заведется стартует и будет прекрасно себе работать без каких-либо побочных эффектов давайте дальше теперь смотрите мы с вами создали наш кластер подключили его да и все члены кластера работают в режиме подписка когда они получают изменения получить генерирование разными мастерами как бы режим следования за мастером исходя из того то что наша репликация синхронно у нас возможны конфликты то есть когда вы одновременно изменяете какие-то данные на разных мастерах да и разные реплики получают разные копии данных то есть силу того что операции применяются в разном порядке давайте посмотрим вот пример того как кластер выглядит после того как мы выполнили join а то есть у нас есть два три masters вывода между ними передаются логе блоге проектируются в разном направлении и применяются на словах к чему что мы что поддразнивать в потрясных хронизации данных да то есть смотрите на каждой реплике будет своя собственно история выхлоп просто в силу того что репликация синхронной то что потоки от разных мастеров могут между собой перемешиваться по-разному да а если истории изменения в клок локально в состоянии разная дата это означает то что и порядок применения операции который был выполнен на каждом конкретном и снасти может отличаться да а если наши операции коммутативной как например пернул операции скажем replace the и данные которые мы получим в результате этих репликах будут отличаться вот небольшой пример предположим у нас есть 2 мастера да у них вы клок 00 предположим и оба мастера будут у нас упадёт по две операции маркерная об 11 12 от 21 22 соответственно а вот здесь показана 2 квант времени когда каждый из мастеров понял по одной локальной операции зелененьким циферкой показано изменение в ответ очень компоненты вы клок первый мастер меняет свой вы клок 2 мастер менять свой вы клок после этого 2 массе выполняет еще одну локальную операцию до увеличить свой вы клок а первый мастер получает по репликации операцию со второго мастера это отображается красный циферка 1 вы клоки первого члена кластер а дальше ситуация развивается до 2 мастер получать операцию с первого а первый соответственно 2 операцию со второго верно сработать только репликации мы в конце первой мастеру вполне свою последнюю операцию 2 мастера и получает посмотрите вы клоки на 1 нулевом quanti временно с одинаковой 00 но последнего quanti времени мы тоже мем одинаковые вы клоки 22 до казалось бы то что данные должны быть одинаковые нос мы посмотрим на порядок операции который выполнил каждый из мастеров мне что то что они разные до представим что то операции реплей с разными числами и тогда получится что ими одинаковые вы клоки мы имеем абсолютно разные у отличающиеся версии данных до на обоих репликах на самом деле это иногда это нормально иногда это нет и такую ситуацию мы тоже умеем разрешать каким образом во первых мы можем с вами выполнять операции записи да не на всех случаев случайно образом выбранных реплика каким-то образом сортировать то есть связанные данные модифицировать на какой-то конкретно мастере да но это то что владислав рассказал в предыдущем каким-то образом затрагивать то что ладислав рассказывал просто сами разнесли операции записи до по разным мастерам и таким образом и своим получим и вешал consist in систему то есть предположим изменений или и ключ от 1 до 10 на 1 мастере 11 до 20 на другого мастера да они обменяются своими дельтами и в итоге получит от суд одинаково картину мира новейшая косицын системы и 2 есть способ предположим что по условиям задачи которые мы решаем и сами можем каким-либо образом определить какая приоритет операций изменения ну скажи мы можем timestream поста вида либо версию либо еще какая то есть бизнес логика которая позволит нам понять да какая операция физически произошла раньше ну скажем в общем он сказал что мы имеем некий внешний источник упорядочили порядка давайте посмотрим чуть поподробнее наши пути вот первый путь что он подразумевает то что у нас есть некий роутер да это вовсе не обязательно должна быть отдельная сущность это может быть частью приложения это может быть частью мастер это может быть толстого или даже шар шар шар который мы используем до который применяет к себе операции на запись до тем или иным образом их forwarded на мастера not for kid их таким образом чтобы связаные значения из менее посредством значением уходили на некий один мастер до 1 блок значение шел одному мастер другой богачев у другого мастера при этом операции чтения мы с вами можем направлять на любой член кластера да но при этом нужно учитывать то что репликация синхрона да если скажем врать нас прошел вас в одного мастера do to ride если мы хотим тут же прочитать желательно делать тоже с него да это надо учитывать своих приложениях ты скажи мы сделаем врать здесь от читаем здесь может противника и запаздывание даже если это один и тот же клиента даже если тоже вам он получил окей на свои процесс запроса к мид и второй подход tarantul с недавних пор есть такая вещь как бафа replace триггер до который может применяться вычисляться при операциях репликации в данном случае мы сами не ограничены тем что мы должны мотивировать запросу мы сами шьем запрос туда же куда мы хотим как у нас получилось либо да пусть вас есть некая там другая логика при этом при выполнении репликации да на входе потока данных у нас есть некий триггер это триггер в общем случае достаточно простой он читает ту строчку которую вы ему прислали сравнить с той строкой который уже сейчас хранится да и просто принимает решение какое какая этих строк имеет приоритет и он либо игнорирует запрос по репликации либо применяет его данный подход на уже кое-где применяем да она у него тоже есть свои недостатки во первых вам нужен внешний источник синхронизации да то есть ваша задача должна его предусматривать предусматривать ну скажем ваш оператор вносит изменения по какому-то абоненту да в принципе для операций по данному абоненту вы мы можем использовать время на компьютере оператора да потому что вряд ли у нас несколько операторов по одному и тому же абоненту достоинству связи вносит изменения ну так не бывает фоток спас нас там сейчас человеком да по нему вносят дельты да какие то эти операции пошли разными путями но при этом месте мы каждой операции можно приделать некий версию то данные операции при прохождении через триггер и оставить только те которые являются их актуальными ну и надо заметить то что так как этот триггер этот игры выполняется на каждый дельту пришедшую публикации на каждый запрос да это повышает нашу xp это циpкa expense of может оказаться это лишнего чувствительная нагрузка до но при этом оно позволяет нам иметь консистентную копию данных масштабах кластер и и третий аспект смотрите наша репликация синхронно что подразумевает то что по выполнению коми то вы не знаете если ваши изменения на каком-то другом тени кластер а если вы на мастере позвали коми там сказали у киева мастер по каким-то причинам тут же рассыпался перестал работать вы не можете быть уверены что что ваши да найдет еще сохранились для того чтобы решить эту проблему в протоколе репликации тарантула из такой путь ок ок ок и каждый мастер у себя хранит свои знания о том какой последний акт пришел счастлива что такой а когда свет получает к себе некую дельту да эта операция по мне то есть это дельта помечено его сыном мастера и его идентификатором то в ответ он счет специально к пакет которым запаковывать свой локальный вы клок после применения о давайте посмотрим как это может работать вот смотрите у нас есть мастер который выполнил себя четыре операции так это его сведения об этом слове и вы клок слева предположим что в какой-то квант времени слив получил первые три строки до его вы как увеличился в 30 так вот это вот у нас аппетита компонента соответствующая первому члену кластера да это его собственно компоненты от мастер вот а как пока не пришел получив данные три строчки slave отправляет ок пакет в этом пакете он зашивать свой вы клок который был на момент данного пакета и пусть тот же квант времени у нас slave мастер отправил еще одну строчку дату свой клок у слова увеличился исходя из этой картины мастер номер один может точно знать то что все первые три операции которые он произвел уже применились на этом слове у данные ок состояния хранится для всех своего с которым работают мастера нём что-то независимо мы в конце склеив отвечает четвертым а к пакетам после этого мастер знает то что слов ним sync ранен них одинаково состояние и данный механизм мы можем использовать прикладном коде да когда вы сделали эту операцию коми да вы не сразу же возвращаете пользователю ok на его операцию какой-то осмысленный ответ сначала вызываете специальную функцию который выполняет ожидании того момента когда lsn слайго известный мастеру сравняется с л с сыном вашего мастера на момент завершения коммита то есть смотрите вы и сказали commit когда вас был с ума хироус он был 3 до вам не нужно ждать полна синхронизация но точно дождаться вот этого момента предположим что у нас первый вызов изменил три строчки 2 звонил одну строчку давай после первого вызова хотите убедиться то что ваши данные при синхронизировались вот это состояние уже означает то что ваш первый удар при синхронизации как минимум один слайд как конкретно это посмотреть этой скотины в последней части разрешительной части презентации посмотрим а теперь передем к таким моментам как мониторинг ну то есть репликация синхронная репликация синхронно да очень просто мониторить вас выпекаться развалилась дай вы в ответ на все свои операции получаете ошибки вам мастер говорит то что мне не удалось про синхронизироваться да и очень быстро к вам прибегают люди прибегают систему мониторе я говорю то что алярм у нас беда нас ничего не работает да если репликация синхронно то ситуация немножко более запутанного мастер отличается все хорошо все нормально я работаю принял записал да но при этом ни одна реплика vossen неживая и ваши данные иметь не имеет избыточности при потере мастеру поступь теряйте свой дан поэтому очень хочется мониторить до понимать что же все-таки происходит синхронно репликации в каком состоянии наш кластер да где находятся наши реплики синхрон они с нами отстают не отстают справляются с нагрузкой для того чтобы освещать базовый мониторинг из такая сущность таран только бокс info если в консоли просто наберете бог-сын фонда увидеть очень интересно такую табличку когда садились себе различные показатели да но самое важное это идентификатор до данном случае это единичка означает то что lsn данного мастера да будут храниться на первую позицию всех вы клоков который здесь есть до его уникальный идентификатор очень полеты на самом деле вещь да то есть если скажем вас произошел конфликт при джоне то отличить один мастера другого до которые конфликтовали вы может только по уникальным идентификатором потому что подле вас уже произошел конфликт также к локальному состоянию относятся такие величины как лрн до этот номер последней строки до который данный мастер записал к себе в журнал выполнил в данном случае это означает то что наш первый масса стоим с вами выполнить 5 операций и в клок это известное ему состояние тех операций которые к себе применил то есть это означает то что он выполнил одну операцию свою собственно своих собственных да и для мастера номер два он выполнил одно его операцию по репликации после локального состояния да мы и сами можем посмотреть что мать 100 данный инсцет о состоянии кластер его репликации вот это вас утратил и репликейт этот раздел содержит себе известные все известные данному instance'у другие члены кластеров том числе его самого то есть смотрите первый член у нас имеет идентификатор а1 и совпадающие виды это он сам свой lsn очевидно что он себя ничего не реплицирует и себе ничего не реплицирует поэтому других здесь элементов нет а вот этого наш второй член кластера идентификатор 2 его ю т а л с н да мы можем сами посмотреть то что его lsn единичка соответствует тому одна сцена который записан в и клоки здесь мы с вами рассматриваем мастер мастер репликацию да когда у нас мастер номер один одновременно является мастером для второго члена кластера и его словам то есть следует за ним давайте посмотрим за состояние собственно состояния как слово описывает сущность up stream она имеет такие трибут и как статус статус follow означает то что мастер один следует за мастером 2 до idol это то время которое произошло локально с момента последнего взаимодействия с этим мастерам ну то есть как бы мы не ждем поток непрерывно да то есть когда она мастер какие-то изменения отправлять свой дельту когда мы отправляем какой тогда мы тоже о своем взаимодействует этот вот отдел показывает то время время с момента последнего взаимодействовать на что если idol становится большим дан скажем 10 секунд 20 секунд час до что она что-то не так то что мы в течение часа не получали от мастер никаких сообщений так не бывает и 3 лога что он означает вы смотрите мы с вами говоря запаздывание кроме lst на и сервер айди каждая операция влоги маркируются еще там с темпом то есть локальным временем под кат во время которого данная операция была записана в лоб на мастер который выполнил sleeve при этом сравнивает свой локальный таймс темп до который в на нём сейчас существует ставим с темпом того той дельта который он получил до их самый последний полученных для последней строчкой он выводит здесь здесь мы видим то что между тем как операция была применена на это мастере тем как и выполнил второй сервер у себя произошло 36 микросекунд ну на самом деле это немного датой для большинства задач это более чем достаточно в принципе если ваш клиент несколько отдален от кластера то скорее всего вот этот лак он даже не заметит но просто потому что время взаимодействия между вторым мастером и его и клиентов и в обратном клиента на первый мастер будет превышать и тариф чистенько секунд нас банально даже больше чем пин и теперь атрибут downstream downstream показывает то что мастер знает о своем конкретном слове а именно тот самый ок который стойкам отправляет данный downstream означает то что последний раз slave его slave он же мастер под номером 2 отправил ему свой в клок который был равен 5 1 и данный мастер знает то что значит все пять его строчек который выполнил у себя уехали сюда и таким образом используя то структуры вы можете в своем коде да просто написать маленький watcher до который мониторит downstream до сравнивать lsn который там записан стимула ценам который был на момент постановки the watcher и да и когда он достигает или превышает целевое значение вы просто выйти то что все мы синхронный все хорошо да то есть можно как это говорит что данного разъехались давайте посмотрим дальше да когда она все хорошо все понятно теперь от у нас мастер упал 2 мастер у нас сожаление каким-то причинам отвалился до первым делом нас меняется статус не то что он disconnected да лакуна с ней меняется просто потому что как бы строчка который мы применили на так и осталось новых точек мы не получали до но при этом нас растет idol в данном случае наш отдел райан 1600 2 секунды только время наш мастер был мертв и так как минимум нам хорошо но и видим некое сообщение об ошибке что произошло до нас с вами рассыпал сетевой connect что делать если нас такая ситуация произошла на самом деле очень просто мы с вами просто разбирается что случилось нашим мастерам привлекаем какого-то администратор или ответственного работника до перезапуска ему поднимаем и в результате репликация ретро это да каждый какое-то время тайм-аут когда мастер входит в строй мы к нему подключаемся подписываемся на его экологии получаю на к себе и наш мастер сухари стабилизируется но при этом есть одна маленькая проблема представим себе у нас был слов которые путин то причинам выключился кончилось питания сгорел диска или так далее тому подобное не было достаточно долгое время из это долгое время мастер который его обслуживал ударил себя экологии например у нас диск заполнился сборщик мусора подобрал каким-образом данный slave может продолжить работу в принципе никаким потому что той дельта которую ему нужно применить топтать стать синхронным с кластером уже нет этих фактах данных нет да мы не можем просто откуда ты на них нет куда взять и в этом случае мы увидим вот такую то интересно ошибку то есть у нас статус уже не disconnected от стоп до и вот специфическое сообщение то что нету уже лог-файла соответствущих так интересен но на самом деле данная ситуация она в те же не фатально предположим что у нас более чем два мастера да и на каком-то из мастеров эти логе еще сохранились на потому что как бы мы опера тяжело ги разливаем seamaster он сразу они хранимых такое на одном пути каком то тогда получится то что данная реплика подключается ко всем известным и мастером на каком-то из них найдет не хватает и логов да она в пункт себе у себя все эти операции и и вы клок увеличится до и она догонит да да гонится до актуальном состоянии кластера после этого она можно попытаться переподключиться чуть более сложная ситуация когда logo в принципе вообще у нас с вами не осталось до и продолжит работу реплики мы не можем просто подошла с нет данных нам их не откуда взять мы их не можем из памяти там сочинить правильно что для этого нужно сделать это мы можем туре прикупили инициализировать чтобы что мы сделаем мы запомним you уникальный идентификатор но можно записать его на бумажку в блокнот куда-нибудь куда хотите потом и данную реплику зачищаем локально доля и масштаб шоу ты логии так далее и после этого данную реплику мы присоединяем за ну с тем же самым людям каким который у нее был вот посмотрите да вот эта строчка до показывает как нам перенеси лидировать для чего это нужно смотрите мы с вами обсуждали то что у нас есть и фигур space кластер до которой мы храним идентификаторы и еды проблема заключается в том что пространство идентификаторов она ограничена и мы не можем с вами при потере реплики просто так как выбрасывать разбазаривать поэтому и взяли будет если мы его перри используем то есть если мы с вами используем старый you need to master на котором мы делаем join его обнаружит уже как зарегистрированным присвоить эти реплики тот же самый youth отправитесь на вход и вскоре начнется заново реплика будет работать если же предположим что ей до у нас нету да ну скажем на сгорел диск на реплики все полностью не этого нам неоткуда и восстановить там и начать мы должны зайти в space кластер найти в нем ту запись которой соответствует потерянные реплики ну просто потому что скажем она долго частью не отвечать 2 не отвечаю нигде нет все мы нашли бывает в . удалили и и после этого мы эту реплику цепляем как абсолютно новую и все на становится хорошо оно регистрируется получается насчет делает подписку на дельты все они работают хорошо да как бы наш просто становится консистентными целостно давайте смотреть дальше еще третий аспект предположим так получилось то что какая-то из наших реплик не работал какое-то время до скажем она осталась его кластера на сутки мы не хотим позволять данные реплики работать но при этом оно тут на работе она может продолжать работу до мыса нато не хотим терять заново детализировать но при этом мы не хотим дать пользователям возможность читать очень сильно устаревшие данные для этого 2 300 кредит как кворум ну вот к примеру смотрите данная схема указанная в конфигурации при старте реплики означает то что данная реплика не вернет вам управление из конфигурация до тех пор как она не догонит как минимум два мастера на время не более чем 0 1 секунды до ехать и тени 30 секунд это реплика не сможет догнать вам ошибку той смотрите вы выдаете эти параметры запускаете реплику она начинает быстренько быстренько догоняться загоняется когда ты то состояние что это выполняться до они от sony 0 1 секунду это что это как усилку можно шин в синхронно да вы можно сдавать другие значения и становится рабочий да вы можете читать писать и так далее в зависимости от того как будут настроены как настроено ваше приложение на самом деле очень удобно и последний момент я говорил об ой для но начиная тоже с недавних версий тарантулов репликации есть keep-alive пакеты ну скажем бывает такая ситуация когда неаккуратный админ поставил мой пятибук дроп да и вы видите то что данный connect развалился где-то через 30 30 минут до или 30 секунд 80 того какой то и мало цвет вашей системе но это ситуации она как бы не очень хорошая да и мы хотим не побороть для этого есть keep-alive пакеты в полость пакеты из тайм-аут если скажем реплика не видит от мастер никаких изменений в течение тайм-аута она отослать ему keep alive пакет говоря то что я желаю если в течении 4 times out of master и slave не видит keep-alive пакетов друг от друга то они разрывают конёк пытается стать его заново да и это тоже позволяет нам мониторить состояние канала между репликам реплика и мастера оказался очень у на самом деле удобная вещь да то есть не для того чтобы какие-то проблемы а просто для мониторинга если у вас придется связь в это сразу видите вы можете допустим завести какую-то систему мониторинга повесить туда alert который админу пришлет сообщение то что у нас не работает сеть админ прибежит починить у нас удача заработает и еще один аспект да мы говорили то что все изменения у нас идут по кругу до предположим у нас 6 реплик то каждая реплика получит все дельты пять раз много если у нас 10 реплик тона плачут девять раз еще хуже это считаться тоже можно решить когда мы уже имеем про нецензурной клацать мы можем немножечко поменять конфигурацию у топологию кластера так чтобы избыточность уменьшить вот есть такие требования да то что мы применяем это уже кто как про нецензированная миром потому что не инициализирован и мы должны запускать с общей конфигурации с полный также мы должны быть уверены то что любой реплике есть маршрут от любого мастера который производит изменения да и каким-то образом чисто логически обеспечить баланс между избыточностью каналов да и надежности вот пример допустим топологии на здесь есть 6 реплик да но при этом из быть равна всего три мы сами можем потерять любые две реплики это наш кластер останется целым да и вместо того чтобы передавать каждый дельты пять раз мы будем передавать только 33 раза почти в два раза это улучшение от стандартной схемы на самом деле достаточно эффективно оказывается но в принципе как итог тому можем сказать если у вас есть проблемы с реплика то что нужно сделать перебью нужно проявить конфиг вторым делом как места нужно сделать бэкап потому что когда вы начинаете чистить свои склады что-то очень интересно делать разные операции пытаться разобраться до вы рискуете потерять свой дан очень легко и быстро вы можете просто попортить до ударить нечайно какой-то лишний лак да и все ваш к серверу не старта не стартует вы потеряете данные после этого можно попытаться разобраться самому если не получилось пишите к нам чать я думаю все кто знает о tarantino это ваш у нас есть чатик мы постараемся вам помочь частности я или влад мы там все присутствие если же мы в чате не смогли решить вашу проблему дату и надо написать в тике tommy kid хобби эти ticket и нам раздаю да нам приходится делать чтобы вам помочь мы потихонечку разгребём и сделаем в принципе все для котов хотят наши вопросы спасибо за доклад вопрос такой что произойдет может описать если слоев применил в лоб отдал окно ок не дошел то есть commit vlog'а был акт был отправлен но не дошел до мастера slave думает что он применил commit костя об этом не знает смотрите у нас с вами это возможно только в той ситуации я справился connect ну да да да пошлости сипим используем te pido и у нас принципе пакеты не теряются заработала злая асинхронность ислам и а ну допустим что произойдет если данный connect не то что то не то что когда порвался по keep alive мы увидим то что он порвался connect так слоев это увидит и когда connect порвал сон пытается восстановить заново и в процессе восстановления коннекта он отправит свой текущий выкл окончил себя знает что он применил а не будет так что мастер попытается заново нет продублировать не в не попытаются то есть смотрите у нас есть процессоре лейдаун читает журнал а он не повторяет смотри у нас как в на слив на этом состоянии он приходит мастер говорит дай мне все дельты начинай с какого-то момента и мастер ему потоком их лупит если скажем оборвался connect данную слив у себя применил то оба они об этом знают процесс у мастера тухнет но и помещен он понял что ок не пришел потому что keep-alive вот этот говорить да да да да да спасибо рост спасибо за доклад скрыть пожалуйста на сколько хорошо тарантул виртуализацию то есть у вас допустим 6 инстансов в одном кластере если шарден гам то возможно понадобится шесть на шесть тридцать шесть на дорогих машинах имеет смысл запускать смотрите у нас сейчас есть конфигурация да где у нас порядка десяти машин на каждой машине порядка 4 8 месяцев тарантул работает то есть таранто не имел как бы нас тарантул тонизирует как правило около 3 типа 1 seconde лидирует на processing один утилизировать на работу сетью до 13 лидирует на запись журналы если скажем у вас машина имеет 16 ядерной бы может понять 4 тарантулов как неких припять прибить я драм да и они будут об этом прекрасно между собой работать спасибо пожалуйста спасибо вопрос такой если я правильно помню когда у нас умирает реплика мастер начинает копить их слуги даван наделяет что реплика придет и как бы будет их забирать есть какой-то механизм настроить чтобы он понял что все она через какое-то время не вернется экологии да у нас есть на самом деле два механизма первых если ваша реплика умерла до то скорее всего вы хотите space кластер вот меня здесь нет вот я сейчас на конференции меня там что-то умерло я хочу чтобы мастер допустим через час понял что реплика не вернется экология можно не копить чтобы диском забивать он у нас есть механизм который включается тот момент когда их кончается место подлоге то есть смотрите сос подлоге кончается место то включается механизм форсированный очистки логов вот тогда понимаю то что во первых у нас у мастер должен быть snapshot да потому что мы не можем удалить theologie которые идут от последнего фото просто мы не сможем сделать рекавери не смог что то есть вот есть дата то есть механизм специальные который видит то что место подлоге кончилось если вы не не делать никаких действий место подлоге в конфигурации задаю правильно украла у если вы хотите подлоге видеть отдельное место вы можете их просто писать на отдельный раздел выделить раздел не описать нету в конфигурации параметров к параметрам как бы мы мы хотим есть мысли о том что надо сделать но по факту данная задача решается очень просто вы просто делать отдельный раздел куда пишите логе и когда он не сможет записать логе начинает форсиер он формирует а чувству мусора да и то если мы не делаем никаких дефолтных действий да а как правило мы не знаем то что рипли камер ладанов работал мониторинг мы удаляем запись кластер все-таки дошел понимаются это дело уж руками спасибо пожалуйста что модель query есть на дочери есть я лично запускала в к время на продакшен я знаю что есть надо акире точно причем на docker там растут использована альпин есть какой-то образ linux а так принципе концентрации вполне себе всё работает и мыть с темно докеров и спасибо за доклад правильно я понимаю что если произойдет партии shining по сети допустим есть у нас три мастера до 1 не может установить соединение там с двумя другими тока каждая часть будет принимать по-прежнему запросы на запись писать у себя в vault и грубо говоря ждать пока она сможет с его передать то есть это как раз обратная сторона синхронности да то есть опциона репликации дает вам высокий перформанс задач не нужно ждать когда кто-то вам ответит о кейдана поэтому не защищены от разделения но при этом мы рассчитываем то что у вас придет либо keep-alive до либо еще каким-то образом мониторинг виде то что вас развалилась репликация и какие-то десятки нужно будет предпринять и если это случится во время сборки кластера то у нас может возникнуть по сути конфликт правильно память то при сборке вам новости по вот вы говорили там про мастера который самый мастер именно призвана да да да с наименьшим айтишником грубо говоря если одна реплика видит три мастера другая видит 2 то они могут выбрать разных правильно а нет такого понятия как видит смотрите у нас вы когда не цитируйте реплику она ничего не знает кроме того конфликт который вы передали в этом конфликте выкладывать ей айпи адреса до всех мастеров учиться-то анализов или join да так всё спасибо спасибо большое за доклад вопрос такой по поводу кворума так там вы сказали по поводу того что есть три лидер репликация нет смотрите у нас есть лидер не репликация join и то есть именно когда вы делаете то есть смотрите в чем проблема проблема в том что мы не можем позволить себе разводить использу кластер для этого мы делаем так что с поискать это модифицируется только на одно - и и каким-то просто произвольным образом выбираем одного мастера так как этот произвольный способ вот этот вот если алгоритм до который выбирает мастер с минимальным видно я просто хочу уточнить немножко мне хотелось бы добиться консистентной sti и хотелось бы писать в одну точку и хотелось бы чтобы это . выбиралась автоматически смотрите вот у ну то что рассказывал влада у нас такой есть ваш ордер у нас данные по враг идут всегда она указана в конфигурацию мастера в чем здесь проблема проблема в принципе можете потому что это делать сами дано вам тогда нужно если вы имеете имейте несколько точка входа до вам нужно эти . фото сортирует между собой так они чтобы они имеют иноков конфликт данная ситуация у нас сейчас решается используя внешние системы вы используйте какой-нибудь звуки perdo в этом звуке 1 вполне можете у нас есть клиент звуки пирог взводится специальный watcher до звуки переведи то что скажем мастер один какой-то умер и перестраивать выбрать другого лидера да на которые будут опираться записи спасибо пожалуйста"
}