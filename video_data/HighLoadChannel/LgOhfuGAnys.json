{
  "video_id": "LgOhfuGAnys",
  "channel": "HighLoadChannel",
  "title": "Eventual consistency в stateful-сервисе / Дмитрий Исаев (Яндекс)",
  "views": 256,
  "duration": 3131,
  "published": "2024-10-29T03:07:33-07:00",
  "text": "Здравствуйте коллеги Разрешите для начала представиться меня ещё раз зовут Дмитрий плюс-минус 14 лет в профессии за это время занимался там как-то и тестированием и разработкой и администрированием в общем эникейщик знатный А сейчас работаю в Яндексе в должности руководителя группы разработки и вместе с группой разработки мы разрабатываем Яндекс метрику Что такое Яндекс Метрика Яндекс Метрика - это инструмент веб-аналитики который позволяет передавать с сайта данные Ани же метрики в backend метрике А где они будут сохраняться обогащаться нормализоваться и всё это дело в итоге будет отображаться в виде отчётов графиков и рекомендаций которые пользователь владелец сайта может использовать для аналитики своей площадки для аналитики своей аудитории и для аналитики источников трафика которые приходят на его сайт если рассматривать Яндекс метрику с точки зрения каких-то компонентов то можно выделить такие компоненты Как клиент который крутится в браузере Он написан на жава скрипте и он посылает в Кэн метрике данные которые мы как раз там сохраняем есть API через который наши партнёры могут данные заливать которые будут также у нас сохраняться обогащаться нормализоваться и тоже будет что-то по ним отображаться есть веб морда которая всё это дело красиво отображает и всё это дело Разумеется разрабатывает не одна команда а множество команд как минимум backend метрики разрабатывает аже две службы А в каждой службе от двух команд располагается и все эти команды используют такие языки программирования как JavaScript Java c+ + сегодня речь пойдёт про компонент Яндекс метрики который называется движок метрики и Он написан на c+ Plus Что такое движок метрики это распределённая отказоустойчивость система потоковой обработки данных проще всего её представить в виде графа где вершинами этого графа будут сервисы сервис - Это от двух до множества машин которые мы называем шардена обработчика а рёбра этого графа - это связи по которым перетекают данные из одного сервиса в другой и в качестве транспорта у этих рёбер используется шина под названием и zaurus это Open Source решение от Яндекса регулярно про него тоже рассказывают коллеги так же в голове очень удобно визуализировать этот Граф в виде Воронки и если смотреть на эту воронку сверху там где в поперечном сечении воронка имеет максимальный диаметр то там данные в нашу воронку попадают из шины под названием Лок брокер в миру её ближайший аналог - это ач кавка там есть множество разных топиков топик данных которые присылает в нас наш GS клиент данные которые через API в нас попадают данные от наших партнёров и коллег по Яндекс тоже оттуда мы засасывание в себя и вот так вот эти все данные по Воронки сверху вниз перетекают от сервиса к сервису обогащаются нормализуются и в самом низу они встречаются в йф сервисе который будет строить сессии пользователей которые мы в метрике называем визитами Что такое визит с точки зрения обывателя визит - это действие на сайте которые он производит от их до сих я допустим захожу в 12:00 на сайт выбирать марку бетона выбираю её 15 минут и ухожу оттуда вот для меня Это и будет визит мой Что такое визит с точки зрения аналитика или разработчика это упорядоченное по времени множество событий Объединённых одним интервалом времени как отличить одну сессию пользователей он же визит от другой если между ближайшими событиями внутри этих сессий прошло больше какой-то временной дельты то это два разных визита соответственно Если меньше то это соответственно один визит и здесь Обратите внимание у нас каждый визит это некоторые журнал событий это у нас такая своеобразная реализация ивент сорсинг для того чтобы построить нам а каждый раз визит или сессию восстановить нам надо полностью перечитать журнал из стейта сервиса и только после этого мы сможем узнать что же в итоге получается и так У пользователя у нас получается множество таких журнальчик более того здесь схема чуть сложнее каждый следующий визит что-то знает о предыдущем визите что здесь имеется в виду предположим вы 5 лет назад посетили сайт погода yandex.ru и вот с тех пор Вы каждый день заходите на этот сайт у вас получается там какая-то сессия визит происходит и каждый ваш следующий визит знает что вот именно 5 лет назад произошёл ваш визит в первый раз он знает о нём и получается так что у нас стейт пользователя для одного пользователя на одном на одной площадке - это журнал журналов То есть у нас есть журнал визитов и каждый визит разбивается ещё на э ещё меньший так скажем детализацию на большую точнее детализацию когда мы видим каждое событие мы можем непосредственно каждый визит мутировать и они так вот подтягивают друг за другом какую-то Мета информацию Если представить всё это в виде схемы и здесь какой-то наивной реализации которая когда-то имела место быть то потоки данных в метрику выглядит следующим образом у нас есть пользователь который Мы представляем в виде его устройств Ну даже браузеров ну здесь для простоты это будет телефонный ноутбук эти данные от клиента попадают в шину по шине они проходят попадают в наш конвейер по конвейеру по воронке спускаются до сервиса который будет строить сессии и там каждое устройство данные из каждого устройство будет обрабатывать скорее всего каждый отдельный шарт обработчик у него будет свой стейт и он будет писать на на выход какие-то визиты если это формализовать то получается что метрики попадают во входящую очереди с сайта каждая Метрика попадает у них есть какие-то ключи у у каждой метрики они попадают в свои диапазоны и соответственно скорее всего и телефон и ноутбук попадут в разные хоть возможно что и в одну очередь попадут но здесь для простоты в разные они попадут понимание каждый шарт обработчик сервиса будет читать соответственно свою очередь будет писать свой стейт который который мы храним в базе данных под названием и на выход будет перекладывать данные исходя очередь это те самые сессии или они же визиты А теперь хочется эту всю схему немножко усложнить как бы мы понимаем то что у нас человек один устройства у него разные но это они принадлежат одному человеку и может произойти следующая проблема например вы 5 лет назад в первый раз посетили сайт погода Яндек ru с ноутбука и потом Вы принципиально начинаете пользоваться только телефоном и посещаете этот сайт только с телефона соответственно Мы помним что каждый следующий визит помнит про предыдущую историю пользователя но визит из телефона вот в той реализации наивной который мы с вами видели чуть ранее не узнает о том что первый визит произошёл с ноутбука потому что они друг про друга ничего не знают значит усложняем эту схему и получаем вот здесь вот такие связи Что у нас шарды обработчика начинают в Соседские статы шарахаться и получаем следующие требования что каждый шарт обработчик владеет своими данными То есть он их может писать и удалять но при этом на чтении он может ходить ещё и к соседу это порождает массу проблем Ну и главные из них - это отсутствие изоляции и гонки в данных отсутствие изоляции очевидно потому что мы ходим в соседский стейт за который мы не отвечаем за который мы не можем ни писать ни удалять и мы не знаем когда ту там новые данные появятся что вследствии поражает порождает гонки а что такое гонки предположим один шарт пришёл в соседский стейт что-то оттуда прочитал и через секунду буквально туда шарт владелец что-то записывает получается что первый шарт который оттуда что-то прочитал он построит какой-то агрегат который будет не консистентность чтобы в итоге финальный результат правильный был что он знал про все данные от соседей решения в лоб какие здесь можно придумать Ну здесь их опять же масса можно придумать какие-нибудь глобальные Локи уйти от много микросервисной архитектуры в Монолит там M использовать использовать огромный стейт там многофазные коммиты синхронные запросы у них в принципе ну они рабочие у них правда есть два фундаментальных недостатка они либо не масштабируется либо медленные из коробки можно что-то здесь заставить быстро достаточно работать но это будет непропорционально дорого выхлопу который мы получаем значит начинаем рассматривать интересную альтернативу для которой надо ответить на пару вопросов эти пару вопросов звучат так которые мы должны для себя озвучить Можем ли мы моменте построить конечные агрегаты не финальной версии скажем так то есть не совсем конечные и Можем ли мы немного немного это понятие растяжимые каждый сам для себя решает сколько много для него или нет но всё же немного пожертвовать в скорости построения финальной версии агрегата если ответ да то наш подход который мы хотим использовать это будет eventual consistency здесь eventual скажем так в широком понимании то есть нам Event Нужно привести ты разных сервисов и те агрегаты которые они строят тоже должны быть консистентные между собой то есть знать друг про друга Если вдруг что-то Меняется для этого что нужно сделать посмотрим как это устроено в принципе широкими мазками наши сервисы общаются через очереди всегда есть очередь по ней текут объекты вот которые с сайта в нас приходят их каждый сервис обрабатывает сервис родительский в дочерний может при этом начать подмешивать в эту очередь х события которые мы называем командами также могут существовать ответы на команду если тот сервис который записал команду требует обратной связи на это дело Итак что такое команда команда - это некоторое требование от одного сервиса к другому что-то выполнить это не Объект который надо взять сохранить у себя в стойте или как-то обогатить или обработать это и буквально требования что-то выполнить например один сервис другой может попросить удалить из своего стата что-то типа всё спасибо больше не надо забудь про это навсегда или наоборот Достань пожалуйста из своего стата для вот такого-то временной метки ближайшие события построй вокруг них агрегата и отдай мне его обратно пожалуйста также эти команды могут содержать кусочек стата это такой Event Car подход когда мы передаём вместе с событием какой-то агрегат чтобы тому кто получает эту команду не пришлось идти в чужой стейт И вычитывать что же это такое команды могут требовать обратной связи а могут не требовать Такой типа tcp или udp подход если нужна обратная связь мы сервис который обратную связь даёт он записывает на вход сервису ожидающего обратной связи ответ на команду если обратной связи не требуется отправил и забыл навсегда и хочется чтобы команды были инде патентные Что это значит что если вдруг мы в стечение каких-то обстоятельств читаем несколько раз одну ту же команду то результат её выполнения не повлияет на стейт внутри этого сервиса и на исходящие данные либо эта команда вообще не будет обработана В общем случае схема будет выглядеть так у нас есть пара сервисов которые принимают на вход очереди пишут на вход очереди и вот здесь один сервис пишет второму сервису как объекты которые он должен обработать прямо в одну очередь всё это пишет Так в эту же очередь прямо записываются и команды с которыми что-то второй сервис должен сделать второй сервис также пишет наружу что-то что нужно обработать следующему за ним сервису и при этом он может вот на команду типа один возвращать ответ во входящую очередь первому сервису где он её в хронологическом порядке также обработает и и примет какое-то решение которое ему нужно делать далее И это же совершенно точно справедливо для разных шардекор одного сервиса по большому счёту У нас вот это когда сервис представлен множеством машин в нашем случае это плюсы будут это будет один и тот же бинарник Единственное чем они отличаются Это только потоком входящих данных которые они обрабатывают и исходящими данными по факту вроде бы один и тот же сервис но при этом даже эти шарды между собой могут общаться также через очередь и здесь возвращаемся к нашему продукту опять же надо ответить на те два вопроса Можем ли мы в моменте строить не финальную версию агрегата и Можем ли мы чуть-чуть пожертвовать скоростью ради достижения более качественного результата и здесь мы вспоминаем что у нас есть визит когда визит начнётся мы не знаем когда он закончится мы не знаем предположим сервис вот мы уже смотря в прошлое мы понимаем что визит или сессия длилась 15 минут Ну предположим в 12:07 когда сервис получил первую пачку данных он знает что этот визит уже закончился или нет нет наш сервис построени сессии этого не знает э получается так что здесь какая-то совершенно размытая ситуация непонятно когда это начнётся Когда это закончится метрики могут Разумеется приходить не в хронологическом порядке потому что мы живём в интернете а там ну вот Загуляла где-то событие и пришло пару часов спустя как такое произошло Ну фиг его знает Вот так происходит Почему можно много вариантов придумать но Факт есть факт Ну и с разных устройств у нас могут приходить события для одного пользователя и получается так что наш сервис может строить Визит в моменте только из тех данных которые мы уже получили то есть по факту наш продукт как бы он вот с самого начала своего существования то есть сама природа этого продукта она мы постоянно стремимся к тому чтобы построить финальный результат вот нас пришли первая пачка данных для визита мы построили его пришла вторая пачка мы достроили перестроили старую версию ударили новую записали и так далее посмотрим как же это на примере выглядит вспоминаем У нас есть два устройства ноутбук телефон первое телефон второе ноутбук и вот здесь у нас есть какой-то стт который хранится в базе данных wdb и там есть партиции оди и партиции 2 Внутри этой базы данных в первой партиции хранятся данные для визита с телефона во второй партиции хранятся данные для двух визитов с ноутбука в хронологическом порядке они произошли первый с ноутбука второй с телефона третий с ноутбука и вот представляем такую ситуацию что вдруг откуда-то гулящего событие прилетает для нашего телефона а и это событие совершенно точно попадает в тот интервал визита который уже есть существует и получается нам надо его перестроить что-то с ним сделать Что делает первый шарт обработчик он во-первых читает своё состояние читает соседское состояние всё это дело наносится на какую-то временную прямую обогащается визит телефона получается теперь у нас не три события внутри визита А4 строится из него визит и обогащается этот визит знанием о том что первый визит был с ноутбука осуществлён и к чему это приведёт приведёт Это к тому что внутри состояние первого шарда обработчика во-первых это события добавится наружу мы запишем Новый Визит обогащённый новую версию какую-то удалим при этом старый автоматически и вспоминаем что у нас каждый следующий визит знает что-то про предыдущее и мы видим то что у нас третий визит с ноутбука происходил и он когда был построен ничего не знал про визит вот с четырьмя событиями внутри который на телефоне произошёл Что должен сделать первый шарт обработчик мало того что он запишет в исходящую очередь новую версию визита он ещё и запишет в входящую очередь второго шарда обработчика одного сервиса команду которая требует от этого шарда обработчика перестроить Визит на который повлияли первого шарда то есть такой типа EV дн подход Когда у нас что-то произошло в одном сервисе и он раз уведомляет другого только в рамках одного сервиса разных шардов Что делает второй шард Он также читает своё состояние читает состояние соседа наносит это всё на временную прямую перестраивает свой старый визит там в нём ничего не меняется но возможно что-то обновится в Мета информации которую он черпает из предыдущего визита который с другого устройства произо и в итоге мы получим на входе просто обновлённую версию визита для ноутбука и на этом как бы закончится вот эта карусель данных которая по очередям гоняет достоинство и недостатки данной системы но очевидно то что масштабироваться здесь мы можем не бесконечно К сожалению вот эти Крос шардов подходы на чтение они могут ушатать нашу базу данных К сожалению и здесь с двумя шарда обработчика это не так страшно когда они там кросс шардов запросы на чтени делают эти теперь представляем что их будут сотни вот как у нас сейчас 240 шардов у одного сервиса обработчиков и вот каждый из них могут одновременно пойти в другую партиции соседа и просто огромную очередь запросов туда выстроить это будет как бы так сказать медленно работать достаточно и тогда возникает вопрос А как же это у нас сейчас работает Ну слава Богу у пользователей всё-таки не по 240 устройств а там 1 2 3 ну пять устройств у пользователей и там действительно кросс шардов запросы будут они будут просто там в одну Там две соседских партиции ходить ещё достаточно быстро а но тем не менее мы понимаем эту проблему и как же мы будем её решать мы можем уйти от классической Event driven подхода к Event Car подходу когда мы у нас один шарт обработчик второму будет посылать не уведомление мол пожалуйста Перечитай мой стейт потому что у меня там что-то поменялось и перестрой свои агрегаты А мы буквально будем сразу же в соседский шарт обработчик посылать кусочек данных какой-то агрегат то есть не вот эти вот все события которые у нас там были придётся ему вычитывать а прямо агрегат сам визит можем ему прислать и сказать вот всё что после этого визита идёт Будь добр перестрой пожалуйста у себя и это будет достаточно хорошо работать недостатки этого улучшение Разумеется есть это усложнение разработки и тестирования и можно незаметно для себя в определённый момент по шине начать гонять добрую треть сво своей базы данных она у вас так будет летать за этим Нужно следить и вот как бы пробежав по первому примеру когда у нас общаются между собой одни шарды обработчики одного сервиса теперь хочется Более сложный пример рассмотреть например Когда у нас уже сервисы общаются между собой с помощью команд и плюс хочется рассмотреть пример где будет обратная связь на эти команды происходить и здесь мы будем двигаться точно так же мы рассмотрим продукт рассмотрим какая проблема у этого продукта возникает в процессе разработки и дальше как мы её решаем и вот красный пример - это склейка оффлайн событий что это такое у нас есть визит про который мы уже знаем что это такое Это какое-то онлайн событие и есть оффлайн события это Например покупка обуви какая-то пример из жизни Вы заходите на сайт обувного магазина выбираете себе там кроссовки но онлайн их не заказываете потому что не уверены в размере обуви и там час спустя неделю спустя 20 дней спустя Приходите в магазин мерите обувь покупаете и вот мы хотим вот эти два события между собой склеить то что Визит онлайн вёл к конверсии в оффлайне очень круто звучит как Кстати мы получаем эти оффлайн события вспоминаем тот самое API наши партнёры вот этот магазин как раз сам в нас присылает эти оффлайн события и мы их уже там запускаем в нашу метрику там происходит какой-то колдунство И это всё дело склеивается между собой Какие требования У нас есть к этому продукту Ну во-первых те данные которые в нас приходят вот эти вот оффлайн события сам факт покупки они приходят от нашего партнёра и для партнёра человек это человек Он вот один всего лишь есть а для нас человек - это множество его устройств его браузеров плюс идентификатор пользователя на устройстве или в браузере может регулярно меняться и скорее всего у нас будет маппинг их пользователя идентификаторы их пользователей в нашей один ко многим то есть мы раз и размножаем сразу же получается что у нас множество претендентов Кому мы можем приклеить это событие следующее продуктовое требование Которое хочется отметить это то что да данные которые проклеивать в течение 20 дней с момента онлайн визита впоследствии Мы хотим это масштабировать до 30 дней и обновляться в течение 3 месяцев с момента привязки что это такое пример из жизни допустим вы покупаете уже не обувь А квартиру у вас время на Принятие решения здесь увеличивается вы делаете сначала визит там на на сайт застройщика потом Приходите в в оффлайне к этому застройщику заключаете договор и это какое-то оффлайн событие то есть мы берём ваш онлайн визит и склеиваем его за заключением договора и потом 2 месяца спустя вы оплачиваете эту сделку и мы хотим обновить это дело 2 месяца спустя то есть вот этот визит с приклеенным оффлайн события мы его там что-то меняем у него статус нам нужно это обновить здесь К сожалению возникает такая проблема что мы не можем ввести бесконечный вот этот журнал то есть этот ивент сорсинг прекрасный он здесь будет с трудом работать 3 месяца сырых данных держать это не хватит Ника никаких баз данных просто не хватит это бесконечное количество SSD нужно будем целые дата-центры для этого строить и ещё это как будет масштабироваться сомнительно непонятно В итоге здесь сразу в голову приходит какое-то холодное хранилище или медленное хранилище например какой-нибудь S3 А в нашем случае это ydb поверх хдд сегодня кстати от моего коллеги был доклад по этому вопросу ydb Over HDD так что можете послушать или по приставать к нему он здесь в аудитории сейчас сидит а что это такое Ну во-первых получается что у нас есть быстрое хранилище оно на SSD там у нас вот самое актуальное окно данных которое мы обновляем хранится и там у нас хранится журнал событий для каждого пользователя супер подробные А в медленное хранилище попадают данные из быстрого хранилища и туда попадают не вот эти вот супер подробно расписанные каждый визит А вот прямо непосредственно сам агрегат то есть сам визит туда попадает и быстрое хранилище это где-то 150 ТБ данных одна табличка там сейчас 105 ТБ у нас в ydb весит А в медленном хранилище где-то 400-500 ТБ сейчас вот этих холодных данных на хдд живёт и в перспективе мы это планируем ещё до 300 на 300 ТБ увеличить данных Итак Какие проблемы здесь сразу же возникает Ну во-первых с учётом того что у нас тут связи один ко многим То есть у нас одно оффлайн событие размножается множество Ола у нас невозможно нам заранее определить куда же послать эти данные и чтобы какой-то из этих сервисов их обработал у нас получается два сервиса с разной логикой работы данные хранятся разные в разных базах с разной структурой данных то есть надо понять в кого чего посылать плюс вот эти вот все данные они Разумеется в разные шарды каждого сервиса попадают и непонятно У какого сервиса какого шрда будут данные к которым надо приклеить это оффлайн событие далее нам надо заставить как-то наши сервисы в нашем случае едва между собой общаться чтобы они договорились кто будет обрабатывать эти данные потому что у них вроде бы итог должен получиться один и тот же их работы а вот данные их структуры по-разному хранятся кто чего делает в какой момент непонятно Ну здесь можно конечно к монолиту вернуться но есть свои минусы Мы помним Ну и дублирование обработки недопустимо в общем-то как и дублирование данных которые мы наружу записываем как эта будет схем выглядеть вот если её рассматривать В общих чертах вот приходит офлайн событие оно размножается на какое-то н идентификаторов пользователей есть два хранилища медленное и быстрое кто же будет это обрабатывать как мы решаем эту проблему Ну разумеется мы решаем её с помощью команд вот этих технических событий которые мы подмешивать прямо в очередь и во-вторых мы вводим ещё один сервис который называется координатор он же Великий арбитр такой царь сервис будет в нашем случае это цар сервис координатор получает во-первых Все оффлайн события он скажем так всё знает об оффлайн событиях он знает На какие Для каких пользователей потенциально оно может приклеится Какие сервисы могут потенциально приклеить кому он в данный момент точно приклеен То есть он такой К сожалению наверное это такая точка отказа в нашем случае здесь Но тем не менее Он у него великое знание есть по поводу всего офлайна и он же отправляет команды на проклейку отклеить - за собой вниз по течению и на вход он принимает результаты выполнения этих команд Как в общем случае это будет выглядеть вот есть координатор у координатора есть тоже стоит который Разумеется хранится в idb у него там знания о всём оффлайне хранится на вход он принимает оффлайн события и ответы на команды на выход пишет только команды что какие команды у нас в общем-то существует в нашем случае это три типа команд в вашем случае это может быть Какое угодно другое количество зависит от вашего продукта У нас во-первых есть команда изволь найти претендента к которому мы можем приклеится второй тип команды - это приклей непосредственно к этому визиту который ты нашёл наш оффлайн события либо от клей этих трёх команд хватает для всех сценариев далее мы это рассмотрим более подробно координатор также ничего не знает о нижележащих сервисах это некоторые упрощения Он просто пишет одну очередь и эту очередь могут читать кон сюмейе потребителя два два сервиса они могут либо одновременно читать это либо как у нас это сделано Они читают это по цепочке зачем это сделано чтение по цепочке вспоминаем У нас есть быстрое хранилище и медленное хранилище данные медленное хранилище попадают из быстрого хранилище и если вдруг мы посылаем в быстрое хранилище команду на поиск при претендента и быстрое хранилище у себя вообще никаких отметок в памяти не находит о том что здесь когда-то были данные для этого пользователя значит в медленном хранилище их точно нет этим как бы этим трюком мы убиваем двух Зайцев во-первых мы не гоняем по сети килотонна трафика Потому что здесь я показал там оффлайн событие одно приходит зачастую мы мапи его в сотни идентификаторов по которым для которых нам надо найти претендентов и вот быстрое хранилище - это такой своеобразный фильтр который мешает пройти вот этому просочиться этому мусору дальше по сети и во-вторых медленное хранилище оно по-настоящему медленное То есть если мы туда будем его заваливать трафиком то оно встанет колом рано или поздно Оно просто не будет успевать читать данные из своего стата И вот так эта схема будет выглядеть если мы добавляем координатор обычно как выглядит Граф обработки данных То есть это какой-то циклический ориентированный Граф перетекания данных от сервиса к сервису у нас же здесь появляются Вполне себе петельки где данные заходят крутятся крутятся крутятся в этой петле и рано или поздно они из неё выйдут и теперь надо наверное рассмотреть чуть подробнее здесь нужно повнимательнее будет вам к сожалению смотреть это не тривиально здесь будет рассмотрен далее пример гонок и того как это дело плюс-минус привязывается и как координатор работает и вот представляем ситуацию у нас приходит оффлайн событие которое размножается на три претендента вот такой простой способ одно олайн событие у него есть соответственно какой-то ключ он попадает в один шарт координатора у которого есть какой-то стейт далее координатор указывает для этого оффлайн события что у него статус на данный момент это поиск визита то есть лучшего претендента и у нас координатор Ну вот эти идентификаторы пользователя это возможно разные устройства в худшем случае каждый из них пойдёт в свой шарт обработчик сервиса с быстрым хранилищем и там уходит команда Пожалуйста найди для указанного идентификатора ближайший визит к оффлайн Что делает быстрое хранилище а быстрое хранилище может передать по цепочке медленному хранилищу эти команды во-первых может передать а может не передать если оно не передаёт значит данных нет И значит ответа у нас координатор не получит если он не получает ответа на команду он по этому поводу не печалится если он не получил ответа Значит его просто нет нечего делать с этим и здесь мы видим то что только первый и второй шарт передают в медленное хранилище результаты выполнения команд на себе они могут внутри этой команды передать Как знание о том что я у себя что-то нашёл вот и ты у себя Поищи либо я у себя ничего не нашёл Но я знаю что потенциально У тебя что-то есть пожалуйста найди это у себя медленное хранилище в свою очередь медленным оказывается и тут случается гонка у нас первый шарт обработчик медленного хранилища оказался более шустрым и обрабатывает свою команду быстрее второго шарда обработчика и отвечает в координатор каким-то визитом координатор радостно меняет принимает это эту команду с этим ответом меняет у себя статус на то что мы теперь в процессе привязывания находимся этой оффлайн команды этой оффлайн этого оффлайн события и посылает команду на связывание в первый шарт обработчик быстрого хранилища который там уже сами вместе с медленным определяться кто из них будет привязывать потому что они по цепочке расположены тут у нас в этот сразу же после отправки координатором команды на привязку у нас медленное хранилище всё-таки изволит обработать данные для второго пользователя и отвечает И тут вдруг выясняется то что данные в шар у шарда рабочи 2 лучше мапи с этим оффлайн событием вот и что будет на это делать координатор координатор просто у себя мето ставит в йте что Да я сейчас привязываю офлайн события к какому-то визиту но у меня есть лучший претендент И когда я получу ответ на команду о привязке мне надо будет отвязать и перепривязать в итоге оффлайн события к визиту то делает медленное хранилище у нас в следующий раз а у нас тут первый шарт обработчик наконец-то отвечает на команду привязки и координатор узнаёт то что всё мы первый шарт обработчик какого-то из сервисов привязал у себя оффлайн события к визиту в этот же момент координатор меняет статус у оффлайн заказа на в процессе отвязки и посылает команду на отвязку В общем опять происходит вот этот круговорот вот этой команды медленное хранилище возвращает ответ на эту команду в координатор координатор её получает меняет статус на процессе связывания посылает команду в шарт который может это привязать В итоге медленное хранилище возвращает ответ на эту команду что всё хорошо привязано и в итоге мы переходим к консистентной координатора что мы привязались к лучшему из возможных вариантов при этом мы вот этим привязыванием и отвязывается статы как в медленном так и в быстром хранилищах и плюс Мы помним что медленные быстрые хранилища также записывают наружу какие-то визиты и они тоже приходят рано или поздно к консистентной состоянию Итак достоинство недостатки данной схемы Ну во-первых работает она достаточно быстро 2-9 минут почему тут такой разброс 2-9 минут Ну потому что опять же вспоминаем у нас один офлайн заказ может размножиться на сотни потенциальных претендентов к которым мы можем приклеить и Вот эту вот Карусель может 9 минут там происходить рано или поздно но это очень быстро потому что мы На огромном объёме данных производим поиск потом мы рано или поздно приходим к консистентные Тай Ну и масштабируемость этой схемы потому что у нас каждый раз команда и данные для этой команды уходят ровно в тот шарт который владеет каким-то тейтом он не будет общаться рядом ни с кем он работает только такой Сам с собой он такой изолированный абсолютно Ну и недостатки данной схемы разумеется это сложность реализации и тестирования тестирование особенно доходит до абсурда что приходится выкатывать через об эксперименты такой выкатывает всё там такой Ну вот хуже не стало точно там вроде бы и Так значит катим дальше разумеется это всё и функциональными тестами обмазанный всё но интеграция очень сложная особенно обновление тестовых данных потому что вот эти вот петли они совершенно непредсказуемо работают К сожалению даже вот в тестировании очень сложно заставить это дело всё детерминировано работать ну и я имею в виду что под детерминированность работы что будет совершено одинаковое количество проходов по вот этой вот петле которая у нас внутри конвейера будет это к сожалению добиться очень тяжело чтобы оно было одинаковым здесь рано или поздно ивели мы придём к правильному состоянию но Сколько для этого понадобится итераций мы предсказать заранее не можем Ну и немножко фек Циферки здесь нужны не для того чтобы шокировать вас х Как много т большие или наоборот там чтобы вы сказали ха-ха там Ничего себе какая ерунда а э цифры нужны для того чтобы продемонстрировать А вот эта методология вот этот подход эти трюки которые сегодня здесь описаны они как минимум работают на нагрузках которые вот продемонстрированы на этом слайде вот в обычный погожий день у нас от JavaScript клиента пролетает где-то 1,5 млн запросов в секунду по сети мы гоняем более 80 Гигабит в секунду у нас больше 80 микросервисов они Вполне себе даже не микро Некоторые из них ещё бы не мешало посильнее подбить это сервисы Как stateless так и stateful и те сервисы которые stateful хранят своё состояние внутри базы данных idb и там хранятся в данный момент сотни терабайт это такой у нас кэш в этой базе данных хранится Ну и наружу мы пишем миллиарды визитов миллиарды сессий в сутки Ну и базовые технологии с помощью которых можно реализовать всё это дело это этоке и они кстати вроде бы обе в орсе и хранилище который мы в основном используем для прекрасна масштабируемая оно тоже в орсе можете пользоваться и построить похожие системы на этих технологиях Всё спасибо вопросы Спасибо У меня есть вопрос пока Яго подру центру так Я надеюсь что хелперы сейчас до нас доберутся А я пока свой вопрос задам до Коли до Коли это можно терпеть Ну всего лишь пол милна псов Ну прям слово Ну у нас Хало или где когда миллиард уже будет ну оно справедливости ради бывает и побольше бывают новогодние распродажи и прочие штуки и там привалило мл там отстреливает немножко не про это рассказ был тут конечно представляет из себя скажем так две части это фронтенд бэнда и Энд бэнда и фнн - это вот сервисы первой линии единственная задача которых только получить трафик и записать его в шину которая в нашем случае вот типа пач кавка Лок брокер которая Она же idb кстати внутри и поэтому в общем-то это всё дело выдерживается там четыре дата-центра кажется сотни машин всё это дело тащат Ну это даже относительно немного на самом деле вот эти сотни машин это даже ерунда Если сравнивать с какой-нибудь рекламной сетью или там биржами какими-нибудь там у них там до тысяч Может там до десятков тысяч даже может доходить но они что-то в ран тайме считают справедливости ради А мы в ран тайме ничего не считаем мы просто перекладываем это в шину хорошо так появилось ли у нас хелперы с микрофоном Пока нет Поэтому придётся мне спасибо за доклад во один быстрый чем не удовлетворил вдг почему он вдруг оказался медленным а второй вопрос уже про саму архитектуру то что нарисовано на картинке называется обратная связь она бывает отрицательная бывает положительная положительно система разнесёт были ли у вас лавины Особенно с учётом того что тестирование сложное или вы как раз об экспериментами от Лавин избавились Да спасибо в общем-то Так давайте первый вопрос M Почему во-первых mce мы использовали Действительно это первая реализация которая была и она в моменте она работала нормально она устраивала нас по скорости но к сожалению с количеством пользователей которые начинали пользоваться вот этой фей проклейки офлайна например вот в частности оффлайн был реализован через Muse интенсивность данных которые нам надо было проклеивать вот в моменте времени она лавинообразно начинала расти и серно у нас на каком-то окне работал То есть он копит копит копит буфер там у нас в какое-то хранилище мадс там в дупе или в нашем случае it этого хранилища мы накопили какие-то окно и начинаем на нём строить отставание здесь вот увеличивалось со временем и оно доходило там иногда даже до суток а пользователь который заливает свои олайн события он хохочет его вот типа я залил вот где оно в отчёте почему оно появилось там день спустя типа Какого чёрта Я хочу видеть это вот прямо сейчас а ещё мы вспоминаем то что эта штука может перестраиваться Например у нас же есть Граф связей потому что всё равно какая-то аппроксимация вот эти вот связи один ко многим там пользователь которого нам прислали Мы про их пользователя ничего не знаем они про нашего ничего не знают У нас есть специальные люди которые занимаются вот этой склей кой пользователей и связи могут меняться и вдруг тут выясняется то что связь поменялась и нам надо типа этот оффлайн заказ теперь вообще не мог приклеить к этому визиту его надо оттуда срочно оторвать приклеить к другому визиту найти его и этот мадс он просто начинал задыхаться постепенно как наивная реализация нормальная как ф of концепт для продукта эта штука работает а вот чтобы в риал тайме это строить Ну реалтайм сильно наверное сказано 29 минут но тем не менее это уже гораздо лучше чем сутки вот а что касается Лавин Да именно с этой штукой мы сталкивались Разумеется на этапе тестирования от это было мучительно больно эту штуку разрабатывать ээ и там такое количество инвариантов вот этих вот статусов Кто в кого в какой момент приходят какие гонки где раз там разные статы у разных сервисов у разных шардов разные статы их надо всех привести И не забыть чего-то оторвать и там мы Да мы сталкивались с ситуации когда у нас бесконечно вот начинает гонять просто там гигабайты трафика летают ты такой думаешь почему у меня всё тормозит что что происходит вроде бы очередей как-то чего-то нет а ничего не работает и в итоге да и у нас есть мониторинги специальные которые отслеживают такие аномалии скажем так как раз постоит координатора мы понимаем что если там какой-то заказ всё никак не может прийти к своему финальному какому-то состоянию вот этот оффлайн заказ значит это время пойти всё-таки разобраться что это за ины вариант такой проблемный и пофиксить его Спасибо сле ваш вопрос да спасибо Дмитрий за доклад Ну частично ты уже рассказал Вот про Ну у меня вопрос был про чтение этих данных то есть мы пытаемся эти данные как-то держать в консистентность и получается вот кто потребители вот помимо вот этих процессов построения отчётов Вот кто читает эти данные и получается Он также читает их через координатор или уже напрямую эти Стой так как они уже консистентные а в смысле результат работы вот этого всего дела Ну там Да я могу чуть-чуть назад вернуться там давайте ммммммм здесь Наверное я не совсем явно это рассказал А вот здесь вот это медленные и быстрое хранилище они на выход могут что-то записать там Ну пусть это будет база кликхаус там любимая конечно же которая не тормозит а и туда Каждый раз когда мы что-то приклеиваем к визиту или отклеивается новая версия визита и тот кто хочет что-то читать Вот эту сессию пользователя он читает это либо кликхаус либо там может быть та же кавка например мы какой-то топик записываем и Они читают При этом они читают это опять же В каком виде это опять старый добрый ивент сорсинг получается Мы в них записываем версию визита они такие Ага спасибо получили потом мы записываем в неё Допустим мы хотим удалить эту версию визита или обновить её мы записываем типа минус версию этой этого визита события на удаление старого визита грубо говоря и Новый Визит записываем и они чтобы получить всё-таки финальную версию визита им надо знать вот эти все версии визитов которые мы они рубо говоря двигаясь по этому журналу визитов ну как вот у кошелька биткоина например да кажется да там там все вот транзакции записаны там там там финальные числа нет которые денег у тебя на остатке тебе надо все транзакции посчитать которые были тогда ты узнаешь Сколько у тебя денег здесь ровно тоже самое хочешь узнать финальную версию визита изволь посчитать всё это друг за другом вот откуда они это берут Нет они изв это не берут у координатора медленного быстрого хранилища они это забира каких-то источников которые медленное или быстрое хранилище заполняют это может быть либо кликхаус в нашем случае либо Лог брокер он же там вот в миру как о поч кавка такой вот и уже из топика забирают Ну и из топика эти данные могут перетекать там с помощью специальных Дета трансферов из топика автоматически перекладывается куда-то на жёсткий диск и там уже можно mapus операции над ними выполнять например каким-нибудь по крону или там в C например это всё сделать Спасибо Спасибо ещ вопросы Добрый день Скажите пожалуйста сколько лет этой архитектуре и почему c+ Plus Простите сколько это сколько лет этой архитектуре этой архитектуре вот это вот э конкретна архитектура ей где-то год год жизни почему c+ Ну это скажем так Метрика она существует уже лет не собрать Ну больше 10 лет и она с самого начала была на плюсах написана Можно ли её реализовать на другом языке Можно конечно но так сложилось просто Что у нас весь стек на плюсах фреймворк который мы используем РТ называется это причина почему мы например не одну шину к брокер используем а две шины к брокер где-то вот на входе и выходе в наш конвейер А между сервисами мы используем шину it это требование фреймворка которые мы используем он называется bigt и он кстати о нём тоже были доклады возможно он даже в опенсорс я сейчас располагается и он тоже на плюсах написан и здесь вот как вот В связи с тем что c+ Plus используется каким образом вы Решаете проблемы Вот ну связаны которые c+ Plus и C с собой приносит Ну в частности там проблемы с с эффективным использованием памяти там и прочее То есть вы просто эти микросервисы убиваете заново поднимаете Да или как Как Ну и что мы делаем если вдруг проблемы какие-то появляются Они конечно же появляются даци памяти дефрагментацию памяти Как вы решаете понятно да Ну это просто гигиена разработки мы используем умные указатели если мы используем то есть сырые указатели не используешь и всё хорошо в принципе если так но это плюсы и умные указатели Это конечно не спасает всё равно где-нибудь Там накосячить наверняка будет какая-нибудь многопоточность внутри сервиса и там тут у тебя обект какой-то умер А ссылка на него до сих пор существует и такая ерунда Да у нас сервис просто падает и если ошибка это детерминированная То есть она стабильно воспроизводится то сервис будет стабильно падать и это будет яркий признак для дежурного который за сервис сейчас отвечает что что-то идёт не так можно было бы писать что-то в логе там ч поставить но логи никто не читает скажем так А вот если сервис падает у тебя конве не работает ты начинаешь быстро фиксировать проб а вы принудительный шатдаун сервисом не делаете да А а Что значит а периодически рестар нет периодически мы их не рестартиране также называется что есть какая-то ручка или какой-то признак того что сервис вдруг начинает тормозить или медленно работать И в этот момент Значит его надо грохнуть да такая есть дабо Спасибо вот у нас есть Следующий вопрос здесь Спасибо за доклад У меня два вопроса связанных А первый насколько динамично у вас меняется топология вот этих шардов Ну то есть добавляются и выкидывают шарды Угу и второе по боттл неку какое у вас вот на самом мощном звене какое оно требует оборудование так Ну так как часто меняются шарды нечасто надо признаться мы там с запасом их сразу накидали Поэтому пока год мы держимся нормально у нас всё живёт на текущем текущих железка Сложно ли менять эту вот топологию нет в нашем случае не сложно у нас просто есть сервисы Ришар так скажем здесь я эту детализацию не стал так вот подробно расписывать нас есть очередь может читать сервис единственная задача которого прочитать вот Одну одну шардирование создаём новые очереди и шар начинает в новые очереди всё это дело раскидывать Вот так это можно сделать То есть можно без шарден там остановить сервис который пишет восходящую очередь его остановить подождать пока за ним написанная очередь уже разгреби перешагивать его выход новое правило в конфиг внести и запустить раньше мы даже делали на горячую перешагивание но мы как-то сложность реализации там бешеная на самом деле была то есть вот чтобы там сам писатель там тормозил там следил за тем Какая очередь куда записалась там версия сообщения начинается это мы один раз это написали воспользовались ей и потом нам через 3 года это понадобилось ещё раз сделать мы такие Чёрт оно точно ли работает до сих пор вообще такой И там ты крести такой и начинаешь перешагивать это дело но в итоге проще реально с таймов второй вопрос - Это какие железки нам нужны здесь на самом на самом таком бутл неке Я так понял координатор да да координатор здесь будет главным бутл неком Ну справедливости ради тот объём данных который он через себя проса сыю вот эти офлайн это где-то 600.000 заказов вот не размножением ещё это на самом деле ерунда Ну на фоне вот этого полутора миллионов запросов в секунду которую пережёвывает там основной конвейер это вообще ничто какой там есть botle Neck который может У нас есть табли справочник куда Какие в принципе у нас есть вот эти оффлайн заказы и Для каких пользователей она существует и в неё К сожалению нам приходится ходить из разных сервисов грубо говоря такой глобальный справочник для многих сервисов и многих шардов и они могут её задушить здесь какой выход то есть железа для этого много не надо это опять же у idb здесь скорее потребуется возможно Вот это рди таблицу в wdb А в wdb Каждая таблица - это набор партиции Ну как там например в кафке тоже там партиции есть это какая-то мера параллельности здесь это тоже самое будет И у нас допустим есть табличка в которой у нас 15 партиции и вдруг мы начинаем душить её Ну вот количестве запросов которые в неё приходим не проблема накидывай в неё ещё партиции она сама автоматически на горячую меняет своё шардирование внутри и мы начинаем сразу же в большее количество потоков в неё способную Ну способны в большее количество потоков в неё ходить пока так живём Могут ли здесь быть проблемы могут Да мы за этим следим местом и у нас там куча мониторингов это всё обмазать не так придётся что-то придумывать для этого у нас наверное отдельные выступления есть целое представление то как мы пользуемся вот драйвером или библиотекой работы сди бимы там дробим данные на так чтобы каждый запрос в свою партиции шёл вне планировщика мы там дневные таблицы не ТТ используем данных вот чтобы удалялись данные из таблиц а заводим там допустим таблица на сутки а через через сутки мы её дропаем это потому что быстрая операция не как ТТ там линейно надо будет всю таблицу проходить Ну и там много трюков можно ещё уточнить вопрос просто из вашего обс ворвусь и попрошу вопрос придержать для дискуссионной зоны потому что нам нужно оставаться в тайминг мы и так уже выбились Вопросов много поэтому у нас есть будет возможность задать все вопросы спикеру через буквально через минуту в дискуссионной зоне Давайте же поблагодарим спикера за этот интересный рассказ Спасибо и вам спасибо за ваши интересные вопросы а ты не убегай Нужно ещё выбрать какой из них был самый интересный так Ну давайте вот последний вот человек который задавал вопросы как раз про последни вопрос поднимите пожалуйста руку чтобы вас смогли найти хелперы вот что ж ещё раз вам спасибо за ваше внимание спикеру Спасибо за его доклад"
}