{
  "video_id": "DfaTNXCsYRg",
  "channel": "HighLoadChannel",
  "title": "Подходы к реализации шардинга в современных [No]SQL-системах / Константин Осипов (Tarantool)",
  "views": 3429,
  "duration": 3016,
  "published": "2019-06-03T09:01:53-07:00",
  "text": "небольшая корректировка этот отпра не про то вот все все правильно мы про мы поговорим про то как sharding устроен целом в частности у нас но вот стульчик на пол шишечки значит для начала обо мне можно и погромче пожалуйста сначала бы мне я делаю тарантул до до этого дело mais quel если вам интересно система у нас это будет в 3:00 митап у нас есть стенд приходите мы расскажем теперь о том как родился этот доклад нас на самом деле tarantul 2 реализации шарден га и вот когда мы работаем над ошибками мне кажется и оба силу ну ладно когда мы работали над ошибками появилось такое и осознание кругозор того что вообще в мире есть какие есть решения как между ними выбирать и собственность этим опытом хотелось поделиться даже вам поделиться ну нужно сначала как-то врубиться в контекст поэтому часть доклада будет посвящена тому вообще какие проблемы технически при реализации шарден га есть вообще как он может быть устроен в принципе вот и провяжем а связь между архитектурными решениями во второй части мы поговорим и соответственно производительности band's базы базы данных и и плюсами минусами вот ну что это конкретно о чем конкретно идет речь то есть вот когда мы так секунду а указка все да а есть когда мы соответственно когда вообще смотришь на но как sharding устроен отвечаешь на 3 вопросы которые здесь попробовал записать вот это по какому принципу просто базового у нас есть кучу данных нам нужно как-то не одна кластер распределить по какому принципу мы их распределяем на кластер дальше после того как мы распределили да где вообще информация о том как данные хранятся границы то есть это метаданные это нужно для того оказаться того как мы ответим на этот вопрос мы у нас устроен защитит ответ на это вопрос у нас по-разному делается ли балансировка и сам роутинг данных роутинг запросы в данных и третий вопрос это вот как организовать роутинг то есть как выполнять сложные запросы после горизонтального раз разбиения как сделать так чтобы они не тормозили кстати в целом вот я могу быть достаточно нудным докладчиком если меня не троллить поэтому прерывать меня во время выступления очень даже почетно если что я возьму вопрос отложить до конца выступления отвечаю на него в конце выступления то есть я ценю вашу обратную связь и это доклад для вас поэтому давайте попробуем как-то вместе во всем этом разбираться ну вот самый такой краткий обзор того как какие бывают шар функции это представлено этом слайде это либо хэш либо range вот то есть система очень простая либо мы фишеру им до по ключу определяем номер сервера или номер или адрес или что-то такое и размещаем данные на этом сервере либо мы них они хэш-функции вычисляем а мы сравниваем ключ с каким-то там минимальным максимальным берем берем весь диапазон ключей бьем его на под диапазоны да и точно также каждому диапазону назначаем свой сервер и один из тезисов этого доклада заключается в том что он посмотрите зачем зачем вообще нужен но искали как он появился почему он вот про sharding в основном да ведь долго еще появления но и сквере были горизонтально масштабированы масштабируемые базы данных и то-то рада то это vertica это таймс т.н. много решений вы вот кто то слышал об этих решениях в зале слышали да то есть получается что но и скулит они не про горизонтальный масштабируем тоже горизонтальным что беру не было дано иску или а про что тогда и основной мне кажется ответ на это вопрос это то что на сколь про популяризацию горизонтальном масштабировании то есть мы не можем взять там дорогие enterprise система не недоступны большой аудитории но проблема становится настолько широкой что нам нужно как бы горизонтально масштабировать практически все тепло и но не практически все но гораздо больше чем по для чего предназначались enterprise системы вот так появляется no es que el и если опять же посмотреть на какие то основные принципы по которым но и сколь строится изначально это то что он строится для того чтобы упростить именно работу с большим объемом данных то есть а упростить это что значит это значит что ваш система предсказуемая она масштабируемая да то есть вот мы все знаем как сложно там тюнить индексы в сиквеле как ты тоже это целое прям сообщество людей у каждой базы данных они между собой ведут религиозные войны крестовые походы иногда организовывать друг на друга и все потому что база данных сложные вот она и сколь система они как бы говорят вот смотрите у нас очень простая модель танки вылью там или что-то калом стороны что такое data structures top но зато она и она еще хорошо в горизонтальном масштабировании масштабируется то есть честно все если каких-то вещей модель не поддерживает туда мной сколь говорит вы должны делать это в своем приложении сами вот и один из тезисов которые бы хотел кого проиллюстрировать вот тем тем что я сейчас буду рассказывать это то что на самом деле с развитием но и сковали движения этот этот вот изначальный тезис изначальный посыл которому она возникает от популяризации упрощению он уходит в прошлое то есть у нас возникает сложности внутренним устройством даже но и сколь систем вот мы сейчас проект сложности тоже будем говорить то есть смотрите вот есть два они космических способы позиционировать данные hаши рендж и один космический про которая поговорю кратко космический означает что вы меня извините мне не нравится что я даю что мне слышно собственный голос вот так вот мне лучше ведь лучше меня слышно таким образом да нормально отлично то что как бы да если заку режиссера там что-то не нравится то пожалуйста увеличить мощность отлично значит гораздо лучше вот смотрите давайте возьмем хэш если как-то чуть подольше пожить и изучать историю но и вскоре движения то самые первые горизонтальной масштабируемую систему использовать консистентной хэширование консистентные хэширование но правильно следующим образом мы берем все наши ключи и хэш-функцию да кстати кто может быть знает вот может быть куча народу же знает как устроена консистентных ширме ну так не особо да вот не спасибо значит мы беремся ключи мы вычитаем от каждого ключа хэш-функцию но значения хэш-функции мы представляем не виде диапазона виде кольца закручиваем диапазон в кольцо и для того чтобы понять где находится на каком сервер находится каждый ключ мы также фиксируем перебора да пусть у нас есть какой-то сервер 1 этаже вычитаем от неё хэш-функцию это же это значение также попадает на кольцо вот у нас появляется здесь 3 сервера соответственно весь диапазон очень легко понять какому сервер он принадлежит то есть вот вот этот допустим диапазон принадлежит первому серверу до фрагмент кольца вот этот фрагмент второму тот фрагмент 3 до то есть то что после чтобы все что между двумя серверами начинает принадлежать одному из сервер если нам нужно добавить еще один сервер в эту систему а мы мне внезапно перестал работать виктор значит если нам нужно добавить еще один сервер эта система вычисляем от него хэш и он попадает где-то вот сюда вот допустим или вот сюда например ну кыш и даже с болями не случайная функция и когда у нас появляется новый сервер мы берем наше кольцо и вот это диапазон мы допустим разбиваем на 2 под диапазона и перри распределяем данный на новый сервер вы смотрите вот уже даже этот подход она люстрируют основные проблемы которые решают shopping то есть этот выбор хэш-функции вот мы и выбрали прекрасно на просто вычитаем х до принцип принцип по которому у нас где у нас хранятся метаданные но нас всех метаданных это список наших серверов в общем то и все никаких особых метаданных здесь нет если у нас три сервер это у нас одно распределение из нас 4 сервера у нас другое распределение как в сеть роутинг данным да как угодно вот если вы знаете сколько серверов вы можете из любого из любого клиента попасть на анну в нужную вам точку до но обратите внимание так как это одна из ранних реализации она не отвечает на такие вопросы как granular ность балансировки вторичные ключи до равномерность балансировки то есть вот ну гранулярный с равна вверх наверное где-то рядом вывод из строя сервера то есть вот нам хочется вручную вывести из строя сервер и сейчас как мы увидим когда это будет развиваться подход но реализация шарами мы видим как на эти вопросы постепенно дается ответ это же вот ну вот какая проблема этого подхода то что он отлично работает если у вас серверов там допустим на тысяч потому что все кольцо бьется на очень маленький диапазон если количество диапазонов маленькая-маленькая сами диапазоны большие то естественным образом получается неравномерность потому что когда мы фиксируем допустим этот сервер 1-2-3 мы вычитаем как функцию hash это ну нормально он чаще дает нормально распределенной случайной величиной неравномерную хороших функций она достаточно случайно то есть получается что у вас будет очень сильно отличаться сервер на котором самый большой диапазон находится по нагрузке и серы на котором самый маленький диапазона по нагрузке находится естественным образом возникают такие сервера вот чтобы добиться более менее равномерности тоже одна из ранних один из ранних подходов пещерами которые точно также очень маленькая имеет состояние метаданных то есть вот это технической проблемы где хранить метаданные почему она возникает представьте себе что вам нужно сделать 3 балансировку то есть часть данных перенести с одного узла на другой после того как в туре балансировку завершить уж как вы и делаете неважно вы должны обновить все клиенты чтобы они узнали о том что на узел появился и то есть метаданные то есть откуда клиенту знает эту информацию и чем толще вот эта вот информация да то есть мы сейчас поговорим что она может быть представлена таблично она может быть значит задаваться вручную так далее тем тяжелее происходит это обновление то есть она тяжелее просто пирог от языка заходить организовать его без простое кластером представьте себе что вам нужно обновить резко все клиенты ну и у вас получается небольшой downtime до или пик полей пенсию какой-то небольшой вот эту проблему собственно сложно было решить и задачи которые перед ранними горизонтальным масштабируемые но искали системе ставить они были гораздо проще поэтому вот просто использовались например как в главе подход с математической функции которая рекуррентно задано и она берет и весь диапазон на делит на по диапазону то есть вот отличие от предыдущего подхода когда у нас используется хэширование здесь у нас используется ну можно сказать что это хэширование но это хэширование которое сохраняет порядок до ликер а куда нужно направлять clicker чтобы он щелкал вот смотрите представьтесь как работает главу концептуальным вот у нас изначально диапазон не разбит это целиковый диапазон ключа здесь какой-то 0 а здесь какой-то там and max до или здесь какой-то там аардварк а здесь еще что-то назад да вот значит после того как мы добавляем еще один мы берём и диапазон делим пополам ровно пополам но все просто да то есть часть данных извиняюсь и кликер огонь часть данных остается на старом сервере часть данных уезжать на новый сервер после того как мы добавляем 3 сервер мы берем и откусываем от каждого из предыдущих одну треть потом от каждый при давлении 4 мм и откусываем одной четвертой от каждой 3 до что что-то получается в конечном итоге нам у нас количество диапазонов таким большим что мы начинаем откусывать от восьмушки 16 да то есть вот совсем маленький частям даже не восьмушки там 1 5 1 десятую допустим вот смотрите мы добавляем 5 сервер нам нужно от 1 4 откусить одну одну пятую это получается 1 20 так далее но значит сама функция назад а направлять на ноут отлично значит вот когда принципе вот такая такая функция который тоже используется ранних 7х и какое основное преимущество первых она охраняет непрерывность диапазонов то есть нас данные перестает размазываться она очень равномерно режет то есть идеально режет диапазона да и она очень просто загну то есть метаданные очень легковесная нас все что нам нужно знать это количество серверов то есть функции на вход принимает ключ и количество серверов и выдает выдает номер сервер на котором находится этот ключ значит давайте немножко теперь перенесемся так сказать в наше настоящее посмотрим как это все таки в реальность всеми устроен есть вот эти системы вот вот это консистентная хэширование это подход amazon динамо главы насколько я помню использовался поныри итак угловые угловые и принципе благо как глава хэш изобрели пару математиков которые будут гугле но вот для того чтобы уже даже в динамо пайпер говорилось о том что для того чтобы обеспечить а на мерность распределения данных поддиапазонов при каширования да то есть при добавлении еще одного сервера хорошо бы ввести так называемые виртуальные сервера или виртуальный баки тогда то есть вот если вот кто вообще знаком с понятием bucket на мой взгляд очень пустующий термины to save здоровый какой ведро и потому что на самом деле там как бы предполагается что это хэш bakida но на самом деле это просто часть данных который хранится на одном сервере в чем идея значит мы берем и говорим что у нас есть не 3 сервера физик фактически а допустим десять тысяч серверов или 1040 уж бы еще изначально хочешь bass изначально задана тысячи 24 багета и когда и собственно дальше все то же самое то есть вы вычисляете хэш-функцию но попадает она не на физический сервер а на виртуальный сервер в bucket а виртуальный сервер приписывается уже конкретному серверу с помощью отдельной таблице приписки то есть у нас таблица с 1024 строк и в каждой строке записано что такой конкретный рыба-кит принадлежит какому-то конкретному серверу соответственно равномерно здесь обеспечивается изначально да то есть когда мы хотим перри балансировать кластер мы переносим какие-то рыбаки ты с на новую на новый сервер конкретное мы можем решить что вот отсюда мы берем три отсюда мы берем 5 да и таким образом обеспечить как равномерность процесса так его предсказуемость то есть вот смотрите опять же во время выборе балансировки важно откуда данные пережает и куда то есть важно избежать штормов потому что если вы добавляете новый сервер и внезапно весь кластер начинает на него сливать свои данные до или наоборот при балансировке вы начинаете данные перемещать между старыми узлами то вы создаете сетевой шторм это тоже влияет на производительность поэтому вот подход бакетами он позволяет это делать предсказуемо и downtime таким образом то же минимизировать down time или лески polite насти потому что что может ну как бы как делать коуч bass как делать мангу как делать другие вендоры которые используют подход с баки там берется инициируется процесс переноса баки то на сервер конце когда этот процесс завершается мы обновляем вот эта вот таблицу маршрутизации помните мы говорили что все вопросы они очень связаны друг с другом появляется таблица маршрутизации и надо где-то хранить ее надо а там арно обновлять ее надо на все клиенты а там ну атомарной либо канси хотя бы консистентной как-то доставлять да вот появляется вот эта кошка когда нам нужно обновить таблицу маршрутизации она повела ночь короткая потому что все что мы должны сделать это в момент когда мы завершили уже перенос до приостановить допустим нагрузку на конкретный bucket сказать что вот теперь для этого баки то праймари в находится вон там все запросы идут оттуда такую небольшое окошко потом можем переключиться на следующий баки вот собственно здесь картинка из имитацию на коуч которая иллюстрирует этот процесс вообще эту организацию здесь показано зеленым концептуально как раз приди на одна коллекция коллекция в копыте называется по случайному совпадению bucket вот что физически на 6 серверах находится на каждом из них изначально там сто семьдесят семьдесят один bucket то есть все тысячи 24 bucket они равномерно распределены при добавлении еще одного мы можем забрать каждого сервера на новый нужно чтобы обновить таблицу против и сюда вот смотрите помните вначале говорил что но и сколь системы начали с того что были максимально предсказуемые вот этот подход который только что описал он максимально предсказуем в том числе он максимально предсказуем при просто стандартной работе вот представьте себе у нас идет нагрузка на кластер нас добавляются новые документы за счет случайности хэш-функции можно более менее определенно говорить что каждый новый запрос он каждого вставка или обновление равномерно распределиться по всему кластер представьте себе что у нас сам поток сама нагрузка на запись она связана с каким-то временным рядам или данными которые связаны более менее с одним баки там есть вот представим за счет чего такое может быть вот смотрите изначально манга тебе тоже реализовал да да двойки у них также был подход кэшированием из бакетами в manga тебе 20 появляется rangers in space разбиение разбить просьбой sharding остаются bucket и bucket примерно 64 мегабайта конфигурацию становится равен кстати вот обратите внимание что здесь размер баки то он не ограничено там что слабак то фиксирована да а здесь наоборот размер баки то маленький вот но то каким образом ключ попадает bucket уже определяется них ушел а каким-то минимум и максимум да то есть вот у каждой баки то есть некий мин-макс изначально вот этот пакет админке до -7 из пяти тот от минус 7 степени до 25 и так далее да вот и соответственно если у нас bucket перерастает то есть ключей в нем становится больше чем 64 мегабайта и данных в принципе большим тонн диапазон разбивается на два под диапазона и создается новый баки опять же смотрим на это система с точки зрения роутинга появляется во первых два процесса первый процесс это сплита 2 подпроцессов ребаланс то есть у нас как bucket может разбиться на на две части то в принципе в мангой независимой процесса может как разбиться на две части так и уехать куда-то рых резко увеличивается собственно таблица маршрутизации да то есть у нас заселят того чтобы размер одного байка всего лишь 64 гигабайт у нас их могут быть там не то что тысячи а десятки тысяч до на каком-то среднем кластер но как зачем они все это делают зачем вообще появляется ранчпар тише вот кто кто может ответить зол на это с рук не вижу да чему манга тебе и сравнить практически хардинг портишь я буду разорвать что если в запросе базе данных указывается range то есть я вот тут то соответственно такой запрос идет на воле рока он более локальный верно то есть вот смотрите какой трейдов здесь происходит рядов между ускорением чтение ускорение записи помните я начинал говорить про то что у нас может быть нагрузка на записи распределение равномерно и чаще всего в реальном мире нагрузка на запись распределена неравномерно мы добавляем что-то в конец коллекция коллекция например автоинкремент или даты или и так далее вот в этой ситуации постоянно будет происходить с плиты или баланс и bucket of потому что в основном будет нагрузкой тина один конкретный последний баки но преимущество достигающую ся при этом это то что запросы которые допустим выбирают все значения to select и уже по определенным диапазон они начинают выполняться быстрее потому что не нужно делать вам придётся на весь шарди равный кластер то есть вот этот компромисс в mundo тебе документации явно представлены и этот компромисс на мой взгляд является трендовым то есть о чем я говорил о том что все но и сквозь системы они близится к тому чтобы лучше поддерживать сложные запросы то что в классических интер правильных системах там всегда было дают и за именно поэтому mongo db добавляет ранчпар тишине то есть вдвое киева нет 2 key он появляется что они здесь конкретно пишут что для запросов которые там не включает ключ нужно делать мо придется по всему кластер соответственно какие-то запросы забросы больше меньше случае если раньше парте shining используется другой недостаток то если у вас неравномерное распределение данных записи данных то вы попадаете на неравномерная нагрузка вашего classic поехали дальше значит дадим туда показу давайте разберем 3 пример как roach как рощ отличается от всех предыдущих тем что появился на позже изначально но искать системы си они называют называя себя new искали системой и соответственно казалось бы архитекторы должны были думать о том как максимально поддерживать сложные сколь запросы и транзакции власти и действительно подход к короче изначально отличается от подхода мангой и они вообще не пытаются реализовать хэш bass хардинг у них изначально только range bass charming то есть весь диапазон данных бьется а также как mongo db на баке ты каждый bucket размер пакета помогло он задается 5 конфигурации но по умолчанию на равен примерно 64 гигабайт опять также происходит с плиты до перебалансировка пакетов которые слишком большие и либо если одна она достала подержать себе слишком много bucket of the bucket и выносится на другую ноду то есть очевидно держи держа в уме о необходимой сложных запросов как road сразу реализуй транспорте шин но что интересно как к кровати хранить метаданные вот смотрите секунду вот тема хранения метаданных и роутинга mongo db изначально манга тебе если кто-то помнит вообще не хранит не поддерживала sharding вообще пользовался манга тебе 10 да ну вот там не было шарлин га вообще поэтому когда манка тебе реализовали sharding они информацию о собственно распределение стали хранить на отдельную выделенном сервере который называется mon bebe конфиг сервер то есть вот этот информация метаданных и для того чтобы сделать весь sharding прозрачным для клиента они реализовали так называемый манга роутер который автоматически работает sconfig сервером и направляет запрос на конкретный шаг более того в бинарном протоколе практически невозможно понять работаете вы напрямую с манга 100 раджим либо с манго роутерам но на сегодня конечно же уже версия там 36 манга отговаривает от использования запросов на сторож напрямую она говорит что надо использовать роутер и мы сейчас немножко поговорим о зачем и есть небольшая подсказка да то есть можно понять в запросе когда на сервер когда выполните запрос он пришел через роутер или нет потому что ставится специальный флаг протоколе что вот запрос пришел через роутер то есть возвращаясь к крауч манга тебе хранит информацию о пакетах на специальном конфиг сервере это три реплики обычно рекомендуется ставить стреле блики этого конфиг сервера когда происходит 3 балансировка сплит и так далее тукан фиксируй автоматически обновляется и раутер узнает об этом косвенно за счет того что ему приходит ответ от сто раз уже что извини дорогое у тебя от схема которых ты владеешь по и текущее представление о распределении данных она устарела сходи на роутер и выключи выключи новые представления вот у коуч bass изначально тебя система делалось только на горизонтальное масштабирование то есть они в принципе не ставили перед собой задачу делать сингл над системы и естественно и мне хотелось создавать какой-то отдельный сервер который бы хранил информацию о баки более того если посмотреть на кого на на как роща прошу прощение под капотом то под капотом они используют rock baby и кто знает что такое rock baby вообще я вижу две три руки но спасибо мага тебе делал доклад значит то есть срок забита просто storage engine да все что он умеет это создать там коллекцию сохранить в нем документ по его значению там есть то речные индексы но сейчас их оставим в покое то есть это мвд сторож спасибо ответственно используя рокс дебилка крауч все свои данные абсолютно все в том числе метаданные представляет как просто огромную коллекцию для того чтобы можно было вот эту сложную структуру во-первых таблицы до высокого есть таблицы есть матери ну там потенциально материала стилус есть системный каталог то есть эта информация о охраняемых объектах таких как собственно сами таблица юзеры пользователя юзеры роли привилегия и в том числе мета-данные опыта пологие кластер и вот все это хранится в одном этом глобальном пространстве key и value для того чтобы как-то дифференцировать данные разных таблиц первую очередь до каждое значение каждый ключ префикс суэца айди таблицы и аиде индекса да то есть нам же еще вторичный ключи надо поддерживать поэтому в каждом ключе добавляется ади таблица иди индекса соответственно с этой точки зрения как roach представляет информацию о роу тенге просто как отдельную коллекцию которая лежит в этом огромном кластере но распределение эта коллекция то есть вот где находится самой первой грубо говоря рут блок этой коллекции заранее известно что здесь интересно потенциально это означает очень большую нагрузку на этот вот блок поэтому они сделали что-то вроде системы dns и кэширует большую часть той с них есть метаданные 1 уровне метаданные второго уровня и большая часть метаданных второго уровня они курсируют на каждом узле есть вот понимаете вот как насколько связаны проблемы роутинга и проблема хранения метаданных то есть если метаданные слишком жирные а как роч изначально поставил себе делать задачу делать базы данных петабайт на ваш оба дать то есть какая-то идея вы хотите подгрести табачного масштаба мы даем вам совместимость по бинарному протоколу мы даем вам значит и те же фичи тот же сиквел но только большой и вот если у вас задача петабайты и базы данных делать a bucket у вас 64 мегабайта то у вас может быть сотни тысяч пакетов соответствие для того чтобы эту информацию кэшировать они просто вот эти в метаданные втором уровне все также хранят на каждом окажем сторож значит что что при этом достигается вот смотри вот смотрите мы поговорили про то что если используешь портишь никто более на ворами равномерной нагрузку на запись если использовать ранчпар тише никто потенциально более производительные чтение давайте приземлимся немножко в реальном мире у нас чтоб чего больше чтение или запись и реальный мир многообразен у нас с ним было не больше у нас может быть больше чтения нас быть может бы нас нет одного ответа да то есть вот какую мне мной сколь там базу данных выбрать блин вот мы сейчас посмотрим плюсы-минусы но что интересно ни то ни другое не отражает реальный мир в его полноте то есть смотрите когда возник домен гривен до когда возник нойз quelle появилась такая штука когда men гривен дизайн и идейная сколь она же какая что у вас есть только келью и все что вы можете сделать с этим киви ли это атомарном менять 1 документ да нет нормализация никакой поэтому люди но оказались вынуждены решать проблемы от омар насти больше на приложения как собственно прикладные приклады программисты выкручивались берется документ да это документ начинает представлять собой некий бизнес кейс допустим сопин basket да корзина покупателя вот единая сессия покупателя в магазине и эта сессия хранит в себе всю информацию о покупателе о собственно там где он был что наложу набрал в корзину и так далее за счет от омар насти обновления одного документа мы можем гарантировать что вот любой запрос который поделать он обновляет этот документ на всегда получаем его назад целиком да может быть там какая-то личная информация у нас нет нормализации зато все быстро и горизонтально масштабируется этот домен древне дизайну словно то на пальцах вот но понятное дело что такой подход он приводит в дублирование данных там нет никакой аналитики ну вот туда еще можете микро сервисную архитектуру навесить у вас еще будет зоопарк систем по которым размазанных данных данные но то есть это с точки зрения там опять же давайте вернемся к тому что носика и ничего не изобретает это просто популяризации вот в процессе этой популяризации мы там наступаем на множество кораблей да вот соответственно если вернуться к реальным сценарием то мы хотели бы что сделать мы хотели бы нормализовать наши бизнес сценарии и иметь связанные данные рядом все-таки то есть давайте еще разберем этот вопрос точки зрения вторичных ключей вот представим себе что наш запрос идет по вторичному ключу вот у нас есть такая красивая картинка да как roach который изначально нацеливался на сиквел хорошо поддержим сиквел а теперь нас запрос по вторичному ключу вторичный ключ как я вам описывал это будет грубо говоря о иди таблицы там ну представимся что там числовые идентификаторы там 152 номер индекса 3 доп опять же представимся это третий ключ в таблице и дальше ключи ключи этого индекса да для того чтобы зарисовывать запрос по вторичному ключу к крауч сначала сходит в баке от который содержит данные вторичного ключа по указанному диапазону допустим это будет вот здесь вот где-то получит значение первичных ключей и потом сделать мапри deus на кластер то есть мы ушли от mapreduce на только мы отложили его на один шаг но все равно для любого запрос по вторичным ключа нужно делать вам придётся хорошо это или плохо представимся что у нас хорошо нормализованный базы данных это означает что практически любой сценарий делает на придешь поэтому опять же из-за чего это возникает из того что базы данных не предоставляет прикладному программисту какого-то удобного механизма сообщить о связанности первичных и вторичных ключей то есть представим себе что мы могли бы сказать что вот смотрите вот этих данные вот этого вторичного ключа они очень часто используются вместе с этими данными первичными ключа и отдавать их разместим на одном баке те вот собственно и этот подход это подход валь ди би или я его в начале назвал космическим стал breaker партий shining так называемый и на мой взгляд является самым интересным передовым да то есть когда мы говорим о разных решениях то мы должны понимать что прогресс он но насчет еще только идет то есть как устроен sharding в альте у вас есть кто-то вот может быть знаком высоко нормализованные данные чаще всего в складах данных data were house of и там есть целый процесс по нормализации до extractor он формула трансформ лаут который берет данные из множества источников их преобразуют к единой схеме нормализует часть чаще всего очень хорошо нормализует чтобы можно было аналитику выполнять да ты опять же зачем нужна нормализация нужно нормализации для аналитики если мы стремимся к тому что у нас объединяется лтп и olap системой то нам нужно все-таки нормализовать данный но их нужен для множества других причин организовать тоже но в частности для этого и чаще всего в data warehouse их использовать так называемая высоко нормализованы snow snow flake imma do то есть снежинка яма снежинка схема звезда то есть когда у нас есть некие факты и есть некие размерности факт фактор был и domains and rebuild английский то есть что такой факт это информация о каком-то допустим конкретном заказе а что такое домен шин это либо продукты либо клиенты либо значит регионы вот это типичный домен то есть для того чтобы представить один бизнес кейс нам нужно информации как из фактов так и издание да то есть мы берем то есть факт это вся самая мякотка бизнес бизнес кейса одно для того чтобы ее как-то представить человека читаем и виды на нужно его за joy нить займемся таблицы то есть как это вообще все положить на старски ему значит в на на sharding и прошу прощения нам нужно каким-то образом первую очередь сортировать факты потому что фактов много да у нас может быть не так много там регионов как у нас много покупок она зафер очередь придумать как а каким образом нам объединить сделать локальными обращения к join который обращается к региону когда делать какой-то отчет нам нужно сделать так чтобы регионы которые связаны с определенным фактам а находились локально для этого есть два механизма первый механизм это то что мы да спасибо дастину вы первый механизм это то что мы регионы допустим сделаем автономными на каждом сорбе второй подход это если мы найдём способ каким-то образом допустим мы видим что основная часть продаж по этому конкретному региону увязать ее с описанием этого региона то есть мы как бы делим не протекционизм нет ни ко мне только факт табличку а нарезаем все связанные таблички находим способ нарезать все связанные таблички не для всех бизнес кейсов это доступно но вот в валь ди би конкретно есть возможность указать общей ключ сортирование для нескольких связанных таблиц то есть представим себе что у вас есть некий некие ну бизнес-кейс айдида вы его добавляйте в первичный ключ каждой таблицы и все таблицы у которых такое значение первичного это этого ключа они окажутся гарантированно одном сорбе то есть вот следующая грубо говоря шаг к тому чтобы сделать максимальное количество запросов локальным это вот такого рода сортирования таким образом значит все три вопроса выбора short функции выбора выбора места хранения метаданных и выбора способа доступ способа вычисления запроса не получается взаимосвязаны интересным является противостояние в этом смысле между манго и коучем у коуч если вы зайдете на вебсайт очень много критики man bodybeat каким он к тебе не правильно и они изначально допустим используют значит or did реплика сет то есть у вас у каждой репликой реплика сет есть актив есть пассив и вот нагрузка идет только на этих очень сложно и малым он mongo db топология что есть конфиг сервера есть роутинг сервера и так далее теперь у коуча изначально как и у допустим того же как роуч получается монолитная схема то есть в каждом инстансе есть определенные сервиса есть сервис вот давайте посмотрим на эту картинку есть квари сервис есть indexing service data service да то есть управлять этим казалось бы гораздо проще а теперь представим себе что у нас гораздо больше запросов с map рядился чем собственно данных то есть у нас очень большая нагрузка на вычисление поэтому последним коуч и появляется возможность масштабировать разные сервисы независима друг от друга то есть если мы сравним это с мангой да вот смотрите у манга отдельного роутера он занимается вычислениями шар занимается хранением да здесь мы говорим что у нас проще схема но мы приходим к тому что каждый каждый слой можем независимо масштабировать у все скота 45-50 минут хорошо значит давайте подведем какой-то итог по дизайнам вот зеленым я попасть попробовал подсветить какие-то дизайнерские решения которые выигрывают на фоне остальных просто по картинке по самой табличке мы не видим что нет ни одного решения с условно идеального да то есть которого было бы все зеленое вот лучше всего наверно на этом фоне выглядит последняя табличка в валь ди би но проблема валь ди би ее сложность все равно в сложность для я даже вписываю тарантул потому что мы похожую схему использую ложность вроде без состоит в том что вы должны заранее подумать то есть мы приходим потому что но и сколь система перестает быть простыми вы должны заранее подумать какой у вас бизнес сценарий и организовать хранение данных соответствии с этим бизнес сценарием да вы получаете нормализацию вы получаете высокую производительность но вы должны очень много думать заранее вот подход к учат тут я к сожалению не успеваю рассказать про реализация вторичных ключей в коучем позволяет коучу гораздо быстрее выполнять вторичные ключи запроса повторюсь выключать ключам чему manga manga вторичный ключи а не класть не shard local у коуча events или consistent globo вторичной ключи вот этот подход к говорить нам о том что мы придем к тому что все системы все но и сколь системы будет поддерживать в грубая все возможности да то есть полный портфель иванович мы придем тому что у нас разряд конвергенция рынка и все вещи будут у всех но в результате система с которой нам придется работать будет уже без безумно сложными то есть уже мангу действием на сейчас мы посмотрим он гораздо сложнее был там со стен время к сожалению закончилась остальное я думаю в кулуарах аплодисменты и у нас есть буквально несколько минут на вопросы я напоминаю один вопрос от человека все дополнения и уточнения в кулуарах и нужно будет выбрать лучший вопрос окей так а прошу вас да это был сложный доклад но вы знали на что шли спасибо за доклад мне тоже страшно вопрос у меня такое большинство но если решение фактически становится полезно только тогда когда у вас огромный объем данных по сути скажем все остальные типы решений могут покрыться подглядки или mais келли то есть фактически вы рассказываете доклад для 5 кампаний в нашей страны а зачем тогда мой сколь добавляют горизонтально масштабирую для тех же пяти компаний но это интересная да как что за аудитория пришла на доклад но зачем вы здесь я думаю что все таки мы горизонтальном масштабировании придем другое дело что действительно может быть средней там пользователь будет это видеть через облака через сервис другой тренд какой можно дать комментарий все-таки задача моего доклада было сказать проблема выбора показать что нет идеальной системы и показать что большинство систем добавляют новые фичи усложняется за счет этого вот я надеюсь я смог отобрать им можно вопрос раньше вас хотелось бы может быть в двух словах услышать перспективы развития вот таких баз данных именно с учетом ускорения памяти удешевления памяти и то есть как вот это влияет на развитие таких ты вылез тораджей значит если вы да вы говорите продан 3dx point in where am the это никак не связано я бы смотрел в первую очередь если смотреть на какие-то революционные технологии который позволил бы нам вернуться к вертикальному штабе ранее это gpu вот если gpu с там четырьмя там тысячами чипов на одном креста но на одной плате значит стану достаточно универсальными для того чтобы на них эффективно выполнять да и тбс workout то мы вернемся к вертикальным масштабируем систем сожалению liksys ускорение индексации само по себе нам не нас не не упростить то есть мы останемся на вертикальную на горизонтальным стаде равом кабисов те все идут в горизонтальном собирание будут идти еще довольно долго и там ну опять же intel вирус smd это там множество трендов но главные тренды от и гипер конвергенция которая не позволит вернуться то есть короче рэнд состоит в том что большинство систем будут горизонтальным масштабируемым в том числе мой сколь и posts прошу вас следующий вопрос спасибо за доклад я хотел бы ответить на вопрос от предыдущей у человека который спрашивал дело в том что текущая технология похоже технологиям и не обязательно на должно именно для базы данных применяться то же самое хранение файлов на множестве серверах там тоже по сути делаешь родирование там тоже нужно выбирать где хранить как и получают вопрос отвечу хорошо базу данных и храните метаданные ваши файловый кластерный файла висел в базе у нас очень много возникает проектов которые пытается изобрести базу данных от сиди там я не знаю uber на это с праздником вернет вам использовать свои там за кибер тоже базу данных по сути вот у вас странные файлы системы типа sefa под капотом имеют свою базу данных это все связано с тем что нету бесплатных на рынке решений которые взял и поставил если бы для таких систем был сиквел light распределенной я думаю что никто бы не не грабил бы там своей со своей база просто сигнала это нет который бы был бы надежно как старая шутка что в любой достаточно большой системе можно найти глючно наполовину сделано имплементацию lisp коллеги прошу вас вот новый старый fondation тебе вообще никак не говорит как шарди данные то есть этом самом все должен делать и у меня связи с этим вопрос как часто в продакшене приходится решать данные нужно ли вы в своем приложении быть к этому готовым я неправильно делаю доклад и то что люди которые курили коуч допустим лимонка они ну как бы знают насколько это сожалению детали которые требуют от drops of a вообще от админов а то псов знание погружения насчет foundation они используют за схему что как роуч то есть там под капотом огромный cave люстр со своими плюсами минусами и конкретно foundation я думаю что за счет того что они стартовали гораздо раньше чем как ростом больше минусов чем плюсов и в принципе я уже даже не знаю как роща от foundation был в свое время обладал отличным маркетингом за счет этого не отлично продались и как бы но насчет технологии . у нас есть время на один последний вопрос прошу спасибо за доклад сейчас говорили про иски light вот что вот если он был распределенную было бы здорово я читал про такую штуку как арклайт база которая построен на базе скилла это но является распределенная можете про него сказать что нет итак какой вопрос был лучший мне понравилась прога пуну которую тема про горизонтальному stabber не вопрос кто задавал вопрос право порт вот так все замечательно еще раз аплодисменты спасибо спасибо"
}