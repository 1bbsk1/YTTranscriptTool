{
  "video_id": "Ekf50vlI8Dc",
  "channel": "HighLoadChannel",
  "title": "Квест по синхронизации аналитического и оперативного хранилищ в реальном времени / Андрей Комягин",
  "views": 267,
  "duration": 3222,
  "published": "2024-04-17T01:10:21-07:00",
  "text": "друзья Всем привет Меня зовут Андрей я из компании Stem labs мы строим большие нагруженные системы класса Big Data А сегодня я вам предлагаю всем вместе со мной пройти увлекательный квест по синхронизации аналитического и оперативного хранилища в реальном времени самое важное без потерь когда у тебя сотни и более терабайт данных а кликер кликер тут вижу кликер отлично ну прежде чем погрузиться в эту увлекательное путешествие давайте сделаем небольшой обзор предметной области Чтобы понимать в принципе семантику данных нагрузки и так далее Итак предметная область это большая я бы сказал глобальная то есть она работает масштаба всего мира треком система для обеспечения единого процесса отслеживанию статуса различных групп товаров то есть есть товары и это фактически по экземплярной контроль движения товара в логистических цепочках начиная от процесса производства далее там у нас логистика идет перевозки дистрибуция и так далее все заканчивается либо ритейлом либо списанием либо другим конечным состоянием в которое переходит товар и фактически это является его концом жизненного цикла Если говорить о архитектуре самого процессинга Да это не предмет нашего доклада конечно но важно понимать в принципе как устроен процессинг в принципе абсолютно типовой есть некий источник событий или документов эти события поступают входящую очередь ну и далее собственно стоит пол обработчиков которые по сути реализуют консьюмеры да то есть они реализованы как консьермеры они высчитывают сообщения задачи из очереди производят какие-то бизнес логику а что с собой представляет бизнес логика фактически это просто смена состояния товара при движении товар меняет свое состояние меняет владельцы текущего меняется у текущий статус и так далее некий набор атрибутов Ну и в конце цепочки у нас естественно оперативное хранилище то есть нам состояние обновленное нужно сохранить мы используем в качестве оперативного хранилища mongo db кстати в прошлом году кто был на моём докладе я подробно рассказывал про топологию кластера оперативного хранилища самое интересное в этом докладе было то что мы эту тапологию меняли на лету без Down тайма но к сожалению пришлось это делать под так как объемы выросли в общем Посмотрите будет интересно Если кто-то хочет познакомиться именно с архитектурой такого решения а мы идём дальше ну так у нас регламент ограничен по времени я конечно тоже обозначу рамки о том о чем я не буду рассказывать в рамках этого доклада То есть я не буду естественно подробно останавливаться на типах БД их различиях не буду делать обзоры аналитических СУБД тут тоже Каждый выбирает под себя решение под свои требования не буду подробно останавливаться на архитектуре субдем Ну естественно мы будем касаться определенных аспектов это неизбежно надеюсь что все в зале понимают что такое olab и oltp сценарии и нагрузка Какой профиль для них характерен тоже не буду здесь делать какую-то лекцию по этому поводу Ну и вот Вишенка на торте Да Многие слышали такую аббревиатуру она означает дата трекинг фактически контроль изменений в базе данных и об этом мы поговорим достаточно подробно но я не буду делать обзор реализации различных БДС мы будем рассматривать эту технологию только в разрезе монго db Итак описание задачи то есть задача формулируется максимально просто но по факту простой не является это синхронизация оперативных и аналитических данных а Поднимите руки у кого вообще поднят отдельный аналитические кластер отлично значит Эти люди все понимают всю боль и понимает почему не надо гонять аналитические запросы на оперативных данных отлично остальные видимо с этим знакомятся всем будет полезно интересно наше исходные требования но первое самое важное требование это конечно же надежность то есть любая синхронизация она должна быть надежной то есть мы должны сходиться в ноль то есть передавать данные в аналитическое хранилище без потерь Мы также должны обеспечивать постараться по крайней обеспечить минимальное отставание данных в аналитическом кластере вот оперативных данных обеспечить естественно высокую отказоустойчивость всего решения и постараться чтобы вот это вот синхронизация не создавала дополнительный паразитной нагрузки на оперативное хранилище то есть минимизировать вот эти все издержки Ну Особенно это важно часы когда нагрузка наибольшая дочь у нас есть пиковая нагрузки часы на и больше нагрузки и так далее Вот хотелось бы чтобы эта нагрузка была приемлемой исходные данные что мы имеем мы имеем оперативное хранилище и в части нагрузки на это хранилище нагрузка на запись это 10.000 операций в секунду нагрузка на чтение чуть побольше полтора-два раза 15-20 тысяч операций в секунду и размер дата Сета у нас 150 плюс терабайт зачем я вот делал обзор предметной области для того чтобы вы понимали вообще в принципе О каких данных мы говорим что мы отслеживаем то есть мы говорим о истории движения товара о истории с менее смены состояния определенного товара мы используем для хранения вот этой истории фактически формат json естественно лист объектов Просто и то есть это тоже важно для понимания потому что это важно для процессинга с точки зрения процесса Итак начинаем наш долгий Путь самурая он был длительным наверное делали много попыток реализации подходов смена подходов продлилось наверное вот эта вся эпопея чуть больше года но мы таки сделали это и пришли целевому решению оно нас сейчас полностью устраивает начинали мы как и начинают все в принципе то есть у нас было просто оперативное хранилище все запросы аналитические мы естественно шарашили прямо напрямую в оперативное хранилище Ну как вы понимаете ничем хорошим это не закончилось И с постепенно объема росли и в общем это все перестало работать и у нас были большие проблемы далее мы использовали некую форму и тела то есть мы по сути имплементировали интервальные выгрузки чтобы сделать вот эту переливку более контролируемой но опять же это решение нас не устраивало потому что оно упиралось в потери которые были реальными нам пришлось городить всякие сверки перепроливки В общем это был кромешный ад и мы тоже решили что надо что-то менять мы так жить не можем дальше Ну и последнее вот Вишенка на торте вершина айсберга это cdc это то что собственно мы сейчас используем Давайте подробно остановимся на каждом из этапов эволюции пройдем его буквально быстро Итак трация номер один прямые запросы в оперативное хранилище Да с этого все начинают оперативные данные не появляются единомоментно да то есть мы можем себе позволить эту историю когда во-первых это допустим не Big Data или не высоконагруженное решение объема небольшие в принципе на этом можно жить и не городить никакой огород в виде там отдельного аналитического кластера Мы тоже с этого начали то есть в погоне за функциональными возможностями никто не думает про архитектуру как правило К сожалению поэтому на старте проекта мы действительно приземляли все аналитические запросы прямо в оперативное хранилище плюс Здесь тоже они на поверхности все это действительно дешево и сердито я бы сказал не нужен отдельный аналитический кластер Ну и можно быстро выводить функционал на провод то есть новый отчеты клипать как горячие пирожки в общем-то практически бесплатно практически минусы они в принципе тоже очевидные то есть постепенно Особенно с ростом объемов данных у нас все больше и больше оказывало влияние паразитная нагрузка на оперативное хранилище то есть паразитной нагрузка я понимаю аналитический запросы бизнес требовал все новых и новых отчетов а это значит что надо было создавать индексы дополнительные индекс У нас тоже разрастались как Снежный ком по мере роста объемов данных в оперативном хранилище все больше и больше это оказывало влияние на вставку Ну и самое наверное на мой взгляд ужасная вещь Это смешение лап и запросов То есть фактически Ну как у нас нету Серебряной пули Да все понимаете да Что любая база данных она предназначена для определенных целей мозга тебе она не предназначена для аналитики то есть ее можно использовать но она для этого не предназначена такой подход оказывал естественно влияние на производительность основных бизнес-процессов у нас были деградации постоянно какие-то оберты что что-то медленно процессится никого это не устраивало ну и соответственно Я уже упомянул момент связанный с деградацией по мере роста датасета размер датасет у нас стремительно рос ну и соответственно деградации тоже У нас нарастала нагоняла нас мы пошли дальше и на итерации 2 Мы подумали Все хватит и попросили отдельный аналитический кластер заказчик богатый В общем он мог себе это позволить Нам его предоставили Мы подумали Нам надо как-то в него переливать Данные как это делать самый простое решение в лоб это интервальные выгрузки берем и все дельты изменений за интервал переливаем периодически делать это можно контролируемо управляемо то есть мы можем выбрать какие-то часы когда нагрузка на систему относительно небольшая и эти запросы переливки гонять плюс Здесь тоже вот очевидно 1 я перечислил то есть контролируемая нагрузка управляемая второй плюс это то что мы не гоняем больше у лап запроса на оперативных данных соответственно можно все индексы удалить аналитические все у нас как бы база выдохнула при вставки по крайней мере у нас деградация ушла и это уже тоже было достижение мы порадовались но по мере эксплуатации этого решения нас это решение эксплуатировалось Достаточно долго там вылезали различные болячки то есть интервальные выгрузки у нас базировались на так называемой дате последние операции да то есть товаром делаются операции по мере его движения логистической цепочке и подайте последний операции мы делали выгрузки то есть брали интервал и выгружали все операции по дате последней операции То есть это бизнес дата за этот интервал но проблема в том что иногда происходили обновления в базе данных при которых это бизнес дата не обновлялась А значит это Дельта изменения она не попадала в интервальную выгрузку и фактически мы не видели это изменение и не передавали в аналитический кластер это первая проблема вторая тоже очевидная проблема Это то что нет нормальной обработки операции Дели Да если что-то удаляется полностью то в Дельте этого нет И нам пришлось изолентой сбоку прикручивать в общем-то Костыль который нас вообще от слова совсем не устраивал и не устраивал никого в том числе потребителей поэтому мы помучились реализовывали вот эти вот различные сверки переливки доливки и так далее В общем надоело и перешли к следующему этапу решили все переписать итерации 3 финальная или cdc против TL а cdc дата кепча то есть это механизм который фактически построен поверх журнала опережающий записи Ну все его знают как волн монго тебе это называется оп и Он позволяет вам отслеживать абсолютно все изменения любого типа который происходит с базой данных доступ к журналу опережающей записи есть через API есть там набор инструментов которые вам позволяет журнал читать и самый большой плюс вот этой cdc именно реализации Вот именно в mongo debino как впрочем в многих других базах данных это то что эти изменения отслеживаются на уровне протокола репликации репликация у нас уже есть да то есть естественно мы не используем там Standalone какие-то инсталляции баз данных у нас кластер в кластере есть репликация и поверх репликации накрутили вот эту штуку которая называется cdc Классно Классно никакого оверхеда потому что в принципе Она уже и так есть mongo db реализации cdc называется Change и Ну как я уже сказал Она работает вокруг блога это отдельно выделенные AP То есть вам в принципе не надо самому читать оп есть специальные API у mongo db которое позволяет вам трекать все изменения Вот кстати пример на языке пайтон приведён просто для простоты Значит есть коллекция инвентаре в базе данных называется на ней метод Watch мы указываем режим работы лукап об этом чуть позже поговорим что это означает этот метод нам возвращает курсор и собственно по нему и оперируемся читаем все изменения которые происходят в базе данных очень крутая фича именно вот в чень стримах то что можно отслеживать изменения не только в одной коллекции коллекция напомню для тех кто не в курсе это аналог таблицы в обычной базе данных Но и на уровне всего дипломата То есть можно там набор коллекций отслеживать изменения весь дипломонд трекать и так далее То есть очень классная вещь плюс утилизируют всю мощь кто работал с монго тебе знает что есть такая штука классная как evergational Ну если кто-то с обычной базой данных То есть это грубая Да здесь это чуть мощнее это целый фреймвок который позволяет делать фильтрацию агрегацию и многие другие вещи на данных базе данных много коннектор да то есть Казалось бы что бери используя пиво У тебя же есть API которая позволяет тебе использовать мы продолжили наши исследования обнаружили Что вендор предоставляет компонент который называется монго коннектор и этот компонент реализует два режима момент два режима первый режим это Sync который позволяет читать данные из топика в Кафки и сохранять эти данные mongodb ну этот режим классный но нам не совсем подходит здесь хотя мы уже Смотрим Да по чикавка пачкавка у нас в архитектуре есть нам подходит то есть Нам ничего изобретать заново не нужно с точки зрения архитектурного ландшафта и второй режим второй режим Он собственно про то что мы делаем Это режим Source и этот режим позволяет отслеживать изменения с помощью API через три мозг и публиковать все эти изменения которые собственно есть публиковать собственно в топике апочкавка классно решили Почему бы не попробовать это решение потому что уже написана интеграции с той же Кафка Да и попробовали немножко остановимся на параметрах и настройках mongo коннектора что здесь есть что здесь на что важно обратить внимание кстати документация по коннектору не очень в интернете и многие вещи приходится просто Анализируя Исходный код узнавать Ну понятно что есть настройки подключения к СУБД есть можно указать базу данных на которые мы должны тратить изменения можно указать до уровня коллекции на которой мы должны отслеживать эти изменения можно настроить mapping топиков куда Какие изменения писать то есть допустим коллекция пишется в топик а там коллекция B пишется в топик б и так далее более сложные какие-то маппинги естественно в коннекторе уже поддерживается из коробки пакетный режим обработки то есть батчинг то есть он копит изменения и сохраняет их в кавку уже в виде пакета пакет Можно конфигурировать как по количеству сообщений в этом пакете так и просто по времени ожидания изменений то есть подождать допустим в течение секунды эти изменения Все отправить в топик Я уже сказал ранее что-нибудь стримс реализует и использует всю мощь агрегеевич вот есть следующий параметр Я здесь прямо привёл конкретный пример чтобы просто долго не рассуждать а сферических конях вакууме как это вообще использовать то здесь прям написан фактический запрос но в данном случае что что делается что происходит Мы фильтруем по префиксу название коллекции то есть мы подписаны на изменение сразу нескольких коллекциях по префиксу префикс задается видео регулярного выражения далее мы анализируем все документы в этих коллекциях И если в документе присутствует политики Type со значением 0 то такие документы нас устраивают все документы остальные нас не нам они не нужны и они отсеиваются плюс что мы еще делаем прям сразу же за один присест это мы добавляем статические метаданные то есть в данном случае мы добавляем поле storage Main чтобы дальше дальнейшие источник потребитель вернее мог использовать эти метаданные для своих нужд очень классная фича мы ее используем следующий важный параметр он самый внизу в таблице это Change streaming Классная штука это фактический режим работы коннектора по умолчанию Если вы не задаете этот режим Он кстати не задан То есть если вы скачаете конфиг просто у коннектора никаких режимов у него не будет и по умолчанию вам стримы возвращают просто Дельту изменений То есть вы имеете Дельту изменений Вы должны каким-то образом ее из этой дельты получить измененный документ мы долго плясали с бубном искали разные другие режимы нашли режим апдейт лука включили его там есть определенный артефакты о которых мы можем поговорить чуть позже но если успеем но эта штука нам дала фактически измененный документ не просто Дельту изменений Да и описание этих изменений которые случились А уже измененный полный консистентный документ классно естественно коннектор реализует логику обработки ошибок тоже уже все из коробки не надо это реализовывать свое приложение есть так называемые Эра толеранс фактически режим обработки ошибок ошибочные документы можно скидывать в отдельный тупик тоже здесь указывается в конфигурации тоже очень удобно а далее можно очень удобно читать тот же AppLock То есть уплот можно читать полностью а можно читать с определенного там штампа с определенного времени это очень важно Например если вы делаете какой-то репроцессинг повторную обработку и так далее То есть вы указываете конкретное время с которого надо прочитать оплок Да блок репликации Локо опережающей записи и с этого времени собственно все изменения перерабатываете плюс продюсер который встроен естественно монго коннектор Он поддерживает импотентность не нужно париться об этом на уровне приложений патентность уже поддерживается в самом компоненте включаете режим депотентности у вас демпатентная запись тоже очень удобно Итак сравним коннектор что же лучше да По каким критериям будем сравнивать Ну критерий вот мы выбрали вот такие вписали табличку Первая колонка это собственно критерии сравнения и остальные две колонки это собственно мы сравниваем голый и коннектор первое это собственно Требует ли ваше приложение реализация там интеграции с change 3 маме да то есть нужно ли знать особенности API их как-то учитывать на уровне своего приложения в случае использования голого пи Конечно же да В случае использования коннектора нет потому что ну всё очевидно вам придётся работать только самой кавкой А все изменения будут в кафку писать коннектор удобно поддержка режима Синг да естественно это фича коннектора поэтому коннектора здесь плюсик у API здесь минус Ну понятно что это немножко другая фича но тоже полезный бонус для каких-то других задач может пригодиться масштабирование из коробки опять же в случае использования API никакого масштабирования у вас нет И вам придётся весь келинг всё демпфирование нагрузки и так далее Всё писать на уровне вашего приложения а здесь есть по сути Кафка вам позволяет масштабироваться мы можем топик от протекционировать соответственно увеличить количество консьермеров все Вуаля мы как бы повысили горизонтальную масштабируемость работаем быстрее продемтирование я уже сказал То есть естественно входящий поток непредсказуемый есть пиковая нагрузки есть когда нагрузка минимальная очередь в принципе позволяет вам сгладить эту нагрузку это удобно не нужно это городить уже в архитектурном лапша ландшафте вам уже сам коннектор предлагает это решение навязывает какую-то конкретную архитектуру в принципе если у вас есть Кафка во сне интегрируетесь Это очень полезно с точки зрения сложности и стоимости эксплуатации инфраструктуры конечно Просто голый API Это дешевле в какой-то степени это проще но я бы не сказал с точки зрения если используется коннектор то здесь конечно дороже потому что вам придется поднимать отдельный кластер отказоустойчивый Кафка плюс закладывать какие-то мощности под коннектор потому что коннектор тоже где-то должен работать ну и По максимальной гибкости интеграции тоже такой абстрактный термин Да конечно API дает вам какую-то гибкость потому что вы можете сделать все что угодно Вы можете реализовать свой коннектор какие-то свои фишечки но в целом как бы при использовании современных подходов проектированию архитектур когда везде используется синхронщина Кафка и очереди коннектор зачастую подходит в большинстве кейсов Давайте немножко посмотрим на формат самого события cdc Event как он выглядит но mongo db в принципе все данные представляет формате json и хранит Джейсона То есть это принципе документ ориентированная база данных и сами ивенты тоже сидисишные они тоже формате json сам cdcent выглядит следующим образом нас интересует здесь поле плейлот до схема она простая то схема это строка до у нас хранится строка строка опять же этот же сон То есть как выглядит естественно есть идентификатор самое важное атрибут наверное один из самых важных атрибутов в пилоте это Operation Type Я уже сказал Да у нас была проблема помните с телом что мы не могли нормально реализовать логику обработки делитов так вот здесь просто Delete это просто есть там сердце делит и так далее все их здесь можно реализовать свои обработчики на все случаи жизни фактически даже удаление коллекции здесь можно отследить и обработать нам это не требуется но тем не менее API Это позволяет делать Далее идет Собственно сам документ Full document это g-son который представляет уже измененный документ То есть если у вас включен режим апдейт лукап в это поле будет записан полная полная измененный документ забыл упомянуть про кластер тайм кластер тайм тоже передается это фактически время когда это изменение случилось когда события произошло фактически на базе данных тоже важный важный параметр мы его используем для слайсинга данных то есть часть данных нам не нужна в документе мы ее отсекаем по там штампу частью данных мы Передаем ее дальше нашим аналитическим потребителям естественно есть ключ и вот последние структура апдейт description это собственно описание тех документов то что изменилось да то есть какие поля у нас добавились Какие обновились какие удалились и так далее Все вот эти все метаданные вы тоже можете использовать в своей бизнес-логике при обработке они все тоже есть вы видите Как выглядит вообще архитектура решения тоже упрощенная естественно я не буду здесь какую-то монструозную архитектура показывать просто простая архитектура понятная всем есть база данных то есть наша оперативное хранилище на базе mongo db дальше идет собственно коннектор который работает в режиме Source Mode коннектор пишет все cdc ивенты во входящий топик Кафки кафкаин Далее идет наше приложение наши микросервис который занимается там дополнительный бизнес логикой по обработке этих cdc ивентов об этом чуть позже тоже скажу буквально кратко и после процессинга уже нашего пост процессинга все события попадают в результирующий топик который мы называем Кафка Out Ну и из кафкаут уже понятно все эти данные могут вычитывать наши аналитические потребители которые здесь на схеме представлены в виде консил номер один консьер 2 и консилер консьюмеров может быть много нижняя часть диаграммы здесь вот интересный квадратики используется Это для специального режима работы cdc процессора то есть полностью отказаться от репроцессинга и какой-то ручной синхронизации нам не удалось все равно иногда требуется что-то руками Перри синхронизировать но мы это полностью автоматизировали то есть мы сделали специальный режим ручной синхронизации и повторной обработки которые фактически это набор задач которые поступает нас в специальную контролирующую очередь которые тоже построены на базе Кафка который называется Кафка Control и процессор вычитывает эти задачи исполняет их и отправляет опять же в кавка вот аналитическим потребителям то есть синхронизировать данные по какому-то товару по его идентификатору или конкретное состояние все это можно делать с помощью вот этого микросервиса он не обязательно должен быть вашим ландшафте возможно вам достаточно просто поднять коннектор и работать с cdc ивентами напрямую в вашем приложении такое тоже сценарий возможен возможно он там на 90% всех устроит Но в нашем случае потребовалось дополнительное обработка Что делает cdc процессор в нашем случае Тот который на схеме тот микросервис который мы допилили вот в эту инфраструктуру Ну первую очередь он считает сырые cdc ивенты из топика это понятно дальше мы реализовали функционал связанный с фильтрацией есть дополнительная фильтрация реализованные на уровне вот этого постпроцессинга мы здесь отсеиваем ненужные документы что-то модифицируем есть слайсинг я говорил что нам вся история изменений не нужна и наши аналитическим потребителям она не нужна более того как бы мы не хотим забивать каналы передачи лишней информации поэтому мы слаще весь трек изменений и начинаем передавать эти изменения начиная собственно с кластер тайма с того времени когда это изменение случилось да то есть все только отправляемся только новые события а то есть выделяем фактически конкретный набор объектов в листе по диапазону по временному диапазону делаем Ричмонд то есть обогащение по справочникам То есть это удобно и это нас избавит от головной боли когда там допустим наш аналитический потребители полезут в эти же справочники Будут их в общем-то до 10 не хотелось бы поэтому мы сразу все обогащаем и отдаём им уже обогащённом виде Да иногда это нехорошо Но в нашем случае это удобно добавление технических метаданных изменение формата то есть вот мы нашли объектов например распиливаем на отдельные объекты и Передаем это уже виде отдельных объектов результирующую очередь из которой читает потребители опять же удобно и режим репроцессинга плюс на Вот это cdc процессор можно накрутить дополнительные плагины фактически Добавить дополнительный функционал и в общем-то расширить его возможности мониторинг Да важная составляющая любого решения запускаться в прод не имея каких-то метрик в общем-то это самоубийство фактически естественно Мы тоже прикрутили мониторинг А я здесь показываю только часть его по сути естественно мы мониторим все новые узлы то есть сам коннектор что он запущен работает не упал Но самое важное это мониторинг очередей то что у нас фактически не растет лак во входящей очереди Это говорит о чем что процессинговое приложение оно позволяет вычитывать всю входящую нагрузку То есть если бы лак возрастал То есть он не успевал обрабатывать Это означает что его нужно масштабировать поставить дополнительный инстансы для процессинга Ну и собственно исходящие очереди та же история аналитические потребители тоже должны успевать вычитывать все то что мы им отгрузили иначе собственно тоже те же проблемы будут очередь не бесконечная там Конечно есть ретеншенные все такое но надо вычитывать надо успевать на это все повешенное естественно Куча алёртов и в принципе служба эксплуатации там дежурная смена все это может отслеживать и удобно контролировать подведем итоги Итак что же мы построили вспоминаем исходные требования к задаче что у нас было да то есть первое требование это надежная синхронизация оперативного хранилища с аналитическим кластером без потерь естественно Если сравнивать будут только подход который использует и тел и cdc но случае etl что было ранее сказано У нас есть завязка на бизнесовый атрибуты на бизнес дату последней операции которая не всегда обновляется К сожалению бороться с этим бесполезно то есть какие бы там организационные моменты не принимать кто-нибудь все равно что-нибудь обновит руками в базе данных это возможно и плюс нельзя гарантировать что какая-то бизнес-логика тоже гарантированно обновит эту дату соответственно тел также отсутствует нормальная обработка делит операций тоже никого это не устраивает cdc все это есть то есть мы получаем все абсолютно все изменения документа Независимо там обновилась какая-то бизнес-атрибутика или нет И естественно все тоже самое аналогичным образом унифицированным мы получаем и событий типа делит то есть оно Ничем не отличается с точки зрения cdc от других событий минимальное отставание данных в аналитическом кластере конечно и тело это уже не про минимальное отставание то есть там изначально в решение заложено интервальное то есть на 4 перспективная выгрузка то есть мы грузим интервалами которые находятся в прошлом оно по определению отстает cdc мы все изменения получаем в масштабе близком к реальному времени Да конечно ваш процессинг должен успевать это все обрабатывать но в целом проблема этих нет по поводу минимальной равномерной дополнительной нагрузки до и тел уже в принципе неплохое решение точки зрения что мы в принципе убрали основную паразитную нагрузку но тем не менее Вот этот дополнительный Запрос который позволяет получить Дельту изменений Да по интервалу Старт Date Time And Date Time он остается и от него никуда не избавиться на уровне cdc вся эта магия черная не нужна и можно спокойно работать на уровне протокола репликации читать изменения в блоге все то есть это проблема тоже уходит как класс потому что в cdc она не актуальна и высокая отказы устойчивость при сбоях в случае TL там естественно pip-line состоит из нескольких э обработчиков последовательных и в принципе возможно отказы то есть много достаточно точек отказа и не всегда понятно что отказало и есть ли у нас потеря именно из-за этой по этой причине возникают всякие сверчные отчеты и так далее приходится что-то дополнительно сверять чтобы убедиться что потеря нет Или они есть это очень сложно В случае cdc в принципе это все проще плюс есть встроенный механизм Эра толеранс То есть если что-то где-то споткнулось какой-то битый Документ и так далее то возможно повторная обработка можно посмотреть что это за документы можно отмотать уплот назад то есть установив тайм штамп с которого читать это всё 3 процессе То есть это гораздо более нативная история более простая Итак финальные итоги мы прошли три операции до целевого решения занял у нас это практически год как выяснилось Ну по крайней мере для нас cdc оказался лучше чем просто банальное тело лоб Ну и мы считаем что в принципе для решения Вот таких разногруженных класса bigdata это в принципе более правильный подход мы сравнили использование голого API через стримов с использованием коннектора Да тут свои плюсы и минусы У каждого подхода есть и нельзя однозначно сказать что это чисто плюс и всегда Используйте коннектор Нет я так не могу сказать но здесь Каждый делает выбор сам под свои задачи под свои требования Ну и в нашем случае тоже не обязательный шаг это опция мы разработали предложение cdc процессор которые делает дополнительную бизнес-логику то есть пост-обработку поверх сырых событий тоже вам это может понадобиться может не понадобиться всё зависит можно часть э этих фич закрыть просто функционалом коннектора то есть коннектор в принципе достаточно навороченный и может часть ваших требований покрыть Всё Всем спасибо Ну и напоследок хотел сказать что все трюки и все вот эти вот фишечки имплементированы профессионалами не повторяйте на своём опыте а Хотя может быть и стоит повторить Спасибо Андрей Друзья вот этот qr-кодик вот этот QR котик по нему надо оставлять свои оценочки свои комментарии спикером пожалуйста на свежую прям голову прям сейчас вот Достаньте телефончики и напишите нам пару строк это очень важно спикеру Это очень важно организатором для того чтобы собственно собирать обратную связь и формировать все лучшее для вас Давайте начнем справа налево потихонечку пойдем по вопросам Добрый вечер о Добрый вечер Спасибо отличный доклад систематика консалтинг сапонико Денис у меня такой вопрос Ну мы идея с валогом прекрасная и отличная но мне кажется Все упирается в кафку В итоге у вас написано в реальном времени вы получаете события но Кафка скорость кавки зависит от того какой скоростью из неё читает но у нас свой cdc процессор тоже написан значит и тут есть очень серьёзная проблема с этим коннектором в том плане что в случае ошибки Он откладывает это событие в отдельный Лог и если у нас есть события на изменение одного и того же объекта ABC и события B упало влога ошибки мы не имеем права накатывать события C иначе у нас порядок обновления атрибутов нарушится и объект будет не консистентным в отчётной системе и всё поплывёт Мы сейчас с этой историей бо выстраивая отдельную очередь изменений на этого конкретного объекта Интересно как с этой проблемой боретесь вы Спасибо Да я понял Здесь наверное больше это завязано именно на семантику вашего бизнес-процесса на семантику ваших данных То есть у нас вот такого нет хотя да мы тоже ловим эти биты документы битые объекты но мы не выстраивали какой-то сложный логики там по блокировке обновлений его статуса в дальнейшего и так далее То есть принципе да Ну естественно первый вопрос то что вы сказали Кафка - это действительно просинхронность и важно её быстро читать но это издержки нет Серебряной пули нет идеального решения здесь мы пошли на эти издержки понимаю в общем-то все минусы всего Андрей ты запоминай Пожалуйста вопросы потому что у нас как по традиции за лучший вопрос по твоему выбору будет подарок это самая сложная история в докладе да Еще раз добрый день да-да Привет доклад да Вопрос такие замуж даже коротко получится ответить Я правильно понял что это исключительно промонго То есть например там и так далее не пробовали Да это исключительная история про монгу точнее стримы это именно реализации cdc Мне кажется это одна из самых хороших реализаций то есть обзор принципиально специально не делал потому что у нас просто мы за границу доклада выйдем Да промонгу к сожалению не так устроено Да даже не заходили или есть какие-то заходили но у нас под нагрузкой у нас нет Вот с таким профилем нагрузки во-первых То есть у нас задача Вот именно cdc на постгрисе она не стояла хотя мы смотрели туда тоже как там это реализовано то есть на проде ничего нету И спецификой вашей данных есть связанными данными то есть нужно чтобы синхронизировать их условно взяли такие положили Вы имеете возможность данных обеспечить зависимость какая-то но смотрите у нас все события То есть это фактически Список объектов хронологическом порядке и мы всегда можем консистентное состояние восстановить более того эти события Они пронумерованы то есть у них есть индексы то есть какие-то индексы выпадают из хронологии то есть не только по времени но и в принципе по номеру по своему порядковому номеру то мы можем понять что это допустим нарушилось консистентность и нужно с этим что-то делать нет устал в сторону еще консистентности то что если например условно не в рамках одной таблицы например есть две разные таблицы между собой связаны условно что-то раньше доехало что-то не доехало нет У нас вот история изменений Мы специально так проектировали чтобы все эти данные лежали в одной коллекции Да здесь поэтому такой проблемы нет там не надо ничего звонить всё было спасибо здесь вот вопрос был хорошо Здрасте Спасибо за доклад насколько я понимаю коннектор в режиме который выбрали он на каждое обновление по сути идет в мангу и вытаскивает Этот документ чтобы отдать его вам в кафку поэтому два вопроса отсюда Почему Вас не устроили дельты вы не могли просто не знаю складывать в аналитическую базу дельты по каждому полю просто А ну по сути Всё равно всё раскладывается в базе и второе вы не заметили какое-то увеличение нагрузки у вас на каждое изменение еще раз идет чтение по документу по каждому заметили Да все действительно так но в принципе вариантов Других нет то есть либо мы читаем полностью документ сами на уровень приложения либо коннектор это делает за нас ну гораздо более оптимально потому что там есть подход с точки зрения аналитических потребителей они к сожалению не могут применить эту Дельта сами потому что у них фактически хранится некий snapshot уже с обогащенными данными Ну и они не могут восстановить целостное состояние товара применив Дельту то есть они сами это не посчитают никогда Спасибо можно еще пару слов много говорили про то что есть точки отказа прийти при подходе сетель и вы теряли данные Потом сложно понять Какие данные потери Какие Нет А можете пару вот примеров А что за точки отказа и как-то происходит то есть по сути это просто выгрузить запросом положить выгрузить положить что может сильно пойти не так ну во-первых прилетели там же остаётся тот же in richmand и всякие вот эти вот слайсинги и прочее всякие штуки А ну основная проблема на самом деле с потерями вот если вот честно говорить Да это связанное с вот этой бизнес датой операции То есть она не всегда обновлялась есть товар у него есть движение и при движении товара не всегда обновляется дата последней операции в силу особенности бизнес-процессов плюс возможные действия непосредственно с базой данные прямые когда СТП приходит что-то правит в базе данных не меняя дату последней операции а значит мы не увидим это изменение Да понял спасибо вот один вопрос Ты только что спасибо уже продублировалось чата второй вопрос из чата Насколько часто приходится вылазить руками в обработчики чтобы реализовать новую логику обработки или они написаны были один раз универсально и дальше переиспользуются Ну мы же вот наверное больше трех месяцев в принципе не меняем бизнес логику постпроцессинга и я не вижу ближайшей перспективе что такие изменения будут Ну то есть универсальная такая достаточно универсальная но в то же время расширяемая история То есть можно что-то Дописать Спасибо дальше Вот где-то микрофон вот вижу микрофон Андрей Спасибо за вашу историю можете пояснить Почему противопоставляете понятие cdc и etail Или например Потому что например в моем понимании cdc это часть etl процесса например вы говорите что etl критерии Что данная загружается какими-то пачками по там скажем бизнес логике там за какой-то период это просто по сути организация потоковые передачи в режиме скажем реального времени и она тоже просто может быть частью того же детейли Ну там например для организации стейджинга чтобы потом с него просто забирать данные совсем понял почему именно такой противопоставление двух понятий я понял вопрос но Да действительно Может быть я здесь вел небольшое заблуждение вас но под идеалом я именно понимал интервальную выгрузку то есть вот конкретно интервальную выгрузку когда у вас по диапазону идет выгрузка изменений действительно cdc можно тоже сказать что это часть и тела определенного Да здесь в принципе с этой точки зрения Да Одно и то же но я именно понимал Под этим интервальную выгрузку я вот сейчас сделаю отсылку у нас год назад кажется был интересный доклад etl Versus elt Как раз мне кажется cdc подход это больше elt когда мы сначала данные загружаем потом с ними работаем да А итл это мы сначала трансформируем потом загружаемся всё-таки вот там вот микрофон такой вопрос Можно чуть подробнее рассказать про механизм реплея вот когда вам нужно перезагрузить данные в каком-то диапазоне Правильно ли я понял что здесь участвует как раз третий топик Кафки Control и вот ваш процессор Можно ли получить подробнее раскрыть как это делается Да конечно то есть принципе вот этот режим синхронизации Он позволяет вам практически там задание то есть набор заданий и очередь заданий которые cdc процессор исполняет То есть вы можете указать что я хочу по такому-то идентификатору товара перевыгрузить историю более того вы ее можете перегрузить там сказать Я хочу перегрузить индексы первые 5 7 10 или перевыгрузить всё с момента тайм штамп А по тайм штамп или там а до конца Ну то есть вот такие вот всякие фишечки они поддерживаются спасибо понятно И у вас это собственно Легко делать потому что вы не дельтыми оперируете целиком документы Да всё верно Спасибо за доклад такой вопрос не услышал возможно холодный старт потребители получат первоначальный слепок первоначальный слепок Да это больная история но у нас она не стояла в принципе ну как не стояла стоял на самом деле то есть мы делали тоже переливки в аналитический кластер Но это вне рамок вот этого решения то есть мы им уже просто доливали то есть с определенного момента Мы перешли на него и стали все новые изменения туда переливать Всё что старое было не долито мы до сих пор доливаем другими инструментами то есть не было Просто такой задачи да Понятно спасибо Давайте ещё один вопрос остальное в оффлайне спикеров Что делаете Делаете не надо ли делать если источник совсем падает и восстанавливает из бэкапа с Владом не знаю монго диби совсем падает Ну нет он не падает ну как бы он в принципе может упасть ну смотрите У нас большой шарнированный кластер То есть у нас 12 шардов более там 30 серверов инсталляции и каждый шар тоже там очень сильно реплицирован и у нас не бывает такого Что ложится полностью источник то есть таких проблем нет отката назад по времени по каким-то причинам кто-то накатил какие-то изменения в базе которые поломали все необходимые релиз откатить назад откатить изменения назад которые что-то поломали Да по конкретным товарам и просто у нас есть DC только на пострессе он работает и там есть нюансы вот когда из выкапывается но мы не используем cdc для контроля Вот таких вот ну каких-то действий Да разрушительных когда что-то ломают Нет это мы этими кейсом решаем отдельно в рамках просто техподдержки если что-то происходит Мы не отслеживаем по большому счёту Ну и у нас нет такого то есть у нас базы данных потребитель не работает у нас не работает только бизнес-логика я наверное забыл ещё раз у нас есть никто не на манги другой источник приёмник через кавку всё также источник сгорел совсем восстанавливался из бэкапа на сутки назад в аналитике изменения есть суточные приемники источники уже нет данные разбежались данных как-то устанавливаться и проще аналитика у них уже как вы отстает Нет мы такой задачи Не решали Спасибо большое но мне кажется с одной стороны конечно это хочется отметить что мы Не решали Ну типа как же так да страны Мне кажется это хороший пример того что в каждом случае АйТи должно решать те задачи которые перед ним стоит и не делать какой-то Инжиниринг вот если у ребят Ну такой проблемы нет но может и хорошо Андрей Выбирай лучший вопрос вот мужчина первый задал вопрос мне очень понравился они тоже cdc-процессор свой реализовали прошу Спасибо за вопрос и подарок от нас от организаторов конференции тоже небольшой памятный подарок Спасибо большое что ты пришел к нам что поделился своим опытом Надеемся на дальнейшее развитие и новые истории вашего покорения cdc и вашего оперативного хранилища друзья аплодисменты"
}