{
  "video_id": "s8P8HYEFDpc",
  "channel": "HighLoadChannel",
  "title": "Как Tinkoff.ru использует Greenplum / Дмитрий Немчин (Tinkoff.ru)",
  "views": 8393,
  "duration": 3100,
  "published": "2019-05-14T14:57:50-07:00",
  "text": "еще раз я меня зовут дмитрий аникин я вам всего я немножко мы в тиньков ru используем замечательную субботы грим план немножко обомне я тимлид команды которая занимается поддержкой бэг-энда хранилище данных в тиньков ру до этого дела и чем только не занимался и or call мучил и недвижимость продавала и производством и kb занимался и маркетингом и всякой прочей фигне собственно потом все это бросил пришел к ним плану и там остался о чем я вам сегодня буду рассказывать ну во первых как мы пришли к game plan что у нас было до этого и что наш конечно представляет собой сейчас соответственно какие потребители данных у нас есть нашей системе репликации данных и 40-го а нашим жади застава recovery для гриб лама подойти и немного расскажу о нашем опыте проблемах с которой мы сталкивались местами не очень очевидных о борьбе с нагрузкой и немножко советов начнем в начале было слово и слово было счас это к чему к тому что все наши конечно изначально строилась на продуктах sas фактически это был один сервер для обработки данных данные лежали но складывались в виде со 100 блинчик на исходы шину первоначально процессу были только этель то есть полезли в базу источник что-то туда забрали на ходу преобразовали положили все хорошо при небольших объемах как только стали расти тут же уперлись в процессоре память на сервере обработки тут же уперлись производительности схд она не бесконечна и поняли что надо что-то менять причем менять радикально собственно нужно было какое-то сильно масштабируемое решение и рассматривали нити зa рассматривали эти рада то рассматривали что-то от oracle а в итоге выбрали грим план за хорошую масштабируемость за понятность за кучу всего что уже на тот момент он умел и в общем стали с ним жить ну для тех кто никогда этого не видел для тех кто видел повторить просто как грим план работает очень просто запрос приходит на мастер есть выделенный мастер мастер отправляет запрос на сегменты сегменты это маленькие под газ и каждый из сегментов обрабатывает свою небольшую часть данных результат отдает мастеру мастер агрегирует как-то финальный сортирует отдает клиенту собственно как и в любой битве базе классической картинка из документации отпевал плюс climb лама помимо того что он такой замечательный хорошо масштабируется и все параллель от это пост с минус в том что на самом деле это не совсем под газ но об этом чуть позже но для с точки зрения клиента это может быть вполне себе postgres который просто обрабатывает куча всего довольно быстро и параллельно я пришел в компанию в пятнадцатом году грим план у нас уже тогда был game планов в тиньков где-то 2013 года существует активно используется в россии одни из первых пользователей тогда заметную часть читали еще напрямую из операционных источников что в конечном итоге приводило к некоторой боли часть данных уже тогда реплицировали но коробочным продукта матюнин тире плетей то нем чуть позже расскажу в данных у нас было порядка двадцати терабайт и каждую ночь запускалась порядка двух тысяч рабов которые собственно строили хранилищ боевые кластер а их уже тогда было 2 каждый был из 16 машин тестовый тоже 16 машин еще был жив сейчас и уже разобрали везде под грим плам были только шпинделе никаких там модных нвн гей создает всего не было как-то все это жил о потихоньку процветал во что это превратилось сейчас сейчас у нас уже больше 70 терабайт грим план это данные внутрь объем самой базы то есть данные сжаты ну там с разными коэффициентами сжатия команда выросла больше нас уже больше 100 человек в том числе в наших региональных офисах разработки у нас есть свой dior которыми сегодня расскажу у нас есть своя репликация которая тоже расскажу уже не 2000 а порядка четырех тысяч рабов строят наши хранилища и в основном это уже нормальные и lte и боевые кластеры 2 по 36 машин тест 36 машин частично мы уже используем современные нормальной январе карты и частично счас ssd частично еще шпинделе но хотим пойти в совсем flash генплан посмотрим что из этого пользовательская активность собственно потребителей данных у нас основных два направления это отчетность и аналитика отчетность строится в сад бизнес-объект часть отчетов собранные в соси какие-то рассылки или что-то такое часть отчетов в собранном или нами или пользователями в apache zeppelin некоторая часть в табло который мы недавно стали использовать собственно и теле и lte процедуры все управляются либо сам либо информатикой 960 значительная часть счас информатики не так много клиенты ходят обычными совершенно обычным софтом для работы с тем же под весом до то гриппом который мы очень любим на у него есть некоторые нюансы а janity workbench которая специально писалось под грим план и с грим планом долго пятой версии нормально работает с пятой версии уже не работает и сейчас по моему не закрыли этот проект ну и там какими-то гибели раме кто чем хочет мы в общем-то это не контролируем здесь так очень верхние уровни во очень на пальцах архитектура хранилища собственно левая часть которая до пунктирной линии это все как бы все что до нас рак новые базы какие-то файлы на хранил как базу ds которую мы сваливаем данные которые потом обрабатываем очками позже расскажу а собственно вот все что между пунктирными линиями это уже наша часть это уже хранилище у нас несколько цодов и собственно все вся левая часть она тоже держится но какими-то своими средствами мы в этом не участвует наш часть это собственно нескольких серверов в они естественно резервируются ну в одном сосуде в другом соде два боевых грим плана и средства визуализации отчетности доступа к данным они тоже резервируются но там довольно прозрачно происходит собственно какие-то данные мы еще забираем из операционных источников но уже очень мало какие то данные наш этель ну и altima врач + коврик по привычке называем эти забираем собственно схд но значительная часть лежит уже в грибами и там обрабатывается репликация метель это замечательно этель это хорошо но с ростом объемов очень быстро все приходят к тому что постоянно лазить в операционной базой нужно вредно вообще за это можно по шапке получить хоть а потому что вы создаете бешено нагрузка потому что вы учитываете довольно много собственно поэтому все хранилища очень любят то что называется сиди сечения . то есть затягиваем к себе постоянно маленькими кусочками маленькими патчами потом ночью запускаем свой страшный адский и lte обрабатываем все эти данные и спокойно строимся вначале у нас был была система под названием потяните репликейт ну картинка конечно примитивная и довольно страшно но тем ни менее картинка здесь к тому что на каждый грим план был свой сервера тюнить в принципе система хорошая но даже есть вот такая очень красивый вот мордочка с классными даже бардами с управлением всей системы слугами здесь же прям вот все круто здесь все настраиваться но про красивой но у нее были помимо вот как левак можно было несколько минусов которые для нас постепенно перевесили все ее плюсы оно очень хорошо работает при небольших объемах если у вас не очень много изменений в операционных базах ну или в принципе вы как бы не очень большие хорошо если у вас стабильная схема данных в источниках тоже хорошо ну то есть если у вас там нет релизов каждый каждый день каких-то изменений структуры данных в источнике нормально но окуните она хоть и очень сильно умеет работы с гриппом он вроде как используется ним по-мужски и механизмы для загрузки данных у нее есть несколько проблем во первых она загружает данные довольно небольшими патчами и создает достаточно большую нагрузку на диске кластер игорем план что всегда печально во вторых если ей надо было сделать о чем какой-то апдейт апдейт почему-то не прошел она переходит в то что называется рубай ролл она реально построчно начинает проводить апдейта грим плам этого катастрофически не любят как общему многие mpp системы но для него это прям прям совсем смерть при нагрузке на источник получается заметной лак репликации с этим ничего не сделаешь любое изменение д д ля любого объекта требует его полной перегрузки да здесь сразу что называется disclaimer это все речь о потяните репликейт на момент где-то на начало 2017 года но с тех пор мы не использовали сейчас возможно там все поменялось на темени и тогда было так то есть если где-то в какой-нибудь там я не знаю таблицы каких-нибудь транзакции авторизации но в чем-то таком большом у вас добавилось поле далеко не факт что от unity сможет добавить поле в гренками и вам придется вот это вот этого монстра перегружать соответственно вы убьете источник вы там ну в общем много проблем огребете да ну это что не нравилось в общем то лично мне винда под этим делом как бы все остальное в линуксе хотелось чего-то теплого лампового родного вот чего мы хотели от своей сидиси системы во первых снизить нагрузку на приемнике сделать batch если регулировкой размера и в том числе сделать возможность скажем полдня ничего не грузить ну то есть днем работы пользователя днем работает отчетность остановили репликацию вечером взяли все данные которые скопились в один патч собрались закинули в базу там она часто напряглась но зато мы залили туда вот прям вот так ну и все в таком духе автоматически применять del перевожу мы просто хотели спать потому что кто-то что-то за релизе у ночью у нас что-то упало как правило упала что-нибудь важное в репликации потом это долго догонять просыпаемся ночью разбираемся злимся вот это все хотели масштабироваться легко и ну как бы не добавлять каждый раз новые сервера тюните что что-то такое делать хотели общем-то добавить просто новый приемник в уже готовую систему и жить счастливым да и хотели минус собственно примерно все это и получили до в наше решение не это как бы не конечный наш продукт мы используем сначала оракал golden gate не помню уже каких версий ну суть в том что мы golden gate am сливаем из множества рак новых баз там что-то их больше 20 сливаем к себе в одну большую базу о д с из которой мы уже нашими процессами сам описанными выгружаем данные в какие-то временные файлы фактически все сливки сжаты и применяемых аккуратного все грим планы с такими размерами ну там почем каким хотим с частотой какой хотим эти процессы независимые то что у нас называется onload собственно выгрузка из одесса делается с помощью нашей же см описанный библиотеки очень быстрый и отличный я надеюсь что мы в каком-то обозримом будущем вопросы и отдадим fly сделан собственно на механизмах climb la mano гпв 10 внешних табличках и вот этом все система сейчас переваривает довольно много у нас только в грибами храниться больше двадцати терабайт данных репликации это с учетом году пышного сжатия причем там коэффициент разжижать и плавают то есть где то если по средней температуре по больнице смотреть я думаю что это 50 60 неджат мы загружаем данные уже не только и 40 la но используйте все ноет право тем наверно отдельного доклада лак репликации не больше двух часов от падания строчки в источник до попадания к нам можно сделать меньше он просто не хотим грузить гимпла мы мы можем выгружать истории изменений отдельных hadoop никак бы не держать ее в гренками потому что не всегда она нужна на самом деле для построения хранилищ и также можем некоторые куски данных если очень надо отсылать в кафку прям стримом и . шо следующая часть моего доклада это designs to recovery для gimp лама тут стоит сказать что на тот момент когда система создавалась фактически ну вот такого дизайн sky recovery как как мы его видели его не было при чем больше того в таком виде его наверное нет и сейчас есть много других решений но для нас у них есть некоторые минус собственно game plan это изначально отказоустойчивая система у вас есть праймари сегмента у вас есть зеркала если праймари сегмент собственно праймари нас пояса упал зеркало с ним все хранится постоянно зеркало поднимается для клиента это прозрачно клиент просто подключился к базе и как бы все хорошо все работает но у нас на грим плам завязана вся отчетность и порционные управленческих там какое угодно и завязана вся аналитика а для нас аналитика крайне важно и если что то где то случится я не знаю там рубильник в салоне выключит фонда ракета прилетит метеорит да все что угодно в общем всем будет очень грустно без глинт мама соответственно нам нужен был какой-то dear так как у нас хранилище строится на там 90 процентов с а сам соответственно у нас есть свой планировщик который запускает саджа бы нам надо было запускать перенос объектов из этого планировщика да и при этом сразу а говорит что перенос нам нужно было естественно по объектный почему естественно потому что вот строители хранилища у вас там прошло типа там две тысячи рабов осталось еще две тысячи рабов посередине вы падаете вам надо переключиться на другую базу ну как бы не перезапускать же все в построении охраняющий хочется перезапускать только половину чтобы там чуть-чуть упавшие до запустить от дальше она все побежала и все счастливы в общем то мы хотели помимо чистого dior мы хотели создавать бэкапы и проверять их потому что непроверенный backup не считается backup естественно мы хотели минимизировать время доставки данных на резервных базу ну и регулировать нагрузку на каждой стадии как на базы и так и на сеете на исходы и на вот это все да соответственно бэкап и не только для того чтобы они были просто бэкапы а в том числе для того чтобы можно было их забрасывать на тесте ровный контур и там в каком то виде на контур разработки тоже собственно что мы пробовали пробовать какие-то очевидные вещи которые pivotal безначальный вентиль game plan в том числе рекомендует пробовали джипе трансфер между кластерами между цветами пробовали и gpf dice там выливать куда-то и тут же забирать но собственно на какую-то схд посередине пробовали не приземляясь на диск gps там сливать в именованные pipe и и тут же из них читать на другой стороне это общем все замечательно но бэкапов нет потом сделали совсем просто взяли на фейсе ну большое делаем на нее дамп делаем с ней restore у нас вроде как есть и бэкапы и все хорошо но очень быстрому упирались производительности это не самый на фейсе не и поняли что с ростом нагрузки нам будет тяжело и очень дорого потому что у нас там больше 140 сегментов базы то есть больше 140 инстансов подвеса запуск одного джипе дампа одного объекта это соответственно 140 потоков записи это хорошо а если вы хотите 10 потоков там не знаю захотите 20 потоков потом такие же потоки ristora вам надо это все в параллель чтобы работала ну в общем решение получать какие-то крайне не бюджетной даже несмотря на размеры нашего гриб лама все равно не бюджетные пришли мы под к такой схеме которая сейчас работает с небольшим изменением мы делаем дамп на локальные диски локальные диски отдельно о троих массивов которые собственно под базой это счас ssd небольшие но и хватает дальше дпс sh gps с вашим запускаем на каждом сервере с и пивку которая просто переносит на соответствующие машинки другой кластер там рис торе мысли от истории вы складываем это все на хранил q хранила конечно было 2 они синкансен хранились потом поняли что пирсинг а там не хватает и в итоге перешли на кластерную файловую на lizard fs она пока общая ну в общем достаточно неплохо себя показывает есть специфика но неплохо собственно место лизарда хотели использовать сев или что-то такое но вот тогда выбрали визит вот как это выглядит сейчас сейчас эта система переваривает перегоняет между сотами больше 30 терабайт каждый день задержка от построения объекта на боевой базе до доступности данных этого объекта на резервный до двух часов мы научились поддерживать базой частичной работающий на зеркалах связано говорит что у изначально глостера одинаковые по количеству сегментов но если что-то где-то упало на одной другой базе но база при этом доступны то для нас это уже хорошо восстановить мы сможем ну естественно первое требование сразу бей дизайн была выполнена что объекты ставятся в очередь на перенос с планировщиком но и управляется это маленьким в приложением и с этими консольными конфигами чем в этой системе не хватает у нас много данных и мы растем растем постоянно растем сильно нам не хватает инкремента да у нас есть партиций да можно переносить отдельно партиций никто не мешает но мы этого сейчас не делаем и перестраивать переносим объекты полностью ну в том числе потому что довольно большой геморрой получается дифференцировать объекты для этой системы в общем хочет хочется именно вот инкремент и плюс бывает так что там активная partition довольно большая а как бы они активны и против который ничего не делает а на пожар и оно довольно маленькое общем уже не так сильно влияет хочется большей управляемости сейчас мы используем философские утилиты и собственно джипе дам же перестал пить чем больше того утилиты которые уже там триста лет де брикет это надежнее их пока ничего нет все новые со своей спецификой хочется обрабатывать несовпадение до del а также как мы это делаем в системе репликации этого пока нет и если с доверием что-то dd разъехался ну скажем был релиз на боевой базе что-то про альта реле на резервные забыли или просто этого не сделали нас просто объект упадет табличку тропы запускаем заново и нам нужен вакуум вот здесь нюанс выяснилось что любая пиво толстая утилита дампа создает в нем пламя транзакции с уровнем изоляции сериала izum.ua нюансы в том что если у вас есть в базе хотя бы одна любая транзакция с уровнем изоляции сериала и забыл забудьте про вакуум он просто не работает то есть вы вы запускаете он говорит что он отработал он возвращает вам управление но ничего не сжимает причем влоги он честно напишет где-нибудь там далеко что ну вот тут было сериала я забыл транзакция и вот у меня что то не очень получилась пожать вроде как бы все выглядит так как бы не делай таких транзакций по своим объектом все будет хорошо на самом деле нифига если есть любая транзакция по любому объекту любой даже другой не связанный с ним объект вы не сможете про вакуум и напоролись мы на то что у нас резервная база там вот такого размера а боевая почему-то вот растет приходится вот это все останавливать вообще все дамб и останавливать аккуратно вакууме или вообще пересоздавать объекты иногда и запускаться дальше проблема была актуальна для тёплого версии 4 три пятерки по моему тоже самое ну да кстати пятерки то что возможно это потом поправятся возможно нет собственно вот отчасти из-за этого мы хотим уйти от венгерских утилита используют что другой даже кстати с новыми утилитами джипе backup по моему которые используют копи он сегмент торта же самая история хотя там казалось бы не не так необходимо сериала и забыл транзакций можно без этого на самом деле обойтись но оно там есть ну и напоследок немножко нашего опыта немножко советов немножко капитанство немножко не очевидности как мы готовим грим план как мы с ним живем grip вам это mpp систем mtp ширк над и все классно очень хорошо проверятся очень хорошо масштабируется но есть нюанс ин-т пищ органов по русски значит что у вас каждый маленький кусочек базы обрабатывать свой маленький кусочек данных из этого следует очень простая вещь старайтесь равномерно раскладывать данные по кластеру в грибами есть дистрибьюшен key ключ распределения таблицы может быть распределена рандомно похожу по моему собственно по всей строки может быть распределена по одному или нескольким полям собственно строительство складывать так чтобы у вас все все лежала ровно почему потому что архитектурно все подобные системы у них есть одна проблема они работают со скоростью самого медленного сегмента если у вас я не знаю словно там рейд где-то просил по производительности на одном сервере то вы будете работать со скоростью этого рейда если вы положили криво данные и обращаетесь к таблице вот в которой криво лежат данные много быть работать фактическая скорость этого запроса плюс помимо того что вы уже как бы разложили все хорошо и все красиво учитывайте свою нагрузку то есть в ваших генах должно быть тоже ну желательно крайне желательно равномерно распределять временные данные потому что если запрос память не вошел что вполне нормально для гимпла манжини и memory он начинает писать временные файлы на диск соответственно вы написали кучу временных файлов на одну машину и эта машина потянула за собой кластер он не упадет просто будет купить этого можно избежать можно избежать архитектурно можно избежать запросами можно избежать просто опытом и нюанс который мы на которой мы не обращали внимание но он у нас вылез не так давно когда мы стали большое количество пользователей пускать писать производная сквозь в базе избегайте большого количества no love в ключах joy на собственный график ниже это ludovic на кластере один сервер выбивается больше того на самом деле с точки зрения база выбивается один сегмент на котором всей финалы скапливаются сделать с этим в общем случае нельзя ничего ну то есть в каких-то случаях да во многих вы можете объяснить пользователям ребят так делать не надо вот здесь вы лучше отделить эти нал и потом их при джоне эти все будет хорошо и поставьте просто условия там в левый join но это вот совсем общего решения мы пока этому не видим в своих запросах конечно мы это исправляем но пользовательская нагрузка такая интересный нюанс про собственно спел файла про временные файлы которые грим план схватывает на диске если запросов память не вошел есть несколько параметров базы низко параметров кластера которые влияют на спилы это ограничение объема спилов на запрос ограничение количества столов на запрос и ограничение объема спилов на себе не начали с того что ну собственно зачеркивать началось база внезапно начала падать когда в нее приходили запросы с хренову тучу временных данных они просто забивали диска все падал ук смотрели мы же можем ограничить объем ограниченный объем все хорошо падать стали в два раза реже но падали почему падали непонятно вроде как бы мы не смотрим в джипе ту кити объем спеллов который не ряд запросов вроде там ничего такого криминального нет а потом мы вспомнили что мы же культурные люди мы же почитали документацию мы же послушали совета пиво того сделали xfs под гриппом с размером блока 16 мегабайт на который грим план долго-долго вот под подписывался под это дело чтобы очень быстро с этим работы и вот все круто но база пишут word файлы очень мелкие и очень много вы пишите там н сотен файлов по 4 килобайта и для база не 4 килобайта и для вас не 4 килобайта а вот файловая и операционка с вами согласны у нее размер блока 16 метров все приехали написали тысячи файлов забили винт как никак вот на ровном месте собственно тогда начали резать уже количество спилов и прям прям очень что называется консервативным под ноль ну и собственно вы же просто для начала это было каким какой-то очень странные вещи что вроде мы в размер не уперлись а же все легко нюанс тоже которого у нас возник от клиентов которые напрямую ходят постоянной синхронизации метаданных собственно история в чем у нас довольно большой кластер ну конечно не как у некоторых по паре петабайт но большой у нас несколько десятков тысяч объектов там больше двух миллионов атрибутов и вроде казалось бы это не очень много но клиенты которые изначально не точились под game plan тот же любимый мной даты гриб они любят фоне по чуть-чуть маленькими кусочками синхронизировать метаданные это приводит примерно к такой картинки что на аналитической базе у нас 300 тысяч а то и 500 тысяч запросов каталожным таблицам причем махонький маленьких-маленьких но постоянно идущих сутки причем запросов которые не сказать чтоб они мега кому-то нужны для людей которые как бы быть ближе кукол тебе но этой фигни она что такое 300 тысяч запросов но грим плам не очень это любит и это приводит лишней нагрузки на мастер и собственно в принципе медленной работе с каталогом там при создании внешних табриз там не знаю перестройку них юхи мы все что угодно собственно когда отдела обнаружили и попросили пользователи так не делать отключить этот of the sink или делать его только аккуратно только когда надо по нужным схему эти триста тысяч запросов убрали казалось бы мелочь но если можно избежать лучше избегать вторая штука ну которая для нас как бы было очевидно а вот для пользователей для наших оказывается был не очевидно для них это типа большой большой пузырь с такой весь параллельный бла-бла-бла история была такая звони-звони первичная поддержка будет админов курит вот ребят тут висит связь от пользователя но висит висит из о сервисе состоянии айдолом transaction айдан transaction мы стараемся либо отстреливать либо ну очень аккуратно с этим обращаться потому что либо какая-то большая загрузка небольшая выгрузка либо кто-то не закрыл транзакцию в общем ничего особ страшного но проблема в том что вакуум каталога не работает он у нас большой соответственно база замедляется ну в 1 насчет делать сессия от пользователя не закрыта два часа ночи убьем убили ладно на следующий не то же самое тот же пользователь то же время что-то не так убили с утра так что это пользоваться переслушать а че ты делаешь ничего я вот пяточком подключаюсь к базе и делаю силе звездочки из таблички файл грим хорошо что с табличка и такая такая а ты знаешь что на 60 гигов есть то я вот забираю типа это она у вас 60 а у меня там будет 200 но гаража ты dream 1 все замечательно ну ты бы пришел и спросил геймплеем и есть внешняя таблица ты сейчас забираешь в один поток можешь забирать в 140 потоков ну там в н потоков и собственно вместо там четырех часов у него эта связь и висела 40 минут и все были счастливы и довольны и он порадовался получив новое знание собственная к чему вот эти последние два слайда были к тому что вообще учите своих пользователей потому что гриппом все-таки имеет свою специфику довольно большую как это делаем мы у нас есть на вики учебник с лучшими худшими практиками ну такой пока катки но мы его постепенно дополняем у нас есть замечательная история рассылки с разбором худших запросов то есть реально мы берем самые долго висящие запросы от пользователей в базе садимся смотрим что с ними можно сделать как правило там какие-то для нас достаточно очевидные вещи для пользователей не всегда оптимизируем это дело и рассылаем на всех пользователей хранилище кто вот напрямую выходит показано что ребят вот было такое сделали вот так сделали вот так запросто работой в пять раз быстрее и общаешься замечать и это очень хорошо работает всем советую да ну и соответственно не не пускайте никакой лтп в green плам как бы лучше куда-нибудь выгрузить и нужные данные не знаю ворог в нормальный пор ко сну в обычный и там пустились ковыряются да ну и берегите сон нервы админов о том кто то может промахнуться собственно две команды вроде как не сильно отличаются и запускать то надо было вторую идея была почистить логе старше месяца нуб очистились не логе а весь каталог праймари сегментов вообще всех при этом разработал то есть прям вот работал и и все и клиенток они подключались а потом мы сделали ключевую ошибку мы ее восстановили ну дальше я конечно не поднялась мы ничего за пару дней установили вот так что вот так у нас весело ну и собственно все это вместе наверно примерно так берегите диски под грим планом даже если у вас там и на внж даже если вы такие молодцы все равно и о наше все убирайте лишнюю нагрузку используйте возможности грим плам а если речь о грим пламя 5 ветки там помимо собственно внешних таблиц которая умеет смотреть на сисви умеют выполнять скрипты и что-то еще такое делать есть да и помимо jetcash dfs то есть внешняя табличка направлены прямо на данные в hd офисе есть еще pics of pixiv оч крутая штука которую очень хотим по использовать вы можете сделать внешнюю таблицу и в пролей цепляться к фактически любому утюгу который умеет джитибиси хотите положить руку пожалуйста вот да ну и естессно помните что это не oltp в следующих версиях emplome обещают под в лтп нагрузку что-то там такое поделать но пока не надо вот прям не надо и учите пользователей или не пускайте их собственно вот так вот мы научились готовить зеленую слил вот что еще из нее примерно можно сделать как-то так хоть его задавайте вопросы есть микрофон сейчас будет добрый день подскажите пожалуйста у вас сейчас это хаки пользователей идут на основной сервер и не планировали вы переводить их на dior contour пока идут на основной ну планировали хотели но там вопрос лицензии всего остального а там по текущей нагрузки какой примерно процент ресурсу занимает именно пользовательская нагрузка в рабочие непонятно наверное процентов 60 довольно много хранилище ну собственно по стране конечно у нас достаточно хорошо оптимизирован там еще конечно есть куда копать достаточно хорошо на вас немного из спасибо здравствуйте три маленьких вопросик вопрос номер 1 на чем база vds парка ну собственно потому и golden gate и 40 . вот такой еще вопрос вот вы сказали что переливаете для ну собственно основе дампов вы переливаете на девелоперские среды то есть в процессе перелив ки этих дам в какой тебе зли чего не идет с продавили данные с из переливаются но у нас там большая часть и так уже обезличена ну то есть как бы вот реплики операционных источников мы туда не заливаем части обезличивает сейчас заливать просто доверим там разные есть понял и вот смотрите вот вы сказали про оптимизации запросов и сказали про табло но вот оплотом идет просто сюр как бы по четыре по пять листов как бы не понятно так как вот столовую боретесь как бы с тобой мы на самом деле так себе боремся бы сказал то есть у нас там вообще какая-то часть выгружается кого-то в spark табло ходит spark часть крем план в общем табло у нас появился совсем недавно и табака так нам хватает вот таких вот реальных запросов от наших же разработчиков всех оптимизировали такие вот поняло было что-то было очень много проблем как по базам данных и мы очень грустим и очень интересно ну как с этим борются специфической понял спасибо у меня еще вопрос скажите пожалуйста настроили у вас соседи emplome ну нет то есть у нас есть автоматизацию наших релизов и 10 и в смысле самой базы или смысле того чтобы на ней делаем объя за то есть там не базами данных сложно как то придумать не в этом смысле нет у нас есть своя система как это авто релиза и вот этого всего но он как бы не совсем сей все-таки ручной пока там частично тестирование автоматизированный релизы частично автоматизированный и там версии всяческих гробов и или где-то хранятся но не совсем прям прям сей-то есть обычная разработка там когда происходит потом сели скрипты и накатили я например на данном скрыты накатили будет какие тут же вы прогнали переходные где то еще что то такое пока просто вот процесс релиза без без вот этого спасибо а вот мониторинг что вы обычно мониторить это как у вас стран система в грибами место нагрузку дисков нагрузку лупу текущие запросы размеры спел файлов ну объема каталога в том числе объем из как называется в общем строк который надо правок уметь сессий длительные пустые не пустые разные периодически чего отстреливаем выглядит топ размеров объектов то там длительности запросов некие такие порты which a оделся и силы автоматом эту сессии у меня про них был слайд его вырезал можно сказать исполин из политкорректности оказался нюанс у нас мы в какой-то момент упирались это что нам не хватает соединений в базу мы поднимаем там макс connections поднимаемый экспорт tranzaxis потому что у типа надо поднимать чтобы там все было хорошо но есть нюанс больше тысячи его сделать нельзя причем эта хоть кодом забита прям вот в коде open source навага emplome по крайней мере там полгода назад было там замечательная штука если вы ставите макс при полипе начнут больше 1000 и его сбросив 50 и все как зачем мы сделали вообще никто не знает но вот так но соответственно и да всоси и строим постоянно на там у тестеров разработчиков на вас пару часов напротив там зависит от группы ну собственно тролли от ресурсного очень но так тоже с довольно консервативны и к этому всему подходим стреляем лучше пить лучше пусть переподключиться короткий опрос бывает ли у вас несколько отлично то есть несколько полей насколько сильно проседает производительность когда несколько полей ну не радикально если это не 20 вот таких вот огромных полей там типа по 4000 марта нет ну не сильно остро кого я в смысле прям текст или большие varchar ну ворча рада там существенно мы стараемся этого не использовать вот то есть если если такая ситуация то на генерить и себе суррогатных ключей как беги began to какого-нибудь это будет быстрее ну пушкаш от него считать точно быстрее при вставке дмитрий спасибо вам большое за так вот я здесь вот два вопроса про базу давайте но куда его уже вижу два вопроса по базовому dice скажите вы golden gate укрепи catia вы сразу пришли ли про боль и какие то другие решения там типа дабы визита этом шаре процесса черты new ну ничего не пробовали все расскажу пришли пап ну как бы посмотрели самое очевидное и 40-ого ракал чем лить golden gate amp опробовали взлетел и и довольно неплохо показывать а скажите еще а как вы контролируете до гарантированную доставку и аппликации every дата или или не как-то с ним интересно поехали нет во-первых мы мониторим мы делаем как бит таблички в источниках в одессе и в грим пламя их сравниваем но это вот такая как бы что при занятии как попал под по дубленку или в нем этот инструмент из внешне нет мы просто ли мы этим же процессом реплицирует к себе вот это все хоббит таблицу мы есть просто джоби к торну не забитом скрипты которые ее 1 минуту обновляют все там одна строчка не просто собственно смотрим разницу таймс темпов но это с точки зрения того что в принципе источник реплицируются по данным вас есть отдельные какие-то вещи по качеству данных собственно этим ну отдельные люди отдельные там скрыты какие-то это все там проверяются понятно спасибо zdravstvuite еще вопрос я здесь же насколько по сброс в green пламя отстает от основного мастера пока сильно то есть в грибами версии 43 которая у нас продакшене puzzles 82 15 но с тех пор там некоторых вещей из девятки мозга свой грим план 512 которая сейчас актуальный это не не мастер полупан собственный это вот стабильный релиз это позор с 83 по-моему там с мелочью 384 черта какой-то жест опилками из девятки под грим план 6 который выйдет весной ну я надеюсь допиливают до 94 понятно а вы упомянули что у вас система продолжил работу на реплики после там удалению ранах и я так понимаю что кластер в принципе разваливается как-то автоматом но насколько возможно проблемы сплит brain of сплит брейна ну мы не ловили и в принципе при нормальной работе и в этом вообще быть не должно если у вас как бы если вы не занимаетесь странными вещами типа построения 1 кластеров двухсотых ли какими такими штуками технически но спреды brain там наверно возможен но на самом деле у каждого инстанса пояса внутри кластера свое хранение и как бы я не знаю то что надо сделать чтобы получить предмет в этом все а по поводу кстати отказоустойчивости там есть один нюанс сегмент на зеркало пережает автоматом и вы этого не замечаете ну пользователь не замечает и пока в зеркало не упадет все счастливы а вот мастер мастер тоже можно резервировать но там автоматического файла там нет хоккей и тогда такой еще вопрос раз у вас система такая прикольно надежно что на реплики вполне спокойно продолжает работать без большие они бы особых максимум там пару часов не рассматривали такое опасное и возможность бэкапить именно что реплику и в дополнение вал и тогда и не нужно особо заботиться о сиро лайза уровне транзакций но естественно это если у вас одна база на одном серваке у нас одна база но снаружи а внутри то как бы там 36 серверов и на каждом еще четыре базы из них идет дам тут более приближена к под груз и наверное забыл вопрос нет с точки зрения по сбросу как бы это нормально и совершенно метод сдам перевал если все хорошо но здесь и его love довольно много и опять же вопрос вот мы сдам пили но пока без этого backup они восстановились мы его не считаем нормальным уже много раз на порывались из vertica из game по мы с чем угодно что как бы мы бы как-то сделали все хорошо а потом а ну в общем что-то пошло не так при этом all ну вот как то так спасибо небольшой опрос по источникам вчера ваш коллега рассказывал о том что они используют общественности кассандр под фронтэнда у вас из таких нетрадиционных зубы д как это выгрузка выводы ваш есть есть у нас есть вы но отдельно вот от этой всей системы там что-то мы выгружаем сказки в ходу у какой в таблице быть bass там просто кладем capsis фишки иногда затягиваем это в гриппом кассандры если честно не помню скорее всего такой есть кассандра у вас несколько что-то где-то тянется кто-то пишет сразу выйти и сам оттуда как-то забираем кто-то пишет сразу собственно в hd fs мы тоже туда это собираем там празднуем ног сегодня просто вот это основная часть ну тогда вот вопрос по этим источником у вас для хранилища в принципе вот такой абстракция там как банковский день применяется для этих источников от актуальность для вот этих всех не реляционных людей по моему нет ну то есть далеко не везде ну там как как мы же как это мы же не банк мы же эти компания с банковской лицензии много всяких дней в продуктовых и банковских данных вот таких не реляционных вещах ну конкретно там транзакции каких-то авторизации счетом счет такого насколько я знаю там и нет то есть это все все равно реляционные вещи которые вот-вот этими же процессами тянутся там естественно там финанс как это называется забыл ну в общем да там банковский день вот это все как как называется там без бизнес-день вот закрытие бизнес дня там какое-то такое ну да спасибо расскажите пожалуйста по свою релизную политику то есть как у вас происходит от этап разработки до релиза если разделение для бизнеса там каких-то мартов и рейза инфраструктуры что имеется ввиду под релизе инфраструктуры например какие-то задачи для внутренних схем слоев данных мы то насколько я знаков не разделяем то есть если задача дошла до релиза то как бы что это там ds что еще что то что это конечный март все равно пойдет в один люди скорее всего ну по крайней мере пока ну ясно разделяется релизы отчетности которые там какие-то свои юхи в базе может переломать какие-то universe в саду что-то еще это отдельная история отдельная история конкретно и теле и lte вот это все более-менее каждый день ну каждый с понедельника по четверг в пятницу релизе нельзя выходные визит никто не хочет можно чему не работает а работает извинить вопрос небольшой про разработку если это понимаю с более-менее вот стандартная схема там и со звонками у вас на века унаследую надо там с детальным слоем с ником семантическим слоем вопрос как ну там хаим слоем отчетов как минимум ну да плюс более более-менее там такая переработку принципе надев контурах решается вопрос синхронизация когда идет параллельно разработка дион детального слоя данного параллельно ним идет разработка чего то есть когда нам систематизированный подход к этому решению появился не так давно у нас это называется пробирки то есть есть есть буфер таблиц в которой но у разработчика из portal я делаю такую задачку мне нужны такие такие такие такие объекты в буфере смотрятся если эти объекты как бы все на одну дату ну загружено на одну дату дальше если я правильно помню ему строится какие-то вьюки на это дело чтобы использовать их как источники если не нужны как приемники то они копируются просто еще раз внутри базы в схему которую только он будет использовать вот только его пробег ну соответственно задачу сделал получил какой-то релизный пакет все это удалилась на фиг из базы это на дэви да ну надави на тесте на тесте там тоже собственно систем и это все автоматизировано об этом через дыру или что-то подобное руками делается скачки оренбург процесс в смысле через живу вот это заведение списка табличек я не помню по моему руками а вот дальше уже автоматом дальше там уже какой-то там api сделано которая все это делать но я под подробности боюсь соврать я там так сбоку припека не говорю что база работал все время говорят закончилась всем спасибо за внимание"
}