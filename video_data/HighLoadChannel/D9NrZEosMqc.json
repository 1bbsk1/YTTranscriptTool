{
  "video_id": "D9NrZEosMqc",
  "channel": "HighLoadChannel",
  "title": "Flashcache в mamba ru / Александр Яковлев (ЗАО Мамба)",
  "views": 275,
  "duration": 2372,
  "published": "2017-04-07T15:04:42-07:00",
  "text": "я думаю вы готовы для того чтобы послушать как его используется технология flash кэш компании в mambo я рассказывать об этом будет яковлев александр поприветствуем здравствуйте меня зовут александр яковлев я работаю в компании мамбы уже больше более трех лет занимаюсь системным администрированием сегодня я хотел бы рассказать о том как мы внедрили модули от компании и фейсбук модуль для ядра операционной системы linux который был разработан и этой компании выложен в open source достаточно давно более 4 лет назад и как мы его вот выбирали потому что были альтернативные решения и как как тестировали и внедряли сейчас секунду александр не идет дисплей не идет неудобно мне надо время контролировать сколько прошло время да да где я где мы начали использовать этот модуль этот модуль используется на наших кластерных серверах я буду рассказывать о две группы кластерных серверов первое это серверах где хранятся все фотографии их 10 серверов фотографии почти миллиард 950 миллионов и все фотографии по юзер айди по пожарным по всем серверам вторая группа серверов это 48 москве серверов где также боюсь ради вся переписка пользователь их контакта и какая-то персонализированная информация альбомы дневники и прочие сервиса которое относится пользователь также по шар жену поделена между серверами прежде чем рассказать о самом модуле я немного расскажу про вообще откуда возникла задача что-то найти внедрить вот как бы какие проблемы откуда про ешь пришли проблемы если кто то знает наш сайт доить ангарский или вообще знает и then графское сайта современности то тот видит следующая тенденция что за последние 3 года все веб-сайты резко изменили подход к фотографиям пользователи то есть раньше там в анкете было 2 3 фотографии очень низкого качества с neo тентом на мобильный телефон со дна с мегапиксельной камерой и вот это все устраивало это было там примерно 3 года назад за прошедшие три года что я работаю в компании общей исходящий трафик фото серверов выростом 600 мегабит до 2 с половиной гигабита при примерно том же количестве пользователей реальности конечно он еще более вырос и если учитывать новую аудиторию эта задача бизнеса потому что бизнес хочет чтобы фотографии пользователя были яркие и четкие чтобы когда я захожу просматривать фотографии там девушек и меня не было сомнения написать ей не написать ей тут она плохо получилось тут вообще непонятно что и то есть фотография должна быть четкая это делается так длина для можно сказать для разгона аудитории чтобы вновь чтобы внутри сайта было запустить с большим темпом внутреннюю активность плюс большинство сайтов современных уже предъявляет следующие критерии что вот пока ты не загрузишь минимум три фотографии тебе там даже нет не разрешат кому-то написать а если ты загрузишь 5 то тоже можешь всю детальную информацию посмотреть также вот взрыва активности на фото серверах привело это внедрение рекомендательных систем различных на сайте когда после просмотра одной анкеты тебе предлагают по каким-то критериям смотреть еще несколько сайт думает что вот там дженнифер лопес очень сильно похожи нас на нижние анкета приведенные внизу таким образом те максимально хотят удержать на сайте чтобы ты не ушел чтобы ты был вовлечен этот процесс чтобы у тебя не было даже никаких поводов сказать вы ниже мне что-то не нравится я уйду вот эти все бизнес-задачи привели к тому что очень сильно выросла нагрузка на фото сервера я уже сказал в 4 раза плюс увеличилось количество крапов еще их три года назад их было три сейчас 6 вот сейчас готовится 7 все крупы стали более тяжеловесные появился крупы hd качество также так как мы начинаем с помощью хороших фотографий разгонять вот эту внутреннюю активность что все начали друг другу писать очень сильно выросла нагрузка на сервера где к хранятся ваше сообщение и внедрение различных сервисов там допустим вот девушка может в своей анкете задать вопрос который увидит все и если вы при посещении thanked и ответите на этот вопрос она получит ваш ответ и сразу поймет стоит ли с вами начинать вот эту переписку то есть добавленный бизнес-задачи чтобы как можно большим человеком ты написал и как много больше как можно больше людей тебе ответили исходя там на твоих предпочтений твоих фотографии к сожалению всегда все бизнес-задачи для улучшения сайта для привлечения более аудитории приводит к тому что растет нагрузкой из не надо что то делать для эти эти проблемы которые мы получили это самая основная проблема что для обычно для бы для серверов баз данных и что для класс 1 фото серверами с фото серверами это увеличивающая нагрузка на диск то есть очень сильно подрастает и о на файловый на на диск и вечернее время уже начинает все притормаживать вот тут вот у меня графика того как сильно начал расти нагрузку на диск на фото серверах начиная с конца там 13 года в какой-то момент есть взрывной рост это выложена рекомендательная система когда те во всех случаях предлагаются посмотреть еще еще красный график это вот там некий прогноз того что что было бы если бы там в начале октября не были проведены соответствующие работы это первая проблема то что растет нагрузка на диск очень сильно на фото серверах вторая проблема это то что очень катастрофически начинает заканчиваться место вследствие того что мы сделали много крапов мы сделали минимальный допустимый разрешение фотографии раньше можно было там закачивать фотографию 200 на 200 сейчас там уже разрешение что-то 600 на 600 соответственно фотографии стали большей боли тяжеловесными и место место начинает просто заканчиваться тает на глазах это две проблемы на фото машинах и на класс тельма искали серверов тоже существуют две проблемы люди начали писать друг другу стал на стали знакомиться чаще друг с другом нагрузка на дисковую подсистему выросла также есть вот там мой прогноз что это еще будет она расти и расти потому что нас пик посещаемости это конец ноября перед новым годом из с дпс серверами вторая проблема здесь не место кончается здесь просто не хватает запас производительности можно сказать что там если очень нужно то в дневное время нельзя никакой произвести не альтер не посещать статистику это только можно делать в ночное время хотя все хотя основном и и те же самые тяжеловесные таблицы в этих sharding серверов там таблица с сообщением занимают не более 250 мегабайт но все равно нельзя и хай треть это приводит в дневное время к деградации посещаемости падают хиты поэтому здесь хотелось бы конечно сбить нагрузку а главное получить за про запас производительности который позволит работать системы и в дневное время чтобы что-то можно было заменить произвести какой-то срочный alter вот эту проблему мы увидели уже начали видеть летом и она еще не так очевидно понятно что ее можно отложить она вырастет там через год когда на фото серверах закончится место через год когда утилизация достигнет каких-то 70 80 процентах в прайм-тайм вечером но мы решили решили не следовать русской поговорке пока гром не грянет мужик не перекрестится и начать и начать работу за заранее при этом из сервера которые сейчас обслуживают что фотографии что всю переписку они изначально не с самого плохого качества не самого плохого уровня фото сервера это дело вский сервера на 10 с десятым рай дом из состав ских дисков 6 соток с большим количеством памяти 128 гигабайт но все равно при этом они в вечернее время достаточно подтормаживают мы сквер сервера это сервера на 4 сосновских 15 карл рпн их 6 соток и объем оперативной памяти и 32 до 64 гигабайт на серверах конечно 64 проблемы не не настолько критична к на критичные на серверах 32 гигами когда мы поняли что надо что-то делать встал вопрос что первое конечно очевидное решение это докупить серверов и сделать перри sharding допустим добавить пару серверов фото ферму из каждого фото серверов взять по несколько префиксов и перенести на новые сервера ровно та же самые смазка с серверами докупить по несколько серверов и с каждым из фильма шины унести по одному префиксу на новые машины все поделено по юзер айди ваши фотографии все ваши фотографии лежат на какой-то конкретной машины вся ваша переписка тоже лежит на конкретной машине поэтому убирание префиксов там нам из фильма шинах по 10 префиксов лежит по одному если забрать станет очевидно лучше но сервера сейчас при нынешнем курсе доллара стоит дорого и этого делать совсем не хотелось потому что убирание префиксов займет очень долгое время и это не конструктивный подход потому что ходил вложение денег это самое простое что может быть и в тот момент когда мы думали что же что же придумайте тасс фото машинами было очевидно конечно же надо всего лишь он же на xi включить кэширование прокси кэш начать кэширование ответов либо над мпф с либо на ssd и очевидно нагрузка должна спать упасть но нам очень нужен был юзер пудж то есть чтобы данные с каша можно было удалить по запросу инвалиде ровать кэш но вот юзер пор jun jin jin икса только в платной подписки был на то время не знаю как сейчас но конечно мы могли бы купить 10 версии и забыть об этом но в случае с mysql сервера между под туда же не купишь engineer что делать если там 10 минут поисков угле привели к тому что я нашел два сравнительно одинаковых решения одно от компании facebook называется flash кэш второе называется б кэш оба решения работа базируется на одном и том же подходе это что производится каширование блоков блочного устройства в случае слышь-ка шум который мы внедрили flash протестирован и опробован наверно версиях ядра linux 26 18 до 26 38 и автор в своей документации говорит что если вы хотите использовать его на каких-то версиях ядра отличных от приведенных то он вам с удовольствием поможет его собрать флеш каш есть в репозитории рпн key для вот для моего centos 6 есть в репозитории ела репо то есть проблемы с установкой не будет флешка работает через flash кэш также как и bk сработает через девайс maker почему я выбрал почему мы выбрали flash потому что при сравнении флешка hp кэш во всех тест которые я находил утверждал что там bk чуть-чуть быстрее но там flash кошачьи видно написала компания фейсбука она выложила это в open source этого dance арси их репозитории лежит уже четыре года соответственно выложив это они наверное это сделает только после того как после того как они протестировали его опробовали на всех своих серверах и доказали что действительно надежный продукт он уже более там двух лет не менялся он стабилен и плюс то что они его сами используют на месте серверах они у них на месте серверах в той документация профлеш кэш было написано что на серверах используется sata диски а каширование идет на ssd это и объяснил выбор в пользу flash каша следующий вопрос который надо задать себе почему вы думаете что каширование в данном случае приведет к успеху если кто-то знаком с тем с нашим сайт сайтом и не с какими-то рейтинг web-сайтом то про фотографии очевидно что до самые популярные фотографии самых красивых девушек запрашиваются большее количество раз будучи за кашированного флешка шире еще какой-то cashier это собьет нагрузку в случае с перепиской пользователей в это тоже было очевидно что хоть нет что это собьет нагрузку может быть не так не так не так сильно как случае с фотографиями переписка просматривается часто вы зашли сегодня вы увидели что вы вчера переписывались какой-то девушка но уже даже не помните о чем был разговор потому что там вы переписываетесь одновременно с двадцатью девушками вы открыли и переписку из ложках подтянулась вся история flash представляет как ибо кэш также представляет четыре метода каширования первый называется right from 2 rate our own 3 right back 4 райтон и 4 райтон или мне кажется что он вообще мало где используется каши руется только запись первые три используются это максимально часто используемые режимы рейд around каширу ему только чтение то точении которое было произведено но кэшируются запись не кэшируются в рай фору каши руется и чтение и запись причем данные которые попали с начала в ssd они тут же они немедленно перейдут в диск без задержки в right back можно назвать каширование там отложенная запись то есть данные которые попали воссоздать чуть только через какое-то время перейдут на hdd это время вот этой отложенные записи это по умолчанию выставлен выставлено 120 секунд facebook в своей документация говорит что они используют рекомендуют использовать именно рейкбек режим из-за того что он очень быстрый но он менее надежный потому что там в любой момент может пропасть питания в стойке и тогда эти данные которые еще были вы создали еще не успели перейти в hdd невозможно но они могут там потеряться paroc rob пацану конечно это редкий вариант и но стоит использовать right back на изначально когда мы поняли что мы будем использовать flash кэш мы начали использовать самого безопасного режима right round где кэшируется только чтение пробовали этот режим и на мысль серверах и на фото серверах и только после двух недель работы когда не побилась не файловая система на фото серверах не чтоб не произошло см с килем мы пережили режимов write from использовали его еще недели три когда получили что и это все работает очень надежный и не про из не происходит никаких fa cup of the уже выбрали режим right back in a master серверах используем right back где очень много записи на фото сервера где запись вообще ничтожна по сравнению с чтением потому что там конечно пользователи грузят в секунду там 50 фотография читается в эту же секунду в миллион раз больше а пост после того как вы запустили flash cached начали его использовать его и спой его установить и настроить очень просто всего лишь у него 3 бинарник а это создание флэша удаление flash дестрой каша и после ребута мощный тот кэш который был создан до ребута его продолжить вот вы его включили флешках начинаете использовать и вас интересует метрики того насколько это популярно над насколько это полезно и к чему это приводит первое что можно здесь увидеть это то что мы используем режим кэширование только чтение чуть ниже я к сожалению обвел красным это то что каждый заполнен на 83 процента второе кашел равно 1 это скорее всего это скорее исторический момент потому что флешка разрабатывался более 4 лет назад и когда ssd был были маленького объема и facebook наверное в тот момент использовал 32 или 64 гб ssd и они максимально старались чтобы в каше были только только данные самые популярные релевантные данные поэтому можно создать черный и белый список того что должно быть закодирована то чего не должно быть закодирована дальше механизм вытеснения с каша рекламе policy умолчать и льный механизм фифа 1 шел первый вышел есть второй механизм lr ее ли streisand reviews по тому как часто используются это тоже наверное все исторические факторы связаны и с тем что ssd были маленький потому что в моем случае ssd 256 гигов и мне кэш до 100 процентов даже никогда и не доходит то есть данные вытесняется с каша путем того что действительно фотографию или сообщение удалили изменили так далее то есть не происходит искусственного тиснения что не хватает просто местные ssd ну и третий параметр очевидный который хотелось бы знать это хитрый первое число это hit rate на данную секунду второе число это средний хитрый за последнее время наблюдением вот там среднее время хитрый то меня 62 процента и доходит он даже до 72 там но бывает она да и 57 то сикхи trade очень хороший и можно на него закладываться и это очень хорошо сбивает нагрузку в тут такой хитрый ты это слайд которого не надо пугаться я его сейчас расскажу очевидно что вот когда мы выбрали что мы будем использовать кошка флешка чтобы сбивать нагрузку на серверах которую возникла путем внедрение бизнес-задач что это не так вот вы решили и тут же все произошло первое нужно куда-то доставить ssd на фото серверах это верхнее беловский компания dell во многих большинство серверов предусмотрено внутри серверов дополнительно 2 место под ssd даже если вот уход сводные диски все занято вот туда мы поставили ssd случае с дпс серверами это нижний сервера от компания supermicro там все четыре стола то были уже заняты и нужно как-то было доставлять ssd можно было конечно же вот этот 10 рейд который там есть пересчитай под машину в 5 рейд из трех дисков получить хотя бы один свободным но он сорок восемь машин перед это плевать при этом надо постоянно носить базу на бы копны и сервера и переключать нагрузку вот это совсем не хотелось сделать потому что это там задача может быть растянуться на три месяца нашли достаточно элегантное решение что мод можно за 500 рублей купить вот такую рамочку в эту рамочку вкрутить ssd диск и и поставить вместо ее сидирома то есть выламывается сидиром и вот такая рамочка вставится вместо сидирома и вот у нас дополнительное пятое место под ssd логичный вопрос возникает что ssd с в данном случае только один случае с фото серверами делов скими их там два и можно сделать зеркальный raid и быть уверенным что вот зеркало вас спасет случае если хоть один диск хоть один из создаем лед а вот случае с дпс серверами где мы поставили только 11 ssd надо что-то думать на самом деле все очень просто что надо всего лишь мониторить вывод смарт-цели обращать внимание вот на эту метрику медиа вера вот индикатор который в принципе это время жизни ssd в новом создает и начинается со ста и потихоньку это уменьшается чем будет чем дороже с с этим это меньше уменьшается то есть в самом дешевом какой-нибудь но не знаю там smart 2 я тестировал это может там за два-три месяца magic если это взять дорогой intel или дорогую прошлую версию самсунга то это может хватить на 2 года уменьшается очень медленно если вот это число приближается к 10 то надо думать уже заменят ssd скоро вылетит при этом ssd мы используем уже достаточно давно и на других серверах не связанные с флешом и этом порядка двух лет то есть через наши руки прошло более 50 sd и вот этот индикатор еще раз ни разу не подвел то есть все достаточно честно как только этот индикатор приближался к нулю тогда и действительно призвать происходило проблема с ssd теперь о результатах основной результат конечно это вот мы сбили очень сильно нагрузку то есть нагрузка была утилизация на файлов на ее фото 3j было порядка шестидесяти процентов прайм-тайм она в этот же прайм-тайм тут же упала до 20 процентов вторая вещь что я говорил что вторая проблема с фото серверами это очень катастрофически быстро начинает заканчиваться место из-за того что мы закачиваем хорошего качества фотографии из этой проблемы мы тоже вышли то есть сначала я flash кэш попробовал не изменяясь версию рэда текущих дисков она была 10 то есть все пополам от общего объема это всего было три три три терабайта на один сервер 33 333 терабайта всех 10 серверов с десятым рай дом утилизация дисков упала вообще даже до десяти процентов тот предыдущий график от как раз касается 6 ряда в тот момент когда мы увидели что утилизация дисков она 10 процентов она настолько незначительно мы решили что давайте-ка мы попробуем сэкономить место и и 10 рейд переделать 6 рейд чтобы получить больше места мы вот все 10 серверов пересчитай пили и уже с 33 терабайт получили 54 терабайт то есть дополнительно flash кэш принес нам просто так двадцать один терабайт место которые активно используются при этом у через при при этом утилизации немножко подросла на шестом ради оно было где-то 10 процентов сейчас на всех боевых машин их шестом gd она около 20 процентов выше не поднимаются ночью она опускается вообще там до двух трех процентов до смешаны в случае с дпс серверами утилизацию получилось сбить она не сбито так хорошо как там когда 60 процентов она сбита до 20 здесь она всего лишь сбито на 20 процентов примерно с пяти десяти пяти до тридцати пяти но все равно это можно при признать хорошим результатом к тому что в принципе мы ничего мы немного денег вложили мы всего лишь докупили ssd и в там какие-то рамки копеешный сервера то мы не покупали и нам мы уже так здорово выигрывали в производительности в случае с дпс серверами мы даже 4 сервера сняли их было 48 сейчас к 44 с этих четырех серверов мы префикс разнесли по другим сервером и вот эти четыре сервера просто перенесли на другие нужды то есть мы получили четыре просто бесплатно дополнительных сервера грубо говоря мой доклад на вообще можно было бы вот я завтра знаю выступает дениса не кино с компанией mail.ru у него доклад называется как тарантул нам позволил сэкономить 1-ми там несколько миллионов долларов то мой доклад можно было назвать как флешка же помог сэкономить минимум пятьдесят тысяч долларов а еще и больше потому что вот этих фото серверов их сейчас 10 с отцовскими дисками и там с нулевую практически нос утилизация максимум 20 процентов мы думаем тем чтобы их оставить всего 4 то есть докупить sata шины и диски 4 терабайтный ssd может быть поставить туда тоже ssd поставить четыре терабайтный таким образом освободить освободит 6 серверов и отправить на другие нужды и тогда в данном случае флешках нам позволят еще еще сэкономить большие финансовые вложения ну вот и все из и самое главное конечно же используйте flash кэш это очень хороший продукт и мы прямо очень благодарна компании гу компании facebook за то что они не просто его выпустили они им поделились и так вопросы а вот вот тут кто-то хотел спасибо большое за доклад было очень интересно у меня вот такой вопрос если mays l сервер стоит уже полностью на ssd дисках знаю это невыгодно в плане финансовым но вот в плане производительности будет ли выгода от использования но вот на других серверах нас лавовых серверах тех данных которые не класс there'sa ван и мы используем ssd и там это потому что используется ssd потому что базу чем маленький там 100 гигабайт и можно поставить недорогие 128 гб ssd в случае здесь класс there'sa ванными бас данными базы данных очень большие 450 гигабайт и нам нужно было его еще что-то пересчитай пить мне кажется что будет хуже логично что будет хуже просто если вы изначально строить архитектура вот вы покупаете недорогие sata шины и диски и всего лишь производите каширование на ssd то есть вам если вы с нуля хотите что-то делать логично поступать так они покупать сразу подвесе объем данных ssd понятно спасибо здравствуйте спасибо за доклад у меня вопрос про то насколько сильно деградирует производительность на шестом рейде по сравнению с десятом в случае выхода из строя диска спасибо очень сильно но мою но во первых если у вас flash если вас конечно флешка шо вы достаточно уже прогрет то есть в случае ребута вы получаете не прогрет и кэш и все начинает тормозить там первые 20 минут немножко отпускает до 2 часов и где-то через восемь часов вас вот получается утилизация вот близко к заявленной то есть если у вас уже прогретый кэш и в этот момент вы вылетел диск на шестом рейде ничего страшного не будет я уверен что любил пройдет без последствии то есть но деградации производительность 6 рода будет не настолько сильная что приведет к которого knock некоторого к проблемам на серверах то есть главное в какой момент вы получаете вылет дисков прогретом или непрогретом каши но в общем то фейсбук об этом и прям предостерегает в документации что и все хорошо только если кэш прогретый и вот что вот у них происходит прогреть и каша это где то 30 минут и они пишут что в этот момент конечно с остаточными дисками творится сталинград несколько вопросов а первые вы говорили что вы искали решение кэширование на яндексе и что там освобождение кэша было только в платной версии вы не рассматривали varnish допустим по папе можно дёрнуть любой улу сказать больше людей если честно varnish рассматривали конечно но в тот момент хотелось найти одно решение которое можно применить и там и там то есть из фото серверами sql серверами а так бы получилось что нужно два решения одно там типа varnish для фото серверов и какое-то другое для москаль серверов то есть только из-за того что хотелось внедрить одно решение не 2 и вы установили нет в есть много споров о том какие лучше собираются рейды на чем софтверные хардварные есть плюсы и минусы как вы считаете вот с вашей точки зрения не позже если лучшие почему но в серые да железные конечно из нужно производительность 10 рейд если нужно место и не нужно производительность возьмите 5 или 6 right с вашей точки зрения лучше все-таки 10 хардварные но так исторически сложилось что-то сервера всегда были куплены с железным рядом с большим количеством флэш-памяти то в самом raid-контроллеры то есть там либо 512 мегабайт кэш контроллере либо гигабайт окончил неожиданные сбу и сосуды в качестве крыша то есть допустим счетчик миди индикатор не сжигал а там что-то что-то какой-то выбрать нет ни провести вот эти неожиданные сбой это как раз к тому что мы начали кэшировать только ридом и не дым и все-таки это фейсбук но мы скептически относились мы боялись что это все разрушит что все наши базы на мыске ли там в какой-то момент разлетятся и проблем не было в случае с ssd нет там два года используем вот этот счетчик действительно работает то есть были случаи когда даже из с нулевым счетчиком который вот просто было надавил машине и не было и даже если там данные он сгорит и пропадут данные не был ничего страшного мы его оставляли это еще про это такое создающий проработал полгода но вот такого что этот счетчик там был сорок-пятьдесят ssd вышел устроить такого не было спасибо спасибо за доклад вопрос такой в прошлом году зарелизили новую версию lvm с поддержкой функциональности кэша на ssd и сравнивали производительность тестировали смотрели в эту сторону нет к сожалению спасибо здрасте вопрос такое вы сказали что допустим каширин с создаете отличается питание может пропасть данные не совсем понятно лежит это только для right back а когда у вас данные сначала попали на и ssd и только вот это отложенная запись когда все данные будут по истечению 120 120 секунд или того времени чтобы поставить все цели будут перелит она hdd вот в этот момент может произойти какая-то нештатная ситуация во время допустим перелив ки этих данных что-то пойдет не так но на самом деле вот этот хэш он в случае straight back a mon pere системные то есть если какие-то данные вы еще не успели перелиться со создана hdd и вам вынуждена вы штатно погасили сервер то есть вы сказали стоп flash кашу от монтировали файловой системы и reboot нури то в случае загрузки вы можете сказать флешках лаут то есть вы данные ssd те которые не успели не успеет и были перелит и ssd на хдд они увидятся и будет перелит в следующий момент то есть только один режим он считаю вот вы документации персистенции flash не каши вот этого right back режимы не протух аид спасибо такой вопрос вот вы когда начали экспериментировать с кэшированием при записи вы несколько недель проверяли что у вас все работает хорошо это было на глаз что ничего глобально не сломалась и ли вы смотреть что там пол процента пользователей никаких проблем тоже не испытывают мы смотрели error log москве или что там нет ничего неожиданного и мы смотрели там допустим варлок место тоже нет ничего неожиданного на того чего мы не ждем плюс конечно же систему мониторинга за без которой алексей говорил сегодня утром там есть метрики которые там ходят под б с сервером selected сообщение тестовые сообщения из определенных сервисов определенных поля тестовых пользователей и конечно вот это все должно отрабатывать то есть определенный метрики zabbix а говорили что все хорошо понятно с на симон"
}