{
  "video_id": "mhrqLeDduaM",
  "channel": "HighLoadChannel",
  "title": "Точные рекомендации для пользователя: как мы научили систему выбирать контент / Даниил Бурлаков",
  "views": 1105,
  "duration": 3117,
  "published": "2022-03-21T14:07:14-07:00",
  "text": "так отлично так звук есть тогда можем знать всем привет меня зовут данил бурлаков и сегодня я буду рассказывать о том как развивались рекомендации яндекс музыке и как они сейчас работают или как вот меня чуть более формально представили о том как мы кого-то учили чему-то отлично тогда пару слов обо мне сейчас я руковожу разработкой рекомендаций медиа сервисов пару слов о том кто такие медиа сервис медиа-сервис это объединение нескольких сервисов извините за тавтологию яндекс музыка кинопоиске его онлайн-кинотеатр индекс афиши и другие может быть немножко обидно звучит и другие но других сервисах я не писал рекомендации отлично и мне тогда для старта маленький вопрос в аудитории а кто-нибудь из вас пользуется индекс музыкой о огонь супер а кто-нить пользовался из вас индекс музыкой пять лет назад о нашлись а 10 до 10 уже не нашлось отлично тогда вам всем будет интересно вы узнаете из нашего внутренней культуры почему мы считаем музыку розовый на самом деле 10 лет назад а яндекс музыка появилась как часть большого поиска и она служила тем грубо говоря местом где можно пока давать ссылки на легальное прослушивание музыки 10 лет назад был достаточно большой проблемой и яндекс как раз организовал отдельный портал где была полностью легальная музыкой достаточно большая база и вот кажется не очень видно на презентации но на самом деле музыка в тот момент было чисто розовый и с тех пор мы так и называемые самое интересное из тех времен что несмотря на то что это было так давно рекомендации были уже тогда и вот даже вот вы можете увидеть на сайте тогда он был собственный пример радио к сожалению назвать полноценными рекомендациями это нельзя поскольку работали они очень просто и не были персональными они просто рекомендовали случайные треки популярные жанры или в этом случае ради от исполнителя для них то исполнителей редакции собрала специальные плейлисты похожих треков на этого исполнителя а если таковая полу листа не было она играла просто толпы папуль арн их в жанре исполнителя не самое интересное но даже при такой простой реализацию этой фиче были пользователя и пользователи которые ею пользовались слушали ее больше часа в день представляете фича не делает ничего кроме как подсовывают случайные треки и уже много слушают очевидно что когда это заметили по всем метрикам занялись тут же развитием и улучшением сейчас я давайте сразу оговорюсь о том что когда буду говорить что мы улучшаем рекомендации так далее это очевидно говорим об об экспериментах у музыки очень достаточно большая иерархия метрик построена там для текущего например состояние для нас это основная метрика будет количество пользователей у которых есть подписка плюса но очевидно что это замечательный высокоуровневая бизнес метрика практически невозможно прокрасить изменениями рекомендации изменить на ее влиять эффективно поскольку там в подписку индекс плюсов ходит тонны сервис яндекс такси диск кинопоиск и вот там одна из них это музыка очевидно что мы вносим какой-то вклад в этом месте но увидеть это на изменениях очень сложно эта метрика крайне чувствительна поэтому чен и мы тут же переходим каким-то более чувствительным метрикам такие как например конверсия совместно мы смотрим на пользователи которые пришли уже на яндекс музыку и смотрим их конверсию в подписку чуть более опять же ближе к рекомендациям опять же будет дал рекомендации считается сколько пользователя нас реально пользуются рекомендациями каждый день дальше очевидно идет рецепшен и как естественная практик доу и последний метрика является естественно полное время прослушивания полный иерархии метрик яндекс музыке засушить своего отдельного доклады у нас есть также как и в большом поиск есть понятие своих искусственных метрик которые являются еще более хорошими прокси но вот собственной истории рекомендации начиналось 2014 году когда мы запустили первый движок рекомендаций и первым же своим запуском мы выиграли практически 10-20 процентов на времени слушание в день у пользователя вы знаете вот с первого же а запуску вы сразу же получаете большой профит это прям огонь и отсюда сразу начали все это копать давайте я кратко расскажу как бы lust он тогда backend рекомендации can рекомендации был крайне просто устроен как это все в целом работала у нас есть клиенты даже в те далекие времена клиент был ни один не только frontend уже были даже сделаны мобильные приложения в которых это можно было слушать клиенты говорят я хочу послушать например радио по жанру обращаются в бэг-энд музыки который до этого просто выбирал случайные треки а теперь в эксперименте он ходил бы контент там вот я рядом еще нарисовал несколько баз на самом деле на еще до запуска персональных рекомендаций на самом сервисе собиралась довольно много фидбэка но соответственно есть данные о прослушиваниях проектах а если вы там заметили на предыдущем слайде были замечательные кнопочки мне нравятся мне не нравятся тем самым пользователи могли во первых исключать из этого рандома часть треков которые мне нравится и наоборот из лайков ну составлялся специальный плейлист который они могли потом отдельно сами слушать а вот эти все данные собственно хранит у себя бы канг музыки и их надо показывать пользователю независимо от того включен или рекомендации не включены поэтому в момент запроса за радио backend ходит по всем базам занимается авторизации пользователей всем подобным и пока intel уже получают красивые данные о том что у нас есть такое то пользователи с такой этой истории и другое из которых делает рекомендацию и отдает ее на клиента в чем же заключается рекомендация вот здесь маленький красивый трюк которые можно рассказать как сокращать нагрузку на абакан до рекомендации очевидно что можно было бы для радио например ходить за каждым треком вы хотите получить вы пошли получили его поставили играться пошли за следующим на довольно неэффективно представьте себе что рекомендации работают очень хорошо я надеюсь что они работают очень хорошо а тогда фактически пользователь никогда не скип ит ему он только нраве ему нравится эта музыка которую играют он просто ее слушают тогда представьте отличную вещь мы можем отдать пачку треков любимый наш размер 5 мы даем 5 through a и когда клиентах получает он просто ставит в очередь воспроизведение и пока все хорошо он включает один за другим если вдруг не дай бог пользовательских нет или не дай боже дизлайк нет тракт который мы ему поставили пачку сразу же можно выбросить и попросить новые тем самым мы обеспечиваем себе возможность что на активные действия пользователя мы сразу же реагируем мы мгновенно меняем рекомендации но с другой стороны фактически если рекомендации работают хорошо нас эффективно очень сильно снижается нагрузка на саму рекомендательную систему на базы на всю страну отличная идея и так собственно вот так все достаточно было просто устроена про backend внутренность вот этого квадратика backend имели я расскажу чуть больше ближе к концу лекции чтобы рассказать про текущее состояние дел дальше собственно речь была развитие 2015 году собственно назрела необходимость сделать из этого полноценный продукт потому что только радио по исполнителю и там радио по жанру это не так интересно можно сделать намного больше во первых можно улучшать качество самих рекомендаций чтобы они лучше попадали в пользователя а вторая вещь очень тоже интересное и классное можно сделать разные радиостанции очень разные во первых можно делать теперь персональную радиостанцию не привязаны ни к какому жанру и можно делать радиостанцию по настроением то есть грустные музыки по занятие музыка для бега или фоновая музыка и так далее вот какую то конкретную задачу у пользователя что соответственно очень-очень сильно продвигает это все и в 2012 году пятнадцатом мы радостно запустили на всех пользователей специальные отдельное приложение отдельный сайт прям вот все как полагается была запущена отдельный продукт очевидно дальше долгое время мы начали заниматься развитием этого дальше уже развивать как бы с точки зрения продукта было интересно потому что мы могли теперь доносить еще дополнительный фидбэк когда-то вот еще вот при первом запуске 2014 году когда я говорил о рекомендациях вот если вы недавно слушали такая отзывы как раз соседней аудитории вот как раз недавно на самом деле жили в одной системе и мы как бы в самом тогда зародыши нашим мы рекомендовали фактически одинаково как мы так и дзен говоришь нас есть какие-то айтемы и мы их как-то рекомендуем вот с этого момента мы начинали постепенно расходиться мы получали все больше и больше данных специальный из про музыку мы соответственно много анализировали прям mp3 файлы прям the audio которые мы используем соответственно мы начинали в качестве характеристик треков использовать там соответственно сколько грустной треки пользователь в целом слушает насколько бодр и этот трек bpm трека и подобно все технические характеристики audi а мы вот момент начали использовать но довольно быстро мы у первых некоторую стенку из которой мы и ребята из цены пошли немного разными путями мы в этом месте у первую стенку того что нам хотелось бы использовать более тяжелого алгоритмы и собственно та же самая задача решалась нами чуть-чуть по-другому мы решились сделать следующее в отличие от данных которые это важно что эта лента она плюс-минус бесконечной так устроены все виды у нас есть решение переметнуться на соседнюю сторону мы можем сделать плейлист и а и в 2018 году очевидно там вот как раз разработку всего этого подхода про которые там сейчас расскажу будет занимала еще где-то 2 года мы где-то в 2 тыс 2017 начали работу над ними и главные идеи здесь была такая если мы варим плейлистом раз в день или раз в неделю это прям супер классно потому что теперь у нас в оффлайне есть дофига времени чтобы рекомендовать у нас есть столько угодно тяжелые модели мы можем применять вообще идеальный же мир давайте подумаем как это все было была устроена значит как всегда у нас есть логе данных соответственно огромное количество событий пользователей то есть ответственно пользователь лайкнул skip null дизлайк ну вот его нас дальше есть какие-то базы бэг-энда в которых хранятся соответственно там данные тома доступности треков разметки правообладатели о том что трек такое это принадлежит какому-то исполнителю ну и так далее мы соответственно собираем все эти данные и радость научим на них наборы моделек я вот в отличие доклады цены сейчас не буду подробно влезать в матричные факторизации все остальное очевидно мы делаем ровно то же самое нас крайне похожие модели в этом месте мы учим тут же и нейрон очки которые смотрят в прямо в контент трека нейрон очки которые смотрят за коллаборативный фильтрацию естественно мой отдельный учим матричные факторизации затем мы вот обучили все эти множества моделей и запускаем мо прежде изжогу и в этот момент мы говорим в ней прямо за несколько операций ему придется мы варим прям готовые плейлисты для пользователей то есть выходом определился появляется табличка в которой у нас там в одной колонке грубо говоря пользователя в другой колонке у нас грубую оля его плейлист список его тракт в этот момент итоговый результат из кластером определился мы перекладываем в базу манго сбил и дальше все должно быть счастлива наш счастливый мир был не так долго когда мы это все воплотили в жизнь потому что тут же появились некоторые down side и такого подхода down среды были такие во-первых довольны ресурса емко как вы уже могли заметить там у юджина и у нас ровно одна и та же проблема теперь вам придется строить на всех пользователей дальше вам надо было построить только на пользователи которые пришли и захотели послушать плейлист то извините сейчас вам придется построить на всех нас как и у джен это сотни миллионов пользователей это крайне накладно строить плейлисты второе map редис кластер и не отличаются сверх надежности и все кто работал когда-нибудь мы придется он знает что там время сходимости задач довольно случайно да и иногда мо придешь кластером закрываются на там обновление или ещё что-нибудь не дай бог с ними случается и поэтому в принципе вас может случиться такое что там а сегодня мы придешь кластер не работает если у вас нет плейлистах для пользователей то как бы у вас большая проблема и вы ничего не можете с этим сделать определены такой от проблем которые нас будет решать дальше очевидно чуть более техническая задачка ну как бы если вы имеете банковую заливку в какую-то real-time базу типом манга тебе но вообще-то было вы всегда будете балансировать между тем с каким бесом вот туда льете свой банк с тем что вы из-за этой базы при этом могли читать она не падала то и соответственно время заливки вы балансируете с тем насколько ваша база вообще живет при этом ну и очевидно что сама оффлайн часть когда мы сказали что мы хотим сколь угодно тяжелые модельки очевидно тоже занимает огромное количество времени для там плейлиста дня это занимало по 10 часов очевидно что эта цифра немножечко является балансом между временем оптимизации грубо говоря кода и ресурсами которые у вас есть на этом а при диск кластере благо наши рекомендации идеально параллели цена все возможные потоки и поэтому там эффективно удваивая размер ресурсов квот который у нас есть на мапри диск пластырь мы фактически можем выиграть в два раза по времени но это конечно же тяжелый путь потому что ресурсы стоит денег надо оптимизировать код как же мы решали этот список почти не возможных проблем все очень просто давайте придумаем 1 красивый find а вот будем решать проблему в начале с нагрузкой на базу давайте заведем отдельную базу в которую мы будем заливать наши новые построенные плейлисты и отдельную базу который будем использовать в качестве каша из которой пользователи уже будут забирать свои готовые плейлисты то есть мы будем использовать дополнительную базу качестве каша то есть пришел пользователь он посмотрел в базу каша нашёл и там свой плейлист или не нашел после чего запросил базу если вдруг не нашел то соответственно запросилась основной базой который льется bulk новых плейлистов какие проблемы здесь решаем первые проблема мы решаем во-первых мы тут никак не сказали вообще-то плейлисты бывают пользователи бывают в разных часовых поясах вообще хорошо будем обновлять плейлисты не по москве или не как-то там сошлась мапри детдома а вообще-то как бы по времени в этом месте с известным мы позволяем в этом месте сказать что мы хотим переключиться с одного пола листа на другой в такое-то время и посмотреть в каши для каждого пользователя в свое время сделать эту перекладку 2 классная вещь в этом месте которое позволит нам решить с тем что нам надо варить только на активных пользователей а давайте подумаем вот теперь у нас есть две базы в одной базе у нас лежат по сути пользователь в которых мы сказали что мы знаем что они пришли и папа ложись или свой плейлист а во второй базе у нас есть грубо говоря все запасные плейлисты для всех пользователей вот в этот момент мы понимаем что если пользователь не пришел и не попросил у него все еще есть запасной плейлист ему не надо его переваливать очевидно что и и венчали его надо переварить то есть там новинки месячной давности никому не интересны apple свой плейлист ампулу годовалой давности тоже рано или поздно их надо обновлять но если у пользователя уже есть запасной плейлист его можно не обновлять там нет большой спешке в этом месте поэтому эффективную разделив такую базу мы сможем эффективно строить плейлисты только на тех кому его надо обновить вин отлично а с надежностью мапри disclose тирану к сожалению мы не можем починить бредис plaster или сказать что он всегда работал поэтому как всегда если носить эре есть базах которые всегда лежит запасные плейлисты мы можем хеджировать вычислений мы можем даже до того как мы дождались чит bk пользователь не на предыдущий плейлист заранее построить новый плейлист положить backup следующий который будет готов если что-то надо будет изменить и дальше когда мы уже получаем фидбэк пользователя в течение дня на этот полились построить правильный вот дальше у нас отвесной либо мы узнаем о проблеме ирана и тогда мы или быстро начинаем синеть либо мы узнаем о проблеме поздно но тогда у нас есть backup винвин ну и как всегда никому не помешает доброй порцией оптимизации и примерно оптимизация my и улучшениями наших моделей занимались следующий год мы а и по пути оптимизации мы радостно по заметили slaed очень интересную вещь среднем наши рекомендации работали десятки секунд на пользователя это очевидного единственно возможное место где такое применять этом определюсь там на самых тяжелых пользователей доходило до минуты очевидно что мы начали оптимизировать общее циpкa время чтобы в целом количество job которые мы используем на мапри dice процессах уменьшалась и соответственно все быстрее сходилось но очевидно что после наших оптимизация там оптимизации могут быть довольно просты и часто это можно просто подложить правильные каши также мы разработали свою и memory базу которая нам помогало очень быстро работать в этом месте и последнее но и самое важное мы оптимизировали сами модели когда у вас есть возможность экспериментировать с многими тяжелыми моделями мы довольно быстро можете перейти к вопросу а не сделает ли вам такую же модель но попроще которые учат тот же самый таргет но как-то попроще вы могли всегда найти там мой любимый пример в этом месте у нас когда вот мы еще запускались мы использовали две тысячи мерные бединге то есть по сути им beating аудио под построенный по аудио трек а был представлялся и 2000 цифрах расчет расстояний в таком пространстве косинуса тут довольно дорогое долго и тяжелое занятие однако нам удалось с некоторыми хитростями уменьшить размер на в 40 раз до 50 и сохранить все качестве даже немножечко выиграть по качества потому что в меньшем размеренности пространстве оказалось чуть лучше свойства наших викторов они лучше искались ближайшие и быстрее что самое важное довольно быстро мы пришли к тому что мы начали работать в районе 5 секунд пять секунд все еще не runtime не звучит как runtime но мы пошли дальше мы подумали следующие но вообще там рекомендации крайне классно параллели со мы можем проверить прям каждый запрос у нас есть куча алгоритмов наше верхние уровни устройство крайне похожи на устройстве вот слушали доклады на который у нас есть на верху самый большой котбус в которых кормятся куча факторов различных вот все эти факторы на самом деле их расчет можно делать не последовательно один за одним об параллельно и благодаря тому что мы раз параллельные вычисления в ранд аниме мы добились того чтобы эту штуку работала уже за секунду с учетом того что с секундой от знают для большинства бы киндеров звучит как смертный приговор а вот но к счастью в этом месте нам помогали front-end и которые работали две секунды рисовую рисую все картинки поэтому там наши секунда была не настолько тяжела в этом месте но очевидно что потом мы еще оптимизировали в этом месте самое важное что вот когда мы сделали 1 б когда выкатили рекомендации в онлайне мы были дико счастливы потому что все работал и все было отлично тут же попробовали сделать первый эксперимент в котором мы захотели сделать так давайте сделаем ядро пользователей для которых мы будем варить в оффлайне все и тем самым для core аудиторию нас будут готовы и плейлисты а для онлайн ему уже сделаем вот типа если пользователь не пришел или вдруг у нас факап какой-нибудь случился мы тогда радостно своим плейлист в онлайне но довольно быстро мы поняли что есть классный финт от того что мы можем варить прямо в оффлайне прям когда пользователь щелкнул что он хочет плейлист мы можем учить контекст мы знаем и время в какую-то району это сделал мы учитываем можем фидбэк весь до момента когда он открыл этот плейлист учесть и соответствие мы можем учить больше информации о пользователе в момент когда он попросил рекомендацию и получилось так что если мы хотим делать оффлайн плейлисты для korra аудитории то мы делаем на самом деле более плохие плейлисты туристы проигрывающие по метрикам чем если бы мы их делали все в в онлайне и в этом месте мы радостно переключились полностью но онлайн практически все наши рекомендации теперь работают в онлайне так но подождите кажется что мы здесь сделали маленький круг помните вначале рассказал что мы отдавали пачки по 5 треков они кажется что плейлист там это просто пачкой но чуть большего размера и вариться на раз в день вот на самом деле рекомендации прошли в этот замечательный путь по спирали мы нашли начали начали от радио в онлайне в котором были супер простые модели в которых было все достаточно просто и мы убирали сильно в таймс темп в тайминги после чего мы увидели что у нас есть достаточно большой возможность прироста качества за счет того что мы усложнили модель есть мы могли сделать модели сложнее они считались бы дольше мы выиграли бы много качество очевидно что вам следующее удвоение как всегда любая система быстро достигает своего ощущение бы уже не давала профита и вот переходу в офлайн в первый раз когда мы перешли мы выжили практически весь профита сон осложнения модели которые мы могли эффективно получить после этого там дальнейшее усложнение моделей делая их еще толще еще больше не давал оба большого профита и в этот момент мы перешли к оптимизации инженерные задачи и как бы сделать от все работало быстрее ему нам удалось перевести в онлайн но этот переход был не только чисто инженерный нам еще удалось донести дополнительную информацию про пользователя в этот момент мы выиграли опять по качеству то есть первый момент когда мы переходили из онлайна в офлайн мы теряли информация о пользователе но выигрывали так много усложнив scheme все моделями что это это компенсировала то теперь мы пошли в обратном направлении соответственно отыгрываем всю real-time информацию про пользователь который у нас есть огонь супер и собственно опять же тот дизайн что я показываю не финальный к сожалению не руковожу дизайном поэтому них еще не все готово эксперимента я б по дизайну все еще идут а вот поэтому не берусь сказать что это последняя версия как оно будет выглядеть вот в любом случае на какие-то части пользователей это уже выключенным так или иначе вот мы теперь говорим что ради у нас опять возвращается на главную страницу музыки и дам соответственно соседствуют вместе с нашими умными плейлистами теперь давайте немножечко поговорим о внутреннем устройстве как это все устроено внутри чуть больше подробностей во первых за эти долгие время мы у нас я там рассказывал что у нас была база кассандра манго mais quel который там используется для разных хранения разных наборов данных пользователей в частности теперь мы полностью переехали на нашем внутри яндекс узкую разработку и яндекс б соответственно в ней мы получили и большие качество таймингов и надежность ну и что самое интересное для нас мы получили это как достаточно большую бесплатную поддержку для нас поскольку база который мы используем сами располагались на нашем железе и маинтини ли мы их сами в этом месте индекс до bass нам представлялось как сервис мы были очень туда переехать начали хранить значительно большее количество событий на пользователя когда все все только начиналось самые первые рекомендации работали на двух тысячах событий на пользователя всего сейчас мы обрабатываем в runtime и до 30000 событий на пользователя 2 ну и наверное одна из самых важных вещей нашего окончательного перехода теперь мы учитываем мгновенно все действия то есть ответ на вывод вы сделали какое-то действие например слушаете радио сделали skip нём мы мгновенно получаем информацию об этом степи и мгновенно обновляем все nb динки все нее рамки с учетом этого скипа у нас не не real-time processing а чистый real-time processing в этом месте поэтому мы абсолютно мгновенно прямо на следующее же действие пользователя можем учесть весь вид есть фидбэк то есть соответственно вы сделаете skip прогоняется весь алгоритм рекомендации с учетом его и обновляются им бединге пользователей и все что можно про пользователь узнать обновляется а также вот я уже говорил чуть позже скажу чуть подробнее про in memory базу данных наш backend в основном написан на java и в этом месте нам нужно было эффективная реализация кашей ближайший аналог наверное в open source будет memcache единственное отличие будет том что mem cache достаточно медленная база по по нашим меркам поскольку для наших задач когда мы на запрос делаем сотни тысяч обращение к нашей базе заданными а там соответственно связях от трека артистом от артистов к языкам от артистов жанра мы так далее мы имеем огромное количество веер запросов которые приходится задавать к этой базе и в этом месте использовать просто java вы и каши можно так когда-то начиналось все но это приводит к большим проблемам с горбач коллектором внутри джаве возникает большое количество проблем поэтому нам нужно было плюс-минус standalone реализация база данных которая работала со скоростью сравнимый с кашами java при этом так как главный ее часть разработки началась во время когда мы делали мапри deus для плейлистов все в офлайне это ей базе надо было уметь подниматься просто вот в офлайн джоби внутри mapreduce а теперь давайте поговорим немножечко о том как устроен и рекомендации прямо внутри на самом деле крайне похожи на то что вам только что рассказывал один опять же на пришедший запрос например за персональным станции мы отбираем десяток тысяч кандидатов для пользователя опять же примерно 10000 там нет ровные отсечки откуда мы берём их также как и джинни мы выбираем некоторые вектором beating пользователя и находим ближайшие музыкальные композиции к пользователю которые с наибольшей вероятностью ему понравится но при этом у нас есть огромное количество знаний о самих метаданных трека и о метаданных пользователя например мы знаем что пользователь много слушал какого-то артиста и у него вышла новый трек классный кандидат может быть им беден даже у него не настолько сильный может быть даже он пока ещё не сошёлся его послушал слишком мало пользовать возможно это вообще первый день то этого трека но уже классно мы можем просто знаешь что это новинка их не так много мы можем добавить их кандидаты соответственно вот этот набор кандидатов он собирается из куча разных источников в итоге мы получаем такой достаточно большой пул дальше нам пришлось экономить немножечко ресурсов и ускоряться вот один из этапов наше ускорение был в том что мы привели производим при ранг предварительные ранжирования это опять же котбус но достаточно меньшего размера в нем меньшее количество деревьев и используется меньшее количество чей такой достаточно упрощенная модель задача ее собственный выбрать из вот тех приблизительно 10000 кандидатов там бывает 15000 бывает 8 отобрать 3000 для основного ранжирования и уже в основном ранжирование мы используем я не шучу около тысячи факторов о паре user through это совершенно различные факторы о том как банальный там лайкнул ли пользователь артисты этого трека или там score какую-нибудь матричной факторизации на нем или предсказаний нейрон кино сколько это грустный through ну и довольно интересные факторы там например какова история взаимодействие пользователя этого артиста например если вот мы выделим всю историю взаимодействия пользователя артиста как долго пользователь например непрерывно слушает его у неё есть только play или лайки на нем и нет скипов и дизлайков и подобное вот все подобные факторы их прямо огромное количество мы всех радостно считаем на каждый запрос и собственно из финального ранжирование вот эти три тысячи треков мы теперь был упорядочили наилучшим способом для пользователя мы набираем итоговую пачку 5 треков для пользователя в этих замечательных 5 треках нужно проверить фильтрацию например мы не хотим повторять пользователю трекера мы не хотим повторяться непрерывно играет одного и того же артиста мы хотим сделать некоторую предку чтобы радио выглядела достаточно разнообразным такие правила кажутся очевидными но есть и менее очевидный например есть треки которые называются одинаково наверно в их тоже не хотите есть от с по треки которые звучат одинаково двоих наверное тоже не хотите не они могут быть в том числе от одного исполнителя но они могут по разному называться часто бывает такое что на 3 называющийся по-разному это какие-то переиздание на самом деле звучать дико похожим на источник и нам их надо отличать между собой поэтому здесь мы боремся с огромным количеством дубликатов и фильтруем и таких фильтрацией на самом деле больше десятка которые нам надо сделать получается итоге вы и 5 треков которые мы отдаем бы кант музыки которая дает их клиентам уже ссылочками на mp3 файлы теперь overview как это все выглядит собственно нас есть runtime развернутый так же как у всех нормальных людей в облаке в там же на тех же самых кодах поднята нашей memory дата база в которой мы собственно храним независимые было бы данных нужные для нашей работы то есть слон говоря тот ключевая вещь потому что in memory дата база оперирует был обоими кусочками наших данных соответственно у нас ни один единственный большой блок всей всех данных а всех треков которые мы обновляем ral должны обновить разом а мы их делим на маленькие кусочки например статистики по прослушиванием отдельно матричная факторизация отдельный результат расчета нее рамки отдельно поймете что эти вещи между собой достаточно независимо их можно за достаточностью свободы и обновлять независимо друг от друга и причем с разной скоростью очевидно что там статистике банальные можно обновлять очень быстром отличные факторизации там мы тоже пересчитываем раз в день для там популярных треков надо рассчитывать там раз полдня блага у нас не настолько текущий контент на сколько в день и в этом месте с это ясно основным источников данных для нас является ис-3 некоторый аналог внутри нас реализованный и из него собственно мы регистрируем новые было бы данных которые у нас появляются и нашей memory дата базы их забирает к себе заодно контролирует чтобы не перегрузить сеть чтобы там все машинки разом не начали качать новый блок данных и положили сеть дальше нас есть группировка worker of который занимается построением этих блогов данных закладка их в истре а собственно данные для этих блогов они либо соответственно сами рассчитывают либо соответствие запирают исмо придешь мастеров также нам определюсь и мы расчитываем как раз наши матричные факторизации там же мы забираем разные выгрузки бэг-энда там о наличии прав и так далее теперь ну собственно некоторые треков и реками взаимодействие с оффлайн процессами также как наверно у всех есть большие проблемы с тем что есть супер разные по типу данных есть соответственно данные которые надо обновляется либо примерно real-time есть данные которые надо обновлять раз там в 15 минут там например данная доступности треков или там есть данные которые нам нужно обновлять очень редко там раз в неделю например там пересчет нейронах данном случае можно выполнять очень редко результаты каждых оффлайн процессов чтобы все работало достаточно стабильно ведь вот этих независимых кусков про которые можно подумать их не 2 3 и не 10 штук и больше 70 с текущей реализации поэтому требуется крайне большая независимость каждый из этих процессов чтобы все работало достаточно надежно если вдруг какой-то из них отказал мне отказала вся система можно было локально разобраться с одним из них очевидно что вы должны проектировать свои данные так чтобы вы переживали не обновление какого-то набора данных например плохая фича рассказывает следующие как много прошло времени с например 1 лайк она исполнителя в принципе в природе до текущего момента ну а ляг характеризующие как давно исполнитель появился на сервисе или стал популярен это плохая фича потому что если ваш snapshot продолжает не обновляться вашу фича у всех радостно начинают увеличиваться увеличиваться и увеличиваться это очень плохо поэтому например такая же простая фича но время до по к моменту построения ставшую туз с 1 лайк а до момента построения snapshot который будет во все время применения константы будет уже хороший вы будете независим не настолько зависим от обновления ваших данных очевидно что это всегда плохо должен вы не обновили данные во время или слишком поздно но так можно переживать это значительно проще ну и как всегда мониторинге мониторинге мониторинге примерно на всех уровнях на отмониторить логе проверять консистентной snapshot of в общем за долгие годы мы много с этим стилем так ну и в качестве итогов собственно я как уже сказал могу подвести только итоге внедрение внутри имели я не говорю о дизайне дизайн тоже имеет огромное влияние на наше метрики поэтому пока что мы я поговорю про и внедрение мэра тут тоже надо понимать что это не была едина момент на то есть мы не сказали что вот мы и ждали долго упорно копили все наши наработки новые базы новые факторы и потом раз и решили всех внедрить естественно этот был процесс крайне итеративный каждую внедрение был очень маленьким мы заменили новый вектор применили базу данных увеличили скорость ответа и так далее затем чтобы так же как и всем вам я думаю было интересно сколько же мы в итоге наработали мы сделали собственно обратный эксперимент и откатили практически там большую часть наших изменений на небольшую группу пользователей тем самым этих пользователей были рекомендации оля годовалые полуторагодовалой давности не том что их качество этих рекомендаций просела а именно с точки зрения того что вот мы использовали алгоритмы настолько старые которые мы не улучшали и вот сравнение с этим мы получаем огромный прирост и дал рекомендации и двухнедельного рецепшена и нам даже удалось практически прокрасить конверсию в подписку профессиона 2 процента но все равно для рекомендации и довольно большой результат огонь супер спасибо за внимание буду рад услышать ваши вопросы спасибо большое и сейчас мы я напоминаю что можно задать онлайн вопрос и мы выведем соответственно на сцену доклад так я смотрю есть руки давайте начнем вот с отсюда дальше потом дмитрий спасибо за доклад хотелось бы узнать поподробнее как вы отличаете похоже музыку то есть не то что это одна и та же под разными именами то что она действительно похожи да здесь достаточно интересный подход естественно и я не мог рассказывать про большую часть имели в этом месте я здесь мы на хайло виде поэтому большей части рассказывал про инженерная часть здесь довольно интересно у нас есть отдельный процесс на самом деле вы знаете таком сервисе shazam которые узнают музыку то у нас соответственно есть внутренний сервис тоже распознавание музыки на самом деле он появился не только по причине того что мы хотели сделать классный сервис для пользователя чтобы они могли нажать на кнопочку узнать что там сейчас играет в такси например у них а еще по требованию правообладателей в том что они хотели чтобы мы проверяли соответственно отсутствие дублей в базе сообщали им об этом там есть тоже некоторый набор требований в этом месте поэтому соответственно с тех времен осталось достаточно большое количество на работа как работать аудио и соответственно мы и говорили о том что их им бединге должны быть крайне близки ну если им некоторые хэши для того чтобы это все быстро работалось чтобы мы могли находить быстро дубликат это приближенные дубликат это не ровно совпавшее mp3 это достаточно классная тема видео locals and steve кашинг если когда-нибудь слышали про такое вот собственно используется этот подход для того чтоб объявляет что два трека являются близкими дубликатами спасибо большое за вопрос и вот пошел молодчик спасибо за доклад на нем вот хотел уточнить например пользователь покупает билет на концерт на яндекс афиши естественного это планируете смотрите потом рекомендациях предлагать ему этого исполнителя чуть больше а если какая-то такая вот метрика что пользователь находится долгое время в области где находится кого там фестиваль концерт так далее вы собираете эти данные тот же и на основе его местоположения и пробуйте как связать это с музыкальными событиями или этого к слишком сложно не это действительно классный вопрос идея такая была довольно много исследований в этом месте проведена мы смотрели на взаимодействие фидбэк в яндекс афиши и в яндекс музыке соответственно мы смотрели на то как можно использовать данные яндекс музыке о том что ты слушаешь для рекомендации новых концертов новых музыкальных событий и так далее и наоборот использовать данные покупок в афише внутри рекомендации музыки оказалось что вот эффект который вы подчеркнули о переходе из покупки концерта в прослушивании музыку он достаточно мал он есть но он очень маленький мы использовать идеальный очевидных не хочется выбрасывать это важный фидбэка пользователь особенно когда они мало известно но на самом деле намного более полезный фидбэк был в обратном направлении как раз наоборот рекомендовать концерты которые идут значительно более классно своим когда пользователь видит персональную подборку его любимых исполнителей который всегда он слушает более того мы там специально на яндекс афиши были приняты довольно строгие правила какого за сколько времени до концерта можно его рекомендовать специально для нас увеличили эти рамки чтобы мы могли рекомендовать более дальние концерты но намного более персональные спасибо за ответы еще маленький вопросик раньше в приложении была кнопочка поиск музыки ну что-то вроде встроенного shazam и а потом она пропала вот будет ли она возвращено и куда она делась и почему опять же я не могу не отвечаю за дизайн поэтому она точно есть и наш backend все так же радостно работает наш внутренний shazam также прекрасно работает как всегда мы им активно пользуемся внутри спасибо но это внутренняя кухня так сейчас молодой человек вот там и потом вы кстати у нас есть онлайн вопросы что мальчик а микрофончик нему сначала сначала вот туда просто спасибо большое за доклад у меня два маленьких вопроса 1-я возможность задать ждал наверно лет пять у меня очень интересует почему я когда-то активно пользовался индекс музыкой почему в рекомендациях помимо треков которые возможно не понравится регулярно закидываются треки которые своём плей-листе потому что мне кажется это слегка нелогичным в чем суть есть в этом какая-то подоплека как как это работает а может пояснить из есть в плейлисте это что ты имеешь ввиду я не помню как это раньше называлась в общем можно был в яндекс музыке составить свой плейлист а понял так вот того что ты хочешь прослушивать потом ты переходишь на flow того что ты хотел бы послушать из похожего и среди прочего тебе баллада то что у тебя при есть то что слушаешь и так ада в этом месте как раз идея достаточно большая в том что в отличие от большей части сервисов там том числе и от кино поиска в музыке пересушивание того контента с которым ты уже взаимодействию довольно типичная вещь поэтому мы точно не хотим убирать эти треки которые ты там за лайкнул или добавил в какие-то плейлисты добавление в плейлистов является довольно важным сигналом для нас очевидно поэтому мы его активно используем но вычищать его чтобы его не рекомендовать тоже неправильно но твой вопрос абсолютно валиден потому что это одна из главных наших проблем это разнообразие как сделать музыку интересный и при этом разнообразный потому что очевидно что если мы будем ли когда пользователь только то что он за лайкал этот довольно не интересны и рекомендации это другая крайность по сравнению с не персональными вот в этом месте конечно же нужен баланс и второй тоже очень маленький вопрос я так понимаю вы ещё смотрите что пользователь ищет в поисковой строке возможно в самом яндексе да если какая-то взаимосвязь между тем допустим я ищу какой-то музыкальный инструмент в покупку и рекомендуете ли вы как-то исходя из этого музыку допустим человек является музыкантом профессиональный мастер есть такое так давай отвечать буду по частям да они естественно используем все данные до которых мы можем дотянуться мы используем поиски в яндексе в том числе чтобы рекомендовать музыку но используем чуть более простым способом чем ты описал мы в основном используем данные о поиске каких-то артистов или треков то есть тогда когда сам поиск яндекса определяю что при этом поиске был релевантен какой то объект и там в отдельном окошке показывается там либо ссылка на википедию и либо объектный ответ об этом человеке вот тогда мы это используем когда пользователь просто ищи там условно говоря виолончель нет мне используем это спасибо так вот барышни вопрос будет спасибо большое за очень интересный доклад у меня два вопроса первый уже частично задали мне было интересно вот у вас очень много интеграций внутри медии сервисов и как много их за границей медиа сервисов сети яндекса очень много сервисов покрывающих нам просто огромную часть жизни каждого человека вот может быть еще что-то кроме большого поиска этого что-то вы уже сказали может и щеки тебя ткань и конечно же у нас огромное количество места с которой мы пробовали рано или поздно забирать данные просто как казалось что медиа сервис и между собой довольно много шарит информации много интересного можно достать то есть из просмотров фильмов и просмотр и прослушивание музыки они достаточно тесно между собой связаны там нет прямой связи но достаточно близко аналогично афиша аналогично большой поиск один пользователь может искать прям музыку когда мы идем чуть дальше например в поездки на такси это почти не приносит никакого профита бесполезно мы использовали данные уже несколько сагре гера ванны например вы наверно знаете что есть рекламные сети яндекса она использует некоторые алгоритмы внутри себя для определения там пола пользователя и так далее она догадывается по тем страницам который он посещал возраст и так далее мы много следили за этим тоже но оказалось что там буквально 5 10 прослушиваний пользователя весят намного больше чем все что знает rs я той музыки которая понадобится пользователь потому что там было довольно забавно если вы будете прибирать конечно какие-то экстремальные примеры ный примере кей-поп вы наверное ожидаете что там пожилые люди слушать этого не будут но на самом деле в вероятность что они его послушать будете всего лишь в два раза меньше чем у молодежи спаси большой второй вопрос remix делайте с ремиксами как-то отдельно их обрабатывать вот вы говорили про похожую музыку он мне как пользователю яндекс музыке и очень часто ремиксы просто накидывают один за другим о возрождении такое к сожалению remix они не подпадают часто под определение что них такое же название не подразделение что это такая же музыка они часто бываю действительно достаточно друг по-другому звучащими и все еще пользователи они в этом месте должен ассоциируется крайне хорошо в этом месте мы дальше работаем над улучшением нейронах и вторая вещь который мы здесь работы мы собственно берем разметку правообладателей часто они присылают нам о том что и треки являются дублями друг друга и мы пытаемся выбрать не remix мы попытаемся выбрать студийную версию мы пытаемся выбрать не запятую версию если это условно говоря motto мы продаемся брать не клин версия экспрессе твержу и так далее мы пытаемся делать выбор версии трака наилучшим образом но к сожалению в этом месте сильно зависим от качества разметки особенно когда контент исполнители поставляется двумя разными правообладателями тут к сожалению нам только помощи редакции там вот на больших артистах она помогает довольно с этим но естественно не на всех может в яндекс многие языки порядка 10 миллионов трек 100 миллионов траков довольно сложно да разметить всей жизни спасибо так пожалуйста молодой человек иметь дом будет тут стороны здравствуйте бы большое спасибо за доклад ситуация такая если 3 какой-то попал в рекомендации он реально пользователь понравился он часто попадают рекомендации вон там про like an и потом пользователи его скипнуть только из-за того что уже надоел часто попадает означает ли это что этот трек потом не будет попадать им рекомендации или не появится он там значительно долго нет такого нет ну то есть это важный вопрос про разнообразие как ну типа в этом месте означает что мы skip даст нам понять о том что как бы мы слишком много его поставили но что есть насколько как часто надо повторять реки которые жду уже слушал если не скипнуть а постоянно продолжать слушать те треки которые ты заслушиваешь 100 раз и продолжаю говоря есть кактус в этом месте то мы будем продолжать радостно считают что ты вот такой пользователь тебе нравится повторы и ничего не будем менять себя спасибо и вот сейчас вот здесь молодой человек данил привет спасибо за доклад я попробую свернуть все таки сторону хай-лоу да спасибо ты говорил про то что вы используете и нам или базу данных и у нее есть какие-то преимущества по сравнению но там с похожими решениями типом нам кажут можешь вкратце рассказать в чем ее основная как бы суть и за счет чего она выигрывает собственно одна из основных идей здесь в этом месте в том что оно разрабатывалось для мо президент в этом месте время подъема база до работоспособного состояния часть очень важна и потому скольку поднимается мапри dijo бы ей надо запустить все базы и начать рекомендовать в этом месте запуск бас важное место мы используем порядка 40-70 гигабайт данных которые требуют в этом месте подъем их занимает весьма большое время поэтому мы используем полностью им от подход то есть все было бы данных которые мы используем просто напрямую напиться в память вы более того так как у нас в яндексе свой собственный мапри deus это позволяет сделать следующую вещь мы когда закачиваем файлы в мо придет jopu мы не кладем их на диск мы закачиваем сразу в оперативную память в этому если мы используем некоторые внутренности нашего сервиса благодаря чему мы сразу оказывается файл уже в оперативной памяти и за вжух мы его мгновенно получаем в программе поэтому там фактически время подъема для нас равно нулю и от 1 гигантская для нас преимущество второе преимущество заключается в том что мы фактически используем очень простые коллекции то есть там это наборы у сун горя хэш-таблиц расположены огромное количество хэш-таблиц которые расположены в этой базе то есть мы поддерживаем произвольные вложение там станут одной хеш-таблицы внутри другой хэш-таблицы так далее более того с точки зрения писания для нас это очень неудобно потому что данные которые мы закладываем такой блок мы просто описываем обычным классом на джаве спеша по одной аннотации на каждое поле со словами я хочу чтобы это поле все реализовала сидите реализовалась и дальше это вещь сама понимает как ей надо поднять как и надо стерилизовать эти данные поднять обратно время нашей работы было где-то примерно в два раза меньше чем просто джавой коллекции то есть это крайне высокая скорость для отдельностоящий база которые вот фактически полностью независима и вообще никак не нагружает garbage collector потому что теперь мы можем менять прямо на живую сейчас пока следующий вопрос у нас готовится маленький тоже что такое уточнение правильно ли я понял что вы еще с используйте индекс database для хранения обработки данных и если это так то в каком варианте там есть два варианта один дикий ты-то 2 это сервер лес режим каком из них мы используем сосна her last сервисный режим так секундочку сначала вот здесь молодой человек просто суда вот привет как я понял историй вы ушли в бочке первую очередь чтобы тягать там тяжело нерон очки информ стив там вообще какие угодно сложные а потом все-таки ушли в онлайн ну потому что как бы совесть проснулась все такое но да есть способы получается как-то облегчить те тяжелые розочки которые были разные не знаю какой вы использовали но не получается ли так что теперь вы снова как бы ограниченную своей гибкости сидят на том что вы уже ускорили не можете также быстро круто природе пути за как раньше то есть в синхронном режиме на там таком около инсценировано очки или используйте паттерн типа как у дзена когда но онлайне все-таки готовый вектор запрашиваете можете в стриме там типа обновлять более сложными модельками давай отвечу этот момент старею вернее такое на самом деле я не рассказывал по большую часть подробностей e-mail именно в этом месте длятся на самом деле подобная возможность научила нас как правильно с этим работать на самом деле мы не избавились от времени не полностью его уменьшили самом деле мы его переложили большей части во время обучения моделей то есть мы перекачали большое количество времени которые мы тратили в inference и в обучении модели мы стали учить более сложные модели они учатся теперь и дольше и сложнее но inference на них стал быстрее и вот подобные вещи мы активно используем мы не используем прямо напрямую подход зина чтобы запрашивать готовые данные в этом месте но мы используем как огромное количество преда расчетов на пользователя очень похожая идея и используем их на стараемся чтобы все nb динки которые были были вот прям самые свежие которые только есть потому что мы считаем что для нас учет последнего фидбэк это самое важное что есть и по всем нашим экспериментам мы видим именно это но никакого ограничения у нас нету хорошо пожалуйста вот здесь вопрос и потом дальше за вами даниил спасибо за доклад а скажите пожалуйста как вы вашу модель обходится случаями когда одним аккаунтом пользуются несколько человек над в моей семье например да ест ребенок соответственно я со своего индекс аккаунта могу в машине детскую мужчину ставить либо что даже чаще и колонка алиса на мой аккаунт и вот дочь приходит включая детскую музыку потом еще и говорит алиса мне нравится кого там эта музыка не попадает в мой плей-лист мои любимые вот сам и вот это классный вопрос прям огонь действительно это одна из наших больших чин g потому что запуском алисы это стало проблему очень большой на самом деле мы активно работаем над узнаванием голоса чтобы мы могли разделять пользователей которые говорят с алисой также очевидно что если ты пользу отдаешь там свой телефон другому человеку начинает там лайкать что-то другое ну мы тут к сожалению бессильны практически но на колонке блага у нас есть голос говорящего в этом месте мы можем разделять эти потоки и мы как раз активно работаем над тем чтобы улучшать жизнь пользователи у которых есть дети которые тоже хотят слушать музыку слушать не тоже самое что родители спасибо и давайте последний вопрос к сожалению нас можно будет продолжить потом вопросы в дискуссионной зоне сейчас просто большое спасибо а вы сказали что во все модели вся база влазит а в памяти при этом говорю что 30000 параметров на пользователя это ж сколько у вас память и получается используется так 30000 это было количество кандидатов количество факторов и но все-таки более меньше это одна тысяча вот а + а мы счастью не храним всю тысячу на каждый из 10 миллионов треков из 100 миллионов треков которые у нас есть базе мы естественно храним некоторые специальные агрегаты которые позволяют их быстрее считать и не хранить полные вектор ну например листья нужно популярность артиста то те важно сохранить популярности на артиста в которых там миллион а дальше каждому из треков подтянуть по связи между треками исполнителем и популярность исполнителя этого трека тем самым тебе даже не надо хранить всю тысячу цифр вот прям вот на всех и довольно сильно экономит память спасибо большое давайте поблагодарим данила за ответ на вопросы и он небольшой подарочек от организаторов конференции и сейчас самое сложное нужно выбрать вопрос который больше понравился в аудитории тут все очень просто вопрос про колонка это про ногой на спор колонку хорошо тогда мы неплохим надо человеком и спасибо большое а сейчас я предлагаю всем переместиться у кого остались вопросы кто хочет перекодировать заняла вместе с ним дискуссионную зоны то на выход и чуть-чуть за колонну направо"
}