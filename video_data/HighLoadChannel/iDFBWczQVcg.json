{
  "video_id": "iDFBWczQVcg",
  "channel": "HighLoadChannel",
  "title": "Performance as a service: делаем быстрее и дешевле через сервисный подход / Кирилл Юрков (Самокат)",
  "views": 1095,
  "duration": 2586,
  "published": "2023-01-19T07:01:38-08:00",
  "text": "всем привет а мне даже слышно какие технологии сейчас от листаю свой докладе к нужному месту все я готов надеюсь вы что-нибудь видите все отлично замечательно да действительно меня зовут кирилл йорков и я и сырье тимлид в такой компании которая называется самокат для тех кто не в курсе что такое самокат это как мы любим говорить оффлайновые сидел только не для доставки пакетов этой доставки товаров вот и сегодня я буду рассказывать справа нагрузочное тестирование погнали собственно для того чтобы доклад не было очередным каким-то где мы все супер классно автоматизировали получили кнопку и вы после этого ушли ни с чем давайте сразу обозначим две цели которые я хочу до вас донести и первое это то какую концепцию мы создали и кастомным названием назвали ее performs вот хотим поделиться тем как эта концепция выглядит для чего она и что вообще это такое ну и потом рассказать что как отличается эта концепция нашего вот пир фокса какого-то от типичного нагрузочного инженера погнали и лучше всего как мне кажется про это про всё рассказать с точки зрения того как мы к этому сами пришли да то есть как в самокате мы достигли вот этого самого performs и началось все довольно просто и стандартную я пришел самокат и до этого там был какой-то табун аутсорсеров который наделал какие-то страшные perfomance test и забюрократизированные домик документация какой-то странный мониторинг который показывает непонятно что и мы подумали сейчас мы все это причешем станет все хорошо забега и залетает если бы не пару но еще мы узнали то что самокат планирует очень-очень быстро развиваться и этот рост уже был виден допустим начале того года когда я пришел было всего шесть продуктов а к концу года должно было быть уже 16 до больше чем в два раза и растет и трафик и растут и продукты сложность технологии и это все как бы и подразумевают что мы будем долго что-то причесывать вот и еще один фактор это мало людей а вообще я был один на старте такой продуктовый какой-то нагрузочные инженеры и вот это все нужно поднять и запустить задачу а звучало следующим образом нужно было внедрить практику нагружена тестируем на каждый релиз чтобы у нас не было никаких производительности проблемно прадеда все пратт работает хорошо это все бегает и крутится ладно взяли это и погнали погнали думать а как вообще внедрить такое классное нагрузочное тестирование чтобы у нас все было хорошо летала и бегала и первый подход который приходит наверно всем на ум это такой типичный продуктовый как я называю подход это когда выделяют какие-то функциональные команды туда добавляют менеджера разработчик тестера и нагружаться само собой который сидит ли что-то там делает и мы решили подумать насколько эффективен такое вообще подход вот в нашем сценарии и для этого мы сделали какую-то модель вот допустим есть какие-то шесть команд и еще потом сколько-то и в них есть какая-то своя эффективность она основана на чем она основана на том насколько потенциальный инженер внутри этой как продуктовой команды может находить performance проблемы и вот у нас есть какие-то зеленые ребята которые находят классные такие все performance проблемы такие прям детективы есть такие желтые или бледно-желтые ребята они нормальные находят сколько-то сколько они находят в целом какое-то вылью дают есть разгильдяи они в counter-strike играют на работе они ничего не находят в общем плохие типы мы их не любим но тем не менее они есть и давайте что-то более визуально понятно и посмотрим поставим оценки по десятибалльной шкале и каждый такой команде исходя из их эффективности мы дадим оценку да и у нас будет какой-то итоговый балл допустим он 5 3 но это условные какие-то баллы они могут быть другие и еще у нас есть x x это значит то что у нас есть сколько ты еще потом команд продуктовых и от них непонятно какой на самом деле был ждать до поэтому это неизвестная величина и нам это не очень нравится этот бал не очень стабилен он зависит от каких-то факторов и целом как на него влиять на мне очень понятно давайте помимо этого посчитаем ещё денежку ведь это же сколько это все стоит и вот мы тоже платим какие-то условные доллары каждому этому инженеру и допустим мы платим ему за то насколько хорошо он работает или насколько он много просят в целом за его эффективность у нас получается какой-то вот такой тоже бесконечный прайс да как это бесконечная деньга в общем сколько мы готовы увеличивать эти деньги непонятно украшал ладно если мы столько платят денег может быть мы можем как-то улучшить все таки этот балл за общее качество нашей технологической системы с точки зрения перфоманса ok давайте попробуем провести мит апы ведь у нас есть крутые эффективные performance инженеры они вон там есть зеленых командах сидят и ждут возьмите наши знания мы готовы делиться но допустим мы провели этим этапы все посмотрели такие а ну на свой велосипед у вас свой почему я должен что-то переделывать и вообще наверное надо базу знаний какую-то завести ее кто-то должен поддерживать 1 эти авнира и все так в общем тяжело поднимается особо не едет ну да кто-то может быть взял и на пол балла за счет этих митапов улучшил свой балл за качество ну а в общем зачете это практически ничего да он все так же не стабилен этот бал окей может быть мы проведем тогда обучение и всех научим быть супер спицами они будут делать красная наглую нагрузочное тестирование и у нас все запоет засияет окей обучили какой-то состав ребята стали супер спицами и ушли в другую компанию работать потому что там больше платят например или взяли и за скерри ли команды в два раза пришли какие-то другие performance инженеры и что их опять обучать значит они будут приносить пользу на проекте сильно позже и в общем история какая-то странная да окей раз у нас столько денег может мы будем нанимать сразу супер специалистов рок-звезд нагрузочное тестирование и они нам все сделают прекрасно тоже нет заранее вас разочарую их просто столько нет на рынке или они уже где-то сидят и вы их просто не найдете ладно что можно сделать какая есть альтернатива альтернатива вроде очевидно мы можем взять тех эффективных чуваков которые у нас сидят в зеленых командах и собрать их вместе нанять немного людей а много знаний объединить их в одну сервисную in stores команду которая будет обслуживать наши продуктовые и предоставляет нагрузку как сервис дознания будут консолидированной в одном месте они будут внедрять какие-то крутые практики и мы в самокате начали ровно с такого подхода мы последовательно внедрялись в разные команды к нам приходили с перф он с тестами мы что-то автоматизировали и быстро поняли то что на самом деле эта история имеет много подводных камней первый из них и самый основной это контекст контекст который у нас отсутствовал у продуктового командах когда у них релиз а когда фича какая-то выходит или вообще появился новый метод а мы его не покрыли тестами как об этом узнает нам наложить какие-то бюрократические в общем процесс и доставить какие-то сложные т.з. это все сильно замедляет и в целом а дешевле ли это ну по факту наверное в моменте да но на дистанции нифига то есть это тоже долго и просто мы деньги распределяем по времени потому что мы делаем то же самое что и делали инженеры внутри продуктовых команд ничем не отличается и тогда мы подумали что можно сделать сервисной команды чтобы оно работало более эффективно и тогда мы посмотрели и увидели классный подход abs который хорошо себя зарекомендовал внутри таких методологии как devops performs perforce нет пока дата опции тест abs короче опций есть практически везде да уже куда ни плюнь и есть какой-то abs и мы тоже решили попробовать в фундаментальном смысле этого понятия лежит автоматизация каких-то технологических процессов да и поэтому мы решили создадим команду которая будет предоставлять инструментарии внутрь этих продуктовых команд а не с помощью база знаний которая будет тоже писаться и этими экспертами из команды performs будут сами писать своей performance тесты будут использовать самые крутые практики которые будут спускаться сверху вниз из команды перф abs здесь она внизу представим что наверху и тогда у нас будут классные perfomance test и вроде из-за экспертизы есть кому обратиться если у нас появилась какая-то проблема все хорошо да ну давайте мы с вами оденем поварские колпачки и попробуем взять котел и приготовить такого пир fox инженеры который бы нам подошел вот наш котел и первый основной ингредиент в нем такая основа это performance инженер почему он потому что научить моменте кого-то искать персона с проблемы это вообще не так проста как научить писать скрипты да и поэтому экспертизы должна все-таки находиться в этой команде а значит основным из них королей будет performance инженер помимо этого у этой команды будет необходимость внедрять практике отказоустойчивости и стабильности потому что ее основная метрика это отсутствие проблем напротив до performance проблем и мы берем пару половников в общем срыв этот котел закидываем дальше у нас есть необходимость интегрироваться с нашей инфраструктуры наш инструмент должен как-то с ней жить как-то в ней работать и доставлять какие-то наши изменения до конечного пользователя поэтому мы хотим добавить туда пару ложечек devops для того чтобы дружить с инфраструктурой дальше нам необходимо чтобы наш инструмент хорошо ложился на рельсы того как уже в продуктовой команде пишутся тесты и соответственно нам нужно знать как это вообще делается как пишутся авто тесты и поэтому здесь очень не лишними будут практике авто те стинга добавляем тоже пару чайных ложечек и дальше берём специи в общем такую баночку с ребятами которые называются девелоперам тест мы планировали чуть-чуть сверху присыпать ей но у нас крышечка общем отвалилась какой-то момент и все специи упали в этот котел потому что мы сильно углубились в разработку этого инструментария но об этом позже у нас получился супер performs вышел из котла и пошел что-то делать и вот что делает этот франкенштейн давайте посмотрим как получился у нас какой-то flow и так здесь представлены 2 зоны ответственности сверху перф abs снизу продуктовая команда мы со своей стороны предоставляем инструментарии базу знаний и на основе нее ребята из продукта пишу тесты делают запуске и потом появляется новый товарищ в этой схеме который называется quality gate хвалите gate эта штука которая посмотрит на результаты этих performance тестов и скажет нет ребят в продакшен не едем есть проблемы и продуктовая команда пытается найти эту проблему или она сразу ее понимает фиксит а если не понимает то приходится экспертизы к нам и мы ищем ее вместе поэтому этот блок на стыке как только проблем нет мы поехали в прод хорошо что ж у нас такое внутри инструментария как мог вообще подходили к его выбору у нас на тот момент уже были какие-то тесты написанные на инструменте который называется а поджи метр наверно многие с ним знакомы вот и была какая-то даже автоматизация вокруг него все нам вроде нравилось кроме того то что ну невозможно на большом количестве проектов использовать же метр потому что он в детей лежит в виде xml н каких-то страшных у них нету of the camp литов у него нету перри использования или есть но она сложная вот потом мы узнали как выглядит эта система внутри самих продуктовых команд и там разработка бэг-энда и тестов чаще всего ведет на код ли не запускается это все через грейда ну или каком-то похожим на g-unit фреймворке вот но еще одним фактором это сжатые сроки поэтому посмотрели в сторону всех существующих diys и же метру и выбрали java джимми тардис сильно которую я в конце дам ссылочку он был довольно сырой но он нам давал практически все то есть g unit тесты можно питать писать на котлах и под капотом чистенький g метр идеальный движок окей мы начали в него активно кантри бьетесь ешь усами и кодом дорабатывать и параллельно мы разработали свою библиотеку для нагрузочного тестирования проблема в том то что функционально тестером довольно сложно объяснить тоже как писать тесты поэтому мы решили этот процесс сильно упростить и сделали свой такой фрей марк подружились грибом и получилось классно вот так вот выглядит скрипт на отжиме 3 я наверное как я сказал многие его видели вот такая древовидная структура все скрипты пишутся внутри ей это больно сложно но можно и вот так вот стал выглядеть тест с помощью нашей библиотеке идеи цели autocom плиты перри использование все плюшки идеи гита и так далее окей что под капотом под капотом три компонента два из них я назвал даже все назвал и disel нам давал с ходу практически все что нужно грейбл собирает прокидывает переменные seo и работает с параметрами живым наша performance factory это такая обертка которая реализует пропагандируемый нами рпс подход в тестировании добавляя штуки типа автостопа в классные метрики генератор отчетов и дебаггер классно давайте посмотрим теперь вот на такую сложную схему это то как этот инструментарий лег на тот flow который у нас был для доставки релизов прот и тут есть слой инструментов первое с чего все начинается это с умершей квеста дамир ships мы используем гид лап он триггерит создание динамического стенда средствами кубера филма и туда же заезжает еще один инструмент который отвечает нам за нашу сиди и часть это арго сиди туда же подкидывает обфусцированный дам который делается средствами air flow подкидываете заглушенный контуры все у нас готово я . в котором мы можем кидать любую нагрузку до нагрузка кидается теми инструментами которые я уже назвал это опять же метр dsl заглушки у нас пишутся нам мог сервере но он чуть-чуть мутированные и еще важный момент в моменте никогда не знаешь какой профиль нагрузки сейчас актуален если ты хочешь повторять ситуацию продакшена поэтому у нас для этого есть авто сборщик который интегрирован с наша библиотека которой внутрь теста подкинет всегда актуальные данные и ты в моменте узнаешь как вот этот релиз конкретно жил бы на продакшне есть польза реализовался прямо сейчас после того как тесты прошли внутрь и умершие квеста сразу видны результаты quality гита то есть мы видим какие проверки мы прошли и упали ли по времени или где то есть performance проблема можно для катится или нельзя отдельно откидываются классные отчеты внутри гид лапе джаз который генерируется автоматически все круто тогда едем в прод если плохо кидаем на доработку и проходим этот весь путь заново окей как мы это вообще внедрили да и сколько времени прошло четыре с половиной месяца с момента а performs до момента когда первая команда начала полностью использовать этот инструмент нас было 2 до 4 человек в команде и здесь стоит отметить то что вот все вот эти штуки которые я говорил мы очень старались сделать самостоятельно да мы же до этого в котле зачем-то себя подготовили и поэтому мы погружались во все практики да нам помогали безусловно devops и разработчики но мы старались абсолютно минимизировать и это участие для того чтобы потом ордерами этой практике была одна команда и это нам сильно помогает участвовать в ее поддержки и делать это быстро качественно тогда мы действительно за заморозили все другие работы на время внедрения и здесь стоит сказать то что уже через два месяца мы могли полноценно пользоваться этим инструментом самостоятельно и даже покрывать какие-то супер критичные точки которые нам казалось что нужно сделать моменте и немножко отложить разработку этого фреймворка вот и сейчас мы находимся на этапе когда мы планово внедряем его в каждую команду самоката проводим микро какие-то обучения и ребята берут его сами и внедряют сами в общем находимся на этапе внедрения ну естественно как без проблем рубрика любимого внедри отара и первая проблема была с инфраструктурой когда мы оценивали вообще влезет ли вот этот вот и fox подход в нас мы посмотрели на инфраструктуру и казалось что там вообще все готово блин динамические стенды есть база какие то есть бери да вообще запускай но нет с динамическими стендами было проблемы какие-то и оказалось что дамп они очень хотят к нам идти без опускаться и в общем такой моментик да ну еще какие проблемы были в основном самые сложные проблемы это концептуальные концептуальные проблемы их было много но ключевая из них это то что если у вас в релизной цикле присутствует нагрузочное тестирование то ваш тайм ту market ощущает боль да потому что нагрузочные тесты не делаются быстро они там у кого-то 24 часа кто-то там не знаю два часа гоняет это все ощутимо и time to market явно страдает поэтому мы решили принять для себя заранее несбыточную цель дам как мы думали сделать свой максимум и успеть за 15 минут провести вот все от нажатии кнопки провести тест развернулся стенд до момента получения результатов ok проблемы с продуктовыми командами здесь две проблемы это откуда команде продуктовую взять община это делал ресурсы и зачем вообще это делать да понятно то что с каждым можно провести беседу что-то ему рассказать но он может и в конце про сказать нет да и все ты пойдешь дальше со своей классной разработкой сидеть и ничего не делать проблема сугубо личное теперь не я пишу тесты и не я их поддерживаю и сложно к этому привыкнуть как решать наверное это будут не самые крутые советы но я все же и дам потому что какие-то из них в кому-то могут пригодиться с инфраструктуры здесь нельзя быть готовым вообще ко всему и я советую закладывать просто риски до если у вас слабая бюрократия заложите 2x если сильно и бюрократии изложите 4x здесь может быть масса сюрпризов концептуальная проблема мы таки успели за 15 минут первое что мы сделали это придумали свой кастомный тип тестов ну а второе это основная проблема лежала в разрезе бас то что много терабайтный базы очень долго катится на динамические стенды но это никак не исправить но наши магистр devops и с черной магии сэтов с файловые системы решили эту проблему ресурсы и мотивация решаются средствами внедрения quality это если ты обязательно должен пройти quality gate у тебя не возникает вопросов а зачем это делать чтобы пройти и чтобы пойти в продакшен да и откуда искать ресурсы ну найдутся потому что в продакшен ехать по прежнему надо и поменять мышление пока не удалось но я вообще в процессе и уверен что получится итоге я вначале говорил то что есть две цели познакомить вас с тем кто такой пир фокс и и рассказать про разницу нагрузочного инженера и pure focus инженера первое я думаю чуть-чуть выполнил хотя бы поселил в ваших знаниях что так тоже можно в втором пункте я думаю сейчас преуспеть и различает свою них практически все то есть это та в какой среде они обитают да то есть сервисная продуктовая команда то какие практики внедряют и насколько они глобальные как работают с результатами насколько уровень их влияния большой и кажется то что пи fox инженеры это некая такая точка роста просто для нагрузочного инженера и она довольно логично и правильно и но скажу сразу это вот наш какой-то performs инженер может быть у вас будет другой если вы захотите это внедрять вот уверен то что он может быть абсолютно разной ну и в заключение хотелось сказать то что сервисный подход естественно не панацея и для его внедрения нужно быть уверенным то что у вас хватит экспертизы на то чтобы его внедрить и здесь еще важно что даже если вы сделаете просто сервисную команду которая внутри себя агрегирует экспертизу может быть это будет ваш сервисный подход очень дешевый но который принесет большой в илью а где это нужно автоматизировать там вообще допустим все подходов много комбинируйте и нагружайте всем спасибо вот в конце ссылочки как я и обещал спасибо кирилл за доклад и у нас мы масса вопросов раза вот приветствую спасибо за доклад очень интересная тема а собственно парус вопрос можно первый вопрос у вас не было такого такой абстракции как сообщество вот ты в начале рассказывала что комьюнити вот в рамках условностям или съесть такой концепт как сообщество которые вот по идее у вас были такие да у нас безусловно были сообщества но как я сказал мы начинали сразу сервисного подхода мы не играли фруктовый подход в котором бы это сообщество сыграла бы какую-то роль и тогда второй вопрос вот насчет того концепта как вы нагружаете как вы отслеживаете то что ваш инструмент является узким местом у нас просто есть мониторинге на двух сторонах на объекте и на источнике нагрузкой на уровне собственных того скрипта который вас выполняет , цель как вы на нем мерить собственно то есть какие инструменты в этом используйте транзакций метрики да но изначально это просто метрики самой тачки с которой все это запускается то есть паника совы можно понять тоже присутствует полностью утилизированы и все она больше не вывозят можно понять по тому какое latency при написании какой-нибудь ну при выполнении конкретного кода перед тем как он отправится можно ну сотнями рик метрик обложить мы в мире очень много если я буду сейчас перечислять то наверное это будет долго у меня есть доклад который рассказывал на первых он в про мониторинг как раз его можно будет посмотреть он есть у нас вот по первой ссылочке в telegram канале спасибо ну то есть был вопрос концептуально насчет того самописный метрики добавляли нравится да там есть инкрементируем самописные метрики александр дешевые спасибо за доклад у меня нервы полтора вопрос а кто вас анализирует результаты нагрузочных тестов если это продуктовые команда как вы убеждаетесь в том что они их проанализировали правильно и ничего не упустили это такое достаточно размазанный процесс то есть первичный валидаторы the quality gate если мы допилим его логику до совершенства то наверное больше никто не потребуется д но пока она не совершенна и поэтому у нас есть два эшелона и анализа результатов это может быть команда продуктовая и она может ничего не заметите сказали что все хорошо и последний шел он это мы потому что мы собираем метрики по всем тестам которые проходят если мы замечаем что где-то есть какая-то подозрительная активность деградация что-то нам кажется в общем таким что на это нужно обратить внимание мы туда подключаемся можем провести эти тесты сами без продуктов и команды смотреть of integration каких то еще что то то есть это подлог бы изучается уже нашим отделом спасибо а еще если не секрет то что за особый тип тестов которые вы придумали сами у нас нет не секрет у нас в компании принято считает то что для мишин критиковала продуктов мы хотим держать 4x нагрузки вот и соответственно мы до того как мы выезжаем внутри этого динамического стенда мы проверяем каждый продукт на то какую нагрузку он может выдержать в этот момент мы смотрим как выглядит проблема то есть когда вот он упал до ему стало плохо и описываемые наших требованиях к вот этому тесту и соответственно когда продвигает тест внутри динамики на уровне 4 x мы сразу можем понять есть ли описанная perform с проблемы внутри этого теста или нет и это нам гарантирует только то то что на продакшен и вот сейчас в моменте это не взорвется а уже те сущности которые такие довольно сложные и тонкие там память чуть-чуть течет там например еще что то это можно посмотреть чуть-чуть потом то есть в моменте это не слишком нужно для нас есть это принесет нам слишком маленький вылью при условии того что тест будет длиться так долго понял спасибо да и вот у нас александру давно ждет своей очереди этот как вы убеждаете если полноте написаны тестов то есть вопрос здесь внедрение в практику по которой мы так много говорили это одно а вот как вы убеждаетесь что они действительно выполняются команда мечтать с теста который на ds или они написали подготовлены вашей команды они действительно полные корректные действительно способны убедить убедить всех что целый соглашения они выполняются каким-то определенным сириза я понял ну во первых на quality ките можно проверять покрытие да если что-то не покрыта то соответственно все но ты едешь дальше можно воткнуть еще куча проверок но на самом деле здесь вопрос уровня ответственности да то есть мы в действительности несем уровень ответственности какое-то за то что у нас происходят какие-то проблемы на продакшен контуре это наша метрика то есть сколько их произошло но конечные как бы ответственные это продуктовые команды потому что они будут это фиксить они будут ну то есть зарабатывает тех долг и все остальное да и здесь поможет только уровень осознанности да то есть если они халтурят и делают что-то не так они по-любому смогут обмануть все что угодно там обойти любые процессы но это в конечном итоге их минус а то есть performance это у вас как ценность компании она прям сверху можно сказать да но не только к данным на брат хорошо на второй вопрос на самом деле как вы подготавливаете окружение для нагрузочного тестирования то есть вот вы сказали про какие-то там по данных мне первый лучик в первом если этот тип a memory dump не не то скорее всего а вот как от именно подготовка у нас есть инструмент ну то есть есть несколько сущностей есть база и есть окружении в виде других сервисов которые относятся к другим продуктам то что касается базы она подрывается просто внутрь динамического стенда обфусцированы и полностью соответствует продуктовому контуру то что касается интеграция с другими сервисами это заглушки стандартный база используется всегда то есть там не может быть такое что в одном случае нам придется ли динамо david другом сочи мой скил и вам все это надо поддерживать у нас довольно ограниченный стек в этом плане везде на всех продуктов используются по сгрыз и поэтому нам очень удобно внедряют подобные решения они универсальны и там мы это как раз таки там пополам это не услышал похоже что-то какой вопрос был там и там по про которого говорили это есть дампа поза кассандра of us и равная то есть маскировка да ну то есть там нету перс данных дашу кирилл спасибо за доклад попросят про кролики gate как я понял он у вас один на всех и связи с чем вопрос вас все команды должны укладываться в какое-то абсолютное значение типа там отвечайте 100 миллисекунд или вы смотрите только на деградацию конкретных сервисов ну на самом деле он никак не один для всех да у нас есть определенные требования для каждой команды да есть общие для всех но он должен укладываться где-то в абсолютное значение где это могут быть проценты то есть это довольно индивидуальная метрикой если мы говорим про что-то общее в общих метриках могут быть действительно абсолютное значение да спасибо просто у меня зовут алексей возник такой вопрос даже 2 наверное каким образом вы собираетесь работать с из туры разработки и доносить ценность до продуктового команд спер фокса как таковом и второй вопрос какие именно навыки этого пса и срезать не имеете фон инженеры то есть тепло и метрики зубы первый вопрос только не очень-то слышал это можно чуть ближе надрать еще раз как вы собираетесь работать с культуры разработки соответственно майнце там там разработчиков и каким образом будете доносить ценность performs я так понял да на момент еще этого не произошло полностью да я это сейчас в процессе мы внедряемся когда в каждую команду мы проводим ну какие-то мастер-классы по поводу того как вообще с этим инструментом работать у нас есть большая база знаний по поводу того как это нужно писать да действительно ее могут какой-то момент не соблюдает там какую-то практику делать не очень хорошая но мы это увидим потому что если ты пишешь плохо perfomance test они плохо работают у нас есть по поводу них метрики и мы обязательно придем мы посмотрим а почему в общем происходит всё не очень хорошо да это вот первые момента второе но есть довольно высокий уровень осознанности в наших продуктов командах и когда я не понимаю то что они делают что то что не совсем понятно и или с другой точки зрения может быть заранее знаю то что это может быть неправильно у нас есть прямо прямая связь обратная да куда они приходят и обращаются за помощью и мы в рамках не и реализуем в общем какой-то правильный подход второй вопрос про что был второй вопрос о скиллах который должен иметь performs инженер для по серия sub с точки зрения де мопс практик у нас есть ну то есть ряд практик который крутится вокруг инфраструктурных вещей для интеграции с нашей допустим библиотека и она должна куда-то подкидывать она должна как-то это все запускать и это все старались писать мы самостоятельно там под контролем чутким наших devops of да и какие то вот соседи процессы они вот полностью в нашей юрисдикции находятся ну если просто у нас есть какие-то сервисы допустим там какая-то заглушка тоже которые мы там занимаемся чтобы она там правильно развернулась чтобы у нее там за скейлер из какое-то пространство чтобы она перес пользовала какие-то штуки тоже общей там данные и так далее то есть здесь действительно нужно хотя бы в этом хорошо понимать да то есть не обязательно прям быть какой-то воплощением практике какой-то пилот не обязательно да но понимать обязательно стоит с точки зрения практика сергея вот этот инструмент это часть только того чем мы занимаемся потому что у нас команда она внутри включает себя и performance кей и сергей инженеров и соответственно практики которые мы внедряем отдельно от этой истории или основываясь на данных этой истории да они уже лежат в срр а помните сегменте сергей роли сопрано вас услышал performance клей для performs клей можете чуть-чуть раскрыть по-разному называют этих инженеров в них есть тысячи названия из столиц но нагрузочный инженер нагрузочным тестировщик performance кий performance инженер все один человек его называют везде по разному кто то делает эти названия я решил вынести их в отдельную концепцию и эволюционировал the perv abs и у нас еще множество вопросов завишу да привет вопрос отслеживаете ли вы на заглушках объему порождаемого трафика с верхнего сервиса который вы грузите потому что тут я вижу есть нюанс то что внешне вы не downstream сервиса вы заглушили заглушкой статистику берут с продакшеном если у вас резко возрастет объем трафика на downstream сервисы и это вы не отслеживаете вы дуете только в продакшене когда они в этот момент упадут или или не упадут будем надеяться я понял вопрос нет не отслеживаем на данный момент но это вот одна из грани нашего развития здесь дело в том то что заглушка вообще в целом не обязательно не обязательно сильно соотносится с сервисом который мы заглушаем то есть оно может быть вообще поставим может быть это какой-то даме действие внутри твоего скита вообще да и как-то по мере эту заглушку то есть у тебя там ты закинул в ко вкусам из нее вычитал до имитировал взаимодействие с другим сервисам но по сути у тебя ничего не инкремент но вас нигде кроме запроса внутри твоего теста поэтому это такая довольно скользкая тема в внутренние стоит действительно углубляться то есть надо посмотреть и это надо контролить но я думаю то что она прям сильно сложно в общем на данном этапе будет наверное нужно сначала приводить это к какому-то определенному состоянию то есть нужно выработать практику как взаимодействие с заглушками потом уже к этому прийти покаяние добрый день вот спасибо за доклад и мы тоже двигаемся в сторону pure focus а вот ну своим путем немножко другим вот мне такой вопрос я увидел там кубер и мне вот очень интересно вы всю структуру под тесты разворачивайте только во время запуска то есть вас pipe разворачивает все в том числе и накатывает со само приложение и как раз таки и разворачивают генераторы нагрузки всё верно насколько я понял и вот тогда у меня вопрос в вк uber запихали g метр или вас это тоже не получилось мы не пользуемся для метров то есть мы пользуемся только его api который на самом деле джо новый код его никуда не нужно записывать ты просто выполняешь jar ник по сути она они вас как разворачивается как отдельные получается микро сервисом он разворачивается как серых джобар если бы дали прикольно спасибо вот эти пожалуйста микрофон спасибо за доклад скажите пожалуйста что делать если мы проводим and vendor скала по под windows который требует очень мощной машины что вообще запустить чего требует очень мощную машины что в принципе запустится ну там условно там сотни гигабайт оперативки и там это десятки сотни я а если не секрет какой уровень нагрузок то есть вы выдаете с этого всего ну там десятки там тысячи не меньше десятки тысяч рпс все равно звучит так то что под это не нужно столько ресурсов но допустим нужно но на самом деле в нашем спектре не лежит тестирование подобного софта какого-то да у нас для всего есть api и мы внутри этой концепции развивались поэтому перед нами такие задачи не стоят но если задуматься о том чтобы нагружать вот ваш кейс как-то то я бы все-таки не отказывался тоже такого подхода потому что на самом деле g метр слишком многогранен для того чтобы что-то не нагрузить в общем это нужно сильно постараться им поэтому я думаю то что здесь все вполне ляжет например на подобные вещи из не будет нужно столько ресурсов просто сложно это вся сидеть строить процесс ну просто технически сложно да друзья у нас осталось время на один коротенький вопрос да кирилл я тут привет спасибо пошлешь доклад очень интересно на самом деле ни один коротенький 2 может быть не очень больших вопроса первый если возвращаться к схеме развертывания доставки фичино просто запускать снимай динамический стенд как правильно подходить конфигурации этого динамического стоянно должен ли он полностью повторяет продакшен чтобы чтобы ты понял что твой продукт на продакшене будет работать точно также как эндемический стань классный вопрос очень хороший мы долго над этим думали должен ли он соответствовать или не должен и пришли к тому то что на самом деле даже само наличие динамического стенда это уже полностью несоответствие production ну и вообще ничего не соответствует production ну кроме продакшена но тем ни менее результаты где-то хочется смотреть как ты их получаешь и имитировать эту нагрузку до до того как его выстрела вроде поэтому у нас есть есть те этапы которые я описал первый это то что мы тестируем сначала в максимально приближенной не динамики да и находим там performance проблему после этого у нас есть тесты внутри динамики и здесь мы можем провести экстраполяцию с кейлин как-то понять да то есть как она общество относится то здесь response и там 200 миллисекунд здесь 100 миллисекунды например мы можем предположить то что у нас относится в два раза но это это низ не . в который нужно искать истинность это . в которой нужно моделировать поэтому мы здесь не пытаемся понять наш сервис деградировал наш сервис чуть-чуть стал хуже чем вчера или там еще что-то мы здесь понимаем одну конкретную цель вот у него отстрелят там что не будет если мы его сейчас запустим где-нибудь или не отстрелит и ищем только явной performance проблемы на этому как бы в этой концепции до поэтому мы разворачиваем на данный момент один к одному вот если мы можем это делать максимально стараемся в эту историю то что отличается эта база потому что она виртуализированные и отличаются но там любые технологии которые мы используем да они тоже получается будут на кубе понял и спасибо я понимаю что шквал вопросов и вопрос тогда еще раз я предлагаю продолжить окей вот у тебя последний давай совсем коротко да ты сказал что нагружаете да а точно ли всем командам нужно нагружать свои продукты проходить в этот колледж если у команду если есть какой-то продуктом с десятью запросами в 10 минут нужно ли мне проходить котики для того чтобы вы кажется народ к сожалению нужно да потому что а как вообще понять нужно проходить кому-то на груше на тестирование не нужно это вот садится какой-то умный человек такой ну наверное да или ну наверное нет или какой-то признак на придумали этот запрос тяжелый ну не про дни тяжелый вот этот тяжелый да вот это нужно короче нагружать и вот пока ты поседел вот это вот подумал то на самом деле уже тест на пробежать и сказать тебе все хорошо и поэтому если у тебя 10 рпс там какие то и в 10 минут ну сделай это и будешь уверен то что здесь все хорошо ну в общем процесс выстраивать вокруг того что понимаете нужно или не нужно нагрузочное тестирование согласовывать какой-то аналитикой очень сильно в эту порваться она дольше часть чаще всего и дороже чем просто проводить нагрузочное тестирование всегда и не делать полутонов полу правил для каждой какой-то индивидуально команда вот этот сервис вот на него не надо да он поедет сам так далее вот так просто это не единая практика не единый флауи ты всегда мне будешь путаться всем большое спасибо кирилл спасибо кажется количество вопросов говорит сама за себя"
}