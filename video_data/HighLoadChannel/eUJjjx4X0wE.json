{
  "video_id": "eUJjjx4X0wE",
  "channel": "HighLoadChannel",
  "title": "Балансировка на скорости проводов  / Андрей Домась, Вячеслав Морозов",
  "views": 1731,
  "duration": 2969,
  "published": "2018-08-16T04:37:27-07:00",
  "text": "всем привет как как называется тема нашего доклада вы наверно уже прочитали вот здесь так она написана начала представимся кто мы такие мы вдвоем делать этот доклад сначала я потом мой коллега зовут нас вот так я андрей дома я ведущий системный администратор социальной сети одноклассники моего коллегу зовут вячеслав морозов он технический директор компании nsp о чем мы будем говорить о чем мы будем рассказывать сначала я расскажу о том как устроена балансировка в одноклассниках о том как она была устроена когда-то о том какой эволюционной путь она прошла и почему и что оно представляет из себя сейчас потом поговорим о проблеме с которой мы столкнулись а именно осин флуди как мы эту проблему пытались решить при чем здесь нфвк к нам помогло и как она как она устроена под капотом вот об этом мы будем говорить вот ну начнём с первой тему с балансировки в одноклассниках балансировка в одноклассниках выглядеть примерно как-то так вообще одноклассники с точки зрения балансировки вот так выглядят наверно здесь ничего нового вы не увидите это типичная архитектура был на село балансировки любого проекта да есть куча серверов приложений которые что-то сервис в мир и от этого внешнего мира они отделены вот такими кубиками балансировщика me vs тут написано это linux выручил сервер это самое наверное распространенное open source на и решение балансировки на четвертом уровне когда-то давно вот этот лес выглядел тоже совершенно классическим образом он состоял из двух компонент ядерный модуля и ps это верхний прямоугольничек и л директор д такой демон написанный на перле айпи вы собственно трафик балансировал а л директор д он умел делать всего лишь две вещи во-первых он умел зачитывать конфигурационный файл с диска и настраивать этот ядерный модуль что он правильно балансировал а во-вторых л директор д мониторил состоянии серверов приложений на тот случай если кто-то из них там сломался и надо бы его из балансировки выкинуть и со всей с этой схемы было очень много проблем самого начала во первых при нашем участии то все активно дорабатывалась до того момента пока нас это все не начни не перестала как-то удовлетворять и тем не менее там осталось много очень багов до сих пор она все очень плохо документирована это сейчас говорю про и пвс модуль плохо документирован этот модуль особенно в части каких-то тонкостей работы алгоритма балансировки там в основном вся документация по этим вещам это какие-то цитаты из списков рассылки почтовых поэтому модулю самый директор d тоже штука была довольно таки мерзкое во первых эта штука была однопоточный она в один поток проверяла состоянии вот этих вот серверов приложений и если сервер всего два это наверно не проблема вот но если серверов приложений несколько сотен из них значительная часть например отвалилась недоступна то значит в один поток проверить все эти сервера повисеть на каждом недоступном сервере на тайм-ауте вот на все на это могло запросто уйти 30-40 минут то есть 30 40 минут много серверов недоступных находилась балансировки и пользователи испытывали от этого киты проблемы вот это очень плохо было и она все настраивалась только из файла то есть не как там во время работы нельзя было повлиять на этот директор да чтоб он как-то перри конфигурировал модуля и псы с точки зрения автоматизации это не очень нам нравилось мы хотели иметь другие возможности чуть позже станет какие именно и в какой-то момент мы решили эту схему немножко переделать взялись сначала за этот самый директор д взяли его и переписали написали свою директор д назвали его по-другому кейл в с монитор написали его на джаве конечно мы все пишем на джаве и что эта штука умеет делать она умеет на во-первых отдавать всякую разную статистику умеет отдавать то что нужно в мониторинг чего не умел делать его директор д умеет конечно же мониторить в многопоточном режиме сервера приложений и что немаловажно вот и такие вас монитор он умеет настраивать и перебил с не из файлика а из специальной конфигурационной базы данных вот а надо заметить что у нас все компоненты нашего проекта нашего портала они все настраиваются из этой базы данных базы данных имеет разные интерфейсы есть веб-интерфейс для пользователя есть разные api-интерфейсы для всякого рода автоматики в общем вот так это у нас работает что же делать такие вес монитор когда ходит на сервер как он его проверяет на сервер с приложением на сервере с приложением запущен специальный скрипт здесь он называется чек пиль специальный скрипт который слушает на специальном слой служебном порту ктп запросы окей у вас монитор след туда get запрос со специальными параметрами каждый параметр представляет из себя какую-то какую-то проверку есть много параметров много проверок этот скрипт выполняет эти проверки операционной системе ну например что есть какой-то интерфейс что он значит поднят что они соответствующий адрес есть если все проверки выполняются удовлетворительно то считается что сервер пригоден вполне для того чтобы они у посылать трафик скрипт отвечает на этот запрос ухе у вас монитору соответственно до сержем все хорошо или сервером все плохо помимо этого скрипт ходит на специальный служебный порт приложения которые запущены на его сервере и спрашивает у приложение как она себя чувствует возможно там внутри какие-то ошибки уже в приложении происходит она почему то решил что неплохо бы ему этому приложению из ротации убраться проверяется куча параметров операционной системы проверяется статус приложение как я уже сказал все приложения у нас настраиваются из специальной конфигурационные базы данных и мы решили что у нас для каждого каждый instance каждого приложения в этой конфигурационный базе будет иметь вес с которым этот ресурс должен находиться в балансировке и вместе со своим статусом приложению поэтому скриптик возвращает этот самый вес то есть скрипт пришел к приложению спрашивает как ты себя чувствуешь приложение говорит со мной все окей я хочу быть балансировки вот с таким-то весом учитывая что конфигурационные базы есть они api это дает широкое поле для автоматизации что мы собственно и используем у нас на счету есть разные автоматические штуки которые могут которые могут специально заданными пачками выводить фронтис балансировки возвращать их возвращать их медленно заданном режиме и так далее и все это работало довольно долго довольно неплохо пока у нас не появилась такая вот штука котором внутри себя называем маркетинговым словом облака мой коллега олег анастасии сейчас рассказывал о том что это такое как работает если значит с точки зрения балансировки очень коротко это такая такая штука которая может произвольно запускать и останавливать из нас и приложений у каждого из нас есть какой-то ip адрес на который ну как-то будет посылаться пользовательский трафик и балансировки в этой связи интересно какие инстанции тому же умерли какие-то новые создались чтобы значит параметрам балансировки менять и как же мы поступили в этом случае в этом случае мы поступили довольно тривиально облака публикует во внутреннем горизонте dns специальная записи кучу айпи адресов один ip адрес это адрес контейнера да ну или начались контейнер умирает то его ip-адрес из этой записи исчезает ты или записи маленький окей у вас монитор периодически ходят в dns во внутреннюю resort соответствующую а запись для соответствующего для соответствующего сервиса который участвует балансировки вот собственно если коротко так работает наша балансировка и вот однажды это было в 2016 году мы задались таким вопросом насколько мы устойчиво к разного рода атакам в том числе где дозу в том числе xin флуду и вот первый квартал 16 года пик в самой большой атаки было зафиксировано вот на таком уровне 120 миллионов пакетов в секунду 120 миллионов пакетов в секунду но если пакета с установленным флагом синта это 120 миллионов соединений в секунду до кому-то прилетела что довольно немало мы начали считать экспериментировать насколько мы готовы к такому пику и выяснилось что суммарно все наши сервера приложение они такую атаку переварят запросто ну здесь не только сервера здесь еще есть два кубика балансировщик балансировщика my все вот так вот каждый балансировщик это только 200 тысяч соединений в секунду после чего он довольно быстро погибал а что это значит это значит что чтобы такой пик перекрыть нам нужно 600 балансировщика в 600 серверов а если вспомнить что они резервируются по схеме мастер slave до 3 умножить на 2 это нужно 1200 серверов только чтобы вот с таким сен флудом справиться в будущем наверное сыров еще больше понадобится это довольно расточительно для того чтобы бороться только с одной атакой таким количеством железом да а чем плохо собственно и ps почему он так почему он так себя ведет при таком количестве соединений такая схемка как работает мтс в двух словах пакет прилетел в ядро и попал сетевой стек в ядре работает модулей пвс в неё там таблицам ковки и он каждый пролетающий пакет с таблицей балансировки сравнивает и если он видит что пакет который нужно сбалансировать сейчас мимо пролетает он этот пакет вырывает из сетевого стека чего-то там в нем меняет может быть там айпи заголовок сверху еще навешивает инкапсулирует египет и запихивает его заново заново в этот же сетевой стек который пакет еще раз пролетает и уже выходит нужном направлении от там к серверам вот и проблема здесь налицо это две проблемы во первых сам по себе сетевой стек эта проблема в линуксе потому что он очень как бы сказать развесистые такой универсальный комбайн который на все случаи жизни вспомните количество всяких тузов которые значит configured и мониторит этот самый стать там много всего очень есть для медленной работы вот собственно первой проблемой вторая проблема то что пакет по сетевому стыку проходит по сути два раза там полтора ладно раза именно поэтому только мэр 2 миллиона пакетов в секунду может пропустить ввс при том что железо было довольно неплохое и вот какой то момент времени к нам пришли ребята из компании нефы они говорят что вот схемы работаю в самая плохая вы никогда там не тоби чистом хорошей производительности мы можем предложить другую совершенно технологию не надо никакого ядра linux берём маленький стек сетевой который работает в юзер спейси вот пакет сразу будет туда попадать вот маленький степ этого быстро пролетать и значит уходить тоже в нужном направлении и заявлялось какой-то бешеный совершенно прироста производительности и там сотни раз вот мы поверили конечно что из-за того что вытесняем визу space вдруг такой прирост и решили протестировать это решение собственно первый тест был вот такой мы сразу получили без малого тридцать миллионов пакетов в секунду 30 миллионов соединений в секунду через такой балансировщик железо было вот такое как написано под графиком внизу не самые топовые но и не самое плохое к сожалению график не сохранился после тюнинга это все выросла еще до 42 миллионов соединений в секунду через один балансировщик через один сервер вот если сравнить два вот этих вот графика который к сожалению в разных цветах то получается что действительно это решение работают в 200 раз быстрее только за счет того что стек сетевой очень маленький юзер спейси как это все выглядит с точки зрения эксплуатации штуковина запущенная на простом linux сервере там работает в user спейси процесс это вот нижний квадрате где написано н.ф. он на все деньги на сто процентов циклу поле входящие буфера сетевой карты выгребает оттуда все пакеты пропускает через себя там внутри реализован сетевой стек маленький с применением de pe детей библиотечки и запихивает обратно в буфер восходящие буфер сетевой карты и пакет и уже оттуда вылетают выглядит это все просто на самом деле с точки зрения эксплуатации там масса возникает проблем потому что во первых hype детей отрывает сетевую карту от ядра linux то есть из операционной системы сетевой интерфейс вообще пропадает был этаж 0 до потом он исчезает его там больше нет а это все нужно мониторить с этим нужно как-то жить пришлось ребятам сделать кучу костылей так это назовем например они сделали плагин для на это snmp д который в iv миг стандартный подмешивают информацию о этих вот исчезнувших сетевых интерфейсах то есть грубо говоря через snmp этот хвост со всеми сетевыми интерфейсами выглядит как обычный linux хост без всяких вот этих вот извращений с типе детей это же касается там и кое каких других штук типа по типам статус файлов боингов процесс и так далее как работает hail ability всей этой штуки она работает точно также как у и ps а то есть запущен heep live двух машинах прп друг друга пингует при этом с мастера на backup передаются 10 передается информация о типе соединениях при этом интерфейсе синхронизации типе соединения он оставлен такой же как у и ps а это значит что мастером может выступать например и ps а бэкапом efs или наоборот что весьма удобно при переезде с одного решение на друга и собственно вот у меня наверное все дальше мой коллега продолжит и расскажет что там под капотом у нас еще раз большое спасибо андрею за интересными 1 часть я хотя бы немножко подрос вы подробнее рассказать как у нас устроено все внутри и как мы все-таки получили такой просто завались по сравнению со стандартным концов с решением весом наш продукт в построен соответственно использовали на библиотеку от intel и открытым погиб детей который позволяет нам получить прямой доступ сетевой карте вижу с поясом и что самое главное мы используем на полную катушку все кадры которые нам дается на этом на большом интеловском серваке часа до 690 который был у ребят одноклассник что нам это дает соответственно как им это использую собственная др это всех the country's наше внимание поскольку diptyque просто как driver is easier моды и там ничего интересного нет с нашей точки зрения как работает решение л.с. он же тевес он построен аль-инсан стеки одинцовой стека многоядерность для обработки пакетов и спорта чем просто пакет как пришел на ядро там обработался и я туда же отправился в назначении так называемого рандом капли шин вариант а мы же у себя используем не уже по-другому мы беремся ядра которые нам доступны и делим их на условно две группы первой группой ядер собственно занимается полингом сетевой карты и вычитанием пакетов и передачи пакетов а вторая более большая группа я держу выполняется прикладную обработку диктор пакетов а именно балансировку сетевой стек и так далее роутинг все что необходимо зачем это сделать во первых это нам позволяет более эффективно использовать ресурсы наши многомерной системы то есть у нас может быть некое подобие конвейера что 1/100 не читает пакеты при этом раскладывать на остальные ядра и то есть у нас конвертации происходит мы можем варьировать сочетание я der который прием передачу выполняется я давно которую планет обработку трафика таким образом зависимости от степени функционала которым необходимо реализовать мы можем достичь наиболее фиктивными утилизации всех ресурсов системы то есть не будет такого что одно ядро поставить за того что допустим да очень долго прием пакетов идет ну и третье это нам позволяет также наращивать масштабируемость нашего решения просто добавляю ядра обработки прикладного трафика если мы видим что у нас прикладной скажем так обработка засыпается не хватает можете мы добавляем просто ядер 2 делится между ядрами и соответственно мы получаем наш необходимости процент производительности как мы производим балансировку трафика сначала как это делается в линуксе классическом подходе linux используют так называемый раз с технология то есть рис с кевин это когда сетевая карта сама считает хэш пакета и с этим hашем кладет пакет в ту или иную очередь аппарат на очередь соответственно ядром системы читать очереди обрабатывает пакет от танка причин делать мы такой подход не используем потому что он имеет следующий недостатки если вы например хотите проживать 6 от минус пакетов на со дна сетевой карты и разложите допустимых на 10 учредит вас не получится только из-за того что пропускная способность экспресса не позволит количество накладных расходов на собственно шину будет гораздо больше чем полезная информация средства плюс к этому ресурсу самой карты ограничены на каждую очередь аппаратную выделяется свои внутренние ресурсы которые можно было бы использовать и более эффективным если мы 1 очередь поэтому мы решили сделать все немножко по-другому вот наши отдельные rx сидра как раз и реализуется к скажем так скажем тот функционал rss а только мы делаем это программа плюс ко всему для того чтобы максимально утилизировать ресурсы вся экспресс а мы берем вам помимо чем я максимум три очереди на на ядро это нам гарантирую что мы точно закроем всю просто способность шины не упремся в нее а плюс к этому соответственно внутри самый карта блокируется больший ресурс под очереди и средства больше вы фреза с пакетов так же нам даёт пользу с точки зрения потери пакетов ну и главное наш хэш который читает наши rx ядро он способен учитывать не только простые пакеты на 1 капсуле как он куб столицу вилановы икстлан играя неважно главное для нас это посчитать кэша l3 и l4 заголовку полезных данных чтобы исходя из этого хэша уже разложить по нашим я дам выполняющим прикладную обработку зачем вам это все нужно опять-таки это нам все нужно чтобы вы могли линейно масштабировать нашу производительность просто добавляя ядра также надо отметить что поскольку наших коллег из одноклассников достаточно разнообразной железо то не все карты одинаковы и метре над хеш-функций или одинаково набор очереди и поэтому классический подход к до 40 а ядерный сервер читается в 45 р с одной карты не всегда возможно соответственно как собстна происходит обработка трафика на нашем прикладном ядре эта картина общая не только для нас но и для вас это общий принцип обработки стелла трафика поступающей пакет повышенному хочу ищется в таблице сессий известно если ставится сисек существует начну нас еще установлена на отбалансировано известное значение все что нужно сделать просто применить некой интерполяции например ай пи и питона и сделать и отправить все равно значение это достаточно просто это не стоит заострять внимание самое интересное происходит когда сайте нету и нам нужно балансировать и вот здесь начинается самое интересное почему-то работает как у нас то сделано так называемый слово page в рамках от услуг по чему должна сделать следующее мы должны определить во-первых пакет вообще нужно балансировать во вторых соответственно суду я должен работать ну и в третьих реализации отправки отправка одинаково видео но все равно требуется неких ресурсов и на с точки зрения тратить пакет первый раз чтобы менты netflow как сделан этого слова почвой песен то что на чем он проседает сожалению в перрисе используется одна общая таблица на все ядра которые есть системе и все я да соответственно когда получили пакета сетевой карты наш мир с а высчитываются хэш-функцию по двум параметрам только по факту это порт источники пор и я достиг источника и исходя из этих параметров влезут в эту таблицу чтобы соответственно поискать там соединение либо и байт новая чем это плохо таблица добавку добавление в таблицу нового соединения производится под спину о ком то есть эксклюзивный доступ одного ядра соответственно это крайне негативно сказаться на масштабируемость решения поскольку чем больше ядерным вышибу драться на смело кики ниже наш производительности соответственно после того как соединение успешно был добавлен api весом начинает следующие стали его нужно сбалансировать и многие scheduler и в прессе такие как вайтран дроби а используют также spylog для того чтобы сделать монтировку что также сказывается негативным производительности решения но и третье самое плохое что может быть это удаление самого соединения удаление соединения производятся по таймерам то есть таймер потухает это мир это отдельный поток который грунтом завтраку не важно который грубо говоря извне лезет в таблицу но поскольку для доступа к таблице использую еще третий механизм так называемой россию вид копий апдейт то удаление такой модификации такой таблицы по версию очень тяжелая и потому что для того чтобы играем в одну запись нужно дождаться пока все я на в системе пройдут мне эту критическую секцию в этом и все то есть это также негативно сказывается на производителя решение более того в таком варианте удалять может только грубое одно ядро то есть 1 дрочил дарить сессию то все остальные будут за как он закончит что также кататься феста сказано признательности как мы это обошли поскольку у нас использовать свой хэш который позволяет нам однозначно каждый пакет одно и то же стекла 101 своих на свой ядро не только некуда больше мы используем кэш по всем четырем парта полям здесь присоединение отрезаем fool to pull мы можем себя внутренне для каждого ядра имеет свою таблицы соединений которые только это ядро обращается спешно избавиться от дополнительно блокировки spylog а то есть это первый момент который мы обошли следующий момент мы переделали алгоритма балансировки так чтобы они максимально как ничего невозможно работ то ли по ядрам то есть не лезли в чужую память и работали только локально у себя например вырыта по факту статический алгоритм там и статическое распределение и его достаточно просто переделать именно на то чтобы только одно ядро знал что нужно сделать и третье самое главное все моя пьеса основана на том что есть так называемый стикиз с то есть я грубо говоря сессии на 80 порт и на 440 3 порт по сюда пойдут на тут на один и тот же сервер в офисе соответственно для этих целей используется та же самая та же самая таблица соединений вы можете стать то есть создание запись грубая пришла нам сессии которая нас должно быть стики наносим даже запомнить на какой сервер все последующие сессии должны пойти там предела допустим 30 секунд для этого я ps берет создает первой записи так называем template об и потом по это еще создает запись собственно сессии то есть два раза больше работы делать которые еще можно снижать производительность из такой результат у них а мы же переделали все это налог free алгоритмы соответственно реализовали отдельную таблицу пирсе стэнд насти которая поиск который реализован с помощью with free алгоритму то есть никаких ожиданий никаких бизе лупов ничего такого а если нужно добавить в эту запись новую таблицу то там использовала creed который построен на кассе классический способ и также для протоколов которые имеют некую динамическую составляющую должны игру буря учитывать like работ на всех я даже таких например лист connection в италии скорейшем то есть надо посчитать у кого сейчас данный момент меньше соединений этот сервер отдать у нас есть некие жареные данные котором также мы с помощью ла при апдейте информация необходима также в целях достижения высокой производительности мы реализовали свой роутинг быстрый и быстро инкапсуляция данных также позволил нам и будущем очень будущего нашем продукте на нарочи линейной масштабировать просто добавляя да и поэтому когда на тестах мы пришли к на класс ником и нам дали сервис сорока литрами мы не у первых классической проблем что все я драть стоят и ждут на одном каком-то ресурсе нас еды максимально эффективно обрабатывать свою часть трафика времен теперь он теперь это самому интересному как собственно наше решение противостоит sinful чтобы не свалиться а сначала какой пьеса сделал обвес построена на достаточно простой схеме есть некая пороговой значит другая table записи в таблице становится больше никого значения включается демон который производит убью убивается но соединение в таблице выполняться по произвольным образом то есть там некий рандом считается и бежит по таблице и грубо говоря в каждом хлебать убивается и днями который если может убить опять-таки поскольку это построено на спинках и на механизмы это крайне медленно то есть вас при большой нагрузке вот ck кстати показали теста если больше двухсот тысяч соединений сервер просто перестать отвечать потому что он все время висит в процедуре было не соси более того почему это неэффективно он убивает произвольным образом и грубо говоря на свободе цельсию в другом хэш баки кинем в том котором вам нужно создать у вас все равно не будет никакой пользы от того что вы убили соединение другому рыбаки но вновь пришедший соединение полезет другой баки то там места не будет мы срез на немножко по-другому сделали нашем варианте мы очищаем таблицу именно по факту прихода соединения если мы видим что-то места нету мы смотрим если в этом хэш боккетти какие-то соединения подозрительные которым мы можем убить оценим по разным критериям каком он состоянии сколько было пакетов когда после время удобства и так далее и соответственно можно убить мы просто его закрываем помечаем не валидны и перри используем этот же объект и тот же то есть мы не выделяя киники локации памяти ничего перри используем и тот же объект в этом же боккетти для много соединений то есть мы убиваем сразу двух зайцев мы не выделяем память мы не ищем место куда встать у нас все уже готово это нам также позволяет достичь высокой показатели именно убывание соединений создание нового поскольку мы нам нет нужды стоять и вдохните цепи блогах соответственно все эти наши улучшения привели к тому что к раз мы смогли показать вариант священника издательстве 200 раз из хорошие пропускной способностью с точки зрения sinful да что мы не завалимся на этом то есть просто избавившийся просто конечно но избавившись от ехать из пилотов от всех ненужных абсолютно синхронизацией и сделав немножко учить по мне распределение потоков данных можно достичь и производиться 10 раз больше чем простой концов решения спасибо спасибо за доклад меня такой вопрос у вас поддерживаются динамическое добавление хоста она которого осуществляется балансировка да конечно если вам интересно вы хоть что конечно фронтами это было одно из требований ребята знак латников чтобы было не надо было по 30 минут стать добавление там до серого все что делается в очень больших числах скажем так что там за одну секунду несколько десятков тысяч лидеров добавить или удалить конечно все это реализовать тоже тоже со своими особенностями сделан ребята лет спасибо ну так вопрос а ну а 200 тысяч соединений это и активные и не активны а это в секунду мест виду что 200 тысяч или вы имеете ли вы себя все еще мы сравниваем объекта 200 тысяч на я просто выключать минуты на своем балансе у меня 50 тысяч активных соединений и около миллиона не активно но смотрите мы имеем ввиду именно 200 тысяч активных секунды новых с ним тоже которые им на пришли которые отработали которая балансирует секунду то есть вы наверно иметь еду конферанс это то что вас миллион почивающи что но там есть белок до но я смотрю вот но и же таблицы и она полностью то есть вот ну есть таблица на активные баланс которая миллион правильно у миллион 100 тысяч сегодня раздалось там накопилось миллионы все здесь 200 тысяч новых в секунду поступает имеется ввиду пришли и нашли место если надо освободили там размер таблицы тогда будет сколько там миллиарды получается вообще есть таблицы ну откуда куда этой морально есть активные соединения которые активны и и неактивные которые потом офтальмологическое то время но хорошо я поймал здесь надо пробежать по всей таблицы и выкинуть и не отъезде сделать это дорогую операцию с луком хорошо теперь я вам скажу значит весь используется одна таблица ее размеры стандартных 24 что с миллионом fantasy которая именно для того чтобы хранится день это все соединения просто не не создалось и там хранится дней для того чтобы его состоянием антоний таймера так далее был весит таймер используя классические linux вата мира у нас таймер сна используется свой на каждом ядре свой таймеры и грубо говоря пробегает часть этой таблице соответственно по поводу шла просто миллиардов сессии а spylog не таблицы если вы имеете ввиду drop and rita это соответственно у нас всегда будет проблема потому что как минимум от размера таблицы она будет зависеть потому что мама берется грубо говоря пробегается цикла 1 32 но также он был проблема что даже одно соедини чтобы удалить вам нужно провести полный цикл так называемый write из дождаться пока все потоки выйдут проблем или у нас с а на сколько я понимаю наши коллеги цена классиков проводили со стандартным настройкам это значит 22 часов таблицу то есть те 16 если вас 200 секунду вас уже все забито все работает нормально если будет два миллиона желе вы говорили ну я ничего не могу сказать как москве проблемы со 150 тысяч и больше не останавливаться не может и не будет а если вас постоянно валит с новыми s7 то слушайте у нас проблемы начались при 0 2 миллиона введение в секунду вот когда у вас начнутся я не знаю не много ли не могу понять вы хотите чтобы я посчитал сколько а певец завалится зависит от трафика зависит от объема и в зависит от количества ядер ну тут вам проще сказать сколько вы получили без секунду и все будет понятно заваливаться на завалить если вы видите что у вас какой-то порог вы уперлись там 200 тысяч это всё начал вызывать ну вот вы заваливаете вот вы больше 50-ти можете сделать просто не понятно что вы хотите видите теста не знаю посмотрите когда у вас там все завалится как мы можем так рассчитать выходу с доски в обеих хотел женить можно должны поблагодарить за доклад актуально интересная тема меня есть несколько комментариев и вопросов 1 комментарии вы упомянули сию linux вообще говоря он не ждет ожидании то есть его смысл в том что мы отметим несколько морель на с нескольких я не потом когда у нас референса отпускается мы находимся через момента организации всё отпускаю то есть ни одна hydra никогда не ждет но смысл сию в этом могу сообразить если вы посмотрите ка то есть явный вызов candy candy снова решил который потому что у вас должны scheduler сработать на убивание 1 сессия когда baked чистится если вы имеете виду я имею ввиду еще вообще-то все api вы я могу пока мечеть все об или корейка понимать я могу пока метель конкретно россию но синхронизацию сию она говорит о том что мы вас должны относить все все . ты сейчас работы задами данными и когда все уходят по сути это реформами в том что когда вы сможете сделать собственно когда он даже из к наследованию все точки пройдут это все ну дорогая операция даже финская under который атамана построен примерно 2 и даже любопытно как вы должны вернуть дождаться вот вас 40 ядер вас одно ядро удаляет reside кедр сейчас что-то выполнять сколько вы будете что мы не ждем и смысл кто будет очищать great кофе апдейт он называется ритка 5 в том что мы всегда читаем виду и не когда блокируется обновляем мы тоже параллельно и переписываем поинты по нервам они очищают вот вопрос потому что же когда потом чешется нам нужно искать место освободить кто кто последний тот пустил он должен куда-то запушить это память чтобы ее можно было использовать последствия ну окей то есть весь цикл давайте сваливать тогда разделять проблемы непосредственно сию к проблема синхронизации проблема очистки большая проблема зачем вы seafood фильтруйте на айпи весе то есть у нас есть син cookies у нас есть синтаксис 200 тысяч соединений это вообще не проблема для тинк вовсе тем более на мощном железе а я думаю коллегам syntrax есть очень большие проблемы вот она глючная почему-то с а из гэбэ и драйвером она как-то плохо дружит например серравале пытаются от него и вот такого много там постоянно вылезает ну как-то с а потом пытались общаться там как бы вы попытались фокси анализа решили другим путем вот таким нас это устраивать ну ладно ки тогда здесь вопросов нету по поводу непосредственно реализацию вас очевидно там есть 10 5 стек вы не сказали что он умеет ну понять дело что мы можем отработать если на 9-ке и над мы позволяет 10 миллионов пакеты в секунду держать на ядре добыть мы не сможем миллион сессию держать вопрос что вы делаете в этом пистолете найдем это вообще написаны вами , клипа это взят какой-то призрачной рецензиями по нас все написано mimaki ими можно заселить окей если вы сами писали вы можете сравнить ваш засекайте стек с тем что есть в операционной системы в зрелых ну во первых цена вопроса данное решение tl4 то мне нужно полноценно терменируйте себе стрессе списались и поэтому там bestex скорее всего и peace при миссисипи реализовано соответственно поэтому там нет терминации она там не нужна собственном гланд а что вас пистолет принципе нет то есть выпаса база в этом решении тогда это идол жители дома пакетная обработка если вас интересует терминация у нас есть другой продукт стремятся ну конечно ну и конечно все это если вы хотите получить хорошую производительность а вообще в чем смысл использовать и а по сравнению допустим с линуксом процессора он один тоже да конечно так секунду не меняется там никакой магии не то просто вы специализируетесь что ваш процессор занимается обработкой поэтому если вы хотите высокую производительность вы делаете как вот используйте паники и вот процессор на все 100 процентов только обработка пакетов то же самое с тисе пи стеком уходить эффективный стек вы должны разработать с учетом того что он будет вас сделать если это прокси to practice терминации про бросая 7 и шифрование этого значится свои подходы к реализации общего решения которые есть на рынке который мы видели там где валенки хиппи и так далее они к сожалению именно того что как и linux не общее и вашему нашим и не дают достаточно производиться который бы нам хотелось например если вам интересно у нас есть a generator присесть и полноценности стек и он на 4 я драк выжимает полтора миллиона соединений в секунду держит полноценных типе соединения полноценном ты и так со всеми делами чисто из за того что мы знаем что нужно делать генератор нам не нужно там данный клад выгонять мы за оптимизировали кучу вещей поэтому на ваш вопрос нет мы не используем стайкой мы хотим и мы должны сделать произойти максимально чтобы конкурировать поэтому делать по силе на решение конкретных задач поскольку это сейчас не подписываться писать меня бы бы можно тогда hашем спасибо станислав компании мегагруп мне наверное вопросы попроще вы трафик через свое решение прогоняете в одну сторону или в обе а это симметричная же в одном табличный я и ещё один вопрос рассматриваете вы ли вы то что в пакете за четвертом уровне если да то что делаете с с или нет если ваш вопрос нет мой как мы не терменируем и ничем делся цель мы только l4 мусора и я имею добром как защитка куда со если использовать или формате нет adidas мы используем только то что на четыре рода да да сам мы не защищаем это даст мы противостоим sinful и мы сами не подам идеи в этом и для того чтобы просто к сессии жевания там справа другие параметры спасибо станет добрый день подскажите такой вопрос насколько я понимаю вот этот то есть замена ппс а вот сама сам суд который выводит ну то есть проверку ваших сиропов и критичны делает он то остался тот же самым вы сохранили только интерфейс да да насколько я знаю тут же intel директор downtown по тупому работы фантазерка и там по сути утилиты линуксовые там там теле из те или иные там а у вас там как реализовано с помощью ваши java там какие-то интерфейс а собственно все точно также реализованы и ребята сделали команда то вот есть и пиво и садом она называется этим айпи весами управляет они сделали эти vs adam точно такой же с такими же аргументами все это точно также работает наш соус по-прежнему думаешь что там ну вы поступили создан дергается да спасибо спасибо за доклад вы пробовали какие-то другие варианты обойти этот стек соответственно например frank монет мы используем то есть вам понравились результаты и и вы в компоненте скатах самая быстрая это и нет map утверждает автор и прочих просто них это intel делать у них хорошей оптимизации чистого железа сего спасибо за доклад а вопрос такой вот вы говорите что вы переживаете до суда соответственно если на вас приходит там теоретический предел карты там 40 мегабит 60 миллионов пакета в секунду до как быстро у вас есть легитимной какие-то цессии которые вы тратите все до какого размера таблицы собственного сбой вот эти 60 миллионов эти 200000 вы имеете этого кита или сама таблица суть в чем у вас балансир который должен отбалансировать входящие потоки чтобы следующий пакет от одной сессии попал к другу если попадает собственный на тот сервер соответственно там не из этой сессии потеряется собственно вас сервис будет недоступен то есть задачу в чем чтобы из этой таблицы не вымылись легитимный пользователь был момент атаки так получается что и вас идет атака там 40 гигабитной карты 60 миллионов пакета в секунду вот какой размер должен быть какие-то и mode и должны быть чтобы не вымыть вот эти вот несчастные там сто тысяч соединение в секунд то есть получать это до момента с тысяч раз больше у вас пакета есть на каждые 6000 фейковых пакетов у вас будет ожидать его оставить ну это как бы первый ряд скажем так памяти завалить а второй вариант а когда у вас идет всем плутон достаточно примитивный вы знаете что сильно не было подтверждений пришелся на мы можем эти все быть в первую очередь а сессии которые уже установились полезное у них и состоянии tepees st70 пришло там какой-то полезно трафик пошел это с точки зрения если мы не хотим убить и уже существующей сессии если вы иметь ведро там уже сами метите которые конечно вы тогда должны пропускать обратные пакеты через я вы сказали у вас в одну сторону кто пришел мы видим сян сян а к видим окна мы считаем лаки считает половина как бы половин трафик это мы видим можем принять решение как он станет сессия ну точно кто подозрительный товарищи лето хороший товарищ 2 должна посчитайте нам не успеть вымыть то есть у вас большие таблицами ну большие таблицы и главное что мы дропаем те сессии которые не мы считаем что они просто еще не установились им иначе не повезло это не защита вы сами же этому да это мы сами if any money service деградирует при больших а так возможно тут надо бороться память о данном случае спасибо большое за интересный доклад у меня вот пару вопросов первый вопрос я правильно понял что основной вклад в ускорении все таки это было удаление у того мьютекс и который был на хэш-таблицы который брался при заведении новой сессии ну а а также я скучал . чувства ну да да да да да и также выбираем сердцем не очень хорошего который убеждает спам сект ну и рсу хороший навес виду что избавление от такого способа умывания вот это вниз основной вклад ну конечно предыдущем слайде было ограничение по вычитания из ну понимаете это уже просто понять для весьма не особо актуально это для принципе просто для нас самих а потому что у вас в принципе столько не вычитает но linux чтобы называть порядок а сколько может ли случиться нам сейчас очень искаженного еще раз с настроить то там где то может быть миллиона ядро будет лучшем случае пакет но пакет еще можно и еще один вопрос вы когда разделили хэш-таблицу вы делили и эпох и шан конечно если хэш у вас участвую я так понимаю api адрес отправителя до вполне и суп и при разделении нет нет афишу нас участвуют полностью все четыре полностью тапочек асинхронный процессоров специально строя возможно спасибо извините пожалуйста вот спасибо за такой насколько я понял вы как раз отказывайтесь от раз с пользу того что учитывайте пакеты по большому счету одним ведром то есть вас есть специализированные ядро а вы проводили какие-то замеры по поводу того что если вы делаете вот это ядро по сути узким местом у вас пакеты на другие цикута будет зачастую кэш мисс ну то есть как например смотрите если бы вы не сделали rss то пакет у вас уже бы находился в кэше в прогретом это было бы немножко быстрее чем когда он попадает ну каждый раз на новое ядро неспециализированные обрабатывается это первое и ну как бы проводились и такие замеры вы сознательно как-то приняли это решение а и замер и прочее проводились конечно даже не нами это intel рекомендую и кстати такой вариант включить rss да что это будет там не то что качать кабинетный вариант а потому что если нас допустим 40 га битная карта конечно нам проще и надо будет не проще просто нужно будет два ядра чтобы вычислить все таки одно ядро там больше 40 миллионов никак не вытянет что это по крайней поэтому там два ядра используется две очереди по поводу вашего вопроса то что вы имеете еду кэш подогреть так называемым дерек дата его видимо как вариант это не проблема все 2 cache дата его не пишет в первом пишет второй который между общим даже когда пакет идет в хостовый памяти карта не знает какой я должна вычитать он пишет в общем вообще кэш поэтому особой разницы нет но на самом деле это первая и передача собственно пакетов дам достаточно дивизиона сделаем и не педантом адресатом 4 буфера у нас компонуется метаданных для того чтобы более эффективно обрабатывать скажем так не ездил сам пакет разрезая и я правильно понимаю что таблица сессию вас для каждого cybus моя получается она не вообще она да да разбито но поскольку у нас тащишь использую как статическое распределение мы можем поделить между кадрами говорят точно да что вот этот хэш всегда будет с таким концом будет попадать на федору вот картинка предыдущая там был айпи веса отдельный соответственно модуль который до таймер который удалял вот удалению у вас точно также выделен циpкa и обед лето в как раз если мы выделим а делится по у нас не будет эффект от того что нам при заблокирует доступ таймер считается том же цикле где свобода это когда он свободно там нечем там грубо говоря к пете кооперативную многозадачность реализована в рамках одного ядра почти там часть приняли обработали часть таймер почитал его так он укроется чтобы иметь соответственно доступ без блоков спасибо можете задать спасибо аплодируем"
}