{
  "video_id": "evM_e3blBL0",
  "channel": "HighLoadChannel",
  "title": "Устройство индексов в почте Mail.ru / Рустем Гафаров (VK, Почта Mail.ru)",
  "views": 617,
  "duration": 2763,
  "published": "2024-10-29T02:48:13-07:00",
  "text": "Всем привет Меня зовут Рустам Гафаров я разработчик из компании ВК Наша компания занимается наша команда занимается разработкой систем хранения для проектов почты и облако mail.ru и Сегодня я расскажу про устройство индексов почте mail.ru это третий доклад нашей команды на этой конференции послушав все три доклада можно получить общее представление о том как хранятся данные в почте mail.ru в двух словах по структуре доклада начнём с концепции посмотрим как она была реализована в первой версии и перейдём к эволюции первой версии до современной реализации и сделаем соответствующие выводы тут спойлер мы стали экономнее на 600.000 долларов в год только на хостинге Итак сегодня mail.ru - это десятки миллионов почтовых ящиков что в общем-то соответствует нашей ежемесячной аудитории уникальных пользователей количество писем в каждом ящике может достигать нескольких миллионов а иногда бывает больше при этом наш редж имеет rps на чтение А в Пике до 7 млн запросов в секунду и э на на чтение и до 30.000 запросов на запись а в секунду соответственно в то же время мы получаем а десятки миллиардов сообщений ежемесячно и эти цифры постоянно растут но нам сейчас они нужны э для того чтобы а разобраться с какими проблемами сталкивались И как мы их решали забегая вперёд и учитывая предыдущие доклады моих коллег можно сказать что ST mail.ru состоит из трх подсистем подсистема хранения вложений подсистема хранения писем и подсистема почтовых индексов подсистема почтовых индексов это некий аналог таблицы размещения файлов в файловой системе То есть когда пользователь открывает какую-то папку входящей приходит запрос на список писем в подсистему хранения индексов и она ему даёт список этих писем и видит э этот список писем далее нажимая на какое-то письмо запрос также приходит в под систему почтовых индексов письмо собирается отдаётся клиенту и оно открывается и в общем Суть в том что практически в любом запросе к нашему почтовому ржу участвует подсистема почтовых индексов и это са на самом деле самая высоко производительная высоко нагруженная подсистема в нашем почтовом реже очевидно что есть какие-то общие свойства общие требования к такой подсистеме почтовых индексов в частности конечно же мы должны обеспечивать целостность и персистентность То есть если например пользователю какое-то пришло письмо и оно положись в подсистему хранения писем то мы должны это письмо Показать ему в списке писем то есть соответственно должна быть запись в подсистеме хранения почтовых индексов и соответственно если как мы показали какое-то письмо то оно Должно открываться корректно И никакие технологические сбои не должны приводить к каким-либо нарушениям этой целостности и персистентность и в то же время можно заметить что когда мы работаем с почтой в основном Мы работаем с какими-то чаще большими выборками то есть Мы открываем письма которые хранятся в определённые папки а берём их объединяем в какие-то треды эти выборки удаляем в корзину как-то фильтруем и так далее И на самом деле почтовый индекс - это такая наша внутренняя СУБД которая предоставляет интерфейс управления таблицы и размещения писем в нашем сторедж и нашими бизнес данными является информация о том Как и где в каком состоянии находится письмо и вот эти вот бизнес данные они также могут быть проиндексированы средствами СУБД и тут Важно не путать почтовый индекс - это наши бизнес данные а они в свою очередь могут быть проиндексированы средствами СУБД и заглянем чуть детальнее что у нас есть в бизнес данных а тут модель данных в основном определяется тем что видит пользователь У пользователя есть ящик в этом ящике есть какой-то набор папок в каждой папке есть какие-то письма эти письма имеют определённое состояние па тоже имеют определённое состояние то есть эти письма могут быть непрочитанными помеченные как важные и так далее И вот эта информация о том как и где лежат организованы письма в почтовом ящике это не сама почта это метаданные О письмах это и есть наш почтовый индекс и вот если брать эту модель данных реализовывать в классической какой-то обычной модели реляционной СУБД то схема данных получится достаточно простая то есть вся информация о том какие есть письма у нас в же она будет хранится в таблице mess А в таблице messy будет какой-то связанный ключ с таблицей mailbox в таблице mailbox у нас будет храниться описание ящика соответственно письма объединённые в определённые треды будут иметь записи в таблице треды информация о том в каком фолдер лежит это письмо будет в виде связанного ключа а записано в таблице фолдер будет сделать Селект выборку из такой таблицы мед Нам необходимо будет сделать селек с каким-то Джой фильтра а если мы будем добавлять новую запись в таблицу мед Нам необходимо будет сделать какую-то транзакцию в общем случае записью в несколько таблиц и тут всё просто но у нас есть особенность у нас в почтовом индексе порядок количества записей это 10.000 миллиардов записей Откуда берётся такое число как я отмечал ранее у нас порядок количества ящиков - это десятки миллионов в каждом ящике может быть несколько миллионов писем естественно в реальности это может быть больше но когда мы проектируем такую систему мы соответственно должны это всё предусмотреть Поэтому перемножать эти порядки мы можем заметить что у нас может быть до 10.000 и больше миллиардов записей и в то же время важно заметить что в паттерне нашей эксплуатации количество запросов на чтение оно сильно больше количество запросов на запись то есть мы чаще на два порядка в 100 раз читаем чем пишем на сторож И это тоже надо учесть Ну и очевидно что такие объёмы данных они должны как-то шардирование хорошо изолируют и мы можем взять объединить наши почтовые ящики в определённые подмножества и каждое это подмножество обрабатывать Независимо в рамках одного шарда Но чем больше мы будем брать шардов тем соответственно будет меньше ящиков в одном рде и тем соответственно будет писем в каждом рде соответственно Чем меньше будем брать шардов тем больше будет помещаться ящиков в каждый шард и тем больше будет таблица месседжи в нашем рде Ну и тут очевидно что мы захотим уменьшать количество шардов для того чтобы снизить наши эксплуатационные издержки и в любом случае придём к тому что у нас появится таблица месед в которой будет большое количество записей и количество записей будет соответствовать каким-то порядкам который максимальным порядкам которые поддерживают современные СУБД и это в общем-то нормально это всё будет работать хорошо наверное но как мы отмечали ранее у нас производится выборки из нашего индекса по самым разным полям то есть соответственно для того чтобы делать выборку по ящику намм необходимо будет строить индекс по ящику и соответственно будет аналогичные индексы строить по фолдер по по каким-то трендам которые объединены письма и так далее И вот в общем случае нам уже для того чтобы проиндексировать эти данные нужно много ресурсов в виде памяти и процессора и тут особенность заключается в том что если у нас приходит какой-то апдейт ирт по определённой записи то в индексации будут участвовать все записи этой таблицы и Хотя они могут принадлежать вообще совершенно неактивным ящиком то есть и мы каждый раз их будем индексировать по многим индексам тратить на это память и процессор и в итоге это как будто бы не очень эффективно поэтому мы решили эксплуатировать свойства изолированности ящика более и решили представить один ящик в виде отдельной какой-то маленькой виртуальной базы данных и в общем-то если так как у нас А количество запросов на чтение сильно большее количество запросов на запись то мы хотим такую базу данные такой базы данных держать прямо в памяти то есть мы хотим наш Каширова наши индексные данные ящика прямо в памяти и при этом запросе чтение соответственно выполнять прямо на данных которые уже лежат в памяти а запрос на запись для сохранения целости мы будем мы хотим писать в тный сторедж и потом в кэш Ну понятное дело что подъём таких ящиков памяти они занимают какое-то время и соответственно нам нужно как-то максимизировать наш кэш иид и чтобы А мы не сильно много тратили накладных расходов на постоянные подъёмы таких ящиков из персистентное для того чтобы вс-таки подм происходил быстро из нного Режа необходимы данные этих почтовых ящиков индексные данные этих почтовых ящиков размещать на SSD дисках и в общем-то вот эта концепция она Ключевая она реализована в нашим индексом демоне который называется милита у этого Демона Этот демон представляет интерфейс доступа к ящику в виде отдельной базы данных внутренний фреймворк который называется Иман в имане у нас там какие-то серверный порт для примо входящих подключений быстрое логирование и так далее и первый комит на самом деле был сделан ещё в 2012 году С тех пор в мисли очень много изменений происходит но концепция в целом осталась в то же время неизменно и код остаётся контролируемым и он максимально что в общем-то говорит о том что он максимально удовлетворяет нашим текущим бизнес требованиям и разберёмся внутри как устроен милита И начнём немного издалека в частности с формата хранения почтовых индексов до определённый момент у нас был такой фиксированный формат хранения данных и тут особенность заключается в том что от нашей продуктовой команды нам часто приходят запросы на добавление каких-то новых фич То есть например на приходит и вот эти Новые фичи иногда требуют изменения хранения новых данных в наших индексных записях То есть например на приходит продуктовый какой-то запрос для того чтобы объединять письма какие-то модные новые треды и соответственно нам нужно сохранить какой-то тред ID в наших записях и мы хотим добавлять вот такие новые записи в наши индексы не зависят от уже существующих записей и до определённого момента у нас были какие-то зарезервированные поля Когда мы могли откусывать ка какие-то там в нашем примере ID и их использовать но к сожалению заре поля могут заканчиваться это приводит к тому что ще можем э блокируем добавление новых фич Потому что если мы захотим что-то мигрировать то миграция у нас в общем-то она невозможно потому что размеры наших почтовых индексов они составляют несколько петабайт и схему данных мы так быстро поменять не можем поэтому э Нам нужен какой-то не фиксированный формат записи данных который обеспечит нам добавление новых полей независимо от уже существующих и э избавит нас от каких-либо процессов миграции Поэтому в качестве базового хранения э базового формата хранения данных милита а был выбран наш тал тал - это наша внутренняя структура данных где мы первым байтом пишем тип этого тала далее пишем длину данных этого тала и дальше у идёт соответственно данные этого тала а данные этого тала это поля этого тала они состоят из они имеют формат аналогичный самому талу То есть есть идентификаторы поля есть длина данных поля и сами данные и вот талы такого такого формата они хороши тем что при сериализации неизвестные типы талов и неизвестные типы полей талов мы можем легко пропускать и приведём пример небольшой пример тала который может быть у нас это Например tle mess размер имеет там 1923 байта и пару полей и subject там соответственно с размерами и с данными Но на самом деле мы тутли добавили к этим Тап немного контрольных сумм и каких-то тегов Для более оптимальной индексации и получили формат который мы называем внутри XR XR - это ровно те записи которые мы реализуем на диске и поднимаем в кэш Когда нам нужно поднять ящик в кэш и количество типов талов у нас на самом деле не такое большое У нас есть бо который описывает состояние ящика У нас есть фолдер те папки которые видят пользователь в графическом интерфейсе плюс у нас есть сообщение это наиболее часто встречающиеся используемый тал есть какие-то которые опит ещ какието другие талы которые в общем-то не вошли на этот в этот слайд И как я отмечал Мы в общем-то такие талы и сохраняем на диске их поднимаем в память когда ширу ящик индексные данные ящика в памяти и разберёмся чуть детальнее как это происходит до определённого момента индекс Фато и каждый раз когда мы М пытались его поменять мы сталкивались с тем что нам нужно эту запись найти на диске прочитать записать и это приводило к тому что мы неэффективно тратили iops и по-моему до 2000 какого-то двенадцатого года или даже раньше а это приводило к тому что мы не могли утилизировать Свободное пространство на диске то есть как бы а Свободное пространство на диски было но аоп сов естественно не хватало и добавление какого-то активного ящика на такой диск приводило к тому что в общем-то все запросы всех клиентов начинали тормозить эту проблему тут мы на конференции мы освещали очень много раз и я много видел это в соседних докладах но Поэтому в качестве более оптимального формата хранения данных в милита использовано не хранение готового индекса на диске а хранение некого лого последовательного Лога изменения индексных данных то есть мы храним последованность операции которые меняли наш почтовый индекс и далее Когда нам нужно поднять этот ящик в кэш мы проигрываем этот эти наш Лог и получаем актуальное состояние записи в кше достоинство такого метода в том что в общем-то для того чтобы добавить в наш последный Лог одну операцию изменения по модификации индекса достаточно всего лишь сделать практически одну файловую операцию и в итоге мы приходим к тому что у нас готового ило на диске нет того индексного файла на диске нет У нас есть xlog это как Лог модифицирующих операций которые модифицируют индекс и как в примере на слайде У нас есть четыре операции мы эти четыре операции в ик слоге эти четыре операции можем проиграть и получить актуальное состояние в кэше но очевидно что тут есть проблема а потому что ило Копится и содержит избыточность например пользователю пришло какое-то письмо он его удалил и каждый раз в и слоге мы будем содержать две записи и каждый проигры такой мы будем сначала в кше создавать новую запись а затем её удалять или как в примере на слайде Мы видим что пользователю пришло какое-то письмо оно не прочитано провалилось куда-то в ибо потом пользователь его прочитал Прочитав он понял что ему это письмо не нужно он его удалил переместил в папку ш И на самом деле это избыточно достаточно хранить одну запись в которой будет информация о том что письмо находится в корзине оно уже прочитано и вот такой очистки Лога от избыточности мы называем компакты и в результате компакта мы получаем спш То есть спш - это такой из очищенный от избыточности xlog и в общем случае для того чтобы сделать компакт Нам нужен какой-то предыдущий снапшоте xlog и после компакта мы получаем новый снапшоте А предыдущий снапшоты xlog они в общем-то потеряют свою актуальность могут быть удалены либо использованы в качестве какого-то бэкапа или ещё для каких-то других целей и в общем-то вот эти две основные идеи когда мы сделали нефиксированных записи и начали хранить Лог модифицирующих операций индекса это основные идеи милита первой версии и у нас появился формат хранения данных который называется Ифа именно такие файлы мы проигрываем и получаем актуальное состояние почтового индекса в нашем кэше и после вни 1 нас было очень много хороших штук в частности мы э разблокировали поддержку новых продуктовых фич а удешевить стоимость серверов из-за того что начали более экономно использовать память и процессор и конечно же стали лучше утилизировать свободные э дисковое пространство за счёт того что более экономно тратить начали тратить АОС но очевидно что у милита 1.0 было очень много проблем э в обще они обуславливаются в основном всем тем что мы использовали локальную файловую систему в коде милита были прямые вызовы в локальную файловую систему и надо было это как-то решать и это порождало две под проблемы во-первых у нас был всё-таки сохранилась проблема рандомного ао а потому что мы её решили в рамках одного ящика мы сделали последовательную запись в рамках одного Икс Лога но запросы по ящикам они приходят параллельно и это приводит к тому что у нас всё равно остаётся пробле то есть в рамках одного диска случайное обращение к диску оно всё равно остаётся и остаётся проблема рандомного и вообще в целом можно сказать что у нас были связаны обработкой и хранения данных внутри милита Это приводило к тому что мы в общем-то не могли дать хороший сложно было дать что-то надёжнее чем какой-то рейд и если например какая-то машина могла выйти потеряться на время пользователь это могло приводить к тому что пользователь мог потерять доступ к своему ящику на некоторое время и доступ к яку мог вернуться после того как машина возвращалась в строй и поэтому естественно надо было разделить обработку и хранение данных и это в общем-то основная идея второй версии милита и она в общем-то А как раз нужна для того чтобы это эта идея помогая сделать автоматический ф рассмотрим детальнее как мы разделили обработку и хранение данных поэтому немного вернёмся к нашим снапшота логам пока в частности к снапшота тут Всё достаточно просто можно за ме Snap - это такая большая штука xlog - это маленькая потому что там всего лишь изменения и snapshot он в общем-то и потому что заместо старого снапшота у нас появляется новый спт А старый спш при этом не изменяется и такой спш он Хорошо подходит для хранения во внешнем бло реже и тут всё просто тот Кто знаком кто кто видел предыдущие доклады моих коллег может сказать что знает что у нас для хранения BL ST есть внутренняя разработка которая называется zep такой L Stage который обеспечивает высоконагруженные и тут в общем-то Всё достаточно просто и нужно Что что-то такое же хорошее для Лога Но xlog в отличие от снапшота не то есть он постоянно меняется у нас добавляются новые записи Когда происходит компакт существующие Записи могут удаляться и при этом мы конечно же хотим xlog писать со скоростью последовательной записи на диск Вот но влита 1.0 для каждого ящика был свой отдельный илок И если бы мы взяли какое-то Готовое решение и в этом готовом решении в соответствии каждому ящику завели бы кото какую-то готовую его сущность таблицу спейс документ то скорее всего это привело бы к тому что мы некорректно эксплуатировали бы это Готовое решение потому что у нас количество ящиков десятки миллионов а поэтому мы тут как в том анекдоте концепция изменилась немного поменяли э концепцию Мы приняли решение сохранять А все логи одного Демона милита в один общий Лог тут важно понимать что только клоги только изменения и только только одного Демона милита и тот кто знаком с нашей компанией давно знает что для часто изменяющихся данных у нас хорошо подходит Тарантул Тарантул обеспечивает нам с движком MTX обеспечивает а скорость записи равной последовательной записи на диск и плюс мы можем Тарантул объединить в кластер что в общем-то даёт нам возможность Крос репликацию нашего Лога между дата-центра и что позволяет сделать автоматический failover В итоге основная идея второй версии милита она заключается в том что мы разделили обработку и хранение данных и у нас появился новый ж формат которого называ который мы называем экстаз где x - это общий префикс T - Это абревиатура от таран epta У нас есть таран У нас есть ПТА в зех лежат снапшоты И мы продолжаем илита работать с ящиком как с виртуальной баз данных и которые зашивать рассмотрим чуть детальнее как это происходит внутри лита внутри лита У нас есть ру кэш где каждая запись такого кэша состоит из данных почтового индекса над этим над этими данными почтового индекса есть некий контекст XL db мы его чуть попозже рассмотрим есть контекст подключения к внешнему реджу и вот тут вот ящики они обрабатываются все изолированно у нас полностью исключено взаимное влияние ящиков друг на друга потому что запросы обрабатываются в X db изолированы на данными которые лежат в виде отдельной записи вру кш это позволяет исключить взаимное влияние как я сказал Но помимо этого это ещё хорошо позволяет распараллелить обработку запросов по разным ящикам и исключить различные взаимные блокировки и в общем случае когда нам приходит какой-то запрос которого у нас нет в индексе в ру кше Нам необходимо дропнуть один или несколько ящиков в примере Например у нас это ру лозер мы его дроп и далее создаём новую запись Руше в которую необходимо поднять наш снапшоты и логи из стод exas и прежде чем перейдём к тому как это происходит рассмотрим чуть детальнее Что из себя представляет xlock db xlock db - это такой наш внутренний фреймворк для разработки встраиваемых баз данных Он позволяет поднять данные в памяти и проиндексировать эти данные по различным полям то есть построить несколько индексов и при этом xlog db он не привязан к формату хранимых данных и все методы сериализации индексации и так далее они в общем-то должны передаться э должны быть реализованы прикладным разработчиком и передаться при инициализации в виде какого-то конфига колков а при инициализации и xlog db такая такой паттерн он конечно же немного усложнённые разработчика который использует такую библиотеку но в то же время он даёт хорошую гибкость взамен рассмотрим чу дене Как происходит подъём такого ящика в кэш используя db изначально нам приходит некий запрос от клиента по определённому имейлу и нам нужно получить XL и snapshot мы по этому имейлу идём в Тарантул и получаем из Тарантула наш ило вместе с логом мы получаем ещё идентификатор снапшота который актуален для этого Лога после этого получив идентификатор снапшота Мы идм в нашу зеп из зеп скачиваем наш бло который представляет собой с ите у нас на руках есть xlog и Snap и нам нужно эти данные зан в кше и их проиндексировать в общем-то это мы и делаем то есть когда у нас данные заренков про которые мы говори горили В начале мы эти тапы все в соответствии с нашей логикой применяем на наборе сырых структур получаем какие-то внутренние наши представления в виде а готовых структур после этого строим массивы указатели на эти структуры на каждый индекс который нам нужно построить мы строим отдельный массив указателей далее Мы из этого массивы указателей строим наши b+ деревья которые как раз используются для того чтобы производить запросы по выборке из нашего xlock db тут есть несколько оптимизаций во-первых мы изначально сортируем массив этих указателей в соответствии с логикой индекса а сортировка происходит многопоточность она пропускается потому что мы ещё и дополнительно В снапшоте храним информацию об отсортированной с логикой индекса и это нам отсортировано позволяет добавлять в листе наших b+ деревьев сразу по 2.000 записей что в общем-то ускоряет процесс построения БП деревьев и когда данные уже заренкова Мы всё-таки несмотря на это актуализируя наш кэш сходим в Тарантул посмотрим какие-то новые получим из ик слога новые записи которые нам неизвестны если они вдруг появились потому пото что в это время могла быть какая-то поклад письма или ещё что-то после того как мы получили потенциально новый xlog мы опять применим его на нашем кэше кэш актуализируется эту операцию назовём актуализацией кэша и после этого уже обработаем запрос клиента и вернём ему результат соответствии с тем используя тот индекс который нужен клиенту запрос на запись чуть посложнее посложнее в том плане что тут произойдёт актуализация и при этом ещё произойдёт новая запись в tarantul потому что нам нужно сохранить новый запись влоге мы сохраним новую запись влоге и далее пойдём добавим её также в кэш Если вдруг у нас почему-то не получилось Добавить запись в кэш то мы просто к клиенту вернём ошибку о том что его запрос не обработан А этот ящик из кэша дроп и на следующем запросе Когда придёт следующий запрос Мы также возьмём X snot и опять поднимем этот ящик в кше в целом тут можно заметить что запросы на чтение и запись Когда уже ящик поднят и когда он уже в ше они достаточно простые основная сложность Т заключается в том чтобы быстро поднять ящик в память и быстро развернуть его и быстро эти данные проиндексировать у нас на самом деле на эту тему есть очень много оптимизаций которые к сожалению не вошли в этот доклад мы можем обсудить их после доклада отдельно Вот но в итоге топология развертывания нашего лита 2 сегодня имеет следующую схему У нас есть кластер Тарантула в котором лежат наши логи и есть два кластера Зета в пте у нас лежат Наши снапшоты в зеп у нас лежат наши это как раз результат наших оптимизаций там у нас лежат редко используемые редко изменяемые поля индексных записей и в общем-то В итоге эта реализация на текущий момент нас полностью удовлетворяет потому что мы как раз удовлетворяем Тем требованиям которые были выдвинуты изначально пеем хорошую целостность и персистентность и плюс у нас достаточно очень быстрые выборки из индексных данных ну и переходя к заключительной части можно хотелось бы отметить о том как работат милита на продакшене сегодня у нас каждый ИС милита может поднимать вот таких вот 50 сся до 50.000 ящиков в виде виртуальных баз данных и у нас запущено 126 демонов э на 201 сервере и и что интересно мы ширу менее 10% данных которые мы храним в нашей зете на самом деле Там цифра сильно меньше она даже не 10 а по-моему п не 5 67% и каширу такой Малый объём данных Мы обеспечиваем наш кэш Хит в 99,5 про это очень хороший показатель и о чём он говорит он говорит о том что в подавляющем большинстве случаев запросы всех наших клиентов обрабатываются очень быстро и они обрабатываются на данных которые лежат в памяти Ну и оглядываясь на эволюцию милита можно сказать что до 2012 года за счёт того что мы не могли утилизировать свободные пространство на диски и упирались в апсы мы вынуждены были поднимать до 3000 серверов для хранения почтового индекса после 2012 года и внедрени милита 1.0 количество таких серверов уменьшилось в два раза но оно уменьшилось два раза не только за сч внедрения 1.0 там Мы также перенесли хранение очей в отдельное облако и хранение тел писем с их заголовками в отдельное облако и поэтому в общем-то основной прорыв произошёл там в районе 2019-2020 года количество серверов сократилось в пять раз каждый инс милита научился обрабатывать в восемь раз больше ящиков чем он обрабатывал ранее и это естественно сказалось очень положительно наших финансовых результатах Мы тут с коллегами посчитали немного и вроде бы нигде не ошиблись и М это в общем-то выливается в порядка э-э на хостинге только мы стали экономнее на 600.000 долларов в год это значительная сумма А И это только на хостинге помимо этого есть ещё экономия на эксплуатации и поддержки естественно там наверное сумма соизмерима но важнее то что при этом мы обеспечили очень хорош очень хорошую отказоустойчивость автоматическую отказ устойчивость мы можем мы проводим ежеквартально у нас во-первых пять дата-центров и мы ежеквартально Като учения и отключение одного из дата-центров оно в общем-то никак не влияет на наш можно отключить и два дата-центра в этом случае у нас часть ящиков они перейдут в редон режим и плюс работая с ящиком как с контейнером Мы очень сильно облегчили администрирование нашей системы ящик очень легко тся восстанавливается переносится между какими-то шарми и так далее Ну естно как явно используем ресу бы попробовать поделиться вот таким вот выводом Если у вас есть какой-то паттерн нагрузки такой что у вас много каких-то родительских сущностей например их миллионы и при этом в каждой родительской сущности есть какие-то дочерние записи их тоже может быть миллион это могут быть профили пользователя какие-то объекты телеметрии какие-то ещё не знаю пользователи и вот если у вас нет сквозных запросов по всем этим данным и итоговый порядок получается очень большой то возможно вам тоже подойдёт вот этот вот паттерн когда вы родительскую сущность представляете в виде одной виртуальной баз данных и прямо на ней делаете запросы по этим данным которые вам нужны у нас на самом деле на эту тему Есть ещё одна Success ST мы буквально недавно закончили миграцию всех пользователей проекта обла mail.ru это который аналог Яндекс диска и Google диска на литы и теперь там тоже Мы работаем очень надёжно очень быстро и при этом сэкономили достаточное количество ресурсов в общем-то на этом и вс Спасибо за внимание готов ответить на ваши вопросы Рустем спасибо очень содержательный плотный доклад Спасибо ваши вопросы у нас есть приз Так что есть за что побороться Ну давайте вот начнём со второго ряда Спасибо большое за доклад очень интересно вот вопрос такой технический А какая примерно получается Какой размер у вот этого снапшота который нужно загружать Ну он зависит от количество писем в ящике он если в ящике там миллионы писем то он может быть там сотни мегабайт Вот но в целом в любом случае подъём происходит достаточно быстро но тут сейчас этот вопрос он очень хороший но тут интересней вопрос в том Сколько занимает каждая индексная запись если включить все оптимизации при расположении в памяти там по-моему в почте сейчас 45-50 байт каждая запись занимает потому что она очень хорошо оптимизирована спасибо Вот Нет давайте так сначала Привет Спасибо за доклад два вопроса Первый Как вы оптимизируется xlock то есть кажется что этого нельзя делать на записи это какой-то багра процесс Да тоже хороший вопрос Спасибо на самом деле э прихо компакт происходит Ну в результате различных событие это может происходить периодически или в зависимости от размера и Лога и он происходит без блокировки то есть приходит какой-то асинхронный процесс и производит компакт подменяет там идентификатор известного последнего снапшота и в следующий раз когда мы поднимаем наш илок мы получаем актуальный фитор и соответственно скачиваем этот спш из ПТА Окей И второй вопрос небольшой Вот вы показали цифры что с 10% кша Да вы получаете ши в 99% Угу Но это сколько из них реально активные ящики То есть если у вас сто 10% кша но при этом только 20% ящиков реально активны то это кажется чуть-чуть менее А это очень хороший вопрос на самом деле Да вот в том плане что у нас были немножко цифры на этих слайдах Но наш PR отдел запретил говорить про количество ящиков Вот и поэтому наверное я точно не смогу ответить сколько активных Яков сколько неактивных это какая-то там это поэтому мы привели характеристику это 50 млн юзеров это наша ежемесячная аудитория наш Мау вот может быть мы как-то в оффлайн в кулуарах можем обсудить детали спасибо спасибо Вот здесь Да вот от таких У меня два вопроса у вас параллельный у вас параллельный доступ к вот этому поде то есть ощущение такое что вы сказали что есть сшт есть Лок и он типа ещё пере проверяет актуализирует что у него там Лок может быть обновился то есть вот это состояние in Memory вот этого xlog плюс снапшота Оно ещё на нескольких нода может жить одновременно Да оно может быть на нескольких нода но тут э так как оно может жить на нескольких нода мы стараемся этого избегать наш фловер такой что если один раз ящик Закирова на одной ноде то он не должен Каширова на другой ноде мы всегда будем ходить ходить ровно в эту ноду за этим ящиком Но если эта нода упала Мы в следующий раз закрашиваем этот ящик в другой ноде и соответственно будем всегда хранить толь эту оставшуюся ноду тако широва Поня а ещ тако вот а у вас врали в кам формате хранится Ну в формате таран там же тоже там есть какой-то номер транзакции данные ну там типа просто в бинарном формате просто значение какое-то Да лежит или как Да просто в бинарном формате а добавляется там типа в конец нет у нас просто у нас есть какая-то запись появляется новая запись добавляется амы при добавлении Лога мы не добавляем в таран какой-то е какую-то ещё запись мы просто добавляем новый кейс новым UE вот а ещё последнее Вот сортированный вы сказали вы сортирует в памяти там вот деревья строите б деревья это получается в памяти Да конечно это же весь db в памяти индексы его тоже в памяти это тоже тратится на это память да этом ве смысл собственно Понятно Я понимаю а сшт сортированный в снапшоте есть информация о том как отсортированы записи почтовом индексе для каждого поля которое мы индексирует дам есть индекс по фолдер индекс по ящикам Ну по ящику там индекс не нужен потому что Нашот соответствует одному ящику то соответственно у нас есть информация о том в каком порядке У нас должен быть там вот вот эта запись в нашем индексе по тредуэй базе там типа в SQL базе Нет это это наш snaps это сугубо наш внутре поло не я имею в виду поблочно в него не ходите Нет поблочно мы его не ходим да мы его целиком поднимаем Поня Всё спасибо Спасибо Давайте дальше вот там я вижу руку ещё где и вот тут вот сюда и туда да спасибо большое за интересный доклад у меня такой вопрос возможно покажется что кейс очень частный но представим что любая система у нас падает нас могут рубанул президент в одной из стран их перебили Сколько времени замт всего что в базе восстановление из кэша ящиков при такой схеме кэша из Лога наверно или из Лога да то Ну как сколько займёт времени Подъём в память всех этих баз и так далее или пришли к нам Допустим спецо или кто-то сказал что ребят вот этот вот ящик Вы удалили А вчера на него пришло письмо убрали мы его потому что у него активности не было ящики поднимаются из кша как бы это нормально зависит от размера ящика если ящик очень большой то он может подниматься Достаточно долго Сколько времени как раз занимает ресурсы Ну то есть если всё поднимать вот вообще всё Если всё поднимать если например в ящике там 10 млн записи то это может занимать там до 5 минут до 7 минут то есть это на один ящик или на один ящик на один ящик вот если ящик очень большой и в нём очень много записей если мы его весь полностью поднимаем А все ящики Ну там наверное надо смотреть какую-то статистику по количеству ящиков и так далее тут Наверное сложно ответить тут как бы надо как-то посчитать наверное цифру и сходу там такой эксперимент мы не делали и Наверно я затрудняюсь точный ответ дать сколько времени займёт подъём всех снапшоте ну плюс надо понимать что количество э ящиков которые нас кш оно всё-таки меньше чем количество существующих ящиков которые есть в системе Ну сильно меньше в продолжение этого вопроса вот допустим мы 5 минут восстанавливаем этот ящик на этот ящик приходит новое письмо оно сразу появится или после того как будет проведена мы во восстанавливаем ящик у нас ящик будет заблокирован и у нас информация о добавлении в xlog она в общем-то где-то в очереди простоит и когда мы всё поним снимем блокировку у нас это добавится и мы актуализируя получим эту запись Угу спасибо большое спасибо Вот наверх отнесите пожалуйста вот сейчас вы добрый день спасибо за доклад у меня такой вопрос вот есть Ну грубо говоря групповые рассылки где много получателей а вы как-то оптимизируется такие э письма у себя Если вы их условно все храните и планируете ли ели много получателей То есть если хоти Спам фильтр оптимизируем Не ну если вы понимаете о чём я не то чтобы супер много Ну там ну а кстати это слайд пропал последний Опять да для презентации Если кто-то вдруг захотел Спасибо большое Да спасибо большое за вопрос Я извиняюсь что отвлеклись А когда у нас много получателей это будет много добавлений записей в xlog соответственно ти чтобы Добавить запись вло нам в любом случае надо будет поднять Ну мы просто добавим эти записи в xlog как бы тут каких-то оптимизаций нет Когда нам нужно сделать чтение как бы нам нужно будет сделать чтение Ну то есть у нас там сотню ящиков они у вас все хранятся и продублируйте это про хранение именно писем вот ваш вопрос как будто бы про то как происходит поклад письма То есть это ведь не то как мы поднимем ящик и покажем его пользователю если если письмо приходит там несколько раз нескольким пользователи но оно продублируйте маленькие и А ну по сравнению с атами но вот если вы прислали какой-то большой ач там на несколько гигабайт и сделали это там несколько раз в нескольких многих письмах то вот там где дупликация произойдёт если вопрос дупли с атам понятно Да вот ели дупликация в самих письмах Ну в тело в самих письмах насколько я знаю Нет это под система хранения писем Спасибо наверху и вот сюда передайте Здравствуйте спасибо большое за доклад А вот в этом во всём процессе мне интересен как построена пагинация то есть а она вот как раз в рамках оптимизации но зачастую мы просто поднимаем весь в память но когда Да прямо вообще все записи учитывая что одна запись там 45 байт - это достаточно мало а ну вы сейчас под записью Говорите именно индекс ную просто да индекс ную запись да да да да то есть Нам нужно Мы ведь когда работаем в графическом интерфейсе мы видим ровно ту информацию которую нам выдаёт почтовый индекс А И вот второй вопрос А Вы сказали что у вас Ну ящики Да там на 90 там 5% кэш у вас всегда в памяти 99,5 Ну да 99,5 а а есть какая Ну то есть есть так такой механизм который допустим Ну помечает ящик Как горячий Да часто используемый и допустим ящики там в которые там кто-то там заходит раз в полгода и вы там эти ящики помечается что их не нужно там подгружать Да конечно же вот у нас же есть ру кэш Когда у нас ящик лежит милита если допустим у нас есть а какие-то активные ящики они приходят их нужно поднять в кэш мы понимаем что полгода кто-то в предыдущий ящик не заходил мы его просто дропнется там много места есть какие-то там хит В смысле дроп из кэша из кэша Да ну Ну да совсем мы его никак Мало ли Все спасибо не хотелось бы да давайте наверное последний Здравствуйте спасибо за доклад А скажите пожалуйста Существует ли какая-то цифра какое-то количество записей в этом Икс слоге которое бы его способно было Ну положить Не знаю там чтобы не влез в память а там на самом деле много нюансов происходит в процессе эксплуатации то есть иногда какие-то может быть массовые миграции или ещё какие-то переносы вот были в облаках у нас и это определяется тем сколько есть отведено памяти под Тарантул это как бы не определяется какой-то производительностью нашего кода который мы делали это определяет сколько есть памяти в тарантули сколько мо можем положить естественно когда у нас и очень большой компакт может занимать очень много времени тут точными цифрами оперировать наверное будет трудновато Ага спасибо А скажите ещё используются ли как-то вот эти индексы для поиска писем Ну полнотекстового наме полнотекстовый поиск Нет это отдельная подсистема поиска ном быст пи по имени по-моему там есть какой-то у нас хитрая индексация по сабк или по получателю или отправителю где мы используем две или три буквы и принимаем решение о том как бы нужно нам полно текстовый поиск по письму сделать или быструю выборку можем сделать из индекса спасибо Ну теперь знаем как положить ного можем пожелать вам удачи давайте пожелаем Здравствуйте спасибо за доклад подскажите пожалуйста а как реализована у вас вот система доступности то есть что я имею в виду вот у вас сервера москита милита Извините Вот соответственно Каким образом они друг с другом общаются говорят что они доступны и что например конкретный ящик Вот вы говорили ходит в конкретный сервер да Э спасибо за вопрос тоже хороший вопрос вот эта логика она определяется у нас клиентом и клиент получая ящик он грубо говоря знает идентификатор шарда в котором в который ему нужно идти Но помимо этого у него ещё есть два запасных шарда в которые он может ходить и если этот шарт в который нужно сейчас идти недоступен он проверяет два оставшихся и идёт в два оставшихся То есть это логика которая разруливает на стороне клиента а сами миколи друг про друга ничего не знают то есть файвер на стороне клиента файвер на стороне клиента Но может показаться что мы в браузер добавили какого-то клиента или ещё что-то нет не так конечно нашим клиентом является наш наши ашки их там больше 30 штук и вот эти а мы им даём грубо говоря какое-то СДК и там всё это реализовано и это всё делается достаточно прозрачно то есть не нужно там клиенту каждому писать какой-то свой фр это всё в виде какого-то СДК клиентской библиотеки отдаётся клиентам и соответствующие наши коллеги из других команд их просто используют Кому нужен там счётчик на главный кому-то нужно там вбить что-то показать и так далее Понятно спасибо пришло время выбрать лучший вопрос Ох это очень трудно сделать потому что все вопросы э забываются Я бы вот наверное молодой человек спросил про Вот вы да про размер снапшота как раз там да Руслан и А у нас два раза приза А да точно да точно забыл на же один у Я хотел просто ещё воду подарить но можно это сделать Рустем И тебе тоже Спасибо И призы памятные подарки от организаторов хороший доклад У спасибо большое давай удачи всем спасибо большое M"
}