{
  "video_id": "-HSpk9geqEM",
  "channel": "HighLoadChannel",
  "title": "Как сделать платформу удобной, или Авито PaaS спустя 5 лет / Александр Лукьянченко (Авито)",
  "views": 756,
  "duration": 2804,
  "published": "2024-10-29T02:38:59-07:00",
  "text": "Александр лук Янченко Давайте поприветствуем да Всем привет Меня зовут Александр ченко Я руковожу платформой мата и сегодня мы как раз поговорим про платформу вот начнём сначала с таких общих концептов про PL engineering devs Да посмотрим на проблематику затем будет основная такая контентная часть мы будем смотреть различные хронологически Что именно мы проходили какие концепты мы собственно из этого вынесли Да какие идеи для построения платформ вот внутри посмотрим на такой стандартный путь Custom jy Map внутри платформы глазами пользователей немножко про архитектуру И на самом деле Вот в конце у нас будет такой супер слайд Значит на котором мы все эти концепты са аккумулирует удобные масштабируемые класны п формы вот начина с платформ инжениринг И devops на самом деле многие компании команды за адопте devops подход в котором продуктовые команды Как полностью владеют жизненным циклом разработки там эксплуатации релизный цикл и так далее при этом зачастую есть инфраструктурная команда которая всё равно там реализует какой-то общий слой Да тоненький это может быть например там тот же кубернетес стабильный со всеми фичами которые она предоставляет продуктовым командам Вот и здесь как бы есть огромный плюс что идёт владение всем жизненным циклом разработки нет каких-то неожиданностей блокировок о инфраструктур ную команду Вот и Но с другой стороны есть такой плюс-минус что продуктовые команды должны владеть Ну больше экспертизой Да должны быть какие-то навыки дополнительные Вот и мы в таком вот режиме шли где-то примерно 2 года с пятнадцатого по семнадцатый э когда мы предоставляли такой довольно низкоуровневый Интерфейс Да какие-то компоненты в продукт Вот и столкнулись с проблемой что на самом деле этот подход В общем случае не очень скели из-за того что продуктовые команды Ну всё равно в них в конечном счёте не хватает какой-то экспертизы Да чтобы её полноценно закрыть самостоятельно и как следствие она начинает задавать какие-то дополнительные вопросы нагружать Саппорт инфраструктуры платформы да Для того чтобы решать свои проблемы и задачи и это та точка в которой мы перестаём скели да здесь начинаются проблемы и вот где-то как раз в семнадцатом году мы сделали такой большой пиво когда мы сказали а давайте мы будем делать не просто крутой слой там инфраструктуры платформы А сделаем её в таком в формате self-service platform то есть некоторой такой системе скажем самообслуживания Вот и будем предоставлять продукт только какие-то высокоуровневые интерфейса там cli утилиту графический интерфейс через который продукт Всё может сделать что им нужно на стыке с нефро вот звучит как нарисовать саву сейчас будем смотреть э что в конкретики значит в таком как я уже сказал хронологическом порядке да там от самого начала зарождения до текущего момента какие-то из вещей мы сами ещё находимся скажем так так в активной фазе и их э собственно разрабатываем делаем значит первая история начнём с банальной э темы которые как раз были сегодня на других докладах уже затрагивались это собственно зоопарк решений невозможность централизованного управления и раскатки Практик э В чём это состоит история что когда мы отдаём возможности Да и какое-то владение вот релизный циклом эксплуатации в продукт продуктовые команды Ну начинают внедрять какие-то свои технологии делают какую-то кастом сиину вот в итоге мы как бы здесь не можем контролировать значит какие-то внедрять общие практики и так далее делать Вот с другой стороны продукт в то же время тратит как раз на это время и вместо того чтобы пилить конечную бизнес функциональность кучу времени тратят на какие-то инфе штуки которые безумно интересные но не помогают бизнесу с точки зрения конечных задач Вот и какие здесь примеры проблем есть стандартные Да базовые которые с которыми все сталкиваются То есть например Это мы раздаём всем там прекрасные сборки шаблоны с помощью которых можно быстро выкатывать там сервисы что-то делать но в итоге из-за того что можно кастомизировать значит продуктовых командах потом мы не можем отдавать обновление то же самое всякие проверки качества хотим например внедрить там проверку кодового покрытия там не можем вот реальные кейсы которые у нас были там хотели раскатывать ещё там довольно скажем так несколько лет назад да во времена там Конный деплоймент принудительно чтобы нельзя было так взять и жахнуть на 100% там какой-нибудь сервис продакшн и приложить важный бизнес-процесс тоже нельзя потому что есть там какие-то отключаемый проверки Да там ну как бы продукт владеет всей этой механизмом и стандартные истории про экзотические базы данных фреймворки и прочее вот и всё это естественно в итоге приводит к тому что ещё и время тратится не на бизнес-задачи да то о чём мы уже говорили Вот какие здесь два подхода первых мы используем первый довольно стандартный Да это собственно унификация при этом здесь унификации речь идёт не про пайплайн там C Да а вообще в целом Вот про всё взаимодействие с платформой мы унифицированы вот использовали именно одни и те же рельсы не было такого что где-то там какая-то комна понятно что она всегда есть есть кейс это тот самый 1% который Нормально мы его выносим за скобки Он вне платформы Вот и тех радар К сожалению тех радар когда он выполнен в виде прекрасной красивой там картинки и таблички не очень работает поэтому второе здесь вторая механика - это встраивание тех радара прямо внутрь платформы то есть делаем так что абсолютно нельзя никак например взять какую-то экзотическую базу данных за использовать там язык другой притянуть и так далее Вот и с другой стороны с точки зрения скорости разработки Э мы Первое - это следуем истории такой как сокрытие сложности под простым интерфейсом Что это значит вот любую технологию которую мы берём это может быть и тот же кубернетес да такая-то большая технология это может быть какая-то маленькая там не знаю волт какие-то ещё механики там работы с безопасностью и так далее мы их просто анализируем А что именно пользователю нужно Да и вытаскиваем либо вообще ничего не вытаскиваем потому что в большинстве случаев там Технологии Они вообще могут быть скрыты под капотом либо вытаскиваем в каком-то простом интерфейсе там несколько опций Вот и при этом стараемся провоцировать правильную архитектуру то есть прямо на уровне платформы продумывать Как сделать так чтобы пользователь платформы сразу же заведомо скорее всего построил какое-то правильное решение поверх неё вот во что это в реальности превращается первая История - это конфигурирование как одна из точек соприкосновения как бы платформы и бизнес приложений то есть мы говорим что О'кей описываете именно то что специфично для вашего сервиса например там база данных на чём Он написан там сколько реплик по дефолту нужно и так далее и дальше уже на уровне там C сборок локального окружения этот файл процеси с помощью сервиса превращается в нечеловеческие там многотонные Ям с которыми уже работает именно машина да система которая там деплоить его в кубы значит менедж это всё и так далее вот при этом пользователь как бы от него скрывается сложность там вот этих всех низкоуровневых интерфейсов вот если говорить про этот подход здесь есть сейчас уже опенсорс нае решение называется ell которая позволяет как раз такую механику сделать То есть здесь есть важный как бы момент такого ПТФ and User разделения что мы как платформа формируем А какие именно потребности Да понима именно нужно значит конечным пользователям платформы и прямо задаём что например можно в нашей платформе использовать наме сервисы там на Гош писать на питоне использовать базу данных тамс пос и так далее и уже конечные платформенные пользователи они просто описывают это в простом Application интерфейсе а контроллер как например Cu она берёт это преобразует в конечные там большие многотонные там нечеловеческие деплой сервисы Игрес в которых всё учтено и собственно в ку вот значит Какие ещё вещи это готовые компоненты естественно там например микросервисная архитектура она приносит с собой огромное количество проблем одна из которых - это сетевое взаимодействия Да в нём куча нюансов поэтому Например если рассматривать взаимодействие там сервис профайлы limits Каким образом они взаимодействуют мы сразу же даём механику контрактов Да которые описывают стороны клиента со стороны сервера Как именно эти сервисы будут взаимодействовать коды генерацию который учитывает там всякие CC брейкеры тайм-ауты и прочее если нужно асинхронное взаимодействие то мы даём не просто кафку да а оборачиваем её в готовый такой компонент с которым удобно легко работать в нём есть вся там телеметрия под капотом масштабирование горизонтальное и так далее если нужно там мощно как-то горизонтально помас и в рамках сервиса организовать паттерн очередей то там мы используем обёртку над пульсаром и так далее То есть мы от пользователя скрываем какую-то сложность под капотом вот в итоге здесь что важно что пользователь он вс ещё получает все необходимые возможности Да важно не сузить это до тестери в которой разрабатывает там инженер Да но Значит кастомизация она остаётся со стороны платформы то есть мы полностью её контролируем то есть человек если что-то вносит какое-то изменение Мы можем взять и централизованно внести какие-то изменения с нашей стороны вот и соответственно вот получаем этот прекрасный берм там вне какие-то политики новый софт обновляем платформу и так далее вот ну первая была такая довольно стандартная проблема идём к следующей уже такие более значит более зрелый уровень первое с чем сталкиваемся - это большое количество точек входа в платформу Что это значит Есть огромное количество всяких там фан дашбордов observability инструментов c там инструменты для перфоманс тестирования и так далее они все довольно разрозненные то есть есть куча разных точек входа о которых нужно знать естественно разработчик с ними каждый день работает и скорее всего подавляющее чем-то это число знает запомнил добавил закладки и так далее но всё равно то есть есть как бы такое м разнородность да большое количество каких-то инструментария Вот и связанный с этим момент который я думаю проходила каждая платформенная команда - это низки adoption вот вы делаете какую-то новую фичу Да новый опять же интерфейс и видите что он адопт медленно Да он там может adoption занимать там больше года да Ну просто потому что это ещё одна какая-то фича ещё какой-то интерфейс Вот и здесь Какие примеры Ну есть банальные вещи типа там создание сервиса где нужно там проклинай в какой-нибудь c там нажать на релиз особенно если он какой-то там кастомный да с какими-то параметрами вот э это простые вещи на самом деле с ними разработчики обычно справляются потому что это критичный путь Да разработки есть более сложные вещи Например если они стоят сбоку performance тесты нужно помнить как именно генерить там ленту запрос какой интерфейс её скормить и так далее была у нас история по внедрению подсчёта стабильности вот всех компонентов Да мы хотели точно получить такие сес level индикаторы Сколько именно мы там девяток у нас отдаёт каждый компонент соответственно это тоже новая фича это новый запрос на конфигурирование Да Каждый должен написать какие нефункциональные требования в его компоненте должны соблюдаться и затем из этого уже могут быть построены там какие-то числа статистика вычислена и опять же Это ещё дополнительный там интерфейс Вот соответственно adoption здесь идёт долго а что в итоге с одной стороны это потеря времени на вот этот вот переключение контекста и вообще в целом высокая когнитивная нагрузка Да при этом Со стороны платформы Она часто не замечается потому что мы настолько погружены в это да мы со всем этим работаем постоянно разрабатываем Мы не видим что инструментов и сервисов очень много и с другой стороны что как бы те важные шаги которые не стоят на критичному пути они просто банально теряются и то что мы хотим чтобы вас в компании там было ма в итоге Ну там не соблюдается не используется вот ну и для платформы adoption механика идёт медленно какой подход здесь используем мы да стараемся его делать вообще во всех частях жизненного цикла разработки это мы вот пользователя нашего пользователя платформы просто проводим по вот этому всему пайплайн разработки как единому потоку то есть делаем так чтобы у разработчика было вопроса А как именно нужно перейти там в следующий интерфейс Что именно там проделать С какими командами и так далее как это выглядит То есть если рассмотреть такой стандартный очень базовый путь там от от идеи да от проектирования создания сервиса проходя через разработку там тестирование выкладку в продакшн эксплуатацию выглядит это так то есть например идёт создание нового сервиса Да Мы вирм спрашиваем там что какой сервис нужен какой технологии дальше сразу же снабжаем ссылками там где репозиторий как перейти на там карточку сервиса и так далее То есть у разработчика нет вопросов Что там делать дальше сразу же есть линковка потом он перешёл у него есть готовая там структура он прыгнул У него сразу же описан контракт Он его изменил пошёл подключать зависимость Да и ссылка он прыгнул в интерфейс посмотрел Какая Какая именно ему нужна там связь какой сервис в доменной зоне перешёл посмотрел его ашку Нажал на кнопку скопировал соответственно команду вставил её в минал в визар выбрал какие именно rpc эндпоинты нужны структуры значит добавил зависимость то же самое с там базой данных например редисом и в итоге у него получилась готовая та самая конфигурация он руками её там не пишет соответственно там все ны dependency баз данных ему добавились при этом он здесь как бы нигде не думал а что делать дальше весь этот путь полностью как бы замкнут вот дальше он может пойти например на генерировать клиенты то же самое ему сразу же будет уже готовые там куски кода Да инициализация зависимости где не нужно указывать Как именно подключаться к этим базам как это сделать отказу устойчиво это автоматически инжектится есть там переменное окружение со стороны платформы которые это сразу же всё поддерживают вот при этом дальше после того как уже вся кодовая там база логика написана идёт фаза там тестирования выкатки также заходится в единую точку на дашборд на сервис нажимается кнопка выбирается Как именно нужно выкатить например говорим Не хочу ни о чём думать хочу значит автоматически чтобы это всё прокатилось система запускает там собирает сборку тестирование релизный цикл значит в автоматическом режиме канареечное выкатывает сервис проверяет нет ли там пяти соток тайм-аутов в итоге говорит Всё прекрасно всё выкатилось идти и заняться чем-то другим либо перейти на ссылку и например посмотреть низкоуровневое мапи на там кубернетес сущности как это всё выглядит и затем уже пере ти к фазе эксплуатации Например если что-то вдруг взрывается посмотреть какие именно были релизы в одной точке значит как там Какие сервисы возможно деградировали в этой окрестности перейти на него посмотреть там связи и так далее да все ссылки в одной точке вот что важно то есть какой здесь концепт Мы вырабатываем что на каждом этапе У нас есть линковка да со следующим нет каких-то разрывов и разработчикам в конечном счёте не нужно запоминать Как именно по системе ходить то есть всё это как бы так нативно а в таком customer journey Map складывается а Какая следующая проблема Когда у нас платформа становится довольно зрелой и все там шаги автоматизированы мы сталкиваемся с тем что у нас идёт некорректное использование инструментов И к сожалению да ну или там не знаю это такая такая Такова реальность что бы разработчик зачастую выбирает путь попроще То есть если можно что-то обойти сделать как бы иначе значит всегда будет выбран именно этот подход В чём это выражается Например у нас есть там такая я думаю довольно распространённая проблема там шина данных она даёт определённые гарантии Да мы её скрываем под коробочкой красивой удобной но как бы реальность такова что тот инструмент который используется под капотом кавка даёт для масштабируемости гарантии Там At least Once Delivery но например со стороны разработчиков это гарантия там не соблюдается идемпотентность там не проверяется да В итоге мы когда происходит деградация встречаем с проблемой А значит по поводу э среды исполнения Да мы уже почти 8 лет живём в продакшене с кубернетес и все знают что это супер динамичная среда Там постоянно переезжают какие-то поды инстансы сервисов как нагружают ноды и так далее но когда мы смотрим на реализацию сервисов видим что на самом деле крайне редко Они реально учитывают проблемы того что например кто-то может какой-то один из инстан сов взял там и начал например там немножко деградировать Да и например там у сервиса 100 инстан один деградирует это уже большая проблема вот это такой момент бывает архитектурные проблемы То есть например разработчик смотрит хочу с интегрироваться с какой-то другой доменной зоной видит удобно там уже есть база данных всё разложено и значит идёт из другого сервиса прямо напрямую в хранилище подключается получаем там жёсткую связанность невозможность затем Независимо разрабатывать сервисы там обратной совместимость API если это не как-то не контрактова то то тоже идут изменения о пишки скорее всего они там глубоко не проверяются на обратную совместимость В итоге получаем слом и так далее на самом деле такой список я уверен если посидеть можно написать его там больше сотни пунктов Вот и как бы проблема здесь Какая что на самом деле скорее всего системно да Со стороны даже там обучение Да увеличение экспертизы решить это невозможно поэтому здесь у нас получается четвёртый подход в котором мы следуем следующему принципу что платформа должна изначально строиться устойчивой к таким кейсам вообще к мисюра Да и проверяться АА Действительно ли она э выдерживает какие-то там либо странные поведения либо какое-то использование которое не предполагалось Вот и при этом каждая технология которая внедряется в платформу должна быть проверена что действительно ли какие-то её особенности не торчат наружу Да как вот типа at least on Delivery А что то есть в них не нужно разбираться разработчику иначе это ломается Вот какие здесь механики делаются на самом деле их довольно большое количество просто для примера Да как э что это в реальности например мы сейчас делаем вот проект там по ха инжинирингу то есть мы берём там если говорить про atlast on Delivery подмешивать там дубли да Для того чтобы можно было проверять э Действительно ли клиенты готовы да к таким дублированию А значит тоже кейсы например деградации по цпу нагрузки или там утилизации сети их нельзя проверить ну в стандартных условиях то есть эти деградации Мы также там на продакшене эмулировать сервер то он обратно совместим по ашке и так далее соответственно есть какие-то вещи которые можно заложить прямо в платформу то есть что мы автоматизируем если мы знаем что например проблема шумных соседей решаема да то мы там Ну например у нас есть шелер который Берт смотрит на выселяет каких-то особо шумных сервисов Да там сно и соответственно даёт лучший сервис с точки зрения ку Бернеса всякие механики и так далее Вот то есть в итоге мы соответственно готовы и устойчивы к использованию стороны продукта странному вот Какая следующая проблема это будет такой пример на самом деле шире нот на м можно смоделировать решение инцидентов Да мы видим что например пока происходит какая-то деградация Она довольно долго разматывается и решается и с другой стороны это как уже такая причина можно сказать это сложность навигации и поиска причин этой деградации то есть мы видим что разработчики конечные которые сами же разрабатывают эту систему довольно долго ищут А что же именно сломалось и здесь как бы проблема так в итоге формулируется что у нас с одной стороны есть огромное количество крутых там дашбордов всяких полезных метрик всякие логи трейсинг и так далее но как только происходит какой-то инцидент который не очевиден на самом деле у конечного разработчика пользователя платформы начинают бы разбегаться глаза Он не знает куда именно посмотреть чтобы быстро локализовать проблему Да потому что вот этих вот маркеров и источников огромное количество Вот и здесь мы приходим к пятому подходу что на самом деле любые какие-то действия особенно вот таких вот стрессовых ситуациях как починкой Да какой-то деградации мы стараемся реализовывать так чтобы просто проводить пользователя за ручку Да по какому-то пайплайн То есть если это какая-то механика с деградацией мы даём лда механизм в котором пользователь видит там набор сервисов кто сейчас деградирует отсортированный по ошибкам сразу же прыгает видит там отсортированный по эн поинтам где именно сейчас скорее всего проблема Затем в трейс и соответственно видит локализуется и как результат соответственно пользователь не думает в момент что именно ему делать дальше система его просто за ручку провела и пришла к решению этого там инцидента Какая следующая проблема когда платформа покрывает все основные потребности мы начинаем сталкиваться с проблемой того что становится типа слишком Удобно да Что называется то есть разработчики перестают задумываться Об использовании ресурсов то есть начинают их использовать неэффективно Да потреблять больше чем нужно и ещё есть история того что например то что происходит в ран тайме это больше как бы такой not My Job То есть если там что-то случилось значит это платформенные инженеры должны высадиться и решить проблему и здесь две вещи мы делаем первое - это мы стараемся все такие Механизмы как например там capacity менеджмент строить как раз жизней цикл разработки а с другой стороны всякие истории вида там стандартных деградации типа Out of memory Да если там падает сервис и Это скорее всего проблема реализации самого сервиса всякие приближения к порогу по перформанс например там лаги в шине данных очередях и так далее мы их пропущ пользователю Да чтобы пользователь это об этом узнавал Да не не проживая этот опыт там с нуля да для себя то есть мы заранее иму об этом рассказываем вот При этом если говорить например про capacity менеджмент встраиваются блоки которых прямо и в релизом цикле и там на карточке сервиса на каждодневной основе разработчик видит А сколько именно он там потребляет Какие тренды употребления если там есть какие-то аномальные выстрелы можно сделать Дрил посмотреть А в чём именно проблема вот Какая следующая проблема на самом деле заключительная и уже такая больше архитектурная э если посмотреть на платформу то по сути платформа - это такой же продукт как любой другой и имеет свою историю развития и в определённый момент мы начинаем видеть что на самом деле вот эта вот вся система это не какой-то тонкий набор там не знаю devops инструментов механик и так далее А это прямо Ну такая нормальная довольно большая программная система при этом видим что появляется клубок связей между разными доменным зонами инфраструктуры довольно широкими аэ если мы говорим про какие-то вот фичи которые торчат прямо напрямую к пользователю здесь появляется необходимость контрибьютор ней точки Да там вглубь инфраструктуры какие-то низкоуровневые механизмы Вот и довольно жёсткая связанность То есть например команда которая владеет там пайплайн там не знаю создани баз данных должна Ну как бы вкопать и ВГ внутрь платформы и значит законтрил разных частей всякие uui пайплайн прочее вот и есть ещё такой Побочный эффект который на самом деле бьёт уже по пользователям что когда они приходят в сапор и спрашивают А что у меня здесь не так здесь непонятно кто конкретно отвечает да за эту зону потому что здесь есть и там одной платформ команды зон и там другой и так далее вот поэтому здесь мы приходим к седьмому подходу значит который Я рекомендую рассмотреть Если вы ещё находитесь там на фазе значит либо проектирования либо уже активной разработки платформы это сразу же изначально проектировать так чтобы платформа была чётко разбита на слои Да они обычно всегда есть Сейчас мы посмотрим пример и делать так чтобы платформенная команда полноценно владела своим симом то есть не было такого что например за интерфейс ную часть отвечает одна команда за какие-то там низкоуровневые штуки Да там реализацию отвечает другая Да это будет поломанный Experience в конечном счёте например про слои платформы то есть мы выделяем там слой инфраструктуры инструментов там для разработки релизный цикл там и эксплуатация с точки зрения эксплуатации уже конечной на пользовательской там всякие инструменты обсер ability и так далее при этом на примере инфраструктуры то есть грубо говоря формируется такой кубик и каждый инструмент соответственно каждая вот эта зона которая прорабатывается на этом слое она не торчит следующему с помощью каких-то низко уровневых механик она как бы как такой кубик предоставляет высокоуровневый API и все сложности скрывает под собой при этом пользователи конечные вот в случае например с Ирой они не взаимодействуют напрямую А какие-то сложности эксплуатации Как раз на уровне вот этой вот зоны инкапсулированный соседей шифрование трафика сбор телеметрии Надёжность взаимодействия и так далее То есть это всё инкапсулировать на общую архитектуру то во-первых она слоистая и во-вторых есть всегда ключевые Точки которые всё равно придётся добавлять Ну какую-то функциональность от разных платформ нах команд поэтому здесь сразу же надо предусматривать истории про плаги нези емо это плагин зру емо входных точек там дашборд UI API и так далее там релиз Наго цикла и прочего То есть у нас это довольно такая острая большая проблема возникла которую мы как раз сейчас только приходим к тому чтобы к вот этой значит архитектуре а в итоге подойти и как раз эту проблему порешать Вот давайте подведём итоги и булет на Да вот эти все концепты посмотрим значит что первое первое - это мы унифицированный цикл то есть делаем так чтобы не было какой-то какого-то зоопарка и весь тех радар встраиваем прямо сразу же в платформу второе - это скрываем сложность делаем так чтобы какие-то инструменты не торчали напрямую пользователю а были скрыты за каким-то простым там интерфейсом так далее соответственно весь жизней к чтобы он был без разрывов такой стройный и проводим пользователя за ручку по нему чтобы не было каких-то вопросов Что делать дальше какие-то там пропадали важные компоненты этапы сразу же строим платформу готовый к мизу то есть тому что какие-то вещи могут быть не учтены со стороны уже там чит разработчиков продукта и финальное действия которые мы ожидаем от разработчиков соответственно их встраиваем сразу же повседневные инструменты то есть делаем так чтобы не было возможности что-то не использовать если мы точно хотим чтобы это была Маст механика Вот на этом всё спасибо и Давайте вопрос да Александр Большое спасибо за клад Что рассказал и показал много вашей внутренней кухни Я думаю сейчас будет достаточно много вопросов и нач вот непосредственно с того дальнего угла Да здравствуйте Спасибо большое за доклад Я честно говоря может прослушал извиняюсь не услышал как вы боретесь с апм то есть допустим у меня пог на сервисе тут вдруг я захотел кассандру по каким-то определённым техническим требованиям Что вы делаете Как вы её втягивает или оставляете нату команде какое решение у вас Спасибо Да если мы со стороны платформы принимаем решение перейти там с по на Касан и хотим чтобы посо больше не было да то здесь есть вопервых мотивация понимание зачем мы это делаем и соответственно видение как мы должны эту технологию внедрять соответственно если это а значит технология которую можно максимально значит адаптировать и сделать так чтобы мы на неё безболезненно переходили Да всей компанией А значит со стороны платформы мы стараемся делать так то есть так чтобы это продукт э там минимально затрагивала вот если это механика Ну например постгрес на кассандру которая меняет полностью схему взаимодействия Да там бизнес логику сервисов и так далее во-первых я бы сказал нужно хорошо подумать нужно ли это да точно точно ли мы хотим туда идти если мы принимаем такое решение ну здесь на самом деле есть Ну стандартный подход у нас который называется Там техпроект технические которые мы просто запускаем всю компанию и соответственно раскатываем это ну скажем так либо в принудительном режиме либо даём какие-то бенефиты возможности там для лёгкой миграции Вот это в случае если невозможно Это решить со стороны платформы если мы говорим проп каких-то механик которые мы хотим видеть скажем так в стандартном реном цикле цикле разработки мы стараемся их складывать вот на вот этом критично пути чтобы пользователь Просто ну в идеале не мог не заметить да либо не использовать какой-то компонент который мы считаем обязательным вот две стороны дате Вопрос вот тут первый ряд слева другую но так или иначе столкнулись с такой штукой что вот у наших пользователей у них есть потребность некую кастомную нашу платформу то вся э кастом начинает ломаться То есть когда пользователи работают через API всё хорошо А когда пользователи начинают вокруг API что-то делать то всё это начинает разваливаться как только мы выкатываем какой-нибудь апдейт как вы с такой практикой вообще работаете тут вопрос что мы поднимаем подм то есть Бао Какая механика то есть конечный пользователь платформы по-хорошему Вот как раз не должен знать каких-то особенностях реализации Поэтому если это действительно честный API Да вот высокоуровневый фасад который торчит со стороны платформы не должно быть такого что мы что-то меняем внутри и это ломает пользователей это ключевой концепт это сложно да то есть есть какие-то вещи которые ну нужно это закладывать на этапе проектирования и у нас в том числе есть даже сейчас где мы например какие-то там манипуляции не можем так легко провести Вот Но если е это учитывать на этапе проектирования и действительно Отдавать вот только вот этот вот высокий уровень то ну проблемы нет то есть мы полностью это в стороны платформы меняем пользователи бесшовное этим начинают пользоваться и всё так вот вопрос Привет Спасибо за доклад ради компания учитывая вот то что ты показывал на квадратике за платформой скрывается там целый зоопарк и по поводу кейсов от пользователей как Вы готовитесь ко всем этим как руте вобще смысл покрывать все эти кейсы или там в каких-то моментах можно залить эту фичу и пусть пользователи тестируют Да ну это на самом деле одна из самых сложных историй Потому здесь есть два аспекта первый Аспект что во-первых эти мис юзы сложно вскрывать то есть мы можем не знать что такое миз может быть Вот поэтому это вопрос ресерч и скорее всего даже В некоторых случаях там каких-то инцидентов да которые произошли и вот мы значит их уже там раскручиваем решаем Вот Но значит это скажем так опыт который мы прожили мы сейчас понимаем что А значит вот в как раз ключевых компонентах особенно которые влияют на стабильность системы всей мы Ну вносим какие-то дополнительные механики То есть где-то это CS Инжиниринг который специально расшатывать и проверяет там что система устойчиво где-то это просто на уровне А ну например какой-то валидации да это контракты какие-то архитектурные паттерны там не даём например ходить там в одно хранилище нескольким сервисам и так далее То есть э какой-то такой подход но это всё про проектирование и подходы Да эти концепты И к сожалению нельзя разложить в технику То есть это всё про проработку так вопрос Ты главно ещё вопрос запоминайте лучше выбирать Да да Александр Спасибо за доклад у вас выглядит очень платформа зрелой и ну уже такой достаточно мощной Скажите думали ли вы над распространением её может быть что-то пососи может быть каким-то образом ещ поделиться Да спасибо за вопрос мы сейчас так э постепенно выводим решение как раз то что вы там на прошлом докладе про эти мысли тоже рассказывали То есть у нас наша внутренняя платформа сейчас как такой базовый рельсы Да общие выводятся как внешний продукт можно в кулуарно беседе об этом подробнее поговорить Вот Но в общем такое движение есть так вопрос сейчас отсюда Александр Большое спасибо за доклад Скажи есть два вопроса на самом деле первый это про капасити и что происходит с командами когда они там не доули зру те ресурсы которые э собственно начинают потреблять Ну то есть там есть ли какая-то история с с квотами Найс либо на ресурсы и второй момент - это про технический авторитаризм То есть как это происходит может быть r&d или cdf по тем сервисам которые вы предоставляете в и их настройкам потому что там я видел на слайде был дис там было довольно мало конфигураций но например по кафке Может ли потребитель у вас настроить размер очереди Угу Да по первому вопросу значит очень хороший про capacity менеджмент Да здесь есть естественно квотирование оно не нас Space не нас Space на это не техническая механика это грубо говоря Значит такой ну более высокоуровневый процесс в котором там порк структуре настраивается кто сколько там может потреблять Да кто сколько соответственно заказал и имеет железо и дальше если это какие-то пороги нарушаются то есть софт квоты Да которые там например на релизом цикле показывают что вот там оно превышено вот таким образом оно как бы контролируется Да и есть периоды обновления этих квот Да постоянные периодич вот плюс есть система такого Аля внутреннего биллинга которая тоже показывает вот эти вот ну грубо говоря потребления Да кто сколько по разным там разрезам потребляет для того чтобы не было такого что мы какие-то зоны там не видим что они на самом деле наоборот Овер потребляют или да имеют очень большой запас хотя им это не нужно вот по поводу значит внедрения каких-то технологий здесь Да это больше технический доклад если говорить про развитие платформы платформа - Это продукт и по сути Есть Ну там большое количество продуктовых процессов Да которые включают в себя фазы Discovery фазы общения с пользователями сбор реквест Да их соответственно приоритизации э Каз Девы для выяснения какие именно проблемы Да прошлый Опыт есть вот поэтому вот этот вот сет набор довольно стандартных прок Management механик Ну они присутствуют и с помощью них как раз идёт понимание что именно дальше нужно платформе как её развивать Да и перед тем как мы перейдём ещё к следующему вопросу большая просьба отсканировать QR код и дать докладчику фидбек обратную связь может новые идеи для докладов тем и так далее вот так так страниза вопро Да Добрый день Александр отличный доклад отличные инструменты хочу спросить то есть вся эта м как бы скажем так автоматизация призвана сделать evergreen правильно что ещё раз Ну сделать evergreen То есть чтобы вы когда при раскатке приложения Да не получали какой-то датам Ну фактически да то есть я видел на слайдах там было откатить да то есть был возврат что это за инструментарий А ну если говорить про релизный цикл это значит ну высокоуровневые оболочка наша То есть все инструменты значит визуализация и так далее если говорить про сами инфраструктурные технологии мы используем кубес Да в мульти центрах и так да да не хотел перебивать Нет я имею в виду не кубернетес да Как скажем так слой в котором работают подики да бегают Я имею в виду вы используете для Ну там процесса скажем так выпуска релизов Да вы же перед тем как вы релиз выпускает продукт Вы же ну прогоняете какие-то тесты То есть если что-то проходит неудачно Вы же должны будете откатиться То есть как вот продукт доходит до релиза как вот этот процесс организован Ну это Это стандартный грубо говоря C релизный цикл мы используем технологии Значит на основе которых это построено Да долгое время мы использую там Team City в качестве C сейчас targ workflow и соответственно поверх этого строятся все там стандартные пайплайн которые включают в себя там прогон Юнит тестов интеграционных и так далее разворачивание сред в отдельных там кубернетес кластерах и затем выд Production которая там по мульти дата центрам это всё раскатывается здесь свой инструмент jp То есть получается релиз уходит он же 100% стабильно бывают какие-то Нет конечно же бывает Поэтому здесь Да здесь механизмы речного релиза оно идёт по процентно есть механики автоматической проверки Рик именно в рамках этого нового речного релиза и ролки лбе есть ручные есть автоматические Мы ещё не пришли в то чтобы везде был Continuous deployment но значит такая ну как бы фича есть и сервис это используют А вот после идёт какая-то опять теро когте нет после какого-то тестирования нет дальше если это этот релиз стабилен то будет выпущен следующий он уже будет протестирован то есть тестируется артефакт дальше если артефакт полностью протестирован он уже выкатывается покрытие Да идёт там множество механик То есть это и покрытие тестами и какие-то ещё там там очень на самом деле у меня потом могу в кулуарно беседе рассказать есть такой вот пайплайн типа Quality Gates да что именно проверяется довольно большой набор вещей разного уровня на разных стадиях перед тем как что-то попадёт в продакшн да Супер это мы в кулуары переносим А сейчас вопрос отсюда Александр М банк Большое спасибо за доклад Александр У меня вопрос такой ты сказал что у вас есть механизм переноса шумных соседей куда-то там на отдельное оборудование Но обычно сосе неум просто так а они шумят под нагрузкой вот как у вас обыграв вот этот момент что вы сервис переносится на другое оборудование под нагрузкой А ну тут на самом деле механика работает так то есть это всё работает в рамках ну скажем такого как коммунальных кубернетес кластеров Да их несколько штук там в своих дата центрах Вот Но значит специфика такова что Что такое шумные соседи на самом деле Да если так э давать определение то есть есть сервисы которые потребляют какое-то количество ресурсов при этом у каждого из сервисов есть опять же там реквесты которые автоматически определяются Сколько именно он скорее всего Да там сколько он запрашивает сколько ему нужно и значит история выглядит так что даже когда у нас Ну вот эти вот реквесты выставлены да и всё автоматически подобно в реальные значения А какие-то из физических нот мы используем либо be Metal Да там либо какой-то гибрид с облаками но важно что вот эта вот нода да в в терминологии кубернетес в какой-то момент времени может оказываться сильно там более утилизированные чем другие при этом это не значит что это катастрофа и деградация это просто означает то что те сервисы которые работают на этой ноде они будут отдавать худший перфоманс ну там это больше Лен и так далее поэтому механика здесь работает так она следит на высоко за высокоуровневым метриками как раз видит что вот здесь идут перекосы то есть здесь у нас шедулер сделал так что Ну тут всё хорошо и значит перформер утилизация но такая значимая утилизация которая уже влияет на конечные там цифры вот и значит это есть специальный механизм который стоит смотрит за этим если находит такие кейсы то просто отстреливает и даёт возможность кубернетес сделать второй раз выбор лучше соответственно эти инстансы переезжают и в итоге система так само балансируют более стабильнее нет простое нет потому что любые сервисы они масштабировать Александр Большое спасибо за такой доклад сам тоже раньше работал с платформами в Вот и у меня такой вопрос потому что если мы уходим в такую абстракцию когда вот мы делаем благо для разработчиков то есть они могут очень быстро выкатить сво код протестировать его и это очень быстрый цикл разработки то мы как бы на другой чаше весов у нас идёт поддержка всего этого Потому что у нас возникает там инфраструктура потом кубер потом ещё какой-нибудь ещё какой-нибудь прослойка и так далее То есть очень много прослоек и поддержки если что-то не работает у разработчиков Да не получается запушить вход свой Да не разворачивается им достаточно сложно определить в чём же проблем просто нам либо будить всех вот этих вот все эти уровни то есть Каким образом у вас это организовано а к каким командам обращаться именно поддержке я понял вопрос Да отлично На самом деле тема саппорта всегда горячая в платформенных командах потому что здесь есть два значит два момента первые - это инциденты в инцидентах обычно мы как бы вот таким Дрил дауном мы понимаем Да быстро где именно происходит какая-то Ну там проблема Да фундаментальная и здесь высаживаются сразу же с разных слоёв инженеры быстро смотрят Там метрики алерты и так далее и реагируют на них это если что-то масштабное вот это именно фазы инцидента Да если говорить про удобство использования и понятность да то есть например разработчик взял и посадил себя в сервис там Мбит панику Да из-за которой не выкатывается сервис Да он его выкатывает смотрит что-то с платформой не то да что-то она как-то криво работает не выкатывает мой прекрасно работающий сервис Вот и здесь Значит мы ведём двумя путями Первое - это мы стараемся всё заложить прямо внутрь платформы чтобы она стандартные типичные кейсы отлавливать То есть например мы знаем что если там не знаю кш лубков Да есть Прямо собранный набор вот этих вот кейсов да в момент релиза то мы это репортить пользователю то говорим что вот на самом деле скорее всего проблема вот здесь вот твоя паника да то есть вот эта история и это как бы такой селф сервис режим Да разработчик это видит пошёл сам починил Это первый момент второй момент естественно покрыть всё нельзя поэтому у нас есть довольно ну такой развесистый пайплайн и вообще механика саппорта Да там Первая линия и вторая линия по зонам которые собственно т от разработчиков вопросы Да они приходят спрашивают то что как раз осталось непонятным и дальше там первой линия определяет Ну какую скорее всего зону это этот вопрос относится она подхватывает и решает проблему при этом здесь я бы сказал наверное подавляющее большинство вопросов Это как раз истории про удобство Да и про вообще понятность того как взаимодействовать с платформой и здесь это в том числе один из инпутов то есть Анализируя затем те вопросы которые задаёт пользователь это раз один из тех самых продуктовых процессов уже можно как раз сделать выводы о том что работает не так понятно да какие вещи нужно Да есть какая-то внутренняя типа валюты там RV вот как вы лимитируется пользователей то есть по квестам или по реальному потреблению если по реальному как это всё в онлайне отслеживайте Да мы здесь важный момент значит что мы не лимитирует эти квоты уже в ран тайме Да мы их лимитирует грубо говоря релиз цикла Да и вообще историчности потому что ну квотирование естественно идёт не на сервис Да на какую-то уже более там Ну какую-то оргструктуру там какой-то например команду или ещё там больше там Юнит и так далее Вот и значит э Здесь важно что сервисом вообще вот вся история реквест лимитов и так далее Она автоопределение что то потребление которое по факту получается суммарно Оно укладывается в квоты и если оно не укладывается это отдельный Да механизм то он во-первых это репорт Ну на основных дашборда Да где мы показываем эту информацию сводную во-вторых в релизом цикле то есть там банально когда разработчик пытается выкатить сервис у него всё красится там в красный цвет да и это такая как бы софт квота говорит что сейчас есть проблема Да квота превышена здесь в эту зону нужно посмотреть вот у нас есть механизм р как бы десь механики то есть что мы можем взять и как бы просто отрубить возможность релиза Вот Но по факту Мы её не используем потому что обычно Ну это кейсы которые Ну решаются и без Ну в общем без такого без необходимости жёстко что-то рубить Вот то есть резюмируя это такой высокоуровневый механика Да дополнительная сбоку которая просто даёт Первое это обсер били понимание Как именно компоненты потребляют ресурсы и второе Если вдруг что-то выходит за кво то сообщает об этом для того чтобы был какой-то экшн со стороны команды разработки Вот спасибо да спасибо Саш похоже пока вопросы закончились либо кто-то хочет тебе задать их лично Давайте ещё раз Александра поблагодарим Спасибо"
}