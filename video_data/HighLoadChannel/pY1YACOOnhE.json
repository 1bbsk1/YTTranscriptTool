{
  "video_id": "pY1YACOOnhE",
  "channel": "HighLoadChannel",
  "title": "Поиск совпадений и дедупликация в потоке / Леонид Юрьев (Positive Technologies)",
  "views": 708,
  "duration": 3431,
  "published": "2017-04-22T14:48:17-07:00",
  "text": "давайте начинать помаленьку зовут меня нет работу и еще компания позитивные технологии рассказывать я буду про довольно интересную штуку в принципе такой алгоритм подход ником случае математика который мы тоже собирались патентовать но вот решили отделить чистым математику они рассказать открыто конкретную реализацию как поставил для патентование давайте начнем уже о чем за год вот там в тезисах было заявлено пробили от файлов значит выявление между ними совпадений и все этот потоки важно а вот что-то не понимаю так большие сборы самое важное потоки в потоке от означает что мы не имеем доступа к сему образу данных вас есть некая круто вами магнитная лента и она вот мимо нас проскакивает мы видим только ники кусочек информации максимум кроме того то что сами накопили то что рядом этому следует проще гайки с произвольного доступа это вот то что в тезисов на самом деле если мы это как бы забегая вперед рассмотрим проблему то у нас все это сведется к тому что нам нужно делать сегментацию бинарного потока данных произвольного в котором не за что зацепиться я бы него нет мы про него ничего знать не знаем а нам нужно как-то порезать потом расскажу про идею решения и собственно про метод уже к росту риони конкретного игрока ну давайте начнем дальше что первых что считается совпадение непростой вопрос потому что то что и как сравнивать богатого аудио треки сравнивать порно видео важно еще что-то и за несовпадение они могут быть непонимание сложно на часами совпадения не мог быть разного размера и их полезность зависит от того как мы но в любом файле вот любых двух файлов могут совпадать два бита они нам ничего не даёт могут совпадать там четыре байта они могут спать несколько раз это тоже то значит бесполезной информация если мы будем рассматривать мелкие совпадения то мы получим очень много много таких каких-то бесполезных фактов которые сами по себе ничего не дают просто мешанина вот если будет совпадать какую-то какая-то большая пастор качал обойти это относительно полезную информацию и вот еще такая штука как у стойку устойчивость и скажем потому что спрашивают ваш алгоритм как было подходит что он будет делать при квартиру разбавим нулями про это не говорили мы говорим про торта по простую вещь для начала давайте мы будет просто смотреть на подстроки то есть пост почки пойдет и рассматривать какие-то и совсем коробки совпадения то есть например сорок два байта или больше кто игла понятно да вот это число здесь очень символически потому что какой-то конкретной сферы применения она обрести говорилось я туда хочется значить рассматривать один килобайт это по 10 байт в этот любой творец вот здесь просто доказательства 2 давайте пойдем дальше делать бегает эта задача была первых и публикация то что было заявлено это при копировании вы не хотим чтобы у нас делаем очередную копию киста тогда елизово систем хранения там 10 и мы поднимаемся там дела с процента скале фигуры был ранее скопированный каким то есть дубликат информации поставить копье следующие такое предотвращение утечек имеет следов болезни некие секретные файлы это ключ не только не важно что а мы сидим на периметре дикими да еще где-то и хотим предотвратить утечку воды него каким-то новым происходит мы веками мне если файл по чем то причинам теперь мы хотим на 10 10 леди какие-то фрагменты такая довольно таки по не понятны и просты и аспекты и мокко любила но я постараюсь зачем вы ставшие пройти не хотела кислой добавляет это правда без них как бы словно вести в тему так что все было понятно и его отлично дальше работа здание сами богом что взгляду что некий способ когда вообще мы работаем до минска него что-то является падение выявляется падения файла чувствуете значить нарезается на руки считая каждую функцию даже работа такая коробка выдавать только четыре байта мне начальство самое главное приклада это что что при нем каждый бит его зависит равна вероятно оказывается он дал обет если вас план на блоке мы поменяем любой вид to do this имеет как это половина 1 раз а вот можно залить если исходному меня хотя бы один бит у нас до леса поменяется половина петр после это такая теперь математическая конструкция сам больше не показать какие следствия всего такого подхода смотри нравится совпадение парами формации выпьем когда у нас они совпадают полностью его случаях искали дальше различие совпадение мы видим побочным то есть точностью до блоков эти блоки большие то будет точно если маленький то смеси больше это будет неточно если совсем мелкие top garden совпадение мы выясним точнее хотя они могут быть менее полезная 5 . что читать совпадению далее обработка блоком не равного размера подход применяется при резервном копировании собственно в чем суть его базируется на предыдущем порезать разные блоки просто подряд значит последнего на больше не кратно его либо дополнили гуляли либо вытянули ну и самое главное что граница блоков кратный разберу сами блоков понятно да 1 по 4 килобайта парик мы порезали они так у нас линейно последовать найдут это очень-очень важно ну что здесь начать происходит какой здесь по 2 капли есть если начало файла мы вставляем и валерий бантик у нас как бы весь хвост тебя цепочка блоков меняется меняется и границы меняется дайджест его то что происходит такой эффект называем вот отваливается то есть вся картина при анализе мы видим как будто весь ваш дальше поменялся такой просто недостаток следующая значит регулировать мало кто знает но при анализе текстов такой вот самый стандартный подход просто про него расскажу в чем суть и вера в это день пицца на самом деле с английского singles сложно быстро словами проговорить то стали забрали на слайдах собственно пациент последовательно берем и словах меняем блоки в один блок глеб несколько слов и они идут вложения вот для текста под замечательно потому что мы как бы по таким блоком очень хорошо увидим совпадение текстов если например просто послала смотреть на набор слов маму принципе языке конечник и везде одинаковы ну вот это все примерно тоже самое только чуть более формально значит мелирование подходит только для текстов и сама экрана здесь мы на границе слов если у нас нет слов не структура думы не знает за что зацепиться он теперь вот наших задач и вернемся я так потом постарайтесь корейцам чтобы потом на вопросы время было больше не к исходная формулировка нас есть набор образцов но набор исходных файлов на таких много они принципе большие то есть вот тот самый миллиард и каждый файл по гигабайт например вполне какие-то из них могут быть конфиденциальный то есть секретный что это значит что мы эти файлы не можем вынести вот принести в точку анализа потом там где мы будем как бы выиграть дубликаты из падения что же нужно сделать нам нужно найти фрагменты из всего вот этого набор просох причем эти фрагменты могут быть других файлов частично или полностью и все это еще может быть в потоке not as the organs потоки когда у нас мимо пролетает информация эти четыре как формации мы хотим быстро мне видеть наши фрагменты причем можно это как вы расширить на то что сами образцы мы тоже хотим обрабатывать поток и сама еще одна важная старайся наши данные произвольный пусть мы не знаем их структуру это какая-то после знает что вообще что угодно это не текст и хотя могут быть и текст и произвольно информация не ничего не знаю вот если принять эти условия то что у нас получается во-первых обрабатывать мы сможем только дай устами почему потому что исхода информацию у нас много отдается нам позвать позволит сделать не казать объема то есть мы можем порезать на куски кому по мегабайту если очень много информации да и вместо каждого мегабайта там и спорту не знает омс назвать пример дальше обеспечить некую конфиденциальности то есть можно выбрать такие хэш-функции которые значит необратимые и таким образом снег таскать цифровые отпечатки выведена на города еще мы можем узнать безопасно куда-то передавать если рассмотреть штатные варианты вот фиксированным блоком блоками или значит шума и слов-то эти моменты не подходит почему не подходит потому что у нас возникают сложности на нас хвост не должен отваривается за то что я говорил если начали файлов 1 до этих ставили удалили мы должны увидеть это совпадение ну а нет у нас структуры нет слов поэтому регулирование на принципе не подходит какая была вот пойди то есть вот я подвожу к решению думы смотреть на наши данные как на не куртку сквозь прицел то есть мы на данный на поток данных смотрим через сквозь узкую щель и пытаемся там увидеть некую чтобы увидеть не куб у нас есть оценочная функция которая вот через год принципы очередь и данные как-то анализирует и сказанном вдруг она каким своим законам увидит эту утку нам поможет флажков вот примерно так уведи муку будем стрелять здесь больше у нас проходит как из-за нашего очередного будка опыт с правой стороны некая других тельный автомата и сказал точно нас будет скользящее окно такой стандартный подход при обработке информации еще раз это некий фрейм который мы мысленно двигаем долю нашего файла и вести нас по току как бы у нас тот поток сам двигается до протекает сквозь и так но дальше нас есть оценочная функция которая анализирует данные только в этом очень скользящим окне выдает нам результат периодически если тот результат условно говоря истина мы проводим границу блог тут надо сказать про вероятность почему вот вроде все детерминировано но на самом деле если такие характеристики будем и тайским описывает у мы сможем сказать то картина вероятностью почему потому что сами данные у нас непроизвольное фиг знает какие и любые как бы формулировки любые дальше выкладки они будут токов терминах вероятностей потому что данные произвольный давайте на простом примере все это прокрутим давайте возьмем скользящее окно в 10 брать будем двигать вдоль файла будем считать им до 5 вот такую самую странную хэш фонд туда и обозначать границы когда вот дайджест значением до 5 град на 42 что у нас при этом получится среднем каждая 42 полиция будет границы хочу доказывать поверьте это будет так значит очень важное справиться границы которыми приведем они будут зависеть только от содержимого скользящего окна вот этих данных которые сейчас мы рассматриваем и далее если мы таким образом приведем границы то когда мы заметим совпадения как только у нас содержимое скользящего окна в двух файлах значит в образце и в текущем потоки совпадет вот содержимое окна совпадет или больше мы начнём проводить одинаковые границы эти одинаковые границы высекут некий блок никого размера так вот дальше когда мы хотим одну границу с чем следующую границу у нас будет одинаковый сегмент бог одинакового размера уже если он совпадает и мы увидим это совпадение поэтому на самом деле вот здесь мы начнем изменять замечается совпадения размера окна + блок это очень важная ставится нужно как вас мыслить и сейчас то есть не просто размером в окно иноком phrasebook значит что-то плохо если копнуть в теорию то на самом деле длина блока будет подчиняться обратного 3 что такое обратная связь отрицательное биномиальное распределение нужно спать и ранее pascal там вот ниже формула то есть я вероятность это сказать и график которого синие при что мы здесь видим что блоков единичного размер очень много потом так плавно плавно скатываемся к нулю в принципе да 01 никогда не достигает пределе только соответственно если мы вот в качестве модули там остатка деление возьмем 42 то у нас в приятности будут вот так для единицы будет 1 42 для стола там такая сложная формула но не можно посчитать там около 1 процент получается чтобы нам здесь хотелось ну скажем так хотелось бы другу определение точно бы хотела чтобы больших сильно больших блоков не было почему не было больших блоков потому что если мы подрезаем блок 1 мегабайт будет редко но будет то совпадение или несовпадение мы увидим с точностью до вот этого огромного блока а если у нас будут блоки условно гони больше там 100 байт то у нас точность повысится гораздо понимаете да дальше чтобы еще хотелось на самом деле хочет учитывать его листики ведь реально если посмотреть в какие-то данные могу анализировать то бывает так что там и тут куча идет куча нулей а потом раз пошла полезная информация ну или наоборот была полезна информация потом пошли 0 или были каге бинарные данные пошел текст его им там на бы хотелось чтобы вот при такой пересечении такой как гордиться которой человек увидит у нас тоже отсекал с границы вот такие эвристики нам бы хотелось учитывать при двигаемся дальше ключи к решению очень долго на самом деле не быстро пришло понимание но ключевые моменты такие можно комбинировать несколько функций несколько сцен очень функций причем комбинировать так чтобы снижать снижать вероятность плохих случаев тормоз плохой такой кучей значит это генерация сегмента очень большой длины вот вероятность такого события должна сильно падать сильно снижаться при комбинируем увеличивается ни в коем случае опять же нам бы хотелось все таки мелких сегментов поменьше но сейчас проникать скажу отдельно дальше без внутреннего состояния то есть все это должно строго работать от содержимого окна просто если мы этот принцип нарушаем у нас нарушается локальный границ и дальше данным как бы наши блоки граница боков не синхронизируется но вообще никаких совпадений не увидит и четвертый пункт он такое самое на самом деле абстрактный но ему нужно вам объяснить 8 чем фишка что он вы на самом деле не можем ни в коем случае избавиться от мелких блоков нас всегда будут блоки там один байт 2 3 4 даже если нам нужно чтобы идеально были там посылок battery по 42 вот эти мелкие нам все обязательно нужны почему потому что если мы будем пытаться навес блоки одинакового размера сильно изменим их как бы карту распределения тот означает что мы на самом деле близко подойдет близко в этой схеме с той схеме которая работает по блокам фиксированный размер а вот если у нас будет как бы все варианты так или иначе мы на самом деле будем в данных обнаруживать некую картину плекса на хутор да и вот эти утки они могут между собой растает стоять на разном расстоянии поэтому и ни в коем случае нельзя кататься выставить именно как-то упорядочить строгого эти интервалы поэтому нам между ними тоже будут интервалы в один байт их 2 x 345 вымочить можно поменять картину распределения что чтобы не было того вала которая там пума график в принципе эти блоки мелкие нам тоже нужны такая него значительно избавляться так теперь суть решение что мы делаем мы делаем наша основная скользящее окно чуть увеличиваем его запускаем еще одно маленькое мы берем несколько оценивающий функций и выстраиваем их некий порядок ну например здесь три функции там да она не воды можно гораздо больше здесь дальше мы маленькая поздно скользящая knot любим в большом и для каждой позиции считают значение всех функций и заполняем вот такую таблицу как написано и дальше в этой таблице мы какое ищем перевал утку именно как написано начали долю 2 потом спускаемся вниз снова будет строки и так далее очень простая вещь я специально вот все что можно упростил чтобы легко карту входило собственно это все дальше на вопросы будут да давайте поздно свойства посмотрим это этой штуки так что у нас будет у нас сохраняется принцип на границе зависит только данных до окно стало больше но они не зависит как только данные у нас упадут в размер окна мы начнём проводить границы одинаково точно как бы граница синхронизируется а далее мы можем здесь использовать любые подходящие функции ну то есть любой функции данного класса которые обрабатывают скользящее окно и вот сжимай выдают там ники булевы признак есть торонто или нет и для худшего случая для больших сегментов у нас вот такая формула вероятности то есть это отрицательно вероятность того что все функции выдали ложь самое важно там вот n в степени то есть это всё лагерь экспоненциально извиняюсь бывает надеюсь я смог или поехали дальше причем подчеркивает а вот следующая позиция будет степени на дальше еще еще плюс 1 плюс 1 плюс 1 поэтому как очень хорошо то что мы хотели распределение размеров блоков есть вот такая интересная формула но кто умеет читать быстро и не знаю там или быстро вникать уж там упрочит прочесть я скачивал морочить голову в принципе все считается все считается можно оценить там три компонента можно посчитать если перейти и к конкретике вот пример распределения то есть можно подобрать такие функции что будет вот такое интересное с протяжении тут уже хотелось бы остановиться чуть подробнее значит что в чем здесь суть что у нас вероятность значить коротких блоков блоков нужного нам размера 42 до она примерно одинаковые а потом резки оболонь и почему там такая неровность на самом деле это не просто как бы теоретически вычисленная что-то это результат симуляции применение алгоритма на конкретном какие возможности добавляется сейчас некая так сказать горизонты во-первых тем мелкие блоки которые как бы нам не нравится мы можем суммировать филировать мы их можем точно так же по принципу шинглас когда слов объединять зачем нам это может хотеться во первых сам по себе мелки блок размером 1 байт он как бы не несёт никакой полезности да то есть а падение одного байта как я говорил это довольно бесполезной информации но тем не менее эти блоки нам все равно такого размера нужны потому что они позволяют нам как бы правильно видеть картину так вот мелкие блоки мы можем суммировать в этом она никакой проблем не создает само по себе зингер здесь что он дает мы уменьшим количество индексов того самого размер своего отпечатка потому что так мы уже мелкие блоки слиперы да и по крайней мере этом в три-четыре раза объем индексом для мелких блоков и уменьшили далее на самом деле мы здесь легко можем применять эвристики то есть функций которые не просто выдают что-то а подкрашивают на блогах пусть они могут сказать вот этот блок не просто бинарным текстовый и мы дальше для этого блока можно какие-то другие опытом алгоритма снятия отпечатков приводить применять и женюсь и следующий пункт на самом деле того чтобы вот не вникай некий дебри уже но с точки зрения практики они полезны для разных функциях мы можем поменять разный размер внутренних ложных окон . говоря когда мы заполняем таблицу результатов то не каждая не все функции нет функции не обязаны для каждого для каждой ячейки могут через одну выдавать или еще как то есть отсутствие результата это тоже отрицательный результат а на деле это дает что это чем меньше мы как бы реально используем размер окна тем быстрее мы синхронизируем границ так ну давайте вот дальше я очень надеюсь на вопрос потом потому что очень много пришлось урезать и упростить значит точность и достоверность что здесь подразумевается но по точностью это насколько точно мы объявляем границу совпадений достоверность насколько можно верить нашим результатом становятся впала в достоверностью все просто у нас есть вероятность коллизии да и собственно чем шире у нас дайджесты чем они больше тем меньше вероятность ну все достаточно прочь просто с точностью но как говорил мы видим различия ваще синхронизируем как бы границы и видим различные вот у нас есть размер окна плюс размер блока и так или иначе мы можем посчитать для этого плотность вероятности теперь следующий пункт баланс детализации посмотрите ещё раз то есть на картину лучше взглянуть по-другому когда у нас есть много мелких блоков да это плохо но на самом-то деле у нас например та та та распределение которое показывало не примерно мелкие блоки встречается там папа единицу по одному байту по два по три так же часто как и те которые нам нужны и вот по 42 на самом деле они вносят в разные вклад в картину то есть объем информации которая содержится блоки размером сорок два сорок два раза больше чем блоки которые в один байт и вот если мы это перемножим то есть видим некую миру влияние у нас получится вот такой вот график распределения его на самом деле нужно нормировать про него там есть некие свойства да он в принципе в конечном счете это давно сходиться к нулю первый предел ну и все это если нормировать то как бы сумма всех блоков все она должна быть равна н так вот подбирать подбирать наши функции конструировать a set of sky bar резак нужно так чтобы максимизировать вот для желаемой а.н. вот это значение это значение тот мир влияния ну как это делать сейчас не хотелось бы обсуждать или лучше чуть чуть сложнее математика и она требую но какой-то лекции что ли уже и такой богом бумагомаратель стоят сказал давайте пойдем дальше связь размера индекса вот как это все связано тут несколько соображений которые просто я довожу как бы до до сведения что ли то есть просто выводы если мы хотим что-то получать результаты точнее достоверно суд будет больше яндекс яндекс это некое промежуточное представление вот наши как бы цифровые отпечатки до исходных brasov причем такая важная мысль вот размер этого индекса это фактически описание по колмогорову да то есть предел пока могу раву всех результатов которые мы можем выдать это очень важная мысль то есть когда мы пытаемся сделать если кому-то захочется сделать яндекс меньше то чем меньше и moody мы его делаем тем в принципе менее полезны или более грубой будет наши результаты ну соответственно даче меньше индекс тем больше у нас пропуска в тем меньше достоверность и можно еще это так формулировать что если потянуть за уши теорий в сравнения мор множеств да и наши образцы и файлы рассматривать как множество байтов упорядочены то тогда вот как бы то насколько мы сильны сожмём яндекс это собственно то насколько сильно мы допустим ошибку при сопоставлении этих множеств вот примерно такие мысли значит в чем новизна все это дело но честно скажу наш мы искали долго я искал значит и среди патентов судил горит move значит есть похожее решение по схожей похожие схемы аналогов не найдено вот стопроцентных таких но опять-таки здесь отрицательный результат это результат то есть может кто-то все это изобрел уже просто не знаем поэтому давайте искать вместе с толщиной этот доклад я приглашаю кто-то что-то знает подобное давайте делитесь что вполне возможностью уже изобретено но тем ни менее принципиальные отличия если это сравнивается с чем-то другим внутри у нас значит это нарезка бинарного потока не поиск поток вот поезд под строк это как бы отдельный атом серы обработки там 50 алгоритмов далее там если не больше нет здесь не поиск под строк здесь нарезка бинарного потока водка могу роскошь янгирова нее 2 пункта то что имелось ввиду это собственно мы можем применять технологию шиндлера вания да как будто у нас есть слова хотя нас англии со слов у нас нет то есть мы можем нарезать все на мелкие блоки довольно легко и просто а потом использовать под корнем гравирования для всего для обработки этого дела тоже очень удобно получается ну и гибкость в чем гибкой что на самом деле можно комбинировать различные функции различны эвристики эвристики в том числе какие найти начало zip файла и найти там еще какую-то сигнатуру понимаете под пони провести границу и тогда приз в составных документов очень четко были особенно при анализе в потоке мы очень четко будем находить границы тех каких-то файлов блок в который там линна передаются вот другие способы есть но они вот всей этой комбинации ну никак не дают давайте вопросах вопрос скорее такой ну практически это сейчас был представлен научный подход или есть уже какие то конкретной наработки то есть что такое утка что ищем какие-то конкретные области есть нужно искать ник экстракт утку там просто результат то есть не оценочная функция должна вернуть либо ложь либо истина это просто как бы пакет с утра очень абстрактно то есть утка может быть что угодно или даже не пикантно там все-таки там все-таки это булево боль его значение и полон вы сейчас рассказывали научный подход вот продукты какого-то конкретного правильно то есть у вас практического применения там пояс сигнатур или чего то есть бурака дуга аратов чего там видеофайлов аудио файлов или что это поиска дубликатов бинарных данных любых то есть изначально образцы они вот как бы из кореи структуры в этом вообще без структуре структура да то есть там это может быть вообще биржевые котировки ну да если захочется омолодиться это все дела у вас на полу где-то или это просто сейчас смотрите это готовые сосна да надо очень хорошо отключать вот это вообще дело собирались патентовать да и даже там за ход был просто математику не потяну не не это памятовать поэтому от математику мы как бы отделили от некой практической реализации и вот всю математику все такое белая пушистая щас как у пытаюсь рассказать а какие то конкретики применения как каких-то областях это все-таки наверно пойдет через патенты поэтому я сейчас не хотел об этом говорить могу каких-то там собственно лучше конкретики by ну пока пока его нет хотя области применения вот они там как бы были вначале здравствуй спасибо очень тесной так лация вопрос один как-то все работает на самом деле вопрос первый смотрите вот высказалась есть несколько функций которые оценивают бинарный потока некую реку данных как как как вы сказали как это контролируется скорость работы этих функций на потоке да то есть если какой-то бэк бэк пряжи да то есть влияем ли мы на скорость самого потока что будет если мы перед пристали успевать с оценкой ну понятно что мы не успеем обработать данные то есть конечно функцию должен был удачно быстрый так бы вы сами решаете когда будете что-то делать какое-то конкретное применение они должны быть достаточно быстро и понятно ну если вы не успеваете обрабатывать поток у вас есть 2 варианта его приостановить либо прервать обработку все настолько оцениваете вероятность совпадения основана как на как на какой-то свои исходные оценки доскажешь tankadere от нас 5 процента до не смотрите можно на самом деле можно углубиться в математику и поставить цель ну например какие-то параметры более более точно задать ну например что мы хотим видеть совпадение размером сорок два байта с вероятностью там девяносто девять и девять процентов до то есть пропускать одну сотую допускать отклонения в одну сотую процента это уже более точные как бы параметры более точной постановка задачи и под это под эти как бы параметры скажем до входящие можно подобрать вот набор функции после положительные оценки когда на 3 вы получили единицу вы идете посмотреть на исходные данные нацеленный без дефисов для точного совпадения как титров нет ни идем в это можно делать но правда смысла в этом нету почему потому что во первых проще взять дайджесты шире вторых но хотя хотя на самом деле вы как бы можно и так сделать да то есть можно этот подход использовать для генерации никого индексы просто быстро что-то там сказать искать да а потом делать сверху вполне возможно но в реальных применения как не доходило потому что проще обычно взять даже с большего размера и на другие как бы математические законы перед скажем так есть какие-то механизмы для обучения автоматического или ручного для калибровки размера окна который вы ищите и оценивайте данные в процессе работы на живой высокий смотрите вот у нас есть вот эта магическая формула она на самом деле самодостаточна полностью вот честно то есть вот н это примерно у нас размер окна да и дальше там p1 пока это вероятности значить того что какая-то конкретная функция для случайного набора до нас выдаст значение истина и это полностью самодостаточная вещь вот отсюда можно все вычислить дел том что довольно вкусная штука вот как бы анализ вот этой змейки да вот это в этой таблице какие функции как это можно заполнить как проанализировать как посмотреть там ну такой я песка что ящик пандоры ну довольно много то есть можно неделю вот устроить симпозиумы неделю про это говорить но самый простой вывод здесь что функцию из которых больше вероятность выдачи положительного результата лучше переместить вниз но ведь функции их вероятность оценки нас зависит от того как какие данные на на входе от типа данных но у вас могут быть какие-то предположения заглянул область применения какая-то прям да почто вы оптимизируете вы можете сходить из этого ну например вот на верхнем уровне проще запустить функцию например которая ну скажем так еще сигнатуры какие-то если они у вас применимы а то есть и мы хотим искать например я точно обозначать начало каждого zip-файла к примеру брали какого-то там еще архива то пусть это первая функция у нас ищет эти сигнатуры говорит нам истину когда но я нашла вот вот последняя функция но пусть он будет то само деление на 42 средняя там пусть еще что то делает пусть например пытается отличить текст от не текста там или 0 и от ну как то так просто когда характер данных сильно меняется распределение тоже обозначает границу и это уже будет работать а таких функций можно набрать там но если хотите тысячу в зависимости от применения себя спасибо за доклад у меня такой чисто практический вопрос как вот принципе построена скажем так объявление набора уток который ищет насколько я понимаю когда мы считаем столько hash-сумму нас в принципе на борт тех данных которых мы ищем в потоке получается возможно даже больше чем изначально был а у смотреть во первых утка здесь еще раз это булево значение и она нам нужен только для заполнения таблиц что использовать качестве оценивающий функции так как бы ваше личное дело что был гулевский результат грубо говоря берем какой-нибудь ноутбук какого-нибудь высокопоставленного чиновника снимаем с него диск там грубо говоря каширу каким-то выбор там по этим сша мы ищем что этих и шить это не должны появляться больше посмотрите кашель и дайджесты это все-таки один из вариантов цифровых отпечатков у них есть свои свойства можно делать это как вы говорите то что было в предыдущем докладе когда там птицы практически снимались отпечатки с а у него это тоже вариант то есть дайджесты можно использовать разные как таковые мы просто у нас есть две задачи первая задача это как бы поток который вообще бесцветен аккаунтом плане монотонен и неизвестно чем заполнен до нам его поделить на блоке мы эту задачу решили а дальше для каждого блока мы можем как бы уже там десятком способов снимать свои отпечатки причем то что я говорил вот чуть дальше вот здесь вот второй пункт мы можем на самом деле подкрасить блоки да то есть когда мы в поток исходный непонятно какой разбили на блоке мы можем учесть эвристики каждый поток награде цветом типом то есть сказать скорее всего здесь текст азии скорее всего аудио азии скарифа не понятно что это понял не то можно ли это применять уже как бы к данным второго порядка когда например мы взяли видео там но если видео сжато одним куат кодеком там у старым кодеком она как бы отличается если мы там как-то это прогоним разобьем на кадры усредним то тогда картинки будут как бы совпадать то есть можно если более глубокий анализ да но принципе мотив если мы все смешаем то будет очень сложно а вообще вот я сам начальник первым white амиран со 2 под строк то есть мы как бы для для упрощения решаем задачу разбиения потоком непонятно какого потока на блоке когда мы его разбили довольно гибким способом когда мы можем понять в принципе доказуемо понять что получить блоки разумного размера которая нам интересую с мужем раз предъявлением так или иначе и привязать эту разбивку еще как бы к реальной структуре данных до учесть эвристики то дальше мы можем по-разному анализировать вообще их вот тут как бы основная сложность там мог быть и как начали это плохо про это проговорил торопился но основная сложность том что если у нас бинарный произвольный поток то мы вообще не знаем какой стороны к нему подойти то есть монотонная последовательность байтов без начала и без конца фактически 0 и хорошо есть начало есть конец но внутри может быть 10 файлов может быть один может быть здесь фрагментов да пусть мы не знаем как вообще к не подойти как и порезать здесь решается сдача порезать порезать то что как бы поведать вроде как нельзя а вот теперь мы порезали раз порезали дальше можно уже анализировать на себя чувствует сильно ускорил походов доклады много времени давайте упростим задачу представим что у вас текст мы знаем что нас блеать экой секретное как она будет нарезана вот как мы будем убирать по пробелам весьма то смотрите если с текстом то на самом деле с текстом на естественном или искусственном языке все гораздо проще там все такие слова вырезаем по словам бушинги жонглирование то нет менялся нагрева нет вообще для многих языков том селе искусственные хорошо работает для двух одинаково слово будет получатся одинаковые хэши смотрите если если вообще у нас есть слова да то наверно это истории ну не про нас bosch если не вникал завелся что мы защищаем какую-то секретную информацию которую чаще представим в текстовом формате brain и но это в смысле в эту вас какое-то применение вы про себя говорить или вот просто вопрос вы изначально озвучили задачу это как это защита секретом там скажем там периметре сидим мы периметру не доверяем то есть мы единица что можем периметру скормить какой-то преобразована информацию которая будет намочить там наводящий поток понравился соответственно получив вот и слов мы чтоб получаем то есть мы делать берем хэши получаем набор какой то информацию это по словам правильно ну вот примерно вот так обычно это работает почему именно так потому что само слово само по себе если вы отдельно про хэшировать то набор слов как бы он языке конечен и у нас в любом случае будут совпадения между абсолютно не совпадающими текстами просто из-за того что не на том языке написан и а вот если мы хотим сделать то что делает de cire нет да то есть заимствование 1 диссертации с другой например увидеть тогда вот там что на гребне как раз такие работы мы берем последовательно несколько слов насколько маленький тут слуга вообще это отдельная штука зависит сам целая теория есть какой у вас язык как он устроен ский ну я не спит я не спец по обработки естественных языков ну примерно опять таки еще раз во первых нужно очистить от перевести канонической форме то есть удалите лишние пробелы знаки препинания перевести там к регистру убрать окончания и так далее да дальше 80 от вас нет ложатся и текст вот у меня там нет никого сейчас поясню просто у вас информационная емкость до 1 шинглас одного блока должна быть какой-то разумность ночь разумный у вас есть слова которое ну условно гаи мусорные или почти мусорные бесполезные предлоги например да а есть какие-то слова сочетания специфические термины для какой-то области и так далее и так далее и у вас как бы информационная емкость или уникальность осанкой уникальность она растет нелинейно то есть одно слово само по себе это как бы один попугай а вот два слова или три подряд каких-то специфических это уже там 100 попугаев поэтому вот эти все словоформы надо учитывать на самом деле и как бы взвешивать шингу анализа рынка не просто по количеству слов а по их емкости но компьютеру не играть гаджете нарезать вот вы сейчас описали ситуацию когда человек нарезает когда он знает допустим же dns не делается корпус текстов для каждого слова у вас есть как часто нам диаграмма зифа там называете для tig больше как часы как я стану встречаю дальше для каждой предметной области часто формируется набор словосочетание каких-то до тус терминов которые как-то особым образом подкрашиваться дальше все это учитывается ну то есть это как бы отдельно как бы не то чтобы наука но отдельный процесс я к чему все это то что насколько это защищена от статистического и от реверса то есть когда у нас будут просто оцениваться допустим если мы нарезали по словам мы можем подобрать определенный там последователи статистику частоты встречания там к слова священством предложения вы имеете ввиду что здесь сделать реверс восстановить текст по по дайджеста в этом хотя отдельные там тоже хитро сети как это делается и все-все-все озвучен нет ну там есть методы защиты но они достаточно просты и вы оба первых и бы конечно не должны делать мелкие shingle и во-первых ног а вот это не то надо компьютеру сказать как это делать вот скажите конечно так сказать компьютеров прогнуть я программист скажите это что не заходить чтобы насколько нарезать потому что говорим каким-то понятием не слишком мелко не слишком крупное компьютер таких слов не понимает это человек понимаете можно съесть там грубо и провести определенные черты еще раз у вас есть языку волосистой корпус языка she называется то есть например для вы должны резать он был так чтобы у него например было ну хотя бы там триллион комбинация чтобы их нельзя было перебрать чтобы нельзя было сделать радужную таблицу да ну вот все так нет я отвечал прос по словам и так давайте возьмите микрофон и спросить еще раз я щас еще у нас вообще еще 15 минут как минимум до обеда а потом я еще вашем распоряжении на части хотите но немножко если вернуться к математике мы режем мы построили наши функции какие-то да теперь мы мы ожидали какой-то там структурированные более-менее данные о посылаем на вход белый шум почти идеально идеального белого шума не бывает но почти идеальный белый шум то совершенно случайный набор и байт и немножко рассказать как будут распределяться разрезание блоков это поможет потом но немножко осознать математику вашу смотрите еще раз у нас в принципе если мы будем выбирать функции которые рассчитаны только на какую-то конкретную информацию до предполагаешь например это текст или вот подобные этому на что-то что-то рассчитывают то а потом и подойдите белый шум или что-то подобное это случайные данные конечно будет плохо то есть у вас функции по-хорошему должны быть такие чтобы для того набора данных который вы реально будете explo то есть какая-то сфера применения может прилагать что у вас там не совсем произвольные данные а там наполовину произвольные да какой то они какому такому классу относится к может ограничить но в принципе если вы нарушаете свои собственные предположения то ничего хорошего не будет куда то ничего хорошего но как будет выглядеть по распределению как блоки будут нарезаться но смотрите еще раз вот у нас виде там функция вот написано toffi нас есть произведение отрицательных вероятностей все вот вы для для того набора данных которые подаете на вход вы должны как-то для своих функций определиться насколько они будут успешны какой вероятностью не выдает результат истинно ну и собственно дальше все просто просто считается никакого другого ответа дать нельзя то есть размер нарезаем их блоков он будет тоже случайном случае белого шума а нет смотрите еще раз не случайный там там же степень то есть у нас будет примерно извиняюсь сейчас бился и будет примерно вот такая картина все равно то есть вот этот хвост в худшем случае у вас будет вот такой хвост идти мочить сходиться к нулю вам он может быть достаточно длинный можно вопрос здесь здесь вопрос собственно как выбирать эти функции вот оценочные но самое простое вот один из вариантов я как бы сказал что там делить на 42 можно взять например на было простых чисел до и по возрастанию то есть вначале взять самой большой а потом поменьше поменьше поменьше например вот если самый простой вариант который реально работает это считаем некий дайджест берем набор простых чисел и начинаем остаток от деления почему числа простые потому что чтобы вот это дальше сейчас я чтобы вот эта формула на самом деле работал а точнее вот это функции должны быть не взаимно коррелирующие от никто брать не спросил пока но то есть и функции коррелирует а у вас между ними как бы события зависимой формула совсем другая а здесь для независимых событий формового такая поэтому если взять простые числа и братья остаток от деления дайджеста на простые числа то с очень точно с очень большой вероятностью у нас как бы нет между этими функциями корреляции все будет хорошо ok давайте последний вопрос и потом тогда ловите леониды задавайте вопрос уже в кулуарах с этих два вопроса такой вопросик касательно применения в области не поиска какой-то информации сжатия например бинарную потолка где публикацию далеко отсюда правильно я понимаю что в общем случае размер индекса по которым мы будем мочить наши фрагменты это просто диапазон значение хэша который мы выведем и держать например для функции с низкой вероятности коллизии такой индекс и мочить все блоки это не выгодно с точки зрения ресурсов может быть и не выгодно смотри какие блоки взять если вы возьмете блок например возьмем дальше установить 56 назад на горе напрашиваются если это какие-то данные реальные например мы будем нажимать то скорее всего возникнет такая ситуация что можно будет индекс как-нибудь оптимизировать например там сделать какой-нибудь и в нём я вам шло не будь такой и функции которые матч отношу на шутки они как-то оптимизируются для того чтобы потом поиск совпадений для взят смотрите были простыми функции нам нужны для того чтобы непонятно какой поток непонятным случайный поток порезать на блоке как только мы его порезали нам эти функции не нужны больше все они нам нужны в процессе нарезание то есть когда мы строим индекс либо потом когда мы анализируем некий поток чтобы на такие же блоки порезать теперь собственно что размер индекс из чего будет состоять у вас исходный объем информации будет порезана не на блоке никого размера чем блоки крупнее но и для каждого блоковых собственно будете хранить конечно за дальше с да ну например позицию какую-то где он начинался может быть его размер если захотите но вот давайте прикинем месте возьмете там сша 206 32 байта добавим мочить смещение там еще win64 чем восемь байт 40 байт на один сегмент он один блок какого размера блоки вы хотите взять если возьмите блоки размеру мегабайт может вам так удобнее будет очень хорошая компрессия если возьмете блоки в килобайт ну тоже будет неплохо но гораздо индекс будет гораздо больше то есть вот то что я говорил там дат вот сейчас смотрите вот у нас мы можем как бы достигать желаемого баланса детализации то есть у нас всегда будут блоки меньшими маленькие блоки размером в 1 байт но мы их можем вот этот график пик этого графика то есть миру влияние всегда как бы подвести под любую величину вас там может быть мегабайт и средне эффективная длина блока получается мегабайт и тогда инвент индекс будет да но если вы но опять же если у вас блоки большие то вы какие-то мелкие совпадения не видите то есть если у вас блог мегабайта совпадения больше большая часть совпадение например по килобайт у нутовые к их не заметите конечно тут как бы все очень демократичность так можно сказать здесь добавляешь там лавку добавляешь спасибо большое за лекцию вот вопрос такой если мы хотим мы учитывать наше знание о этих данных то тем самым что мы строим дайджест мы же фактически теряем это знание то есть смотрите дайджест это один из вариантов построения оценочной функции точненько и так у нас есть два независимых процесса это первый процесс это нарезание потока на блоке да вот мы нарезали дальше эти блоки можем как-то анализировать мочить совпадают они или нет сравнивать да так вот один из самых простейших способов сравнения просто использовать дайджест но бы взять большой даже с довольно широкие там 200 asha 206 дату историков два байта и там вероятность коллизий очень маленькая это это простой понятный способ но если вы знаете природу ваших данных какой-то причине знаете или в процессе значит разбиений на блоке вы применили эвристики как бы отдельные блоки подкрасили то есть вы понимаете что здесь нам бинарные данные какие то непонятно какие а вот здесь текст то вы принципе можете использовать разные методы снятия дайджестов то есть даже с это просто один из вариантов цифрового отпечатка вы можете использовать чуть-чуть что он что как бы что угодно но только на самом деле чем принципиальная вещь на чтобы это сделать вам вначале нужно этот исходный блок исходный поток информации непонятно какой как-то порезать вот мы эту задачу решили а дальше все у вас все кафка как бы все на вашей стороне тысячи связанный вопрос фактически если мы пометили что какая то часть этого потока допустим это видео файлы или аудио файл он на самом деле мы уже обретаем некоторые state дальнейшей обработки как вот это вот нет смотрите мы стоит это это не тот стоит это результат но мы не должны ни некий стоит использовать для проведения границ то есть вот наши границы когда мы обозначаем граница блоков мы должны проводить stateless то есть только на основе анализа скользящего окна тогда если мы делаем именно так у нас получается некий закон локальности то есть место проведения гора нет зависит только от содержимого скользящего окна если содержимое скользящего окна совпадает выбросить его как бы в исходном ником файле в образце и при последующем анализе то границы будут проходить одинаковым начинают проходили кого синхронизируется и вот здесь должен предстоит лес именно то есть только от содержимого скользящего окна а когда мы этот результат выработали дамы кроме проведения границы можно выработать некий дополнительный признак the heavy листика может нам выбором выработать некий классифицировать границу сказать что здесь начинается текст или заканчивается текст или еще что такой точняк уме то информацию и нас дальше мы можем ее использовать для какого-то анализа там построении индекса там снятия отпечатков что хотите спасибо и давайте тогда закончим на этом сейчас будет часовой перерыв потом приходите слушать про администратора vkontakte прадеда supra нейросети еще кучу чего интересного"
}