{
  "video_id": "F1LwlD6bFrQ",
  "channel": "HighLoadChannel",
  "title": "Docker & Puppet: как их скрестить и надо ли вам это / Антон Турецкий (Badoo)",
  "views": 212,
  "duration": 2638,
  "published": "2017-04-22T14:47:32-07:00",
  "text": "там работая в компании баду системным инженером то есть это исключительно отдела эксплуатации к программированию отношения постольку поскольку вот и сегодня я смотрю что зал достаточно пол не то есть технологии я думаю что для всех кто пришел на ну во первых это не новая технология но это такой модный новый тренд я думаю что большинство из вас права продукт уже слышали раз вы пришли послушать и мне хотелось бы после вот там нет а по про docker и всего остального увидеть руки тех кто пробовал от продукт просто запускать а на него смотреть есть такие общем зале отлично те кто запустил докер в продакшен работать с ним сейчас я могу даже поднять вторую руку они от вот чуть больше чуть больше вот хорошо ладно значит тем кто еще не запустил это вполне возможно будет интересно итак значит начнем содержание о чем я попытаюсь рассказать вот в рамках этой сессии про докер я попытаюсь рассказать вам предысторию того вообще почему правильно этот продукт посмотрел я посмотрел наш отдел вообще в целом наша компания из сиама выбирали почему мы к этому пришли потом расскажу вам про подготовку железной составляющей то есть как было подготовить вообще нашу текущую инфраструктуру до того как внедрить туда этот докер как начать все таки с ним работать и использовать его в буду расскажу про ключевые узлы данной системы той что я необходимы и достаточны для того чтобы начать не работать что у вас должно быть дальше мы коснемся темы моего любимого папито и я расскажу о том как получить какой-то рабочий прототип на отдельно взятой машине как это дело раскатать на множество серверов потому что у нас их на данный момент порядка трех тысяч единиц чуть меньше дальше мы коснемся вопроса системы сборки образов для запуска контейнеров которые мы собрали с помощью папито и существующих наших ресурсов расскажу вам про диплом сервиса с точки зрения отдела эксплуатации потому что тепло и сервис разработчикам и тепло и сервиса отделом эксплуатации это такие немножко разные сущности и поэтому я считаю что не стоит обратить внимание я обязательно коснусь вопроса того что не бывает никакого внедрения по крайней мере на моей практике не было еще такого ни разу что ты выдумаешь про продукт в к таком идеальном вакууме ты его внедряешь точно так же у тебя нет ни одних там кораблей проблем с которыми приходится справляться грабли с докером были я думаю что они еще продолжу быть мы будем их исправлять я коснусь нескольких моментов которые мы нашли поправили и пока живем с этим счастлива я коснусь момента того чего хотелось бы видеть в неком рудные пи данного продукта и некоторые мои соображения про который я расскажу после общения с жеромом который приезжал москвы с компанией docker некоторые видения совпадают и то есть некоторые штуки про которые я скажу о том что хотелось бы их видеть они действительно могут сделано в каких-то следующих релизах может быть в течение одного года может быть чуть больше и в конечном итоге я постараюсь подвести заключение в котором расскажу чего мы в году хотели от докера и чего мы имеем на данный момент итак начнем начнем с того что же такое все-таки докер все вы я думаю это уже знаете это не система виртуализации это в принципе это даже не какой-то ноу-хау все было придумано до этого как в свое время сказал джерома сказал о том что если бы не придут не не придумали и не выпустили бы этот первый там бета или альфой релиз докер а то кто-нибудь пришел бы на месяц на 2 на 3 чуть позже придумал бы что-то вот такое же и назвал бы это иначе потому что сама суть и главные идеи докер на данный момент это в нужное время в нужном месте то есть тем кто знаком с джейлом free bsd то те кто когда-то работал с linux контейнерами с системой в это должно быть достаточно знакомо al-exe хорошая штука но мне очень не нравится интерфейс то есть в дочери и то есть это добавили это достаточно хорошо вторая проблема перед тем как вот начать внедрять я смотрел порядка двух или трех месяцев на развитие продукта я пытался смотреть на чем она переключается на я пытался смотреть на нашу инфраструктуру смотреть на самом docker и пытаться вот понять какой же стороны подойти вот в какую часть инфраструктуры попробуете применить для начала это был достаточно такой трудный выбор и вот самое главное что было сделано это вот все-таки нашли какую-то первую точку составили не некую карту наших хотелок и пожеланий из всего того что в принципе можно достичь с помощью докеры о чем написано на их сайте на всех без practices мы решили выработать свои там несколько пунктов которые как только мы их достигнем мы пойдем дальше итак начнем с того что мы хотели получить изначально докеры мы хотели получить гарантированный некий гарантированный state любого из наших внутри написанных сервисов то есть это значит следующее что если мы когда-то сервис собрали в образ запустили ее в какой то момент времени мы хотим иметь гарантию того что если нам придется откатываться по каким-то там причинам недостаток кода на стороне фронтэнда или какие-то там еще прочие вещи там не работает новая версия не до конца тестировано мы хотели хотели и хотим быть уверенным в том что если сервис мы запускали хотя бы раз и он работал нас устраивал к этой версии мы можем откатиться всегда это один из основных бенефитов перед началом внедрение докера дальше мы хотели получить и упростить немножко систему выкладки сервисов такого деплоя сервисов на оконечное оборудование на котором он работает с точки зрения отдела эксплуатации то есть у нас на текущий момент существует такая текущая схема диплом с нашей стороны и сбоку к ней не нарушая того что было мы пытаемся добавить и точнее отчасти уже добавили какой-то процент сервисов видим новую диплом мы подумали что вот этого достичь с помощью докер тоже сможем также не маловажный был момент того что мы хотим иметь для например такого классического понимания сервиса то есть это сервис без там никого пирсе стас тораджа своего какой-то самописная татуировать для которого достаточно там оперативной памяти какого-то набора данных где-то там никого загрузчика в которой с помощью этого загрузчиком этом из условные базы можем закинуть нужные нам данных дальше работать там в памяти или по какой-то логики данного сервиса и мы хотели получить резервирование которых заключается в том что мы скажем так должны быть уверены в том что если у нас выходит физически из строя там сервер или кластер серверов мы гарантированно там там железе который стоит у нас на складе просто в очереди мы можем взять развернуться и быстро запустится с минимум каких-то потерь и такая тоже достаточно интересная штука которая нам показалось что мы хотим и не попробовать и посмотреть как это будет сделано с помощью докер они с кучей разных машин которые другой реплицируют дублируют и так далее все это собрать в рамках там например одной машинки мы смотрели на задачу такой некой балансировки внутри одного сервиса и пытались и пытаемся достичь того чтобы в случае каких-то плановых обновлений или изменение наших сервисов downtime у нас был минимальным то есть мы его хотим сократить такой не тот самый последний момент который мы тоже хотели получить в силу того что у нас конторе присут присутствует нас ли такое наследие как совсем старого оборудования но она относительно старый присутствует оборудование там средних лет может быть годичной давности присутствуют какие-то тупые железки все это может быть в рамках одного кластера и на какой-то момент времени за счет того что некоторым из наших сервисов нужна была такая полная изоляция другого сервиса там по портам еще почему-то мы не всегда могли взять и запустить на одной машине три разных там по типу например сервисов которая служит такие совершенно разные данные и очень часто мы имели такую картину что например у нас работает какой-то один сервис на относительно там старом саней какой-нибудь 4170 ему кроме оперативной памяти скорость вообще это даже от памяти не сильно важно но от кроме потребление там порядка 12 гигабайт может быть в пиках своей работы оперативной памяти сервера больше ничего не нужно то есть у нас мы занимаем тем самым какое-то электричество место в стойке сетевые порты все остальное при этом наша железы простаивать процентов наверное на 70 такие случаи были и мы хотели сделать некое уплотнения то взять там либо топовый синюю неважно какой сервер и подсадить на него ни один сервис а подсадить на него там 510 или там сколько влезет зависимости от анализа и ожиданий эту цель мы достаточно удачно достигли итак начнем всяко более копнем чуть глубже и так исторически сложилось так что в компании баду основной единственный дистрибутив на данный момент времени и пока мы упомянем не планирую менять остается сюзи linux enterprise соответственно плюсы минусы данного дистрибутива я не думаю что есть очень много людей кто использует его может быть там у себя на своих машинах дистрибутив достаточно консервативный на текущий день порядка по моему трех или четырех дней назад с компания зарелизили 12 след в котором наконец то есть там свежие кадры все остальное там наконец-то появился систем да и прочее с чем столкнулись мы на момент нашей текущей актуальной стабильной версии которые мы используем на всех наших машинах первое когда вы открываете сайт там докеры хотите его поставить на какое-то свое железо вы понимаете что одно из первых требований это версии ядра версии там 3 8 и выше соответственно в дефолтной поставки и слез 11 смотри мы имеем вот последнее security патч версии ядра у них 30 101 то что было чуть раньше не сильно различается то есть здесь мы принимаем решение о том что гидра нам нужно в любом случае выкатывать новые так как мы в стандартном дистрибутивно мне дали используем определенные свои почти которые нужны для работы каких специфических сервисов соответственно чтобы выкатить новые ядро нужны эти почти переписать линейка и изменение достаточно большая его нужно несколько раз собрать как-то тестировать посмотреть плюсы минусы и после того как мы принимаем решения то что если вы хотели б ушла без причины ушло потому что приличное количество времени на данный момент мы остановились на версии регистрации зимой а он даже соответственно в консервативном смеси в зависимости от там разных требований самого докера нам пришлось обновлять кучу там внутрянки всякий linux учился и 5 билл сайфер out и так далее следующим интересным моментом был обзор и тестирования тех стороны бэкенда где вот как бы крутится ваш контейнер лежат какие-то данные него неважно постоянное хранилище временное хранилище для логов или счет вот этот вопрос выбора между 100 раджим соответственно мы рассматривали два варианта это стандартный девайс maker плюс x 4 и рассматривали очень горячо любимый сообществом сюзи linux enterprise и вообще upon сьюзи в том числе файловых систем и btrfs игру вся эта штука там принципе никому не новое то там живет я не знаю сколько наверное живет linux практически только это все дело живет на момент времени до докера скажем так мы его настолько тесно не использовали соответствует было еще эта вещь которую пришлось немножко и последний чем у нас изменения таблицы раздел эти договоренности о том как должна выглядеть как у нас выглядит изменение от большего не майся на тех или иных мастерах здесь у нас появился еще один такой тимплей до создание машина правды корпус это прямо к примеру мы злодеи вот это все было подготовлено бра бра такое большое терского братвы но соответственно нам нужно было рассказать потерялся не стоит нам нужно было рассказать хозяйства на так как мы получили по фигура логичным и сцепление там соответственно нужно было сделать только автоматику того чтобы хотим использовать вместо на 1 машину хотим использовать 10-15 кто-то 50 машину который приходит к темам работы списывайте у нас закончилось тем что бы по касательной допустим вот именно вами на создание отдельного как бы при государе вас пакет в автоматическом режиме и за исключением того если мы например машин отправляем неевреем стало ли мы не ставим новую машину машина была под каким-то другим проектам и нам нужно на ней изменить там таблицу разделов то есть здесь возникает необходимость участия инженера как бы тут все в зависимости от то есть иногда проще и там скину все сервисы машины отправить его сетап и смотреть на нее как на новую машину из чего состоит докер что вообще ему нужна для того чтобы работать самой основная единица это docker host это может быть там вашей локальной машины в нашем случае это некий сервер на который мы собираемся крутить эти образы на текущее состояние для нас docker host это обычный linux сервер на котором остановится у зеленых enterprise 11 смотри используем на данный момент времени мы докер 120130 который вышел не так давно мы посмотрим изучаем и так далее и выбор такого стороны бэкенда у нас полна btrfs есть еще такая штука то есть сам docker host может хранить ваши образы удалять собирать делать с ними все что как бы заблагорассудится но так как нужна какая-то централизация то есть мы же не можем там с 1 до краха стать делать там торгов какой-то кидать его на другой хочет запустить другой компании потому что это отменяет всю идею того что все это быстро централизовано круто и так далее нужен докер registry докер registry ты такой набор скажем так архивчик of и метаданных который хранит информацию там оля очень похожи на структуру комментов детей он хранит в неком централизованным месте также он хранит вот определенный лаэртом плеер или набор лееров которые нужны для поднятия вашего контейнера каком-то своем месте что умолчание предлагает докер не предлагают docker hub docker hub штука клевая вопросов нет они даже предлагают приватной репозитории за деньги где ваши какие-то там ценные данные вашей компании никто не увидит не узнает но все это находится где-то там в облаках где-то далеко и так далее а нам нужно быстро и сейчас и нужно этот желательно локально то есть если там у нас две площадки 2 that data center то в каждом дата-центры но нужно иметь свой registry для того чтобы быстро тягать эти образы между центральным местом и между конечно сервером на данный момент у нас докер digest а это по большому счету точно такой-же обычный docker host на котором запущен контейнер с докер виджеты официальной дальше мы используем для авторизации потому что из коробки авторизации нет эта штука важны мы используем обычного барзиков авторизацию в engine.exe и там просто для tig юности и любви работы докера между registry и конечной точкой к с целью не вопрос мы используем видимо я перепутал слайды это соответственно слайд придут про предыдущий момент о чем я рассказывал зачем нам нужен паппет на данный момент времени как мы раскатываем от одного там какого-то эталонного hasta мы собираем данные которые мы меняли и уже делаем это централизовано в на любом количестве но дальше дальше такая довольно интересная история мы рассказывали я рассказывал на конференции квот fest в этом году есть видеозапись кому будет интересно рассказывал о том какую уж мы придумали классную в отделе эксплуатации какую мы придумали классную карту сервисов в которой мы хотим видеть и понимать на какой группе хвостов или на каком посте работает какой сервис какой тип у этого сервиса какая версия там сборки у этого сервиса то есть это как бы такой эталонное состоянии там площадке вот с точки зрения наших сервисов как должно быть когда мы начали внедрять брокер мы увидели там всякие клёвые штуки типа экспо за в dockerfile и так далее и тому подобное поняли что в нашей базе знаний не хватает одного немаловажного момента мы не храним информацию по тем портам который обслуживает данный конкретный сервис соответствие эту информацию мы достаточно легко внесли и получили уже есть такой карты там году сервис есть мы мы добавили parts мы получили не кино лишь бы и со стороны эксплуатации этот но лишь бы соответственно представляет из себя там пару табличек в базе данных из которых мы можем генерить там зависимости наших пожеланий что угодно мы можем формировать и зато в лесу не какой-то там список для мониторинга наших сервисов на можем там выбирать в порты занятые свободные определять что мы можем гадать какие порты можем отдать под новый сервис и прочее с точки зрения такой автоматизации сборки и выкатки этих образов и в том числе тестирования работает таким образом наша старая текущая система тепло и использует очень сильный плотно паппет мы не можем взять сломать и построить что-то новый поэтому в задачу нашу входило просто взять и как-то с минимумом из изменений построить систему такую дублирующую рядышком соответственно вся конфигурации описании наша инфраструктура содержится изначальную в этой базе знаний потом из нее формируется некий ямал с помощью обычных скрипников и мы решили что все данные у нас уже есть как мы как как ими рулить на удаленных системах мы знаем почему бы нам не создать систему сборки на каком-то выделенном сервере соответственно система сборки из себя представляет при первом приближении обычную директорию в этой директории существует под директории с именем сервиса и в директории с именем сервиса уже присутствует динамические формируемый докер файл существует количество файликов которые необходимы для сборки данного контейнера и существует избыточная достаточная информация с теми файлами которые необходимо потом при создании образа на следующем этапе это работает следующим образом нам нужно не обходить сервер необходимо налить сервис нарастить версию что-то там поменять изменить конфиге мы поменяем либо в базе знаниях либо мы правим манифесты побед а после чего в автоматическом режиме попить приходит на машину для сборки проходит по каждому сервису если он видит какие-то изменения в этом сервисе запускается автоматическая процедура докер build который соответственно и формирует уже имидж для запуска контейнера здесь есть как бы два состояния первое состояние контейнер у нас собрался соответственно все хорошо и второй состоянии контейнер не собрался на каком-то там шаги отвалился что-то не там файлика не хватает или инструкцию dockerfile указан неправильно и в зависимости от мы поступаем уже по разному если на все пошло хорошо соответственно представляем так нашей сборки проставляем дату сборки версию которую мы собрали делаем параллельно пушкам наш registry формируем какую-то задачу надел тестирование о том что вот собрался новый пакет его где-то там нам запустить его как то что ты погонять и если там все хорошо вы там ставите галочку о том что все хорошо с этой сборке и отдел эксплуатации понимает что это то сборка которую там девелопер собрал тестирование тестировала по первым тестом ничего плохого не выявлено мы можем это дело запускать production если что-то не собралось соответственно приходит отбивка отдела эксплуатации о том что вот что-то там пошло не так и здесь уже в ручном режиме разбирается админ инженер так как докер в случае ошибки сохраняет им тот последний свой стоять на котором он не смог что то сделать админ смотрит руками начинает заходит вот стоит правит какие-то инструкции делать какие то действия и процедура вся проходит по новой в автоматическом режиме после уверенности в том что все уже собралось все хорошо как бы чем нам здесь помогает парк в данном случае он поддерживает полностью в нужном нам состояние вот это вот окружение для сборки он выполняет их затем на основании своих либо изменений либо не изменений соответственно оповещает как-то в случае если это необходимо и сборка прошла удачно мы делаем push в наш репозиторий для тестирования мы можем в автоматическом режиме на выделенных машинах делать пул из этого репозитория мы можем синхронизировать всегда состоянии нужные нам на конкретных машинах состоянии репозитории мы можем ненужные нам старые образы и контейнеры просто с машины выкидывать то есть вот их просто раз и нет там на файловой системе чисто нет какого-то такого загрязнения виде того что разработчик там закинул новый binary старый не удалил и потом вот все это движется на протяжении полугода то в итоге в итоге структура директорий вырастает во что-то неимоверное и еще у нас работает как такой помогал к скажем так он делает не совсем rc скрипты для запуска нужных контейнеров он просто выдает инженером в случае необходимости что-то запустить он выдает ту команду которая нужно запустить на основе соответственно этих данных которым используют при сборке это иногда полезная штука потому что там ключей приза для запуска контейнера может быть там масса и не не всегда их удобно запоминать для тем более того или иного сервиса что вот как выглядит downtime и вообще перезапуск сервиса штукой перезапуск сервиса там условно классического в отделе эксплуатации для того чтобы нам выкатить новый динаре нам нужно пойти на машину где работает этот сервис взять но игнорь удостоверится в том что все confident него подходит но хотя бы запускается там не сиквел птица соответственно что нам нужно сделать вот как бы здесь при пример такой удачной работы сервиса и там клиента который что-то спрашивает соответственно что у нас происходит если мы придем делаем обновлением первое что мы должны сделать это остановить демон соответственно момент остановки демон все запросы которые там прилетают на этот сервис выдает все что угодно там тайм-аут как cannelle то есть здесь все уже дальше зависит от изобретательности не знаю верстальщиков или там дизайнеров которые красиво отрисую то почему не работает чем нам здесь может помочь докер в данном случае все на самом деле просто и прозрачно ничего нового здесь нет то есть что такое балансировщик всем известно всем понятно понятно что это можно размазывать не в рамках одной физически машину можно просто сделать балансировщик сделать несколько об стримов сделать какой-то умный логику который будет вычислять жив ли up stream посылаете туда пакет или нет но в случае с докером и из только система которая уже на данный момент построена вокруг него есть определенные продукты которые вот ты просто взял поставил они ну то есть они работают все это сделать там сильно проще что мы попробовали что мы сделали в данном случае мы взяли подняли сервис эти сидим это очень такое легковесные простенькое в хранилище она очень здорово реплицируется там выстраиваются различные цепочки с помощью него можно сделать что такое что там можно надо совершенно спокойно убрать потушить все данные переедут на оставшиеся участники этих нот и тогда ли он достаточно легко весь он настраивается достаточно тоже просто то есть вот оно как бы было поставили заработай дополнительно то есть зачем он нам нужен он нам нужен для того чтобы например у нас работает старый демон мы хотим запустить новую версию нашего софта порт который обслуживает данный сервис слушает у нас в данном случае не сам сервис а слушай того nickelodeon сир у которого есть информация в его обсудим их когда мы запускаем новую версию новый контейнер мы соответственно сообщаем по определенному пути в эти сиди о том что я вот новый контейнер с таким-то сервисом я запустился у меня все хорошо я работаю в таком-то адресе слушаю такой-то приватный порт на такой прям внутри приватной сети которую мы там формирую с помощью бриджа данные эти приезжают есть такая класная штука как он где он соответственно тоже цепляется к этому эти сиддик этой базе кирью слушает смотрит на определенный ключей смотрит изменений если изменение пришло там в зависимости от поднялся контейнер или наоборот опустился старый он вносит изменения по заранее заданным шаблоном х прокси и делает соответственно рилот из плюсов здесь получается что здесь получается что мы все это делаем локально мы не трогаем никаким образом внешнюю сеть внешней сети я считаю то что выходит за рамки вот там это х интерфейса конкретного сервера на которую мы это делаем мы не трогай никакие dns и dhcp и так далее то есть мы принимаем как бы причиняем в любом случае какой-то минимум изменений внешней системе то есть мы работаем на одном деле взятом хостить на самом деле тема еще такая гораздо больше я попытался вот вкратце объяснить как можно к этому подойти с применением докер как бы плавно подошли к тому как бы вот все это красиво так звучала что-то там сделали тепло и поменяли там сервера настроили ядра каких протестировали много всяких разных слов но как бы не получилось сделать так чтобы не устроить нам какой-то может быть небольшой downtime что-то пошло не так первое на что мы наступили это соответственно на нагруженном с точки зрения сети и количество запросов там запросов ответов сервиса при попытке запустить докер просто вот в базовой конфигурации то есть мы там сделали вот этот стандартный bridge мы сделали над запустили в приватной сетки наш сервис соответственно про кинули один каждому на внешний интерфейс порт первое что мы получили это мы получили проблемой на в контракту я думаю все там били в донецке я думаю что неоднократно прибыл из full варианты исправлений это когда мы берём и поднимаем значение таблицы контроле к мы его поднимаем все равно там остаются промотать поднять его до такого состояния чтобы этой таблицей хватило но как бы ни о чем это не очень нужно соответственно как мы вот данную проблему на части сервисов мы и решили решение достаточно простой мы на данный момент можем запустить докер с мэппингом на физически существующей интерфейс и для нужного нам сервиса или портам и v2 скажем так в два правила таблицы raw можем записать что вот мы не хотим например для соединения которые идут вот с определенного парта куда-то и для тех соединений который приходит на этот порт мы не хотим какого-то конечным треккинга вот просто говорим на у трек в данном случае нас этого избавил от переполнения таблицы и там от выпадания просто сервера из состояния своей работы следующий момент который мы тоже поймали девайс rx4 все просто прозрачно мы знаем как этим пользоваться мы знаем что кайтом л.м. это на первый взгляд здорово работает здесь мы поймали какой-то просто не то что куча ну какие то их довольно неприятные моменты когда у нас слои отваливались при попытке запустить новый контейнер или работы старого контейнера у нас разваливалась именно сама fs вот в этих вот снова шортиках нужно было делать ей двоих . соответственно потом бились какие-то метод данные очень происходило что-то не очень понятно где то не где-то она ищу с гитхабе докера как выяснилось что мы с такой проблемой не одни одни как бы тут выбор пал такой что давайте попробуем тогда btrfs почему нет интересная штука почему нет соответственно btrfs версия далеко не последнее которое сейчас есть на сайте разработчика файла система интересная но я думаю что в том числе для меня в ней на данный момент достаточно большое количество каких-то загадок в поведения во всем остальном как надо смотреть но с момента переключения на сторону виде btrfs проблемы вот такого характера который было не было больше ни разу и самое последнее такая интересная штука на которой мы тоже наступили это волей мэппинг как выяснилось что при попытке зама писем link внутрь контейнера мы замыкаем просто фактическое место положение на той директории ли того файла который мы этом случае попытки завопить какое то какой какую-то директорию и в эту директорию если подмонтировать какие-то устройства то есть нашем случае это там некий лук девайс и мы это можем сделать на момент старта ну дальше мы получаем интересную проблему докер-контейнер при старте читает табличка прок mount и соответственно при запуске весит про клаус переносится в контейнер дальше происходит следующее что мы хотим от монтировать лоб девайс на ход системе и сделать этого не можем потому что у нас держит процесс мы хотим примонтировать новый лук девайсы чтобы он появился в запущенном контейнере без привилегированного запуска это сделать тоже невозможно то есть но в данном случае получается что не использовать те директории которые вы собираетесь динамически именно включать и выключать подойдем к тому что хотелось бы вот чего хотелось бы видеть в докере на данный момент и этого нет без каких-то дополнительных телодвижений 1 этот централизованный мониторинг то есть хотелось бы видеть состояние тех холстов на которых крутится контейнер хотелось бы видеть из коробки мы выбрали на данный момент co2 за как слушать собирать статистику отправлять куда-то дальше мы используем там графа но increase db мы это все храним то есть мы это получили но это как бы определенный труд определенные там заморочки так далее из коробки чего-то такого рабочего мы не нашли очень хотелось бы видеть управление инфраструктурой как вот кластером то есть хотелось бы зайти в одну точку и понять где у нас там сколько ресурсов свободно сколько контейнеров запущена где мы можно запустить что-то еще опять же такого прям что поставил работает нету несколько продуктов посмотрели не все соответствовали ожиданиям на на данный момент так по большей части пользуемся шепардом но больше пока присматриваемся то есть пока управление на самом деле из серии набор скриптов через api и то есть shipyard пока не сказать что прям сильно интегрирован очень интересный проект написанные тоже на голову для организации таких внутренних приватных каких-то сетей это вот есть вив которого он очень клёвый ну вот его также нет из коробки какой-то такой серьезной интеграции у докера пока с ним нету его попробовать стоит в том случае если вы пытаетесь что-то настроить и очень сильно не хватает возможности управления окон высвечу именно с помощью каких-то там тонких настроек докеры то есть понятно что мы можем сказать только вы не использую никакой bridge libre что сделаю сам тебе и сам тебе его сделаю выдам и дальше ковыряться уже вручную супн-8 чём то есть на данный момент она работает просто со стандартами там будет шутил с linux вами и пока больше ничего не даёт хотелось бы это видеть и жиром сказал что у них на данный момент 5 человек в отделе разработки занимаются процессом интеграции докеры понга свеча это я считаю такие достаточно хороший приятный новости в заключение чего вот к чему от чего мы шли чему пришли в году с докером мы смогли уплотнить за счет и изоляция контейнеров и сервисов между собой мы смогли построить схему при которой мы можем запускать более сервиса на одном сервере причем разных типов как таким дополнительным плюсом и толчком к которым пинку он даже сказал наверное создание карты сервисов докер на что же привело мы получили клевую штуку очень нужная уже теперь ну возможно вы и получили конечно не без докер у но просто здесь вот так в параллель сделали классно мы построили систему сборки и тестирования для новых сервисов не сломав старую то есть мы построили рядом при этом не перестраиваю особую структуру сильно не трогай структуру того что было на данный момент есть гарантии того что если мы сервис запустили когда-то положили вредности мы запустим уже в любой момент времени на там любой другой железки ну и как бы как иногда наверное важно участвовать каких-то новых там тенденциях смотреть что-то изучает в данном случае от докер итак судя по всему мы не сильно отстали особенно после общение на этапах и наяки вот собственно все спасибо вам за внимание спасибо что пришли если есть вопрос часто добрый день спасибо за доклад меня зовут алексей и у нас доброшу я честно говоря не понял большую часть из того что вы говорили вот что слишком специфичная очень слабы с этим знаком вот но у нас компания сейчас только развивается и идет движение в сторону со вот и проблемы которые у нас сейчас возникло даже не столько на серверах сколько в отделе разработки что у нас много разработчиков и каждому нужно держать рабочую версию с большим количеством ну там небольшим пока там три сервиса да вот и они должны как-то обновляться вот была идея решить эту проблему через докер или через пакет собственно и ну если у вас такая проблема доводили разработки и как вот подает лет докер для решения и и ну вот интересные проблемы которые обычно встречаются случаи тестирования да тут запускаете какой-то тест тест оспорить какой-то набор данных базе данных и каждый следующий тест который запускается он должен использовать исходный набор тестов базе данных и самые невыгодные что получается чтобы на прогнать теста там последовательно или как то еще нам нужно базу запустить данными наполнить дан тест прошел данные удалить данные налить и запустить еще раз в случае с докеры мы с вот этой возможности купил райт во первых вы можете получить на основе одного имиджа там хоть и 100 контейнеров там за раз с одним набором данных и гонять изолированное эти тесты то есть в этом случае вам это может подойти и в том числе там запускать один контейнер но при этом опять директории с вашими новыми by ногами вас каждый раз будет там свою ногу вот то окружение которое нужно для тестирования папин в данном случае не поможет ни как паппет это на самом деле такая подготовка железа подготовка платформы для запуска чего-либо то есть в данном случае по пятому нет то есть само использование контейнеров для того чтобы вам поднимать такие эталонные тачки в большом количестве быстро без использования всех ресурсов вам это может подойти тут идет именно рабочие места разработчика да вот как бы интересы такая вещь чтобы не погнув чтобы каждый человек имел актуальную версию проекта до который состоит из нескольких то модулей там до сервисов есть отличный проект который такой этот проект где ребята написали такой проброс сессии как раз таки это сделано для разработчиков они поднимают некий талон не контейнер мопед honda на каком-то сервере разработчика то есть есть некие того что то там поднимается и когда разработчик случится там по два старом порту заходит на машину падает не на физически хвост он попадает уже на запущенный контейнер где содержатся актуальной версии его там binary конфигов всего остального то есть возможно я сейчас название честно не вспомню если вам интересно вы можете мне вот либо написать по контактам либо я там в коридоре меня можно будет найти я просто вспомню название этого проекта их скажу то есть это интересная штука которая как раз таки сможет вам реализовать вот эти вот такие отдельные окружения для домашних директорий пользователя для вот сборки для работы разработчика спасибо здрасте anton спасибо за доклад хотел у вас спросить и здесь до далеко до хотел у вас спросить вот вы упомянули о том что проблемы с монтированием болью ну и как вы живете то с этим вы то есть их не монтируете получат проблема с монтированием уволена заключается только в том случае если вот в той директории который вы пытаетесь монтировать внутренние это в какой-то инфраструктуре под монтированы некие внешние девайсы которые динамически обновляются в данном случае для таких ростов мы избавились от такого поведения то есть мы ушли от лук девайса к обычной директории наполнены уже не kymco в том то есть там всякие мув линки и так далее то есть но то есть по факту мы взяли вообще ушли от лук девайсов но давно понял без слов девайса спасибо спасибо за доклад пара вопросов до 1 этапа петровский модуль до по-питерски модуль который для сборки джокер контейнеров за позор силе нет понятно какой то сделать я могу поделиться этими знаниями но у меня такая проблема за open source сеть она заключается не за того что жалко да из-за того что мне вот кажется что там я что-то пишу не оптимально это понятно зачастую только мне это вам интересно единицу поэтому я но очень редко это делаю если вам это действительно я вам могу там прислать координаты на почту или там выложить к себе на github вы посмотрите на уровень интереса было бы здорово спасибо а еще вопрос нет ли у вас проблем с регистрацией контейнерами в условиях много- сегментный сети вас вообще сеть в основном плоское или присутствует рамках одного дата-центра множество различных сегментов с точки зрения критичности безопасности данных которые там протекают точки зрения безопасности наверное на более горизонтально робот 1d c то есть есть ограничение на данный момент есть какие-то подсети в которых есть цели на физических железках но преимущественно вот разграничение трафика между машинами лекала стиральной машины подпитывать этот последний вопрос предложение собственно с точки зрения оркестрация slide не использовали прав ли ты то за последние три дня второй номер 3 вопрос не пробовал просто не смотрел то есть как бы точнее как бы я почитал доки но пока не запускал то есть я в курсе что это но попробуй в живую него ладно спасибо так вопросы из приложения почему именно докер а не об нпз есть какие-то специфические причины вопрос этот был нами то пью два дня назад все-таки от он ваза это катализации докер это не виртуализации в докере идея докера первоначально до 1 контейнер один процесс на самом деле я с этим утверждением не совсем согласен для все-таки больше согласен с тем что один контейнер это одна задача и процессов может быть более одного но при этом у вас в контейнере нет никакого и не то нет вообще ничего вас есть 1 де трай системные у вас есть там lip контейнер которые как раз таки как то вот так делать не гибрид между основной системой и контейнером openweather все-таки это больше системы виртуализации и то есть играх это там побольше будет чем именно в контейнере еще один вопрос из приложения как вы отслеживаете зависимости например программист стал использовать свежую либо в соусе еще старая какая роль разработчика в состоянии создание контейнера ну как бы скомпилировать binary наверное все таки статически можно считали бы тогда то есть если встает вопрос зависимости присутствие либо там новостей или еще что то но мне кажется более правильным решением будет собрать такой binary который не будет требовать этих зависимости на конечной системе то есть собрать именно образ который будет работать если ты собираешь образ используешь какую-то новую библиотеку тебе никто не мешает в этот образ запаковать эту библиотеку и не использовать системных библиотек на хвосте то есть надо рассматривать контейнер и образ как такую независимую единицу которую ты можешь допустить где угодно если нужны какие-то либо положил к себе внутрь антон спасибо это были вопросы зала поэтому мой следующий on ну а верх молодцы что докер и вообще на технологии осваиваете вот было спич про shipyard такой меньше но еще до этого что-то говорили у меня в голове по обоим этим вопросом было месяц вы пробовали молельный минус марафон чтобы решить этот размен смотрел я смотрел внимательно от гугла кибернет которые они предлагают мне в принципе все понравилось за исключением того что какая основная цель чего хотелось бы тут это артист регистрации хотелось бы самому указывать на каких машинах что запускать подобных вот видите вернется то есть они решают эту задачу как нам нужно некий вычислительный узел у нас есть вот набор кластеров мы хотим что-то посчитать где-то будет запускаться как бы это дело твою то есть мы отдали тебя на утку количество машин считай здесь хочется сейчас пока на этом переходном этапе хочется просто видеть общую картину но при этом указание системе где что запускать выдавать в руку вручную я его просто не смотрел то есть спасибо за совет я обязательно посмотрю то есть громче громче говорите проточный фильтр хорошо коллеги подождите у нас если индекс нас подошёл индекс докладчика проявись отлично хорошо нас есть еще один вопрос пока он на готовимся расчета клочка кто вопросов было море тогда из приложения например productions сервисы запускается по пятнам а если сверх моргнет электроде стойка кто поднимет сервис перезапуск сервиса осуществляется в автоматическом режиме но при наблюдении в консоль инженерным у нас есть специальный окна так называемого в пик трафика когда мы можем заниматься перезапуском сервиса если это не какой-то там fatal или что нибудь такое у нас есть автоматика который готовит весь набор команд для инженера но перезапуск сервиса осуществляется только с помощью инженерного контролем пока он не скажу что я готов я смотрю никакого перезапуска не будет вот здесь поднимали рук да конечно я рассказывал на прошлом веке про систему риск от которые мы используем для разворачивания серверов но это по факту как бы по если там diash степи все это там советской перловый наборов таят профилей у нас получается как у нас получается первоначальные сетапу совсем с уровня железа это там их скат который как раз таки по игре куча всего вокруг как только машина заступилась автоматически вам либо подписывается либо нет в попить и определяется ее роль и дальше папе ту же настраивать такую паппет настраивать операционную систему то есть x как настраивает у нас железку и готовить операционную систему вопит пробегается и делает уже такую готовую для запуска приложения системы то есть работает этот такой связке - по определенному сценарию до автомате то есть у папе то есть возможность использовать внешние там некий скрипт который принимает решение подписать не подписать да мы подписываем прибегая к помощи этого скрипта то есть мы не подписываем все ключи но мы подписываем то что мы хотим подписывать как будто это в автоматическом режиме"
}