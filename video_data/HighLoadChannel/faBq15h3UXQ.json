{
  "video_id": "faBq15h3UXQ",
  "channel": "HighLoadChannel",
  "title": "Опыт моделеварения от команды ComputerVision Mail.ru / Эдуард Тянтов (Mail.ru Group)",
  "views": 1240,
  "duration": 2594,
  "published": "2019-12-05T08:20:49-08:00",
  "text": "да как уже сказали меня зовут эдуард а я отвечаю за компьютер vision mail.ru и сегодня я поделюсь какие мы получили за несколько лет решении различных задач и inside aiki практике мы используем чтобы как можно быстрее обучать нашей модели как можно лучше и все такое или он искал предыдущем докладе что я буду рассказывать как устроены нейронные сети вообще этого не планировал делать поэтому сейчас критически важный момент так как да я знаю что есть трансляция ее будет смотреть люди а именно моя мама но очень важно чтобы люди к встрече здесь тоже все поняли поэтому поднимите руки кто понимает как устроены свёрточная сети так очень хорошо что я три минуты назад составил этот слайд начну с вершины и сети работают так значит они называются так потому что если операция свои руки и операция свертки она графически кодируют некий некий признак на изображения значит здесь на экране показаны три слоя и визуализация того что распознает каждая свёртка в этом слое задача распознавания лиц мы обучаемся раз по дну распознавать классифицировать лица разных людей соответственно первом слое сеть учат минимальные примитивы то есть какие-то градиенты переходы цветов и так далее то есть самые самые простые вещи на основе этого уже следующие слои сети выделяют все более и более сложные концепции наш второй слой на основе примитивов выделяет уже какие-то черты лица нос глаза губы и так далее уже последний слой в данном случае выделяет уже целый модель или лиц знаю может быть разных национальностей разных всего там разных типов носов и так далее вот и что очень важно в не раз сетях это то что мы не задаем ни одну из этих сверток вот это сама все учиться то есть мы только подаем данные на вход это набор людей которые мы разметили каким-то образом что-то персона 1 персона 2 персону 3 то есть у нас на вход модель получает картинку в конце ответ это такая то персона и вот эту всю магию она делает сама это очень выгодно отличает нейросети и в частности с вершины на рассвете от всего того что было до этого где разработчик машинное обучение руками там придумала как можно вот эти все признаки математические выразить и поместить в какую-то модель вот собственно теперь к теме доклады что такое вообще компьютеру вижу mail.ru и каким проектом мы занимаемся начну и наша команда решать широкий спектр задач то есть мы только улик концентрируемся на решение за лишь компьютер vision и мы представим решение в нашей такие продукты как облако mail.ru это наше фото и видео об почты mail.ru и vision би ту би так называемые это решение который мы продаем для наших готовых клиентов на основе наших технологий ну и парочку примеров значит в облаке это наш первый такой основной большой клиент хранится 60 миллионов 60 миллиардов фотографии и мы внедряем туда многое много разных вещей на основе машинного обучения так называемая умной обработка например как уже говорила юля но у нас есть на физ-ре как не шин и линн марк recognition то есть распознавание достопримечательности то есть мы пользователю всего фотки прогоняем через модели каким-то образом их размечаем на там разных людей и что позволяет нам группировать все его огромное количество фотографий группировать в разные под папочки сразу по разным людям или по разным впечатлением в отпуске который он был то есть например вот я был в сантарене и он у меня винодельни санта санторини видим вот это здорово облегчает поиск по вообще галерею в который все больше и больше фоток бренд для польши то мы делали распознавание acer то есть распознавание текста с картинки я не умею не чуть подробнее расскажу то есть суть такая что на картинке код текст и надо вот четко понять какой то текст и выдать этот текст для би ту би вещей мы например зачем я использую ноутбук у меня же есть кликер для битве продуктов мы например делаем распознавание подсчета очередей то есть стоит например очередь на подъемник на лыжи и вот он посчитать вот эту массу или сколько там человек мы чтобы поиграться и как бы тесте технология делали сделали некий прототип в свое время для нашей столовой то есть от фотка наши столовые мы рассказываем там людей но там вдали у нас есть очередь из людей стоит кассиром и мы соответственно считаем сколько людей в каждой из очередей каждому кассиру и сколько минут нам примерно стоит таким образом лучше балансе маркеров столовые вот и осень лишний план доклада он состоит из четырех частей это постановка задач на то как как какая особенность вообще постановки задач в машинном обучении чем он отличается от стандартных не знаю бэкон доски как мы готовим данных как мы обучаем нашей модели и как мы производим inference а именно запуск модели на продакшн начнем с постановки задач и значит я утверждаешь постановка задачи это как бы очень критически важная вещь так как и любая практически разработка имели она занимает минимум месяц но это еще в лучшем случае если вы знаете что делать вот обычно занимать несколько месяцев если как не так поставить задачу то вот ты потом ладошку продукт продукту он говорит все это не годится я хотел другой вот это очень часто кейс и вот чтобы этого не было нужно предпринимать некие шаги начнем особенность на 4 продукта это его очень тяжело описать что нужно именно то есть нельзя просто взять и написать в жире тоска сделаем нет распознавания лиц и все то есть обычно менеджер к тебе приходят и говорят сделай красиво вот вот я хочу также все работало а уже какие-то мелкие детали nikon и может быть даже не знает и не думала никогда не подумает пока не увидит финальный продукт откуда и что же вы сделали вот и значит давайте на примере поймем вообще какие могут быть проблемы вот допустим у нас есть задачи распознавания лиц вы получаете и уже радуйтесь звоните маме тамура отличная задача на сложные можно год перед там все отлично вот ну можно ли ее прям срываться и делать участок сорваться и делать то потом в конце вы узнаете что в этой задачей с некоторые особенности например то что бывает разной национальности роза дата сайте например не было азиатов или негров или еще кого-то и вы вообще их не распознаете продукту это казалось надо вот или наоборот вы потратили лишние 3 месяца на распознание национальности я вас только русский продукции вы могли этого не делать вот что делать с рисованным персонажами вообще непонятно как надо их распознать не надо вопрос кто то читать надо кто-то не надо дети таким бездетным отцам как я якобы для меня все дети на одно лицо я вообще абсолютно солидарен с моделью когда на всех детей пихает в 1 л тоже не понимаю чем эти дети отличаются но для людей у которых есть дети у них совершенно другое мнение и обычно они еще твой руководители вот им очень не нравится шик дети клеится там с какими-то другими детьми или бывает еще забавные фазы распознавания когда голова ребенка апреля запустили там с локтем клеится или из головы лысого мужика ну то есть в общем но за это казнь или это всего этого ты не знаешь на старте задачи и это очень важно определить поэтому с продуктом надо работать и общаться в самом начале на самих данных то есть нельзя от него принять никаких устных каких-то объяснений и только надо смотреть именно на данные желательно из того распределения на которому собственно модель будет работать вы дели в эту в процессе это обсуждение с граница некий тест и вдт сайт на котором финально можно запустить модель и проверить так ли она работает как хотел продукт вот и желательно еще конечно часть теста dtc то отдать продукта чтоб он вам лучше бы не имели к нему никого доступа как машина как разработчик машинного обучения потому что вы запросто можете переключиться на этот тест сны как вообще и постановка задачи машинном обучении это такая постоянная работа между продуктом и специалистом по машинному обучению потому что даже если вначале вы хорошо поставить задачу то дальше по мере разработки модели будет появляться все новые и новые проблемы все новые и новые особенности которые вы будете узнавать про свои данные вот это все постоянно добс уже с продуктом это что транслирую всегда своим ребятам машину к тем кто занимается машинным обучением что надо брать ответственность на себя из помогать продукту ставить задачу почему так потому что это достаточно новая сфера и продукт еще не как бы у них нет опыта таких введения таких задач как люди учатся решать такие задачи они учатся на ошибках если вы не хотите чтобы ваш проект и любимый был ошибкой и кто-то на не мучился то надо как вовлекаться и брать ответственность на себя и учить продукта нормально ставить задачу ну как вы это умеете как вы знаете выработать ники чек-листы некий политики вот это здорово помогает потому что каждый раз если я дергаю или мои коллеги меня дергают к приходит как новый интересно задача мы все бежим делать и вот все вот это шлем с рассказал я забываю вот но этого для этого важно иметь его в плане код чек-листы чтобы себя одергивает окей дальше данные собственно данные то очень супер важно в машинном обучении как я уже говорил на первом слайде что вот какие intent все учатся то есть мы данные подаем модель на выходе метки и все вот она сама выучиваются поэтому данные это прям супер critical и для deep ливинга для совершить для нейронных сетей соответственно чем больше данных как правило подаешь тем лучше вот этот график и синий а это означает что deep лёнинг обычно имеет кучу профита вот в лифте данных вот а-стар алгоритма где-то уже скотт момент они полудня не могут получить профит вот ну да дата сайта обычно в машинном обучении грязно их размещали люди они очень сильно ошибаются то есть нам кажется что как бы все понятно и люди все отдельно разметить но нет они часто невнимательный очень много ошибок и мы пользуемся такой техникой шибером данные которые у нас есть обучаем на них модель а потом с помощью этой модели прочищаем данные и повторяем все заново то есть такой клуб мы делаем наши давайте подробнее на примере того же самого распознавания лиц вот ну допустим откачали вконтакте разных пользователей все все в от арки эти пользователи собрали вот у нас есть пользуется вот у него там сколько 544 от арки мы из него выделяем лица которые есть на изображение и прогоняем через reccagni шин получаем им бединге с помощью которых можно их кластер заводь ну то есть мы склеили с помощью нашей модели похожие лица и выбираем только самый большой кластер то есть из предположение того что аватарки пользуется ли в основном про него ни про кого то там то есть там не рандомно иллюзия вот он средстве на весь муслима таким образом вычищаем и после этого мы можем повторить снова этот лук то есть мы снова можем на прачечных данных обучить модель с помощью этой модели прочисти данные этого еще очень круто и можно повторять до сходимости там 1 5 например второй пример очистки данных никогда данные грязные внутри класса когда есть много классов и некоторые из этих классов похоже вот например мы делали поиск по изображениям и мы собрали некий dt сайт на основе семантически похожести на основе классического компьютеру vision и и вот у нас получились какие то классы их на самом 200 200 тысяч на самом деле получилось и вот первые три класса вы можете видеть это раз крашеные яйца ну как бы они в разных домах но это один класс по сути если взять и обучить стандартная модель куна вот если всегда на то есть взять эти данные подать в сеточку применить к ним метр cleaning этой об этом позже подробно расскажу и предсказывать эти классы то из этих дублей которых на самом деле там оказалось много получается очень плохо еще можно делать мы можем взять опять же обучить модель 100 года с тех данных которые у нас есть и запихнуть datasette в в это обычная модель получить им бединге mb денги такой вектор которые характеризуют то что изображено изображено на изображение и покласть рисовать и установить некий порог схожести то есть достаточно консервативный по которым можно объединить классы и все яйца положить в одну корзину вот это здорово улучшает качество и мы часто используем практически всегда искал для вот этой кластеризации алгоритмом комплит link лишь селинг это эротические алгоритм кластеризации в котором очень удобно можно задавать порог то есть как раз то что постоянно нужно для прочистки он генерирует сферические кластер а так как мы чувствующим метрическое пространство этих рейтингов то это важно и он имеет один квадрат что в принципе ok еще раз bottom line вот этого в части который рассказал продано это то что мы обучаем модель на данных и потом прочищаем данные и вот и тут все повторяем значит далее значит иногда данные настолько тяжело достать или настолько тяжело разметить что не остается ничего кроме как начать и генерирует вот это позволяет генерить огромное количество данных но для этого надо чё-то чё-то программировать значит самый простой пример это например оси распознавание текста хорошо бы развить и что вы понимали что такое разбить текст то есть вот у вас есть картиночка вам надо каждую строчку выделить каждое слово выделить каждый подписать это короче сто сто таких листов вам будет там две недели распознаватель у асессор это очень сложно вот поэтому очевидно что можно на генерить каким-то образом текстом как-то его по изменять ее выше модель на не мучилась и мы вывели для себя что я самый лучший удобная вещь для этого и топил это такая библиотека питания для работы с изображениями есть в ней есть все что нужно для того чтобы работать с текстом то есть шрифты и цвета и нужно каким угодно образом аршака лить ваш текст чтобы вашей сети была очень сложная она не при обучалась под простые вещи вот на иногда нам нужны какие-то объекты реального мира например товары и вот одна из этих картинок она автоматически сгенерированную какая левая правая левый шов провалил на самом деле обе вот они обе сгенерированы автоматически то есть качество офигенная если не присматриваться там в какие-то очень мелкие детали то отличить или в круговой реальности невозможно делаем это с помощью блендера это такая программа специальное которая но есть кто-то знаешь такой 3d max вот это open source на налог ну собственно это основное такое важное преимущество то что он open source и не надо платить там несколько тысяч долларов за эту программу и у нее есть отличный питона пик который позволяет всего например эти объекты полок накидывать различными способами конфигурирует рандомизированы получить огромный dtc тут этих футболок у него в качестве рендеринга используется ray tracing и достаточно затратно операция но зато она воды выдает отличное качество этот самый главный вопрос где брать модели как правило их на покупать вот но если вы бедные студенты вы хотите с чем-то поэкспериментировать то всегда из торрента вот я рекламируюсь и джипе русском вот там есть какие-то ломаные модельки их можно почерпнуть но понятно что для продакшена надо закупать или заказывать у кого-то отрисованы этим окей продано и все перейдем к обучению метры cleaning что такое метре клининг это когда мы хотим обучить нашу сеть таким образом чтобы она похожие объекты в метрическом пространстве складывала в похоже регионы если я здесь привожу пример с достопримечательностями и достаточно необычно потому что это просто задачи классификации у нас есть десятки тысяч достопримечательности мы просто классифицируем казалось бы на хрена здесь муж дмитрий калинин что же у нас здесь для того если вы пользоваться стандартными методами классификации например там софт макс мучить специальную функцию потерь таги ряд вот такие вот ну то есть три класса в метрическом пространстве разобьются на такой вот на такие точечки и можно видишь в районе нуля они очень близки и это создает на самом деле много ошибок и она больше хотелось чтобы точки как были более компактные для этого можно применять различные техники метры дмитрий клининга например так называемый центра us идея которого крайне простая значимы просто стягиваем точки к центру к обучаемому центру с каждого кластера каждого класса и в итоге все на становится гораздо более компактная пишется здесь строчек на питоне и работает очень быстро все хорошо вот но мы попробовали мультиметра к ним почти в каждой задачи применяем мы от пробовали кучу разных методов их достаточно большое количество и пришли к выводу что энгр saw макс это вот самый топчик который выдает самые лучшие результаты ну и собственно среди research комьюнити от считается стоит музарт значит что происходит вот допустим мы распознаем лиц и вот у нас есть два человека желтые точки одного человека фиолетовый другу но если воспользуетесь стандартным софт максом так будет какое-то разделение просто какая-то разделяющей плоскость ниже ними значит можно чуть-чуть сделать можно нормировать все mb денги и все веса и перейти на окружность и тогда можно заметить что за разделение классов уже отвечает вектор угол между классами и этим можно пользоваться одного такого деления не достаточно очевидно то что мы просто поиграли с числами ничего от этого не изменилась нужно каким-то образом за ford сеть больше угол между классами чтобы они стали более компактная делается это математически то есть мы видим что за разницу классов теперь отвечает косинусы и можно сделать такую фишку как затребовать боль усложнить задачу нейронной сети таким образом чтобы она думала что угол между точками 1 класса больше чем он на самом деле шпана их пыталась все больше и больше же значит их есть несколько вариаций этих курсов максов и все они как бы играются с тем умножать на м этот угол или прибавлять или умножать и прибавлять короче кучу всего сама состоит азарт итар тасс вот там прибавление но вот лучше всего работает ну просто так и на самом деле это достаточно просто встраивается в pipeline и очень просто понять чего происходит то есть даже без всяких сфер вот черно деле на счет обучаем офис recognition вот у нас джек николсон мы прогоняем через сеточку которую мы в процессе обучения получаемым beating дальше мы классифицируем то есть мы даем скорая насколько сеть считает что каждому из лиц подходит это изображение и вот фотография николсона 20 самый большой скоро отлично мы применяем к этому вот эту вот штуку с увеличением с увеличением угла и по сути мы заряжаем скорость 20 до 13 то есть мы усложняем задачу специальным образом для при обучения модели а дальше мы делаем все как обычно soft макрос центра перед это специальные штуки которые всегда применяется специально функцией потерь то есть обычное линейное слой заменяется на слой arctis который пишется в 20 уже строчек не в 1020 вот но дает отличные результаты и минимум его вверх и данного внедрения это очень круто и по итогу орфей случае всех там возможных ну практически на большей части задач и задач лучше всех прочих методов он отлично встраивается в софт макс задачи классификации и реально там улучшает качество то есть не обязательно ожидать задачах если конечно чтобы использовать метры клиент или похожих можно в обычных задачи классификации к их вообще в принципе большинство 2 чем я хотел рассказать этот трансфер лёнинг трансферы леоненко это когда мы хотим обучить сеть на одном задача например на классификации классов на изображение и взять эту обычную сеть и попробовать и чуть-чуть доучить на нашем тоски таким образом как бы мы переносим знание из одной задаче на друга и вот мы и делали как я уже говорил поиск по изображениям и вот допустим мы хотим чтобы вот наш клэри эта машинка и мы хотим похожие машинки чтоб на возвращались ну логично что надо заучивать надо брать сеть который уже училась на большом количестве изображения есть такие конкурсы под названием имидж нету и упал imagex dtc ты которым там миллионы картинок и на нем уже обученные на другие классы и может быть даже на эти классы уже обучена сети мы просто ее как бы да да тренированным и если мы применяем вот как мы наш первой итерации мы взяли эти данные почистили как я уже говорил дальше применили к нему метры cleaning фейс и к обучили все как все обучилась вроде хорошо начинаем тестировать смотрим за счет мы запрашиваем пеликана но к великану начали перемешиваться воробушки то есть embedding получился тематические верное это птицы но расово неверные то есть надо уж надо чтоб чтобы этих воробьев там не было при том что самое обидное в изначальном модели с которым мы до обучались вот это опытный magic он знал эти классы прекрасных различал и здесь мы видим вот этот эффект который свойствен всем нарастить а мы под названием catastrophic fucked in то есть он сеть при обучении забывает предыдущий task и она может у забыть ваше на porsche и это как раз то что мешает в данную задача достичь лучшего качества очень лечится это достаточно интересно с помощью техники под названием новых distillation когда одна сеть и учет другую наш как это выглядит что там у нас уже знакомый pipeline который мы оставили не трогаем и просто обучаем метры cleaning на классом значит вспомним что нас есть сеть которой мы предприняли вот это вот up on my jay's она выдает мы заморозили просто вычисляем и им бединге на всех фотографиях на которых мы учимся в нашу нашу сетку и она у нас генерит какие-то скоры классов их достаточно много на опытным и жестом пеликана воробьи машины люди вот все вот там вот этого много и она вода будет соответственно вот это на некое распределение чисел по этим классом мы от нашей сетке которые мы учимся мужем от подкупать отпочковываются и учим еще один embedding который специально подобранными just ask и он тоже выдает нам такие же скажи ему for team с помощью bain & rick ross and р.п. мы заставляем сеть выдавать похожее распределение этих вероятностей таким образом с одной стороны верхней части мы учим новый task а в нижней части мы заставляем сеть не забывать ее корни и как бы помнить о тех классах которые она раньше знал если правильно это сбалансировать в условно этом пропорции 50 на 50 то это позволяет оставить всех пеликанов топчик ее выкинуть оттуда всех воробьев вот когда мы применили мы получили целый процент в минаева лишь престижем то есть в точности и это достаточно много вот это короче если ваша сеть забывает task предыдущий task то можно использовать но возрасте лишь на эту отличную работу дальше дополнительные голову базовая идея уж простая допустим моющим физ-ре корнишоны вот у нас есть вдт сети набор person но также часто вот эта сетах есть и прочие характеристики лица например сколько лет какой цвет глаз какая национальность и там и все этот проще это все можно добавлять как еще один сигнал и учить отдельные головы на предсказания этих данных таким образом у нас сеть получает больше разнообразного сигналы может лучше выучить основной task к примеру мы опять же возвращаемся к detect очередей вот стоит толпа ее нас все детектив и часто толпы вот такие то есть видно что только голову в этих толпах и в dtc также помимо размеченные тушки есть ещё и голова это можно прекрасно использует то есть у нас получается сеть распознает и тело полный полный баул windows и голову и такой у нас такой трюк на дал пол процента выберешь при сижу на это тоже достаточно малого практически бесплатно и на inference и то есть на в работе на продакшен мы вот это дополнительно голову которая предсказывает голову мы ее просто отрезаем она ничего не делает и вообще без эффекта на продакшн более сложный и интересный кейс с озером распознаванием текста с изображением значит стандартный pipeline такой вот у нас есть постер с пингвином на нём написано некий текст мы с помощью detection и выделяем балдин боксами этот текст дальше мы это текст подаем на recognise об этом месте с подробнее расскажу и он уже нам генерит текст на основе все это поэтому я здесь выделил что допустим наша наша сеть ошибается и вместо y penguins гинер или очень похожие букву это на самом деле очень распространенная проблема в озере когда сеть путает очень похожи между собой символом и вот вопрос как как этого избежать как вот пинк пингвинз при перевести penguins тоже мы люди когда на это смотрим на очевидно же ошибка потому что ему здесь пингвин написано не вот это вот что то написано соответственно каким то обычно это делать так каким-то образом встраивают модель словах word моду в либо inference сетей либо при обучении соответственно таким образом мы придаем кит статистически знания тексте в саму модель как мы для этого использовали штуку под названием bp и это такой алгоритм компрессии который вообще был изобретён то в 90-х нет для deep линга и он щас очень популярен и используется в deeply ринге уже смысл в том что он заменяет частые подпоследовательности в тексте на новые символы знаешь как это работает допустим у нас есть такая вот строка и это все что у нас есть и мы хотим ее от байт прн кодить вот что что мы делаем мы находим что символы а а два подряд там самые частые допустим в нашем в нашем слове мы заменяем их на новый символ z повторяем операцию дальше мы видим что бы у нас самая частая последовательность под последовать заменяемая на y теперь у нас z и y самое часто под последовать мы и заменяем на x и в итоге мы таким образом кодируем некую статистические зависимости в распределении текста и если мы например встречаем слова в котором какие-то очень странные подпоследовательности или их вообще нету то значит вот какой то не такой слов знаешь как как это встраивается все в распознавании вот мы выделили слова пингвин и запихнули его все вручную нарастить которая нам выдало пространственный имбилдинг в котором вот это вот координате вне закодированные данные про пространственные символ ждали мы используем рекурентные нейросети это такие специальные сети для работы с последовательностями мы используем конкретно трансформер даже не рекуррентная сеть и она нам генерит некие скрытые состояния вот эти вот зеленые столбики в каждом столбике зашита распределения вероятности по мнению модели какой символ вот этом отрезке изображения и дальше с помощью сети сей некая специальная штука за рамками моего доклада мы раскручиваемся все эти состояния и получаем наши предсказания с ошибкой вместо и на значит мы хотим напомню что отойти ну отойти от символов к словам и поэтому мы отпочковывается от состояние в которых защиты информации символах и натравливал еще одну рекуррентная сеть который уже как бы должна вышел и валом начать распознавать и она уже в свою очередь predict этот эти bp и случае вот это ошибочный последовательности получается 3 бы поешь keeping ю л н с что значительно отличается от реальной последовательности который для пингвинз то есть она пингвинз то есть если мы посмотрим с точки зрения обучения сети то сеть в качестве букв ошиблась только в 1 из 8 то есть 12 половиной процентов ошибка а в терминах папой она ошиблась сто процентов это гораздо больше сигнал сети что что-то не так и надо исправить свое поведение и когда мы это впилили мы получили но мы исправили это подобного рода ошибки получили в weight количество ошибок на словах а минус 0 25 процентов достаточно много когда-то шлифуешь там девятки после запятой вот так что это очень круто работает вот это дополнительно голову она естественно принтер насти сносится то есть вся основная сеть выучилась лучше за счет вот этого всего в целом both online at pro головы то что их всегда можно добавлять как правило не практически всегда приносит некую новую информацию некий новый пункт в модель она обучается лучше а во время инферн сайт ничего не стоит а не откидываются очень крут последние про обучение что хотел сказать это впп 16 осеклась по классике значит так исторически сложилось что сети обучались на gpu в единичной точности то есть в p32 но со временем все поняли что это избыточно и например можно инферно сделать на фото 16 то есть просто сделать play на точность все вычисления и качество моделей от этого никак не страдает но при обучении это не так значит если мы просто возьмем и посмотрим на распределение градиентов информация которая обновляет наши веса при распространении ошибки то мы увидим что в нуле огромный пик я все еще много значений рядом с нулем и если мы просто переведём все viso в 16 то получится что мы отрежем вот тут левую часть от красной линии в районе нуля то есть мы будем обнулять очень большое количество обновлений градиента на лени весов а правая часть вы раньше впп 16 рабочие она вообще никак не используется то есть очень сильная концентрация v 0 и в итоге если обучать простуд в тупую на впп 16 тут будет серенький график то есть мы учились ошибка падал и падал а потом взрыв и нихера не работ вот а график который снизу он как раз от папы 32 и впп 16 с обучением так называемыми экспрессивен значит использует два трюка значит чтобы просто умножают на 128 то есть если мы возьмем вот этот зелененькие графика переместим чуть в эту сторону то в принципе в p16 фонтен поинты хватает на то чтобы обрабатывать ведь числа то есть мы с гелем таким образом все градиенты именно мы стелим лосс иначе в самом начале и также мы храним мастер версию весов в p32 которые используются только для обновления а в самих во всех операциях вычисления форвард и бег от пососи тени никак не используется значит все это хорошо вопрос коктейль использовать как это реализовать и начну видео сделал своему мы используем python есть разные фреймворке мы используем подпоит очень удобно и очень популярны и и nvidia сделал для него специальную сборку с так называемым апексом так как раз от тебя скрывает почти все все что там происходит и у нее есть два режима это automatic expression это вот как раз код который написал то есть при обучении добавляется буквально две строчки который оборачивает лосс и процедура инициализации моделей оптимизаторов и что по сути делает м.п. он monkey патч от все функции то есть например он видит есть функция свертки она получает профит от fascination получает то давно и заменяет на свою которая сначала делает кастов по 16 а потом делает вычисления как он делает так aimp отделы для всех функций которые которые могут использоваться в сети для каких-то не делать потому что нет профита вот так же второй вариант но это очень удобно в принципе 99 процентах случаев это всегда подходит не требует никакого вверх и да все очень просто есть еще в 16 optimizer если для особо упоротых вы хотите все контролирует ваши сетей можно вот этим заниматься и не так-то просто с этим в 16 optimizer в нем огромное количество ограничений но зато можно получить больший контроль мы провели сравнение для обучения при обучения сравнялись мы на карте теста т4 здесь мы можем видеть вот на inference и у нас как бы ожидаем ускорение в два раза то есть мы sp32 перешли в 16 вся память ужас два раза все вычисления ускорились два раза примерно вот но при обучении мы видим что вот этот фреймворк апекс он дает относительно без него если работать то он даёт 20 процентов ускорения и в итоге мы получаем обучение которое в два раза быстрее и качество обучения при этом никак не страдают то есть вообще халява надо пользоваться чему чего чем я рекомендую заниматься при обучении сетей так сорян inference то есть вывод машина на продакшен вы модели на продакшене о чем подробно рассказала или она на предыдущем доклады и ананаса чему рело что мы приносим я куча framework of и в какой то момент когда появился по этож мы начали приносить все меньше и меньше фреймворков но python плохо тем что непонятно какого деплоить вы даже с одним из докладчиков ягод только что говорил он запрещает своим разработчикам запрещал прогремели до нашего с ним разговора использует python своим специалистам по машинному обучению то что непонятно какого deployed вот есть несколько вариантов как мы это делали полтора года назад появилась у nn exceed специальный фреймворк для конвертации модели между различными фигурками вот и можно использовать и на на x плюс к подавая но на extender атф и теперь можно использовать пальто 6 + + давайте разберем каждый из них значит one x и kfa2 каффа 2 это еще один фреймворк очень смешанный с поэтом потому что разрабатывают facebook и и на самом деле все это очень сложно все это связка очень сложно потому что потому что развивается гораздо более быстрее чем кафе 2 и в кафе 2 тупо отстает по фича мотает арча поэтому вообще в принципе не каждую модель которые обучили в поэт арчи можно конвертнуть в кафе 2 и приходится часто переучивается с другими слоями чтобы вообще мощь конвертнуть во вторых там нет ну я здесь отразил что нет операции об сэмплинг газ не раз ней был interplay шин вот и приходится переучивать но это большая тупостью там нету это довольно стандартная операция вот приходится все время переучивать и все это приводит к тому что чтобы конвертить модели мы завели под каждую модель специальным докеры мышь в котором мы описываем прибиваем все версии чтобы когда в очередной раз обновиться какая-либо из версии мы не тратили время на их совместимости еще что то то есть usb 1 под каждую модель 1 докеры мешает мне очень удобно есть счет тендер rt это такой фреймворк от envy леди от invidia которая оптимизирует инферн сити и там оптимизирует различные операции в архитектуре и если мы посмотрим на левую часть этого python синенький столбик и т32 в зеленый fat16 ну там почти двухкратно ускорения и но если мы посмотрим на тендер от эту там 4-кратное ускорение ну прям это заметной разницы не заметишь мы это тестировали на теслу это 4 это новая архитектура видеокарта от invidia где появились так называемые тендерные ядра которые как раз очень хорошо утилизирует вычисление фото 16 вот то есть разница в два раза если вас есть каретный модель которая работает там на десятках видюх то как бы есть все шансы все мотиваторы попробовать тендер т однако это тоже имеет свои проблемы потому что там еще больше более чем в кафе 2 и там еще меньше всего поддерживается там еще чаще все меняется в смысле обратная совместимость короче там очень плохо каждый раз надо немножко пострадать чтобы конвертнуть но однако и если модель требует того то есть она достаточно широко используется у нас mail.ru такое было часто нас есть очень супер нагруженная модель это тензор то имеет смысл использовать и немножко пострадать ночной последнее это python часы + + полгода назад разработчики порча осознали всю боль у людей которые используют их фреймворк и выпустили специальную штуку под названием торт скрипт которая как бы конверте 3 тыс реализует подходящую модель статический grub значит мы сразу же начали ей пользоваться сразу же словили кучу багов с обители разработчикам о них починили через пару месяцев и в принципе сейчас ads from work достаточно стабильный практически все модели можно от converse дам единственно не хватает документацию и там малой и плюс веком несложно то что они пошли ваши файлы посмотрели в суши нам нужен а вот тем кто работает на питанием тяжеловато но зато там реально идентичная работа с питоном там прям в плюсах будет запускаться от жителей интерпретатор питона и вот там будет запускать форвард паз и вашей модели что гарантирует идентичность плюсов питон поэтому теперь переходим к одному этому free word подведу итоги о чем я сегодня говорил о том что постановка это супер важно и надо с продуктом общаться обязательно данных и перед тем как начать делать задачу желательно иметь уже готовый to set на котором в финале проверяться значит сами данные мы чистим с помощью кластеризации обучаем модель класса рисуем данные чистим повторяем операцию с обучением метры cleaning помогает очень много где если вы делаете трансфера лёнинг то есть при добыче на какой другой сети то если вам нужно чтобы сеть не забывала старый тоска использовать и много distillation также используйте несколько выходов для сети то есть несколько голов которые будут учиться разных сигнал чтобы улучшать основной task и флеш-натс я рассказал что надо использовать акс от invidia сборки брэд питт арчан иной принципе используют потушите плюс плюс мне на этом всё я надеюсь что полученный от меня inside какие-то практики вы сможете применить на практике и сделать ваши модели лучше чем же еще можно сделать да просто лучше этого до статьи мир уже лучше спасибо эдуард прям так сразу мы сегодня книжек не даем ты в курсе вопрос ответа на вопрос важнее это правильно а привет ещё раз у меня зовут антон видать уже верх над выставляться записал русанова вопросы могу с обычных вопросов с каких хорошо провокационный вопрос а вы работаете на продукт продукт который допустим вас есть mail.ru облака там почта она чик данных тренируйтесь как правило науку собственных если ну то есть мы не можем например залезть в данных почте если нам надо обучить модель документов распознавания там паспорта или еще что то мы не можем залезть почтово дано нам тупо не дадут безопасники то здесь как бы даже если мы очень хотели мы не можем это сделать поэтому мы идем в google в другие вопросы собственные вещи я вкачиваем их и там где-то находим данные обычно мы так поступим со сном данного пан царственная либо нам их предоставлять случай би ту би например сам клиент может там не знаем предоставить фотки нам вес у него проходная с номерами где машина открывать шлагбаум он может предоставить на фотке номеров или мы можем об качать экс каких-то сайтов обычным и так дел а второй маленький вопрос вы показывали модели которая там central los angeles автомакс а вы не пароля архитектуру поставить на съемках на тему пробовали но это обычная не особо что дает ну то есть мы несколько раз это пробовали вот особого эфире там также пояснять сам по себе он построен на этом он как бы работает круто ну фас не помешаешь триплет волосам но до сути но это по сути же одна сеть но ты просто пропускаю за одну сеть двоим вендинга да ты про это до нее не работает так хорошо ну это по сути примерно то же самое ты что ты независимо про гонишься за ну сеть что 2 ну то есть два выхода сыпь но ты можешь в одну сеть встроить вот эта ложь либо может на большая на вертеть триплет lose our fails или чего угодно глобальна разница от сельских сетей честно я не вижу мы их несколько раз пробовали какого-то именно профит от такого с этапа мы не увидели то есть если вот отвечать на вопрос станет и последний для acer как artisan сленгом с чем смутил янгом разные языки вас документ копыта не знаю а вам надо определить и если что-то написано вас идет русский допустим вас скорее всего английский добавляя должна здрасте у вас китайский да у нас для этого есть еще одна голова в голов не бывает мало как бы которая предсказывает для каждого символа и для для каждого символа она предсказывает его язык и мы его используем при inference и спасибо кейт дальше еще вопрос у нас понятно лидеры по вопросов привет я запрещаю запрещал пай тож не потому что его deep ловит сложно не только поэтому а потому что в нем сложнее легче сделать перри мудреное решения и он гибки да и иногда это плохо когда надо быстро пробежать даёте услышу нога а вопрос про тензор то или комментарий я не знаю вот видео в своих бич парках очень сильно упирает на то что когда они показывают bench по тендер т они это делают вот с фиксированным окно полотенце и без не и и это позволяет им еще сделать x2 потому что они могут больше объектов упаковать в они батч да то есть вот если вы сравнивали нет окне мы сравнивали прямо один в один начали ну то есть берешь подаешь 16 до понял вот трафик мы сравним так еще ждем руки подняты вверх все уже устали до у нас еще один доклад следом ну а начинаю традиционный свой обратный отсчет 321 спасибо и двора сила замечательный доклад"
}