{
  "video_id": "mQiS-_fzTbc",
  "channel": "HighLoadChannel",
  "title": "Паттерны проектирования приложений на Apache Kafka / Александр Сибиряков (Scrapinghub)",
  "views": 7021,
  "duration": 3004,
  "published": "2020-04-14T10:47:20-07:00",
  "text": "на самом деле я этот доклад рассчитывал на очень маленькую аудиторию но потом не получилось вот значит смотрите прежде всего это моя разработка значит мы много делали разных решений на кафки и тем самым мы обработали обкатали несколько разных хороших так скажем паттернов да и теперь я их просто собрал воедино обобщил и собираюсь вам представить представили это все на примере нашей наши архитектуры и хочу вас попросить чтобы вы это все брались небольшой не большим скепсисом потому что мы не заходи мической тусовки у нас не получается так все классно выкристаллизовать как это получилось в банды четырех наши паттерн это скорее знаете рецепт как пельмени приготовить вот но вкусно и качественно давайте начнем значит вот наш продукт cup of the extract на входе url и на выходе структурированные продукты статьи вот такая штука здесь вы можете увидеть значит название продукта описании ссылку на основную картинку другие картинки предложения да там есть цена валюта доступность этого продукта артикул бренд ну и остальные свойства тоже значит то в такую штуку получить нам нужно загрузить страницу в браузер здесь вот мы найдем заголовок найдем картиночку цену вот эти все вещи которые вам перечислил все это должно отрендерится и попасть в ней раз сеть у нас есть это может использоваться для например разработки сервисов персональных новостей для меди медь мониторинг агентств упоминаний по отрасли региону компании конкурента сбора упоминаний продукты используются в ритейле для сбора сбора цены и доступности и ну по сути главная цель это как бы мне по эффективнее выставить цену значит внутри как я уже сказал нам нужно скачать тренды ведь это все в браузере применить нейросети извлечь микро форматы и сочтем или и в итоге мы получим структуру с полями требования у нас идут от бизнеса и они следующие эта штука должна масштабируемым быть должно быть завернут в сервис мы бы хотели иметь производительность 10 рпс на момент запуска и в перспективе и и отмасштабировать достар ps она должна быть прозрачно то есть поддаваться какой-то отладки должно понятно быть что происходит внутри должна быть предсказуемая задержка это обеспечивает успешный пользовательский опыт и так сказать изятоп шин ну и должна быть модульная да то есть мы должны иметь возможность легко что-то воткнуть новый заменить компонент и так далее ну и понятно надежность если мы работаем надежно у нас все хорошо в жизни не болит голова и наши клиенты счастливы технологически стык мы выбрали следующий мы взяли кафку от контент клауд как уже вам хорошо рассказал предыдущий докладчик готовить кафку хорошо это нужны действительно иметь хороший опыт для того чтобы делать ну для всех критических компонент мы взяли джаву и нативный клиент везде где у нас работают до это сантис и нейросеть вот мы там взяли питон потому что это очень комфортно для дети сами теста и взяли вот клиент не конкурентов ска именно кафка python все это работает у нас угловом купер на this engine вот так выглядит наша архитектура на входе у нас http запрос от клиента он попадает выпей сервер затем он попадает в роутер где применяются правила которые написал аналитик затем это все выпадает приоритетную очередь где вы запрос придерживается до тех пор пока у нас не освободятся бра zerg только браузер и освобождаются он нам попадают браузер и начинают происходить загрузка контента рендеринг исполнения java скрипта в итоге мы снимаем snapshot все это отправляется в нейросеть и в извлечении мир некра форматов делается это независимыми маркерами и оттуда происходит слияние данных мер джерри и в итоге возвращается ответ назад и пиа сервер кафка здесь работает практически везде и такой сценарий мы называем request тёплой связь компонентов осуществляется вот таким способом у нас есть процесс в котором есть один консьюмер один продюсер у этого процесса есть входящий топик есть исходящий топик есть какой то бизнес логика элементы этого сценария следующий обязательно есть точка входа куда принимаются кафка запросы есть обязательно . выхода да куда отправляются ответы связь через отдельные топики кафка сообщения мопед цена 1 кафка запрос как один к одному это очень важно потому что у нас есть вот эти внутренние механизмы кафки связанные со смещениями и это избавляет от уйма головной боли дальше разберём откуда эта головная боль берется prequest каждый содержит некий до индификатор и это как бы в итоге позволяет нам реализовать логику опять же дальше коснемся какую именно преимущество перед описи состоит в том что вот вас вашим компонентом совершенно не нужно заниматься обнаружением соседних компонент с которыми они должны коммуницировать и вообще весь сетевой слой для вас будет абстрагировано кафка клиентам де-факто кафка клиент принимает на входе некоторый конфиг который статичен и в общем то все на этом да то есть для микро сервисов которые у вас работают кубер нить из это существенно упрощает жизнь недостатки тоже достаточно существенные графиков к кластеру пропорционален количеству компонентов ну представьте себе мы получаем один запрос почти типе сервер и потом мы его должны переслать через брокер столько раз сколько у нас компонент да то есть с у нас их 6 соответственно шестикратный трафик идет на брокер ну прямо скажем это подходит не для всех и для всех задач также если у вас есть необходимость скажем повторить этот запрос зациклить его внутри то есть риски что он просто-напросто будет зациклен там вечно до создавать постоянно нагрузку очень важно иметь механизмы защиты и мониторить такую ситуацию также очень часто например хочется скажем добавьте мне новое поле выше типе запрос мы его добавляем понимаем что у нас есть некоторая схема которая заведено в каждом топике и эту схему придется обновить в итоге 3d play всего вот такая проблема ну и есть очень важный момент с тем что не у всех языков хороший кафка клиент и это в итоге выливается в ограничение на выбор платформы теперь несколько слов собственно о паттернах вот мы сейчас выделяем из этого архитектуры первый паттерн почти теперь к сервер значит требование у нас здесь были такие хотелось построить и 5 сервер для внешних клиентов работать он должен протокол http 11 до в перспективе 2 общаться формате джейсон и уметь должен принимать пачку урлов здесь вот фундаментальный момент есть да то есть мы стыкуем 2 по своей природе разные технологии и чтить и пион синхронный всякий раз когда вы отправляете запрос вы ждете пока придет ответ в кафки есть консьюмер есть продюсер эти сущности работают независимо есть вы можете отправить сколько угодно запросах пригонять сколько угодно за цветов совершенно независимо друг от друга и отсюда возникают всякие сложности да как не как правило вот есть необходимость вот этого серверного контекста есть необходимость держать открытых соединения происходит следующее приходит ощутите пи запрос он помещается открывается новое соединение приходит у него запрос это соединение сохраняется в серверный контекст потом происходит отправка запроса восходящий топик до и после этого процесс ждет пока не придет ответ во входящий топик соответственно когда приходит ответ он снова находит то соединение куда нужно отправить ответ и отправляет его туда один запрос у нас выглядит вот так у нас есть url есть тип страницы до то есть условно горит тип извлечения которое нужно применить этих узлов запросим может быть много все это трансформируется в несколько кафка запросов внутренних то есть у нас вот так вот каждый кавказа просто однако вко сообщения на то есть таким образом результаты на эти запросы они могут придти в разном порядке в каждом сообщении у нас содержится идентификатор кафка запросов идентификатор соединения и номер партиции куда прислать ответ вот эти три вещи они составляют основу паттерна дальше идут наши специальные штуки у нас там есть url идентификатор пользователя есть домен тип извлечения аспекты здесь следующие в этом паттерне которых нужно потом подумать нам надо подумать как именно отправлять ответ как это все будет масштабироваться и что делать если происходит отказ кафки значит для отправки ответа просто-напросто сделаем отдельный отвечающий thread в нем будет работать постоянно консилер из топика с ответами будет происходить поиск соединений всякий раз когда приходит ответ будет генерироваться джейсон и ответ будет отправляться по частям значит тут вот есть такая ремарка да дело в том что стандарту нам предписывают или же знать размер ответа который мы должны будем вернуть заранее полностью до или же отправлять танками до или же отправлять не знать ответ не отправлять щенками отправлять данные а потом просто закрыть соединение после отправки все что пришлось хорошо в нашем случае у нас урлов много размер данных предсказать не извини возможно потому что мы не знаем что мы скачаем и ответ может быть очень большим чтобы эвакуировать да поэтому самым разумным просто-напросто отправлять танками как только пришел конкретный результат но сразу его весь отправили все масштабирование вот сделаем несколько процессов и пиа сервера и на значимых жестко на партиции у нас есть три процессы 5 сервера и есть 9 партиций всякий раз отправляя запрос и 5 сервер запишет туда номер партиции куда ему вернуть ответ вот такая схема она позволяет в общем-то крутить ответы так чтобы не потерять их и вернуть их пользователю минус в том что оно нифига на лету не масштабируется вот сценарий отказов кафка смотрите ну так может случиться что один из брокеров просто у нас потеряется как было рассказано что тогда случится мы приняли какие-то запросы они у нас там в плане и не могут вернуться назад выглядеть в этой ситуации тот клиент у которого были тайм-аут и он соответственно палица по таймауту тот клиент у которого таймаутов не было будет долго висеть в конечном счете все равно отрубится нашим тайм-аутом на сервере что случится если мы не можем отправить кафка запрос но тогда принял успешной отправке в топик пошлем сразу же приписать клиенту сразу и закроем соединение то есть в таком случае мы даем явно понять клиенту что у нас тут не порядок и мы ничего не можем сделать поэтому очень важно калмыки от метода стенд в продюсере обрабатывать элементы паттерна следующие обязательно ищите pi server стыдом от прав щекам ответа есть необходимые поля в сообщении идентификаторы к запросы соединения номер партийцы жесткая привязка partition и варианты у этого паттерна тоже есть следующие можно отрезать например кафка запросы да тогда получится вариант когда вы отдаете проекцию данных через и чтить и пи условно говоря процесс делает потребление но консью мид с какого-то топика строит проекцию данных например flash мопеда ее через ищите пед дает без ответа через кафку да то есть можем отрезать ответы в нашем варианте соответственно тогда это получится просто напросто сбор данных через а че тебе то есть вы можете принимать какую-то телеметрию сразу платьев кафку следующий паттерн да это ищите типе с другой стороны почти теперь форме клиента задача тут следующее у нас вот в нашем наши постановки у нас есть браузер и да но я решил обобщить и и для вас для того чтобы она была более применимой более понятно да поэтому я решил убрать браузер здесь оставит здесь ищете пи клиенты в нашем случае обращаемся к веб-сайтам почти типе клиенты они могли бы обращаться к сервисам на то есть сервисы могли бы быть какой-нибудь фейсбук пиксель яндекс метрика google analytics все что угодно везде куда вы можете послать все свои асинхронные события как правило у всех сервисов есть свои штучки им нужно ограничение нагрузки соответственно нам тоже бы хотелось бы не плодить много этих очистите пи клиентов и просто напросто эффективно использовать каждый вот такой клиентский процесс а чтобы нам в общем не засунуть все в один топик и не раздавать оттуда что тут в общем мудрить то получается вот такая фигня как правило наши события они приходят вот такими вот с копами до большими пачками причем неравномерными и получается что если у нас есть один из степи клиент он он просто-напросто работает на не очень эффективно сначала начинают посылать запросу кастла потом посылает запросы к ступе до при этом делая паузы между запросами потому что ну нам же нужно быть вежливым по отношению к сервису у нас есть в итоге нам хочется это все дело хорошо так перемешать до чтобы просто напросто не простаивать между запросами тогда получается вот такая картина и соответственно момент простое мы могли бы просто напросто пасовать кому-то другому запрос и вот она приводит к тому что мы такой клиент будет использовать сто процентов процессор на во времени это очень эффективна как это сделать да то есть у нас есть хранилище есть какой-то worker который пишет в это хранилище вот здесь у нас на картинке конкретно изображена реляционная база данных есть worker который читает из хранилища у нас есть топик из которого будут читать клиенты в этом топике количество партиций пропорциональное количество клиентов и каждый клиент жестко привязан к своей партийцы при этом извлекающего are here by kevin квартир он имеет распределение по partition на основе хостов то есть один и тот же хвост всегда попадёт в ту же самую партицию таким образом у нас появляется возможность на клиенте реализовать логику вежливого входа и например хорошо расчитывать надежно дела и между запросами или наоборот скажем долбить этот сервис там в 10 соединение открытых там или в 20 если нужно тут есть еще один трюк вот если вот эту вещь проигнорировать до условия запуска выборки которая генерирует которая происходит вот в этот вот топик да вот нам очень важно в этот топик генерировать в выборку не сразу да а попридержать запросы в хранилище да для того чтобы у нас накопился некий материал для замешивания и вот для этого нам поможет вот такая штука у нас в диковинку квартире можно сделать отслеживание смещения консилера на то есть нам известен нам последние смещение сообщения в топике вот сейчас только что товарищ упоминал хай watermark это анна и известно позиция пенсию мир группы до на этой партиции соответственно мы можем вычесть 2 значения и получить лак на этом топике и средства условие выборки могло быть вот таким кто калак меньше кого-то порогового значения мы понимаем что консьюмер подбирается уже к краю топика тогда мы генерируем выборку вот это нам дает дает возможность просто напросто попридержать данные в базе до тех пор пока клиент не будет готов их разобрать и сразу же ему запулить максимально разнообразную выборку возможную на тот момент схема таблици могла бы быть следующий то есть у нас есть сервис праймари ключ понятно пусть реализованный запрос и номер партиции здесь могут быть и другие поля другие параметры по которым потом будет осуществляться выборка то есть еще раз алгоритм такой значит лак в меньше некоторого порогового значения и нам нужно выбрать запросы так чтобы в результате у нас размер пачки соблюдался да то есть мячик это фиксированный при этом у нас в пачке не должно быть больше какого-то разумного предела запросов к одному сервису да для того чтобы был было место для остальных сервисов тоже можете добавить какие угодно свои условия скажем миша сэмплинг по пользователю какой нибудь приоритет все что угодно тут может быть сделать это одним или несколькими select a mi там не обязательно иметь реляционную базу данных там может стоять тарантул hbs редис все что угодно да хочешь память и потом это все просто напросто как только вы выбрали запросы нужно их удалить после отправки в топик опять же отследить кал бег на вызов send элементы паттерна следующие хранилище которые позволяют гибкие выборки добавляющий процесс хранилище извлекающий процесс следящие за смещением клиента и чтить и пи клиент ну и вот есть трюк такой да на случай если вы вдруг на момент разработки интуитивно чувствуете что вдруг потребуется это потом может потребоваться это все потом масштабировать да вы можете назначить несколько партиций на один ешьте типе клиент даты позволит вам отмасштабировать это все просто увеличила количество клиентов назначаю меньшее количество партиций каждому клиенту ну это как бы вот этот трюк дальше фреймы фреймы больная тема для многих смотрите у кафки есть архитектурные ограничение на размер сообщения там для некоторых приложений это может быть критическим условием до передачи больших сообщений и в то же время как бы очень неудобно увеличивать допустимый размер потому что увеличивается давление на память в брокере всякий раз когда вы например говорите что теперь у меня максимальный размер сообщения 50 мегабайт брокер должен предала целовать буфер да на пачку сообщений каждая которые каждый из которых 50 мегабайт все это должно значит как-то проходить в итоге еще и в репликацию должна работать итоге на самом деле получается что исходные настройки брокера до потопа что он был остер оптимизирован он они уже становится суп оптимальные ну и просто это чисто не с точки зрения самой эксплуатации очень непрактично радость обычно как все происходит что то нафига чили выкатили начали потерять начались потери данных нашли разбираться почему оказывается большие сообщения не пролазят ну давай увеличим размер сообщения остановили всю кафку поправили конфликт снова запустили ну как бы не инженерный путь прямо скажем да поэтому это анти паттерна чтобы мы могли сделать сделаем надстройку над продюсером и костюмером с тем же и 5 продюсер делит большое сообщение на части фреймы и посылает окон сервер собирает из фреймов исходное сообщение вот так вот выглядит сообщение внутри кафки вот так вот она выглядит на уровне пользователя на уровне клиента то есть видите что у нас есть блоки где сообщение занимают больше одного сообщения внутри него кафки формат сообщения могут быть следующим у нас есть сообщение самой кафки ключ-значение да и в значении у нас могла бы быть обязательно должен быть номер фрейма количество фреймов на которые большое сообщение разделена но и сам вот этот фрейм ключ очень нужен потому что всякий раз когда вы наконец всем мире будете это сообщение собирать по кусочкам так может случиться что сообщение придут просто-напросто в в не определенном порядке да могут может перемещаться несколько ключей да и вам нужно будет передавать по ключу тут еще возникают довольно сложные моменты с со смещениями и всеми механизмами которые завязаны на эти смещения вот конкретно здесь все красным показал представьте себе что у нас есть консилер до который перес который был остановлен вот на этих вот смещения таким образом при повторном запуске с этого смещения да и лири балансе да когда у нас скажем консьюмер который читал с этой карте ци перестает читатель начинают читать с другой партиции то есть она переезжает на другой консьюмер у нас буфер накопленный просто-напросто пропадет да и это сообщение будет в итоге потеряно сложности я вот сейчас вам пример показал там еще есть куча вытекающих поломается во время ребаланса поломается во время сика автоматический commit тоже будет работать не очень корректно на больших сообщениях он может коммитить середину вот и и ridge он может удалить часть фреймов потому что претензию в папке работает на основе сегментов и удаляются всегда по сегментами может так получиться что одно сообщение в одном сегменте в сообщении в другом сегменте ну и в итоге это все заканчивается вечно висящими сообщениями в буфере которые на самом деле просто обработать нужно тем не менее вот видя все вот эти проблемы это не означает что этот паттерн как бы не применим да есть куча приложений где вот скажем потеря сообщения оно допустимо дань явление ничего серьезного не случится вот но тем не менее передачи больших сообщений требуется тут еще одна один прикол есть компакты топиком да но это можно порешать тем что сделать внутренний ключ помним да что compact это пики комп акция происходит по ключу всегда останется последняя версия сообщение с этим ключом вот есть ссылки по теме отличную презентацию сделано в linked in там они очень детально разобрали вот все аспекты вот этой темы и есть наша реализация мы ее обкатали в продакшене в open source пока еще нормально не выкатили потому что все времени нет если вы захотите посмотрите элементы паттерны в общем следующие нужно сделать отстройку на ткан семерым и продюсером можно сделать кроссплатформенная настройку почему бы и нет следующий паттерн в паттерн обработки ошибок ошибки у нас следующее чаще всего это потеря сообщение самая больная наша ошибка то есть пришел клиент он нам послал http-запрос а мы этот запрос где-то потеряли потому что у нас был написан кривой код который упал а когда процесс падает все что у него было в памяти соответственно нас потеряна и в данном случае клиент ждет а мы не знаем что происходит в нашем случае это очень часто происходил потому что браузер не выдерживал свой тайм-аут зависал лично что-то там делалось могло быть завершения аварийный компонент заканчивать вечным ожиданием на клиенте поэтому очень важно вернуть ошибку средств решения у нас для этого просто следующее мы сделали общий топик ошибок все компоненты пишут свои ошибки туда потом этот общий topic ошибок собирается мейджором и если ошибка пришло от кого-то раньше чем пришел ответ вы просто напросто отправим эту ошибку сразу же вы это сервер для того чтобы обеспечить успешный опыт клиенту этот компонент по совместительству еще делает мониторинг таймаутов от ситуация происходит когда у нас запрос систему попал на при этом у нас он где-то был проглочен и мы в итоге снова его или потерялись или слишком сильно долго с ним задержались вот мы занимаемся еще отправкой ответа построена на кавказ 3 мс там делается join на входе в процесс есть топик запросов топик ошибок и топок с извлеченными данными на выходе один топик с результатами логика такая штатной ситуации происходит слияние двух сообщений от наших алгоритмов извлечения в одной отправка ответа в случае ошибки генерации сообщения об ошибке отправка ответа а вот если нет сообщения в течение двух минут тогда тоже происходит прав к сообщению и тайм-ауте на этим самым и себя охраняем от ситуации когда мы потеряли внутри запрос мы стараемся качестве написать код везде всегда отслеживать цент от продюсера вот но тем не менее такая ситуация происходит и чаще всего оно происходит не потому что наш код где-то глючной да потому что один из компонент который мы используем он где-то задержал запрос в какой-нибудь очереди элементы паттерна следующие есть общий топе куда пишут свои ошибки все есть компонент смотритель он сливает запросы ошибки и проверяет и мало периодически если нужно эту штуку можно использовать для мониторинга активности компонент условно говоря задача следующее у вас есть компонент который должен стабильно писать какие-то сообщения доставлять последние данные каком состоянии соответственно нужно это отслеживать вот делается вот такой контейнер который периодически подписывается на топике проверяет а пришло ли сообщения во время ли оно пришло так далее еще один паттерном протоколирование трафика вот смотрите пример logo это наш лог прямо из production а вот мы здесь видим слева время вот здесь мы видим что пришел нам url в белфаст телеграф в 50 минут пять секунд потом роутер это значит он уже вышел из роутера дальше он попал в download fit это значит что он уже вышел из очереди и улице и хранилище виде реляционные базы данных это заняло 200 миллисекунд потом мы видим что он было тренды рен и вышел из браузера это заняло три секунды дальше запустилась на нем извлечение до который сгенерировал а три сообщения и все это случилось в течение еще 4 секунд 4 секунд и потом практически сразу же мейджер выпустил сообщение с результатами в стороны и 5 сервера вот хочется уметь такой лак это супер практичная штука для отладки каких-то потерь и вообще всего что происходит частенько к нам менеджеры приходят и говорят ну чего вот я посылал запросы и отвалилась с тремя лотами где чего покажите мне мы открываем логе греппа им по его запросам показываем чувак вот смотри мы все сгенерировали вот смотри вот логин generic сада под твоим запросам все тебе ушло у тебя кривой клиент иди разбирайся все прекрасно понимаете когда у вас есть вот такая штука соответственно задача стоит следующий много компонент время разработки у нас много потерь результате богов до хочется прозрачности решение такое сделаем один консьюмер который читает все топики системы опять же уже понимаем такие проблемы будут он пишет сразу два logo полный джейсон на случай если мы хотим прям детально поразбираться и краткий текстовый вот прямо с руками выбранными полями для того чтобы мы могли потом погребать по нему посмотреть проблем этот следующее очень большой трафик на это консьюмер там получается что опять же он пропорционален количеству компонентов генерации джейсона довольно требовательных циpкa но можно джейсон не генерировать соответственно элементы такие конден консьюмер которые читают все топики и некоторые преобразования необходимый формат запись мой доклад наверное был бы не очень полным да если бы я бы не упомянул здесь кавказ 3 мс потому что это очень объемный framework ним сэр хранится в нем реализованы уже паттерны виде прямо готовых решений но все это высокоуровневые вещи виде потоков процессоров топологий вот на них можно делать преобразование типа branch фильтр map то есть можно разделить поток отфильтровать поток применить к к нему какую-то функцию с джой нить 2 по то сгруппировать очень много всяких классных fitch можно проходить разными типами окон и делать агрегации на этих окнах можно делать агрегации по группировкам окнам позволяет хранить локальные крики value хранилищ состоянии с копии в топике все это очень удобно для генерации snapshot а для отдачей какой-то проекции но есть вот такая штука это такой вечно работающие сквер запрос который ждёт новых данных вот есть такая книжка уже перед докладом уже упоминали они есть на русском языке нам крупно повезло что она есть на русском языке вот давайте поговорим об общих принципов проектирования pipeline of вот смотрите очень важно чтобы пропускная способность последующих компонентов было быть выше предыдущих дело тут вот в чем как только у вас пропускная способность на том же уровне или ниже до или вы ее вообще не рассчитывали у вас может возникнуть ситуация когда у вас растет лак то есть где-то в пай плане происходит затык и какая-то часть pipeline а просто не успевают это решение будет постоянно требовать поддержки и постоянно требовать внимания чтобы этого не было вот простой принцип у вас пропускная способность должна быть последующих компонент должна быть выше предыдущих хороший второй принцип это составить таблицу топиков вашей системе до следить за тем что у вас куда пишется в каком виде как происходит распределение данных попортится понятно и правила рассчитать пропускную способность еще перед тем как вы что-то начнете писать код вы уже можете сесть написать в блокноте сколько данных к вам придет сколько эти данные занимают какой трафика не генерирует сколько записей вы ожидаете и сразу же прикинуть да сколько вам потребуется процессов и партиций стоит который стоят на этих процессах нужно помнишь перед тем чем удалить данные без уведомления то есть если вы где-то недосмотрели в своем пай плане там возник высокий лак то рано или поздно настанет момент когда данные которые не успели обработать вашими маркерами будут просто-напросто удалены ну и вот очень важный момент нужно следить и проектировать систему так чтобы был согласованный приход данных и разбор условно говоря как бы хорошо когда к вам приходит примерно с той же производительностью что и разбирается да если у вас скажем приходит большой объем данных быстро раз в сутки и потом есть у вас сутки чтобы этот объем данных разобрать нужно делать систему скажем с трехкратным запасом так чтобы случая простое вам пришли данные сам за двое-трое суток вы бы их успели разобрать вот так выглядит наша таблица топиков просто вам показываю для примера у нас есть шаг в плане есть компонент есть ходящий топик есть исходящий топик докуда компонент пишет есть то как устроен ключ да то есть это всегда за поедешь ник запроса часто и лишних запроса есть алгоритм разбиение поэтому плечу но кстати споткнулись а то что в разных кафка клиентах дефолтный алгоритм разбиение по ключу разный понимаете особенно это вставляют когда вы делаете приложение для кавказ 3 мс с использованием разных платформ и клиентов вот продолжение таблицы собственный алгоритм портишь энинга схема которая там принимается способ назначения консьюмер она этот топик есть когда мы назначаем жестко консьюмер да как выпей сервера есть когда мы просто-напросто подписываемся самым как бы у нас может быть либо лансинг и некий комментарии что там происходит вот сейчас несколько слов о том вообще как другие люди команды используют кафку я попытался посмотреть другие сценарии использования и подумать как-то оценить как наши паттерны которые вам рассказал могут применяться самый простой сценарий это и тел да когда у вас есть какой то объем данных вам нужно его переложить из одного хранилища в друга и соответственно кафка используется как буфер пример например вот загрузка из ходу по традиционной базы данных или загрузка там из реляционной базы данных ps3 здесь может быть также какой-то push во внешнее ищите сервисы могут работать вот такие наши паттерны архетипе клиент фреймы ошибки чаще типе сервер часто очень клавка используется как главная артерия все события большого сервиса пишутся в кафку пользовательские сообщения обновления контента клинике метрики добавьте свое есть асинхронный обмен данными между компонентами условно говоря какие-то компоненты пускают задержку при обновлении то есть например самый простой пример доставка push-уведомлений добавление объектов поисковый индекс агрегация кликов статистика какие-то отчёты понтифф роду все это и боже делать асинхронно такая штука работает ну это все применяется в линьки ты не его vita столько из того что мне известно дума еще очень много компаний и работу здесь вот такие паттерны internet of things представьте себе у вас есть большой производственный процесс conti печь и вы снимаете температуру давления с разных частей системы и все это пишите в кадку а потом происходит агрегация раздача проекции вот и такая штука работает например северстали для этого подойдут вот хорошо вот такие паттерны чтит и серые клиент ошибки есть очень большая тема сейчас его андрей менорке так что это когда в кафку или в любую другую систему обмена сообщениями записываются все состояния приложения до на какой-то момент который когда-либо у этого приложения были значит есть такие понятия как события они же нотификации есть команды с этими событиями есть запросы запрос отличаются от команд тем что они никак не меняют данные команды это как раз вот именно записать какое-то событие новое изменить состояние самый простой пример это например движении средств по счету то есть вы видите что там 100 рублей но в истории лежит например тот факт что там сначала пришло 200 потом было сто потрачено вот такая вещь сейчас набирает популярность это очень горячая тема я вам советую посмотреть в нее потому что очень круто иметь возможность прямо размотать посмотреть лог до всех изменений какой-то сложной системы и вот тут такие вот паттерн и до 5 же ищите пи клиент ошибки и протоколирования трафика тут должен зажечь просто просто очень хорошо распределённое приложение тоже интересный паттерн я подслушал подсмотрел вернее сказать в интервью с г-н шапира она продуктовый менеджер в confluence значит представьте себе такой какой-нибудь mmorpg world of warcraft вот и там у вас есть скажем до пользователя один в китай 2 в америке все внутри игры они находятся в одной игровой локации и вам соответственно нужно как-то их обслуживать соответственно с точки зрения разработки очень непрактично например китая китайского пользователя подключать в америку да то есть логичнее ему сделать локальный кластер да куда он будет ходить вот американец будет ходить свой локальный кластер при этом эти кластеры они будут соединяться через кафку друг с другом через com через но обмениваться событиями и так далее соответственно вот такая вот паттерн да это вот действительно может работать в мама lpg вот смотрите сегодня мы поговорили о таких паттернах request реплей в нашей постановки ешьте теперь к сервер а че типе клиент с очередью фреймы обработка ошибок протоколирования трафика ну еще несколько слов было об истории исков к стремимся это тоже офигенно большая большое количество разных паттернов ну вот напоследок вот такая цитата сказки от франца кафки она нам кратко дает понять что прогресс это не просто вера до прогресс еще нужно сделать спасибо спасибо большой александр друзья вопросы человек с микрофоном уже бежит к вам вот смотрите на верху мужчина в белом и дальше петербург готовьтесь я так понял что он новосибирска екатеринбурга нет вопросов готов будьте добры добрый день андрей компании яндекс спасибо за доклад вы показывали pipeline как ваши как ваши да мы показывали поплыть pipeline как данные проходят через вашу систему и все это время клиент подключены и папе и php api ждет ответа потом вы показали логе я так понимаю то что клиент ждет около 7 секунд так да к сожалению для того чтобы там сходить в интернет что-то от рендерить скачать около 5-7 секунд но мало какая система может позволить себе такую долгую задержку какую долю этих семи секунд занимает сама кафка я думаю там до 200 миллисекунд плюс это еще можно на ск накрутить на самих клиентах то есть можно зажать это время друзья еще вопросы поднимите руки вот смотрите да кого добежите 1 год будьте добры петербург тоже 20 что москве слова поехали спасибо за доклад меня зовут евгений у меня такой вопрос вот для большинства этих паттернов если в докладе заменить кафка на рыбе темпе у или что-то они же валидность не потеряют ну ребята по-другому немножко все работает но в целом про паттерны и про подхода не готов ответить довольно сложный вопрос потому что у ребят и как минимум ретенция сообщение по-другому устроен там много всегда там нет партиций ну то есть заменили на очередь партицию том что года то есть это реально надо обдумать чтобы ответить на этот вопрос и такой еще вопрос можно ли как-нибудь масштабировать вы говорите есть проблема с масштабирования и пиарь можем ли мы сделать входящий топике динамическими чтобы иметь возможность масштабировать ее 5 как мы хотим ну тогда придется сообщение тогда придется соединения пользователя перенести чтобы перенести вам придется сообщение приложение разделить да то есть это как бы уже сложно следующий вопрос вот пожалуйста здесь добежите быстренько друзья я вам хочу напомнить что на стендах можно подходить задавать вопросы наберитесь смелости и учитесь там без красивую розу на есть спасибо вопрос про консалтинг есть сообщения о том что из изменился стоит и это сообщение не влазят в топ топик по размеру вот как в этом случае быть может ли можно ли использовать в этом случае фреймы ну очень сильно зависит от того насколько ваша система все архитектуры устойчива к потерям к сожалению пока что вот в текущей архитектуре кафки если вы идете путем фреймов то скорее всего вас будут какие-то потери в каких-то экстренных ситуациях да но если ваша система готова потерям да да окей тогда можно если нет внешнее хранилище и еще один вопрос 21 ряда пожалуйста здравствуйте спасибо за доклад но частично вы чувствительны в предыдущем вопросе как раз хотел спросить по поводу фреймов фреймов новый дизайн есть недостаток как раз мы можем не собрать никогда этот фрейм да вот и просто как альтернатива в принципе фреймом и как бы вопрос использовали ли вы такой подход если у вас процент больших сообщений он не 100 а ну скажем там где себя в из общего потока в принципе недорого можно использовать внешнее хранилище для них и собственно у вас все проблемы отпадают но появляются проблемы саппорта внешнего хранилища как бы дали как бы другая цена получается да но за табой дизайн у вас там уже никаких проблем со сбором вот вы пробовали или не пробовали с внешним хранилищем мы не пробовали а фреймы пробовали и в целом если у вас все нормально настроена она работает но мы понимаем что могут быть по не зависящим от нас причинам различные проблемы финальный вопрос перед тем как уйдем в и пожалуйста привет саша кривощеков яндекс и да скажи пожалуйста уже упоминал что есть patterns in driving и тех чего power кафки но мы помним что есть еще и detection периоду ты сталкивал своей практике с тем то что нужно выбрать либо ты пишешь event максимально подробно то есть что изменилось ли body состояния либо ты пишешь сам факт что изменился такой the dish ник и как раз таки и идешь уже в 3 хранилище чтоб получить его изменения ну прямо скажем с такой конкретной проблемой не сталкивался на очевидное решение какие разложить по разным по разным топиком не от смотрю тут есть две стороны то есть если ты кладешь большой объем данных 50 на может не поместиться либо во фрейм либо же ты забьёшь просто себе диск мусорной информации и обратной ситуации если ты кладешь только один никто ты нагружаешь третью систему запросами и я так понял в практике ты с этим не столкнулся ну короче да внешнее хранилище мы не использовали вот прямо вот для хранения каких-то больших толстых данных вот соответственно да но в целом понимаешь можно попытаться скажем разложить это например например просто положительно диск блогом да и поднимать это спасибо я вот прям вижу будущее короткий мастер класс в дискуссионные зоне от александр сибиряков и через 30 секунд сейчас мы вручим памятные призы какой вопрос больше всех понравился помнишь сейчас тоже книжечку вручим вдв чем книжку про event ривен системы наверное вот тому кто про их спросил а где вы махнуть рукой вот-вот вариватт раз и вот два нужно разорваться так хорошо тогда вы просто вы сейчас выберите вам тамблер или книжку а соответственно вот вашему коллеге достаньте что-нибудь еще спасибо большое саше спасибо тебе тоже памятные призы"
}