{
  "video_id": "oWXaQrCNK-Q",
  "channel": "HighLoadChannel",
  "title": "StatsHouse: метрики Вконтакте / Григорий Петросян (ВКонтакте, VK)",
  "views": 622,
  "duration": 2826,
  "published": "2024-04-17T00:59:41-07:00",
  "text": "непривычно себя видеть таким большим так можно презентовью пожалуйста здесь друзья режиссеры презентавью друзья маленькая заминочка сейчас все начнется высокие нагрузки это иногда проблемы иногда ситуации в которых нужно что-то пофиксить Георгий Давайте попробуем еще раз и надеюсь что теперь все заведется Надеюсь сейчас подождем минутку чтобы здесь появился правильная картинка почти но нет стоило бы рассказать какую-нибудь шутку но я не заготовил шутки можем практиковать опрос пока У кого какие ожидания вообще от доклада что кнопку кнопку все сделать Да вот кнопка все сделать все сделать хорошо это то как мы вообще стараемся чтобы была одна кнопка и она делает все она решает сложную сложную проблему Ну вот PowerPoint проблема какая-то прям нерешаемая почему-то сразу после пары других проектов знаешь сколько лет идут конференции сколько эволюционирует программное обеспечение технологии мы делаем нейронные сети да Но самое самое страшное и самое наверное никогда не пофикшна будет это запуск презентаций запуск презентации и сопряжение кликера с ней да миром PowerPoint Excel давайте пока мы ждем еще какие-нибудь веселые вопросы и ответы если выходить из этого зала какому стенду нужно пойти чтобы получить самый лучший приз Кто как считает голуби кстати голуби Это хорошо большой голубь Тирекс это агрессивно отлично У кого графа на поднимите руку а у кого Киба она поднимите руку а блин графа на немножечко немножечко наверное впереди Странно что типа она так много а ты когда-нибудь переключил слайда голосом голосом Нет я вот подхожу уже к тому чтобы просто силой воли Глядя на этот экран начать переключать но пока не работает сейчас все заведется ничего страшного это какой-то Баттл Да графа накибаны потому что мы например используем везде графа ну О тут что-то изменилось кто-то использует ибо Ну скоро скоро все изменится это уже интригует сейчас сейчас была правильная картинка да И пожалуйста если можно сбросить таймер то будет здорово Ну что я желаю тебе удачи и Давай еще раз начнем все заново Да Кажется таймер мы не победим Ну ладно начнем Привет сейчас секундочку это все еще не работает я здесь вижу статическую картинку Ну ладно пойдем Привет Меня зовут Гриша я руковожу команды interstructure ВКонтакте это наша система мониторинга это основная система мониторинга ВКонтакте о чем мы сегодня вообще будем говорить мы поговорим про мониторинг в больших масштабах мы поговорим Как эффективно собирать и хранить метрики мы поговорим как не дать сломать наш мониторинг И в частности как устроцхаус и в конце будет анонс Итак про мониторинг в больших масштабах Ну что такое вообще мониторинг мониторинг общеробилити такие достаточно широкие понятия там есть и метрики и логики трейсы и экземпляры но я бы сказал что фундаментом мониторинг являются метрикой как наиболее высокочастотный наиболее точный сигнал Что такое метрики метрики это измерение размечены какими-то ярлыками вот мы измерили время доставки первого байта из нашего сидена разметили страной регионом операционной системой это у нас на входе и на выходе У нас есть картинка и мониторинг делает Так что метрики превращаются в картинки если по простому наверное самая популярная картинка на хэйлоуде надеюсь что вы никогда не сталкивались но когда мониторинг не работает Это плохо Почему это плохо В общем достаточно очевидно Потому что если мониторинг не работает и мы об этом знаем то значит компания стоит поезда в прод не ходят диплои остановлены параметры никто не меняет потому что мало ли что вдруг рванет но есть и другая ситуация когда мониторинг на самом деле не работает мы об этом не знаем И это Разумеется хуже потому что мониторинг никуда не девается Просто становится СМИ становится наши пользователи поэтому что мы вообще хотим от мониторинга Мы хотим от него буквально пара вещей во-первых мониторинг обязан работать он обязан работать всегда чтобы не случилось мониторинг должен жить то есть вот нас пожар взрыв выход из строя всего на свете мониторинг должен быть потому что мы смотрим на мониторинг чтобы понять Вообще что происходит и второе не менее важное но о чем часто забывают он обязан работать быстро Ну во-первых потому что быстро подняты не считается упавшим Да если мы быстро отреагировали Ну как бы проблемы Как будто и не было Поэтому если у нас есть данные высокого разрешения и с низкой задержкой Это нам очень помогает Но есть еще и вторая часть вторая часть про то что обзоробилити Мониторинг это во многом возможность и способность людей визуально Представлять что сейчас происходит системой визуализировать то что происходит в продакшене прямо сейчас и когда люди могут это делать эффективно удобно и быстро они начинают это делать они начинают разрабатывать Глядя на графике они начинают изменять параметры Глядя на графике они начинают лучше представить себе системы и делать лучшие системы это очень важная часть мониторинга Ну когда говоришь мониторинг многие сразу говорят Прометей и действительно Если у вас есть небольшая команда несколько человек У вас есть небольшой парк серверов то Прометей это прекрасная система он существует он работает он известный надежный популярный если Прометей решает ваша задача - это прекрасно сложно заключается в том что Прометей не всегда решает задачи мониторинга в частности Когда нам нужно масштабироваться Когда у нас число разработчиков вырастает сотни раз а число серверов вырастает в 1000 или десятки тысяч раз то Прометей не справляется он не справляется то что он плохой потому что просто он для этого не приспособлен это не кластерное решение и тогда нам приходится искать выбирать или самим изобретать какое-то другое решение чтобы выдержать масштаб нужный нам масштаб и если мы говорим про масштаб то масштаб на котором оперируем мы приблизительно Вот такой у нас есть 350 миллионов метрик каждую секунду с 15000 серверов это то что попадает в наш основной кластер тут хочется с одной стороны похвастаться что это очень много но с другой стороны мы сами планируем через пару месяцев чтобы это было миллиард с лишним это гораздо более красивое число Я пытался как-то визуализировать что это такое да вот у нас есть фотоаппарат У него Матрица 60 мегапикселей у нас их 6 и каждый пиксель это измерение и к этому измерению помимо вот самого пикселя как случае с фотоаппаратом у нас еще привязаны наши ярлычки Да там например это регион страна метод API код ответа что угодно вот что нам захотелось привязать мы привязали и вот такой информации у нас 350 миллионов каждую секунду и Казалось бы может быть это все-таки не очень много мы можем это записать но нам потом нужно это еще и прочитать нам нужно эффективно и быстро ответить А вот дай мне вот те пиксели которые относятся к Тюмени метод API такой-то код ответа вот такой-то и пожалуйста сделай это за год чтобы я посмотрел как он менялся в течение всего года поэтому мониторинг с моей точки зрения в больших масштабах это вот задача ровно перенести много данных за одну секунду и у нас нет права устать немножечко притормозить мы не можем уйти организовать забастовку Да мы должны каждую секунду как часы переносить данные метрики из приложений в систему мониторинга вот задача которую мы пытаемся решать Итак как же мы будем доставлять и хранить наши метрики Ну чтобы организовать это эффективно нужно посмотреть на природу данных и самое важное в метриках это то что они очень хорошо сжимаются они очень хорошо агрегируются в частности по времени допустим вот есть наш сидел и у нас есть 6 миллионов запросов каждую секунду и мы хотим знать время по допустим трем регионам допустим всего три региона если мы подойдем к задаче в лоб то мы делаем 6 миллионов записей каждую секунду довольно много но если мы говорим что нас данные не интересуют то можно делать секундные агрегаты и тогда у нас регионов всего три мы агрегируем все три записи и выиграли в несколько миллионов раз кажется что это хорошо давайте делать так поэтому как мы это делаем На каких этапах мы можем организовать агрегацию многоуровневую агрегацию во-первых У нас есть машина Server на котором может быть запущена сотня другая приложений это может быть воркеры PHP enginex еще какие-то приложения у которых у всех есть метрики которые нам хочется собрать и получить и на машине этим занимается агент который отвечает за локального агрегацию и между машинами которых у нас может быть тысячи или десятки тысяч Мы также хотим агрегироваться чтобы данные с разных серверов у нас были про агрегированы перед вставкой этим занимаются компонент который мы творчески назвали агрегатор и если у нас например 100 приложений и у нас 10 тысяч агентов то 10000 получается миллион у нас Мы можем в миллион раз жрать наши данные если все идет хорошо перед ставкой как же у нас общается наш агент и агрегатор нас система Разумеется кластерная поэтому агрегаторов нас много рядом с каждой нотой базы данных которая организованы в несколько шардов по три реплики в каждом и каждый Агент подключен к каждому из агрегаторов у нас идет вставка Агент рассматривает абсолютно все агрегаторы как шарды для вставки он не знает про шортирование внутри базы и уже реплики между собой обмениваются кусочками которые вставлены на каждую из реплик чтобы каждая имела полный набор данных также но все-таки агенты знают про вот эти вот тройки реплики и в рамках этих троек осуществляют теперь как же нас общается агрегатор и база они находятся на одной машине и Наша задача максимально эффективно ставить данные чтобы мы собрали данные за секунду и мы вставили за одну вставку вот наша ведерочка это наша секунда данных Мы хотим ее вставить раз в секунду и все но у нас много машин данные приходят с небольшими задержками поэтому мы позволяем им всем собраться мы организуем такой конвейер записи в котором У нас есть пять ведерчик пять секундочек и пока они едут в базу агенты могут досылать данные Но если кто-то опоздал то все Увы он сюда не попадает Или например если у нас вдруг база притормозила что происходит тогда у нас немножко более сложная вставки в базу у нас на самом деле есть два конвейера есть Real Time конвейер который отвечает за данные которые вот прямо сейчас мы хотим видеть то что прямо сейчас происходит Это самый приоритетные данные мы вставляем их в первую очередь и потом если у нас есть свободный ресурс если база работает Быстро мы успеваем оставляем данные У нас есть виртуальная очередь исторических данных которые мы вставляем уже Best Ford исторические данные могут образоваться потому что Агент не успел сбоинула сеть полетел диск на базе Он дал стал медленнее работать или например 200 серверов просто находились за свечом который не работал а потом вдруг он стал работать и у нас данные за пару дней все появляются мы не теряем данные Мы очень стараемся терять данные нигде вот таков путь в целом метрик от приложений к базе теперь про то как не дать сломать мониторинг И на самом деле любую систему как мы сказали метрики прекрасно сжимаются вот здесь мы в несколько миллионов раз сумели их сжать но есть и обратная сторона медали метрики прекрасно разжимаются и допустим у нас есть Алиса Алиса писала метрику фу и этой Метрика у нас было 100 мегабайт в секунду окей Все хорошо потом пришел Боб Боб Он рок-звезда ниндзя и он решил что вот он жить не может Вот если это Метрика не будет размечена сервером потому что он хочет видеть Что происходит на каждом сервере в отдельности ему очень важно Почему нет серверов допустим Нас 1000 из 100 мб через 100 ГБ Ну в принципе живем но метрики разжимаются бесконечно потому что потом у нас приходит Кэрол и Кэрол решила что она очень бы хотела видеть разбивку по пользователям и по URL запроса и может быть она даже по-своему права Потому что когда она вот метрику начинала писать это была какая-то маленькая темный уголок какой-то админки с двумя пользователями с тремя фиксированными рылами но потом кот эволюционирует функции перемещаются перемещаются и получается так что мы получаем 100 ТБ в секунду возможно питобатикунду потому что кардинальность сколько у нас может быть разных комбинаций наших ярлыков она потенциально бесконечна Например если URL это то что разработчик ставил прям из запроса такое бывает что делать очень часто когда рассказываешь что вот у нас система не справляется записью люди говорят А я знаю надо поставить кафку но разумеется это не решение Кафка нас не спасает во-первых Ну тут это не слон и удав Это Кафка которая вот наелась наших метрик во-первых потому что как только у нас есть очередь у нас появляется у нас появляется задержка что мы получаем Мы не видим наши метрики Мы не видим того что происходит сейчас мы начинаем лететь без приборов у нас увеличивается задержка и мы ждем во-первых а во-вторых как бы с нашим объемом данных данных мы ушатаем любую каску как бы любая Кафка не бесконечна поэтому это не решение нужно принципиально защищать нашу систему от перегрузки нужно как-то более правильно подходить к этому что мы можем делать на самом деле есть всего два варианта всего два решения этой задачи и Вы всегда должны определить вы делаете либо одно либо другое это backpressure или это лоуд шеринг сброс нагрузки Это что такое Это вот ваш звонок очень важен для нас вам ответят первый освободившийся кто-нибудь ваш запрос вычитает первый освободившийся воркеры А пока его нет ваш запрос никто не вычитывает и вы будете ждать таким образом система сама себя Защитила вы ждете в системе хорошо Есть второй вариант это вот Черная дыра это мы просто выкидываем данные это тоже работает Если лишняя нагрузка просто как бы дернул других вариантов нет нужно выбрать либо это либо это если мы говорим про метрики если мы говорим про мониторинг Real T мониторинг ожидание звучит неправильно поэтому как бы хочешь не хочешь мы выбираем девнул Итак у нас есть бюджет сколько Мы готовы ставить данных каждую секунду у нас есть 3 секунды 0 Ты один это два и в первую секунду мы пытаемся вставить в два раза больше данных во вторую секунду мы пытаемся вставить в три раза больше данных третью пять в два Что делать Мы не можем выйти за наш бюджет никак Поэтому просто выкидываем все лишнее и вставляем ровно по одному кусочку данных звучит Не очень но в случае с метрикой мы можем сделать немножко лучше и нехитрый трюк такой вместо того чтобы просто выкидывать данные мы будем то что мы оставляем умножать на коэффициент сэмплирования То есть если мы оставили 3 данных то что мы оставили мы умножаем на 3 достаточно контрактивно что это вообще работает и зачем это я постараюсь это вот проиллюстрировать такой картинкой сэмплирование работает вот если вы понимаете что изображено значит он действительно работает потому что по сравнению с прошлой версией картинки здесь меньше у нас во сколько в 100 раз данных и это широко применяется не знаю те же например все современные игры они автоматически это делают Просто когда они понимают что они выходят за бюджет кадра за миллисекунды они начинают уменьшать разрешение чтобы мы получали кадр каждые 16 миллисекунд и с графиками это тоже работает когда мы смотрим на графике Мы обычно смотрим на агрегаты на очень крупные агрегаты Дайте мне данные за какой-то промежуток времени с агрегированный по тегам Обычно по большему числу всех наших тегов мы агрегируем и поэтому зачастую мы можем цитировать очень сильно И никто даже этого не заметит поэтому как как не дать сломать наш мониторинг мы во-первых бюджетируем ресурсы обязательно Мы бюджетируем сеть диск CPU и дальше наш бюджет мы Честно распределяем между всеми пользователями это могут быть команды это могут быть метрики внутри команды если у нас есть свободный бюджет например неиспользуемый нам не жалко дать тому кто хочет немножко больше записать чем его честная доля но если например уже бюджет весь выбран то дальше все начинают ужиматься и как бы Извините если больше мы будем вас автоцентрировать как только вы превышаете свой бюджет и примечательно то что вот этот подход абсолютно не специфичен для мониторинга Если вы занимаетесь инфраструктурой программированием в принципе то вот это то как следует строить инфраструктурные системы Мы всегда решаем задачу планирования ресурсов Поэтому мы всегда рисуется бюджетируем дальше мы всегда честно распределяем и дальше у нас должно быть стратегия что мы делаем если мы опер за лимит Либо мы заставляем ждать либо мы выбрасываем данные Ну что ж теперь немножко Как устроен Стас Хаус нас есть немного компонентов Агент агрегатор база и опишка на вход у нас есть наши метрики А на выход У нас есть наши графики и первый у нас идет Агент Агент занимается агрегацией между процессами он бюджетирует сеть канал до агрегатора и защищает таким образом и сеть и защищает агрегатора от перегрузки по CPU потому что ребята занимаются агрегацией с очень большого количества агентов поэтому Агент бюджетирует сеть и автоцентрирует если кто-то превышает бюджет также называется шортированием записи и фриловером между разными агрегаторами в случае чего Если вдруг агрегатор недоступен по какой-либо причине то Агент занимается локальным хранением чтобы данные никуда не пропали дальше данные попадают в агрегатор агрегатор агрегирует данные между агентами с разных машин Он точно также занимается бюджетированием и автоцентрированием только он занимается бюджетированием в данном случае уже диска мы бюджетируем Сколько мегабайт в секунду Мы готовы вставить в нашу базу агрегатор приоритизирует запись потому что запись Real Time данных всегда важнее чем запись старички данных и также хоть мы сказали что мы выбрасываем Да мы стараемся выбрасывать только когда по-другому никак А где уместно мы используем поэтому за счет как раз если например у нас что-то с диском то Агент сам перейдет в конвейер исторический и будет уже с низким приоритетом вставлять какие-то данные которые не удалось вставить вот прямо сейчас Ну что теперь база база занимается хранением данных и некоторыми другими вещами в частности она занимается даунсэмплингом для хранения Потому что если у нас есть данные секундные нам не хочется хранить секундные данные все время нам хочется хранить секундные данные какое-то время потом хранить минутные Или например часовые данные и вот этим даунтэмплингом занимается база также она организует для хранения обеспечивает репликацию И разумеется на обеспечивает выполнение запросов от того что стоит за базой немножко про базу мы сейчас используем clic House как основное хранилище какие у этого есть плюсы ну главное это то что кликао существует и эксплуатацию умеет кликхаус его могут поднять его могут починить его могут забакапить как бы понятные всем известные плюсы Разумеется клехаус масштабируется и кликают в целом не тормозит Поэтому кликаус нам подходит очень хорошо как мы храним данные мы храним данные максимально Просто у нас есть всего три таблички которые выглядят вот приблизительно вот так у нас есть секундная минутная и часовая табличка плоские большие таблицы в которых У нас есть колоночка с временем с метрикой дальше идет теги дальше идут значения разные счётчики сумма минимум максимум перцентили и так далее и так далее У нас есть чтобы все было проще только 16 тегов потому что как известно этого хватит всем и что важно мы не храним нигде строки Я здесь нарисовал строки но внутри у нас хранятся только числа только им 32 потому что это на порядок эффективнее чем хранить строки в базе Ну что ж за нашей базы у нас стоит наша опишка и это единая точка входа для получения данных которые наш мониторинг собирает так единая точка входа единая точка доступа Разумеется мы контролируем этот доступ к тому что можно Кому нельзя мы ограничиваем нагрузку на базу и мы занимаемся кэшированием в памяти мы занимаемся максимально эффективным кошером насколько мы можем это эффективно делать Мы можем это делать неплохо потому что у нас есть Точная информация в какую секунду у нас какая секунда данных должна быть проинвалидирована мы управляем ставкой И как только мы вставили какой-нибудь секундочку мы можем сказать что дорогой кэш вот эту секунду и только ее нужно проанализировать А все остальные данные можно продолжать держать в памяти и отдавать очень быстро и тут уже у нас есть свой встроенный интерфейс который выглядит как-то вот так для большинства пользователей простых смертных кто использует его что-то вот такое Это мы смотрим на метрику мы можем здесь сделать разные выборки мы можем сделать здесь разные группировки можно сделать сравнение с прошлым выбрать интересный материал времени и так далее Это мы все можем делать интерактивно без какого-либо языка запросов что очень важно тут край низкий порог входа открыл накликал что тебе интересно сразу Получил информацию у нас есть и даже борды пока что простенькие но надеюсь скоро они будут очень крутые и еще моя вот любимая вещь Надеюсь сейчас видео нас не подведет можно вот бесконечно повторять на то как другие работают еще Можно бесконечно смотреть видео видео да да Можно бесконечно смотреть на наш Live режим У нас есть кнопочка Play если нажать кнопочку Play то данный счет обновляться раз в секунду и можно будет прямо вот смотреть на такой график и например менять какой-то параметр в прозе например Мы хотим рассказать какую-то фичу Ну и нам немножко страшно Мы открываем график нажимаем кнопочку Play и начинаем крутить ручку и так как у нас данный высокого разрешения Так у нас низкая задержка мы сразу увидим проблему просто вот мы крутим Ой кажется оно график пошел куда-то не туда Все стоп отбой ручку скручиваем обратно Итак вот что такое сегодня это высокое разрешение метрик это низкая задержка почти realtime данные это хранение этих данных за годы и это 350 миллионов метров в секунду которые мы собираем с 15 тысяч серверов и теперь как и обещал анонс сегодняшнего дня Stat House это Open Source проект дальнейшая разработка будет происходить на гитхабе мы вот только-только из нашей уютной монорепы перебираемся на github мы будем очень рады фидбеку любому фидбеку discotions так далее если вам кажется что интерактивная система мониторинга которая является высокодоступной которая является масштабируемой которая является мультитентной в которой одна команда не Может наступить на ноги друг друга команде и которую мы используем прямо сейчас продакшене если вам кажется это интересным то посмотрите на Стас Хаус предвосхищая сразу вопрос У нас есть совместимость с графаной и у нас есть такая альфа-превью совместимость с прометеем с крейпинг Remote Write fromql В релизе 11 у нас будет полноценная совместимость тоже Меня зовут Гриша Это был доклад про Спасибо большое Спасибо за доклад обалденная тема это очень классно когда появляются новые высокопроизводительные инструменты особенно таких лидеров рынка как ВКонтакте друзья Давайте задавайте вопросы Я думаю они вас точно появились Она часто все-таки ответила Я тоже хотел задать вопрос Прометей А ты Опа и ответил задавайте Давайте все равно я отвечу и так понимаю уже вопрос происходит ли валидация данных при вставке происходит легализации архивных данных приставки я наверное не очень понял вопрос У нас нет какой-то криптографической аутентификации данных Мы в целом доверяем агрегаторы доверяют данным которые присылают агенты но наверное вопрос другой Ну вы сказали валигируют с последней секунды мы знаем что она вставилась вот она валидируется а если происходит еще доставка из архивных данных а нет наверное речь про кэш Да мы инвалидируем Да да конечно мы полностью контролируем и вставку и алтаем данных и исторических данных Поэтому всегда знаем если например вот мы смотрим на график да и у нас за не знаю вот за сегодня и какой-то сервер был оффлайн И тут он стал онлайн Да вот с него Пришли данные Но эти данные пришли за утро За раннее утро Разумеется мы проинвалидируем ровно этот кусочек и мы эти данные увидим Спасибо за рассказ У меня два вопроса Первый вопрос от вы сказали подробно о том как собираются метрики А можете немножко раскрыть А как Агент узнает Какие метрики вообще ему собирать Как конфигурирование происходит Это первый вопрос и второй вопрос Если какой-то контроль Ну вот вы сказали что Агент там лимитирует да и делает некий А вот от того чтобы сам Агент не захлебнулся от того количества метриков которые в него пытаются засунуть Спасибо большое вопросы так первый был про конфигурацию да агента основной У нас сейчас ну как бы есть две модели сбора в принципе и есть вопрос конфигурации потому что собирать если это пол модели сейчас основная модель у нас является Push То есть у нас происходит Просто локальный Пушков с метриками похоже на наверное то как делается То есть это просто localhost vdport но вот в интеграции с прометеем мы рассматриваем как раз полноценную интеграцию пул модели Да и там уже агрегатор будет на основании данных сервис Discovery рассылать агентам задание для сбора С какого порта нужно что собрать вот вопрос про перегрузку Ну вот отчасти он на него можно ответить словом UTP То есть у нас есть достаточно большой буфер на входе и если вдруг мы перегружаем этот локальный буфер то данные просто теряются вот никто не тормозит на практике мы на одного агента писали кажется когда мы там сидим тестировали мы 400 мегабит на машину писали метрик Вот наверное его можно ушатать если прямо специально написать нагружалку намного ядер но на практике он достаточно эффективно всё делает так Григорий вопрос из чата пока есть время как происходит анализ аномалий перцентили если агрегируете метрики на этапе доставки в хранилище попробуй ответить Да вот было много про агрегацию вопрос Вот что такое агрегация Да что можем сагрегировать потому что если у нас есть счетчик мы их складываем все хорошо да Вопрос Что делать если у нас например есть значение мы меряем время Мы никогда не делаем никакую бессмысленную или не имеющую с точки зрения статистики операции да то есть если например мы не пишем перцеми а мы пишем только мин Макс там сумм и количество да то мы обновляем только эти агрегаты То есть у нас мы присылаем данные приложения присылают значение как есть мы их агрегируем и мы поддерживаем точные правильные агрегаты перцептили точно также у нас есть полноценные которые мы честно агрегируем если мы рисуем какие-то данные Мы в них уверены мы делаем только операцию которая математически правильная Спасибо скажите пожалуйста вас интересный красивая картинка была с живым графиком Life Live стриминг или что-то вроде такого Если я правильно понял данные берутся из Клик Хауса оклик Хауса талаб системы она не очень любит стриминг в реальном режиме времени как вот вы это делали Спасибо за вопрос все так данные берутся из кликаоса и более того Вот я как человек не очень Я очень люблю такие решения глупые в лоб когда мы рисуем эту картинку с точки зрения интерфейса мы просто делаем перезапрос всего если мы говорим сделаем Мне Live режим сегодняшний Да мы будем делать перед запрос сегодняшних данных каждую секунду и нас просто спасает кэш Ты что происходит на самом деле на самом деле происходит то что абсолютно все данные кроме последних пяти наверное секунд мы просто вернем сразу из кэша из in Memory cash и никакого запроса не будет уже только маленький запрос за несколько секунд и число этих запросов Мы лимитируем у нас есть семафор то сколько запросов мы позволяем делать просто одновременно чтобы не перегрузить с точки зрения запросов clic House и мы разделяем пытаемся разделить запросы на тяжелые и не тяжелые чтобы тоже некоторую честность обеспечить между ними Добрый день спасибо за доклад вы сказали что система кластерная и отсюда возникает вопрос как Агент выбирает какому инстансу ему нужно обратиться и отсюда вытекающие вопрос Как вы достигаете консистенции данных Это все средства не кликался или что-то поверх свое и второй вопрос по поводу алармов триггеров вы эту тему пропустили если что и как работает спасибо большое сейчас я надеюсь перестаю до правильного слайда Да вот он у нас каждый Агент подключен к каждому агрегатору У нас есть Он просто знает весь набор Например если у нас есть четыре шарда и три реплики он знает все 12 он подключен ко всем 12 и дальше по по метрике и по нашим ярлыкам у нас освещается шортирование между ними всеми и собственно сейчас и мы точно знаем у нас данные вставлены или не вставлены да то есть мы как бы не просто посылаем куда-то данные типа идите данные вставляете Да вот Агент делает запрос и этот запрос возвращает Окей в момент когда у нас данные точно вставлены если они точно не вставлены вот тут как бы вопросы тут мы пытаемся сходить тогда сделать расходить на другую машину и тут возможны ситуации Да когда у нас могут быть какие-то накладки потому что у нас кликаут как система не гарантирует транзакцион транзакционность наверное если вопрос был про это то ответ приблизительно такой то есть мы да и мы мы вставляем одновременно во все реплики получается что у нас в одном шарде каждая реплика получает третьей нагрузки на запись и потом уже бэкграунде за счет просто именно репликации самого Клик Хауса каждая реплика получает полный набор данных так там еще один вопросик был следующий Нет уже вопросика тогда вопросы из чата есть кейс с инфометриками когда значение Не важно но у них много лейблов с важной инфой Да я Извините пожалуйста Там был второй вопрос я забыл второй вопрос понял я вспомнил И он был про alerting да alerting Безусловно важная история и я полностью пропустил Потому что сейчас Скаут занимается исключительно метриками внутри ВКонтакте есть отдельная система Watch Dogg которая занимается алертингом она регулярно вычисляет некоторые выражения и если они говорят что нужно алертить она alertip мы планируем строить часть которая занимается Вот вычислением эффективным перевычислением это все перенести в Stat House и уже наружу вот приблизительно чтобы разделение было как Прометей менеджер возможно приблизительно такая модель думал круто круто Ну что возвращаемся к вопросам с чата про кейс с инфометриками когда значение неважно но у них много лейблов с важной инфой так происходит их доставка и хранение когда значение не важны много лейблов Например они содержат действительно какой-то за очень важную информацию Да спасибо за вопрос просто же хороший сейчас мы просто если значение во-первых вернемся к тому что если информация важная и вы не готовы ее терять то это не метрики это уже какая-то аналитика который надо писать как-то по-другому метрики это то где мы хотим посмотреть на картинку которая должна быть приблизительно такая и если вдруг какая-то Метрика потеряется то ничего страшного Это мы не деньги считаем Поэтому если вам нужна аналитика то это аналитика но зачастую хочется иметь какие-то вот богатые метаданные значения может быть не важно сейчас мы просто можем записать нолик и оно будет храниться но мы ближайшее время приступим к тому чтобы у нас появилась поддержка например событий Да когда мы можем на таймлайне отмечать интересные события которые тоже у которых есть ярлычки которые можно выбирать и которые позволяют нам коррелировать график с тем что происходило скинуть диплом или еще чем-то а дальше мы приступим Разумеется к логом потому что Разумеется логи тоже Важны и Разумеется Мы точно также хотим собирать и хранить все логии используя все эти же подходы понятно и блиц вопросы Будет ли клиент на Python Да будет обязательно у нас даже есть но просто мы его не успели закончились в ближайшее время будет и последний вопрос он состоит из двух Какой объем метрик сейчас хранится у вас и второе Какое хранилище диски используется под баз данных Какой объем Вот про объем в байтах я на самом деле не подскажу У нас сейчас наш основной кластер По моему это у нас 8 шардов по три реплики насколько эти машины мощные я точно конфигурации Не знаю Я думаю недостаточно мощные У нас есть SSD диски для которые мы используем для realtime данных для секундных данных и HDD диски на которые мы используем для всего остального вот таким образом мы храним тут кстати был доклад о том что в скором времени стоимость SDD дисков и HDD дисков сравняется и потом SD диски Будут дешевле дешевле дешевле возможно я конечно только за SSD только за скорость друзья последний вопрос Если вдруг из зала возможность участвовать в розыгрыше классного подарка вот я вижу вижу вон там Я просто хотел по шардирование спросить у вас оно как бы статическое то есть Что происходит если отваливается агрегатор Куда идут собственно прибитые к нему метрики или но как динамический перераспределяется там при этом Спасибо хороший вопрос а у нас такая немножко гибридная схема потому что мы шортируем по метаданных для чего для того чтобы у нас была эффективная агрегация поэтому мы смотрим метаданные и на основе этого направляемся на один из агрегаторов Но если он недоступен то у него в есть его реплики которые как бы куда мы готовы записать если что и тогда мы пишем четные секунды одну реплику а нечетные в другую мы таким образом без координации получаем вот то что у нас Мы готовы к тому что агрегатор выйдет из строя или будет недоступен для кого-то и мы не потеряем агрегацию внутри шарда Ну что пришло время сделать самое сложное это выбрать вопрос который тебе больше всего понравился меня забрали персика почему-то не смогу вручить лично Наверное мне понравился вопрос про защиту самого Агента от перегрузки до про UTP потому что он вот правильной идеей на потому что то что мы делаем Это строим системы которая защищены от перегрузки Я на всех на всех этапах Так а кто задавал этот вопрос поднимитесь пожалуйста чтобы вас нашли наши помощники и вручили вам классный приз от команды ВКонтакте Григорий Спасибо тебе большое это очень классно что появляются такие продукты и то что большие компании делятся своими наработками сообществом сейчас это действительно очень важно Спасибо большое и от конкуренции холод тебе тоже небольшой Презент Спасибо друзья Спасибо вам большое Ждем вас на следующем докладе"
}