{
  "video_id": "iZavvatyDb8",
  "channel": "HighLoadChannel",
  "title": "Как обслужить миллиард пользователей и отдать терабит трафика / Игорь Васильев (ПАО Сбербанк России)",
  "views": 12124,
  "duration": 3053,
  "published": "2018-08-16T04:57:48-07:00",
  "text": "прежде чем начать мне бы хотелось понять вообще насколько хорошо зал знакомы с принципами и маршрутизации могли бы поднять руки если кто-то понимает как это работает знай она понимает отлично хорошо тогда я думаю что вам эта презентация будет более наверное понятно и интересно нежели людям которым слове не не хватает знаний в этой области ну постараемся рассказать какие то основные базовые принципы чтобы какое-то понимание сложился всего зала так о чем мы будем говорить в рамках этой презентация в рамках этой презентации мы сначала поговорим с вами об основах балансировки это собственно полу поможет вам создать общую картину того как разбивайся как раз является балансировка какие есть нюансы какие есть потенциальные проблемы и на что следует обратить внимание при проектировании вообще сервисов балансировки и может быть архитектура распределенных приложений потом постепенно мы перейдем с вами к архитектуре популярных интернет сервисов в частности немного поговорим про то как сделано фейсбуке немножечко поговорим на тему гугла и закончим это все обсуждением сидел и речь узлов вообще что это такое почему это полезно если говорить с точки зрения сети то как вы знаете люди которые проектируют высоконагруженные систему чаще всего проектирует их с учетом законов закона gustafsson и закона ангела к сожалению когда речь идет именно высоконагруженных системах чаще всего уделяется внимание таким вещам как база данных сколько мы можем приземлить пользователи на конкретном сервере но полностью игнорируется какие-то сетевые вещи например полоса пропускания от мажьте zatarra коммутатора это тоже ресурс количество пакетов который может обработать нашего оборудования это тоже ресурс и очень часто бывает такая ситуация что люди когда начинают масштабироваться они первое что придумывают а давайте мы поставим балансировщик перед нашим приложением и пассажир new не всегда балансировщик может переварить те объемы трафик о которых мы говорим на сегодняшний день например самый производительный интерфейс это 100 gigabit ethernet а мы говорим с вами о торопите трафика и миллиардов пользователей естественно если мы говорим по поводу балансировщик авто у них есть такое понятие как таблиц сессии которые они у себя терменируют эти объемы могут быть большие если мы говорим с вами про миллиард пользователей потому что один пользователь может открывать сразу несколько сетевых подключений сторону приложения и чаще всего эта проблема опять же тянут за собой память ну и скорости лука по наверно по этим таблицам и вот у меня вопрос наверное гнал кто-нибудь вообще думал на тему того как можно вообще отдать торопить трафика или приземлить миллиард пользователю себя на сервис есть какие-нибудь идеи берешь и делаешь на хорошо в общем говорить мы с вами будем про и не каст они каст в принципе том или ином виде встречается практически у всех он есть у гугла например вы знаете что dns-сервера 8888 44 они как раз работают на основе технологии и никас многие седин используют и не каст в частности это клауд flair может быть кто нибудь слышал примерно три года назад была была атака на спам house размер 400 гигабит в секунду им помогли как раз защититься от этой атаки клауд флаер они просто смогли абсорбировать этот объем трафика с помощью технологии не каст кратер который точно также известен в россии активно использует они каст facebook применяет но несколько в другом виде мы об этом поговорим вот ну тем не менее как работает и не каст если посмотреть вообще на эту схему тут достаточно легко себе представить что есть два маршрута например верхней и нижней верх не имеет метрику 30 а нижние имеет метрику 40 вот если говорить с точки зрения принципов маршрутизации маршрутизаторы все обмениваются маршрутами с помощью динамических протоколов выбор наилучшего маршрута чаще всего зависимости от протокола считается на основе каких-то костов которые он сравнивает выбирать наиболее лучше чаще всего например в джипе используется чем меньше метрика тем более предпочтительный маршрут в бюджет и например это может быть количество автономных систем которые мы которые проходит пакет прежде чем достигнет такого пункта назначения ну представим себе что нас есть пользователь есть сервера трафик от пользователя в данной схеме пойдет по верхнему маршруту и попадет на серверах допустим мы добавляем еще одного пользователя этот пользователь точно также попадает у нас на сервер такого что такое никас давайте представим себе ситуацию что мы добавляем точно такой же север в другом сегменте сети который имеет одинаковая айпи адрес в этом слу в этом случае сама сеть разбалансирует трафик между серверами то есть трафик от пользователя 1 пойдет на 2 сервера от пользователя 2 на 1 это очень на самом деле подкупающей подкупающая конструкция в том плане в том что во первых мы можем балансировать трафик без балансировщик of то есть на основе самой сети б у нас если говорить относительно сети тона как вы знаете представляет собой большинстве случаев некоторую паутину особенно если мы с вами говорим про интернет и по факту пользователь всегда идет на ближайший сервер это очень хорошо сказывается на скорости обслуживания то есть задержки становится меньше потому что пользователь всегда идет на ближайший трафик автоматически разбалансирует и так далее так далее но на самом деле для тисе пи и не каст это всегда челлендж потому что есть ряд нюансов которые связаны именно с адресацией серверов с двумя одинаковыми айпи но прежде чем об этом говорит давайте вспомним на самом деле как работает тисе пи почему это важно как вы знаете там есть 3 этап стадия 3 итак этапного квитирования это когда касты обмениваться сильно отсека пакетами причем они устанавливаются соединение дальше происходит некоторая обмена информацией собственно и завершается сессия с помощью опять же обменов пакетами отсека find финна цика цика так вот здесь есть один интересный нюанс если посмотреть внимательно на любое нарушение вот это вот последовательность вам не позволит установить сессию с конкретным сервером то есть там используются такие вещи как sequins номера определенная последовательность и от ответов пакетов с флагами и так далее и если мы говорим с вами на тему и не каста то возникает такой нюанс который связано с так называемым searching persistent стоять когда на сессия обслуживаться на одном хостел давайте представим себе что у нас есть 2 х 100 который разделены маршрутизатором вот он посылает сен запрос сходством отвечает 17 а дальше происходит какой-то обмен и поскольку в рамках 10 есть такая штука которая называется тисе пи окно это когда хасты между собой отправляет определенный набор пакетов и дальше через некоторое время они отправляют запрос получил ли ты от меня всю информацию нужно тебе сделать какой то ритм ретро ндс вот допустим мы добавляем какой-нибудь второй хвост с одинаковым айпишник am в этом случае есть велика вероятность того что трафика от изначального hasta улетит на этот хвост и скажем по он пошлет пакет отсека а в ответ ему он ему отправитесь и перифет из оси будет сброшена это вот одна из проблем которых возникая а который возникает при и не кости решать эту проблему можно двумя способами оба этих способа впринципе решает проблемы с так называемым flow клинингом первое это методы хэширования связанные с и симбе балансировкой и 7 5 колко к small типов и 2 это тисе пи си использование писем прокси на прежде чем будем говорить против писсинг прокси мы с вами поговорим по по поводу хэширования и вообще что такое симки балансировка как я уже сказала из семьи балансировка это балансировка между равнозначными каналами связи или разно значными маршрутами она бывает например чаще всего эту сетевиков это ситуация возникает когда вам необходимо сбалансировать трафик между каналами связи если говорить про поводу и не к что-то это балансировка между серверами трафик который попадает попадает на мачте затар у которого есть возможность отправить трафик на по двум одинаковым маршрутом например или на два одинаковых as the он использует хэшируются функцию или планировщик для того чтобы разбалансировать пакет а вот в первом ситуация если посмотреть слева которая балансировка между каналами то там возникает потенциальная проблема которая связана с this out of ордер то есть если например задержки на одном канале различается от другого есть велика вероятность того что пакеты могут пройти неправильной последовательности это приведет к сжатию спят на их падению производительности приложения поэтому не рекомендуется ставить пир пакет рекомендуют ставить per flow большинстве телекоммуникационном оборудовании можно сделать случае с балансировкой между серверами потенциальная проблема дети пересечь прежде чем продолжить дальше говорить по поводу вообще хэши руси фунтов давайте поговорим на тему того как это работает в темно коммутационное оборудование только мне к ционом оборудование есть такая штука которая называется vip vip это forwarding informational bass то есть это грубо говоря база данных распределены в которой содержится информация о том где какие маршруты находится через какие интерфейс их отправить и вообще что нужно делать с пакетами которые пришли на телекоммуникационное оборудование когда выполняется определенные lookup если у нас используется из семьи балансировка то в этом случае пакет к которому которую капитана оборудование скажем так он попадает на функцию отвечающий за и стимпи балансиров которые можно разделить условно на две части первое это хэш-функции ли пан сироп планировщик 2 собственно это хэш таблица хэш таблица она на самом деле представляет собой большинстве случаях киев ул грубо говоря значение где есть некоторый bucket который напиться к определенному по интеру которую по факту ссылается на запись фиби через какой интерфейс нам отправить какие может быть заголовки нам переписать и на какой-то миг скоб идет потенциально здесь точки зрения балансировки есть 2 2 важных момента первое это какая функция какой понесет планировщик мы используем и второй собственно как ослица работа с таблицами если говорить относительно хэш-функция то здесь бывают варианты вариант как я уже говорил per flow и пир пакет например если мы говорим с вами о том что мы диплом и не касту себя в сети то нужно использовать балансировку первых floater пакет никто не использует более того оператора связи стараются не использовать пир пакет на своих сетей сетях большинстве случаев все настраивают именно пир flow это просто нужно запомнить хотя соблазна того чтобы использовать пир пакет очень велик потому что вроде бы как лучше утилизации каналов но на самом деле все приходят к выводу того что очень страдает приложения поэтому мы это делать использовать не будем второе это хэшируются функциях сырую функции на самом деле в большинстве случаев представляет собой математическую какую-то форму большинстве случаях например могут выполняться такие операции как сор или серси но в любом случае результат всегда математически предсказуема он постоянен это вот примерно я не знаю как 3 умноженное на 5 равно 15 на если вы 5 умножить и на треть тоже получите 15 на этом она это особенности хэшируются функций в принципе можно играть и обеспечивать например консистентной хэширование однако есть некоторые особенности связаны с реализацией у некоторых вендоров например некоторые вендора могут поца подмешивать в результатах хэшируются функции функцией некоторое подобие скажем соли я не знаю там или какого-то дополнительного параметры который может выступать например ай пи адрес роутера иди это делается для того чтобы избежать возникновение ситуации с который называется эффект поляризации fib от на самом деле главная проблема в основном для операторов связи когда у них одни канала перегружены другие не загружены но об этом мы еще поговорим какая есть здесь особенность вот в традиционном подходе наверное в книжках даже книжках курсах например компании cisco systems или в документациях очень часто написано следующее что 1 есть там такая вещь как называется power of ту то есть распределение хэш bucket of между naxt копами осуществляется по определенному принципу например если у нас 8 маршрутов то и 8 хэш bucket of the они будут распределены 11111 то есть один к одному но тут есть интересный нюанс а что будет если у нас вдруг вылетит какой-нибудь и симки маршрут здесь возникает проблема remapping осуществляется с начиная с того же пакета который у нас вылетел то есть он рим опять все остальные и это ведет к тому что в большинстве случаях приведет к тому что будет затронута 87 процентов подключений и для не каста это проблема то есть представьте себе у вас есть восемь маршрутов который указывает на 8 серверов вылетел всего один сервер 87 процентов цессе просто отлетела это проблема и эту проблему в принципе можно решать по разному кто то использует например консистентная хэширование что есть оборудование других вендоров кто-то использует несколько другие к конструкции но первое что бы хотелось сказать на самом деле относительно традиционного подхода не надо становиться заложником технического маркетинга потому что если один бендер делает так это не значит что другой вендор может делать как-то иначе вот есть удара который делают в принципе консистентной хэширование суть там заключается несколько в другом там есть грубо говоря группа реальных таким bucket of который представляет собой виртуальный таким bucket а если вы знакомы например с тем как это делается хеширование в пир toupper applications или вообще знакомы с принципом кости консистентными хэширование то можно наверно представить алгоритм кохар 2 алгоритм который по факту представляет собой некоторые циферблат на котором есть несколько точек которые связаны с реальным значениям хэша и дальше ближайшие просто связываются с этим я реальным как бы баки там в котором находится реальные данные и таким образом обеспечивается консистентной хэширование как это выглядит в реальности если вылетает хэш baked скажем на кпд в данном случае он просто перезаписывается какао б это позволяет решить вопросы скажем что нам делать с этими которые скажем так продолжает работать они просто будут продолжать работу потому что те которые отлетели они отлетели а те которые работали они продолжают они будут продолжать работать есть здесь конечно один нюанс который связан с тем что что нам делать когда будет добавлено обратно новый сервер потому что это приведет к тому что пользователи которые пошли на примерно next up a и b в данном случае после отказа не должны будут быть перекинуты по идее на новые сервера это опять же вопрос решается например с помощью таких техниках динас маппинг и ты нас данном случае очень активно помогает скажем так обеспечить некоторые тоже flow pinning об этом чуть попозже поговорим если кому то интересно почитать на тему того как работает вообще консистентной хэширование что это такое здесь есть две ссылки по теме первое это собственного ссылка как раз на алгоритм консистентной хэширование просто чтобы было понимание что такую вещь можно сделать и второе это пример реализация например в перце в операционной системы для лайтбокс свечей kukuruza если говорить относительно телекоммуникационное оборудование то консистентная хэширование как минимум поддерживается на коммутаторах милан огс это с петром спектру масик вроде как заявлено у джунипер от честно говоря не проверял различных white box вендоров которые используют in пью trident и выше что такой эффект поляризации особенностей себе балансировки ранее в принципе затрагивал этот вопрос и он кстати очень интересно с точки зрения того что может использоваться для стабилизации никас трафик я думаю что если кто-то знаком с этим эффектом то все наверное видели подобную то подобный пример картинки когда есть грубо говоря три маршрута с разными метриками первые r1 r2 и r4 у него коз 30 дальше r1 r7 рррр 5 r4 у него cost 30 и r1 r7 и r8 r4 у него косо 30 если посмотреть в принят на r1 и на r7 то у них как минимум есть по 2 и 7 маршрута по которым они могут раскидать этот трафик так вот если говорить относительно оборудование р 1 и р 7 например верхней канал это интерфейс 1 а нижний канал это интерфейс 2 на r7 то же самое верхнее канал это интерфейс 1 а нижний канал это интерфейс 2 если это оборудование скажем так имеет одинаковую версия операционной системы одинаковую модель она имеет одинаковую схему подключений и у них настроен одинаковые режима балансировки например используются там не знаю sur sa и перестанешь написал спорт дистония порт то в этой ситуации возникает ситуация с поляризация трафика то есть грубо говоря трафик который идет от r7 в сторону r6 он на самом деле не попадает на r5 на r4 то есть этот линк становится не до не до загруженным принципе на этом эффекте можно играть и он позволяет реально стабилизировать и не каст некоторых топология есть такая конструкция как сеть колосса может быть вам знакома очень часто использовать используется и и подобие в нейронных сетях сейчас многие дата-центра строятся по данной архитектуре особенно всякий архитектуру типа спалив на самом деле они большинстве случаев представляет собой либо некоторое подобие сети колосс либо больше похоже на сети бенеша так вот давайте представим себе что у нас есть некоторые дата-центр которым есть 9 маршрутизаторов тире коммутаторов который выполняет эту функцию есть пользователь который хочет подключиться к кому-то сервису который расположен у нас дата-центре пользователи тот может прийти через любых 3 операторов связи которые подключены к маршрутизаторам r1 r2 и r3 допустим он приходит к нам через r1 маршрутизатором допустим что все можете затар имеет одинаковую модель они одинаково подключены друг другу и одинаково настроены с точки зрения балансировки и r1 допустим раскладывает по левому ленку трафик вот так вот разложился r1 разложил точно также как r4 и r7 и по факту r7 что происходит у нас здесь точки зрения отказов ленка допустим откажет на слинг между r1 и r4 в этом случае трафика переплети скажем может полететь на r5 и r6 допустим он полетит на r5 но так как r5 имеет одинаковую схему подключения точно такую же как скажем и r1 то есть примерно слева направо это 1 2 3 интерфейс я вся конструкция таким образом подключен трафику все равно попадает на r7 здесь есть более интересный кейс когда скажем отказывает полностью родин в этом случае опять же ничего страшного не произойдет трафик может прийти на верного вина r3 он точно так же попадет на r4 улетит в сторону первоначально вас сервера но есть не все идеально в этой картине например если откажет ning между r7s один в этой ситуации трафик улетит на второй сервер и произойдет пересечь эту проблему можно в принципе решать опять же используя принцип виртуальных таким bucket of это как раз вот если в кейсе если у вас используется оборудование с традиционным кэшированием но это опять же нужно проверять на конкретным оборудованием достаточно поднять например по 2 туннеля с сервера с 1 в сторону r7 и r9 если откажет ning между s1r симптом эль между r7s один будет автоматически стоять он будет автоматически установлен через другие маршруты то есть эта конструкция будет выглядеть например вот так в этой ситуации трафик попадет опять же на нужный сервер то есть грубо говоря за счет некоторого подобие присутствия рекурсии в плане look up of которые есть на r7 это потом мы на этом эффекте можем играть и в принципе обеспечить стабильность никас сервиса без каких-либо балансировщик of на самом деле если говорить относительно моего мнение вот этой вот конструкция бы я наверно не использовала она очень хорошо ложится для седанов потому что усилена в чаще всего это грубо говоря на две стойки которые стоят где-то на ком на колокейшене и чаще всего там лимитированное количество оборудования на них это касты ставить еще одно желез который выполняет функцию балансировки и наверно это не нужно поэтому и предпочитает поставить там два коммутатора остальное все забить каширу 7 серверами вот для них наверное такая конструкция боли больше подходит потому что если мы говорим с вами по поводу больших дата-центров где реально сотни тысяч серверов такую конструкцию наверное де плоть будет несколько сложновато потому что все таки есть жесткие требований к тому как это все должно быть подключено что везде должна быть одинаковая версия софта потому что в некоторых версиях софта на оборудование какие-то вещи могут меняться ну и если говорить относительно выводов то вывод здесь первые во-первых и семьи очень замечательно себя чувствует современной архитектуре дата-центров то есть это в топологии x полив и в топология стекло за следовательно если чувствует себя хорошо и 7 ты знаешь чувствовать себя и и не каст второй момент очень многое зависит от того как у вас построен дата-центр очень много зависит от того как у вас за дизайне на амортизацию на три сети и вывод третий это очень важно то какое вы оборудование используйте это какие вы используете принципы балансировки как я уже говорил лучше использовать пир flow даже не лучше это всегда нужно делать и второй момент если говорить относительно других игроков рынка ну я имею ввиду там фейсбук гугл и кого-то другого то вся обычно использует сурс айпи не станешь написал спорт distortion порт этот одна из таких рекомендаций относительно unicast теперь давайте попробуем перейти к функции и методы писсинг прокси этот метод основывается на принципах асимметричной мушки зация и здесь есть очень интересный нюанс если мы говорим с вами принципе про айпи мушки зация как таковую то айпи марте зация она выполняется грубо говоря имеет пир хобби север то есть это значит что каждый маршрутизатор который делает lookup он делает lookup независимо от другого и опираясь на ту информацию которая есть у него в таблице можете зация то есть если у нас пользователь допустим идет на r1 маршрутизаторы r1 смотрят на себя в таблице видеть что сервер доступен через муж ти затар r2 он туда его отправляет r2 видит что может сервер доступен через можете завтра f3 отправляет в сторону r3 r3 видит что сервер у нас directly connected то есть непосредственно подключен отправлять сразу сторону сервера у сервера стоит дефолт в сторону раутер 3 он попадает сторону раутер 3 и дальше роутер 3 см и так даже нас пользователя он видит опа а пользователю меня в таблице через может находится доступен через можете завтра r1 и трафик уходит на r1 вот это вот называется конце вот ситуация называется симметричным можете зация и эта сама ситуация лежит в основе такой вещи как dessar дарик сервер return суть его заключается в следующем что есть балансировщик который находится в л2 сегменте на него попадает трафик этот балансировщик находится импов по отношению пользователю между пользователем и сервером и дальше он отправляет пакета на любой серверов который имеет одинаковое печник здесь очень интересная конструкция заключается в том что первое по еде нельзя назначить один и тот же айпишник на сервере который находится в одном углу одна мини потому что это будет конфликт айпи адресов и так далее так далее что в этом случае делают обычно на серверах поднимает loopback интерфейс которому назначается этот айпишник у нас внутри а на физике на самом деле у него присутствует айпи адрес который скажем так ну никак находится у ника было скажем так он уникален в этом l2 сегменте в этом жил 2 сегмент стоит у нас балансировщик на котором написано соответствующие маршрута что конкретный вид у нас доступен через вот эти вот уникальные api адреса и вот по балансировка ас является между этими адресами вы смотрите что интересно получается пользователь идет на балансировщик балансировщика трафик уходит на сервер а вот сервер от трафик уходит напрямую мимо балансировщика он уходит на мушке zatarra это сделано за счет того что скажем так на вышестоящем можете за 3 прописано маршрут на vip через балансировщик но балансировщик и прописан маршрут на vip через уникальные печники которые есть на серверах на самом сервере дефолт road напрямую на маршрутизатор эта конструкция на самом деле уже позволяет вам обслужить достаточно большой объем трафика потому что если говорить например про linux новую реализацию то вот в качестве такого балансировщика песен прокси может выступать и пвс это linux ver чел сервер там по масштабируемостью ситуация выглядит следующим образом доступно 256 bucket of то есть это можно подключить до 256 серверов и если представить относительно пользовательского трафика то чаще всего трафик от пользователя в сторону сервера маленький а вот от сервера пользователю трафик идет большой и как-то делал замеры no voy a shark собственном мусаси который подключался сессии в к фейсбуку и там получилось примерно следующее что на где-то 512 килобайт приходилось порядка 12 мегабайт трафика то есть это почти не знаю там в двадцать раз больше чем трафик больше в сторону пользователя нежели от пользователя к серверу но у этой конструкции есть определенные проблемы во первых проблема заключается в том что есть некоторые нюансы связаны с масштабируемость ил-2 сегментов и проблема заключается в там проблема 2 это всякие бродкаст flooding поэтому большинство крупных напустим хотите будете это power за топ как раз типа фейсбука и гугла они переходят они давно уже перешли в сторону муж teaser умных дата-центров и вот там у них используется несколько другая конструкция нас прежде чем начать рассказывать про эту конструкцию наверно расскажу про опыт яха яха эту конструкцию решала несколько иначе они использовали никаких туннелей они это делали через 10 то есть они красили трафик с помощью 10 и такие дальше можете зации выполняли этого трафика внутри дата-центр на основе 10 значению боитесь заголовки и эта конструкция не очень нравится больше красиво выглядит именно с туннелями и так как делается элт редиса допустим у нас есть можете зиру и сеть в дата-центре скажем где балансировщик и находятся в одном поле в каком-то в одной стойке а сами сервера находится в 3 стойки например у нас еще они могут быть географически как-то разбросано по-разному дата-центром налита вода alter в этой ситуации я все идет собственно точно так же примерно как этом у нас есть vip который прописан амортизаторах через балансировщик трафик от пользователя поступает на балансировщик а вот дальше начинается самая интересная балансировщика устанавливается айпи туннель айпи и лигры я например но в большинстве случаев используют айпи который подключается к серверу и на сервера опять же поднят этот loopback с этим видом и трафик от балансировщика уходит на какой-то серый вот данном случае например на сервер с 2 в этом случае мы вообще не привязана ни каким ил-2 доменом как я уже говорил и реальности балансировщик может находиться в любом поле если говорить относительно фейсбука то фейсбука сделано следующим образом что у них балансировщик и из постоят пир под то есть они имеют все одинаковые режима балансировки то есть грубо говоря среди вас урса и пили body станешь на пи и каждый кластер он по факту ограниченными на потом ну как что использует facebook facebook что по поводу него известно 2012 году к эта компания полностью отказалась от аппаратных балансировщик of они начали использовать как раз и пвс активно прикрутив к нему различные питон скрипт eprint проинтегрировать свою ит-инфраструктуру 3вт в общем проект получил название shift и было была презентация на тему того как они балансируют трафика красно европа этом 2016 если будет интересно можете попробовать погуглить посмотрите и второй момент google что по поводу него известно google отказался от аппаратных балансировщик of насколько я знаю 2008 году они активно разрабатывали собственный балансировщик который получил название маглев маглев несколько другая реализация в фейсбуке это реализация не предполагает никакой интеграция с linux kernel и она скорее всего работает в юзер спеси аналогично тому как работает решения temple de pe детей из дополнительного что могу сказать что этот балансировщик основан даже не на том что он хранит у себя какие-то таблицы состоянии сессии так далее а он больше основан именно на консистентными хэширования если говорить от но относительно того где вы можно пощупать насколько мне известно он используется для балансировки в google клауд engine может быть все все знают по поводу их облачных услуг они пытаются конкурировать с amazon а в с и собственно там у них есть такой балансировщик давайте посмотрим на тему фэйсбука как все-таки это выглядит в фейсбуке ну в первую очередь наверное хотелось бы понять прежде чем говорить о том как это все выглядит и как это скейлится чтобы получить некоторые ровно брать посмотрим что происходит сессии у нас есть пользователь вот пользователь допустим хочет открыть какой-нибудь facebook и первое что он делает на самом деле это отправляя запрос где находится facebook dns ему отвечать что это и печник такой-то этот-то печник у нас прибит какому-нибудь например бэг-энда на котором крутится печки процесс дальше он делает get запрос и он отдает и чтим и возникает вопрос как получить больше рпс а больше рпс получить можно следующим образом поставив например l7 балансировщик в данном случае это может быть какой-нибудь engine x или прокси ген вот поставили балансировщик и вас возникает следующий вопрос а что нам делать нам не хватает равно производительности вы ставите альфа балансировщик например который обеспечивает как раз 10 песен прокси схемы sultry dessar и в этом случае вы уже можете отдать например терабит трафика потому что если у вас этот сервер может переварить 10 гигабит трафика у вас за ним находится 256 серверов каждый который имеет 10 гигабит и сможем может отдавать 10 гек трафик то это уже будет примерно до 2 с половиной то рабита трафика что нам делать если нам не хватает добавим еще один такой балансировщик и включим не 7 приносите и вот у нас получается что конструкцию металл 3 лвл файл 7 балансировку то есть она на самом деле многоуровневая но что делать если нам этого не хватает мы можем поставить а как это выглядит в фейсбуке вот у них есть и дата-центров десятки этих балансировщик of которые выполняют функцию то есть шив дальше идет сотню прокси серверов и за ними находятся десятки тысяч викендов дальше как нам делать что нам делать если нам вдруг этого не хватает а давайте включим dns балансировку вот вам второй кластер что нам делать если нам двух кластеров не хватает это центр мы добавим еще один дата-центр и вот постепенно мы приходим к такой вещи как content delivery network что это такое content delivery network по факту очень сильно помогает нам оптимизировать подключение между пользователем и нашим сервисом если посмотреть на классическую реализацию это выглядит следующим образом у нас есть какой-то сервер скажем задержка между этим сервером 250 миллисекунд вот он проходит round trip 3 этап на квитирования дальше идет время на на калькуляция запроса на стороне сервера скажем она занимает 500 миллисекунд и дальше мы отдаем пользователю например в рамках 5 5 ртт страницу каждая ртт поскольку на занимает 255 на 250 нам получается 1210 того 2000 миллисекунд на одно подключение так вот сидим позволяет на самом деле это все дело оптимизировать потому что он всегда находит ну мы предполагаем что сидим находится ближе к пользователю скажем задержка 100 миллисекунд у нас пользуется между седины нашим сервером более скажем так оптимизирована подключение она либо прогрета уже либо оптимизирована тебя окна там подчинены какие-то параметры итак давайте посмотрим как это выглядит вот от клиента в сторону сидел на задержка 100 миллисекунд его запрос проходит по уже существующему тебе подключению которые есть между сидений нашим сервером сервера обрабатывать 500 миллисекунд дальше за счет того что нас оптимизированы типе окно все данные помещаются в рамках одного окна населен либо сидел может кэшировать эти данные отдает дальше сторону пользователей вот здесь смотрите что интересно получается 5 ртт 500 миллисекунд то есть того у нас 1100 а экономия 900 миллисекунд вот этого то что позволяет сделать в первую очередь сидим и как же нас пользователь попадает на нужную узел есть два варианта первый это вниз mapping собственно global server load balancing и 2 и 2 это global unique стать можете зация ближайшему вот о чем мы говорили до нас mapping насколько мне известно используется активно у кого технолоджис и фейсбука global они карт используется вклад флаер кратер вас ли то есть это в основном тема больше для сидел как работает dns маппером давайте представим себе что у нас есть dns у нас есть пользователи у нас есть дата центра в этом случае пользователи хотят понять как бы им попасть на какую-нибудь скажем вебсайт который может крутиться в рамках этих четырех дата-центров она не отправлять запрос в в dns дальше лезет в сбоку в базу например гейб и database в котором находится информация в котором находится сопоставлений информация байки адресах пользователях ближайшими узлами он получит делать запрос по внутреннему api например дальше тот ему дает ответ и он отдает dns и дает пользователям что нужно идти на соответствующие центры обработки данных либо узлов и пользователя расходятся по разным узлом вот такая конструкция используется у фейсбука они в принципе них этот сервис называется картографа они его активно используют более того я знаю что linkedin то же самое используют они меряют периодически эту загрузку там с помощью java скриптов которые подгружается на страницу и так далее так далее какие здесь нюансы нюанс первое тагил качество база данных hype то есть чем качественнее база тем больше тем лучше мы будем распределять трафик момент второй это каширование dns запросов вот кэширование данных запросов на самом деле большая проблема потому что пользователь например запросил через dns оператора связи этот результат за кашира вался дальше мы уже был в балансировке ограничены как делает facebook на самом деле facebook делать следующим они отдают всегда вам земным запись в которой от отеля равен 30 минутам она ссылается на запись у которой отеле равен 1 минут это в первую очередь сделано для чего 1 не таким образом пытается бороться с проблемы которые свернуться кэшированием и второй второй момент чтобы ускорить восстановление если говорить по поводу интернет пилинга как он устроен то это terlan тир 2 и тир 3 оператор есть по поводу global и не каста то тут ситуация такая что почтительно размещать узлы в тир 2 операторах и важно смотреть на политику перинка мы можем ограничить более подробно по этому поводу я бы рекомендовал посмотреть презентацию кратеров labs есть бюджет и никас просто о сложном и на на ноги пятьдесят девятом была хорошая презентация вернитесь хорды не casters дальше чем больше насоса узлов тем меньше радиус поражения просто потому что пользователи могут быть раскинув то второй момент с точки зрения длинных сессий подключений со длинны подключение со временем на самом деле отваливается статистика вот укажу на turks такая по поводу global и не koston ну и вино из mapping вес global и не каст собственно не всегда оптимально оптимально использовать этот этот и другую комбинацию linkedin например делает это перед живым то есть не тепло это своей и не каст адреса первый регион например и май рашен цис например там не знаю азии так далее уже балансировку по и не касту осуществляет рамках региона глобально надо на с уровню кого-нибудь есть какие-нибудь вопросы отлично добрый день добрый день а вопрос по поводу dns балансировки вот один вариант это по географическому месту до сделать а если из одного места очень много пользователей там какие дополнительные алгоритма есть ну смотрите во первых чаще всего пользователей которые идут из одного места их много а это чаще всего пользователи например одного оператора связи вся эта информация в принципе есть в интернете предположить что например этот оператор связи работать скорее все в конкретном регионе например москва пример абоненты мгтс дата в данном случае это информацию опять же ничто не замешана ничто не мешает ее прибить туда попать сетям и чаще всего именно так и делают то есть информация заносится папа testimony по конкретному айпи дальше а в большинстве случаев такие сервисы как facebook или google скорее всего мерят конкретные печники когда пользователь заходит и на стали назначать какой-то скоринг внутри своей системы то есть они считают задержку до конкретного узла видит что у нас вот есть более оптимальный узел этих пользователь мы будем в будущем управлять именно этот узел они это делают кстати постоянно и делали почему потому что интернет не статичная конфигурация то есть добавляется новый период между операторами связи уже появился более оптимальный маршрут иметь смысл перенаправить добрый день у меня аж три вопроса дано войти во первых как решается проблема с tcp общался прессинг прокси смысле как решается проблема с тисе пи пи пи общем ну что значит себе общем чтобы иметь в виду при установлении этой цепи сессии так у нас в некоторых вариантах идет обмен ты пьешь нас уже в сена и финансах пакетах так так и это по идее должно быть синхронизирована с а конечная брендами так приземляется записываясь непосредственно быка на конкретном сервере они на балансировщик балансировщик или просто мотивирует по факту он он марширует эту информацию может заносить это опять же как я понял и просто не корректно его назвали что sin прокси но он да да скажем так просто в принципе на сленге которые используют люди которые делал deployed подобные схемы они чаще всего называют его 10 песен прокси потому что балансировщик все равно так или иначе видит информацию но ему просто делать mapping ил-4 грубо говоря он либо ее заносит в некоторую таблицу как это делать на примере пиво с либо делается на основе хэширования да я понял вот когда вы рассказывали про и icmp и там как у нас пряди одинаково функция все хорошо с маршрутами как эта схема переживает собственно говоря динамическую жизнь так сказать то есть когда у нас ход свататься карты когда у нас маршрут и отмирают а потом опять возникают но я уже говорил об этом основной тесно что здесь нужно обращать внимание это поддержка консистентной хэширования на телекоммуникационном оборудовании в цис например это не поддерживает я понимаю но вот и или у нас идет по одинаковым именам тогда возникает проблема приход свой пикард мы старый интерфейс у нас сдохну вставили новую карту у нас нумерация поменялось так либо у нас по тому как это ну номера велось в случаев подтягивать через динамический протокол у нас интерфейс упала поднялся он уже в другом порядке но смотрите относительно к во-первых замены установки новых карт не всегда при замене и установке новой карты допустим знамени старые съезжает индекса это у нас был я там 8 вылетел 1 интерфейс мы вставили дополнительную плату на 8 и там уже другой и там уже другой в иметь добавили еще один из новых из новой карты из 2 смотрите здесь на самом деле эту ситуацию можно разделить на два этапа первый этап это когда у нас вылетает какой-то in haren хоп то есть он как конкретный маршрут и второй этап когда он добавляется если мы говорим относительно того когда он вылетает то с консистенции хэширование этой проблемы нету потому что количество хэш bucket of не обжарится она не меняется она постоянно то есть грубо говоря если у вас вылетел этот микроб то он просто при программируется ближайшем там который есть в таблице по round robin у то есть например индекс топ а вот в циски это выглядит так что если у вас вылетел was a justin сразу происходит прямо перед хэш пакетов например там было восемь стало 7 до то есть она например 2111 как-нибудь так раз кидается и в этой ситуации той ситуации это действительно проблема а при консистентной хэширование у вас просто тот который вылетел запишется нужно другим значением например следующего там на коп а вот здесь потенциально проблема единственная которая возникает это когда вы добавляете новый хост но опять же при добавлении нового hasta это эта вещь можно обходить например обходить каким образом когда у вас на кластере живет 2 и печника например один основной который вы используете в конкретный момент для обслуживания второе на случай если например будь проводить какие-нибудь работы связанные с обслуживанием или после восстановления отказа опять же пользователю всегда можно отдать сначала один айпишник он будет подключаться к нему потом по 2 дождаться пока все пользователей приключиться на 2 pro анонсировать этот первый потом снова переключить их через dns на этот первый и потом про анонсировать 2 вот пожалуйста вам восстановление это первый момент если гореть относительно телекоммуникационного оборудования могу сказать следующее что если винда ра который использует например функцию грейс хелбиг вот есть такая штука улановский хасиков когда они помимо того чтобы прежде чем вернуть этот хэш bucket уже грубо говоря в балансировку они мониторят какое в какое-то время пока если матч или нет хитов по этим хэш bucket как только нету он его переписывать читать что пользователи не балансируется поэтому он возвращается в саму конструкцию балансировки такая вот fitch например есть умела ноксом спасибо и последний вопрос от котова там уже в конце про балансировку и цифры употребляли мне показалось что это чисто теоретический пример потому что вот эти вот нет вот эти дальше а я нет нет да это вот вот это вот еще до этого вот эти ци еще там была стелла с пола нет обид и так далее более 27 а вы вы вы имеете ввиду вообще вот про вот эту вот конструкцию про два с половиной то рабита да да мне показалось что это ваше чисто теоретические рассуждения потому что ну как не смотрите смотрите за как я знаю цифры по трафику sph пышных фронт эндов никто столько не отдает но с поспешных front end of да никто столько не отдает действительно их больше отдает например там сидена всякие фото и так далее но если мы говорим о том что мы вам отдаем видео такое может быть я знаю я отдаю видео ну хорошо тогда собственно просто я говорю о том что вы можете представить что с помощью этой конструкции вы можете отдать два с половиной то рабита без проблем учитывая что на суммарный трафик вас получился больше чем по-моему сейчас есть в интернете в интернете да вы знаете что например у касперского которая предоставляет услугу по защите видеосъемка сетей сидел более 50 терабит в курсе что например akamai а когда транслировало недавно вот презентацию apple у них видеотрафика было почти порядка 45 гигабайт от раббит орда но вот это примерно как был общая емкость интернета которого так можно оценить общий емкость а у нас началась несколько большую общая емкость интернета на самом деле гораздо больше потому что что такое терабит терабит это грубо говоря 10 уже суточных интерфейсов вы не видели ни разу конструкции по поводу суточных интерфейс вот сеть фейсбук например вся построена на 100 гигабитных интерфейсах у них между сединами они отдают очень много трафика но я дома не даёт примерно сравнивал с ютубом ну нет у youtube отдает больше youtube отдает сейчас наверно где-то терабит 10 вот а вы говорите что нет такого трафика у вас получалось больше существенно больше где вот в этой картинке ну вот в этой карте картинки но смотрите хорошо давайте посчитаем ради интереса просто допустим у нас вот этот вот l4 сервер он имеет скажем 10 гигабитный интерфейс на вот у нас летит пользовательский запрос скажем я не знаю там который занимает сколько вы хотите чтобы он занимал запрос да ну например там мегабит на вот возьми гигабит и в влезет нам 1000 в 10 10 тысяч а вот примеры запросов отдает ему скажем сервер я не знаю но на мегабит скажем 10 мегабит трафика например потому что страниц это несколько больше там может быть картинки видел еще что-то может отдаваться на ваш сервер в этом случае перекос будет несколько больше мне кажется мы просто немножко про разное говорим ну возможно я говорю о том что сейчас стабильно не набирается трафика столько сколько вы сказали что вот сейчас мы на балансируем но смотрите давайте так я не констатирую сейчас конкретно что набирается у всех столько трафика или нет я говорю о том что есть решение которое позволяет в принципе этого трафика отдать если вы не согласны с этим решением не судьи я вот как раз это и хотел уточнить что это не реальная схема а предложение по тому как это можно делать вот эта схема вот эта схема это реально это дел то что делает facebook у себя это то что делает facebook у себя эта схема абсолютно реальная с конкретными цифрами нет цифры цифру фейсбуку свои цифры вы знаете на лирe миллиард пользователей коллеги извиняюсь я вынужден вас прервать предлагаю продолжить общение клара тогда кому время нашего вышла уже предлагал поаплодировать спикер еще раз"
}