{
  "video_id": "UQYLghUUKRM",
  "channel": "HighLoadChannel",
  "title": "5 GB трейсов в секунду, или Как устроена система трассировки в Ozon / Владимир Балун (Ozon)",
  "views": 4660,
  "duration": 2532,
  "published": "2024-04-17T01:10:23-07:00",
  "text": "Да спасибо Всем привет Меня зовут Владимир Я работаю ведущим разработчиком в Озоне где уже больше года занимаюсь разработкой системы трейсинга и в этом докладе Я хочу рассказать о том как устроена система трейсинга внутри озона И как у нас получается справляться с такими достаточно большими нагрузками хочу рассказать подробнее плане доклада то есть что вы сегодня узнаете в первую очередь вы узнаете Что такое трейсинг и какие проблемы Он решает Затем вы узнаете особенности архитектуры трейсинги внутри зона И поймете за счет чего у нее получается справляться с такими достаточно большими нагрузками и в конце дополнительно Я хотел бы рассказать о различных инструментах и возможностях которые можно делать поверх какой-либо существующей системы распределенных и прежде чем вы с вами начнем Я хотел бы сказать что в этом докладе я буду не один Знакомьтесь Это Петя пейте это вымышленные разработчик из инфраструктурной команды озона и вместе с ним на протяжении всего доклада мы будем решать различные проблемы которые у нас будут возникать как я сказал Петя работает в Озоне и Озон сейчас достаточно большой у нас три своих дата-центра более 6000 серверов и более 4000 микросервисов И все мы знаем о том что микросервисная архитектура сейчас достаточно популярна То есть это нам позволяет Независимо что-то масштабировать Независимо что-то развертывать то же самое время мы знаем и о том что микросервисная архитектура есть своего рода минусы и банальный самый очевидный минус это когда микросервисов становится очень много и бывает иногда достаточно трудно отследить А кто с кем и как коммуницировать Или как вообще выполняется какой-то один выборочный запрос и конечно же в зоне работает не один лишь только Петя представи там работает еще разработчики и скажем что это разработчик Аня и они допустим занимается какой-то продуктовой разработкой и вот в какой-то момент Аня нужно понять а кто ходит сервис 3 или Куда например ходит сервис 3 и у Ани сейчас нет быстрого инструмента который позволит ей ответить на этот вопрос все что и придется делать это идти в Исходный код этого микросервиса если он открыт конечно же или эти команде разработчикам этого микросервиса и уточнять это у них хотелось бы отметить что и тот и тот вариант рабочий но не долгие чреваты потенциальными ошибками потому что можно не увидеть вызов какой-то строчке входа Да там или спросить у разработчиков какую-то неактуальную информацию и есть другое еще достаточно популярный пример представим что они все также разрабатывает какой-то микросервис скажем сервис 1 и в какой-то момент этот микросервис начинает отдавать очень много ошибок и вот они за микросервисные архитектуры сейчас не может понять От чего зависит от ошибки проблема в моем микросервисе или сопутствующих которым этот микросервис обращается Как клиент или может быть ошибка в базе данных и мне кажется это ситуация достаточно популярна поднимите пожалуйста руки те кто хоть раз был на месте Да действительно проблема достаточно актуальна и хотелось бы сказать что в этот момент Аня будет разбираться и понимать а что же происходит где у нас проблема а пользователи в этот момент конечно же будут страдать и например получать посылки Не сегодня а Через несколько дней и в эту ситуацию вписывается Петя и он понимает что нужен инструмент который позволит быстро анализировать подобные ситуации Исходя из этого анализа принимать какие-то решения и Петя начинает думать а может быть уже есть этот инструмент может быть кто-то придумал этот инструмент и Да действительно такой инструмент есть он называется распределенная трассировка или дистрибьюте трейсинг И в чем заключается идея вот практически любой запрос Ну практически любой состоит из разных частей например для того чтобы нам обработать какой-то заказ нам нужно что-то сохранить базу данных записать что-то в кэш там обработать оплату и так далее И вот каждая эта операция называется сном в рамках распределённой системы распределенных трассиров а набор спавнов или коллекция образует своего рода трейс и при помощи тресса мы можем четко ответить А как выполнялся какой-либо выборочный запрос где были какие-то тормоза или где допустим произошла какая-то ошибка в каком-либо из микросервисов и как это абстрактно работает но мы в самом начале добавляем индификатор и потом пробрасываем этот индификатор через каждый микросервис и на выходе всего этого собираем комплексный трейс но дети понимает что писать что-то свое А это дорого а б это долго и Петя начинает думать а может быть есть что-то готовое может быть что-то можно взять и поверх этого всего что-то делать и Да действительно таких систем Существует несколько но Петя принимает решение выбрать Ягер хочу отдельно сказать это никак не связано с тем что есть такой популярный напиток как герместр и то что он вазоне пользуется неплохой популярностью нет Дети выбирают Ягер из-за того что у него достаточно модульная и масштабируемая архитектура плюс ко всему ядер достаточно популярной И третье Я считаю на важнейшая причина пьеденного выбора это то что Ягер разрабатывает uber А убера в принципе точно такой же стык как и озона и это Петя в будущем позволит дорабатывать этот инструмент в случае необходимости под какие-либо свои нужды раз я заговорил про ядер Давайте взглянем на типичную архитектуру ягера она достаточно простая и примитивная есть какие-то приложения которые хотят отправлять эти самые операции то есть испаны и они живут на каких-то кубовых нодов и вот когда приложение хочет отправить эти спаны эти приложения отправляют я гироген а я Гир Агент селится на каждую кубовую ноду и туда Спан улетает по юге Агент накапливает эти спальные и интервально пачкает бочами отправляет ягорь коллектор Потерпите коллектор собирает эти спаны воедино то есть собирает из них комплексный трейс и дальше сохраняет это хранилище хотелось бы сказать по дефолту два вида хранилища этой ластик или Кассандра не принимает решение становиться на эластике потому что эластик достаточно хорошо себя зарекомендовал как хранилище для логов а все мы принципе знаем что логи этой тряси и это немного похоже Да и к тому же у петь еще была неплохая экспертиза власти и последний компонент архитектуры который я хотел бы осветить это ягорь это тот микросервис который занимается удовлетворением пользовательских запросов когда кто-то хочет найти какой-то трэйс или множество трейсов и Казалось бы все хорошо но Петя внезапно обнаруживает то что в Ягерь есть логи то есть мы можем какому-то спану добавить и не хотелось бы еще писать логике в систему трассировки запросов Да к тому же вазоне уже есть система логирования и не хотелось бы дублировать логику разных под системах поэтому Петя принимает решение делать так когда кто-то пишет логи в какой-то трэйс эти логи будут уезжать систему логирования Ну скорее всего сейчас сразу возникает вопрос А как тогда это всё будет работать как пользователь сможет получать логи в каком-то трейсе Да и Петя принимает решение дорабатывать архитектуру ягера он добавляет отдельный микросервис Называется он к Вере прокси В чём заключается идея когда пользователь хочет получить какой-то трэйс он одновременно с этим идёт за трясцом я герберри и параллельно в систему логирования дальше мы получаем логи мы получаем сам трейс мёрджем это всё и отдаём клиента то есть Для клиента это работает абсолютно прозрачно он не задумывается о том где вообще хранятся эти логики для каких-либо спанов и в итоге получается то что в Озоне появляется система распределённой трассировки И как я уже говорил логи хранятся только лишь в одном месте и вот дальше достаточно типичная ситуация с которой Петя сталкивается Когда какие-то разработчики в чатах задаются вопросами А почему мой запрос тормозит да Или Почему ещё чей-то там вопрос тормозит и вот Петя тоже вмешаться в эту ситуацию потому что Петя понимает у него есть огромная выборка данных которая говорит о том А сколько каждый операция выполнялась и Петя принимает решение делать такой инструмент под названием критический путь что это за инструмент спросите вы А это такой инструмент который позволяет сказать А какие операции нет смысла оптимизировать а какие нет то есть представим какой-то огромный запрос состоящий из тысячи операций то есть тысяч спавнов и вот в огромных запросов достаточно Трудно бывает проанализировать А какую операцию нужно оптимизировать для того чтобы уменьшилось общее время выполнения всего запроса но имея критический путь мы можем четко на это ответить потому что Спан или часть Пана находится на критическом пути тогда и только тогда когда уменьшение продолжительности этого спана существенно скажется на уменьшение продолжительности всего запроса и если говорить простым языком благодаря этому инструменту разработчик просто понимает что имеет смысл оп играть а что нет потому что это никак не скажется на общем времени выполнения всего запроса конечно же хотелось бы сказать что Ягер не знает что такое критический путь поэтому пейте снова приходится дорабатывать архитектуру ягера и он добавляет отдельный микросервис под названием Трейси сатапи В чем заключается идея теперь Ягерь коллектор помимо того что собирает спаны и образует комплексный Трэш он еще собирает статистику по спадам по три сам извиняюсь и отправляет пачкой в микросервис третья где эта статистика агрегируется сохраняется и на выходе у нас получается критический путь но хотелось бы снова сказать если ничего не знает о критическом пути Но конечно же я тоже ничего не знает о критическом пути поэтому петь и здесь приходится дорабатывать интерфейсера и он лишь рисует чёрные линии как раз-таки внутри спана Как символ того что весь бан или часть Пана находится на критическом пути и дополнительно иногда бывает интересно посмотреть не на один какой-то выборочный рейс А на множество агрегированных по какому-то признаку и мы точно также предоставляем такую возможность и умеем рисовать вот такой вот симпатичный Flame Граф в графане благодаря этому в Озоне сделал появился инструмент который позволяет подходить к различным оптимизациям более создано то есть повторюсь у разработчики разработчики теперь четко понимает какую операцию имеет смысла оптимизировать А какую нет потому что это никак не скажется на общем времени выполнения всего запроса и дальше Петя сталкивается с такой проблемой когда рейсингом начинает пользоваться все больше и больше людей то есть банально трейсинг начинает набирать популярность Исходя из этого то есть слов становится все больше и больше и больше И к слову нагрузках сейчас у нас приходит 5 Гб трафика секунду из которых миллион 400 в секунду и 32 миллиона спанов в секунду хотелось бы отдельно сказать на распродажу к нам приходилось 7-8 Гб в секунды но сейчас стабильно это 5 гигабайт если мы посмотрим на эти цифры в принципе это достаточно большие цифры мы не можем точно также посмотреть на то а как пользователь взаимодействует система трейсинга когда ему нужно посмотреть на то как выполнялись какие-то запросы как правило пользователь не анализирует тысячи этих тросов Нет он лишь останавливается на каком-то не на каких-то нескольких максимум десятках этих трассов и поэтому исходя из всего мы можем просто банально Не сохранять часть резцов использовать сэмплирование хотелось бы сказать в ягере есть по дефолту хэд Бейс сэмплирования В чем заключается идея Head Backs импирования вот решение а сэмплирование три часа принимается самым первым сервисе исходя из его коэффициентамплирования но Петя здесь четко осознает что если мы дадим каждому микросервису свой коэффициент темплирования то это будет в будущем потенциальная проблема потому что если в какой-то момент мы захотим всем обновить коэффициенты сэмплирования нам придется идти в Исходный код каждого микросервиса Да и менять это там и кажется это будет достаточно проблематично поэтому Пет принимает решение снова дорабатывать архитектуру ягера и он делает такой микросервис под названием sample Manager чем заключается идея теперь коэффициенты сэмплирования хранятся не в этом микросервисе а в sample Manager и каждое приложение интервально через 30 секунд ходит и опрашивает этот коэффициент семплирования что это вообще дает Ну как я сказал мы можем в одном месте менять эти коэффициенты сэмплирования к тому же если какой-то микросервис начинает очень активно себя вести то есть возможно начинает даже доносить до нас с фанами мы можем выставить его коэффициент скомплирования в ноль и он просто перестанет нам отправлять эти данные и к тому же если вдруг возникнет такая ситуация что нам захочется отключить систему трассировки запросов мы банально берём Выключаем включаем всем коэффициент темперирования в ноль и просто все эти микросервисы перестают нам отправлять данные телеметрии и вот когда включается сэмплирование стоит сразу помнить о том что нужно делать принудительное сэмплирование каких-либо важных тосов допустим это те трейсы в которых находятся информация какой-либо ошибки Или те трейсы в которых находится какая-то отладочная информация и что делает Петя вот трясение испаны а в спанах есть теги мы просто помечаем какой-то спам при помощи тега Error и Леди баг если мы находим этот тег в каком-то спальне мы принудительно сохраняем этот рейс невзирая на политики сэмплинированием на коэффициент темплирования здесь хочется сказать это не будет работать потому что решение сэмплирование отрица принимается в самом начале и что приходится делать Но приходится принимать решение в конце и поэтому петь делает В чем заключается идея Tail Basic то есть теперь решение сэмплирования триста принимается в самом конце когда уже весь рейс собран в одном месте они вначале то есть банально простыми словами мы делаем проход по всему трясу то есть по каждому смотрим на каждый если находим тег erro Леди баг мы принудительно сохраняем этот рейс невзирая на коэффициент темплирования и благодаря этому можем обрабатывать 5 гигабайт трафика при помощи семплирования и умеем принудительно сохранять важные трясти в принципе в принципе можно было бы на этом заканчивать Да но нет стоит понимать что когда мы включаем сэмплирование нам сразу же начинает приходить вопросы на тему того А где мой трэйс или где еще чей-то там трэйс и Петя сейчас ничего не может ответить кроме как сказать Ну к сожалению ваш трейс не был зациплирован Петя понимает что это проблема И Петя хочет Возможно не решить ее полностью но по крайней мере постараться и минимизировать и что Петя делает Петя добавляет отдельное хранилище и встраивает ее внутрь каждого коллектора То есть получается своего рода встраиваем база данных внутри каждого коллектора но пока абсолютно не Понятно зачем мы теперь храним тросы не только власти но еще внутри каждого коллектора и к тому же query Proxy дорабатывается теперь помимо того что он ходит за трассами через власти он еще ходит за трассами Давайте разбираться Зачем это все нужно а нужно Это для того что вот это встраиваемое хранилище то есть ИНН и база данных мы так называем в команде хранит абсолютно все трясы в течение 15 минут это значит То что практически любой разработчик или тестировщик может в течение 15 минут найти любой трэйс в системе трассировки запросов дальше трясы Точнее за темплированная часть резцов уходит уже в постоянное хранилище власти силач и к тому же Петя подумал насчет того что нужно сделать принудительное сохранение отрисов постоянно хранилище чем идея когда трейс сейчас находится встраиваемой базе данных то есть же если к нему кто-то обратился велика вероятность что в будущем к нему снова начнут обращаться к задаче или отправят кому-то в сообщения поэтому когда кто-то обращается к сусу когда он находится во временном сториже он берёт и уезжает постоянно то есть принудительно сохраняется и я заговорил про историч хотелось бы пару слов сказать а что это вообще такое реализован он с использованием бадджера баджеры такие велю базы данных написано Go все три сыты хранятся в оперативной памяти трясы там жмутся с использованием алгоритма снапи и суммарный объем оперативной памяти используемой 3 ТБ и да да Петя знает о такой проблеме что во время диплоев точнее редиплоид коллекторов все эти тряси теряются и хотелось бы также сказать что 3 терабайта оперативной памяти Ну достаточно много по моему мнению можно Что сделать можно принципе это все нарезать на кусочки Да но я как говорил в Озоне есть три дата-центра и что Петя делает Петя добавляет в дефекатор каждого трейса индификатор Исходя из этого агенты понимают В какой именно дата-центр им следует отправлять трэйс это нам дает то что мы не гоняем трусы из одного до центра в другой и уже внутри этого центра мы используем шарнирование Почему мы шортируем по 31 скорее всего возникает вопрос мы делаем Это так что нам обеспечить чтобы все тросы какого-то все испаны какого-то адреса попадали в один и тот же коллектор чтобы в будущем по ним провести Tail Base сэмплирование и хотелось бы сказать немного роутинги А как вообще агенты понимает В какой коллектор им отправлять этот рейс у нас что касается сетапа сейчас 30 корректоров и 2100 агентов и у нас в зоне есть инфраструктурный сервис под названием warden warden это своего рода сервис Discovery который просто подписан на большинство авентов внутри кубернетиса и банально простыми словами он знает о местоположении большинства микросервисовна агенты просто подписываются на местоположение корректоров и отправляют они знают где уже находится каждый коллектор коллекторы конечно же используют консистентные хеширование то есть находится на кругу с большим количеством виртуальных чертов вот здесь отдельно отдельно хочу сказать что На данном этапе Петя переписал Ну наверное процентов 60 и я хотел бы теперь то А как вообще Выполняется обработка какого-то одного выборочного трасса вот сейчас у нас архитектура коллектора состоит в плане состоит в виде пайплайна и этот paypeline состоит из разных процессоров то есть что это нам дает в какой-то момент можем удалить какой-то процессор добавить новые заменить местами какие-то процессоры то есть архитектура получается достаточно гибкая и давайте рассмотрим первый процессор он называется группой 30 шардит чем идея спальны приходят в коллектор и из них там собирается трейс дальше этот рейс уходит процессор metrics где по нему по нему анализируется его размер Его длина и дальше пути немного расходится он уходит встраиваем базу данных Где сохраняется на 15 минут Затем он уходит процессор статистик где принимается решение о том будет зацимлирован этот рейс или нет Если этот рейс будет засэмплирован он уезжает власти Конечно же он не поштучно уезжает власти мы используем балки и пачкой отправляем все эти да нные власти благодаря этому а разработчики или какие бы то ни было тестировщики чтобы не было еще какие-то другие специалисты имеют доступ ко всем тристам в течение 15 минут и дополнительно мы не гоняем из одного до центра в другой и здесь я хотел бы озвучить ту проблему с которой мы обсудили в начале это Когда какие-то разработчики задаются вопросом так А куда же все-таки ходит мой микросервис или куда ходит еще чей-то там микросервис и Петя сейчас не может быстро ответить на этот вопрос Петя придется анализировать множество тросов и разбираться но петь и понимает что у него есть огромная выборка данных которые показывают Кто куда когда ходил все что остается сделать это просто преобразовать эту выборку в виде графа зависимости хотелось бы отдельно сказать что в лагере есть депозиции Граф но пару слов хотел бы сказать об этом графике он не реал тайм и к тому же он рисует Граф зависимости сразу для всей системы а вазоне более 4000 микросервисов иногда банально Не хочется рисовать зависимости для всей системы иногда хочется нарисовать граждан зависимости для какого-то одного выборочного микросервиса и Наверное вы догадываетесь Что делает в той ситуации когда какой-то функционал нас не устраивает Да пишет что-то свое и Петя принимает решение писать dependency Граф Он берет и переиспользует микросервис Трейси сатапи И помимо теперь статистики по критическому пути туда еще отправляется пути по каждому спану эти пути агрегируются сохраняются и на выходе получается вот такой вот игра хочу сказать тоже пару слов об этом он Real Time Что значит Real Time это значит То что он показывает кто куда когда ходил например 5 10 или 15 минут тому назад К тому же этот Граф зависимости позволяет строить зависимости для какого-то одного выборочного микросервиса с нужной нам глубиной входящих или исходящих соединений и хочу отдельно сказать убить еще или есть ряд идей потому что как его можно будет улучшать на ребрах графа в будущем можно будет хранить РПС процент ошибок или к примеру тот же самый и благодаря этому в зоне появляется такой инструмент который может предоставлять полную картину взаимодействия микросервисовна и ну и благодаря этому инструменту разработчики могут проанализировать или админы Кто бы это не был Ещё кто куда когда ходил 5 10 или 15 минут назад и Петя дальше внезапно сталкивается с такой проблемой Что кто-то начинает слайсы из миллиона спанов или кто-то начинает слать спаны огромного размера Ну то есть банально кто-то начинает если неправильно использовать систему распределенной трассировки эти понимают что это проблема И Петя принимает решение делать так чтобы решить эту проблему и что Петя делает Петя добавляет отдельный процессор внутри каждого коллектора и называется детектор в чем идея ши детектора ши-детектор просто анализирует рейс если трейс некорректен с точки зрения системы распределенной трассировки он его отбрасывает Давайте посмотрим теперь на возможности этого самого детектора а что умеет делать он в первую очередь он умеет фильтровать некорректные трясы допустим трясы у которых нет рутового спада также он умеет фильтровать некорректные имена спанов фильтровать по их количеству спанов по их суммарному размеру и точно так же он умеет еще фильтровать резцы секретными данными допустим хитросы в которых есть пароль или какие-то токены не имеет смысла это все сохранять Как работает фильтрация у нас просто есть регулярка по которой происходит как раз таки обнарушение этих самых секретных данных и хотел бы сказать что петь вновь приходит идея в голову потому Когда работать архитектуру ягера и он добавляет еще один микросервис он добавляет отдельными точно таким же кодовым названием детектор и как это сейчас работает у нас есть процессор внутри каждого коллектора и вот этот процессор shi-детектор просто детектит некорректный трейс как только этот процессор понимает то что трейс некорректен он отправляет информацию по этому трассе в отдельный микросервис под названием shadetector She Detector поднимает метрику на эту метрику завязано lert и команда какая-либо разработки помещается о том что она как-то неправильно используется система распределённой трассировки и дополнительно если поднимается Метрика связанная с токином или паролями сюда ещё включается команда информационной безопасности и заводит тикет на эту команду и благодаря этому мы предотвращаем неправильное использование системы трейдинга Ну и конечно же дополнительно оповещаем команды разработчиков о том что они как-то неправильно используется система распределённой трассировки вот здесь Петя много чего сделал Но у него все равно еще есть ряд идей потому что можно будет делать в будущем с этой распределенной трассировкой и он хотел бы с вами этим поделиться в первую очередь и так давно Петя переделал пользовательский интерфейс ягера Почему он так сделал потому что один раз пришлось уже туда добавлять критический путь велика вероятность что в будущем снова придется добавлять какую-то фичу плюс ко всему хотелось бы подвести интерфейс инфраструктурных сервисов внутри озона под один какой-то общий вид и к тому же в чем проблема пользовательское интерфейса ягера вот когда у нас идет выборка допустим 20 слов вот с каждым крестов у нас отдаются спаны в виде жетона этих спанов могут быть тысячи или не дай Бог миллионы и в принципе это не нужно делать хотелось бы просто дать Мета информацию по этой выборке допустим состоящей из 20 и когда пользователь хочет посмотреть какой-то Один выборочный тресс он уже проваливается туда и там уже отдается вся информация по этим планам также Петя хочет сделать такую фичу под названием экземпляров В чем заключается идея достаточно типичная ситуация когда у нас есть какой-то график responstime и как правило на эти графиках есть какие-то пики как символ того что какой-то запрос или ряд запроса выполняется медленно разработчикам часто хочется понять А чего зависит медленное выполнение этих самых запросов и разработчик мог бы выделить некоторые участок этого Пика и потом просто взять и перейти в трассы которые как раз таки характеризуют медленное выполнение этого запроса также Петя не так давно осуществил переход от ластика носик тебе что такое сегде скорее всего не слышали Это внутренняя разработка это база данных apient Only документы ориентированная которая разрабатывает система логирования Почему вообще решили делать переход на себе потому что это база данных хорошо себя зарекомендовала внутри озона как хранилище для логов А как я уже говорил логично в принципе это немного Похоже да к тому же это он либо за данных У нас есть хорошая экспертиза по этой данных потому что сопутствующие команды разрабатывает и вот когда мы осуществили переход от эластика на Сеги Биби мы выиграли примерно два раза по памяти и в будущем у нас есть такая вот глобальная цель Мы хотим в будущем отказаться от мемори базы данных и постараться перейти на segdb но здесь стоит сказать почему вообще это делаем потому что 3 ТБ оперативной памяти но это как-то дороговато Да и не хотелось бы терять трясы во время редиплоев Также хотел бы поделиться такой под названием корреляция трясов В чем заключается идея может быть какая-то выборка трассов допустим 1000 каких-то трясет есть те трусы которые ответственны за медленное выполнение запросов А есть те трусы которые ответственны за быстрое выполнение запросов и разработчикам часто хочется понять А чего зависит медленное выполнение моих запросов От чего зависит быстрое выполнение моих запросов и что можно было бы сделать можно было бы взять какой-то тег допустим вот как правом верхнем углу что какой-то элемент не найден в кэше и сделать корреляцию Именно поэтому тегу мы делаем корреляцию смотрим Что у нас зажигаются те трейсы подсвечиваются точнее которые отвечают за медленно выполнение запросов потом делаем корреляцию по тегу что допустим какой-то элемент найден в кэше и смотрим Что подсвечивается как раз таки те трейсы которые выполняются быстро и вот исходя из этой функциональности мы можем четко ответить От чего зависит быстрое выполнение запросов А чего зависит медленное выполнение запроса и хотел бы еще отдельно поделиться такой механизмы квот то сейчас система трейсинга достаточно высоко нагруженная и хотелось бы нам будущем сделать так чтобы мы немного нагрузки с бэкэнда сместили на клиента то есть мы выдавали клиенту определенный элемент или квоту если клиент превышает эту квоту в какой-то интервал времени он просто перестает нам отправлять данный термитрия да да в будущем конечно же хотим быть его пин Source и поделиться со всем этим сообществом Но скажу вам честно нам до этого еще достаточно далеко и что получилось в итоге В итоге Петя переписал большую часть Он создал возможность хранения абсолютно всех в течение 15 минут повторюсь абсолютно любой разработчик или тестировщик может сейчас найти трэйс в этом временном хранилище в течение 15 минут также сделал способный обрабатывать 5 Гб трафика в секунду создал возможность отслеживать связи микросервисов внутри озона и отвечать на вопросы а кто куда ходил 5 10 или 15 минут тому назад также добавил возможность простого поиска путей различных оптимизаций то есть разработчики теперь поменяют какую операцию имеет смысла оптимизировать А какую нет иначе это никак не уменьшит общее время выполнения всего запроса и дополнительно он сделал неправильное использование системы трейсинга практически невозможно и вот здесь хотелось бы сказать что Петя Да Петя конечно молодец Но вы еще лучше и поэтому если вы решите по пути петь и то разворачивайте трейсинг сразу а не когда сервисов уже станет очень много потому что банально это будет сделать в разы труднее также сразу задумывайтесь над фильтрацией некорректных Вот честно Уверяю вас найдутся те люди которые будут как-то неправильно использовать систему распределенной трассировки и дополнительно Не бойтесь переписывать что-то под свои нужды потому что очень часто свои решения бывает много эффективнее и гибче каких-то сторонних А на этом все пожелаем петь и дальше удачи в разработке системы трейсинга голосуйте за мой доклад Спасибо большое красота какая друзья добавляйтесь в чатик если что не успеем пишем туда О смотри Спасибо большое заклад Тимофей компания СДЭК сразу хочется спросить не уволился ли Петя после всех этих приключений Я думаю что у вас с этим все хорошо вопрос такой насколько ну и упала ли у вас стабильность ягера после всех этих наворотов которые вы сделали добавление каждого компонента в систему в общем как бы стабильность на снижается насколько стало хуже Ну вот Спасибо за вопрос я хочу сказать что вот именно к деку медленный возможно возможно я понял вопрос Спасибо хочу сказать что у нас был очень большой трек и он как раз таки назывался под кодовым названием стабилизация трейсинга потому что Ягер он был нестабилен когда допустим я не знаю эти цифры которые существуют сейчас Но раньше где-то мое мнение 35 тысяч спавна в секунду ядер начинает очень плохо себя чувствовать отбрасывать спальны где-то что-то плохо сбоит потом у нас были омы то есть как раз таки вот то что мы писали это своего рода была и стабилизация всего этого а девушка с микрофонами махните руками вот раз давайте быстро в режиме блица Очень спасибо за интереснейший доклад очень очень еще даже более актуально в сфере B2B это все дело потому что для бизнеса еще важнее быстро доставлять эти логики так далее два вопроса у меня во-первых все это касается второй платный Кстати я готов заплатить Спасибо короче первый вопрос все это касается только http запросов между микросервисами У нас например очень много если очень грубо говорить микросервисов скажем так участков системы которые потребляют данные из разных систем кто-то мы пишем вредит потребляет другая система что-то мы пишем там через всякие брокеры типа Кафки там еще что-то такое много чего еще такого можно придумать что передает данные не напрямую запросами через всякие промежутки Можно ли как-то в этом случае обработать Да конечно У нас клиентских библиотеках есть логика не только на пробрасывание контекста через то есть умеем пробрасывать контекст через репетиции это через метадату Насколько помню работает через кавку на достаточно типичный выход когда допустим у нас есть какой-то пилот и мы делим его на две части допустим был бизнесовой частью какой-то информации где там допустим Окей спасибо второй вопрос забыл я сейчас вспомню еще хорошо спасибо Вот Будьте добры Привет Спасибо Женя приходил Петя условный Вася со словами У нас тоже микросервис как бы но он старый он Legacy Мы тоже очень хотим участвовать и не поглотила ли Это Вася трудозатратами Спасибо за вопрос Да бывают такие кейсы То есть у нас есть Framework scretch который позволяет запускать микросервиса в рамках озона очень быстро и там уже это инстанцируется все из коробки То есть это очень просто но бывает случаи когда кто-то приходит каким-то старыми красителям каким-то и сам пытается что-то устраивать как правило он идет Не к нам А команде библиотеки потому что команда библиотеки трассировки занимаются другие люди и вон там уже больше коммуницировать иногда все-таки приходят какие-то вопросы там как это с интегрировать но это очень редко Спасибо Привет Step View Time по памяти относительно ванильного Джаггера вопрос Да нет со всеми вашими работами Ну я хочу сказать просто есть такая тенденция что трейсинг может не развиваться но он будет развиваться как бы это глупо не звучало Почему микросервиса становится больше больше становится нагрузки это все становится все больше и больше и это мне кажется немного неправильно измерить нагрузку при 1000 микросервисов и допустим через год через на двух-трех тысяч микроселица вот то есть просто шел трек планомерного развития прессинга и стабилизации будущем нагрузкам и это сейчас продолжается потому что с каждым годом больше нагрузки больше микросервисов Смотри ты Кремень прямо тебе предложили сравнить себя с ВК ты вообще даже не повелся Меня зовут Андрей Вопрос такой почему выбрали ластик на него были идеи в плане того что если leg House я понял да да Как раз таки да Как раз таки насчет пихауса Мы думали если мы когда-то захотим делать будущий аналитику то сохранять растение только сейчас cdb но еще дополнительно В клик-хаус Но честно хочу сказать рейтинг зарождался в 2018 году мог бы ответить так что это исторически сложилось потому что Петя была Экспертиза и потому что ну ластик хорошо себя зарекомендовал как для хранения логов это похоже ответ будет Спасибо кстати эти все это время ему делали массаж евреи Спасибо за доклад очень интересно детектор Когда вы метрику это показываете разработчикам про это Они знают пока достаточно сухая информация но в будущем Конечно все мобилити расширяется дается больше больше информации там и мониторинг системы и так далее будущем конечно хотим больше предоставлять что вот такое такой-то такой-то правила нарушена сделай вот так вот так пока достаточно Спасибо а вот можно передать обратно второй вопрос и вот сюда Принесите пожалуйста здесь вот у нас будет Спасибо я готов заплатить кому нет нормально Все короче второй вопрос Да я вспомнил очень такая серьезная боль для скажем таких достаточно небольших команд разработки Вот вы очень очень много переписываете как я понимаю готовых решений Как быть в случае когда например Через пару лет через два-три года основной вот этот инструмент Джаггер допустим перепишется внедриться очень очень крутая фича вам уже нужно это дело поддерживать обратно к себе это прям серьезная боль как вот это дорого все дела долго опять Да но я хочу сказать мы давно отказались от обновления ягера и ушли в собственную стезю вообще вообще друзья все что не успеем потом Напишите в чатик и мы проконтролируем спикеры поотвечали вопрос насчет компилятора и представление первый момент какого уровня вложенности компилятор будет представлять как принимаете решение об остановке Как смотреть тут ведет себя компилятор если Да но это второй вопрос просто я не знаю причем тут компилятор у нас трейсинг не инстанцируется так как ВК допустим при помощи аннотации у нас это просто инструментирует каждый разработчик вот поэтому как он выберет в каких местах он проставит там и будут эти спаны что касается первого момента Какова глубина этих спальных но глубина будет зависит не совсем от суммарного размера но тем самым никакую глубину не ограничиваем просто ограничим количество этих спанов То есть у нас по-моему лимит 86 400 если этот лимит превышается мы уже отбрасываем эти Спасибо за доклад Расскажи про коллектор Я правильно понял база натянута поверхность информацию в какой коллектор Да вот как раз таки есть у нас клиентский Роутер у нас есть сервис Discovery warden и он подписан на большинство ивентов простыми словами он знает большинство местоположение агенты подписываются на это все у них есть адреса этих коллекторов и используется клиентский роутинг Да и дальше Просто там используется консистентное хеширование то есть располагаем эти коллекторы на кругу это все стоит Если не ошибаюсь текущих цифрах и потом Просто берем хешиваем 3 понимаем какой коллектор отправлять скорее наверное чет Тогда просто догонку Почему не отдельная мама ребаза который бы просто коллекторы ходили ну здесь был небольшой кейс почему это так сложилось Бывают такие случаи когда кто-то отправляет рейс огромного размера допустим запустился какой-то демон который в цикле отправил спам 100 миллисекун уснул отправил спас 100 миллисекунд уснул и так объективная причина разные бывают продолжительные действия и в свое время нам хотелось мёрзшить эти Трейси то есть они поступают коллектор мы их до мерщиваем в принципе можно было бы рядом поставить мемори база данных Я не спорю Ну хотели выиграть скорее всего такой ответ Спасибо доставка пакетов у вас происходит особенно часто Хороший вопрос по поводу Египет агенты находятся на той же самой ноте что и клиентские приложения и поэтому просто через память прокидывается это изменение график друзья прошу прощения для всех микрофон должен быть перпендикулярно Понятно интерфейсу Вот так вот да спасибо большое мужская глубина обычно изменения в графе использования происходит при диплоид Ну там добавляется функции это у вас происходит автоматическое поднять спам коэффициентов при теплое то есть там можно сделать делается там график день после тепло это максимальный потом снижается и потом стандартного значения или это даже не было такого имеется ввиду теплой коллекторов или тепло каких-то микро сервисов здесь мы ничего не используем никак не меняем этот коэффициент понимаете что какие-то функции если они будут просто пропадать Я понимаю но это выставляется в рамках этого микросервиса и дальше критика по времени черная полосочки Кто определяет саму критику система или какой-то инженер выставлять там столько-то Я имею когда они появляются а имеется ввиду как детский путь считается Да но у нас есть два вида есть Real Time подсчет критическая пути то есть этим занимается но по сути там задача сводится к поиску длинного пути самодельного пути в графике приблизительно вот а что касается множество трейсов это уже пакетная обработка то есть запускается определенная джоба которая берет набор этих данных и просто я не помню интервал то ли раз пять минут берет и пересчитывает это все агрегированное по множеству Спасибо большое красавец"
}