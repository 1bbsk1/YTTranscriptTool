{
  "video_id": "SH6wilDg6BY",
  "channel": "HighLoadChannel",
  "title": "Как создать автоматизацию детекции и оценки сбоев в Production / Дмитрий Химион (Авито)",
  "views": 630,
  "duration": 2654,
  "published": "2023-01-19T06:56:46-08:00",
  "text": "всем привет так времени совсем мало доклад ага кратно врезался по размерам вот давайте стоило сразу начинать тема доклада собственно автоматизация детекции сбоев их оценка значит меня зовут дима химия работу в компании рита занимаюсь разными штуками разработка инцидент менеджментом обеспечением качества в общем штуками стоящему рядом небольшое интро это достаточно большой продукт у нас больше тысячи разработчиков это примерно 60 команд разработки это около 200 250 релизов в день естественная часть из которых проходите всегда гладко это тысячи микро сервисов это несколько дата-центров это больше ста крп с это 50 миллионов уникальных пользователей день это действительно большой высоко нагруженный проект и мы много лет назад естественно начали работать с инцидентами чтобы они не повторялись чтобы их можно было как-то держать в узде им противодействовать и этот процесс он обрастал подробностями в конце концов был частично автоматизированы и вот этой этим опытом сегодня хочется поделиться давайте посмотрим у кого на этой неделе был сбой в продакшене ну очевидно у нас были не раз вот эти штуки происходят достаточно часто именно поэтому этот опыт может быть очень ценен значит как мы пойдем по докладу первое что мы обсудим это сборе что это такое ключевые какие-то моменты по ним чтобы понять как дальше будет строиться наш наша беседа расскажу про проблематику инженерии которая имеется расскажу как работает наша система расскажу ключевые моменты и факторы успеха для построения подобные системы расскажу чем мы платим за то что у нас есть вот такая автоматизация расскажу бенефиты в конце концов который она дает который на мой взгляд должны побудить вас построить что-то подобное но подведем итоги по задаем за вопросы итак небольшое погружением разберемся как выглядит инцидент какие бывают типы и как посмотреть их масштаб вот маленький коротенький инциденте который происходит достаточно часто он понятно и видно общее поведение какой-то метрики ключевой и видно ее падения есть инциденты куда более сложные из тяжелые инциденты продолжительностью больше часа мы видим здесь вот наложено несколько метрику них разная сигнатура падения разная глубина падения разное поведение при восстановлении и так далее вот есть куда более до сложные паттерны и деградации наших продуктов по поводу типов из опыта могу сказать что наверное основных типов инцидентов 3 это аналитические связанные с потерей бизнес данных на основе которых делать какие-то принимаются бизнес-решения до что хорошие это изменение нехорошая значит стартует стартовал этот продукт успешный ли этот функционал и так далее технические проблемы о которых сегодня и пойдет речь в основном так как я сижу разработки и мы в основном с ними боремся есть продуктовые инциденты когда технический продукт работает хорошо нет никаких признаков деградации но ему все равно плохо пользователи все равно страдают и так дальше масштаб сбоев масштаб сбоев по опыту это равно потери наверное нужно выделить три основных канала потерь которые можно прыгать и мы трека им это финансовые потери да что мы теряем что бизнес теряет во время инцидента это потеря данных каких-то критических данных для пользователей для нас для бизнеса и потери клиентского капитала когда пользователь недоволен он говорит знаете я пойду в другое место и там сделаю тоже самое и так значит это все приводится к общему знаменателю и можно все инциденты приведя к общему знаменателю как-то отранжировать следующие почему мы стартовали вообще автоматизацию небольшой кусочек нашей аналитики значит вся практика началась 2017 году вот в тот год было замечено и за процессе на примерно 70 инцидентов здесь я взял просто последний квартал в восемнадцатом году мы вышли на нашу не знаю наверно крейсерскую мощность по про по процессингу инцидентов и это не значит что мы видели все и дальше стало понятно что мы уперлись ну скоро упремся в свои свою пропускную способность а хотим видеть большие понимаем что процессе далеко не все и можем понять и оценить далеко не все вот а достаточно много ручного труда занимала сбор информации по инцидентам особенно оценка потерь по разным каналам это кропотливый длительный ручной труд вот соответствует нас там году у нас появился mvp и начала развиваться автоматизации концу года по статистике мы уже два из трех инцидентов редактировали автоматизировано вот ну дальше видите что этот процесс уже полностью автоматизировал ся и мы наверное сейчас где-то на 98 процентов видим то что у нас происходит вот и так как работает эта замечательная система первое что я расскажу это про общую информацию модель сбор информации по инцидентам определение границ формирование из инцидентов сбоев расскажу там почему какая разница между инцидентом с боем по крайне мере наш терминологии до сбор контекста это как это все дальше переходит в человеческий процесс по проблемой наш youtube ну и нотификации естественно и так общая информация наш стек это питон клик house графит с десяток интеграции сейчас это выглядит как три ретроспективных сканер а3 сканера которые в runtime не работают и показывает нам какую-то критическую информацию с небольшим отставанием от реального времени два прототипа которые сейчас проживают свою со взлет попытку взлета и вот сразу сказать что на этапе у нас ушло примерно 4 месяца и на полноценный такой продукт который мы более-менее стали довольны и два года и так чтобы лучше понять как работает эта система мы разберем сегодня один кейс вот было достаточно плохое утро который началось 840 и закончилась 12 мы видим здесь пачку пачку каких-то сбоях непонятно самом деле сколько здесь их максимум их здесь 13 до по количеству деградации но сколько их реальность не понятно и вручную на это ушло наверное не знаю часа четыре ковырять все со всеми участниками этого процесса общаться собирать информацию лазить по мониторингом коучим систему в общем заниматься ручным сбором чтобы понять финального что все-таки случилось и так все это засунут а вот в такую схему которая автоматизировано да это флопа которым проходит вот эта цепочка деградации чтобы потом рассказать нам а сколько же здесь в реальности было сбоев и так первое что делает эта модель редактирования мы берем какой-то временной интервал и говорим и а ну-ка расскажи нам вот здесь вот есть какие-то деградации или не значит что происходит вот сам отмеченный 1 2 3 4 5 это как как идет логика то есть мы сначала собираем данные данные значит по которым у нас модель принимает решение они состоят из двух частей из тех что мы выгребаем в ран тайме и тех что мы берем из кэша потому что они являются при калькулировании то есть они при дрочи ты ваются на опережение так как в ran to me это слишком много времени занимает чтобы делать такие тяжелые вычисления вот дальше все залезает в модель деградации которые собственно уже и в себе и содержит логику принятия решения о том что такое норма что такое не нормы в конце два варианта либо у нас не было инцидентов и наговориться хорошо либо на говорит что у нас инцидента есть и мы идем дальше итак мы говорим что инцидент у нас есть вот они выглядят так она находит 13 инцидентов с какими то там с темпами вот что происходит дальше дальше происходит насыщение этих временных точек метриками то есть что происходило в этот момент по ключевым метрикам после чего есть система div рейтинга div соткой die fighting это система которая позволяет нам понять это ложноположительные срабатывание или нет то есть такое вообще может быть в нашем продукте или нет это набор правил которые фильтруют возможность той или иной ситуации говорит что вот такой может быть такое не может соответственно пройдя через где фрикинг мы получаем что у нас есть два ложноположительных срабатывания который мы выкидываем и говорим что мы с ними мы не знаем что это такое быть ими еще не сделаем наша модель говорит что это не деградация на самом деле это конечно может быть деградации или отголоском какого-то другого типа инцидента но наша модель данные об этом не знает мы их мы говорим что мы их не процессе идем дальше нас осталось 11 инцидентов и следующее что мы делаем это находим границы инцидентов то есть когда же каждый из этих инцидентов начался когда закончился как-то выглядит ну есть нехитрым математические алгоритмы которые понимают где метрика начала себя вести в какой момент времени уже не совсем не совсем обычно и когда она все-таки восстановила свое нормальное поведение это становится интервалом каждого инцидента это и того мы например не смогли например на этой стадии найти границы еще одного инцидента и того мы процессом 10 инцидентов которые нас действительно остаются валидны my понятными и мы с ними понимаем что делать следующее следующее что происходит это сбор признаков инцидентов мы лезем в разные места это база данных в метрике сервисов в то как у нас работают дата-центры как работку ciroc убирается как работают бизнес-процессы и так далее и собираем по нашим инцидентом достаточно развести контекста это инцидент становится такой многомерной сущностью которую дальше мы снова пропускаем через систему де файтинга который говорит что вот такая комбинация общем вот такое представление цедента она не валидно и какие-то инциденты могут отвалится или нет ну допустим мы говорим что еще один инцидент у нас отвалился до для красоты и все остальные нас обросли какими-то подробности что происходит дальше дальше происходит определение группировка сбоев уже точнее инцидентов в сбой то есть много-много маленьких точечек они превращаются в одну в один сбой понятным который имеет начало и конец мы понимаем что вот здесь была одна проблема и она у нас становится уже полностью оформленной мы понимаем что вот вот это одно возгорание это другой возгорание и соответственно у нас уже вот эти все инциденте ки группируются и выглядит уже как два сбоя и того у нас из вот этих 13 непонятных деградации получилось 2 сбоя у нас начало горела одно потом начала гореть другое вот это выглядело как какая-то рваная 1 деградация вот вот такая интересная штука что происходит дальше дальше когда у нас есть собранный инцидент простите свой canon съесть собранный сбой мы можем по нему финально сделать оценку потерь потери как не помню выполняют по нескольким разным типам и соответственно много каналов давайте посмотрим как это выглядит значит здесь начинают работать у нас различные математические алгоритмы это здесь в общем можно применять разные штуки в основном это численные методы это всевозможные интерполяции и экстраполяции построение for костов аппроксимации всевозможные чистки данных с помощью там различного рода фильтров типа ранса фильтра и так далее вот ну и соответственно это все приводится к общему знаменателю денег давайте посмотрим как это работало и как эта штука зрела вот мы видим здесь с левой стороны у нас есть длительный инцидент и на ранней стадии у нас получалось строить вот такие for касты по метрике естественно этот farkas никак не проксиме руется да никак не сходится с поведением общем поведение метрики им воспользоваться нельзя если вы посмотрите и сзади потери они будут лгать заведомо после доработок нашего алгоритма у нас фитинг с общим поведением метрики стал сильно лучше это с правой стороны мы видим что инцидент то есть farkas очень хорошо фетиса с наиболее ожидаемое поведение метрики в этот момент времени ну и соответственно точность расчета потерь здесь сильно лучше есть другой тип проблем который связан с тем что у нас рядом с одним из один там находится другой соответственно происходит дата коробку и наш алгоритм который строит for каст по поведению метрики соответственно считают потери он снижает мы видим здесь на левой стороне что штрих пунктирной линии показан это как бы он должен был быть и вот зелененькая оно видно что она просела и мы андрасте мы этим потери вот с этими штуками мы тоже поборолись и их победили и получилось штука вот которая показана справа ну есть соответственно совсем сферические штуки когда наш алгоритм расчета потерь он показывал типом и папе потеряли там два ввп страны за 2-часовой деградацию естественно такой фильтровали сделали pounding наших алгоритмов чтобы они никогда не вываливались логично возможные рамки в принципе да ну и получилось уже зрелый механизм который даже на ночь и на день может строить достаточно хороший фуркад поведение метрики как показано справа и соответственно считать артерию что еще интересно интересно что у всех меток из отскок мы учились его считать как после окончания инцидента как с левой стороны показано так и бывают такие моменты что инцидент с бой ещё не закончился да какая то вот бизнес метрика она отскочила ей уже стало хорошо и она восстанавливается и вот алгоритм начал учитывать с различного рода разные поведения восстановления метрик и за созрел и теперь какой он работает для нас весьма надежно и точно были интересные кейсы когда есть деградация и есть отскок по метрике которые в 3 раза больше потерь значит ли это что мы заработали во время сбоя в данном случае да значит ли это что вы можете брать прикладывать свой продукт и зарабатывать на этом ну наверное нет вот но такие вот интересные штуки тоже всплывает и так идем дальше пустого км и каньон заканчиваем оценку потерь по разным каналам нас начинается соответственно приоритезация этого тикета с боем сбор лейблов которые нужны разным командам чтобы это не знаем по фильтрам каким-то находить свои ticket и сбор attachment of сбор description по которым мы опять же лезем в купер найти там разные дэш бороды собираем чтобы на разборе инцидента можно было воспользоваться этой информацией более качественно разобрать инцидент не соответственно лезем в наш бас и смотрим что там какие команды были за affection и да ну то есть понимать какие бизнес-единицы у нас какие команды разработки в этом всем поучаствовали чтобы их можно было позвать на разбор инцидентов что происходит дальше дальше получается появляется вот такой ticket который мы собственно пуляем в дыру вот соответственно у нас есть блок потерь который сейчас естественно за летом цензуры я не могу показать что там происходило значит сбор данных по инциденту у нас вчера был так рад прутков детектор если него ссылка что можно кишки поковырять есть список дипломантов которые были прямо перед инцидентом есть всякая всякая всячина которая говорит о том что деградировала там внутри бэг-энда во время инцидента и так далее плюс есть вот такие картиночки с алебардами чтобы можно было понять вот он инцидента вот messengers на направо на верху здесь как плохо видно но у него там сигнатура восстановления было очень длительная ядра ля-ля-ля-ля и вот эта информация она достаточно полезная на разборах инцидентов что дальше происходит мы завели ticket жиром если у нас пролетает какая-то обновленная информация по инциденту мы ее обновляем в герате кити и дальше соответственно все эти данные мы складируем отдельно в базу в базе клик house кладем который подальше проливается в 2-х и на основе них мы строим достаточно развесистой у аналитику по сбоем который происходит по виду что происходит дальше также происходит notification летит сообщение флаг канал где сидят инцидент менеджеры мы и там значит есть инцидент вот такая эта дата вот со скольки ты доставить это был был инцидент вот вам тикет разбирайтесь соответствие мы его там процессами дальше ребята в проблем менеджмент забирает его уже крутить понимать это какая-то проблема которая 500 раз уже было и 501 раз стрельнула и нужно ее снова как бы подсветить и посмотреть section этими что там с ними делать и так далее и так вот как работает эта штука какие бенефиты мы из этого выгребаем первое у нас есть количественная оценка потерь который мы несем инцидентах мы не говорим типу на стану плюс минус 100 200 инцидентов год нет мы можем сказать конкретные цифры можно поставить сколько мы там потеряли в по каким каналам какие команды сколько потеряли кто кто больше инцидентах участок то меньше мы можем сказать например что у нас почти 70 процентов сбоев спровоцировано людьми то есть какими-то действиями разработчиков или тестировщиков или тех кто катил что-то фратаксин мы можем сказать что 850 тысяч рублей потерянных и это человеческий фактор все остальные рублей это какой-то какие-то иные факторы на основе этих данных у нас есть расписание наименее опасных часов для технических работ для всяких учений обновлений там не знаю конфигов и джинсов миграции баз данных переездов и сама дата центров другой и так далее у нас есть метрика доступности а вид и для пользователей никто которая строится на пинговании страницы и доступности 500к не 500к а это куда более достоверная информация по возможности пользователя взаимодействовать со вид и пользоваться им полноценным мы знаем сколько обращений в поддержку генерирует наши сборе сколько оверхеды работы генерируют наши инженерии нашему саппорту мы знаем тренда и динамику наши инженерии от года к году мы можем видеть как развивается на инженере стагнирует где лучше где хуже становится и соответствие можно принимать решения о том куда крутить какие штуки релизные процессы тесты покрытие так далее у нас есть представление о том каким должен быть сбой в авито какими свойствами должен обладать с длительностью сигнатуры и так далее и у нас есть в конце концов runtime информация о состоянии vita которую можно не искать по тысяче чате к мазать его на место посмотреть все ок или не ок если неё кто сильно не ока или так чуть чуть вот соответственно от слов к цифрам например мы можем построить каркас по количество сбоев на двадцать второй год сейчас это примерно 500 боев мы можем узнать и страны штуки например когда реально разработчики начинают работать и когда они заканчивают работать и когда они например обедают например мы можем посмотреть что во время окончания спринтов инцидентов у нас на 20 процентов больше а в конце каждого квартала мы начинаем пир фармить быстрее катить быстрее и жестче с допущениями соответственно у нас на 3 инцидента становится больше то есть например следующий месяц у нас будет жарким мы уже знаем мы также поняли из инцидентов что цифры от года году то есть абсолютно цифра инцидентов их 300 500 1000 вообще говоря имеет на самом деле маленькое значение она ничего после сама по себе не говорит вот как это не парадоксально давайте посмотрим почему вот у нас есть тренд роста количества инцидентов 19 20 21 год большой рост вообще плохо но если пересчитать потери в этих инцидентах в относительных величинах да вот что у нас девятнадцатый год это было 100 рублей грубо говоря в двадцатом году при том что инцидента стало два раза больше у нас общие суммарные потери снизились на 23 процента за сперма не выросли да и куда важнее смотреть на среднюю стоимость инциденты и мы видим что она у нас год от года она падает она сокращается и это куда более интересный показатель для нас соответственно вот разбивка из инцидентов по распределение их по окнам длительности то есть инциденты длительностью 30 минут и меньше там от 30 до часу и так далее вот такое распределение мы видим что три года у нас примерно одинаковое распределение да что мы идем примерно в одном темпе при том что у нас количество инцидентов растет есть целевое состояние это штрих пунктирная линия когда у нас 80 процентов инцидентов они являются лежат в зоне 30 минут почему а вот такая интересная информация первое всего 30 процентов инцидентов 35 прошу прощения грят генерирует 70 процентов потерь и это все длительные инциденты короткий инцидент и не ряд всего 30 процентов потерь что интересно что 19 20 год у нас сигнатура этого распределения изменилось на 6 половиной процентов то есть шесть процентов из длительных инцидентов переехали в короткие и абсолютное количество потерь изменилась она сократилась соответственно отсюда есть и понимание того к чему мы можем должны стремиться суммы должны вот наш целевой график спустить всего на 4 процента 4 процента инцидентов ну допустим у нас их будет 500 в этом году соответственно 2020 инцидентов нужно перетащить из правой части влево это наша цель вот вот такие интересные цифры а теперь о проблемах и трудностях с которыми придется столкнуться это качество актуализации метрика это актуальность владению продуктом это ложноположительные срабатывания и слепота и частичные видимости инцидентов и точность расчетов давайте посмотрим поближе на каких метриках точно не получится построить подобную систему это высоко волатильны и метрики или рваные метрики ну то есть когда у вас в метрике нету данных вы не можете просто принять решение здесь как бы плохо был или так и должно быть да соответственно высоко волатильной метрики с ними проще ну той с ними можно работать но нужно огромный багаж алгоритмов который будет их приводить во что то что будет вам говорить хочешь по то ценное вот соответственно у вас все такие метрики ну вряд ли у вас получится соответствие подходящие метрики метрики с низкой дискретизации то есть которые пишутся раз там в 10 минут или раз в 30 минут они будут ограничивать вас то эти инциденты которые меньше окна дискретизации вы будете просто пропускать соответственно для подобной системы актуальные и применимы только высокочастотные метрики которые пишутся там не знаю хотя бы раз 30 секунд раз в минуту ну соответственно какое-то окно дискретизации которая вас устроит вот соответственно следующая штука происходит изменение метрики ее поведение вот такое да эта команда взяла и перевела streaming метрики с одного места в другое метрика вот так выросла естественно это аномалия была детектирования как с бой хотя это не сбой люди могут просто взять в командах и увидеть бак и пофиксить его и сигнатура метрики изменится соответственно с этими штуками нужно как-то жить нужно как-то их отлавливать и отфильтровывать фол спалить его следующая вещь актуальность владение продукта сегодня одним куском продукт владеет эта команда завтра другая послезавтра 3 вот что-то падает между стульев и с этим тоже нужно работать потому что нужно понимать а кого-то называть на разборок понимать какая команда здесь ответственно и так далее соответственно положительно срабатыванием это по сути говоря правил работы вашего продукта что в рамках вашего продукта может быть и чего не может быть это эти правила не сильно коррелирует на мой взгляд с правилами всяких инфо сек систем которые ловят всякие уязвимости и там есть фолз позитив снимите вот здесь примерно такая же штука хорошо если у вас фолз позитивов меньше пяти процентов по сбоем ну вот такая картина вот к ней что это крайне мере стремиться и так самое интересное слепота частичная видимость инцидентов когда мы делали в своем веб и мы его сделали простым как топор мы начали видеть вот кусочек инцидентов и что-то вот сбоку там дымится горит мы чувствуем что из-под двери идет какой-то дым и запах мы не можем ничего сказать да там наверное что-то плохо но больше ничего мы можем сказать естественно чтобы объять все авито система пришлось слегка изменить и усложнить вот мы естественно стали видеть сильно больше и это естественно поднимается и доверие к системе и показывают полную картину мира но продукта он не стоит на месте тысячи разработчиков каждый день по 250 раз и это правда него меняют и соответственно происходит сползания до продукт начинает развиваться он уползает из-под вашего инструмента потому что он развивается вот соответственно это еще один челлендж это ещё одна трудность которой придется френсиса из которой нужно будет то есть придумывать механизм и как с ней бороться следующая штука это точность расчета инцидентов это наверное вторая по важности функциональность после модели определение деградации это та штука которая формирует доверие к вашей системе если вы неправильно посчитали потери люди могут перестать доверять этой системе у нас на ранней стадии был ты были такие кейсы что у нас был инцидент длительностью там полтора часа ребята вот произошла плохая ситуация мы выкатили инцидент считали там за там 500 тысяч рублей не схватились за голову хотели уже там вешаться что они так жестко на faq opel и так много потеряли а потом вы посмотрели у нас тут чего-то фитинг не очень защитился мы его пересчитали и потери уменьшились в два с половиной раза мы такие вот знаете здесь в два с половиной раза мечники вы чё офигели мы тут уже как бы стреляться уже готовы были вешаться а вы нам было так в общем это эта штука которая отвечает за доверие к вашей системе сюда нужно будет инвестировать много математика часов чтобы ее сделать надежный следующая штука ключевые факторы успеха который на мой взгляд с определяют возможность создания подобной системы у вас компании это команда необходимости ломай нс этом первом нужен человек как в сознании программирования чтобы всю эту балалайку описать программной оболочки тестировать такую систему достаточно сложно поэтому навыки тестирования здесь нужны прямо extent of мощная и вам нужно будет найти кого-то кто сможет придумать как всю эту штуку тестировать вам нужен будет идеолог и визионер понимание того куда и как эта штука должна развиваться нужно понимание как работает бизнес и как работает продукт так как создать систему которая понимает как работает продукт невозможно без того чтобы понимать как сам продукт работает вот такая тавтология соответственно кто-то сознание математике мы соответственно аналитик который может быть все данные кукле какие-то графики чтобы можно было смотреть тренды можно было смотреть какие-то ошибки и так далее вот ну или это может быть несколько человек или 1 человек следующая штука без которой ничего не получится это мониторинг мониторинг это источник данных источник метрик соответственно нет развития этого мониторинга вряд ли что-то получится и того значит первое что над сказать вам необходимо определиться в том с тем что такое свой для вас какие сбой вы хотите детектировать до их есть несколько типов вам нужно понять на какой вы нацелены надо принять что сразу ничего не получится и нужно будет несколько моделей перестраивать iterative прийти к той штуке которая для вашего продукта будет видеть те самые сбоя о которых вы хотите знать дальше для автоматизации нужны две вещи компетенции перечислены и мониторинг соответственно с какой-то группы людей нужно будет дружить особенно с командами не the ringer ваш продукт это живая система она развивается и нужно будет какую-то часть операционка всегда закладывать за то чтобы понимать что ваш ваша система она достаточно хорошо видеться с поведением продукта вот соответственно вы всегда будете жить в том что вы видите не все и вы приближаетесь к тому чтобы видеть просто большую часть или подавляющую часть вот потому что видеть все это очень дорого соответственно автоматизация в нашем по крайне мере в нашей компании нашем продукте находит много новых интересных способов применения она начинает notify ци ровать например наш саппорт о том что у нас сбоит например маркетинг пиара еще кого-то в общем всех бизнес интересант of которые хотят знать о том как сейчас работает продукт в простом представлении они через миллиард непонятных дашбордов соответственно сейчас мы видим в 45 раз больше инцидентов чем то что мы могли процессить в прошлом это это хороший показатель да это достаточно высокая полнота картины и у нас нет такого чтобы не понимаем что происходит в таком огромном продукте соответственно еще одна вещь которую нужно сказать это то что мы можем посчитать сейчас свою разработку мы можем понять в какой команде с инженер кай хуже чем в других и мы это понимаем количественно они методом ленинского прищур а и потому что у кого-то есть подозрение что в той команде что-то хуже нет мы у нас все это оцифрована мы можем по как бы по общему знаменателю смотреть все команды и понять где что хуже где что лучше вот такие пирожки давайте вопросу да нужно во-первых вручите подарок спасибо большое за отличный доклад спасибо большое так давайте вопросы ну вот например вот слева дмитрий привет спасибо за доклад очень круто у меня два вопроса первый по поводу нотификации в slug в текущих реалий столкнулись с ограничением от сладкой и сделали переездом другой какой-нибудь мессенджер для уведомлений и второй вопрос по поводу определения команд которые виноваты в инциденте как вы обрабатываете корнер кейсы ну например какие-то инфраструктурные сбое которые сразу несколько сервисов затрагивать но это например там с базой проблема вот как в таких случаях происходит значение команда спасибо окей ну диверсификация по массажем сейчас понятно есть ну то есть нет никакой проблемы сделать интеграцию с api и любого другого мессенджера которым вы пользуетесь сделать несколько каналов модификации что-то такое по поводу значит копания в глубь у нас есть егерь то есть у нас есть tracing запросов соответственно он позволяет нам понять из каких мест у нас начали эти ошибки и эти ключевые просто места выделить в волейбол и выделить их в описании в description соответственно тикета и самое когда главное у нас есть пас через который мы можем понять кто владеет конкретным кусочком продукта и привлечь конкретную команду к разбору инцидент вот как так по центру дмитрий спасибо большое за отличный доклад было очень интересно и реальное чувство 8 осаго докладе меня зовут дмитрий симонов основателя сибирского клуба у меня главный вопрос как пользуются результатами работы вашего отдела выстроенный ли бизнес-процессы по развитию улучшению вашего продукта собственно то для чего ваш отдел создавался ну и в догонку там маленький вопрос вы упомянули что при увеличении о общего количества инцидентов расходы на них уменьшились хотелось бы пояснение спасибо смотрите значит был тренд между 19 20 годами в девяностом году у нас было меньше инцидентов но стоимость их было больше то есть у нас были более тяжёлые инциденты в которых мы теряли больше в двадцатом году мы поработали у нас было несколько технических проектов которые по грейс пул degradation у если быть точным которые просто демпфируют для пользователей внутренние сборы сбои в работе продукта и для пользователя функционирование авито проходит ну более сглаженной они меньше видят инцидентов поэтому один студент становится дешевле вот соответственно при росте инцидентов получается вот такой вот такая себе натуры значит по поводу наш нашей команды у нас сейчас формировался прям отдельный юнит инцидентом проблем менеджмент наши основные цели это собственно влияние на количество и свойства инцидентов чтобы они для бизнеса становились дешевле их было меньше меньше это кстати говоря не не самая главная цель чтобы они были дешевле чтобы мы их видели быстрее обнаруживали быстрее и чинили их быстрее то есть мы сейчас развиваемся в область того чтобы вот этот весь процесс по детектирование коммуникации инцидента его починки его влиянию на работу продукта на бизнес на пользователей сокращать вот соответственно у нас есть за много лет накопленные данные на основе которых мы можем строить разные инициативы которые помогают нам собственно понимать вот было так стало так вот эта инициатива помогла не повод дмитрий спасибо большое за развернутый ответ из вашего доклада понятно что вы все это можете делать мы вопрос был про то как вы реализуете эти ваши возможности как выстроенный бизнес-процессы по передаче вов команды инцидентов и о том что как вы контролируете что эти инциденты исправляются а все теперь понял это человеческая часть этого процесса это проблем менеджмент то есть вот эти инциденты которые мы реагируем и пуляем в jira значит они процессе ци дальше они привязываются к паспортам у нас есть инцидент менеджеры которые с командами разбирают каждый конкретный инцидент по инцидентам пишется экшена этим анилин куются в а по смартом соответственно у каждого экшена этим и есть ответственная команда я и соответственно у нас есть выгрузка всей нашей жиры в пвх и мы можем трогать по каким инцидентом какие action найти вы были сделаны где они зависли и соответственно мы можем коммуницировать с командами можем коммуницировать соответственно эскалировать на руководителя команды или на руководитель разработки кластера если это необходимо я ответил на вопрос спасибо большое за развернутый ответ ну и в догонку последнее уточнение развивается ли эти процессы потому что понятно что команде можно складывать в копилку эти инциденты и так как они будут иметь минимальный приоритет они будут лежать там месяцами а то и годами а вот если работа с этой приоритизации если бизнес-процессы по увеличению этих приоритетов ну вот это вот и все да да да давайте расскажу как это работает у нас есть такая штука как как тмм тематике моду в которой значит команд каждая команда она проходит некоторые review своих процессов разработки тестирования работы с продуктом значит там продукт discovery проблем discovery и так далее и собственно в этой темке есть пункт о том а как команда работает с инцидентами соответственно если команда с инцидентами инциденты в вагоне так что найти мы не делает это все ложится на плечи и на оценку руководитель разработки который должен в блоге вместе с продуктом балансировать как быть их долг и продуктовые развитие вот как это приземляется как бы не спец ability команды вот но это это как бы тмм это другая просто область давайте да вот сливаясь вопрос привет два вопроса первый это является ли ваш продукт ваша система некой службой такого оформленного мониторинга для команд разработки или поддержки конкретной команды то есть можно ли там свою систему или в свой функционал и заложить некое поведение который бы ваша система специальным образом бы мониторила и выдавала в команду результат то есть это же никогда не инцидента а именно как реагирование окей сейчас расскажу значит надо понять что эта штука да вот один из этих трех сканеров ретроспективный то есть даже если он видит что что-то горит сейчас он говорит но оно еще горит поэтому я ничего делать не буду вот когда вновь догорает все восстанавливается говорю о догорела отлично вот теперь я посмотрю что там происходило вот соответственно у него цель какая увидеть инцидент в прошлом и сказать чего нам стоил и собрать по нему контекст runtime а вы и штуки ну то есть система оперативного реагирования на сбой это немножко другие вещи вот у нас есть сейчас 3 runtime сканера вот это их ответственностью но это интегральные штуки которые говорят грубо это последнее узел обороны что если команды на уровне своих мониторингов а лифтинга и триггеров из майры модификациях куда-либо вот они все это про любили то вот эта штука является интегральным показателем что есть проблемы вот это дополнение это некоторый рубеж естественно сейчас вот мы сделали runtime мониторинге до который вот с отставанием где-то в две-три минуты говорится в общем о состоянии вид это хорошо нехорошо ну я думаю что дальше начнутся можно наш функционал туда как-то подключить чтоб нам там раз и вот одним флажком приходило что он сейчас плохо мы вообще ничего не делали ну это в принципе теоретически это возможно такого пока нет наверное где-то через два-три квартала это будет и второй вопрос вот вы сказали что ваша система мониторинга она не может уползать вот собственно продуктовых каких-то решений команд и так далее вот как вы выстраиваете вот эту синхронизацию если недовольства от команд что опять вы пришли требуйте от них что-то им покупают ну вот честно по взаимодействию с командами что у них зал деятели с какие-то метрики не знаю вот у нас как то взаимодействие скорее такое что ой мы тут переехал у нас приехала метрика мы забыли об этом сказать мы там из монолита выпили лись там там трафика не стала трафик вырос в другом месте на сервисах о извини что мы тебе эти метрики не отдали забыли не знали но ничего такого что мол чё ты сюда пришел вообще и что ты хочешь нет вот такого и никогда не слышу вот скорее слышишь что ну вот команд меня меньше не в каком-то чате типа вот у нас будут переезжать метрики мы тебе мы к тебе придем напишем что вот у нас вот эти вот метрики какие-то мешки critical переехали вот скорее вот такой контекст вообще говоря вот auditing нитрида актуализация метрик это нужно заворачивать в мад алгоритмы вот и делать кругаля свою внутреннюю allure телку что метрика грубо говоря изменилась так ее поведение что это не является инцидентом это является новым поведение метрики так что вот возьми эту метрику и посмотри что с ней вот так скорее сказал такие спасибо да простите вижу ваши руки можно немножко выбиваемся из тайминга давайте вы перенесете свои вопросы вот туда вот цифровые кулуара мы то сейчас дмитрий проводим давайте еще раз поблагодарим за отличный доклад"
}