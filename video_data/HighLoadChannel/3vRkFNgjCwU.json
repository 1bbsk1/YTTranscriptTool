{
  "video_id": "3vRkFNgjCwU",
  "channel": "HighLoadChannel",
  "title": "Обновления данных в поиске за секунды. Быстробновляемые атрибуты в поисковом движке Ozon / П.Портнов",
  "views": 426,
  "duration": 2539,
  "published": "2025-01-17T02:29:37-08:00",
  "text": "Привет всем я рад что мы дошли Я рад что многие люди победили борьбу с мерчем мы пошли на доклад это Большая победа Меня зовут Петя я из команды базового поиска в азоне как было сказано Сегодня поговорим о том как действительно мы добились того что смогли определённые поисковые атрибуты обновлять с достаточно высокой скоростью удовлетворяющий бизнес Работаю я в основном на Джаве и на Расте много осор много интересных проектов Приходите на стн тоже и будет о ЧМ вес план на самом деле достаточно масштабный из девяти условных пунктив мы поговорим о том что вообще такое поиск Какие проблемы в нём есть И откуда они идут Как хочется их решать и как решились проблемы превозмогая трудности решать мы Ну как обещал Давайте сначала решим что такой поиск очевидно что если мы ищем некоторый кошачий лоток то мы хотим чтобы нам выдался именно кошачий лоток очевидно первая задача поиска - это выдать эти самые лотки чтобы ну вы могли их заказать Но на самом деле базовый поиск собственно та команда в которой я работаю отвечает Не только за этот вопрос это и например формирование фильтров которые применимы именно к этим товарам Я не думаю что для кошачего лодка нам столь важно Сколько оперативной памяти в нём есть история о том что нужно всё-таки посчитать количество товаров А это как вы знаете мы всё-таки за хайло Мы за то чтобы не полностью перебирать и всякие хапер логи использовать ну и наконец такая история как хайлайты это плиточки которые рисуются без явного запроса пользователя тоже в общем-то то что на самом деле есть Запрос к базовому поиску немножко стоит упомянуть и то как это устроено под капотом потому что оптимизировать мы будем на этот кусок есть достаточно подробная диаграмм того из чего стоит поиск Какие в нём есть компоненты но в рамках вот этой статей скажем так вот этого доклада нам достаточно упростить э схему до такого то есть то что идёт с левой стороны пользовательские запросы достаточно рассматривать как просто некоторый из нескольких сервисов в результате которого доходят запросы Ну давайте посмотрим на два основных потока данных с одной стороны у нас есть данные для индексации то есть Есть огромное количество товаров и сервис индекс как-то подтягивает документы для нас не столь важно как и через каку их отправляет на мастер 2 Маер сервис написанный на дво и занимающийся Ну в общем-то формированием поисковых индексов с другой стороны у нас есть поисковые запросы та самая проблема которую мы хотим решить отвечать на на поисковые запросы пользователей Эта история скажем так идёт с другой стороны вот у нас пришёл запрос базовый поиск на этот самый запрос вернул неры ответ это может быть там разная сущность но принципиально в общем-то структура такая важен момент скажем соприкосновение двух этих сторон как у нас взаимодействует индексация и ответы на пользовательские запросы это происходит здесь тот самый мастер на самом деле накапливает постепенно какой-то объём документов раз документик два документик ещё какой-то документик вроде цифры три формирует в какой-то момен стони и реплит его на базовый Ну в общем там через через S3 сейчас как раз таки да S3 с другой стороны Когда мы уже хотим запросы обрабатывать это происходит на базовом более подробная вот та красивая схем есть тейки Сергея Саяна на хабре Ну и Да тот самый дисклеймер пишем на Джаве это и хорошо и плохо У всего есть плюсы минусы Думаю здесь найдутся люди которые за и против Ну теперь посмотрим что же хранится в этом поисковом индексе который мы вот так вот репли дисклеймер номер поиск сейчас основан на библиотеке ач цен есть мнение что это восхитительная библиотека есть мнение что это ужас используете что-то другое Аа здесь посыл в том что это та технология которая у нас действительно используется и размышляем мы именно в её терминах объектом некоторые атомарные сущностью в поиске а является документ То есть фактически товар э на сайте дальше возникают две классические для поисковых движков в сущности обратный индекс - это штука которая некоторые термы Ну например слова или чиселки отображает в документы Например вот у нас есть там слово телефон оно встречается в куче документов вот мы говорим что по слову телефон есть этот этот этот документ так данные хранятся Это хорошо потому что используется для поиска по тексту оно прекрасно отбирает намм первичных кандидатов то есть зачем нам обходить все документы если можем быстро по обратному индексу достать ровно то что нам скорее всего нужно и уже дальше всякие фильтрации применять но минус это основывается на прекрасной структуре которая похожа нас деревья подобная структура но минус что доста долго обновляется с другой стороны структура прямого индекса думаю всем известна на самом деле такое около колочное представление То есть у у нас есть документ у нас по нему можно получить значение это прямой индекс используется в основном для фильтров и возврата некоторых значений оно даёт нам доступ к каким-то конкретным значениям для конкретных документов и Да это сжатое колочное представление как правило многих может возникнуть вопрос Зачем нам нужен прямой индекс если мы увидели что обратный индекс такая прикольная универсальная штука при фильтрации для которых чаще всего это используется не всегда тривиально Допустим мы Тим сделать какую-то фильтрацию например по цене с ум курса курс штука динамическая она не может лежать в поисковом индексе поэтому можем запросом передавать например некоторый набор текущих курсов в рамках запроса получаете из вот этого прямого индекса значения как-то их преобразовывать и уже на основе этого применять фильтрацию то есть от прямого индекса случаях зни ба эно иди значения могут явно заправа нашими клиентами Ну разумеется системами выше и в этом плане в обм та классическая база данных всё-таки мы даём операции для чтения данных но проблема чтобы произвести обновление на самом деле во всей этой иерархии нам нужно реально сформировать новый документ не обновить в старом а прямо-таки создать новый и вот это вот обновление разослать на фактически это занима ного деся минут Когда нам нужны быстрые поля и какие числа тут можно назвать Ну поисковый индекс это там сотни гигов там бывают до 500 например в них лежат десятки миллионов документов то есть таких товаров по которым мы действительно ищем А порядка 10.000 полей это могут быть разряженные поля но в полях может быть что угодно цены атрибуты вот эти самые свойства типа количество памяти и так далее они Разумеется разреженных много и в среднем мы выдерживаем там 22.000 РПС в Пике более 65 Ну и речь Разумеется не простых запрос ответ А достаточно монструозный индексу гуляем ищем то что нужно но как я уже сказал для бизнеса ко что нужно обновлять быстро вот эти десятки минут Нас не устраивают самые очевидные примеры это Например цены на примеры Да цены и стоки Потому что есть сезон мы любим особенно в ноябре потому что у нас огромный трафик в этот момент и продавцы активно обновляют цены тоже самое со стоками стоки то доступность товаров обнов динамически И разумеется Мы хотим чтобы если товар недоступен он как можно скорее пропадал И наоборот если он появился в наличие Он снова же как можно быстрее рисовался у пользователей это ну в конце концов забота о пользователях которые хотят видеть то что они могут купить а не только посмотреть типа показываете красивая и если мы посмотрим на паттерн работы с вот этими конкретными полями это те самые фильтрации и получение значений то есть ровно то что делает прямой индекс оже но к сожалению новская форма нам не подходит Почему Потому что во-первых мы уже увидели что требует обновления всего документа во-вторых там нет вот этой возможности обновлять Ну снова же нам нужно сформировать цельный документ ну и наконец та самая репликация происходит вместе со всем остальным каскадом полей которые там привязаны к этому документу Это как мы уже решили очень долго Ну давайте поймём Какая же основная идея что жерева вохм обв с давайте это делать быстрыми полями что мы требуем от себя то есть Что является минимально необходимым для этой реализации вот этой эфирной сущности во-первых обновлять значение полей документа отдельных с этих самых быстрых полей во-вторых доставка выполняться должна за минимальное время И разумеется минимальное время это какая-то абстрактная чис мене мину при мы не хотели бы чтобы обработка вообще поисковых запросов замедлилась то есть очевидно если мы внедряем новую сущность Мы за что-то чем-то за это будем платить Ну например у нас могут стать более долго выполняющиеся поисковые запросы мы этого не хотим и хоть ожидаем что может он замедлится но статистически Не значимо ну и есть хотелки это не обязательно то что мы сможем в рамках этого проекта сделать но то что хотелось бы переиспользовать существующую инфру очевидно у нас написан огромный движок поиска и не хотелось бы писать ещ один поиск под 2 с по поля во-вторых это должны быть не 2 с поно поля А это должна быть какая-то универсальная штука чтобы мы всегда могли сказать дежурный Заведи пожалуйста новое быстрое поле и Вуаля у нас в индексации появилось поле которое быстро обновляется Ну и в конце концов такая сложная Техническая хотелка Мы хотим чтобы если мы обрабатываем запрос желательно видеть ровно то же состояние а не то что мы вот применили фильтрацию в ней видели одно значение поля запрос ещё не доработал обновление пришло и мы увидели уже другое значение Ну как-то странно но это наиболее сложная штука и если посмотреть на это с такой архитектурной точки зрения то как будто бы мы хотим себе просто но не совсем просто мы хотим это внедрить Существую То есть как бы эмулировать прямой индекс но своими волшебными способами Ну и здесь стоит посмотреть на то как устроен интерфейс работы с прямым индексом в поком движке пусть у нас есть некий сущность которая просто что-то ищет про прго нам Вер некий что мы переместились к этому документу и только после этого мы можем запросить Значение этого документа Ну и вернуть его пользователю который нас собственно попросил это самое значение прекрасно Ну в общем несложный интерфейс проблема поисковые запросы далеко не простая штука И вот этих фильтров и прочих может быть много допустим я ищу довольно сейчас Ред колега 425 может за тре чтобы в поле был текст Lego чтобы либо был текст 4256 где-то либо был артикул Ну тоже 4256 но чисел кой Кроме того Мы хотим чтобы там был какой-то фильтр в стоке чтобы применялся какой-то фильтр что там доставка за 60 условных единиц и наконец что он доставляется куда-то в пункт номер 129 и вот эта сложная история рождается в то что чтобы обработать один запрос документ должен обойти все вот эти фильтры все вот эти преобразования один Документ и то После этого мы сможем обработать следующий документ откуда возникает проблема что если мы добавим сюда свою нам придётся очень много раз в неё ходить мы не сможем некий бачек загрузить Окей ну попробуем прикрутить сюда наше абстрактное некое Как же оно внедряется в эту схему ну Давайте попробуем Давайте теперь мы будем в прямом индексе хранить не значения Мы же знаем что он долго обновляется он нам не подходит но будем хранить в м некую постоянную чисел Ну например ID товара на самом деле он даже сейчас уже там тогда как выглядит па вот этими быстрыми полями абстрактно Мы как и раньше переместили курсор на нужный нам документ сходили в прямой индекс обычный но постоянный запросили ID товара который на самом деле оказывается ключиком в нашей Теперь мы сходили за этим ключиком наконец-то в нашу получили реальное значение которое нас интересовали его кто ние запра зву тся обнов Прямо таки средством по ключу вставляем значение тривиальная операция замечательно но есть ограничения время доступа должно быть сопоставимо со временем доступа просто к очевидно если у нас поход в будет супер долгим по делаем вот это запрос Ну тер усв это будет капитально замедлять поиск потому что вот этих товаров в рамках одного запроса Может там сот про нам требуется для этого Либо чтобы у нас был стриминг открыли один конект во внешнюю кв и обмениваемся гето операциями либо какая-то локальная репликация то есть что value у нас сформировалась локально Ну и хотелось бы оптимизировать тот случай что всё-таки у нас скорее всего ключи это целые чиселки если мы можем это использовать для себя то замечательно но всё-таки математика есть математика вдруг подойдут существующие к Давайте бенчи очевидно это как себо обычный прямой индекс опуская то что он медленно обновляется то есть Насколько быстро можем из него читать Ну вот это некая одна абстрактная операция в секунду Разумеется ни там были на конкретных числах Ну допустим мы все эти данные положили в массив Пусть наша будет локальным массивом замедлились ну умножим 07 приемлемо замедлились но сдвинем куда-нибудь фильтр вниз допустим побили этот массив на слои зачем-то попозже обсудим Зачем замедлились Ну около в два ра опять же ф ниб внизу так плохо но реальные квшки которые можно им Беди здесь вобще ничего нет про то чтобы в сетевые совсем медленные db в 1000 раз оказалось медленнее на наших данных то есть мы просто заполнили Ro db нашими данными попробовали встроить как бы вот в этот пайплайн и замедлились 1000 раз как-то не очень хорошо ldb используется насколько мне известно на всяких ДНС серверах Ну уже не так плохо всего-то 30 раз замедлились чуть-чуть чуть-чуть менее плохо но всё ещ ужасно ну и наконец у него есть сделать реплицируемый так плох для нашей задачи Разумеется но хотя бы лучше Роб то есть всё не подходит всё супер медленно надо делать свою Это я сейчас типа описал что это не not Сим этому действительно нам это понадобилось мы обосновали мы доказали мы графики нарисовали или таблички Ну хорошо попробуем рассмотреть простую модель полей посмотрим как мы будем доставлять обновление в нашу чтобы смо что же мы тут можем локально с этим делать простейшая схема тот кто на индексирует данные Пусть он просто в каку складывает то что он хочет У нас и так сейчас апдейты идут через кафку просто быстрые будут через отдельную свою качку которую мы будем быстро высчитывать прямо-таки на базовых без вот этого два мастера там же где мы работаем с данными там и считаем вроде модель супер простая опять же вся кака нам всяких гарантий прикольных на даёт типа кака конеч и более того вычитывать из неё данные может быть очень долго как это выглядит Ну апдейт очевидно много и один и тот же товар Может в том числе много раз обновляться особенно во всяких распродажах вот мы проставить значение товара X в единичку потом мы его проставить в для товара Y в девяточку потом у нас ещё какие-то апдейты были ещё там теперь X2 А теперь вообще X троечка чтобы теперь нам Вычитать при свеже для свеже запущенного базового всё вот это состояние у нас же могут перезапуска поды ему нужно будет Что сделать посчитать Вот это значение вот эту вставку на самом деле посчитать вот эту вставку посчитать вот эту огромную кучу вставок посчитать ещё одну вставку теперь в двоечку и ещё одну в троечку мало того что нам нужно прочитать достаточно большую большой объём данных Ну опять же потому что адев может быть всё больше и больше мы хотим много быстрых полей Так мы ещё и будем почитать кучу во-первых ненужной Информации а во-вторых какой-то момент у нас старые данные могут вс-таки стать недостижимые Ну вс-таки можно построить но всё у нас не бесконечно ресурсы тоже нужны И к сожалению это не подходит нам что мы сделаем мы вспомним что у нас всё-таки есть ластер и для обычных апдейта всё-таки любые апдейты идут через него что мы будем делать А для уже живых базовых просто будем проксирование Что кроме этого теперь мастер будет выпускать некоторые снапшоты с определённой периодичностью как это выглядит Ну как и раньше спам апдейта в буп но в какой-то момент мы решили Так прошло достаточно много времени можно сформировать snapshot Это не тот спш который нужно полностью Каждый раз реплицировать как с обычным индексом это просто какой-то слепок как у нас выглядит вот это кив полностью Ну то есть все наборы пар ключ значения и теперь соответственно Когда у нас есть такой спш и мы свеже запустили базовый поиск вот конкретно ему у которого сейчас данных Нет не нужно вычитывать всю кафку ему достаточно позиционировать на снапшоте выкачать быстро из S3 например Ну так в общем-то и есть спш накатить только после него идущее обновление Ну и продолжить дальше читать что ему там свежего прилетает в этой кафке не больше хорошо как будто бы всё архитектурная проблема решена можно закрепляться но доклад ещ не закончился по грустной причине гораздо более сложно оказалось придумать то как же удобно хранить данные локально учитывая специфику данных Ну что мы хотим ВС те же дешевые обновления чтобы мы вычитали каку и пря быстренько вставили хранилище локальное умение захватывать текущее состояние Ну это такая всё ещё хотелка но не хотелось бы там в рамках запроса видеть два состояния ну и наконец Ну какие-то гарантии Для этого ну и всё-таки эффективно использовать тот факт что у нас ключи целостные значения я уже говорил мы хотим это пользоваться мы всё-таки что-то оптимизировать но хотим и как будто бы классическая структура для этого это массивы Ну то есть индекс - это ключик далее значение пока что не говорю ничего про то как у нас на самом деле распределены ашнико просто посмотрим на то что нам для этого придётся сделать А ну как минимум нам понадобится хотя бы Copy on Right массив По той простой причине что мы живём в прекрасном мире где у процессоров чуть больше чем одно ядро не говоря о пушках но на цпш ках тоже далеко не одно ядро и в общем-то никто нам не гарантирует что просто вставку мы увидим сейчас Ну один из подходов к этому здесь есть люди которые гораздо лучше в меня разбираются в дво многопоточная смотрю да да да Но скажем так объяснение упрощённое соответственно чтобы данные у нас правильно были видны нам нужно Ну например копировать весь массив но как-то копировать массив в котором десятки миллионов документов Ну как-то жирно самое простое решение давайте мы массив объём на блоки и когда мы хотим что-то обновить мы будем на самом деле копировать только те блоки которые мы непосредственно апм затронули Ну то есть вычитали из каки какой-то маленький батк там за несколько секунд сделали эти вставки Обновили только то что нужно но к сожалению даже так это будет отвратительно по памяти Даже хуже чем предыдущий подход из-за вот этой страшной картиночки Я не заставляю смотреть на эти жутки графики тут есть дополнительное объяснение что он говорит документов которые когда-либо у нас имели дишни ноль всякие исторические товары там id0 какой-то странный товар выглядит как пасхалка Можете даже проверить а он ну там практически ничего нет С другой стороны супер свежих документов у нас полно очевидно новые товары у нас ну лежат в поисковом индексе Но вот эта вот зона - это товары которые обладают какими-то язичниками между нулевыми и максимальными На текущий момент и мы видим что ну где-то есть где-то нет они очень равномерно размазаны и получается что у нас там будут и не полные блоки с одной стороны и с другой стороны эти блоки будут то есть мы не только не получим какой-то Профит от того что мы на блоке побили но ещё и памяти будем больше тратить звучит как-то страшно и получается что простые поля были прекрасны Ровно до того момента когда они перестали такими быть и оказались совсем не простыми И как я говорил данные не очень приятная штука ВС оказалось гораздо хуже во-первых цен у нас много Я думал что вот у нас есть счётчик вот э цена товара там в некой валюте забьём на то что валют много на самом деле цен полно обычная цена цена со скидкой как-то по-разному применяют скидки Для клиента с азон картой цена цена в иностранной валюте у на наме нам даже больше н более того приме наличие цены Сазон карты то есть Есть ли вообще вот э цена Сазон карты вопрос почему это отдельное поле Ну видимо кому-то это бизнесов понадобилось диапазон цен то есть мы можем там ещё заранее распределять товары что вот это в категории там 05.000 этот 5.000 10.000 и так далее например и Это ужасно Это много данных где вот эти костыли Ну вот тут нам хватит массива уже не работают более того стоки то есть сколько у нас товаров Это тоже не одна чисел это даже не не були типа есть нет это много разных полей в обычном прямом индексе которые занимают действительно катастрофически много места там локации Это несколько полей всякие полигоны это куча бизнес логики связанной с тем что есть разные склады есть разное разбиение участков кто-то куда-то доставляет кто-то куда-то не доставляет и для разных пользователей мы должны рисовать разные данные то есть чтобы у нас эта штука работала у нас должен быть не счётчик сколько товара а фактически портирование вот этих полей быстрые и ужас в том что вот эти Поля доступности это во-первых десятки гигов А во-вторых это списки до этого мы ВС ещ работали в ментальной модели что у нас значение это ну просто чиселки получилось совсем не так и нужно придумать что же нам делать теперь с локальным хранением чтобы в общем-то не продолжаться Ну и чтобы хотя бы нас поды поднялись С таким количеством данных супер простой подход у нас были массивы лонгов теперь у нас массивы массивов лонгов сохраним реализацию значение просто будем каждый раз новые массив создавать такой нав Туй но простой подход мы платим банально за факт использования каждого массива плюс 16 бай стате шепелёва в любом докладе должен быть ЛВ Ну уже как-то много более того есть плюсик У нас есть прекрасный ц мы двист можем понтоваться тем что нас больше чем оц и как следствие из этого нам не нужно париться по очистке памяти Ну там перестали им пользоваться замечательно вариант для отбитых типа нас это давайте ручками ровать памяти то есть берм предыдущий подход но теперь храним там честные адреса и самыми как-то лору память лору в общем-то для этого есть методы но проблема а когда освобождать память как я уже говорил поискового трафика много и на один под конкурентно могут приходить там десятки сотни поисковых запросов каждый из них может обращаться к полю В како же момен освободить сасо во-первых это Мар его там двигать не очень дорого но ВС же но главная проблема нак гдето надо хранить то есть мы снова платим какой-то кусок памяти вся а потом ещ всякое выравнивание и прочее просто чтобы сделать то что за нас и так умнее гораздо дела Ну и мы опять же теряем автоматический компак мы будем ручками освобождать память у нас будут дырки в памяти Ну как-то типа Давайте писать свой ц Видимо хозя у нас есть готовый замечательный видосик на тубе потом посмотрите все он прекрасно описывает Что такое программировать на си Ну а здесь мы как будто бы хотели тоже самое делать Ну и альтернатива очень упоротая Сначала мы её обсуждали она звучала замечательно Чем больше мы е обсуждали тем больше Мы понимали что ь какая-то Ну вроде так было возможно возможно сейчас кого-то огорчил но нет Давайте вместо этого мы будем на каждую вставку в какой-нибудь огромный файл диски дешевле условно говоря дописывает дописывает дописывает значения не вижу проблем Ну просто пишем и снова же ним какой-то адрес значениях минус Мы хотим это быстро делать и Значит мы захотим в какой-то момент это мапить в память как только мы захотим это память мы получили ровно предыдущую проблему только теперь она ещ зачем-то в файле лежит нам ВС ещё нужен Access поэтому явно нужен У нас ВС ещ идт дефрагментация Нам всё ещё нужно писать Ко и опять же мы делаем какой-то свой Ну то есть Нам как-то нужно вести свой ло ещё и файле ещ еже во Масси это какая-то ди они звучали хорошо сначала ровно пока мы не посмотрели на то как у нас дишни можно делать Так у нас есть равно мастер посередине а давайте в нём просто будем нормальный счётчик заводить то есть заведём какую-нибудь табличку и будем в рамках поискового индекса просто ввести счётчик у тебя какой-то неважно Теперь у тебя для быстрых полей ноль у следующего оди и так далее минус в том что ну вообще-то нам нужно как-то это рассылать например через тот же прямой индекс но это не просто во-вторых нам нужно между несколькими мастерами это High ability система с магие типа переключения мастеров и прочее и устойчиво генерировать вот этот дишни не продол Пать данные а достаточно здесь сбиться на единичку у нас просто весь поисковый индекс будет в неправильном состоянии это как-то тоже сложно то есть ну как будто бы с блок Джеком и счётчиками Но на самом деле что-то не то Ну и истинные любители алго сиков поймут Ну вообще-то вам нужна мапа но мапа - Это не просто Ну во-первых она должна удовлетворять вки нашему перф и быть корректной корректность вс-таки важно нам важно как вообще эффективно делать снапшоты Ну очевидно у нас нету вш метода сдела мне сшт вот такой специальный Ну и Зато у нас примени всякие варианта хранения значений То есть как хотите все вот эти три отбитых варианта до этого так храните значени ну Давайте напишем таблиц как будто последни кусочек этого пачем нехватает для рабо нашем по штаб первый закрытая адресация если у нас случилась коллизия мы продолжаем связаный список Вот это ячейки никуда больше не ходим даже условно говоря не нужен ножен для атики на самом деле Простая история но много места на информацию связанные списки это снова же много памяти ровно потому что нужно хотя бы указатель на следующий хранить извиняюсь в Это история о том что Мы вс-таки где-то выполняем блокировки они локальные но мы их выполняем А здесь у нас достаточно большой поток обновлений с одной стороны с другой стороны огромное количество читателей и блокировать читателей Это плохо Ну и наконец нам легко спровоцировать если мы не напишем свою кошма А это Ну давайте будем это сложно но есть друг на открытая адресация другой подход мыта чтото делать с этикетом мы просто По какому-то предсказуемо детерминировано правилу идм в следующую ячейку Ну например ровно следующую Или например квадратичное апробирование идм В О вче В де например вот такое что-нибудь главное по детерминировано правил это уже интереснее но нужно эффективно хранить ключи и значения Ну всё таже проблема нужно уметь эффективно работать с памятью както пото безопасность иние этоша случа Ну первая проблема решается тривиально плохо делать Так потому что Слот - это Пусть кортежи он занимает там дополнительную память Ну на ВС те же хедеры классическое решение использующееся много где включать всякие фасту силы Ну давайте мы отдельно храним ключи отдельно значения и вариант длина одинаковая обеспечение потока безопасности уже сложнее у нас у таблицы единственный писатель Это приятно у таблиц множество читателей это гораздо менее приятно это если Вы посмотрите всякие конка библиотеки случае множество читателей как будто бы один из самых сложных и наконец если присутствует ключ то Ну мы хотим увидеть значение а не то что мы записали ключ а значение оказывается ещё в памяти как-то до нас не доехало но на самом деле это не так сложно решается Вот это наше простейшее представление есть ключи есть значения что мы будем делать Мы двист у нас много ресурсов Давайте читать состояние прочитали состояние пока что всё хорошо как обеспечить что мы будем видеть значени всегда если мы видели ключи Ну на самом деле Давайте просто сначала писать значение строго чала и это будет происходить Ровно до записи ключей на самом деле нам здесь даже хватило мы живём на x86 64 там это реализовано достаточно приятно Для нас Что за чтение Мы практически ничего не платим практически про это будет уточнение Когда же мы делаем й такое бывает это шма который нужен для корректности мы просто формируем новое состояние с новыми массив из того писа переносим туда данные и только после этого см обновляем ссылочку теперь читаем уже это Ново состояние Ну и есть эксперимент что мы хотим вот поменять У меня даже написан тесто будем Смотреть насколько это Заведётся но както надо удалять данные удаление данных это большая проблема потому что вот мы вставили К7 потом вставили К1 тут он не влез тут он не влез Ну вставили сюда М1 смотрим так тут нет может быть он следующим тут нет Ой а тут пустой значит ничего нет Всё мы не можем прочитать обходить Всё мы тоже себе не можем позволить что же делать двигать не можем так мы порам других читателей читатели ждут консистентной просто будем записывать что вот тут что-то когда-то было больше этого чего-то тут нет но это на самом деле не проблема даже удобненько опять же у нас практически не случается удалений рекция происходит раз в сутки это прекрасный компромисс для нашего случая Ну и теперь мы ищем просчитали здесь просчитали здесь Ура К1 вс-таки Нашёлся замечательно Ну и заключая Давайте решим проблему с памятью данных много мы это увидели но посмотрев на данные мы увидели что на самом деле локации зачастую повторяются очевидно у нас куча разных товаров может быть на одном складе и данные по доступности у них могут совпадать Ну просто вжух дупли 2 три где-то в памяти будут лежать объекты но они будут Дурова у нас не будет двух одинаковых кусков где-то просто решается выигрывать много памяти но ещ больше памяти можно выиграть алгоритм сжатия мы знаем что это списки и мы знаем что эти списки отсортированы Ну так давайте просто выполним сжатие Мой коллега написал замечательный алгоритм для сжатия этих данных и в общем выиграли много памяти от этого И это та самая последняя деталька нашего конструктора которая позволила таки запустить проект что мы получили я говорил много чисел типа много немало и так далее диро в рамках обычной индексации Ну по 13 млн документов немало а поток из 250 групп обновления в секунду не может может не звучать как гигантская чисел но в контексте того что мы обновляем только релевантные товары и на распродажах э чисел будет больше Разумеется мы там Тестируем на производительность на больших числах это в общем-то то что мы стремились получить время доставки обновления единица секунды и замечательно смотреть на вот эти кавка графики где там лак практически всегда на нуле держится а мы замедлились минорное запросов но не зря мы в рамках других проектов боремся за перфома базового поиска чтобы такие проекты могли немножко отжать это свое задаче Ну и памяти немало Ну и вот Это нулевой ла в ка это пря приятно смотреть на графиках ну всякие ментальные выводы иногда нам приходится расширять любимы библиотеки это кстати сделано чисто оми средствами в рамках цены Нам очень важно смотреть на то Какие данные У нас есть вс-таки мы люди умные мы понимаем нез сделать универсальное решение в рамках Бине Иногда нужно думать на чем же реально работаем Ну универсально можно Нужно постараться нам требуется предпринимать некоторые усилия При работе над многопоток сиченики сроков сроки это болезненная тема Ну и на самом деле большое Спасибо всем тем кто сделал это возможным тестером продуктам руководителям Ну и всем кто поучаствовал в подготовке этого доклада людей много Спасибо всем предлагаю оценить доклад Я попробую отвечать на ваш вопрос У меня тут даже листочек для этого есть а если что ловите меня на стенде или просите меня призвать Спасибо Да листочки штука полезная учитывая что по нашей доброй традиции Озон кроме нашей супрематический матрёшки дарит ещё один дополнительный подарок от себя Давайте начнём с ближайших рядов Привет Спасибо за доклад в рамках Вот и построение таких систем понятно что очень интересно смотреть на обновление данных но иногда случаются проблемы и данные нужно откатывать и расскажи как вы решили проблему отката индекса соме Да история с откатом данных на самом деле работает и в рамках нашей простой индексации То есть если мы на самом деле когда пере выпускаем новый индекс у нас сохраняется там условно говоря старый если мы наблюдаем проблему для этого тоже там есть метрия есть методы автоматического обнаружения есть в конце концов дежурные если мы обнаруживаем проблему с текущим индексом то у нас есть возможность откатиться на предыдущий в том числе поскольку сейчас у нас к обычному поисковому индексу привязано быстрое хранилище мы фактически откаты на старый обычный индекс в том числе нижа Ико ско Ну пока существует старый индекс НАТО можем откатиться него в том числе дублируются обновления которые могут быть релевантны затей что на одежде спикера один из жев с надписью откатывать зала доб событий к примеру удаление товара изменение цены либо изменение описания товара - это немножко ну разночтение бизнесов знаем что действительно будет быстро обновляться то есть стоки локации это вот две вещи которые сейчас должны быстрыми а соответственно у них отдельная своя кавка отдельно через неё идут апдейты никак не скажем так не заторможенный тем что у нас параллельно могут идти изменения товаров и прочее которые идёт через обычный Там обычный поисковый индекс плюс ко всему внутри даже быстрых полей сейчас уже заложен механизм что мы можем на самом деле для каждого из них использовать отдельный канал Ну фактически кавка топик То есть если мы захотим некие из них там ну даже не столько приоритизировать сколько сделать независимо от других апдейт то это уже сейчас можно сделать ровно за сч того что там фактически для каждого поля Независимый поток может быть Следующий вопрос тоже из правой части зала прошу Здравствуйте спасибо за доклад Подскажите пожалуйста как у вас на холодном старте оптимизирована вот скорость поднятия подов Это только вычитка снапшота или кавка всё-таки тоже как-то перечитываю Ох отличный вопрос в обще все вопросы отличны нельзя говорить Хороший вопрос кто-то кто-то говорил Так нельзя говорить но вообще все вопрос топ а холодный старт поскольку скажем так проект достаточно новый сейчас у нас в явном виде данные из быстрых полей в том числе дублируются в обычном индексе это сделано во-первых на случай если на новай система с ней что-то пойдёт не так чтобы мы могли переключиться легко на обычный и для того чтобы холодный старт был ускорен как работает холодный старт пока у нас на пот не загрузились быстрые Поля это значит загрузить сшт а б накинуть в него вотт свежий апдейт и докатиться до вот этого состояния Из которого можем уже там в Реал тайме считать вот этот момент мы считаем из обычного индекса Ну вот эти медленные do то есть мы вот этот маленький кусок платим в рамках отдельного пода тем что он временно смотрит на немножко более медленные поля но он быстро поднимается быстро раскатывается тут есть ещё там идея куда это можно оптимизировать и всякие CP ба задачи и прочее но пока это просто работает за счёт фолк на медленное хранилище в котором В общем тоже корректные данные фразы Конечно затирают есть недобрая шутка что однажды спикер не поместил в презентацию последни слайд Спасибо за внимание и Ох В общем много разных трагедий случилось так что никто никто больше не хочет побороться за супрематический Вот если бы ты отправился в прошлое и мог бы поменять архитектуру решения зная всё то что ты сейчас знаешь что внёс ли бы ты какие-нибудь изменения с точки зрения глобальной архитектуры Я не думаю что здесь бы многое поменялось её преимущество в текущем виде что она стала простой достаточно для понимания и на текущий момент она достаточно расширяемой чтобы я сделал иначе более такой широкий вопрос я бы пропустил вот те многие шаги которые казались на первый взгляд хорошими а потом оказалось что данные у нас совсем нехорошие но мне кажется у многих такое было что вы занимаетесь проектом он идёт Хорошо потом понимаете нету проб решаете иного переть если мы плохое не делали хорошее сразу делали средняя часть зала слева Ну здравствуйте меня небольшой такой как бы слегка шутливый вопрос отличная презентация что я заметил вы используете в Озоне О2 У меня вопрос Озон это О3 Почему используете О2 это вопросы нейминга самое холивар самое весёлое скажем так движок на2 сформировался немножко до того как я присоединился к команде насколько я понимаю и о том что оо3 это одна из молекулярных форм не думал я что про тако буду на лоде рассказывать одна из молекулярных форм кислорода Озон собственно кислород тоже одна из молекулярных форм кислорода О2 Ну и здесь просто Игра слов про то что О2 А ещё я немножко там Радж у нас пропагандируют будет ещ О4 такая молекула такой молекулярный кислород тоже есть это будет Каси так называтся Следуй вопрос Центральная часть зала Привет Спасибо за доклад Меня зовут Наташа и Комтех по поводу Кафки Да ты вот говорил что у тебя был слайд а допустим когда Ну кладутся значения по одному и тому же ключу в кафке есть такая штуковина как компакт топики Угу пробовали ли вы стало или лучше а пока что не пробовали размышление про это было в том числе Я когда там позавчера прогонял доклад меня там коллега который как раз просматривал доклад сказал что вы се скоро придумаете автовакуум и будет просто там кусочек ПГ насколько я помню здесь Да есть много вещей которые ещё можно улучшить и в том числе момент с тем что в Кафки скажем так выполнять некую дупликации он работает да пока что мы это не сделали Но это то что может нам дать выигрыш но сейчас мы выиграем со счёт того что на самом деле у нас пока наиболее долгий период это вычитка С3 и применение вот этого всего локального формирование локальных данных ну скажем так с ством данных и будем каку оптимизировать в том числе спасибо Просто жизненно необходимо придумать для хайло Да шутку про кафку отличной от манной Кафки потому что манная кавка прописать её не на Да да да манная кавка как-то немножко затёр Так ну что ж у нас достаточно вопросов чтобы побороться за супрематический а тебя я попрошу в начале выбрать того кому достанется приз от се я думаю сечас там пону ня есть ещё пару секунд чтобы выбрать это сложного что это самое сложное Ну вообще да мы же код пишем а не вопросы выбираем Так давайте возм вопросы Я тут записывал честно возм призы это второй был вопрос третий про холодный старт остальные подходите ко мне можете порыва эти почи это тоже то давайте приоритизация событий специальный приз от Озон выходи на сцену Аплодисменты гостю конференции который принял непосредственное участие в том чтобы сделать её лучше сюда прошу И второй вопрос про холодный старт кто его задал кто задал вопрос выходи на сцену супрематическая матрёшка кастомная на каждой конференции будет свой дизайн и небольшая книжка про Выход из зоны комфорта Муа да нам айтишникам прежде чем выйти из зоны комфорта для начала Неплохо бы в неё зайти и посмотреть что это вообще такое Ну хоть разочек и наконец для нашего спикера огромный тебе спасибо что был с нами наш специальный Презент аплодисменты спикеру"
}