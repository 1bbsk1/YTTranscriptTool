{
  "video_id": "SD101h3sW7g",
  "channel": "HighLoadChannel",
  "title": "Kafka. Как мы строили корпоративную шину данных, которая обрабатывает до 3 млн сообщ./сек. / И.Гаас",
  "views": 6891,
  "duration": 3142,
  "published": "2022-03-21T13:13:02-07:00",
  "text": "здравствуйте друзья я очень рад вас всех сегодня видеть и также хотелось бы поприветствовать не только нашего оффлайна аудиторию но и онлайн здравствуй андрею за дал хорошую тему с какой скоростью почта россии умеет обрабатывать сообщения все мы знаем что иногда не очень быстро но сегодня я попробую вам рассказать что происходит внутри и с какой скоростью хочет растит на самом деле обрабатывается общение тема моего доклада карта как мы построили корпоративную шину данных который обрабатывает до 3 миллионов сообщений секунду и для начала немножечко истории как выглядела почта россии приблизительно на момент 2012 года на картинке увидите фалмер именно так интегрировались все почты сервисы почты россии цифровых сервисов было немного порядка 10 штук все это были монолиты и в общем-то интеграцию в этой схеме всего лишь стоп проблема в том что в 2012 году и чуть далее перед нами была поставлена очень сложная задача почта россии до того времени было дотационным предприятиям и находилась на содержание государства но именно в тот момент нам сказали что мы должны выйти на самоокупаемость теми методами которыми почта россии работала это было сделать не возможно в кратчайшие сроки и почта россии решило огромные силы большые финансы вложить цифровизации о своих сервисов и уже за 2 года мы написали более 20 цифровых продуктов соответственно их стало уже 30 мы пришли к микро сервисные архитектуре у нас появилась порядка 100 микро сервисов и соответственно выросли интеграции их стало уже тысячу дальше в таком режиме развиваться было невозможно потому что в этой схеме каждая следующий цифровой сервис должен был сделать тысячу интеграции тысячу обойти команд 1000 договоренностей тысячу форматов данных это занимало уже не месяца даже это занимало от полугода и больше поэтому наши доблестные архитекторы селе посмотрели что есть из паттернов что есть из архитектурных решений и предложили попробовать архитектурное решение сервис-ориентированной поставить в центре какое-то решение с которым будут интегрироваться абсолютно все то есть уйти от full мышь прийти в звезде мы попробовали раскрою тайну здесь встала кафка почему разберемся чуть попозже отель а пока что посмотрим на то как изменилась статистика на 2016 год у нас количество интеграция упала с тысячи до 100 а до двадцатого года мы смогли выпустить еще 200 цифровых сервисов и при этом количество интеграций у нас увеличилось совсем незначительным все благодаря этому решению давайте попробуем разобраться почему кафка что мы рассматривали изначально мы хотели получить какой то продукт open source потому что почта россии очень любят концов правда на самом деле то есть у нас очень много о концов с решений мы хотели производительность не ниже миллиона request of в секунду почему потому что на тот момент у нас уже была big data и это bigdata она существует по сей день немножко пав топлива а на каждую ночь подымает 20 петабайт данных и обрабатывает и дальше на графиках я буду показывать именно как big data пропускает эти 20 петабайт через нашу ковку мы и хотели масштабируемости мы понимали что данных будет все больше и больше что цифровые продукты в почте россии будут только приумножаться увеличу и мы хотели очень легко масштабировать соответственно нашу шину мы и хотели управляемость мы понимали что мы очень сильно рискуем когда ставим единый продукт в центре всего потому что если он откажет по каким-то причинам вся почта перестанет работать не какой-то ее кусочек соответственно нам очень важно было чтобы были рейд ними и самое важное что нам нужно было это какая-то возможность на ли тут трансформировать или обогащать данные зачем нужно было это возможность я покажу чуть подальше и расскажу но это было очень важно что мы рассматривали соответственно все возможные брокеры очереди рыбе темпе а починку и на тот момент только появилась кафка достаточно когда хайповый продукт попробуем разобраться почему кафка пойдем от обратного почему не rabbit может кто-нибудь поднять руки кто пишет на и рунге в этом зале но приблизительно точно также мы выбирали почему не робит почему не актив и не а починки он нас не устроил в управляемости он нас не устроил в производительности хотя он достигал а то и на самом деле производительности но кафка в наших тестах выдавала больше мы тестировали кафку в течение шести месяцев и после этого решили что хватит терпеть пойдем production но перед тем как уйти в продакшен мы решили посмотреть что мы хотим получить кафка это отличный продукт но эта шина не более того она нужна была корпоративная широ какие требования мы предъявляли к именно корпоративной шину это должны быть дополнительные интерфейсы для работы с клиентами а не только то что существовала в кафки в кафки до сих пор капкан этих это дополнительный слой для работы с большими данными потому что в наших тестах кафка показала что сообщение больше одного мегабайта очень критически роняют ее performance это система а у дефекации и авторизации нам нужно обязательно заботиться о безопасности тем более что у нас очень сильная безопасность безопасность там обеспечивает фсб российской федерации это система контроля лимитов обязательное условие потому что если нас какой-то один сервис за доносит а мы упадем мы потеряемся сервис это система верификации трансформации обогащение данных тора ничего создавалась эта система то есть мы должны навету проверять данные трансформировать если нужно обогащать эта система эксплуатации поддержки обязательная вещь то есть нельзя взять продукт если мы не знаем как мы будем его эксплуатироваться и поддерживать и это система собственно говоря мониторинга мы должны представлять что там происходит мы должны быть уведомлены об авариях и жевать мы превентивно что у нас получилось почта россии давайте пока пауза-пауза идет я вам все-таки тогда расскажу предысторию почему важно нам трансформировать и обогащать данные внутри этой системы тем более что в докладе этому будет уделено мало времени это не функция кафки среди приложений почты россии внутри огромная связанность предположим что есть какой-то тарификатор который знает что-то тарифы то вы как клиент пользуйтесь на сайте почты россии калькуляторов соответственно связаны с следующее вы указываете вес посылки или и габариты откуда узнать тариф как рассчитать цену нужно обратиться к тарификатором это сейчас и собственно говоря из-за этой связанности нам нужно чтобы между сервисами всегда проходили данные обогащенные то есть предположим вы получили посылку в отделении собственно говоря первое сообщение которое прилетает она просто говорит о том что клиент посылку забрал мы ее отдали а потом это сообщение еще до обогащается на основе чего клиент забрал эту посылку ведь вы написали извещение подписали подписали что какой там был паспорт какой вид документа потом он от сканируется и обязательно сложится в наших хранилища и из этого мы все состоим единое сообщение и это называется обогащением то есть изначально у нас бедные данные и мы к ним постоянно подливаем добавляем по мере того как произошла работа с этими данными и так хочет россии 21 год кошелек здесь полная схема из чего он состоит давайте рассмотрим компоненты соответственно здесь вы видите балансировщик и все очень просто про них даже рассказывать не буду это несколько индексов между ними типа лифт для того чтобы сохранять и пи адрес просто для отказоустойчивости а далее что мы видим мы видим что есть шлюз авторизация этот компонент мы написали самостоятельно и про него я одна следующем слайде подробнее расскажу скажу сейчас что он как раз управляет теми требованиями которые мы выдвигали корпоративной шине у него есть своя база данных мы это после сквер достаточно стандартная база единственное к чему тут могут возникнуть вопросы почему патроне потому что умеем соответственно почти все наши кластеров продакшене которые используют пожгли они всегда на патроне так же мы видим на этой схеме что у нас есть некий нз арест прокси этот компонент он довольно хитрый он существует для того чтобы внешне какие-то сервисы выдавать как собственный сервис кафки то есть какие-то справочнике какие-то интеграции например с какими-то внешними системами данных например с 1с кай чтобы клиент а мне нужно было ходить за этими данными в ту систему они могли напрямую покрыт так они всегда работаю с кафкой интегрируется с кафкой спросить этого кафки получить эти данные это портал соответственно портала это для обслуживания поддержки для сопровождения здесь мы видим с кафе кафе это файловое хранилище да мы изобрели велосипед не скрою мы построили написали свое файловое хранилище для чего она нужна когда там плела приходит больше одного мегабайта данных мы их не сохраняем очередь мы тогда их сохраняем на общедоступного файловое хранилище которая доступна всем порез ту и в сообщении помимо помещаем суку все и очень быстро работаем уже с сообщением в котором только ссылка на это файловое хранилище далее всем кому понадобится данные из этого сообщения они увидят эту ссылку они смогут туда дойти и по собственному токи но получить доступ именно к этому полю система мониторинга понятно тоже достаточно классическая там grafana прометей ластик ластик подлоге сама кафка парковку мы еще лучшим подробно поговорим этому и здесь ничего рассказывать и маршрутизатор про который я тоже расскажу подробнее в эти теле по каждому компоненту шлюз авторизации мы его называем api gotway что он делает написали моего сами пишем и все на джаве цвет кому интересно то есть он его цели это предоставить rest api для наших клиентов не только кафка минетов ну еще предоставить rest тот кто работает с кафкой можете сказать но ведь у consuming то есть да но консульт появился намного позже мы выбирали кафку когда она была еще apache кафка и приста там не было предоставить ауте фиксацию соответственно на листе мы должны проверить что токен позволяет этому клиенту вообще получить какой-то доступ контроль и учет лимитов так как все работает либо через этих либо через rest понятно что через эти втом кафка сама может ограничивать лимитами что делать когда работает через rest соответственно api gotway должен еще контролировать и учитывать лимиты как раз для этого ему в том числе для этого ему нужна база данных где он их сохраняет и он должен заблокировать доступ если кто-то нас начал до 10 или превысил свои лимиты и соответственно предоставить внешней интеграции через прокси а через который я рассказывал до для чего нужен прокси смотрим следующий компонент соответственно в портал тоже мы его написали сами довольно простой портал его цель это управление от фикации авторизации здесь она именно в этом решении она локальная без всяких удap active directory и прочего прочего то есть именно через портал создается . создаются собственного ряд топики и раздаются право на них это управление элементами здесь же можно выставить лимит и сколько сообщений мы будем принимать в секунду в минуту какого объема то есть все все все лимиты которые мы предусматриваем и а мониторинг состоянии явно жоп мы там видели на схеме что у нас есть продукт в виде маршрутизатора так вот мы мониторим его джо бы так там качестве среды вероятно стоит то соответственно я джо бы и самый сложный компонент если про него много рассказывать то в принципе нам понадобится еще 40 минут до поэтому я постараюсь побыстрее мы его называем мажьте зато рам что он делает по сути он делает всю магию с данными он смотрит на все топики и у него написано логика логика работы с данными соответственно он смотрит в один топик выгребает оттуда данные если нужно трансформирует если нужно обогащает также если нужно он умеет их фильтровать то есть он грязные данные может не пропустить и перекладывает их для концу мир оффтопик 2 соответственно у нас система перекладывания сообщений то есть пишется всегда в один топик считается всегда из другого . и так по сути почти для каждой системы могу сказать что на данный момент топиков у нас почти тысяча шине ну и соответственно до работа с данными верификация трансформация обогащение также интеграция с внешними сервисами что мы здесь используем я уже сказал что здесь ходу но hadoop нам понадобился в основном для того чтобы легко и удобно запускать явно скинули то есть сам hd fs мы здесь особо не используем ярко джо бы в йорк жабах мы запускаем 2 фреймворка первая из них который в real time следит за всеми данными там мы используем сам за и второй это так называемые banjo бы их всего у нас две это обработка логов и выкачка каких-то больших словарей данных к себе побачим последую там spark вот и все все что написано пацан за там еще запускается а подчеркнул для чего кэмэл да потому что он просто предоставляет удобный набор скажем так инструментов для обработки сообщений то есть именно он умеет делать сплит сообщение именно он умеет марш сообщение он умеет делать transform сообщений чтобы нам не писать всю эту логику каждый раз самим мы просто взяли канал в принципе можно это написать самому вместо сам за например попробовать использовать fling но нам сам за очень нравится он достаточно удобен а тем более что это нативное решение от того же linked-in который в свое время открыл кафку они очень классные интегрируются вот такой муж cyzer если коротко посмотрим на некую статистику на данный момент в почте россии есть три своих сода 2 в москве 1 ватт лири между ними мы запустили мир maker таким образом у нас во всех судах все данные одинаковые по кафке расползаются количество серверов которые мы закупили чтобы запустить эту систему в 3 сотых это 98 серверов все они br металл в каждом из них по восемьдесят два ядра единственное что это я написал снасти под трейдингом вот по 300 8 и 6 гигов оперативной памяти и по 20 терабайт счас дисков в каждом сервере по два сетевых интерфейса нам обязательно понадобился bonding потому что дальше я вам покажу трафик этого кластера и количество клиентов это и консьюмер и и продюсеры которые обращаются к этому кластеру ежесекундно 40000 на самом деле каждое отделение в россии в каждом городе в каждом районе может быть в вашем доме есть отделение работает именно через это решение если это решение становится все отделение пороси встанут команда которая разрабатывает это решение поддерживает и соответственно помогает с интеграциями разработчиков java у нас 6 человек аналитиков 12 эксплуатация первым стал шесть человек на самом деле там два devops а и 4 инженер поддержки давайте разберемся зачем здесь аналитики смотрите когда приходит запрос на новую интеграцию появляется новый цифровой сервис то именно аналитики из каши д договариваются в каком формате они примут данные а потом сами пойдут ко всем консилером и договорятся в каком формате они будут эти данные отдавать то есть по сути команда разработки цифрового сервиса вообще не должна думать о том как работать с данными они должны что-то выплюнуть и дальше их не волнует вот эти аналитики они сами договорятся составить схему они нарисуют т.д. для вот этих шести разработчиков и они на нашему мужестве затари обработают в нужном формате данные выложил нужный формат и обогатят всеми нужными полями чтобы все компьютеры могли с этими данными работать ну если мы услышали из чего он состоит давайте посмотрим какие на нем нагрузки rxtx это нормальное состояние нашего кластера в среднем 2 гигабита в секунду пике 78 гигабит в секунду обрабатывает эту кластер на сетевых интерфейсов именно для этого нам понадобился bonding потому что один сетевой интерфейс становится батоны при таких нагрузках посмотрим на скорость сообщения я обещал вам три миллиона в секунду энди у нас есть здесь мы видим скорость сообщений в секунду по на самом деле здесь под каждому серверу да и там намного больше этих сообщений то есть мы просуммируем 12 серверов у нас именно кафки но на каждом сервере практически на каждом сервере скорость обработка сообщений до 3 миллионов в секунду это график скажу честно он не сам атлетический я его взял с нашего мониторинга единственное что он как пила я же покажу следующий график вы поймете про что я говорю на самом деле выглядит это вот так если расширить немножко область видимости то есть среднего сообщение количество сообщений которые мы обрабатываем в нормальном состоянии это от 500 тысяч до миллиона а вот эти пики до 3 миллионов в секунду это когда наша bigdata обрабатывает свои 20 петабайт данных и прогоняет их через нас соответственно так как обработка 20 петабайт данных и таба чего я jobo и она начинается приблизительно 6 вечера и идет до 9 утра чтоб никому не мешать мы видим вот этот рост при этом кластер кафки как я сказал стс 12 серверов в каждом на в центре и он с этим справляется посмотрим что творится на серверах в это время как видите подсыпал все отлично кафка практически не употребляется по на самом деле и то что мы там закупились раз огромным количеством процессоров сильно не повлияло скорее эти процессоры нужны будут вот другие задачи то есть кафки они не особо нужны как мы добивались такой производительности ну самое важное это выделенные сервера мы пробовали запускаться на виртуальных серверах честно получилось не очень то есть и дело даже не в циpкa а в том что появляется во первых latrons на память во вторых невозможно выделять большие блоки памяти а именно память дает такую скорость и в третьих это вот второй здесь момент вы длины и счас диске то есть для кашки точно также как для кассандры под пар тишины очень выйдем в удар важно выделить отдельные диски поэтому нам пришлось покупать сервера с корзинами по 20 по 24 диска и забивать их небольшими там до терабайта маленькими дисками чтобы на них вешать портишь не только тогда мы стали получать приемлемую скорость если вы смешаете на одном диске все пар тишины вы никогда не получите такой performance как у нас на картинках выделенный свой контур player 2 да возникла проблема которую мы очень долго сначала gebo жили потом решали нам пришлось полностью построить отдельный сетевой контур для кафки потому что когда кафка работает и перерабатывает все эти сообщения перемалывает на ее свечах никто больше работать не может ну что чтобы вы не пробовали делать какие бы мы свечи не брали и ультрасовременные от хвалы и суперскоростные нет то есть когда работает кластер кашки работает только кластер кафки соответственно пришлось построить отдельности вы кун для этого решения чтобы она никому не мешала работа ей свою очередь тоже никто не мог помешать работу и самый важный тут пункт откуда берется это скорость до это то что по сути мы победить дарим не кафки кафки не нужна память скажу честно кафка у вас при любых нагрузках не будет выедать больше 20 его оперативки ну почти не когда вы этого не увидите куда память делась память в кошелек операционной системы то есть по сути мы работаем даже не с дисками мы закрыв крышкой операционная система за нас заколлировал а данные которым мы чаще всего обращаемся именно за счет этого мы можем с максимальной скоростью их читать и обрабатывать на этом кластере еще немаловажно и то есть как затюнить java машину для того чтобы кошка могла работать с такой производительностью как я уже сказал кафка у вас много памяти не будет потреблять и здесь вы видите что мы ее даже ограничили чтобы нас больше двадцати четырех гигабайт оперативки никогда не скушала потому что они и не нужны то есть если она их съест больше мы не получим пути прироста но зато она их съест из кэша у операционной системы останется мало кэш и вот тогда мы получим деградацию что здесь еще очень важно заметьте мы используем уже один garbage collector то есть мы не используем стандартный garbage collector и мы для него выставляем всего лишь один очень важный параметр это макс без и пауза в миллисекундах то есть мы garbage collector сообщаем что неважно сколько мусора у нас в java неважно что как он этого добьется и что он будет делать но больше чем на 20 миллисекунд он не имеет право остановить processing за счет этого мы никогда не получаем длинный стоп зовут и еще вы можете увидеть что соответственно мы еще это спейсайд здесь рулим за что отвечает от параметрами the space сайт он отвечает за то когда запустить fools and garbage collector а то есть здесь мог говорится что если меньше 50 процентов занята от 96 мегабайт то пуском вообще никогда не запускается и если у нас фритюр а 80 процентов да и 50 соответственно если мы выжили уже больше 50 процентов то давай уже можно запускать full scale параметры кафки соответственно и топорами той самой кафки здесь я привел не все параметры но самые важные которые нам пришлось выкрутить выкручены они от дефолтных значений от от ну в пять раз мы их увеличили какие-то параметры какие-то в 10 раз на порядок ничего не увеличена то есть соответственно мы увеличили количество background рейдов этого что нам нужно чтоб больше было 3 до слова у нас параллельная обработка следующий параметр а j3 мы тоже выкрутили по умолчанию он по моему достаточно маленький мы выкрутили максимальное значение для наших дисков важно это значение найти для себя из-за количества ваших дисков если вы поставите на одном диске такое значение вы просто а и о своего диска убьете в ноль у вас не будет ничего работать соответственно мы выкрутили количество сетевых трейдов потому что у нас ведь как я уже говорил становилось очень часто батал ником и нам нужно было разогнать сеть и из за этого же последние два параметра мы увеличились intel isef буферы накипи для того чтобы большим утилизировать информации поступающей по сети соответственно replica watches это параметр который контролирует количество процессов отвечающих за репликацию внутри кластера мы тоже у выкрутили по умолчанию он один но нам важно получать быструю репликацию ну и соответственно сколько максимально request of parallel мы будем обрабатывать в каждом топике он тоже не такой большой по дефолту кафки давайте поговорим про преимущества которые мы получили от такого решения решение однозначно упрощает большое количество интеграций как я говорил что схеме flash на каждую интеграцию почты уходил уже от полгода и больше в данном случае интеграция делается максимально может быть 30 дней 30 дней если очень сложная трансформация данных очень сложная если это вообще без трансформации то есть ну так повезло что продюсеры консьюмер каким-то образом мы оказались очень адекватными и работают в одном формате два дня два дня на интеграцию это очень сильный драйвер для старта большого числа новых проектов и сервисов как я уже говорил смотрите вы львиную долю работы разработчиков цифровых сервисов забрались сюда и мне надо работать с данными и мне надо заботиться визит стащу их должно заботить это их данные отдаются в g sony какие данные да ладно там кафка разберется грязные сухие там любые без разницы то есть они не тратят на это время они просто пишут и учета пишут собственно говоря вы за них делаете всю интеграцию и начинаете конфитюром отдавать его в нормальном виде это высокий уровень надежности из коробки но также написано кафка на что на самом деле по дефолту даже по дефолту у нее реплик для каждого топика это минимум было кстати мы этот параметр не сильно то и крутим крутим нам хватает 2 реплик это достаточно надежно ну для каких-то там супер важных топиков там с персональными данными ну там 300 и и соответственно сервис на сервис-ориентированной архитектуры вот такие преимущества данного решения но у него есть сожалению недостатки про которые мы тоже с вами проговорим соответственно как я уже говорил это становится единым сервисом отказа если он у вас откажет у вас становится все сервис но если вы конечно не захотите еще цел bags сделать какой-нибудь как умеешь да то есть если кафка отказалась отдал мы еще full мышами и скорее всего вы не захотите со временем может превратиться в бутылочное горлышко к сожалению мы проходили этот этап то есть и действительно так как данное решение является очень сильным драйвером созданию большого количества цифровых сервисов она позволяет очень быстро их запускать в какой-то момент сервис и цифровые начинают создаваться и появляться намного быстрее чем команды интеграции успевает делать это интеграции и уже цифровые сервисы ждут когда пытается интеграции такое может быть это довольно дорогое решение для внедрения и поддержки если у вас совсем не большие масштабы то есть если у вас 5 10 сервисов вы не планируете их увеличивать на порядок там хотя бы в пять раз до если у вас нет такого количества интеграции если у вас маленькая связанность есть у вас самодостаточными сервисы которые не требуют интеграции вам просто не нужно такое решение это очень дорог такое решение дает очень высокую нагрузку на сетевое оборудование то есть если вы захотите построить у себя такое решение сразу думайте о том какой будет контур ситилинк и требует соответственно отдельного железо вот такие я выделю минусы такого решения давайте попробуем сделать выводы я думаю каждый из вас уже какие то выводы для себя сделал да я выскажу свое мнение если вы меня вернули на 8 лет назад и попросили построить такое решение то наверное я использовал вот те же самые технологии я был взял ту же самую сам за ту же самую кафку я построил бы точно такое же решение во всем может быть там пересмотрел бы где-нибудь тогда уже был в наличии keystone да то есть не стал бы делать плакал локальную выдачу токена но в целом построил бы тоже самое если не а такое решение просили построить сейчас я бы пересмотрел архитектуру часто скажу я бы не стал писать файловое хранилище использовал из 3 я бы отказался скорее всего от hadoop а потому что мы не используем dfs его посмотрел на запуск явно если уж нужен такой скиту лир но его можно и в q перезапускать до самсу я бы она не стану трогать канал тоже кафку оставил бы но вот какие тут у hадeюcь я бы точно убрал но стоит ли создавать такое решение тут решать уже каждому из вас спасибо за внимание это иван вас друзья ваши вопросы поднимайте позвал 100 руки первый ряд сходу я вижу руку и потом вот в правой стороне зала выпуск не может немножко не в тему упоминалось больной вопрос российской криптографии требования регулятора это понимаю реплики в разных сотах или в одном из раз лимит реп реплики в разных цветах или одну из разнообразных т.е. соответственно как с требование регулятора российская криптография чем закрываете канал может чуть посоветовать при такой высокой нагрузки нет этих у нас нет а требования от регулятора конкретно к этим данным потому что это не аттестованный контур то есть данные те которые покрываются требованиям российского регулятора по криптографии у нас в отдельном контуре они никогда не проходят через кавказ такую проблему нельзя ну вернее она была поэтому мы через кафку мы не возим такие данные прост которые запрещено ввозить потому что у нас и сертификации партии pulmonary же нет 2 сразу в догонку большинство этих серверов сервисах микро серовато синхронные сервис и каким способом ну и соответственно в клиенты обычно идет синхронный запрос он послал запрос через ftp ждет нам ним рамках него ответа то есть либо вы сервисе переделывали под асинхронный режим либо оставили какую-то прослойку у нас например очень много сервисов которые изначально были синхронны и начинает работать с шинами и тут начинаются хороший хорошего по спасибо большое на самом деле да но дело в том что у нас изначально все сервисы писались подъезд а это асинхронный интерфейс api socket поэтому мы наоборот сейчас страдаем от того что мы не можем предоставить синхронные на интерфейсы у нас интерфейс и асинхронные но как раз все сервиса умеют работать с этим так я вижу вопрос есть за колонной в правой стороне зала поднимайте руки еще если вы в онлайне нажмите справа трансляции кнопочку выходите в эфир и и он спасибо за доклад вот правильно я слышал что цифру 30 тысяч до полного скале клиентов континентов 40000 до 40 . а вас я не услышал если вас хотят да это коварный вас вокруг этого вообще вы сказали что вас команда вроде как довольно маленькой аналитики отвечают за формат данных которые там ссылается принимается все-таки есть какие-то вас внутренние либо написанных отдельно политики либо автоматизированным способом проверки консистентной стенами что это потапов там джейсон или кто что хочет тот стой и шлет как это все ну да у нас валидация я говорил что она проходит все таки на уровне кафки да мы совсем не валидные данные не примем протокол действительно у нас всегда практически jison у нас еще есть департамент сервисной архитектуры где работает достаточно большое количество людей которые как раз описывают в каком формате должны быть идеальные данные называется rtm да это формы взаимодействия то есть соответственно в каком формате должны быть данные для там описав каком формате мы данные передаем в таможню в каком формате данные должны быть чтоб мы обменивались с алиэкспресс либо с кипой у нас есть и регламентом лишь документы правильно ли я понимаю что проблемы несовместимости мир данных на стороне консьюмер продюсер это не ну то есть вы отвечаете за шину а это уже решается команды консилер и продюсер если кто-то изменил формат на входе или этель джабба где-то вас там который конвертирует данные упала то есть это проблемы карте мира продюсеров за которых вы не отвечаете на как команда с одной стороны да то есть если нет давайте разберемся то есть если консьюмер стал ожидать другие данные не оповестив нас действительно его проблему потому что мы то будем ему в топик данного и складывать в том формате в котором мы умеем если продюсер изменил даны это не просто валидацию не пройдут но это тоже станет его проблема то есть это действительно то есть при любом изменении взаимодействие либо изменение формата данных они обязаны прийти или сначала clam и об этом сообщить все через ваших аналитиков цикла дарует спасибо так я вижу на первом ряду руку а потом на последнем ряду простите спасибо большое за доклад скажите пожалуйста какие у вас гарантии доставки данных например случае dc -1 что еще раз какие у вас гарантии доставки данных например в случае если какой-то сот желтом на регламентные работы учения гарантией доставки данных но вообще у нас sl55 девятка соответственно но за последние года четыре у нас не было вообще ни одного случая когда бы мы остановили работу доставки данных доставки данных через приложение но вообще педиатр отлично я вижу на последнем ряду руку если вдруг я не вижу вашу руку сделать так чтобы мы вас услышали здравствуй а спасибо за доклад скажите пожалуйста вы проводили нагрузочное тестирование вашей системы проводите ли вы регулярно например вы знаете и текну приделанным на текущих на текущем железе и например почему выбран счас вы думали думали ли вы о флешках или например добавить дополнительных карточек в качестве для бой например с 4 интерфейсов или вам просто это не нужно потому что interconnect между dc не поддерживает таких скоростей спасибо большое за вопрос хороший вопрос да действительно я не рассказал про наши планы давайте с этим немножко разберемся коротко во-первых нагрузочное тестирование мы действительно проводили но мы его не проводим повторно так как у нас не появляются соответственно новые требования зачем мы и так знаем как работает наш системы по поводу переходить на full flash пока что не планируем потому что просто от нам негде взять такие нагрузки мы не понимаем откуда возьмутся такие нагрузки чтобы нам понадобился full flash по поводу собственно говоря планов вот единственное что в планах до в ближайших нашим так сказать баттлоге у мы хотим во-первых перестать использовать apache кафку и привести на конфликт кафку за счет того что там есть схема registry и мы хотим перевести данные на евроформат на не необычным джейсон для того чтобы больше работать с контрактами то есть вот и ответ на ваш вопрос что за собственно говоря мы еще не пришли до тоже очень хотим на самом деле вот и в планах возможно либо попробовать избавиться полностью от ходу по либо по крайней мере его обновить на данный момент у нас hadoop достаточно старой версии 27 хотя бы перейти на тройку спасибо вот все же по тестированию пусть вы проводили нагрузку перед запуском да и сколько вот вас попугаев насчитала что от вас текущая прот нагрузка сколько вы держите там 3 5 10 12 поэтому нам больше не нужно задумываться пока она из изменением flash или других например железных обновлений но максимальную нагрузку которую можно например вот вы сказали мы померили попугаях вот у нас допустим как и сколько там агаев насчитали на нагрузочном тестирования сравнивая на считанных попугаев с текущей нагрузки вы говорите мы можем выдержать и стирательную нагрузку то есть нам больше еще нужно делать там сколько-то лет такой у нас прогноз там допустим нам текущих мощности хватит там три пять десять два года года я понял о чем вы говорите скажут прогноз да нет мы не проводили такое тестирование мы действительно не знаем потолка и мы не знаем в какой какое количество времени мы проработаем под нагрузкой в три миллиона секунду например даже то есть он может быть мы там через сутки загнемся при такой нагрузке не проводили не знаем и еще один вопросик это скорее про java то есть мы как на какой версии java сейчас у вас работает кафка и про мониторинг самого джона в принципе вы мониторите лед вы просто накрутили настроек у вас есть вообще железная конфигурация и в этом графике по работ красивом они там мониторинг признается что наверное есть самой кафки но живем и кафки вы не мониторить или мониторить отдельными красками как работает gc и вот эту всю историю мы конечно мониторим gc и мониторим сам обжаловал машину я просто не стал приводить эти графики потому что они не с тем интересны мониторинг у нас мы внутри максим все что дотянулись что можно мониторить и это не только про этот про это приложение да вообще в почте россии а java версия похвастать нечем там 8 версии конечно есть центре зала вопрос подними руку пожалуйста но если еще вопрос поднимите руку или это будет последний она есть еще один здравствуйте подскажите пожалуйста а приходилось ли вам вводить реплей данных то есть если трубка накосячили на консоме я вам надо сместить опыта назад и перина катить вот все вот все сообщения переработать но это больше вопрос к intention а потому что у вас большой объем данных через кафку проходит и его хранить если вдруг нужен реплей да ну во-первых хриплые у нас обязательно есть и вы зря думаете что у нас маленькие ритм стоны вы же вели вида количество серверов у нас в каждом по 20 дисков скажу честно у нас retain шины минимальный это месяц максимальные зависит сильно от нашего законодательства какие-то данные мы годами обязаны хранить соответственно мы это храним как бы клок а в том числе в кафки для того чтобы что случится у конструировал мог перед встать отличный в правой части зала на первом ряду это будет завершающего просто спасибо за доклад вы на самом деле от момент вскользь в конце упомянули я так понимаю ваша команда отвечает за все интеграции между всеми сервисами профессионал во всех интеграция участвуют вы собственно не выйти что ваша команда и будет действительно бутылочным горлышком когда пул задач по интеграции будет расти расти расти расти и действительно это будет уже не месяц гораздо больше не проще ли было выкатить кафку ну и определенный какой-то инструментарий как сервис дать командам возможность самим договариваться то есть как они через вас будут работать я почему спрашиваю кейс знакомы как бы спасибо большое отличный вопрос на самом деле я уже не хватало времени много не договорил да и мы не только не думаем мы это проходили я говорил что это становится ботаником да и честно говоря сейчас я бы говорил я бы архитектуру полностью переделал я бы убрал бы в разработку отсюда да и я бы дал им действительные инструменты вас пусть это будет то же самое сам запуск будет тот же самый camel но пусть трансформацию данных пишут сами команды пусть они встречаются договариваются не могут вдвоем пусть сообразят на троих да то есть как бы сейчас этого нет нет этого человеческого общения мы вот с помощью этого решения мы ломали коммуникацию между командами и это на самом деле самый крупный недостаток этого решения это то что пусть это будет эксклюзивчик михаила да да то есть то что говорить вот это я не очень планировал новой абсолютно права действительно это так то есть во первых она станет battle никому вторых вы сломаете коммуникацию между командами и сломаете там не знаю им мозг тем что они не будут больше работать с данными"
}