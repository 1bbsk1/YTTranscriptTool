{
  "video_id": "65b_u0dzLts",
  "channel": "HighLoadChannel",
  "title": "Автоматическая рубрикация текстов / Злата Обуховская (Rambler&Co)",
  "views": 3906,
  "duration": 2471,
  "published": "2017-05-14T22:53:05-07:00",
  "text": "мой доклад называется автоматической рубрикации текстов ну изначально я хотела этот доклад делать вообще про алгоритмы алгоритмы потом мне стало понятно очевидно что алгоритм без практики они вообще никому не интересно у нас же тут backend кант и все такое вот поэтому на самом деле доклад будет не совсем про автоматическую рубрикация скорее про то какие виды задач для работы с текстами вообще решают сервисах которые с большим количеством текстов работают ну так получилось что в нашем холдинге вообще очень много всяких контентных сервисов там тоже шишечка у которой сервисов текстов вообще много много там афиша где есть какие-то описания ивентов каких неочевидные сервисы типа rambler косы которые на самом деле этим контентом но фишер ским допустим активно пользуется и у всех этих сервисов у них встаёт вопрос о том как контент быстро автоматически обрабатывать вот допустим в нашем сервисе в котором я работаю в rambler новостях ну текстов тоже очень много это новости как вы поняли не у нас есть какой-то маленький робот который их регулярно скачивает а дальше он готовит текст вот каким образом он делает кластеризацию то есть складывать тексты в новостные сюжеты дальше он эти тексты вернее кластер и текстов классифицирует по рубрикам собственно откуда название доклада при рубрикации взялось вот а потом вступает в работу редакция которая исправляет ошибки который робот наделал соответственно у нас достаточно много большой поток документов большой поток пользовательского трафика поэтому нам важно эти тексты как можно быстрее приготовить и пользователю их показать вот соответственно от работы этих алгоритмов по кластеризации классификации все очень сильно зависит плюс нам хочется конечно как можно меньше тратить на модерацию ну вот давайте посмотрим предыдущий слайд там видите последнем пункте редакция вступает в дело пока редакции нет модельеры там у нас что томас копы новость продакшна не попадает на сайт не попадает поэтому нам тоже хочется минимизировать скорость работы людей вот соответственно вот такие вот наши будни это кластеризация классификация и редакция ну как я сказал уже скорость качество минимизации ручного труда какие сейчас есть с этим проблемы с кластеризации проблем на самом деле не так меньше не так много как могло бы быть но они тем не менее есть у нас вечно склеиваться какие-то не связанные события ну например вечные проблемы у нас в перми у нас допустим события про какой-нибудь пожар в москве постоянно подклеиваем к событию про пожара в каком-то другом регионе это вообще какая-то ерунда и это сильно сигнализирует о том что с качеством работ алгоритма все плохо но соответственно если алгоритмы работает плохо то это выше повышает скорость повышает затраты ручного труда и редакции ну и скорость доставки контента падает вторая задача по классификации или рубрикации я буду эти два термина использовать ну и регулярно поочередном классификация тоже у нас есть проблемы постоянно классифицируется текст не в те рубрике в той же злосчастный перми у нас в рубрику там стиле жизни попадают какие-то криминальной хроники короче нам хочется улучшить жизни в перми в целом и для этого нужно улучшить наши алгоритмы работы с текстами у нас есть понимание в принципе чего мы от этих алгоритмов хотим у нас есть понимание как текущие алгоритмы устроенным в чем их недостатки ну а дальше начинается и research а какие подходы можно попробовать еще и мы начали исследование некоторое время назад и вспомнили что все алгоритмы у нас так или иначе завязаны на чистоты слов в документах возможно это не очевидная мысль достаточно но слова в коллекции текстов на распределены неодинаково вот допустим если мы по рубрике политика построим какой-то топ слов то у нас наверх вылезет но всякая чушь томске предлоги там союзы и более менее значимое слово тут глагол быть и ну которая тоже на самом деле этом к тематике круто реки политика не имеет никакого отношения ну чё делать чтобы понять какие частоты допустим для и рубрике политика является какие слова для рубрике политика являются характеризующими ну допустим можно для этих слов посчитать еще частоту их встречаемости в документах но что имеется ввиду имеется ввиду скольких документах это слово встретилось понятно что если слово встретилось во всех документах то оно какое-то как бы обо всем типа глагола быть' и вот если слово встретилось только в документах относящихся к рубрике политика то это слово хорошо на тему политика характеризуют если мы посчитаем таки частоты то мы отсортируем по этим частотам мы получим такой вот хороший списочек кстати да произведение частоты слова в рубрике там произведения с частоты слова по документам заяц tiff и gif и она произведений часто используется как такая метрика релевантности слова документа но в данном случае не документа рубрики картинка которая иллюстрирует мысль о том что слова вообще в коллекции документов распределены неодинаково она про закон ципфа потом а то что каждое следующее слово она достаточно по частоте вообще вот это список всех слов во всех документах новостей которые встретились там за 20 дней в мае месяце короче каждое следующее слово по чистоте оно достаточно сильно отличается от предыдущего слова по чистоте и вот мы имеем какую-то такую картинку мы запомним этот факт для того чтобы его использовать в понимании алгоритмов который мы будем исследовать чтобы решать наши проблемы с кластеризации классификации все такое ну выводы понятно там у слов в документах есть какая-то чисто по у рубрике есть какое-то свое распределение частот и так далее дальше мы вспоминаем что из школьного курса что алгоритмы вообще классификации делится на дискриминационные генеративный дискриминационные про то что построить там в пространстве слов как-то разделяющую поверхность и за счет эту поверхность чуть классифицировать генеративные а не про то чтобы понять структуру текста частотную структуру текста и на этом основании сделать какие-то выводы что вот эта кучка слов у нас принадлежит такой рубрике такой такому классу то кучка другому классу соответственно из генеративных алгоритмов генеративных моделей самая простая это мешок слов и и лучше объяснять на словах а на детских кубиках представим что у нас есть какие-то такие две строительные компании которые строят слова некоторые строят здание ну документы здание из определенного набора блоков вот у нас есть зеленая строительной компании у неё там такое распределение там блока в квадратных к блокам треугольным и синяя компании вот вопрос в зал вот вот как думаете вот такое вот здание вот его кто построил зеленый хорошо а ведь кто построил есть распределением так зеленая или синяя-синяя почему синие ну да по дизайна на синие похоже но на самом деле нет вот смотрите да по процентовки получается что эти два здания они на самом деле похоже они оба зеленые они не на 100 процентов как бы зеленые а вот на какой-то процент вот тут вот у нас 3366 а тут 25 и 75 это больше похоже чем пятьдесят на пятьдесят то есть мешок это такая очень простая штука вот процентовки посчитали на что больше похоже тут классы есть ну вот как бы на примере текста типа мы в тексте выделили два слова на тему из мешка политика 2-ой при текста из мешка финансы ну вот финансы победили ну конечно модели у этой есть недостатки потому что она предполагает что у вас тех сделан только из одной темы железобетонно но на самом деле это не так подход который улучшает эту модель в нем в принципе все то же самое есть строительной компании только теперь они строят не просто к из каких-то строительных блоков не строительных блоков таких архитектурных поттеров при этом эти архитектурные паттерн и они могут не просто так взялись они взялись из того что было в наличии вот у нас здесь было там 5 кубиков в сумме и там три треугольника в сумме вот мы их там как нарезали но здесь нарезка конечно не полная но допустим в этой нарезки не может быть какого-то архитектурного патерно из четырех треугольников потому что здесь у нас мы просто нет четырех треугольников значит дальше что происходит дальше а дальше у нас происходит вот что нам попадает на вход объект который мы хотим классифицировать мы хотим понять вот это здание его как бы кто построил мы тоже вот ищем в нем какие-то вот эти вот архитектурные паттерн и и как бы думаем о какая строительная компания могла их реализовать ну вот тут по всему выходит что синяя компании победила потому что она могла и тот и другой реализовать зеленая не могла вот на примере текстов как бы тексты тоже разрезаются на какие такие вот куски и эти куски они происходят из каких-то рубрик ну сколько кусков ну чем больше кусков относящихся к той или иной рубрике то рубрика и победил здесь тоже есть проблемы потому что ну в реальных библиотеках которые этот алгоритм реализуют разбиение на куски она делается там каким-то оптимальным образом и вообще найти такое разбиение достаточно дорого ну просто ваш алгоритм будет там медленно работать как бы в теории вот при этом если у нас появляется еще один застройщик то нужно прям вот строить заново перестраивать все заново искать эти новые разбиения новые архитектурные решения тоже может быть дорого вот в нашем допустим в наших новостях всегда у нас в яндексе находится 10000 свежих кластеров то есть ну как бы если аналогия про домики то 10000 домиков их нужно как-то постоянно там перри разрезать или что-то с ними еще делать короче подход вроде бы забавный но есть проблема значит следующий подход который казалось бы избавляет нас от недостатков предыдущего называется загадочным названием латентное размещение дирихле ну я лично думаю что его придумали люди которые вообще не парились они такие говорят давайте мы наши дома будем на архитектурное решение развязать случайным образом случайным образом как ну давайте кубик бросать вот а как нам подбросить кубика что это за кубик такой и при чем здесь вообще дирихле почему латентное размещение дирихле откуда такое название дело в том что мы в этой модели с начала склеиваем строим вылепливаем несимметричный кубик у этого кубика столько грани сколько у нас архитектурных решений для латентных топиков вот и все эти грани имеют какой-то вес вот и delphi ленту при том что нам нужно выбрать для эти грани веса и как мы выбираем веса вот сейчас очень такая грубая на самом деле аналогия математики меня закидают чем-то очень плохим короче мы типа на горку песка которая имеет которая похожа слегка на и распределение дирихле кидаем яблоко яблоко где там в песке застревают и расстояние значит от вершин до этого яблока вершины это у нас наши архитектурные решения нас и в предыдущем примере было перри в этом примере их перед короче расстояние от этих архитектурных решений до яблоко это как бы vesa кубика вот мы изобрели такой несимметричный кубик дальше мы его подбрасываем несколько раз для каждого блока который у нас в рубрику то есть в блоки из которых строит застройщик входил и получаем номера соответствующих архитектурных решений куда это блок попадает ну то есть какие-то такие у нас получаются эти архитектурные потер ным после работы нашего этого кубика дальше ну казалось бы похоже на предыдущий алгоритм вот у нас есть вот эти вот архитектурные потеряны теперь нам нужно классифицируемые документ как-то с этими архитектурными потерянными слой сравнить и тут отличие состоит в том что мы их просто не выискиваем в готовом домики мы точно также подбрасываем кубик которые каким-то случайным образом наш классифицируем не документ на эти потерянные разрезает и дальше мы уже эти паттерны и сравниваем с тем что у нас получилось для рубрик вот в этом примере типа у нас есть два каких-то случайных разбиения слов документа на латентные попки и мы поняли что там финансы зарплата это про финансы я размер установлена это вообще какая-то фигня и не про финансы вовсе почему эта штука вообще работает ну потому что за ней скрывается хорошая красивая содержательная теория вот в эту теорию нужно верить по идее и пытаться эту штуку использовать как говорится есть ложь наглая ложь и статистика вот если возвращаться к практике по на самом деле не все так радужно ну ld и там было темно семантический анализ вероятностно семантический анализ они реализованы во многих библиотеках в том числе там в таких вычислительных фреймворк как с парком haut но конкретно для нашего сервиса они не подходят для нас нужны какие-то такие легковесные библиотечки на питоне потому что мы программисты на питоне это не порок я считаю первое решение которое мы попробовали бы была библиотека ген сима это библиотека то ребята с гугла они обещали нам фантастическую скорость и онлайн новость это значит что можно не хранить их индекс в памяти можно его хранить в каком ухе распределенном хранилище и они предоставляют набор интерфейс чтобы зато хранилище читать по мере работы алгоритма штук она действительно быстрая по нашим испытанием но например на секунду за 10 там строят модели и как бы классифицируют вообще мгновенно практически но топике которые она выдает они достаточно странные и ну мы будем дальше их еще анализировать пытаться понять как приготовить потому что но совершенно не очевидно почему такие топике выделяет генсек а следующая библиотека это s'oliver она больше такая для песочницы каких-то академических исследованиях я вне клёвой документации она много чего умеет там много примеров вообще если вы интересуетесь каким-то машинным обучением там и программировать на питоне то стоит документацию и то что прочитать так вот мы пробовали салют результат и там были более понятные то есть топике там выделяется более понятные но она не очень хорошо пара лепится потому что там подразумевает что у вас весь индекс содержится в память то есть на нескольких новых вы не можете это развернуть и таким образом это решение она тоже нам подходит ну так на серединка на половинку есть еще решение которое лично мне рекомендовали коллеги соседнего отдела это библиотека от гула машинного обучения все я россии воронцова вот оно написано плюс х имеет удобную это не очень обертку и мы на нем наверное на него посмотрим ближайшем времени значит какие выводы из этого всего когда вы решаете какие-то алгоритмические задачи нужно неплохо было бы исследовать теорию которая стоит за вот этими алгоритмами они просто так что вот мы в библиотеку что-то в кинули и там получить какой-то не интерпретируемый результат ну то есть нужно хотя бы там приблизительно понимаете что эта штука делает внутри себя вот во вторых библиотеку нужно выбирать исходя из своих практических задач из архитектуры сервиса из каких-то еще ограничений вот но и счастье как-то она всегда не определена если вы хотите что-нибудь почитать про людей то есть сложная математика от и андрей венджи наверно это имя слышали те кто проходили какие-то курсы по машинному обучению на кроссовере есть статья воронцова она тоже на английском правда у него есть некоторые выжимки из этой статьи на русском вот они тоже провел день если хочется математики их можно почитать ну и я очень сильно всем рекомендую читать документацию кулером потому что там куча примеров куча графиков и кучу каких-то интерпретаций авторов от этого всего на этом меня наверное все вопросам спасибо за доклад вопрос такой вот вы рассказали то что вы использовали ген сим вот вы когда его запускали вы использовали какую модель там просто есть word век есть по-моему было они собирались пилить параграф ту век вот как раз чтобы документы можно было как у стрельцов и еще есть вот это л да мы использовали aldo конечно же для этого в общем весь этот рассказ при алгоритмы тут и был но еще мы использовали ну латентный семантический анализ в win7 это называется pls ой вот и у нас получилось что топике который выдает полоса и они вообще-то более понятны более того он выдает по топику на рубрику а л д а он выдаёт много много топиков и там на опорные слова они мешаются из разных рубрик то есть рубрика допустим какая нибудь про политику может легко там замешиваться в топик про спорт и это вообще ну так тяжело интерпретировать ну да вот просто на лекциях там воронцов о том рассказал то что как раз что aldo что полоса как бы главная проблема из того что они работают на вероятностные основе они постоянно могут генерить все новые и новые результаты и поэтому чтобы это как бы как то редактировать нужно именно настраивать модели что но для своими это с специфическими коэффициентами ну то есть вот то есть прям встраивать уже вот тут вот так вот там так ну я правильно понимаю что речь тут идет про регуляризация ну да про это наверное стоило сказать но я была не уверена что такие прям подробности интересные бега rtm он действительно отличается от обычного aldo тем что там они придумали какую-то хитрую регуляризации вот это вот статья который я привожу тут в примерах она как раз таки правоту регуляризации регуляризации то что это про как бы тонкую настройку коэффициент до чтобы оптимально найти коэффициенты чтобы они у нас там но как бы не упирались в локальный минимум там локальный максимум а как то там хорошо находились такой вопрос можно тащить пожалуйста л да это обучение с учителем да и латентные значит назначать что вы не знаете какие фичи используют ваш лёнинг то есть для вас это черный ящик грубо говоря вот здесь какой-то кластер там размеченных данных вы его скармливаете нет да это обучении без учителя ну в этом-то и его вся прелесть наверное мне стоило уделить побольше внимания тому что наш предыдущий алгоритм который сейчас у нас новостях для рубрикации работает это как раз такие машины обучения sea port вектор машине если вам это о чем то говорит им нас есть проблемы с тем что при добавлении новой рубрики нам всю эту модель нужно переобучать в моделях которые подразумевают обучение с учителем нужно собрать большой обучающую выборку мы выяснили что у нас то машинное обучение но хоть как-то хорошо работает когда обучающий выборка равного тысячам объектов то есть понимаете это нужно посадить редакцию они должны 8000 текстов разметить ну в принципе это можно сделать там за недельку то есть исторические данные есть какие то там у ребят агрегаторы новостные которые уже кучу данных содержит различных да но новости это такая штука которая постоянно меняется понимаете если даже взять ту рубрику политика появляется в ней какое-то новое имя какая-то новая локация и у вас частоты слов меняется у вас раньше был один политика высокочастотный а потом стал другой политик высокочастотный поэтому постоянно нужно эту модель переобучать а это неудобно и нам хотелось чтобы это делалось как бы полуавтоматически вот у нас есть что у нас есть редакция волшебная которая постоянно модерирует документы она разбрасывает их по правильным рубрикам вот казалось бы давайте мы возьмем вот эти вот размеченные документы модерирование редакции закинем этот самый aldo и он нам но не то чтобы с учителем практически без учителя приготовят рубрики вот в этом была вся идея затем чтобы использовать aldo против машину обучение которое есть сейчас еще вопрос какой то морфологии streaming или вы полагаетесь на то что ли машин лёнинг все колокации выделить сам все там синонимы где нет нет конечно машин learning ничего такого не сделает это же не искусственный интеллект не skynet которые помню мышц захватить нет естественно морфологию делать нужно сейчас у нас делается морфологии с помощью аута такая старинная библиотека имени алексей с acchi kocchi которые там ну именитый лингвистом он в яндексе работает последние лет 50 вот короче очень давно самого создания yandex вот но-уооо то есть проблема у него течет память нам приходится наш маркер пизы перезапускать но который делает морфологию каждые 10 запусков потому что иначе он съедает памяти кладет всю воду вот мы хотим отказаться со временем at a lot of пользу , чьи библиотеки поймав едва ну насколько я знаю ее создали именно потому что у отца имел некоторые проблемы несмотря на именитого лингвиста в авторах вот для примеров которые ну там делать считала какие то что ты слов по рубрике там все слова не нормализованы они нормализованы как раз таки вот этим поймать и 2 там было очень много всяких мусорных слов которые вряд ли как то вы могут нам власти рисовать документ да конечно именно поэтому я пыталась донести мысль и что вообще-то слова неплохо отфильтровать пути фадеев более того ребята из ген сима они в своей документации пишут что у них даже есть у их модели там у них есть такой объект dictionary который коллекцию хранит там есть специальные методы для того чтобы отфильтровывать высокочастотные слова слова и низкочастотные слова вот но я не пишет что ну все зависит от того как вы вот эти слова приготовить и приготовить и хорошо и будет вам al diar работать хорошо приготовить и плохо ну извините как бы волшебных методов не бывает привет такой вопрос вот как раз на этом слайде право алгоритма ты оставил сказал там были тайминги build 7 порядка 10 секунд а правильно секунд до а два остальных какие-то книги чтобы относительность понять вообще быстрыми но здесь секунд это контекст задач это быстро быстро или медленно смотри да ты прав во первых нужно было сказать 10 секунд это на чем вообще на какой коллекции вот коллекция там было из 50 тысяч документов по 9 рубрикам вот и 10 секунд это только построении индекса после того как мы построили индекс нам нужно эти топики по индексу выделить для классифицируем документа нет происходит тому достаточно быстром меньше секунды вот с аленом ну бегом я не использовала ни про это говорила с аленом я тайминги тебе не приведу потому что ну я их не помню но в по ощущениям понят что это не мне метрика да да там быстрее лучше было ощущением примерно то же самое просто там-то таки были более понятный запреты и второй вопрос а что мы можем принять за метрику качества вот у нас есть некий классных текстов мы хотим из него получать некое качество качественная кати реализацию чтобы ты взяла за метрику качеству которое можно не вращениях мерить а в числах или близких числа величинах метрики качества это вот такой несколько странный вопрос потому что люди которые занимаются там классификации знают что качество мериться по полноте точности там и их производных типа tom cat россии в мера и так далее я хотела рассказать про то что такое полнота и что такое точность но решила что в контексте вот данного доклада где мы в общем то но информация перевалом занимаемся просто разбираем то как какой-то отдельно взятый сервис по обработке текстов решает свои проблемы ну я решила что давайте вот мы в математику настолько сильно не будем лезть потому что это все таки формулы там их нужно как-то осмыслять в рамках наводок лада там когда у тебя там меньше 1 секунду на слайд это бесполезно я могу отдельно рассказать про то что такой полнотой что такое точность злата привет спасибо за доклад у меня вопрос примерно от паши задал вопрос про митрий качество а вы же собираетесь от своего переходить на л.д. железновы же должны падук то доказать что мы за implement сеть или новый алгоритм новую модель и вот у неё такие показатели как вы докажете продукту ему неважно как там что под капотом как работает ну естественно когда если говорить о такой же совсем практике практике то мы вообще собирались наших продуктовых планах написать специальный интерфейсе где разработчики будут ставить оценки тому что выдаст у нас новый алгоритм и по этим оценкам у нас будет по какой-то формуле уже осуществляться по счет качества и эти цифры мы будем уже давать нашему продукту и говорить что вот смотрите попробовали так здесь получилось так а вот раньше пробовали и было вот так ну то есть естественно мы про это все заранее подумали мы уже делали там всякие машина обучаемые штуки и понимаем что чтобы что-то улучшает нужно сначала что-то измерять спасибо еще один вопрос а инженерно то есть получается что есть ген чем то есть вы на фитиль и модель соответственно считали вот у меня была проблема socket лен я она почему-то не могла нормально работать с разреженными матрицами имеет с бензином в этом аспекте было все хорошо то есть там были нормализованы разреженной матрицы и развили размерность моего вот это пространство в матричном она там была там на порядок меньше по памяти влезала а у вас как это все устроено то есть как вы после того как за фитили модель как вы делаете predict когда вам на флот попадает новый документ как это инженерно решается инженерно у нас все разрезано на несколько worker of каждый исполняет какой-то функции все размазана при этом еще по нескольким машинам нас есть какая-то модель ну сейчас это в продакшене tsm которые тоже там живет в памяти там свои результаты какие-то дам пятна диск и соответственно все worker и которые работают с результатами этой модели они тоже должны жить на этой женой нам хотелось бы чтобы не жилиной разных но отдых поэтому ну вот мы хотели попробовать генсек думали что там можно как то вот какой то сторож положить там его промежуточный результат нет нельзя но у нас не получилось да мы в итоге ничего не получилось там прикажете поэтому там какие-то были программные инженерные проблемы с как раз таки с обращением вот к этой матрице которых памяти висит и нам вот во первых у нас конечно не 10 секунд поднималась то есть там есть 2 проблемы первое это на считать gain симам неким ну там файлы с расширением до то где хранятся собственно коэффициента дальше была вторая проблема это каждый маркер должен перед тем как подняться вы читаете эти данные то есть это у нас уже занимала там полторы минуты то есть worker который готов к работе кпереди кто он уже там там полторы две минуты у нас просто маленькая флотского приложения про стартовала с этими данными вот и третье predict за сколько секунд новый документ разбирался относился к конкретной рубрики ну в продукте нет все нужно измерять нас как бы сейчас все находится на стадии исследования мы проводим пробуем разные подходы и пытаемся понять вообще для наших данных они подходят или нет потому что данных у нас как бы не то чтобы очень много но вот 10000 вот этих вот новых документах день не так много я бы сказал вот у нас обучающая выборка было 500к документов корпус и и у нас не делит рубрику нас 370 рубрик то есть мы попытались category заводь там продукты питания то есть просто относить автоматические мерили качество модели паркер из но там нам удавалось там но не больше семидесяти пяти процентов по покеру то есть это много лучшие наши показатели были а знаете что если у вас так много классов то может быть вам ну попробовать нил до что-нибудь попроще я вот недавно читала статью ребята из серфинг bird не говорят что мы замучились делать тематическое моделирование с помощью all data ну и всяких таких сложных модных сделали просто какой то типа мешок слов там на частоте встречаемости у нас точно так же на сначала идет и d векторизации потом этот ген simas верх над ним это ну там из когда делом predict это ближайший нотка ближайших соседей то есть мы еще пытаемся конкретный новый документ так как рубрик много мы пытаемся посмотреть по ближайшим соседям там 100 ближайшие соседи берем и уже относим конкретной рубрике таким вот образом хорошо можно рисовать на бумажке прошло и обсудим это да давайте продолжим там еще были вопросы даже вращения после доклада поговорите там есть привет спасибо за доклад я был не сначала возможно ты давала ответ на такой вопрос если текст очень короткий размером 12 предложения это будет работать нет на сайте есть витамин от меня классифицировать не будет если текст очень короткий 12 предложения то ну не нужен там олди и ну какие то что это использовать использовать старый добрый тив рельефные меры там б м 25 и вот это все мы когда-то лет 5 назад делали ранжирование постов микроблогах новым twitter конкретно и у нас получилось что как бы подходы основаны на подсчете частот слов даже там не важности какого-то пользователя не на количестве его там френдов и чего-то еще они дают больший вклад в релевантность просто текстовый fitch чем что-то более сложное то есть для когда в идеи и имеете дело с маленькими текстами не стоит городить крутые сложные штуки вот вообще не стоит а если еще и рубрик мало то может быть имеет смысл какой то не знаю этом базу знаний сделать просто ну если базу знаний то вам нужно какие-то факты выделяет что ли из этих текстов или что посадить человека чтобы правило просто прописал руками там ну а если у вас вот 10000 текстов в день пусть небольших а маленьких как вы человек на это посадить или twitter у вас реальному да да вот понятно и еще такой вопрос вот ты рассказала про питон стек а вот я например пришел пишу накала жюри и вот я могу завязать жару библиотеки какие-то или может быть лазерные есть что-то может посоветовать для такой задачи нумер java он прекрасен и удивителен вот там но много чего есть насколько я знаю может ты просто что-то посоветуешь нет ничего не посоветую на самом деле последний раз когда имела дело с библиотеками для обработки текстов на java это было когда но мы реально не могли найти штуку которая выделяет части речь и оказалось что там стенд вратарский выделять или частей речи написаны наяву это вот нам на java извините это единственное решение вот поэтому про мир классификации там не дай бог i'll die in a java я знаю вообще очень мало ну я питон программист это не порок я считаю а вот стать еще тут упоминали а вот а еще одно я не знаю насчет перформанс самой steam lubby lubby вреда от яндекса она морфологии тоже делает но там низком перформанса совсем быть небольшой насколько я понимаю да и плюсом обещание обзор на мои ст м она делает streaming немане морфологию это немножко разная не нет она же там какой-то версии выдавала там какая часть речи там все такое выделение части rich вы знаете чтобы выделять части речи вам нужно на самом деле иметь грамматику построенную искусственную вот и я честно говоря недумаю что индекс делитесь своими грамматика my потому что это и хлеб у них там сидит отдел лингвистов которые эти грамматики строят и все такое а построить свою грамматику чтобы эта штука может быть они поэтому попросил окон source получается да да но мы не нашли на самом деле подходящее решение для выделения частей речь до сих пор вот в поиске и в конце концов мы на самом деле там использовали платное решения вот одно и малоизвестное научно-исследовательской лаборатории и для английского языка я так понимаю будет гораздо проще такую штуку найти до для английского языка это вот как раз та самая штука на java от стэнфорда ей там тоже об ангеле да и 50 лет там где что то типа того она очень старая она работает с русским языком все плохо мы что-то ушли в сторону куда пасибо себе пожалуйста слушай тогда спасибо злате волшебному rambler"
}