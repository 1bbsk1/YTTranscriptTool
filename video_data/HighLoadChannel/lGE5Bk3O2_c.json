{
  "video_id": "lGE5Bk3O2_c",
  "channel": "HighLoadChannel",
  "title": "Внутри S3 / Павел Левдик (Yandex Infrastructure)",
  "views": 548,
  "duration": 2649,
  "published": "2024-10-29T02:57:43-07:00",
  "text": "Паш ты как готов ныряем в инфру Яндекса вместе с Павлом Павел Лев Дек уже на сцене уверенный и светлый Всем привет Я Паша я разработчик сервиса object storage в Яндекс облаке и я катаю гусей некоторые сложности с кликером происходят один другой Паш Ну технологическая конференция технологические проблемы вот всё нормально отказ устойчивость две штуки а гусём называется наш энд s3p он так называется потому что Он написан на языке го Go п S3 примерно получается английское слово гуз Что значит гусь Возможно вы слышали прос FS это наш высокопроизводительный Клинт для S3 с помою него подмором ноутбуке или виртуалке которая будет работать с баке этом S3 его название также является отсылкой к нашему гусю в двух словах Что такое S3 S3 - это аббревиатура Simple storage Service Это протокол сервиса хранилища который придумал Amazon и который за многие годы стал стандартным объектного облачного хранилища когда мы создавали свою реализацию S3 Мы понимали что в в ней будут хранится огромное количество объектов в него будет идти высокая нагрузка по rps поэтому по этим параметрам нам нужно было уметь масштабироваться наш сервис должен быть отказоустойчивый то есть при выпадении любого Хоста и даже целого дата-центра сервис должен продолжить работать продолжить обслуживать нагрузку от клиентов и сервис должен быть консистентных то есть если пользователь залил в сервис какой-то Файлик то следующий запрос на чтение или на листинг должен вернуть этот Файлик а типичные требования крупного клиента S3 - это иметь в Бакет больше миллиарда объектов общим объёмом больше 1 пиб и иметь нагрузку больше 1000ps самые крупные наши клиенты имеют в десятки раз больше значения этих показателей А всего в наших инсталляция сейчас сотни миллиардов объектов сотни петабайт и сотни тысяч rps для примера Вот график РПС запросов от наших клиентов в Яндекс облаке за последний год за год нагрузка от наших клиентов выросла в два раза и в Пике достигала больше 120.000 ПС вот очень простая примитивная схема устройства нашего сервиса отдельно мы храним метаданные и отдельно мы храним данные и гусь отдельно ходит за метаданными и за данными качестве хранилища метаданных мы используем по в качестве хранилища данных мы используем сервис него я сейчас чуть подробней расскажу mds - это низкоуровневый сервис который работает напрямую с жёсткими дисками это аббревиатура от Media storage Раньше он использовался для хранения медиафайлов впоследствии он стал универсальным хранилищем для всех типов данных этот сервис был создан внутри Яндекса нашей командой примерно 10 лет назад его клиентами являются другие сервисы Яндекса такие как яндекс-диск Яндекс почта в том числе S3 и другие и за многие годы он зарекомендовал себя как надёжное и масштабируемой хранилище мои коллеги раньше уже делали доклады про МДС на различных площадках в том числе Вадим затеев год назад на лоде делал большой доклад про МДС кому интересны детали тот может пройти по QR коду сохранить ссылку и потом посмотреть запись немного цифр МДС - это триллионы файлов экзоты данных сотни Гигабит миллион rps 300.000 дисков в итоге когда мы создавали свою реализацию S3 у нас уже было идеальное низкоуровневое хранилище МДС нам нужно было создать поверх него протокол S3 поэтому суть этой самой прослойки S3 она в метаданных в умение их хранить и обрабатывать и именно об этом мой доклад вот Чуть более подробная схема нашего сервиса запрос от пользователя приходит на балансер балансер перенаправляет запрос на какого-то конкретного гуся Гусь ходит в метаба зу постгрес получает оттуда метаданные по объекту путь объекта в МДС идёт в МДС читает данные возвращает данные клиенту когда мы создавали S3 Мы понимали что в одном баке может лежать огромное количество ключей баке тов много Поэтому рано или поздно все данные не влезут на один сервер по поэтому очевидно было что нам нужно шардирование то есть хранить разные куски данных на разных шарда один шард поса представляет из себя мастера и две или больше реплики мастер находится в одном дата-центре реплика одна реплика находится в другом е реплика находится в Рем аппликацию Это значит что когда на мастер приходит какой-то пишущий запрос он записывает эти данные к себе а также ждёт пока эти же самые данные не будут записаны ещё хотя бы на одну реплику и в случае если мастер умирает то происходит автоматическое переключение мастера на самую актуальную реплику без потери данных как я уже сказал Мы понимали что данные даже одного достаточно большого Бакета не влезут на один сервер постс Поэтому нам нужно хранить разные куски данных на разных шарда если лексикографически расположить всё пространство имён объектов от пустой строки до бесконечности И разбить эту прямую на кусочки то вот эти кусочки мы называем чанка чанк - это диапазон имн ключей Бакета и вот эти самые чанки мы можем хранить на разных шарда дальше перед нами стояла задача придумать По какому правилу распределять эти чанки на разных шарда популярным подходом является статическое шардирование когда мы берём хэш от сущности берём остаток отделения на количество шардов и таким образом понимаем на в каком рде должна лежать эта сущность этот подход имеет некоторые минусы в этом случае сложно управлять нагрузкой если нагрузка от клиентов совпала таким образом что какой-то шарт начинает перегружать по месту или по цпу то в моменте сложно что-то с этим сделать и также в этой схеме Сложно делать шардинг то есть накидывать новых шардов в инсталляцию если нужно докинути поэтому мы использовали другой подход мы использовали динамическое шардирование чанк может лежать Ну на совершенно произвольном шарден где Какой чанк лежит мы сохраняем в отдель в отдельную метаба зу на схем для примера изображены чанки какого-то Бакета и на Первом рде лежат два чанка этого Бакета на втором рде два чанка этого Бакета на третьем рде вообще нет чанков этого Бакета на четвёртом и пятом шар по одному чанку и вот этот маппинг мы сохраняем отдельно это даёт нам возможность легко управлять нагрузкой То есть если какой-то шарт начинает перег по месту или по цпу то просто переносим чанки с этого Бакета на ой с этого шарда на другие менее нагруженные шарды и в этой схеме нам Легко делать шардинг то есть если нам нужно докинути нагрузка и таким образом можем накидывать шарды без всякого тайма для работы наше схемы нам нужны несколько поддерживающих вспомогательных фоновых процессов самый важный из них - это мувер когда один шарт начинает перегружать по месту или по цпу то р переносит чанки с этого шарда на другой менее нагруженный шарт Вот для примера чанки какого-то Бакета и допустим второй шарт начинает перегружают в этот момент и переносит чанк вот здесь вот например чанк от буквы Y до буквы Z переносит его на другой на третий шарт который менее загружен запущен на нескольких серверах Он моет параллельно с разных шардов с одного шарда в один момент времени возможен только один Move Чуть более детально Как работает р Сначала он блокирует чанк на запись потом Он копирует объекты в новый шард потом он удаляет объекты из старого шарда и потом он разблокирует чанк на запись блокировка здесь нужна для поддержания консистентность чтобы абсолютно все объекты которые пользователь мог заливать в этот чанк чтобы они ну Ровно все перенеслись на другой шарт M использует двухфазный комит он работает Ну получается с разными шарда и тут не может быть какой-то одной единой транзакции Ну и кра краевой случай когда например мо в середине своей работы ну падает остаётся висящая двухфакторная двухфазный комит который отдельный процесс Потом приходит смотрит состояние на разных шарда этого комита и в зависимости от состояния либо там откатывается везде где надо Либо наоборот комитет Везде где надо р работает единица секунд во время его работы запросы на чтение продолжают выполняться а запросы на запись покрываются ретра бэнда должен работать вот это самое короткое время единиц секунд для того чтобы пользователь в этих редких случаях когда он натыкается на работу с чанко который сейчас перевозит M чтобы пользователю не долго было ждать вот этот вот этот запрос пока энд трает во время работы ра для того чтобы наша схема работала и для того чтобы чанка происходил достаточно быстро почти незаметно для пользователя чанки должны быть небольшими экспериментально мы выяснили что чанке должны быть порядка 100.000 объектов поэтому для работы нашей схемы Нам нужен ещё один вспомогательный процесс это сплиттер сплиттер приходит и делит большие чанки на более маленькие чанки вот для примера на схем чанке какого-то Бакета и допустим клиент каким-то таким образом подавал нагрузку что чанк от буквы с до буквы Y разросся стал большим в этот момент приходит сплиттер и делит этот чанк на два более маленьких кусочка тоже более детально Как работает сплиттер он блокирует чанк на запись определяет границы деления пересчитывает объекты в одном кусочке в другом кусочки и после этого делит чанк на два маленьких кусочка И после этого разблокирует ча здесь блокировка нужна для поддержания консистентность счётчиков про счётчики я чуть позже расскажу сплиттер тоже работает Быстро единица секунд и даже быстрее пока он работает чтение работает А запись покрывается ретра бэнда Ну и чуть-чуть деталей есть разные возможные политики примитивная политика это делить Чан пополам но мы в основном используем политику деления 80 на 20% потому что достаточно частый паттерн записи клиента когда он больше пишет возрастающие ключи больше пишет в конец там допустим чанка поэтому здесь могут быть разные политики У нас есть несколько типов базы данных первый тип базы данных - это ремета на ней как раз-таки хранится тот самый маппинг чанков На каком рде какой чанк лежит Кроме того на ремета лежит информация по баке тамм и статистика по количеству и по размеру Мета У нас тоже имеет возможность шардирование нам этого не требуется Поэтому нам хватает одного шарда Мета второй тип баз данных мы называем S3 db это уже те самые основные шарды на которых лежат чанки на которых лежат объекты то есть там лежит информация по каждому объекту имя метаданные и путь обекта ВД как раз-таки шардов у нас много десятки и третий тип базы данных - это гамета она служит для Discovery в ней хранится список шардов Мета и S 3db гусь на старте получает только параметры подключения к гамета потом уже ходит в гамету получает списки шардов и устанавливает подключение к каждому отдельному рду в итоге ещё чуть более детальная схема нашего сервиса с уклоном на работу с базой данных запрос от пользователя приходит на балансер балансер перенаправляет на гуся Гусь когда-то давно уже сходил в помету поел списки всех шардов после этого он идёт в стриме чтобы понять В каком чанке лежит запрашиваемый объект и на каком рде этот чанк лежит после этого он идёт уже в нужный ему шар 3db На этой картинке он идёт во второй шарт 3 db02 получает там информацию по запрашиваемому объекту там метаданные путь в МДС идёт в МДС читает данные возвращает данные клиенту ещё одна важная задача которую нам нужно было уметь решать Это уметь оперативно понимать Сколько всего У пользователя в бате лежит объектов по количеству и по размеру нужно для пользователя чтобы он мог видеть сколько у него в баке объектов это нужно для определения квоты можно или нельзя очередной Новый объект записать не превысили ли мы квоту и это нужно на уровне отдельных чанков для работы наших фоновых процессов сплиттера и му Вера примитивный подход - это на каждый запрос обновлять счётчик допустим пользователь записал Файлик Обновили счётчик там удалил Файлик Обновили счётчик этот подход имеет минусы когда нагрузка от клиента становится достаточно высокой то разные запросы начинают конкурировать начинают хотеть обновить одну и ту же строчку ждут друг друга начинают страдать тайминги поэтому мы использовали другой подход мы использовали очередь счётчиков когда пользователь делает какое-то мутируют то мы сохраняем запись об этом событии в отдельную очередь например залил объект мы пишем плюс один объект плю 15 МБ удалил объект пишем минус оди объект там -2 МБ И потом отдельный фоновый процесс схлопывается эту очередь и обновляет уже исходные счётчики это избавилось облаке мы тарифицируется То есть если у пользователя в баке какой-то объект пролежал например 15 минут и 5 секунд то мы должны забили этот объект ровно за это время с точностью до секунды а не за час не за сутки и так далее И эту задачу нам также помогает решать тот же самый процесс который схлопывается был добавлен И в какую точно секунду объект был удалён и это даёт нам возможность считать биллинг с точностью до бай секунды в целом уже всё хорошо и та архитектура про которую я рассказал даёт нам возможность неограниченно масштабироваться но всё-таки есть некоторые неприятные паттерны которые нам мешают самый очевидный такой неприятный паттерн Для нас это когда клиент записывает объекты в Бат и использует тайм СМП в качестве ключа это могут быть логи это могут быть ещё какие-то события в этом случае все ключи которые записывает клиент всё время монотонно возрастают каждый следующий больше предыдущего в этом случае вся нагрузка от клиента идёт в самый последний чанк а раз в какой-то конкретный чанк то значит в какой-то конкретный шарт и это плохо потому что это не даёт нам возможности размазывать нагрузку от этого клиента и масштабироваться Вот пример на схемки этого паттерна допустим есть три чанка от пустой строки до 14 от 14 до 15 и от 15п до плюс бесконечности И клиент пишет там 1459 он попадает в средний чанк потом 152 15503 1504 1505 и все вот эти уже последующие запросы попадают в последний чанк в какой-то конкретный шарт и если нагрузка от клиента достаточно большая тут разные запросы могут иметь разный вес но порядок это там сотни rps или тысячи rps от клиента могут Ну заставить шар деградировать начнут страдать другие пользователи нану расти тайминги и так далее Вот пример графика нагрузки от клиента с похожим паттерном Это не совсем такой паттерн это но похожий когда клиент ходит по алфавиту то есть Видимо он там полистил все свои файлики и что-то он с ними делает подряд разными цветами на графике изображены в разные шарды то есть видно что в каждый момент времени вся нагрузка приходится в какой-то один шарт потом что-то меняется вся нагрузка начинает идти в другой шарт потом в третий потом снова в первый и это плохо потому что в этом случае мы не ну не можем размазать нагрузку от этого пользователя чтобы решить эту задачу нам на помощь приходят ханки Катин изображён большой и сильный гусь который прёт вперёд благодаря Ханка Что такое ханки если чанк - это диапазон имён ключей Бакета то Ханк - это диапазон хэшей имён ключей Бакета если лексикографически расположить не пространство имён а пространство хэшей и разбить эти хэши на кусочки то вот эти кусочки это будут ханки - это диапазон хэшей имён ключей это сокращение от ш Chun И вот теперь уже эти ханки Мы также можем хранить на разных шарда теперь тот же самый паттерн от клиента который пишет всё время последовательно если ключи 1504 1505 1506 шли всё врем подряд попадали в один чанк то если взять хэши от этих ключей то хэши попадают совершенно различные ханки и значит совершенно Рандомные шарды и происходит автоматическое равномерное распределение нагрузки вот наш Great Success а график того самого нагрузки от того самого Бакета и виден момент когда мы перевезли Бакет в ханке нагрузка пошла равномерно сразу на несколько шардов и абсолютное значение этой нагрузки снизилось в несколько раз на каждый отдельный конкретный шарт при переходе на модель хаков происходят некоторые изменения например в запросах листингов листинги - это ну клиентский запрос на список ключей если в модели по чанкам на запрос листинга мы могли ходить только на часть шардов Бакета А на оставшуюся часть шардов не ходить если Мы понимали что на вот этих оставшихся шарда Нет не может быть ключей удовлетворяющих параметрам запроса то в случае с модели с Ханка нам нужно всегда ходить на все шарды собирать листинг со всех шардов но так как в модели с Ханка бакеты умещаются на меньшем количестве шардов благодаря равномерному размазывания ещё некоторые изменения происходят в работе мура в модели с чанка работает более точечно перевозит какие-то конкретные чанки разводит какието конкретные шарды А в модели с нка нагрузка от Бакета на каком-то рде превышает какой-то порог то р просто удваивает количество шардов на котором присутствует Бакет то есть допустим Бакет лежал на четырёх шарда размазывает его по восьми шардам таким образом нагрузка автоматически размазывается в два раза больше шардов на каждом отдельном рде в два раза меньше В итоге ханки стали для нас решением всех проблем с балансировкой они дают нам автоматическое равномерное размазывая занятостью и загруженностью шардов и свободно можем продолжать делать шардинг то есть накидывать новых шардов в инсталляцию без всякого тайма я катаю гусей уже много лет если вам понравился доклад голос за него и Если у вас есть вопросы я буду рад на них ответить Спасибо Паш Спасибо Москва руки в небо Спасибо за доклад очень было интересно Павел такой вопрос по поводу сплиттера а представим У нас есть чанк который нужно разбить Ну ре да сплите и получается у него пусть будет 140.000 140 единиц Да ты говорил там какое-то количество единиц Ну пусть 140 единиц каких-то помоему тысяч да И вот делит допустим пополам и остаётся два чанка по 70 каждый Да дальше в эти чанке идёт запись или они такие как бы не полненькие и лежат Ну это зависит от паттерна клиента если он по данно данным путям продолжает писать нагрузку они могут продолжать увеличиваться допустим снова до 100000 дойти их снова поделит сплиттер на два более маленьких всё вопрос Да потому что я думал а что с ними будет спасибо спасибо тут мир ламоды на груди поэтому можно догадаться где где человек работает да буд добр и Скажи кто ты и что ты делаешь а потом Вопрос Раз раз Игорь или рорн Спасибо было очень интересно всегда было интересно как работает Яндек S3 изнутри у меня вопрос такой из двух частей но а касается использования баз данных Аа значит первая часть вопроса касается того почему было решено а делать собственное решение шардирование поверх постгрес не использовать какое-то готовое например тот же Яндекс db да Или yab db я знаю что там довольно похожие алгоритмы шардирование реализованы внутри А и разбивки на шарды в том числе их перемещения и так далее и втра чись вопросы Я не совсем понял как именно работают счётчики но по описанию прямо очень было похоже на меж 3 кликхаус вот поэтому вопрос как бы что там используется и если это не кликхаус то почему Окей Ну про вопрос насчёт выбора как бы хранилища и способа шардирование Ну самый как бы общий ответ на это это то что так исторически сложилось и там в том числе у нас была команда которая в тот момент имела опыт в похожей схеме шардирование с погром но как бы к слову в сейчас в некоторых инсталляция S3 Мы также используем wdb Ну там в качестве хранилища Вот и насчёт вопроса про счётчики Ну счётчики должны быть Ну по крайней мере в нашей схеме консистентные с текущим положением вещей Сколько сейчас реально лежит в базе там объектов столько и должен показывать счётчик счётчик Ну вместе с учётом очереди счётчиков они консистентные если добавился объект то добавился запись в очередь и либо если уже пришёл после этого процесс схлопывание то значит он хлопнул и обновил информацию в счётчике в каждый момент времени вот эта информация консистентная как бы если в если бы мы хранили счётчики где-то отдельно в другой системе Ну я сходу затрудняюсь представить как это всё должно было бы быть и какие бы какая была бы разница и какие последствия но по крайней мере там скорее всего были бы вот эти моменты с тем что они не не были бы консистентные если бы они хранились в другой системе Вот надеюсь что ответил на вопрос да спасибо спасибо Вот Там сверху буде сабры Павел спасибо Меня зовут Александр Руденко Я работаю в Крок клауде Мы тоже делаем S3 Вот и у меня вопрос такой Вы делитесь очень крутыми цифрами Как как много псов Какие гигантские сру буты проса сыт ваш Стри и у меня вопрос такой как сплиттер успевает за супер дико быстрыми клиентами которые заливают сотни тысяч объектов там в минуту Как вы успеваете спли ить эти шарды когда поток объектов огромный и ещё баке тов у вас тысячи и миллионы возможно Как вы успеваете вот по ним по всем следить что у вас есть жирные шарды О'кей Но на самом деле клиенту достаточно сложно перегнать сплиттер клиенту надо по сети Ну делать множество запросов если он ещё и данные льёт то много там данных лить а сплиттер нужно отработать локально на одном шар порса и просто пересчитать объекты Простите я уточню вопрос Меня интересует именно технический момент как вы отслеживается момент что пора шарт слить То есть это что это перебор Батов и просмотр статистики что скорее всего сейчас хорошо чуть-чуть понял кажется вопрос сплите не шарды сплите отдельные чанки Да просто отдельный фоновый процесс пробега периодически по счётчикам видит Какие счётчики превысили лимит всё идёт этот чанк и Сплит Угу я понял то есть перебором не в момент когда запрос приходит какой-то Да Да нет не в момент понял ещё один вопросик если позволите э Amazon э про родители ри у него есть очень много информации про то как шардирование э Как раскладывать объекты по шардам правильно чтобы добиться высокой производительности вот ээ вот ваш подход со сменой раскладывания э с чанков на ханке Да да да вот он даже по вашему графику видно что вместо одной высокой кулины Да мы видим очень много маленьких То есть вы для того чтобы ответить на один большой листинг объектов да Вы теперь должны поднять намного намного больше шардов что соответственно вовлекает значительно больше дисков это с одной стороны более производительное более масштабируемые решение Но с другой стороны это сильно повышает одного запроса заметили ли вы падение тенси После перехода вот на такую модель Точнее не не падение а увеличение так вот я запомнил один вопрос но возможно их было больше слева мы видим пику но мы видим пику на одном как бы диске грубо говоря да на одном шарден очень много пик они меньше Но они вовлекают в себя огромное количество шардов это по идее сильно увеличивает время ответа на листинги Давайте отвечу операция листинга - это действительно более дорогая операция чем там того же просто чтение одиночного объекта так как надо сходить на несколько шардов Она более дорогая Для нас и ну длина тайминги этого запроса равняются таймингу походы в самый ну не знаю медленный шарт какой шарт то есть запросы листинга во все шарды идут параллельно и просто запрос завершится тогда когда Ну самый медленный из шардов ответит и в этом смысле тайминги не отличаются от таймингов в модели с чанка только тем что шардов может быть стало больше для сбора листинга и там по статистике больше чаще попадаются более медленные шарды но ну ну и в общем в итоге мы на текущий момент не заметили деградации таймингов листингов спасибо Вот справа молодой человек Добрый день такой вопрос а как вы боретесь коллизия в Ханка и как выбиралась хэш-функция для них поясните пожалуйста что значит коллизия Ну когда допустим два ключа Ну два ва путя получается к два ключа К объектам имеют одинаковый шкод Окей но они Ну если они прямо совсем одинаковые имеют ход они будут иметь они всегда будут лежать в одном ханке и их не разделить но в плане работы нашего AP это ну не представляет Ну никаких дополнительных сложностей потому чтото оно идт за кон немом и ну нейм уже ризот однозначно который нужен а нет в этом смысле вопрос не по работе AP А какой метод вы используете для разрешения коллизий Ну он нам не нужен а то есть это не страшно если два объекта вот так совпало прямо имеют одинаковый хэш они будут лежать в одном ханке на одном рде но но и что Угу а О'кей а второй вопрос про требования к хэш-функции которая выдвигались к Ханка Были ли какие-то Ну как бы требования были такие что чтобы она была простой иди как бы она должна вычисляться и на стороне базы данных так она прямо там в некоторых местах имее Ну является частью SQL запроса и там на ране бэнда Ну и в общем там мы выбрали достаточно не знаю простую очевидную хэш функцию я сейчас входу затрудняюсь сказать насколько легально её огласить поэтому на всякий случай не буду если что в луаре можно смогу поделиться хорошо Спасибо Спасибо теперь смотри география добры Да Добрый день Меня зовут Андрей вопрос очеред счётчиков как я понимаю отдельно считает размер и количество файлов А запись файлов идёт отдельно Как происходит синхронизация и не было ли проблем что там выходили за квоты за счёт того что очень счётчиков тупила либо ну Да хороший вопрос такое может быть Ну то есть этот э лак того пока обновиться уже счётчики на всех бэнда например которые обсчитывают квоты этот лак есть и в целом клиент Может на какие-то там секунды до залить объектов выше своей квоты и только потом мы поймём что Ой он уже превысил и следующие запросы уже не пройдут в принципе это возможно и просто последующие запросы уже будут возвращать ему ошибку Ну то есть просто существует этот лаг Ну мы стараемся его минимизировать он короткий Спасибо Спасибо будьте добры вот да спасибо за доклад э Володя Меня зовут Я Тинькова Э не отношусь никак к S3 просто интересный вопрос у вас на слайде было сказано что вы укладываете Примерно там в пару секунд там 1 тирет да Вопрос Является ли это вашим прямо конкретном SL Если да то учитывается ли в этом SL фейлы То есть когда происходит фейл вы также укладываете в это время насколько насколько я помню вот у нас нету конкретного комита на SL по тайминга и но это не точно Но если он есть то вот эти тайминги муве как бы к нему не относятся они крайне редко происходят Ну по отношению к клиенту клиенту достаточно сложно попасть на тот момент Вот на этот момент и они скорее вот эти тайминги из общих соображений того сколько готов поджать клиент Ну по аналогии с какими-то другими возможными проблемами в сети или Ну вот и так далее Спасибо и жирную точку Вот с первого ряда поставим и пойдём в кулуары Привет я Женя из ВК у меня пара вопросов первый Как я понял у вас происходит сейчас секундочку переформулируйте запросы но существует такая вероятность что например запросы на чтение на один шарт э на одну запись будут настолько массированные что этот шарт не будет выдерживать вот от этой ситуации Как вы защищаете А И второй вопрос были упомянуты реплики базы данных а ну которая хранит Мета данные мне интересно реплицируемый так хорошо можете ещё раз первый первый вопрос повторить а вот сейчас когда тебе камеру сунули в лицо под стрессом Задай нормально да то же самое Да Короче э Вы шарди ете запросы там по хшм как-то ещё А ну там шарди ете данные а но предположим что э по одному конкретному там имени запросов больше чем может выдержать Один шар то есть в один ключ имейте в виду Да да О'кей нуно Хороший вопрос и от этой ситуации Ну сейчас и вообще ну в общем мы Не защищены Ну точнее не совсем так в данной схеме мы никак не разделим на разные шарды уже запросы прямо в один ключ они должны быть в том числе между собой консистентные Да вся нагрузка в один ключ будет всегда идти в какой-то один конкретный шарт есть у нас в этом месте некоторые Ну защита в виде лимитирование rps в один ключ она Ну я вот этот момент не совсем хорошо помню но то есть она по-моему где-то на стороне на уровне нса перед кэндо реализуется и ну и в итоге достаточно странный паттерн у клиента Который прям не знаю 1.000 rps льёт в один файл то есть зачем ему не знаю так много Зачем ему писать файл который он каждую миллисекунду перезаписывает или Ну в общем это достаточно странный паттерн и Да всегда эти запросы будут идти на один шарт и у нас есть некоторая защита в виде лимитирование rps в один ключ окей а второй вопрос напомнить Ну да про репликацию данных да да Ну это происходит на стороне МДС как хранилище данных и Да он реплицируемый внутренние инсталляции Яндекса инсталляции м ДСА Там есть ну возможность варьировать там либо это три копии либо две либо полторы но в общем это уже детали работы М ДСА спасибо спасибо а теперь махни пожалуйста рукой все кто задавали вопросы ты должен выбрать Кому мы подарим сувенир вспомнить кто из них О чём говорил Окей у меня там был подарок от Яндекса и ещё так понял есть подарок конференции Да я думаю тогда не знаю подарок от Яндекса человеку с запросом про листинги Где вы махни рукой вот Ага значит сейчас волонтёрская почта сейчас отнесёт это на второй этаж да и второй подарок вот последнему задающего вопрос там про репликацию и Да отлично тебе тоже памятный от конференции Спасибо большое за выступление Понятно Что гораздо легче разрабатывать чем чем выступать Вот Но это нормально"
}