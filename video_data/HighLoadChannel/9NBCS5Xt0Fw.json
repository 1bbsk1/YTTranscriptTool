{
  "video_id": "9NBCS5Xt0Fw",
  "channel": "HighLoadChannel",
  "title": "Система распределенного, масштабируемого и высоконадежного хранения данных / К.Коротаев",
  "views": 431,
  "duration": 2843,
  "published": "2017-04-25T08:02:58-07:00",
  "text": "кирилл коротаев кандидат физико-математических наук руководитель отдела advance and research компании parallels закончил мвт по специальности компьютер сайнс специализируется на виртуализации системах хранения данных производительности систем и безопасности мой доклад называется система распределенного масштабируема во высоконадежного и так далее разные свойства хранение данных для виртуальных машин и не только значит начинает свой доклад с предыстории дело в том что для того чтобы понять что мы сделали и зачем нужно немножко рассказать о том что делает наши компании какие продукты чтобы вы понимали потому что система хранения данных на сегодняшний день безумно много и в принципе изобретать еще одну свою собственную может показаться не но не очень разумным значит предыстория такова наша компания занимается виртуализации но все вы наверное раз в жизни использованы виртуальной машины в vmware квн где она так далее или слышали о них по сути виртуализация позволяют вам взять несколько физических серверов и на них запустить гораздо больше чем это количество серверов виртуальных серверов при этом виртуализация позволяет делать такие трюки как миграцию быструю виртуальных машин между физическими has to me кроме того виртуализация например в случае с жертв торджи сын сторожем позволяет делать хавела берите тысячи если какая-то машина физическая выпадает вы можете быстро всею виртуальной машины поднять на других либо нас породах ты свободных либо как-то перераспределить вот в чем проблема значит проблема состоит в том что большинство наших костров хостинг-провайдеры которые предоставляют услуги веб-хостинга используют вокальные стороны что есть как правило вот такая стойка в ней есть n машин и у каждой из них есть какие-то локальные диски они объединены в рейд по сути на самом деле hail ability большинство хостинг-провайдеров не предоставляют например если сломался secu они выключить сервер заменить железо воткнуть обратно там возможно запустить fsck и так далее соответственно простой может быть очень большой лучшем случае несколько минут в худшем часы в принципе существует решение sense to reach существует она давно надежно такие железки которые объединяют множество дисков в один виртуальный пул такие железки предоставляют всякие позитивные свойства 2 тораджи хави ability они как правило вылечивают данные сами если какой-либо из дисков умер но они дорогие со стороны бывают гораздо больше сложнее с еще более всякими навороченными свойствами и даже целые огромные комнаты с дисками в принципе как я сказал since the rich и нацелен на то чтобы хранить данные избыточно сама излечиваться в случае выхода железо из строя естественно там есть надежные механизмы быстрые каналы для передачи данных типа файбер channel есть средства для мониторинга и нахождение узких мест но к сожалению наши клиенты любят экономить сын сторож стоит как правило от 100 тысяч долларов и больше ну часто до миллиона локальный диск стоит как все мы знаем меньше сотни поэтому хочется найти решение которое бы в принципе обладала всеми этими замечательными свойствами которые перечислил и при этом не заставляла компании инвестировать в другую инфраструктуру ну понятно хочется бесплатно сыграть халат ну попробуем все-таки что не придумать значит мы начали думать какой стоить нам нужен если мы его будем делать сами то что же мы хотим сделать идея такая что нужно все по максимуму простить нужно отталкиваться от нашей цели и задачи цель и задачи как я сказал у нас исполнять виртуальные машины что же это означает на практике давайте подумаем дело в том что виртуальной машины довольно требовательная виртуальная машина внутри имеет файловую систему как мы все знаю файловые системы они особенные они требуют так называемый strong консистенции вообще консистенции или согласованность данных это но если взять определение то это хорошо заранее установленный и понятный предсказуемый порядок в котором выполняются операции например стран консистенции это такой порядок когда после того как писатель и я записал данные любой из читатели независимых будет видеть эти данные сразу не когда-нибудь системы частью ли к сожалению пользуются этими свойствами и требует их и база данных почему так происходит но представьте себе что файл схема состоит из двух частей журнал и метаданные с данными нарисованы у меня здесь предположим вы переименуйте файлы или что-то или создаете файл вначале в запись и помещается в журнал по своей команды flash на диск эти данные попадают журнал после этого синхронно через какое-то время эти данные попадают в то место где они реально должны быть когда вы делаете следующее изменение через какое-то время место в журнале может быть перри использована под новую запись ну и потом она тоже попадет на свое правильное место зачем так делает файловая система за тем чтобы в случае выключения питания вы могли проиграть журнал и доложить все вот эти изменения которые мы делали которые мы реально записали на диск в их правильные места при этом журнале мы можем точно определить место до которого нам нужно проиграть эти изменения если в этой точке что-то дальше идет из прошлого значит нам нужно остановиться и ровно 2 точки мы заметили что-то журнал все просто знаешь какие еще бывают виды консистенции вот вид консистенции которая требует система называется strong консистенции ли стрип консистенции еще бывает sea queen show консистенции когда все читатели видят изменения в одном и том же порядке бывает вид консистенции и венчал консистенция я немножко неправильно рисовал на самом деле венчал консистенции является частным случаем вик консистенции а правильно да она более строгая значит with consistent цвета случай когда вы меняете какие-либо данные и читатели могут не увидеть сразу этих изменений даже если вы сказали что вы записали это ведь консистенция существует так называемая но то есть период времени когда читатели могут видеть старые данные по консистенции чуть более строгое определение которое говорит гласит о том что рано или поздно через какое то время которое нигде ни кем не определено при отсутствии постоянного изменения объекта или данных и при отсутствии каких-либо проблем с железом то есть когда отсутствует процедуры самовосстановления и прочие чудеса вы увидите свои записи рано или поздно значит если кто был на докладе евгения полякова что делает со своим первым миллиардам то он пытался убедить его слушатели о том что я буду рассказывать неправильная вещь на самом деле день чего консистенции ничего вам не гарантирует нет никакого периода времени гарантированного через которое вы увидите свои обновления и даже если вы раз в сутки ранить и какой-то демон который что это синхронизирует это не значит что гарантия раз в сутки и ну или максимум сутки потому что демон можно точно также умереть может не до делать свои дела ну и плюс если вы внимательно слушали его ничего консистенции гласит о том что вы видите свои изменения только если вы файл не постоянно изменяете то есть только в этом случае данные смогут за про похититься и в конечном итоге везде будет одна и та же копия одна и та же версия случае если вы давно и постоянно изменяете вы будете везде часто получать разные во времени состояния понятно что день чего консистенции и похожее с ней консистенции не годятся для файловых систем поэтому сразу нам пришлось отнести различные хранилища данных такие как наподобие amazon s3 это свифт и прочие другие и пришлось думать дальше дальше мы подумали что же мы хотим получить мы хотим получить такую фичу под названием grow on demand который означает что виртуальной машины должны в принципе расти сколь угодно в размере и но если есть вообще в кластере свободное место то мы должны иметь возможность выделить потому что изначально юзер как правило цирк не очень много места но со временем ему нужно все больше и больше дискового пространства по сути это означает что мы должны иметь возможность сова целовать объект не на одном локальном диске а разрезать его потому что на одном локальном диске может не хватать места например у нас есть 10 терабайт ных дисков на везде есть по 100 гигабайт а мы хотим выделить терабайт извините хочется иметь такую возможность значит как я сказал это сразу приводит к мысли что мы должны пилить наши данные на части секунду что-то не работает значит как только мы расписываем наши данные на куски начинаем их распределять по и независимым компьютером тут же мы увеличиваем резко вероятность потери данных потому что все наши куски они зависимы с этим надо как-то бороться и очевидно что такая система хранения данных должна обладать redundancy реплицировать свои данные и храните фактор распределена но есть и плюсы в таком подходе плюс состоит в том что восстановление данных может делаться одновременно с кучей дисков и таким образом время простоя может быть очень маленьким очень и время простое время восстановления данных например 1 терабайт можно остановиться 10 минут что в традиции на рейде практически невозможно обычно там часы или даже сутки есть еще один плюс от этого дело в том что вероятность потери данных обратно пропорциональна квадрату времени восстановления то есть чем быстрее наша система умеет самого излечиваться тем мизерная вероятность потери хоть каких-либо данных поэтому решили что мы будем пилить все наши имидже виртуальных машин на куски фиксированного размера и будем их распределять в сети реплицировать но собственно это была основная идея кроме того мы планировали еще учесть топологию кластер и прочие всякие тонкости значит что-то тут с пультом значит какие у нас еще были идеи по упрощению системы понятно что мы не хотели делать posix ную файлы из тему и нам в принципе не нужно все что мы хотим это хранить большие имиджа виртуальных машин соответственно нужно оптимизировать столько под большие объекты и мы можем предполагать что метаданные изменяются редко это тоже очень важное свойство потому что создать контекстную файлы систему хорошую при этом чтобы она была оптимизировать под метаданные практически невозможно ну чтобы она и между по скорости было как локальный диск еще одно важное допущение которое мы имели в голове то что скучать спаси ситуации почти ситуации которые почему-то очень многие разработчики не рассматриваю то что могут умирать не только отдельные сервера но и целиком дата-центра то есть часто например делают предположение что ну хорошо в стойке может пропасть питание ну ладно но в целом это центре не может и никоим образом даже не тестируются autre ne закладываются в дизайне на это значит что такое идеями подошли к то по дизайну начали думать как же нам сделать такую систему хранения данных в чем проблема вот я сказал что нам нужно делать чтобы наше хранилище обладала strong консистенции свойствами почему это не так просто но представьте себе что у вас есть объект черненьким нарисован он хранится на 3 серверах в 3 копиях 3 репликах есть вот версия один объект а потом клиент меняет содержимое объекта какую-то его часть но сервер номер три к сожалению был какой-то причине доступен и не обновил данные таким образом у нас на сервере 12 вторая версия после этого мы испытываем полное выключение питания во всем дата-центре и так происходит что сервер номер три у которого старая версия файла точнее объекта возвращается первым на самом первом разгружается естественно сам по себе он не знает о том что у него старая версия данных ли and который к нему коллекция тоже не знает именно в этом плюсы even чего консистенции вы можете вернуть клиенту старые данные а потом когда-то до синхронизовать мы так не можем сделать потому что вы система не ожидает что после изменения файловой системы вдруг можно увидеть старые данные ну соответственно потом возвращают стальные сервера все становится еще хуже как только мы начинаем пилить наши файлы и системы имидже на куски также у нас есть объект иметь файлы системы который разделен на 2 части черную и желтенькую значит изначально все хранится в трех копиях версия 1 дальше мы меняем первую часть объекта версия 2 и вторую часть объекта но опять так как в предыдущем сценарии по одному серверу были недоступны по какой-либо причине после этого мы теряем питание в дата-центре и возвращаемся с каким-то набором серверов они нибудь отца если внимательно подумать то мы заканчиваем жизнь вот с двумя комбинациями который вообще не существовали в природе плохо система уж точно развалятся значит нужно что то делать опять же из этих картинок следует что нужно каким-то образом 35 версии объектов то есть кусочки кусочков наших чернаков почему это не так просто каким образом можно трогать версии объектов ну есть несколько вариантов например трансакционного но это работает медленно потому что если вы хотите транзакционный изменить содержимое объекта его версию который где-то сбоку лежит то нужно либо базу данных либо транзакционные файловую систему типа btrfs а либо еще какие то делать ощущения в любом случае записи будут гораздо медленнее ти поэтому мы придумали другой вариант мы решили что мы будем вести версии но оплатить их будем ни тогда когда объект меняется тогда когда один из серверов не смог изменить объект клиент знает об этом если он не смог например на один из трех серверов записать он знает об этом и может сказать кроме того что реши картинок следует что сами сервера в принципе не могут сказать об туда и туник данный ринит потому что они не знают что на других серверах соответственно версии на них хранить тоже немножко получается полу бессмысленно вот таким образом мы решили что нужно иметь некий методы это сервер который бы trocal версии объектов и вот подошли к простому дизайн в виде трех компонент клиенты которые общаются с кластером по обычной сети ethernet и today the server и чанг сиротенко сэр это сервера который собственно хранятся my чанки или данные они умеют делать или драй там какие-то простейшие операции типа создать чанг и методы это сервер который собственно знают как разбитый файлы где они лежат и какие версии где актуальные стоит и этих объектов понятно что этот это сервер в нашем дизайне является single point of war с этим нужно тоже что то делать нужно как-то обеспечивать халявы выберите этого сервиса на эту тему мы начали думать думали думали думали базу данных использовать не хотелось у него большая вакансия мы были нацелены на watenshi там не знаю меньше существенно меньше допустим 100 микросекунд и при этом хотелось чтобы этот это сервер мог обрабатывать 100 тысяч запросов в секунду ну то есть на вырост решение было поэтому от баз данных мы сразу отказались и придумали решение точнее если быть совсем уж честными частично придумали частично подглядели в статье про гугл fs решение очень простое есть состояние в памяти полное состояние которое хранится у методы это сервера когда он естественно исполняется мы используем примерно 100 байт для описания каждого чан к таким образом 64 мегабайта описывают тридцать два терабайта данных и методу этот сервер записывает все изменения в журнал на вокальной файлы с теми понятно что журнал может расти бесконечно нужно с этим тоже что-то делать маленькая проблема как мы эту проблему решили но значит нас есть состояние в памяти мы пишем вокальный журнал в момент когда он достиг какого-то размеру например один гигабайт мы создаем новый журнал новую дельту куда начинаем писать все новые изменения и делаем параллельно записываем асинхронно snapshot состоянии памяти момент когда snapshot записан можно удалить предыдущий журнал и теперь snapshot + фото дельта журнал описывают новое состояние ну и когда журнал новый опять разрастается до какого-то размера процедуру можно повторить если кто был на презентации где рассказывать про вывел д.б. это примерно то же самое на самом деле там точно также есть таблицы отсортированные ки вылью периодически сервер сбрасывает snapshot состояние на диск ровно та же идея вот на эту процедуру естественно можно там сколько бесконечно повторять как мы добились тайлер и берите нашего метода это сервиса очень просто кто значит такой алгоритм paxus поднимите просто руки не так много удивительно вообще говоря pax его гаретом это классический алгоритм очень советую прочитать эту статью она написана неким таким метафорическим языком называется part-time парламент и и написал очень известный в компьютер союз человек если вы импорт автор безумного количества статей статья это уникальна тем что она описывает метафору греческий остров paxus у которого есть ну понятно как у любого государства у них есть депутаты грубо говоря и они должны большинством голосов принимать законы проблема в том что депутаты у них совмещают помимо своих депутатских обязанностей еще и скажем так коммерческие обязанности у них у всех есть там какие-то дела и они постоянно отсутствуют на острове ездят вот но депутаты могут посылать гонцов сообщениями обратно на остров там ну типа я заявили о против вот и одновременно ни в какой момент времени в парламенте они присутствуют большинство тем не менее закону нужно принимать согласовано большинством и все должны понимать какое состояние законов вот по сути pax из алгоритма описывает ровно эту процедуру то же самое с серверами мы запускаем несколько инстансов методы это сервера они все согласовано принимают решения комитетов журнал в случае если кто-то умер выбирается новый мастер он с актуальным состоянием принимает на себя роль начинает обслуживать кластер в течение нескольких секунд это происходит естественно прозрачным образом для клиента ну перейдем немножко уже к результатам того что мы получили значит немножко pro performance по производительность я не знаю большинство знают или нет если сталкивались но для нас вот было немножко с сюрпризом что ли да что полу гигабит у который дает примерно 100 мегабайт в секунду можно получить нас скорость записи 100 мегабайт в секунду то есть фактически максимальную гигабит ную скорость при том что мы записываем данные в 3 репликах на 3 сервера как это происходит вот я нарисовал здесь происходит это через так называемый chain трейд то есть клиент слева посылает райт request на 1 thinkserver тот передает по цепочке второму второй передает третьим и обратно распространяется и колледже значит почему при этом получается максимальная скорость потому что ethernet full duplex и в момент когда мы получили первые 64 килобайта от меня нарисована красными кубиками мы тут же начинаем писать на первом цыси их на диск и пересылать и синхронно 2 серы по цепочке таким образом если взять request в 200 56 килобайт распилить его наш 3 4 килобайта на кусочки то можно как бы нарисовать вот такую картинку увидеть что во времени они практически одновременно записываются на всех трех серверах вот получается довольно интересно и неожиданно потому что решение в лоб когда клиент рассылает нужное количество копий напрямую на сервера но очевидно в три раза медленнее еще кое-что про performance мы поставили себе цель сделать так чтобы наша система была скажем так использовать по максимуму кэширование дело в том что виртуальные машины в отличие от веб приложений и систем который использует я ничего консистенции они как правило часто чувствительные клад ansi задержка поэтому мы решили что на клиентских машинах мы хотим сделать ssd кэширования ну понятно скорость рандомного доступа увеличивается в десятки раз ничего такого но как это сделать на самом деле оказывается что это не так тривиально потому что кэшировать все что попало все что читает клиент это немножко глупо вы будете вымывать постоянно нужные данные кроме того ssd-диски пишут гораздо медленнее чем вы можете читать со всех этих человек серверов поэтому мы решили что кэшировать мы будем на втором ready то есть если какой-то блок с данными читается первый раз нас это не волнует это означает что ну он редко читается там в раме посидит в рамках какое то время а вот и со второй раз считается то скорее всего это означает либо что данные пересчитываются с директом либо что не хватает рамках а на ноги и данные реально пересчитываются поэтому их запиши роль кроме того то чего не стоит кэшировать на ssd-диски это секвенсер доступ если кто-то послед начитает большие объемы данных кэшировать их на ssd пол бессмысленно потому что на чтение ssd ну допустим в два раза быстрее чем вращающийся диск но не в десятки раз кроме того backup всякие программы любят вычитывать все подряд поэтому такие паттерн и мы тоже избегаем как зовут и аптеке что данные читаются второй раз или последовательно мы сделали очень простую вещь вместо того чтобы тратить все request и ну нужно как-то ограничить по памяти мы взяли сделали 8 в хэш-таблицу большого размера который описывает там допустим историю последних 100 гигабайт access of и это таблица служит сразу двумя двум целям во-первых она lookup таблица мы всегда можем быстро найти если какой-то блок есть в каше или окрасился в прошлом во-вторых он одновременно служит как и веру таблица дело в том что вот эти восемь элементов если файл точнее если блок пересчитывается то элемент перед омские цел начало массива ну восемь элементов это как список грубо говоря вот получается очень просто кроме того мы разбили кэш на три части ну вот фильтр вот этот который описывает access и в прошлом историю сам кэш и еще мы ввели такое понятие как будто ешь дело в том что очень часто виртуальная машина на старте ну или вообще там не только виртуальная машина читают одно и тоже про его любое приложение когда взлетает она читает какой-то набор при define данных вот эти данные мы пишем на самом после способ открытия файла сразу 200 мегабайт в течение первых двух минут это резко ускоряет старт виртуальных машин например после выключения сервера при этом в сеть сервис оптически не ходит что тоже большой плюс легче кстати ssd cache в нашем случае призван еще и сократить сетевой трафик кроме того секунду access доступ мы помечаем еще и таймс темпами для того чтобы избежать таких неприятных ситуаций когда был один большой файл он секунд шоу читался или писался потом файл ударили на его место положили много маленьких файлов мы хотим чтобы рано или поздно если они действительно трогаются чтобы они записывались по этому еще и смотрим на таймс темпы ну и в нашем каши райт просто инвалиды эти блок за кошерными данными есть еще одна тонкость если вы когда-либо в своей жизни будете реализовывать ssd кэширование нужно учитывать что часто возникает такая неприятная ситуация как тир лифт блоки например вот часть блоков за кашира вася часть нет если потом читаются все подряд то не в коем случае их нельзя пилить и читать через 1с с d1 ссср отличного диска дело в том что 1000 диски на таком паттерне общем-то умирают и дают ужасно плохую скорость они не считают это секунд шел доступ ну естественно там скорость падает до в общем случае достается в секунду еще одна тонкость которую мы уяснили пределами ssd cache а дело в том что ssd неожиданно ведет себя с чтение мирой томи одновременно если есть запись его ssd-диск to ride и начинают резко пакете соответственно когда вы делаете такой кэш нужно понимать что если с него много читают писать туда оптину ни в коем случае нельзя иначе он станет медленно еще одно использование ssd который мы нашли это создал журналирование на чанг серверах наш танк сервера в принципе изначально мы затачивали под вращающиеся диски но и создать диски позволили нам малой кровью с небольшим там объемом данных скажем 10 гигабайт реализовать несколько интересных фич такие как например чикса минг данных мы чек самим все страницы с данными проверяем контент на чтение то есть на доступе к этим данным и кроме того делаем периодически такую процедуру под названием стробинг то есть мы учитываем все данные там с какой-то скоростью и проверяем что они все еще читабельны их контент правильный таким образом надежность хранения возрастает ну и как я уже упомянул причина по которой мы активно внедряем ssd в наш сторож и хотим его там видеть это то что все обычные work лады все программы они чувствительны к задержка если у вас есть какая-то программа которая читает сто файлов маленьких например конфиг файл и то понимаете да watenshi умножается на 100 сразу же вот что мы получили на выходе мы получили систему хранения больших файлов или имиджей которая скейлится до петабайт там можно хранить виртуальной машины исполнять их оттуда контейнеры можно использовать для лобзик стороны расскажу ниже как мы планируем сделать для шарит хостинга для других нужд может быть кому то нужно для файлового архива при этом мы получили довольно хорошие цифры и обсказал близкие к идеалу по производительности поверх обычную ethernet сети то есть скорость сравнима с сенсорами мы получили 13 тысяч а псов на маленьком классе через 14 машин по 4 дисков каждый это очень скромно и железо обычно кладут гораздо больше дисков получили шестьсот тысяч и операций в секунду с использованием ssd-дисков что вообще говоря покрывают очень такие серьезные запросы там например люди из мам вы рассказывали если кто видел про 5000k квестов в секунду понятно что с такой системы хранения данных можно легко такую нагрузку похудеть и при этом 1 терабайт то есть потеря одного диска систему лечатся за 10 минут что достаточно быстро напоследок я бы хотел рассказать про некоторые эксперименты уроки которые мы вынесли которые нам на наш взгляд были бы полезны всем тем кто только там может быть первый раз в жизни пытается создать какую-то систему хранения данных или просто распределенную систему во-первых мы при дизайне нашего нашей нашего старриджа использовали асинхронный не блокирующий дизайн то есть у нас есть по сути винт лук если кто знает так устроены еще две программы довольно известные пингвин x и через прокси есть у нас есть event log который обрабатывает этих ровно через ipo все входящие и исходящие пакеты все делается через асинхронное его если у нас есть какие-то потенциально был чуть-чуть операции например создание файла который может стать заботиться или удаление файла вносим в другой сыр от как джуббу что нам это дало во первых это дает фантастическую скорость дело в том что в такой модели программирования не требуется никакой синхронизации и всегда горячее secu каши и реально можно такой сервис может отвечать на там сотни тысяч request of секунду например очень прокси на 1 кори способен просматривать трафик на скорости 10 гигабит в секунду еще что мы вынесли для себя то что июня тесты из раз тестирование естественно обязательно новые тесты это я подразумеваю что есть какие-то разные под системы вашем продукте их нужно независимо тестировать например в нашем случае это был показ это распределенный журнал это там разные системы логин га то все пятое-десятое все это оказалось очень удобно когда вы собираете систему из кубиков вы уже знаете что она работает как эмулировать пропадании питания наверное это самое ценное что мы для себя придумали дело в том что если вы будете постоянно выключать питание по-настоящему так тоже делаем ночи говорят а медленно хочется иметь возможность как-то тестировать быстрее как это сделать оказывается это не так не так сложно достаточно послать сервису сигнал стоп он залипнет риска это время вы можете сделать секунд либо убить если вы делаете секунд то это вы ему ли рунете пропажу сети на время если делаете сиквел то регулируете выключение питания почему реально вы эмулируется эти две ситуации дело в том что если вы просто берете процесс то сервер сервер на котором в радиусе от процесс hydra он узнает что программа умерла и начнет отвечать на сетевые пакеты с помощью рис т.п. сигнал рецепт и с той стороны это тут же увидят и клиент увидит что сервер умер и соединение закрыто мы этого не хотим когда выключайте питание как правило клиент ничего не понимает он не видит никаких пакетов он их отправляют в никуда и ответов не видит так вот секунд six топом ровно этой ситуации эмулирует вы замораживаете процесс ядро знает что socket все еще открыт там какие то есть какая-то очередь она да какой то меры принимает пакеты но приложение на том конце не понимает и не может отличить эту ситуацию от выключения питания это очень удобно еще одна вещь которую мы который мы были удивлены скажем так я знал про это в прошлом веке для десять назад это было нормально то что еда и диски не делают флаш данных когда вы посылаете команду к сожалению это правда и по сей день многие диски рейды и даже ssd диски не пишут ваши данные когда вы спросите например очень много создать дисков тип по оси z или intex 25 которая для ноутбуков для desktop систем они ничего не пишет на самом деле у них огромные пиши когда вы посылаете фаш они вам говорят что да мы сделали все но по факту если вы выключите питание ваши файлы система будет разрушена а данные не будут записаны ну понятно зависит от размера крыша поэтому для любой системы хранения данных нужно тестировать ваше железо мы были этому факту удивлен и еще один факт то что создать скорость записи на ssd диск зависит от данных оказывается практически все современные создать диски внутри реализуют компрессию поэтому если вы будете тестировать скорость записи на ssd диск через dd например сделав зиру записью нулей то увидите фантастический скорости типа 500 мегабайт в секунду здорово вы обрадуетесь а по факту вы получите 100 мегабайт в секунду то есть пять раз хуже это очень печально еще одна вещь которую мы узнали для себя открыли это то что чиксу минг ничего не стоит дело в том что современных in the sea пью есть инструкция которая позволяет быстро вычислить север cicek сумму со скоростью 4 гигабайт в секунду то есть со скоростью доступа к памяти соответственно наш совет если вы делаете какую-либо систему более менее надежно чиксу идти все что только можно скорость от этого не страдает а по факту вы будете обнаруживать плохой железа что мы и обнаружили несколько раз еще одна важная вещь если вы делаете какие-либо сервисы по хранению данных или по обработке таких запросов с данными нужно помнить о том что у вас есть очереди там где есть очереди их длины нужно контролировать если кто знает как устроен тисе пи пи пи есть алгоритм как джастин протокол который позволяет тисе пи как-то контролировать бендис который собственно дает applications нам пришлось реализовать то же самое потому что в противном случае клиент и один клиент мог бы выдать миллион рандомных запросов на террора и все остальные резко бы скажем так пострадали если культурно сказать кроме того есть еще один еще одна важная причина почему нужно контролировать длинный очередей дело в том что длины очередей связаны с размером памяти если у вас клиент может на генерить сколь угодно много запросов на чтение то это очевидно что с другой стороны эти данные могут накопиться на отправку таким образом вы можете спровоцировать ситуацию когда в класс тиви неконтролируемый рост объема памяти потребляемой все эти ситуации нужно уметь учитывать это было довольно неприятно ну соответственно нужно уметь ограничивать количество дескрипторов одновременно открытых connection of так далее тому подобное еще одна замечательная новость это то что в линуксе файлы есть два вызова для синхронизации данных сен косенков с они не работает не пользуйтесь ими более того они могут исполняться сколь угодно бесконечно долго ну если вы делаете надежности муха не данных то всегда выгодно использовать of data sync они фсин который синхронизирует также и метаданных file into the thing of порядка где-то в 4 раза быстрее вот еще одна интересная вещь то что если вы восстанавливаете в распределенной системе какое-то состояние то его всегда гораздо выгоднее восстанавливать парами очень часто люди как думают у меня все распределено я вот щас накидают джо бы всем серверам и они будут друг с друга какие-то данные высасывать это не работает это не точнее это работает но работает скажем там в разы а то и в 10 раз медленнее чем если вы аккуратный тщательно спланируйте процедуру восстановления потому что секунд доступ к данным он всегда гораздо лучше ну и напоследок пару вещей значит как мы сделали но это юзер экспириенс как сделать унификацию кластер а в распределенной системе мы решили что мы введем имя кластера то есть имя которой администратору будет удобно использовать нокс нам пришлось также еще ввести уникальный индикатор кастера для того чтобы защититься от различных ситуаций например когда администратор создал два кастера с одним именем мы от это возвращаемся то есть мы стараемся предотвратить все возможные human errors про призов венка нахождение сервировку осели в принципе уже было рассказано в одном из докладов что есть несколько вариантов например через явное задание списка серверов или через zircon в 3 через multicast мы кроме того реализовали еще и схему через dns как в гугле то есть можно в dns прописать какие сервера отвечают за кластер вот я хотел бы еще раз сказать две вещи по поводу системы хранения данных в этом году на конференции луизой было представлено два интересных доклада я просто хочу про них упомянуть потому что на мой взгляд они заслуживают реально того чтобы взглянуть на них первый доклад это microsoft research они построили систему подобную нашей они взяли порядка тысячи дисков по моему по 15 в каждом сервере построили кластер такой что все узлы со всеми могут полностью насытить 10 гигабит здесь гигабит гарантируется во все стороны в таком кластер его них это получилось не очень дорого всего 50 тысяч долларов но что интересно то что мой концов получил уникальные результаты во первых 2 гигабайта в секунду с каждого клиента одновременно это колоссальная скорость во вторых они установили мировой рекорд по сортировки данных есть такое соревнование обычно делают уникальные алгоритмы кластерные которые как-то там распределяю джо бы сортирую данные вот microsoft на обычном кластере обычными алгоритмами исключительно за счет такого 100 раджа отсортировали 1 и 4 терабайта за минуту она килобайтах записей это мировой рекорд почему я об этом говорю потому что то куда движется мир он движется от специфических алгоритмов заточенных под задачи периодически он движется в сторону более правильных решений то есть правильный сторож виртуальные машины виртуализации и обычный алгоритм и часто так просто так проще вот этот типичный пример когда так проще отсортировать данные поверх обычного ставра дженни используя специализированный алгоритма есть области в которых и невозможно создать или очень сложно затрат на создавать специализированные алгоритмы второй доклад я его не написал здесь но хочу упомянуть связи опять же с тем же евгением поляковым который сказал такую вещь что strong консистенции системы хуже скейлится чем увенчал консистенции значит это не совсем правда и у microsoft azure построена вся настроен консистенции системе и второй доклад который вот я хотел упомянуть создай google в этом году представил очень интересную систему ну там название какой то сложная но идея очень проста они представили реляционную базу данных гил распределенную consistent из киева было естественно нет по моему spider ну и название забыл вот я рассказываю то потому что на самом деле идея очень простая ровно тот же тоже та же самая ситуация гораздо правильнее иногда дать пользователям привычные им тузы со сторон consistency соски гелем чем заставлять их пользоваться более сложными какими-то этими интерфейсами и быстро осталось мало времени расскажу как мы хотим сделать облик сторож поверх нашей системы в ближайшем будущем у нас будет значит у нас есть имидже которые хранятся в нашем распределенном 100 ради мы туда можем напихать наши объекты прям поверх обычной файлы системы например x4 но идея очень простая мы просто берем и делаем что-то типа шарден га то есть мы просто копируем объекты по айди и распределяем по имиджу вот и все а имиджа можно смонтировать на каком-то конечном наборе машин соответственно предоставить к ним доступ в принципе такое решение скейлится достаточно больших объемов да вот десятка петабайт вот собственно и все если кто хочет попробовать нашу систему хранения данных у нас доступна бета ее можно скачать поставить там есть виртуальные машины контейнеры распределенный сторож может быть кому то будет интересно спасибо есть вопрос какой который это конечно когда печальная история так вопрос можно конечно вот тут звучали тесты системы там звучали гигабайты в секунду что такое есть данные о размере блоков и задержках при этом это намного интереснее было бы размеры блока чего какими данными тестировали и какие при этом мы тестируем разными данным начиная 4 килобайта wax ice age 3 4 килобайта wax с а мегабайт него и так далее show all и tense в оденсе как вы понимаете зависит только от того какое количество клиентов одновременно injected свои запросы сейчас у нас вот у нас есть совсем страз тестирование когда в 10 раз больше клиентов чем должно быть но на реальной системе дай watenshi вся в пределах там 2-3 секунд понятно спасибо ну соответственно расчет на то что в худшем случае от лица должна быть меньше секунды существенного хорошо еще добавлю что в случае со знака шурале младенцы естественно уменьшается тюрем спасибо у меня такой вопрос можно открыть слайд где вы показывали с последовательную запись на 3 сервера вот и скажите пожалуйста вот в те кто этот центр где у вас организована ваши сервером ваш кластер насколько я понимаю что у вас в данной архитектуре сервера должны быть соединены вот именно сервера на который идет запись они должны быть соединены конкретно друг с другом нет обычные за рациям если у вас 1 1 connection идет на каждый сервер то как поживаем скажите пожалуйста убиваетесь даже походную axe 3 на полной скорости соединения у вас смотрите вот здесь вам нужно гигабит здесь гигабит тыс вот на этом сервере получается full duplex к нему приходят гигабит и ухает ягодиц и на это точно также приходит и уходит одновременно с каждого сервера вот и если смотреть по вашему график где-то в середине графика у вас одновременно сервер месту и одновременно тоже сервер против нет в чем в 2 в одну на сервер приходит и уходит они по цепочке передают данные клиента тут клиентах генерирует он пишет же должен получить подтверждение подтверждение это маленький пакетик данные за вот этот и еще один таком вас при восстановлении данных со скоростью 10 терабайт сколько без него но эта цифра зависит от размера пакета дисков тем бы с ярко не может какая скорость соединения с машинами - полностью оптимизируем надеемся на то что худший сценарий на своем на сегодняшний день и он основной это 1 гигабит на 10 гигабит их мы тоже тестируемся но это не наш основной пока сценарий на дело в том что 10 га битной сети есть скажем так наверное меньшему 10 процентов хостинг-провайдеров если продакшен решением же размещать такого системе реально а сколько можно рассчитывать на восстановление потому что все таки мне кажется 1 терабайт это все-таки немецкого синтетическая получается значения нет если у вас одна стойка скажем там сколько в ней 48 серверов 1 терабайт здесь можно получить накидайте на 9 это при разбивке совершенно разные они спрятаны внутри еще один еще одна причина почему мы изначально не хотели делать систему хранения данных которые ориентированы на мелкие объекты дело в том что ну лично на мой взгляд это обман когда системы там общих старик расскажет вам о том что они надежны потому что как только у вас есть миллион мелких файлов вы можете их реплицировать там несколько недель и в худшем случае понимаете да поэтому мы сразу приняли решение что у нас есть большие объекта если мы хотим делать обжиг сайт что мы занимаемся чем-то типа шарден га то есть ogre героем объекты в большие имиджа и храним в сортовой геккона спрятаны внутри спасибо спасибо"
}