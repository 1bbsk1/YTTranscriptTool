{
  "video_id": "2dNtrCyhmXw",
  "channel": "HighLoadChannel",
  "title": "Эффективные надежные микросервисы / Олег Анастасьев (Одноклассники)",
  "views": 2914,
  "duration": 3485,
  "published": "2020-04-14T11:15:30-07:00",
  "text": "если кто забыл меня все еще зовут олег анастасьев я работаю главным инженером в одноклассниках кроме меня в одноклассниках есть еще шесть тысяч машин на которых работают более 15000 задач как просто на железных серверов так и в виде контейнеров в нашем собственных 4 облаках о которых я рассказывал в одном из кайло дав эти задачи предоставляют более двух сотен различных сервисов и сегодня мы попытаемся вместе с вами построить один из таких сервисов связанных с задачей место чинга и так вместо гири каждый пользователь имеет какие-то чат и он может создать чат один на один с другим пользователям а может сразу с несколькими очевидно что средний пользователь и активно участвуют только в ограниченном количестве чатов одновременно по нашим данным на 5 процентов активных чатов приходится 95 процентов запросов основное что хранят чат и это сообщение большинстве случаев запрашивается не более 13 последних из них но эти цифры не работают когда нужно получить последние сообщения в чатах для отображения вот тут в списке чатов эти сообщения маловероятно что будут востребованы второй раз в веб-клиенте таких сообщений нужно достать только для видимых чатов а в мобильных для всех чатов хранящихся на устройстве пользователя мы будем строить сервис для хранения сообщений которые сам по себе достаточно масштабной более полу триллиона сообщений в 5 миллиардов чатов и эти все данные занимают около сотни точнее более сотни терабайт без учета репликации будем строить высоконагруженные сервис так интереснее более сотни тысяч чтений в секунду и всего восемь тысяч секунду записи как любой самый современный месенджер нас также должен поддерживать полнотекстовый поиск вот тут можно ввести что поискать и получить все чат и с найденными сообщениями а это значит сообщение нужно периодически обходить для индексирования ну и другие задачи которые связаны с обходом и анализом полного набора данных антиспам и проч и проч и проч и так наш сервис будет хранить приблизительно вот такие сообщения сами сообщение это не только текст но и какая-то другая структура здесь вот у меня есть и девчата эти сообщения которые изобразил в виде времени его написания автор сообщения тип сообщения может быть это может быть когда текстовое сообщение это может быть видео звонок например какой то левая под система может отметить сообщение как спам у него есть текст естественно есть какие-то а то че то есть у нас получается вот приблизительно вот такая табличка и этот наш сервис должен обслуживать если следующие операции над этими сообщений я не не на ту кнопку нажал и вот такие операции get me сейчас получает список последних сообщений пользователя передается кто смотрит на сообщение потому что для разных пользователей могут показываться разные сообщения и девчата и временной промежуток с какого по какое мы хотим увидеть сообщение это самой частой самая частая операция нашего сервиса это самая тяжелая операция нашего сервиса она получает последнее самое свежее сообщение для данного пользователя кто смотрит по списку чатов то есть час здесь придется целый большой большой список айди чатов ну естественно как и любой сервис сообщений наш должен поддерживать добавление нового сообщения в чат поиск по которой я только что говорил индексацию давайте попробуем вот это все теперь реализовать с использованием современной микро сервисные архитектуры конечно и так пользователей с мобилки или слева либо напрямую либо через какие-то фронты работают с нашим сервисом сообщений через наш любимый протокол в котором реализована бизнес-логика данные же мы храним отдельно в отдельном постоянным хранилище отказа устойчивым и надежным конечно и в принципе ну и все наверно пара запускаться попробуем реализовать для начала самый частый вопрос нет места час для этого нам надо выполнить по визитам вот такой запрос в базу данных есть только одна маленькая небольшая за проблема таких запросов поступают более сотни тысяч в секунду есть некоторая нагрузка наш сервис ложится масштабирование это то что мы умеем микро сервисов правильно добавляем issues of нашего сервиса и обнаруживаем что нагрузка теперь у нас проваливается в базу и теперь ложится на что дальше ну мы можем либо масштабировать базу теперь либо воспользоваться знанием что 5 процентов это большинство запросов к нашему сердцу будем кэшировать тем более в нашей базе данных есть кэш натурально но оказывается что кэш часто использованных данных помогает немного оказывается есть тонкости в других методах сервиса вызовы get lost my сейчас и индексирование приводят к тому что в кэш базы данных попадают записи которая нам в общем-то не нужны и вытесняют оттуда нужные нам те которые мы хотели бы видеть по get me сейчас так как методы get lost my чудеса яндексе вами работают по значительно большему объёму этих данных чем get messages то есть происходит замусоривание кэша ненужными сообщению то есть мы теперь либо должны на порядок увеличить количество памяти расходами под каш базы данных либо управлять кашами явно из прикладного кода помещая туда именно те сообщения и тогда когда это нужно приложению вводим систему memcache снимаем нагрузку с базы данных при гроздь и нагрузки можем масштабировать memcache независимо вот мы в принципе пришли к типовой архитектуре который чаще всего приходят многие в процессе построения нагруженного сервиса поэтому давайте проанализируем какие тут могут быть потерями и так в первую очередь мы затрачиваем цапу на марше link иди marching паршин когда процесс преобразования данных хранящихся в оперативной памяти формат пригодные для хранения или передачи прикинем где он происходит в нашей схеме сначала наша прикладная логика должна за машей запрос передать его по сети на mem cache cache элладе маршале обрабатывает маршрут ответ и передает назад прикладу ху прикладу х дима вши этого обратно то же самое происходит на ноге с базой данных насколько здесь могут быть велики потери интуитивно кажется что немного наверное ну тут есть недавно проведенные исследования группа исследователей из google и scan for universities и вот здесь вот снизу есть ссылка на это исследование для интересующихся полезно почитать так вот они замедлены на одной из своих реальных production system и получилось что а на маршалинга debarge энинг тратится до 85 процентов дополнительной нагрузки на цикл что приводит к почти 30 процентов задержки дополнительный на миде вторая проблема это чрезмерный чтение и запись они возникают когда приложение читает больше данных измен крыша чем это действительно необходимо и вносит дополнительные расходы по циклу или сети в нашем примере с хранением последних 13 сообщения memcached для того чтобы получить последнее сообщение каждого че то нам пришлось бы прочитать все 13 которые мы храним в каше стерилизовать передать и посетители вызывать на странице сервиса отбросить 12 из них и оставить только одно самое свежее согласно тому же исследование если только 10 процентов если запись содержит только 10 процентов реальный нужных данных как у нас в данном случае приблизительно то мы тратим почти в полтора раза больше циpкa и в два раза больше сети чем это я действительно необходимо третья проблема это сетевые задержки и затраты на передачу трафика тут мы теряем как время просто на факты удаленных походов на memcache и базу данных так и на время на передачу нужного объема данных запросе ответе и мы тратим тем больше чем больше у нас чрезмерные чтения или записи но самое плохое то что эти потери растут прямо пропорционально количеству запросов в кэш или б д которые нужно выполнить для обслуживания одного запроса от клиента и вот это n может быть довольно большим сильно зависит от креативности разработчика нашем же случае с получением последних сообщений чатов он может легко достигать десятков и даже сотен кто виноват разобрались теперь давайте разберемся что с этим делать поскольку большинство нагрузки натурально попадает на мем крыши и их связь прикладной логикой часто их и оптимизирует доходят до довольно экзотических вариантов допустим я видел решение связанные с попытками уменьшить расходы на полу на машине мкаде маршами для миру я сетевые карты таким образом что контроллер сетевой как-то занимается этим я выдел попытки уменьшить сетевые задержки построением мимо ушей на инфраструктуре сетевых свечей очень много людей у порывается именно по оптимизации связи memcache приложение но тут легко заметить что большинство проблем возникает именно из-за факта удаленности необходимых данных от приложения так мы можем решить эти проблемы просто двинув данные поближе к логике таким образом мы приходим к микро сервису с состоянием микро сервис состоянием объединяет прикладную логику сервиса с кэшем в памяти одного и того же процесса это специализированный кэш в нем реализуются операции необходимые для прикладной логике это позволяет полностью избавиться от потеть связанных с сетевыми задержками потому что теперь между этими компонентами нет сети прикладной интерфейс встроенного кэша позволяет реализовать нужный функционал без чрезмерных чтения и записи теперь прикладная логика и кэш обмениваются данными в памяти одного процесса и мы можем выбрать приемлемый для обоих формат данных что позволяет полностью убрать или же сильно снизить затраты на машин и димаш ник совмещение же базы данных в одном процессе сервисом решает те же самые проблемы на второй ноге и наш архитектуры и позволяет просто интегрировать все компоненты в случае необходимости так мы получаем сервис всегда работающий быстрее при одинаковой реализации приложения за счет экономии в два и более раз на потерях которых теперь просто нет а может быть она будет глючить если не будет тормозить для этого давайте вспомним что может пойти не так в простейшей распределенной системе состоящий из двух машин клиента и сервера клиент посылает запрос все обрабатывает и присылает ответ что может пойти не так может произойти пропажа участника это остановка клиента или сервера это крыши to reboot который в которой пошел сервер может произойти потеря входящего и исходящего сообщений это может произойти из-за сети отказ сети переполнение буфера сетевого тоже может быть причиной подобного это тайм-аут участник не отвечает в пределах указаны временных рамок перегрузка диска ушел сервис слаб для джавы стоп зовут джи-си сработал 6 сценарий не правильный ответ то есть участник отвечает неправильным значением тот который другой участник не может понять самая частая причина подобного в архитектуре с сменка шум это неправильный формат данных memcached произвольный отказ византийский участник производит произвольные сообщения в произвольное время это могут быть какие-то умышленные действия ти какого-нибудь злоумышленника типа найдутся такие это могут быть ретро и которые вышли из под контроля и какой-нибудь как бизнес-логика зациклилась в троих и дела и дела и делает это и нагружать более подробно с тем что про то что с этим можно сделать можно посмотреть в докладе вот тут ссылочка внизу я его делал как раз на эту тему и этом только семь сценариев отказа могут быть только в двух агентов теперь давайте посмотрим на нашу архитектуру и попробуем прикинуть что там может сломаться это клиент делает запрос на наш сервис и сервис делает запросу memcache и делает какие-то изменения в базу что может сломаться допустим сервис делают memcache может сломаться memcache не может мин кашне падать волшебный эльфы пишут мин пиши правильно но может отказать сеть может оговорить машина даже если волшебные пишут memcache что машины делают реальные люди на реальном заводе может ли отказать база данных еще более возит волшебный эльфы пишут базу данных мы все знаем о какой никогда не отказывает но сеть машина тоже они могут оказать оба вместе естественно в любой последовательность по закону мерфи в самой неблагоприятной конечно и таким образом у нас семь сценариев отказа возможно на этой связи семь сценариев отказано этой ну и сам стоит вот сервис тоже может отказать там то уже волшебных эльфов нету это мы уже пишем его и клиенту придется как-то иметь с этим делом все эти отказы связаны между собой то есть нам нужно отдельно рассматривать все эти отказы и комбинации между ними для того чтобы правильно запрограммировать нужную их обработку что естественно не делает код проще а вероятность ошибиться и накосячить в таких случаях никогда не пропадать теперь давайте посмотрим вторую схему те же самые три машины организованы по варианту сервиса состоянием здесь схема шарди меня будет немного другой если в первом случае все три машины отвечали за множество ключей к то во втором каждой из машин будет отвечать только за треть к каждой из которых не пересекаются друг другом соответственно коммуникация будет идти только в прикладную логику как-то так ведь кэш и база данных не требует более сетевой коммуникации на каждой из этих связей возможны все те же семь сценариев отказа но действие которое необходимо предпринять разработчику при отказе 1 3 и остальных абсолютно одинаковые обработав отказ по этой первой трети остальные будут обрабатываться точно так же и это потому что отказы никак не связаны друг с другом в рамках обработки одной операции соответственно код будет проще давайте теперь посчитаем вероятности отказа пусть p эта вероятность отказа машины что будет с доступностью данных по множеству ключей к если откажут memcache у все в принципе сша нету база легла и мы не можем это делать что будет если база откажет приблизительно то же самое а что будет если сам сервис откажет тоже мы получим полный недоступность по пол всю зиму множеству то есть если любой компонент откажет мы получим не доступность сервиса по всем ключам из множества к то есть вероятность недоступности будет вот такой приблизительно форму варианте же сервиса состоянием вероятность отказа одной машины будет равна вероятности недоступности только 3 ключей а чтобы получить отказ по всему множеству к должны отказать все три машины то есть формула вероятности отказа для всех ключей будет такой какой из них лучше математики взяли давайте мне математики подставим цифры по простому допустим вероятность отказа одной машины 10 процентов очень большая вероятность для целей иллюстрации вполне достаточно посчитаем формулу ух ты а получается что архитектура сервиса состоянием на порядки надежнее но никто же не ставит по одной реплики равна давайте поставим вторую реплику каждому компоненту слева то есть теперь мы поставили слева шесть машин будет ли эта схема надежнее всех безнадежнее для всех его полного множество к то думаешь что левая схема надежнее думаю что правой схема надежнее молодцы на самом деле правая схема будет она надежнее мы его можно посчитать на досуге сколько реплик нужно сливы стороны поставить чтобы получить такую же надежность справа но здесь даже не это главное доставив еще одни и реплику мы снизили вероятность отказа каждого из компонента на порядок но взаимосвязи никуда не делись а главное то что реплицировать можно и сервис состоянием соответственно снизить снизив вероятность недоступности 3 ключей тоже на порядок а по всем ключам на три порядка получаем что сервис состоянием всегда будет надежнее при одинаковом и даже меньшем количестве машин ну чтож хорошая новость состоит в том что мы знаем что хотим сделать и почему плохая что готовых фреймворков на эту тему мы не нашли а значит будем велосипеде из того что есть итак с чего начнем мы хотим чтоб наш сервис был высоко доступным а это значит что должна обеспечиваться репликация и быть определены какие-то гарантии к системности данных между репликами и масштабируем чтобы в случае увеличении нагрузки мы могли на ходу добавлять мощности ну а поскольку сервис теперь совмещен с данными тори sharding то есть перераспределение данных папа масштабирован им там тоже должно должен поддерживаться это конечно можно реализовать самим можно это интересная задача лет на несколько но быстрее взять что-то готовые допилить слова репликация консистентной себе шарлин намекают на то что мы можем найти что-то готовое современных базах данных поэтому надо определиться с ней в первую очередь мы хотим чтобы база данных которым совмещать было на языке приложения потому что так значительно проще переиспользовать код приложения а значит упростить demar шиллинг интеграцию с приложением так же мы хотим чтобы эта база данных было упал source потому что мы знаем что мы сейчас был велосипедике допиливать в одноклассниках большинство бизнес-логики пишутся на языке java и поэтому мы выбрали кассандру как базу данных которые обеспечивают высокую доступность масштабируемость sharding репликацию консистентной выбрали базу данных теперь нам нужно взять наш сервис и встроить эту базу данных внутрь как это сделать если посмотреть скрипты запуска кассандры вкус становится понятным что запуском занимается вот этот класс а значит все что нам нужно сделать это включить необходимые библиотеки в класс после нашего сервиса и вызвать кассандра демона вот таким заклинанием вот есть вот этот самый кассандра ямал если кто администрировать кассандру то узнает что это настроечный файл кассандры все даже в принципе нам не понадобилось 3 пунктов у нас есть но до базы данных внутри нашего сердца базу данных строили теперь вместе с кодом нашего сервиса у нас есть данные и теперь давайте посмотрим как это меняет маршрутизацию клиентских запросов вот сверху у нас есть клиент по имени лёха он хочет попереписываться в чате ключ которого попадает в определенное множество ключей b для этого кожаный мешок лёха использует какой-нибудь фронт именно поэтому я обвёл его в зелененький квадратик для сервиса без состояния фронту почти все равно на какой именно работоспособный instance сервиса делать запрос он может сделать например вот на этот или вот на вот эти со всех этих инстансов цена обслуживание запросов будет одинаковый им нужно будет сделать запрос и заданными посетили и базу данных у которых они скучные есть для сервиса уже состоянием каждый из инстансов имеет какую-то часть данных локально каждый имеет свой интервал ключей который я обозначил здесь как а b и c и теперь люди владеющие интервалом ключей б не зачем куда-то ходить по сети все данные для выполнения запросов доступны в памяти процесса либо на на локальных дисках а у остальных not нет данных нужного лёхи интервала бей придется работать с ними по сети а значит цена обработки запроса для них сразу увеличивается нади морщинки сеть то есть для того чтобы запрос по интервалы ключей обрабатывался эффективно мы не должны обращаться к не вокальным данным а сделать это мы можем только если клиенты будут вызывать сразу нужные места сервиса то есть нужно маршрутизация запросов учитывающая топологию кластер а то есть информацию распределение данных по нотам мы можем реализовать такой роутинг на фронте в виде библиотеки тогда такая библиотека делала бы запросы по ключам сразу на нужные ноды и эти ноты могли бы работать только с локальными данными распределение данных диктует база данных мы только что ее строили вот только что устроили кассандру давайте посмотрим как кассандры это делает к сандалик каждый ряд имеет ключ составленный из двух компонентов park shin ki на основании которые на основании про хищники на основании которого выбирается но да и классе рынке которые в распределение данных по нодам не участвует но определяет порядок в котором записи будут baci упорядоченной внутри партиций записи с разными противники попадают в разные партиции и могут быть распределены на разные ноты а записи с разными только класть и линки находятся в одной партиций и всегда будет находиться на одной ноте поскольку вместо нежели порядок следования сообщений в чате важен разных чатов между собой нет то для нас про чистенькие правильно будет выбрать а девчата пластинки эти сообщения с точки зрения кодов распределенные данных участвуют следующие компоненты парке shiner на основании значения park shin ki вычислять токен все доступные значения которого принято отображать в виде кольца пар тишине раза дается один раз на весь кластер и не может быть изменено впоследствии токен metadata второй компонент это глобальная структура в которой хранится разбиение кольца на интервалы и соответственно интервалах ли интервалов ключей нодом на которой хранится первичная реплика и наконец-то репетицию стратегий определяет как организуется хранение вторичных реплик и сколько их вообще должно быть репликация стратегий можно указывать разные для разных таблиц при желании указав рипли разных дальнейшем свечи для разбивки space все этой информации достаточно для того чтобы вот в эти 3 строчки составить полную карту нахождения всех данных и их реплик в нашем кластере имплант мапуту здесь будет содержать соответствие между максимальном токе нам интервала и адресами всех реплик попадающих в этих в этот интервал включить точнее кто кинг передав этот нпф map на клиента легко сделать нужную нам маршрутизацию стоит иметь ввиду что топология кластера меняется время от времени туда добавляются ноды убавлю убираются но да и так далее поэтому необходимо организовать обновление этой информации также стоит подумать что делать если клиент вызывает сервис пользуясь устаревший топологии например вскоре после ее изменений тут могут быть несколько вполне очевидных стратегий стоит подумать какая подходит под предложение взглядами теперь ценой давайте тебе с этими данными по работы итак у нас есть вот такая таблица и нам надо получить список сообщений то есть выполнить вот такой запросу как мы бы делали это через драйвера базы да ну вот пример который я взял прямо с мануал что-то делается в строчки один мы строим кластер и устанавливаем с ним соединение с базой данных строчки 2 мы отправляем какой-то запрос на базу данных получаем безант сет и строчки три мы начинаем использовать полученные данные на самом деле сесть на все немного сложнее потому что там есть настройки куда соединяться с кластером мы нужно соединений какой-то должен быть есть не так что конечно просто для того же чтобы сделать подобный запрос изнутри есть вот такой класс и вот такой метод он выполнит переданный запрос с указанным концы скосил эрлом и параметрами и вернет лизал set обратите внимание он статический никаких соединений нет это потому что сервис уже работает в контексте своего кластера базы данных ему некуда соединяться он и есть база данных соответственно сообщение сообщение для дед мы сейчас получит вот приблизительно такой код аналогичным способом для id мы должны выполнить запрос добавления записи точно таким же способом как видите все очень просто если покопаться в каире процессоре то можно найти и более эффективный способ получения записей из базы данных он не так просто выглядит на поэтому я его тут не нарисовал мы используем в реальности конечно более эффективный вариант но даже вот этот не эффективный вариант будет быстрее и того что вы получите использовать любой клиентские драйвер вот мы и подобрались теперь к самому интересному реализации встроенного каша сообщений для начала давайте посчитаем сколько данных он будет хранить и так у нас всего вот столько данных и пять процентов активных чатов из которых мы хотим хранить в кэше только последние тринадцать все исходные данные есть можем посчитать что в память к нам попадает довольно большое количество сообщений и данных это без учета репликации нужно еще поделить на количество нот и умножить на x factor из всего набора методов сервиса в кэше нам нужно реализовать только первые три так как поиск индексированием мы как раз не хотим чтобы были в кашин давайте начнем с реализацией get me сейчас пользователи за открывает округу и выбирает чат с лёхой то есть мы через wi фронт получаем на нужный нам distance запрос где-то месяц далее приклад ухо спрашивает есть ли эти советы чатов каши нет выполняет запрос очистки вали процессор в базу данных грузит данные помещает их в кэш и и возвращает клиенту готова почувствовал возмущение в силе кожаный мешок лёха через мобильное приложение делает то же самое удачно попадает на другую реплику где происходит в принципе тоже самое все хорошо заботьтесь о состоянии легкие лиза предлагает ему сходить пообедать то есть вызывает метод это сервис создает сообщение со встроенной базе данных и добавляет новое сообщение в in process кэш его состояние теперь будет привить над таким все сообщение отправлено лёха получает пышный телефон запрашивает сообщение в другой реплики прикладу х отдает данные спеша там ведь они уже есть лёха остался голодным очевидно мы должны как-то сообщить другим репликам что появилось новое сообщение мы можем использовать информацию о топологии кластеров точно так же как мы делаем это на клиенте вычислить серии applique и модифицировать их все с новым сообщениям вызов например какой нибудь спиц метод этот и тогда мы вставим это новое сообщение в наш каши все хорошо или get the думаю что все будет хорошо пессимисты собрались салидо стоит посмотреть распределенная система в одноклассниках где объясняется что может пойти не так проц а проблема в том что метод этот будет вызываться по сети и тут нас поджидают наши знакомые семь сценариев отказа то есть этот метод может отказать для всех либо для части реплик и мы на самом деле не можем ждать и и вели троить бесконечно потому что вполне возможно что реплик больше нет и довольно и будет и будет нет очень долго может крыш какой-то произошел до или сеть все что угодно могло произойти однозначно то что при такой схеме не все нотификации будут достигать все реплики и состояния к шее будет устаревшим для некоторых записи такая несогласованность имеет свойство естественно накапливаться со временем давайте посмотрим как эту же самую ситуацию будут отрабатывать кассандра при выполнении инсульта она тоже определить необходимые реплики данные которых нужно изменить формируют для них требования а изменение называется мутация в терминах кассандра и попадается послать это требование по сети поскольку нас кэш и надо кассандры желтой реплики это один процесс то такая мутация не может тоже быть доставлено точно так же в силу тех же самых причин в этом случае кассандра сохраняет такие мутации локально и persistent на на диск когда в связи желтой репликой восстановится такие пропущенные мутации будут отосланы ей при первыми возможность это называется hand обычно такие финты сохраняются некоторые ограниченное время иначе вместо на дисках рабочих not могло бы закончиться из-за одной единственной достаточно долго не работающий 0 для восстановления согласованности в таких тяжелых случаях служит риддере п.р. это сравнение реплик при чтении и в результате обнаружения расхождений формируется мутация на устаревшие реплики с правильными данными последний метод это streaming реппер это как пакетный фоновый запрос который обходит абсолютно все данные ноды и сравнивает и сравнивает все данные всех реплик в результате так как объем передаваемых данных может быть очень большим вместо индивидуальных мутаций формируется файл из таблицы содержащий нужные строки который передается на устаревшую но тут целиком через отдельное соединение так называемый стриминг и эти данные могут быть за монтированы и использованы этой новой как будто это родные данные на диске все эти меры хорошо работают и позволяют поддерживать консистентной реплика кассандры реализовывать их самим было бы долго и сложно мы с вами люди заняты поэтому мы бы хотели делать так чтобы каким бы образом новые данные не попали на локальную apple ку ка саблин модифицировал бы нашу бизнес-логику которая в ответ добавило бы недостающие данные в кэш и у нас и состоянии каша была бы can систем то есть мы можем определить вот такой интерфейс для listener и изменений от кассандре ли реализовать его в нашей прикладывать сесть на сам по себе он работать не будет нам надо немного дописать кодов кассандру оказывается мест в которой нашли нужно дописать все 2 все мутации независимо от того какая подсистема x формировала проходит вот через этот метод объект mutation здесь содержит список новых данных для различных таблиц но можно просто брать эти данные сова девались перебрать рисовать брать и свой весь streaming заканчивается вот таким он комп личным runnable взяв список таблиц достаточно просто пройтись по ним и котором по пути засовывая данные вылез на рассмотрим ту же ситуацию доставить мутацию невозможно но не из-за проблем носить и а потому что сервер пошел в ребут или случился крыши состояние нашего каша в памяти пропала и теперь финты нам не помогут потому что hand и содержат только новые данные а для get me сейчас нам нужны старые тоже а их мы можем получить только дочитав их из базы данных дочитывать придется для всех чатов этой ноты что приведет к лавинообразно мы увеличению количества чтения из базы данных сразу после старта а чтение будет идти с нескольких реплик одновременно то есть шанс что мы уложим весь кластер внезапные нагрузки внезапные нагрузкой достаточно велик уже делать есть несколько стратегий наиболее общие применимые это брать наш кэш делать snapshot и иметь какой-то брайта hotlog но у нас ведь кассандра мы писать код кельвин давайте воспользуемся ей мы можем сделать вот такой кий space с использованием недокументированные стратегии репликации local стратегия это стратегия на самом деле означает просто то что данные этого кисть из этих таблиц и реплицировать на кластер не нужно они локальные для нам минут и создать там вот такую таблицу и потом собственно при операции помещения каких-то данных в кэш мы пишем в эту таблицу данные при вершине мы удаляем , лиц и данные и когда случается reboot мы проверяем пустой кэш пустой загружаем вот таким вот селектор получаем старые данные те которые были в каше в память и плохим там получаем новый готова скорость вот этой вот загрузки ада зависит от конечно скорости дисков и размера кэша очень сильно мне удавалось достигать скорости до полумиллиона ключей в секунду но конечно секач большой то время не которое проходит путь это достаточно быстро с одной не хочется ждать пока загрузится с диска кэш при полном рис при рестарте простом бы не старте но до в результате допустим выкладки новую версию что же делать тут нам помогает разделяемая память разделяемая память не пропадает предоставьте процесса самый простой способ получить туда доступа это открыть вот такой файл в джаве это будет дорик борьбу селе например но лучше использовать уже готовую структуру доступную в ванне о на гитхабе вот в этом пакете обратите внимание на shared memory map его потомков там есть разные потомки на всякий вкус и цвет не проблема подобрать нужный этот класс можно использовать за монтировал серьезный темпов с pdf шум что делается обычно по дефолту в линуксе но мы также используем их учитель bfs что позволяет сделать более крупные страницы памяти на уровне операционной системы а значит уменьшить количество tlb миссов и убыстрить работу с памятью в особо интенсивных случаях теперь данные ниоткуда грузить не надо shared memory map только проверяет быстренько валидность каша что очень быстро и так теперь у нас снова наша ситуация с тестором реплики теперь сталь стал быстрым и часть запросов успевает проходить до того как придут hand и и лёха получает снова устаревшую версию пишем и снова остаётся голодным почему так процесс посылки hand of the sun или асинхронный бой дизайн поскольку при чтении кассандра обращается к нескольким репликам для определения согласованности данных и может отбросить устаревшие данные с рис так ну той ноды hand и в общем-то не обязательны для соблюдения согласованности наше приложение в 15 раз более нагруженной и на именно на чтение чем на запись и такой вариант согласования реплик на чтение с нескольких not будет слишком тяжелыми поэтому мы делаем так мы запускаем сначала сетевые сервисы наши встроенные базы данных ждем пока она примет hand и от остальных not кластера и только после этого запускаем обслуживание прикладных запросов сервисом с этого момента на сервис могут поступать запросы от клиентов но до находится в согласованном состоянии вот такой процесс ожидания завершение каких-то операций на класть или очень напоминает задачу распределенной координации мы пробовали несколько разных вариантов имплементации и остановились в итоге на простейший но до при старте периодически спрашивает у всех у всех других not которые могут иметь и реплики для неё есть ли для меня hand и повторяет она это до тех пор пока все реплики ответят что больше hand of нет и продолжает старт с этим разобрались поехали дальше теперь можем подступиться к самому тяжелому запросу получить последние сообщения в списке чатов этот запрос до должен просмотреть списки сообщений по множеству чатов грязные чат и могут быть расположены на разных родах а значит нет одной ноты владеющий всеми данные запрашиваются данные в большинстве своем не активных чатов как мы видим активен здесь вот на картиночке только один чат из многих значит этих данных по неактивным что там нет в кэше и загружать их в кэш смысла нет маловероятно что они станут активными ближайшее время данные таких старых чатов если они не в каше скорее всего согласованы так как на старых данных скорее всего уже произошли все необходимые процедуры согласования в кассандре hand & repair и скорее всего отработали а если час становится активным его согласованность потенциально может нарушиться то он будет помещен в кэш и перестанет быть старым возможно мы сможем что-то сэкономить на согласование данных посмотрим как бы мы реализовали этот метод сервисе без состояния клиент вызвал бы метод сервисе прикладная логика которого просила бы мем каши для каждого chateau из списка и базу данных для тех старых чатов которые были вытеснены из кэша сервисе состоянием нет единого сервиса с полным набором данных о прокси ровать вызовы с одного инстанса на другие неэффективно наиболее эффективно иметь реализацию этого метода на всех участников и на сервисах и на фронте клиентам тогда клиентская реализация используя информацию о топологии кластера разбивает список ключей чатов на несколько списков для каждого инстанса сервиса свой выбирая эту реплику и вызывает ее передав ей список с ее аиде чатов то же самое с остальными репликами сервисы обрабатывают запросы возвращают данные каждую свою часть на клиента окленд доставшиеся ответа по всем нужным ключам объединяет результат готово можем рисовать наши часики внутри же каждой реплике выполнения было бы таким приклад ухо обращается в кэш откуда получает данные для телеф чатов и в базу данных для неактивных но база данных в этом случае будет делать сетевой запрос для возможного согласования реплик данных а это дополнительная сетевая коммуникация но мы знаем что согласованность на старых чатов которые не в кэше нам неважно и скорее всего данные там согласованы поэтому мы могли бы убрать большую часть сетевые коммуникации в нашем самом тяжелом запросе запроси в данной базы данных только с локальный реплики с локальных дисков стандартно матери кассандры такого способа нет но наш сервис не ограничен публично мать и и мы можем сделать это вот как то так здесь очень много всякой разной маги на самом деле все то же самое мы делаем statement передаем параметры немного просто по-другому отличие же на самом деле вот в этом вызове он значительно быстрее так как много накладных расходов связанных с сетевыми активностями просто не делают чуть покопавшись можно еще больше сэкономить тут но тут уже совсем у нас в экрану же это кот не влезет слишком сложно быть ну что по написанием это всякого кода давайте теперь попробуем с этим всем пожить первое с чем мы столкнемся это увеличившееся время тепло и теперь сервис совмещен с базой данных и на старте тратит время на вот эти активности если выкладка происходило плавно по одному instance'у сервиса за раз то время выкладки растет пропорционально количеству реплик это может занимать значительное время что делать выкладывать не по одному а параллельно по зонам доступности мы например часто выкладываем сначала 1 1 instance чтобы убедиться что все как-то работает а затем параллельно все остальные в этом дата-центре в последующих dc все идут сразу с параллель так время выкладки сокращается до количества было бегите за + 1 вторая проблема это более частые рестарты б.д. почему база данных совмещена с приложением соответственно когда мы выкладываем новую версию приложения мы рестарту им базу данных тоже интуиция говорит нам что нельзя ли стартовать вообще-то базу данных можно уже и сломаться что же делать а на самом деле ничего это хорошо что мы стартуем базу данных это позволяет нам отладить отказы зон доступности в контролируемой среде плюс мы получаем бесплатно и регулярное тестирование отказа все всей зоны доступности сейчас когда вы на нее смотрите когда она птица они ночью когда вы спите а вот что является реальной проблемой это взаимное влияние прикладного кода и базы данных почему база данных совмещена с прикладным кодом и очень легко можно с чего-нибудь так накатить чтобы мы например съели весь цикл или память закончилась и таким образом мы в общем из одной баги можем уложить реплики сервиса и приехали что же делать если делается такой код а можно и всегда на самом деле так делать мы выкладываем одну зону доступности на более долгий срок и ждем что сломается максимум что мы можем сломать это вот эту единственную зону доступности а все остальные работают и обслуживает клиентов если функционал какой то уже совсем стремный то тогда можно конечно рубильники должны быть которым можно подключить какой-то функционал на ходу следующие чем придется столкнуться это апгрейт на новую версию встроенной базы данных почему мы тут на использовали много всяких недокументированных функций правильно а эти же не документирую функции они потому и недокументированные что они могут поменяться и интуиция тут нам подсказывает что они лишь поменяю сейчас все и мы потом чем потом дел на самом деле архитектура базы данных меняется очень медленно мы этот паттерн используем уже около 9 лет и эти девять лет наблюдений за кассандры показали что сущности остались те же они только переименовываются время от времени плюс современная база данных распределена должна поддерживать валенка braids то есть когда вы на одной ноте базы данных запускаете одну версию на других другой для нас саблей базы данных означает что мы просто сделаем просто соберем наше приложение с другой версии библиотеки вот и все выложим один дата-центр ждем что сломается если уж совсем все переломались мы все данные все-таки потеряли в этом дата-центре откатят катим восстановим эти данные из реплик которые работают на старый версию этого цельсия состоянием эффективнее и надежнее так как между компонентами тебя нет сети а значит и потери связанных с ней теперь у нас есть гарантии консистентной sti в кашах между кэшем и базой данных а их часто забывает про построение систем с выносным кашу это относительно просто реализуется все самое сложное уже реализовано в кассандре и ванне и потому мы давно и широко используем во множестве сервисов если высоконагруженные сервисы на миллионы запросов в секунду есть и хранящий большие объемы данных есть со сложной логикой обработки выборки очень много лента обсуждение посты нотификации даже наше облако она все построено по этому поттер если вы хотите узнать больше то можно посмотреть распределенной системы в одноклассниках доступна в записи новый граф одноклассников уже был это же доступен только в записи но это будет prograf построенный по этому же самому потому восстание машин будет завтра в десять это не про этот паттерн у тоже интересно все это все что я хотел вам сегодня рассказать спасибо спасибо огромный олег во-первых ты прекрасно справился с 10 технической трудности которая возникла спасибо большое только макет а вот тебе грамм а то вдруг ты собираешь и и полезную штуку который пригодится друзья я думаю что вопрос 3 мы успеем задать смотрите что-то еще следующий будет происходить 1830 поэтому нас полно времени давайте по задаем вопросы а ты выбираешь здравствуй спасибо за доклад а как такой архитектуре сделать редактирование сообщения либо его удаления нет проблем с кассандрой записи там и и есть проблема с если мы говорим о кассандре да и наверное подразумевается паттерн 5 five right да то есть мы прочитали что-то на клиенте по модифицировали и записали да с такой архитектурой как раз ritmo de file врать не нужен потому что все изменения происходят локально прямо же это бизнес логика которая меняет она локально находится в сервисе мы натурально если нам нужно а то марина это сделать можно сделать log локальный gridlock сделать все необходимые изменения записать их спасибо отлично слева следующий вопрос спасибо за доклад интересная концепция есть много вопросов но задом хотя бы один смотрите мы изначально уходили от сетевой коммуникации чтобы сократить марша линк чтобы сократить влияние там сетевых драйверов и так далее но дальше у нас стает вопрос репликации у нас есть 2 основных механизма the stream repair но насколько я помню он сам по себе медленно неторопливо и мы предпочитаем редре п р который быстрый но ретривер требуют чтение с нескольких реплик в том числе по сети вот получается что мы таким образом возвращаемся к тем же самым сценарием отказа мы возвращаемся к тем же самым коммуникациям и ключевой вопрос о неужели действительно это настолько эффективнее чтобы разрабатывать целиком свой фреймворк и мы все равно получаем экономию там цепью и о и прочее ну то есть неужели есть эффект ну просто нестандартный все-таки подход современном мире ваш вопрос ответ раз попадется следующий следующий вопрос уже на портится мне кивал спасибо за доклад хотел спросить вы сказали что вы рассмотрели фреймворке не нашли похожа как я вижу по вашей проблеме вам был очень хорошо подошла actor на я модель и сейчас как бы а к акторы которая реализована для java ide как будто что вам бы подошла она позволяет вам имеется встроенный кэш уже из коробки она позволяет вам масштабировать чтение она позволяет делать residence вашего applications то есть соответственно если да надо умирает этот актор просто сми грир уютно следующем году все будет работать и ряды по старинным крипера тоже делает что риддере ps3 мем creeper he этого не она тоже делать не знаю нужно смотрел наверное нет потому что она делает скорее всего это и через кассандру она она в памяти держится стоит последний стэйт и если она упадет она считается кассандре либо snapshot как вырезали и реализовали у себя snapshot свой точно так же на считается на в шоке накатить поверху ивенты вы строили какую-нибудь реальную систему таким образом она о сработает ну я сейчас устрою построить и расскажите может быть есть о чем поговорить после да и отдельно ребята увожу правило один вопрос в одни руки дайте микрофон шар в тесте если время понял то вы храните один чат в одной базе данных скажите просто зачем вы это делает 2 базы данных или в нескольких скажете зачем почему в двух которых если приходит ответ от участника карета вам нужно реплицировать и это реплики это не две базы данных это реплики то есть это ноды одной и той же базы данных базы данных называется кассандра у нас это распределенная база данных она состоит из многих not do каждые данные хранятся несколько раз на в кластере на то есть мы там вот когда картинки и желтенькая и синенькая это были реплики то есть это 2 разных ноды удаленные по сети но они реплицируются то есть это этот счет это первичная и вторичная реплика в данном случае вопрос вот углу камеры а здравствуйте вопрос такое когда мы на essence выкладываем новую версию библиотеки с новой базы данных на другом инстансе у нас база данных еще с предыдущей версии как происходит взаимодействие hand of в таком случае это проблема на стороне разработчиков базы данных потому что если они хотят чтобы их пользователи использовали новую базу да ну новую версию базы данных они обязаны с должны тащить они должны соблюсти совместимость между этими сервисами иначе пользовать их пользователей не смогут обновиться никаким образом соответственно натурально есть баги но бывает так что они хотели но не получилось но это уже как раз и выясняется на этапе пробное тестирование про него выкатки на этом сегодня а так в принципе с точки зрения выкатки новых версий она ничем не отличается от выкатки новых версий на в обычной ситуации когда это ну когда это база данных она эксклюзивный пользователя машину повернись налево там вопрос еще скажите у вас получается кассандра она должна умный реплицировать то есть оно не данные должна с одной ноты реплицировать другие данные с другой потому что то есть вы еще управляете репликации кем мы не управляем это это делает кассандра само по себе то есть я конечно рассказал не все просто не могу в 40 минут там высота кассандр девушка у вас мальчиком до общались один час должны между собой реплицировать а другой учат она должна с другой новый реплицировать это это вот как раз в за шитов топологии класть не зашита это это то чем занимается кассандра ее и топологии и кластер а то есть она это определяет да там есть там есть аппликация этим она занимается распределением и репликации данных накласть и и мы сознательно этого не писали сами потому что это очень сложно на самом деле сделать так чтобы это не учила и не тормозила на то есть куда она реплицировать там мы будет же а мы будет да все что мы сделали мы написали вот этот лишнего интерфейс мы получим нотификацию что он новый да нам пришли а нам-то мы должны знать куда точно то решится мы должны знать куда от реплицировали после тура учить этому клиенту на эту да да да вот весь был вот который составлял вот эту карту репликации его показать но понимание в случае власти след себя просто право около колонны человек справа около колоннами в безрукавке подними руку чтоб тебя было видно а у меня вопрос может ли эта концепция архитектурная ложится на при размещении сервисов в контейнерах но что а при значении х в decker контейнерах перри размещение а при размещении самих сервисов а в докер контейнеры когда у нас куча вот именно таких микро сервисов работают в контейнерах в нашем облаке а как вы решаете проблему шарит memory shared memory получается у нас ездит понижения shared memory в облаке это одна из причин почему мы делали своего от оси вас контейнер шейд мэмри отличалась длине для контейнера это просто выглядит как вол им который пробрасываю ца внутрь контейнера первый ряд вот здесь здравствуйте алексей судя по вашему доклады вы не стали использовать штатные механизм каширование кассандры сделали свой узкоспециализированный не совсем понятно зачем но я не совсем понял не было точно объяснено вы в этом к же тоже хоронили данные именно шокированные по рингу кассандры то есть распределение включив была точно такая же как у ринга кассандра и вы просто утащили алгоритм ринга прямо в мобильный клиент алгоритм чего алгоритм поиска нужных нот в ринге вы прямо в мобильный клиент тут нет не так все чудеса то мобильные клиенты ходят на api фронты да и вот в api фронтах есть вот это вот логика а есть рот то есть у вас промежуточный роутер но дамы внутрь инфраструктуры все-таки из интернета не очень хорошо а поэтому есть api фронты и они уже делают в этом же направлении черен вопроса сверху олег большое спасибо за доклад очень интересное решение как нарушая за плита использовали инструменты да и построили реально элегантно и техническое решение мой вопрос риторический а как же oracle у нас не было у оракла исторически у нас был microsoft sql server и я вам скажу очень дорого спасибо друзья последний вопрос вот здесь 1 рядом здравствуйте спасибо за доклад вот по поводу из себя ближе к тебе скажем так я поработал уже с кассандрой и знаю немножко внутренности вопрос такой вы обретёте записей удаляете что-то а как вы боретесь с могилками это не очень по доклад давай в дискуссионной зоне я подробно объясню просто это не по докладу раз такое дело еще один вопрос спасибо большое за вклад у меня такой вопрос скорее больше любопытства а вот вы говорите что когда приходит нотификация по мне тишина эти фетиши на какую-то ноду вы сразу же добавляйте записи кэш обязательно делать или можно просто запустить и фетиш он не обязательно должен украинец на самом деле не совсем кэш это ну я назвал алкашом для простоты на самом деле это резидентная in-memory база данных тоже хранилище так скажем да и мы это делаем потому что нам нужно иметь консистентную копию то есть если сообщение куда-то приходит то она потом про читается сто пудов же я тебя посылают и прочитаешь это сообщение мне нужно чтобы это сообщение уже была в каша а то есть мы то есть мы не делаем так что мы хочет люси сообщение лезем в кэш то мы не находим и поэтому лезем в базу именно но из причин почему мы делаем всю кэш ну то есть резидент ную в памяти хранилище с апликэйшен интерфейсом то есть с операциями которые именно на прикладу хуже ориентиром спасибо ну какой вопрос был лучшим а вот кстати самый на интерес выходить нам расскажи как тебе знать в микрофон по-прежнему еще раз где работаешь почему для тебя это было важно здравствуйте меня зовут иван я работаю в компании bright box ну я пишу не на джаве peшил на сишарп но у нас тоже архитектура такого сока нагруженным приложение до сбор телеметрии и и мы используем редис в качестве кэша но мы его используем как скорее именно как кэш то есть если мы лезем туда если там пусто мы лезем базу данных вот поэтому я решил почтить что в докладе слова каша спасибо огромное спасибо огромное всем кто с нами сейчас"
}