{
  "video_id": "GM8vEhlBbF8",
  "channel": "HighLoadChannel",
  "title": "ORC и Parquet. О форматах и их использовании на базе HDFS / Александр Маркачев (билайн)",
  "views": 415,
  "duration": 2331,
  "published": "2024-04-17T01:10:20-07:00",
  "text": "Всем привет Добрый день рад всех видеть на последнем докладе последнего дня к этой конференции рад то что вы собрались силами и все-таки пришли И сегодня я хочу затронуть такую тему как форматы данных как их правильно использовать как правильно хранить чтобы всем было хорошо начнем с предыстории всего в мире на текущий момент 97 за тобой данных и ежедневный генерируется порядка 220 терабайт еще к 2025 году ожидается двукратный рост по сравнению с сегодняшним днем в этом плане Билайн не является исключением у нас растут данные как за счет новых так и за счет обработки до обогащения старых данных и одна из задач от инженера это эффективно управлять этими данными для того чтобы они занимали мало места и в то же время были легко доступны Привет Сегодня мы рассмотрим один из аспектов работы как раз форматы Итак я маркачев Александр дата инженер команды голосовой антифрод Билайн и сегодня на докладе мы Познакомимся с основными характеристиками форматов рассмотрим их применение на базе hdfs и обсудим внутреннее устройство и оптимизацию всего форматов огромное множество жизнь и Аура многие другие Но все они не подходят для наших целей для хранения и быстрого доступа кто-то из них либо занимает слишком много места кто-то из них слишком медленный А кто-то И то и другое вместе но среди них выделяется два формата UFC и паркет они оба прекрасно подходят для наших целей у обоих есть свои плюсы и минусы Поэтому нам нужно придумать какие-либо метрики качества по которым мы будем их оценивать для нас основными является это степень сжатия и скорость доступа но также на практике со временем мы узнали что необходимо периодически менять Столбцы местами необходимо добавлять так необходимо чтобы была поддержка всех типов данных поэтому мы вывели эти метрики Для того чтобы Вы могли их увидеть и не исследовать самостоятельно для начала чтобы вообще понимать Как устроены форматы и как правильно Нам использовать необходимо определиться структурой начнем с пакета паштет был создан в 2013 году в марте и изначально поддерживал все не все многие языки программирования не только живым подобные поддерживался многими системами использует его на текущий момент такие гиганты как Амазонка Билайн структура у файла паркет с официального сайта выглядит вот так но она абсолютно непонятна там есть заголовок есть группы строк колонок и страниц поэтому Давайте разберем подробно чтобы не путаться в этом страшном графике начинается файл с файлхедера содержит он магическое число пар один которое позволяет нам прочитать и определить что это паркет файл для дальнейшей его оптимизации заканчивается постскриптам сейчас заканчивается файл футером он содержит метаданные определения схемы информация о группах строк метаданные остался способен на содержать дополнительные данные например такие как местоположение словаря для столбца содержит статистику по столбцам и между файл хедером и файл-фотором находится группа строк это разбитие одного конкретного файла на множество маленьких блоков для того чтобы в случае необходимости нам не приходилось вычитывать весь огромный файл предположим вы записали 17 ГБ весь поэтому там есть группы строк но и этого мало в нем есть также развитие по колонкам каждая колонка можно прочитать отдельно что важно если мы говорим о широких таблицах представим что у вас есть 10 тысяч колонок и вы можете благодаря данному формату выбрать только одну из них и не считывать диска все что значительно уменьшает нагрузку на диск и ускоряет ее обработку в конце в колонке есть также группы страниц пейджа страница это уже физическое представление данных набор значений для каждого столбца файле имеет собственное описание схемы данных которые указывает на тип данных на размер конкретной страницы в кодирования также там как евро-групп есть словарь возможность ссылки на словарь для того чтобы декодировать данные в принципе на этом структура файла паркет заканчивается и можно приступить к орси был создан на месяц раньше чем паркет но ему не повезло он поддерживал языки и был предназначен именно для хайва в основном в 2016 году его доработали и он стал поддерживать подобные клиенты стал поддерживаться многими системами сейчас как я слышал внедрили поддержку и в импалу у него очень расширился диапазон диапазон применения диапазон пользы и создатели его постоянно совершенствуют используют его такие гиганты опять как Facebook Twitter netflix Amazon Билайн вы могли заметить то что я повторил некоторые компании по той причине что на самом деле нет единственного правильного ответа потому Какой формат использовать нужно в зависимости от ситуации принимать те или иные решения об этом мы поговорим в конце структура орси выглядят еще страшнее чем структура у паркета там есть также заголовки есть они же полосы есть участки колонок страниц есть по скрипт поэтому начнем и начинается он также как вы паркет с файл только этот уже что позволяет нам прочитать его как заканчивается и это отличие от паркета потому что не содержит в себе все метаданные он содержит только информацию после постскрипта уже идет файл Footer который содержит все метаданные которые содержат и паркет но уже в сжатом виде что позволяет нам сэкономить крупицу данных крупицу место на дисках за счет сжатия и в то же время позволяет очень быстро отсеивать ненужные файлы если у вас много в рамках одной таблицы в файл фоторе хранится именно колонок название количества строк каждого страйпа статистика информация о каждом строке также там могут храниться словари и многое другое между Скайпы это аналог групп в паркете То есть просто развитие одного большого файла на множество маленьких внутри невидимого для глаз простого человека в свою очередь Skype делятся на блоке делятся они содержат индексы они содержат сами данные и содержат Skype Footer Скай футер это аналог файл футера только для конкретного Скайпа содержит все метаданные для конкретного строя и позволяет его прочитать очень быстро И что самое главное в отличие опять-таки от паркет Если другие страйпы были повреждены Вы можете прочитать независимо от них и вы не потеряете все данные одного файла потеряете только их часть если такое случилось так Я уже сказал Прошу прощения забыл перешел блок индексов в свою очередь содержит информацию конкретного столбца конкретной конкретного страйпа то есть такие данные как минимум максимум сумму количество строк в конкретно данном столбце и наконец между блоком индексов и файл фотора содержится сами данные их физическое разделение на колонки что позволяет нам прочитать один маленький блог конкретного сайта колонки в свою очередь в отличие от паркета делятся на блоки индексов и блоки данных в каждом блоке индексов содержится метаинформация для прочтения конкретного небольшого блока который вы задали этот небольшой блок по умолчанию равен 10 тысячам элементов и позволяет лишний раз не грузить вашу жесткий диск и ваш процессор для того чтобы здесь реализовать данные потому что у вас есть эта информация Вы получили нужные данные узнав о внутренней структуре мы можем понять то что оба формата является колончатыми оба формата имеют статистику и формата можно добавить Столбцы также из того что мы не узнали структуры но мы узнали при тестах в паркете можно менять местами Столбцы А в орси нельзя Это особенно важно на период разработки Когда у вас часто меняется схема данных невозможность добавить середину столбец может оказаться очень критичной понятно что мы можем добавить конец и потом просто ссылаться поменять но это не всегда удобно поэтому на период разработки очень удобно использовать паркет узнаем об остальных пунктах которые мы не смогли заполнить узнаем как сжимают сами форматы Какие алгоритмы дополнительные для сжатия у них есть И с какой скоростью мы можем получить данные из этих форматов для начала нам нужно ввести какой-либо какой-либо метрику качества либо таблицу мы взяли таблицу на 80 плюс колонок с пятью 100 миллионами записей Васи свет данная таблица занимал 204 ГБ Если мы без сжатия дополнительного просто запишем это в паркет то мы уже получим 47 Гб в России мы еще сильнее сожмем и получим всего лишь 35 гигабайт Но кроме внутреннего встроенного сжатия этих форматов у них есть также алгоритмы например из лип поддерживается обоими форматами но только у пашкета есть возможность сжимать в Zippy понятно это нам мало О чем говорит ну алгоритмы сжатия алгоритмы сжатия что это такое вообще Поэтому Давайте посмотрим Бенчмарк очень большой с кучей форматов остановимся на некоторых подробнее например снапи Это достаточно старые алгоритм для форматов он поддерживается уже многие годы и он все еще входит в список лучших форматов для сжатия потому что он достаточно хорошо сжимает и делает это достаточно быстро также есть лз-4 это на самом деле на текущий момент самый быстрый формат сжатия он сжимает намного слабее чем снайпе или другие представленные тут списки Но делает это крайне быстро что позволяет нам не только быстро сжать но и быстро прочитать следующий это за СТД этот формат наоборот один из Новых его добавили не так давно во всей паркет он имеет 23 настройки 20 степени сжатия Вот например на этом бенчмарке мы видим Четыре из них Чем сильнее Мы сжимаем тем мы делаем Это медленнее что в принципе видно на скриншоте в то же время если мы слабо сжимаем это не значит то что это плохо потому что мы можем быстро прочитать благодаря этому и отдельного упоминания стоит злить даже на данном бенчмарке он занимает второе место благодаря тому что очень хорошо сжимает зажимает он прекрасно для орси была сделана дополнительная доработка уже Если не ошибаюсь в 2015 году и собрал лучше за 4 от Zippo и от себя самого благодаря чему он может быстро читать может быстро записывать И не потерял качество сжатия Да он благодаря тому что взял из разных форматах разные метрики он не доходит по скорости до либо но тем не менее он остается прекрасным выбором для узнав о том что и узнав о том что паркет поддерживает более слабые алгоритмы сжатия мы можем предположить то что пакет будет сжимать хуже чем ls4 и теперь осталось заполнить оставшиеся три пункта узнать как всё-таки быстро мы будем получать данные Какие типы данных поддерживаются Ну и может быть есть какие-то особенности у этих форматов которые заслуживают упоминания Как видите мы сразу перешли к России как будто бы забыв о паркете но мы не забыли о нем на самом деле мы провели огромное количество тестов для того чтобы убедиться что будет лучше для долгосрочного хранения и аналитики и пришли к выводу то что в большинстве случаев в России окажет достаточно значительные преимущество по сравнению с паркетом Вот такая вот есть у нас табличка с метриками Это все та же таблица на 500 миллионов записей и 80 колонок Как видите паркет в снапе в лучшем случае показывает 33 ГБ при сжатом формате В то время как 10,8 паркет в лучшем случае 211 Итак Давайте теперь посмотрим как мы можем оптимизировать наши данные нашей табличке для того чтобы быстро эффективно ими воспользоваться иначе этой таблицы нет никакого смысла Что такое оптимизированный кто знает начнем с маленькой таблички И на самом деле Тут ничего интересного как маленькую табличку не оптимизировать все будет одинаково на самом деле Там разница буквально десятки килобайт и в оптимизации вот маленьких таблиц с не знаю 100 тысяч 200 тысяч элементов не имеет никакого смысла Но если очень хочется если реально счет идет на миллисекунды то можно отключить индексы отключение индексов идет с помощью но к сожалению во всех версиях и всегда она отличается этой командой почему это происходит К сожалению неизвестно Но даже После включения этого индекса в файле можно увидеть такой список который означает что у вас все еще 10 блоков индексов как раз то чем я говорил в начале 10 тысяч элементов по умолчанию таблица на 100 тысяч элементов видим 10 блоков индексов тогда мы можем пойти и сделать этот блок индексов очень большим так чтобы он был больше таблицах и фактически физически мы отключим этот эту индексацию со средней таблицей Уже немного интереснее если мы говорим про нагрузку на кластер то простая сортировка таблицы позволяет нам в три раза уменьшить нагрузку на него то что вы тут видите нагрузку на кластер без сортировки 18 тысяч а сортировкой 6000 учитывайте то что неучтенные тут данные это нагрузка на оперативную память нагрузка на диске нагрузка в конце концов на сеть которая также есть вот в период всего задействования кластер то есть эти 18000 секунд также мы выбираем меньший блок данных диска то есть вместо того чтобы прочитать все блоки индексов потом проверить есть ли у них нужные нам данные мы выберем только один конкретный небольшой блок чем значительно ускоренного за счет чего происходит это тройное ускорение уменьшение нагрузки на цпу и немаловажный но небольшой бонус в том что мы сократим занимаем занимаемое место на диске какие можно сделать из этого выводы но во-первых индексы позволяют начать меньше данных таблиц поэтому для средних таблиц не стоит их удалять и сортировка значительно ускоряет запрос почему это происходит если мы берем все данные для прочтения мы вычитываем во-первых Весь вот этот блок данных метаданных потом мы смотрим те таблицы те страйпы в которых может потенциально находиться нужные нам значения и вынуждены прочитать его а потом уже отфильтровать забрав нужные нам значения Ну это на самом деле не имеет большого смысла в случае с сортировкой мы прочитаем ровно один Страйп в котором содержится ровно та информация которая нам нужна разумеется если у вас не один элемент а 100000 миллион и они будут расположены в разных но это все равно будет быстрее чем прочитать всю таблицу Спасибо это дате и наконец самый так и наконец Самое интересное это большая таблица больше 100 миллионов записей Если у вас есть это считается Большая таблица Объясню почему я привязываюсь к количеству записей в таблице либо партиции а не к их реальному объему Потому что от этого зависит настройка индексов То есть если у вас количество индексов становится слишком большим Вы очень сильно у увеличите нагрузку на кластер потому что вам необходимо будет прочитать эти индексы обратите особое внимание на параметр RT Ray Index stride 60 тысяч это как раз изменения блоков блока индексов для и также еще один важный параметр него мы поговорим в конце в случае с настройкой просто сортировкой мы можем получить даже ухудшение производительности при реально больших таблицах как у нас допустим 500 миллионов записей это происходит мы вынуждены Прочитать все вот эти вот огромные блоки индексов в случае с в данном случае их будет порядка 50 тысяч в случае если Мы настроим как примерим у меня до 60 тысяч у Вас могут быть другие цифры то мы в три раза сокращаем нагрузку на кластер на Что является значительным в случае с выбором элементов если мы не сортируем то мы выберем Весь блок но очевидно что если мы просто отсортировали и настроили мы выберем маленький блок 10000 элементов Ну и в случае с той настройки которая показывал 60 тысяч элементов и приятный бонус действительно приятный мы сократили место занимаемое на диске данной таблицы просто благодаря сортировке и настройки в два раза какие выводы можно из этого сделать во-первых сортировка она действительно важна это ключевой элемент вообще для всех таблиц неважно в России или паркет для их оптимизации то есть нужно понимать Какие данные вы храните как их будут использовать если вариантов использования много выбираете тот который будет использоваться чаще всего индекс является обязательными и они являются обязательными для настройки в случае если мы говорим про большую таблицу это важно потому что иначе вы ухудшить показатели и не раскрылись потенциал формата опять же почему это происходит если мы берем большую таблицу мы вынуждены прочитать ее сначала всю проверить те индексы в которые у нас потенциально входят нужные нам элемент потом его отсортировать ну и таким образом мы потенциально можем прочитать в данном случае 2 гигабайта данных случае если мы отсортируем мы уже прочитаем только один конкретный блок либо несколько либо несколько если они находятся в разных блоках Но из-за количество метаданных которые мы должны прочесть это 50 тысяч в нашем случае мы увеличиваем объем читаем его читаемого до 6,3 МБ И наконец случилось с настройкой так как блоки достаточно большие мы прочитаем ровно один блок и метаданных будет меньше что же такое он способен давать ложно положительный результат что позволяет нам гарантированно выбрать те значения которые нам не нужны Но в то же время он может выбрать и те значения которые нам не нужны тем не менее это позволяет опять уменьшить объем читаемого С Одесского уменьшить нагрузку на кластер пример если мы берем без Блум фильтра поля мы прочитаем оттуда 52 миллиона элементов Ну в нашем случае просто было и всего 52 миллиона случае если мы берем фильтр оттуда же мы выберем уже 2 миллиона элементов что довольно значительная разница Подводя итоги мы можем сказать то что у оси большая поддержка типов данных потому что у России изначально был предназначен именно для хайва и поддерживает все форматы которые есть скорость доступа в случае если мы настраиваем таблицу она будет выше чем у пакета в случае если мы не настраиваем но возможно то что вам и не надо И тогда зачем вам выбирать между Россией пакетом и наконец есть маленькие приятные бонусы такие как у пашкета поддержка спец символов допустим вы хотите назвать колонку собака стул вы это можете делать ворси не можете но оси есть во-первых поддержка это не та поддержка которая есть в реляционках но очень похоже в случае с орси когда мы хотим заменить какие-то данные в таблице он создает дополнительный файл который говорит что вот эта вот строчка теперь имеет такое это значение это очень удобно Если мы говорим про огромные таблицы чтобы их каждый раз не перезаписывать из-за необходимости поменять там 100 200 300 значений Вы можете добавлять таким образом оперативно до тех пор пока не решите что ну у нас там 300 дополнительных файлов они уже тормозят систему и просто перезаписать уже тогда все но до тех пор Вы можете накапливать эти изменения что будет полезно в случае с большими данными есть также несколько параметров которые мы не затронули в данном беседе конкретно фильтр он говорит о том как часто будет ошибаться Чем ниже значения тем реже он будет ошибаться но тем больше на это потребуется времени это как раз поддержка которой я говорил и два приятных бонуса для тех кто не может сделать два раздельно кластера или для кого У кого реально огромные объемы данных и переносить их между двумя кластерами для тестов это слишком долго проблематично можно задать маску и кодирование для конкретной колонки и только те люди у которых будет доступ к ключу смогут узнать какие данные там хранятся возвращаясь к начальной таблице мы сократили объем занимаем на диске в три раза мы ускорили обработку в 30 раз в случае с выбором единичных элементов в случае с группировкой в три раза в случае с парком не стоит значительный там двухкратный примерно но можно также заметить то что паркет за 4 по параметрам не сильно отстает от HT тут я не смогу вам показать Можете написать мне в личку и я вам пришлю для того чтобы достичь примерно таких же показателей Как настроенные в России пакету приходится тратить в два раза больше ресурсов кластера То есть если взять для орси миллион миллисекунд паркет в лз-4 требует 2 миллиона миллисекунд в принципе физическое время то же самое нагрузка на кластер в два раза больше Подводя итоги Ну во-первых голосуйте за мой доклад и пока голосуете можно подвести итоги неважно Используйте вы паштет используете данные в случае если вы используете RC подумайте о настройке размеров блоков таких как Stripe и блоки индексов колонок индексы у меня остается вопросом потому что индекс это не всегда хорошо как мы убедились на Большой таблице если их слишком много они оказывают огромный вред для для скорости обработки Ну и настроены лучше чем паркет на этом мой доклад закончен жду ваших вопросов слушай мира понимаю что такое структура данных а может не понимаем А сейчас мы это выясим на вопросах коллеги прежде мы приступаем к вопросам есть qr-код задавайте вопросы заполнять анкету дать нам обратную связь и он первый вопрос Здравствуйте Скажите пожалуйста можно получить код для голосования потому что не сканируется А все раздаточные материалы к вам придут на почту и там Все будет Спасибо большое Очень хочется отлично Попробуйте Добрый вечер Спасибо большое за проделанное исследование У меня на самом деле несколько вопросов первый это вот те слайды Когда вы показывали замер производительности вы не упомянули может быть я не услышал чем вы читали с парком или хайпом Если разница вот от нагрузки Каким средством вы обращаетесь к данным то есть парком Если разница это во-первых во-вторых хотелось бы узнать какая у вас версия с парка и Еще хотелось бы узнать Вот например я замечал такую историю что если мы работаем с третьим с парком у него есть опция выбрать более свежий формат паркета тестировали подобное что-то если разница самого свежего паркета с орком от 17 года доработкой Хороший вопрос мы тестировались на тех версиях которые у нас установлены потому что именно нам они были актуальны что касается спарка проверялась Spark 2 3 1 2 3 2 и 301 что касается еще версии с 3.2 для спавка был доработан оптимизатор обращение Поэтому если у вас парк 32 то вы получите еще большую скорость потому что там наконец-то нормально адекватно оптимизировали пушит фильтр вот Обращаемся мы соответственно смотрели туда и сюда получается Разницы нету вот те замеры производительности чем читать вам или из парка но вы говорите сейчас про миллисекунды которые я указывал миллисекунды которые я указывал движок для финальную таблицу давайте я откачу а уже нельзя откатить финальная таблица где у меня было куча значений с секундами Вы про неё но там мы же и хайпом читали Там было два разных блока вот я прославит где вот было то что в три раза например производительность возрастает если мы там применяем сортировку или индексы Вот это на чем тестировалась парке друзья Мне кажется это очень классная тема для дальнейшего обсуждения кулуарах либо онлайн а у нас следующий вопрос в случае с хайпом это те цифры которые вы видели в случае с парком есть прирост просто не такой значительный и у нас следующий вопрос Добрый день спасибо за доклад вопрос в паркете есть логические типы данных типа gson Map и там прочее вот орси Поддерживает ли логические типа там есть один или два Не совпадающих но 99 процентов форматов фарси покрывается формат паркета покрывается во все наоборот Нет еще один вопрос продолжаю тему по поводу спарку с парка есть такая фишка когда работает он с пакетами он может записывать файлы метаданных Камон метаданные Где будут просто футеры всех паркетов которые у него лежат директории по существу он может считать статистику всех паркетов и даже не читая весь скажем так большой пул этих Ну то есть файлов может определить допустим минимальное значение вот такое возможно Это хороший вопрос на самом деле я не смотрел но если говорить про таблицу которую мы используем там используется в любом случае вся эта дата всех файлов которые есть в конкретной таблице поэтому я так понимаю что этот функционал тоже обеспечен Ну мне кажется неплохая тема для следующих исследований следующих докладов следите за нашими анонсами А на следующий вопрос Здравствуйте У меня два вопроса Первый скрины для саморазвития моего оба формата складывают метаданные в футер Почему делать это в хедер есть вот такие-то причины потому что оба формата является не перезаписываемыми и если вы сначала Укажите фото а потом Запишите данные то Footer будет неправильный поэтому указывает всегда в конце понятно И второй вопрос насколько я знаю паркет также поддерживает фильтры бума и со спарками тоже вроде бы эффективность используется и это не так на самом деле возможно поддерживается какими-нибудь внешними сервисами Вот Но если говорить про внутренний настройку формата я такого не находил Спасибо за вопрос Следующий Ну да он остался последний вопрос Выбери лучший вопрос здесь они очень богатый выбор А есть еще вопрос Да можно тогда вдогонку он не совсем относится к России к паркету а пробовали ли вы работать с архивированными партийцами чтобы вот улучшить производительность и оптимизировать также в какой-то степени А что вы понимаете архивированными партициями То есть те данные которые там не используются из-за архивировать и потом например с парком обращаться к ним и не знаю как этим архивированным партийцам по hdfs пути но также как exip архиву допустим но фактически алгоритмы которые используются вот снапизипы прочие это уже является архивацией Когда вы сохраняете вы уже архивируете дополнительная архивация не знаю сверху ничего не даст а Следующий вопрос Прошу прощения еще один смотрите в паркете между колончаном реальными колонками Да может по существу находиться мусор который не будет то есть любой паркетный она бы он будет пропускаться и для чего сделан для того чтобы можем записать рок-группу и добить скажем так чтобы у нас поиск блоком лежали каждое один большой паркет Да но в каждом блоке лежала свою группу можем добить это еще мусором скажем так вот Поддерживает ли такое есть вообще данные по поводу версии Возможно ли так Допустим у нас очень большой файл Да и чтобы у нас жаба не читала кусок этого и кусочек этого да потому что они могут лежать на разных остальных блоки вот можно ли Ну то есть их также разбить так чтобы поделить ровно Да можно но также стоит обратить внимание на настройку Skype Вы можете указать размер страйпа равные размеру вашего блока на hdfs и таким образом файлы будут стремиться всегда размеры всегда будут стремиться к вашему размеру блока что позволяет не забивать Его мусором не занимать лишнее место а как это реализовано то есть мы же тоже не можем подогнать это байт в байт аж дефисные блоки должны быть Ну чётко но если говорить про байтбайт то Да конечно он будет добивать мусором если говорить не про байтбайта там не знаю Вы хотите 10 килобайт добить То лучше использовать хорошо Ну традиционный вопрос от ведущего вывели лучший вопрос и два два вопроса но мне понравился молодого человека и нигал лучше на пятом ряду да приз нашел своего героя Всем спасибо Мы закончили конференцию Я прошу выйти аплодировать ребята девчата сегодня"
}