{
  "video_id": "3tMrCIxynoA",
  "channel": "HighLoadChannel",
  "title": "«Очки верткальной реальности»: исправляем опечатки в поисковых запросах / Тигран Салуев (Joom)",
  "views": 967,
  "duration": 2939,
  "published": "2019-05-15T03:25:00-07:00",
  "text": "всем привет меня зовут тигран он уже чуть больше полугода и сейчас я расскажу немного про то как мы в joom сделали свой сервис по исправления опечаток и ошибок в поисковых запросах от пользователей ну сначала немножко водных какая у нас общая постановка задачи у нас есть сервис джон эта площадка для онлайн-торговля продавцы заходят через специальные перегружать нам свои товары пользователи заходят покупают эти товары продавец связывается с логистами они это все как-то доставляют и вот так то все работает у нас маленькая команда часто пример 60 разработчиков мы чуть чуть больше двух лет уже обрели я что касается поиска то на него у нас приходится примерно 40 процентов покупок ну то есть вот эти все сценарий когда поискал что-то кликнул выдачи товар и потом нажал купить вот эту последнюю со всеми остальными источниками вроде там лист они главные страницы какой-то рекламы фейсбуке это где-то 40 процентов ну и такой от колеблющийся доля в зависимости от всяких изменений которые мы выкатываем в поиске и на главной странице и в других местах которые связаны с покупками у нас где-то низко миллионов поисковых запросов в день и из этих поисковых запросов примерно на 10 процентов приходится пустые выдача прям ничего не нашлось грустная картинка ничего не нашлось поищите еще вы ищете по другому вот и это очень большое число 10 процентов пустых выдачи это звучит как кошмарно много на самом деле казалось бы хоть чем то можно найти да и соответственно мы конечно хотим это исправить ставим выдача это вообще довольно плохо само по себе почему потому что специфика нашей платформы такая что очень многие покупки пользователи совершают спонтанно листал рекомендации понравилась картинка купил ну если вы полистаете там бывают неожиданные классные вещи которые улучшают неожиданным образом ваша жизнь о которых вы обычно не знаете вот поэтому когда пользователь пришел и решил что-то поискать это уже априори какой-то более заинтересованный пользователь которого мы вообще . тим удовлетворить в том чего он ищет и кроме того видя пустой вы еще пользователь может лишить что у нас какой-то ограниченный ассортимент это то что мы регулярно видим в отзывах на play маркете например того нет сегодня от удаляю вот это печально все поэтому мы конечно хотим чтобы если у вас это есть там и уж точно человек нашли да и проблема в том что пользователь и они тоже не совершенны как и мы вот это я взял топ слов из запросов с пустыми выдачи мы собственно вот то есть реальные запросы от наших пользователей да просто ассасина вот соответственно нам захотелось какой-то мент сделайте сервис который бы исправлял это дело и заменял футболки на футболке которые может наш поисковый бэкенд перевалить ну мы конечно хотели бы ничего не делать потому что как известно если вы написали код то значит пошло не так но тут есть проблема если мы хотим например взять какой-то внешняя пик которая бы для нас то дьявола как мы сделали это например для всяких переводов описание товаров это плохая идея потому что у сервиса довольно специфический лексикон всякие разные тары которые у нас есть еще не попали в академический словарь и даже гугл не исправлял гироскутер на гироскутер как он это умеет возможно вы имели ввиду гироскутер когда я готовил эту презентацию также были какие-то попытки прикрутить существующее решение прям к поисковому движку до того как я пришел в команду ну и вроде как они не дали какого-то убедительного результат поэтому мы решили сделать свое и сейчас я вам немножко расскажу как это в принципе делается и что у нас получилось мы построим какую-то модель которая будет предполагать какую фразу пользователь имел ввиду когда заданном то что задал в этой модели будет параметр который мы понятно обучим по каким-то нашим данным которые у нас имеются что же касается по стране этой модели она зиждется на вот таких двух основных предположениях что пользователь пишет чаще правильно чем неправильно и совершают они ошибки чаще чем другие ну понятно если мы не предполагаем что пользователи чаще пишут правильно чем неправильно то скорее всего мы по большому счету не можем ничего сделать кроме как нанять 100 филологов чтобы они сделали нам словарь который бы обновлялся чтобы не следили за трендами в области того что продавцы выкладывают на платформу и в таком духе а и второе предположение пользователь ошибки чаще и другие ну это предположение которое позволит нам сделать относительно качественная модель ну мы увидим в общем почему это важно и почему это нам помогает модель такого рода обычно делятся на в 2 крупных компоненты это модель ошибок и модель языка на запросе вынесенным в название доклада я покажу как распределяется цветность между этими двумя компонентами это кстати тоже реальный запрос из логов мой любимый любимый потому что в нем есть сразу все виды проблем с которыми мы сталкиваемся тут есть орфографические ошибки да вот это оно и есть опечатки буква к стоит очень рядом с буквой u поэтому я предполагаю что пользовались всякий хотел виртуальную реальность не вертикальную собственно модель ошибок у нас занимается тем что для каждого слова входной фраза генерирует какой-то топ кандидатов типа какие слова пользователь теоретически мог иметь ввиду как-то транжира ванны но обычно на первом месте стоит исходное слово если она словарная да ну и дальше какие-то близкие слова по каким-то метрикам с которыми мы сейчас разберемся и когда она сделал это для всех слов которые есть у нас запросе модель языка выбирает какие же из них собрать фразу мы видим что вертикальная действительно тут ближе всего оно было бы ошибочно до взять ее в качестве ответа соответственно модель языка у нас знает какие 8 языка потому что знает немного нашем языке она знает какие фразы люди используют в нашем языке какие не используют какие слова там могут идти друг за другом а какие скорее всего не мог идти друг за другом и так далее собственно применённая вот к той нашей фразе она нам выдаст правильный ответ потому что вертикальный реальность никого не интересует по крайней мере среди наших пользователей по крайней мере 2018 году ну и сейчас я расскажу вкратце про то как устроены эти две модели а именно те их варианты которые мы взяли для нашего сервиса после какого-то и сердца модель ошибок подбирать нам кандидатов для каждого слова в том числе правильно написано во что подобрать кандидатов нужно откуда-то тихань изотов брать то есть нам нужен какой-то словарь я перед этим говорил что слово диета плохо потому что словари не успевают за нуждами нашего 50 специфического сервиса скольких словарях русского языка вы найдете слова гироскутер но тем не менее для работы модели ошибок конечно какой-то ну ладно мужем как вариант мы можем просто взять какой-то топ популярных слов из какого-то корпуса и у нас получится хоть набор слов который можно считать словарем это был такой шумный слали в нем будут основных слова которые нам нужны будут скорее всего популярны и ошибочные написания но это не страшно мы это порешаем на уровне модель языка с помощью предположения что правильный слова все-таки встречаются чаще чем неправильно будем считать суаре нас есть а теперь нам нужно для каждого слова найти кандидатов похожих на нему но самая простая директор приходит в голову это взять просто ближайший по левин штейн слова на всяк случай расстояние поле мишей но это количество элементарных операций men которые нужно сделать со словом что получить другое слово там удаление одной буквы добавление одной буквы или замена одной буквы на другую иногда добавляют еще обмен двух соседних букв местами но проблема такой на иные идеи в том что что она наивная до для слова колонка которая была на слайде ст популярной словами кандидаты колонка и калинка равна вероятное что наверное не то чего мы хотим поэтому мы взяли менее тривиальную модель модель брела moore это в каком-то смысле обобщенные расстояние palvin штейна сейчас я объясню суть в том что когда нам нужно прикинуть насколько различаются это два слова слова поступившие нам от пользователя и слова которые мы нашли в словаре мы бьем их на какие-то более короткие фрагменты скажем по 2 по 3 буквы всевозможными способами и читаем вероятности там первого фрагмента пользовательского слова перейти в первый фрагмент словарного слова 2 2 и так далее эти вероятности элементарных фрагментов перейти друг друга будут параметрами нашей модели который мы пытаемся обучить из наших исходных данных ну и потом мы все эти элементарные вероятности перемножаем у нас получается итоговая что это какая-то словно условная величина которую мы будем называть вероятностью того что пользователь написал там слова x имею ввиду слова y словарная ну давайте на примере посмотрим как то работает вот у нас есть написано из ошибки слова аксессуар с 1с и есть словарное слово аксессуар с как нам посчитать по модели бы его moore насколько они близки друг другу мы видим что по левенштейн ого расстояния единица да просто плюс минус одна буква s по модели брела моря мы должны разбить эти слова на равное количество квестов короткие фрагменты всевозможными способами давайте посмотрим пример такого избиения вот здесь нас 5 фрагментов все длины не больше трех букв и собственно величина посчитано помогли либо его мура вычитается по такой формуле эта вероятность того что пользователь имел ввиду фрагмент а как еда печатала напечатал а должны на вероятность того что пользователь хотел напечатать фрагмент часто писать окна писал к с и так далее ну и мы видим сразу что это какое-то не очень удачное разбиение да потому что вот эти три вероятности выглядят нас какие как очень маленькие часто ли вы пытаясь написать букву опишите sua наверное не очень но посколько берется максимум по всем избиением скорее всего итоговый результат будет равен вот этой величине с более удачным разделением обоих слов на фрагменты видите все буквы друг под другом эти все вероятности близких единицы это единственная вероятность кого-то среднего средней величины которая собственно и бум внесет основной вклад в итоговое число и соответственно это и будет скорее всего наш расстояние по модели брил амура и связи с этим у нас и дает два вопроса как эффективно вычислить в то что я описал и как эффективно найти вот этот топ кандидатов знаю что процедуру вычисления такая нетривиальное потому что она мы понимаем что если я говорю разбить слова на всевозможные фрагменты то вы сразу понимаете об экспоненциальная сложность это не то чтобы мы уснули в продакшен да но к счастью это разваливается к ритму динамического программирования относительно несложным очень похожим на алгоритм который вычисляется расстояние примешь тайну если кто знает там два слова между которыми мы хотим найти листы они выписываются в соответственно в столбцы и строки и мы заполняем вот такую таблиц где каждая ячейка нужно придавать вот эту ищейку посмотрим это то самое расстояние на между ними жду целиком этими словами между какими-то двумя префиксами например между префиксом че и префиксом chase ну вот например как бы мы заполняли вот эту ячейку предположение что предыдущие две строки уже заполнены эта ячейка это вероятность того что пользователь напечатал чья имею ввиду чип соответственно мы должны теперь разбивать эти два слова на фрагменты но мы бьем будем бить их только на два фрагмента например вот так и соответственно здесь будет вероятность слова уже короче примете другое слово короче который уже посчитано в нашей табличке а здесь будет вероятность холод а вот этого конечного фрагменты берите другой конечно фрагмент данном случае в пустую строку которая является параметром нашей модели мы их перемножаем перемножаем еще другой вариант разбиения снова уже вычисленная часов таблички параметр модели берем максимум и таким образом заполняем у таблицы таблицы теперь что касается необходимости многократно вычислить такую штуку для многих словарных слов это необходимость не так страшно потому что если мы теперь например захотим сюда подставить другое словарное слово похоже да это слово пользовательская не меняется здесь мы будем перебирать например свой чиж мы можем заметить что достаточно вычеркнуть только последнюю строку а предыдущие 2 оставить потому что тут ничего не изменилось и таким образом получается что на каждое словарное слово при грамотном порядке обхода нам нужно будет ну примерно одну строчку заполнять новую и это уже довольно быстро потому что словарных слов у нас обычно не так уж много раз мы говорим про европейский язык то наверное 100 тысяч более чем достаточно для практических необходимостей и того модель ошибок у нас выглядит так мы должны построить и словарных слов бор да это специальная структура данных которая позволит нам обходиться уарн is love вот в том самом грамотном порядке при котором нам надо редко выкидывать много строк до при котором мы в среднем будем как при каждом новом слове и давать одну строку и дописывать одну строку мы пробегаем по эти структуры данных прям честно по всем словам но возможно с каким-то мелкими оптимизация me если слово там совсем маленькая вероятность получилось то наверное слова которые еще длиннее уже не надо пробовать если она уже длине и исходного и при этом мы составляем топ кандидатов ну кучей например топ 100 топ 20 сколько вам нужно и сложность всего этого дела получается пропорциональные размеры словаря и длине входного слова размер словаря у нас константа на которую мы в принципе можем влиять но такая сравнить небольшая но длина слова обычно приемлемая если мы говорим про продакшен ход конечно стоит и ограничить и для совсем длинных слов ничего не делать и прелесть еще одно в том что конечно результаты всего это дело можно закрывать сохранить куда-нить freddy's и каждый раз заставать они будут актуальны вечность пока вы не решите переобучить всю модель целиком кстати об обучении нам осталось определить чему же равны вот те самые наши параметры модели вероятности перехода фрагмента во фрагмент вероятности того что пользователь хотел напечатать единый фрагмент напечатал другой для этого нам потребуется к это обучающее множество например подходе который мы реализовывали это был список пар правильное слово неправильное слово составить список пар такой это конечно непростое дело собственно это он будет иметь самое большое влияние на качество нашей модели вручную развейте достаточное количество слов тяжело есть вариант брать правильные слова из какого-то прям настоящего соловья русского языка и искусственно в него водите печатки но такое конечно вы сразу делайте вид что вы самый умный и знаете какие то вероятность у каких опечаток поэтому это наверное не очень интересно ну и простой подход когда у вас есть много данных вы можете просто взять корпус текстов для всех слов посчитать частотность и сказать у типа если у меня есть слово и к нему есть близкое по левенштейн у там достаточно там плюс минус одна буква например другое слово у которой в 10 раз менее вероятна та ну наверное одно из них это правильная 2 это опечатка потому что люди часто пишу тайные редко опечатываются если вы так сделать у вас получится довольно шумный datasette там будут там популярные прилагательные в то мужском и женском роде и менее популярный тип в то же самое но в среднем роде что не в таком духе но в целом будет работать в целом это можно прилично обучиться собственно когда у нас есть это множество пар каждой парой мы должны давайте посмотрим на пример пар который мы будем учитывать например если я хочу прикинуть вероятность переходов фрагмента ела фрагмент и то у меня есть вот такие релевантные пары которые я бы хотел иметь в своем обучающим дата сети и не релевантная пара специально добавил что вы обратили внимания порядок имеет значение что вы что переходит ну если хочу вычислить теперь еще вероятность чуть более длинных фрагментов то у меня остается еще меньше или ватный резидентных пар соответственно к дайте пару нас есть мы должны из них вытащить вот эти вот элементарные переходы фрагмента во фрагмент вот опять возвращаясь к аксессуару с аксессуаром мы хотим их как-то выпишут друг под другом так что максимальное количество букв соответствовал друг другу и только под ошибочными были там или другие буквы или пустые строки если буква пропущено это делается алгоритмом модификации алгоритмы для построения стояния примешь тайну в которой вы запоминаете как именно расставить буквы чтобы было минимальное расстояние и теперь когда у нас есть вот такая табличка такое соответствие мы уже видим как нам что учесть мы видим что а перешла в а капелюш лавка с пришла в пустую строку более длинные фрагменты тоже а к перешло в ксс пришла в с и так далее вплоть до любой длины фрагменты какой вы хотите мы делали до трех букв ну и соответственно это все то есть мы обучили мы реализовали знаем что как считать давайте посмотрим что получилось во сколько мы все все обучение построения словаря и обучение это проводили на ком-то корпусе текстов которые мы взяли с нашего сервиса в нашем случае на джума мы взяли поисковые запросы мы взяли комментарии и мы взяли тексты отзывов к товарам то конечно же наша модель выучила все наши специфические слова прекрасно она выучила их даже лучше меня если честно поднимите пожалуйста руки кто из вас знает как правильно писать куруми или канекалон прекрасно ч 5 я вижу вот кому интересно кигуруми это такая японская пижама во весь рост в виде кого нет животного или персонажа а канекалон это искусственные яркие разноцветные косички которые надо вплетать волосы чтобы вас были классные разноцветные косички и модель ошибок в принципе поняла чем-то делать это логарифмическая вероятность посчитано я по моделям зеленое это правильные ответы мы видим над ними в топе конечно и неправильные ответы потому что это довольно часты опечатки от пользователей и они попали в наш умный словарь но это не страшно потому что у нас есть еще модель языка в целом если у нас топ 5 или топ-10 входит наше правильное слово это очень хорошо это значит кресел о мы справимся и справимся правильно но минусы тоже есть а собственно как бы главная сила является же и главной слабостью это довольно специализированная модель если я возьму не популярные слова на joom а популярность салона наш с вами конференции то скорее всего я получу не очень убедительные результаты и ну собственно так оно и есть к сожалению пользователи не ищут на джунгли calls пока что вот ну что с этим можно поделать мы видим что логарифмические вероятности сразу стали диска меньше они были порядка четырех-пяти стали порядка 30 и ну идея которые напрашивается сама собой это ввести какой-то порог что если у нас нет кандидатов больше этого порога то значит не получилось не судьба совершенно работающая идея прямо так мы сделали и в модели языка тоже можно будет так сделать мы передём к в стык модели языка которая будет исправляется то что мы наворотили с шумными данными в предыдущей моделью в модели языка нам нужно оценивать уже целые фразы на сколько они похожи на правду насколько вероятно что пользователь хотел именно это то есть нам будет нужно вычитать какую величину которым будем называть условной вероятностью использования фразы собственно в целевом языке для которого мы обучаем это все дело в русском нашем случае ну и первое что здесь делается чтобы упростить это все это переписывается вероятность всей фразы видела такого произведений условных вероятностей и w&t от слова вот этой их конкатенации я обозначаю фразу соответственно это вероятность безусловная фразы то есть просто какая-то абсолютная вероятность того что фраза используется в нашем языке тепло что а я хотел сказать случайную фразу и сказал эту примерно так это условной вероятности до вероятность того что вы хотели сказать только одно слово вероятность того что вы хотели сказать второе слово плюс алвеша сказали 1 и так далее и следующим шагом это упрощается исходя из предположения что на вероятность к за какое-то определенное слово которое вы хотите сказать больше влияют slug 3 вы сказали недавно чем те которые сказали давно да вот например в нашей модели мы предположили что ну давайте просто читать что к последние два слова влияют короткая память алгоритма на слова но результате получается самым неплохи и сейчас я немножко скажу чуть позже почему нельзя просто взять и из идеалистических соображений . пусть 10 слов влияют пусть 2 что нам с этим делать вот эту условную вероятность мы можем легко оценить через встречаемость соответствующих фраз в тексте вот эту нас вероятность что вы скажете раму + a wish его уже сказали что мама мыла она довольно велика в русском языке ну мы можем взять об этике наш корпус посчитать в нем уже статистику по инграма мда по целым фразам в плоть до какой-то длины нашем случае нам нужно длину 3 и поделить сколько раз в этом языке вы говорили маму араму на сколько раз вы говорили мама мыла да и тут вылезает валидный подход хороший собственно но вылезает проблема что если вдруг в нашем корпусе нея не встречалось вот эта длинная фраза из трех слов то есть от равен нулю но правильно ли это можем ли мы какое-то число лучше вылить потому что если мы не можем нам нужно на какой-то бесконечный корпус текстов чтобы он не пропускал вот такие вот случаи ну и для решения этой проблемы используется довольно уже нет реальной техники сглаживания данных ну вот здесь несколько из них которые есть в литературе мы пробовали оба stepped быков это типа самый простой который можно придумать он работает только если у вас очень много данных ну как у гугла например в чьей статья это вели к ней зерна и смутно когда более state-of-the-art подход более сложный поэтому я не буду его рассказывать много формул много ну и нам осталось мы уже незаметно обучили модель языка да я в минус скорость мы взяли корпус посчитали встречаемости всех фраз и теперь мы умеем считать эти условные вероятности из них умеем составлять вероятность целой фразы осталось соединить это с моделью языка модель языка у нас как мы помним выдает нам еще на каждое отдельное слово вероятность того что вот этот к нему кандидатов исправления имелся в виду вот эти вероятность мы тоже не выкинем мы их учтём потому что они отображают то насколько вообще кандидат вот это слово кандидат далеко от пользовательского потому что наверно если пользователь ошибся в 10 буквах то даже из такая фраза очень вероятно и мы не хотим на нее исправлять до поэтому мы будем перемножать вероятность фразы на вероятности как отдельных слов и у нас получится вот какой то итоговый скоро фразы со всеми словами замененными на какие-то конкретные уже кандидат из словаря которые можем посчитать и в котором будем ранжировать ответы ну и тут есть такой подход что можно если вы считаете что рукоять из двух моделей performed лучше чем другая что она лучше обучилась как там просто у вас более удачно с ней получилось то можно соответствующим вероятность возвести cut положительную степень и тогда ее вклад будет больше в ответ и например например мы так делаем модели языка мы возводим в степень что-то типа 5 и у нас за счет этого немножко нивелируется вот эти эффекты шумные модели ошибок который выдает нам ошибочные слова и нам осталось теперь когда мы умеем считать score всей фразы со всеми словами замененными а словарные слова найти лучшую у нас для каждого слова есть сколько-то кандидатов но это обычно какая-то константа 1 ну штук 20 например если слов но вы-то всевозможных фраз можно составить ну 20 в степени количества слов это много но за счет того что мы упростили все до двух последних слов мы можем заметить что вот в этой формуле если я хочу максимизировать ее пока этому слову мне достаточно помнить знать только ираке предыдущие два были оптимальными да а предыдущие два у них всего там 20 в квадрате вариантов если у меня накажет вобла двадцать кандидатов поэтому вычисления максимум а вот этого сводится к еще одному греху динамического программирования сложность квадрат как где это то количество кандидатов то есть мы идем и каждый раз помним типа top н квадрат вариантов ну например мы начали мы хотим подобрать наилучшее первое слово но у нас n кандидатов и мы не знаем какой из них самый классный потому что может быть тот который в самом низу круто сочетается со вторым словом его нельзя выкидывать окей оставим их все вся н штук дальше хотим второе слово выбрать у нас есть n вариантов предыдущих слов инвариантов вторых слов н квадрат разных комбинаций и мы опять не знаем какой из них лучше всего оставить а вдруг какая-то комбинация которая все еще не очень с третьим словом круто за сочетается поэтому он помним всем квадрат и вот когда мы уже привези живым третье слово у нас получится м куб вариантов из них мы можем оставить только варианты с разными последними двумя словами то есть у нас будет кандидаты с разными последними двумя с вами и варьирующимися первым и вот по ним надо взять максимум и поэтому на каждом шаге у нас будет оставаться n квадратов раз кандидатов и алгоритм поэтому работает довольно эффективно в целом модель языка это ни разу не узкая часть самая узкая часть это модель ошибок которые мы продвигаемся по слою здесь все быстро классно можно не переживать ну и что касается того что у нас в итоге получилось первых как у нас в итоге получилось мы готовили данные напиточки в юпитере на выделенные машинки для mls большой оперативкой реализовали применение моделей на уже посчитанных с уже почтительным параметрами нога сделали джузеппе сервис на нескольких машинах в разных аил availability зонах нашего дата-центра навесили на это все кэширование в одессе вернее на модель ошибок потому что оно самое медленное ну и благодаря да поэтому каширование на стороне поиска по учили в целом средний фпс то есть в части архитектуры у нас сейчас нагрузка и нет никаких проблем и мы провели эксперимент до если на всякий случай мы выделили какие-то группы наших пользователей по моему по 10 процентов пользователей одни взяли за оба и своим другим подключили исправление опечаток вот этим сервисом и посмотрели что у нас улучшилась в группе в которой мы исправляем опечатки ну в ней стало в два раза меньше пустых выдачи пять процентов я считаю что хорошо оставшиеся 5 процентов это в основном даже не столько ошибки и опечатки сколько уже идут товары которых просто нет у нас на платформе например по всяким юридическим причинам денежные метрики джимми и revenue да это суммарная стоимость проданных товаров и наша суммарная выгода не изменились статистически значимо что несколько удивительно по крайней мере для большинства людей которым я показывал этот доклад но у нас улучшились you x метрики сократилось время для покупки после первых поискового запроса в принципе чуть-чуть сократилось количество поисковых запросов то есть пользователи стали находить то что они ищут быстрее и примерно 5 процентов джим вин до то есть пять процентов денег перетекло из запросов без ошибок популярных в запросы которые исправила печатник то есть реально мы видим что пользователи стали быстрее находить то что нужно покупать это примерно с такой же частотой но можно предположить что у нас преданные пользователи которые когда видят пустую выдачу думают как исправить запрос чтобы все-таки найти ну-ка вопрос о том насколько заинтересован пользователь который уже пошел в поиск ну и конечно я наверное должен сказать что на этом работа не закончена и что еще бы я сделал и хочу сделать и скорее всего сделаю конечно мы хотим сделать переобучение модели модель обучается на пользовательских данных которые упомянул запросы отзывы которые постоянно появляются новые в довольно большом количестве модель на них переобучать было бы хорошо ну во первых потому что есть надежда что у нее станет лучше качество авторы потому что мы рискуем пропустить какие-то тренды очередной канекалон про который мы ничего не знаем о пользователе знают поэтому в идеале конечно хорошо бы организовать регулярный процесс для переобучения это сложно это нужно делать валидацию куют нетривиальную но нужно далее мы конечно хотели бы сделать модель на других языках это сейчас буду только русский а у нас много языков у нас много покупателей в европе и много выхватили в таиланде тайский язык это совершенное чудо для русского глаза там даже нет пробелов в том понимании в каком они скромно из нас это все поэтому очень интересно еще я бы хотел конечно попробовать более продвинутые алгоритмы сглаживание потому что те самые статистики использования фраз корпусе которые о которых я говорил у нас получились довольно шумными потому что корпус все-таки не гигантский все таки это не google кстати именно по этой причине я не могу просто взять и вместо трех слов использовать например 5 потому что данный попить ислам будут супер шумные и скорее всего все станет только хуже и конечно недостаточно research эй я успел сделать на тему классификации исправлять не исправлять я говорил что сейчас у нас есть просто порог если фраза кандидаты ведь вероятность выше этого порога то мы ее берем если ниже то нет не получилось исправить оставляем что пользователь дал а вообще это довольно серьезная тема в статье специалистов из google описывается целый классификатор для каждого слова решающий надо его исправить или нет обучающийся причём на каких-то печах гугла невероятных которых у меня нет поэтому эту сторону конечно хорошо было бы копнуть но и если вы можете еще что-то предложить то пожалуйста сделайте это как только закончу доклад это почти сейчас значит ну несколько статей на по теме из которых значительно имели были было подчеркнуто бы подчеркнуть эти подходы которые мы использовали презентация будет вложено вы сможете посмотреть их там спасибо за внимание и одну секундочку традиционный харренхолл мы нанимаем бы киндеров и мальчиков мобильных разработчиков подходите к нам на стенд общайтесь с нашим head of recruitment который там юридически дежурит привет спасибо за доклад есть несколько вопросов первое это про модель ошибок и генерации кандидатов к моделям ошибок я так понял что достаточно трудоемкий процесс у вас и почему вы не использовали какие нить механики алгоритмы виде sims пел который базируется на том же левенштейн и у него подход заключается в том что мы используем только удалению слов за счет эту генерация кандидатов происходит намного быстрее слов или букв a book вот это первым то есть почему именно им или вот в этой части то есть там в принципе уже можно посчитать вероятности перехода одной буквы в другой просто имея корпус и второй вопрос это как вы боритесь с такими ситуациями когда у меня есть фраза vr фраза корм для кошек вероятность слова кнuгa меня предположим что в одном не за этом слое кошек есть ошибка да вот и у меня вероятность слова корм для и для кошек она будет очень сильно смазав за счет слова для потому что для практически ну со множеством слов о но прекрасно сочетается то есть это такие предлоги частицы которые not просто сохраненным количеством слов сочетаются и у меня например будет кандидат ножек и у меня получится корм для ножек ну потому что просто слова ножек встречается в корпусе чаще чем слова кошек манят непонятно может и служит я посвящаю отвечу но сначала насчет первого вопроса но этот подход который я использовал тут он считается ну понятно вольно таким софистика этот но он больше может выучить и гипотетически я могу получить днем лучше точность 700 сейчас не пробовал вот конечно было бы интересно но вообще считается что вот за счет именно того что он перебирает все кандидаты но относительно быстро и умеет вот целые фрагменты учить теоретически он должен приводить к лучшим результатам например если мы исправляем незабвенная вся вся та пропаже мягкого знака во всем языки от наверно не очень вероятно исправление отца вся более вероятно то есть там примерно такое соображение почему надо делать так не так понял что вот тот край который нам ну на выходе получается по которому генерится кандидаты там в каждой ну то есть у меня есть исходное слово и я гинер у с учетом типа вхожу в это слово эту поэтому троих генерит с учетом вероятности перехода из одного состояния в другое правильно и таким образом набираем кандидатов то есть вот так таким способом но ведь это же ну то есть те же вероятность можно на тот же simple переложить это будет несколько раз быстрее может быть а не пробовал понятно и вообще второй вопрос насчет резко кошек ну там модели построен так что теоретически это все компенсируется то есть на самом деле если мы говорим про для для ножек я не думаю что мы сильно более вероятная фраза чем для кошек это просто конкретный пример ну то есть любой практически предлог это он очень высокочастотным словарь если мы разбираем фразу например на биграмм из лода но к счастью у нас не так уж много осмысленных слов которые похожи на предлоги в русском языке окей ладно спасибо так я помню спасибо за доклад интересно как вы боретесь со ступенями короче если поискать на джуме золотое кольцо но он подмешивает очевидно золотой и кальций вот какой трек эти такую тему типа из быстро фиксить и как и как типа как это фиксить нормально но мы не быстро фиксе мм и не выслеживаем кто там что вел в поисковых запросах да и не сталкиваем вас на эту тему просто пользователи не так уж часто на самом деле сшиваются ты где-то один процент запросов очень сильно смазывает распределение запросов с adjust который правильное пользователи имеют удачи кликать на него поэтому вот такие запросы они имеют не очень большое значение на веру джим vii какие-то метрики да то есть мы стараемся решить эту проблему системно ну вот конкретные запросы не трогаем кольцо в кальции это потому что он пробует поискать наверное еще папе воду на английский язык по описанием товаров на английском языке и видимо переводчик он кольцо превращает кальций ну короче типа качество вы особо не трогайте да получается вы не надеетесь на статистику номер рекламы и треком его но я же говорю вот эти запросы с ошибками они не очень чистые поэтому она на самом деле не так сильно влияет у нас есть проблемы с качеством на безошибочных запросах и мы с ними тоже боремся понял еще сильны и лиза affective это перформанс или не особо поскольку везде все кэшируется performance не очень и сколько стоила по времени разработки все это дело но это был месяц работы меня от начала research а до б тест понял спасибо так спасибо доклад огонь интересная реализация spell checker а у меня вопрос как вы проводите от таки не за цию изначальные поисковой фразы там же может быть много нюансов там разбивать по точкам не разбивать всякие особые знаки препинания специфика в общем как вы это делаете вам во всех этих деталях я стеснялась и где также не помню да там сделаны какие то какие то вот такие мелкие вещи сколько команд количестве реализованы но я не могу сказать что здесь как системное идея по которые я сделал там я не понимаю как ответить кроме как он показать интересно считаете ли вы фолз позитив срабатывания вашего с получать до какой процент но чтобы когда это все выехала у нас не поехали запросы которые нормально работают мы собрали несколько вручную сетов для проверки мы собрали из топа запросов пустыми и результатами из топа запросов по деньгам то есть хороших и собрали просто случайную выборку из всех запросов вообще ну и на хороших запросов из ошибочных там черти в 99 чем-то процентов получилось точность то есть там все хорошо на запросах которые без покупок что-то типа по моему 90 какая такая цифра но поскольку там если что-то написано правильно она результатов то это что-нибудь чего нас нет мы не стали сильно переживать и последний вопрос в вашей модели вы используете модель языка и модель ошибок и соответственно получаете две вероятности и перемножаете их и у них есть свои леса как вы высчитывать эти леса эмпирически спасибо спасибо за доклад есть такой вопрос и если будет появиться продавец который начнет продавать скажем очки вертикальной реальность которые не укладывается вашу модель языка можно ли будет его найти попадет ли на выдохе ну его можно будет найти по очки реальности скорее всего то есть по полной фразе он не попадет но у него будет не очень высокий и скорее всего ранг из поискового движка потому что средняя слова у него не будет потому что и справиться в не то но как бы это мало говорят еще один вопрос используете ли вы исправление ошибок для продавцов то есть если в самом объявление будет ошибка нет нет операции у нас есть премодерация да на все товары они проходят ручное одобрение ещё сколько-то этапов каких-то автоматических проверок название там по-английски на английском и они приводятся авто переводчиком на все остальные языки для пользователей и автоперевозчик там в общество в значительной мере делать нормально"
}