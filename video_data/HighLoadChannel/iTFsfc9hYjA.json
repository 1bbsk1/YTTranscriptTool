{
  "video_id": "iTFsfc9hYjA",
  "channel": "HighLoadChannel",
  "title": "API AppMetrica изнутри, или SQL без SQL'я / Ефим Пышнограев (AppMetrica, Яндекс)",
  "views": 1268,
  "duration": 2490,
  "published": "2018-01-16T13:11:36-08:00",
  "text": "спасибо что пришли меня зовут ефим я работаю в яндексе в команде джавы и сегодня я расскажу о том как устроена и 5 в сервисе от метрика цель моего доклада во-первых я хотел бы рассказать целиком о сервисе какие у нас есть команды какие компоненты как они взаимодействуют какие перед нами стоят задачи затем более подробно потом чем именно я занимаюсь занимаюсь построением отчетов мы используем для этого crack house и я покажу какие задачи у нас появляются какие проблемы как их можно решать и наконец потом как устроен наш reporting ай пи ай это внутренний формат для того чтобы пользователь мог построить нужный отчет пожалуйста поднимите руки те кто слышал об от метрики почти все слышали отлично все равно пара слов об этом мы это систем мобильной аналитики у нас в минске есть команда из детей которая делает мобильную библиотеку автор приложение может строить ее к себе затем он выкладывает это приложение в google play или appstore пользователи им пользуются есть дикие автоматически отправляются события на наш backend мы эти события сохраняем как-то обрабатываем и затем разработчик может строить графики отчеты по самому приложению для того чтобы он лучше понимал аудиторию например он может узнать из каких городов его пользователей какими устройствами они пользуются к у них размер экранов и так далее мы существуем уже более 4 лет изначально мы образовались как ответвление от яндекс метрики поэтому многие архитектурные решения у нас похожи и все то что я рассказываю в какой-то степени применимо к яндекс метрики тоже мы создавались для того чтобы помогать приложением индекса но довольно быстро у нас появились внешние клиенты сейчас это основное наше направление на сегодняшний день у нас более 4 тысяч активных приложений это те приложения которые отправляют достаточно большое количество данных мы установлены более чем 250 миллионов устройств эти устройства каждые сутки присылают нам более чем 12 миллиардов событий здесь схематично изображена архитектура нашего сервиса у сервисы две точки входа это пользователи которые отправляют события и разработчики которые строят потом графики по этим событиям когда пользователь отправляет события то она поступает на один из наших багандов backend написан на си плюс плюс здесь он обозначен как и дрон ядро принимает все события и пишет их наше хранилище как-то их обрабатывает для того чтобы потом их было удобно использовать и когда разработчик хочет построить отчет то он обращается к нашему вы банкир фейсу либо напрямую через и pr к нашему java бэг-энда и строят конкретный отчет становимся на том как можно хранить данные для того чтобы затем строить отчеты у систем аналитики есть два классических подхода к этому вопросу сохранения уже с агрегированных данных и хранения сырых данных здесь видно что у нас есть два бэг-энда которые работают с событиями агрегацию на самом деле можно делать на любом из них если бы мы делали агрегацию в ядре то мы бы хранили уже с агрегированные данные в хранилище если бы мы делали агрегацию на жалобы candy это значит что мы работаем с сырыми данными хорошего первом подходе то что отчет строится тривиально у нас все данные есть хранилище они уже с агрегированные достаточно их просто взять и показать пользователям и при этом если у нас отчетов не очень много и все эти отчеты не сильно кастомизируемый то этот подход будет занимать еще немного места хранили еще но здесь была довольно сильное предположение о том что отчетов немного они не сильно кастомизируемый для нас это совсем не так основная наша задача это построение отчетов по этому мы не можем ограничивать их число поэтому мы используем второй подход это хранении сырых данных как вы уже заметили мы используем базу данных клик house мы агрегирует данные на лету это хорошо тем что мы никак не ограничиваем себя набором отчетов у нас есть все нужные данные достаточно их просто взять и построить то что нам надо но здесь появляется довольно серьёзная задача разработчик когда строит отчет он не готов ждать минуты или часы он хочет получать ответ быстро за секунды поэтому нам нужен инструмент который позволяет это делать частью такой инструмент есть это база данных crack house она была написана яндекс метрикой для аналитических задач задача не odex метрики практически полностью такие же как у нас поэтому этот инструмент хорошо нам подходит после того как клик house был написан в метрике использовался там у нас он довольно долгое время использовался в других сервисах индексом где то год назад он попал в open source мне интересно кто из вас использует клик роуз продакшене пожалуйста поднимите руки не так много людей я надеюсь после этого доклада вы захотите попробовать применить его для ваших задач клика us у нас работает в кластере у нас 48 машин на этих машинах находятся почти 500 терабайт сжатых данных также у нас есть помогать или бизнес информация в маске это словари и разные другие вещи для того чтобы строить отчеты конечно событиям и не склады в майский поэтому его размер гораздо меньше кластер клик house у нас устроен следующим образом во первых каждый хост реплицировать на другой хост в другом да это центре это сделано для того чтобы мы могли без потерь переживает смерть одного дата-центра все наши машины в хаусе разбиты на множество партиции это сделано для того чтобы хранить данные одного приложения только в пределах одной партиции и тогда запрос к одному приложению не будет нагружать все серверы в кластере клик house изначально задумывался для работы в кластере поэтому у него есть хорошие встроенные средства для работы с распределенными таблицами внизу видно что на уровне sql запрос в локальную таблицу и запросов распределенную таблицу ничем не отличаются они отличаются только на том уровне как мы создали эти таблицы здесь табличка events это таблица с движком мер 3 это локальная таблица по таблице иван иван спарте шин это таблица с движком дистрибьютер она распределена по нескольким хастам и когда crack house получает запрос в эту таблицу то он сам автоматически знает куда ему нужно идти за данными пример нашей таблице у нас много таблиц на все они выглядят примерно одинаково это классические olap таблицы них очень много столбцов порядка 140 в каждой таблице мы храним дату события мы храним айди приложение которое прислала события дата события необходимо потому что мы используем движок нож 303 обязательно работает с датой в первичном ключе потому что потом он схлопывается разные партиции иди приложение тоже нужно первичном ключе потому что все запросы которые к нам поступают они все к одному приложению дальше в таблице есть девайса иди это идентификатор устройства которая прислала события это дивой среди генерируется ядром когда он впервые встречает событию от какого-то устройства то он случайно назначают ему девайса иди при этом этот девайс аиде должен быть равномерно распределен и почему это важно дальше будет понятно затем в таблице есть очень много столбцов и the payload события это те данные по которым фактически мы строим отчеты за те несколько лет что мы существуем мы прошли достаточно большой путь потому как мы храним данные изначально мы создавались инженерами яндекс метрики и яндекс метрики большинство отчетов строится всего по двум табличкам эта табличка хитов и табличка визитов поэтому изначально у нас была всего одна таблица events все события прилетали в эту таблицу и различить их можно было только по значению столбца eventtype иван type не входил в первичный ключ включался поэтому такой подход работал плохо события у нас неравномерные и разные типы событий приходят совершенно разной частотой например установок гораздо меньше чем клиентских событий и поэтому раньше когда мы хотели построить отчет по установкам нам приходилось выгружать с диска всю таблицу хотя это было не нужно довольно быстро мы от этого избавились сейчас у нас такой подход что для каждого типа событий мы делаем отдельную таблицу это делается для того чтобы отчет только по этому типу события можно было строить только по этой таблице ничего лишнего не загружать типы событий их всего порядка 15 самые распространенные это общие события например star цехе первый запуск приложения окончания сессии ведь я события которые у всех приложений общее есть клиентские события это события который шлет разработчик приложения для каждого приложения не свои например у приложение кант приложение карт клиентские события это поиск написание комментария просмотр информации и так далее есть события атрибуции это клики установки они нужны для того чтобы анализировать эффективность рекламных кампаний есть события потом что приложение приходят push-уведомления это тоже необходимо для того чтобы анализировать эффективность push компании разработчик строят отчет о своем приложении он делает это либо через веб интерфейс либо напрямую и здесь перед нами стоит задача в каком формате нужно описать отчет для того чтобы с одной стороны пользователь мог работать в терминах бизнес-логики но с другой стороны это дед этот язык был достаточно мощным для того чтобы можно было построить почти любой отчет этим языком должен пользоваться наш интерфейс и внешние клиенты для того чтобы просто не поддерживает две разные реализации одного и того же этот язык должен быть как я говорил достаточно гибким при этом он должен закрывать сложность реального из келли он должен скрывать нашу модель данных для того чтобы мы могли что-то поменять для того чтобы у нас было пространство для маневра и этот язык должен быть устроен таким образом чтобы запросы флик house можно было делать оптимальными все запросы у нас сейчас крутятся в одном кластере и в этом кластере работает много пользователей поэтому мы должны использовать клик aws по назначению для того чтобы пользователь не страдали из-за того что кто-то другой запускает не оптимальные запросы напомню что crack house это аналитическая база данных и правильное использование crack house и это когда мы строим отчет по очень большому числу данных но при этом в результате мы получаем небольшой отчет этот язык называется reporting ай пи ай он пришел к нам тоже из яндекс метрики он был разработан давно мы его успешно используем он почти не отличается от того что использует метрика сайтов только в каких-то мелочах и taipei крутятся на машинах она написана на джаве у нас шесть машин обрабатывает эти запросы они находятся в 3 дата центрах эти диман они хранят состоянии поэтому реплицируются тривиально мы посчитали что в сутки мы получаем около 200 тысяч запросов при этом каждый запрос достаточно большой он обрабатывает в среднем 100 миллионов строк или хаусе какие бывают отчеты для того чтобы было дальше понятно о чем речь здесь нарисована количество пользователей приложения с операционными системами ios и android с февраля по май этого года можно заметить что почти во всех отчетах есть какая-то величина метрика которая интересует менеджера или разработчикам здесь на количество пользователей эту метрику мы разбиваем по измерениям здесь измерения это месяц и операционная система для каждой пары месяц операционная система у нас есть один столбец и мы строим отчет всегда для какого-то сегмента пользователей в данном случае это пользователи конкретно этого приложения только с февраля по май и пользователи только ios и android дальше например я покажу как у нас устроен reporting и пьянь сверху есть параметры ешьте типе запроса снизу реальные запросы которые уходят в клик house я буду показывать это на примерах от самого простого к более сложным а затем я расскажу как на самом деле crack house исполняет те запросы которые мы ему присылаем и что нужно сделать чтобы он исполнял их лучше самый простой отчет который можно построить это отчет всего из одного числа количество пользователей здесь есть параметры это промежуток дат параметры аиде приложения эти три параметра у нас всегда будут для любого отчета наших таблицах первичный ключ построен по этим трем параметрам для того чтобы как раз не читать всю таблицу с диска читает только одно небольшой части и здесь есть еще один параметр это метрика которая нас интересует метрика количество пользователей метрика состоит здесь из двух частей то что слева : the namespace это то к чему мы делаем запрос а справа это сама метрика то что именно мы хотим посчитать space в самом простом случае это просто одна табличка в хаосе но на самом деле это может быть любое публичное выражение это может быть join 2 табличных выражений union под запрос другой namespace но здесь я буду везде показывает таблицы просто потому что иначе запросы не поместится на слайде можно увидеть как эти параметры переходят в sql это тривиально здесь метрика попадает в секцию select это просто агрегатная функция от какого-то столбца в frome мы идем в таблицу иван спарте шин потому что нам нужно делать запрос в namespace events мы используем при этом распределенную таблицы потому что все данные у нас лежат в кластере эффекта у в пробрасываю ца условия на даты условия на эти приложения чуть более сложно это 2 метрики метрика не должна быть одна за один запрос мы можем вычислить произвольное число метрик их можно перечислять через запятую единственное условие то что все метрики должны быть заданы им space а потому что в общем случае мы не можем вычислить за один проход по данным метрики из разных namespace а здесь этот запрос отличается от предыдущего только тем что секцию select добавилась еще одна агрегатная функция отчеты у которых есть только одно или два числа в результате это не очень интересно всегда разбиваем метрику по каким-то измерением для этого в нашей pr добавляется поле дименшенс здесь мы можем тоже через запятую перечислить те палят по которым затем отчет будет разбит успели это тоже выглядит просто диман шин это на самом деле просто какой-то столбец или просто выражение по этому столбцу низ агрегированные которое попадает в секции select затем она попадает в секцию грубой лекция в точно такая же сверху я больше не пишу иди приложение да ты просто потому что они везде одинаковые и последнее что нужно добавить в форматы пьянь чтобы он стал достаточно выразительным это фильтры возможность обозначить сегмент данных которые нам нужен фильтры имеют более сложный формат фильтры может быть написано любое булево и выражение с операторами сравнения с операторами and our при этом можно использовать скобки этот запрос от предыдущего отличается тем что в секцию b у нас про бросилась условия на a platform это как раз то что написано фильтре в этих четырех примерах мы обсудили самые простые запросы все эти запросы идут в одно распределенную таблицу давайте посмотрим как клик house эти запросы исполняют на самом деле как я говорил наш класс стр разбит на партиции и приложение живет внутри только 1 партиции поэтому когда наш букет видит что нужно построить отчет по приложению среди 46 он знает в какую именно партицию ему нужно идти он выбирает оттуда один произвольный хост и отправляет туда весь запрос этот хвост видит что запрос в распределенную таблицу поэтому он знает какие хасты входят в эту партицию и каждому из них он отправляет такой же запрос только в локальную таблицу все эти хасты используя свои локальные данные считают частичные агрегаты возвращают их обратно на 1 хост 1 хост берет все эти частичные агрегатные состояния склеивает в одном и это и есть тот результат который нас интересует этот результат возвращается обратно на backend запросы в одну распределенную таблицу это то что крюка us делает очень хорошо и очень быстро здесь видно что большинство работы это то что хост делает по своим локальным данным хасты по сети не передают много информации поэтому это очень хорошо масштабируется когда мы добавляем хост партицию то мы гарантированно делаем время исполнения быстрее потому что просто больших ростов обрабатывать свои локальные данные и это те запросы к которым мы хотим стремится они очень просты и но минус в том что далеко не все отчеты можно построить только по одной таблице как раз сложно отчеты всегда строятся по результатам нескольких таблиц давайте посмотрим как это выглядит на простом примере разработчик хочет построить отчет по пользователям которые получили push рассылку пошла силки живут у нас в одной распределенной таблицы события в другой таблицы нам эти события и push-уведомления нужно каким-то образом соединить для этого нужно добавить в наш формат фильтров конструкцию кризис и так вандер существование который в котором можно указать где именно мы хотим существование какого столбца и какое условие мы хотим что было выполнено здесь фильтры мы хотим чтобы существовало устройство в пуш компаниях на которая пришла определенная компания black пройди как раз то что написано в секции фильтров можно однозначно перевести в реальные сквер запрос если посмотреть на запрос видно что в нем появляется под запрос другую таблицу в секции а все остальное тоже самое мы метрики измерения берем по той же самой таблице но теперь у нас добавляется еще условия на то что девайс айди находится в результате под запроса в таблицу распределенную push компании и для этих push компания нам важно чтобы она была с определенным идентификатором здесь можно видеть что виде под запроса и мы используем именно глобальную часть global in global in используется всегда когда нам нужно в общем случае пройтись по всему кластеру здесь у нас пока нет никаких предположений о том как располагаются данные поэтому чтобы запрос выдавал правильный результат нам нужно делать под запрос именно глобальным теперь посмотрим как crack house будет его исполнять будет ли это отличаться от прошлого плана исполнения да это будет отличаться теперь это будет занимать не четыре этапа шесть этапов и некоторые этапы помечены жирными стрелками первый этап точно такой же мы отправляем запрос на произвольный хост только 1 партиции затем этот хвост видит что в этом запросе есть под запрос в распределенную таблицу то что там есть global in после этого он берет часть которая находится в скобках после global и на наших под запрос и это под запрос отправляет на все другие хасты этой партиции те другие хасты после in вокальным данным считают все девайсы идеи которые получили эту рассылку и отправляют обратно стрелочкой когда они отправляют эти данные обратно помечено жирным шрифтом это сделано потому что этот набор одевайся иди в общем случае может быть произвольно большим если у нас в результате под запросы получается много идентификаторов то позитив будет передаваться много данных от каждой машины к хосту который инициировал запрос более того результат global и ну и просто и на у нас должен помещаться в память на одной машине об этом тоже нужно помнить не всегда это условие можно выполнить далее 1 хоз получает частичные списке девайса иди в разных других местах он склеивает их в один затем берет весь запрос внешней и отправляет опять на всех остыну вместе с ними отправляет еще тот набор девайса эдик который у него получился после склеивания всех ответов от других ростов эта стрелка тоже помещена жирной потому что он отправляет много данных он отправляет список всех девайс 1 какую-то си реализованную хэш-таблицу всех аст и получают это они теперь проходятся по всем своим локальным данным и когда они фильтруют девайс по принадлежности push компании они теперь используют ту таблицу которую им передали они обратно возвращают частичный результат 1 хоз затем агрегирует все эти частичные результаты в один и так получается окончательный ответ который потом приходит на backend это исполняется сложнее чем запрос в одну таблицу и теперь по сети мы передаем в некоторых случаях много данных поэтому здесь нельзя с уверенностью сказать что добавление 1 х 100 в партицию всегда улучшит дело здесь нужно смотреть на конкретные запросы на данные и следить за тем чтобы мы не загружались эти но с другой стороны теперь мы можем использовать произвольное число событий в отчетах мы можем эти квартиры существования делать вложенными можем делать их несколько и так можно выразить практически любой разумный отсчет который разработчик или маркетолог захочет построить что делать если глобальные под запросы делать не хочется например если не хочется загружать сеть но при этом нужно обязательно делать запросы в другую таблицу можно добиться того чтобы данные у нас были локальной по девайса иди в этом примере мы делали под запрос и соединяли результаты по девайса иди если у нас каждый девайсы где будет находиться только на одном посте и мы будем это гарантировать то глобальный под запрос который был раньше можно заменить на локальный под запрос здесь из global и на исчезла ключевое слово global и вместо того чтобы идти в табличку пушкам point спарте шин мы теперь идем просто в табличку пушкам принц локальную это будет работать точно также как и запрос в одну распределенную таблицу потому что здесь действительно 1 распределенная таблица каждый хост получит этот запрос и когда он будет находить девайсы во внешнем запросе и фильтровать их внутренним то мы гарантируем что девайс живет только на одном посте поэтому ему всегда достаточно рассматривает только свои данные поэтому в некоторых случаях если у нас часто есть под запросы в кли krause и идти под запросы идут исключительно по девайса иди то имеет смысл добиться локальности данных по девайс иди если бы мы делали под запросы по другим столбцам то в общем случае конечно же нам пришлось бы все равно писать global и на использовать распределенную таблицу но если мы используем свойство локальности данных то это свойство нужно еще и поддерживать поэтому при добавлении новых ростов при удалении их ростов нам нужно гарантировать то что это свойство локальности не нарушится в общем случае это сложно в частности поэтому у нас свойство локальности наших данных нет про распределенные запросы я рассказал теперь про сэмплирование crack house поддерживает сэмплирование зачем она нужна что происходит если какое-то большое приложение хочет построить отчет за большой период времени если у нас есть очень много событий это значит что нам не хватит никаких мощностей для того чтобы все эти события посчитать из агрегировать для этого в клика усе есть ключевое слово сэмпл и все таблички с движком мер 3 при создании поддерживают сэмплирование для сэмплирования нам нужно просто при создании таблицы указать ключ по которым будет происходить сэмплирование у нас сэмплирование идет по девайса едим и как раз когда я рассказывал о том что девайс 1 у нас распределены равномерно это важно как раз для сэмплирования для того чтобы мы могли просмотреть и только часть данных и распределение по части данных была статистически достоверным если это не равномерно распределенная иди то конечно сэмплирование делать нельзя ниже видно что запрос с центрированием запрос без сэмплирования похоже они отличаются только секции сэмпл и тем что агрегатные функции нам потом нужно не забыть умножить вернее разделить на k и всем центрирования для того чтобы скорректировать то тот факт что мы проходимся только по части данным сэмплирование работает примерно так crack house проходиться по всем строкам и он рассматривает только те из них если мы пишем сэмпл 1 3 то он рассматривает только те строки у которых остаток от деления дивой сайте на 3 равен нулю но если мы просто в напрямую поставим условие надевай среди это будет не то же самое что использование сэмплирования потому что cly krause когда он располагает на диске табличку с центрированием то он начали располагает данные в порядке первичного ключа затем он располагает данные в порядке ключа сэмплирования таким образом когда мы проходим ся только по 3 девайса иди здесь это видно на нижней картинке мы не должны теперь читать произвольные места с диска нам достаточно брать только те области которые соответствуют нужным девайса иди конечно это работает гораздо быстрее в случае жестких дисков поэтому всегда лучше использовать сэмплирования напрямую если у нас есть запросы в несколько таблиц мы как-то соединяем их через секцию in или через секцию join то все становится сложнее для того чтобы это работало корректно то в общем случае лучше всего добиться того чтобы ключи сэмплирования в двух таблицах которые мы соединяем были одинаковые и чтобы этот ключ центрированием был однозначно привязан к тому столбцов по которому мы соединяем таблицы это важно для того чтобы внешний запрос и внутренней запрос проходились по одним и тем же выборкам девайса идей для того чтобы эти две части запроса были консистентной если это не так то мы будем данные потому что внешний запрос будет проходиться по тем данным который сэмплирование отсекает во внутреннем поэтому нужно здесь быть очень осторожным если например мы бы соединяли таблице не только по девайса иди а еще по какому-то идентификатору не связанному с девайса иди и в обоих случаях если бы мы хотели использовать сэмплирования то в общем случае нельзя выбрать такое ключ сэмплирование который позволит использовать это корректно в обоих случая поэтому в одном из случаев нам пришлось бы пожертвовать нашей производительностью и сэмплирование не использовать сэмплирование сама по себе не очень интересно нашей 5 позволяет использовать адаптивное сэмплирование для чего она нужна когда пользователь строит отчеты он строит много разных отчетов он может построить отчет по большому периоду времени по маленькому периоду и каждый раз для того чтобы это занимало разумное время ему нужно применять разные коэффициент центрирования например если большое приложение считает данный за год то он должен поставить коэффициент 1 100 если за день то может вообще его выставить в единицу для того чтобы пользователь каждый раз не думал какой коэффициент сэмплирование ему поставить мы даем возможность указывать адаптивные пороге сэмплирования это ключевые слова low medium и хай мы относимся к такому адаптивному сэмплирования так что если пользователь указал например medium то мы хотим гарантировать ему то что отчет будет исполняться приблизительно 5 секунд например запрос слоу должен исполняться одну секунду запрос хай должен исполняться не больше пятнадцати секунд как это можно реализовать когда нам поступает запрос на построение отчета то мы должны каким-то образом оценить сколько строк будет обработано этим запросам сколько времени он займет и затем нам нужно выбрать коэффициент сэмплирования таким образом чтобы это время было не очень большим но как вы видели запросы бывают произвольно сложными запросы могут быть произвольное число таблиц с любыми фильтрами и поэтому в общем случае быстро оценить скорость исполнении запросы не представляется возможным поэтому здесь мы пошли по более простому пути мы просто сохраняем информацию о том насколько каждое приложение большое насколько у него большие таблицы и зная то за какой период строится отчет это насколько большое приложение мы подбираем к и всем сэмплирования это работает очень быстро и на практике это дает неплохие результаты сэмплирование можно использовать не только для того чтобы делать большие отчеты быстрыми по части данных ключевому слову сэмпл можно еще дописывать ключевое слово офсет это будет работать следующим образом если мы пишем сэмпл 1 3 офсет две трети то это значит что мы пройдемся по тем данным путём девайса иди которые при остатке отделения на 3 даёт в остатке дают значение 2 если наш запрос например если в запросе есть секция грубой мы группируем по девайса иди то в память будут загружаться всевозможные девайса иди может наступить такой момент что девайса иди у нас очень много мы не можем их все загрузить в память нам не хватает оперативной памяти но все равно мы хотим посчитать точный результат мы не хотим использовать сэмплирование тогда можно применить такой трюк можно разбить этот запрос на несколько запросов разбить запрос без сэмплирования на запрос центрированием например 1 3 1 3 и соцсетями 0 1 3 2 3 если мы исполним все эти три запроса то они будут проходиться по данным который вместе будут составлять общего выборка но при этом каждый запрос будет проходиться по своему кусочку если затем мы можем объединить эти результаты то мы можем разбить один большой запрос который не влезает в память в несколько маленьких которые в память влезают иногда это полезно одном из наших отчётов мы не можем принять сэмплирование потому что там очень важные данные в которых нельзя делать какие-либо расхождения и в этом случае мы как раз оцениваем насколько этот запрос будет большой будет ли он влезать в память если нет мы просто разбираем на несколько частей запускаемых подряд и потом склеиваю мой доклад подходит концу в кратце о чем я рассказал мы используем crack house храним сырые логи агрегирует их на лету для того чтобы можно было описать нужный отчет мы сделали сделали свой формат и pr мы сделали свой диск для построения фильтров мы это сделали потому что во первых целевая аудитория не должна хорошо разбираться в иске ели для того чтобы посчитать нужные бизнес сущности и мы это сделали для того чтобы у нас было пространство для манёвра чтобы мы могли генерировать самые лучшие самые оптимальные запросы и гибко этим управлять конечно кроме положительных моментов есть в этом и отрицательные моменты как и везде во первых это еще один уровень абстракции его нужно поддерживать разработчики должны хорошо его знать и наши пользователи перед тем как пользоваться нашей системой тоже должны прочитать документацию понять что им нужно делать и сложность еще в том что мы никак не можем сделать формат таким чтобы все отчеты можно было построить с помощью этого и 5 поэтому некоторые отчеты построить им все все еще нельзя либо можно но очень не оптимальна для этого мы у нас есть несколько таких отчетов для этого мы просто генерируем запрос какими-то сторонними инструментами если такой такая проблема будет возникать у наших пользователей у разработчиков то мы всегда даем возможность выгрузить сырые логи приложения эти сырые логи затем можно будет загрузить в свой клип house в любую другую систему и делать любые теперь запросы поэтому наличие reporting api все равно можно обойти можно напрямую работать с данными большое спасибо здесь есть мои контакты если вы занимаетесь примерно тем же то напишите мне будет интересно обсудить спасибо да я готов ответить на вопрос да пожалуйста добрый день подскажите пожалуйста вот вы рассказывали про греха узнал что можно там проецировать не протестировать данные при проецировании всегда поднимается вопрос по хранению ресурсной информации да то есть ресурс навигатор где хранятся или иной партиции так далее как этот вопрос решается в клик хаосе как бы насколько там эти данные распределены или они не распределены сосредоточены просто на нескольких мастерах а вы имеете ввиду где находится информация какими-то данные по ресурс навигации партиций я сейчас сразу пару пунктов вопроса формулируют и соответственно вы сказали что у вас navigate что у вас не идет протестирован и да по девайса иди но в текущем решение локальности локальности нету и можно ли как-то сравнить насколько это идет соответственно просадка по производительности если бы было это локальность по девайса небе на вопрос насчет первого вопроса касты клик хауса внутри партиции координируется между собой через за кипер мы указываем при создании дистрибьютор табличке мы просто указываем путь внуки перри который будет использоваться для координации техас ты в которых указан этот путь этих ас ты как раз смогут между собой координироваться по поводу второго вопроса локальность по девайс 1 здесь все очень сильно зависит от того какие именно у нас запросы насколько много у нас результатов в под запросах либо в join ах с джой нами очень похожая ситуация мы не измеряли этом мы не готовы ответить на этот вопрос в общем случае нужно всегда смотреть на свои нагрузки тут нет одного ответа если вопросов нет я бы еще продолжил ну конечно тогда и малик вопрос вот опять да вы you get можно с ним знакомиться посмотреть каунасе saltatio все открыто с главной страницы до есть документация прошла как сравнивали клика узком против но есть другие аналогичные как бы до платформы там типа других hb из на самом деле друид хранит уже агрегированные данные нет мы не сравнивали мы изначально работали только с клика усам потому что crack house разрабатывается внутри команды яндекс метрики мы мы очень тесно связан у нас много общего кода и поэтому этот вопрос не стоял перед нами образцы фильме александра спасибо за доклад скажите пожалуйста вот вы ходе доклада приводили примеры что можно там хранить все в одной таблички в разных и как-то до конца мы не не свалилась понимание у вас она конкретна и хранится в одной таблички или там одна табличка с come on a tribute вко какой-то непосредственного ивенты в другой спасибо за вопрос наверное я пример были права select вам events потом вы все-таки далее примеры смотрите на самом деле у нас все хранится в разных табличках раньше все хранилось в венцах почему в примере мы selecting из event-ы званцев просто эти примеру синтетические на самом деле конечно это не так это для простоты и скрыть пожалуйста вот когда происходит эволюции схемы какой-то там то есть ивент там просто любите вко на него эволюционирует так далее это как бы это вот на лайки и плюс плюс на бэг-энда да то есть он каждую строку мальчик смотрит муле зайтана в схему или нет или как насколько от болезненный процесс не болезненные и как это происходит но когда ивент эволюционируют ну например квента добавляется просто какое-то поле которая sdk присылает мы делаем альтерна наших табличках ну до того как этот формат начинает поддерживать sdk мы делаем alter мы добавляем это поле заполняем какими-то дефолтным значениями затем ядро обучается увидите этот параметр писать его в базу затем езды к это поддерживает и в реальности начинают прилетать и эти значения то здесь все просто если например добавляется новый тип event-а то мы заводим новую табличку если нам нужно делать отчеты по union у из двух таблиц то мы можем завести табличку слишком мертв crack house это эффективный движок который просто берет две таблицы сливает их вместе эффективно используют индекс то есть все зависит от того как именно расширяется все еще можно вопросы про непосредственно как происходит вам сам вставки доставки выключался может акцентировать sign up ядро принимает события она буферизируется какой-то набор у него накапливается сотни тысяч или несколько миллионов строк и такими пачками он пишет наш crack house то есть backend не производит по persist например событиям в кафку какой-нибудь или ещё куда-нибудь правильно понимаю backend используют очереди мы используем очереди через пир и через другой кластер клик хауса он на съезде на ssd у нас внутренней то есть в качестве очередей мы используем свое решение через закипел и через сколько us просто говорить о том что накапливает лампочку 100000 событий youtube внезапно у него тернова в этот самый к этому паника еще что то мы их потеряем правильно потому что они только и memory pack + а выводим она висят события новый демон еще может писать на диск эти события перед тем как этого им по зима спасибо за доклад мне из-за доклады с было не очень ясно рассказать пожалуйста как работаете с стоит но стоит вопросами типа windows функции прочим таком духе но скажем по 4 членов и ну там конверсии почему ну то есть вот когда у вас есть некий поток то есть формата одно действие прошло после второго как это как это устроено вот так вот запросу такого рода запросам мы считаем ridge начинаем конверсию из сырых флагов для того чтобы посчитать или дальше мы просто пишем этот из carry запрос и считаем его включалась и посмотреть и сиквел запрос windows функции или как как это все устроено то есть то есть без виду функции вещи не считаются простой пример ну на самом деле я могу но если у меня напишите я могу показать как выглядит этот запрос там есть несколько джайнов зависимости от типа рендж и на то есть мы обычно ритор шины rolling ride and shine умеем считать одним запросом у нас нет window функции аатеф это все через join и считаете до мы считаем это через join спасибо"
}