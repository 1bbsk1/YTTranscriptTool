{
  "video_id": "KlTZLA3c0io",
  "channel": "HighLoadChannel",
  "title": "NLP At Scale: вся правда о предобученных моделях в Почте Mail.ru / Дмитрий Меркушов (Mail.ru Group)",
  "views": 443,
  "duration": 2891,
  "published": "2021-10-04T02:44:45-07:00",
  "text": "за всем привет меня зовут дима я руковожу вымышленном обучением в спаме почты mail.ru и сегодня мы поговорим с вами про текстовые модели машину обучения про их эволюцию в нашем сервисе про то как мы их решали про то как мы решали текстовые задачи в конечном счете остановимся на том зачем нам понадобился свой трансформер и почему он может понадобиться также его не перешел кну лась вот теперь пришел к массе давайте познакомимся может быть для постоянных участников этой конференции вспомним что себя представляет антиспам почты mail.ru в первую очередь это такой симбиоз продуктовые логики в виде большого количества эвристик ручных правил и модели машинного обучения которые это все пытаются унифицировать и успешно пытаются это унифицировать соответственно это все держится на инфраструктуре с помощью которой мы например получаем доступ к банку фич и на платформе ядра который обеспечивает работу этого всего в онлайне если говорить про основные цифры то здесь наверное выделяется дневная аудитория который оставляет порядка 20 миллионов ежедневно активное эта цифра уже прочищена от ботов и поэтому ее можно верить в для этих пользователя внутри нашего сервиса циркулирует порядка полутора миллиардов писем сутки при этом где-то 80 процентов от этого количества это спам и от каждой моей презентации эта цифра растёт и может сложиться такое впечатление что мы справляемся со своей задачей все хуже но это не совсем так потому что истинное количество спама которая доходит до у конечного пользователя она не растет более того она неуклонно снижается а тот факт что растет цифра общего процента в сервисе означает лишь то что с по мирам нужно привлекать все новые ресурсы генерирует все больше мощности для того чтобы обеспечить хотя бы доставляемость на том же уровне то есть как будто бы эта цифра про то какие мы хорошие не про то каким плохие верни ее рост все это возможно благодаря зоопарку систем машинного обучения которые мы его сейчас насчитали пошли уже на третий десяток все это надо поддерживать эксплуатировать и про это мы уже рассказали как мы делаем поэтому сегодня на этом сосредотачиваться не будем но вспомним о том какие ключевые проблемы есть в нашем сервисе они на самом деле со временем не очень сильно меняют свою формулировку но постоянно раскрывается новыми гранями для нас и безусловно самое важно здесь мы выделяем тот факт что нам противостоит живые люди это означает что они умеют адаптироваться они умеют адаптироваться к конъюнктуре они у мид адаптироваться к нашим решением это означает что наше решение со временем могут деградировать и да они деградируют деградирует а не как с помощью спамеров так и в общем то просто в течение времени поскольку меняются какие-то паторны переписок и так далее вот все это прямой к тому что решение не должны стоять на месте сравнительно новая история этот тренд который спамеры взяли в своих атаках на тексты и это наверное послужило триггером для создания этого доклада давайте попробуем подумать почему они взяли именно такой вектор за что моим конечно очень благодарна я чуть позже скажу почему ну кажется мы можем только догадываться но кажется что новые тексты легко генерирует с по мирами легче ли чем в любой другой контекст в большом разнообразии действительно можно сравнить на недорого получить большую рассылку наполненную разного качества текстовым контентом более того когда мы говорим про текст и у нас появляется шкала в которой мы можем подбирать качество соответствующее рассылки и понимать щетку стоимость ее то есть чем хуже чем менее разнообразное более не читаем рассылка тем естественно для спамеров она дешевле более продвинуты спамеры могут оперировать самыми передовыми решениями и генерить такие корпуса рассылок которые для наших систем становится значительно более сложными освоения ну и наверное в главную очередь тренд на текст и связано с тем что тексты это все еще самый понятный способ донесения каких-то сигналов аудитории поскольку почва в голове многих по-прежнему ассоциируется с текстами и тексты это самый понятный способ передачи информации в сервисе мы же в свою очередь очень благодарны спамеров за то что они выбрали этот бренд потому что как всем наверное многим из вас известно задача national language processing это такой hyip нового дня который уже даже не первый год с нами и мы очень рады что нам благодаря с по миру выдается туда активная на гребень в общем этой волны забраться помимо этого почему именно m-elle выступает таким главным рупором на пути спама мер по двум причинам в первую очередь то что с помощью машины в обучении мы можем устраивать синергию между сиюминутными эвристик ами объединяя их в новых системах и за счет этого получая даже какой-то дополнительный валью плюс к этому когда мы переходим на рельсы машина вообще не у нас появляется инструменты для того чтобы логика эволюционировала они как ручные правило просто были написаны в моменте и стояли на этом самом моменте достаточно подробным рассказали о том как мы эксплуатируем нашей системы чтобы они развивались со временем не буду сейчас на этом останавливаться но до эту задачу мы достаточно успешно решили про фокусы мои сообщества на текст и я уже сказал и как нельзя кстати этот фокус подоспел при позволил нам успешно противостоять текстовым атакам для того чтобы понять зачем и как мы вообще в текущем моменте времени оказались с трансформером давайте коротко проследим эволюцию текстовых задач в сервисе для этого на поможет такой timeline на котором очень упрощённое представлены основные вехи развития текстового контента со стороны спамеров конечно же в рамках нашего сервиса все началось с простых незамысловатых интентов которые наверное под int n там я подразумеваю какой-то основной посыл который содержится в почте и соответственно все начиналось очень простых интентов не более одного интента на письмо скажем так и паттерны были очень узки наверняка многие из вас на рубеже нулевых и десятых может быть и раньше получали так называемые нигерийские письма которых реализованы тем что у вас в какой-то момент времени у чудо обнаруживается африканский родственнику которого о чудо обнаруживается много денег и которыми он у чудо готов с вами поделиться вот прямо сейчас только надо ему реквизиты предоставить вот и это такой сравнить на один из дкк заря текстовых а так который случилось в сервисе после чего это мыслил логически продолжалась в фишинге там тоже были сравнительно ограниченное количество паттернов но уже цель стояла выманить криденс ваших аккаунтов в разных сервисах далее слева направо когда мы проходим поэтому таймлайну мы сталкиваемся с усложнением интентов и их разнообразием естественно шкала упрощенная она не совсем линейное какие-то паторны нахлёст и ваются но ключевое что здесь нужно понять что происходит усложнение далее идут заработки которые мы очень общего называем заработками на самом деле там содержится примерно все что так или иначе триггерит пользователя перейти по ссылкам и обеспечить сетях спамером после того как мы начинаем успешную обороте эту проблему они находят способы за шум лиза за шум за шум ления контента так чтобы нам было сложнее распознать тот кусок который собственно отвечает за спам сначала это происходит с помощью легитимных form of проект так называемому порожденным спаме потом это происходит за счет за шум ления различного рода либо это литературный куски либо это какая-то абракадабра ну и так важно ключевая мысль который здесь заложена что слева направо и идет ну практически по экспоненте рост усложнения интента в письме как же мы это адресуем на первых порах все было просто когда патронов было немного нашей модели тоже были небольшого capacity датой способности к обобщению все начиналось с простых и понятных историй типа линейных классификаторов над мешком слов и эти штуки хорошо работали несмотря на ограниченные словарь и они действительно хорошо работали пока intent и были простые как только появляются усложнения мы перестаем справляться мы это фиксируем и начинаем думать о с этим делать так в один из таких моментов очень удачно подоспел facebook facebook вообще на самом деле всегда поспевает очень удачно подоспел facebook спас текстом и это позволило нам получить решение нового уровня то есть какой то условно не очень глубокие на все же глубокие им бединге причем ну да на основе world века как наверное многие из вас знают но к те которые умеют решать проблему новых слов в словаре сервиса как раз вчера был очень удачный доклад про вас текстом ребята рассказали как им удачно удалось завести это дело не только для текстовых задач но и для последовательностей улов в общем фас текст это хорошо но не фас текстом единым потому что затем произошел надлом в пространственно-временном континууме и миру был явлен python в общем-то все разделилась на до и после и вместо привычных к манера мы говорим о чем-то лагерь и потому что случился сумме тантала и принес в этот мир python и до мир перестал быть прежним потому что появился реально самый удобный фреймворк для глубокого обучения который характеризовался возможностью бесшовного перехода с питона обучение на плюсовой inference и более того обеспечил нам единой api для различных абсолютной моли архитектуру в продакшене единой пит продакшене мы в общем-то не преминули этим воспользоваться и резко углубили наши решения сначала это был переход от линейных моделей нато mb деньгами фас текста капала связанным моделям различной степени глубины те intent и которые не удавалось таким образом адресовать мы в конце концов пришли к варианту с супер вас классификаторами на основе текстовых сверток позаимствовали концепцию да у коллег из компьютерного зрения из соседней ромашки и радостное и адаптировали и получился очень круто на данном этапе хочется вернуть небольшой кусок interactive один из вчерашних спикеров помог мне осознать интерактив нужен ли вам а мне чтобы вот нате водички поэтому все же спрошу какая часть аудиторию поднимите пожалуйста руки имеет дело с машинным обучением продакшен сервисов кей ну где-то наверно четверть какая часть из вот этой четверти которая подняла руки имеет дело с текстовыми задачами своих сервисов норм почти те же людям смотрите от результатов этого вопроса вообще ничего не зависит потому что я все равно собираюсь скипнуть с целью экономии времени образовательная часть но как говорится знать свою аудиторию давайте вот на примере последние модели на примере сверхточных сетей и рассмотрим то по каким критериям мы считаем нашей модели лучше или хуже чем прочее помимо собственно качества на обучающих выборках и в конечном счете в продакшене в моменте мы выделить для себя четыре критерия первый это возможность адаптировать на новость на изменение словаря сервиса то есть возможность не ломаться под натиском новых слов не все модели как вы знаете этому удовлетворяют если что можно потом будет презентацию посмотреть примеры таких моделей в разбивке по этим критериям соответственно следующий критерий это интерпретировать модели наверное от не самый важный критерий но эта штука которая в долгосрочной перспективе обогащает нас то есть если наша модель получается интерпретируемый то эксплуатируя и и мы понимаем и и сильные и слабые стороны и таким образом лучше понимаем вообще происходящие внутри нашего сервиса ну вот например случае свёрточная сетями это то единственное чего не удалось в явном виде добиться но удалось на самом деле вот подобной штуки периодически делать то есть модель может нам рассказать почему она подняла то или иное решение например и вот уже верхней уровневых слов но в деталях как именно на это делала в своих 50 плюс тысячах нейронов конечно сложнее разобраться далее учет контекста это важная штука потому что как мы знаем одни и те же слова в разном контексте означают разные если модели не умеет это адресовать стоит такая хороший черная дыра для спамеров для того чтобы ее эксплуатировать и последнее наверное самое важное для крайнего среза таймлайна проблем это возможность моделям делать осуществлять внимание на конкретные куски текста то что называется attention важных частей потому что если этого нет то действовать в среднем по больнице для многих типов спама становится затруднительным вы теряете действительно суть текстовой интента среди прочих болтовни казалось бы мы достаточно близко подошли со сверх точными сетями к идеальной картине в который нас все помеченные зелёными галочками плюсую туда еще замечательные метрики на дата сайтах вообще совершенно сетями супер войска обычно все прекрасно так почему же вообще сейчас речь пойдет дальше а трансформера смотрите в общем-то на данном этапе хочется немножко остановиться выдохнуть и заглянуть в будущее и подумать о куда дальше эта картина нас выведет ведь на каждом новом этапе усложняется intent соответственно усложняется наша супер вайс решение это означает две вещи во-первых мы с каждым новым int n там медленнее реагируем на него потому что нужно подбирать все более сложную модель выбирать ее адаптировать заводить с нуля и во-вторых мы расширяем наш зоопарк модели и то соответственно у удорожает эксплуатацию и тут хотелось бы предусмотреть что то такое что будет унифицирована об общаться на все intent и и даст нам в конечном счете возможность превентивно реагировать на новые типы intent спамов вот собственно почему мы идем в сторону трансформеров просто потому что добавляется пятый критерий именно обобщаем оси и смысл тот что в общем-то одна модель должна уметь решать те задачи на которых она изначально не была обучена здесь мы наверное две цели преследуем да как я говорил во первых это сделать модели более унифицированными чтобы они могли решать те задачи которые не решали на этапе обучения и во-вторых унифицировать наш наше наше семейство super white моделей чтобы не было такого что скажем на ум the intent нам нужно было выбирать какую-то новую архитектуру сделать универсальную архитектуру которая будет хороша и для текущих решений и для будущих решений и таким образом сильно упростит нам жизнь как как как кажется в решением для такого подхода должна стать штука когда всю сложность моделей мы вносим в признаки и максимально в таком случае облегчаем собственно головы это получается такая легкая настройка над основным фундаментальным базисом голову остаются супер вас про признаки открытый вопрос к признакам в таком случае добавляется очень жесткое требование быть супер описательными они должны обладать заведомо огромной capacity ну и в общем-то ровно примерно в тот момент в сообществе начальница начали циркулировать успехи различных трансформерах архитектур и стало понятно что это она это то что может нас продвинуть к успеху в решении этой задачи поэтому мы поняли что наш неизбежный путь это адаптировать трансформер в нашем сервисе дальше возникает много вопросов и но главное что путь этот несмотря на всю свою сложность оказался очень поучительным и реально очень веселым вот поэтому это чем хочется поделиться в первую очередь давайте поговорим немножко про семейства трансформеров обычно я начинаю любой разговор про трансформеры сопел с этой картинки картинки из шоу улицы сезам поскольку так вышло что большинство наверное архитектур трансформер на получили свое наименование в честь героев этого шоу я до сих пор очень переживаю что нет трансформера по под названием зиле баба на венди шон когда-нибудь появится в крайнем случае мы сами его сделаем вот а пока мы вынуждены были остановиться на другом персонаже но берти а вернее на самом деле на модифицированном бертин о роберте которая робот ли оптимист bird чем вообще характеризуется трансформеры в целом и роберто в частности но уж если бы мы говорим про архитектуру то это очень глубокая архитектура очень . глубокая . так просто в контексте масштабов доу мы говорили про глубокую свёрточная сеть она реально была глубокая там было там 65 тысяч нейронов да когда мы говорим про роберту то мы говорим уже о сотнях миллионов параметров то есть это как минимум в тысячу раз глубже в основе соответствующей модели лежит трансформер ный блок который работает за счет квадратичного mechanism of attention ну какие опыты квадратично sti есть вопросы и есть вариации на эту тему но тем ни менее механизм sale of attention что он означает он означает что мы можем для каждой вернет каждое слово ним и а сама модель может каждое слово получить свое понимание важности в контексте каждого другого слова и более того наличие различных трансформер он их голов означает что вот это вот понимание важности она варьируется от головы к голове и можно выучить очень кучу разных закономерностей в корпусе текста обучать такую штуку приходится уже не на конкретной задачи потому что обучали на конкретные задачи допустим классификации вы ничего кроме классификации сделайте не сможете обучает и и кута универсальной задача которая потенциально они сами текстовых задач и который потенциально потом позволит решить кучу других текстовых задач и в том числе классификацию в качестве такой задачи в случае с роберта и выбрана задача москвич модуль там достаточно простая идея которы заключается в том что мы проходимся по корпусу текстов и для каждого представителя из этого корпуса типа для каждого письма берем и закрываем процент слов токенов вот и заставляем нашу сеть восстановить эти закрытые слова корректно это сделать вот ну и в общем-то практика показывает что если она успешно учиться решать эту задачу а трансформеры успешно учатся решать эту задачу то дальше она также успешно будет решать задачи другого типа в частности задачи классификации как она это делает за счет добавления специального технического токена целая stocking от слова классе фай до который добавляется в начало каждого элемента корпусу текста ну то есть у нас есть письмо в начало к нему представьте вам целый 100 кен и в процессе inference и модель для этого токена подбирает векторное представление которое мы потом используем как описание векторное описание всего текста признаки на которых эта штука работает это байкер encoding и позишн картинки здесь подробно остановимся на этом не буду можно для себя это представить так что batman coding это такая способ хэширования токен of с учетом еще частотности даже не токина fine грамм ну это не так принципиально когда мы понимаем что мы идем сторону трансформеров очень важным становится а придеться как далеко вы готовы зайти по этому пути вспоминая что доклад называется что-то типа вся правда при добыче ных моделях здесь совершенно точно стоит сделать акцент который объяснит наш выбор в пользу обучение с нуля смотрите главное что нужно сказать что в открытом доступе конечно же есть при допущенные модели больше того они есть на русском языке больше того некоторые известные компании очень любят капица на этом и между прочим небезосновательно крепится чем такие привычные модели характеризуются во-первых они подходят для большинства мелки задач то есть действительно можно брать решать проблемы 80 процентов существующих сервисов наверное тот факт что их не так много означает что в каждом отдельном случае они реально качественно и обучены то есть к ним подошли с умом решили кучу проблем о которых вам думать не нужно и сгенерировали решение так как правило это буквально по пальцам пересчитать особенно пример русскоязычные до при добыче иные модели типа руберт а вот вы можете не сомневаться что взяв ее он действительно хорошо выучил википедию да вам не нужно вкладываться в во весь всю историю с ресурсами на обучение это уже сделано за вас более того некоторые в самом деле большинство из готовых реализацией допускают до обучения на своих данных то есть вы можете дать готова модель если она вам чем-то не устраивает дотянуться не и данных специфичных нашему сервису и получить профит любопытно что при добыче ность относится не только к трансформером но также есть при добыче ный фастекс модели и если я очень хорошо понимаю зачем использовать при добыче ный трансформер то использование при добыче на вас тексты иначе как леностью назвать сложно потому что это вообще не сложно задача взять модель пафос текста собрать не баснословно и количество данных которые нужно чтобы она выучилась и получить обученную с 0 на ваших данных так что же нам все-таки мешает взять и использовать при добыче ную модель и что может помешать вам если коротко то это специфика вашего сервиса в нашем случае это специфичный поток специфичный поток в целом и ключевое что это специфичный поток спама если поток в целом еще можно худо-бедно охарактеризовать как но такой типа средне статистический срез русского языка хотя конечно нифига он не среднестатистический с учетом того что там есть human generated content of the mighty generated content и там есть сленговое составляющая то вот когда мы говорим про спам которого 80 процентов до в сервисе то там все совсем пиши пропало и ну в общем как бы вот посмотрев на спам и на википедию вы у вас возникнут вопросы что это вообще один и тот же язык 2 специфика заключается в нашем внутреннем представление этих текстов здесь я подробно на этом останавливаться не буду но просто у нас в сервисе мы смотрим на текст а не как на набор русскоязычных токенов а как на более модифицированы представление это был осознанный выбор и который сделан достаточно давно и он действительно позволяет решать проблемы дефекта спама лучше причем это не то что не русскоязычных текста он и не англоязычный текст он как бы никакой вот он и из другой вселенной соответственно как результат готовы сложные при добыче иные модели доставляют для нас качество мы проверяли на уровне архитектуру условно говоря предыдущего поколения в случае с трансформерами мы получили результаты на уровне пола слиер носите на тв с текстом в случае соответственно при добыче на вас текста мы получали результат на уровне мишка слов вот в общем-то все время на ступеньку ниже что же мы получаем плюсом если мы выбираем путь с нуля как бы полностью с нуля выбрать модель опустите и так далее несколько дополнительных преференций но даже не дополнительных а ключевых для этой задачи во первых у нас появляется возможность адресовать конкретные проблемы нашего сервиса как можно в машина обучения адресовать конкретные проблемы сервиса ну как кажется инструмента всегда 2 это выборка в которую можно в которой можно отразить соответствующий паттерны и это выбор функции потерь с помощью которого можно заставить делать вашу модель чуть лучше то в чем вы заинтересованы в нашем случае мы к стандартному млм лоссу добавляли триплет лосс чтобы стягивать в 13 плед соответственно кластера стягивать зашумленным и не зачумленные письма характеризующий за одним и тем же интентов чтобы модель училась различать их корректным образом соответственно помимо этого мы получаем возможность очень важную для нас вспоминаем что конференции называется high лот и в общем-то не праздный вопрос о производительности таких монстров здесь мы получаем прямой доступ к фан тюнингу этой самой производительности за счет выбора архитектуры но за счет тонких настроек параметров но ключевые параметры которыми мы оперировали это выбор совместное количество тайн голов выбор количества transforming слоев и разве выбор размерности внутреннего представления там еще можем чем много чего можно покрутить и очень важный момент он очень важный в контексте спама но кажется он немаловажен и в контексте других продуктовых сервисов это безопасность когда вы берете при добыче на модель ваше решение легко от реверсом женились и мимикрировать под него повторить его как-то за эксплойтить его соответственно если вы выбираете путь с нуля то вы закрываете эту лазейку и от реверс инжиниринг монстров 100 миллионов параметров но эта задача пока еще не решена и и таким образом едиственный путь который нам остается это обучать трансформер с нуля чему мы на самом деле оказались очень рады безусловно мы понимали насколько тернист этот путь если говорить про то какие там ключевые засечки то это наверное три основные вещи это развертывание и использование инфраструктуры для сбора выборок как мы понимаем такие большие модели требует совершенно иного объема данных для своего обучения это развертывание использование инфраструктуры для обучения соответственно чем больше модель тем больше ресурсов для обучения и нужно там начиная просто с количество gpu который нужно чтобы она в разумные сроки это выучил и заканчивая сетью между различными машинами если вы уже не ограничены одной машины с несколькими gpu и это поднятие адаптированные со структуры для inference а ну вот эта штука нужно в любом случае вне зависимости от вашего пути идете вы с при добыче иной моделью или обучаете и с нуля если вы планируете использовались трансформер в онлайне инфраструктуру для интернета вам понадобится давайте пробежимся по каждому из этих этапов соответственно этап обучающей выборке мы здесь помним главное что от обучающей выборке мы хотим мы хотим общей от модели возможность обобщения на разные типы спама на все типа спама и в общем то тут очень простое правило собрать как можно больше данных с потока в идеале собрать их все ну естественно все данные не собрать никогда но можно собрать очень большое разнообразие что нам помогло в этом во-первых это сэмплирование то есть мы не просто собирали все подряд а центрировали по неким уже существующим распределением классификаторов таким образом решали проблему когда у нас какие-то особые частотные паттерн изобьют выборки все прочее и это безостановочно мы собираем много данных потому что на потоке как понимать очень много дубликатов есть очень крупные рассылки и чтобы собрать реально разнообразный беду плиссированный datasette в достаточном количестве собирать нужно 24 на 7 без отдыха и до важно сказать что на этапе обучения на этапе сбора обучающей выборке когда у нас появляется физическое место где жить так много пользовательских данных на первый план выходит их безопасность и гарантия того что чувствительный данный не утекут поэтому появляется требования по анонимизации этих данных и их тоже нужно решать на этом же этапе картинка здесь это результат запросов google с текстом тройник сайт вот почему-то в тот день google сломался и выдавал мне сплошные резинки для занятий фитнесом вот вчера я проверил все было нормально но картинка уже осталось параллельно можно запускать трек с и молились о чем самый интересный конечно же трек мы построили его в два этапа на первом этапе мы вооружились всеми возможными худо-бедно похожими на правду моделями семейства трансформеров из различных статей на архиве и бложик of их получилось порядка 25 штук ну и дальше мы радостно них накинулись и задачей проредить этот список парижа ли мы по двум критериям ну может быть даже чуть больше во первых это вообще применимость соответствующий архитектуры для решения нашей конечной задачи то есть классификации во вторых это пруфы в статьях или в тех же блогах по производительности этих моделей и по их качеству то что очень часто мы наталкиваясь на то что есть какой то непонятный score который непонятно как интерпретировать и заявление о том что это очень лучшая модель его в конечном счете мы сократили это список модели кажется до пяти и с этими пятью моделями мы пошли уже в сторону локальных mvp и от этих локальных веке мы требовали мы хотели следующее это померить их качество на таком упрощу в упрощенном сетапе и померить их производительность также упрощенном сетапе вот для того чтобы померить производительность мы завели питон стенд на gpu но питон вот в этом нам помог индекс танк этот шарф соответственно моей танком обстреливали каким-то заведомо под собранным корпусом текстов сервис питон сервис вот в развернутые соответственно в df окружении вот почему это этап не бесполезный несмотря на то что имитирует у нас плюсов inference на бою и инферно спит они имеют один и тот же тип torch predict под капотом вот и поэтому за вычетом логик при пост-процессинга можно очень с достаточной степенью точности попасть в понимании какие-то мегги будут у вашей модели в онлайне соответственно если мы говорим про при пост-процессинг на самом деле пренебрегать им тоже не нужно и там тоже есть корреляция между бетонными плюсами но естественно потом попадания не миллисекунды в милисекунду получается в чем еще заключается mvp в том что мы берем модель соответствующий архитектуры заведомо меньше меньше capacity то есть более простые параметры типа там размер и количество слоев размера голов и так далее и обучаем на меньшем объеме данных на меньшее количество эпох таким образом мы можем сравнивать эти модели между собой смотреть как они ведут себя в динамике обучения а это происходит достаточно быстро когда мы говорим о том как качественно сравнивать их между собой тут очень важен выбор критериев в нашем случае мы оперировали в основном extreme си критериями это способ оценивать соответствующую модель потому как она решает внешние задачи то очень хорошо ложится в ту парадигму который мы оперируем потому что наша конечная цель это решить текстовые задачи текстовых классификации поэтому нас есть готовый выборки мы можем взять и посмотреть как эта модель будет вести себя на них да так мы сделали в общем это и позволило нам конечном счете выбрать модель есть еще intrinsic оценивания штуку которая позволяет как-то понять как какое текстовых какое векторное представление выучила ваша модель насколько это векторы поставления хорошие вот но универсаме ответа как это делать нет мы попробовали там немножко погла стилизовать посмотреть на то что получается в общем порадовались тому что склеилось в кластера но как бы остановились на этом потому как это понятно померить не очень понятно про технически метрики да я уже сказал после того как выбрана модель собрана выборка в принципе можно переходить к обучению и тут в общем может десять лет простого к сложному простой подход выбрать машинку с побольше гappu одну машинку вот и радостно запустится на ней и наверное за какое-то вменяемое количество времени типа вам не придется ждать 2 года чтобы получить на выходе обычный трансформер но как вы понимаете ограничение одной машины это фундаментальное ограничение делает эту историю в суд не масштабируемой рано или поздно вам в любом случае захочется обучиться на в два раза большем объёме данных либо в два раза быстрее вдруг внезапно до такого способа у вас нет при ограничении в одну машину соответственно неизбежен крем в сторону gpu кластера вот это отдельная такая и логическая и техническая задача была с нами вот проеме кажется можно будет сделать отдельный доклад на 40 минут вот поэтому сейчас все что хочет сказать что мы этот путь путь прошли и пока мы его проходили мы поняли одну важную вещь что заводить gpu кластер для конкретной задачи не очень целесообразно потому что gp очень дорогостоящий ресурс и существенно становится касты на его просто его не и поэтому имеет смысл идти сразу сторону гappu кластера на всю компанию вот чтобы минимизировать время простое сделать так чтобы какая-то из команд всегда что-то обучала вот в общем то мы сейчас этим путем идем обучив модель возникает вопрос как довести ее в онлайн мы в и вообще завести вам они мы выбрали путь отдельного диплом вк обернитесь и это inference сервис на gpu один под селиться на 1 га по ответ нам нужен в онлайне и соответственно подразумевается минимальный бюджет на лэйтон все такого сервиса это исключает возможность заведения его на очередях вот что в свою очередь вызывает сложности для обеспечения качественного и солей этого сервиса но нам удалось решить эту проблему во первых мы за оптимизировали все что могли ну то есть там где бы как пример первоначальный коробочный сын там spies который нас почему-то работало несколько десятков миллисекунд вплоть до сотни удалось там разогнать до единиц не сильно потеряв качестве я стала их четыре девятки в итоге нам позволило добиться совокупность решений виде которых мы все-таки вернули очередь мы не удержались вернули очередь но там буквально очередь на пять элементов и совокупность соответственно очереди тайм-аута на ставку в эту очередь и retrieve позволило нам утилизировать gpu до 80 процентов что в свою очередь обеспечила нам возможность минимизировать количество ошибочных ответов сервис теперь после того как у нас есть доступ к inference у трансформера на потоке как этим пользоваться мы соответственно получаем векторное представление на письме которая универсальное очень хорошая и дальше нам нужно использовать это те самые задачи текстовых классификаторов выделений интента спам новый интент а поэтому под каждую такую задачу мы сверху обучаем микро голову после того как наши модели имеет такую описательно способность требования к этой микро голове предельно лайтовые достаточно сравнить и неглубокой fit форвард сети пола связанные или даже буквально однослойный то есть линейной модели ключевой что она уже обучается на своих супер вать выборках не требует такого большого объема и это можно сделать недорого такой сетап позволяет нам весь этот зоопарк прочесать в удобный эксплуатируемый в удобно эксплуатируемый упряжку да потому что трансформер да это монолит это как бы базис всей этой системы но вносить изменения в него нужно очень редко это даже то мне вопросы ежеквартальной поэтому вся эксплуатации трансформера сводится к тому чтобы обеспечить его работоспособность в онлайне эту проблему мы общем решили послать назад эксплуатация емеля она остается в рамках этих самых голов а как эксплуатировать супер вас несложные классификаторы мы хорошо знаем мы делали про этот доклад и все проблемы типа как оба тестировать новые модели как построить их развитие на основе зациклившись фидбэк лук и как делать до обучения с продуктовой регуляризации типа через механизм она ложности лично мы конечно же хорошо знаем в некотором смысле мы получаем унифицированную схему принятия решения на потоке которая умеет общаться на новый типа спама и при этом не требует от нас дополнительных концов на эксплуатацию отдельное слово хочется сказать про осознанный выбор между циpкa и gpu в случае с трансформером такой выбор отпал очень быстро потому что мы их худо-бедно померили enfin с трансформера на циклу и получили полсекунды естественно наш онлайн подразумевает совсем другие цифры и поэтому выбор вынуждена был сделан в сторону гopoдa итак у нас появился тот самый гappu тепло и вк убираетесь и что важно заметить что на одно письмо происходит ровно 1 inference соответственно больше нам не нужна а вот intent головами ситуация другая там выбор стоит между циpкa и gpu можно пойти каждым из этих путей в принципе мы можем мы пошли по пути циpкa мы могли бы пойти по пути группу и ускоримся эту историю раз в десять но посчитали это оказалось экономически не очень целесообразным поскольку мы оперируем в бюджете некоторого доступного нам окна миллисекунд то если мы в это окно попадаем сцепу которые существенно дешевле то лучше в немой остаться вот гнаться за десятыми долями миллисекунд наверное не стоит потому что это будет необоснованно дорого вот сцепу соответственно еще ситуаций осложняется тем что модель не одна а их столько сколько соответствующих intent голов у нас есть продакшене более того с учетом абэ тестов этой истории может например еще удвоится поэтому фирма которая будет на gpu вывозить этот поток и вообще сначала бы посчитать и она окажется сильно больше чем то что нужно для интернета трансформеров потихоньку двигаясь к закату давайте посмотрим на срез характерных цифр по получившийся задачи средство от серных масштабов больше всего мне нравится цифра которая говорит о том какой размер выборки для обучения нас получился эта цифра в 1 терабайт и это реально дофига дамаг которой мы очень долго собирали про это будет следующий слайд помимо объема выборки какие мощности у нас сейчас в онлайне для этой задачи это 100 г пу до 104 на котором мы сочетаем inference трансформеров это 800 циpкa ягер для inference intent голов до этого возит наш поток и это классно пока еще на 24 гopoдa нa как я говорил мы двигаемся дальше если говорить про срез характерных времен то ключевые цифры это конечно же медианная скорости инферно трансформера который нам удалось довести до 10 миллисекунд с учетом логик при пост-процессинге типа там таки низации прочего эта цифра увеличивается не сильно не больше чем в два раза соответственно inference 1 intent головы это совсем другой масштаб это меньше одной миллисекунды это тоже чем мы вообще легко живем и можем позволить себе много таких голов другие более длительные времена этой задачи это одну неделю у нас ушло на то чтобы уходе сейчас на то чтобы обучить модель с нуля два месяца ушло на то чтобы осуществить первичный сбор выборки хорошо что его не нужно делать каждый раз потому что достаточно в эту выборку просто закидывай танков когда мы соберемся ретро нить модель и это один квартал на самое веселье на тот самый малер сечь который стоял в основе всего этого дела и наверное вместо резюме еще один срез характерных цифр он уже про влияние всей этой истории на нас как на команду понятно дело что одно из ключевых влияния на в бизнес целью был его по понятным причинам я не буду озвучивать но даже за вычетом этой истории можно сосредоточиться на том что путь пройденный с нуля дал нам как команде мы получили 15 реально пробившись потолок разработчиков которые в том или ином виде участвовали в этой задаче мы имели четыре команды соответственно получили четыре прокачанных тех лида которые готовы к новым свершениям новые свершения думаю не за горами потому что соответствующий embedding открывать перед нами массу новых возможностей и нужно просто сообразить как максимально оптимально использовать это помимо например алгоритмов классификации да и ключевое это мы получили целый один замотивирован антиспам за мотивированный тем что удалось достичь и замотивированные в общем-то сложностью задачи которые удалось решить вот наверное это основное ключевую идею которая хочется вынести с этого слайда что трансформер с нуля это во первых за вадима в продакшене во вторых даже в условиях холода и в третьих это очень весело очень интересный и прокачивает любую команду абсолютно и любой технический стык поэтому если в вашем сервисе так случилось есть тексты то я думаю что обязательно нужно идти к руководству согласовывать сроки выбивать ресурсы аргументов для этого я надеюсь после моего доклада у вас стало побольше спасибо у меня все соответственно с удовольствие выслушаем ваши вопросы итак мы переходим к вопросам ответ дмитрий важный момент тебе могут выбрать тех кто того кто задаст самый интересный вопрос мы ему вручил подарок и если вы смотрите нас онлайн тоже задавайте вопросы мы вас увидим на экран послушаем ваш вопрос и послушаем ответ дмитрия на ваш вопрос и так все переходим вопросам здравствуйте николай ii вопрос такой через какого персонажа вы назвали получившихся трансформер еще раз честь какого персонажа и улица сезам вы назвали получившийся трансформер в честь а роберто там есть такая это там сестра кого-то короче она есть вот но все шло от берта это вот тот чувак который был на картинке желтенький 20 раз раз спасибо большое за доклад очень интересно меня немножко смутил один слайд с роберта и мне казалось что fair убрала съела еще раз я услышал про фейр нового также то есть в оригинальной архитектуре роберт и насколько я помню они убрали задачу ник синтез prediction качественном не было сил истоки на вот я немножко смутился то есть вы как там модифицировали до получения архитектуру смотрите нет там убранный xprize лишь на остался млм но к листок он там остался сел с браслетом вроде просто sequence разделитель оставили не не о роберте все ок да ладно тогда да и вот второй вопрос самое главное это смотрите а вы как нибудь используете вообще трансформер когда подбираете данные для вот этот терабайт данных сырых для фан тюнинга но обучение с нуля вы как-то вообще используйте prediction и текущей модели чтобы может быть фильтровать оставлять только какие-то данные которые вот совсем новые конечно как я говорил одно из явных один изъян из способов как мы это делаем это для сэмплирования мы sampler у импа обе нам существующих классификаторов до чтобы как-то сделать более разнообразным представление от из чисто им бединге как вот не знаю там кластера новые находите вы не используете для спасибо еще один вопрос будет еще и дальше уже можем вы дмитрий мучить кулуарах гей получиться здравствуйте здесь над мере спасибо за доклад скажите а будет ли минусом такого подхода то что мы по сравнению с при допущенным burton то что если взять какой-нибудь мультиязычный берда он был он был там при добыче на 50 языках а вы допустим обучались в основном наверно на русском и английском то какой нибудь там украинский спам будет проходить или нет смотрите если он появляется если он появляется то нам проще все таки участие это в нашем берти потому что ну то есть да наверное мультиязычный bird сможет потенциально какие то какие то очень такие специфичные типа спама адресовать но если не появится то их проще как бы в нашу первую часть потому что у мультиязычная uber то есть куча других минусов про которую я говорил который все-таки не позволит нам туда сходить но откуда тогда взять обучающую выборку большую допустим на новом языке смотрите если и набирать все время то она будет все время пополняться новыми историями если украинский спам будет появляться в сервисе то он там отразится дальше просто вопрос в чистоты переобучения вот но кажется что это очень редкий история их действительно не имеет смысла проворачивать часто я спасибо"
}