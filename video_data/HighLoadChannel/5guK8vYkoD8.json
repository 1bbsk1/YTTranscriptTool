{
  "video_id": "5guK8vYkoD8",
  "channel": "HighLoadChannel",
  "title": "Инфраструктура поиска Яндекс.Маркета / Евгений Соколов (Яндекс.Маркет)",
  "views": 3103,
  "duration": 2026,
  "published": "2019-12-05T13:40:23-08:00",
  "text": "меня зовут евгений соколов я ведущий разработчик в яндексе работают там уже более двух лет я вам расскажу про инфраструктуру поиска яндекс маркета а именно о том как обеспечить надежность хайло сервиса это позволит вам тратить меньше времени на мониторинге и решение проблем уменьшить время простое сервиса будет больше времени заняться разработкой новой функциональности ну и позволит крепче спать по ночам обеспечение надежности большого hallows сервиса это большая сложная задача потому что ни одна из компонент не является на сто процентов надежной могут падать сервера квартира и даже целые дата-центры могут быть проблемы с обновлением могут быть нестабильные мониторинге новый функциональность может выкатываться нестабильной и даже когда все выход выкатилась и нормально работает могут быть проблемы с производительности я вам расскажу лишь о подходах которые мы применяем яндекс маркете для решения этих проблем конкретно а балансировки трафика надежных обновлений параллельных мониторингах о том как мы имеем отлаживать новые технологии в продакшен и а тестирование производительности на всех этапах от разработки до выходки вся инфраструктура поиска у нас находится в нескольких дата-центрах в каждом дата-центре одинаковая схема обработки запросов рассмотрим отдельно взятый дата-центр в каждом дата центре стоит несколько балансиров их как минимум три и много серверов балансиры получают входящий запрос и роддом на распределяет между серверами балансиры работают ночей прокси на серверах работает наш собственный софт который мы называем report так исторически сложилось типы запросов которые идут от балансира поисковым сервером можно разделить на пользовательский например когда вы вводите в строку поиска iphone и проверка статуса на который возвращается статус жив или не жив все сервера у нас разбиты на кластеры в каждом кластере находится 8 поисковых серверов и один дополнительный сниппет ный поисковые обрабатывают запросы сниппет нам лежит статическая информация которая требуется для уже окончательные выдачи например тайтлы и описание товаров также у нас очень много данных мы разбиваем эти данные на шахты а потом эти шаги группируем на с кластером сервера типичный поисковый запрос может прийти на любой из поисковых серверов этот сервер делает много под запросов на каждый шаг каждого сервера в кластере он получает от них ответы объединяет их и возможно делает еще несколько по запросов нас не бедны для получения дополнительной информации внутри кластера есть поисковый запрос о которых торт только что сказал и дополнительные запросам примерку статуса помимо работает не работает ручка статус возвращает информацию от текущей версии reporto и текущей версии индекса если хотя бы один из серверов в кластере врубается то с помощью ручки статус все сервера понимают что хотя бы один недоступен и тоже перестают отвечают на пользовать такие запросы кроме того если хотя бы в одном сервере отличается версия индекса или версии рип от а то этот кластер тоже считается не консистентными и он тоже не обрабатывает пользоваться кие запросы помимо запросов внутри кластера обычно на каждый пользователь запрос делается в районе 10 запросов на внешние сервисы всего у нас ну мы ходим на порядка 50 внешних сервисов если не доступен один сервер в дата-центре то вскоре весь кластер становится не тот недоступным балансер вскоре понимает что не пользуйте запросы которые почти найтись сервера возвращаются ошибкой балансира это понимает и делает перри запросы на живые сервера и вскоре когда он понимает что хотя бы один из сервер не доступен он перестает посылать польские запросы но при этом продолжает посылать пинги и как только сервер поднялся балансе роботом автоматически узнает и начинают посылать туда пользователь трафик рассмотрим случай когда не доступно много серверов для этого каждый балансер постоянно содержит памяти текущее количество живых кластеров и постоянно считается количество запросов которые может обработать если входящий поток запросов превышает емкость которую может обработать избыточной трафику начинает перри посылать другие дата центры и конечный пользователь ничего не замечают следующий пункт моего доклада надежное обновления в маркете есть определенные требования для процесс обновления софта нельзя просто так отключить сервера потому что они еще могут обрабатывать пользоваться кие запросы нельзя деградировать по времени обработки запроса надо сделать обновление незаметно для пользователя и кроме этого мы катаем обновления как и порта так и индекса несколько раз в день прежде чем сделать непосредственно обновление надо чтобы сервера его закачали для этого у нас есть специальная компонента менеджер обновлений а на серверах работает отдельный процесс агент который умеет управлять самим при портом в том числе и его обновлениями сначала для закачки обновления менеджер обновлений публикует зуб теперь и информацию что доступно новое обновление все сервера постоянно пуль адзуки пир и как только они увидели о том что обновление доступно начинает его скачивать через торренты при этом они все еще обрабатывают польские запросы все вся скачка происходит в фоне для непосредственно процессы обновления у нас есть требования в dc которым делается обновление должно быть не менее 70 процентов живых кластеров всего по всем dc должно быть не менее 80 процентов мастеров то есть может быть случай когда в одном dc много лежит кластеров оба в другом мало и обновление будет происходить в том котором мало для начала обновления мы останавливаем report для этого менеджер обновлений посылает специальную команду стоп report в это время не останавливается он продолжает обрабатывать пользу ее запросы но на запросы pink от балансира он отвечает ошибкой и балансер понимает что этот report больше не доступен и перестает посылать туда польские запросы и как только под обработал все польза такие запросы и понял что к нему больше запросы не поступает в этот момент он может спокойно остановиться после этого менеджер обновление посылая другую команду рилот в этот момент распаковывается новый report новый индекс если надо и все автоматически запускается при этом балансе все еще продолжает пинговать менеджер обновлений тоже постоянно проверяет статус кластеров и как только report поднимается балансер узнаёт об этом через команду пинг начинать туда посылать пользу такие запросы менеджер обновление тоже об этом узнает и в этот момент превышается этот порог 70 процентов и менеджер обновлений может приступать к обновлению следующий следующего кластер или группы кластеров если что-то пошло не так и какой-то классный поднялся то менеджер обновлений попытается еще несколько раз поднять этот кластер и когда это низ если это не случилось то он делает уведомление дежурным они начинают разрабатываться разбираться но обновление при этом останавливаться потому что не выполнено условие 70 процентов если в какой то момент мы понимаем что продакшене оказался какой-то совсем плохой релиз мы начинаем терять деньги в этот момент у нас для этого у нас есть специальный механизм который может откатить обновление сразу на всех квартирах минуя вот этот механизм по кластерного обновления для этого на каждом сервере постоянно хранятся последний три версии report а последние две версии яндекса и ответственный может сделать запустить механизм и все откатиться там в районе минуты следующий пункт параллельно мониторинге возможно у вас была ситуация когда у вас вроде бы работает все хорошо вы смотрите на графики вдруг перестали поступать данные обычно первая мысль это то что упал сервис в этот момент обычно прибегают менеджер root на себе волосы рассказывает что мы теряем деньги вторая мысль это возможно упал мониторинг на слайде представлена схема обработки запросов у нас каждый сет как пишет логе очень много логов это не так access логит также много статистик ошибки разные бизнес метрики все эти лаги потом сохраняются в мисочку и у нас есть отдельная компонента которая называется market здоровье который вычитывают эти логе фильтруют анализируют агрегирует и все сохраняется в графит и потом мы уже после можем посмотреть все эти данные с помощью граф анны но несмотря на то что все за дублирована может произойти много разных проблем это могут быть как софтовые железные так и сетевые проблемы могут не записаться логина диск например закончилось место могут не отправится сообщение в мисочку потому что какая-то сетевая проблемы могут проблем здоровье для это предлагается использовать дополнительный мониторинг который собирает логина стороне балансиры и отправляет их независимый в мисочку и сохраняет в независимую базу данных он сохраняет только базовые характеристики без анализа такие как количество запросов времена ответа и ошибки и даже если у вас основной мониторинг упал вы все еще можете посмотреть дополнитель основные показатели вашего сервиса на с помощью дополнительного мониторинга следующий пункт моего доклад и это о том как мы имеем отлаживать новый технологий в продакшен возможно у вас была ситуация например к вам выходит новый сотрудник пишет какой-то новый функционал и вы сомневаетесь стабильно этот функционал будет работать или нет при этом зачастую сложно это полноценно проверить потому что надо создавать окружение могут быть много зависимости от внешних данных и также если вы используете формирование вам надо вас по хорошему вам надо воспроизведите целый кластер а это обычно дорого сначала вам расскажу о наших об экспериментах а потом нашем подходе который позволяет отлаживать новой технологии продакшене например вы делаете какой-то новый функционал новую формулу ранжирования для этого вы добавляете новые параметры market не формула равно 1 когда этот параметр есть равен единице включается новая формула если его нет или он есть но равен нулю то работает старая формула по умолчанию в сервисе для проведения эксперимента вы выставляете долю трафика например 10 процентов при этом это не доля трафика запросов а доля трафика от пользователей это на для того чтобы у каждого конкретного отдельного пользователя не не галоген decide to с экспериментом the peace эксперимента и выставляется период времени например 7 дней сервис автоматически считает качество это различные бизнес метрики там клики переходы и так далее но для того чтобы проверить какую-то новую технологию не обязательно заводить об эксперимент ждать целую неделю у нас есть отдельный механизм который называется стоп-кран который позволяет мгновенно изменять конфигурацию всех серверов которые обрабатываю запросы и на основании этого можно мгновенно включать и выключать эксперименты на слайде представлена схема обработки запроса есть стоп-кран у него есть api и юрий ленты и веб-интерфейс веб-интерфейсе можно задавать значения для всех параметров это сдаются дефолтное значение стоп-кран все эти значения сохраняет в базу данных и каждый сервер ходит постоянно раз десять секунд и получает свежие значение для всех ural параметров значение бывает двух типов безусловные и условные безусловно и применяется всегда а условно как применяется когда выполняются условия например значение 3 применится когда запрос будет обрабатываться дата-центре 1 а значение 4 применится когда запрос пришел на сайт беру и обрабатывается в классе re5 можно заметить что у значение может быть вас одета не знаю в конце это значение с приоритетом report применяет значение с флагом следующим приоритетам сначала применяется значение с повышенным приоритетом которые с восклицательным знаком потом значения из запроса потом дефолтное значение которое было в стоп кране и если ничего этого не зато на дату применяется дефолтное значение которое при port флагов у нас много для условий и хватает на все известные нам случае можно указывать дата-центр окружении это production and testing при stables и шедоу проплести бланшетт я расскажу позже подробнее можно указывать площадку и the market берут бринкли и также можно указывать номер класса этим экспериментом можно включить функционал на какой-то малой группе серверов и когда вы включаете если что-то пошло совсем не так например ваш софт начал падать то балансира это поймёт и перед перенаправить запросов другие дата-центры если все нормально вы можете проверить это работу нового функционала например пологом по графикам по выдаче если все нормально вы можете включить на другом дата-центре или например полностью откатить и все изменения применятся мгновенно минус этого подхода в том что иногда разработчики пытаются запихнуть все свои изменения стоп-кран и то есть выкладывать свои фичи помимо стандартного по и плана или гизов и тестирования мы пытаемся бороться с этим использованием последняя тема доклада это тестирование производительности на всех этапах разработки до выкатки например у вас есть новый функционал новый канал вы делаете новую функциональность у вас есть несколько идей и их реализации и вы не знаете какая из этих идей будет работать лучше вам надо бы проверить какая из реализации будет работать быстрее или например вы выкладываете новый релиз у вас есть новый собранный бил и бил который в продакшене его надо проверить если деградация между ними или например так случилось что вы уже выкатили новый релиз который в продакшене и тут вы поняли что там есть деградация и вам надо понять место commit где это произошло чтобы потом исправить это я расскажу про наши способы тестирования производительности а конкретно о сервисе нагрузочного тестирования обрести бы кластеры и шедоу постеры сервис нагрузочное тестирование в яндексе уже давно есть такой сервис как танк который позволяет стрелять сервисы но одного танка недостаточно для него еще сделать систему сборки которая будет собирать report надо сделать систему раскладки которая будет раскладывать этот report на кластер генерить конфиги и генерить конфиг для танка и надо еще создавать патроны патроны это запросы которая применяет на вход танк для того чтобы стрелять сервис изначально мы написали скрипт и тестированием который сам имел собирать раскладывать и запускать report сам умел делать патроны он настраивал than стрелял и отправлял отчеты его почти не надо было контролировать про почти я расскажу на следующем слайде основная проблема была том что у нас был дефицит кластеров для стрельб и когда вы сделаете как сделали какую-то новую функциональность вам приходилось давать в очередь и ждать этот кластер для стрельб для того чтобы проверить ваши изменения при этом ожидания могли доходить до недели кроме этого зачастую эти кластер а ночью проставили потому что ночью никто не работает скрипт от запускался на девелоперским сервере и надо было настраивать окружение для того чтобы он умел ходить на кластер до стрельб и он за поскольку на работают долго мы обычно его запускали в скрине но иногда рвались соединения из за этого при двух алисы цеха сессии и терялись результаты кроме этого нельзя было делать автоматические стрельбы релизов поэтому мы написали сервис для нагрузочного тестирования он сам собирает скачивают устанавливать report индекс у него из встроена очередь задач и когда кластер свободный он автоматически берет новую задачу и начинают ее выполнять он сам запуская стрельбы отслеживает вполне нее и отправляет уведомление позволяет делать периодически стрельбы и все это можно управлять через api на слайде представлена схема его работы у него есть api и com онлайн утилита через которые можно задавать задание если над он может пойти и собрать новый релиз он может скачать новый индекс репозитории все это он сам устанавливает и нагрузочная окружение после стрельб с все сохраняет в службу отчетов у комбата можно создавать задавать версию ревизию ревизию + patch можно указывать длительный straight нагрузку это рпс и версию индекса с которым надо все изменения проверить типичные сценарии использования это то когда вы делаете новый функционал вы на каждую свою идею делайте отдельный патч а потом одной командой запускаете все это для стрельб создается новая задача и по окончании вам приходит уведомление отчет там будет табличкой напротив каждого патча будут тайминги для каждого вашего патча другой случай это когда у вас религиозные или бы вы сделали новый билд и запустили и стрельбы между двумя версиями которые сейчас вот продакшене и которое вы только что собрали и собираетесь катить продакшен и другой случай когда вы ищете деградацию вы указываете вашу какую-то базовую версию и список ревизий в которых возможно есть деградация и на выходе вы получаете табличку где напротив каждой ревизии будут тайминги вы можете на основании таблички понять в какой ревизии произошла деградация но у комбата тоже есть недостатки основная основной большой недостаток это то что kombat это не production там нагрузка эмулируется за счет среза запросов за предыдущий день и там может быть разное железо и у нас однажды был случай когда комп от показал ускорение а в продакшене оказалось деградация но случай был единичный для этого чтобы отлавливать такие изменения у нас есть специальный приз требую кластер это такой кластер куда и выкатывается изменение перед выходкой продакшен он сейчас он работает следующим образом балансер большинство трафика посылает в продакшн и небольшую часть на это прицепы кластер там настоящие боевые условия туда идет настоящий пользователь трафик и в случае о каких-то ошибок балансе при направить запросы на продакшен и потом уже сравниваются разные показатели производительность ошибки и бизнес метрики по сравнению с продакшеном и можно уже понять если там какие то проблемы в этом релизе минус этого плести бластера то что запросы распределяется рандомно и может быть такое случится что на приз ты был придет какой-то тяжелый запрос он исказит тайминги но на каком-то значительно промежутки времени показатели одинаковые и у нас есть еще один шедоу кластер он создан специально для разработчиков стоит в одном из дата-центров и запросы которые идут на присте был автоматически дублируется на шэдоу он у него одинаковое железо по сравнение с пристрелим паз и поскольку запросы дублируются можно ожидать абсолютно одинаково поведения кроме того балласт вир настроен так что запросы которые ответы от запросов которые пришли на прицел был конечный пользователь и не увидят и дольше балансира никуда не пойдут таким образом разработчики могут сделать какой-то свой кастомный build выложить его на shadowcaster и проверить его работу при этом без риска того что будут какие-то проблемы у реальных конечных пользователей процесс выкатки или гизы у нас происходит следующим образом у нас есть или из машина который автоматически отслеживает количество коммитов и по достижении какого-то лимита автоматически отводит новый ли из запускаю сборку новой версии когда сборка закончится он автоматически запускает нагрузочное тестирование при этом одновременно все катится testing там проверяется стабильность работы если все нормально ничего не падает то этот релиз выкатывается prestij был тут помимо стабильности еще проверяются тайминги лизма степи тем как хотите production смотрит на тайминге различные бизнес метрики там ошибки и так далее и когда из мастер увидел что нагрузочное тестирование не показала деградации и в при стебли тоже все нормально он нажимает одну кнопку и released автоматически катится в продакшен приедет чем этот последний пункт можно сделать автоматически но мы пока не рискуем в конце подведем итоги дублирующий мониторинге это хороший способ дополнительно отслеживать стабильность автоматизация модель экономит много времени при разработке выкатки релизов shadowcaster позволяет разработчикам тестировать продакшен окружении сервис для мгновенного изменения конфигурации помогает проверять новую функциональность авто мы автоматизации лиза позволяет экономить много времени и делать частые релизы и все в совокупности также позволяет экономить много времени спасибо за внимание евгений спасибо большое же благодарность от оргкомитета вот программного комитета и подарок он поможет тебе снять то волнению которую мы все уже закончилось хорошо ну скажи штырит все хорошо ребята у кого есть вопросы я вижу руку вот по центральному проходу а потом на первом ряду 1 ряд ближе хорошо начнем с 1 ряда спасибо за доклад вопрос достаточно глупый но я не могу не спросить второго шанса не будет стоп-кран на танке есть стоп-кран на танке выключен специально отлично добрый день спасибо за доклад я тут центру подскажите пожалуйста шадова кластер вообще вот эта структура кластеров я мог пропустить но как там насчет системы хранения данных то есть они используют что-то общее у каждого свои если данных много свою тянуть проблема в чем на shadow выкатили не очень аккуратный код если он уходит в живую в систему хранения все про легла под большой нагрузкой стала печально нет у него свой индекс который лежит у него же он может ходить в какие-то внешние сервисы для получения какие-то дополнительных данных но ничего не сохраняет ну а не он пишет там блоги блоги соответственно проявляются опять систему мониторинга но то есть риска что от неаккуратного запрос он все сложат нет нет вообще никакого seba по центру зала еще вопрос есть спасибо за доклад часть вопроса в принципе один и ответил такой вопрос от приз ты был вы говорите что у него есть проблема он часть запросов на себя забирает не такую получается что с продакшном может оказаться что-то запрос тяжелый ушел на продакшене уже а почему не сделать так чтобы на него просто вот так же как наша да просто дублирующиеся запросы шли и от него ответ не уходил клиенту замеряли тайминги смотрели будет работать до так можно но это будет минус один кластер в принципе ничего не мешает и второй вопрос тогда по поводу обновлений вот вы говорите что звуки потом раскидывает обновление на сервер а потом по очереди каждый сервер обновляется до что происходит со схемой данных вот базу данных то есть какие-то если изменения добавления колонок alter требующего у нас нет база данных какой база данных имейте виду индекс поисков ну откуда вы забираете дам индекс он лежит на самих серверах звуки трек просто информации запись о том что появился новый новый новый индекс и все то есть от одна строчка буквально похоже на просто есть что обсудить не только инфраструктуру и на архитектуру поиска давности запрос третьем ряду спасибо евгений меня тоже зовут евгений у меня вопрос тоже продолжение прошлого по поводу структуры данных я правильно понимаю ей и бег это разные проекты и по поводу изменений схемы запросов получается заблочено вы не можете какую-то схему поменять да и я как эта проблема еще пока не назрела я правильно понимаю вас ну с не сильно актуально то есть стараемся делать всё синхронно хорошо 2 когда вопрос у меня касаемо вот того что вы собираете кучу метрик прям большое такое количество это очень хорошо а кто является скажем так клиентам кому и тисе метрики нужны кто их читает есть ли какие-то детекторы аномалий вот когда есть и там есть у нас своя система уведомлений когда какие-то номоли происходят приходит уведомление об этом а клиентами кто читает эти данные мониторинга вы и какие выводы на основе этих данных dell своя система уведомления читает их ну это прямая давай помимо этого люди что смотрят на графике а выводы они какие-то делают из них да конечно ну там что-то ускорило что-то замедлилась я иногда смотрю на графике для удовольствия честно скажу вопрос да и вижу на втором ряду очки позволяют можешь немножко подробнее про стоп-кран сказать как вообще происходит если плохой выкатка релиза я такое что как-то резко происходит откат это как-то пользователь увидит или как вообще то смотри стоп-кран а по сути binary один и тот же то есть у нас вкатывается один пин binary он в продакшене но там есть новая функциональность который просто не включена стоп-кран он включает эту новую функциональность соответственно если что-то пошло не так то допустим report упал the ballas терпи направит это запросто другое место и пользователь ничего не увидят я ответил на вопрос или уточнение поступил вопрос еще вопрос и отлично здравствуй спасибо за доклад а вопрос по а по тестированию кто инициирует сам тест анализирует результаты принимает решение оставлять изменять вы хотите много разных групп то есть есть группа которая делает какой-то новый функциональность она заводит абэ тест потом уже на основании результатов там принимать решения катить не катить там постоянно продакшен или нет получать именно тот кто разрабатывает фичу тот и ее выкатывает анализируют когда да еще вопросы поднимите руку у кого есть вопрос я если они вижу скажите об этом озере евгений такой вопрос тебе понравился больше про оба тестирования еще раз правда тестировал раба тестирование автор выходи пожалуйста на сцену как тебя звать где работаешь почему об этом спрошу спасибо меня зовут алексей работы в компании плеск спрашивал потому что мы сейчас тоже начинаем экспериментировать с оба тестированием и собственно возникает вопрос кто их проводит у меня есть ну как сказать ощущение что проводить тест и должен тот кто собственно пичу делает но есть разные мнения по этому собственно интересовался ну и ваш ответ собственно потерпел что наверное надо чтобы тест анализировал тот кто фичу тоже не отвечает ну да они же лучше всего за не понимает что происходит спасибо огромное"
}