{
  "video_id": "ZGAHlGfW1yw",
  "channel": "HighLoadChannel",
  "title": "Жизнь после шардинга / Михаил Курмаев (Badoo)",
  "views": 5066,
  "duration": 2622,
  "published": "2017-12-11T01:31:31-08:00",
  "text": "еще раз привет прошу прощения заминку на самом деле это было специально чтобы народ подтянулся меня зовут михаил курмаев я руковожу командой этим в компании bodum наша команда занимается разработкой я айти инфраструктуры мы делаем инструменты которые помогают разработчикам из других наших команд реализовать функционал который видят наши пользователи сегодня мы поговорим о нашей борьбе с не консистентной стью я очень хочу чтобы вы поняли что даже сложные задачи можно решать довольно простыми методами и я считаю что в этих вопросах главное понимание того как это все устроено они реализация реализация она вторична поэтому в моем докладе не будет ни одной строчки кода и я постараюсь обойтись без сложных схем и терминов когда мы говорим об архитектуре мы задаемся вопросом а по каким же критериям и и оценивать вы наверняка слышали про критерии быстро дешево качественно дальше обычно добавляют выберите два любых из них мы буду используем другие критерии наши критерии это просто то экономия ресурсов и удобства пользователей я сейчас не буду подробно вдаваться в описании давать какие-то определения в дальнейшем вы поймете что я имею в виду и как эти критерии связанные между собой но эти три критерия будут помогать нам выбирать правильные решения и так для чего нужен sharding если коротко sharding нужен тогда когда у вас все не вмещается в одну базу данных у нас больше 300 миллионов пользователей сотни терабайт данных и мы не можем просто взять и купить одну базу и все туда засунуть поэтому sharding нам нужен на конференциях allowed уже подробно рассказывали о том как устроен наш шар ding поэтому я не буду останавливаться я лишь просто кратко напомню общие принципы и так данные у нас родится по пользователям мы большинство основным основное основные данные пользователей складываем в так называемый спот spot это набор табличек с одинаковым суффиксом в одном споте лежит около тысячи пользователей и структура всех спотов одинаковая такая схема позволяет нам в одну физическую машину запихнуть запихнуть кучу спотов этом эти слоты друг с другом никак не взаимодействуют такие машины мы называем дпс у нас два больших дата-центра один в европе другой в америке между этими дату центрами километры тысячи километров воды под названием атлантика в каждом из им дата-центров находятся десятки дпс of теперь для того чтобы понять где же у нас лежит пользователь нам нужно две карты первое это карты соответствия юзера и диспута иди 2 это карта соответствии с путайте хост дпс эти две карты лежат на обоих серверах и постоянно синхронизируется давайте же оценим на шадрин с помощью наших же критериев он простой да мне понадобилась минута чтобы и объяснить вам основы экономит он ресурсы да он масштабируется линейна в доме для он для пользователей данном случае пользователя это разработчики админы он действительно удобен но каким бы хорошим sharding не был у него есть проблемы с консистентной что же такое консистентной консистентной это когда одни и те же данные в разных источниках одинаковые мне консистентной это когда данные разные все просто когда случается не консистентной скан системы content ность может случиться из-за ошибки разработчиков но это мы оставим отдела лукой либо случай каких-то системных сбоев этим как раз вопросом мы и будем заниматься если бы у нас была одна база данных мы бы решили эту проблему очень просто мы бы сделали транзакцию поменяли все в одной транзакции и баз данных буш заботилась за нас о консистентной стено данный у нас разные поэтому придется нам что-то делать самим зачем же нужна одинаковые данные хранить в разных базах в разных источниках давайте посмотрим на нашу схему в целом у нас есть спад и куда мы распихали основные данные и у нас есть некие вспомогательные сервера что на этих серверах например такой пример такого помогать иного сервера это поиск то есть для того чтобы быстро искать мы берем всех пользователей берем возраста пол местонахождение пользователя все это складываем в одну табличку и теперь можем настроить индексы и довольно быстро искать для того чтобы найти всех девочек москве нам не нужно уходить из-под и мы делаем это все одним запросом но теперь получается что у нас одни и те же данные лежат как спотах так и во вспомогательных серверах и люди очень странные существа они иногда привносят house сами того даже не подозревая например есть в европе городок такое называется бар ли он находится ровно на границе бельгии голландии границам прямо проходит по нему и человек гуляющий по этому городу за 10 минут может легко несколько раз пересечь государственную границу этом и состоит наше приложение но четко отслеживать местоположение и мы должны менять его данные в данном случае уже как спать и так и вспомогательным сервере при этом машина они общем-то не лучше они попадают то сеть теряют то происходит еще что-то и если посмотреть на все это с со стороны посмотреть на всю эту систему оно похоже на какой то большой муравейник там постоянно что-то происходит и кажется что невозможно учинить порядок в этом хаосе но мы это сделать смогли итак наша цель консистентной мы опираемся на три критерия и наш метод очереди но для того чтобы сказать что наш мир самый хороший давайте посмотрим если какие-то другие варианты на мой взгляд есть примерно три пути достижения консистентной sti первый путь это забить на консистентной что это такое мы берем нас есть два сервера мы берем первую транзакцию делаем делаем вторую транзакцию в первых пишем один в одну транзакцию во вторую пишем вторую транзакцию если все хорошо значит данные консистентные если все что-то пошло не так ну данный не консистентной что дальше происходит дальше пользователи как то наверное замечают это пишут саппорт саппорт пишет разработчикам разработчики напрягаются чинит это можно написать какие скрипты которые все это fixed но ситуация в целом остается как бы не очень давайте оценим простота да естественно экономия ресурсов вряд ли нам пьется разработчиков отвлекать на то чтобы все чинить удобства пользователя пользователь не любит писать саппорт поэтому вряд ли давайте посмотрим следующий путь следующий путь это распределенные транзакции двухфазный commit сразу скажу что этот путь очень сложный тут изображена очень сильно упрощенная схема того как это работает и в принципе даже если этот метод может подойти в каких-то отдельных задачах сделать из него инструмент которым можно бороться с не консистентную ну кажется проблематично поэтому сразу перейдем к оценку просто танец экономия ресурсов нам придется много думать и что-то реализовывать тоже нет удобства пользователей не понятно поэтому третий путь наш путь использовать очереди что-то сегодня проблема использовать очереди давайте посмотрим что же это такое и так возвращаемся к нашей изначальной задаче у нас есть данные на споте есть должны на поисковом сервере и нам нужно как-то поддерживать их консистентной с чем мы делаем мы добавляем специальную табличку которую называем очереди и при изменении данных мы записываем в эту табличку некую запись мы назовем это событие очень важно что бы эта очередь была на том же сервере что и изменяемые данные ты чтобы мы это сделали в одной транзакции дальше мы эти события как-то собираем некую общую очередь это может быть уже не мой скальп может быть любую кафка рыбе think you все что угодно и дальше что происходит дальше мы пишем специальный скрипт который запускается интер по крону этот скрипт и называем консьюмер или подписчик он читает записи в очереди дальше он лезет впервые источник есть нашем случае это spot зачем это нужно я объясню попозже обновляет данные в поиске и удаляет записи в очереди для того чтобы все это работало обработка событий должна быть и дым патент на и коммутативное я говорил что не буду использовать сложные термины но это первый последний раз обещаю что же за термин такие давайте вспомним уроке математики сколь а операция сложения является коммутативной от перемены мест слагаемых сумма не меняется операция взятия по модуль является идут по ted на этим один раз взяли по модулю число то потом сколько раз не бери результате равно получается тот же собственно как нам это обеспечить и для чего нам это нужно но обеспечиваем это собственно вот этим нашим вторым шагом для чего это нужно давайте разбираться давайте начнем с иным патент насти предположим мы пропустили второй шаг и прям в событии пишем а некие данные положим у пользователя случилось наступила радостное событие у него случился день рождения нам нужно увеличить его возраст в спорте и увеличить его возраст в поиске мы увеличиваем возраст по в спорте и кидаем события в котором так и пишем у пользователя случился день рождения все дальше от событие должно доехать до council мира консьюмер должен увеличить данные в поиске в принципе все произойдет хорошо то данные будут консистентная но может случиться что-то и третий шаг может не случиться в этом случае события остается в очереди следующий раз конце мира запускается забирает событий и прибавляет еще один год к возрасту так может случиться несколько раз в результате пользователь за короткое время очень сестра прибавит к воды в возрасте это событие ни один патент на нам же нужно чтобы она была in 1 патент на и так потребностью разобрались для того чтобы события был один патент на мы можем прямо в событий написать что возраст теперь равен тому то возраст теперь равен 21 год например давайте посмотрим что будет предположим день рождения случился у девушки не определенного возраста она считает что и всегда 20 лет но наша бездушная машина в очередной раз выставляет ее возраст на ее день рождение в 21 год она привычки лезет в profile и меняет обратно возраст на 20 лет в этом случае мы получаем два последовательных событий одно возраст равен 21 год 2 возраст равен 20 лет эти события не коммутативное важно их порядок но почему порядок может поменяться дело в том что мы хотим уметь откладывать события если мы взяли событие понимаем что ничего с ним сделать не можем мы его откладываем обработаем клонить попозже это может случиться из того что что-то недоступное еще что то ситуация разная бывает мы его откладываем порядок меняется результате получается что в этом случае порядок поменялся и в поиске у нас данные другие получили не консистентной итак для того чтобы очереди работали хорошо обработка событий должна быть in патент на и коммутативное давайте еще раз посмотрим на нашу схему у нас есть спад и у нас есть помогать мне и сервера у нас появляется некая центральная очередь и консьюмер и которые выгребают из очереди данные и каждый что-то делает давайте оценим простота да если следовать никаким требованиям то все довольно просто экономия ресурсов да мы добавили в сервер очередей но в целом все работает удобства пользователя ну пользователи терне пишут саппорт все удобно единственное чем мы пожертвовали это мгновенным обновлением то есть мгновенной доставкой событий но в подавляющем большинстве задач и это не нужно если в поиске у пользователя данные обновятся через пару секунд никто этого никогда даже не заметит но а мгновенным о мгновенном доставки событий мы поговорим чуть позже давайте пойдем дальше дальше мы делаем свою репликацию когда то давным давно у нас была у нас был один дата-центр он находился в европе все было хорошо но пользователи в америке страдали pink между атлантикой 200 миллисекунд мы пошли навстречу мы добавили это центра в америке и у них стало все хорошо но предположим вы сидя в москве смотрите девочек из бразилии вы получаете те же самые проблемы все работает не очень быстро мы пошли вам на встречу мы добавили в каждый сервер реплики то есть с 1 s1 до центром o реплицирует данные в другой этот центр мы назвали эти реплики д б б и реплицирует полос потом то есть нет bsd bbs под под таким образом получается что схема она такая что мы несколько дпс можем реплицировать в 1 tb и наоборот несколько 1tb сможем реплицироваться в несколько д.б. схема такая довольно сложная и москвой в то время это делать не мог сейчас наверное тоже не может не знаем поэтому мы решили написать свою репликацию как она выглядит и так у нас есть спад у нас есть реплика на споте происходит какое-то обновление данных обновлений данных естественно происходит какой-то транзакции как же нам сделать репликацию мы добавляем специальную очередь который мы называем reflection кью и прям в нее пишем те запросы которые мы делали на споте структура реплики точно такая же поэтому мы их можем применить на реплики мы делаем специальный транспорт по которому все эти данные уходят на реплику и там применяется там выполняется те же самые запросы таким образом получается самописная степман без репликация по сути дела тут тоже самую очередь единственное что природа искренне дает нам сделать так чтобы обработка этих очередей подчинялась принципам и еду патент на стелi коммутативности тут мы должны гарантировать последовательность и доставку один раз поэтому эти очереди немножко сложнее но один из минусов соответственно если что-то где-то запирается у нас запирается вся очередь мы не можем пропустить одно событие давайте же оценим просто там в принципе очереди немножко посложнее но тем не менее довольно просты экономия ресурсов мы добавили db сервера но зато у нас теперь еще есть и бэкап с и что можем из них установиться удобство пользователей да вы теперь можете смотреть девочек бразилии давайте посмотрим как же что же случилось нашими очередями с появлением второго дата центра итак у нас есть два это центр европа америка на каждом какой-то количестве без какое-то количество д б б и возьмем задачу задачу модерации все фотографии которые загружают пользователь нам нужно от модерировать мы моделируем только на одном дата-центре в европе нам так удобнее поэтому если пользователь из америки загружает фотографию это событие приходит в европейский дата-центр сейчас позже мы поймем как она приходит что должен сделать консьюмер консьюмер исходя из принципов или патент насти коммутативности должен залезть за впервые источник то есть дпс на другой дпс на другом это центре это далеко мы вспоминаем что у нас есть реплика мы можем прочитать из реплики но дело в том что тогда мы должны гарантировать что событие дойдет после репликации для того чтобы когда консьюмер его начал обрабатывать он смог залезть в реплику и прочитать там уже актуальные данные то есть нам нужна синхронизация и так как же сделать от синхронизации у нас есть д б с а д б б у нас есть две центральные очереди на каждом дата-центре что мы сделали мы сделали мы сделали ошибку мы решили что нужно сразу сделать оптимизацию мы сделали собственного канала с репликации и мы сделали еще один канал для сообщений мы решили что не надо забивать канал репликации всякой мелочью чипа типа непонятных событий он должен работать четко поэтому мы сделали отдельный канал я не говорю что мы там пропустили отдельные специальные кабель по дну атлантики ну просто отдельные скрипты которые работают дальше нам нужно как то это все синхронизировать мы сделали синхронизацию как она работала мы событие пишем специальную метку эту же метку мы добавляем репликацию и когда события приходят на другую площадку мы проверяем vdb а дошла ли уже репликация звучит это очень страшно реализация общем-то тоже была такая и если вдруг где-то репликация затупляет а скапливалась очередь на синхронизацию все работало так довольно не очень скажем так давайте оценим этот метод просто то в общем то нет нам пришлось сделать сложную систему синхронизации экономия ресурсов тоже нет потому что нам собственно все это поддерживать и все это ломалось и на все это уходило дополнительные ресурсы но удобства пользователя да наверно мы подумали и решили все упростить мы подумали что нужно события кидать через репликации звучит как-то странно но давайте подумаем как же это сделать итак у нас есть спорт у нас есть реплика у нас есть очередь событий которая со спорта собирается в центральную очередь на противоположный на противоложное за центре у нас тоже есть центральная очередь событий теперь нам нужно что то сделать в нашей транзакции для того чтобы событие очутилась на противоположенном дата-центре нам нужно что нам нужно сделать нам нужно добавить в репликацию добавление в очередь события звучит как-то очень сложно но давайте посмотрим как же это выглядит мы собственно добавили в реплику еще одну такую же очередь как на споте тоже собираем с нее данные и теперь что мы делаем мы добавляем в репликацию запрос на добавление в очереди получается такая insert матрешка но она работает очень хорошо то есть мы просто добавили insert репликацию когда он приходит на реплику запрос выполняется и события попадает в нашу очередь на реплики дальше мы его собираем и тут уже не нужна никакая синхронизация событие гарантированно придет после репликации все будет хорошо давайте оценим простота да мы упростили все экономию ресурсов да мы все сделали гораздо проще мы выкинули кучу кода связаны синхронизации и мы сделали удобным для пользователей дальше мы поговорим немножко о оптимизации давайте представим себе задачу они пример мы для того чтобы все работало быстрее контакты пользователи храним прямо у них на споте в день где нормализованном видит то есть если маша зашла на баду и познакомилась с 8 петель и джоном мы прямо у неё споте храним этот список к ним их имена для того чтобы показывать эти контакты быстрее и так маша зашла на году за познакомился оси петель джоном но она замечает что с васей петь и разговоры не не клеится они почему-то познакомились друг с другом не внимания не обращают джон на нее тоже особо внимание обращает он занят какими то другими девушками маша идет на чайный шаг она меняет свое имя на мария 3 сердечко ну в общем то справедливо она надеется что таким образом она привлечет все внимание нам же соответственно нужно поменять ее имя в спотах у других пользователей давайте посмотрим как это можно сделать это можно сделать с помощью наших очередей мы кидаем события события уходит на центральный сервер там размножается в три события и дальше каждое это событие приходит в спад и нашим пользователям и обновляет там имена все хорошо но есть проблема в том что спад у нас постоянно растут сервер очередей он как бы остается один можно его оптимизировать но все равно получается такая единая . мы подумали и решили что раз мы доставляем спад и вас мы увеличиваем дпс эту пусть они занимаются очередями мы оставили старую схему как есть она работает и справляется со своими задачами сделали новую мы добавили еще дополнительные очереди на каждый из потов и выглядит это теперь так маша меняет имя тут же кидается события тут же не дает не уходя со спота она размножается в три события и уходит по с потом обновляя имена маша петь ее самое главное джону таким образом получили децентрализованную систему которая не зависит от какого-то центрального большого хранилища давайте оценим простота ну эта система чуть сложнее чем предыдущие но она при этом решает свои задачи экономия ресурсов да мы сэкономили ресурса центральных серверов удобства пользователя да маша счастлива я говорил когда я говорил про очередь я говорил о том что мы жертвуем мгновенной доставкой и выше задач действительно мгновенно доставка не нужно но есть отдельные задачи в которых задержка она очень критично например событие в чате вы можете спросить причем 10 как бы чат и ассистент ность дело в том что для того чтобы сделать нормальный чат нужно записать события в отправителю и записать события получателю соответственно мы пишем события отправителя и синхронизируем эти данные получателю получается тоже самой синхронизация я говорил что мы пожертвовали рилтайм событиям но давайте посмотрим может быть и тут что-нибудь сделаем итак вспоминаем схему у нас есть источник у нас есть приемник в источнике мы что-то делаем записываем в очередь дальше у нас консьюмер который берет из очереди и сохраняет в приемнике на самом деле мы работу консьюмер а можем выполнить прям в том же скрипте когда мы записываем в источник таким образом таким образом после завершения первой транзакции мы записываем в приемник и удаляем из очереди если все прошло хорошо то мы по сути дела мгновенно доставили события если что-то пошло не так где-то там оборвалось то тут уже будет работать наш консьюмер он возьмет из очереди и все равно доставит то есть для подавляющего большинства событий мы сделали мгновенную доставку и так как я говорил наша цель была консистентной мы выбрали метод очереди и на основе него сделали некие инструменты о которых я говорил раньше но любой инструмент он требует постоянной проверки и в нашем случае проверка это мониторинг мониторим должен быть обязательным что же нужно мониторить для меня вся вот эта система очередей она выглядит как система резервуаров и труб между ними события как вода и они переливаются из одного резервуара в другой вытекают и так далее собственно для того чтобы мониторить мы должны мониторить уровень жидкости в резервуарах чтобы вода не выливалась у нас каждый резервуар имеет своего ответственного каждый reserva выполняет определенную функцию и у нас есть два уровня нотификации первый это когда мы начинаем писать письма 2 это когда мониторинг начинает звонить будете среди ночи эти уровни они настраиваются отдельно для каждого для каждой очереди их настраивает собственно владелец и таким образом мы исключаем ситуацию когда что-то вдруг неожиданно сломалась из неочевидного что нужно мониторить это самые старые события как я говорил мы можем откладывать события и в свое время мы этот параметр не мониторили и мы находили события которым было по полгода а каждое событие это где-то не консистентной соответственно после этого мы стояли мониторить события по их возрасту этот параметр тоже настраивается и все хорошо мониторинг нам говорит о том что что то сломалось но где именно сломалась он нам обычно не говорит тут написано графики для того чтобы понять где именно сломалась часто приходится делать какие-то дополнительные исследования в этих исследованиях нам помогают графике мы рисуем графики на все что можно если мы опять возьмем наш резервуар с водой то мы понимаем какие графики нужно рисовать нужно рисовать графики объем воды графики всех входящих потоков и исходящих соответственно если что-то сломалось мы видим что что-то сломалось мы понимаем когда начался собственно рост в очереди запоминаем это время дальше смотрим другие графики и видим что вот в данном случае входящий поток увеличился значит проблема где-то там очереди оттуда начали начали больше поступать итак друзья мы разобрались какие проблемы возникают в результате хардинга поняли что очереди являются простым и эффективным методом борьбы с этими проблемами мы буду активно используем очереди и надеюсь вам они помогут ваших проектах большое спасибо за внимание с удовольствием отвечу на ваши вопросы спасибо большое за идея троен кирилл анастасию да просто красавица за презентацию отдельное спасибо поехали а здравствуй михаил спасибо за доклад мне зовут слава мне два вопроса клон 1 искать и что вы реплику используйте как бы как вы наверное говорили здесь потому что ход реплика это не backup ну если вам ужасно рошана из прошлого ты это восстановить данные с прошлым на самом деле у нас была один раз когда мы восстанавливались езды bb шик когда дпс у нас умерло и нам пришлось вы становитесь дабы бывшим то есть ну да спасибо и второй вопрос если у вас очередь все равно вот на локальной машине на споте вы не думали над решением просто читать бинарный лак и уже оттуда класть и вы думали до сих пор думаем но это на самом деле мы думаем как это реализовать но вот прям вот сейчас пока не может просто у нас как раз именно так сделанному бинарного почитаем потом в кафку это все работает спасибо большое спасибо спасибо за доклад скажите пожалуйста добрый день но стихи вопрос оля секундочку сейчас первый ряд потом вы если бы машина прям аж 3 сердечко 10000 друзей и она три раза подряд поменяет имя значит лишь то очередь попадет 30 тысяч сообщений да и если какое-то из редиса этих очередей на из ее друзей не доступен то эти очереди у нее будут крутиться по кругу ключей просто попец и когда у нас сложиться к этот бск на соответственно копится очереди входящие на остальные dbsk то есть это они не такие большие получаются то есть не знаю там уж десятки тысяч нет не так страшно спасибо и еще и почему для обновлений очередей между европой и америкой не используется та же репликация то есть если на америке они обновились почему просто не за реплицировать их на европу передавать эту очередь репликация для переноса очередей не для очередей то есть обновить очередями равной америке например а потом из спад а в дбд bb за реплицировать обновленные данные но нам там нужно соответственно что-то после этого сделать например у нас произошло событие a replica ушла а дальше там нужно что то сделать это не из не изменить не просто изменить данные это как бы какие-то действия . де рации например понял спасибо следующий вопрос спасибо за доклад любая модификация данных это ну как бы завис с это событие в клея то есть в очереди сколько вы приблизительно накопили уже различных уникальных событий как таковых не кажется насчет событий не скажу я могу сказать что бы приблизительный консьюмер of у нас где то наверное в районе пару сотен а то есть вы именно делите консьюмер awp однозадачный я не сказал у нас на самом деле используется published up strider то есть у нас есть события и соответственно консьюмер подписываться на нужные события но консилеров у нас пару сотен событий ну наверно примерно с тобой столько же либо больше она взялась его здрасте михаил спасибо за интерес доход я прямо перед вами прям так не вставая и без очереди сейчас подождите добрый день я прошу прощения смотрите а если маша зашла через какой-нибудь пьян находясь в европе как будто бы из америки а под и например удалила свою связь с джоном а потом безлепкин отправила что она ему поставила там какой-нибудь плюсик или еще что-нибудь я по поводу конфликтов которые могут возникнуть при при такой репликации что мы что-то что-то убрали вообще а потом на это пришел апдейт как вы с этими конфликтами разбираетесь на самом деле сказал что там систему чуть сложнее и у нее действительно есть ли способ засекать события то есть мы специально сохраняем логику да мы отправили события и грубо говоря мы можем то есть мы изменяем контакта дальше мы посылаем специальные sing события которое было можно послать по тем событиям по тем приемником куда уже посылали ну то есть там разрыв времени то остается понимаете я пример что вы можете послать но здесь условно говоря я сделал делить да и следом придёт апдейт ну на то чего уже нет но смотрите во первых david он на не делить он просто выставление статуса но я понимаю соответственно то есть маша маша удалил джон у нас контрактов а дальше решила опять его добавить или что но находясь на дата-центре рядышком она нет она привязана к своему это центру она не может сама переехать то есть оно все и апдейты идут в и из-под бедный джон честное слово то есть вы хотите сказать что от того нахожу себя в европе или нахожусь я в америке это не играет с точки зрения много где где происходит первоначальная изменена марается потом всегда один хорошо тогда по главное чтобы вы не пользовались системой под именем маша фишка в этом у запретить не может можно да точно самые интересные 2 моментов в системах подобного рода это распределенной транзакции и перебалансировка ваши отношения в bodog к транзакциям понятно а вот про перебалансировку в как-то умолчали расскажите пожалуйста вы переносите spot очень просто мы их я врач это не просто потому что пока вы его переносите туда пишут и оттуда читают на самом деле у нас есть возможность переноса спотов мы для этого есть инструменты мы просто говорим о все пользователи на спад и они становятся в режим и reddon если они заходят они как бы видят что извините тут там технический вопрос но сейчас мы не используем этот инструмент у нас есть инструмент миграции пользователей он более такой щадящий он переносит пользователя когда он в оффлайне и мы для того чтобы перенести spot можем со спота этого которое нам нужно перенести перенести все пока на другие споты а потом просто перенести пустой спад на которые не идут данные и дальше уже в этот спад опять на переносить пользователи то есть система такая очень гибкая можно и так можно и так вот добрый день михаил большое спасибо за доклад крутую систему сделали появился вопрос а пробовали рассматривать другие варианты баз данных или это вот та но чем развернулись принципиальное решение от другие варианты какие по сброс до например но дело в том что сейчас очень тяжело взять и все перевести на по сгрыз потому что ну проекту больше 10 лет и и вот взять и все перенести ну то есть нового необходимости не видим про вопрос был именно первую очередь с тем что исторически так сложилось и уже работали с тем что есть да ну и не научились на самом деле смазку или мы научились и обрабатывать очереди хотя там тоже есть проблемы ну если вы репликацию дописали сами как бы понятное дело что остальные тоже научились спасибо друзья есть вопросы еще а вот пожалуйста а почему не были использованы носке или системы они же как раз решают проблему за публикацией ну потому что наши разработчики хотят писать es que el но и скоро система тоже есть у нас кое-где даже используется кассандра иногда действительно не хочет зачем менять просто хочешь новые платья до есть еще вопросы говорите сейчас или замолчите навсегда хорошо спасибо большое спасибо"
}