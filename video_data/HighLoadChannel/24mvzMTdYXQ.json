{
  "video_id": "24mvzMTdYXQ",
  "channel": "HighLoadChannel",
  "title": "Как мы заставили Druid работать в Одноклассниках / Юрий Невиницин (OK.RU)",
  "views": 1046,
  "duration": 2707,
  "published": "2018-08-16T04:35:44-07:00",
  "text": "всем привет меня зовут не венеции юрия из одноклассников и занимаюсь системой внутренней статистике кому нравится мысль о поддержке 50 терабайт не аналитической real-time его системы основанный на microsoft sql server в которую ежедневно лаги руются миллиарды событий в сутки я расскажу про наш кейс миграции такой системы с escuela на скалу ночную базу под названием druid-а вы узнаете несколько рецептов его использования зачем нам вообще нужно статистика она нужна нам потому что мы хотим все знать про свой сайт поэтому мы лоббируем не только поведение железа трафика дисков в циpкa но мы лоббируем каждое действие пользователя каждое взаимодействие между подсистемами и все внутренние процессы практически всех наших систем система статистики тесно интегрирована в процесс разработки наши менеджеры ставят цели отслеживают их достижения смотрят какие-то свои ключевые показатели администраторы разработчики следят за работой всех систем расследует инцидент и аномалии автоматический мониторинг постоянно смотрит и на ранней стадии выявляет какие-то неполадки и строят прогнозы по превышению каких-то лимитов также постоянно у нас идут какие-то запуске fitch эксперименты апдейты что-то меняем и эффект от всех этих действий мы отслеживаем через систему статистики по этому нам без нее никак если она отваливается то мы не можем выкатывать изменения на сайт статистика у нас представлена в основном в виде графиков обычно мы показываем сразу в несколько дней на графике чтобы визуально было видно как идут дела хуже лучше или как обычно здесь график моих экспериментов с друидом я притормаживал загрузку и смотрел как быстро он сможет загрузить накопленные данные также график можно разложить по параметру по-любому в данном случае по таблице и так же у нас есть долгосрочные графики которые показывают нам всякие годовые тренды здесь мы смотрим здесь например это рост количества записей вдует посуточной мы начинали с 13 миллиардов и сейчас у нас в районе 35 вроде так все не про а кстати когда у нас еще есть даже борды мы можем пачку графиков сохранить дашборд и удобно смотреть в графике по одной какой-то теме в принципе даже если пользователю нужно пару графиков посмотреть он все равно не открывает отдельные графики а открывает целиком дашборд что ещё усиливает нагрузку на систему в принципе пока объем был маленький нос и скулы справлялись но объем rose rose замедлялось загрузка замедлялось выдача графиков и мы дошли до такого объема когда у нас тяжелая какая-то отдельная статистика уже начала отставать в час пик на полчаса и при этом в среднем время ответа одного графика было шесть секунд ну что такое 6 секунд это значит что кто-то получал график за 2 секунды а кто-то за 10-20 а кто-то и за минуты и когда ты раз следующий инцидент какой-то то тебе надо открыть и посмотреть десяток графиков каждый следует из предыдущего поэтому ты не можешь их открыть одновременно все и поэтому нужно 10 раз ждать от по 10 20 30 секунд и это все дико бесит вот ну можно было что-то как-то выжимать еще производительность сервера доставлять но примерно в это же время microsoft поменял политику лицензирования и если бы мы продолжили использовать sql server то нам пришлось бы отдать миллионы долларов поэтому мы решили сразу сделать все как надо а именно мы хотели чтобы у нас статистика не отставала чтобы график открывался не больше чем 2 секунды а дашборд целиком не больше чем за 10 секунд ну и конечно мы хотели чтобы это было отказоустойчивого мы хотим переживать потери дата-центра мы хотим легко доставлять машин и мы хотим иметь возможность модифицировать затачивать это под себя поэтому мы хотели чтобы это было еще и на java собственно только друид именно это все нам и предлагал также в нем есть при до грига ция который позволяет еще немножко сэкономить объем и индексация во время вставки данных и он поддерживает все те типы запросов которые нам нужны для работы нашей статистике поэтому казалось что мы легко можем подставить друид место sql server и разумеется что мы рассматривали не только друид вообще первая мысль которая возникала каждый раз когда что то тормозит это давайте уберем microsoft sql server и поставим туда по отгрыз ну деньги цезарь и sheets решит но всех остальных проблем это не решит не решит проблему с доступностью не решит проблему с масштабированием также смотрели influx новый flux оказалась та часть которая отвечает опять же за высокую доступность и масштабируемость она закрыта поэтому дальше мы его не рассматривали prometheus как бы при всем уважении к его производительности это все-таки система больше заточена под мониторинг и там совсем нет ни высокой доступности не масштабируемости и нужно как-то плясать чтобы это всё сделать посмотрели на append sdb но опять же система больше под мониторинг и там нету индексов по всем полям поэтому для нашей системы производительность опять же под вопросом также я добавил клик house в этот обзор потому что он всех интересует но мы его не рассматривали так как на тот момент его не был вот поставили мы druids мигрировали терабайт и наших данных вот буквально в день переключения из кулона друид у нас просмотр графиков выросли в 5 раз и дальше начали запускать всякую тяжелую статистику которую боялись раньше запускать ну понятное дело не отдали миллионы долларов и сопровождает наш друид команда из одного человека сейчас у нас друид в 12 нот забирает 500000 событий в секунду в час пик и при этом у нас есть большой запас прочности эти числа получены на продакшен данных это нет ни тесты ну вроде красиво выглядит но сейчас я расскажу как же мы этого добились через что пришлось пройти для начала я расскажу про сам друид он состоит из нескольких компонентов я быстренько по ним пробегусь и на схеме покажу как они взаимодействуют у движения есть несколько внешних зависимостей первое это сторож сторож же друид хранить свои данные и больше ничего не делает только хранит также ему нужна база данных для сохранения информации о том где у него в сторону что лежит также ему нужен звуки пир для discovery и также через у кипер друид объявляет на каких новых какие данные доступны для запросу и кэш сам друид также состоит из нескольких компонентов первый это real-time real-time надо загружать свежие данные и обслуживают по ним запросы 2 это историческая но доисторическая но до исторические ноды держит всю массу данных и обслуживают запросы по ним когда мы говорим что у нас кластер на 300 терабайт мы говорим именно про исторические ноды в них 300 терабайт и они по ним ходят и что-то считают брокер отвечает за распределение вычислений между историческими и real-time алыми нодами координатор отвечает за распределение данных между историческими нотами и за репликацию и также есть индексируются сервис который позволяет в бэкграунде с помощью task-ов выполнять какие-то задачи в том числе загружать данные как это взаимодействует вот у нас данный real-time берет эти данные индексируют и нарезают на сегменты по времени ну например по суткам сегмент это минимальная неделимой единице данных друиде все операции с данными запросы выполняются по сегмент на каждый новый сегмент real-time но до записывают в сторож и сохраняет копию себя чтобы обслуживают запросы по нему дальше она записывает метаданные что в сторону же по такому-то адресу появился новый сегмент и координатор периодически эти метаданные перри вычитывают и когда он находит новый сегмент он приказывает через в гипер и нескольким по количеству реплик историческим нодом скачать эти данные исторические но ты их скачивают и также через гипер объявляют что у них этот сегмент появился и когда real-time получает это сообщение от звуки пера то она удаляет свою копию данных чтобы освободить место для новых данных запросы обрабатываются следующим образом в них участвует всего три типа но ты-то брокер real-time исторический запрос должен прийти в брокер у брокера есть полное знание о том на каких новых какие сегменты находятся и он разрезает запрос по сегментам и распределяет по нодам таким образом каждая нота выполняет свою часть вычисления и за счет этого друид может обрабатывать большие объемы данных очень быстро а также за счет колонн личности хранения дальше все эти ноты отдают ответы обратно брокеру и брокер отдает их обратно клиенту когда мы говорим про высокую доступность а другие нам обещает высокую доступность и там у него в списке зависимости есть база данных для метаданных которые бывает москву или под gris там еще есть apache дерби но это нельзя использовать для продакшн это только для девелопмента сразу возникает вопрос что будет если откажет моя скуэр собственно будет вот что real-time но да не сможет записать метаданные соответственно координатор не сможет их перечитать не найдет новый сегмент исторической надо его не скачает и real-time не удалить свою копию но продолжит качать свежие данные соответственно начну данной капицы в real time нотах и понятно что капица они могут не бесконечно но тем не менее мы знаем какие у нас есть доступные ресурсы на real-time но дух и мы знаем какое у нас поток данных поэтому у нас есть какой-то запас времени предсказуемый за которой мы можем починить наш mais que al и ровно то же самое будет происходить при отказе с тораджи и координатами мы кроме мы решили перестраховаться и мы мои sqweel заменили на кассандру так как она дает высокую доступность из коробки и кроме этого мы ограничили real-time в ресурсах таким образом чтобы когда данных в ней в нем накопится слишком много то удаляются самые старые данные и освобождается место для новых и для нас это очень важно потому что случай когда мы долго не можем поднять май скул и соответственно данных накапливается много скорее всего происходит когда происходит какая-то большая авария и следовательно самые свежие данные важнее всего в этот момент звуки пиром все не только лучше но и хуже лучше потому что закипит сам по себе отказоустойчивый и у него из коробки есть & replication и казалось бы что может случиться ну вот когда мы работаем с гитлером мы хотим быстро узнавать об изменениях и в том числе если какая-то но до зависла то мы хотим быстро в этом узнать чтобы не запускать запросы в пустоту соответственно мы ставим маленький тайм-аут друид работает звуки первым таким образом что когда у него происходит тайм-аут он переподключается на другой install звуки пера и перезаписывать туда все свои данные и перечитывают все данные пера и хранит он там информацию о том на каких исторических нодов какие сегменты находятся и когда сегментов становится много да еще и в 3 репликах то данной группе перри тоже становится много слишком много у нас вот когда были эти проблемы snapshot вы keepers достигал 6 гигабайт и что происходит в этом случае допустим падает или зависает мода звуки пера все клиенты которые были к ней подключены друиды отключились пошли на другой instant звуки пера и начали перезаписывать свои данные и перечитывать остальные как их много тот трафик на звуки пивных но дух поднимается в потолок и это провоцирует тайм-аут и на других клиентах другие звуки другие друиды начинают отваливаться перезаписывать и перри учитывать и так эта лавина трафика растет вплоть до того что звуки пир может потерять синхронизацию между своими собственными нодами тогда он начинает гонять снапшоты туда-сюда но что в этот момент видит пользователь по факту оказалось что когда брокер в частности теряет связь звуки первым он теряет свое знание о том где какой какие сегменты на каких исторических но дух лежат и поэтому он выдает просто пустые ответы то есть по факту если звуки пир лег то и друид не работает вылечить это полностью нельзя но что можно сделать первое что нужно знать что можно удалять данные пера они там не пуск ну ничего страшного если они потеряются друид просто их перезапишет и если проблема звуки первом уже началась то это гораздо быстрее пустить звуки пир удалить данные поднять его пустым чем ждать пока она рассосется сама в этом случае теперь когда мы это знаем мы немножко повышаем тайм-аут и чтобы у нас на каждый чих на каждый флаг эти перезаписи вот этот весь трафик не начинался соответственно в этом случае происходит другая ситуация допустим историческая но-дори стартует некорректно когда она или стартовала некорректно она не удалилось вторую сессию и пера и она начала создала новую сессию записала туда кучу данных и пока старая сессия еще жива пока тайм-аут не прошел в руки перри хранится две копии этих данных если таких not рестарта нет много сразу то соответственно много данных будет продублирована поэтому нужно для звуки передержать запас памяти чтобы она просто у него не кончилось так как если она кончится но он работать перестаёт и по этой же причине нужно обязательно корректно завершать исторические но ты так как они в этот момент удаляют свои данные и перым причем они это делать могут долго нас shutdown исторических not занимает примерно полчаса и это лучше лучше подождать чем другой вариант есть еще такая особенностью исторических not когда они стартуют они с диска поднимают все свои данные смысле не данные о смотрят какие у них есть сегменты и потом информацию об этом записывают звуки пир и так как данные по историческим нодом размазанным или менее равномерно то если их стартовать одновременно то они и внуки пир начнут писать одновременно и опять это риск того что поднимется трафик и все начнут тайма учить поэтому их стартовать нужно с какой-то паузой чтобы разнести в момент записи в зуке пир по времени кроме этого мы сделали еще две вещи мы перепрограммировали немножко работу звуки перрон так чтобы друид вычитывал данные пера только те ноды которым они нужны а нужны они только real-time брокер и и 100 и координатором а как раз историческим но там все эти данные не нужны и не нужно знать о каких других исторических not как и есть сегменты и также все это ненужный indexing service у и всему маркером которых там может быть много и еще мы из тех данных которые пишутся в звуке пир мы убрали все лишнее и оставили только то что необходимо для выполнения запросов это сократила объем данных звуки перри в три раза там очень много лишнего пишется и вот эти две меры оранжевые сократили вот тот трафик который может возникнуть или примерно в восемь раз соответственно ну мы минимизировали возможность плохого сценария насколько могли теперь расскажу как происходит загрузка в друид посмотрите пожалуйста на схему загрузки данных по ходу загрузки друид чтобы освобождать память периодически сбрасывает данный на диск по частям и чтобы обслуживать по ним запросы по этим частям он подтягивает их с помощью map ну и к концу загрузки одного сегмента таких частей там накапливаются много и вот с тем что их накапливается много есть два момента первый момент это то что real-time но да я сейчас говорю только про real-time может попортить до причем не только во время крыша g em или там какого-то ребута сервера неожиданного а даже во время обычного корректного рестарта и происходит это вот почему процесс сброса данных на диск состоит из двух частей это надо сбросить сами данные на диск и нужно куда-то сохранить позицию с какого места начинать после рестарта и эти две вещи они записываются совершенно независимо друг про друга они ничего не знают и конечно же не атомарный и дальше понятно в зависимости от того что потеряется у нас или потеря данных происходит или дублирование и избавиться от этой проблемы можно если не не использовать real-time ноды тогда нужно загружать данные с помощью indexing service а который в новых версиях друида умеет отвечать на запросы тоже поэтому в этом случае свежие данные будут доступны но вот когда мы ставили друид index in service не умел отвечать на запросы и нам пришлось просто это братья чинить слову нам это удалось и второй момент это вот связаны с тем что много частей сбрасывается на диск это деградация производительности по запросам чем больше этих частей на диск накапливается и особенно на тяжелых данных и тяжелых запросах тем мы еще хуже то есть как бы тяжелые данные понятно что тяжелее обрабатываются вот чем больше данных сброшена на диск вот во столько раз примерно хуже будут работать эти запросы по свежим данным чтобы в этой проблеме разобраться нужно понять как устроен вообще сегмент в друге день как мы по возьмем пример вот у нас здесь несколько хостов сбросили число вызовов за 2 пятиминутки как мы помним других база колон ночная поэтому соответственно эти данные будут храниться как-то колоночка ну вот у нас будет колонка времени это будет просто массив чисел записанный на диск там сегмент число вызовов также будет просто массив чисел и для хвостов будет создан словарик это будет отсортированный набор строчек имен хостов и сами хасты колонка хостов будет записано в виде массива чисел опять же который указывает нам на номер в этом словаре и для быстроты фильтрации по нашим данным так как у нас здесь могут быть миллионы строк и 1000 хостов там есть индексы индексы представляют собой bitmap и по одному bitmap луна каждый хост в нашем случае и в этом бит мат и единички указывают на те номера строк которые суд в которых этот хост собственное участвуют чтобы объединить если нам надо отфильтровать 2-х статус соответственно нужно взять два bitmap а объединить их через или и полученный bitmap опять же единичками укажет нам на номерах of номера строк и дальше нам нужно понять как работает запрос теперь по этим сегментом возьмём типичный запрос нам надо посчитать число вызовов по нескольким классам хостов общее друид при обработке запроса сначала он возьмет первый фильтр первую регулярку и ей он пройдет по словарю по всем там тысячам хостов найдет нужные из них которые попадают под маску возьмет bitmap и им соответствующий объединит и сохранит промежуточный bitmap дальше он возьмет вторую регулярку второй фильтр сделает то же самое пройдет по словарю возьмет bitmap объединит получит промежуточный 2 биг мам и так для всех фильтров получили пачку промежуточных bitmap of опять же объединили в конечный bitmap и он уже нам покажет какие именно строки нам нужно просуммировать и как вы думаете в какой пропорции вот ресурсы потраченные на этот запрос типу в какой пропорции разделяются между фильтрованием и между фильтрованием и между подсчетом суммы вот я взял профайлер и выяснил что еще хуже пять процентов времени мы считаем сумму 95 процентов времени мы фильтруем и теперь посмотрим что происходит когда мы сбрасываем данный на диск во время загрузки вот наши данные мы начали скачать качаем качаем сбросили часть данных на диск у нас получился такой мини сегмент здесь также есть три колонки словарь и bitmap индекс качаем дальше с большим сбрасываем вторую часть диска на диск опять у нас мини сегмент три колонки словарь индексы ну и дальше дальше дальше но вот если посмотреть на это по отдельности по частям то мы заметим что колонка числа вызовов ну и также и время и h100 она по этим частям разрезалась пропорционально а вот что с фильтрами здесь со словарями и с индексами у нас за каждую пятиминутку каждый хост каждые пять минут сбрасывают свои данные соответственно за каждую пятиминутку вся 1000 хостов присутствует и в этой части пятиминутки в этой пятиминутки в этой пятиминутки в следующих следующий каждый раз словарь один и тот же он большой он никак вообще не разрезан поэтому чем больше таких частей тем больше вот это тяжелая часть обработки которые 95 процентов занимает тем больше оно раз копируется и запрос начинает тормозить что можно с этим сделать это можно контролировать число вот этих частей сброшенных на диск например если у вас суточный сегменты вот все тормозит по свежим данным то сократить его до часового тогда количество частей сократится также пропорционально ну и тормозить будет пропорционально меньше также есть еще два параметра которые позволяют контролирует просто частоту сбросить этих частей на диск это максимальное количество строк в памяти и интервал сброса на диск ну то есть например 1 5 минут сбрасывать на диск вот об эти параметры можно поднять то есть не раз пять минут сбрасывать а раз в полчаса и не каждые 100 тысяч строк а каждый миллион строк например сбрасывать на диск тогда этих частей станет меньше работать будет все в разы быстрее еще есть важный момент что иногда так случается что вот те 95 процентов 80 из них занимают обработка регулярных выражений они bitmap of и мы на это наступили мы при миграции все наши фильтры сделали типом регулярного как регулярные выражения потому что мы не знали этих особенностей мы думали какая разница вот так делать не надо нужно для когда мы фильтруем по точному значению использовать фильтр типа селектор так как он бинарным поиском находит нужное значение и достает сразу bitmap соответственно это работает в 1000 раз быстрее чем регулярное выражение ну вот но это ещё не самая интересная сейчас будет самое вот самый важный прорыв тормоза все же знают ленту которую нас кролем с кролем вот в соцсетях лента это такое место куда все команды все проекты сбрасывают пишут свой контент типа там человек по local food очку и конечно все эти команды хотят смотреть и писать туда статистику у нас статистика ленты пишется в одну табличку и туда пишется в сутки восемь миллиардов строк и она тормозило даже в друиде когда мы перевели она все равно тормозил мне так сильно но но это не самая страшная самая страшно что когда она тормозило то она перезагружала весь друид и тормозило все и у всех и было в этой статистике такое поле вот комбинированная которая состоит из нескольких частей там палок или фотку на главной и ну там было больше там был ещё тип ленты там вот то что это избранное там друзья группы если какие-нибудь видит и участвовали ту виджеты если реклама какая там участвовало ту рекламу и вот так вот несколько таких полей собирали 14000 комбинаций соответственно словаре было 14 значений и 14000 bitmap of и что мы сделали мы разрезали это поле на маленькие части вот прям по словам отдельно у нас были лайки отдельно фото видео отдельно главное отдельно тип ленты отдельно widgett отдельно реклама все отдельно и когда мы это сделали статистика ленты ускорилось в 10 раз и еще и мы в размер данных сократился в два раза и сейчас я на примере поменьше проиллюстрирую как это происходит вот возьмём опять палок или фото на главной пусть у нас будет такое поле мы можем полагать фото на главной в альбоме в группе то же самое мы можем полайкать видео в трех местах мы можем полагать музыку также мы можем пошарить фотки на главной в альбоме и в группе мы можем пошарить видео пошарить музыку и то же самое мы все можем прокомментировать и того у нас получилось 27 комбинаций событий до и соответственно в словаре у нас 27 этих строчек и 27 bitmap of теперь возьмем запрос мы хотим посчитать сколько было лайков соответственно этот запрос пройдет по 27 значением словаре регулярной выберет из них 9 достанет 9 bitmap of объединит и пойдет считать теперь давайте разрежем это на три части 1 будет экшен и мы полагали пожарили по комментировали второе будет объект фото видео музыка и будет место на главной в альбоме в группе и тот же запрос сколько было лайков этот запрос пойдет всего по одному уже словарю action в котором всего три значения и всего три bitmap а соответственно 3 допустим что это тоже регулярка для чистоты эксперимента 3 регулярке он проверит одну один bitmap достанет и по нему пойдет соответственно в прошлый раз у нас было 27 регуляров в этот раз три в 9 раз меньше и bitmap of также было девять а теперь один в 9 раз меньше опять же то есть получается что мы вот эту вот часть обработки которые считают сумму ничего с ней не сделали от тяжелую часть мы сократили в 9 раз теперь если взять общую производительность то мы увидим что она ускорилось в шесть раз и это мы всего лишь взяли словарь из 27 строчек и разрезали на 3 поэтому я призываю всех всегда разрезать длинные измерения длины вот такие выражения на маленькие части это в разы ускоряет ну вот хорошо мы починили наши запросы все работает быстро но вот приходит пользователь и хочет посмотреть эту тяжелую статистику скажем за год за год там 2 терабайта на нашем кластере это значит что нужно по 11 гигов с диска поднятий на это потребуется семьдесят четыре секунды пользователь знает что его данные тяжелый и он готов подождать но вот что будут делать остальные пользователи эти семьдесят четыре секунды они будут материть друид они будут меня материть и весь отдел статистики и что как вот мне жить знаю что это может произойти до друид поддерживает приоритеты запросов но он сам их не приоритизирует ему нужно выставить запросу приоритет прямо в запросе мы это попробовали мы привели низкий приоритет к тяжелым данным запустили и в принципе отпустила стало полегче но все равно тормоза остались потому что оказалось что приоритеты работают на уровне очереди это значит что если часть тяжелого запроса уже попала в обработку то всем все равно придется подождать потом легкие запросы быстрой проскакивают вперёд и тяжелое запрос опять занимает все ресурсы и опять нужно будет ждать соответственно возникает такое ощущение что система работает натужно на пределе и что мы с этим сделали мы воспользовались тем обстоятельствам что у друида есть вся информация я запросе о данных поэтому мы сделали простую приоритезация которая представляет приоритет по количеству данных которые этот запрос пройдет независимо от того тяжелая таблица или нет просто мы буквально в мегабайты взяли посчитали выставили при реки приоритет и при этом мы сделали пять очередей 1 очередь для самых тяжелых запросов для самых легких и промежуточные и в них раскидываем запросы по вы численному приоритету и соответственно у каждой очереди есть приоритет на уровне операционной системы таким образом быстрый запрос и теперь уже вытесняют тяжелые и теперь наконец-то друид заработал так как от него это этого ожидаешь подводя итог хочу вспомнить о чем я тут говорил первое что мы у нас есть запас времени чтобы починить маску если он сломается второе данные пиры можно удалять нужно держать запас памяти корректно завершить исторические ноды и стартовать их с паузой real-time может потерять данные это нужно помнить также чтобы не тормозили запросы по свежим данным нужно подбирать размер и сегмента и помнить про сброс на диск обязательно использовать селектор фильтр там где это подходит для точных значений так как он в тысячу раз быстрее чем регулярке самое главное разбивать большие-большие словари на маленькие так как это очень сильно ускоряет работу с фильтрами и помнить что приоритеты работают только на уровне очереди спасибо за внимание пожалуйста настало время вопросов у нас есть 10 минут до у меня два вопроса во первых спасибо за доклад очень интересно вот вы сказали что внесли туда некие изменения эти изменения они были внесены на уровне пол request of или все таки они были внесены личного одноклассниках и как вот если мы захотим день использовать друид у себя нам надо будет это героически повторить при допросе как это вот но мы не open source или так как у нас очень много завязок на еще другие внутренние системы то есть это у тебя ещё такой вопрос вот было сказано типа вот применять фильтры select a toy себя как выйдет на товары это да вот то есть там получается то есть какой-то свой магический язык запросов да там g-100 нее формируются за russie понял там есть еще из ql в новых версиях он более менее уже есть все понял спасибо спасибо за доклад не пробовали использовать фишки любых time series баз данных такие как там сэмплинг или метрики для того чтобы тяжелые запроса можно было за считанные секунды получает там года десятилетия down сэмплинг мы используем как раз больше не активна как раз кейс по тяжелым запросам да да спасибо как вот происходит импорт данных дают у вас импорт вы что имеете ввиду из сказки в квартиру и нет у нас есть свое промежуточное хранилище на кассандре и мы забираем из кассандры а понятно к фирма стерилизацию всего раз спасибо за доклад подскажите вот вы сказали про bitmap и и словари а как индексацию как-нибудь используете для ин-тов для чисел нет у нас очень мало данных числовых ну вот в фильтрах поэтому мы это не используем у нас все как строки здравствуйте вы сказали что можно отказаться от real time ноды от тогда как будет но случае отказа сторож случаться отказов майский что будет происходить . а то же самое у вас будут копиться только не данные не в квартирах а а сами worker и начнут плодиться потому что там используют как бы тоска запускается на загрузку каждого сегмента условно у вас будут тоски запускаться и они начнут копиться но они также выживут все ресурсы когда-то еще хотел спросить вы вывели как-то экспериментально идеальный batch который стоит отправлять на записью друид или вы просто вот полно у нас друид работает в такой схеме что он самый учитывает данным просто их вычитывает и все и все сейчас а в итоге других подходит только если если ты готов смириться с потерей данных судя по схеме но это real-time real time и либо ты готов починить его либо смириться если нет то используют indexing service да хорошо спасибо for вопрос самом true иди мы явно придем структуру хранения данных мы затачиваем ее утробно под тут же сон который приходит под статистику если например мы захотим анализировать там данное продажа когда то нам нужно будет много схем определить как происходит описания друиде теоретически там нужно задавать только метрики по сути то есть если у вас есть там уж цена там не знаю сумма продажи а вот все остальное фильтры там тип объекта не знают тип товара его можно на лету добавлять и друид его возьмет и все нормально то есть он будет на лету подхватывает любые измерения да по ним анализирует я могу тут по сути отправлять любые тнт ну да хорошо и а сегмент они там так запущено что они режут цены по времени я могу задать свой сегмент например там по типу действие да у меня будет фиксированных опять секрет по типу действия вы можете там есть такая штука то можно продавать и вам нужно будет чередовать но по времени они все равно будут нарезаться то есть это по времени обязательно от этого вы можете большое время задать 5000 лет не будет ну правда 5000 лет у вас сегмент будет real-time на деле же но он все равно будет по времени скажите пожалуйста вы упомянули что вы рассматривали разные варианты house и прочее и а почему кассандры не рассматривали и внезапно ее выбрали для хранения метаданных но не собственно говоря как хранения словно кассандра на как быки и вылью на точно все что вы вывели параметры требования скажу она подходит патенте все таки да но только не под аналитику она подходит для хранение киева или понятно хорошо и второй вопрос а real-time ноды где хранится хранят все давно и архип или в хеппи вот те данные которые сброшены на диск как я говорила жена диска это понятно что фармить этом в хит или he у нас happy там сейчас в новых версиях есть of hip вариант тоже спасибо да подскажите еще а по железу на как и на чем у вас цвета крутится сколько все это потребляет там супу памяти и так далее у нас три навскидку чтобы сравнить там с индексом и так далее но вот real-time ноты которые нас 12 который вот это все индексирует у нас там 12 штуках там 40 цифру ну 40 ядерные накрыты на каждой ноги или суммарно да на каждой ноги и я не помню сколько памяти ну гигабайт 90 может спасибо спасибо за доклад такой вопрос вот когда вы разрезали ваши метрики по частям да я понимаю что то похоже на тегеран через базу база данных о чем-то похожи можно так сказать спасибо добрый день здесь вот подскажите пожалуйста вот вы говорили то что вот графики строятся то есть запросов которых вам приходят на анализ как сервису статистику он достаточно велико его достаточно велика ваша команда формирует ответ пользователю о динамике по графикам или же вы подрисовать какие-то средства по визуализации по построению графиков на основе друида у нас есть система чартов там пользователь сам задает что ему надо он сам пишет дайте мне количество лайков там ну то есть на курсы период пишет сам запрос вот этот вот таком виде удобоваримым виде то есть там есть просто список фирм ну вот как поле один там экшн и поле ввода что какой фильтр туда поставить окей такой вопрос это самописная система или же какой то взяли за основу там описаны спасибо и сколько людей вашей компании или вашем отделе в состоянии разобраться почему он тормозит или почему он не работает я задаю этот вопрос потому что я наблюдаю использование друидов некоторые компании в которой один человек разбирается в этом он находится в другом часовом поясе и и не позволяет компании но у нас на данный момент два человека могут разобраться случае каких то проблем но в принципе есть инструкция как определять в каком месте проблема ну а дальше не знаю в зависимости от этого что-то делаем налево внедрили внедрили где то год назад спасибо польша давайте поблагодарим нашего спикера и и"
}