{
  "video_id": "_JibrzANYa8",
  "channel": "HighLoadChannel",
  "title": "Использование haproxy/iptables+etcd+confd / Сергей Пузырёв (Mail.ru Group)",
  "views": 4834,
  "duration": 2839,
  "published": "2017-07-30T00:47:31-07:00",
  "text": "коллеги мы продолжаем сейчас у нас доклад сергея пузырева из mail.ru group он будет рассказывать про использование вопросе х прокси и 5 balls эти сиддик он в де для автоматического сервис discovery в переменчивых сетях поприветствуем не слышно отлично а меня зовут сергей пузырев я system administrator в mail.ru я занимаюсь проектом поиск на удивление до umma.ru есть поиск я люблю сервисы которые не требуют внимания я system administrator и я не люблю работать система администратором очень много я люблю делать так чтобы мне нужно было работать меньше и я очень не люблю делать так что мне нужно работать много поэтому одно из решений которые мы пытаемся использовать в работе я сегодня вам пишу сначала я расскажу пару слов о том что вообще такое сервис-ориентированной архитектуры приложение достаточно большие приложения можно строить с помощью комбинации их из большого количества небольших простых достаточно в своей сути сервисов которые общаются друг с другом это немного похоже на unix только это не пределах одного компьютера конечно это в пределах приложение и принципе здесь на самом деле почти что те же самые то есть приложение должны делать мало вещей но должны делать и хорошо они не должны быть сильно связаны друг с другом они должны использовать более менее стандартной процедуры связи между друг другом приложении не должны знать о том кто ими пользуется ты уже неважно что именно ты летел теле теста дым или файл одно ему совершенно неважно соответственно если мы используем сторож статике то столь же статике неважно что о нем лежит видео картинки что угодно его задача просто хранить блок и отдавать лобби больше мне делать нечего и слишком слишком делать но слишком умным он быть не пытается точно также как феликс клиенты не хоть не должны ничего знать о сервисах о том как сервиса устроены опять же если мы используем себя в стороны статике то мы просто в него заливаем а как туда клочки уже говорили стража статике это вещь сложная но клиенты это не волнует клиент хочет положить блок и хочет забрать блок как собственно тапочки уже и говорили а где она там ляжет сколько там будет factory блэка ции как это будет синхронно и асинхронно между дата центрами как она будет восстанавливаться случае аварии их вообще это не волнует они хотят просто ни о чем не думать разработчики тоже не хотят ни о чем не думать то есть слабость концепта слабо связанных сервисов позволяет не беспокоиться о внутреннем устройстве других компонентов из-за этого разработчику ну проще ему нужно меньше думать в голове и разработчики они тоже такие же люди как мы они тоже не хотят на самом деле думает слишком много и возможно 4 он возможно несколько кажется очевидным но слабо связанные сервиса тем не менее друг с другом взаимодействуют и пользуются друг другом давайте посмотрим на то что такое сервис вообще сервис характеризуется ну в целом моего но в том как я буду дальше употреблять слова сервис я имею ввиду какое-то более-менее изолированное приложение у которого есть три обязательной части ну частей обязательных у него 2 3 часть у него опциональная во первых должен существовать протокол общение с сервисом то есть то собственно какой сервис он оказывает например ну дальше там будет несколько примеров но это ключевой момент то есть сам сервис характеризуется протоколом они тем как он внутри устроим я уже об этом говорил но на всякий случай я повторился во вторых в нашем случае вот конкретно в нашем граните так исторически сложилось что все сервисы это по большей части сетевые сервисы которые работают поверх ты себе протокола не поверх каких-то более вышележащих протоколов ну так получилось у нас есть самописная вышележащей протоколы но в точке но в контексте рассматриваемой доклада это не очень существенно соответственно каждый сердца из точка входа в нашем случае это всегда пора идти плюс порт в общем случае конечно у сервиса сейчас это особенно модно со всякими архитектурами наподобие риск это чаще всего что типе точки входа но ними едиными потому что сервис это совсем не обязательно что тебе сервис и сервис и могут пользоваться друг другом я уже говорил но здесь мы приходим к самому важному элементу о чем собственно весь доклад когда один сервис хочет воспользоваться другим сервисам этот сервис должен знать где это другой сервис найти и эту информацию необходимо ему как-то сообщить какие у нас есть простые сервиса например у нас есть совершенно то путчей сервис memcache я думаю им пользовались он работает протокол memcache у него есть всегда ip адрес и порт и сам он ничего не пользуется он просто запустился слушает поп и работает больше он ни о чем не думает клиент с ним общается клиенту нужно просто знать куда постучаться по себе или по и теперь memcache кстати работает по и т.п. иногда это может быть удобно и ничего собственно сложного сервис чуть-чуть посложнее клиент хочет отказов . engage существует такая штука как маг раутер ее разработала компания фейсбук и макро утра позволяет например редактировать записи которая идут в мак раутер в 2 разных бакан демин каша клиент в этом случае общается с макро утром по протоколу memcache и опять же он ничего не знает о том как макро утру строим дальше его не волнует сколько томим к шее он просто хочет общаться с memcache сервисом которых будет отказа устойчивым мокроты в свою очередь знает где у нее есть два бэг-энда memcached и он реплицировать туда данные клиент ничего не знает о нем кашах а маг раутер должен знать о bim кашах но при этом клиент должен знать где находится макро утром ок раутер должен знать где находится memcached здесь у нас уже 3 место которое необходимо сконфигурировать первый клиент должен найти мем маг раутер второе макро утра должны идти первым engage третьем а крутую должен найти 2 memcache и у нас всего лишь всего лишь маг раутер пойдем дальше такая архитектура на самом деле ничего особенного это любой средней php сайт яндекс несколько php not стоишь статике часто он внешней мускул мастер мускуса их под мак раутером маг раутер просто его люблю поэтому так с ним получилось часто используется один memcache сервер и балансировка изнутри php но тем не менее вот в таком несложном на самом деле сервисе который состоит из пары тройки серверов физических у нас наблюдается уже 16 мест которые необходимо сконфигурировать джин изложен знаете а php он должен знать о том где находится memcache вместе с mac раутером жены должен знать где находится стать explorer я бы наверно дальше не буду читать картинку правда то есть мы можем посчитать количество связей между сервисами и посмотреть к сколько мест необходимо сконфигурировать и убедиться что их здесь 16 а у нас всего лишь простой пашка сайт а что мы будем делать когда у нас будет сложно и приложение вот эта картинка она не моя у меня есть у конспект в работе и мы его используем и он действительно достаточно сложный коллеги на следующие конференциях на следующий такой будут рассказывать про пустых можете у них поинтересоваться картинка гуглится по запросу openstack диаграмм можете сами посчитать сколько здесь вместо необходимо сконфигурировать на самом деле тут есть небольшие читы потому что подходы которые я рассказываю в этом докладе используется в пластике но тем ни менее там сильно нетривиально со временем конфигурировать вставляет делать то что я не знаю но сначала мы делали вот так потом мы начали думать как бы что вообще как решать эту проблему как сервисом рассказывать как им общаться друг с другом самый простой путь это мы просто берем конфиги и записываем в них где находится сервисы которыми хочет пользоваться наш сервис то есть в примере с макро утром и в макро утра забиваем в конфиг маг раутера адреса memcached очень быстро очень легко а вообще ничего не нужно то есть пошли на configure лири столкнули работает нам больше не о чем не надо думать пока у нас два физических серверов у нас все хорошо даже когда у нас там 30 физических сил нам почти нормально они у нас вылетают раз для месяца и ничего у них не происходит когда у нас три тысячи у нас начинается проблема потому что они никогда не работают все вместе все время несколько штук лежит все время не какие-то сломаны где-то стойка упала по сети где-то сервер вы ломался где-то какое-то сервером надо выломать где-то разработчики зашли на продакшен случайно набрали римом в лоб и в общем 33 бог открылся римом и картинка вот вот вот вот это вот она она она она характеризует ситуацию это момент номер 1 момент на co2 из за того что у нас конфигах куча ip-адресов и непонятных портов с племени понятно то захочет конфиг сможет что за фигня а там потом цифры и ничего не понятно и приходится держать документацию документации имеет свойство отставать от реальности и посреди ночи у тебя сломался то сломалось приложением ты идешь пытаешься понять что за фигня еще не находишь материшься потом кое-как чинишь плачешь ну есть как бы некоторые проблемы третий момент что когда у вас внезапно есть сервис которым пользуются тысяча других сервисов и он внезапно меняет например свой and point and точка входа помните да вам необходимо переговорить другую тысячу сервис чтобы дети приходили в другое место зараза и это больно потому что потому что больно при configure 1000 мест это тяжело в принципе хоть какую архитектуру вы будете использовать поэтому тривиальные реальное решение давайте вместо ip-адресов используют dns очень легко на самом деле разницы особой нет просто мы вместо котором мы исправляем место в которое мы будем вносить изменения мы пытаемся переместить из конфига демона который мы пользуемся в днс зону ну как бы часть проблем решаем а часть проблем мы создаем таким путем потому что у нас все равно требуется документация потому что которое будет отставать от реальности у нас все так же это будет ну это будет лучше работать чем api адреса в большом проекте но незначительно лучше потому что никакой связи с реальностью с тем что сейчас работает а что не работает в конфигах самостоятельно появляться не будет у вас будут стойка также падать и необходимо будет идти исправлять это где-то вручную если софт не умеет ну не приспособлен с этим жить асов часто среди мире способным жить особенно когда софту 10 лет особенно тяжело в случаях вот например с макро утром и несколькими кашами бы кантами особенно тяжело добавлять новый инст он самим к шее например вы хотите уже репликацию не один а тут фу не два а три и вы никак не можете только правкой dns от исправить эту фразу вам все равно надо идти править конфиг и возможно не в одном месте а потом где-нибудь еще править конфиг и сделать это те же самые проблемы когда их сервала некие проблем никогда много тогда проблемы есть 4 dns этой из коробки он он априори асинхронный он никогда не может быть синхронным мы не можем внести изменения в dns и получить вот щелк и чтобы нам стало хорошо нет у нас будет приходить даже tl 60 секунд у нас может быть несколько минут доплыв эту информацию эта информация до демонов поэтому для нас там тоже не очень подходит нас еще и вот такая проблема есть потому что на сотню это нормально для нас работает на 1000 уже плохо но все используем мы все говорим что мы используем систем управления конфигурацией это модно но часто к сожалению видно что этого не используется казалось бы нас спасет но не все так хорошо потому что система управление конфигурации опять же на большом проекте вызывает она решает проблемы но вот так проблема который я говорю это не та проблема которая решается системой конфигураций потому что раскатка системой конфигурации на 1000 машин занимает много времени очень много времени константин и смогу докладывал о том как у них работает попив можете поинтересоваться сколько у них занимает раскатка на все это это даже не минуты это скорее часы и поэтому мы опять не можем быстро реагировать на изменение в инфраструктуре этот момент номер 1 момент номер два учитывая что систем правление конфигурации это часто куча шаблонов и куча а структурированной информации при неправильной организации шаблонов заранее вы можете получить проблем при расширении то есть для того чтобы вам какое-то кажется банальное место изменить которое при hard case in point of в конфетах вы бы смогли исправить очень истра нам придется переписывать шаблона тестировать шаблоны тестировать это в div окружении потом выкладывать в продакшн и вы на простую процедуру увеличение количества бы кантов с двух до пяти потратить полдня работы поэтому сожалению систему про инков и грации тоже не полностью решает задачу мне кажется что основной проблемой здесь том что как ко всей этой штуки привлечен человек который собственно комитет который конфигурирует это все неправильно фактически у нас опыта ломаются роботу друг друга эксплуатируют мне кажется мы там лишнее то есть вот как то так давайте посмотрим что можем сделать существует системы обнаружения сервисов они в характеризуется несколькими вещами во первых мы вводим две концепции сервис может регистрироваться в системе обнаружения сервисов в этом случае он стартует и регистрируется после этого другой сервис который хочет по пользоваться этим сервисом который нас только что зарегистрировался идет систему обнаружения сервисов и спрашивает а если у тебя такой сервис и где он если он тебя есть всем отражения сердцем говорит смотрю мне есть вот такой и сервис который хотел узнать куда ему сходить он узнал и пошел куда ему нужно или не пошел если он не смог это очень похожи на dns но это не совсем dns а скорее это совсем не dns так получилось что исторически система нагружения сервисов и достаточно много он снизу список нескольких штук которые в принципе существуют они также часто работают как средство распределенной блокировки и использовать их соответственно необходимо с учетом этого это немножко расскажу конкретно том стыке софта о котором пойдет дальше речь что такое эти сиди и ты сиди это специальное хранилище данных которые строго консистентная транзакции в нем выполняются только на всем кластер и сразу точнее на кворуме кластера и поэтому и они строгость реализованы нельзя внести изменения спирин этой кластер когда у нас нет формы ни в одной половине это невозможно и 10 работает по алгоритму ровд алгоритм ровд это скажем так взгляд на показ если кто-нибудь слышал про paxus и попытка сделать процесс более простым сожалению а все равно остался достаточно сложным поэтому я оставлю описание авто за пределами этой конференции нужно это просто слишком долго но тем ни менее как работает с и d на каждом сервере у нас запущенный этот демон который связывается с остальными это cd and get a cd демонами с помощью первичного входа то есть мы на каждом сервере запускаем with a cd и мы должны указать хотя бы один уже находящийся в кластере это дальше они сами друг друга найдут это все магия зашита вегетации и следующий раз они уже будут знать как им стартовать и когда нам необходимо будет пообщаться системы обнаружения сервисов именно и т.д. мы всегда будем общаться с вокальной петлей то есть это создать всегда слушает локальную петлю на любом сервере который у нас в принципе есть следующий кусочек мозаики и так он в двд это служил какие-то cd то есть этот сохранив ключей и позволяет эти ключи изменять он в d позволяет подцепиться к ключу и отслеживать изменения в и т.д. делается это через http long polling то есть can где фактически представляет собой что теперь клиент агитация дожди тебе сервер он воды спрашивает у и т.д. смотри есть такой ключ я хочу за ним следить и и вешается в лонг по linger как только в гетто cd кто-то другой этот ключ записал или изменил или удалил апдейт ну что угодно это cd обрывается не нет и дает ответ поэтому реакция происходит очень быстро и то есть если происходит транзакция то клин подключенный к мвд сразу же об этом знает таким образом нет отсюда используется как шина сообщении мвд умеет реагировать на изменение в гетто cd и что-либо запускать изначально он был приспособлен для того чтобы брать данные ты сидишь а планировать подкладывать конфет и рестарта демон но по факту он может иметь собственную просто дергать какой то скрипт который будет делать более сложную логику если то что можно запихать в конт-бет сама по себе не работает и учитывая что это cd это просто что теперь с ним вполне можно жить общаясь с ним кром или в этом или любым другим вашем любимом артисте клиентам на питоне на чем угодно ну понятно же что теперь можно спокойно писать эти маленькие скреплена чем угодно не использовать каждый если его логика вам не подходит просто моего использовали ну так получилось как это примерно работает здесь есть та самая схема с клиентом макро утром и он кашами что происходит я нарисовал я нарисовал и и т.д. отдельно потому что рисовать его в каждое место не нужно это cd присутствует на всех машинах на каждой на каждой машине которые у нас есть есть это cd и каждый раз когда мы общаемся с и т.д. мы общаемся с петлёй и уже в этом году он это важно а на некоторых машинах присутствует кант двд не присутствует у нас новинка шах он где занимается тем что он читает втц данные и конфигурирует демон сконфигурированный на локальной машине то есть машины это фиолетовые прямоугольники оранжевые прямоугольники это демоны которые запущены на машинах клиент это какой-то абстрактный клиент в этом может быть сервис это может быть какой-нибудь наше приложение но она хочет попользоваться mac mac раутером макро утра свою очередь это клиент каменка шам таким образом смотрите у нас есть разные типы она здесь клиент у нас есть мак раутер memcache и помимо этого у нас есть общие компоненты can в.д. и announcer что происходит дальше когда у нас стартует memcache вот этот например рядом с ним запускается маленький большой скриптик этот большой скриптик пытается понять пытается определить and point по которому доступен нам кашу он берет эту строку то есть and pointed вами каша а нам все авто знает где запущенным кэш и пишет какие-то cd привет я memcache я доступен здесь 2 м к же делает точно также и он пишет привет я memcache аят доступен здесь и теперь видит а цены есть две записи про доступными мк h&m кэш один и он каждого и у них разные пи адреса потому что это разные машины и где-то сдпр информации уже есть теперь у нас стартует после этого маг раутер много кроутер и точно также есть announcer который рассказывает лепится нет я то cd это не одна машина get a cd это классный сервис instance которого находится на каждой машине а поэтому я просто не стал бы везде рисовать нужно будет непонятно хотя вот видите так тоже вышла непонятно ну бывает анонс анонс 12 где доступен макро утра и can в.д. читает из еды cd то что он туда написали она очень рамен кашей после этого вот этот кон в.д. узнает что memcached доступны по этим двум адресам и конфигурирует маг раутер чтобы маг раутер ходил именно в эти memcached дальше если один из этих memcached упадет где-то cd через несколько секунд проект появится ключ и соответственно эта запись исчезнет мвд который следит за ключами в и т.д. об этом узнает и перри конфигурирует макро утра так что в макро утра перестал использовать миров и memcache если появится третий memcache и то правда опять же об этом узнает потому что он в.д. на mac раутере отслеживает vcd изменения всех ключей ну условно memcached звездочка то есть как только там появляется хоть какой-то memcached концерна vag роторы об этом узнает и перри конфигурирует макро утро для клиента вся эта штука происходит прозрачно он просто общается с макро утром и крем свою очередь узнает как и пообщаться с макро утром с помощью все того же dcd как это работает но вот это но примерно так на достоин достаток он достаточно простом кейси из плюсов здесь она работает по-настоящему быстро то есть это происходит мгновенно ну вот случае сменка шоу макро утром это будет сходить меньше секунды потому что все эти демоны очень быстро перезапускается и если у нас не отправляемся связанностью кластером не разорваны и все у нас нормально то нам вообще не требуется помнить о том где находится эти memcache и мы их не важно где стартуем и они сразу же появляются в постели туда сразу идет нагрузка и с точки зрения вот меня как админа я становлюсь от этого счастливы и радостны мне хочется прыгать и бегать от счастья кроме того мне не надо даже думать о документации потому что мне не нужно ни где записывать что у меня где работает у меня навсегда уже записано что где работает я всегда это вижу потому что документация появляется от того что где запущена они наоборот то есть первичен запуск они документации в случае когда мы работаем с кодом сначала мы подходим а потом мы запускаем есть конечно несколько минусов они написаны я думаю даже комментировать не стоит потому что система я система действительно усложняется и действительно становится страшно что делать если как бы но все разваливается у нас дата-центром как бы приходит кошмар на самом деле здесь есть два момента первый пункт сложность сложность не так страшно особенно учитывая что у нас очень много сервисов сложность с храп кодом сложность dns сам она значительно выше потому что здесь мы один раз настроили после этого рутинные операции по перемещению сервисов стоп у сервиса в старта сервисов добавлению изменению они становятся почти бесплатны и и мы не правим постоянно днс зоны мы не prime конфиге в нем мы не боимся копипастить цифрах и из одного места в другое оно все само работает поэтому она с одной стороны сложно но с другой стороны на самом деле проще потому что время она экономит знатно во вторых страшности конечно страшно вдруг вся эта штука еще нет и придет ей кирдык здесь нас спасает это cd такое такое счастье что даже случае если происходит полный краж там всего что можно всего что можно предположить это cd остается доступным на чтение писать в него в разумный кластер нельзя читать из разорванного кластер а последнюю последнее изменение которое показал мы начать кластеров прилетели можно каждая новая tcd всегда хранит полный дам всех данных которые у нас есть эта цель не шарден данные поэтому даже если у вас у машина пущены все сетевые интерфейсы то что она получила в это cd pack было в кластере на ней остается если у вас отвалился дата-центр то вас все равно останется к вам в кластере в части дата-центров которые у вас остались все вместе и туда даже если вам повезет можно будет писать но как минимум оттуда можно будет читать поэтому несмотря на то что изменение это происходит часто они имеют исходе не каждые две секунды поэтому даже если у нас все развалится на все равно более менее нормально и есть еще один минус это третий пункт что демоном необходимо вообще говоря в хорошем случае у месье tcd работать нативно потому что потому что так обычно происходит лучше потому что многие демоны не любят riot потому что мы можем ошибиться в шаблонах которые будут конфигурировать ну с помощью которых он где будет конфигурировать наши демоны мы можем ошибиться где нибудь еще если это будет код который сшит прям в наши сервисы это будет работать стабильнее и лучше это необходимо это не необходимо это желательно потому что если бы было необходимо то такие штуки как он в даю и скрипт океана ассиров не появлялись бы мы пытались решить несколько проблем и на самом деле системой серди скоринга в таком виде котором я писал вот в таком они полностью наши проблемы решить не могут есть несколько моментов наше приложение конкретном приложении в поиске большая часть этих приложений это приложение стартует читает там 40-2 sti гигабайт в оперативную память площадь это в памяти и потом начинаем потом наконец-то нам начинает лесу не спорт и начинается этими работать и так она живет к бы долго записи у нас мало у нас очень много чтения но это поисковая система понятно что поиск почти ничего не узнает о том как бы когда с ним работает клиенты ну к сожалению из-за такого типа жизни демон у нас стартует 5 минут соответственно если у меня будет изменяться какая-то мелкая штука я не могу позволить себе пять минут простоя сервиса ну никак потому что они все одновременно через партнерства правда я же специально добился того что они мне все быстро она теперь мне все быстро с вами реставрация так жить это не могу вот поэтому это первая проблема поэтому ситуация с кан vds постоянным рестор там демонов она плохая мы не можем себе позволить вторая проблема как любом достаточно долго живущим проекте поиск мыло существует 2008 года если память не изменяет есть достаточное количество кодов которые никто не хочет возить разработчики уволились ну я не буду объяснять что такое да я думаю тут все это понимает и соответственно мы никак не можем научить наши демон работы с хиты cd некоторые наши демоны некоторые можем которые как бы в горящий разработки это вот из тех двух проблем которые нам некоторые мы не можем которые мы не можем решить сами 3 это третий пункт это собственно задача которую мы хотели решить чтобы нам не было необходимости перри конфигурировать 1000 мест и нам вроде бы становится лучше но у нас есть первый пункт по которым мы не можем перезапускать ничего и еще один момент но это общее требование к любой системе то есть нам не должно стать хуже с момента как мы и внедрим если у нас сейчас используется хардкот или dns то если мы внедряем систему обнаружения сервисов мы не должны получить деградацию по сравнению деградацию в надежде описанию с тем что у нас есть поэтому мы пошли дальше и начали внедрять более новые более хорошие к стали сначала вот так вот подумали а потом мы начали думать и решили сделать немножко хитро закрученный сервис такой чтобы приложение не имели необходимости вообще меняйте свою связь с какими-то другими сигну чтобы какие-то наши сервисы клиенты не имели необходимости менять тип связи и сервисами серверами в разных контекстах один тоже сейчас может быть как клиент такой сервис так и сервер извините очень много страшных слов я них начинаю путаться это все мы уже видели вот так вот она у нас живет нормальном режиме и в этом случае can't be configured нашего клиента клиенты дата приложение которое не хочу работать вот и поэтому мы решили сделать вот так мы добавляем к прокси между клиентом и макро утром опять же клиентами кроутер это просто абстрактные какие-то примеры можно засунуть сюда фактически что угодно какой мы получаем от этого профит клиент общается с х проще к прокси слушает петлю клиент в конфиг клиента у нас забит забита что mem cache доступен по 121 : 11 211 всегда везде на любом сервере который захочет воспользоваться макро утром вот этим у нас запущен к прокси который ли станет петлю и в конфиге всегда забита петля он в свою очередь configured уже не клиент мвд configure the proxy а как cialis не петлю и проектирует эти обращения в настоящий маг раутер где находится настоящий маг раутер х прокси a proxy узнает из квд он в свою очередь из еды cd здесь мне кажется ничего сильно сложнее как бы не становится сразу вопрос почему набок раутеру не только прокси для того чтобы с помощью х прокси общаться с ним кашу на самом деле и туда можно вставить но маг раутер умный и он работает выше чем по протоколу tcp он работает он влезает протокол ним каша и поэтому макро у только сожалению приходится configure ти ручную но макро утру почти не хранить состоянии поэтому ему restart не страшен обычно как бы демоны стараются уделить наставит вас состоит в и демоны которые они хранят состоянием можем легко перепутать они быстро стартуют демон которых хранят состоянии мы не можем работать потому что не стартует долго и вообще это больно поэтому и к ним оценка стрелец море видяха про все чем мы делаем дальше мне уже как бы видимо немножко повторяюсь я просто почему мы делали именно так потому что мы не могли ничего не путать теперь мы можем перепутать и можем работать только х прокси это бесплатно ну это условно бесплатно но намного более бесплатно чем пять минут простоя демона из за изменения какого-то где-то маленького can физика а во-вторых учитывая что сервисов много и кода разного много конфигурировать нужно много эти шаблоны писать мвд тоже доставляет раз написать все шаблоны для прокси сильно проще и мы потом просто ни о чем не думай просто везде забываем петлю каша это будет там под 1с 211 замазку ли это будет по 3306 но все положение всегда общаются с петлёй они вообще ничего не думаю нам даже перри confirm как ничего нигде никогда не нужно в третьих ухо праксис чудные же поводу с которой можно снимать статистику которую часто из приложения достать трудно это как бы достаточно бесплатно это не было целью но это внезапно можно использовать и это помогает в четвертых к прокси как известно это не просто prophecy это еще и балансировщик если у нас есть простые демоны которые не требуют блокировки которые требуют блокировки которые возможно релизов имеет возможность быть реализованы с помощью к прокси мы можем и бесплатно получить опять же ну и как бы дополнительные плюшки тайм-аут think no connect и дополнительные отслеживания активные чеки соединений до брендов это все вопросы есть как вы знаете и нам нет необходимости устраивать это в демоны которые общаются напомню по не стандартным протоколом которые вовсе даже не ешьте теперь есть несколько проблем потому что ну как минимум одна фактически осталась только одна вовсе не умеет в египет египет демоны существуют константин рассказывал про графит графит работает по ее д.п. он умеет по тисе пи но когда метрик много 10 это сильный overheat здесь а к проксиме тебе не умеет принципиально поэтому мы используем донат над имеет как бы тот же функционал ну при необходимости он может условно заменить к прокси в этом кейсе но он не умеет вот никакие дополнительные плюшки х прокси поэтому с ним можно жить и ограничены использовать если у нас реально проблема с производительностью х прокси например то мы можем заменить донат и мы можем ставить дэн от местных a proxy и для теперь но в этом случае все проблемы хендлинга tcp connect of time out of и всего такого чем я говорил они должны быть все таки решены клиентам если мы используем эти себе режиме здесь показан как раз пример со 100 тыс карбоном для графита когда одно здание общается с пою т.п. и здесь как бы много компонентов не указаны на подобии announcer просто не не влазят в эту схему экран маленькие руки большие такие дела вот почему мы собственно все это делали нам стало почти безболезненно двигать сервиса между разными местами с этими уро обладают интересной особенностью если нам необходимо условно если мне нужно вытащить сервер с одного стойки поставите в другой что это вам не нужно перекат фигурировать ему сеть я не могу сделать по другому поэтому у меня эти процедуры они болезненные и и даже просто взять опустить 1 м конечно одном сервере поднять его на другом и так чтобы заменить мы несли она необходима перри configure сеть ну вот так у нас сеть устроена эту задачу мы решали с помощью вот этого инструмента и в целом она становится сильно проще потому что нам нет необходимости идти править 25 сетевых графиков потом еще тысячу конфигов демонов второй момент это когда я говорил у нас есть мак роутер или какой-то другой демон похожий роли в одним есть несколько memcached и memcached внезапно нужно сделать больше эти штуки с помощью dns они решаются в принципе с помощью это не решаются потому что мы можем написать правильные шаблоны и правильно разложить данный вид отсюда при стадионов так чтобы вы расширялись и города наши были шелковистыми третий момент это тоже как бы такой дополнительный момент который достался нам бесплатно демонов очень много очень много что-то запущена 35 лет назад и мы об этом вообще не знаем о но не в мониторинге документации на это нет обнаружим только это падает если у нас все засунут авито совету мы всегда видим кто с кем общается во-вторых мы если у нас есть a proxy мы еще и видим сколько общается мы получаем все метрики бесплатный в третьих у нас в этом случае отделен момент конфигурирования демона самого демона который работает и конфигурирование клиентского демона вот не тайниках совмещены в обычном кейси они объединены из-за этого можем просто взять обе считаться в про те когда мы конфигурируем клиентский демон и получить проблему это не выдуманный кейс я с этим кейсом столкнуться два дня назад у меня админ просто ошибся и внес другое внес другой порт на 10 играх это выкатилась сервис начал деградировать мы начали грустить происходит это просто потому что люди несовершенны так бывает но если мы используем роботов то роботы при правильной эксплуатации сильно более совершенно чем люди если за ними правильно следить и когда у нас появляется новый сервис вот только в этом случае нам необходимо конфигурировать что-то вручную если мы просто перри конфигурируем старые сервиса у нас кусают автоматически и у нас нет необходимости постоянно коммитить бегать что-то вручную ри стартовать жестко мониторить и все такое я наверное закончил я готов послушать ваши вопросы сергей привет ещё раз доброе слушай такой вопрос если есть несколько дата-центр можно ли как то при публикации сказать что смотреть на меня я ближе я думаю мы можем сделать это примерно таким путем демон при публикации себя при анонсе он может понять в каком он дата-центре запущен и написать об этом в это cd таким образом там будет написано я memcache я мем кажется я нахожусь в дата-центре таком-то и соответственно клиентский демон в свою очередь он тоже знает к вам дата-центре он запущен он может выбрать из геноцида нужные ключи наверное это можно реализовать с другой стороны честно говоря мы такие если не использовали поэтому возможно я смогу подумать задать более нормальный ответ но наверное это задача все-таки более сетевая я бы еще посоветовал посмотреть в сторону и семьи с тем чтобы но сетевыми средствами нет нет забудьте про 7 и скорее всего единственный случай который я мог сказать спасибо давай не спасибо за доклад меня зовут роман компания буду небольшой уточняющий вопрос по количеству инстансов и т.д. когда я читал документацию по нему там было написано если не ошибаюсь что их не целесообразно использовать скажем больше девяти из за того что в противном случае они будут очень много трафика гонять общаясь между собой и будут дольше приходить консенсуса вы сказали что вас на каждой машине по инстанцию а максимальный размер кластера именно вот который общается между собой какой у вас получается документация действительно рекомендует не формировать кластер а это cd разберем более девяти более чем из девяти not потому что ровд становится менее эффективен в этом случае но это сады умеет работать в режиме это cd прокси и поэтому у нас существует 9 полноценных но это и это создает совершенно прозрачна нас существует кластере 9 полноценных нот и сколько угодно дополнительных слоев not которые славят полноценные ноды и содержат на себе копию данных но писать в них можно только с помощью прозрачного проектированию с точки зрения клиента эти надо ничем ну ничем не отличаются полноценных но с точки зрения raw протокол они действительно не участвуют в выборах здесь есть интересный момент что в случае если одна из мастернод падает the proxy много может попасть ну может заходить помощью системы распределенный блокировок который детсады является про ксенон можно обучить попытаться захватить лук и чтобы какая-нибудь вообще надо так который повезло захватить wog смогла стать полноценной мастернодой тогда мы используем я разворачивал 9 not a tree в каждом этот оценки у меня было три площадки здравствуйте у вас был хороший доклад спасибо у меня есть 1 вопрос это не сравнивали любы собственный консул и это cd между собой и почему выбрали ccd потому что консул в общем-то умеет делать все то же самое то есть там тоже из консул template которую меньше планируется что нужно ну и собственно сам консул умеет чекать что сервисы зарегистрированным например в нем и к тому же добавок имеет возможность получить сервис прямо под dns у которым предоставляет одним из протоколов вопрос понятен это отличный вопрос кстати sky dns изначально например приспособлен к тому чтобы отвечать по dns ответ сожалению нет потому что на момент когда экспериментировал с этими системами и начинал с этим экспериментировать мы не попали столько еды cd sky dns и перка консул как-то прошел мимо меня мне стало грустно а потом я уже честно говоря с ним не разбирался если у вас будет возможность я бы был рад выслушать вашу а и второй вопрос можете подробнее рассказать про announcer собственно этот анонсирует каким образом то есть старые legacy сервисы и новые которые собственными вы делаете то есть каким образом они попадают with a cd анонсе в нашем случае ночью это простой ваш скрипт тег на уровне wilder бла-бла-бла который с помощью системы запускается одновременно с основным демоном больше там точно никакой магии нет то есть можно обучить этот скриптик он там буквально с точек 10 наверно можно учить этот скрипт и проверять вокальную доступность про то и только в этом случае анонсить каждый раз мы наносим там на 10 секунд спать но мы наносим ключик спаррингом на 10 секунд доносим каждые пять секунд как только у нас тут случилось у нас скобы случилось и стенды отслеживает то чтобы они старались демон и announcer 100 пались и стартовали одновременно со стенда умеет эту штуку в общественном думает много штук я ношусь к тем людям которые любят стенды можете меня закидать эта опция называется бен стул и я и очень доволен я ответил вопрос у меня маленький вопросик как вот обрабатывать следующий успев допустим по какой-то ошибки возможны и какой-то серию стартует носе анонсирует как другой или например если поднялось 2 маг раутера как клиент должен выбрать какой из них использовать arccatalog кого там первый кейс к сожалению это обычными sconfig то есть никого автоматического средства для его хендлинга не существует вы точно так же можете зайти в конфетка ошибиться о нем правда поэтому мне кажется не требование не следует требовать от роботов слишком многого не все таки тупее чем бы я сделал что-то не то от нее не стоит требует от робот слишком много они помогают людям в тех задач в которых люди устают но не заменяет людей второй кейс извините я против контекс повторите пожалуйста допустим если просто поднимется на два нормальных маг раутера хотя нужен только один клиент должен выбрать как-то 1 он может вот фото интернет-мем кэша или еще что-то как-то тут происходит это над обрабатывать один с автоматически это опять же никак не произойдет вы должны ним вы должны в этом случае в квд обрабатывать эту ситуацию сами то есть эта ситуация на допустимая когда у вас например есть горячий маг раутер холодный макро утро должно работать из них только 1 во первых есть два варианта либо они просто друга носят в разные места и con видео пытается понять каким они будут работать это например кейсы актив актив но при этом для каждого клиента каждый лет использует только один в этом случаев в кон вдв больше в шаблоне должны сделать там брендом а второй случай вы можете а носить макро hooters в одну и ту же точку просто одному будет получаться носить а другому нет потому что распределенной блокировка и все такое то есть вот с помощью ноги the cd не только как бы для этого можно использовать и можно использовать как я уже говорил как систему распылены блокировок можно выбирать мастера из нескольких равноправных демонов и так что все остальные не смогут это достаточно безболезненно процедура она не только переца спасибо супер сервер спасибо за доклад такой вопрос если допустим у нас ночью перестроилась там как вот там memcached paladi еще что-то перестроились подключение к сервисам есть какой-то даже пор чтобы он показывал вот бы подключение разных алиса чтобы встал открыл vin коду куда запрос ведут на определенные там ноды чтобы при переключении перед перестроений то есть система чтобы при атоме можно показывать какие ноты живые какие эти по активные медитации да это просто карте типе сервис и мы можем из него читать каким угодно способом то есть мы можем просто построить отдельное приложение которое будет садиться и рисовать там какую-то статистику можем смотреть это кром если я правильно понял вопрос потому что может быть его неправильно понял я не то говорю что ожидается таким образом самой саммита cd не умеет есть проекты пдв бой но по-моему он похож просто на яндекс директорий и ничем особенным как бы более не примечателен нарисовать такую штуку мне очень сложно потому что нужно просто сделать пару а потом верстать страничку просто планировать страницу на основании того что у нас есть в pcd это момент номер 1 а во вторых как вы сказали ночью что-то происходит и мы приходим и утро мы хотим узнать это cd работает с помощью logo и этот лук можно прочитать я честно говоря не помню человека читаемый ли он на прочитать что изменилось ночью можно точно и прийти с на посмотреть но это похоже не могу сказать возможно это похоже как она бинарного куску для то есть его может быть не достаточно удобно читать но точно можно просто узнать что происходило за последнее время спасибо за доклад вопрос следующий вы считали накладные расходы которые получились у вот этой дополнительной инфраструктуры на накладные расходы по памяти по лестнице по процессору то в процентов конкретно по памяти и процессора не считали просто демона стоят холодная не вызывало вопросов о лэнсе не происходит дополнительно деградации учитывая но если деградация там несколько сотен микросекунд на х про все не критично кроме того если мы не используем характер схеме то у нас отсутствует какая-либо деградация вообще потому что общение происходит точно также как раньше происходило у нас разница при своей только момент когда при своей конфигурации если мы используем к прокси туда мы получаем еще один уровень проксирование беру space kerbal space вообще говоря это происходит быстро мы не замечали и сам вид особенно мастернода в случае большого количества на запись он поедает конечно типу прилично но кейс постоянно записи это цех больших количествах это был не наш кейс видимо у кого-то он есть но у нас его нет спасибо девочки спасибо за доклад последний вопрос хорошо вообще классно приятно быть последним вот все работает автоматом крутится роботы все сами скажется заебись а вот не страшно ли этого как-то тестируете перед тем как вот конце кон в тестом скрипты все дела вот чтобы не случилось такого что ночью все сломалось тест есть вас или как это общем как честно говоря с трудом представляю вот я поэтому как это тестировать продакшене особенно потому что запущенный кластер останавливать очень страшно страшно было вонзать но страшно было об этом думать в начале потом со временем привыкаешь и тестировать в бою когда вы новый сервис и вводите конечно необходимо отводить ваш template но никто не мешает вам отводить как бы в нерабочей обстановке если вы меняете тобою но вероятно придется делать либо отдельный класс тира либо develop a template в каком-то соседнем нами спейси это cd когда-нибудь что угодно случается коллеги я предлагаю в это в кулуарах продолжить общением благодарим сергея"
}