{
  "video_id": "TnHnEZFwJHY",
  "channel": "HighLoadChannel",
  "title": "Событийная архитектура распределенных систем на базе NServiceBus / Борис Тверитнев (DABY Tech OÜ)",
  "views": 3240,
  "duration": 2940,
  "published": "2019-01-14T00:11:30-08:00",
  "text": "привет коллеги меня зовут рис твери днях я очень хочу спать я надеюсь что в can эти по крайней мере я приложу к этому все усилия я работаю в двух стартапах один из которых я снова со своими партнерами вот поэтому название этих фирм вам ни о чем не скажут но наверное упомянул компании в которой работал до того как докатиться до старта пирса работал я в лаборатории касперского работал в сенге это большая социальная сеть немецкая и в парочке других компаний поменьше так что никакого давления в этих компаниях я занимался разработкой крупных распределенных систем часть из которых я разработал мне посчастливилось построить на inservice бас который очень нежно люблю вот и к моему несчастью другие системы я вынужден был вынужден был строить без inservice бассов соответственно сегодня я вам буду рассказывать о продукте компании по текила software in service баз и о том как с его помощью можно построить событий на ориентированную архитектуру сервис бас это фреймворк даже скорее не фреймворка платформа с помощью которое предоставляет набор готовых паттернов с помощью которых разработчики могут построить даже не мог построить а просто получает уже готовую инфраструктуру и не тратят свое время разработку и поиск уже каких-то известных решений методом проб и ошибок а это позволяет сконцентрировать все усилия команд разработчиков именно на бизнес-логики этих самых систем последние оговорка интерес base for more платный поэтому мне пришлось приложить достаточно усилий чтобы убедить программный комитет олега романа остальных в ценность этого доклада вот но не спешите уходить потому что во первых мы будем рассматривать такие вещи абстрактные концепции которые применимы к практически любому технологическому стеку а во-вторых в конце презентации вас ждет приятный сюрприз от компании батика вас авто так поехали содержание сегодняшнего доклада рассмотрим немного теорию о о преимуществах и недостатках событийной архитектуры посмотрим к чему она базируется на синхронным обмене сообщениями на рпц например рассмотрим особенности реализации паттерна попс publish субскрайб в двух архитектурных стилях шины брокер а в конце увидим сделаем с вами небольшой такой примерчик с примерами кода на inservice bass к сожалению я не могу показать прям в живую хотя и очень хотел по техническим причинам но если вам будет интересно посмотреть как все работает живую вы можете меня поймать потом зоне отдыха и мы с вами можем написать какую-нибудь сценарий ну и на сладкое у нас будет реализация долгоиграющих процессов также известный как саги очень модно сейчас говорить о сагах средствами inservice баса и в конце посмотрим немножечко на альтернативы beach sex распределенных систем это сильная связанность сильно связаны и системы плохо масштабируется вот и слабы связанная система обладают таким замечательным свойством что их можно масштабировать точечно соответственно событий на управляемой архитектуру это один из способов снизить существенно снизить связанность распределенной системы ну и по моему мнению это один из самых эффективных способов хотя он требует серьезного пересмотра архитектуры системы поэтому сегодня будем много говорить об архитектуре будет на самом деле молоко до событий на управляемой архитектура это архитектурный принцип согласно которому компоненты распределенной системы взаимодействуют между собой генерируя и потребляя события собственно хребтом событий направляем архитектуры кровью его можно сказать является паттерном мена сообщениями publish субскрайб в одном способе коммуникации зависимость между потребителем и вызывающей стороной и сервером развернуто ровно наоборот по сравнению с традиционным request респон сам потребители событий не вызывают ни инициирует взаимодействие не напротив ожидают слушают события и реагируют на события которые генерирует публикатор и лимитер направлении коммуникации всегда строго односторонней и публикатор и событий строк говоря вообще никак не зависит от ответов подписчиков поэтому могут выполнять полезную работу событие которыми обмениваются компоненты в этом вполне штаб skype как правило именуется в прошедшем времени в пассивном залоге например у реквизиты а сам этот с тем чтобы подчеркнуть что эти события извещают подписчиков о событиях которые уже произошли в домене сервиса публикатора правильно реализована и событийная архитектура имеет несколько очень положительных следствий 1 но она очень сильно снижает связанность системы связана система это сложная функция от нескольких переменных но вот 2 2 типа связанности топологическая эта зависимость от физического положения компонент и не возможность динамического перемещения этих компонента или подключении новых компонент говорим хардкорен физическое положение наших серверов и временная связанность блокировка ресурсов которые могли бы быть использованы для выполнения какой-то полезной работы эти два тип связанности существенно снижаются событий на управляемой архитектуры второе очень важное положительное следствие эффект от применения этой архитектуры это возможность выстраивания границы технологических границы технологических компонент сервисов соответственно доменом бизнеса компонентом бизнеса которые они реализуют принцессе например вашей компании есть разные дела эти разные отделы имеют свои уникальные какие-то наборы обязанности ни один из отделов не пересекаются по своей функциональности с другими собственно мы хотим добиться того же самого в совке которые реализуют и тот самый который автоматизирует моделирует этот самый бизнес как следствие соответственно сервисы в событий направляем архитектуре представляет собой абсолютно автономные совокупности программных компонент которые эксклюзивно ответственны за реализацию 1 конкретный бизнес-функции причем они могут себя включать разные части компоненты из разных слоёв приложения могут включать себя частью а и часть контроллеры пользовательской интерфейса контроллера и пей не один компонент за пределами этого сервиса не может вызывать функции которые относятся к этому бизнес домену их соответственно все данные которые нужны для реализации этого этой бизнес-функций принадлежат эксклюзивно этому сервис и больше никому грубо говоря у каждого сервиса есть своя база данных автономность это абсолютно логично следует из событийной архитектуры потому что если минимальный контракт между нашими сервисами это событие они больше не не имеют ничего общего между собой не шарят только схему то при условии что сервису для обработки любого события нужны только данные которые он получает в этом событии и данные которые есть его базе данных него просто не может быть никаких внешних зависимостей значит строить событий на управляемой архитектуру мы будем на трех компонентах на транспорте с помощью паттерна и архитектурного стиля в качестве транспорта будем использовать очереди сообщений поверх которых мы будем реализовать publish субскрайб обмена сообщениями и все это будет как бы собираться воедино склеиваться архитектурными принципам шина давайте поговорим поподробнее про каждый и из этих компонент и посмотрим какие почему именно они и какие ограничения не накладывают на нашу архитектуру сначала рассмотрим транспорт почему скажем так синхронная взаимодействия плохо подходит во первых требования к этому транспорту событий на управляем архитектуре потери данных не допустим это одно из условий при которых она работает потому что если события будут теряться или не доходить до своих адресатов следы этих событий будут пропадать система будет находиться в каком промежуточном состоянии мы никогда не сможем ее установить кроме того поскольку подписчики и публикатор и должны быть независимыми то транспорт должен также снижать временную связанность технологии типа рымарь molten daf cf http джейсон всевозможного сорта графики ела и вся относится к классу технологий могут просить же колы и буду дальше называть рпц и ворксе есть две большие проблемы описи и сложно масштабировать это требует больших усилий и orb isis фундаментально не устойчив к сбой давайте посмотрим что происходит в сценарии обработки http запроса на наш сервер приходит http-запрос с данными о заказе как вам туда пользователь пытается провести какой-то заказ начинаешь приложение в транзакции начинает работать с базой данных все это при условии что мы работаем под нагрузкой то есть мы говорим о системах которые находятся под высокой нагрузкой в какой-то момент в базе данных может произойти до блока их базы эта база выберет наш поток в качестве жертвы это водоблока база откатит транзакция в нашем коде всплывет не обработаны эксепшен которым мы в лучшем случае залакируем и отдадим пользователи очень информативное сообщение о том что что-то пошло не так и попробуйте снова вот данные мы потеряли клиент если речь идет о деньгах скорее всего не будет пробовать я бы не стал на самом деле пробовать заново и это приводит к тому что мы теряем деньги мы теряем заказ мы теряем данные мы теряем клиента и мы теряем репутации и с очень сильно упрощая на самом деле сценариев еще много но не надо проходить этой частью быстро также рпц обладают очень высокой степенью временной связанности дело в том что клиент вызывает сервер и блокируется он ничего не делает ждет когда сервер ответить ему даже асинхронные библиотеки здесь не сильно помогают потому что полезной работой клиенты но не выполняет вот в то время пока он ждет под нагрузкой в рпц системах построенных надписи и принципах какой-то момент наступает насыщение и эти системы выходит примерно стабильные показатели производительности но если рост нагрузки продолжится дальше то соответственно в какой-то момент это обязательно произойдет у каждой системы этот предел свой собственный вот но при росте нагрузки наша система насчет отказывать просто значит системах с глубоким графом зависимостей она не фокусе на системах с глубоким графом зависимости от козы любого сервиса на нижних уровнях могут приводить каскадным от отказом всей системы соответственно в системах построенных на рпц надежный системы показателей надежности системы в целом равняется надёжности самого слабого звена в этой цепочке зависимости а это означает что нам придется закидывать железом критически важные сервис и второстепенные сервисы и третье степенные сервисы если нам нужно обеспечить целевой показатель надежности системы в целом часто предлагаются в качестве выхода кэш типа конечно скорее взаимодействие в самом деле кэш не является решением кэш скрывают проблему но как говорится грязь есть грязь сколько не красит тут у меня есть для вас история из практики сын маминой подруги работал в крупные социальные сети кстати знаете как узнать что вы работаете в крупной социальной сети если при поломках в этой сети из twitter и начинают поминать какую-то мать в ваш адрес значит вы работаете в крупной социальной сети однажды весь сайт этой социальные сети я начал просто ложиться мы говорим о десятках страниц сотен сервисов которые работали но бэг-энде причем самая обидная без какой-либо видимой системы и причины понятное дело twitter немедленно предал анафеме всех разработчиков этой компании но разработчики оказались не лыком шиты и устроили развернули во внутреннем messenger полномасштабные военные действия образовались коалиции полетели взаимные обвинения такими фальцем вообще неизвестно чем бы это все закончилось если бы сын маминой подруги не проверил один очень важный показатель а именно коэффициент показать попаданий в кэш как он надумал буквально за секунды до падения всей платформы показатели резко обрывался в ноль система была примером лучших практик он был крест микро сервисы лот balancing вот это вот все но система рухнуло в одночасье когда какой-то дятел догадался погонять приемочные тесты на продакшене вот происходило потому что каждый сервис ходил другим сервисам за дополнительными данными единственный запрос http запрос на какой-нибудь home page расползался по всему графу сервисов в этой платформе вот собственно с этой этой истории я пытался продемонстрировать вам силу эффекта временной связанности которая наказывает на система построенные на рпц асинхронный месси дженкс совершенно естественным образом обладает низкой связанностью потому что обмен данными происходит асинхронно вот но есть еще одна особенность систем построенных на очередях сообщение замечательная особенность у них практически нету предельной нагрузки которая приводит к отказу в этой системы в какой-то момент прирост нагрузки производительность будет расти пока системе хватает ресурсов обрабатывать входящие сообщения и в какой-то момент она достигнет максимум на этом максимиана и останется даже если нагрузка будет продолжать расти просто сообщения будут собираться во входящие очереди также асинхронный масса jing устойчив к сбоям давайте посмотрим что происходит значит некий инфраструктурный код называемого шина в нашем случае будет inservice бас рамках транзакций читает сообщение из очереди он передает эти сообщения в наш код приложения если приложение использует киппер системные ресурсы типа базы данных например он инфраструктура тоже включает эти ресурсы в эту распределенную транзакцию если происходит какой-нибудь сбой тот же белок или сетевой какой-то сетевой проблема нашем коде возникает необработанный эксепшен который перехватывается этой инфраструктурой она откатывает транзакцию и все изменения которые мы совершили в рамках и транзакции просто отменять стираются как будто их не было а сообщение обратно ложится в очередь таким данным гарантировать таким образом гарантируется что никакие данные не будут потеряны и это сообщение она снова доступна для обработки интерес басевич карт процесс упомяну inservice баскак такая надстройкой над над очередями сообщений он реализует умную стратегию обработки ошибок там три стратегии первое это немедленные попытки сконфигурирована количество раз призванные побороть случайные ошибки те же блоке или не знаю сетевые какие-то проблемы вот если и это не помогает inservice бас откладывается общение с каким-то тайм-аутом по умолчанию 10 секунд экспансии экспоненциально увеличивает эти тайм-аут и после каждой попытки опять же совершаете попытки сконфигурирована число 1 если и это не помогает значит мы имеем дело систематической ошибкой как правило баги в нашем коде вот и такие сообщения сколько их ни не обрабатывая не никогда не закончится успешно в таком случае сервис вас перенаправляет сообщение в специально сконфигурированы очередь кто и называется дельта такую эти очереди у не творится естественный как только что-то происходит как только там появляется сообщение сразу же это сигнал команде abs починить да значит важно заметить что в процессе обработки нашего сообщения мы могли успеть отправить какие-то события извещающие другие части системы об изменениях которые так никогда и не произойдут очень важно не дать утечь этим событием наружу потому что если мы откатим транзакция сообщению летят в другие компоненты они их обработать наша система окажется в состоянии и рэйчел им консистенсии где проблемы очень сложно отлаживать в лучшем случае они приводят к фантомным заказом которых у вас никогда не было а в худшем случае это чужие заказы в вашем аккаунте вот чтобы такого не происходило можно использовать нативные средства очередей сообщений те очереди которые позволяют включать больше 1 в одну транзакцию тогда это автоматически реализуется если же сервис очередей сообщений такую функцию не поддерживает то тогда это задача инфраструктуры шины или интерес вас интерес bass в данном случае обеспечить гарантию того что случае отката транзакции никакие события отправлены нашим кодом не улетят наружу вас поддерживает нет официально поддерживает несколько сервисов череде и ты машинку revit рамки в фидере этот режиме когда он работает на каждом сервере air service баса и жорке сервис и так далее соответственно асинхронный транспорт обеспечивает нам требования которые мы выдвигаем к транспорту для реализации событий направляем архитектуры вот теперь давайте рассмотрим как в messenger дева синхронными сажен где реализуется publish субскрайб в двух разных архитектурных стилях эта шина и брокер требование к этой компоненте событий на управляемой архитектуру нас будет следующее нам нужно обеспечить автономность сервисов и для этого в системе всегда должен быть единый источник правды касательно определенного бизнес домена кроме того эти архитектурные стили призваны снизить тупо логическую связаны с систем говорим сначала про брокер первые брокеры появились где-то в конце 80-х годов из необходимости интегрировать развивавшиеся до этого хаотичного айти инфраструктуру в крупные процессы в крупные системы при этом по возможности не трогай сами приложения потому что эти приложения писали разработчики которые уже давно не работает в кампаниях исходный код этих систем потерянное вообще лучше не трогать вот так появилась эта архитектура известная в enterprise and aggression архитектор как хаббл смог в центре этой звезды находится брокер брокер физически отделена от всех компонент этой системы и занимается маршрутизации сообщений занимать сможешь муж а где зации сообщений по контенту он занимается трансформацией форматов подключением разных источников данных и высокоуровневая регистрацией процесса носач брокеры java появились немного позже эта часть спецификации джеймс и от брокеров интеграционных они унаследовали функциональность они унаследовали архитектурный стиль брокер и они унаследовали маршрутизацию по контентом проблема с message broker не заключается в том что они могут обеспечить автономность сервисов потому что message broker и не разделяет физическое и логическое представление сервисов для них каждый in point подключен подключенный к этому брокеру он равноценен и любой on point может опубликовать любое событие какую ему вздумается в чем проблема проблема в том что таким данным нету доверия но эти посмотрим пример ваш сервис прилетают два события первая из которых сообщает вам что цена на продукт на следующей неделе будет 1 рубль второе сообщение говорит вам о том что цена на продукт на следующей неделе будет 2 рубля какое сообщение считать истинным первая последняя а если учесть что в асинхронном месседжи нге вообще строго говоря нету гарантия порядка обработки сообщений проблема особенно если учесть что эти два сообщения могли отправить два разных in point а значит нашему подписчику придется реализовывать логику разрешений разрешение этих конфликтов собственно говоря которая не является его ответственностью потому что в первую очередь над publisher или для того чтобы они были поставщиками авторитетной информации сделать он это сможет только в том случае если у них будет единый источник правды в виде центральной базы данных в которую они все будут ходить и сверять данные тогда вопрос зачем было городить весь этот массы джинки если у вас есть центральная база из вы думаете что это проблема абстрактные надуманную очень-то у сына маминой подруги есть другая история за много лет до крупные социальные сети он работал одна компания которая участвовала в разработке единого информационного пространства для министерства это был крупный интеграционный проект построенный на message брокере листок вот этот message broker интегрировал десятки очень экзотичных базы данных на самых разных движках подключал десятки каких-то региональных систем документооборота и эта компания она занималась разработкой портала который служил входной точкой в это информационное пространство портал формировал запросы отправлял их message broker и интерпретировал результаты которые он получал из разных источников данных очень часто в ответ приезжали прямо противоположные значения от этих самых источников данных проблема была в том что это были очень так скажем чувствительные данные на результате которых принимались судьбоносные решения о репутации субъектов которые могли вообще говоря повлиять на их судьбу вот и соответственно перед сыном маминой подруги стала такая техническая моральная дилемма как решать эти конфликты и никому не навредить и он вышел из этой он вышел из этой проблемы совершенно блестящи стилю фримена дайсона которого однажды попросили проконсультировать по поводу какой-то сложной физической проблемы он выслушав просителей уходя сказал я так рад что это ваша проблема в общем поступил например точно точно также предоставив пользователям системы самим решать что делать с этими противоречиями данными понятно что это как бы привилегий работы с госструктурами где можно взять чиновника строить его войти систему во многих нишах такой подход просто не применим значит это история хотел продемонстрировать масштаб проблемы автономность сервисов значит 2 архитектурный паттерна называется шина также известно как enterprise service бас я предпочитаю называть шины паршина это единственное нагруженная смыслом слова в этом термине сейчас попробую дать определение шины своими словами а вы можете пока сфотографировать слайд забудем обо всем обо всех технологиях и сконцентрируемся на сообщениях приложение которые являются частями распределенной системы обмениваются сообщениями синхронно через очереди сообщений данные которые они упаковывают в эти сообщения могут быть динамически перенаправлены логическим адресатом потому что система не привязывается к физическим адресом этих самых получателей а просто передает сообщение инфраструктуре адресую какому-то логическому компоненту инфраструктура отвечает за то чтобы доставить это сообщение логическому адресатов даже если он меняет свои физические адреса ит-инфраструктурой собственный является шиной в шине нету никакого центрального звена нет координатора который сидит посередине компоненты шины работают в прямо внутри процессов предложений которые входят состав распределенной системы ну еще понятнее бывают шины сервисов это примерно то же самое что и вернет только в контексте распределенных систем это набор программных средств и грубо говоря договоренности о том как мы будем взаимодействовать который в целом вместе образуют эту распределенную систему посмотрим с точки зрения топологии как реализуется publish субскрайб подписчики это логические компоненты если мы с вами определения то эти данные упакованные в сообщения могут быть динамически перенаправлен даже если они меняют свои физические адреса вопрос откуда публикатор знает физические адреса и ответ с подписчики с вами ему сообщают когда стартует процессом сервис bass сканирует все библиотеки в этой рабочей папке находит сообщение сообщения события которые обрабатывают этот компонент и автоматически отправляет сообщение подписку публикатор у причем отправляет он их точно так же как все любые другие данные через очереди поэтому публикатор строк говоря и даже не должен быть онлайн в тот самый момент когда подписчик подписываетесь на сообщение сообщение будет доставлен в его входящая очередь когда он поднимется он его обработает публиковать сообщения может только один публикатор за логический и inservice база этим очень строго следит он просто не даст вам подписаться на одно и то же событие от двух разных логических компаниям что значит логических весы которую вы деплоить и могут существовать больше чем в одном экземпляре позволяет вам их горизонтально масштабировать но при этом обеспечивает средство с помощью этого дистрибутора таким образом чтобы для всей остальной системы в весь этот набор горизонтально масштабируемых сервисов представлялся как один логический компонент и inservice бас строго следить за тем чтобы сообщение события всегда была доставлена ровно одному инстанцию сервиса подписчика вот соответственно мы разобрали с вами почему шину почему не брокер и теперь нас да ну . ртк я сейчас фондом на самом деле не полный список песен сервис bass pro многие из которых я даже не успеваю поговорить к сожалению вот но повторюсь если вам интересно вы можете поймать меня в кулуарах я стараюсь ответить на все ваши вопросы вот давай теперь посмотрим на примеры кода мы будем строить с вами онлайн-систему предоставляющую премиум доступ к кому-то на контенту соответственно нас есть сервис продаж и и как бы инстинктивно кажется все очень просто и можно реализовать в один поток мы получаем этот заказ мы применяем какие-то политики цен мы применяем клиентские политики считаем скидки применяем бонусы записываются в базу данных проводим оплату через шлюз записываемся в базу данных помечаем аккаунт как премиальные записываются в базу данных а если происходит ошибка откатываемся назад спрашивается где здесь возможность для publish субскрайб есть один очень простой алгоритм который помогает понять где проходят границы этих сервисов и где собственно можно в сти обмен событийный алгоритм следующий если вы разобьете задайте себе вопрос если я разобью этот процесс на шаги и при условии что первый шаг закончился успешно но я получаю ошибку на втором шаге надо ли мне откатить 1 если вы спросите ваш бизнес ответом вам будет час массаж его конечно же нет бизнесу как правило гораздо интереснее держать клиенты даже если не получилось прям сразу провести оплату скажем нашем сценарии это у нас есть аккаунт пользователя с уже привязанными средством оплаты вот так ли нам судьбоносное важно провести мгновенную плату перед тем как дать пользователю доступ и ответ конечно нет исходя из этого процесс обработки заказа может выглядеть следующим образом services принимает эту команду применяют какие-то ценовые политики и одобряет заказ мгновенно отвечаю пользователю что все отлично мы приняли ваш заказ вы скоро проведем оплату и вы скоро получите доступ к премиальному контента параллельно он как бы публикует события аудио акцепт от которая который подписан серость биллинг биллинг получает это сообщение пытается провести оплату через я не знаю партнер в который проводит оплата если происходит кит ошибки он пытается и пытается и пытается если у нее не получилось после сконфигурирована во количество раз но мы пошлем асинхронно письмо пользователю попросим его обновить данные если прошло успешно все биллинг отправляет свое собственное событие параллельно с этим сервис контент обрабатывает событие заказ одобрен и собственно выдает доступ к премиальным контентом его на самом деле все равно какое событие обрабатывать он обрабатывает оба вот для того чтобы реализовать обработчик сообщения все что нам нужно сделать это реализовать в нашем классе такой интерфейса яндекс обратите внимание количество строк на количество строк кода реализующих инфраструктурными инфраструктурной лоджик логику их ровно ноль и это не преувеличение с помощью inservice бас вы действительно можете забыть тропить система построена на событиях написав буквально четыре строчки кода понятно что в данном случае это идеально сферической система который только и делает что обмениваться сообщениями смысл в том что как я уже говорил самом начале inservice бас берет на себя всю инфраструктуру и нам остается писать только бизнес-кот вас идет с довольно такими хорошими дефолт ними установками все сообщения сохраняются persistent на но мы можем всегда это переопределить если мы реализуем соответствующие маркер интерфейс и дам случае например одной строчкой мы говорим интерес bass что мы хотим установить все зависимости которые могут нам понадобиться для строения всей нашей системы что происходит данном случае если мы используем 8 . в качестве транспорта to ensure и usbasp оставит этот транспорт на той машине куда много за тепло им при старте создаст сам все очереди которые нужны будут для функционирования системы из конфигурирует все права так же мы можем переопределить разные политики например политики обработки ошибок и так далее очень сложно бывает перейти в и на ориентированной архитектуре от представления в коде к представлению физическому интерес бас идет вместе с плагином он называется сервис inside который в real time реверс инженере события которые получают в очереди аудита и строит на основании них фактически интерактивную документацию которая всегда актуально документацию вашей системы в данном случае это flow diagram и с помощью которой можем посмотреть каскадный эффект прохождение событий через всю систему введите не только на которых она обрабатывалась или вот например в виде секунд диаграмма ну и сладенькая наконец сага сзади темен сага пошел из работ посвященных управлению длинным транзакциям в базах данных как управлять длинными транзакциям в базах данных не создавать длинных транзакциях базах данных разбивать их на маленькие транзакции и выполнять их последовательно если происходит ошибка на каком-то этапе уже закончены и транзакции откатываются с помощью компенсирующих действий в событий на управляемой архитектуре долго играющий процесс это сложный процесс которым как правило больше одного шага представляемой в виде конечного автомата перед триггерами для переходов между состоянием и это автомата является как и все интерес басе сообщение соответственно между поскольку сообщение приходит не детерминирована мы никогда не знаем когда придет следующее то это промежуточное состояние нужно хранить где-то inservice бас предоставляет как бы пир системное хранилище куда это все дело складывается он не буду про эту потом можете меня поймать я вам расскажу ещё побольше значит давайте усложним наш пример и добавим защитные механизмы в наши премиум доступ контента скажем если заказ не был оплачен течение пяти дней то подписка должна автоматически аннулируется плюс по истечении оплаченного периода подписка то соответственно тоже должна автоматически заканчиваться может вы заметили уже но у нас в данном случае уже были нами наметки процесса потому что наш сервис контента обрабатывал два события а как бы результат этих событий был один и тот же выдавалась премиум подписка часто хорошим признаком наличия саги является необходимость коррелировать событий по какому-либо признаку чтобы реализовать загул чтобы сказать интересовался что вот у нас блин долгоиграющий процесс наш класс должен над наследоваться от класса сага параметризовано во структурой этого состояния данном случае мы тратим а был ли оплачен заказ текущее состояние ади заказа мы объясняем сервис басу какими сообщениями стартует длина играющий процесс и объясняем ему как с маппет сообщения на state который он хранит в пир системным стороне дальше мы обрабатываем собственно пишем логику которая реализует этот конечный автомат и в данном случае при прохождении события одобрения заказа мы проверяем был ли он к этому моменту оплачен если он к этому моменту не был оплачен мы заказываем time out one series базе есть совершенно гениальная фича шаги могут переходить менять свое состояние не только на основании событий которые приходят извне на основании сообщений но и на основании временного какого-то срабатывания таймера соответственно делается это вот просто рекорд тайм-аутом чтобы обработать этот таймеру нам надо сказать через базу как сзади смочить ну и собственно все очень быстро про альтернативу альтернативы есть надо найти если вы не найдут найти то ищите фидера этот курс ну и последний очень хотела сказать что масштабные решения не требуют инструментов разработки которые не только стимулируют применение хороших архитектурных решений но еще бьют по рукам вот мы помним про ограничение а теперь сюрприз компания потягивая software дарит вам за то что вы досидели до конца и до слушали меня и не заснули бесплатный доступ к их онлайн курсу advanced distributed systems design все что вам нужно сделать это кликнуть на эту ссылочку кликнуть на эту кнопочку и наслаждаться спасибо пожалуйста указывайте волонтерам кого спрашивать потому что сейчас время вопросов и спикер готов выслушать ваши вопросы вы только волонтер так спасибо за доклад очень интересно меня зовут андрей компания playrix вопрос был слайд где дистрибьютор был указан да и можно вернуть да кажется что введение дистрибьютора немножко слабое звено добавляет потому что если например он падает один то сообщение до всех остальных подписчиков не дойдут как как зачем так сделано чем это обусловлено отличный вопрос на самом деле дело в том что не все транспорта очередей позволяют реализовать ботерн competing in cima когда на одну и ту же очередь вешается много процессов которые опрашивает эту очередь в missing you например такого не получится сделать случай если вы горизонтально масштабируете ваш процесс это на многих физических или там виртуальных серверах вам нужен такой компонент который будет разбрасывать эти цены по сути мастерноды который разбрасывает сообщения по своим нодом вот так же но и и задачи будет данном случае для всей остальной системы служить входной точкой как будут объединяющим в единый логический компонент все вот эти масштабирован и инстанции поэтому дистрибутор не обязательно использовать если он используется то тогда он действительно является единой точки отказа но не все системы только вот этой вот части подсистема этого сервиса как правилами тяги руется это запуском дистрибутора на windows называется frolova класса для обеспечения высокой надежности спросить вот там есть вопрос спасибо за доклад очень интересно как каждый раз когда сталкиваюсь какой-то лютый асинхронный всегда концепция кажется прекрасный замечательный до того момента пока ты начинаешь это де бо жить вот так давай расскажи немножко правды а как понять куда теряются событий кто на что не подписался вот в таком ключе классный вопрос сразу видно человек испытал эту боль а ну во первых как я показывал вас есть такой отличный инструмент сервис inside с помощью можем посмотреть кто же что куда ушло посмотреть в динамике как это происходило в том числе проследить ошибки которые происходили в системе был добавить то есть в данном случае n series бас идет еще с набором инструментария который помогает отлаживать эти эту лютая sync раньше на которой как правильно сказал потому что обычно в таких сценариях все что у тебя остается это логе это гораздо лучше чем блоги но это естественно не не гарантирует что вся система будет работать прекрасно и ты всегда будешь знать и ответил на вопрос тогда спасибо большое за внимание"
}