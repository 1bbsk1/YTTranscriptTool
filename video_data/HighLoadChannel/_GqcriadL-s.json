{
  "video_id": "_GqcriadL-s",
  "channel": "HighLoadChannel",
  "title": "FAQ по архитектуре и работе ВКонтакте / Алексей Акулович (ВКонтакте)",
  "views": 10484,
  "duration": 2930,
  "published": "2019-05-15T04:50:50-07:00",
  "text": "всем привет меня зовут лёша я работаю в команде в контакте по контроль работ com не расскажу до тем собирательным ответом на некоторые очень часто задаваемые вопросы как у нас что работает точки зрения платформой структуры то есть не про разработку и на как это в целом по железу во-первых погрому там всех я рад что так много людей смогло доехать первым автобусом видимо да и не уйти на кофе и так о чем будет доклад доклад будет в первую очередь про общую архитектуру как на что работает сфера взаимодействия между ними и их типы и так далее потом отдельно поговорим про наши базы данных и что у нас вместо них потом отдельный раздел который мне кажется довольно важен это про сбор лагов и мониторинг всего проекта в целом и в конце по возможности по времени я расскажу про наш дупло и так коротко как это все дело диплом а первых почему об этом всем рассказываю последние четыре чем-то года я занимаюсь он морем задачами в к связанными с бэк-энд задачами в первую очередь там мультимедиа задачи видео музыка документа голосовое сообщение картиночки истории live трансляции общем кучу всего загрузка хранения обработка раздачи этого дела потом вещи связаны с инфра turn ими задачами мониторинг всего этого дела с точки зрения скорее разработчика чем администраторской логии региональные каши наш протокол отпустишь не об этом а че буду рассказывать а интеграции с внешними сервисами рассылки по шее парсинг внешних ссылок лента rss спину и помощь коллегам часто по вопросам которые я сама убит не в курсе приходится лезть в код который никогда раньше не лазил и познавать новое то есть в целом получается то за это время потрогал много чего на сайте и в вестнике а вот такие знания и так общая архитектура все у нас начинается обычно с фронта то сервера или группа серверов которые принимают запросы они принимают запрос в основном плохо ттп с это у нас основная версия сайта мобильная версия сайта а также всевозможные официальный неофициальные клиенты нашего обед думаю тут все понятно у нас есть прием rtmp трафика для live-трансляций также фронт серверами отдельными у нас есть при получении websocket соединений для нашего streaming api так или иначе для gps и для веб сокетов и стоят и джеймса на серверах для rtb трансляции мы пережили недавно наше собственное решение кайф но это будет за пределами выступления так или иначе это серверов для отказоустойчивости и анонсирует общее 5 и с такими группами чтобы случай проблема одного сервера мы не теряли запросы пользователей все работает для собственно https и websocket of и те же сервера занимаются расшифровкой трафика чтобы забирать эту часть нагрузки на циpкa на себя а не на северо за собой я больше не буду говорить про websocket rtmp только правильно по хотят пышную запросы вот обычно таки то что обычно понимают под проектом веба итак за фронтами устоят бэкон сервера который собственно обрабатывает запросы которые получают фронты об клиентов что это такое это сервера на которых работает кто-то пышный демон потому что хотят вас уже расшифрован он является кпп с сервером меня попросили не рассказывай об осаго я не буду вашим если очень коротко для понимания происходящего это сервер который запускает мастер процесс он запускает пачку дочерних процессов и от передает тем слушаю сокеты они обрабатывают свои запросы при этом чтобы обрабатывать запросы быстрее эти процессы не перезапускается между каждым запросом от пользователя они просто сбрасывают свои состоянии эти глобальные переменные прочее в такую первоначальные веру в или у кого это образное но чтобы просто запросы запросам все-все причем все эти наши бренды это не огромный пул машин которые могут обработать любой запрос мы их разделяем на отдельные бренды а как минимум для того чтобы в случае проблем на какой-то группе машин не она не аффект тело все наши сервера случае проблем допустим на не знаю на видео близкой мне теме человек который сидит в личке там или как они там смотрят слушают музыку и так далее он даже не узнает о проблемах таки топорами случились при этом выбор на какой backend отправит запрос лишает режим сна собственно фронтах по конфигу оплатишь при этом наши сердца собстна разделяются на группы чтобы понять сколько машин нужно иметь в каждой группе мы опираемся мимику ps который в общем то довольно вещь бесполезное случае что у нас разные бэг-энда у них разность запросы и каждый запрос имеет разную сложность для вычисления и и мы оперируем понятие нагрузки на сервер в целом то есть на процессор а также собираем перс потому что собственно + новый сервер то можно собрать профиль и посмотреть нагрузку таких серков нас тысячи собственно на каждом физическом сервере запущена пачка этих к php к ну чтобы утилизировать все ядра такая пример ситуация дальше у нас есть советские что такое советский отрезке это контент сервер то есть хранилище как называемые поста хранил ки это себя вся вера который в 1 чуть хранят файлы вторую очередь они занимаются обработкой собственно как залитых файлов так и всевозможными фон и весьма трудными задачами которые им ставят основной веб front-end и паша цепи ссылки таких я думаю нет смысла рассказывать как работают хранилища но тут думаю каждый представитель вот таких слов анаса десятки тысяч от именно физически серваке которые хранят файлы возите любит на файлике загружать о моих любимых хранить и раздавать часть этих серверов закрыва закрыты специальные puppey по серверами дума кто открывал в канкун network они это видели было не по нише это такое что такое puppey если мы закрываем 11 лак торгует у нас есть 2 варианта отдачи и загрузки файла мастерова который за был закрыт во первых мы можем пойти напрямую типа сервер сто пятьсот и файлик п.ф. а другой вариант что мы его закрываем другим сезон перед ним и идем уже собственно с указанием как часть пути и номер сервера в чем разница по это исторически сложившиеся название для фото оплот об это фото прокси то есть один сервак исторический мог позволить из себя фоточку загрузить а другой числе фоточку отдать теперь это уже не только фотографии но название сохранилось к практическим причинам и зачем это было сделано во первых эти сервера занимается терменируем их этот пес сессии чтобы во-первых снять нагрузку процессор но сохранились я не могут быть большими жирными и медленными при этом а во-вторых так как мы найти сервера загружаем польские файлы мы их обрабатываем качаем внешние ссылочки то чем меньше информации чувствительный мы храним этих машинах тем лучше например собстна ключи шифрования фпс при этом так как машина закрыта у нас другие машинами тоже нашими то мы можем позволить себе не давать им белые внешние печники и сэкономить на флопе а это довольно полезно plus позволяет гарантированно защитить ее машину от доступ извне у него просто нет apes не карта попасть на нее в плане отказоустойчивая такая схема работает также несколько физических серверов имеет общая физическая печника и железка перед ними выбирает на куд-куда отправить запрос тут схема такая же об этом еще позже скажу о других вариантах и спорный момент является в том что в этом случае при наличии одинаково печника на несколько машин знаковым и х 100 х 100 тыс руб в каком ппп в каком к браузер клиента имеет ограничение на количество переменных запросов к одному х 100 и он не может отправить куча кучу запросов на один такой адрес она вовремя собственно более-менее повсеместных это тп-2 я считаю что это уже не так актуально и явно минус этой схемы в том что нам приходится прокачивать весь наш трафик который дает хранилище через какой то еще один сервер это явный минус тут как бы знаем сложно придумать плюсы это схемы при этом так как мы прокачиваем трафик из машины мы возможно пока что все еще не можем позволить себе прокачивать тяжелый трафик например видео по такой же схеме мы выгоняем напрямую это некая жизнь реальная жизнь без отдельная прямая connect для отдельных хранилищ именно для видео более легкой контент нагоняем через прокси не так давно у нас появились более улучшенная версия этих прокси я расскажу о них подробнее и чем они отличаются и зачем это нужно во первых почему сам в сентябре прошлого года компания oracle который до этого купила компанию сан она уволила огромное количество сотрудников сана и можно сказать что в этот момент компания sun club прекратил существование в том виде который она была я наши админы когда выбирали название для новой системы решили отдать некую дань уважения памяти этой компании и назвали но у систему сан и или просто солнышке как мы их называем вот ясно в чем проблема об этих по пышек которые у нас были исторические везде просто 1 уже говорил что несколько физических серверов имеет общий api адрес и нет возможности как-то контролировать на какой запрос придет какой сервер придет запрос поэтому если разные пользователи приходят за одним и тем же файлом это нормальная ситуация то при наличии каша на этих серверах и от фальк оседает в кэше каждого файла от каждого сервера это очень эффективная схема и поделать было ничего нельзя на мы не можем сортировать контент потому что мы не можем выбрать конкретный сервер этой группы у них общая печники все как бы вода и нас и по некоторым внутренним причинам не было возможностью ставит акихиро в региона не добудет не были стоять именно только в питере вот проблем было что было сделано солнышком во первых было сделано изменение системы выбора у каждого сервера свой собственный api индивидуальные но при этом они все имеют общую под-сеть и настроен у нас все так что случае выпадения 1 сервака трафик размазывается по стальным пером той же группы автоматически и получается что с одной стороны есть возможность выбрать конкретный сервер у нас нет и избыточного каширования с другой стороны мы не потеряли в плане надежности при этом есть поддержкой весов то есть мы позволяем себе мы можем положится как ставить машины разной мощности по необходимости так и случае каких-то временных проблем менять весь сосуд работающим солнышком для уменьшения нагрузки на них шпане там прочухали снова заработали и появилась возможность поддержки регионов тыс мы можем эти ставить ребра не только в питере но и других местах об этом еще подробно расскажу ну и наконец забавная вещь связана шарди рование мы обычно контент сортируем так чтобы разные пользователи шли за одним файлом через одно солнышко чтобы у них был общий кэш но когда мы запускали наше приложение клэр значит такой клевер вообще хоть кто-нибудь зале прикольно один вот три человека четыре ладно скажу это такая как бы онлайн викторина где ведущий live трансляции где ведущий задаёт вопросы пользователей в реальном времени должна тщательно выбирать варианты при этом там в этом приложении еще есть чате где пользователи могут просто похудеть и когда трансляции сидит больше ста тысяч человек они все одновременно пишу свое сообщение эти сообщения ссылаются совсем посмотрит то вместе сообщения приходят еще аватарка если 100 тысяч приходит одна его тарка в одно солнышко то она может иногда закатит за тучку вот и чтобы это не происходило именно для некоторых вида контента у нас выключается тупая схема которая размазывает файлики по всем имеющимся солнышком какого-то региона чтобы выдержать такие на всплески за одним и тем же файлом и так как выбили . внутри это просто сервак сын джеймсом и там кэша либо просто в памяти либо быстрый диски натана в м.е. при этом ссылочка получается у нас есть солнышко которая стоит в четвертом регионе это второй сервер группы он закрывает собой файл ps который физически лежит на сервере сто пятьсот пример такая ссылочка теперь про регионы и при оригинальной каши то есть мы добавляем еще один узел нашей схемы это кэш то сервера каширования эта схема расположения наших региональных к шеей нас их примерно 20 штук это место где стоят именно каши и солнышке которые могут просто коллировать трафик через себя это именно кэширование мультимедиа контента это не хранит политики данные на просто файлики видосики по все такое музыка там вот и так во первых как определяется а печник пользователи регион пользователя во-первых мы собираем анонсированы в регионах пппп префикса сетей собираем их себе и в случае fall back а у нас еще есть парсинг базы гейб и на случай если мы не смогли по префиксом найти я печник так или иначе мы в собстна можем в коде теперь посмотреть по озеру его 1 пи несколько регионов то есть те точки к которым он скорее всего наиболее близок географически как это все работает регина решают нас есть некий номер регионального каша где сидит пользователя у нас есть некий нотификатор файла мы просто берем эту пару там за картой штаб к ходить и называется инкремент ему маритен при каждом скачивание тут все просто при этом легенд некие демоны сервисы на наших регионах время от времени приходится наши api и говорят я конечно такой-то т.к. ты мне список самых популярных файлов для моего региона и который нам не еще нет api дает ему пачку чтоб на сортированы по рейтингу едим он выкачивает после выкачивания он начинает себя отдавать все просто 8 и это принципиальная разница отличия по у папы солнышек от к шее что те отдают файл через себя сразу даже в каше это файлы нет кэш сначала выкачивает файл на себя а потом уже начинает его отдавать при этом мы получаем что контент файлов ближе к пользователям и допустим только с мазков сколько шамар заем больше таро битов численной больше нагрузки это трафик который мы сняли спикера это очень хорошо но есть проблема в том что для супер популярного контента не хватает иногда сети на отдельный сервер из нас серваке 40 50 гигабит и бывает что контент который забивает такой канал полностью сейчас мы идем к тому что сделать возможность хранения более одной копии популярных файлов в регионе сейчас мы такого делать не может быть и фалик он лежит в 1 копия регионе этого не хватает и надеюсь что мой до конца года это исправим чтобы всем было лучше вот то есть мы рассмотрели вас есть фронты которые принимают запросу у нас есть бэг-энда которые обрабатывают запросы у нас есть хранил и закрытые двумя видами проксей и нас региональной кашу есть что в этой схеме не хватает в этой схеме не хватает баз данных где мы собственно храним данные кроме наших файла и переходим ко второй части собственно доклада про базы во-первых мы называем их не базами а движками потому что базы данных в том термине какая-то используется обычно в проектах у нас их не существует практически не как так получилось потому что в 2000 лет восьмом и девятом году когда у в кабул такой взрывной рост популярности проект полностью работал на мускуле ммкф и были проблемы man emu сколь и любил упасть и по коробки свои файл после он не поднимался ам-ам кэш начинал постепенно деградирует по производительности на приходилось его перезапускать и получаются у нас есть проект который растет популярность и мы имеем президент на и хранилище который портит данные и у нас есть кэш который тормозит его приходится перезапускать потери каша в таких условиях как бы развивать растущей проект было тяжело и собственно было принято решение попробовать переписать критические вещи которые в который мог бы уже уперлись как проект на собственные велосипед и к чертям на это оказалось очень таким успешным решением ну просто была возможность это сделать и было крайне необходимость паша таких способов масштабирование в то время не существовало не было всех этих cucine рысаки вас но иску или чуть не чувствовал был мускуле собственному с кульмом кэш там пост был все все больше ничего причем и так как эта разработка делала одна и та же наша команда хищников то они все были сделаны единообразно они были сделаны так что зависит независимой а движка они имели примерно одинаковый формат файлов которые не писали на диск они имели одинаковые параметры запуска они одинаково обрабатывались сигналы и они примерно одинаковы вели себя в случае каких-то кривых ситуации какие-то проблем это позволяло админ очень удобно эксплуатироваться эту систему толстом движков не было необходимости поддерживать зоопарк и заново учиться эксплуатировать каждую новую старую базу которая была сделана другой команды по своему это позволяло быстро и удобно наращивать собственно количество таких движков на данный момент мы их команд как команда в целом по написала довольно много это лишь часть названия из них то есть под каждую задачу которая требует какую-то специфическую структуру данных или нетипичные запросы которые не может рисовать существующей движок наша команда просто новую техников пишет новый движок почему нет допустим у нас есть отдельный движок memcache который похож на обычный ну там плюс кучу плюшек и который не тормозит это не прекрасно должны тормозит вот и есть допустим отдельно пмм кэш и the persistent наймом кэш который умеет хранить данные на диске при чем больше увязает в оперативку чтобы не терять данный при перезапуске ну как почему бы нет и куча куча движков подсаке рада задача очереди списке всякие сеты близость но к мерном пространстве очень много всего штата требуется нашим проекту при этом с точки зрения коды нет необходимости представлять себе движки или базы данных как некий процесс сущности инстанции код работает именно с кластерами с группами движков причем именно одного типа допустим есть кластер мамка шеи есть кластер списков мы так далее то есть просто какая-то группа машин при этом коду не нужно знать вообще физическое расположение этих серверов и сколько их вообще то есть он ходит по некому имени интенсификатор у идет в кластер чтобы это работало требуется добавить еще одну сущность которая находится между нашим кодом и нашего движка me the proxy которая по сути теперь является наши какие-то свяжу еще шины как который работы по чистке весь сайт ну просто по-другому не получится при этом у нас нет север от discovery вместо него есть конфликт и прокси которая знает расположение всех кластеров всех шар дав этого кластера этим занимаются админы программистом не важно вообще сколько где чего стоит они просто ходят в кластер это позволяет нам много чего сделать это приза при получении запроса prophesy of for vogue запрос q знака зная куда она сама определяет при этом она является точкой защита от отказа сервиса случаются к этот мешочек тормозит упал то про вся это понимает и начинает клиентской стороне сразу отвечать теперь у нас обед приходите попозже даже не отвечает что позволяет нам снять а тот ожидании тайм-аут который же код ждёт передышка он не ждет а все движок не работает и надо как-то по-другому себя вести . ты должен быть готов что базы работают не всегда мой пункта нормальной жизни так десяток при этом если мы все таки очень хотим иметь какое-то сторона нестандартное решение в качестве нашего движка то было принято решение что мы не вписываемся в нашей прессе прокси который делал на имя для нашего движков а делается отдельно prophecy именно под нужную задачу допустим для мускула которые нас все еще есть у нас кое-где у нас есть db прокси откажем для к хауса которому ещё поговори у нас есть титан house тоже прокси это работает в целом и так у нас есть некий сервак в нем работает там php к php код год питон общем любой код который требует который умеет ходить по нашей маркусе протоколу он ходит локального писи проще то есть на каждом сервере где у нас есть наш код запущена своя локальный прокси это не отдельное место где работает . отказа а на каждом сервере своя и она уже зная по запросу куда прямо на нужный тип при этом если один движок хочет пойти в другой движок даже если хочет против своего соседа он идет через проксю потому что свет может быть в общем случае другом до центре стоять он движок не должен завязываться на какое-то особое знание расположение чего-либо кроме самого себя это но стандартное решение у нас схема по которой работают все дырки нет вчитываться это просто как пример кому то интересно это бинарный протокол на ближайшие аналогам будет являться пар табов то есть это схема с заранее песчаной схемой собст опциональный поля там сложные типы к которым являются расширением встроенных скаляров и так далее на зависть и поесть запросы ночь все работает по этому протоколу кому хочет может интересовать интернете есть описание помочь все работает по нему при этом получается у нас есть причины протокол выполнение запросов dashcam он работает поверх той схемы это все работает проверки хиппи и виде присоединения артизи битум понятно тут надо это обычная схема в целом счет собственных и дпс и дней и так часто спрашивают зачем он видит и как вы с ним живете во первых это помогает нам избежать проблемы огромного количества соединений между серверами есть у нас на каждом сервере стоит рпц прокси на в общем случае может пойти в любой движок то мы получаем десятки тысяч десять дней на сервер это очень просто бесполезная нагрузка просто потому что она есть кучи теперь этой проблемы нет при этом нет необходимости типичного хан шейка это на проблем когда поднимается но там движок или новый сервер тут оно устанавливалось бык учитесь и произведений разом это очень плохо просто становится все ядра в ядро и все для каких-то маленьких легковерных запросов которые вырезают допустим кончу типичный пилот то все общения кода с движком это 1-ый тип пакет в одну сторону один тебе пакет другую сторону все 1 round trip ее кот получил ответ от движка без хан шейков и прочих да это все работает только при очень маленьком проценте потери пакетов понятно в протоколе есть поддержка ли трансмита всяких там таймаутов но если мы будем терять много мы получим наш этики практически ну то есть не очень выгодно чист от океаном и это не гоняем таких сыров нас тысячи там та же самая схема шона карра физический сервер ставится пачка свирлов они в основном 1 поточные чтобы работать максимально быстро без блокировок и собственно сортируется как однако точное решение при этом нет ничего более надежного у нас чем эти движки и вопросу хранения данных система уделяется очень большое внимание при этом все реализовано очень просто и так и такая схема используется в кучу таких решений здесь кипишуй биологии то есть это некий файлик у до в конец файла дописываются события на изменение состояния ли данных в разных решениях называется were the head логин капитальный файл в общем зависимости от как у как назвал на принцип один и тот же но чтобы движок при перезапуске не перечитывал весь этот гигантский берло который может описаться годами в принципе белок рыжики пишут снапшоты то есть состояние на текущий момент и при необходимости они читают начало из него потом дочитывает уже с берлога я еще покажу пример при этом берлоге как я говорила не все являют пишутся в одинаковом формате чтобы админы могли своими инструментами и как-то одинаково администрировать проверяет тому сплетите так далее по необходимости о снапчата такой необходимости есть там есть общая заголовок оказывающих чей-то фильм стал хитом интернета к движка это я мой движок а какой там тело никому не важна эта проблема со лодыжка которую записал как это принципе работает бегал приближусь у нас есть некий сервак на котором работает движок он открывает на запись новый пустой белок пишет пишет в него событий на изменения какой-то момент нам написал какой то размер файла и сам решает либо ему приходит из нее сигнал ты сделок атеистов шел он открывает создаёт новый файл пишет в него полностью свои состояния дописывает текущий размер берлога костику ну понятно конец файла и продолжается по написать дальше начинить не где происходит он не новый белок не создается какой-то момент когда же где запустилась у нас есть на диске белок есть snapshot движок читает полностью snapshot поднимает свой состояния какой-то момент вычитывает позиции которая булавка размер берлога на тот момент создания и дочитывает конец влога для получения текущего состояния ну совершенно и продолжает писать дальше события довольно обычная простая схема ко сну и все наши движки работу такой схеме ну вот у которых есть стоит что он получаем что во-первых репликацию на state and bass мы пишем в белок никакими там изменение там странице что подобно именно запросы на изменение очень похож на то что приходит посетил удачи заменено и и этой же схема используется не просто для репликации но еще для создания backup of то есть есть нас дырочек который пишущий масти он пишет белок поднимается копирование это берлога в другое место куда угодно куда настроить собственно админы и все у вас есть backup то есть движок пишут в конец файла этот конец уносится в реальном времени на другое место и что нужно именно читающая реплика чтобы снизить нагрузку на чтение подсыпал просто поднимается читающий движок который подсчитывает конец берлога и поставок воняет эти команды в себе локально все отставания здесь очень маленькое то вообще совсем и плюс есть возможность как в нормальных базах узнаете насколько реплика отстаёт от мастера как принципе работаешь родирование как прокся понимает на какой шар кластера отправить причем так как отнюдь не покажи мне показывает я отправка пины 15 карт нет это делает прокси сама простая схема так называем first in то есть это первое число в запросе я привел пример для простого монтаж текста протокол ты запросы бывают сложные структурированы это время для простоты то есть дай-ка мне фотографию соточка 500 и берез опыт на первое число в запросе это такое деление на размер кластера это полезно в случае когда мы хотим иметь локальность данных 1 сущность допустим соточка может быть юзерам может быть один ником группы и мы хотим чтобы и каких-то сложных запросов все данные одной сущности были на одном шаге но по возможности часто нужно другой вариант если нам пофиг как она размазано по кластер мы можем делать хэширование я целиком даже сам получаем хэш за такое деление по ее собстна получаем номер шарда оба эти варианта работы только в том случае если мы готовы к тому что мы будем случае наращивание размера кластера мы знаем дробитель увеличивать его в кратное количество раз тип нас было 16 сортов мы хотим больше он не хватает мы можем сделать 32 безболезнен без downtime а если мы хотим наращивать не красноту будет downtime потому что не получится аккуратно перед робить это все без потерь и это полезно но не всегда если на нужно именно добавлять или убирать сервера в произвольном количестве используется кастят на хэшировать на кольце типа какие там а где мы можем но при этом мы теряем полностью локальность данных приходится делать мир запроса на кластер чтобы каждый кусочек вернул свой маленький ответы уже смерть на прокси про xiaomi ритме ждут ответы ну и всякие супер специфические запросы я думаю не будет рассматривать он совсем хардкор это выглядит так что это sip proxy получает запрос она определяет какой класс стр пойти и определяет собственный шар и тут есть либо пишущие мастера либо если кластер имеет поддержку реплик то он посылает в реплику ну там по запросу это всем занимается прокси и так логии мониторинг как вообще у нас в со всем этим исторически пишутся логе те варианты которые не умерли на данный момент первый самый простой очевидный вариант по просто пишем о римом кэш то есть есть некий префикс ключа то есть имя logo некая строка есть на размер этого logo количество строчек мы берем случайное число от 0 до числа строк на -1 и в ключ префикс плюс это число пишем куда строчку logo плюс время потом при необходимости логе прочитать мы делаем мульти get всех ключей сортируем по времени получаем real time protection лог это используется именно когда нам нужно что-то по добавить продакшене ничего не ломая не останавливали пускай отрывка многие машину примут вариантами но понятно что это не локаторы не живет долгое время некие карантине я чуть более сложный вариант который надежно хранит логе в помогаешь и но более-менее но не важно для надежного хранения логов нас есть ручек это называется logs в не поверите он создавался и на для хранения лак логов и он используется у нас очень широко это огромное количество кластеров и самый большой известный класс стр хранит по моему 600 терабайт запакован их логов в себе в одном но в этом движке есть он движок очень старый много лет есть кластеры который работает уже летом по 6 по 7 с этим движком и он имеет некие проблемы которые мы сейчас пытаемся как-то решить начали очень активно использовать их для хранения логов немножко собственно об этом как мы храним логия вкх а сейчас чего вообще нужно вот у нас была схема как мы ходи в нашу движки у нас есть наш код который приходит локальные пепси прокси а тот понимать куда нужно пойти и собственный движок если мы хотим писать логе в клик house нам нужно в этой схеме поменять 2 место во первых нужно заменить какой-то движок на кри хаоса вторых заменитель пищи бокс который не имеет в него ходить на какое-то решение которое умеет и умеет в офисе при этом ну с движком понятно мы заменяем его на сервер лина кластер серовской хаосом теперь вопрос собственно в том решение которое будет ходить и таки тан house об этом сегодня если кому интересно юрин среди нас будет рассказывать в комплект входит днем но попозже коммуниста можете послушать и прямо подробности хардкор надеюсь вот при этом если мы пойдем напрямую из китая house of tree house он не справится таким колесом соединения что машин очень много он просто складывается от нагрузки даже без запросов просто токсичных соединений чтобы эта схема работала поднимается на сервере сакли хаусом локальная держит прокси которая написана так что она выдерживает такие объемах соединений плюс она может буфере zero вать данные в себе более менее надежно и по такой схеме идется бурлаков к этой схемы а да еще когда мы не хотим реализовывать и спичечную схему в каких-то станут нестандартных решениях не в наших например в джинсе особенно поддержали в пентхаусе возможность получать логе появитесь в случае если этот отправитель и получатель логов работает на одной машине то вероятность потерять эти песни пакет в пределах локалхост а довольно низкая и как некий компромисс между необходимость реализовать aspesi второе решение и надежности могут используем просто покупаете такая схема к ней мы еще вернемся теперь по мониторинг здесь два важных различия есть нас отдельно совершенно логе которые делают администраторы по своим серверам и есть логика торы и продуктовые мы как разработчики пишем из коды в сами по себе начала by из татарские коротко то есть на всех серверах работает у нас наяда та которая собирает статистику она целует и графит карбон все потому всем известны название как система хранения при этом используется не в остер какой-то а вот crack house опять же и при необходимости можно как из него напрямую читает так и использовать графа ну там для метрика графиков очагов и прочего это довольно неплохо использовать нас как у разработчиков есть доступ надо тигров они и принципе нам этого хватает с точки зрения продуктовых метрик мы как разработчики понаписали там столько всего для удобства во первых есть набор функций обычных которые позволяют записать аккаунты уникальный аккаунт какие то велю значению статистику и она вот отсылается куда-то частота разберем и потом впоследствии мы можем прочитать всякие фильтра сортировки и группировки проходим к статистике на графике построить в итоге настроить и прочее как это работает а дальше забыл сказать что мы пишем очень много метрик взрыв события у нас очень примерно мы их точно не читали где-то от 600 по моим миллиардов до 1 триллиона в суток в этом мы хотим хранить их хотя бы там пару лет чтобы строить там там по сезонам старение спрошу нужно понимать тенденции изменения наших метрик и вот как-то склеить это все воедино было довольно большой проблемой но все еще не до конца не решили проблему дымка как это работало годами последние несколько лет у нас есть нашей функции которые пишут эти метрики они пишут о в локальный мой крест для собственного этом уменьшения к эти записи и 1 небольшой промежуток времени локально запущена стать зима собирается что было записано и сливает метрики в два слоя сервера флокс коллекторов который агрегирует статистику с куча-куча наших машин чтобы хотя бы он не умирал слой за ними до мы можем по необходимости писать напрямую logs коллекторы но это очень плохо масштабируемое решение по что но увеличивает нагрузку на коллектор это нужен только если мы не можем какой-то причине поднять на машине вам кэш с демоном либо он упал мы это поняли и пошли напрямую так или иначе logs коллектора сливают эти стеку у меня у тебя эта наша база которая кроме все прочие умеет хранить метрики но не столь важно и мы потом из кода можем собственного нее таким около sq элем бинарному собственно делать выборки да сегодня этим в этом году летом у нас был внутренне хакатоны появилась идея давайте попробуем нас нас есть локинг и хаусе вот заменить этот красную часть схемы на что-то что может хранить метрики нашивкой house просто попробовать слетит не сидит такой процент вот нас была схема которая писала локи через китон хаос и предложение было какой давайте мы возьмем и добавим сюда еще один house куча-куча домиков а который будет принимать именно метрики в том формате как пишут наш код по эдипе опять же курсе нормально превращать их в insert и как логе которая понимает титан х услуги и который он имеет прекрасно доставляет apple хаоса и потом как-то это уметь читать и вот эта схема которая была вот со сном каша и мест от земли лоск коллекторами базы она заменяется на такую схему а то есть у нас есть отправка из кода напишет стас house локальное тот пишет kid in house уже превращенные и типичные метрики в из концерты пачками в этот салат их crack house если мы хотим их прочитать там и собственно читаем уже в обход от хауса он не нужен напрямую через crack house обычными скайриме этот все еще эксперимент я думаю ра тоже об этом расскажет сегодня скорее всего но он очень положительно нам принципе нравится что получается если мы из праведники проблемы которые все еще с этой схемы есть там и возможно не полностью перейдем и такими женщина это надеюсь но вот то что вопрос в плане экономии железо я какой-то экономии не видел нужно меньше серверов не нужны эти локальные статье монолог с коллекторов это не нужно но при этом клик хаусу нужны гораздо более жирный сервера тим для которые нужно текущий объект 1 меньше но не нужно быть драть дороже и мощнее так что такое спорный момент ну такие нагрузки а теперь про до play он не что же время осталось классно как мой диплом php на шкаф основной код ну да у нас разработка ведется в ките мы используем гид лап и дитем city для диплом разработчики ветки вливаются ветки мастер из мастера мои для тестирования вливаем 100 и джинка стрижек видит в продакшен собственно ветка production при диплом берётся текущая ветка продакшена берется предыдущий веткой у них считается div то есть изменение файла созданы удаленным общем состоянии и это изменение записывается в белок движка к pifast который вы не поверите может быстро копировать файлы хоть именно изменения и через яндекс только и умеет реплицировать изменение очень быстро на кучу машин нас есть наш автопарк это используется не не бекас копирование на прямой используется кайся привлекаешь на когда один сервер рассылает изменение ближайшим соседям тебя своим соседям и так далее это позволяет обновлять нам код за там за десятки единиц и секунд на весь в нашем парке и собстна когда изменений отвечает да локально реплики она применяет измените эти патчи на своей локальной файловой системы также делается случаю откат ну так же через понятно а к php который мы тоже довольно много диплом понятное дело у меня тоже есть опыт на завод детей в схеме но так как это бинарник это та пышного сервера ты мы не можем делать ему там div это огромный файл размером сотни мегабайт уже без со страпонами символ агрессивным релизный финальных сотни мегабайт весит так просто его не поэтому вариант немножко другой что мы в белок пишем версия то есть с каждым билдом он комментирует удар случится но зато назад мы делаем плюс 1 это верить записывается в белок также придерживаться на все сервера локальные к pifast и видит что в белок приехала новой версии и то есть тем же самым осипом вытягивают себе свежей версии бинарный коню порно наш мастер сервер аккуратненько размазывай посетить серда вытягивается свежий бинарник и плавника аккуратно перезапускает его на новую версию тут в общем то все довольно просто мне кажется для наших движков которые после тоже являются бинар никами схему очень похоже ведь она буквально похоже слайда вся разница выделенного бинарник у нас запаковывается в дроби носки архивы и они просто при укачивании просто тпк чин стал он ставится на систему почему у нас к php теплицы бинардиком одышки и дпс от эпохи вам так сложилось и к говорится работает не трожь и все хорошо работает вот надеюсь было интересно про все не рассказать осло ссылочки я привел две косы интересные ссылки 2 год назад наш обмен на там клюшкин но бывший уже одним рассказывал как это работа страны администраторов там есть коем все прочее купив as the gossip подробности и про такие наши решения два доклада юра насретдинова pro crack house который летом был в сибири и в которой будет сегодня ссылочке и парочка моих про архитектуру с точки зрения разработки час было больше такое пара железо скорее от эммы на каком страны коды это выглядит исторические сейчас все вопросы спасибо вопросов нет есть микрофон наверное не знаю вот там далеко очень далеко спасибо за доклад было интересно а хотел задать вопрос о языке а почему php и и а и думает ли вы когда-то от него избавляться переходить на что то другое но я в этом рассказывал о корре про разработку мы все было по железа все просто вконтакте уже вспомнил существует все время пишется в 12-ом сюда пишется пишется новый код у нас и у нас его миллионы строк его невозможно переписать от огромный объем кодовой базы мы стараемся как-то разделять выносить какие-то маленькие вещи на другие решения на спад и не склеп там начал активно использоваться голеньких то вы не в отдельных сущностей вот но переписать это невозможно это можно ставить проекта нет на год минимум мне кажется если переписывать ну это нереал поэтому мы делаем кпп нас нет других вариантов все спасибо что параллельно но если мы будем переписывать на другой язык которые не позволяют такой скоростью лисица то мы будем показывать нам фичи быстрее чем приписан star code без вариантов здрасте спасибо за доклад не слышно повыкладывать в open source свои движки знаете какая проблема во первых кому не нужны у честно этим они именно и не поставил что они делались от безысходности у нас такие нагрузки специфические такая работа что оба стороны можем позволить надежную сеть дата-центров наших датацентров а травой стороны у нас они умеют только то что нужно нам это не супер универсальное решение их интегрировать вашу симбу тяжело эбу полки перейдя я ношусь инфрастуктуру ну по копируйте полностью свирепеет омс прописи прокси либо вы не может даже пойти в этот движок без ничего и только куда можно наверное но тратить ресурсы на то что не нужно практически никому скорее всего но наши техники не хотят на это выложили там года три-четыре раза какую-то версию get have not его на какие-то вещи мы собираемся вас там они вещи когда нога который мы сразу делаем у с учетом того что мы хотим и как-то обособить и типа кай в котором я говорил пакетом этот хаос это отдельные вещи мы хотим их положить надеемся в этом году что тоже появится с эти движки но бесполезно спасибо пожалуйста простоте алексей спасибо за доклад меня интересует вопрос касательно хранилища непосредственно данных вот вы получаете написали свое хранилище и система восстановления именно snapshot of города есть получается у вас есть на плот и определенного стрима за определенное какой промежуток времени доводилось ли вам реализовывать восстановление именно по одному объекту сквозь эти снапшоты то есть не читая их все а восстанавливая именно историю от например начала до конца регулярно то есть у вас есть до такой механизм они могли бы вы рассказать какими вы столкнулись проблемами у нас бывает возникает необходимость он допустим типичный пример у нас есть какой-то ключи к тому в кэше на условном естественно украшая его удалили ну мы от лишнего не существует почему писали его студент отелям он должен быть кто я удалил почему удалили когда его удалили какое значение было и у нас есть инструмент который позволяет пробежаться по всему ногу и восстановить всю историю изменения которых учатся все время так и мы стараемся хранить берлоге самого начала существования это мишка вот у нас есть поможет такая то мы можем найти там историк учетом 1008 4 года есть истории вот но обычно хватает кого-то snapshot of the rope поднятие дочитали белок после него читать все диалоги за все время это очень медленно но этот терабайт и спасибо но это регулярная ситуация возникает и допустимости просят вас . этот давно-давно удаленный файл кота уже протух мы можем поднять старые белок и восстановить файлы с берлога здравствуйте а вот с точки зрения хранилище самих файлов вас не как-то реплицируются с какие-то распределенные хранилище используется как обеспечивается именно отказоустойчивость но с точки зрения а сейчас мы переходим как было раньше наши очень интересных которые сейчас мы приходим на движок который являет имеет функционал хранения нескольких копий на разных серверах причем он самый называет самым занимается восстановлением копии при потере одной из копий он и вот опасается ткп централизованный квалифицированный мастер который знает лично находится мы идем в него она отсылает нас ходил вместо другое место при необходимости в файле заливаем в него он сам решает куда отправить где хранить куда перед передавать и так далее ну это распределенный хранилище понимаем файлы почему не опять ну свое какое-то не какой-то готовые танцев и глаз tfsi меню какие-то просечка раньше она будет работать наши ребята могут гарантируешь это будет работать лишь вы не смотрели ни не мы смотрим кино специфические шене где мы понимаем что то точно бы там типа tree house она реально прикольная мы пробовал работает брать решение что как говорят маркетингу это клёвая вещь но хранить там супер важные вещи к польские файлы мы себе позволим никому не готово к этому есть команда хищник которые могут там добавить подержать спасибо большое спасибо за доклад не здесь по sip proxy я правильно понял узнает не обо всех единственно заходит в любом случае всех каждый инстинкт тироксина каждым себя оставить обо всех да он большущий большущий файл конфига ой то есть если есть приоритеты он идет сначала куда-то в ближайшие движки или кабинетом и когда целом запрос там первым параметром идет имя кластера все куда отправить спасибо за доклад надо у меня вопрос на вопрос по хп у вас используется на бэг-энде какое-то свое чисто кастомное решение или гида фреймворке может старенький если да то какие ну там где копуха используется как об их об этом где обычный php у нас есть там никаких уроков мы не используем это всякие кроны фоновой задачи очереди там такой наш в свой код там нет необходимости флора там почти нет никакого общение с пользователем qtv потому и на фоновую задачу который просто перес пользовать кодовую базу общую все понял спасибо он скажите а что делает движок нострадамусу заметили это как раз сколько я помню этот движок который может находить точку мы кабина пространство ближайшие или рекомендации прочее могу ошибаться на воде как так алексей добрый день спасибо за доклад скажите пожалуйста овна вы на ваших движках поддерживать традиционность и если да то как откатываете и так далее у нас не традиционности потому что у нас нет они внешних зависимости не внешних ключей никаких случае отката мы можем просто переиграть наш белок назад или пыли сказать движку перечитаю белок такой позиции пропускает какие-то события необходимостью скажем к the left me the lid мы можем пропустить его берлоге вот а именно традиционность рисуется в за пределами движков ну чтобы масштабироваться приходится так вот тут дробить полностью независимой кусками ставить алексей спасибо за доклад меня вопрос про движки ваши два вопроса у вас от ваших кластеров получается multimaster архитектура и если да то тогда как вы умудряетесь сбалансировать нагрузку то есть у вас какой-то мастер это всегда большой жирный сервер нет по стенам и упираемся именно вскоре запись на мастер до минимального несколько будет низко мастеров у каждой обрабатывать случай данных а как вы обеспечиваете консистентной тогда данных у нас все данные хранятся так что нет гарантии что соседняя строчка будет на том же сервере нет проблем подробить понятного используется та же тему картированием получается правильно везде 6 напомнишь них ничего нету и ещё один вопрос по поводу 100 футов на тот момент когда делается snapshot запись в лог блокируется прямая движка зависит то есть тут как лизу человек разработчик допустим в горном которые погибли не писал технически не знаю наверняка вот то что у нас есть на год он не блокирует смотри на грудь и напишет на диск и про подражая просто маленький минут x на время блокировки горит в рай ты все спасибо а тут уж хищника хотят делают а добрый день спасибо за доклад очень интересно следующий вопрос вы сказали вот про roll out допустим там ну изменение кода так в котором покатывается допустим если они вы катались на один сервер но конкретный сервер то есть они предлагают ко всем пользователям это процент коль пользоваться на этом сервере которую то есть какой есть тот момент эксперимента то есть допустим там но тут либо как выбирается сервера для пользователей его либо как покатывается код и не понял вопрос оси проката station к пружине тесты были что ну допустим если скажем так ошибки допустим там врал ауте вы их находите в стейджинг или допустим это может быть отойти продам у нас есть мастер ветка который уже доступна извне можно попасть на нее но внутри там тестировщики разработчики и стрижен который очень маленьких сумм пользователей сначала выкатывается туда он там настаивается так скажем на какое-то время мы видим что все хорошо все отлично то мы поможем этим и production ну то есть на одном конкретном сервер если по минутам единстве там целая группа ко всем юзерам который приходит на сервер ну да они давали какое-то время да пожалуйста доброе утро время до себя доклад скажите вот получается белок у вас он вечный и периодически подрезается когда мы стараемся хранить вечно но издержки которые пишут очень много и мед врезаются голова да она так получается публикация данных дойдет и в берлоге и photoshop но у да собственно тут тут либо ты быстро поднимаешься либо ты не между там багирова не данных без вариантов у нас почему нас не от веса что у нас их много там их они мне последний версия много snapshot of по необходимости можем вернуться на одну из них оставь что-то это не инкрементальные то пульнуть нет никита я вам целиком apacer то есть ток может был независимого есть переиспользовать удалять и так далее ну тогда немножко не понимая смысла хранить весь белок это нужно вот как был вопрос как можно узнать что там происходило в прошлом можем поиграться немного почти все события любого любой точности как она создавалась менялась удалялась а белок у нас он как секционирование дома по гигабайт врежется что можно будет это его это дело нормально использовать спасибо рост привет спасибо за доклад еще интересно скажи пожалуйста каким образом производится аналитика данных а на вычисление какие-то агрегаты вот на это все на этих мешках сделанные есть на том решение которым использовал основном она уже практически вшита в систему записи но там ничего немножко и сверху а сейчас мы нашли хаусе пытаемся хранить агрегаты как стоит и в нем самом и дочитывать уже чтобы не терять уники всякие то есть я правильно понял что первоначально она все-таки на движках работ оставайтесь этим котом использовать вот уже годами да там она уже как уже заранее кучу раз а корректировалась там уже примерно посчитаны агрегат который есть и запустим мы нас проблема старые схемы чтобы не можем полететь или построить нас уже нету данных она на вас схема хотим их иметь произвольном количестве"
}