{
  "video_id": "HIyXcpbsXTw",
  "channel": "HighLoadChannel",
  "title": "Балансировка нагрузки шардированного PostgreSQL не своими руками / Денис Волков (Yandex Cloud)",
  "views": 700,
  "duration": 2969,
  "published": "2025-01-17T02:28:56-08:00",
  "text": "Всем привет Очень круто что вас так много сегодня здесь собралось Меня зовут Денис я работаю разработчиком в Яндекс облаке и Сегодня я расскажу немного о том как балансировать ваш шардирование между самими шарда нам надо делать так чтобы каждый шардов был равномерно нагружен чтобы у нас не случалось никаких выбросов чтобы у нас шарды отдельные не ушаты по памяти по по цепью по другим каким-то Метрика кому может быть интересен мой доклад мой доклад может быть интересен людям которые уже имеют шардирование но при этом возможно испытывают какие-то проблемы если они ещё не испытывают проблемы ничего страшного Возможно вы скоро начнёте испытывать проблемы если вам просто интересны база данных Если вы думаете над тем как бы вам шардирование пойдём дальше Кто мы откуда мы куда мы идём Я работаю в команде в Яндекс облаки как я уже сказал разработчикам Я занимаюсь базами данных и вот работаю в конкретном подразделении которое занимается разработкой в всяких Open Source штук вокруг баз данных мы это всё используем у себя в Яндекс облаке у нас все наработки как бы выложены на гитхабе Мы работаем с обществом Ну вот собственно тут представлена часть команды нашей у Яндекса есть не только инсталляция внешняя для внешних клиентов у них у нас несколько инсталляций вот для внешних клиентов инсталляция есть есть инсталляция для внутренних клиентов Яндекса у нас разный выбор баз Но большую часть времени я занимаюсь постр у нас там в инсталляции миллионы запросов в секунду данных совершенно разные по размерам конфигурация кластера с разным количеством ресурсов и в основном это всё высокодоходный шардирование yex object storage то есть вот в Яндекс облаке доступен такой сервис yex object Stage у него есть в каждой инсталляции своя инсталляция и этот сервис имеет метаба зу метаба зу о том где Какие где Какие данные лежат то есть там внутри используется он диро набором каким-то небольшим набором короче Python скриптов и крона и всё это в принципе хорошо работает Там уже больше 100 шардов и количество шардов продолжает расти Ну и понятно терабайты данных а а вот сейчас попробую абстрагироваться Я вот рассказал конкретный пример А если рассмотреть задачу В общем случае то есть шардирование какого-то любого сервиса Это задача уже с которой неоднократно многие сервисы сталкивались причём понятно что и в Яндексе об этом уже многократно рассказывалось на конференциях вот в том году на худе был доклад про шардирование Яндекс диска сколько-то лет назад был доклад про шардирование почты Я знаю что у Яндек такси тоже куча Бао они шардирование реализовано как-то своими руками Ну и в общем все кто использует пос пишет чтото какое-то своё решение готового решения никакого нет но при этом очень странно вот есть такая База mql Я думаю все слышали по SQL Ну типа внешне достаточно сильно похоже но вот для ля есть Готовое решение для шардирование оно называется Вис а для поса такого решения не существует вот есть витес дате - Это буквально такая штука которая делает из вашего кластера как бы соединяет их логически много в один кластер очень старое решение много Где используется у них там на сайте чуть ли не вот первым планом написано используется в тубе сколько-то лет назад мои коллеги созванивались с разработчиками чтобы узнать о том как можно было бы нам добавить туда поддержку постгрес но к сожалению мы не нашли гго способа этого сделать потому изменения в вие были бы слишком инв Шла бы очень тяжело Мы в общем отказались от этой идеи и в репозитории утеса даже есть ию о том чтобы добавить поддержку постгрес вот как вы видите оно открыто в двадцатом году и с тех пор ничего не изменилось это очень интересно на самом деле так вот а а сколько ещё команд напишет своё решение вот это вот очень интересный вопрос но при этом Понятно про Яндекс я рассказал А сколько ещё существует сервисов не Яндекса Ну и понятно что с этим мириться было нельзя и за дело взялись ответственные профессионалы мы взяли и написали своё Готовое решение для родирования поса оно называется Stat pog quy вот оно сейчас выложен на гитхабе мы изначально писали код на гитхабе пишем код на гитхабе всё выкладываем на гитхабе в публичной репозитории будем продолжать дальше это делать на предыдущих конференциях Я уже рассказывал о том как работает р а вот в том году на лоде ещё пару лет назад ещё на PG Конфи и мои коллеги ещё на ютубера сска о том как работать куар сейчас этот доклад как бы не про спуа немножечко Но для повествования надо рассказать что это такое Как это работает а очень кратко основные идеи А вот так выглядит типичное приложение да то есть есть какие-то бэнды и они ходят в кластер базы данных каким-то образом Ну то есть как бы всё понятно идея у нас была такая что если между бэнда и кластерами шарда мы поставим некоторую прослойку прокси это прокси будет принимать запросы на них смотреть догадываться о том на какой шарт этот запрос надо переслать переслать и дальше уже возвращать ответ обратно энда Ну собственно в куре такая штука называется роутером то есть роутер буквально роут запросы роутер работает по протоколу по это приложение на Go То есть энды как бы общаются по протоколу пог Даже не догадываются что общаются с каким-то роутером а не с обычным по кластером вот за счёт того что роутер написан на мы получили относительно высокую или приемлемую производительность А одного роутера но понятно что Вот в такой конфигурации один роутер - Это бутылочное горлышко и прекрасная точка отказа Поэтому с этим тоже мириться нельзя и мы спроектировали систему таким образом чтобы роутеров параллельно можно было запускать бесконечно много а роутер у себя в памяти хранит информацию о том где Какие данные лежат но только у себя в памяти Э понятно что для того чтобы меджи вот эти правила между роутерами то есть убеждаться в том что на каждом роуте роутере действительно актуальные правила хранятся мы добавили ещё одну сущность которая называется координатором координатор буквально через него происходит управление вот этими метаданными через него происходит добавление новых роутеров удаление роутеров или вывод их в а всю всю информацию об этом а мы храним в специальной базе qdb а там внутри запущен etcd кластер А теперь немного вернёмся снова к автоматической балансировке я немного рассказал про SPAR вернёмся к балансировке зачем она нужна и Почему без неё никак как я уже сказал Я в основном занимаюсь постгрес сом А и мне предоставлена честь дежурить за кластера постгрес ql в Яндекс облаке и в том числе я дежурю за тот прекрасный сервис Мета база S3 и вот я сейчас рассказываю типичную Ситуацию которая происходила регулярно какое-то время назад Вот в 3:00 ночи я сплю раздаётся телефонный звонок Алло привет Это Мониторинг это разговор записывается таким радостным голосом Ну мне там сообщают что какой-то из шардов перегружен понятно что ты там просыпаешься доходишь нотка пытаешься вспомнить как нажимать там эти кнопки понимаешь что Ага вот те звонили по сервису S3 там вот такой шарт перегружен Ну собственно ты заходишь на этот шарт пытаешься понять что там вообще происходит и понимаю что вот конкретно вот к этим данным чаще всего как-то происходят запросы и именно из этих данных шатуша попуу вручную условно говоря находит Что вот есть какой-то соседний шарт вот эти данные можно было бы перенести туда переносят их но в крайнем случае если что-то не получается можно ограничить клиента чтобы он э перестал слать много запросов условно говоря согласитесь Это довольно глупая ситуация э бедный дежурный правда вот клиентов много данных много шардов много такое происходит регулярно происходило регулярно в какой-то момент дежурный просто начинает скрываться так жить нельзя У меня есть прекрасная история о том как я в новогодние праздники дежурил и приехал к родителям в свой родной город и решил пойти погулять на ближайшую гору Я забрался зимой всё было хорошо и тут мне звонит мониторинг говорит Ты знаешь у тебя там на одном из шардов заканчивается место осталось 92% И у меня там было полчаса чтобы до компютера добежать внизу до машины и всё это время он мне звонил типа 92 93 94 95 94 95 а отсечка когда кластер переходит в режим нли случается там на 97% Ну в общем я в тот раз конечно успел добежать но согласитесь это очень глупая ситуация и достаточно стрессовая и попадать в неё ну никому не рекомендую В общем Понятно что так дальше жить нельзя было и мои коллеги написали автоматическую балансировку вот для вот этого сервиса специально мы написали балансировку Я сейчас в деталях Расскажу позже как она проблем при релизе особо не было То есть и багов не было и проблем не было и в прото она уже давно работает и всё с ней хорошо и замечательно и тут Мы подумали вот у нас есть куар и у нас есть S3 для которого мы написали балансировку что если нам взять и реализовать Точно такую же балансировку в нашем куре Ну собственно говоря мы вот так и сделали и вот сечас я расскажу как мы это сде вали концептуально основные идеи вот у нас есть такая штука которая называется балансер это ещё один компонент То есть это также какое-то отдельное приложение на Go которое перевозит данные небольшими кусочками Вот это самая главная идея перевоза когда тебе нужно перевести какой-то большой диапазон данных наивно делать это следующим образом А ты берёшь блокировку на запись на этом шарте для этого диапазона начинаешь её перевозить на следующий шарт как только она переехала ты снимаешь блокировку и удаляешь эти данные условно говоря понятно что в такой ситуации датам на запись э составляет Ну сколько Ну не знаю 1 2 3 10 минут а можно ли без ЗМ тайма Да можно идея такая берём и наш большой диапазон разбиваем на кучку маленьких диапазон которые перевоз которых отрабатывает это мгновенно и перевозим эти данные вот такими микроскопическими частями со стороны будет казаться что никакого тайма нет понятно что мы также во время перевоза берём блокировку на запись на конкретный небольшой диапазон но в силу того что он небольшой данные данные переносятся молниеносно со стороны Вот как раз достигается эффект что тайма Нет это как идея Ну и понятно что вот у балансера времяработы секунд а есть небольшая очередь задач у него там а один запуск - это один одна задача из очереди и задача Да если задач нет то мы проверяем а не надо ли чего-то перевести А как основные идеи весь свой рассказ Я буду сопровождать скриншотами кода и ссылочка на них можно будет подойти посмотреть Потом как-нибудь на странице моего доклада найти презентацию или подойти ко мне очередь задач Как происходит постановка задачи в очередь ну во-первых нам надо собрать статистику на Шарх что это такое мы написали специальное расширение которое расширение для поса оно называется есть Это буквально другого решения кото называтся В этом зале есть какое-то количество разработчиков или администраторов которые работали со статки кэшем и знают что это такое но Я подозреваю что далеко не все знают что это такое ПГ статки кэш собирает статистику запросов по операциям чтения записи на диск то есть буквально там главное что есть есть представление которая называется статки кэш которая содержит бесконечное количество полей и эти поля очень удобно Очень полезно анализировать Например Например можно найти топ пять самых долго выполняемых запросов Долгих Т транзакций или которые потребляют больше всего цпу вот я нашёл скриншот с какой-то старой конференции где как раз-таки пока показан пример использования вот тут мы что-то селекти из представления ПГ статки и можем найти топ пять самых долго выполняемых по CP Time запросов System CP Time Ну и в общем вот так если играться можно много всего найти интересного попросили так сказать нагрузку на ваш постс так вот как я уже сказал было Вот это ванильная ПГ статки кэш расширение мы сделали его форк В чём разница нашего форка от ванильного апг статки кэша Ну во-первых единственное и главное отличие что мы в эту статистику там бесконечно много полей очень много полей мы в эту статистику добавляем ещё одно поле комментарий комментарии из запроса то есть буквально вот у вас есть запрос Select там что-нибудь и в этом запросе указан ключ kange равно что-нибудь Range ра 123 А в результате У нас вот появляется статистика по конкретным Чам к которым обращался пользователь а то есть выглядит буквально это следующим образом А селекти им из нашего представление представление ПГ Command stats Get stats и вот там видим появилось поле Command Keys и туда попал наш ключ А первый шаг Это был первый шаг постановки задачи в очередь собрать статистику на шарда А следующий шаг - это собственно надо найти самый загруженный шарт делается это тоже на Шарх Мы выполняем какое-то количество запросов чтобы оценить насколько они загружены вот тут вы видите два запроса первый с помощью первого запроса мы можем оценить насколько занят цпу то есть мы селекти из нашего представления и считаем User CP п System C а из второго представления мы считаем Сколько места занято в нашей у нашей базы данных то есть мы оцениваем свободное место таким образом Теперь у нас есть какое-то количество критериев которые мы оценили на каждом рде то есть Это буквально список и нужно понять А какой критерий больше всего страдает никакого Рокет Санса Нет это это какой-то псевдокод на Go Если вы не понимаете ничего страшного если понимаете тут в общем ничего сложного мы бежим по всем Метрика в каждом рде и буквально находим ту которая относительно больше всего загружена то есть ничего особенного нет всё довольно просто обновляем максимум если нашли максимум и возвращаем максимум проверка необходимости перевоза следующий шаг тут тоже всё просто если мы нашли что какой-то критерий перегружен надо его перевести типа логично дальше Мы должны понять вот у нас есть самый загруженный и мы знаем критерий по которому он больше всего загружен мы поняли что нам точно его надо перевести А какие данные надо перевести вот какой ключ больше всего страдает больше всего нагружен и мы это тоже находим с помощью нашего представления Keys буквально мы группи все наши запросы по и там уже можем отсортировать тери по цпу или е по другому критерию и так оценить Ну по нужному критерию и так оценить что вот этот страдает больше всего нагружает шарты Больше всего следующий шаг мы знаем какой шарт нагружен По какому критерию и Какой ключ его нагружает больше всего Вот теперь надо понять А куда намдо э данные пере стратегии пространство ключей это такое Да забыл слово Ну в общем какое-то пространство от минус бесконечности до плюс бесконечности упорядоченное множество Я хотел сказать вот от минус бесконечности до плюс бесконечности то есть на нём есть какой-то порядок Ну и понятно что если мы выбираем какой-то диапазон ключей которые надо перевести у этого диапазона есть соседи бы слева и справа соди они могут быть расположены на других Шарх мы не знаем На каких и вот идея такая что если мы попробуем этот конкретный диапазон положить рядом с соседями чтобы потом например объединить их если у нас Мы мы можем прикинуть а Действительно ли этот диапазон влезает на соседние к соседним диапазонам на другие шарды вот если у нас получается их туда переместить то мы их перемещаем если не получается то у нас есть вторая стратегия мы берём все шарды находим самый наименее нагруженный шарт и перевозим этот диапазон ключей туда теперь что у нас имеется у нас имеется Что именно Мы хотим перевести Какой ключ С какого шарда Мы хотим перевести И куда мы хотим перевести всё это как бы готовая задача для перевоза которую мы берём и складываем в очередь в очередь наших задач так вот как происходит Дальше выполнение задач а да вот ссылочка на код если кому-то интересно пожалуйста заходите Как происходит Дальше выполнение задачи перевоза одна задача - это один диапазон ключей один диапазон разбивается как я уже ранее говорил намного маленьких диапазонов То есть это вот основная идея И каждый такой маленький диапазон большой диапазон маленький диапазон это такая отдельная под задача по перевоз очень важный момент Как именно переводится перевозятся вот эти маленькие диапазоны в посе есть такой механизм for daters это когда ты буквально можешь с одной базы данных с одного кластера баз данных в другой кластер баз данных удалённый находящийся в другой сети на другом компьютере запущен брать Connect напрямую и выполнять запросы там реализованы разные for там есть Oracle фв всё что угодно фв и вот самое популярное - это подв то есть мы буквально берём Connect с одного шарда напрямую в другой шарт и в одной транзакции переносим эти данные Это буквально выполняется вот каким-то каким-то короче запросом из серии вставить в удалённую таблицу что-то из текущей таблицы где что-нибудь если вам интересно Больше деталей Понятно Вот опять ссылочка на код и очень важно Вот тут именно сделал скриншот метода который выполняет этот перевоз вот мы тут видим что создаётся какой-то Foreign Data wrapper и там дальше выполняются ещё какие-то запросы которые нужны для корректной работы Если вам интересно Больше деталей вот можно посмотреть и перейти по ссылочки вот мы что-то написали оно в принципе работает да то есть эта штука протестирована на и3 Спасибо большое сервису и3 за тестирования наших идей мне стало просто интересно как эта балансировка реализована у других и я подумал что можно посмотреть как она реализована в idb и в мо db Давайте тогда начнём с Мон чтобы понять как работает балансировка в мо db кластере надо знать как устроен db это супер чно У меня есть схем схем Как выглядит типичный шардирование сверху между шарда и приложением есть какие-то монгас и справа ещё справа ещё есть какой-то конфиг сервер сервер да вам это ну ничего не напоминает как бы я тут про икур рассказывал там была точно такая же схема только в Ире получается шарды это как бы тоже шарды да то есть это по кластеры монс у Мон штука которая роут запросы у нас это называется роутером штука которая хранит информацию о том где что лежит называется конфиг сервером Ну у нас как бы это координатор по функционалу они там чуть-чуть различаются Но это типа детали это неважно но смысл в том что как бы концепту очень похожие подходы Я честно я не специалист по mang db буду буду с вами честен я простой разработчик который открыл документацию в интернете там нашёл про балансировку что-то и вот Я попытался резюмировать А что там вообще есть и как она работает так вот краткое резюме балансировка - это какой-то фоновый процесс фоновый процесс который запускается при прохождении какого-то порога при отсечки отсечки по цпу памяти АО при этом мо db Вот это супер крутая фича которая Я понятия не имею как реализована но мне очень интересно mong db может выполнять параллельную миграцию данных мы спроектировали нашу балансировку Так что там ну не нельзя выполнять параллельную миграцию чтобы минимизировать датам на запись Ну вот в монге это по-другому сделано и очень интересно конечно но в остальном вот в целом вот так вот если смотреть верхний уровне идеи все те же самые понятно что отличаются детали И в силу того что монго просто более зрелый продукт У пользователя Мон есть больше инструментов влиять на то как будет проходить балансировка то есть там какие-то параметры тут поменять какие-то параметры там менять вот в этом пока Ключевое отличие То есть если кому-то интересно вот там ссылочка была Можно перейти посмотреть почитать самое прикольно что я нал я искал код То есть я пытался разбра в коде мон и случайно увидел что у них есть офигенно о том как работает шардирование но эта документация написана не для пользователей моги а для разработчиков Мон Вот это вообще просто пушка-гонка я такого наверное никогда не видел очень классный гайд Я рекомендую если вам тоже интересно зайти почитать там очень много полезной информации Ну в общем кратко в ску используются те же идеи что и в монге в монге используются те же идеи что и в куре всё понятно теперь перейдём дальше как работает Как устроена балансировка vdb кластера А понятно что для начала также поймём Как устроен vdb кластер я я думаю тут не все знают да И я не знал на самом деле как vdb работает вот я сейчас попробую кратко описать э того что необходимо для моего повествования про wdb концептуально логически можно idb разделить на два слоя то есть внизу это слой данных а сверху это какой-то слой вычисления физический слой на этом слое запущены физические машинки понятно что както хранилище там распределённая отказоустойчивые персистентное физические машинки могут быть расположены в разных дата-центра иметь совершенно разные ресурсы самое важное физических машинок запущены такие штуки которые называются динамические ноды логически для упрощение повествования можно считать что динамическая нода - это такой процесс мы сейчас немного забываем про физические машинки переносимся на уровень динамических нот работаем на уровне динамических нот внутри каждой динамической ноды пущены таблетки таблетка - это такой набор акторов на c+ Plus у которых есть персистентное состояние которое они хранят в распределённой хранилище что очень важно Вот Представьте что у вас есть какая-то таблица Т1 диапазон значение этой таблицы там от ну до 10 хранится в таблетке т11 диапазон значений от до 20 в таблетке т12 Ну и так далее То есть таблетка - это такой слепок данных таблицы партиции а если очень важно что я ещё не сказал на одной физической машине может быть запущена одд три много динамических нот на одной ноде может быть запущена одна-две три много таблеток и это на самом деле очень интересно что происходит когда приходит пользовательский запрос конкретной таблице Мы хотим поселек данные запрос приходит на конкретную таблетку и так как слепок таблице уже загружен у неё в памяти она не идёт в хранилище чтобы достать эти данные она сразу даёт ответ пользователю если в таблетку приходит пишущий запрос то через специальный компонент она каким-то образом эти данные записывает в распределённое хранилище и Вот это ещё очень важно всем понять вот я попробую ещё раз резюмировать то есть в idp работает на физических машинках внутри запущены динамические ноды 1Д три много внутри динамических НОД запущены таблетки 1 2Т много а одна таблетка - это слепок какого-то конкретного диапазона значений таблицы этот слепок хранится только в памяти и ну как бы получается что данные то нет смысла балансировать потому что данные это не узкое место в моменте обработки запроса момент обработки запроса узким местом является Ну что может являться там сеть физическая машина динамическая нода таблетка то есть имеет смысл имеет смысл балансировать именно сами таблетки динамические ноды потому что что может произойти например может так случиться что у нас останется ши на кото автоматически переедут все динамические ноды и тогда эта машина будет страдать и тогда когда мы запускаем новый физический сервер надо чтобы автоматически какое-то количество динамических нот начало пережать на этот сервер тоже самое с таблетками чисто теоретически может случиться такое что на одной динамической ноде с Копится какое-то бесконечное количество таблеток и они будут выжить все ресурсы и компьютеры будут не работать в общем нам надо так сделать чтобы при добавлении новых Но эти таблетки автоматически переезжали на более свободные по ресурсам динамические ноды в документации idb я нашёл офигенную гифку о том как типа визуально работает эта балансировка я н оставлю ссылочку чтобы её посмотреть очень рекомендую очень классная и я нашёл в коде где упоминается вот эта балансировка где есть методы баланс что-нибудь и если вам тоже интересно чить код Вы можете перейти по ссылочки но тут мы можем видеть что что можно балансировать ноды таблетки и ещё можно балансировать каналы но я о них не стал рассказывать но смысл в том что данные балансировать Смысла не имеет заключение я тут рассказывал про балансировку проку если вам стало интересно Больше про у нас открытый процесс разработки был есть и будет Да весь код выложен на гитхабе под лицензией Open Source Global development Group буквально Воруй убивай делай всё что хочешь всё можно ничего не запрещено А я вас призываю Пожалуйста зайдите к нам в репозиторий Попробуйте сделать значит git Clone Попробуйте запустить тестовую инсталляцию с кура командой Make Run если у вас получится запустить балансировку вы вообще красавчики это почёт уважения вам пожалуйста нажмите нам на гитхабе на звёздочку мы будем очень рады вам ничего не стоит а нам приятно голосуйте за мой доклад Спасибо большое за ваше внимание это Денис Волков друзья поднимите ручки Давайте зададим вопросы о смотри я тебе знаешь Вот ребята из Яндекса Они же в офисе не могут пообщаться поэтому на конференция можно да да привет Дени Спасибо большое за доклад интересно Помню раньше тоже был доклад про spqr тоже был интересно это мне кажется я тебя видел уже Да там спикерс вопрос в ЧМ при Уса при выполнении запросов есть определённый план запросов и случается такое что данные по одним типом клиентов они сильно отличаются по так сказать начин то есть он клиент у на 10 каких-то купленных продуктов у другой клиент миллион купленных продуктов и им нужны разные планы запросов чтобы оптимально обрабатываться Есть ли какие-то планы по тому чтобы шардирование э проблема celebrities есть какие-то популярные звёзды которые часто Селект записи в вашей таблиц для них ну на них нужно тратить больше ресурсов чем на другие что с ними делать так получилось так спроектирован что нам ВС равно на каких машинках запущены шарды можно в инсталляции может быть шарды с одним цпу и4 пу одновременно в целом кажется это ответ на той вопрос Если вы хотите выполнять обрабатывать зам Т рожа Яндек Меня зовут Андрей я тоже с Денисом работаю мы вместе с пишем я добавлю сейчас мы балансируя то есть в первую очередь мы балансируя данные чтобы выполнить критерии нагрузки по памяти цпу диску и так далее ВТО обв это размера метаданных метаданные в оперативной памяти роутеры Так спроектированы чтобы все метаданные то есть отдельные кусочки э были маленькими по объёму Для этого нам нужно чтобы у них границы были рядом чтобы их склеивать рядом а вот идея того чтобы балансировать третий третья стратегия по планам она для нас новая тут надо подумать Спасибо классный вопрос Кстати да Спасибо будьте добры Ну при условии что вы не из Яндекса как Яндекс Спасибо большое за доклад У меня вопрос собственно говоря наверное больше по шардирование То есть у вас комплексно получается распределение нагрузки со стороны клиентской части вы роутером маршрути зру ете между разными серверами со стороны данных вы данные переносите от одних нот грубо говоря в другие Вот именно по Шарова у ва какие-то проблемы с рованием то есть когда вы некорректно переносили данные с одного нода на другой считая что у вас нагружены нода то есть например какой-то клиент может быть крупный он начинает запрашивать данные у вас возрастает нагрузка при этом арим понима что надо переть пере наю ноду переноса возрастает нагрузка там так как запросы от клиента идут э следом опять же возникает необходимость переноса на более свободную не нагруженную ноду и у вас так начинают прыгать туда-сюда как Кузнечики данные с одной ноды на другую Были ли подобные проблемы и как вы их решали я понял вопрос я понял проблему Но я честно не могу вспомнить чтобы мы с такой проблемой сталкивались Я думаю что мы её предусмотрели Но вот Нам повезло наверное что мы с ней не столкнулись пока вот передай слово коллеге скажи вот товарищ Стар сержант хочет ответить Давай здравствуйте Я тоже Андрей то есть мы пока с этой проблемой не столкнулись но эта проблема условно названа проблема гранулярность оценки То есть когда у нас действительно начнут оценки статистики нагрузки расходиться с реальностью мы её почувствуем мы к этому готовимся заранее потому что эту проблему чувствовал раньше дежурни когда он в очередь положил не тот чанк на переезд компьютеры ошибаются просто делают это реже пока что наши инсталляции не доросли до того объёма Когда мы это почувствуем Но мы к этому готовы Спасибо за вопрос Спасибо можешь оставаться буде соры Да Сейчас секундочку Спасибо за доклад Я хотел бы задать вопрос касательно части про не совсем по теме но всё же вот я увидел что там баланси таблетки как представление в оперативной памяти а вот а сами данные лежат в каком-то определённом хранилище не возникают ли там проблемы не может ли там такой же проблемы возникнуть что все данные в кучу лежат в этом распределён хранилище на одном из физических хранилищ и когда мы вычитывать таблетку всё туда идёт и производительность падает Слушай вот там вот специалист по сидит вот сейчас я к нему подойду по Ну давай давай вчера Бунин делал хотел да Давай я тоже по хулиган я-то не настоящий специалист поди тут искусствоведы просто сидят в зале да да дадада да Привет Э может быть чтобы не канни бализ доклад обсудим после А ну можно а а то мы сейчас у гоним сво имя Я просто думал что докладчик сможет быстро как-то докладчик сможет конечно но не в этот раз Ну ладно нуно ребята Это честно докладчик не может знать всех вопросов но э ели если показываете добры Теперь вы если скажешь кому задать вопрос Это тоже неплохо Денис привет Меня зовут Алексей Сбер Я хотел бы немножко раскрыть тему которую задали первые два оратора первые два человека коры тебя слени гото я ещ разочек во-первых Мне кажется что таскать самого тяло это не самая лучшая стратегия потому что мы будем постоянно перегружать Да может быть имеет смысл мелкой гранулярность Да и где-то там посередине принимать решение что хватит Да там или давай потащим не на этот шарт а на другой как-то так и тогда система будет работать более мягко Ну мне кажется что этот подход Да тут где-то не хватало ещё повествования о том что у нас был какой-то жирненький ренж который постоянно нагружается но он нагружается именно из-за какого-то конкретного ключа и нам надо бы взять этот ключ вынести в отдельный диапазон и уже таскать его а не все остальные не весь остальной большой диапазон вместе с ним абсолютно абсолютно и второй момент - это как раз про у нас нет никакой уверенности в том что перетащи в какой-либо проблемный ренч а проблемный он не потому что он большой А например потому что там плохие планы запросов Да плохая какая-то селективность Да мы перетаскивай на другой шарт и там эта проблема также повторяется и собственно здесь это уже лишит за гранью того механизма который ты описал Да но это тема очень актуально интересно всё-таки ну как вы её трека ете Как вы с ней боретесь смотри мы задумывали да спасибо за вопрос Мы задумывали СР Как такую простую тупую штуку которая буквально роут запросы что там происходит Дальше на Шарх нас вроде как пока по крайней мере Вот на этом этапе развитие сра предполагаем Вот вот данном этапе мы предполагаем что там запущены кластера Ну вот у нас например и люди которые до этого использу они Как умеют пользоваться если они видят что у них какое-то какие-то запросы начинают тормозить или выполняться неэффективно у них есть в Яндекс специальная страничка диагностика производительно запрос найти и как-то поис следовать А что вообще происходит Вот такая у нас пока идея Здравствуйте спасибо за доклад Вопрос такой Вы не рассматривали варианты какой-то избыточности То есть все данные только в одном шарден упал по какой-то причине это физическая машина с других шардов Нельзя будет копию каких-то Ну там часть данных лежат там в двух шарда и тоже самое ну одна нода этор или там мастер реплика под капот Так кто такой шар в терминах - это поер может состоять из одного дтх сколько угодно хостов там Понятно мастер реплики мы предполагаем вот вот сейчас прямо у себя внутри Пользуясь гаем е у ва сломался во-первых дежурный починит вот он дежурный он починит вовторых Ну он типа код скажем так сервис написан так чтобы компьютеры не ломались даже если они сломались мы минимизируем ущерб от ломания шарда То есть как бы в этом случае не помогает Но если уж совсем всё плохо У нас есть пы ием вост на этом уровне отказоустойчивость реализуется именно отказоустойчивость у шарда мы делаем всё возможное чтобы можно было чтобы шарт не сломался чтобы мы его быстро установили Спасибо сть добры А спасибо большое за доклад Меня зовут Александра банк у меня такой вопрос насчёт архивации удаления данных вот мы хотим почистить данные например мы там хранилище год 3 года держим и нам что-то надо удалять А вот ваша технология она как делает вот это вот сохранение что мы удалили Ну а как вы эти данные удаляете Ну допустим мы по запросу по времени то есть данные пожили там год Ну в смысле у вас уже есть в какой-то пост Сколь кластер Он наверное ещё не шардирование ю туда данные то есть по времени и у нас начались строение чанков и очень большая нагрузка в принципе пошла и вот мы отключали за Ну чисто теоретически если под удалением каких-то данных вы имеете в виду запрос что-нибудь Удали мне Ну ты вы можете послать запрос Через роутер на удаление именно какого-то конкретного диапазона этот запрос дальше выполнится на рде всё что происходит на рде происходит на рде теоретически Это должно работать и кажется то что вам нужно такого механизма как автоудаление по ТТ кажется в ванильном постгрес нет но кажется его несложно реализовать но именно на шарда Ну то есть это не будет какой-то проблемой что там будет ние это всё зависит от того как вы это реализуете но теоретически Если вы это правильно реализуете то проблем не должно быть Спасибо большое Спасибо будьте добры Привет Денис то надо встать чтобы тебя по телевизору пока Денис Привет ещё раз спасибо за доклад Я из теньков У нас тоже очень много развёрнуто Точнее не развёрнуто используется кластеров как раз таки вашем облаке наверно несколько десятков Вот и вопрос собственно такой там на схеме где были изображены роутеры а Меня заинтересовало Как происходит менеджмент сессии То есть коннекто к базам данных потому что я знаю про решение PG bouncer которое как раз этим занимается Вот хотелось бы понять как у вас это реализована связь с ним Ну смотри Спасибо я попробую кратко ответить и приглашаю тебя в кулу вре есть свой пул коннекто до каждого шарда То есть у нас есть два как это условно два Пула пулов первый пул - это входящие коннекты второй пул - это коннекты до каждого шарда то есть мы один раз до шардов взяли и держим их открытыми Ну собственно Вот так это реализовано это такое типа ну я не знаю вы называете это мы это называем есть вот такой режи ра спасибо Так у нас 2 минуты и вот вчера Озон Ребята так сделали они пришли толпой во-первых поддержать спикера во-вторых ответить на вопросы и сейчас тоже самое тоже самое вот Яндекс делает Молодцы зададим сейчас один один-два вопроса и потом можно будет прямо вот толпой здесь 20 минут потусить Будьте добры вы и потом юноша и всё Привет Денис Меня зовут Наташа мтех спасибо за доклад быстрый вопрос Ты рассказывал про балансер А где-то настраивается Как часто балансер Вообще начинает проверки там на CPU eo диск вот э вот штуковина как работает давай так там есть какие-то настройки То есть это приложение и у него есть какой-то конфиг в этом конфиге можно заказать задать какие-то настройки и из того что я сейчас помню очевидна отсечка при которой надо запускать перевоз по каждой метрике и возможно ещё какие-то я честно Так и не вспомню там пока мало что есть мы над этим работаем вот так вот но в целом это всё А да в приходи вот эти двое они Глу целом все эти настройки выглядят Так что очень легко добавить Было бы желание вот у кого-нибудь у каких-нибудь наших клиентов был бы запрос мы бы всегда это сделали Спасибо отлично и горка а потом будем дарить матрёшку Да спасибо Максим конференций вопрос по Древний Рим Если да то в ЧМ мем Да просто вот такой нейминг типа шутеечка я те отвечать на такие вопросы надо так да давай че на самом деле ещё можно отвечать типа голоса в голове на шептали Ну в общем как-то так у меня просьба все кто задавали вопросы поднимите пожалуйста ручки помашите Т сначала надо матрёшку онтика подарить тому кто задал самое важное для коммьюнити важное для коммьюнити вот там был вопрос про Мон db удаление Мне он очень понравился за тем что он да про мон db и за тем что он был относительно лёгкий мне было приятно Отвечать на него а Александра Да да отлично Спасибо И ещё от докладчик сидит я не помню К сожалению вопроса Я помню что тоже было приятно отвечать а первый который как планы запросов Вот это короче отличный был наброс мы не думали ещё об этом да Да отлично тебе тоже памятные призы от конференции и друзья через 20 минут будет теток на экране МТС будет рассказывать что надо делать а что покупать про вендорс решения А сейчас здесь сформируется Броуновское движение из индексов пожалуйста впились в кулуары с удовольствием задавайте все все все все все вопросы получайте все ответы L"
}