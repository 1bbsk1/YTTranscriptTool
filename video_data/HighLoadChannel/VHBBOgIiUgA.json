{
  "video_id": "VHBBOgIiUgA",
  "channel": "HighLoadChannel",
  "title": "Использование непрерывного нагрузочного тестирования для оценки ёмкости и ресурсов / М.Куприянов",
  "views": 1041,
  "duration": 2899,
  "published": "2021-10-04T02:47:02-07:00",
  "text": "да и сегодня я как раз хочу рассказать об одной методик которая позволяет делать это планирование не болезненным более точным и по ночам спать спокойнее и не беспокоиться от того что сервис у вас неожиданно развалиться от того что ему чего-то мне хватило сначала посмотрим на какой-то типичный веб-сервис ну вот очень какой-то простой понятный я буду часто обращаться к и ком модели ну просто потому что нами как просили яндекс маркет очень близко могут быть некоторые отличие у вас обычно веб-сервис состоит из какого-то количества front end of которые встречают пользователей и ну прям какой-то от гораздо большего количества сложных брендов которые общаются между собой ходят соседей ходят сами в себя делают чего хотят ну в общем есть какая-то такая вот конструкция но они же не все одинаковые на то есть точки зрения ресурсов с точки зрения ресурсов некоторые бренды фрагменты они чуть более использую слово жирные плохое слово в общем чуть более крупные чем остальные и мы вот здесь видим что там есть frontend один какой-то очень у нас распухший есть backend какой-то совсем огромный и пара бэг-энда чуть поменьше но этого тоже не достаточно чтобы понять что вообще и вот как они выглядят сколько не потребляет мы живем в мире высоких нагрузок и поэтому мы говорим всегда о горизонтальном масштабировании сервисов у каждого сервиса есть большое количество и инстансов вот теперь мы добавили третье измерение стало что-то похожее на правду вот есть у нас такой замечательный сервис он обслуживает запросы клиентов он что-то там отвечает но наносит им какую-то радость но в остальное время он проедает наши деньги тем что там крутится на цыпа употребляет память потребляет диск ну насчет с этим делать мы не хотим просто так деньги тратить но в тоже время мы хотим что пользователи были счастливы от того чтобы это понять сколько нужно реально ресурсов для того чтобы такой сервис джилл нужно построить кто-то прогноз давайте подумаем что нужно для того чтобы такой прогноз построить что нужно первое нужно поговорить с бизнесом я нарисовал здесь никого человечка с названием аналитик на самом деле это какой-то абстрактный представители бизнеса он вам или она или они вам расскажут о том что значит у бизнеса есть следующие планы на следующий год у бизнеса всегда есть планы по захвату мира они четкие и понятные количественные это там третий квартал следующего года плюс 35 процентов к аудитории четвертый квартал плюс то и так далее эти люди все это знают и все с вами сам с удовольствием расскажу что еще важно календарь пиковых нагрузок эта история скорее про маркетинг скорее про маркетологов потому что маркетологи они прекрасные люди но они любят устраивать различные акции и на своих акциях они подтягивают пользователей на наш сервис волнами прим ну то есть вы никогда у вас была там 100 посетителей в час у вас на волне пришло 500 и поэтому а календарь пиковых нагрузок это как раз календарь маркетинговых акций скорее всего будет и вы тоже должны его знать что потребуется отойти мира границу масштабирования о чем это все мы живем к сожалению в сильно неидеальном мире и любой горизонтально масштабируемый сервис масштабируется не бесконечна всегда есть какие-то архитектурные границы и ограничения но банальный пример там какая-то архитектурная ошибка там в табличке с пользователями хранится не знаю время посещения и тут вам приходит бизнес и говорит о мы хотим пользователь там в 10 в двадцать раз увеличить следующем году ну и все у вас есть беда ее нужно как-то решать конечно решать эту проблему будет там не стар я не диво опция ее будут решать разработчики владельцы соответствующие компоненты но вы как devops как и среде как админ не важно как это назвать всегда должны с ними общаться и должны стран следовать ему текущие планы бизнеса на следующий какой-то период и разработчика которая узнали неожиданно том что пользователь стал вам есть раз больше они скажут господи боже мой нам нужно запланировать какую-то доработку ну хорошо предположим эта проблема как-то решено последнее что нам нужно для прогноза это текущая емкость сервиса и вот это как раз тема текущего доклады час мы подумаем и я расскажу о том как ее можно оценить как ее можно посчитать и померить если все вот это у нас есть то наш прогноз можно сказать готов мы точно знаем когда у нас будет такая аудитория мы точно знаем сколько нам надо ресурсов мы знаем какой бюджет заложить но надо сказать что если мы говорим 5 же про highload это очень большие деньги с точки зрения ресурсов и планирование часто прямо годовое а раз она годовое ту деньги там тоже будет немало и ошибиться ну тоже не хочется давайте для того чтобы построить наш прогноз делаем несколько допущений допущение номер 1 наша архитектура огонь но вот я говорил уже границах горизонтального масштабирования предположим что у нас вообще все замечательные здорово написана и планы бизнеса таковы что они не порушат нашу идиллию и все сервисы великолепно горизонтально от масштабируются там в течение того периода на который мы смотрим предположение номер два поведение пользователей не хаотично ну это скорее все чаще всего правда вряд ли у кого-то из вас есть какой то сайт или сервис на котором пользователь просто рандомно прыгают по страничкам выполняет всякую странную историю скорее всего у них есть какой-то паттерн там логин поиск покупка что-то фидбэк что-то еще чаще всего они работают по одному и тому же потому их много у нас нагрузка и так далее ну то есть в целом мы можем предположить что нагрузка пользователя она в какой-то степени определено детерминировано и третье предположение о нем уже неоднократно говорили сегодня и вчера это то самое правило парето когда 20 процентов компонентов в нашем случае потребляют 80 процентов ресурсов чаще всего это так это могут быть пять процентов которые потребляют 95 процентов ресурсов могут быть 40 которые потребляют 60 не суть важно во всех сложных системах нужно на оцень оцень научиться оценивать что то такое что будет крупным измеримым и потом это все аппроксимировать на всю систему целиком и это дает хорошие результаты замечательно у нас есть допущения давайте как-то найдем нашу емкость сервисов да я буду вам показывать иногда котиков для того чтобы разбавить эту довольно сложную тему первый пункт первый вариант это классическое тестировании перед релизом что это прямо будет наверное единственное что сегодняшний день будет из классики мы собираем какие-то логе или журнал из production system мы делаем из них то что в яндексе называется патроны но я даже не знаю кто застрял мире какую-то синтезированную нагрузку и направляемые на наша нагрузочный стенд разработчики притаскивают туда релиз мы проводим нагрузочное тестирование мы получаем какие-то показатели там рпс разгадке или еще что-то там и отдаем обратно в разработку разработка понимает что это их устраивает или нет и либо повторяет цикл тестирования либо отправляет сервис production цикл дальше повторяется журналы сервисов нагрузочное тестирование тогда здорово это все прекрасно работает и есть три отличных зеленых плюсов это быстро это безопасно это показательно при правильной организации вот дьяволом в детали в третьем пункте давайте посмотрим на минусы сложно правильно генерировать нагрузку почему потому что для того чтобы правильно спроектировать синтетическую нагрузку вам нужно взять ее в какой-то правильный момент времени и в правильном месте у вас пользователя в пятницу вечером это не те же самый пользователь что в понедельник утром пользователи в праздник совсем не такие как пользователи ночью и тот момент времени когда вы будете брать сэмплы образцы для вашей нагрузки он критически важен и тут возникает масса ситуаций конфликты с точки зрения тестирования нужно либо брать актуальная бра либо брать какие-то фиксированные в какой то момент времени в общем здесь на самом деле все сложно ну и последний очевидный минус нужны ресурсы под стенды если ваша система большая то скорее всего вас стендов будет ни один возможно даже несколько стендов на каждый компонент потому что там же разные ветки тестируются вот это все в общем какие-то денежки какие-то ресурсы понадобится решение хорошая но кажется что мы можем что то получишь придумать а идея у нас такая давайте тестировать все непосредственно в продакшн действительно зачем нам testing для таких вещей страшно но на самом деле это реализуемо и давайте подумаем посмотрим точнее что для этого нужно пункт 1 управление трафиком вот это прямо аксиома если вы не можете различными очень гибкими путями влиять на то как ваш трафик попадает на ваши бренды вы не можете ничего тестировать продакшен то есть у вас должна быть какая-то инфраструктура балансировки какая-та инфраструктура распределения нагрузки это должно быть второе село очень важная вещь термин известным на всякий случай еще раз про говорю это некая численная метрика которая определяет ну скажем тут даже не максимальный некоторые характеристики вашего сервиса когда он считается работоспособным то есть условно если ваш сервис отвечает за одну секунду и вас это устраивает при этом генерирует 0 ошибок это тоже вполне себе соло нормально но вот и село получается всегда от владельцев компонентов всегда абсолютно вы не можете сказать как там devops админ не важно что ваш у какого-то компонента из и лотом одна секунда об этом должны сказать владельцы этого компонента и вы должны это где-то зафиксировать обязательно это прям сразу из раздела советов на будущее всегда сик фиксируйте такие договоренности потому что через год все поменяется все забудется а они станут третий пункт сенсорно dance in some очевидная вещь чем больше у вас есть каких-то не знаю коллекторов данных обработчиков логов каких-то хуков системы которые погружаются снимают информацию сохраняют это все в каких-нибудь базах данных тем лучше тем вам проще будет если что разбираться ну и следующий понятный очевидный пункт это работающие alert и сенсоры прекрасные но если при срабатывании вас ничто не уведомит если у вас сервис разваливается а вы об этом не знаете или знаете но через пять минут они через одну минуту или там не через пятнадцать секунд то это плохо то тогда он нельзя тестироваться в продакшн и последний компонент который нужен надежный выключатель большая красная кнопка об этом сегодня тоже даже много уже говорилось печатал неважно какая там что это будет это должно быть что-то что очень быстро и очень надежно выключает всю вашу систему нагрузочное тестирование сразу подчистую может пойти не так все что угодно вы не знаете имейте запасной вариант как будем грузить есть несколько вариантов начнем с казалось бы очевидно нагрузка с синтетикой собственно это почти то же самое нагрузочное тестирование в тестинге которые мы делали то есть точно также анализируются журналы запросов из потом продакшен среды формируется какая-то синтетическая нагрузка и эта нагрузка летит в продакшн понятно что она как-то маркируется что-то там тестовое так далее но мы погружаем тот же самый production system это классно работает и есть три огромных плюсов первое нам вообще не нужны фактически дополнительные ресурсы ну то есть нам нужно штука которая будет нас направлять трафик это мизер и все любая нагрузка мы можем хоть один рпс отправить хоть тысячу рпс хоть миллион неважно мы можем все что угодно мы боги здесь и самое важное но правда не в контексте этого доклада но на самом деле очень полезнейшая вещь то что нагрузка такого рода она является комплексной мы оцениваем всю систему целиком каждый ее самый слабенький элемент каждый винтик которая может отвалиться под нагрузкой мы его найдем увидим покраснеет помним про сенсоры какие минусы минусы 2 но они у вы огромны 1 минус это мусор журналах и это звук на самом деле еще страшнее чем здесь написано что такое мусор журнала все ваши тестовые транзакции пролетят production систему и осядут не только влогов да они потом подтянутся в базы данных могут сразу потянутся зависит от там все устроено какие-то аналитические системы и вот этот мусор он может с одной стороны в самом таком не сложном случае испортить жизнь аналитику который случайно я где-то учтет но так и с другой стороны привести каким-то более серьезным проблемам как-то например перегрузка база данных за каким-то шумом вы же не один раз будете тестирование проводить это плохо с этим нужно разбираться не всегда это на самом деле даже возможно потому что ваших большой и стройной системе сервисов могут оказаться какие то счас компоненты куда вы можете даже особого доступа не иметь второй большой минус это проблема на интеграционных стыках трафик те стинга улетит в интеграцию вы можете случайно списать с клиента деньги нынешние книга такого не случится но это тоже возможно это может быть какая-то более плохая история когда онлайн-мир состыкован с оффлайн мирам и в результате ваших тестовых каких-то манипуляций с продакшеном запустится какая-то реальная операция оффлайн мире и ладно если это будет курьер которого отправили не потому адресу это не так страшно может быть гораздо хуже давайте подумаем что можно сделать для того чтобы продолжать тестироваться в продакшен но избавиться от минусов этого решения мы придумали вот что давайте будем грузить пользовательским трафиком но правда это же здорово то есть мы просто берем пользовательский трафик вот у нас вот такое по площадке нарисован у нас какие-то три площадки у нас как-то разбалансирован между не и трафик и мы такие разберем на первую площадку сбрасываем не 34 процента 50 сенсоры зашевелились графики подросли а если не 50 если 80 уже все покраснела мы приближаемся к соло все стоп отлично мы зафиксировали какой-то там аудитории какой-то там на рейс процентах неважно емкость сервиса посчитано владельцы сервисом и они сообщили все здорово минимум дополнительных ресурсов комплексном все то же самое как предыдущем пункте но правда пропал один очень важный пункт пункт который назывался любая нагрузка мы не можем из ничего сформировать пользовательскую нагрузку если у нас есть люди если они что-то делают то мы можем этим трафиком управлять если их нет но извините ну и структура трафика меняется да это ровно то же самое о чем я уже говорил то есть трафик вечера пятницы не такой же как трафик утра в понедельник и так далее то есть когда вы будете тестировать вам нужно очень четко и очень правильно понять и поймать какой-то момент надо с этим тоже что-то поделать что дело с дефицитом пользовательской нагрузки но есть разные на самом деле варианты как один из а давайте тестировать не всю там площадку или локацию целиком нагружать давайте возьмем один единственный instance и на него будем наливать пользователь трафик у нас же highload у нас же трафика это много но и один instance но вот мы точно найдем трафика чтобы один из нас про грузить казалось бы логично второе что делать изменением структуры трафика во времени ответ такое давайте тестировать 24 на 7 в конце концов мы строим автоматику мы как бы об обложили всеми сенсорами артами и так далее мы можем все это выключить очень быстро что нам мешает не прерываться в нашем тестирований хорошо давайте посмотрим как в это могло бы выглядеть ну и на самом деле выглядит вот есть такие какой то м запросов пользователя ну какое-то число есть вот та самая площадка какая одна с балансиром на ней инная ну точнее z данном случае инстансов и под ним распределяется как-то равномерно трафик появляется новая сущность назовем ее сплиттер что делает этот маленький сплиттер он от входящего потока трафика отщипываем нрп с н запросов в секунду и отправляет их на скажем так на 1 instance который мы считаем нагружаем и а все остальное отправляет дальше балансировку ну вот она там пуска дальше вертится собственно говоря логика на вот это м мы можем растить до тех пор пока не поймаем какой-то ник зафиксируем некоего максимальное значение а дальше мы из него считаем уже емкость ком всего компонента получим простой формуле не такой сложный так как там было про типе давайте посмотрим что дальше будет с этими формулами делать вот у нас есть текущая нагрузка мы посчитали емкости и получили утилизацию компонентов ну очевидно да и знаю утилизацию компонента и зная емкость сервиса мы получаем поднос потребности в ресурсах все задача схлопнулась прогноз потребности в ресурсах у нас есть можем считать планировать и так далее посмотрим как это реализовано ну по крайне мере у нас в маркете я думаю каждый может реализовать по-своему то мне сказать что сложная конструкция сейчас они все подробно расскажу и посоветую как ее хорошо применять про его конструкция выглядит примерно так вот эта красная часть сплиттера которым я говорил появляется еще один дополнительный компонент тег называется fall back not of all bad маршрутизатор он ведет себя довольно просто вот вы отправляете те самые нрп с на instance номер один не нагружая mainstage но если вдруг он накрывается чем-то черным мы просто что берем ее передавать fall back маршрутизатор перенаправляет весь наш трафик на обычную балансировку общую и ничего страшного не происходит но подумаешь мы потеряли один instance не страшно мы не потеряли пользовательский трафик вы вообще ничего не потеряли но хорошо в то те же моменты когда система работает исправно не станут живой трафик льются мы снимаем различные метрики как со сплиттера так сын солнца заметьте появляется такая штука как эталонной инстанции для чего они нужны и кстати вот кто знает для чего нужны талон иисуса в данном случае правильно сравнивать один instance под нагрузкой может показывать всякое абсолютно и для того чтобы понять насколько он корректные чисел отображают нам нужны ну какие-то нормальные обычные инстанции в продакшена который льется обычный продакшен трафик вот эти самые талоны выбирайте любые два очень хорошая практика кстати выбирать еще дополнительно несколько инстансов на соседних площадка для того чтобы полностью понимать что происходит итак все эти instances мы снимаем с них метрики что входит в метрике до в общем вы все это знаете входит туда рпс количество запросов секунду на сервис входит туда квантили таймингов как сервис отвечает это может быть в разрезе по ручкам извините по путям по каким-то это может быть общее суммарное тайминги какие именно квантили в этом выберите все зависит от вас это на самом деле все описывается вот тем самым село о котором я говорил ранее и конечно же ошибки точно также в различных разрезах это могут быть общие пятисотке это могут быть какие-то специфичные смотрите сами метрики вот они попали в базу и от туда их читает некая некая сущность которая назовем управляющий модуль это мозг все операции тот самый мозг у которого есть красная кнопка что все это прекратить и вообще он все делает мозг на самом деле это тривиален абсолютном и делает он одну очень простую вещь он принимает решение уменьшать или увеличивать значение n и влияет собственно горит сплиттера там повысить вот это вот и или уменьши вот это вот и все и сохраняет свое текущее движение какую-то базу метрик сплиттер видите здесь вот буковка m подросла и встала больше этих самых весов мы переходим на следующей итерации трафика на instance номер один становится больше продолжается все тоже самое и дальше по циклу и так бесконечно логика работы этого управляющего контура очень просто первое собрали достаточный набор метрик что такое достаточный набор мы позже поговорим второе если instance под нагрузкой не показывает деградацию то мы повышаем если показывает понижаем а если у нас есть сомнения мы не делаем ничего это все что делает управляющий контур пара живых примеров пример номер один вот смотрите здесь зелененьким обозначен instance под нагрузкой графита график количества запросов в секунду на какой-то там рандомный наш сервис и пары эталонов здесь видно что очень хорошо кстати заметно что с утра и до вечера трафик заметно тяжелее вы видите что инстанции не вытягивает до 600 рпс который он вытягивает а все остальное время у забегая вперед немножко скажу что 600 руб с от искусственное ограничение для данного instance а потому что мы не на ралли мы на мне нет задачи кого-то обогнать или замерить сколько мы там вообще вытянем умерев наша задача спланировать ресурсы и понять емкость сервиса а для этого достаточно знать что компонента а вытягиваешь суд рпс на одном инстансе и нас это устраивает и владельцев это устраивать все мы не будем насиловать сверх поэтому поставили какой-то мальчик и для сравнения видны талон образцы трафик на них ведется меньше тут вот есть некоторые всплески это какие-то рестарты компонентов продакшен в общем выглядит примерно так и в результате мы получаем еще более интересный график собственно утилизация текущей емкости в процентах это уже вся компонента целиком и здесь мы что видим во первых мы видим то что за зеленый график что по совокупности всех локации мы за сто процентов утилизации не выбираемся никогда но если мы отключаем одну локацию в яндексе это дата центр поэтому без одного дата-центром и в целом тоже но есть пара моментов деградации на которой стоило обратить внимание и на них обратили внимание их починили что в таком методе можно улучшить много что на самом деле но мы сейчас поговорим для начала о такой проблеме как леденцы сенситив сервисы для некоторых сервисов ухудшения таймингов ответа вообще недопустимо а мы все равно их ухудшаем потому что мы не начать не можем понять насколько нагружен сервис не доведя его до предел а этот предел он все равно хуже по таймингу но если у нас есть какой-то вот такой critical сервис и если мы можем для него организовать решить для него проблему со сбором вот этих мусора флагов то мы вполне себе можем завести отдельную площадку для вот такого компонента и поместить туда мы так назвали это теневой instance по сути это полная копия продакшен инстанса сервиса на которую льется продакшен трафик но ответы и и уйдут в никуда то есть есть площадка вот она работает вот запросы пользователей вот у вас из z инстансов под балансиров все тот же сплиттер врезается в поток запросов это необязательно должна быть весь поток запросов вам принципе достаточно кого-то небольшого кусочка этого потока потому что вас наверняка инстансов много и вам хватит этого трафика и копируют определенное количество запросов в секунду трафика на тот самый теневой instance как мы помним это обычный иисус ответы которого игнорируется считается все точно так же но можно делать хулиганить с этим теневым инстру сам как угодно вы можете делать более жестокие село можете его как канарейку использовать можете вообще сделать тестовую площадку или этот трафик использовать для того чтобы где-то там и не знаю логе себе собиратель для следующего последующего тестирования во фланец что угодно она работает но эта методика не работает в нескольких случаях она замечательная новость игры к сожалению есть несколько проблем проблем номер 1 она работает только для тех сервисов которых критично время ответа если мы говорим про какую-то batch систему ну вот все что все что завтра основана на обработке на на трубу большой обработки большого количества запросов она плохо таким образом оценивается почему на самом деле там все понятно потому что вам таком случае нужно метрики собирать довольно странным образом я чуть позже скажу в общем для классических веб-приложений работает хорошо даже если там базы данных даже если там транзакции все замечательно ну вот вторая проблема это таки да тайминги на очень критичны это решает вопрос с теневым из он сам с площадкой и самое наверное больной момент который иногда стреляет это когда трафика на локации мало условно вам нужно иметь на площадке трафика столько чтобы загрузить контрольные инстансы ну хотя бы каким-то нормальным уровнем нагрузки обычным + 1 тестируемый если у вас трафика столько на сервис есть все замечательно если его не хватает но мы в конце концов проход говорим вы надеетесь у нас хватает какие советы по реализации такой методике и по использованию себя можно дать пункт первый и самый важный сервис воевал объектов во первых заводите по несколько разных и салона каждый компонент лучше всего это работает когда вы ограничиваете его таймингами по разным как понте нам там по 99 по 95 и может быть там по 80 ошибки безусловно да несколько разных и соло позволяют вам отловить также изменения структуры потока трафика то есть иногда могут случаться какие-то условно ложные срабатывания которое происходит из за того что у вас сменилось модель пользования вашим сайтом в каком-то какой-то степени это тоже пар очень полезно знать в общем несколько разных и село хорошо соло для копейске тестирования должно быть жестким чем больше вы будете это самое для пользователя отклоняться тем лучше можете позволить себе только 10 процентов хорошо можете 50 еще лучше то есть все во что это выливается это в то насколько вы пессимистично оцениваете емкость вашего сервиса и как много там дополнительных ресурсов вам придется там заказать или заложить на следующий там какой-то период 10 процентов на практике иди плохо вполне себе хорошо работает и третье регулярно уточняйте это что значит это означает что вам всегда необходимо быть в контакте с разработки с владельцами сервиса с теми кто его пишет этот бренд или frontend бывает ситуации когда где-то в кулуарах владельцы одного компонента договорились с владельцами другого компонента о том что они будут вот на эту вот вот на вот этот вот путь на этот запрос отвечать там медленнее или быстрее вы об этом не узнали и зло на самом деле съехал и вы показываете уже не те результаты которые должны но дело в том что к сожалению реальность такова что владельцев сервисов не очень беспокоит прогноз по тому как много у них ресурсов следующем году будто что это будет ищем году а им бизнес fitch пилить на ну так бывает нормаль в общем нужно дружить с разработкой понимать что там происходит и просить ребят чтобы они уведомляли вас если с какие-то изменения в каких-то в таких вот элементов из алого соло происходят теперь вернемся к управляющему контуру там была в логике его работы 4 пункта поговорим про 2 из них пункт 1 собираем достаточный набор метрик как мы его будем собирать у каждого сервиса у него есть тайм-аут ответов чаще всего заложен какой-то максимально если мы говорим о классических онлайн-сервисов без потокового видео без прочего это условно секунды или минуты ну всякое бывает вряд ли это больше вы знаете для каждого компонента какой у неё тайм-аут максимально собирать результаты собирать метрики чаще чем вот один раз там за все этот период понятно что нужны там квантили и так далее но чаще их собирает бессмысленно потому что вы захватите ответы с при периода предыдущего и много предыдущий период у вас мог бы могу больше совершенно другое значение вот-вот нрп с ну то есть они нерелевантные не надо в общем правило очень простое 1 1 минуту нормально обычно для обычного веб-сервиса вряд ли у вас в тайм-аут и там сильно больше второй момент таких-то и мало интервалов нужно несколько но очевидно то что вы можете поймать какой-то всплеск там крон какой-то запустился еще что-то произошло в общем на одной из минут у вас показала повышенное что-то ошибки там my time тайминги неважно интервала должно быть несколько и вот тот самый шум его можно отсекать медианой очень удобно мы пользуемся рекомендую и второй момент управляющим контуре которыми я хотел ещё поговорить он наверное самый такой сложной с точки зрения тех всяких различных корнер кейс из которые возникают если есть сомнения не делаем ничего так вот как давно мы должны сомневаться когда какие моменты пункт 1 но мало трафика я уже об этом начинал говорить но и продолжу у вас может не оказаться достаточного количества трафика чтобы нагрузить ваш единство с под нагрузкой или эталонные и в этом случае вы ничего сделать не можете если вы считаете что ваш инстинкт рпс но у вас есть только 90 вы не можете принять решение не об уменьшении не оба по величине слоев вы должны ждать когда у вас появится трафика больше второе деградация эталонных инстансов к сожалению все мы несовершенны и в продакшен порой попадают какие-то баги которые приводят к ухудшению емкости сервиса в целом и вот для того чтобы понимать что увеличение трафика на тестируя в инстанс не привело на самом деле к деградации вам и нужны эталонные нс и если вы видите на них что все плохо что они тоже пробивают от с л о а который надо настроенного свои ит инстансе тестируемом это та же самая ситуация вы сомневаетесь вы ничего не делаете вы ждете разрешение проблем и скорее всего она решится там через какое-то скоро и время потому что там мониторинге сенсоры и так далее разработчики наверняка знает уже чинит бегут ночь я не знаю как у вас у всех сервисы разная а вот эти поднимите руки те у кого ночью на сервис нагрузка такая же как и днем вот прямо примерно вот по сути действительно есть класс у маркета ночью ходят в основном какие-то странные личности с искусственным интеллектом поэтому смотреть их результат нагрузочного тестирования по ночам но вообще не интересно там ерунда но вот если вам повезло и у вас ночью такой же поток нагрузки то вам придется доводить методику совсем до ума чтобы у вас автоматическое отключение все катберт срабатывал чтобы вас ночью ночи могут быть какие-то другие соло на самом деле то здесь можно играть вот этими числами в общем ночь не всегда то время которое вам нам нужно смотреть но если вам нужно ну я вас поздравляю есть такая классная штука это уже тоже из серия рецептов называется оно безопасно рпс что такое безопасно рпс это некоторое значение которое стоит спросить у тех разработчиков которые делали вашу компоненту они вам скажут что наш instance нашего ваши компоненты вытягивает ну предположим 100 запросов в секунду там да мы тестировали мы проверяли 100 запросов мы вытянем всегда в общем ставьте 50 и пусть он не под этим где-то под мы условно подпишемся и безопасно рпс как раз нужен для того когда вы очень долго не знаете что делать то есть у вас есть сомнения мало трафика у вас есть какая-то деградация вы не понимаете что происходит ну вот как бы вы же не это же управляющий контур он робот он не может пойти там куда-то и спросите что ребят вообще творится в общем если есть проблемы с принятием решения уходите на безопасный псы так и живёте до того момента когда у вас снова появится возможность что-то предпринять как долго ждать это индивидуально для каждого сервиса некоторые ну критично часы ну то есть если что то идет не так то в течение часа есть какие-то сомнения что происходит сервисом нужно быстренько все сворачивать и выключать откладывать на безопасный ps это один из вариантов супер брейкера целом он кстати очень хорошо работает и им даже лучше пользоваться чем просто на выключать там нагрузку или пири-пири балансировать ее некоторым вообще нормально если там в течение недели они не получают какого-то фидбека от контура аналитики и максимальный fps этот как раз то значение выше которого для инстанса прыгать не надо но мы знаем что наше единство снять 600р ps ну и замечательно нам не надо знать что он там в какие-то моменты там доходит до 700 мы очень за него рады но 600 нам вполне достаточно больная вещь журналы они же логе это уже прямо из практики наталкивались 50 процентов случаев на нагруженном из трассе логе съедаются место ну то есть вот как только вы ставите что вы считаете что ваши сервисы готова под большую нагрузку скорее всего не упадут из-за дисков ну вот почти всегда то есть как только нагрузка в два в три раза превышает то что идет обычно все это место быстро кончается где-то опять же если сенсоры успели с работой tomoya лифты какие-то прилетели замечательно здорово если нет но об мы потеряли инст с благом и один из нас под нагрузку и мчу страшного нет но очень хороший момент когда мы считаем сколько вытягивает ваш сервис нужно понимать а сколько это будет занимать потом на дисках флагов ну то есть все что вы намерили для тестируемого инст вас она будет правильным и для всех остальных если он там в 10 раз больше места потребляет значит то же самое сделают остальные товарищи уже в реальной жизни и второе а логе писать нужно буфере zero ваном если взять тот же джинс там есть опции записи по умолчанию он пишет x-sync ровно то есть если вы включите буферизацию вы избавитесь от многих потенциальных проблем особенно это важно если вы живете в облаке если у вас есть какие то странно агрессивные соседи на в данной физической машине и они могут так нагрузить вот вывод что запись в логе резко замрет но подробнее об этом лучше конечно рассказывать каких-то отдельных историях но имейте ввиду логе писать лучше либо синхронно там либо у ферри зова на неважно еще лучше в сеть вот на предыдущем выступлении рассказывали что сети в дата-центрах сейчас большие это правда они сейчас там 25 гигабит 40100 но практика показывает что обычные компоненты которые живут в облаке не загружают сети не загружают ее очень сильно ну только если выделить книги не рите какой-то там видео контент или пир toupper контент это возможно но обычная транзакция обычные там такие соединения они потребляют мало если вы можете писать логе все эти если у вас есть коллектор который это все соберет что-то уложит пользуйтесь сеть это недооцененный в наше время ресурсы на самом деле ее нужно загружать и просить типовое солей на сеть 99 процентов это часто правда для очень многих инсталляций и смотреть на тайминге выше 99 но такое нет вам может повезти и вас может быть совершенно шикарные там сетевики оборудование так далее у вас не будет никаких там провалов и все в этом духе но ваши клиенты то могут находиться где-то дальше ну то есть имейте в виду что все что выше 99 перцентиль а в таймингах это всегда очень сомнительно на это на смотреть с определенной долей настороженности и если в конце концов вы наткнулись на то что в вашем сервисе появилась деградация по емкость я показал там на графике два пика было что с этим делать конечно нужно идти в команды разработки надо это сдавать если вы настроите автоматику по нему замечательно но как вы видели этот процесс довольно шумный и не всегда робот чё кто может определить а вы не захотите совершенно точно перегружать команду разработки каким-то шумом иначе они вас слушать перестанут поэтому когда вы приходитесь настоящий деградации они сначала открывают глаза таким квадратные большие горят смысле у нас все тесты проходят мы вот релизный цикл мы ничего такого не добавляли у нас все хорошо откуда какая деградация а дело в том что очень мало кто на самом деле тестирует на емкость ну хотя бы вот на потребление памяти или там циpкa на один запрос ну вот кто у кого в релизных циклах есть такие то есть такое тестирования вот 1234 так ну 5 ну короче на весь зал 10 человек по моему не наберется сами понимаете здесь очень помогает timeline вот здесь современные методики паттерны и работы типа печатал биллинга и так далее они приносят некоторую боль и в том что когда если раньше вы выкатывали релиз и вы были уверены ага вот здесь стало все плохо вот здесь выкатывался релиз значит наверно релиз в этом во всем виноват то сейчас у вас выкатывают релиз а потом запускаю через день новую фичу или еще круче а бы тестирование и вы видите деградацию но и и вообще ничем связать поэтому у вас должен быть timeline там должны быть релизы нам должны быть включения всех вич на нем должно быть включение экспериментов все этот нож должно быть на общем какую-то корпоративном тайма и это прямо новой север то есть без него разобраться в причинах произошедшего будет исключительно сложно придется понять там всех задействованных в общем большой привлечения людей большого количества заведите второй момент очень важный эта трассировка пологом ну тему совершенно отвлеченная да то есть про неё многие говорили если у вас клиент пришел на ваш сайт у него уже должен быть ему присвоен его запросу какой-то маркер по которому можно отследить все путешествие по всем брендам вы на букинге тридцать третьем должны влогах видеть что этот запрос такого-то клиента и он пришел с 32 бэг-энда вот тогда-то эти системы есть они много где описаны рассказан документировано исключительно полезная вещь очень рекомендую в использовании ну собственно на этом у меня из советов все большое спасибо всем точных прогнозов задавайте вопросы кстати здравствуйте спасибо большое за доклад у меня возник вопрос просто не очень знаю насколько часто выкатывайтесь пруд но насколько это выгодно раскатать новые версии вести тесты и откатиться назад чем держать танк справа like окружением с типично земельные базы там например но именно отдельный стенд и тестировать там это вопрос нам раз и сразу же тут даже докинул capacity это же еще про изменить про изменения параметров сервера то есть у вас есть какой-то запретишь на сервер и у вас выделено там полтора giga оперативки и какую-то цифру какой-то сваях в я выставлял и вы понимаете то что вы например хотите понять если вы увеличите какие-то параметры там виртуальной машины для еще чего то можете ли вы это менять на продакшене делать или вы такое но на самом деле надо я начну со второго вопроса ответ да мы можем и вот этим как раз тот самый отдельные тестируем иисус и хорош то что можно на нем менять параметры можно делать абсолютно такими же как у остального production а можно на них повлиять и посмотреть как поменяется емкость и это фактически бесплатно то есть если у вас в компоненте но например там 100 эстонцев поднять чтобы поднять и циpкa например у всех вам придется где-то этот цикл взять это много а у одного скорее всего резервы найдутся это очень удобно и возвращаясь к первому вопросу который говорил так вопрос был о том насколько выгодно иметь площадку до отдельно иногда это очень дорого исключительно дело в том что у вас могут быть какие-то сад пассаж модели сервис или просто смежники и вам придется либо это все промакивать либо договариваться скорее вопрос насколько выгодно это выкатывать вопрос на прот а потом откатываться обратно было ли у вас такое и может быть проще и дешевле замок ать все дело в том что вот как раз нагрузочное тестирование именно в части определения емкости она хорошо работает в пруд на проводе и в тестинге очень не всегда возможно на самом деле определить почему потому что мы берем какие-то вчерашние условно журналы даст продакшен уют проводим нагрузочное тестирование они показывают что все хорошо но вчера было ну например нерабочий день вы выкладываете в рот у вас во вторник деградация то есть это не показатель на и хороший вариант если тестировать именно в тестинге извините за тавтологию это вот тот кейс когда я говорила о теневой площадке прямо разворачивайте ваш стенд пестики и гоните на него production трафик вот текущей вот тогда это хорошо работает и когда вы показывали нам слайды о том что вы вводите часть нагрузки на загружаемый стенд и сравниваете с эталонными но эталонные ли это стенды если вы у них снижаете нагрузку вот тут надо понимать границу то есть если вы понижаете нагрузку на эти стенды несущественно надо определяет для каждого компонента то они до сих пор эталоны и но если у вас уровень нагрузки на них падает до какого-то пола совершенно то есть там 12 рпс здесь так далее то вы не можете уже и считать эталонами для того чтобы такого не было можно смотреть на соседнюю площадку там балансировка ничего не происходит то есть если трафика вообще много то снижение там на пару десятков или сотен даже рпс в целом она ничего не скажет если же это существенное изменение то вторая площадка там тогда все хорошо спасибо бросайте спасибо за доклад у вас был слайд где вы предлагаете использовать теневой instance ответа которого игнорируется вот там есть момент что если вас запроса не иду патентные прилетают то это может защитить данные на проди вот в этот момент как-то решали то есть когда умный сплиттер или как но смотрите как бы если сервисные дым патенте не хочется поднять зеркальную площадку то мы возвращаемся ко всей этой боли к поднятию целиком стенда с боками и вот это все конечно это лучше всего работает на каких-то конечных и дам патентах багандов если же нет сплиттер вы можете сделать отфильтровать чай за продота будь не показательно лучше всего тогда подымать действительно стенд смог ими будут все да спасибо кого микрофон махнуть рукой вот добрый день собственно такого вопроса если у нас реально нагруженное приложение и ну например у нас есть под ним база кафка что угодно и так получается что если мы туда заводим бесконечное количество трафика для приложения это все равно недостаточно но чтобы загрузить базу и то есть получается так что предложение то мы нагрузили мы выгрузили из него все что возможно там сделали свой н и даже больше это не показатель потому что мы упремся в базу потому что там типа придет пользователями на общее количество то условно говоря на 11 мы можем 500 у нас в общем пользователь 10000 когда к нам не придет 15000 мы умрем ну да это правильная мысль спасибо за вопрос здесь в виде вот это решение наверное эту проблему вряд ли сможет решить необходимо наверно синтезировать вам трафик и действительно прогонять делать погрузку на быть либо тестовый стенд либо в продакшен о синтезированный трафик именно в сторону базы и и потом вычищать ну к сожалению если ваш и станций является узким горлышком и больше чем он на самом деле а кто вам мешает 2 instance подключить или 3 проблема то ведь здесь нет под словом тестируем и из нас может быть кластер заводите кластер и погрузите базу"
}