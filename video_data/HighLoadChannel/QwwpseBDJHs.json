{
  "video_id": "QwwpseBDJHs",
  "channel": "HighLoadChannel",
  "title": "Отказоустойчивая обработка 10M OAuth токенов на Tarantool / В.Перепелица, И.Латкин (Mail.ru)",
  "views": 2797,
  "duration": 2839,
  "published": "2017-04-22T14:48:00-07:00",
  "text": "всем привет меня зовут он сандерсон вообще обычная владимир перепелица но как разработчик я обычно подписывают man сандерс это мой псевдоним я на данный момент архитектор облако mail.ru но сегодня мы расскажем про наш некоторый побочный продукт не облачные га меня зовут егор латкин я дает его работу над этим проектом очень долгое время потом перешел в облака ну как бы до сих пор теперь работаю над облак собственно вам уже сегодня рассказали что тарантул супер оптимальный производительные у него есть классное прекрасное дисковое хранилище винил вам рассказали как можно работать джейсон документами и вообще какой он классный но что еще что обычно не рассматривается относительно тарантула это возможность писать код внутри него обычно базу данных рассматривать просто как хранились вот она может хранить так это может хранить и такие бенчмарка youth но отличительная возможность тарантула это непосредственно писать код внутри и работать с этими данными довольно эффективно очень эффективно поэтому мы как раз и расскажем о том как мы внутри тарантула строили нашу систему о чем мы будем рассказывать мы рассмотрим нашу систему на протяжении и развития сначала посмотрим какая она была изначально 3 года назад потом мы посмотрим какие у него появились проблемы собственно как мы их решали как мэри использовали мастер мастер репликацию в tarantul и как мы делали свою реализацию ровд как мы объединяли и вот шарден гам ну и собственно посмотрим на все вместе а то есть что получилось так ну давайте я расскажу примерно о том о чем вообще занимались о чем проект все вы наверное сталкивались почты mail.ru и как вы знаете в почту могу можно зайти не только мэлоун ящиком и каким-либо другим ящиком от других почтовых сервисов там от google от яндекса и так далее чем занимается проект когда человек логиниться другим ящиком он но он хочет чтобы собирали чтобы мы могли собирал почту с других ящиков взносы вазочка с которой он зашел и отображал это все средства mail.ru так вот mail ходит за почтой фактически с помощью а у стойки nov то есть основная задача wales хранение allows токенов это как разборка почта вот от сюда мы приходим собственно чем занимается проекта проект занимается хранением и refresh ему а у стоки нам еще какие применения есть у тех же токенов во первых как я сказал сборка почты вход внешней почты и еще есть такая штука хранения адресных книг то есть адресные книги с тех ящик из которых вы заходите также хранятся у нас и обрабатывается каким-то специальным образом кроме того существует несколько других проектов которым также нужна авторизация через другой сервис котором вы зашли и также необходимо выдавать и tools talkin' для для работы с ней для работы с приложением все наверное знают что такое что из себя представляет лаос talkin' быстро напомню чаще всего из структур там из трех полей иногда 4 по сути это акциз talkin' с помощью которого можете осуществлять действия там получить информацию пользователей скачать список его друзей и так далее есть refresh токен с помощью которой вы можете получать новые акции стойки на причем сколько угодно и есть некоторые отметка времени expires то есть либо момент времени когда токен zx париться либо через сколько он за x париться что значит заек спорится значит что если вы придете сакс истоки нам который уже zx парень но у вас не будет доступ к ресурсу так вот поговорим немножко о том что из себя представляет проект теперь как он был реализован где-то в 2013 году очень простая реализация мы представим что у нас есть некоторые фронты которые но все что делают они просто кладу токен и в нас читают токены и есть отдельная сущность как называется рефрешер и и задача рефрешер собственно ходить заново макс истоки ну примерно в тот момент когда он zx париться структура базы тоже довольно просто у нас есть мастер ли реплика есть лайв реплика и лишним очевидной репликация вот вертикальная черта представляет собой такое условное разделение дата-центров то есть у нас в одном дата центре есть мастер со своим фронтам и со своим refresh у нас есть другом дата-центре slave со своим фронтам и рефрешер тоже но который ходит мастер по причину которые мы чуть позже тоже объясним какие вообще возникают сложности и основная основная суть и идея проекта собственно основная проблема здесь именно вот в этом самом часе который живет один то есть если вообще посмотреть на проект ну что подумаешь развита холодной масштабы 10 миллионов записей которые нужно решить в течении часа то есть если поделить там одно на другое посмотреть то мы получаем где-то рпс порядка там 3000 проблемы начинаются в тот момент когда что-то перестает рефрешер например какой нибудь manting базы или там она упала или упала машинному всякое случается вот проблемы начинаются такие если наш сервис по какой-либо причине то есть наша мастер база не работает 15 минут мы получаем 25-процентный а у той что есть просто четверть наших данных она не валидно она не от refresh ноге нельзя пользоваться 30 минут половина данных час все у нас нет ни одного действующего токена и после этого нам нужно очень-очень быстро вот предположим база лежала час все мы ее подняли и нам нужно весь все 10 миллионов очень быстро обновить это уже не 3000 rp с поэтому как бы сервис получился довольно-таки холодным с другой стороны где-то спустя два года после запуска на тот момент когда он запускался он выполнял свою расчетную нагрузку работал хорошо нормально но прошло два года в него добавили разной логике дополнительных и индексов стали вторичную логику добавлять общему тарантула кончился процессор это было неожиданно мы как-то не особо на это смотрели ну тарантул быстрый но тем ни менее любой быстрый ресурс можно израсходовать до предела в общем мы столкнулись с такой проблемой нам помогли обмен и да это был первый раз когда что-то закидывал железом мы поставили самый вот топовый прос который мы могли туда поставить это дало нам порядка полугода вот по темпам роста по всему но за это время нам нужно было как-то решить проблему для решения проблемы нам попался на глаза новый тарантул той система была написана на старом tarantul на старом tarantul и 15 который за пределами mail.ru практически не встречается в новом тарантула 16 было обнаружено мастер мастер репликация и если ты слышишь мастер мастер репликация первое что приходит в голову о а давайте поставим в 3 дата центра три копии то есть поставим между ними реплики и все у нас будет прекрасно все будет работать можно пожалуйста подай не падает то есть вот сделаем вот так три мастера 3 дата центра 3 рефрешера каждый работает со своим мастером можем уронить один можем уронить 2 вроде бы все будет работать но какие здесь потенциальные проблемы вообще вопрос даже в зал кто-нибудь видит какие-то потенциальные проблемы которые могут возникнуть вот в такой схеме когда у нас есть три дата-центра в каждом них нас реплика еще refresh работает с каждой репликой что эти плохого с точки зрения токенов и здесь в общем основная проблема которая здесь появляется это то что мы берем и в три раза увеличиваем количество запросов на wales провайдер то есть мы refresh практически одни и те же токены и причем столько раз сколько у нас реплик это как бы совсем не дело и какие здесь могут быть решения очевидное решение нам нужен какой-то консенсус между самими но там то есть они должны каким-то образом решить кто из них будет данный момент как бы главным лидером существует несколько concerts алгоритмов первый из них это access довольно сложная штука ну нам не удалось нормально разобраться в том как сделать что-то полноценной и простое на ней в итоге мы остановились на рафте рафта очень простой консенсус алгоритм в котором собственно происходит выбор лидера мы можем работать с этим с этим лидерам до тех пор пока не знаю не произойдет перевыборы этого самого лидера то есть при разрыве соединений либо по каким другим обстоятельствам давайте посмотрим как мы это сделали а вот здесь начинается самое простое то есть казалось бы обычно у вас база данных либо обладает каким-то свойствам либо не обладает 2 с базе данных либо есть ровд или показ либо его нет здесь же у нас случае тарантула изначально из коробки на данный момент нет мира авто не баксов но мы берем готовый модуль который есть в поставке тарантула битбокс он позволяет нам соединять ноды между собой соединяем ноды full меж то есть каждая но до коннектится к каждой нодди а дальше а дальше в общем то все просто мы поверх этих connection of реализуем выбор лидера тот собственно выбор лидера который реализован в равки после этого у нас каждая нота начинает обладать свойством она либо лидер либо фолловер ну либо она не видит ни лидера не фолловер а если вы думаете что реализовать ровд сложно вот пример кода на самом деле круто очень простой но это кот налоги кто-то не видела по сути все что мы делаем мы действительно делаем запросы на другие другие удаленные сервера на другие реплики тарантула а тут первую строчку там пул кол подсчитываем количество голосов которым мы получили с но да и если мы если у нас случился кворум за нас проголосовали мы становимся лидером начинаем как бы вот это хард бит то есть начинаем оповещать но том что мы живы если мы проиграли выборы мы как бы стартуем выбор и заново ну через некоторый промежуток времени выбор опять старта ну ты мы сможем опять проголосовать или быть выбранными ну собственно после того как мы реализовали кворум после того как у нас есть лидер мы можем наши рефрешер и которые вот изначально refresh от направить во все ноды то есть не только в соседнюю а вообще во всем но при этом сказать им работать только с лидером что мы получаем мы получаем вот тот трафик который у нас был утроенный так как раздает его 1 но да то есть он на каждый рефрешер уйдет где-то в одну треть то есть поэтому мы получим тот самый рейд запросов которые мы получали в случае одного мастера но только здесь мы можем спокойно потерять любой из мастеров случаться перевыборы рф решили переключиться на другую но как и в любой распределенной системе этом с горлумом шел с чем-то здесь тоже бывают проблемы возникают проблемы очевидна проблем сплита если у нас допустим вышел дата-центр не то что вышила потеряла связь связанность между это центрами то ну во-первых нужен механизм который заставляет систем продолжает жить и плюс необходим механизм который должен восстановить целостность системы ну как раз где эту проблемном решает раунд всего алгоритма что здесь происходит допустим вот у нас вышел do the line дата-центр получается но до которая стояла там она остается покинут она не видит других нот и мы ее в таком случае называем abandon то есть на самом деле catia авто такого нет что надо именно несет какой-то отдельный смысл когда она покинута но что происходит с остальным кластером остальной кластер видит то что много потерялась происходит перевыборы лидером становится как новый но тут скажем верхние и они продолжают работать потому что между ними по-прежнему сохранился консенсус то есть больше половины not увидят друг друга основной вопрос вот что происходит с тем рефрешером который ушел из дата центра ну с дата-центром тоже именно дата-центр ушел в именно потерялась связанность между ними то есть рефрешер все еще может ходить все может нормальный refresh токены самостоятельно что вот получается что у нас refresh и цитокины там там в связано в связанности и у нас есть отдельный рефрешер который как подключен к been done но я и не понятно если смысл вообще refresh токены и вот на этот вопрос ответит нам владимир да это хороший вопрос мы в процессе реализации системы задались им и первая мысль была нет ну конечно же у нас же консенсус тамблером все если скоро мы потеряли значит не refresh потом пришел пришла такая мысль а собственно почему здесь я немножко отвлекусь и и расскажу как работает мастер мастер вот если мы посмотрим на реализацию тарантула на реализацию его мастер мастера предположим у нас в определённый момент времени существуют две ноты и одна и другая мастер у нас есть переменная ключ x значение которого один предположим в условно одно и то же время то есть на самом деле и то время пока репликация не дошла до этой ноты мы одновременно изменяем этот ключ на два разных значения с одной ноты мы в одной ноги мы ставим 2 в другой ноги мы ставим 3 что происходит далее далее происходит следующее ноды обмениваются своими реплика цион ими лагами то есть по сути они меняются значениями ну да вот с картинку вы уже видели в общем с точки зрения консистентной sti это в общем какой-то ужас да простят меня разработчики тарантула что я рассказал как устроен мастер мастер как он работает но тем не менее это тот чем можно работать более того если посмотреть на наши данные из вспомнить что это то есть если нам нужна строгая к высокая консистентной да это не подходит нужно работать по другому но вспомним наш токен wales talkin' состоит из двух вещей риттер fresh talking который живет ну условно бесконечность по сравнению с access токи нам и акция stocking который живет один час но при этом у нас наш рефрешер несет вот функцию refresh которая из refresh токена всегда может получить любое количество access token of при этом все они будут действовать момента выдачи тот самый час поэтому даже когда мы по сплелись под рассмотрим теперь эту схему у нас есть две ноты сначала у них было все нормально работали с лидером отрыв эшли получили 1 access token он там реплицироваться это так со стойким есть увсех случился сплит у нас фолловер стала бэндом но дай то есть у него нет кворума он не видит ни лидера не других фолловеров при этом мы разрешаем нашему рефрешер у refresh и токены которые живут на а bombing море то есть если там не будет сети там и все то есть да ну конечно же работать не будет но если это простой сплит в сеть порвалась ничего страшного то есть эта часть будет работать автономно после того как сплит закончится но до присоединиться то есть случаться перевыборы либо она присоединится к существующему кластеру они обменяются своими данными с но при этом и 3 токен и 2 токен они одинаково рабочие то есть нам все равно то есть это токен из одного дата центра или токен из другого это токены одинаковые собственно после того как сплит закончится кластер воссоединиться у нас произойдет очередная процедура refresh а только на одной ноги и а нас реплицируется то есть на какое-то время наш кластер расходятся и каждый риф решит по-своему но после того как он соединяется мы приходим опять к нормальному как нормальным консистентными данным это дает нам следующие обычно то есть когда делается ковар умная доступность она реализм то есть для работы глостера нужно н пополам + 1 случае трёх нот это собственно 2 в этом случае конечно же запросов максимум наружу будет идти вот ровно столько сколько нужно но в нашем случае мы смогли пожертвовать вот мы говорили про ту проблему большого рейта запросов на короткое время то есть на какое-то время там сплита downtime и так далее мы можем себе позволить то есть нам достаточно просто чтобы жила хотя бы одна но да мы будем ее рф решить мы будем в нее складывать данные то есть при этом в пределе если это вот предельный сплит все посвятила но у всех есть сеть то есть мы и дату получим тот самый тройной рейд запросов но это не страшно поскольку это кратковременное мы не предполагаем постоянно работать вот в плите обычная система находятся в курами связанности и вообще все ноды работают до общем осталось у нас еще одна проблема которой мы говорили самого начала мы уперлись в полку по циpкa очевидное решение как решается эта проблема мы берем и шар дерущимся давайте простой вопрос зал кто знает что такое шарден кто применял shearling что второй вопрос кто писал свой sharding отлично ну давайте для тех кто не поднял руки быстро напомним у нас есть представим что у нас есть два шарда но это база данных каждый из них реплицируется у нас есть некоторая функция которая на вход приходит какой-то ключ по этому ключу мы можем определить как он шарди у нас лежат данный например если мы кодируем себя по емейлу то части milf у нас хранится в одно широко часть другом соответственно мы всегда можем знать где хранятся наши данные существуют в принципе два подхода к реализации шарлин га первый шар закон клиентский и вов можешь рассказать нам просто в общем все просто либо вы берете вот эту функцию пишите выбирайте как хотите самый простейший которые приводятся примеры to her sey 32 года там ключа чуть посложнее этом глава сумбур в общем разные функции консистентных хэширования они возвращают номер шарда вы можете эту функцию поместить на свой клиент у этого подхода есть плюс и плюс здесь несомненно у вас база данных вообще ничего не знает про sharding это очень просто вы подняли базу база стандартно работает а у вас где-то там сбоку sharding но у этого есть и минусы и минус довольно серьезный во-первых у вас клиенты довольно толстые вам нужно вот если вы хотите новый клиент сделать вам нужно в него принести эту логику шарден га но это не самая страшная проблема самая страшная проблема заключается в том когда у вас одни клиенты работают по одной схеме а другие клиенты по другой вывод схему решили изменить шарлин га элос один пошел туда другой пошел туда при этом сама база ничего не знает о том что вы по-разному шар витязь мы выбрали другой способ мы выбрали sharding внутри базы данных в этом случае код базы данных становится сложнее но мы можем использовать все такие же наши простые клиенты любой клиент который умеет подключаться к этой базе идет в любую ноту на любой ноги выполняется функция она вычисляется какой но дай нужно связаться и какой ноги нужно передать управление и собственно делает это клиенты проще база сложнее но в данном случае база данных полностью отвечает за свои данные ну и как те кто знают sharding вы наверняка знаете самое сложное в шар ринге это не sharding а и sharding и вот когда база отвечает за свои данные ришар ним делать намного проще чем когда у вас есть какое-то количество клиентов которые вы например еще и не можете сразу обновить ну и собственно как же этот sharding применяется к нашей схеме да посмотрим как мы это сделали у себя но опять же вот эти шестиугольники тарантулы мы просто возьмем нашу старый старую тройку тройка виктора хотинская тройку тройку нот и ну вот эта у нас ровд кластер назовем его шар дом номер один поставим руку точно такой же назовем его шар 2 и возьмем еще и соединим все ноты друг с друга что нам это дает во-первых как мы уже говорили у нас есть ровд в котором внутри тройки серверов мы знаем кто у нас лидер камаз фолловеры вообще вот то состояние кластер благодаря новым связям и прекрасно знаем состоянии чужого шер да то есть мы отлично знаю кто является лидером во втором что те кто является фолловеров и так далее то есть в общем случае если к нам хотя если пришли к нам на на 1 шард мы всегда знаем куда перенаправить пользователя если он на самом деле требует не первый жертв давайте посмотрим простые примеры допустим пользователь запрашивает какой-то ключ который лежит на первом шарди он приходит от левую базу левая базу прекрасно левый sharp прекрасно знает кто и кто не является лидером она идет лидера получает ключ или пишет ключ и возвращают уже пользователю очень простая ситуация допустим пользу приходит в ту же ноту ему нужен ключ второй ключ который лежит на втором жерди опять то же самое первое первый шаг знает кто является лидером во втором что где он идёт в эту ноту получая данные либо пишет и возвращают обратно клиенту очень простая схема но есть некоторые сложности на самом деле с ней вы наверняка уже догадались собственно основная проблема здесь игорь какая проблема не много ли слишком за и у нас всего два шарда по 3 ноты у нас уже 30 то есть вот такой схеме у нас получается каждую ночь у нас каждый это 6 на 530 соединения мы ставим ещё 1 шард получаем уже семьдесят два соединения в нашем кластере как-то многовато вот формула как эту проблему можно решить решаемые тоже с помощью тарантула как ни странно просто возьмем и поставим еще парочку тарантулов только назовем их уже не шар domini базами а просто прокси и вот как раз таки вот эти прокси и будут заниматься шарнирами они будут вычислять ключ они будут выяснять кто является лидером автомобильном шарди а сами ровд кластер они останутся как бы замкнутая в себе и будто работать только внутри внутри самого жар да таким образом когда клиент приходит напроксен теперь клиент уходит только на прокси мы выясняем какой же артур хочет и если ему нужен нужен лидер он идет на лидер если мы не важно кто он идет на любую из ноду из этого шарда все должно просто и сложность в итоге вот опять формулу получается констан но линейно относительно числа узлов то есть если у нас будет скажем трежер да по три узла в каждом мы получим чуть меньше соединений в разы собственно схема с прокси она рассчитана именно на рост когда шар до становится больше двух случае двух сортов мы получаем одинаковую схему там и там и там количество соединений одинаково но с ростом шар дав 248 16 так далее мы заметно экономим на соединениях ну собственно вот мы прошли весь этот скажем так пусть то есть мы начали с мастер мастер а потом мы реализовали ровд потом прикрутили к нему sharding потом прокси в общем переписали все это все это составляет вот для нас такой просто кирпичик кластер этот кластер если обозначить вот так то наша итоговая схема будет выглядеть довольно просто да очень простая схема есть так подумать у нас остаются наши фронты которые все что делает это кладу токен или забирают токи нас есть reflect a system рефрешер и которые собственно refreshed токены забирают sand ocean сущность берут refresh таки надут волос провайдер кладут новые акции стойки но помните мы еще говорили то что у нас есть некоторая вторичная логика из-за которых у нас собственно улетела на сто процентов циpкa давайте вот этот проблема практически внесем на другое другой этот другой кластер одной из таких проблем является адресная книга адресная книга он здесь он показан то что это фактически один к одному если у вас есть токен пользователя ему соответствует как это адресная книга этого пользователь и данных там количеству столько же сколько и токенов то есть нам по сути нужно точно такой же кластер что опять него период сто процентов циpкa на одной машине applied реплицироваться и так далее ну и мы ставим еще один одни пачку рефрешером для refresh адресных книг то есть refresh адресных книг все-таки немножко другая задача она более редкая ну надеюсь понятно чего нужно ну кстати есть кто-то забыл вот примерно так выглядел вот маленький кусочек кластера соответственно нас есть один такой кусочек для токенов и еще один для адресной книге есть еще один топ и которым мы хотели раскрыть это у нас вот наш очередь она зачем она хитрые довольно таки зачем мы как бы городъ или меня слышно по моему вроде лучше зачем нагородили вообще свою очередь можно было взять что то стандартная здесь все заключается в том вот в той модели refresh а токенов которую мы делали у нас токен после того как он получен живет один час при этом когда подходит и во время истечения его нужно refresh то есть это необычная очередь это дедлайн очередь токен должен быть отрыв решен до определенного времени при этом можно конечно и флешек просто по этому самому времени но давайте посмотрим с другой стороны предположим случился какой-то удач кратковременный или нет но у нас есть некоторый объем токена в которой уже проект грн и если мы будем их риф решать кто у нас проект появится еще какое-то количество токена пока мы будем рф решете пропиариться еще то есть в принципе нет мы конечно все догоним но лучше было бы сначала порыв решить те которые вот-вот умрут вот там грубо говоря у них осталось 60 секунд а потом на оставшиеся ресурсы к их есть refresh и те которые уже все равно по x появились и соответственно в последнюю очередь то есть мы рефрешером более дальний горизонт в пять минут для того чтобы реализовать такую логику на чем-то сторонним придется попотеть в случае тарантула это реализуется очень просто вы можете посмотреть исходники разных очередей которые есть для tarantula ну в нашем случае рассмотрим простенькую схемку то есть у нас есть to pull the pool в котором лежат самое тарантул его и данные у него есть какой-то идентификатора идеи по которым у нас есть прайма реке и вот для того чтобы сделать очередь которая нам нужна мы просто добавляем два поля статус и время статус обозначает состояние этого токена в этой очереди время это вот тот самый xtr time или еще какой-то time мы строим по ним яндекс а дальше у нас есть ну возьмем основные две функции сочи ready to put it x задача пута просто принести и положить данные то есть нам дают какой-то пилот путь сам выставляет статус и время и кладет их нас появляется новый табу у нас приходит тык-тык смотрит на тот самый индекс создает iterator начинает у него смотреть смотрит не взятые таски таски рейде и смотрит на время пришло ли время уже их брать или они заек спаренные ли еще что то если task-ов нет по какой-либо причине то есть он встает в ожидании я не знаю говорили на предыдущих докладах или нет ну тарантула есть помимо просто встроенного лола у него есть еще примитивы синхронизации между фэдерами каналы то есть любой файбер может создать канал и сказать я на этом канале жду любой другой fly where который что-то меняет в базе ли еще что то делает может этот канал разбудить передать его сообщение то есть поэтому если нам нужна функция которая чего-то ждёт она может ждать освобождение тоска прихода времени или еще чего то она создает канал особым образом его маркируется помещает куда-то и дальше ждет на нем если к нам принесут токен который нужно вот срочно обновить допустим put он сделает на тифа и в этот канал одновременно если какой-то токен случайно за релизиться то есть его кто-то взял тайком на flash предположим другой сценарий тег берет токена refresh или просто бред task тронулась другая особенность которой я не знаю в какой базе это еще есть можно отслеживать обрывы соединений клиентов то есть мы знаем во первых в какое соединение какую задачу мы выдали мы запоминаем это вот некий совершенств то есть мы ассоциируем что вот с этой сессии были связаны какие-то задачи предположим процесс refresh опадает просто рвется сеть мы не знаем 3 флешбэк он не сможет он его положить обратно или нет у нас срабатывает disconnect disconnect находят сессии все задачи и автоматически их высвобождает то есть при этом высвобожденная задача через тот же канал вот может передать эту задачу другому буду он ее быстро возьмет и обработает ну в общем эта схема то есть под ней вы можете предполагать может быть лежит очень много кода на самом деле нет мы его покажем даже на слайдах данного времени остается не очень много в двух словах постараюсь объяснить фактически put все что дело тут если вы знакомы эти налоги тарантулом просто кладет space данные которые захотел пользовать положить в очередь его указывает статус оказывает как текущее время если это у нас простая фифа очередь по которой как раз индекс будет по этим двум полям и все и возвращает task случае тык лайка чуть-чуть посложнее но в принципе тоже очень просто как бы сказал им заводим интер атор и ждем до тех пор если у нас не была тоска уже готова ждем появления нового тоска функция tekken она просто помечать так как взятым но самое главное что она делает она вот как раз запоминает кто что взял и вот нам он диска ногти мы можем как раз проверить вот эту вот всю ну как раз и высвободить все эти тоски который забрал конкретный пользователь но конкретно и соединение все довольно просто я думал потом вы можете посмотреть так что основной вопрос который может возникнуть возможно ли альтернативы конечно невозможные womage узкого еду слова нужно мы могли конечно же взять любую базу данных ну скорее всего это был бы тоже тарантул но не суть важно то есть могли просто пользоваться базой данных как базой данных при этом нам бы пришлось для нашей логике вот для логики работы с внешней системой срыв режиме так далее все равно построить очередь потому что мы не можем просто рыв решить эти токены по запросу то есть нас не прогнозируемая нагрузка тогда будет нам нужно все равно поддерживать систему живой но тогда мы чтобы делали мы бы клали в задачу от ложи в смысле очередь отложенные задачи нам нужно было бы следить за консистентную что у нас в базе и то есть она есть в очереди она есть в очереди оно должно быть в базе нам все равно пришлось бы сделать какую-то отказа устойчивую очередь кормом и плюс ко всему если мы помещаем одновременно данные и в память и в очередь очередь скорее всего нам тоже при наших нагрузках понадобилось бы memory то мы бы потребляли больше ресурсов случае нашим у нас база данных одновременно и хранить токены и одновременно за логику очереди мы заплатили двумя байтами плюс еще 5 ну то есть плюс 7 байт над apple и у нас есть логика очереди случае любой другой очередь мы под мы бы потратили намного больше в пределе там до x2 по memory ну собственно подведем итог нашего всего рассказа для начала мы решили нашу проблему а у той же то есть она у нас была она регулярно случалось внедрение данной системы позволило нам избавиться вообще от этой проблемы мы получили возможность горизонтально масштабироваться за счет шарден га ну и попутно мы решили проблему которую сами же себе здесь в процессе этого всего и создали то есть убрали н квадрат соединений до линейного и попутно улучшили вот ту самую логику очереди под нашу бизнес задачу то есть разрушить то что мы еще можем отрыв решить случае каких-либо там проблем задержек вот должен сказать что проблемы возникают не только по нашей вине у нас что-то у балом то есть тот же google microsoft индекс они могут что-нибудь у себя на у wales провайдере выкатить и у нас в общем горка не отрыв решенных токенов так тоже бывает мы конечно же получили очень много опыта при написании собственной реализации ровд вообще рекомендую про и протоколы алгоритм довольно простой но очень хорошо понимает помогает понимать как это все устроено и как это все работает засим у нас все в общем делаете расчеты в базах данных непосредственно рядом с данными это очень удобно и очень производительно масштабируема гибкая ну и вообще тарантул прикольный на общем если есть вопросы задавайте пожалуйста спасибо большое владимир пасе большой игры спасибо за доклад очень интересно нет парочку вопросов есть первый вопрос это вот хранимой логиков tarantul и и так далее самый централи мне главный вопрос это как наливаются обновление логике где хранится код этой логике как бы вот запросто в общем смотрите как работает обновление логике то есть вы знакомы слова нет в lua в общем весь код все там условно указатели структуры и так далее они хранятся в глобальном ты блин то есть когда у вас запущен на базу нет global в globally ладно с биллом знакомы в общем у вас есть некий snapshot кодовой базы который сейчас работает то есть вас приходит request он лукавит функцию globally находит ее выполняет в общем работает с ней континент на мы можем приконнектиться к базе и сказать загрузи новый образ то есть вот там лежит новая кодовая база сделана не и простоту файл она начинает его прогружается подгружает новые функции при этом те которые сейчас уже выполняются они выполняются в старом скалки то есть они видят старые функции она специально так пишется то есть можно писать так можно писать не так мы специально заморочились что вывод можно было на горячую накатывать кот кот раскладывается просто пакетами то есть собирается пмк у нас там то есть собирается рбмк кладётся рядом с тарантулом то есть выход к кода производится следующим образом мы собираем новую рбмк отдаем админом админа ее накатывают сконектиться говорят бокс рилот есть свой модуль который это делали ладит да нет ну собственно да интересно что у вас есть собственный модуль для вело один блок на гитхабе он на гитхабе хорошо спасибо собственно я не знаю второй вопрос это как шарды добавляется то есть ну вот как бы это через через рилот через кода добавляется да то есть фактически до список сортов хранится на зло именно в конфиге ло и соответственно чтобы получить на успеть просто reloading от ее у нас как все работает в случае если у вас вот выпал кластер целый то надо бы надо идти по через кодов да это все ну не у нас как бы есть этот отдельный модуль тоже конфиг называется фактически ну это как бы там уже надстроек антанту она как бы не встроена в него сейчас конфликт приложение определяется именно в этом файле именно рилот этого файла и в дальнейшем уже дополнительно перезагрузка до дадут нам возможность перезагрузить ко мне я думаю вопрос был про автоматически ришар ding нет автоматического ришар дин кандидатов и секретарша если выплат кластер то но только наливкой код это все делаю гу ручками мы пришли к тому что автоматически ришар ding в нашем случае зло то я в него дня прыщики flower это да это очень умен наиболее еще еще такой вопрос спасибо короткий вопрос в один из 7 апреля понимаю ровд будет из коробки на сколько мы знаем это лучше отрисовать кости хорошо вот там говорят в 18 будет то есть да наш раз он не интегрирован с репликой то есть протоколом репликации то есть репликация отдельно мы сделали это все на вот отдельных connection ах которые только отвечают за выбор лидера такая реализация позволило нам это сделать быстро просто и без вам за нее в ядро но конечно же чтобы получить нормальную клубную базу которая это все применяет еще крепления налогом да это нужно вот это к ядерной команде спасибо за доклад сегодня вот с утра был доклад о проблемах который возникает в распределенных системах вот у меня связи с этим вопрос вы свой раз как-то не знаю во лидировали тестировали и так далее искали в нем проблемы или просто написали и запустили сейчас на про было бы не было ли снимки проблем и интересно какие инструменты вы использовали для доказательства того что он ред спасибо ну естественно проблем возникало просто масса особенно возникали проблемы с самим тарантулом периодически но слова бога нефиг селе все довольно быстро я сейчас как корректных проблемах я уже не вспомню но в любом случае простейший дебаг на на уровне там не знаю принтов и ну каких-то дополнительных средств помогал решить основные проблемы связаны с неправильно работа самого алгоритм там выборов или харди ты и так далее насчет корректности сложно сказать пока проблема не замечали ну я скажу так я тоже как бы не совсем все помню но происходило это приблизительно так сам алгоритм рафта он доказал то есть он корректен до проблема была в имплементации поэтому когда это нужно было потестить мы выставляли очень маленькие тайм-аут и на все запускали это все причем запускали в режиме вот множество но ту же из шарден гам и начинали фигачить когда она рассыпалась то есть мы разбирались почему плюс условно говоря просматривали этот код где что там неправильно может быть сделано ну исключительно так после того как она стала работать при падении поднять и так далее то есть нам этого достаточно то есть так как мы не следим за строгой консистентной стью данных нас данное решение устроила если бы мы отвечали за данные да и и бы хорошо было бы от во лидировать вот через утилиту альфира например ну то есть с нам просто фактически нужна была информация о том кто как бы является лидером данной ситуации что вот с ним вести такой диалог иглам чтобы происходил перевыборы а вот именно вещи связаны с репликации на рафте это вот как бы нас особенно сильно не волновало благо есть хороший репликация уже стороны и еще один маленький вопрос вот вы рисовали там 2 2 2 прокси как бы до на таран то ли не вопрос собственно говорят что балансирует трафик между этими прокси и как то вы на таран туле вот этот я не знаю там у вас есть там может быть ровд для прокси ну почему вы не почему бы не одна самом деле простой connection pool connection pool да ну то есть как бы но здесь фактически не важно в какой прокси ходить можем просто равна дроби на выбирать себе прокси ходить не и все понятно окей спасибо а собственно сами прокси это тарантула которые несут в себе данных спасибо ещё раз за доклад собственно как раз в рубрук такой вопрос реализовывать их но tarantul it never kill в принципе нет никакой разницы реализовывать их на таран туле либо реализовывать их сбоку то есть лола достаточно быстрый его обойти можно там не знаю если уже взять совсем хардкорный 7 главное что нам не нужно дополнительного кода какого-либо то есть мы просто берём вот условно говоря shard говорим что он больше не shard и оно все работает вы сами писали реализацию интерпретатора его или взяли какую-то готовую я затрудняюсь ответить на этот вопрос поскольку во первых это то что встроен автор antilles тарантул строим lua jit насколько я знаю если его и почили то из-за багов это вот рассказывали вчера пролоджи ты поиск в нем всяких вот ну да я знаю то что там патч-корды только для того чтобы там он нормально с вампирами работал так вот в принципе свалится gets но в самом tarantul и поэтому ну как мы со своей стороны не реализован интерпретатор моего списали бизнес-логику для нашего нет что мы делали это мы подключали и фифа фифа это вот вообще вот такая вещь можно любую сильную функцию вытащил вставил за там не знаю 10 минут и все плавно и аккуратно с память спасибо большое за заклад а подскажите еще вот про механизм репликации он его резервирования к сервера действительно площадке далёком нормально работает то есть это отдельных каких-то механизмов он один да это стандартный механизм репликации у нас удаленность небольшая у нас питаться в москве в нашем случае мы не тестировали на большой дальности разве не интересно talkin' получают а рядом с владивостока ходить просто клиенты нашей системы они там все тут спасибо ну в общем если все если есть там вопросы желание обсудить позадавать вот посмотреть на какие-то примеры кода приходите в этот кубик 17 прям как новая версия то разлада в два часа начинаем митап тарантула будем говорить и отвечать на вопросы"
}