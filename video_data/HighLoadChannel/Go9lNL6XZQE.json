{
  "video_id": "Go9lNL6XZQE",
  "channel": "HighLoadChannel",
  "title": "Как регулярно терять один ДЦ и не волноваться? / Михаил Кабищев (Ozon)",
  "views": 710,
  "duration": 2881,
  "published": "2025-01-17T02:21:16-08:00",
  "text": "Всем привет Так меня зовут Миш кащеев Я работаю в компании озон и Сегодня я расскажу вам как мы дожили до того что мы можем регулярно терять один из наших дата-центров и практически не волноваться скажу Пару слов про себя Я работаю в it уже больше 15 лет поработал в различных компаниях маленьких стартапах крупных компаниях в основном так сложилось что большую часть своей карьеры я проработал в е коме и так или иначе я всегда был связан с инфраструктурной платформенной разработкой в разных компаниях это называется по-разному но всегда это было что-то Вот вокруг инфраструктуры а не продукта и на текущий момент я руковожу департаментом пас мы занимаемся созданием внутреннего облако в Озоне Я работаю около 6 лет пережил не одну сотню инцидентов почти не посидел но успел немножко повысить за это время всё-таки стресс как бы он есть но да ладно давайте перейдём к озону Озон - это не только один из крупнейших ю Комов в стране Да и не только в нашей стране это ещё и большая развесистая инфраструктура Мы живём в трёх дата-центра совокупное количество серверов наверное уже перевалило за 10.000 там мы примерно единицы не считаем но порядок он вот такое вот и в этих дата-центра запущено порядка 6.000 различных сервисов это не какие-то отдельные экземпляры это 6.000 различных приложений которые совокупно дают наверное количество в несколько десятков тысяч подов в кубе но Озон не всегда был таким большим и когда-то давно порядка ше лет назад когда я прихо в компанию и какое-то время мы жили в одном дата-центре Соответственно в этом дата-центре мы уже запускали cuber там крутились какие-то бизнес-приложения у нас уже была достаточно хорошая автоматизация для баз данных мы умели автоматически делать файлове запускать реплики А мы затащили к себе различные хранилища CF ris кавка В общем такой стандартный а стек для современного времени но у такой архитектуры есть явные проблемы потому что как мы знаем что приложение нужно делать доступными и надежными А вот такая вот архитектура Она явно не не позволяет делать такие вещи и в такой архитектуре могут сломаться различные вещи здесь я хочу провести аналогию между софтом и железом и начну с софта Да с каким-то приложением если у вас падает одна реплика шего приложения то глобально для ва этова и в случае озона такие инциденты скажем так небольшие случаются каждый день какой-то пот Может в Случайный момент умереть приложение в целом и команда разработки вообще никак этого не замечает замечает только если это становится регулярным какой-то историе А вот если приложение ваше приложение оно падает целиком То есть все копии вашего приложения становятся недоступны то это может стать большой проблемой для бизнеса потому часть какого-то функционала она полностью не работает и проводя проводя аналогию с железом если у вас падает каким-то причинам падает или ломается какой-то из серверов он затрагивает множество приложений это какие-то десятки подов в целом Это тоже не так страшно потому что каждое из приложений по отдельности должно переживать подобную историю соответственно и вся система тоже переживёт А вот если у вас упадёт один дата-центр то будет всё очень плохо потому что Ну явно Озон вот в той архитектуре он бы просто лежал бы и всё и мы бы не могли ничего делать А мы жили какое-то время с такой архитектурой ну И становилось Понятно что так жить плохо надо делать несколько дата-центров три - это очень хорошее число соответственно Мы решили что давайте вот мы разъест в три а ещё лучше что мы сделаем так чтобы мы могли спокойно жить без одного из дата-центров сказано практически сделано мы начали большой проект по переезду в несколько дата-центров Мы приняли решение делать Всё достаточно эволюционным а не революционным путём поэтому какие-то из компонентов мы растягивали на несколько дата-центров например кубер мы тянем в целом и до сих пор Мы растягиваем кубер на несколько дата-центров это такие достаточно большие большие квестра а дальше в зависимости от каждого приложения от каждой технологии оно работает по-своему например каку точно также растягиваем на несколько дата-центров квара фа живут такими парами и например наш наша автоматизация вокруг постгрес умеет автоматически раскидывать мастера синхронные асинхронные реплики по по разным дата центрам для того чтобы в случае какой-то аварии или какой-то проблемы всё всё легко восстанавливалась мы занимались этим проектом достаточно длительный период лись всё было классно и по нашим планам в случае какой-то аварии соответственно мы продолжаем работать на двух оставшихся дата центрах какие-то Да там компоненты которые или какие-то данные которые жили в первом дата-центре они становятся недоступными Ну и вот сравнивая предыдущую и следующую картинку Например можно заметить что мастер в этом случае должен переехать во второй дата-центр асинхронная реплика становится синхронной и мы запускаем ещ одну реплику вот мы переехали в такую инфраструктуру Всё у нас три дата-центра всё классно мы думаем что здорово как бы никакие проблемы нам не страшны Но наше счастье было не супер долгим А и В начале двадцать второго года в самую первую рабочую пятницу двадцать второго года Это было какое-то там 15-14 января А вечером мы собрались с ребятами Как из озона так и с коллегами которые сейчас работают в других компаниях делились впечатлениями о том как мы провели все новогодние каникулы и в какой-то момент у всех Начали звонить телефоны стало приходить огромное количество алертов и мы поняли что происходит что-то нехорошее и оказалось что в одном из наших дата-центров отключилось Электричество и пропала с ним полностью пропала связь то есть мы не могли компоненты не могли связаться между собой Мы даже не могли понять чего там происходит это затронуло не только нас а и другие компании в том числе возможно среди присутствующих тоже есть коллеги из других компаний кто пострадал в рамках этого инцидента как я сказал у нас были проблемы с доступностью по факту какие-то компоненты лежали совсем или очень медленно переключались В общем все наши надежды на то что сработает куча автоматики они не оправдались и мы поняли что мы в общем-то не совсем готовы к тому чтобы один из дата-центров отключался всё не так здорово как Как нам казалось и во время этого инцидента на звонок собралось Да я напомню что был Пятница вечер на звонок собралось ну наверное несколько десятков или пару сотен даже человек из разных команд и в какой-то момент наступила пауза потому что не было понятно чего делать Да мы мы к такому не были готовы и кто-то из ребят в какой-то момент сказал Андрей надо что-то делать Андрей это один из директоров внутри озона и это вот это вот фраза надо что-то делать Она стала неким таким ммом внутри компании которою мы иногда её говорим для того чтобы сказать Да всё как бы нету ну проблему надо решать э и Давайте двигаться вперёд а что пострадало сильнее всего в во время той аварии во-первых очень долго сходился кубе так как он растянутый соответственно Control Plane очень долго понимал что какие-то ноды вылетели соответственно он не с большой задержкой информацию получал сес Discovery и это влияло на то куда отправляется трафик У нас есть несколько эстеров они остались с времён а зона достаточно такого взрослого старого механизм фловера отработал Не так хорошо как нам хотелось бы поэтому были проблемы с с бизнес логикой были проблемы с фом потому что у него тоже отвалилась часть его нот соответственно были проблемы с записью и также то о чём мы как платформа думали не так много были проблемы в различных сервисах потому что не все они были готовы к тому что какая-то из какой-то из кусков инфраструктуры начнёт деградировать и они Ну например Это сказывалось то что неправильно Или вообще не было таймаутов для каких-нибудь запросов поэтому приложение слишком долго ждало чего-то как физически происходила эта авария Да вот бы хорошо Зелёная линия потом в какой-то момент отключается электричество нас начинает шторми начинается условно такой переходный момент когда что-то переключается что-то нет в какой-то момент всевозможными усилиями мы восстановили работу в двух Дато центрах где-то что-то было работало на подпорка где-то где-то всё-таки сработала автоматика но тем не менее В общем мы продолжили Работать естественно в этот момент мы были на связи с командой самого дата-центра и и ребята занимались восстановлением электричества и в какой-то момент это электричество восстановилось соответственно сервера стали загружаться они стали сетевая связанность опять появилась и в общем все системы начали опять пытаться переключаться обратно и для нас начался второй переходный период когда кубер начинает запускать какие-то поды что-то происходит с базами какие-то данные реплицируемый раз и в какой-то момент мы вернулись к к нормальному состоянию после этого ужасного винда когда мы кучу времени и днём и ночью занимались починкой мы провели ретроспективу для того чтобы зафиксировать чёткий зафиксировали востановление обратное и собственно точно также разложили второй переходный период на все временные отрезки помимо вот этой вот временной шкалы Мы также определили влияние на технологии и и бизнес мы разобрались что именно не работало с точки зрения наших услуг как платформы Можно ли было могли ли Дули какие-то поды Могло ли приложение отправить запросу данных Можно ли было задеплоить Хотя конечно во время такой аварии очень странно делать какой-то деплой но тем не менее как физически такая услуга она существует и что очень важно мы определяли А как это всё повлияло на бизнес потому что все вот эти вот услуги и функции они как бы могут напрямую не не влиять на возможность оформления заказа пользователям на сайте и вот это вот супер важное потому что во время таких инцидентов очень важно уметь считать вот это вот влияние в деньгах для того чтобы понимать масштаб трагедии и понимать что любые затраты на технологии или людей или какое-то железо очень часто сильно или там на порядке ниже чем потенциальный ущерб от того что ваш ваш бизнес просто не работает и стало понятно после перспективы нам нужно учиться нам нужно делать управляемое отключение дата-центров для того чтобы понимать переживаем мы это или или переживая соответственно мы начали готовиться к учениям и прежде чем бы перейти непосредственно к к тому как мы готовились к учениям Я хочу ещё немножко такой формальности добавить пару определений про доступность и непрерывность они связаны с предыдущим слайдом есть такое в ателе есть понятие доступность это способность it услуги или какой-то функцио конфигурационное единицы выполнять согласованную функцию когда это требуется и непрерывность бизнеса это способность планировать и реагировать на какие-то инциденты и нарушения бизнеса для того чтобы продолжать или возобновлять операции на предопределен согласованном уровне переводя на простой язык доступность - это как раз те самые услуги возможность запустить пот возможность отправить запрос в базу А непрерывность бизнеса - это соответственно возможность делать какие-то бизнес действия для ваших пользователей и для себя в Озоне мы понимаем что вот во время подобных инцидентов во время подобных аварий для нас в первую очередь нужно восстанавливать заказы то есть возможность пользователей оформлять заказы на сайте это самое самое важное что у нас есть и это то как бы На чём держится и работает Наш бизнес Да наша витрина может каким-то образом деградировать пользователи могут видеть или не видеть например какие-то рекомендации что-то ещё Но человек всегда должен иметь возможность положить товар в корзину и оплатить его Потом мы должны восстанавливать операции это физическая часть нашего бизнеса когда заказы собирают и доставляют покупателям и Вот Потом мы начинаем чинить вообще всё остальное какие-то внутренние сервисы или что-то ещё соответственно подготовка к учения Да по результатам ретроспективы мы составили большой Список проблем которые мы нашли мы сделали регулярные встречи на которых мы обсуждали статус по по этим проблемам Что нужно сделать что сдела Нужна ли ещ какая-то помощь и важно что на эти встречи ходили не только представители платформы инфраструктуры но в том числе и продуктовые команды потому что как я говорил что проблемы были в том числе и в бизнес логике которая не была готова к тому что произойдёт такое отключение мы составили после того как большинство из проблем было решено или по каким-то проблемам мы поняли что их решение может занять слишком много времени и и нам нужно делать уже учение мы составили подробный план действий в котором было написано что вот такой-то конкретный человек в такое-то время должен делать Вот такое-то действие например там отключать Да я забыл упомянуть что в для наших учений мы выбрали немножко другой сценарий мы решили не отключать электричество в дата-центре потому что это очень накладно но мы отключаем сетевую связанность мы эмулировать трактор вскапывает поле и рубаев оптику которая идёт в дата-центр и с некоторыми например физическими складами которые у нас есть такие истории тоже бывали что поэтому мы считаем что это достаточно честная история Вот соответственно есть подробный план действий что вот этот человек выключает гасит интерфейсы Этот человек например смотрит за такими-то метриками это человек вручную переключает какое-нибудь хранилище потому что мы ещё не гото не не были готовы сделать какую-то автоматизацию и есть подробные инструкции Что именно нужно делать И после этого после того как мы составили план действий мы пошли Договариваться с бизнесом и говорить что вот мы хотим провести учение мы договорились про конкретную дату и время обычно и не то что обычно на самом деле всегда мы делаем проводим эти учения ночью потому что с точки Для нас это самое безопасное время и в том числе мы определили договорились про максимальное время простое а которые мы можем себе позволить То есть это как бы та отсечка что после выключения дата-центра В течение какого времени мы должны вернуть заказы и операции в норму А если этого не происходит то мы досрочно прерывая учение включаем всё обратно То есть как мы считаем что учения в этом случае будут завалены собственно сами учения состоят из трёх этапов первой такой разогревочный мы берём какую-то рандомную стойку отключаем её обычно здесь вообще нету никакого влияния это просто такой как бы первый шаг затем мы отключаем ряд это несколько стоек здесь может быть уже какое-то влияние Но обычно это тоже вообще Всегда проходит Не замечено И никакие команды ничего даже Они конечно знают про это Но никакого влияния на что не происходит и самое важное самое важное событие - это отключение дата-центра соответственно потенциальное влияние на всю компанию оно наибольше как выглядят сами учения во время учения мы собираем так называемый штаб в офисе это координатор учения это человек который выполняет административную функцию он чётко следит за тем что все люди действуют Строго по плану он проводит переклички и так далее а также собираются сетевые инженеры ребята которые отвечают за кубер за базы что-то ещё Почему в офисе просто потому что в случа таким образом проще решать какие-то проблемы а потому что на звонке обычно находятся очень много людей Если ты сидишь рядом с кем-то то решить там или договориться быстро о том что нужно что-то как как и что именно лечить Так гораздо быстрее плюс у нас есть сетевая связанность Прямая С нашими дата центрами из офиса Поэтому в случае возникновения каких-то проблем например с пином то точно так же из офиса их гораздо быстрее можно решить У нас есть большой Центральный звонок на котором обычно собирается куда заходят представители всех команд как технических так и продуктовых при необходимости создаются какие-то локальные звонки на например на какие-то крупные департаменты как логистика склад или кто-то ещё для решения каких-то локальных проблем и всегда есть запасной канал связи Почему Потому что мы используем свою собственную звонилку для общения в компании она работает в той же самой инфраструктуре во время учения она точно также подвержена м самым учением и с ней точно также может пойти что-то не так а нам очень важно оставаться на связи Поэтому всегда есть п в виде какого-то другого канала связи Ну и во время учений координатор говорит так значит вот ты делаешь Вот такое действие человек говорит Да я делаю потом он говорит я сделал координатор всё это фиксирует и у нас появляется опять чёткий тайм по результатам каждых учений обм как и та самая авария которая была в начале д второго года мы отключаем один из дата центров быстро восстанавливаемся В двух из них работаем какое-то время потом начинаем обратную процедуру по включению дата-центра начинаются опять какие-то переходные моменты переходный процесс и возвращаемся назад И вот Глядя на эту схему Вот первый риод это сам о котором мы договариваемся с бизнесом и по прошествию вот этих почти д лет мы сократили это время до 5 минут а второй переходный период который Здесь тоже показан красным на самом деле он может быть он не такой страшный для нас потому что нам очень важно восстановиться как можно быстрее после того как мы потеряли один из дата-центров А после этого включать мы можем сильно больше по времени чем чем первый период потому что нам важно да то есть как бы сама авария происходит в первый период а затем мы всё возвращаем назад поэтому нам нужно сделать это максимально безопасно и эффективно и мы можем если первый период должен быть в лучшем случае секунды там А в худшем минуты то второй период Может растянуться на десятки минут или на часы когда мы возвращаем всё назад там в том числе начинаются какието процесс ци хранилищ мастера переезжают обратно в дата-центры может реплицировать цеф обратно и так далее И что важно переключение обратно из всей нагрузки в третий дата-центр должно быть мы считаем что оно должно быть обязательно ручным то есть мы удостоверяю о том что третий дата-центр в нём Всё вернулось в нём всё запустило и мы потихоньку можем возвращать туда трафик для того чтобы не потерять ни один заказ чтобы пользователи могли не потерять Никакое качество наших услуг в результате учений да как я сказал что вот мы первая Авария была в начале двадцать второго года первое учение мы провели летом д второго года их было наверное уже штук сть или сем примерно так мы нашли несколько проблем с которыми боролись да это та же самая долгая сходимость кубера как мы его рули тюли параметры всё равно ему нужны минуты на то чтобы понять что большое количество нот недоступно и пометить все поды как нере какие-то поды должны переселиться чего-то ещё и мы пытались пытались крутить В итоге решили это на уровне нашего се Discovery который называется Н он умеет определять что какая-то из зон какой-то дата-центр больше не доступен соответственно он всю зону выкидывает сразу из балансировки не того как кубер поймёт что что внутри происходит кубе Доделываю свою работу Ну и всё как бы всё приходит консистентной состояние а также у нас была достаточно наверное детская такая проблема скажем так мы при возвращении назад в третий дата-центр ловили ситуации когда запускались поды и какие-то поды с бизнес приложениями запускались быстрее чем какие-то демонс которые могут обеспечивать логирование или какую-то другую такую инфраструктур историю соответственно решение было очень простое добавить приоритеты сначала должны деплоить все демонс и прочие системный компонент а только потом уже приложение у нас было несколько в каком-то количестве так называемой сингл реплик приложения это приложения которые работали в одном экземпляре соответственно если они жили на момент отключения в том дата-центре который отключается то функциональность них возвращалась только тогда когда третий дата цент приходил когда Control Plane сходился и это приложение запускалось где-то в другом месте соответственно Дулин мог происходить очень долго и как решение мы предоставляем командам механизм для Р election они запускают несколько копий своего приложения и в один момент времени кто-то из них только делает полезную нагрузку один из дата-центров вылетает соответственно лидером становится кто-то другой Ну и как бы всё продолжает работа ВС классно также Мы не ожидали что у нас будет очень высокая нагрузка на обели потому что очень много команд очень быстро нажимает и пытается посмотреть графики соответственно прометеус Танос графана тоже немножко начинают страдать и они точно также в этот момент находятся как бы в состоянии некой аварии когда третьих мощностей вылете соответственно ну здесь ребята тоже поиграли настройками добавили ресурсов для того чтобы А всё это переживало высокую нагрузку мы ловили и продолжаем иногда ловить не очень приятную историю с какой а потому что кавка by Design во время подобных переключений может приводить к ситуации когда Лидер какой-то партиции переезжает на брокер клиент получает эту мету информацию пытается пойти за новыми данными а брокер ещё не готов их отдавать клиент получает ошибку И большинство кавка клиентов написано таким образом они следуют спецификации самой Кафки и в этом случае они могут сбросить офсет в самое начало и это приведёт к тому что приложение проиграет всю весь топик с самого начала и в худшем случае это просто займёт какое-то дополнительное время А совсем в худшем худшем случае если приложение не умеет водо потент Насть то вы заново можете проиграть какие-то события сделать не знаю там какие-то Записи в базе что-то ещё и э также мы натыкались на проблему с кэширования днса у нас многоступенчатая скажем так многоуровневая архитектура днса соотвественно в какой-то момент было добавлено кэширование некоторых адресов корда Несси внутри кубера и естественно так сложилось что во время учений очередных за Каширова запись которая вела в тот дата-центр который мы отключали что конечно же привело нас привело к к проблеме которую мы тоже достаточно долго раскапывать что я хочу сказать в заключении для нас этот процесс по учением он стал абсолютно регулярным мы жим вместе с ним точно также как с ги процессов подготовки к сезону все команды понимают что как мы Да мы готовимся к сезону Каждый год у нас проходит нагрузочное тестирование точно также абсолютно все команды системы готовятся к к постоянным учениям сейчас мы проводим их каждый квартал иногда бывали случаи по-моему даже что мы их проводили чуть чуть чаще мы стараемся делать это максимально Честно насколько это возможно мы ничего не переключаем заранее не перевозим не делаем какие-то там махинации Ну вот это мы там будем сейчас отключать этот центр Давайте чего-то там под шамани наприём нет потому что в вся эта все эти учения делаются для того чтобы в случае настоящей аварии Мы были готовы максимально быстро восстановить работоспособность А если мы начинаем делать какие-то махинации то если у нас опять повторится такая авария то как бы у нас не будет времени и возможности сделать эти махинации что всё заработало поэтому очень-очень важно делать это максимально честно без каких-то хаков сейчас мы выдерживаем до 5 минут простое это наш Таргет естественно мы отрабатываем многие наши системы отрабатывают гораздо быстрее и стремимся улучшить это значение до одной минуты потому что компоненты Отт чем минута база данных переключаются тоже очень быстро соответственно хочется верить в то что очень-очень скоро во время учений наши пользователи наверное вообще даже не будут замечать какого-то тайма и смогут вообще всегда оформлять заказы у меня на этом всё большое спасибо за ваше внимание Буду рад ответить на ваши вопросы Да есть ещё QR код пожалуйста просканируйте поставьте лайки все дела Привет вот меня зовут Кирилл такой вопрос уче Да учения Они совсем не похожи на настоящий инцидент То есть ты сказал что люди сидят в офисе ночью в одном кабинете и решают срочные вопросы но когда будет настоящий инцидент люди будут сидеть дома Возможно пить чай им на телефон позвонить что-то что будет говорить у вас инцидент так вот как учение помогает пережить реальный инцидент мы да Это хороший вопрос Действительно это так во время настоящего инцидента Если вдруг он случится действительно все будут находиться где-то кто-то на даче кто-то не знаю будет гулять с собакой чего-то ещ потребуется конечно время на то что собрать чтобы люди собрались и подключились но во время этих учений это всё-таки синтетическая история и мы должны минимизировать Этот простой в идеале мы должны добиться того что у нас всё переключается автоматически и люди для этого вообще не нужны и во время учений люди сидят и смотрят То есть как бы сейчас мы не делаем каких-то про активных действий их нет да нам важно Мы мы не готовы ээ просто так э рисковать работоспособностью системы А во время учения Да и то есть да мы все собрались если что-то пойдёт не так то во время учения человек включится в этот процесс и починит Да ну а во время настоящего инцидента Да ему нужно будет какое-то время чтобы аэ достать ноутбук или дойти до до ноутбука и так далее То есть сейчас каждое ручное действие - это как бы минус Что кажд ну если нужно совершать ручные действия во время инцидента то это минус да естественно это минус И поэтому Если вдруг такое действие в плане появляется то к следующим учениям оно должно пропасть вот да спасибо Так Осторожно это товарищ из Яндекса они всё время этим занимаются учениями Главное чтобы не за наш счёт я уве увидела очень интересную проблему что проблема номер один У вас что кавка теряет смещение при потере дата-центра это действительно известная проблема И вот жутко Интересно как вы её хотите решать Давай мы это обсудим в Варах это очень классный вопрос Это это очень Не не быстрый не быстрый ответ Поэтому да мы Давай поговорим про это Да тут ещё ри три тимлида вписываются короче собирай своих это ну это правда Просто это не быстро рассказывать Если что завтра про этот доклад Да друзья когда закончим в микрофон то сразу на выходе из зала дискуссионная зона и можно можно продолжить Да пожалуйста Добрый день Юрий вот такой вопрос наверно в догонку будет понятно что хочется быстро переключаться это всё понятно у меня больше вопрос по восстановлению То есть когда у нас така такой деградированных так сказать Она может повториться То есть почему вс-таки не думайте ускоряться по поводу именно восстановление в цент смотри тут возможно я не так корректно выразился Мы мы не в вальяжно режиме восстанавливаем то есть не так что мы сидим на пуфиках Ну давайте там включим третий дата-центр это не так вопро врем этоо читаем что у нас Мы должны восстановиться за секунды и это нужно сделать максимально быстро Да чтобы мы становили заказы операции и так далее А обратно мы можем потратить чуть больше времени то есть здесь вот во второе когда мы восстанавливаемся обратно нам важно не потерять ни один заказ вот поэтому здесь на это может уйти чуть больше времени чем когда происходит отключение Ну тогда в догонку получается вы как-то пытаетесь учесть ситуацию Когда у вас в моменте восстановления произойдёт ещё один инцедент Да и ещё сильнее всё деградирует имеешь виду если он отключится ещё раз Ну допустим тот же самый отключился либо ещё живой да Второй Ну нет сценарий о том что у нас случится инцидент Когда у нас отключается два дата-центра Мы в рамках этих учений точно не рассматриваем и как всю всю архитектуру Мы проектируем из расчёта что будет отключаться 1/3 две трети Вот спасибо спасибо Так вот микрофон у вас да да да А спасибо за доклад помню у вас был слайд что при восстановлении у вас выбирается какой-то Лидер который по-моему и отвечает за в основном отвечает за восстановление а крайне интересно какой алгоритм используете для консенсуса для лидера экшена raft Пакс или это вообще Нда который нельзя рассказывать Нет это про это можно рассказывать Ты имеешь в виду э сейчас я Отмотай назад кажется вот следующий бы мы используем для сервисов для того чтобы они делали лек есть очень простая библиотека для всех языков библиотеки для всех языков ребята Их встраивают своё приложение Ну и всё и как бы приложение всегда понимает кто из них Лидер и делает какую-то работу Там то что ему нужно делать Ну в общем поняла с за интересный доклад интересно узнать сколько вы резервирует ресурсах во всех дата-центра на случай фловера То есть это 1/3 которая у вас всегда спр находится или же меньше то есть итоговое количество ресурсов которые переезжают из одного дата-центра в другой оно идентично или вы всё-таки в деградированных режиме и по каналам наверняка учитываете что будет большая репликация соответственно вае канал имеют резервацию Да смотри значит очень хороший вопрос Мы резерви соответственно 50% целевую нагрузку мы должны переживать на двух дата центрах поэтому количество железа количество инстан сов любого приложение оно от необходимого в в ВМЗ распределяется между всеми Дант итно их от целевой нагрузки получается 2/3 скажем так А когда отключается один дата-центр то вся нагрузка переходит в два оставшихся вот таким образом да естественно это приводит к тому что мы резерви железо и и канавы в том числе точно по такой же сме дальше с увеличением количества дата-центров этот порядок резервации он будет становиться ниже и дешевле Для нас это всегда неприкосновенный запас или да спасибо большое Миш Спасибо большое за доклад интересно и лаконично Меня зовут Артём Я занимаюсь базами данных и у меня такой вопрос касательно резервирования я понял По ресурсам пож примерно А вот как вы обеспечиваете консистентность данных потому что заказы оплата это всё в базе и как консистентность данных так сказать переносится на dc2 и на dc3 И как вы её проверяете только лишь с помощью синхронной репликации которая работает со звёздочкой и мастер может локально закоммитить если не получил в определённый таймаут от синхронное реплики Вот в общем об этом вопрос чуть Значит мы в общем виде скажем так как платформа мы не проверяем согласованность данны различ хранилищах значит само приложение команда ну и вообще вот подсистема ДСА она даёт гарантии того что как бы мы частично эту ответственность перекладываем на приложение приложение должно понять если оно смогло записать какие-то данные да то как бы даровать тот фак что ели база данных или кавка например вернули ок да что они получили значит эти данные действительно записаны и потом когда произойдёт отключение или какое-то Возвращение то соответственно эти данные можно будет прочитать Они они некуда Не потеряются ситуации когда Например одно приложение Ну чисто теоретически Да оно в рамках своей работы должно записать например что-то в базу и положить какой-то и ты в базу положил а в ты не смог положить Ну по каким-то причинам да то мы это в общем виде никак не решаем это это ложится на плечи самой команды и команда должна гарантировать какими-то способами того чтобы у них это всё случится это всё запишется Спасибо общем ответственность на стороне команде Да на бэнде да пожалуйста Да Вячеслав магнит всех Вопрос такой Скорее всего в тот момент когда вы только настраивали механизм отключения одного кластера оно наверное не сразу начало работать было ли такое что были какие-то Фолс позитива что она отключалась а на самом деле по факту дата-центр там был какой-то небольшой вре небольшие временные неполадки можно было и не отключать Да POS случались Ну в общем-то Как как бы баги бывают абсолютно везде но это если ты говоришь про Discovery когда мы просто часть подов выключили то это не про его Никаким проблемам а просто потому что ну как бы все хранилища продолжили работать да какая-то такая нагрузка с точки зрения rpc Да она полилась только в два из трёх Но это это не вызвало вообще никаких проблем никто по сути кроме команды Discovery это практически не заметил спасибо спасибо Михаил Спасибо за доклад а вопрос такой А у вас както встать мы тебя по телевизору покажем клас Я в телевизоре А у вас как-то обрабатывал кейс частичной деградации когда не весь кластер вышел как будто его переехал трактор А когда например деградировали диски Или например гипервизор сошёл с ума и большую часть виртуалок перевёл на dc2 и у вас там CPU не хватает Или диски долго отвечают но CPU Смотри да значит в этом случае таких проблем не возникает потому что есть резервирование по железу и Да окей Там виртуалки могут спокойно переехать в Ну там в случае да гипервизора в какой-то из других дата-центров и у него есть ресурсы для того чтобы запустить эти виртуалки и в случае если даже они не переехали от чего-то что-то отключилось например там в первом Ну я не знаю например там вылетели все брокеры кафке во втором дата-центре то тогда кавка сама раскидать лидеров партиции по первому и третьему и всё продолжит работать то есть на уровне как бы каждой из подсистем По отдельности все вот эти вот соглашения они выполняются не А если например диски плохо стали работать вот у тебя в dc1 Долго отвечает АПС например ну ну можно сказать что это некая такая как бы бытовая история Да команда получает Арт она понимает что какие-то проблемы с брокерами или с чем-то ещё и принимает решение Ну там выводить например вообще из эксплуатации и вс Ну то есть это не это типа вы считаете бытовая какая-то история не дистр то что на одной из Тачек начали медлено отвечать Дант Долго отвечает он просто не упал А Долго отвечает если весь дата-центр очень долго отвечает то Ну во-первых да помимо алертов мы автоматически будем наливать туда меньше трафика потому что он долго отвечает естественным образом будет меньше нагрузки на себя получать дальше подключится командну там зависимости Ну в случае если мы говорим про трафик придёт алер команде балансировки Заведётся инцидент и мы начнём разбираться что именно происходит спасибо спасибо будте добры Дада хотел спросить Наско вы в среднем отча этот центр по времени Да и приходилось ли вам отключать скажем так на полдня например день двое суток Ну то есть эмулировать отключениях иногда бывают всякие наведённым значит э отключение на продолжительное время было в рамках аварии в начале 20 второго года а учения длятся вот полная фаза несколько часов ночью пока мы думаем о том чтобы сделать их продолжительными чтобы потестить как себя будут вести какие-то хранилища Когда у них накопится большой объём данных которые нужно реплицировать обратно какие-то из команд тестируют такие вещи локально каких-то стендах на какой-то другой синтетике но на такой промышленный масштаб это пока ещё не вышло да как раз спасибо об этом был вопрос Спасибо на последнем ряду предпоследний Да Дада Спасибо за доклад у меня такой вопрос Вы говорили про проблему с сбросом Осе тов Как вы боролись с последствиями и какая профилактика кроме импотент бизнес логики консмед Ну как я сказал что мы можем там вместе с с коллегой из Яндекса и с дить детали в кулуарах базово Да это Индо потент и в том числе такая проблема заставляет командам обращать внимание на их ретеншн потому что вдруг даже если это случится Даже если ты умеешь делать идемпотентность А у тебя в каком-то топике накопились в какой-то партиции накопились миллионы или десятки миллионов сообщений даже с учётом импотент ты можешь очень долго их проигрывать и получать и а на самом деле тебе с точки зрения логики столько данных на такую историю и не нужно хранить вот а там про нюансы разных языков разных клиентов Ну я предлагаю тогда не сейчас поговорить там много много нюансов Спасибо и финальный в микрофон остальные в кулуарах друзья чтобы из времени не выпасть Да пожалуйста Да спасибо за доклад Меня зовут Максим Вопрос вот Судя по схеме у вас общий кластер куртис кака ВС растянуто ро первый вопрос скорее риторический не является ли это Анти паттерном и второй не пробовали ли вы делить по д так как вот такое растягивание оно относительно нормально реагирует когда у вас вываливается полностью но при этом когда начинаются какие-то полупотайной эффекты изго мы пошли эволюционным путём и пошли именно в историю растягивания потому что это было и проще и и быстрее потому что если ты сразу делаешь меняешь всю архитектуру тебе нужно разъезжаться в несколько дата-центров ты делаешь разные кубер кластера тебе нужно в том числе очень сильно инвестировать и например в механизм деплоя потому что тебе теперь нужно если в растянутый ты просто делош один раз и часть перекладывает несколько квесте то это ложится уже на твои плечи и сейчас Мы переезжаем в подход квесте на каждый дата-центр он с одной стороны позволит избежать как бы и упростить нам жизнь например во время учений инцидентов потому что не нужно по сути так пристально следить за кластером како за какой-то частью чего-то ещё потому что ну вылетел и вылетел всё есть как бы у тебя вообще никаких проблем никнет но при этом это требует нагрузки с точки зрения других систем Да деплоя вообще работы в целом всего нашего облака но да мы постепенно движемся сейчас по пути нарезания маленьких кластеров на дата-центры и миграции туда всей нагрузки Да спасибо спасибо У тебя сейчас ещё одна задача махни пожалуйста рукой все кто вопросы задали Надо сначала подарить матрёшку тому кто своим вопросам сделал вклад в сообщество угу вот чей вопрос был самым самым на твой взгляд ценным было Точно по-моему пара неудобных вопросов вот молодой по-моему первый был вопрос Да в Зелёной футболке такой да вот ему матрёшку или ваш сувенир давай Ну давай дадим право выбора что ты хочешь матрёшку матрёшку матрёшку канеш все хотят матрёшку потому что она говорит я уложился в High п+ Да вот да завистливо по аплодировать да да И ещё у нас есть у нас есть ваш приз и и генерально партнёрский вот понятно что ребятам из wb ты отдашь кусочек своей экспертизы вот сейчас в кулуарах А так и и и и Давайте молодому человеку в жилетке молодому человеку в жилетке жилетке по центру Да ваш или или генерально партнёрский а три Конечно да Давайте наш вот отлично так и генерально партнёрский приз уходит за вопрос Давайте последний вопрос про разделение дата-центров про разделение кров так супер тебе тоже памятные призы от конференции тоже спасибо большое друзья значит если вам важен публичный L"
}