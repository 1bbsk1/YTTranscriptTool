{
  "video_id": "J7IePhAZbCA",
  "channel": "HighLoadChannel",
  "title": "Построение HPC/GPU-кластеров для машинного обучения / Дмитрий Монахов (Яндекс)",
  "views": 2397,
  "duration": 2385,
  "published": "2023-01-19T06:59:52-08:00",
  "text": "Здравствуйте друзья так Я забыл кликер Меня зовут монахов Дмитрий в обычной жизни я занимаюсь разработкой ядра линуса в Яндексе И вообще это само по себе довольно интересно но это довольно эволюционный такой процесс разработка ядра и мне посчастливилось стать участником той революции которая произошла вот за последние несколько лет а именно модели выросли до такого размера что они уже не влезают ни в один сервер ни в несколько и даже в одну стойку потребовалось строить что-то огромное и тутто вспомнили что был какой-то чудак который имел опыт работы с распределенными фами системами это и есть использование распределённых алгоритмов Как выглядит цикл машинного обучения в современной эре за последние несколько лет машинное обучение претерпело сильно изменилась вошли в обиход так называемые Foundation модели а именно вы обучайте одну огромную общую модель которая умеет делать одинаково это как вырастить то самое огромное дерево потом и для небольшой модели из 3 миллиардов параметров вам потребуется 400 GP в течение 5 дней после то как вы обучили одну эту огромную модель Вы можете под конкретную какую-то зада бронировка билетов болталка в Алисе Для всего этого базой будет та самая наша Foundation Model поэтому и задача до обучения под конкретное использование она на четыре порядка меньше и когда мы уже наконец-то до обучили нашу модель мы можем начать её использовать реально в иренс это для этого нужен всего о ГПУ и время ответа будет меньше секунды Именно поэтому с появлением Foundation Model когда мы обучаем одну огромную модель а используем многократно и потребовались машины кластера чтобы понять размер и сложность задачи А на первых этапах первое модели berta gpt 3lik 1,3 млрд параметров обучались на обычном самом современном сервере с8 количеством ядер обучались 20 лет в общем не очень практично Ну модели за последние несколько лет выросли ещё в 10 раз и это означает что её нужно обучать Уже 200 лет то есть это абсолютно непрактично обучать На одном сервере Значит надо как-то учиться распараллеливание этой модели на нескольких узлах и CP но хорошо ли подходит нам CPU для этого типичное ядро General purpose вычислителя выдаёт нам 03004 ф операции с с плавающей точкой и в современный процессор можно поставить только два сокета и То бишь с одной системы вы получите лишь 4-5 терафлопс и стоить оди терафлопс вам будет порядка 5.000 долларов но если мы возьмём для этой операции GPU ядра у него слабее примерно в 10 раз но куда ядер там в 70 раз больше И самое главное что в сервер Мы можем поставить не 2 CPU а 8 ГПУ тем самым плотность вычислений на нашем с одного сервера у нас будет в 25 раз больше более того большинство Маши на самом деле сейчас не использует двойную точность использует одинарную точность т32 и производители GPU оптимизировали этот случай И на самом деле Разница в цене одного фпса уже не в 25 ой не в два раза а в 20 раз Именно поэтому все современные кластера для машинного обучения и по факту суперкомпьютеры практически все базируются на GPU потому что они подходят для решения таких задач намного лучше после того как мы поняли что мы хотим обучать своей модели Давайте поймём А как мы можем получить вообще кластер Для такого обучения первая задача Когда у нас есть просто деньги мы звоним звоним в nid и говорим мы хотим самый лучший суперкомпьютер на рынке и самый лучший компьютер на рынке - это NVIDIA superpod действительно и в течение месяца в вашем дата-центре будет вообще с иголочки под ключ так сделали в кембридже так сделал рири Но как любое Готовое решение под ключ вы залочен тем что там уже есть если вы захотите что-то поменять у вас проблемы и вам всё равно придётся наращивать эту экспертизу второй очевидный способ это купить готовые нот ВНО по простой причине умных людей у них много а задачи у них специфические Понятно второй уровень Инженерный сложности те к которым идут Клауд провайдеры они объёмы кластеров которые они строят большие они хотят строить много кластеров И самое главное на таких объёмах они могут позволить себе построить кластер для определённого вида задач который будет именно онный класс задач Ну собственно так подходит Microsoft и свом кластер но есть ещё и третий он же самый сложный путь Когда наши оптимизации зашли столь далеко что мы уже готовы изобретать собственное железо чтобы оптимально решать наш класс задач так например поступает Go мы планировали идти по пути купить компоненты и собрать собственную платформу но чтобы попасть в эту точку нам надо было набраться опыта и научиться сначала настраивать уже готовое железо и первый а да Почему именно мы решили строить свои кластера первое мы хотели Мы понимали что нзр революци трансформер с нами навсегда мален - это будущее мы должны иметь по нему экспертизу поэтому довольно логично научиться строить и планировать кластера для решения конкретно наших задач мы не планировали строить кластера для поиска внеземной жизни для решения астрофизических задач для решения задач химической физики Нет мы не планировали там искать свора белки или что-то подобное класс задач которые мы планировали решать Это именно обучение Трансформеров поэтому это как примерно подобрать нужный транспорт чем самолёт лучше корабля Смотря для чего корабль медленно идёт но много перевозит то есть соотношение производительность соотношение грузоподъёмность скорость для решения разных зада подходит по-разному именно в нашей ситуации подобрать хорошее соотношение компьют и скорость интерконнект супер важно и позволит нам экономить проценты возможно десятки процентов и на наших объёмах это вообще говоря миллион долларов вторая важная история после того как мы запланировали и допустим создали наш правильный кластер его надо эксплуатировать чтобы эксплуатировать и экономить на этом нужно поставить его где-то в дата-центре где дешёвая электроэнергия а довольно странно ставить его в Москве здесь Земля дорогая и так далее лучше построить его там где дешевле и мы сможем его эксплуатировать если мы понимаем как он работает вторая важная история - охлаждение все наши дата-центры современные работают на фрику это позволяет э экономить десятки процентов на охлаждении потому что кулинг работает Так мы собираем воздух снаружи Прокачиваем Через наш сервер и выбрасываем обратно нет никакого принудительного охлаждения воздуха через кондиционеры вот это вот снижение КПД максимально высокий КПД охлаждение позволяет нам экономить десятки про на охлаждение на наших объёмах кластер потребляет вот са потребления электроэнергии Это 0,7с для большого мегаватта для большого кластера если наше охлаждение сделано плохо то соотношение отвода тепла будет примерно единица это значит 1,4 мегаватта в наше потребление 1 м то есть мы намного эффективней с другой Роны ви когда производит свои сервера она вынуждена проектировать свои сервера под эксплуатацию в любых условиях там где пыльно там где плохое охлаждение и так далее И поэтому они вынуждены очень на на радиатор чтобы они не запыт огромное значение на самом деле на экономии То есть это экономия десятков тысяч долларов в месяц банально на охлаждение третье тоже супер важный поит это что происходит с вашим кластером когда он начинает морально и физически устаревать спустя несколько лет ги блё кластеров довольно быстро деградирует в том смысле в моральном потому что вы можете позволить себе купить новое железо которое будет быстрее а решать задачу Ну представьте 3 года спустя Вы можете купить себе новый кластер который будет обучать модели в два раза быстрее Это означает что обучить модель вы сможете не за 2 месяца А за месяц Это огромный бусте нужно придумать что же делать со старыми ПЛН кластерами более того ГПУ со временем начинают деградировать в том смысле что количество ошибок памяти на hbm ГПУ начинает расти довольно значительно и это становится большой проблемой для Вот таких пни кластеров которые работают как единая Представьте у вас один ГПУ ловит сбой Один раз в год в принципе это не проблема но если у вас работает по ГПУ как единое целое Это означает что ваш кластер будет ловить сбои 3-5 раз в день и в таком уже варианте обучать модель уже очень сложно Поэтому с какого-то момента лучше разобрать кластер и отдать его в иренс а задачи Ирен это как раз вот они затрагивают только одну реплику одна реплика один GPU если произошёл сбой мы просто перезагрузили ноду никакой проблемы Именно поэтому нам в какой-то момент нужно учиться в любом случае чтобы сэкономить и отдать GP так сказать на вторую очередь жизни надо научиться с ним работать и ровно Поэтому нам нужно учиться строить свои кластера мы начали с самого простого решили собрать кластер мы его назвали с освал а именно у наших НД инженеров под столами лежали какие-то сервера что мы сделали Наби теми ГПУ которые были под рукой набили их 81 это было самые быстрые карточки которые были под рукой это были Гигабит РОСИ ро X5 поставили включили и оказалось забавные вещи первое дох потом в основном все библиотеки используют ip4 протокол наши дата-центры уже несколько лет ой господи десятков лет уже живут в ip6 поэтому пришлось Дописать некоторые пачи для Open и так далее на самом деле все по опубликованы и ссылки есть в статье в которой ято не используем пото он нам не подходит на самом деле все большие компании че имет сво им потребовалось некоторые усилия чтобы засунуть эти GP В контейнер чтобы они корректно работали и самая большая проблема которую Мы заметили Когда уже всё было готово и собрали прототип а то что ничего не слится когда мы объединяем два узла в запуска мы не получаем ожидаемого линейного масштабирования в попытках понять что же пошло не так один из наших НД инженеров предложил очень простую но супер эффективную методологию которую назвали кардиограмма машинного обучения А вот здесь она изображена это кстати историческая картинка вот ровно С тех измерений тут скорости такие ещё совсем детские а именно мы строим на одном графике потребление энергии GPU и банс передачи информации по интерконнект и тут всё сразу становится Понятно синяя полка - это когда отдельный GPU считает свой кусочек после того как он закончил он должен обменяться посчитано градиентами с другими GPU вообще говоря Со всеми которые участвуют в обучении И как мы видим на это тратится примерно Половина времени Почему Потому что соотношение interconnect у нас подобрано неправильно и Максимум что мы могли получить Вот на этом кластере со свалки было масштабирование 50% это был хороший урок самое главное мы поняли где могут быть засады что-то тестировать на таком кластере уже было можно у наших - инженеров уже было что-то а мы начали проектировать что-то наконец-то приличное на чём можно уже обучать большие хорошие модели первый большой кластер который впоследствии назвали липунов опять же Это был уровень оди Мы купили готовое железо от инра с во А1 ГПУ и решили использовать интерконект ин в качестве интерконнект Почему inf на предыдущих докладах проч вы возможно слышали он порядка миллисекунд десятков миллисекунд вот когда мы имеем дело с вычислителя ГПУ нам уже нужны микросекунд Тен потому что скорость вычислителя настолько быстрая что это может обеспечить только infiniband или какие-то очень специально закрашенный Ethernet то есть понимаете скорость копирование по двухсот гигабитного ини банду - это 24 Гб в секунду И Н 1 микросекунда То есть это по факту равно копированию памяти внутри Хоста и дальше встал важный вопрос А сколько сети нужно добавить в один сервер можно было пойти стандартным путём и сделать так как это делает nidia ГПУ сетевых карт ум не хотели универсальный суперкомпьютер мы хотели чуть-чуть сэкономить и решили поэкспериментировать и соответственно попробовали 2 4 и 8 вот перед вами кардиограмма машинного обучения для четырёх Когда в сервере стоят 8 GPU и четыре сетевых карты и мы видим что на жл жёлтые ступеньке это вот как раз об данными они занимают ВМ совершенно немного времени количество сети бан сети в два раза то мы сократим вот этот вот пичок в два раза уже и в принципе это ничем нам не поможет Но если мы оставим соотношение 8 на 4 это позволит нам строить кластера в два раза большего размера потому что стандартная топология inf Band она ограничена 800 портами И самое главное это позволит нам строить кластера чуть дешевле потому что интерконект в кластерах машинного обучения составляет довольно большую сть стоимости дальше были огромные проблемы как мы пытались заставить выжать максимум перформанса просто банально тот который заявлен А все этапы я описал в статье которую чуть позже суперкомпьютеры Яндекса взгляд изнутри ссылка будет чуть позже Ну здесь я проведу лишь итог в какой-то момент После долгих ночных хоконов мы получили на 130 узлах рише узлах из теоретической пропускной способности в 96 на реальном nccl тесте который эмулировать Отдавать кластер инженерам чтобы они могли начать свои обучать свои модели либо начать тестировать для топ 500 как советовали нам инженеры из N види Мы решили что это слишком как-то расточительно И самое главное что кластер будет все новогодние праздники проста поэтому приняли решение отдавать кластер максимально откладываем замер топ 500 на попозже что стали делать люди из инфраструктуры из рнд они стали строить новый кластер после того как мы поняли что мы уже можем настроить готовое оборудование мы взяли уже готовую созданную в Яндексе CPU платформу Яндекс 4.0 добавили в не самые свежие на тот момен N GP же самое соотношение сети и это уже был полный серьёзный ф Сай кластер из 200 узлов и был ещё второй который был наполовину заполнен и наконец-то у нас Появилась возможность провести наконец-то полноценные тесты для топ-500 Казалось бы А что может пойти не так вот примерно эти ребята тоже об этом думали оказалось что примерно всё перед вами значит для попадания в топ 500 необходимо прогнать очень простой тест он называется современная его версия называется hpl по факту он просто решает задачи систему линейных уравнений перед вами два измерения на одном и том же кластере просто последовательны на самом деле вот здесь вот графики А мы по факту после прохождения видим только чисел производительность которую он показывает два измерения но заметьте они отличаются примерно на пе один флопс даёт для вычисления одного фпса требуется оборудование примерно 1 млн долларов по какой-то причине на нашем кластере расхождение девиации производительности отличается примерно на 1 лн долларов явно не то потом уже догадались смотреть на какие-то характеристики в ран тайме и вот перед вами графики это э график сети внутри обмена по сети интерконнект и видно Ну и очевидно что идеальный график - это когда у вас плоская э плоская фигура потому что производительность - это и есть фигура под графиком и видно что На плохом а тесте есть какие-то провалы вопрос А откуда они вообще взялись и здесь Самое главное надо вспомнить вообще-то мы решаем задачу распределённого алгоритма и мы все очень хорошо знаем что такое распределённый алгоритм дети которые передают друг другу мяч это и есть распределённый алгоритм и мы все хорошо знаем что произойдёт если девочка которая ловит сейчас мячик отвлечся мячик упадёт игра остановится и будет простой очевидно очевидно ровно тоже самое произойдет у нас на самом деле этот график ровно тот же самый который вы видите с детьми только это кардиограмма работы Линка под микроскопом что мы видим это график за о секунду Первое у нас за секунду происходит больше четырёх циклов более того данные передаются парал передаются параллельно с вычислением на GP Это означает что передача данных управляет процессор это важный шаг то есть процессор в наших вычислениях тоже важен не только ГПУ вторая важная часть мы видим когда вычисления в цикле на ГПУ закончились есть некоторая синхронная фаза выя закон Понятно примерно где были проблемы в случае детей поте мячика это задержка игры А в нашем случае перегрев одного единственного ГПУ из тысяч из полутора тысяч которые участвовали в нашем кластере означает что все остальные простаивают и ждут от него данных нам Соответственно что от на тремы дож были Нау находить такие перегрева пу пони что их надо и пото у на был тест просто что хорошо работают под нагрузкой Но то что они могут перегреваться под стандартное нагрузки У нас не было инк нам довольно явно намекнул что это надо делать вторая история помните вычисление CP важен когда мы говорим о переда данных помы говорим о микросекунд или у нас вовлекается Linux шедулер который должен залить таку которая должна за детектировать что обмен данными закончился мы теряем в перформансе поэтому только явное прибивать процессов к CPU исключает Linux из уравнения и мы получаем плю 15% которые мы раньше теряли ну третья довольно Простая история что любые число дробилки выигрывают ИС это дало нам 5-10 о которых мы как-то раньше не задумывались Ну и одна из забавных проблем К тому моменту один из кластеров у нас уже был примерно полгода в продакшене Но оказалось что НК смотрит на одно из устройств которое на самом деле не участвует в тесте но по нему он делает некоторые выводы и вме того чтобы падать Он работает на 10 мелене об этом даже не думали поэтому Когда вы что-то тестирует всегда надо сравнивать производительность Бари Metal контейнер После этого мы наконец-то стали готовы к большому тестированию Ну то есть все наработки были внедрены в первые два самые новые кластера старый мы не трогали потому что он уже в продакшене Мы в нём уверены всё и замеры показали что первые два новые кластера В общем показали довольно приличный перформанс а тот который уже по факту был в продакшене уже 9 месяцев показал какой-то мягко сказать невнятный перформанс после этого стало понятно что надо выводить этот кластер на эксперименты и понимать что же пошло не так и дальше выяснилось несколько простых вещей первое бак в интерконнект который мы давным-давно почини на новых стех адаптивный роутинг по факту был сломан и последний бак который был самый смешной на кластере было несколько GP на которых было ограничено программно ограничено потребление энергии в 350 ват Казалось бы в чём проблема Ну несколько GP работают там на 25% медленнее но мы имеем дело с раден арим Это означает что по факту весь кластер у нас работал как самое медленное GPU Значит на 25% медленней и после того как вот эти вещи были поправления из воздуха Мы просто не знали раньше об этих проблемах Это означает что мы из воздуха нашли оборудование примерно на 4 млн долларов более того было ещ И третье измерение когда мы сделали Второе измерение Мы заметили что график Банд Виса он всё равно ещё не является идеальным и попытки понять что же пошло не так выяснили что наши мониторинги которые опрашивают статус ГПУ они довольно сильно могут влиять на производительность и по факту они стоили нам 2% GPU производительность после того как мы ещ и починили ещ и мониторинги в общей сумме на самом деле мы нашли оборудование которое раньше недостаточно хорошо работало примерно на 6 млн долларов суммарная производительность наших кластеров которые по факту уже заявили в топ 500 составила в сумме 50 флопс С тех пор мы сильно продвинулись но пока об этом не настало время рассказывать дальше важный вопрос на который стоит ответить А как правильно строить а большие один большой кластер или несколько э среднего класса среднего размера кластеров но идентичных мы были в состоянии один большой кластер Когда у нас Вот был один липунов и проблема в том что когда у вас Один кластер он становится тотемом для поклонения с ним ничего нельзя сделать его нельзя вывести в тенс потому что иначе инженерам не будет на ЧМ считать не очевидный плюс его можно растить Но с ним нельзя ничего сделать он становится тотемом с несколькими же кластерами среднего размера мы всегда можем что-то сделать на самом деле это очень практичный Инженерный подход как строится электростанции вот здесь кажется Братская гэз электростанции строится не с одним огромным турбиной выдавать электроэнергию в сеть 24 на 7 365 дней в году Именно поэтому всегда можно одну турбину вывести на ремонт что-то с ней сделать какой-то эксперимент замену а пользователь продолжает получать электроэнергию ровно по этому принципу живёт сейчас наши кластера У нас есть идентичные кластера с идентичным жеод Ду и который распределяет на них нагрузку пользователи не знают На каком заранее не знают На каком кластере будет запущен их нагрузки им не надо у нас появляется наконец-то возможность проводить эксперименты ремонты апдейты вот буквально во вторник у нас будет апдейт одного из кластеров Окей мы наконец-то собрали отдали в про с эм обра не просто так мы это делали для наших пользователей и инженеры очень умные люди но переход вот в распределённые алгоритмы оказался для них в некотором смысле новый эрой потому что они учились хорошо делать модели А вот распределенным алгоритмам почему-то в университетах пока не ут Я думаю это скоро исправится ини задава во А что с моим обучением оно зависло или нет А мы как команда инфраструктуры пытались ответить на вопрос А насколько эффективно обучение использует ГПУ Может быть там что-то можно пооп Зро это была такая интересный вопрос значит на который мы долго-долго значит пытались ответить очень подробный рассказ на это вот в моей статье я Очень советую почитать на самом деле проблем много к сожалению обо всех о них рассказать невозможно в качестве завершения Я хочу перечислить важные правила эксплуатации построения GPU кластеров и то к чему каждый - инженер должен быть готов первое распределённое обучение - Это новая норма распределённое обучение означает использование распределённых алгоритмов распределённые алгоритмы являются самой сложно областью Коте S Если вы собираетесь что-то обучать огромной модели пожалуйста уделите некоторое время каким-то основам это сильно сэкономит ваше время и деньги компании второе Когда в обучении задействованы сотни ГПУ тысячи даже ГПУ и тысячи сетевых карт отказы железа неизбежны мы должны уметь деть и обнажить на любом этапе обучение которое длится месяцами должно быть готово к отказу одного-единственного узла это должно происходить быстро максимально быстро и перезапуск задачи должен происходить максимально быстро иначе вы будете терять во времени только постоянный сэмплинг ML задачи гарантирует что вы не потеряете время Когда ваша задача зависла может быть много вариантов Почему ML задача зависла на самом деле большинство проблем я описал в статьях и дальше важные вещи вот которые граблины которые Лично мы набили первое традиционные способы отладки вот на таких объёмах когда один день простоя такого кластера стоит 20.000 долларов и если вы столкнулись с какой-то серьёзной проблемой мы Все мы привыкли что какую-то сложную проблему можно решать допустим день ой день или неделю Ну хорошая сложная задача неделю решается если мы занимаемся какой-то проблемой неделю Это означает что компания теряет сотни тысяч долларов это неприемлемо надо учить быстро максимально быстро самое главное заранее предвидеть проблемы вторая важная проблема урок который мы выучили размер кластера не должен превышать ту экспертизу которая у вас уже есть в какой-то момент размер кластера обог понимание наш экспертизу и мы теряли в темпе наши кластера простаивать слава Богу Мы научились это решать и сейчас когда мы делаем какие-то Новые эксперименты мы сокращаем размер кластера чтобы время простое его не было столь значительно и мы могли зарабатывать тот экспириенс который нам нужно работать на гораздо меньших кластерах всё Огромное спасибо если у вас какие-то останутся ещ вопросы можно потом меня будет поймать на экспертной зоне у Яндекса всё большое спасибо А что экспертный мы тебя и здесь ещё вопросами попытаем Давайте Если есть вопросы у нас есть время на несколько вопросов Мы с Катей кстати вот те самые пользователи про которых тут рассказывали очень благодарные да Ну что поднимайте вот всё вижу приветствую Спасибо за хороший доклад ВС понятно у меня такой вопрос скорее на будущее планируете ли вы аналоговые процессоры использовать специально разработаны например для тех жениных сетей смотри в этом вся и штука что вот для кажется что пока нет для них ниши в пнг вот кластера строятся в основном всё-таки для пнин и пока использование их в нинг не очевидно возможно какой-то возможно Вот мне кажется что да я про иренс как раз как раз про иренс возможно Ну конкретных планов мне неизвестно Доброго дня Андрей Зубков е враз классный доклад люблю про железки послушать Подскажите какую-то интерконнект варианты кроме ифни Бенда рассматривали потому что ну микросекунда честно говоря даже сейчас уже звучит не очень мало как бы тот же самый HP там заявлял то что у них там типа к пико секундам к наносекунда уже сход А смотри ну всё-таки здесь важен баланс если мы говорим о латен микросекунды Зачем нам пикосекундный латен если скорость копирования из GPU в сеть это всё равно микросекунды то есть вложив гораздо больше денег мы не получим никакого перформанса Ну и самое главное пока размер кластеров не настолько огромный чтобы мы могли изобретать собственные интерконнект А 99% кластеров в топ 500 работают на infiniband и довольно логично двигаться в этом фарватере Да здравствуйте Никита хотел узнать Вы пробовали тестировать Ну не вот пу 100 А вот что это из Китая Вот какие их процессоры Да мы смотрели Значит на эти GPU там довольно интересные результаты там на самом деле самый большой проблема китайских производителей Мне кажется это отсутствие такой хорошей документации вот NVIDIA они Молодцы они сделали Всё очень красиво его просто берёшь и пользуешься с Китаем чуть сложнее было и собственно поэтому видимо приоритет и было был отдан на первом этапе NVIDIA А куда это заведёт в будущем у нас осталось время на последний вопрос А спасибо за доклад А здесь А расскажите пожалуйста что как устроен сторедж Чтобы кормить этого монстра сторедж хранилка сторедж А да конечно смотрите А у нас есть такая А да Я неужели об этом неза так смотрите у нас есть довольно приличная ри система называется ить она Собственно как хранилка данных как оркестратор запуск джебов То есть она вот сочетает в себе две вот эти роли данные скачиваются через и неким прозрачным способом по сети Сеть не быстрая там всего 100 гиби Ну как показывает практика в принципе а больше и не надо Да но весьма Возможно это поменятся то есть есть некоторые реплицировать из которого вот скачиваются данные"
}