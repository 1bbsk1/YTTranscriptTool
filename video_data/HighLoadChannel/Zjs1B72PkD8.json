{
  "video_id": "Zjs1B72PkD8",
  "channel": "HighLoadChannel",
  "title": "Как снять бэкап в распределенной системе, чтобы этого никто не заметил / Иван Раков (GridGain)",
  "views": 1118,
  "duration": 3379,
  "published": "2019-05-15T04:53:51-07:00",
  "text": "всем привет меня зовут иван раков я из компании game и сегодня я расскажу вам как снять backup распределенной системе чтобы этого никто не заметил начнем с небольшого интро в рамках которой какой системы мы вообще все это реализовали apache знает это open source распределенной особо да и платформа для вычисления каширования она может выступать как в роли самостоятельных субд и так и в роли каша для какого-то стороннего ставят по this to reach из ключевых приятных особенностей системы подсчитано это я бы отметил простое позиционирование а именно данные распыляется попортится благодаря так называемый стоит ли с affinity функции и если вы решили по скерри ции поменять топологию кластером то эти партийцы с данными будут переезжать причем делать это фоне это не будет блокировать пользую нагрузку и соответственно любые операции могут происходить одновременно с ребаланса выгоняет есть полноценной распределенной транзакции то есть все честно все транзакционные гарантии и все уровни изоляции это особенно критично для использования нашей системы в каких-то финансовых сегментах также выгоняет есть такая фича как эфенди colocation кратко говоря это умение направлять вычисление близко к данным предположим вы парте циане route ваши данные по какому-то принципу ну вот классический пример это если у вас есть база со щитами и вы хотите повернуть какую-то аналитику к это вычисление для конкретного города вы просто пользуетесь встроенным в патче денаит методом мафии чоран или affinity кол и вы совершенно не думайте о том что ваш запрос найдет данные просто кожа сама пошляться на нужный узел и исполнит нужно наличку на рядом с нужной партиций глен game это решение на основе apache игнайт для промышленных энтерпрайз проектов в принципе почти весь функционал если патчи гноится a great game предоставляет некоторые фичи характерные именно для бизнеса ну например в great game есть дата центры припишем есть security и в частности есть бэкапы компания green systems это основной контрибьютором source проект apache игнайт но единственный примерно половина комиссаров в пачек на это сотрудники grid games гареггин является с 0 движущей силой но не единственный другие люди и другие компании тоже вносят существенный вклад в проект apache знает пару слов о себе я старший разработчик в bergen systems я также к meter в проекте apache игнайт и я непосредственно участвовала реализация бэкапов гением о чем я вам сегодня и расскажу о чем у нас пойдет сегодня речь начнем с того что я опишу проблематику снятие бэкапов распыленных системах объясню почему это не так тривиально поговорим про наивное решение которое про котят в любой распыленной системе где сперси stance также приблизимся к тому как нам снимать бэкапы под нагрузкой для этого нам придется добиться сначала локальной can системности потом глобальной консистентная поговорим о том как можно экономить место благодаря использованию инка ментальных бэкапов затронем вопрос чисто операционный что делать если у вас топологии кластер меняется бак и бока был снят давно и как старый backup снятый на старый то пологи натянуть на современную уже новый то пологом и в конце затронем вопрос можно ли вообще снять backup совсем не останавливая нагрузку и обсудим подход как можно к этому приблизиться начнем с проблематике что же не так с боковыми распыленных систем во первых это характерно в целом для потапов данные постоянно обновляются в системе которые вы хотите забекапить и когда вы снимаете backup вы хотите чтобы состояние бака бака пи было зафиксировано чтобы она представляла состояние системы на какой-то конкретно timestamp данных может быть очень много что я под этим понимаю когда люди используют распыленной системы они это делают по какой-то из двух причин либо по обеим сразу либо слишком большая нагрузка с которой не способна справиться одна физическая машина либо слишком много данных которые эффективно нельзя расположить в одной машине и и писать как это назад я котят просто нужно понимать что проблематика снять пока папа нагрузка становится еще более актуальной потому что бэкап с большим количеством данных и снимать в любом случае банально долго в распределенных системах данные реплицирования что я поднимаю здесь по трипле кации это естественный механизм защиты распределенных систем от потери данных а именно хранения их нескольких копиях выгнать данные хранятся в partition через конфигурации вы можете задать сколько копий каждой партиции вы хотите видеть ваши распыленной системе и когда мы снимаем пока мы хотим чтобы данной были консистентной между копиями чтобы состояние данных каждый копий в бэкапе было одинаковым здесь добавляются еще и транзакционный гранте как я уже чуть раньше сказал выгнать есть полноценный механизм транзакций если бизнес используя транзакции он как правило делает это не просто так он хочет благодаря транзакциям обеспечить какие-то варианты простой пример это вам трансфер между счетами если мы переводим деньги с одного насчет она друг с одной счета на другой нам хочется чтобы общая сумма денег осталась такой же соответственно если мы снимаем backup мы хотим добиться того чтобы и в бэкапе сумма денег равнялась ожидаемый общей сумме денег поговорим про то как снимать бэкапы с вместе с какими-то конфликтующими deezer актив изменениями в кластере пример мы вполне имеем право поменять схему базы данных например добавить новую таблицу и нам хочется чтобы такие изменения попадали в backup атомарном то есть если мы снимаем бэкап и одновременно создаем новую таблицу то хочется чтобы либо портится lip либо таблица целиком попала бы капли бы и не было вообще нигде ни на одном узле то есть такие события нужно порядочно ти я поговорю о том как это сделано в нашей системе также я уже упомянул что-то пологи кластер а может меняться и нам нужно уметь работать с богатыми снятыми на любой старой версии топологии в принципе все давайте для начала рассмотрим существующие подходы в известных распыленных системах как они подходят к снятию бэкапы часть вендоров не париться по этому поводу она предлагает сделать close the shadow либо там выключить кластер совсем либо заморозить всю поле пользовательскую активность и так или иначе за пока пить данные которые автоматически будут консистентными так как они время снятия пока применяются подход имеет право на жизнь но есть существенное ограничение что нам придется выключить наш кластер и не предоставлять сервис какое-то время не всегда мы готовы поступиться такими ограничениями поэтому нужно посмотрите другие подходы тоже некоторые системы например apache cassandra предлагают делать уменьшили consist of backup что под этим понимается кассандра если вы сделаете backup она не гарантирует что данные во всех копиях будут одинаковы но когда-нибудь они станут одинаковыми благодаря встроенному в кассандра механизм repairs то есть рано или поздно данные все-таки сойдутся и состоянии станет снова консистентной какой у такого подхода есть минус если мы можем благодарить обеспечить консистентной в плане одного плеча то нам совершенно непонятно как нам выполнить те продукционных варианты которые хочет достичь пользователь благодаря используем транзакции стоит заметить что в кассандре на самом деле есть механик транзакций но там так называемый lightweight транзак шанс они позволяют а там арно менять один ролл то есть принципе это эквивалентно comprar цвету понятно что традиционно целостность задаваемым такими гарантиями выполнится благодаря меньше ли концы сам покопайся транзакции более сложные эффекты от несколько ключей несколько таблиц и так далее то тут уже непонятно как нам достичь как системности ну будем честны транзакционный целостность никто не заявлялось как основное преимущество таких систем как apache cassandra традиционно санс это что-то из мира субд м.и. и вот конкретно есть такая распределенная сумма дкк крови и она предлагает снимать полноценное инкрементальный консистентной и так далее пока по благодаря такому подходу как би си хим кто слышал что такое и сиссе кей на самом деле это тема для отдельного доклада но кратко я скажу что это подход в базах данных который позволяет за счет хранения дополнительных версий увеличить i can карен силы в ваших операций то есть при определенных обстоятельств в течение записи не будет блокировать друг друга если вы используете эм би си 7 и мехи позволяет вам получить рид consistent view вашей ваших данных то есть если у вас есть им бы сиси у вас есть глобальная version ность у каждой транзакции есть версия все версии упорядочен и соответственно чтобы вам получить традиционно целостными просто фиксируйте какую-то версию интегрируясь по вашим данным и отбрасываете все что за конечно с более поздней версии подход в целом имеет право на жизнь но для того чтобы нам его использовать тут было два ограничения во первых для этого необходимо что вашей системе была миссис м конкретного патче играет нами сиси уже планируется к релизу довольно скоро это будет ближайшем релизе apache и гнать 27 но выкопан им понадобились раньше помимо этого если вы будете снимать бэкап через механизм там висел у вас будет так или иначе overhead либо по памяти либо по временем вы можете поступить двумя способами либо снимать backup методом простого копирования всех файлов и потом получение конкретного состояния уже постфактум при восстановлении благодаря механизмам би си си но тогда вам придется скопировать возможно и старые версии тоже примеси мы храним несколько версий они по-разному горит мы в фоне чистятся удаляются но все равно муки будет на которой вверх от если вы не хотите оверхеды по памяти вы можете зафиксировать би си си версию и проецироваться по вашим данным но такой подход будет работать дольше потому что очевидно что простое вот балка копирование всех ваших данных на уровне в страниц или файлов работает быстрее чем если вы будете дополнять какой-то дополнительный код для анализа в рамках копированием ok начнем с простого как можно сделать бэкап а попросту деактивировав кластер в принципе это то что я описывал в секции close the shadow представьте что у вас есть кластер но вот на примере патчи играет есть узлы и узлы хранят данные данные реплицирования и данные хранится в хранятся в partition то есть вот эти циферки нам 1674 для узла 4 это номера портится с данными по определению персистенции системы эта система может восстановить свое состояние с диска если на красницы что мы можем сделать мы можем отдать команду на деактивацию системы в патче игнайт есть такой механизм благодаря которому все польские пользовательские изменения будут приостановлены рано или поздно и все накопившиеся накопившийся изменения в памяти они будут сброшены диск таким образом на образом на диске мы получим консистентными состояниям все что нам остается это после деактивации взять наш persistent storage взять наш persistent storage и скопировать его в какую-то отдельную backup директорию собственного и в этой директории как раз будет жить наш backup соответственно чтобы восстановиться из снятого пока снятого таким образом пока по нам достаточно будет провернуть обратную операцию и просто положить отложенное состоянии обратно в дисковый сторож подход в принципе простой и понятный но у него есть существенный минус это downtime и как я уже сказал мы не всегда рам который остановить сервис на какое-то время пойдем дальше подумаем о том как нам достичь локальной консистентной sti не останавливая сервис под пользовательской нагрузкой когда я говорю под локальной консистентную когда говорил кальной консистентной что я имею ввиду давайте поясню во первых не хочется чтобы на диске была какая-то каша какие-то кусочки частично записанных данных какие-то страницы у которых не сходятся сердцем и хочется чтобы сложной операции попадали туда атомарном то есть предположим нам пришел апдейт который меняет много данных сразу несколько страниц может даже несколько несколько таблиц и хочется чтобы такой падает попал в локальный консистентной backup либо целиком либо мне попал туда совсем давайте проговорим как разной системы подходят к получению локальной консистентной степок 1 например редис предлагает делать бэкап через вызов методов work через системный вызов как это работает мы вызываем форк у нас for cats и процесс процесса форма точно такое же состояние памяти как и у оригинального процесса операционная система нам обеспечивает что если мы будем отдавать память в любом из двух процессов близнецов то эти изменения не отразятся в другом процессе какие тут есть ограничения во первых для того чтобы сделать форк вам требуется остановить все ваши потоки потому что если у вас есть потоки которые считают какие-то вычисления их состояние не гарантируются форк неком процессе вы можете какие-то инструкции выполнить заново и так далее во вторых сам вызов форк то есть понятно у нас уже есть необходимость чтоб за ворота на нем этого сам вызов fark работает довольно долго то есть ему самому необходимым стоп за ворот чтобы операционная система привела состоянии нужных структур данных в правильное состояние порядок этого стоп заворот инициированного операционной системы примерно от 10 до 100 миллисекунд на гигабайт памяти вашем процессе это значит что если у вас есть процесс уел со 100 гигабайт памяти вы заморозите примерно на секунду у нас есть deployment и где на одном узле 768 гигабайт оперативки поэтому такой подход нам к нам не совсем применим ну и кроме этого есть неконтролируемый memory вверх от если вы будете апдейт странице форма нам процессе операционной системы будет разруливать это через копи он райт будет выделяться дополнительная память и соответственно мы не можем оценить сколько памяти будет за вас и раваны в худшем случае это будет вся память которая доступна процессу и вы можете легко получить allocation феллер если вы используете там больше 50-ти процентов физической оперативной памяти современные реляционные базы данных использует для получения локального консистентной вокальные консистентной состояния подход фазе check point и анализ ряду логов и возможно анпилогов кто слышал что такое чекпоинт в контексте баз данных окей фазе чекпоинт это продвинутый алгоритм мультик пойнтинга чек pointing это вообще процедура сброса накопившийся изменений в базе на диск то есть когда какие-то пользовательские транзакции опадают тут что-то в базе они не делают это синхронно на диск сразу же потому что тогда мы получим довольно большой light in se а ее операции могут вставать в очередь и ждать друг друга и так далее то есть показатель относи будут совсем другие вместо этого мы накапливаем изменения а потом по какому-то признаку либо по достаточному количеству изменение либо по тайм аутом эти страницы дампом диск фазе чекпоинт это подход который позволяет сделать это совсем без остановки любой политической нагрузки кроме этого то есть благодаря facebook понту вы делаете этот самый фазе чекпоинты на диске у вас оказывается там позитивное состояние которого вы хотите помимо этого для достижения традиционной именно целостности базы анализирует в рай то hotlog где-то известный как ледок иногда есть и онду лак с обратными операциями то есть он не анализирует смотрят какие транзакции в каком состоянии приводятся базука протекционной целостному состоянию этот подход тоже нам не подошел во-первых фазе чекпоинт реализовать чуть сложнее не в том смысле в смысле что сам алгоритм сложные а в том что ваши структуры данных нужно привести к определенному состоянию выполнить там определенные варианты чтобы можно было под нагрузкой в любой момент времени снимайте snapshot и чтобы этот snapshot состоянии был восстановим как системного состояния . зачем point это такой более передовой алгоритм но он необходим в современных колон ночных реляционных базах данных мы конкретно пользуемся механизм механизмом более классическим он называется sharp чекпоинт челленджи которые стоят перед распыленной системой они чуть отличаются от челленджи которые стоят перед реляционными субд м вот мы достигаем хорошей производительности благодаря масштабирование за счет увеличения количества узлов горизонтального масштабирования поэтому мы можем позволить заморозить для шапочек поинта локально нагрузку на какой-то очень короткий промежуток времени давайте вкратце пробежимся по механизму sharp чекпоинт и я покажу как он работает и как он получает целостное состояние немножко поговорим про архитектуру стороны же игнайт выгнать используется печь memory есть страницы фиксированного размера и они хранятся всегда на диске если мы используем persistent и опциональных памяти то есть вы можете загрузить в один узел сколько угодно данных и какой-то горячий sap cis страниц будет поддерживаться в памяти ну вот там будет столько страниц сколько вы скажете сколько вы выделить памяти под ваш процессу соответственно когда приходят какие-то политические изменения они делают страницы грязными мы запоминаем дети флакку страниц и накапливаем эти грязной страницы в память когда мы решаем делать check point мы просто берем и сбрасываем накопившиеся грязной страницы на дисков какие тут есть сразу вопросы во первых если мы просто берем и там прям страницы на диск непонятно как обеспечить от омар ность есть по датам от ностью как раз понимаю сложную операцию на картинке сентри апдейт который завтра затронул как минимум 2 страницы и нам хочется чтобы вот апдейт этих двух страниц либо попала в чекпоинт целиком либо не попался всем помимо этого непонятно что делать с конкурентными обновлениями а именно предположим что у нас есть эти страницы которую мы еще не записали в check point и приходит полис к изменение которое хочет ее перезаписать что делать в этой ситуации как в любом случае нам нельзя замораживать попользоваться кий поток потому что чекпоинта не завершился нам давно как этот конфликт разрешить для получения от омар насти мы используем подход чек по и 32 клок кто слышал что такое ретрит лак предварить лоб наверно это что-то в преимущественно из мира дрова в нативных языках этот паттерн известен как шарят и эксклюзив лак в том что мы берем для апдейтов шары клок или ритлок а когда нам нужно сделать чек пойнт мы берем в рай флаг и фиксируем коллекцию грязных страниц то есть подварить лаком мы осуществляем очень мало работы мы там ничего само собой не копируем мы просто фиксируем имеющиеся у нас грязной странице вы дополнительно складываем в конкурентка шмап условно говоря и просто подобрать лаком с лапами странице то есть тут есть это может звучать странно что мы берем реблог для того чтобы делать апдейты но в целом никакого противоречия на самом деле нету окей мы зафиксировали коллекцию грязных страниц и есть поток чуть pointer который просто пробегается по эта коллекция и пишет странице непосредственного диска если придет какое-то пользовательское изменение которое захочет по когда эти страницы поставить дети флаг это изменение не попадет в чекпоинт благодарю благодаря тому что у нас уже зафиксировано коллекция грязных стремится более сложный кейс когда приходит пользовательский поток который хочет подойти грязную страницу которая должна попасть чик поэтому еще не попала в таком случае мы применяем паттерн копи он райт мы ассоциируем небольшой дополнительный регион памяти который у нас называется чекпоинт буфер соответственно как только польский поток приходит и хочет что-то какой-то такой страницы пообедать он берет и сначала копирует ее состояние на момент чек-поинтов чекпоинт буфер и просто перетирает это состояние уже на месте самой странице нужным ей состоянием таким образом нас стали то страница разделяется на две есть старая версия которая нужна для чекпоинта я снова версия которая нужно пользователям он буфер как динамическая структура она растет когда позиций access с грязной странице попавших check point и она постепенно очищается в процессе чекпоинта поток чик pointer идет сначала по обычным страницам потом потёк а то по тем которые попали в плен буферы пишет их тоже соответственно в конце чекпоинта check point буфера кажется пуст кей почему только что научились мы умеем писать грязный страницы на диск и не просто писать с определенными гарантиями мы сохраняем консистентной в рамках сложных операций благодаря редгрейв локинга и мы можем это делать одновременно с конкурентной нагрузкой благодаря использованию паттерн окопе он right как я уже сказал точек по in trade lock берется на минимальное время нужно для того чтобы просто слабость коллекция а после этого мы вполне можем жить и одновременно писать чекпоинты разрешать пользователю дает стремится на самом деле мы ещё не достигли цели сделать локальный консистентной backup потому что backup предполагает наличие всех страниц а в рамках чек поэтому мы записали только грязной станицы и вопрос который перед нами стоит можно ли научиться писать все страницы с такими же гарантиями ответ конечно же можно и сейчас я расскажу как для этого мы заведем специальный чип и а тип чекпоинта который назовем backup чекпоинт это не термин из какой-то профессиональной литературы просто я так его буду называть когда буду там несколько раз на него ссылаться дальнейшем при backup чекпоинт мы пишем страницы в два места сразу мы пишем их на disk storage как мы делаем рамках обычного чекпоинта и мы пишем странице заодно и в backup storage backup storage это то место где в итоге окажется наш backup именно этим объясняется то что нагрузка проседает если мы делаем backup то есть чекпоинт ну то есть чем измеряется труппе системной системы которая применяет какие-то принимают какие-то обновления он в принципе определяется трубку там диска и сейчас видно что в трубку диска сократится в худшем случае в два раза потому что поток чик pointer который изменения которые чувствуют привнес дам пред на диск у него в худшем случае в два раза больше работы он пишет страницы в 2 места в итоге после конца вот этого backup чекпоинта у нас пока пи окажутся все грязной страницы но на самом деле это не все что нам нужно у нас остались еще очень страницы которые были чистыми на момент чек-поинтов с ними тоже нужно что то делать сделать для этого мы просто пробежимся по диску и запишем остальные страницы у нас есть зафиксированная коллекция грязных странице поэтому легко понять какие страницы нам у нас остались это не так просто потому что тут тоже есть фактор с конкурирующими обновлениями а именно окей мы начали наш backup чекпоинт он закончился мы готовы бежать по диску и долго-долго писать наш backup но после этого в системе могут случится следующее чекпоинты которые точно также захотят по обновлять диска что нам с этим делать и тут у вас может постигнуть чувство дежавю потому что этот вопрос на самом деле уже обсуждали и с проблемой конкурирующих обновлений мы уже умеем бороться используем просто копия нравится снова теперь у нас есть поток который делает бэкап и у него есть страницы которые были чистыми которые все еще нужно записать backup storage то есть вот в backup storage уже что-то записано на там есть дырки это именно те страницы которые были чистыми и предположим приходит следующий check point который хочет перезаписать страницу и соответственно у нас потерялась бы старые состоянии страницу нужна нам для бекапа решим эту проблему примерно таким же образом нагрузим поток который делать следующим дополнительной работой заставим его сначала записать старое состоянии страницы в backup таким образом благодаря кооперативной многозадачности то есть чекпоинт поток будет помогать snapshot потоку мы придем к тому что рано или поздно запишем все страницы и зафиксированные именно на момент начала backup чекпоинта то есть мы просто взяли и перри использовали механизм в копи он в райт который мы уже сформировали для чек-поинтов у нас раньше были чтения памяти и запись на диск в рамках чекпоинта конкурирующим фактором были и политические изменения данных и мы использовали check point буфер для решения вопроса копий on right теперь же в рамках пока по мы читаем диск и пишем в backup конкурирующим фактором являются уже следующий чекпоинты решаем этот конкурирующей фактор теперь не из использовать дополнительный память с использования пера тивной многозадачности просто нагружает check point потоки дополнительной нагрузкой кей у нас уже есть на руках механизм именно наши backup check point благодаря которому мы можем взять и безопасно сбросить локально консистентной на диск локальные конкретное состояние на диск но мы все еще не приблизились к проблеме распределенного качественного пока по потому что мы еще не достигли глобальной консистентной тут стоит немного помечтать и подумать как вообще это возможно если бы хотя бы на одну миллисекунду нам удалось сделать closure к системным приостановить все транзакции получить какое-то окно в рамках которого все будет ok тогда мы могли бы взять в это окно и инициировать backup check point на всех узлах и таким образом мы медленно решили бы решили бы нашу задачу результат был бы глобальный консистентной backup консистентная в распыленных системах достигается не так просто приведем такой вот утрированный пример из жизни типичных распределенных систем предположим есть два конкурирующих администратора и они хотят сделать разные действия 1 считают что нужно добавить новую таблицу 2 считает что пришло время сделать бэкап и они сообщают это каким-то узлам и узлы начинают глобальный процесс кластере один по созданию таблицы 2 по созданию backup а проблема в том что узлы могут получать эти команды в разном порядке и на каком-то уже давно таблица попадет backup на каком-то нет ну для каноничности еще какой-то узел может взять окрашиваться чтобы усложнить задачу еще больше соответственно в процессе мы получим всякой там с который непонятно как рисуете такое поведение распыленных систем можно обозначить вот такой вот картинкой и какую мораль можно извлечь из этой басни что если изощряться можно разрулить вот эти фолловер сценарии параллельных параллельного исполнения глобальных изменений но будет проще если мы возьмем и упорядочим такие глобальные события обсудим то как это сделано вы гноится выгоняет есть встроенный механизм консенсуса и благодаря которому в рамках топологии узлы всегда упорядочиваются в кольцо они упорядочены по времени появления в конце соответственно становится просто достигнуть консенсуса по тому кто является координатором можно просто взять и назначен координатором самый старый узел воспроизведем снова нашу ситуацию если два конкурирующих админа хотят сделать два конкурирующих изменения то мы просто будем отсылать сообщения об этих изменений по кольцу и в каком-то порядке они дойдут до координатор оон их права лидирует и порядок прихода этих сообщений на координаторы и будет порядком исполнение этих глобальных событий в кластере координатор уже решит что сначала мы создаем новую таблицу потом уже сделаем backup отправят эти два сообщения по кольцу и так как по кольцу в кольце сообщений не могут обгонять друг друга то мы получим ситуацию когда все узлы сначала создают новую таблицу а потом уже сделать бэкап окей как нам вот этот механизм консенсусе кольцо поможет построить глобальное конфетное окном снова используем паттерн ритлок любые пользовательские транзакции будут вынуждены брать это паладжи рядок при отправке сообщения по кольцу для построения глобальных консистентной окна мы ум на каждом узле асинхронно будем брать топор и жрать лог соответственно как только узел успешно берет то положив ритлок он отсылает координатору и с этим нет проблем потому что есть консенсус кто является координатором слоя ткани отару сообщение о том что лук успешно взят координатор при получении всех сообщений всем отвечает говорит что все окей снимайте это положить локон таким образом у нас есть то самое окно которое мы хотели достичь и если в рамках этого окна запустить backup check point то будет все хорошо и мы получим глобальный консистентной backup подходящий момент с точки зрения реализации это после взятия врать лог но до отправки а как координатору на каждом узле не целовать backup чекпоинт поговорим о том как можно экономить место обычно бэкапы делаются по расписанию например раз в день и данные обычно делятся на операционный сет исторический сет операционный меняется часто исторически не меняется практически никогда вопрос зачем каждый день бэкапить данные которые не меняются на самом деле этого можно избежать наивным решением является при записи странице backup сравнивание ее с предыдущим бэкапом и только если странице отличается мы будем ее писать этот подход не лишён недостатков потому что нам потребуется дополнительный aiop на чтение предыдущего bacopa то есть звучит она у нас изменилась только 10 процентов страниц казалось бы мы должны записать только 10 процентов страницы и потратить только 10 процентов ooops от обычного пока по но в итоге мы потратим сто процентов потому что 90 процентов мы все равно вычитаем из прошлого bacopa вместо этого стоят эти изменения накапливать для этого мы в рамках пейдж memory завели специальный тип страниц который называется tracking странице tracking страница всегда соответственно какой-то диапазон любых других страниц и вне хранятся битвой маске единичка если станет изменилось с прошлого пока п 0 если страница не изменилось вопрос как одновременно снимать бэкап и копить флаги для нового потому что если мы решили снять инкрементальный backup мы используем маску и не можем ее апдейт а политики изменения уже вполне могут прийти и захотеть поменять какой-то бить их для нового bacopa но мы не можем позволить им это сделать потому что нам нужна вся вот эта маска в первозданном состоянии для этого мы поступили довольно просто мы храним два набора масок для следующего и предыдущего bacopa и как только приходит какой-то пользовательское изменение она отдай титбит в маске под названием next и если мы снимаем backup то мы меняем маски между собой то есть не значит что мы продвигаемся покажет ракетницы реально изменяем его просто храним глобальный так который отвечает за то какая половина право и лево или левый отвечает за следующий backup при этом торта что стало новой половиной она лениво обнуляется то есть вот мы условно говоря запомнили что нам нужно эту маску обновить только придет 1 позе дискет дейт мы сразу же обнулим маску целиком ну поставить соответственно один бит на место соответствующему первому апдейт благодаря этому подходу мы научились писать инкрементальные бэкапы которые реально занимают на порядок меньше места и вся кто там выживают на порядок меньше и abs при снятии немного поговорим о том что делать если топология меняется представим такую вот историю что есть кластер есть какое-то распределение партиций по нему и мы сняли backup прошло два месяца с кластером произошли определенные события один узел сгорел другой мы там вазелин и maintenance место этого вильи два новых узлам и распределение партиции уже поменялось и соответственно не понятно что как нам из вот этого всего собрать backup который можно восстановить на новую топологию для этого мы реализовали команду которая позволяет удобно перемещать бэкапы в удаленное сетевое хранилище в так называемый трансфер или shared folder это удобный по другой причине что дисковые ресурсы на узле ограниченным и если мы будем снимать бэкап и каждый день кто рано или поздно неизбежно закончится место соответственно backup перемещается в общее хранилище и если нам нужно вот уже в этой изменившейся ситуации восстановиться мы будем восстанавливать данные по текущему распылению партиции то есть partition это в целом такая минимальная единица переезда информации между узлами и каждый узел знает распыление против новой топологии в метаданных bacopa в самом старриджа известно в какой узел хранил какую партицию в момент снятия соответственно мы просто этом это дату прочитаем и начнем копировать партиции возможно из бака в директории чужих узлов ничего страшного если все партиции хотя бы в одном экземпляре присутствует бэкапе то мы все партийцы найдем и восстановим все будет хорошо тут есть оговорочка что такой подход действительно позволяет восстановиться на любую топологию но у него есть и недостатки тоже он отлично работает если все сущности вашей системе привязаны к partition но вот конкретно выгадает искали индексы они по разным соображениям строятся не по партиям а по узлу целиком то есть и сколь индекса там условно говоря b плюс деревом где есть ссылки на данные у всех локальных партиций и если мы будем восстанавливать backup is shared folder и партиции перемешаются то сколь индексы так или иначе под кнут соответственно ограничением лет является следующей если у нас изменилась топология и изменилось распыление партиции то сколь индексы придется перестроить после восстановления бекапа то есть система будет функционировать в таком ограниченном режиме там не смогут эффективно исполняться сколь запросы но мы все еще можем делать обычные где его или операции по примеру когда-нибудь bacopa синхронно с кои линдакса синхронно до строится и система снова войдет в штатный режим работы теперь в качестве завершение поговорим о том можно ли вообще не остановить нагрузку в принципе уже описал механизм благодаря которому мы умеем строить качественный backup но тут тоже стоит помечтать представим что если бы можно было построить под нагрузкой консистентной срез транзакций тогда мы просто могли бы сделать бэкап чекпоинт на ситха асинхронно на каждом узле построить этот самый консистентной срез и доложить разницу is bright logo срез минус по капчи point для каждой партиций и сохранить вместе с бэкапом остается только понять как работает консистентной средства как а вообще можно сделать пару слов про транзакции в гноится выдано id используется протокол туфа из кометами это значит что узел инициаторы транзакции сначала рассылает припер и квесты потом получает по 1 спонсор например мы о чем ключи которые часто в транзакции после этого узел инициатор подсылает commit request a на комете мы уже реально пишем изменение в сторож вы в рай то hotlog и после этого узлы отвечают и управление возвращается польском потоку который инициировал транзакцию в рамках построение глобальной консистентной окна я уже описал что мы умеем инициировать и координировать глобальные события и в принципе мы вот этот цикл акс responses можно повторять мы можем с координатора не завершать операцию повторять этот вот обмен сообщениями и это можно делать n 1 что хорошо это то что между всеми между всеми stay джами будет отношения hoping for то есть все выполнения стоит же один они hoping for всех выполнение 100 g2 и так далее то есть вот написали такой вот мини фреймворк для распыленного конечного автомата где там для каждого узла можно написать логику что будет на каждом переходе и описать логику перехода и состоянием состоянием того чтобы получить консистенции срез мы на первом этапе им вот такой вот многошаговых горит мы инициируем backup check point на втором этапе мы фиксируем сет препарат транзакции и дожидаемся их завершения то есть это абсолютно ничего не блокирует если появятся какие-то новые препарат транзакции то они вполне смогут остаться то есть их можно ждать уже не будем то есть могут зафиксировали вот такой вот сердце и на этапе 2 мы дожидаемся завершения всех prepared транзакций как только все зафиксированные препарат транзакции завершились мы делаем этап 3 в рамках которого каждый узел фиксирует какие транзакции у него еще не закончены и узлы обмениваются через case от координатора этой информации то есть в чем наша идея получить множество транзакций которые не законченных идей либо обменяться этой информации если транзакция где-либо не закончена то мы не будем ее включать и квадратах от лак вообще везде то есть мы составим суперсет global скидки access ну глобальная пропущено транзакции от объединение локально пропущенных транзакций со всех узлов и у нас есть следующая цепочка из отношения hoping for с одной стороны backup чекпоинт hoping for называемую коту один это вот граница между этапом один этап м2 это все хотели for примером всех транзакций из множества блоков skip текст с этого поменяется благодаря тому что на этапе 2 мы подождали всех транзакций которые в состоянии препарат и это все hoping for апдейтом данных из множества глоба вскипать xs это выполняется ну просто благодаря тому как работа двухфазный commit в указанном кометы в указанном комете все примеры они лежат глобальные левее всех операций по фактическому обновлению данных что из этого следует что мы построили множество транзакции которые мы договорились пропустить и это множество лежит строго правее от backup чекпойнта который мы сняли асинхронном соответственно чтобы нам построить backup который будет обладать традиционной целостностью нам нужно сделать следующие то есть вот мы уже сняли backup чекпоинт получили консистентной sens мы просто берем и тренируемся по уже записаны музыкальном right to high блогом и докладываем транзакция между backup чекпоинтам и consistent к там по системным срезом фильтруете транзакции о которых мы договорились что мы их пропустим таким образом вот мы там сначала бы модерироваться без пропусков потом близко уже can системному связан мы будем некоторые транзакции пропускать и эти все рекорды из вратах отлогой мы разделим попортится и получится так что у нас есть договоренность о том какие транзакции везде пропускаются или везде накатывается и есть вот такая вот композиция есть набор партий шин файлов которые полученного какая то есть набором таких вот корзинок с якорным избрать выход logo соответственно чтобы восстановить такой backup достаточно будет применить обычную процедуру копирования парте шин файлов локальный старридж после этого надо будет да хоть докатить вайта hotlog для каждой птицы уже известно что это залог этом что за рекорды и таким образом при восстановлении мы получим традиционно консистентной пока по которой снятся всем без остановки польских пользовательских операций в целом меня все спасибо за внимание теперь я готов ответить на ваши вопросы здравствуйте спасибо за доклад алексей одноклассники а вот вы упоминали кассандру не совсем понятно чем же ваш подход лучше подходов к сандре когда собственного и блокироваться не перематывать а блокироваться запись в состав делает копироваться эти файлы snapshot и есть камень плагин то есть соответственно всегда можно остановиться на нужное время и никаких общем то проблем нет сиди со второй вопрос а вы как-то тестируйте бэкапы то есть известно что с определенным системах сложно понять у тебя можешь ты достанется за пока по или нет то есть если не более тяги географически распределенная и диски падают дата-центре там тоже падает все этого если вас я процедур тестирования за тысячу сначала на второй вопрос от процедуры тестирования вы видимо понимаете не покрытие с тестом а процедура проверки является ли backup адекватным и можно ли с ним вообще восстановиться до у нас есть такая опция у нас есть команда чек которая позволяет восстановить бэкап без фактического восстановления то есть мы просто пробежимся по имеющимся файлам пробежимся по страницам проверим серси суммы и выдадим зеленый свет если все партиции на месте и все данные на месте и соответственно скажем что у вас быка по коробкам если что то не так нет но мы действительно в рамках отёка пробежимся по файлам пока по если что-то побилось то при попытке там проверить сирхи сумму с этой странице мы упадем вот что касается кассандры подход кассандры нам то есть подошел бы на самом деле но тут как бы в кассандре lightweight транзак шанс и получить как системность по ним чуть проще чем по раскаленным транзакциям то есть гипотетически можно было бы проанализировать в рай the hat логе со всех узлов как-то обмениваться глобальной информация о том какая транзакцию мне есть какой нету но это там довольно сложный процесс получился бы поэтому мы подошли к этому немного по-другому мы вместо того чтобы постфактум анализировать логе мы прямо в ран тайме оперируя satomi транзакций который свинки построим консистентной срез спасибо за доклад вопрос такой что произойдет если вы вовремя вот этого обмена по кольцу контрольным сообщениями там 1 2 3 на какой-то узел вывалиться из топологии ну такое на самом деле несложно handmade то есть сообщение на самом деле я тут уже сейчас начну рассказывать это уже как бы будет матея для день его доклада как у нас работает консенсуса как он сработает кольцо краткий ответ это то что информация о том что топологии меняется тоже распростаняется по кольцу то есть изменение топологии тоже является глобально важным событиям и какой-то узел допустим вываливается предыдущий за ним узел понимаете что что-то не отвечает на сетевые запросы и посылается уже этом ответ следующему после него узлу что там вообще-то этот чувак вывалился эта информация рано или поздно дойдёт до координатора и вот конкретно в нашем случае при получении информации о том что топологии изменилось мы просто берем и а портим процесс снятия концертного среза в этом нет ничего страшного топологии меняется очень редко консистентной срез снимается быстро то есть мы делали замеры это порядка 300 миллисекунд на топологии 16 узлов то есть мы просто повторим операцию заново а у меня такой вопрос а что произойдёт если в вашей системе контрольных поинтов транзакциям никогда не переведется завершенное состоянии это понимаю тогда вы не сможете закрыть этот контрольный point речь идет видимо втором этапе когда мы дожидаемся завершения примером загсе ну я не полностью на самом деле описал алгоритм консистенция среза вот если хотите поговорить об этом детальнее подходите пожалуйста к нашему стенду но краткий ответ что мы там пользуемся варите ческим алгоритмом мы какое-то время все-таки пытаемся дождаться транзакций вот а потом уже включаем информацию что вот такая эта транзакция такой-то транзакции мы не дождались и в принципе если такая транзакция еще не перешла в состояние но не начала к медицина всех узлах вы просто не включив консистентной срез то есть это делается но это там немножко вот уже часть failover часть этого алгоритма а но но тогда же может нарушиться ordering но хотя нет машина только на конце может быть завязано спасибо за доклад вы говорили что скоро появится в этот дар планируется ли вы переведении вот этого всего что вы написали на высоте смотрите на самом деле подход который описал он работает только для транзакции без мгц почему потому что есть инвариант который который мы активно пользуемся то что все фактические обновления дата сторожа совершается между препарат и commit маркерами в случае с mvc это не так первое cc мы пишем изменения в старых сразу до при передам монако меч и соответственно ставим просто метку камедь логин я думаю что мы оставим этот подход для режима когда у нас нет втц а для pc мы просто сделаем что-нибудь по аналогии сколько раз тебе то есть используем самый высота для получения консистентной состоять спасибо и второй вопрос из топологии звезда это будет работать текущую реализация ну с отвечу так что у нас кольцо это не единственный способ организации узлов кластера мы имеем это делать двумя способами вот классическим нашу способом через кольцо и у нас есть режим когда мы доверяем менеджмента есть всю работу по нахождению узлов друг другом кластеру за кипера первым в принципе создан как раз для решения таких проблем и в принципе работу за кефиром можно представить ну как такой вот утрированно аналог топологии звезда когда у нас есть отдельный кластеру узел который координирует все это еда это будет работать все еще сохраняется отношениях и пнд for между отдельными с целями конечного автомата за счет того что есть координатор который все это координирует он такой интересный вопрос вывод упоминали но что изменение стоит отмечается специальными флагами а что если в вашей системе старта not несколько конкурентных пока под например состав ошибутся ну тут вопрос решен довольно просто как минимум у нас есть ordering то есть ли конкуренты пока всегда стартанет после текущего backup а для всех узлов вот имея ордере к нам уже просто разрулить эту проблему то есть во первых в текущей реализации созданием bacopa занимается один поток поэтому он просто не может не сможет начать писать предыдущий ну уже-уже новый backup во вторых координаторы знает что вы нас сейчас есть процедура снятия пока просто за rejected попытку создать следующий пока а то есть пока бы все создаются через ну не создаются часть горят но координатор во лидирует опять отправку сообщения на создание бэкапа то есть всегда вот есть вот . координатор который все сможет справа лидировать но просто один поток это не панацея поскольку если два конкурентных подкопаться сдадут станут один на днях нотах может занять потока другой на других да это не панацея именно поэтому мы ордером события через кольцо и консенсус и координаторы пока продираться через да еще вопрос спасибо за доклад про восстановление немножко можно еще по-подробней не совсем понятно хорошо восстановление в таком кейсе именно вот в этом когда у нас есть и партиции байтах от локи да да ну то есть тут восстановления состоит из двух этапов во первых нам нужно взять файл и партиции просто перенести его в дисковой сторону после этой операции мы получим состояние которое локальный consistent нам на глобальные какие-то различия после этого не давая пользователя нагрузки начать что-то обновлять мы докатим в рай то hotlog который мы отдельно сохранили для каждой партиции и изменения которые есть в этом проектах и блоге мы просто попаем к имеющимся у нас сторону то есть вот это в принципе операция в рай то hotlog и там apple а изменения туда это общий механизм для всех баз данных да то есть тут ничего нового нет ну то есть у нас лог это время но на код то есть раньше был лак это время на перемещение всех файлов партиций самом деле это вот часы на больших дипломатов а после этого накатывала это там уже несколько секунд спасибо у меня еще такой вопрос чем-то 52 тут говорили про бэкап и и что они идут через координата а собственно вот если координата упал что-то случилось и и выбрали другого координатора a backup уже запущена кто-то еще запустил backup в таком случае что происходит ну киднэпинг операции не обязательно покажу но это тоже часть вот этого консенсуса который у нас есть видно это плохие кольцо в таком случае мы просто перри выберем новый координатор и в таком случае даже не обязательно будет необходимо перри стартовать всю операцию в целом вот конкретно в случае с консистентными срезами мы так делаем но в целом если у вас вот есть ну я сейчас на вопрос отвечу в целом если у вас есть конечно автомат то просто можно взять и в момент ухода координатора понять всем кто является новым координатором и при отправить ему аки который и от были от правильно к радиатору на этом этапе то есть у вас так делается и вам спасибо я думаю что тут потому что это был последний вопрос на всем спасибо"
}