{
  "video_id": "Hiy3Wv1mqHM",
  "channel": "HighLoadChannel",
  "title": "Эволюция обработки вебхука Facebook: с нуля до 12'500 в секунду / Дмитрий Кушников (ManyChat)",
  "views": 2147,
  "duration": 2793,
  "published": "2020-07-17T06:45:48-07:00",
  "text": "всем привет зовут дима прошу прощения и она к сожалению не последняя рассказать вам историю участником которой являюсь последние несколько лет и пока я готовил наш доклад все немножко поменялось история буду рассказывать она не про 12000 футов про 25 просто так изменяется нагрузка в нашей системе я работаю компания мини чат где руковожу разработкой и на примере одного из наших компонентов одной из компонентов систем я расскажу вам про то как менялась наша нагрузка как раз наш сервис и какие решения мы применяли и может быть кому-то из вас кто сейчас сталкивается с подобными проблемами или столкнётся в будущем вы найдете для себя что-то новое и интересное и сможете это применить или уже применяете и тогда мы просто с вами обсудим и поделимся опытом второе название этого доклада звучит именно так что делать когда печки уже не подходит но очень хочется итак начнем но для начала хочу немножко вести вас контекст наших задач нашего продукта и вообще и так что такое мини-чат мунча то это сервис и мы не сейчас помогает бизнесам использовать мессенджеры для маркетинга продаж и поддержки наш основной продукт это платформа для messenger маркетинга на для facebook messenger нашим сервисом за все время на текущий момент воспользовалась уже больше 1 миллиона бизнесов из 100 стран мира то есть по всему миру пользуются нашим сервисом и они пообщались 700 миллионами своих клиентов и сейчас каждый день каждый месяц наша через нашу систему проходит восемь миллиардов сообщений такой вот мы небольшой небольшая под формочка как это выглядит со стороны клиенты этого бизнеса это интерфейс мессенджерах в изученного соответственно фейсбук предоставляет возможность отправлять бизнесом различные интерактивные элементы для того чтобы делать взаимодействие с своим клиентам классным и вовлекать и собственно осуществлять маркетинг продажи и прочее здесь вы видите кнопки картинки различные там галерей key а со стороны бизнеса это выйдет примерно так это интерфейс нашего в приложении где при помощи визуального интерфейса представители бизнеса создают и программируют сценарий и диалогов вот пример одного из таких диалогов это наш наше сердце наши системы компонент который называется flow builder вот но это немножко не про печки хотя пищу и под капотом там тоже очень много важно совокупность таких сценарий правил автоматизации мы будем называть ботом собственно если там упрощать совсем и для на и для для себя то можно иногда говорить что мини-чат это конструктор ботов клиента бизнесом который будет участвовать в этом диалоге и давайте будем называть подписчикам так его называет facebook потому что по факту для того чтобы начать взаимодействие клиент подписывается на этого бота вы спросите почему facebook messenger мы ведь тут страна выжившего телеграма telegram безусловно очень популярен в россии но в европе и америке мы соберем номер один является именно фейсбук там порядка полутора миллиардов пользователей по сравнению там 200 металлистами пользователями facebook a telegram а во-вторых в на фейсбуке просто гораздо больше бизнесов которые хотят общаться со своими клиентами не все из них еще дошли до того чтобы делать это в мессенджере но они пользуются фейсбуком и они будут через какое-то время готовы перейти в мессенджеры на платформе фейсбука работает 300 тысяч разработчиков буквально две недели назад была конференция в сан-хосе faith где фейсбук отчитался о таких размерах и то есть 300 тысяч человек по всему миру создает ботов для автоматизации для фейсбука и сенсора и каждый месяц через платформу между бизнесами пользователь управляется 20 миллиардов сообщений то есть мы не чат это примерно 40 процентов собственно примерно вот так вот устроена наша коммуникация с фейсбуком с точки зрения ну того как это происходит клиент бизнеса взаимодействует с телефоном в в этот момент фейсбук получает об этом информацию посылать нам веб хук мини-чат его обрабатывает в зависимости от той логики которые запрограммированным бизнесом и делает запрос обратно фейсбук который доставляет в сообщении в телефон пользователю со стороны же бизнеса у нас есть как я уже говорил в приложении и в этом в приложении бизнес настраивает логику этого бота немножечко про наш технологический стек все это мы делаем в достаточно скромным стояки в основе конечно php в качестве вообще ricoh тип сервера конечно же яндекс в качестве основной базы данных мы используем под gres reddy's пластик search и все это крутится в облаках амазона в общем и все вокруг фейсбучный представляет из себя запрос с джейсон поводом и я как раз хочу на примере того как мы обрабатываем эти в фейсбуке и запроса рассказать про то как менялась архитектура нашего сервиса несмотря на то что вы пауки являются всего лишь небольшой доле нашей нагрузки примерно 10 процентов они составляют но это является наиболее важные чувствительные частью нашей системы дело в том что через них происходит коммуникация бизнесов и пользователей если эта коммуникация тормозит или не работает то клиент просто отказывается взаимодействовать ботом а бизнес теряет ценность вот такой контекст теперь давайте перейдем к нашей истории вернемся на три года назад мая 2016 года мы только запустили наш сервис на сервисе 20 создан 20 ботов из них 10 создали мы сами для тестов и 20 подписчиков у них наша нагрузка составляет примерно 0 рпс и взаимодействие выглядит следующим образом то есть запрос приходит в джинсы джинсы поднимает джонс обращается к вич кайф кайф кайф кайф фильм поднимает приложение на и его вху контроллер обрабатывает логику все просто все все обычно на соответствие отладить отправляет запросы в facebook прошел буквально месяц финансировались на product hunt и имеем вот такие числа то есть до 2000 а выросло количество созданных ботов и уже семь тысяч подписчиков в этот момент у нас появилась небольшая проблема в нашей системе дело в том что а фишка фейсбук она не очень быстро и некоторые запросы могут длиться несколько секунд а если тебе нужны в результате обработки сделать несколько запросов то это может и 10 секунд занимать с другой стороны сервер который отправляет навыки хочет чтобы мы отвечали довольно быстро и здесь у нас произошел случилась проблема потому что если сервер вов хуков понимаете что ему не отвечают то он начинает сначала ругаться а потом и может отключить ваше приложение от входов в принципе первый вопрос проблемы нагрузки но мы еще только-только начали делать наше приложение у нас очень мало пользователем и еще ищем наш рынок еще нашего пользователя и наверняка на не как все вы знаете про такое простое решение которое просто спасло нас тот момент мы на пофиксили нашей системы тем что стали в момент того как запускается в контроллер прерывать обращение фейсбуку мы говорим о том что все как бы все хорошо а сами фоне продолжаем обрабатывать запросы и обрабатывать вук прошло еще полгода сервис выше раз в 5-10 раз десять тысяч подписчиков 700 и 10 еще ботов и 700000 подписчику до подписчиков стало прям сто раз больше параллельно с этим мы делаем новые фичи во-первых мы реализовали live chat это инструмент это возможность не только автоматизировать взаимодействию напрямую представитель бизнеса писать сообщение сообщения своему подписчику а во-вторых мы начали считать статистику нам нужно начали смотреть как получает ли подписчики сообщения и открывают ли их это решение этих задач и увеличила количество звуков которые мы начали отслеживать примерно в 4 раза то есть на каждые отправляемое сообщения мы начали получать три дополнительных в хука опять же чтобы понимать в этот момент в компании работает примерно два человека с точки зрения и там backend разработчиков и количество вещей которые нужно делать достаточно велико мы еще пока и все еще не крупнейшая платформам и пока еще только начинаем свой путь и самым простым решением для нас становится очереди на вас грейси мы не хотим еще ничего реализовывать мы просто разделяем наши потоки одних вы пауков мы одни во фуке мы отправляем очереди для синхронной для асинхронных запросов те которые нам нужно обрабатывать быстро для того чтобы пользователь получил ответ мы обрабатываем синхронно прошло еще какое-то время еще полгода в сервис продолжает расти и здесь мы были растет количество наших бизнесов у них растет количество подписчиков и мы реализуем новую фичу до этого мы все выпуке которые мы обрабатывали они касались только коммуникации в мессенджере но теперь мы решили что мы должны дать нашим бизнесом возможность коммуницировать из их подписчиками именно их страниц и свет мы начали обрабатывать новые типы вы fuck off этого cookies которые касаются ленты сообщи лентой самой страницы основав сном ленты страниц бизнесов обновляются не так редко вы наверняка эти маркетологи которые пусть это после пустота в ленту потом следят за каждым лайком и просто читают но бывают совершенно другие ситуации и в какой-то момент у нас случилась денька и теперь и что это такое ну кэти пэри это знаменитая американская певица она обладает огромным количеством подписчиков трампа фанатов по всему миру и в какой-то момент маркетологи кэти пэри решили сделать фото на фейсбук мессенджера и выбрали нашу платформу и тот момент когда они опубликовали сообщение с призывом там подписаться на бота наша нагрузка в момент выросла в 3 в 4 раза собственно мы поняли что без очередей мы без нормального реализации очередей мы ничего не можем сделать и в качестве решения для очередей мы выбрали редис это было фантастически удачное решение в дальнейшем редис нам позволил рушить огромное количество задач мы сейчас мы используем родить не только для для всего всех наших каскадных очередей но и для решения других задач подробнее об этом вы можете посмотреть я делал доклад примерно год назад на митапе там есть кейсы про как мы делаем мониторинг при помощи разница как мы многие многие моменты вот забегая чуть-чуть вперед скажу что сейчас каждую секунду через нашей disco star проходит один миллион сообщений ну запросов различных и так очереди на рейде си первое решение которое мы сделали было не очень удачным потому что когда мы просто начали складывать в пхукет редис и обрабатывать его одним одним одним процессом то мы соответственно расширили воронку наверху количество входящих в эпоху обработали но обработка все равно занимала какое то время когда же мы попробовали когда же мы попробовали отмасштабировать количество этих запросов соответственно так ну я имею ввиду так вот вот соответственно у нас случился небольшой коллапс дело в том что в очереди могут скапливаться запрос от разных страниц но могут и падать и запросы от одной странице и если обработчики берут запросы ни один оба забор обработчик немножко затупит то может случиться такая ситуация что запросы от одного подписчика 1 бота они просто обработаю неправильном порядке что крайне существенна когда пользователь нажимает делать какое-то действие а ему приходит сообщение наоборот и казалось бы может быть такой может случаться редко но как показало тестирование который мы сделать сразу на наших нагрузках это уже случалось очень часто и мы начали искать другое решение здесь соответственно нам на помощь пришла вся с одной стороны просто то в другой стороны мощь редиса нам удалось мы выбрали в качестве решения то что мы давайте мы сделаем очередь на каждую работа мы сказано сделано мы начали складывать сообщение которое касается каждого каждого бота в очередь и для того чтобы обеспечивать ну как бы для того чтобы у нас просто не поднимайте обработчик на каждого очередь каждой очередь мы сделали контрольную очередь то есть каждый раз когда у нас приходит запрос в эту от какого-то бота в редис по будет появляются получается в редис публикуется 2 сообщения одно в очередь бота одно сообщение в контрольную очередь обработчик следить за контрольной очередью соответственно вот здесь вот и каждый раз когда там есть задача обработать бота запускает демона на бота и бот разгребает очередь демоны разгребает очередь соответствующего бот кроме прочего нам удалось таким образом решить проблему во-первых шумных соседей да это когда один вот на генерировал нам огромное количество блоков и из-за этого тормозило система потому что другие страницы ждали обработку ну и собственно в это решение достаточно легко масштабируется потому что если мы понимаем что контрольную очередь наполняется мы просто дополнена бо вля им новых обработчиков кроме того на самом деле это очереди а не виртуальные то есть это всего лишь соответственно ячейки в памяти одессе и когда очереди когда в очереди ничего нет ее просто-напросто не существует она не занимает ничего с точки зрения релиз прошло еще полгода сервис вырос еще в несколько раз в январе 18 года мы достигли отметки что в месяц мы отправляем 1 миллиард сообщений соответственно нагрузка в тот момент составляла 5 тысяч черкесов в секунду на систему просто вы понимали это не пиковая нагрузка это вот что то такое перманентная да потому что когда приходят очередные другие знаменитые певицы все растет несколько несколько раз и проблема возникли в другом месте проблемой в снова возникли в районе писька их фирма то есть пять крп с это то что писька и вам уже просто не готовы держать и мы начали искать решение опять же посмотрели по сторонам просто все в то время говорили про модный асинхронный processing будет замечательно доклад и мы абсолютно подтверждаем те цифры соответственно мы посмотрели на react к яички и провели быстрые тесты и даже заменив писька и в темный react печки мы получили в мгновенно прирост в четыре раза самое интересное что мы не пошли по гайдам мы не стали собственно переписывать наш нашей обработку нашего процессинга у нас рис и активисты поднимался прямо с и фреймворком на котором мы продолжали продолжали писать то есть он поднимал целое приложение и несмотря на это это позволило справляться справиться с такой нагрузкой как только мы опять в расширили воронку снова случился небольшой collapse ну собственно один на минутку один ряд печки в четыре раза позволил в росте нагрузку как только мы подняли взаим джинсам примерно но сначала мы четыре подняли электрички сервиса потом в конечном итоге до 30 у нас доходило react риски сервисов которые там крутились ну соответственно достаточно долго мы жили жили именно на них но когда мы раз как бы хотели воронку на приеме соответственно начал опять страдать processing и следующем решением которые мы сделали это мы выделили processing в кластеры то есть на примере мы взяли ботов распределили их по кластерам и выстроили такие логические цепочки из редиса под gresso и обработчика в итоге у нас сформировалась такая такое понятие как галактика галактика это логически физическая абстракция над над процессингом она состоит из инстанса редиса инстанция пищи инста прогресса и instance процессе ну собственно эти два это обычный обрекая еще каждый год принадлежит какому-то кластеру и react печки знал о том какой кластеры нужно положить в какой класс стр нужно положить сообщение для данного бота ну и дальше работала та же схема которые я уже говорил все замечательную завелось и мы достаточно быстро поняли что галактики это и есть наш способ масштабирования ну как вы знаете там вселенная она расширяется как по мере расширения наших систем и просто брали добавляя новую галактику следующий этап еще через полгода мы продолжаем расти 200 миллионов подписчиков всего ботов 3 миллиарда сообщений в месяц немножечко просто чтобы по поводу там подписчиков вообще оценивать масштаб там счас сервисов достаточно тяжело но когда я говорю 200 подписчиков просто представьте себе сайт 200 миллионами зарегистрированных пользователей это примерно вот это примерно вот это в чем случилось наша следующая проблема как вы понимаете выпуке это достаточно такая маленькая однотипная задача и вообще печки не очень хорошо подходит для ее решения и мы очень долго долго старались делать это на печке но в конечном итоге поняли что даже react печки не справляется с той нагрузкой в этот момент она составляла примерно в 10 крп с of но проблема реакции была в том что для того чтобы ну там многого во-первых он все равно немного подтекал и его требовалось перезагружать а перезагружать даже при тепло их было необходимо его последовательно потому что мы не можем прервать обработку входящего fuck off как я уже рассказывал в момент когда фейсбук понимает что что-то идет не так он просто отвечает ваше приложение что совершенно недопустимо на платформе как видите 650000 бизнесов активно активно работает вот опять мы посмотрели по сторонам и по мере вообще вот движение мы постепенно вкус его ряд ряд пички разную логику мы раскладываем передавали в процессинге вообще этом в очень эры новые очереди и где-то вот в это время мы просто поняли что реакции че делать одну простую задачу он берет хук и складывает и и очередь все остальное делает уже процессинге посмотрев по сторонам вспомним что у яндекса есть отдали мы приметили библиотечку которая называется upn reste кроме поддержки языка программирования lua у неё был еще модуль работе с редисом написанный на коленке тест за три часа показал то что все что делает 30 сервисов на react печке можно сделать прямо на стороне engine xxi вот так выглядит то что что у нас получилось соответственно какой-то and point мы обрабатываем забираем тело запроса мы соответственно и складываем его напрямую редис опять же это позволило нам достаточно легко увеличить пропускную способность и мы продолжаем справляться с нашей нагрузкой и сервис живет и мы счастливы до соответственно и сельскую сказал ну и соответственно последний этап который который я подготовил это феврале этого года количество подписчиков количество созданных ботов на платформе превысило 1 миллион пятьсот миллионов подписчиков с которыми они общаются и 7 миллионов сообщение отправляется от отправлялась каждый месяц что мы начали делать мы начали улучшать наш решение на но мы начали по-тихоничку откусывать некоторую логику из наших очередей и первичный processing распределение потоков между системами начали делать на 1 соответственно это позволило нашим системам стать еще менее зависимым и еще более производительным как вы здесь видите то есть он и сохраняем отдельно отдельно processing отдельно асинхронная обработка которая касается статистики и прочих вещей которые по сути стала совершенно другой системой конечно это выглядит все просто но там чтобы понимать что вот эта система на тоже не такая простая это вообще это совершенно отдельный доклад там под капотом крутится порядка 500 сервисов которые каждым которые обрабатывают свои запросы вся наша вот эта вот система сейчас работает на 50 м со всех амазона это и редис и это по сгрыз и и соответственно сами обработчики печки вот такая вот история она продолжается за время даже мои последние подготовки ну то есть нагрузка растет дальше когда я буду уступа следующий раз возможно там будет еще один слайд про следующее наше решение на своем опыте мы выяснили что строить архитектуру можно последовательно и изменяя различные части которые в данный момент наиболее уязвимый и при этом применять простые подходы и не простые известные подходы и не расширяя существующие стек вот таким образом напомню как изменилась но система сначала мы начали с обычного fpm очереди на погрейся очереди на рейде си кластеризация замена react peach внедрение react личке замены лекции спина angels плюс слова ну и соответственно переносит логике на джинсы ушло что что могу сказать что highload можно хорошо и классно делать даже на печке спасибо у меня все за докладом еще этот момент здесь вот у нас qr код ребята которые делает наш стенд подготовили бота специально для конференции там можно побольше узнать про наш продукт нашу компанию собственно ну и вообще посмотреть как это выглядит для бизнеса и как это выглядит для их клиента всё спасибо вопросы спасибо за доклад но вы хотели поблагодарить тебя спасибо да и сейчас у нас начинает секса вопросов не забывайте поднимать руку к вам придут с микрофоном добрый день спасибо за доклад у вас есть прогнозы на нагрузку то есть допустим если у вас через пару месяцев увеличится еще чтобы вы еще сделали и многие компании многие сервисы которые работают с php в итоге частично даже делают решение на год то есть если у вас планы использовать что-то ещё кроме пешки спасибо спасибо за вопрос прогнозов по нагрузке у меня нет но повторюсь как бы как вы увидели там примерно за несколько месяцев она может вырасти в два раза сера сейчас растет потом по своим размерам по количеству подписчиков и по всему примерно в 3 раза в год поэтому безусловно мы следим за нашей нагрузкой и ищем какие-то там какие-то решения и я уверен что в тот момент когда что то есть у нас мы будем понимать что где-то у нас новое узкое место в этот момент мы снова посмотрим что то применим по поводу конкретного у нас какая-то в фуке не ну как бы это не задача для было потому что там понимаете там у нас есть другие моменту системе где возможного был бы полезен но мы используем другое решение мы научились делать печки примерно то же самое что делает год но об этом я могу рассказать просто отдельно и об этом рассказывал носите она на холоде наш решение для того как вообще ну то есть не достаточно просто обработать 25000 руб хуков еще нужно научиться тоже не то чтобы их получить до их нужно еще обработать и сделать обработки это быстро качественно обеспечив то что все остальные системы работают нормально вот поэтому вот это спасибо за ваш доклад подскажите вы говорите что вас вы начинали с простой архитектуры и периодически у вас росли нагрузки и вы адаптировали систему под новые нагрузки внедряя новые решения как вы боролись с такой вещью что допустим вас подходит новый пике у вас вот-вот уже не будет справляться системы и вам нужно перестроить архитектуру но на перестройку архитектуры нужно довольно много времени на то чтобы найти решение чтобы что-то переписать что-то переделать а нагрузка растет растет растет и как вы решали проблему что вот в момент когда вы переделываете архитектуру ищите решения вас не проседала нагрузка на сервера и как бы клиенты не отказывались от вашей системы спасибо спасибо за вопрос интересный да у меня есть некоторая информация мы с нашим сидела буквально там несколько дней назад примерно прикинули ну то есть хороший правильный вопрос и ответ на него следующий мы изменяли нашу и архитектуру маленькими шажками каждый раз увеличивая ну как бы каждый раз этот шажок давал возможность он закладывал очень очень большую отказоустойчивость и но при этом эти решения были простые и очень точечные я могу сказать что на разработку всей вот этой архитектуры суммарно за все три года было потрачено меньше недели последовательными небольшими шажками решение с редисом мы делали за одну ночь галактики мы запускали за два дня и ну как бы все остальные решения но как бы я рассказывал джинкс в три часа + тесты и выкладка production поэтому возможно об этом стоит подумать спасибо за доклад мне на следующий вопрос проводили проводили и проводите ли вы тестирование вот этих решений ваших и насколько их сложно тестировать особенно вот случае ло или реактор можешь же те уточните пожалуйста о каком тестирование вы говорите ну принципе пишите ли вы теста для этих решений чтобы понимать насколько некорректно работают конечно но дочь конечно то есть мы пишем тесты на различные обращение то есть разные части компонентов у нас покрыть разными типами тестов а можно поинтересоваться случае ло какого рода вас теста api ну просто приемочные тесты понял спасибо ну там как бы но интеграционные тесты которая проверяет как дмитрий спасибо за доклад минут игорь можно такой вопрос вы сразу на амазоне разрабатывались на первых разработчику сейчас сколько там они и ребят который занимается инженер по бренду сколько она стреляет у человека до 4 спасибо и смотрите такой вопрос на других конференциях слышал как социальных сетях крупных есть грубо говоря есть куча мяу пользователи которые круга меня сильно активным как он на примере с к теперь и у вас есть какие-то механизмы друг гарри вот если завтра бабах какой то пользуйтесь становится известно или бизнес становится супер активным за механизм иммиграции ну то есть на отдельные кластеры его сажать конечно да у нас написанные инструменты на том же письме у нас написано команды просто если у нас заранее подняты какие-то галактики гастера которые более-менее свободные там мы следим за их нагрузкой есть что-то происходит там микротии both as close to рана кластера занимает ну в зависимости от его количества данных которые он успел накопить ну там вот секунд до минут спасибо закладка если можно пару слов об устройство вашу мониторинга какие основные метрики собирается и механизму регистрации кластеров так давай да давайте последовательно устройство нашу мониторинг для мониторинга и мы используем амазонский cloudwatch для того чтобы складывать туда метрики мы используем тот же редис я рассказывал про то что у меня был доклад там вообще еще более подробно про это просто но или подходи подходите я расскажу как бы вообще все детали вторая часть вопроса была обустройство кастрация кластеров устройство регистрации кластеров мы используем штатные средства а вес у него пасибо количество метрик которые мы собираем ну там несколько десятков тысяч что нужен сейчас мы мониторим каждый кластерной очереди ну как бы каждый тип и очередей в нашей системе и соответственно если вся эта система работает собственно на концепции того что очереди всегда пусты ну то есть они наполняются и сразу очищаются поэтому собственно если где-то за случается тут мы сразу это видим понимаем что это наша новая боли и придумываем решение спасибо за доклад вот у вас продукта он очень зависит от как бы технологии другой компании дает фейсбук машин мессенджера топи звуков от того как они работают и насколько вообще как бы какой-то отзыв вы дадите насколько как все это работает надежно да насколько как бы вы часто там stalker с проблемами потому что наверняка там заглючит фейсбук да у него что-то внутри сломается да может быть негатив на ваш продукт ложится да потому что вам клиенты не понимают где проблема да не думаю что проблем у вас может не совсем технический вопрос не это он технически интересный продукт а вы проблема бывают и каждый раз когда мы делаем наш продукт на все наши пользователи прекрасно понимают что мы являемся сервисом поверх facebook messenger и мы решаем это следующим образом мы абсолютно прозрачно рассказываем что происходит у фейсбука у нас есть процессы которые следят за тем что что происходит фейсбуке и мгновенно реагирует для наших пользователей они их модифицируют где в том что там под капотом нашем продукте есть всякие рекламные кампании прочие вещи если мы понимаем что то не так срабатывает механизм и нотификации там сейчас мы работаем над системой отключения рекламной кампании для ну в автоматическом режиме чтобы наши пользователи наши пользователь не тратили деньги но и соответственно я скажу так на facebook при таких нагрузках он достаточно надежным но пока при случаются как у всех вот время от времени наши пользователи достаточно лояльны дело в том что ценность которую они получают и она пока гораздо выше чем те небольшие проблемки но и facebook очень много инвестирует в то чтобы сделать это решение классным они верят в том что ну то есть мессенджер для них это новое социальное пространство куда придут бизнес и общаться с спасибо спасибо за доклад меня два вопроса первую почему вы название доклада написали 12500 хуков в секунду она слайдов и вижу 25000 наверное потому что у вас настолько быстро расти и растет что вы не успеваете печатать даже программки до примерно тогда это происходит и второй вопрос не могли бы вы пояснить каким образом вы решили проблему то что хуки приходят в одном порядке обрабатываются в другом вы показали слайд но там все равно не понятно я наверное могу отмотать если кратко то есть первое сначала мы обеспечили то что не так пошло давайте попробую на словах тысяч то подходите я вообще нарисую схему как бы будут более более точно мы так все случилось сейчас приключиться соответственно что мы делаем во первых мы разделяем очереди на каждого бота то есть мы обеспечиваем что они друг друга не мешает дальше мы собственно обработчик привязан момент когда он допускается обработчик очередь на верного бота он соответственно работает с очередью этого бота и ну обработка всех событий происходит последовательно в общем все так получается один обработчик может справиться с любым ботам даже самым нагруженным даже с ботом кэти перри да потому что дальше ну то есть там дальше работаем ножка другая система в момент когда обработчик уже понял что это за вокруг там на самом деле запускается ну наш аналог того что вы спрашивали про год у нас есть специальный механизм выполнение наш балансир который занимается уже именно тем что отправляет запрос и facebook и там еще одна система контроля 1 user data есть там каждый из этих процессов которые запускаются он уже не только первичной альп на пир вот данный персов скрепер ну вот есть свои хитрый механизм роняешь не скажу просто это это отдельная но как бы это отдельная история там можно полчаса рассказывать как мы до нее дошли вас и вам спасибо за доклад продолжая тему fa cup of вот такой вопрос вы несколько раз упомянули что фейсбук как бы банит если вы там не справляетесь побегом случалось ли у вас такие случаи как если случались то какое самое длительное как бы самой такая сильная факап ситуация была и как выходили как решали да ну соответственно была у нас на пару ну вот да до того как мы перешли на ну собственно на редис и потом уже по сути вот все там вот эти вот этапы некоторые не все как бы некоторые из них как раз сопровождались такую ситуацию фейсбук давайте уточнить не совсем банит он перестает присылать в пхукет да и возникает необходимость ну начать их снова принимать и сообщить фейсбуку что типа ты все все починил у тебя все окей как вы справлялись но соответственно там нужно было сделать две задачи нужно но чаще всего мы решали это дополнительно масштабированием да то есть если там эту проблему было сверх кличкой мы поднимали просто новые дополнительные сервисы быстро решали а потом придумывали в чем проблема и выносили часть процессинга как бы на другую как бы вы еще одну еще один каскад очередей но вторая проблема которая была нам нужно там все наши 650 там там 500 там 200 тысяч ботов перепад писать наук но и здесь соответственно у нас было решение которое позволяет там за 20 минут и это сделать ну кого при тех нагрузках мы на моей самой большой памяти несколько часов вся система находилась вся система находилась таком частично недоступном состоянии вот когда там половину ботов работала половина в этом and потихонечку ночного работать вот как то так но это было уже давно и мы не помню когда это было и так друзья давайте сейчас отпустим диму он будет в кулуарах там обсуждать в зонах для доклада и мы пока будем готовиться к следующему диме спасибо"
}