{
  "video_id": "-IonQrHhfrk",
  "channel": "HighLoadChannel",
  "title": "Опыт миграции 10 TB PostgreSQL-кластера на AWS / Александр Кукушкин (Zalando SE)",
  "views": 1202,
  "duration": 2926,
  "published": "2019-05-15T02:52:16-07:00",
  "text": "здравствуйте меня зовут алекс кукушкин я в зал on уже про babay пол из комьюнити люди меня знают как человек который занимается разработкой патроне это решение для х и выберите по згрлс овой вот и как debate мне приходится делать очень вам много разных вещей как создание новых кластеров размеры совершенно разные методики не сам создаю я создаю систему которая позволяет позволяет все это автоматизировать размеры наших мастеров совершенно разные от нескольких мегабайт иногда потому что мы используем очень активно микро сервисы и самый большой кластер в нашем случае это сейчас уже 12 терабайт занимает и недавно нам пришлось от кластер sangre ровать в amazon по по определенным причинам кто-нибудь слышал о zalando или не слышал хорошо zalando это в общем то такой брат-близнец или сестра близнец для моды которые здесь россии есть у за ланди был сам самым в тот же самый инвестор что углеводы rocket internet вот но разницы в чем zalando работает 17 европейских рынках и рынках 17 европейских стран у нас есть собственные логистические центры у нас очень много активных к стримеров и в технология работает порядка 2 тысяч человек из них не больше тысячи на самом деле девелоперы вот и все это крутится на огромном количестве с grease off исторически у нас еще боль больше 300 кластеров работает в дата-центре куда-то в двух дата центрах и последние 3 года все больше и больше переезжает в amazon и больше 6 600 кластеров 650 у нас в амазоне при этом порядка 160 и 170 на их счету индексах работает остальное это наку вернитесь и вот и собственно чем этот доклад во-первых напишу что мы переносили какие quigo ленты для вот такой базы большой мы можем найти в облаке как сохранить доступ к той базе потому что это база используются многочисленными разработчиками аналитиками есть куча система куча приложений reporting и про это и так далее и тому подобное как обеспечить доступ так чтобы никто общем-то ничего не заметил по возможности как мы мигрировали дату каким образом мы будем бы копить все это в облаках потому что все таки backup 10 12 4 байт это задача непростая ну и в общем то в заключении расскажу что она что мы наткнулись что можно было бы улучшить вот значит что у нас было в самом начале собственная система предельно простая один мастер одна реплика подрисовываю потоковой репликации физическая вот эти два сервера мы заказали в 2015 году у нас от момента когда мы начали в общем то выбирать какое железо нам надо и до момента когда мы получили все это в продакшен никто есть перенесли со старых серверов у нас затянулось это практически все на 3 месяца вот я не знаю почему это так вот мы так быть не должно и в общем-то облака позволяют ускорить процесс доставления нового железа или каких новых серверов и очень сильно вот все крутится на пастбище 96 мы используем виртуальная пин для того чтобы случае файлов ира просто перекинуть виртуальная еппие собственно приложение или люди будут ходить на новый адрес безо всяких изменений над существенно и на диске у нас было десять с половиной дисковое хранилище построена на raid 10 ч 14-ти дисков полтора терабайта итого 10 с половиной терабайт к сожалению тут обрезается не помещается что у нас внутри это база такая используется для хранения ивентов туда поступают различные бизнес ивента информации регистрации пользователей например каких-то покупку покупках о том как товары настолько двигаются на так далее тому подобное наиболее интересные данные это за последние десять дней поэтому мы их называем горячими они идут в отдельную таблицу и по истечении 10 дней есть отдельные группы который при потихоньку это перекидывают в другие таблицы и того у нас полторы тысячи ивентов и соответственно 3000 таблиц может быть и не очень эффективно но вот оказалось то что это наиболее оптимальной на оптимальная структура потому что мы еще начинали с 91 если не ошибаюсь с этой малый каждый день попадает в эту базу 100 миллионов новых ивентов соответственно примерно 100 миллионов переезжает из из горячей областью в архивную область и значит растет перерастала все это примерно по 2 терабайта в год последним данным в этом момент когда мы делали миграцию вот и возникает проблема у нас места свободного 500 гигабайт жить нам остается на этой же на этом железе три года что делать значит нам либо надо срочно заказывать новые принимаю во внимание то что в прошлый раз потребовалось три месяца то есть времени у нас очень мало либо мы должны мигрировать например вал в amazon все где мы в общем то можно получить какую-то эквивалентную систему просто там за пару минут на на этот вопрос на самом деле мы смотрели сильно заранее делали некоторые эксперименты и приняли решение о почему бы не попытаться в конце концов у нас на самом деле цель переехать в собственного облака потому что от ускоряют разработку ускоряет дивидендный цикл ну да будем мигрировать но облака они недешево они очень на самом деле по где облака будут я дешево на каких-то маленьких объемах данных на каких для каких-то маленьких приложений потому что вы можете что-то запустить поняли то что она не справляется удалить запустить что-то помошник помощнее вот начать с малого и при росте тоже муссоном собирают инстанции она будет может быть там чуть чуть дороже но на таких объемах на таких больших базах она будет по нашему опыту как минимум в 2 2 в три раза дороже чем дата-центре вот но на всякий случай если вдруг в облаках мы не сможем справиться с нагрузкой мы должны иметь какой-то путь отступления переключу где должны иметь возможность переключиться обратно в дата-центр от копировать 10 терабайт туда сюда тоже занятия не очень приятные поэтому желательно ноги чтобы сохранить старые мастера реплику как новые реплики в этом классе доцента чтобы они просто реплицировали оттуда вот соответственно приложение и куча пользователей с у нас сотрудников в технолоджи порядка двух тысяч из них на самом деле не всеми доступ к как этой базе но по моим прикидкам порядка пятисот шестисот отбегать каждого объяснять что у нас поменялось что надо сделать чтобы получить доступ от задачи тоже неприятное email никто читать не любит соответственно трафик у нас будет ходить и зато центра в облака и он должен быть обязательно закреплен потому что мы не несем ответственность какие винты попадают какая информация вдруг это какая-то информация о пользователях которые не должна быть раскрыта вот down тайна downtime должен быть минимальный это в общем то очевидно про это говорить не надо хорошо начинаем выбирать как мы новый этап в облаках что мы будем использовать потому что amazon предоставляет очень общем очень большой спектр разных инстансов решений мы можем использовать с мы можем использовать аврору мы можем собственно запустить сами висит у инстанции создать какие-то воли мы на ибице перед по положить туда данные и запустить paul gross либо в докере либо просто там на счету инстансах энергия в общем-то смотреть не особо интересно а вот аврора это был общем-то new ketones блок такой новый игрок игрок появился он во франкфурте где мы в основном держим наши ресурсы в этом году только в январе-феврале для да да да доступен для для общего использования по тестам амазона он очень сильно выигрывает rds там в два в три раза иногда вот замечательные вещи про ворота что все инстанции авроры из одного кластера мастер и все реплики они работают с одним и тем же старым чем если у вас 10 терабайт данных то все эти десятеро пойдет терабайт используется всего один раз для всех инстансов не надо ничего копировать еще одно огромное преимущество то что можно например склонировать и запустить какой-то клон для экспериментов с авророй это как говорится привет колли и за место на диске мы платим ровно столько же сколько платим за джипе то примерно 12 центов за гигабайт месяц но есть одна неприятная вещь мы еще должны заплатить за 22 по 22 цента за каждый миллион запросов к старриджа если у нас очень много риты и bright activity то эти 22 цента за миллион request превращаются в несколько тысяч долларов в месяц дополнительно я просто делал эксперимент мерил вот но в принципе по моим прикидкам если бы мы все это запустили на авроре то цена примерно получилось бы то же самое что мы сделали на счету инстансах так забегая вперед самый главный недостаток то что в ардис и в авроре вы не можете получить супер юзера вы не можете создавать антраст от языки такие как пили прокси а нам пили прокси нужен был поэтому но гол мы не мне не можем использовать аврору вторая опция была это и 3 инстанция у них подключен прямо к этому инстанцию очень быстрый создали диск на envy me диск он позволяет получить совершенно гигантские количества гигантские цифры и о пирсе акконд юри квестов в секунду позволяют прапора к чуть гигантское количество информации не чтения и записи это там измеряется в гигабайтах в секунду но недостаток в том что что самый большой instance а то и 3 16 он позволяет держать только 15 терабайт данных на дисках соответственно в какой то момент мы вчера на упремся и поскольку огромный недостатков этих аяте инстансов то что при перезагрузке этого инстанции или когда вы мы его выключаем данные с от этого диска с этого хранилище они уничтожаются потому что когда какой-то другой пользователь запустит а даже самые инсульте ему надо предоставить чистый диск иначе произойдет утечка и так далее и тому подобное соответственно если мы хотим построить какой-то хай левел был кластер нам надо как и как минимум трин солнца чтобы избежать ситуации вдруг все три инстанса упали нам надо устанавливаться из бэкапа долго долго и больно ну и третья опция это будем использовать обычные инстансы как м4 ar4i ли там новый новым поколением м55 они могут работать только с и без у и без а есть как бы со свои преимущества по сравнению с envy me например и мы можем существующей валиум просто подключить другому инстанцию запустить по сгрыз и у нас обратно работает у нас наши данные живые целы и здоровы и все хорошо вот соответственно это дает возможность апгрейдить instance или наоборот даунгрейд если мы понимаем нам этот intent сейчас не надо мы можем запустить что-то в два раза более слабые платить за него в два раза меньше вот недостатки и без очевидно поскольку это сетевой все-таки сетевая файловую систему too late and sam там порядка миллисекунды какое количество вепсов и пропускная способность тоже ограничено для каждого волюма для 1g petroleum он не может прокачивать больше чем 160 мегабайт секунду или года до дать больше чем 10000 а вепсов для а иван эти лимиты побольше вот но за за это надо платить и платить над за это очень хорошо то есть если например мы создаем 10 терабайтный диск то на джипе то это будет стоить во франкфурте 2190 долларов в месяц и это нам даст 10000 aiop saw на а иван чтобы получить те же 10 терабайт и 10000 9000 а гипсов это будет уже стоит 2270 а что в два раза в общем-то дороже если мы хотим получить 30 1000 вепсов то разница в цене уже получается больше чем в 3 раза почему потому что мы можем из джипе ту эти 10000 поделить на три части и сделайте на 0 рейд соответственно мы увеличиваем пропускную способность за бесплатно саван нам ничего не остается кроме как купайте theory псы и приятного мало как выбрать instance как уже и если вот были на предыдущем докладе колье говорил что amazon имеет свой рынок для продажи ресурсов которые не используются так называемые спад инстанции мы можем просто запустить несколько способов прогнать какие-то тесты это будет дешево собственно тарификация она так и так уже по секундную амазона со спорт инстансов можно получить экономию в два в три в четыре раза вот как теста гонять поскольку каждая база все-таки уникальный мы не можем просто взять и запустить абстрактный pi-pi-bent потому что он просто дает какие-то абстрактные цветы цифры производительности в идеале конечно надо вот взять скопировать полностью базу но 10 терабайт тяжело поэтому мы пошли другим путем мы сделали дамп схемы и у нас была возможность просто все вот эти винты писать сразу в два места мы примерно прикинули и поняли то что джипе то и м4 инстанции в принципе справляются с тем то что нам надо вот а если опять же посчитать сколько это будет стоить на e3 будет гарантированно все работать потому что по производительность там там во-первых пол и терабайт памяти это уже стоит многого дискин вы им я тоже летают но стоит это 4345 евро вами долларов в месяц на 48 x-large на одном инстанции и здесь я должен заметить то что поскольку чтобы сравнение было честным то ой 316 x-large у него всего 15 работ на дисках соответственно мы будем сравнивать сколько будет стоить какой-то другой инстанций плюс те же 15 терабайт он она и без вот это будет стоить это 333 тысячи долларов примерно но если мы запускаем к целый кластер нам надо как минимум мастер и как минимум одну реплику в случае сайт 3 нам надо две реплики потому что устанавливает 10 терабайт и из из бэкапы это долго и больно то цена уже получается более чем в два раза отличается вот потому что и и без и по значит по статистике amazon и ebay с верой в вероятность отказа годовая составляет примерно одну десятую две десятых процента вот даже если мы создаем рейд из 6 дисков то вероятность того что у нас вылетит два диска которые два этих полем и которые на для двух разных инстансов собственного ничтожная мы можем себе позволить жить на дух и потому что в случае если instance крошиться мы используем эти же самые без волю мы подключаем на новый instance у нас все данные там начинаем обратно стримить в случае с ой 3 все-таки вероятность отказа и ситу инстансов она сильно выше то есть на нашей памяти они дохнут как мухи там как а как минимум каждую неделю мы видим одно-два падения вот и в итоге мы пришли вот к тому что выбрали r48 x-large по количеству процов по количеству памяти оно более менее похоже на то что у нас в дата-центре было дисков мы сразу сделали на 20 терабайт хотя на самом деле можно было начать с чего-то с ними меньшего потому что и без позволяет возможность расширять диски прямо на литургии за всякого дам тайма и собираем из них raid 0 right вот как мы будем доступ давать туда опции ту нас немного мы можем переписать dns потому что так или иначе люди люди используют хосту порт они используют айпи либо мы должны какой-то прокси и питта и был целей чей прокси запустить или ему может быть пить bouncer но поскольку самое главное требование то что весь трафик который через интернет идет должен быть закрепленном есть определенный ряд проблем то что мы не имеем права и не можем использовать какой-нибудь vpn вот и это будет в общем то слишком для нас не удобно с свп нам вот соответственно dns , wheels просто сами по себе отпадают печи и прокси это обычный 10 прокси он не умеет петер минировать исэль connection подрисовываю остается в общем спиде bouncer и мы придумали он в общем такую схему запускаем пить мауса на двух разных состав когда будем делать переключение мы просто перекидываем виртуальная обед и в итоге люди будут ходить на пике mouse or думают что они ходят на на старые посты и педи bouncer будет проходить в amazon но видимо lancer должен быть уверен то что он реально подключается к нашей базе а никак не какой-то другой соответственно он должен проверять серверный сертификат сервера в амазоне должен проверять то что к нему идет них нет никто подряд вдруг там что-то слетела с security группами мы будем проверять клиентский сертификата выпить bouncer а вот как это будет выглядеть и как как мы будем это делать мы сделаем свой сердечки это сорите маленький всего на несколько ключей сгенерируем серверный и клиентские ключ и сгенерируем серверные клиентские сертификаты подписанные с помощью приватного ключа это на нашего сердечки это вторить и и настраиваем под верстак чтобы он проверял клиентский сертификаты и соответственно пиги bouncer должен убеждаться то что сервер которому мы коннектимся это действительно наш сервер и собственно с нас настрой с настройкой по сбросу все более менее понятно как как делать такую проверку сертификатов но мало кто знает то что есть в перебей такая опция как клайн сердце равно 1 что она делает она будет проверять валидность сертификата но она не будет рассматривать например не будет брать и сертификата информацию о имени пользователя большинство почему-то или людей привыкли что сертификат будет содержит содержать какую информацию лаги не например я плагин будет использоваться для авторизации если мы просто используем clarins clarins сердце равно 1 то под груз будет проверять клиентский сертификат но после этого он будет все равно проверять пароль пользователи которые конектится соответственно для пользователей ничего не меняется они работают папой по-прежнему uslugi нам из паролем вот пить bouncer все зеркально наоборот мы используем цешин пул молд потому что пользователи у нас много права у них разная соответственно на настраивать какие-то разные qqq кучу разных баск в общем это очень да да дорого и долго не нужно но начиная poms версии 1 7 5 балансире появилась замечательная вещь как ал ал сквере то есть мы можем авторизовать пользователи в пиди балансире запросив информацию из из пяти каталог в поясе вот мы говорим пожалуйста сервер проверяет проверяй серверный сертификат через семь и пожалуйста используя клиентские сертификат для того чтобы конектится к серверу быть потихоньку общем то переходим как мы будем данные переносить опции как опять как как говорится опять не очень много мы можем использовать beach boys backup но поскольку у нас дата центр находится в приватной сети из амазона мы не можем получить напрямую доступ соответственно либо мы должны использовать vpn либо такое куку стороны решение как сыч туннель с вы по венам по определенным причинам водицы них о чем у нас не хотим у нас рута нет привлекать сисадминов тоже неохота тысяч тонн туннелями мы можем сами справиться воск но что-то должно следить что это цель у нас работает самое главное туннель должен работать на протяжении всего времени когда у нас дело делается без backup если упала мы начинаем с самого начала копировать эти 10 терабайт вот самое главное для репликации потом мы тоже можем использовать туннель но опять же должен должен написать хотя бы какой то скрипт костыль ный который будет и про проверять жив он не жив и устанавливать поднимать назад если он упал вот вторая опция мы можем использовать какой-нибудь backup tool за который раба умеет работать с amazon мужским из 3 потому что с 3 уже есть мы можем его использовать таких тут улов у нас уже есть несколько воли печи б крест уже год поддер имеет работать с 3j новые на рынке это волга ему тоже примерно год написан 1 но исторически мы в общем-то для всех остальных кластеров использовали воли поэтому решили хорошо мы будем в общем-то использовать воли мы сделаем мы настроим во-первых наш мастер на то чтобы он архивировать логе в истре мы сделаем боясь backup опять же поступ положим туда быстрее и с помощью кэшбэка по мы построим две реплики вот вот этот человечек это патроне который будет в дальнейшем собственно заходил ability следить за переключение мастера в случае если с ын сон сам что-то происходит вот патроне работает с эти сиди сколько это все заняла по времени чтобы сделать боюсь backup из дата-центра у нас ушло порядка 12 часов развертка бы из бекапа в облаках 9 часов я могу это объяснить в общем столько тем что с 3 у нас находится в том же самом регионе в том же самом месте что что и на наши инстанции то есть он тратит меньше времени на это на копирование этих данных ну и собственно разворачивание архива но все-таки быстрее чем архивирование вот чтобы потом догнать на наши реплики до состояния мастера с помощью накопившихся хвалов мы потратить там потратилась еще порядка 4 часов вот и дальше при в принципе эти реплики поддерживаются в актуальном состоянии просто на катуни молов и она работает достаточно хорошо надежное если у вас много транзакции происходит то эти волны переключается достаточно часто и лак репликации будет там грубо говоря несколько секунд там 510 может быть с этим уже можно жить можно какие-то ещё дополнительные эксперименты делать например мы можем перед перекинуть виртуальный из сасло его дата-центре на пик bouncer и отправить ведь readonly трафик уже в облака посмотреть как она будет себя вести в течение дня например вот мы и придумали как будем переключаться то есть это сначала картинка да да как она выглядела до того как мы начали переключения у нас опять же есть мастер есть реплика на них виртуальная пим мастер и стремится реплика стремится мастера и две реплики в амазонии на накатывают волны помощью воли мы перекидываем для начала виртуальная pen реплики на пике bouncer реплики следующим шагом мы гасим реплику это тоже в общем то важно и потому что если и если мы не используем стриминга вую репликацию топаз гаспре при когда мы делаем shutdown под груз он все-таки архивирует самой последней логе но он создает еще один дополнительный вал в который записывается всего лишь один чек-поинт если мы за промывать им одно из этих реплик то старый мастер нельзя будет присоединить без печали аренда а если у нас реплика продолжала стримить до самого конца самого со старого мастера то соответственно мы даже эту реплику не сможем обеспечить его надо подключить вот поэтому мы реплику выключаем первым потому что в дальнейшем ее можно будет без проблем перенастроить на то чтобы она aol и проигрывала из из 3 следующим шагом переносим виртуальная пи эс мастера на перевал ансар тем самым в общем то из бал избавляем мастер от нагрузки и делаем shutdown да достаточно быстрый точными чаем времени во время shadow на выключаем мастер и промоутер одну из реплик собственно с этого момента у нас все уже работает в амазоне и последним шагом мы запускаем обратно старые инстанции по сброса с новыми recovery con в которые говорят мы хотим восстанавливаться и из 3 при этом это реплика старые просто запускается как есть на старом мастере приходится запускать поддерево нет для того что батат к откатить вот эти реально там 40 или 50 байт и тайны размер записи об одном чекпоинты то есть реально у-сплит brain и нет но под груз думает что есть backup как бы копить собственно про thule thule и для которые умеют работать сестре я уже говорил есть воли печи бы крест вал g в основном мы используем боли но к сожалению он пишет все про низу промежуточные файлы на диск это долго создает дополнительную нагрузку он не умеет работать на реплики даже начиная с пользоваться 96 видео появилось опять для для этого есть пдп крест он собственно решен многих недостатков воли но один из его недостатков вот он не умеет работать с сын в традиционной inформат и информацией которую который можно из инстанса достать то есть амазона есть на всех инстансов такой специальной опять где можно вытащить информацию об данному конкретному инстансе где она находится и в том числе какие-то временные прибыли временный ключи для работы с amazon овский мапей в том числе с 3 и 5 б крест это не поддерживать элджи с другой стороны все это поддерживает он в том числе умеет делать дельта бэкапы у него можно выбрать метод компрессии по умолчанию это было z4 потом допилили добавили туда затмение z с т.д. потому что за 4 не очень хорошо живет в то же самое время воли поддерживают только из этой печи б крест работает только с за клип который в общем-то прошлый век я считаю требует много процессора жмет мне очень хорошо и самое главное достоинство лджи то что он обратно совместим с холли то есть он умеет восстанавливать бэкапы которые сделали с помощью воли вот и опять же провел ряд тестов значит что такое вот bass line на этом чарте значит на тот момент когда я эти тесты гонял наша база уже выросла до 12 читера байт и чтобы эти 12 терабайт прочитать требуется порядка 4 часов но я бы сказал ровно четыре часа мы просто упираемся в производительность она на на нашего конкретного instance а вот чтобы воли сделал полный backup ему требуется 17 часов и он сжимает наши 12 терабайт до 33 . целых две десятых терабайта он работает общем-то воли очень долго потому что он пишет временные файлы на диск в то же самое волны с братли компрессии ужинает всё до двух терабайт и ему требуется на это шесть часов все это можно опять же запускать на реплики самую интересную фишку это дельта бэкап и поскольку у нас я бы сказал базы insert он или у нас многие данные просто не меняются нам не надо их бы копить постоянно каждый раз мы можем их просто пропускать и соответственно во лжи и бы крест умеют это делать они во-первых смотрят на таймс темп файла файлов до даты файлов во вторых они могут анализировать состояние страниц изменялась от страницы или нет и вот дельта backup на следующий день занимает всего 75 мегабайт в сжатом виде и на то чтобы сделать для лета backup требуется порядка двух часов 20 минут в общем преимущество по сравнению со или просто гигантская со следующим минорным апгрейдом postgres сам мы соответственно планируем перейти целиком и полностью на волге для определенных инстансов для определенных bass вот что самое главное мы забыли мы в общем то после того как переключились были в эйфории в течение пару недель не вроде бы все работает никто не жалуется не на эту базу и все пользуются если что то не так то жар жалобы всплывает мгновенно вот мы посмотрели на мониторинг увидели то что нас был график вот это и был график концертов и тоскую здесь порядка чуть меньше двух миллисекунд если не видно это время выполнения одного inserto там куча параллельных потоков где делают эти insert и и после свечи вора у нас тут появилась такая расческа мы внимание в общем-то не обратили потом мой что это такое немного по исследовали выяснили то что дата центре у нас было от ключ отключен синхронно ска мид в амазоне мы про него забыли вот а поскольку лейкен сиу и без и так не самый лучший по сравнению с локальными дисками то его желательно отключать в чем недостаток от отключены синхронного комета и если есть стриминговый репликацию то скорее всего даже при падении мастера ваню не увидел вас ничего не пропадет если instance работает один то при падению есть некоторая вероятность что они несколько последних транзакций которые были за камень записаны вам райта hotlog будут потеряны вот значит вот этот самые нижние две миллисекунды 2 4 дальше 6 и тут от 8 не ну не пожар но мало при приятного мало по поскольку собственно эти insert и идут все равно из очередей никто об этом особо не парился но желательно не тратить производительность на ненужные вещи оставить на что-то нужно вот я тут собрал немного ссылок там патроне воли ул джипе db крест виде bouncer я думаю все знают самое замечательно на мой взгляд ссылка про которых знают не все это если вы хотите сравнить производительность и стоимость и ситу инстансов то есть такой ресурсы и ситу instances info amazon и вся эта информация есть но она раскидано по куче страничек и пока вы соберется соберете всю эту информацию в одном месте вспотеете и забудете чуть вы хотели посмотреть на этом же сайте можно просто вот выбрать я хочу сравнить вот это это и сравнить детально сколько стоит каково сколько у него процессоров сколько памяти какого производительности танки теоретической и практической и так далее вот надеюсь было полезно и если будет подобную задачу надеюсь вы сможете да если вопросы есть спасибо за презентацию есть такой вопрос изначально стадии у вас было указано что у вас сервер стоит на 10 гигабит сети да это характеристики того что после эмиграции в крал соответственно трафик хочу ходить по публичной сети соответственно там другие задержки другой за всем канал как на это отреагирует аппликации большинство приложений за исключением того приложения которые собственно эти ивенты пишет в базу они исполняют запросы которые там занимают несколько секунд для них дополнительные 10 миллисекунд light in se mi между до центра мы облаками это в общем-то не что копейки а вот это приложение которые тащат все из очереди пишет в базу мы в итоге перенесли все таки у amazon как насчет клиентов которые напрямую подсоединиться к базе но опять же в основном все все вот эти вены все вот эти информации которые сохраняются в этих таблицах это ивенты она используется танки бизнес с людьми чтобы строить какие-то отчеты их запросы работают в любом случае в лучшем случае там порядка секунды особенно знает это талант людей которые умеют написать запрос и так что он будет делать секвенцию скан даже при наличии индекса никто не жалуется нет нет нет нет конечно нет спасибо еще раз а вы все серверов облака запустили нет не все но мы как бы стремимся о том чтобы все перенести потихоньку облака но поскольку три года назад когда это решение было принято то все все вот эти старые приложения все старой базы а ник так так называем монолит и это мы не можем просто взять и перенести все один в один по этому работа по разделению всего на на какие-то изолированные куски ведется но она ведется долго муторное пока не видно краев свет в конце тоннеля виден но сколько лет счет сколько честно не знаю о нет а frontend у вас ну сама витрина магазина на ул от него и тучу облаках спасибо спасибо за доклад на следующий вопрос почему именно amazon смотрели в сторону ажура вот этот первый вопрос второй вопрос а сколько понимая вы в облака давайте по упал и через насколько я помню то делалось сравнение различных систем и в тот момент предложения от amazon и показалась более привлекательным не только по ценам а потому какие сервисы они предоставляют это было более трех лет назад с тех пор у нас в общем долги контракты просто так с него соскочить нельзя конечно мы смотрим в принципе на на google и на жюри и так далее вот это кстати 1 из 1 1 одна из причин почему мы стараемся не использовать rds например если мы хотим все таки с мигрировать данные сэргэ сбежать будет тяжело и если мы запускаем под газ сами-то мы можем собственно на наладить или опять же либо репликацию обычную либо архивировать волн и там по рапире про значит проигрывать и сделайте общем то практически бесшумный переход из одного облака в другое такие 2 вспышки экономику ты сказать что хочется молока хоть где-то в полтора больше для для вот данного конкретного про для данной конкретной базы это более чем в два раза дороже по сравнению с тем что было в дата-центре вот входе в это сравнение обслуживания зарплатный фонд дело в том что в дата-центре нам тоже надо примерно такой же обслуживание проводить нам надо регулярно делать апгрейды под груз ахаха хотя бы минорные нам надо софт обновлять операционку в принципе это ничем не отличается от того что облаках сейчас происходит не понял да еще такой вопрос насколько я понял вы принесли вас данных в облаке именно для биосистем таких условных твиттере на ваши хочется это у каждого есть так называемый скелет в шкафу у кого-то этот скелет общем то растет и развивается настолько что иногда он превращается в монстра как в нашем случае значит и томазо изначально задумывалось то что как как какое-то временное решение для того чтобы агрегировать вот эти бизнес ивенты и считать какие-то статистике делать отчет и это не не для не для пользователей на она на наших которые делают заказы на сайте вопрос для тех баз данных которые мне удалось мигрировать вот проблем ты осталась актуальной которая изначально вас было то что у вас появляется железки очень долго появляется нужно потом ну все новое значит разработку ведется на всех фронтах все новые приложения сразу же сейчас разрабатываются для того чтобы где плоть губерний this и по возможности мы даже база держит кубер нить из поскольку мы реально двигаемся в направлении микро сервисов то как правило базы тоже становятся маленькими вот поскольку все там автоматизировано no naku вернитесь то мы имеем возможность там не знаю запустить сотни мелких бас которые там размером где десятки мегабайт иногда там типа сотни мегабайт данных да да да да спасибо персональные данные вы в облака как запускаете контролируйте их соблюдайте законы хороший вопрос не уверен то что имею право на него отвечать то есть персональные данные есть но мы стараемся в общем то по возможности обезопасить все максимально во-первых amazon дает возможность там контролировать кому-то даешь доступ как ты даешь доступ с помощью security group его и прочего вот на и без можно включить шифрование для бэкапов тоже можно включить шифрование какого стабильность характеристик между разными эстонцами и без потому что я так правильно понимаю крыму зон внутри своя кухня и скорее всего не как-то шариат ресурсы между разными без инстанциями железом на которыми без работает ну то есть стоит ожидать никого разброса параметров ну скажем так для если нужна какая-то гарантированный производительность мадонна настаивать на том чтобы покупали вот эти pro-vision таубс которые стоит безумно дорого поэтому мы в общем то выбираем джипе то на джипе туда там вас возможно некоторые фрукту ации то есть и сейчас он значит работает чуть лучше сейчас чуть хуже вот как как говорится это вам решать для самых критичных вещей мы все-таки предпочитаем использовать где где н где нужна стабильная вот эта производительность по epson мы используем три инстансы потому что все таки ла-ла-ла локальный диск in войне он решает все все вот эти проблемы просто отбрасывать при этом на самом деле стоимость обладания вот этими и 3 инстансами намного дешевле чем запустить тот тот же самый ту ту же самую аврору к нам на подобный грузят важный момент вашей презентации получается что вы не делаете решение продакш его уровня в котором есть очень высокие требования к времени отклика если нам нужно высокие требования к времени отклика мы используем майтри вот этот реальный у space pro от протокол монстров шкафу который вы вырос значит я должен наверное тоже сказать то что эти винты там не остаются на вечно мной многие из вот там есть ли там шум полюсе настраиваемые не некоторые мы просто удаляем через месяц для некоторых необходимы для некоторые винты необходимо хранить год некоторые к сожалению всегда вот готовятся какие-то альтернативные решения для для вот этой проблемы чтобы вообще отказаться от этой базы вот но как говорится без дела делаются очень долго особенно в больших компаниях насчет вот этих ой трин солнца с супер быстрыми созданы как известного создает некий параметр тебе w да когда ты когда гарантируется производительность после определенного объема записи после этого он начинает деградировать собственно как этот вопрос решается если понимаем например что производительность данного конкретного инстанса нас не устраивают но в амазоне можем в любой момент от instance прибить получить замену 15 рабат лежит ну значит копирование милон в нашем случае это 12 терабайт и в общем-то копирование сын солнца на ын сон занимает порядка девяти часов и нет если смотрите и если у вас констанцу подключен сколько там дисков и по 6 дисков например вот этих inwin е1 и если один из них например вообще сдох то они не будут делать онлайн замену в нашем случае например для того чтобы получить в общем-то весь весь историю чем мы должны сделать raid 0 если этот диск пропал то все нам ничего не остается чем перелить чем перенесите перри инициализироваться здравствуйте и спасибо за доклад хотел задать несколько с хотел задать несколько вопросов а причина изначальный периода причина переезда в amazon было в проблемах проблемы хостер с железом если пандусом самая изначальная причина значит в некоторый момент в нашей компании был взрывной рост и собственно сисадмины люди которые занимаются обслуживанием железу дата-центре они просто не успевали за всем этим и amazon или либо любые другие облака в общем-то позволяют сократить это цикл разработки очень сильно конечном итоге человеческие время тоже стоит денег и она стоит гораздо дороже железо то есть я не говорю то что вот например если нам надо больше производительность мы просто возьмем сейчас железку побольше до какого-то момента tata nano работает но если реально мы реально упирались в то что от момента там да даже если это какой-то стандартные железо которое доступно у хостера мы мы его можем подпол получить там в течение дня все равно там весь этот цикл по выкладке нашего образа операционной системы и по deployment приложения занимает время это время в общем-то изменять измерять измеряется как минимум в часах в амазоне тепло и вот в данном случае там составляет несколько минут ясно скажите если бы вы сказали что стало дороже нам в два два с половиной раза вы заранее предполагали что суд стоимость такая вот или это было так и нет мы знали мы знали на что идем то есть в заранее смогли оценить нас предала надо давить на понятно хорошо спасибо очень все хорошо если вопросов больше нет то спасибо большое"
}