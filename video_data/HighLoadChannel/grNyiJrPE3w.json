{
  "video_id": "grNyiJrPE3w",
  "channel": "HighLoadChannel",
  "title": "Транзакционная репликация в YTsaurus / Руслан Савченко (Яндекс)",
  "views": 497,
  "duration": 2346,
  "published": "2024-10-29T02:56:24-07:00",
  "text": "и Руслан нам расскажет что такое рациона репликация в териус и для чего вообще нужно Приветствуем Руслана и друзья сбоку есть QR кодик Вы можете по сниться к чату онлайн задать вопросы и пообщаться поже спикером а что такое я уже это название это Странная история у нас проект назывался Wi и когда Мы выкладывали в Open Source придумали название выйти за ius оригинально Я думаю ты расскажешь ещё не менее интересную историю Приветствую у а Всем привет Меня зовут Руслан Савченко Я на Хай лоде выступаю в первый раз и хочется сказать большое спасибо организаторам за возможность выступить и тем кто пришёл послушать и тем кто смотрит трансляции за то что проявили интерес к моему докладу я работаю в Яндексе и вот как я уже сказал значит вступление долгое время проект в котором я работал назывался или по индексом его называли ть и когда значит он вообще шёл долгое время то мы были такие довольно закрытые Мы наруже про себя не особо рассказывали а в этом году мы вышли в Open Source и стали более открытые стали значит рассказывать про то что у нас всё такое происходит и в этом докладе я расскажу про то как у нас работает репликация между различными кластерами yarus и то как мы с помощью этого добились э отсутствия таймов При обновлениях А о чём будет собственно доклад Какой Его план значит вначале я расскажу что такое риу возможно не все в зале знают а потом мы вспомним Как работает репликация классических СУБД А и дальше поговорим про три способа ведения значит репликации в самом vz А первый способ - это такая репликация исключительно на стороне клиента то есть мы тут как бы не при делах второй способ - это репликация через единую очередь Вот и третий способ это такой Значит некоторые метаморфоза идеи с единой очередью благодаря которой мы собственно добились отсутствия таймов при обновлении но для начала что же такое вообще тиус мы позиционирует как инфраструктура для значит инфраструктура для больших данных наверное в зале есть люди которые знакомы там с хопом с хапов стеком вот можно себе Представлять что уус - это какая-то значит обёртка вот некоторого набора таких технологий похожих на хавский стек но завр всё в Единый продукт и написано на c+ и возможно вы уже не первый раз слушаете проти заус может быть заходили на сайт и скорее всего вам говорили что это система mce Да вот Возможно вы слышали ещё что можно делать некоторую онлайн аналитику поверх данных M Ну например есть движки Click House Spark вот их можно запускать поверх данных маюс и это в общем такая часть про которую обычно мы рассказывали когда говорили проу в осос но если говорить прос в Яндексе то есть ещё другая такая другая сторона этого проекта которая скорее про обработку данных в реальном времени да то есть например вы хотите быстро читать и быстро писать У вас возникает что-то то что называется storage и вот у нас есть отказоустойчивый реплицировать с некоторой разницей что у нас есть распределённые транзакции там можно даже задавать некоторые SQL запросы поверх этих данных и ещё одна часть - это распределённая очереди сообщений то есть возможно кто-то из вас был на предыдущем докладе как раз говорилось про что такое очередь сообщение правда в другой системе то есть да там например есть ещё Апа кавка там Панда вот что-то вроде этого Ну как всё можно представить У вас есть некоторый топик топик шардирование в кажы шарт выпиваете данные в конец есть рого в конец при этом можете читать Вот из этого топика уже какие-то диапазоны рока вот сегодняшний доклад он вот будет про часть Ну и там немножко про вот эти самые очереди именно если говорить про популярность нашего продукта то внутри Яндекса он довольно популярен нами боле коман ври Яндекса Т В нашей системе то их порядка это вот именно данные таблиц грубо говоря то есть можно посмотреть нае Яндексе ещ больше но это именно вот выделенные таблиц У нас есть даже отдельные кластера вот строго под таблице И если мы посмотрим на то какой объём данных льётся в эти кластера то это будет 15 в секу чистых пользовательских данных Дато между дата центрами Внутри там будет какой-то оверхед за счёт структуры данных которая это хранит на диске Но вот чистые пользовательские данные будет 15 Гб в секунду реальная нагрузка на железо Скорее всего на порядок больше а Я немножко расскажу про архитектуру наших этих самых Q таблиц Ну вот как строить New SQL систему Да а значит система шардирование и вот шарт у нас как правило обслуживает какая-то одна машина то есть каждый шарт обслуживает какая-то одна машина бывает сценарии когда Значит один шарт обслуживают несколько машин У нас есть такой такая поддержка Но в целом это маргинальный случай и обычно один шарт обслуживается одной машиной Ну либо одним контейнером там в зависимости от того как вы привыкли к деплой вот вопрос А как же тогда отказоустойчивость да берётся если у вас одна машинка и ответ на него шем рованный Вот то есть в есть слой распределенной файловой системы и вот в этом слое распределённой файловой системы есть такое понятие как реплицировать как-то похоже на алгоритм консенсуса вот наверно кто-то знаком с арим чтото похожее нари единст раз нет выбора Лиде лидером является собственно Да она в единственном экземпляре но реплики она пишет вот некоторым подобным образом за счёт этого допускается отказоустойчивость то есть мы любую реплику журнала можем взять и потерять например при там выпадении машинки случайно а какие же мы тем самым предлагаем гарантии а пользователь всегда пишет в какой-то транзакции и транзакции у нас распределённые отказоустойчивые атомарные транзакция может затрагивать несколько шардов и онный транзакции у нас сдела с помощью протокола двухфазного комита при этом координатором двухфазного комита выбирается некоторый шарт нашей системы да то есть некоторый работает при этом координатором за счёт этого транзакции получаются отказоустойчивые вот а для тех кто там знаком с mvcc и snaps isolation можно сказать что у нас изоляция транзакций реализуется по модели mion значит для тех кто не понял что я сейчас сказал я вот буквально на пальцах объясню что имеется в виду Значит каждый шар системы он представляет собой что-то наподобие rdb и когда транзакция записывает новые данные то эти Новые данные для некоторого ключа просто значит сохраняются рядом получается что мы Одновременно храним несколько последней версий нашей строчки с ключом и при чтении мы вообще говоря можем выбирать а Какая из этих версий нас интересует У нас есть режим синхронного чтения когда мы говорим что мы хотим строго последнюю версию и синхронное чтение оно даёт такие приятные гарантии как то есть то что вы записали вы как бы обязательно увидите при чтении возможно вам такие гарантии слишком сильные вы не хотите ждать на всяких блокировках и вы можете пользоваться асинхронным чтением оно быстрее работает но оно не даёт гарантий именно какой-то вот свежести значит обязательной свежести данных это будут будет некоторая там одна из прошлых версий и есть чтение помпу То есть вы указываете какой-то на момент которого вы хотите увидеть состояние ваших таблиц и система показывает то как была как выглядела ваша строчка именно на этот момент времени и вот чтение по по дефолту используется в транзакциях то есть транзакции в уровне изоляции snation Они видят состояние базы на момент старта транзакции Да а как бы при коммите у всех законы значений возникает Рим развилась во времени то ВС что я вам рассказал оно было у нас уже в чем году То есть вчем году у нас был отказоустойчивый шардирование там отдельных НОД или там даже целых стоек Если это достаточно большой кластер Вот и дальше Эта система она развивалась в сторону повышения гарантий доступности за счёт того что мы брали этими кластерами как-то вот собирали разные комбинации например ВМ году мы сказали что вот у нас стоит три независимых кластера в каждом дата-центре Да у Яндекса тогда было три больших дата-центра и в каждом стояло по кластеру тиус и мы пользователям говорили вот как бы пожалуйста вот у вас там есть три кластера Пользуйтесь как хотите А значит это было не очень удобно поэтому мы поставили для репликации ещё отдельную очередь значит пользователь писал в очередь а очередь уже дальше до реплицировать такую систему мы запустили вду А в ре то есть в этом году мы взяли и немножко модифицирован Так что она уже больше не имела единой точки отказа то есть в прошлой системе очередь была единой точки отказа в новой конструкции единой точки отказа нет И мы можем переживать значит обновление без Дан тайма для пользователя то есть пользователь все свои данные могут видеть Но прежде чем говорить про репликацию ипо репликация в классических классической У вас есть некоторый первичный Мастер и некоторый вторичный Мастер и настроена асинхронная репликация из первичного мастера во вторичный мастер мастер вторичный сервер работает такая репликация через то есть Север пишет на свой диск у него там возникают новые фатва Ну вот сейчас я его к себе скачаю и примен и у такой репликации довольно много проблем но во-первых значит фловера из коробки такого работающего нет но скорее всего если вы этим пользуетесь то какой-то фавер сбоку прикручена и вы настроили или там за вас настроили вы пользуетесь каким-то уже продуктом баз данных вот возможно у вас это есть но есть ещё другие там довольно серьёзные проблемы у асинхронной репликации одна из них что э версии должны быть одинаковы Если вы откроете документацию постгрес то там будет написано что для асинхронной репликации версии должны быть идентичные вот Ну и это можно себе понять потому что х Log такая-то довольно системная штука и в ней хранятся разные вещи как бы применимые к логике работы самой СУБД и логично что они должны быть поддержанные при проигрывании этого хд Лога Вот Но для нас получается что если бы мы хотели такую систему об нам пришлось потушить по е есть самая неприятная особенность асинхронной репликации то что вы можете потерять данные вот этот вот самый последний кусочек который уже записал А ещ не успел подтянуть может пропасть если внезапно выключится и дальше можно тему эту улучшать и скорее всего если вы пойдёте по такому пути то вы придёте к тому что называется протоколом консенсуса то есть вы переизбрали возьмёте протокол Пакс или raft я не буду рассказывать как работает Пакс или ф Приходите завтра кажется в Сингапур там вам расскажут в деталях как протокол ф работает я скажу немножко для интуиции что эти протоколы они позволяют значит делать у всех участников и дают строгую гарантию А вот по тому кусочку который ещ не на всех участников он применил или нет то есть вот протокол даёт здесь однозначный ответ Если асинхронная репликация ответа не давала Вот кво репликация консенсуса она даёт здесь ответ такая репликация гораздо надёжнее да то есть мы можем потеть любого участника прожит но а значит из-за того что это всё ещё Ra Head Log и мы хотим видеть реплики идентичными Да в это в этом консенсус реплики получается абсолютно идентичные у нас снова возникает проблема с версиями что а версия программы должна быть одна и та же и возможно есть люди которые умеют работать с такими протоколами и при этом значит иметь разные версии программного обеспечения в одном консенсус Но вот например у нас вти заус подобная репликация через км используется в нескольких разных местах То есть например вот Log шарда про который я уже рассказал или на нашем сервере метаданных тоже используется вот э такой консенсус и мы за много лет привыкли к тому что у нас версии вот такого кворума Это консенсуса они строго идентичные то есть мы не хотели бы видеть там разных версий это неудобно с точки зрения программирования То есть каждый раз когда вот возникает необходимость поддержки разных ээ версий в таком в такой системе это прямо целая отдельная головная боль сравнимая по сложности с разработкой фичи вот а но О'кей значит э вот мы поговорили про кворум с обновлением там тоже всё понятно Давайте вспомним что мы вообще-то говорим про urus и подумаем Вот хорошо у нас есть один дата-центр да Вот я как бы сейчас рассказывал про кластер Азау так вот в рамках одного дата-центра а теперь у нас центров несколько и нам нужно подумать А как бы нам здесь сделать такую систему которая была бы более надёжна чем просто кластер зас в одном дата-центре Ну и здесь такой может быть ответ напрашивается но Давайте просто возьмём вот этот кластер разложим намажем на несколько дата-центров вот эти все консенсус которые там есть аккуратненько распределим так чтобы э там узлы этого консенсуса были бы в разных дата-центра мы бы переживали выпадение дата-центра произвольное Да но Проблема в том что в таком случае нам для обновления придётся всё тушить то есть нам придётся тушить целиком кластера и у этой системы будут Дан таймы при обновлениях Вот то есть да там отказ дата-центра Мы переживаем А вот при обновлениях всё равно данта и мы решили пойти немножко другим путём Мы решили сказать следующее Окей У нас есть отказ при обв значит при обновлении и есть отказ при внезапном уходе дата-центра Ну давайте просто вот эти два сценария совместим и в каждом дата-центре рнм центр ул Ну хорошо это вот там Внезапный уход дата-центра Хотим мы обновлять Ну как бы мы уже умеем переживать Внезапный уход дата-центра Ну это вот почти тоже самое да то есть обновление как бы мы даже Их делали особо не заморачиваюсь на объявление что мы будем обновлять но такая схема она немножко сложна для пользователей То есть получается что все задачи вообще репликации данными между Такими кластерами они ложатся на пользователя и это выглядит для пользователя вот он видит там три независимых кластеров it зас он делает там три реплики Да а и сам вообще в них как-то пытается писать пытается читать из них Но если тут вообще немножко задуматься то оказывается что пользователю клиенту с этим работать не просто у него возникает много открытых вопросов Ну например мы там записали в первый кластер а при записи во второй кластер случилась какая-то там неприятная ситуация там конфликт транзакций или там ещё какая-то ошибка вот что нам в этом случае делать да А Или мы писали писали значит записали в первый кластер а перед тем как записать во второй у нас просто клиент упал может же клиент упасть вот внезапно взял отвалился вот как бы а потом поднялся и что ему теперь делать первый кластер записал А второй нет вот а или там Второй кластер вообще недоступен То есть получается а недоступен при недоступности дата-центра это там на сутки может быть то есть там работы в дата-центре они занимают целый день вот и получается что они где-то должны накапливать вот этот вотт Лог своей информации котора им потом записать вернувшись после тайма кластера и но возможно как бы даже есть в зале люди которые знают ответы на эти вопросы там сами придумывали или что-то читали и для таких людей у меня есть риторический вопрос со звёздочкой А можно ли это сделать так чтобы чтения продолжали быть эффектив читали из ближайшего лае мы не стали придумывать на него ответ мы поняли что значит наши пользователи начинают заниматься вопросами которые вообще говоря должны решать разработчики инфраструктуры то есть ответы на такие вопросы должны давать люди которые там знают как тот же рафт работает Это не должны делать разработчики сервиса которые думают про бизнес логику Вот и поэтому мы значит конструкцию улучшили мы поставили на чёрную очередь перед нашими кластерами конструкци стала выглядеть вот так вот у нас были Три разных дата-центра с тремя независимыми кластерами мы поставили ещё один кластер который сделали центровым Да вот как я вам рассказывал и в нём хранится только очередь репликации пользователи пишут в очередь а из очереди данные попадают на кластер очередь надёжно мы это обеспечили Подождите Вот раньше Вы бы писали транзакции обновляю по ключу и у вас наверно моли быть конфликты по ключу А теперь у вас очередь и какие же конфликты в очереди в очереди не может быть конфликтов просто в конец записываете значит Да Действительно это проблема И с этой проблемой нужно разбираться отдельно мы выбрали такой способ одна из реплик она выбирается синхронной это обязательна реплика на которой есть сжи данны эту саю синхрон реплику это происходит в единой транзакции и с одной стороны мы сохраняем данные для репликации в те реплики которые не синхронные а с другой стороны мы проверяем валидацию конфликтов транзакций через синхрон реплику И в такой конструкции вообще любая реплика может быть назначена синхронно и главное чтобы она имела все достаточно свежие данные очередь у нас надёжная надёжная она благодаря тому что шарды хранят логи свой реплицировать как в прошлый раз в принципе у нас кластера остаются доступны когда с очереди что-то происходит но очередь мы действительно иногда хотим обновлять и здесь срабатывает датам при значит недоступности очереди вот ну ладно мы посмотрели на запись Давайте пом сри на чтение вот чтение я напомню у нас бывает синхронное асинхронное чтение по стмпу и вот синхронное чтение оно работает так же как и чтение пойм смпа То есть ему этим двум чтения Нужно точно знать А где самые свежие данные или где точно есть данные по такому стмпу поэтому синхронное чтение и чтение по стмпу идёт через очередь потому что на очереди собственно хранится вот эта информация Почему она хранится на очереди Ну потому что нам при записи данных нужно как-то валидировать что мы пишем Вот именно действительно в ту синхрон ную реплику которая сейчас по-настоящему синхронная бы эта информация просто лежит на очереди асинхронное чтение может работать Напрямую Вы просто указываете кластер из которого вы хотите прочитать и идёте в этот кластер И если мы посмотрим на как бы табличку гарантий которая тут получается то получится что синхронное чтение асинхронное чтение Ой нет синхронное чтение и чтение пойм оно значит использует очередь репликации оно гарантирует све но оно не доступно при обновлениях очереди асинхронное чтение в свою очередь оно не идт через очередь поэтому оно не даёт никаких гарантий Причём здесь это даже может быть довольно большой лак уже потому что у нас репликация может иметь большой лак Вот Но при этом асинхронное чтение доступно при значит обновлении очереди при до тайме очереди Вот и дальше можно как бы задавать вопрос Ну хорошо а вот эта вот недоступность она как бы большая или небольшая и у нас вообще говоря мы старались эту недоступность делать порядка одного часа в год То есть можно шутить что мы даже там даём тир доступности 4 детки вот мы никогда этого не заявляли прямо но в целом вот оно как-то вот так вот получалось и в общем-то это была довольно успешная конструкция почти все пользователи о которых я вот рассказывал на слайде в начале доклада они используют вот такую схему просто мы на ней выросли Вот и Ну например в прошлом году на был доклад Егора Хайрулина про то как реклама там делает й процессинг вот в этих процессинга Егор в частности упоминал он Это наверно называл и это конструкци который работает именно в такой репликации вот но всё же некоторым пользователям это не нравится да они говорят Не ну подождите как бы там час в год но этот час у вас сконцентрирован то что вот там Полчаса вы обнов кластер в Единый момент нас такое не устраивает нам нужно чтобы у на зака 24 на вообще без перерыва и поэтому мы стали думать а Можем ли мы как-то избавиться от очереди Да и чтобы как бы вообще решить этот вопрос Давайте посмотрим на то А чем в принципе занимается очередь какие у неё есть задачи и может быть можно как-нибудь взять эти задачи перева и избавиться от Дон тайма значит чем же занимается очеред во-первых очеред непосредствено данные которые ре то что клиент записал это понятно Вот на очереди хранятся метаданные то есть некоторые данные о состоянии реплик Да вот та Какая сейчас синхронная или какая сейчас не синхронная это тоже лежит там это нужно для валидации при записи и ещё очень интересное в очереди у нас лежат координаторы двухфазных коммитов Почему они лежат именно там ну потому что координатор двухфазного комита должен быть отказоустойчивый он должен переживать внезапное выпадение дата-центра оста подзе транзакции Вот я думаю что в зале много людей читали книжку Мартина клемана это проектирование высоконагруженных приложений вот он там несколько страниц посвятил тому что бывает если у вас координатор двухфазного комита внезапно отваливается Да всякие разные интересные истории как потом люди мучаются Мы конечно же такого не хотим мы хотим чтобы это было надёжно и получается что как минимум для координации двухфазных котов сущность то есть шарт который делает эту координацию он должен хранить свой Лог на разных дата центрах Вот и после того как мы на это всё посмотрели мы приходим непосредственно схеме которая позволила нам избавиться от таймов при обновлении она разрабатывалась под кодовым названием хаос Я думаю вы сейчас посмотрите на картинке будет понятно почему она так называется вот но немножко для начала вводных то есть первая главная идея мы взяли и сказали что у нас больше нет такой выделенной единой очереди мы на каждый кластер тиус положим свою очередь то есть здесь у нас вот три дата-центра в каждом свой отдельный Независимый кластер тиус и в каждом данные и очередь данные и очередь при этом Конечно мы хотим чтобы эти очереди и эти данные были идентичны между собой и мы за этим следим запись Когда у нас идт то запи идёт в синхронной реплики Да и очереди Но в этот в случае хаоса а очередь начинает работать так же как реплика То есть она может быть либо синхронная либо не синхронная и при записи мы пишем просто во все синхронные реплики то есть все синхронные очереди и все синхронные реплики с данными и вот здесь на картинке две синхронные очереди да очередь один и очередь два и синхронная реплика с данными данные данные один очередь нам позволит потом дорелиза конфликты между транзакциями реплики которые не синхронные они могут подтягивать данные из любой очереди которой им больше нравится да то есть это может быть там либо ближайшая очередь это может быть просто очередь в которой данных чуть больше а это может быть по-настоящему синхронная очередь Да текущее как бы это неважно на протокол это не влияет вот и получается что если мы сейчас посмотрим Просто на очередь и на данные то у нас получится что в общем-то мы задачу решили да мы снова вернулись к схеме из трх независимых кластеров где Мы переживаем выпадение любого кластера Да потому что они как бы не связаны друг другом падени любого дата-центра и можем спокойно обновлять кластер говорить Ну это также как как будто дата-центр ушёл единственное нам нужно всегда писать хотя бы в две очереди да Потому что если у нас будет Внезапный уход дата-центра нам нужно чтобы у нас осталась одна очередь со свежими данными чтобы из неё просто долива какие-нибудь остальные реплики но Давайте посмотрим на то что мы сделали с метаданными из координации двухфазных коммитов значит я напомню что нам для координации двухфазных коммитов обязательно нужны такие Крос DC сущности и мы это сделали следующим образом мы сказали а давайте вот мы сделаем Просто два небольших кластера ВУС которые Крос центровые Да мы это уже умеем делать и мы сделаем так чтобы вот эти метаданные о состояния репликах мы могли бы перекидывать просто между этими кластерами да там данных немного там для каждой таблицы Ну просто состояние Какие реплики сейчас синхронные какие нет Да там представьте себе небольшой Джейсон там ени килобайт на таблиц То есть это в принципе всё можно перекинуть и координации двухфазных коммитов мы тоже как бы скажем сделали переезжаю на другой кластер но немножко не в том смысле что мы транзакцию берём и переводим её координацию на другой кластер мы говорим что ну сейчас хорошо мы вот Давайте на кластере а больше не будем начинать новых транзакций А теперь вот все новые транзакции будут координировать на кластере B Ну а транзакции на кластере а потихонечку Там они закатятся портятся мы подождём когда значит они там закончатся вот и получается что теперь Мы переживаем и отказы дата-центров и желание обновиться ну отказ дата-центров для метаданных мы понятно почему переживаем потому что у нас они центровые просто кластера а обновление мы можем делать Ну потому что мы можем брать и закрывать один кластер уводя всю работу с него на другой если мы посмотрим на чтение то чтение у нас получается вместо очереди теперь использует метаданные Да потому что нам для синхронных чтений и для асинхрон и для этих для чтений по таймстамп нужно прийти именно в ту реплику где эти данные точно есть а это информация как раз лежит на метаданных Вот то есть синхронное чтение работает через метаданные асинхронное чтение опять же может идти на любую реплику которая чем-то понравилась Вот при этом не идёт через метаданные если мы посмотрим на гарантии то Увидим что синхронное чтение и чтение помпу оно теперь и даёт гарантию свежести и доступно при обновлениях потому что мы всегда следим чтобы при обновлении метаданные оставались доступны Да мы переводим на доступный кластер асинхронное чтение как работало счит работало без тайма так и работает без тайма ИС покат обновление у нас три больших кластера тиус на каждом своя очередь на каждом свои данные и два маленьких кластера координации Да кластера с метаданными А иб и Допустим мы хотим взять и обновить первый большой кластер мы просто берём его Выключаем обновляем никому Можем даже об этом ничего не говорить вводим в строй он дорелиза Мы хотим сделать обновление кластера с метаданными мы запускаем миграцию да ждём пока эта миграция завершится да то есть Ждём когда все транзакции которые координирования после этого его обновляем там уже ничего интересного нету И снова вводим в эксплуатацию Вот Но я вам рассказал про значит лью сторону тиус рассказал про то как Мы переживаем обновление Т без Дан таймов и если вам понравился мой доклад проголосуйте за него Спасибо за внимание Руслан агроман тебе спасибо за доклад маленький Президент организаторов и спонсора Газпром нефти Спасибо у нас вопросы коллеги Не забывайте голосовать за доклад и давать обратную связь Это реально очень для они и с вопрос да Привет Спасибо большое за доклад вот у меня такой вопрос Правильно же я понимаю что когда клиент хочет записать какие-то данные он пишет в две любые очереди из трёх ему доступных Ну в случае если у нас три кластера верно в хаос най репликации действительно так это делается для того чтобы можно было внезапно пережить выпадение любой из очереди Ну просто если ты будешь писать в одну то когда эта очередь уйдёт ты окажешься в ситуации что у тебя нет свежих данных дано значит что у нас очереди между собой должны быть консистентная репликация очень аккуратно следит за консистенцией консистентность между очередь и между данными то есть А ну то есть там там есть кворум этих очередей вот там есть нет Нет там где-то делается не через кворум у тебя просто ну все все ну как бы запись идёт таким некоторым потоком и этот поток однозначно воспроизводится та Везде где это нужно Да ну то есть подожди Ну когда вот очередь выводит из на обновление вводится она потом должна реплицировать данные Где Где гарантия что она будет консистентная после тайма то она знает ну как бы свою текущую позицию Она смотрит на другие очереди увидела что они ушли впер это ну как бы можно легко себе представить Представь себе что это вот очередь из топиков Да не я это понимаю но только это же работает только если у нас в остальной очереди нет потока записи коллеги это отличная тема для дискуссии в кулуарах мы можем это обсудить и разобрать тем более Завра вы будете на стенде 14:30 Ну такой вопрос Лучше меня поймать да и у нас следующий вопрос А привет такой вопрос как происходит й Когда у нас выпадает синхронная реплика и не может ли быть так что у нас разные клиенты считают в данный момент синхронными разные реплики И что тогда происходит А хороший вопрос значит этот я отвечу А смотри вот мы к синхронной реплике идём вот в случае хаоса через метаданные А в случае этот там в случае репликации через очередь через очередь Да и то есть мы всегда проходим через точку которая может сказать а вот сейчас в текущий момент где у нас нынешняя синхронная реплика и мы знаем что синхронная реплика она самая свежая это вед может Измени в любой момент может Измени стартует запрос Да и семантика такая что ты получишь ответ в какое-то время между стартом и финишем Да И вот эту прямую себе Представь Вот где-то на этом промежутке ты попадёшь туда где лежат метаданные и они тебе скажут А какая реплика сейчас синхронная на вот этот момент времени ты на неё пойдёшь и получишь данные на вот этот момент времени а потом когда-то они к тебе придут в виде ответа тут ну неизбежно Вот это ну как бы в распределённой системе Нужно точно как бы задавать вопрос какая семантика Да чтения Ну ты ты не ты не можешь сказать следующее ты не можешь получить ответ и сказать что А вот больше ничего в системе не было кто-то мог что-то записать пока этот ответ шёл до тебя Ладно Мне кажется это тоже для дискусии вопрос это прямо интересная тема Как получить ответ из будущего и быть в нём уверенным я я вам Предлагаю это имплементировать всё-таки и у нас следующий вопрос а Руслан Спасибо за доклад Сергей из команды Тарантул У меня вопрос Ты говорил что одна из главных задач которую вы решали - это отсутствие разных версий системы одновременно в одном Сета правильно Ну как сказать мы мы просто привыкли работать в таком сеттинге мы мы Ну мы вот у нас был выбор Да мы могли пойти по пути когда мы бы решили что ладно давайте там давайте строить процессы разработки тестирования и так которые будут учитывать что у нас могут быть разные версии в одном Вот именно консенсус но мы решили что это будет очень сложно мы облома еся и Давайте пойдём по-другому пути У меня вопрос при обновлении которые ты показал Как пример а вновь обновив очередь и её кластер оказывается новый версии други А хороший вопрос Хороший вопрос значит здесь вот между кластерами работает отдельный протокол Да кластера могут быть разных версий то есть наша даже единая транзакция между кластерами Она работает между кластерами на разных версиях и мы как бы аккуратно следим за этим и следим за конечно за тем чтобы версии наших кластеров не разошлись там супер сильно не было бы такого что у на один кластер там свежий а другой мы последний раз обновляли 5 лет назад и всё сломается на это мы смотрим но хороший вопрос Спасибо а какой у вас получается максимальное расхождение версии класе унос примерно несколько релизов в го чтобы расхождение версии было бы на единичку и у нас следующий вопрос наверное последний Да Привет Спасибо большое за доклад смотри если я правильно понял В случае с репликации Да вы переключайтесь с одной на другую то есть там грубо говоря слева транзакции заканчиваются справа мы вот теперь она Prim Она же должна дождаться Да да да конечно Конечно вот я Я показывал и кажется что это датам нет Смотри ты новые транзакции начинаешь стартовать уже на новом кластере вот доступно два кластера Но у тебя данные ещё не самые свежие они не могут проваливаться новые транзакции поди подожди пож подожди ты говоришь про координацию коммитов или про вот тот кейс где мы хотим вывести один из кластеров Да мы с него транзакции переводим другой кластер метаданных и кластер координации Да наверное я про сам данных нету метаданные они маленькие они переезжают вот метаданные перез может возникнуть порядка секунд Хороший вопрос в нашей системе вообще таймы там на единицы секунды там даже больше они могут возникать Ну нада случайно выпадет Мы не сразу это поймём Вот и вообще то есть такие скейлы Дан таймов на десятки секунд Тае больши скорее вот обновление у нас тяжелое Оно занимает вот полчаса от полчаса до часу это серьёзно недоступность выпадение дата-центров это вообще может быть на день да работа на дата-центре с утра до вечера Вот и Ну вот мы как бы хотели избавиться отн таймов при таких сценариев Да спасибо спасибо всем за вопросы и у меня на последний вопрос Выбери лучший вопрос на й взгляд и будет подарок НЕФ А значит мне понравились все вопросы все вопросы были такие чёткие по делу и а он один Да он один значит я наверное выберу человека который задал вопрос прочтение потому что это действительно такой очень тонкий момент на понимание работы распределённой системы вот в распределённой системе вам как-то за эту связь Нужно следить Спасибо всем что к нам пришли Спасибо кто нас слушает смотрит онлайн оставайтесь с нами у нас будет тих толки ещё будут два полтора дня интересных докладов"
}