{
  "video_id": "05jGuGT_nkY",
  "channel": "HighLoadChannel",
  "title": "Как достать всё что угодно со всего интернета / Илья Кучумов (Яндекс)",
  "views": 606,
  "duration": 2471,
  "published": "2023-10-06T07:17:01-07:00",
  "text": "Так давайте начинать Меня зовут Илья Я из Яндекса И сегодня я буду рассказывать какие алгоритмы изучения структурированных данных мы используем в поиске человек я не самый публичный поэтому давайте начнем со слайдика про меня Я работаю в поиске Яндекса и руковожу отделом который занимается разработкой товарного поиска мы занимаемся задачами как получением данных индексации их так и ранжированием улучшением качества ранжирования и различными интеграциями товаров на Большой поиск Яндекса до того как руководить товарным поиском я занимался качеством поиска лавки и руководил разработкой турбо-страниц сегодняшний мой кейс будет построен вокруг практического кейса запуска товарного поиска поэтому Давайте кратенько про него расскажу что это такое чтобы сейчас не заморачиваться про различные интеграции с поиском проще всего смотреть на товарный поиск как вкладочку вот как с картинки видео карты вот так есть и товары вот это вот эта вкладочка сам товарный поиск выглядит как все поиски в интернет-магазинах У нас есть Товарная строка У нас есть фильтры и у нас есть карточки товаров на выдаче внутри карточки товара вы можете зайти туда и до уточнить параметры товара который Вас интересует например там цвета гигабайты и увидеть цены на интернет-магазины во всем интернете и это есть наша Ключевая задача которую мы будем сегодня обсуждать именно вот в этом блоке цен у нас должны быть все предложения в интернете как на маленькие магазины которые вообще невозможно найти без поиска так и на крупные маркетплейсы типа Wildberries озона или яндекс.маркета и вот эту базу товаров нам надо собирать Сегодня мы будем про неё говорить а соответственно про что доклад про построение базы цен но вы скажете блин афише-то обещали что будет про парсинг всего интернета вообще А тут что-то только про цены собираются рассказывать как-то что-то не то на самом деле все подходы которые я буду сегодня рассказывать они работают на всех остальных срезах и в конце я расскажу про другие удачные примеры где у нас получилось применить эти алгоритмы Но почему цены потому что у цен есть два очень важных свойства первое с точки зрения товарного поиска ценность вообще Ключевая функциональность они прям везде поэтому алгоритмы должны быть очень хорошего качества иначе невозможно сделать качественный продукт у которого слишком много ошибок второе цены вообще меняются в отличие от многих других полей то есть самое время частенько цены меняются дважды день на товары А в районе каких-нибудь мероприятий вроде Черная пятница которая идет сейчас у нас за день обновляется пол балла и соответственно это свойство которое накладывает довольно много ограничений на решение и мой доклад будет построен следующим образом я вкратце расскажу как выглядит наш архитектура какие там основные блоки и подробно расскажу про две самые интересные компоненты про парсинг и скачивания Ну а также в конце расскажу про удачные применение парсинга но Давайте поймем масштабы вообще какие у нас масштабы задачи всего по нашим оценкам в интернете порядка 20 миллиардов товаров которые цены которые мы хотим получать примерно полутора миллионов интернет-магазинов также очень важная характеристика по нашим оценкам примерно каждые 24 часа 11 процентов товаров меня свою цену то есть вот если вы взяли базу посчитали на нее цены сутки с ней ничего не делали через сутки у вас будет в ней 11 процентов товаров с неправильной ценой с протухшей и еще раз Напоминаю что Ошибка цены это очень плохо а поэтому выбор у нас в архитектуре Нет надо максимально все насквозь потоковой архитектуре пробивать от начала до конца делать у нас нечего и первая компонента в нашей архитектуре это Крауля он отвечает за две самые важные задачи в начале это про планирование Какие документы качать собственно проскачивание их и на каком-то срезе где это необходимо он занимается отрисовкой Java скрипта А ну в случае если там есть какие-то ленивые загрузки и подобные всякие проблемы скривые дальше всё это дело идёт потоковое в контент системы яндекс.а на неё можно смотреть как это огромная система в которой есть много маленьких микросервисов которые под точно обрабатывают документы и один из этих микросервисов это наш парсера он соответственно вход получает HTML Q и возвращают какие-то структурированные данные в частности там лежит наша цена все это дальше идет опять же насквозь не задерживаясь в хранилище в котором копятся офферы и выкладывается индекс и вот в этой схеме все хорошо по Точно Вот кроме вот этого полного индекса который выкладывается раз несколько часов что мы с этим сделали я расскажу чуть попозже в конце там тоже есть на это как но всё остальное насквозь пролетает Хорошо давайте переходить собственно к задаче парсинга как она выглядит Вот это типичная страница интернет-магазина там много какой-то информации все очень красиво но нас на самом деле интересует какой-то набор данных с этой страницы в частности нас интересует например там блочок цветов или блочок цен Ну вот он там 28.99 руб нас интересует и вот Глядя на такую страницу можно сказать на что-то просто же алгоритм придумать Давайте возьмём просто первую строку которая рубли содержат отлично метод кажется а но не всё так просто и вот один из миллионов примеров которые там ломает и надо там какие-то эвристики писать Например такой А вот у нас есть цена 32.99 руб Но вот повыше положили баннер на акцию которая ещё не началась и там 19.990 руб на э-э Ну будет через несколько дней Но сейчас вот не столько и алгоритм должен не обмануться и извлечь правильную цену Ну соответственно не всё так просто Поэтому нам надо формулировать задачу и пробовать её решать Ну соответственно задача берём html-ку получаем же сончик со структурированными данными отправляем его дальше потребителям но всё-таки не отпускает вопрос а можно ли как-то просто вот охота нет вот и думать неохота мы обучает никакой Давайте простенько Вот и первое просто компания это микро разметка Это отличный подход почти и структурированная информация есть публичная документация например на схему орк прям в мете в ктмэлле размечена например наличие валюта цена вот эта информация лежит все круто быстро реализовывать даете разработчику задачу Или сами делаете за недельку делается очень круто веб-мастера напрямую влияют можете прямо в документацию написать мы извлекаем эти данные Если хотите попадать товарный поиск указывать такие данные вот вот список но есть нюансы полнота не такая большая К сожалению она хорошая но для того чтобы сказать мы покрываем все интернет э-э в интернете все магазины недостаточно и вторая важная проблема мы никак не можем влиять на качество со своей стороны К сожалению При таком подходе если условно партнёр забыл обновить плагин который генерирует вот эту микроразметку на сайте у него всё хорошо а на поиски все плохо К сожалению плоховато Ну и на самом деле из последнего недостатка выходит следующее решение в каком-то виде Мы за использовали идею похожую на блокировщики рекламы Если вы когда-то блокировали рекламу вы такие о Понятно Вот этот блок у него СССР селектор Давайте блокировать условно всегда элемент с таким классом и у меня на сайте не будут на этом рекламы и то же самое идея Мы за использовали для парсинга цен на конкретном сайте Мы можем выбрать селектор сказать бери Вот с таким селектором первый элемент это будет цена на этом сайте и на самом деле это Рабочая идея для которой мы сделали специальный плагин в котором можно выбирать что я хочу на этом сайте создать партию для цены найти его на странице кликнуть туда плагин предложит селекторы которые он видит и можно выбрать самый удачный из этих селекторов и собственно положить такое правило парсинга для этого сайта и в какой-то момент мы даже Слишком поверили в эту идею когда занимались задачей обогащения сниппетов на поиски Ну вот это какой-нибудь там один из миллиона сниппетов который у нас есть на поиске он выглядит так там какая-то информация иногда охота классико дополнительную информацию например процент мы посмотрели на график распределения показов и увидели очень интересную закономерность это кстати не сильно зависит от среза так почти всегда распределен трафик что Примерно 100 самых крупных сайтов дают Примерно там 60 даже чуть больше процентов показывать а это значит что берем накликиваем 100 парсеров и условно там почти решаем задачу собственно Так мы попробовали сделать мы взяли нашу группу разработки заказали пиццы заказали напитки забронировали переговорку договорились сейчас мы будем два часа кликать не отвлекаясь и накликаемся там что-то не получалось накликали 80 пока доделывали верстку запускали эксперимент 10 успела сломаться пока там до разбирались там с приемкой и докаткой в продакшн И еще есть сломалась и Эйфория немного прошла как-то оказалось что подход прикольный но есть недостатки и резюмирую на самом деле про этот подход мы можем влиять на парсинг со своей стороны быстро можем распарсить какой-то конкретный сайт запустить эксперимент проверить Как это работает но к сожалению к задаче парсинга например для товарного поиска это вообще не работает там для того чтобы создать полтора миллиона сайтов даже если вы будете невероятно быстро накликивать по 10 минут на сайт надо потратить 130 лет и они еще разламываются разработчиков это дёргает но отсюда на самом деле вытекает вполне логичное предложение А зачем это делать самому Зачем это делать разработчиками Это же вполне Понятно рутинное задание и ровно так мы модернизировали этот метод мы сказали а давайте создавать эти стартеры это локерами мы берём сайт отдаём этот сайт того кем локер в этом же плагине накликивает сектор дальше мы отдаем этот сэмпл другим талокерам они проверяют что Первый сделал все хорошо если все хорошо добавляем этот парсер стоит парсера Ну и всё как бы задача выполнена но опять же возникает вопрос А почему это не идеальное решение Все можно презентацию тогда заканчивать к огромному сожалению Все эти парсеры они независимые то есть соответственно создавая Один парсер он никак не влияет на другие их также надо создавать и также надо поддерживать а это значит что на создание полутора миллионов парсеров Ну надо миллион долларов что потратить при расчёте там примерно 0,75 долларов за штуку Ну это была бы не самая главная проблема например там размазать на год бюджетом СКК срезать и так далее если бы эти парсеры не ломались И к сожалению ломаются не очень часто по нашим оценкам примерно 30 процентов за месяц это сломаются и для того чтобы в адекватном качестве поддерживать парсер раз в неделю надо переходить эти парсеры раз в неделю это на самом деле оценка сверху если они ломаются пробовать их пересоздавать и соответственно на эту Будет уходить порядка 750 тысяч долларов а вот это уже хороший бюджет и вторая главная проблема даже если там говорить и выбить такое бюджет на любые другие задачи этот бюджет не переиспользоваться то есть нельзя вложиться там словно в разработку и потом везде это применять А тут надо везде насыпать этот бюджет поэтому к сожалению к нашей задаче это не подошло Но какие-то более локальные задачи где условно 100 сайтов или 1000 сайтов они подходят отлично это супер метод Ну и резюмирую сказано о простых методов словно извлекать первую строку с рублями мы не придумали микроразметка отличный подход круто запускать достаточно селекторы можно разгонять покрытие особенно в небольших задачах но к сожалению дорого поддерживать и мы пошли дальше мы сформулировали задачу в терминах классификация вершин дом дерева Если посмотрим на какой-то пример цены на сайте видно что у нас вот там есть какое-то много всякой информации и есть отдельная вершина которая содержится например 499 рублей и мы хотим обучить такую классификатор который говорит что вот здесь содержатся цена А во всех остальных вершинах как бы не были похожи цены нет то есть например на 599 это прайс равно false это Не цена Ну и Разумеется там на скидка по акции либо минус тысяч рублей тоже надо сказать Фолз не это не цена и делали мы это обучение модели градиентного бусинга которая есть три основные группы факторов первая группа она самая логичная это текстовый фактор на самом деле даже вот если сейчас не про что не рассказывать и посмотреть только вот на этот СНиП цена вам не надо вообще никакую часть страницы вы скажете Ну это цена на какой-то товар но цена Возможно там из блока рекомендаций Но это цена и как он работает мы говорим что мы предсказываем для вершины 499 рублей мы смотрим на контент вершины которая стоит слева от нас мы смотрим на контент вершине которая стоит нас мы смотрим на класс вершины в которой лежим мы и мы смотрим на контент вершины которые справа от нас континируем все это в одну строку через разделители и обучаем сетку которая говорит цена ли здесь или нет И на самом деле это очень сильный фактор Ну то есть одного его хватает уже очень много К сожалению есть недостатки надо их немного подполировать и вторая группа факторов под полировка ручными факторами А давайте посмотрим на теги вершин нашего родителя дедушки и так далее мы знаем какие-нибудь значимые элементы у родителей дедушки и так далее например ссылки картинки посмотрим вообще насколько мы Уникальны в дереве Сколько еще таких же вершин Ну и посмотрим там какие вообще в среднем размеры Это продолжение идеи с текстовым фактором Но немного подполировать другие статистики но опять же этого мало потому что может быть несколько похожих блок цен и непонятно Какую выбрать здесь на помощь приходит на самом деле позиция в дереве А давайте вот посмотрим на все дерево целиком и поймем Где мы находимся Где мы находимся можно считать в двух терминах во-первых посчитаем от начала дерева до конца дерева различные расстояния например в терминах сколько слов от начала документа сколько символов сколько вершин сколько картинок это будет первый набор факторов расстояний второй набор факторов расстояний А сколько такие же вершин по Пациенту по классу перед нами и после нас например для вот этой выделенной вершины с ценой будет известно что по контенту Она находится где-то в первой трети и это первая вершина с ценой больше таких не было а вторая вершина это уже известно что она где-то там в конце где-то 80 процентов текста перед ней и есть какая-то вершина С таким же контентом ценой перед ней и группирую все эти три группы факторов вместе мы получаем отличную Модель которая умеет предсказывать цену по html-ке огромному сожалению мы все еще на вход получаем весь поток Яндекс робота и это сотни тысяч РПС как бы мы не старались как бы там лучшая олимпиадники которые у нас есть не пытались это оптимизировать При таком потоке меньше чем десятки тысяч FPS в лоб с оптимизировать у нас не получилось Поэтому мы начали думать а что же можно сделать чтобы ну хотя бы сузить немного этот поток Как можно под оптимизировать и первая логичное действие А зачем нам парсить весь поток Давайте парсить какую-то его часть потому что Яндекс же скачивает еще кучу других страниц там есть новостники Википедия обзоры Ну страницы с фильмами и там нет никаких цен они не интересно для нашего товарного поиска и Давайте обучим просто классификатор который по Орлу и так будет говорить А вообще здесь есть товар или нет причем важно что нам не так страшно сильно ошибиться назвав кого-то товаром Ну партия разберётся он скажет нет Там нет ничего страшного и такой подход позволил нам сотни тысяч рфпс превратить десятки тысяч PS нагрузки на партию при этом сама моделька она очень лёгкая она берёт только урла и тайтл он даже не лезет там никуда в дерево и это лёгкое которая относительно там парсинга почти не потребляет ресурсы но надо идти дальше все еще многовато посмотрим вообще как выглядит снова страница Ну вот на какой-то произвольной странице можно глянуть Ну цена здесь а если глянуть на какую-то другую страницу этого сайта то опять мы скажем Ну цена здесь почему мы на каждой странице заново запускаем какую-то сложную машинерию Хотя все страницы одного сайта похожи Кроме того Мы недавно разговаривали что можно генерировать селекторы селекторы работают целиком на весь сайт А тут мы снова парсим Одну страницу и комбинируя на самом деле знание о том что у нас есть клёвая модель и Известно что в целом парсинг на Хосте выглядит одинаково то возникает логичная отдавайте селекторы генерировать модели действительно же она есть и ровно так и решили мы сделать мы взяли конкретный сайт для него на генерили Канди поверхность селектора Это от вершины поднялись вверх До корня и выписали так и его класса ID моделью проверили что такой селектор Это хороший селектор там почти всегда лежит цена Если это так то все положили его в стейт для парсинга будем его использовать что важно Это маленький Файлик просто записями хостес селектор в нем единицы миллионов записей копейки и его можно регулярно перестраивать вообще без всякого краунда раз день легко и что очень важно Это подход экономит примерно 60 процентов вызовов тяжелого парсинга то есть нам 60 процентов случаев достаточно просто глянуть в этот Файлик проверить если такой селектор и если есть О круто Все не надо ничего запускать вот ответ и соответственно давайте резюмируем как у нас режется поток на вход у нас приходит сотни тысяч РПС классификатором мы срезаем до десятков РПС потом не забыли про микроразметку все-таки Это отличный метод зачем мы выкидывать и потом еще посмотрели на если у нас предпочитано такое класс эквивалентности и срезали еще 60 процентов Ну и вот эти единицы тысячи rps которые остались Ну можно уже и распался дальше Давайте перейдем снова к следующей задаче к актуальности цен напомню почему она вообще важна Как выглядит страница Как выглядит страница товарного поиска у нас цены используются везде То есть это и выбор сортировки это и фильтр по цене это и товары и цены на товары и соответственно мало распарсить качественно постоянно эти ценное держать актуальными Ну первое логичная идея приходит точно также Не ну все просто Яндекс у нас куча железа обходите базу раз в день и будет как бы все хорошо и ровно это мы собственно когда запускали товарный поезд Точно также и подумали мы написали парсер интегрировали в контент систему Яндекс поиска начали заниматься задачами индексациями качества ранжирования и в один прекрасный день прошлой осенью такие Ну всё Давайте тестировать выкладываем базу смотрим что уже получается На полной базе и неожиданно мы выяснили что каждый второй товар имеет ошибку в цене Ну мы не растерялись мы сказали Окей никаких проблем просто актуальностью не занимались Давайте переобудем Ну просто возьмем и сотни миллионов товаров которые у нас есть закажем в переобход и все актуализируем будем так делать регулярно Но к огромному сожалению так не работает потому что переобход всей базы занимает единицы недели там почти месяц и пока мы переходим документы в хвосте этого списка документы в начале уже успели протолкнуть иногда дважды иногда трижды иногда вообще закончится и снятся с продажи и происходит это из-за того что у нас есть по хостовые лимиты мы не можем с определенного сайта качать слишком много как бы нам не хотелось Потому что если например надо переходить 10 миллионов товаров с одного сайта кажется немного то это больше Star PS нагрузки на и это много больше иногда обычно продуктовой нагрузки нам никто такую нагрузку техническую конечно же не простить поэтому надо думать дальше и надо думать А как вообще тогда скачивать если по кругу не получается нужны какие-то ивристики и эвристики такие Ну первое самое понятное эвристика но если мы документы показываем в поиске в товарном либо на Большом поиске ну их надо переходить на самом деле показов относительно всей базы немного поэтому тут никаких проблем все популярные товары мы будем поддерживать актуальными что делать с хвостом и оказалось что цены на товары меняются на самом деле неравномерно есть товары которые меняют цены гораздо чаще есть гораздо реже и в целом у нас получилось это обобщить глядя просто на тайтл на категорию товара и на время когда он последний раз переходили оффер и вот по такому этапу можно научиться предсказывать вероятность изменения один из последнего переобхода и просто в планировании переобходить самые вероятные изменившиеся документы в момент когда вот этот классификатор у нас работал не очень хорошо нам приходилось делать фильтр по возрасту документа ну и соответственно можно таким интерсколдом подсекать документы которые Ну вероятно уже не успеваем По Кого там перейти Ну и финализируя схему она выглядит следующим образом у нас есть краулер который смотрит на стейт товаров смотрит налоги поиска товарного и большого поиска приоритизировать документы скачивает иногда запускает JavaScript отправляет дальше в контент систему Яндекс робота которая фильтрует поток по классификатору дальше отправляет все это дело в парсер который смотрит на оффлайн процесс построения классов эквивалентности дальше этот поток идет в хранилище и в хранилища и индекс выкладывается под runtime но во всей вот этой схеме как я обещал в начале есть маленький нюансик это выкладка полного индекса она долгая единицы часов нас такая скорость не устраивает поэтому мы подумали и поняли а давайте сделаем отдельный быстрый контур выкладки под runtime который сбоку от основного индекса в котором мы будем выкладывать только цены и наличие это на самом деле очень маленький объем данных относительно всего индекса условно единицы процентов и можно прямо на шортиках поднять сторож в памяти который почти за бесплатно Будет поддерживать максимально актуальные цены наличие просто будем заглядывать вот в этот киви сторож помимо основного индекса и вся такая схема целиком с этим быстрым контуром позволяет документы от начала до конца от скачивания до показывая пользователю пробрасывать за единицу минут что уже звучит почти круто мы решили задачу а и Давайте посмотрим на другие примеры использования как я обещал мы используем точно такие же подходы парсинга для извлечения данных например про онлайн-курсы вот так выглядит обогащенный ответ мы умеем извлекать начало профессию длительной стоимость мы умеем точно такими же алгоритмами парсить достаточно сложные характеристики вроде не знаю материала сезона размеров цветов и так далее И на самом деле много других задач Ну про которые тоже там можно чуть ли не отдельный доклад рассказывать А как можно применить алгоритм парсинга Ну и резюмирую сегодня мы обсудили несколько подходов парсингу структурированных данных мы обсудили какие есть эвристики поддерживания актуальности мы посмотрели на архитектуру база товарного поиска и посмотрели на различные другие сферы применения алгоритмов парсинга Спасибо Давайте переходить к вопросам абонент вот здесь есть начале вопросик кто микрофон выдает а он микрофон бежит вот я далек немного от этого всего Вот Но у меня такой вопрос просто ради интереса а можно вот поиск товаров осуществлять по анализу изображения не по разметке а просто по изображению скажем как как скан документ работает Да допустим анализ скана Вот и перевод в цифровую А вот то же самое с сайтами делать какие-нибудь модели которые могут видеть отличать цифры от букв там цену поля примерно запоминать В каком углу находится поле с ценой И там распознавание делать или это нецелесообразно И вообще бессмысленно в сравнению с разметкой Да спасибо за вопрос на самом деле это второй альтернативный способ парсинга по самому изображению и попытки извлечь с него Ну у него самое главное К сожалению огромный недостаток то что зачастую это требует кучу видеокарт То есть сразу когда переходишь к анализу изображений для хорошего качества надо поднимать отдельный контур высоко нагруженный у нас было несколько экспериментов в самом начале с извлечением но они как-то себя не оправдали в терминах вычислительных ресурсов и в литературе каких-то супер удачных применений мы не нашли и поэтому отказались в пользу более текстовых подходов но мы не нашли прямо где-то работает прям драматически лучше потому что есть подход более простой в если запустить JavaScript и прямо трендерить вот эту страницу вот в подходе который рассказывал я можно добраться до факторов на самом деле про позиции элементов и вот уже в таком подходе можно расширять подход который я рассказал уже гораздо проще органичнее Там прям есть локация что вот это элемент занимает пиксели отсюда досюда и можно их дотаскивать уже смотреть структуру и контент который надо извлекать без картинки он уже здесь текст лежит то есть на самом деле гораздо проще вот визуальный обманывать с помощью трендерить браузере посмотреть на координаты Да Микрофон отобрали чтобы это такой задел на кулуара да такой вопрос а бывает что специально вот скажем сайты магазинов меняют позицию цены меняют вот разметку чтобы их вот так не агрегировали или это неинтересно специально делает вам хуже Мы же поиск мы ведем на них трафик понятно но как изголяются магазины чтобы их правильно парсели это отдельный отдельный круглый стол наверное на 3 часа тогда они должны вам помогать в этом поиске получается и как правило вот эти вот разметки делать так чтобы у вас не было каких-то там сложных моделей для поиска но до всех не доберешься до кого добираемся они нам помогают для тех кто считает рассылки К сожалению приходится так вот хитро до них доходить и извлекать данные смотри аж трижды опубликован Вопрос в чатике какие инструменты вы используете для обхода Мы же прям явно говорим что мы Яндекс робот Яндекс бот 3.0 и вот если нам показывают captcha это очень плохо влияет на позиции в поиске то есть у тебя с удостоверением да да подход такой да тук-тук-тук именем королевы Откройте Да есть ли у вас проблемы с ханни-путами И как вы ее обходите не ну проблема Сохрани потами Конечно есть мы делаем примерно следующим образом команда разработки алгоритма автопарсинга во время дежурства занимается кастомная разметкой и поддерживает их то есть дежурный должен начале недели разметить 10 страничек по его мнению мы складываем их в ханипоты и используем мы сошлись в итоге Спасибо пожалуйста вопрос А сколько времени занимает выполнение всего пайплайнов понимаем на надо обновить цену сколько момента Сколько времени пройдет с момента осознания вот этой необходимости Почему задаю вопрос потому что почему нельзя обновлять цену в тот момент показывается товар пользователь он сделан первый запрос Почему нельзя в этот момент взять актуализировать цену и закошировать например то есть это какое-то временное ограничение смотри мы тоже думали про такой подход Ну во-первых он нашел в каком-то виде отражение в реализации если мы показали этот товар то мы как можно быстрее пытаемся его заново скачать но Весь вот этот плане занимает единицы минут и к сожалению там как-то гарантировать что мы это сделали не знаю там за секунду или там меньше тем более невозможно потому что Интернет огромный Ну многие сайты отвечают за единицы секунды то есть там Пока запустится пока не знаю запустим паркеры такое невозможно к сожалению угу понятно спасибо там тебя снимают на видео надо помахать вопрос Да добрый день Вопрос такой не пробовали в дополнение еще к этим методом добавить некий свой стандартный Яндекс может быть метаданные чтобы создатели сайтов просто напрямую сбрасывали эту информацию такой подход тоже есть конечно же у нас товарный поезд точно также можно этим заниматься не У нас конечно же есть API для загрузки сети всех этих цен но к сожалению опять же полтора миллиона сайтов кому-то дорого вот эту разработку делать кому-то Просто они там не догадываются лень Ну часть сайтов конечно отправляет нам напрямую цены это правда А все остальные нужно парсить друзья пока вы готовите вопросы читаю вопрос из чатика парсер с моделью будет когда-нибудь вулпенсорсе смайлик не на самом деле мы сейчас совместно с ребятами США до ресерчем более академические подходы и я думаю они имеют шанс опубликоваться на гитхабе или в виде статейки когда такие порядки Да есть имейл где можно подписаться мой Telegram можно Напомнить к весне Я с удовольствием отвечу и поделюсь новостями да Никита вьюшков Это ответ для вас друзья есть еще вопросы вот пожалуйста Или я Спасибо за интересную доклад У меня два маленьких вопроса махните отлично вон там видишь где-то на границе миров да Вопрос с границы миров Как вы понимаете что цены которые вы распарсили они действительно актуальны ведь это же их уже много не пройдёшь все не проверишь а второй вопрос всё-таки я не понял чего вы делаете с картинкой Вот ты показывал пример где э объявляется какая-то акция Да в конкретные даты и на картинке цена вот что с этим делать Да спасибо за вопрос правильность парсинга определяется стандартным подходом давай возьмём ровное распределённые sample Ну там пропорционально тому как мы показываем документы условно тысячу или там несколько тысяч документов дадим их талокерам И попросим проверить и мы получим на самом деле чиселку оценки правильности наших данных в среднем по базе Ну там с какого-нибудь минимальной дисперсии там в 1%, что Ну достаточно точно отражает реальное состояние дел про картинку которая с ценой на акции её вполне легко переваривать текстовый фактор он видит что прямо в ноги для которым мы сейчас сделаем предсказание Есть информация что это цена по акции там почти любая текстовая модель в том числе наша она вполне понимает Спасибо Следующий вопрос Добрый день скажите пожалуйста пару слов о том что вы используете в качестве Киева или историч и какова его архитектуры Я еще просто в конце вашего доклада то что вот эти горячие данные которые в итоге кладете в Киеве или storage значение что вы используете архитектуру мы используем на самом деле стандартную индексовую штуку соски Это внутренняя индексовая технология это просто сторож который можно потоково наливать данные и ходить просто сказать запросами я думаю там на каком-нибудь хабре про архитектуру сас скорее всего написали не раз если интересно можно написать на телеграмме найду статейку как она устроена вполне хорошо описывается Как работает и как это работает Быстро Илья быстрый ответ какой-то запускается кластеров пожалуйста Да да у меня интересный вопрос Спасибо большое за доклад портят ли вас хотелось бы узнать и возможно боретесь ли вы с этим или нет А я имею ввиду продукты Яндекса Спасибо Не на самом деле мы знаем что парсят нас смысле партит ли их Сбер ты имеешь ты или как мы знаем кто нас спасет Мы даже общаемся с этими людьми там знаем они ходят парся товарную Вертикаль там много полезных данных товарный поиске ну здесь есть преимущество они иногда нам Багги репортятся взаимовыгодное сотрудничество но на это сложно влиять то есть мы не можем запретить обход роботов в том числе потому что индексация Google хорошо когда ты приходишь ты говоришь что ты Яндекс и тебе там капчи не показывает и сразу все данные отдают А вот когда Эти люди к тебе приходят У нас тоже проиндексируется Привет я Спасибо за доклад у меня такой вопрос Например если какой-то владелец магазина против того чтобы вы их парсили как вы это обходите И вообще если такие случаи что они не хотят чтобы вы их партии может быть есть какие-то не знаю в этом заключаете договора но понятно что магазинов такое количество множеств такого огромное количество что всех наверно не обойти как-то с ними наверное со всеми нельзя договориться тут есть стандартное решение Как у всех сервисов Яндекс для индексации есть тег но индекс совершения которые можно указать блоку с ценой или вообще на весь документ мы туда не заглядываем и не используем нигде в нашей планах эти данные а что это за магазинные вот которые не хотят чтобы их парсили вот Озон например хочешь чтобы его парсели с твоей точки зрения не надо от лица озонтеха отвечать с точки зрения озона не получает много трафика с поиска выгодно Точно также и есть еще вопросы друзья махните рукой Давай тогда финальный вопрос из чатика и мы тебя отпустим можно всегда спикеры найти у него на стенде там Яндекс очередной раз построил целую улицу вот на этой улице собирается весь интернет есть ли какой-то фильтр чтобы отсеивать фейковый интернет-магазины и мошеннические сайты Вот это кстати уже интересная задача Ну это прям отдельный доклад на самом деле про то как мы контролируем мошенников ну вся вот эта Служба контроля качества со всеми ручными и полуавтоматическими проверками Да мы делаем активно боремся с мошенниками активно боремся с пессимизацией магазинов которые там показывают товары не доставляют их идеально правда полчаса рассказывать как мы это делаем я понял ну автор вопроса Константин Пронин Константин просто вот тележку Илья Напишите он вам там полчаса будет писать чтобы фильтровать кому кому подарим за лучший вопрос давайте мне кажется первому вопросу кто спросил про картинки и про дополнительные подход парсинга обойди отлично игрушечка можно с ней обниматься когда когда ложишься спать да"
}