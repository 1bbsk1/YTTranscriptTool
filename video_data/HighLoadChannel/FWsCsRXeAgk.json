{
  "video_id": "FWsCsRXeAgk",
  "channel": "HighLoadChannel",
  "title": "По мотивам шестого подвига Геракла / Дмитрий Кырхларов  (Актион Диджитал)",
  "views": 99,
  "duration": 2378,
  "published": "2024-10-29T03:07:33-07:00",
  "text": "Добрый день доклад называется по мотивам шестого подвига Геракла Если вы помните Эта история про то как античный герой разгребаю царя авгия у нас был сходный опыт Почему думаю Вам доклад будет интересен проб больших компаний они специфичны для больших компаний когда компания маленькая начинает расти она в общем-то проходит тот же путь который прошли и мы важно кажется теперь это вот прямо подтверждено и опытом не потерять контроль над техническим долгом Ну и Лучше наверное делать выводы На чужом опыте чем на своём у нас большей частью классическая архитектура ш раз по дороге Надеюсь будет интересно Итак немного обо мне Я в индустрии больше 25 лет большей частью приводил в порядок инфраструктуры в хостингах ге девах маркетплейсах и так далее многие решения здесь были созданы при моём непосредственном в Компани с Иня 201 руковожу департаментом развития инфраструктуры в нескольких словах про нашу компанию а мы помогаем работать всем профессиям без которых айтишка не работало бы это юристы финансисты генеральные директоры специалист пожарной безопасности и в общем всё на свете что тоже очень нужно бизнесу флагманский наш продукт - это журнал Главбух про который наверное все знают но не главбуха единым значит специфика нашего бизнеса когда мы простаивает непосредственно влияет на бизнес-процесс и всех клиентов сотни тысяч ПС У нас есть но мало Где наш лот в основном про то чтобы не потерять контроль над довольно обширной кухней подробно про нас можно прочитать на сайте о Group статистика за сентябрь Вот примерно такая по состоянию на сейчас это порядка 30 офисов по России СНГ порядка 5000 сотрудников 250 из них работают в it нсх зон У нас вот на момент последнего почёта было 1651 начинали мы с 5000 у нас было несколько заходов на то чтобы понять Какие из них нужны какие не нужны ну и в общем вот весь лоуд про то чтобы не потерять контроль вот над этим над всем сертификатов 3.000 продлять надо каждые 3 месяца в общем как вы понимаете Здесь тоже нужна какая-то машинерия руками это всё невозможно сопровождать А ещё в двух словах там про наши объёмы порядка 150 кластеров постгрес порядка п кластеров МС сэ 3.000 виртуалок Ну и так далее значит холдинг был создан в 2001 году на базе компаний которые были созданы ещё раньше и дальше в холдинг вливались другие стартапы другие компании и в общем никто никогда не занимался вопросом унификации IT систем в 2018 год компания пришла Ну уже Прям вообще в плохом состоянии было понятно что оффлайн формат отмирает что журналы кормить уже долго не будут А айтишка ну в общем все проблемы какие вот есть корневые у бизнеса с it которые могут быть там Скорость разработки Надёжность продуктов стоимость сопровождения всё было в невозможном состоянии Ну и в 2018 году начинается новый этап в развитии компании в развитии холдинга появляется первый в истории холдинга технический директор который получил на входе пять независимых айтик которые занимались сразу всем от настройки принтеров до поддержки продакшена а начал приводить это в порядок новые порядки не всем подошли староверы потянулись на выход вынося экспертизу Ну и начали работу над тех долгом в девятнадцатом году я пришёл в компанию с этого момента принимал непосредственное участие в работах начались работы по приведению в порядок эксплуатации инфраструктуры всех вот этих вещей в двадцатом году у нас было 20 тире 30 отказов каждый день то есть это вот когда продукты складываются и не работает то есть это не какие-то такие мелкие сбои всё было серьёзно в дцать первом году мы перестали находить продакшн на рабочих станциях в двадцать втором году До февраля месяца у нас всё ещё оставались места где в случае необходимости разработчик заходил прямо в продакшн и вносил улучшение без репозиториев тестирований вот прямо так в году у нас до сих пор есть сервис без исходников ВС е не доходит руки переписать Вот посмотрите на этот тайм усилия 200 человек на протяжении 3 лет с девятнадцатого по вать второй год когда у нас ну более-менее что-то стало нормально 2 года до момента пока сутки без единой аварии прошло ну то есть вот так выглядит тех долг значит часть которая про пугалки закончена переходим к конструктивной части как мы подходили к вопросу Ну с целью что делать проблем не было получив такое наследство нужно было обеспечить Надёжность и непрерывность бизнеса мы определили Ну цель должна быть как-то измери мы Определи что в случае поте любого из Дант восстановить фун Почему именно такая цель была сформулирована именно такой дот был сформулирован в 2019 году когда в общем-то дроны с неба не падали Ну во-первых это простая понятная прокси цель которую легко транслировать людям и Она легко укладывается в голове а во-вторых для того чтобы её достичь потребуется научиться владеть инфраструктурой научиться владеть продуктами понимать где мусор где полезные данные Кто кто За что отвечает и так далее потребуется автоматизировать рутинные операции есть потребуется привести в бизнес работоспособные состояние с достаточно высокой планкой в плане манёвренности значит наша сетка по состоянию на сегодня выглядит как кольцом центрах один стек М1 второй selectel M sk2 есть офис все локации объединены тёмной оптикой Office используем как э мало используем там у нас есть кворум какие-то вещи но в целом продукты поделены примерно по 50% в каждый Ну если потеряем один из дата-центров восстанавливаться будет проще это вс-таки 50% а не 100% ещ мы не держим двойной запас железа из соображений стоимости наш план прирастать коммерческими облаками Если дата центр всё-таки там как-то повредится так катастрофи вот это важный слайд Он очень простой но почему-то все время это упу из вида Надёжность инфраструктуры на которой развёрнуты ваш продукт равна произведению надёжности каждого из его узлов То есть если вы приложение размещаете в одном дата-центре А базу данных в другом дата-центре то ваше приложение поломается когда пострадает любой из дата-центров или линии связи между ними вот когда на бананах там приложение база вроде всё понятно Когда схема начинает усложняться там дополнительные элементы взаимосвязи между продуктами и так далее вот этот момент в общем часто упускают из виду и это опасно В общем если у вас где-то будут какие-то задачи растащить на два дата-центра стоит оценить этот риск либо у вас приложение должно умело разваливаться пополам и потом соединяться либо жить в одном дата-центре либо может быть вы этот риск просто примете но оценить его нужно е важно что когда Линк у вас между локациями просто упал это простой понятный хороший случай намного хуже Когда начинают расти потери начинает расти ленси начинаются какие-то такие спецэффекты то ли работает то ли не работает в общем вот это всё плохо значит э принципы на которых мы ээ работаем мы всегда готовы к потере одной ноды Ну понятно почему железо горит это не должно приводить к тому что у нас авария мы оперируем понятием домен отказа значит домен отказа - это что угодно что это то что может сломаться вместе Вот дата-центр - это домен отказа вместе с ним может сломаться Вот это всё и соответственно когда мы разговариваем там про какой-то домен отказа Мы всегда должны оценить последствия А что отвалится там случится такое событие естественно мы стараемся минимизировать размер домена отказа Ну Сами понимаете если у вас стоит один балансировщик за которым 100 продуктов И вам нужно поменять конфигурацию и вы её поменяли неудачно у вас ощущение обширные и яркие А если это там просто где-то что-то в одном продукти чуть-чуть то в общем это всё намного легче переживать расширяемая архитектура в случае необходимости Добавить новый дата-центр но по-моему мы ещ просто не доросли до того чтобы прирастать третьем ДЦ Ну и да Нельзя терять контроль над стоимостью владения системы мы рассмотрели коммерческие облака они у нас всегда были под запретом мы довольно много продам в госструктуры Поэтому просто всегда было под запретом жить-то там на L2 между локациями Нам тоже не подол Ну потому что это довольно большой домен отказа мы потеряем сразу ВС если будет петля разнообразные вендерс решения когда там огромный стод реплицируемый частин выт вот так в каждом Дант на базе фа которые реплицируемый Дант Ну мы не можем потеряв дата-центр за 4 часа взять и развернуть вот всё что попа всё что пропало в чистом поле нам нужно какую-то базу подготовить Ну вот эта база это в каждом дата-центре своё облако свой ДНС своя инфраструктура инсталляции деплоя артефакты все рабочие при вании машин мы не рассматриваем средство воспроизведения машин мы КПИ только данные машины у нас всегда только наливаются там нужно было решить вопросы с воспроизводимость мы их решили должны быть репозитории and в каждом дата-центре и то что должно оперативно меняться вся эта конфигурация лежит в ЛТЕ для балансировки и фловера мы используем бгп как внутри локации так и между дата-центра в каждом дата-центре своя инфраструктура мониторинга своя инфраструктура сбора анализа логов и мониторинг и логи собраны сшиты безшовна Ну потому что если что-то случилось разработчику надо посмотреть на графике надо посмотреть налоги он не должен задумываться в какой дата-центр задеплоил его продукт он должен зайти по известному ему адресу и в общем найти то что он там ожидает во Мы тоже решили продукты должны быть изолированы это и в каждом дата-центре должна быть реплика баз потому что восстанавливаться из бэкапов может быть довольно дорого на наших объёмах в 4 часа Не влезть должна быть контролируемая связанность между продуктами без привлечения сетевых инженеров значит если у вас есть база данных в которую ходит сразу несколько продуктов эта штука сопровождается ло вы не можете сказать кто отвечает за данный продукт Потому что всегда там можно на кого-то перевести стрелки Ну невозможно задокументировать повесить алерты невозможно продукт между дата-центра переместить вообще само понятие продукт вот этот вот периметр для которого мы считаем Вот ну это пря продук как-то выделяем насчитали 107 но по-моему будет больше ну и да необходимо регулярные смены базового дата-центра для продукта поскольку Ну если вы этого не делаете оно всё равно начнёт обрастать ракушками рано или поздно какими-то ранта изменениями и так далее как я уже говорил в наших целевых картинах фигурирует возможность расширяться в коммерческие облака этоже делаем для некоторых продуктов Как выглядит сама небула в каждом дата-центре там есть три продакшн кластера кластер - это набор физических машин которые ме ну которые объединены в логическую сущность внутри небулы машины между кострами не пересекаются Кроме того есть инфраструктурный Декстер там ещ другие кластера другого назначения сейчас у нас 15 кластеров от семи до семнадцати машин Ну вот так структура кластера это Некоторое количество физических машин на которых развёрнуты цеф который предоставляет блочные устройства на этих же машинах крутятся гипервизоры под управлением небулы Ну и виртуалка с этого Цефа получает блочное устройство соответственно может переезжать в пределах кластера прозрачно э гиперконтроль об что потеряв любой из кластеров мы не теряем сервис то есть мы об этом узнаём из мониторинга а нет плачущих клиентов с облаками разобрались переходим к сети у нас один продукт в другой напрямую попасть не может там всё закрыто на фаер из внешнего мира в продукт Мы тоже проходим через через промежуточные балансировки нужно попасть в другой Ну например справочные системы обращаются к поиску это происходит через ent через внутренний дмз эти фалы реализованы на сетевом оборудовании в виде Алей были сложности при настройке аце между двумя локациями довольно мало памя коммутаторах ВМ трюки применить Ну сейчас дойдём про сетку поговорили дальше про базы данных немножко ну типичная конфигурация постгрес в одном дата-центре постгрес в другом дата-центре под управлением патрони арбитры в офисе всё это прикрыто H Proxy две машины которые через бгп друг друга резервирует Ну и вот это одна из наших схем Мы её не очень любим Но в общем где не можем отказаться от неё используем Вот это наша основная схема кластер располагается в одном дата-центре и мы используем механизм wg который перекладывает в S3 файлы во второй локации У нас есть ди рудиментарный кластер в режиме R Only на который мы накатывает задержку до 2-3 минут в наших условиях это в общем почти везде приемлемо приятные сайд эффекты В3 у нас лежат бэкапы из которых мы в частности наливаем деф окружение ц там после удаления всего ненужного на дец вот ну и такая схема бэкапа она не требует никакого внимание если мы продукт таскаем из дата-центра в дата-центр оно там всё само про mssql Немножко значит Как устроен кластер mssql есть две машины есть м Дик здесь он на схемах не фигурирует он у нас находится в офисе и эти две машины работают друг с другом по очень большому количеству портов включая Windows R PC на этих машинах есть индивидуальные ашки и есть ещ адрес мастер сервера который ездит с Хоста на хост А в LG у нас ну в общем у нас не получилось завести сейчас говорят работает но мы уже Мы на ранних версиях там пробовали Мы в общем другую схему построили мы убрали кластерный интерфейс в vxlan и растянули между локациями vxlan таким образом с точки зрения кластера mssql Он живёт в L2 сетке адрес ездит с машины на машину никаких проблем Ну да машины Я кажется не сказал машины находятся в разных дата-центров для Нам тоже надо решать было задачу катастрофа устойчивость Ну и каждая машина торчит в своём дата-центре дополнительным интерфейсом в сетку соответственно данного дата-центра и туда можно повесить на можно повесить прокси до тех пор покае СРО это это тоже нехорошо У нас есть нагруженные продукты где мастер просто не вывозит и нам нужно чтение отправлять на слова Как работает mssql когда вы приходите и говорите что мне надо поработать со словом вы приходите через Connection STR на тот же самый линер говорите что мне нужен адрес Сва линер сообщает вам индивидуальный адрес машины которая в настоящий момент является свом и это адрес изнутри VX Лана с которым вы работать не можете Поэтому с прокс у нас не получилось Мы на винде подняли бгп И сейчас всё хорошо мы приходим прямо на адрес внутри Лона получаем адрес слова и пошли туда с бгп на винде есть особенности почему-то считается что автономной системы больше чем 65.000 бем свете не существует для приваток у нас это не так но в общем сетевики с этим тоже справились с базами закончили теперь про мониторинг мы летаем на Victoria Matrix Ну типичная схема куча экспортеров куча скрей которые с этой кучи экспортеров собирают метрики передают в ирте ирте складывают в реджи с противоположной Роны через селекто вытаскивает метрики рисует графики переда на два дата-центра картинка выглядит вот так а менеджеры собраны в кластер и сами разбираются Кто будет отправлять Ар во внешний мир там кто не будет сами селекторы знают не только про свои реджи но и про реджи в соседней локации под фану подложенным никаких проблем переехали во второй продолжаем работать потеряли половину метрик в этой в этих вих метрики у нас хранятся в течение 3 месяцев дойдём сейчас до других Вик есть у ВМ алерта замечательное свойство он на выходе умеет генерировать не только Арт он умеет генерировать е новую метрику и это на пря Выручила мы сейчас на этом механизме считаем SL для наших всех продуктов по большому количеству метрик Мы на этом механизме считаем например утилизацию ядер процессоров в разрезе дата-центров там по кванти тоже в общем очень удобно когда приходят говорят что там всё кончилось закупаем железо очень надо открываешь график Там написано что в п кванти утилизация 30% Ну уже есть и из этих же селекторов есть вот эта самая Вика где данные уже лежат 5 лет Ну они естественно не с десятисекундный нарезкой они там сильно реже это нам нужно там для отслеживания трендов для отслеживания там какой-то аналитики для построения аналитики наши ближайшие цели над В общем интересная задача потому что Нас не устраивают ползущие нам нужно из одного фиксированного состояния в другое фиксированное состояние и вот этот процесс тоже должен быть контролируемым сейчас это на этапе раскатки уже смена операционной системы Мы входим в реестр отечественного по Ну и нужно соответствовать сейчас мы летаем на пожилой нси надо будет переть наст ко умеем это делать для некоторых продуктов тоже вот нужно как-то это всё генерализованные продук миграции продуктов между локациями опять же несколько продуктов сейчас уже умеют так делать основную массу Ещё предстоит обучать Ну и учёт потреблённый ресурсов по продуктам это тоже такой внутренний биллинг для того чтобы дать тем лидам картинку чтобы они могли понимать - что можно сделать с продуктом для того чтобы понизить стоимость владения Ну тут пока на уровне идей и то не очень всё гладко В общем задачки Такие масштабные интересные если вам кому-то интересно поучаствовать в таких проектах напишите мне там в Telegram поговорим собственно Надеюсь было интересно надеюсь на чужом опыте учиться намного приятнее чем налично ваши вопросы Дмитрий Спасибо огромное Так ну что друзья вижу руки Вот как минимум две с этой стороны ахпер сейчас принесёт микрофон напоминаю обязательно переходите по QR оценивайте пишите комментарии поверьте программный комитет а читает всё очень внимательно разбирает а спикеры потом с нетерпением ждут Ну что же что же что же там напишут Итак прошу Дмитрий Спасибо за доклад очень интересный опыт это прямо реально подвиг Геракла вопрос у меня следующий по ходу повествования и в конце вы сказали что в облаках хотите расширяться дальше Вот и в начале то что в Amon не получи ти отечественный надо чтобы отечествен А сейчас же есть коммерческие облака Ну типа Яндекс Да вот мы про них сейчас и думаем Но во-первых это тогда их просто не было Правильно я понимаю Тогда их ещё не было либо это было прямо вот совсем очень сильно дорого но оно и сейчас очень сильно дорого по сравнению там с собственным железом Ну если уж у вас там потерялось половина всего продакшена тут Наверное уже на деньги смотришь по-другому ладно Спасибо Спасибо огромное Следующий вопрос пожалуйста Дмитрий здравствуйте Спасибо большое за доклад очень на самом деле интересная тема ээ Скажите пожалуйста ээ Вот вы получается пришли в 2019 году и начался четвер годовой процесс как я понимаю по стабилизации системы ээ стабилизация там подразумевала изменение структуры ээ немного пере переделывания архитектуры и тому подобное Во время этого процесса ээ что было на по поводу разработки новых продуктов новых проектов они как-то Притормози или вообще остановились типа стабилизация всё Ребят давайте стабилизации сделать тут надо переделать тут надо сделать подго Ну так не бывает Бизнес работает ему нужны фичи То есть это всё операция на прыгающей пациенте Ну вот там было два важных момента первый во-первых ответственность за надежность продуктов была передана мм ехи это инфраструктура и эксплуатация вот с разработкой технический директор разбирался отдельно Спасибо Спасибо огромное вот сзади вас тоже была рука сначала сзади потом вы пожалуйста я пытаюсь следить за очередностью получается не всегда Спасибо за доклад вопрос на старте было 250 человек Сколько было на выходе и в связи с тенденцией печатных изданий Ну с точки зрения формирования продуктов нет ли перспектив и других направлений на развитие бизнеса не пришло ли от бизнеса новых запросов и новых тенденций сейчас по порядку значит в течение всего этого времени количество народу менялось там по минимуму было 170 сейчас 250 Ну вот где-то в этом промежутке Там сначала меньше Потом больше вот так вот было по поводу новых направлений бизнеса Ну мне Ну вот да можно передать микрофон техническому директору нашему Так у нас помощь зала отлично Я предлагаю нашу экспертизу инфраструктур ную Привет Меня Женя зовут смотрите во-первых я Диму немножко поправлю на старте было народу больше там не 250 человек было когда всё это только начиналось вот как Дима сказал было п it онь Боша сть функци которая не it в принципе была сосредоточена внутри всех этих Атик там я не знаю огромный производственный отдел у меня сидел это девочки которые там чего-то делают Там 40 человек их было поэтому когда ВС это мероприятие началось вот этот вот скачок по людям был с 330 где-то до он был в основном связан даже не в том что там староверы не староверы люди 15 лет на одном месте работали по-другому надо Что делать работать не хотят просто был замена состава это пункт номер раз по поводу продуктов мы в первый год запустили сразу зонтичный продукт который все объединяют он сейчас называется АКМ 360 это все наши продукты упакованные под единым лендингов и всем остальным Мы никогда не останавливали У наверно из таких ключевых моментов с точки зрения развития было очень сложно договориться на топ уровне что у нас появляется прямо полоса квота на технический долг в принципе Потому что когда бизнес исторический он у одного акционера ему 25 лет никто никогда ничем не занимался вообще в принципе начать инвестировать в технологии какие-то это кажется Космос очень дорого не подъём ВС остальное Дмитрий к слову Когда на работу строился до этого доклад про я с интересом послушал потому что он меня удивил как в общем-то и всю компанию Когда у нас там бюджет на Дато центра вырос в 10 раз за оди год и вдруг мы через полгода про это вспомнили все и узнали вот поэтому это такие вещи ими нужно очень аккуратно тоже заниматься вот ответил коллеги на самом деле обсуждение я Предлагаю перенести в кулуары есть о ЧМ поговорить Давайте перейдём к следующему вопросу Добрый день Не могу всё-таки не поддержать Вот первый вопрос смотрите Вы развернули два дата-центра в каждом дата-центре развернули своё облако внутри облака подняли виртуалки внутри виртуалок подняли постс и свели их связали репликации к ним ходят приложения и при этом у вас были там вот проценти ножа по надёжности 0,99 ум Умно если я вот сейчас начну перемножать проценти как бы у нас не получилось Там 50% надёжности при такой схеме и собственно вопрос хочу поддержать вопрос первого докладчика может быть на текущий момент проще взять менедж какое-то решение в Облаке например тот же самый постгрес переплатить чуть денег и вам не нужно будет 250 специалистов чтобы там ничуть Мы считали а 250 специалистов это тоже инфраструктуры и эксплуатации вот сре плюс админы - это 20 человек а разработка то всё равно никуда не денешь Не поспоришь Так ну что есть ли у нас ещё какие-то вопросы Я рук не вижу Давайте на всякий случай я ещё посмотрю в чате ребят Не забывайте Если вы смотрите нас онлайн Вы тоже можете задать свои последние вопросы на сегодняшний день вопрос слева так рука была Ну вот отлично Вот Но нет онлайн Пока ничего нет спасибо огромное Здравствуйте спасибо очень интересно А мне интересно вот вы создали два своих облака внутри которых вы сервисы какие-то предоставляете виртуалки там базу как сервис Возможно кафку какую-то да как сервис распределённый и так далее этих сервисов Их достаточно много запрос у разработчиков на новые такие сервисы он опять же есть да и достаточно сложно с крупными коммерческими облаками конкурировать где эти Новые сервисы добавляют много людей Да если у вас их добавляет 20 человек как справляетесь ну разработчики приходят с вопросом разверните мне ещё один постгрес Ну справляюсь минуту заче Нет я имею в виду новый вид сервиса там не знаю Уно какую-нибудь кафку как сервис если у вас её до этого не было понятно Ну ну справляемся вот пришли телефонист там со своими специфическими нуждами справились засунули в кубер маршрути зру ему сеть пришли машин лёрнинг Понадобился им там какой-то какая-то специфическая база Ну тоже справились Я пока не знаю А в чём проблема-то Ну в том что экспертизу на как бы по всем этим вещам которые вы предоставляете в Облаке как сервис на же экспертизу поддержки наращивать и так далее То есть это нарабатывать опытом Да ну это не так часто всё происходит Я вас понял спасибо спасибо огромное пожалуйста вот вижу руку Добрый вечер Уже наверное как вы боретесь с продуктами без исходников Ну например прилетело обновление систему обновились липси обновились политики США и вдруг Неизвестная со библиотека которую кто-то сделал в том тысячелетии отвалилось Да это всё ну в общем среди прочих изменений у нас был большой переезд там с винды на Linux на контейнеры в общем то что касается приложений без исходников это всё сейчас на винде это вот это вот наше Наследие Ну как боремся по каким-то функциональным требованиям Ну да повторюсь моя функция - это эксплуатация вот этой всей кухни разработка знает что им надо переписать у них есть свои планы Ну и они как-то с этим движутся на самом деле торчащей вовне чего-то такого сильно дырявого У нас сейчас уже ничего не осталось Нет как разведённая проблема нус поменялся там ул там на 1 что-то отвалилось там с перестал поддерживать РСА что-то отвалилось Понятно в приватном контуре Ну поскольку это всё вот в приватной сети всё крутится Ну как-то там можно немножко спокойнее смотреть на вопрос безопасности тем более с учётом того что продукты у нас друг от друга изолированы то есть даже если мы потеряем продукт он будет скомпрометирован другие это не заденет но я признаться Ну у нас безопасники в общем за этим следят это не только безопасность update lpc очень популярная мина для постгрес не наступали Ну то есть это сейчас процесс То есть все машины у нас выровнены все постгрес строго одной версии строго определённый набор плагинов в них обмазанный необходимыми метриками есть соглашени как мы эти метрики лаем то есть стандартизировать в этой части и переход с постгрес там вот сейчас 145 на 15 это будет ну такой как сказать подготовленный шаг осмысленный Спасибо огромное Так ну что остались ли у нас ещё какие-то вопросы вижу один вопрос Давайте Можно пожалуйста вот ещё Добрый вечер Спасибо за доклад вопрос тай у вас достаточно много продуктов много пользователей как Я подозреваю и продукты Судя по тому что вы рассказываете возможно Ну используются по всей России Вот в связи с чем вопрос У вас дата-центры герас и думаете ли вы в эту сторону оба находятся в Москве нет Мы приняли решение что сразу в два дата-центра Дрон Не прилетит одновременно Нет я немножко про другое просто у нас тоже бизнес геон по всей России быт Ну в частности вот есть конкретны кеча там тупо медленный интернет а у нас достаточно большие объёмы тяжёлая статика в сиденьи А понятно спасибо супер Дмитрий Спасибо огромное за очень интересный доклад друзья Спасибо всем за вопросы Давайте ещё раз поблагодарим спикера пожалуйста но у меня последний вопрос Какой вопрос тебе понравился всё-таки больше про базу данных супер спасибо огромное тогда а дарим подарок от Газпромнефть за отличный вопрос Всем спасибо большое за интерес"
}