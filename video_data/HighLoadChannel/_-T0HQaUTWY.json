{
  "video_id": "_-T0HQaUTWY",
  "channel": "HighLoadChannel",
  "title": "Lambda architecture для realtime-аналитики — риски и преимущества / Николай Голов (Avito)",
  "views": 6102,
  "duration": 3107,
  "published": "2018-08-16T03:52:40-07:00",
  "text": "меня зовут голов николай кто на предыдущих айла отдых было возможно помните я тогда все про хранилище и проверьте q рассказывал но я подозреваю что я всех у себя в конторе в авито так задолбал хранилищем что мне теперь сказали что я буду всеми базами заниматься и теперь я буду вам не про хранилище рассказывать проект другие люди же рассказали а про то как несколько баз живут вместе и как их можно подружить что делать классные штуки и поэтому я хочу вам рассказать про такую штуку как лямда архитектура так давайте сейчас добавим немножко вовлеченности кто знает что такое лямда архитектура и это хорошо потому что смотрите одна из причин что я делаю доклад потому что что вот общаешься с человеком говоришь вот сделайте лямда а он так кивает ну вроде как неловко признаться что не знает о чем речь но он сам не сам не понимает а теперь я им вот это видео дам посмотрите они тергуд все знать потому что про лямбду надо знать и тоже довольно старая технология вопросам увлеченность 2 кто знает что лямда архитектура многими считается устаревшей и что есть уже новая архитектура которая всем нравится намного больше вот тут чек правильно сказал видите вот уже видите как ручки попадали сейчас после лямда возникла капп архитектуры и тут сейчас еще другие разные буквы придумывают но от нюанс то есть в чем фокус лямда она прошла вот период хайпа упала на дно а теперь она доросла такой стадии когда им и можно просто пользоваться когда она просто хорошо решает задачи и так общая информация большие данные классические определяются через trevi волюм velocity выройте объем скорость и сложность в каждом из аспектов есть свои нюансы и так далее сейчас я хочу сфокусироваться на скорости на velocity потому что фактически сейчас 2017 год на дворе кого мучают проблемы с объемом сейчас есть решив твердь и команда hadoop то есть куча мест куда можно на валить гигантские объемы и вообще не волноваться об этом то есть волюм проблема с волю можно сказать решено на velocity проблем поострее вот я вам сейчас расскажу как ее решать выройте самое сложное это вот возможно следующей конференции когда мы сами с ней с ними разберемся там основная боль и так проблем у velocity чтобы понять как решать проблему velocity ну то есть есть классический подход который позволяет решить проблему velocity причем я когда я говорю правил о стену скорость я говорю про скорость не то что там вот данные там отстают на час или на минуту я говорю про real-time то есть отстают на секунды отстает на миллисекунды и так сказать в классическом большом мире на такую скорость есть 1 ответ streaming собственно сейчас важно отличать что такое streaming и чем он отличается от бочонка тут классическая шутка ну знаете как вот типа все сейчас узнают что ладно все мы говорим прозой то есть бы чинг это самый обычный анализ данных в базе данных то есть у нас есть база нам есть фиксированный объем данных некоторых туда можно писать кидать искатель запрос которых как-то схлопывается то из них достает причем речь идет не только про аналитику это и в том числе и у лтп операции могут быть то есть ну например там снять деньги со счета и так далее в принципе тоже попадает в ботинках вот питчинг он хороший потому что позволяет сложные вещи делать вот но когда речь идет про скорость появился такой подход как streaming принцип очень простой то есть вот у вас есть поток событий и вы на поток событий вешаете счетчики которые в real time он прерван кремики руется он просто подсчет количества событий у вас там идет просто clicker то есть веб метод который + 1 + 1 + 1 + 1 делает отличный подход то есть этому все поняли базовый принцип большим streaming починковский эль streaming этого такие счетчики на потоке соответственно у каждого из них есть плюсы и есть минусы собственно начнем со стримингом streaming он очень быстро то есть реально вы прямо вот в долю секунды меня эти счетчики у вас прямо всего актуальность там задержка миллисекунды может составлять опять же в английской литературе вот это второе преимущество с плюсиком которая у стриминга она называется boundless дейта то есть streaming он реально позволяет работать с безграничными данными потому что мы почти не нужно ничего хранить то есть вот идет такая струя событий там их миллиард 2 миллиарда 3 миллиарда неважно она проходит сквозь счетчика дальше технически она идет в мусор то есть у вас данные безграничны то есть в реальности можете через счетчик прокачать сотни миллиардов триллионы событий но так как вам не надо хранить да вы и не ограничена ничем то есть это очень круто но минусы начинаются минусы минус стриминга 1 тяжело реализовывать сложную логику и тут есть очень хороший пример как вы возможно знаете есть такое понятие не аддитивные меры например каунт distinct подсчет уникальных кукол как вы все возможно знаете или кто не знаете поверьте мне на слово а счетчиком очень тяжело отслеживать уникальность кухне храня полный список ук то есть да есть алгоритмы кибер лук лук новых попробуйте на практике применить вы повесить здесь сразу то есть вся сложная логика когда нужно что-то хранить как-то вот хитрый зависимости обратно делать streaming тут ему очень больно и вторая проблема связана хуже на первую очень тяжело исправить ошибку задним числом то есть условно говоря вы крутите счетчик а потом оказывается что вы например прибавляли + 1 а надо было прибавлять плюс 2 на определенные события и вам ну технически эти события они у вас уже ушли в мусор вы уже не знаете как вам исправить данные там есть разные подходы но все они очень болезненны это минус соответственно бы чем в бочонке можно сделать сложную логику потому что ну те из вас кто пишет naskel вы наверное знаете что у нас келли можно практически все сделать особенно если там есть питон и процедурная логика немножко в общем чего угодно можно посчитать на из келли и точно так же с бочонком легко пересчитывать данные потому что у вас все данные есть и вы если что-то вам не нравится вы берете расчет стираете пересчитывайте заново вставляете легко просто вообще замечательно а можно даже параллельно низкой версии посчитайте выбрать ту которую лучше ее и вставить это вообще удобно но с другой стороны есть временные задержки то есть временные задержки пока данные во первых долетят в ваш матч разместятся там правильного всех индекс ухудшаться и когда уже потом вы выполнится запрос на всех данных это может быть долго и вторая проблема это объем то есть если для стриминга триллиона это не проблема то для биллинга это могут быть проблемы особенно если у вас объемы данных неожиданно подросли а вы живете не в амазонии вот диски у вас уже кончаются и вот прямо вот поставка через месяца место уже кончать кончится завтра очень болезненно соответственно плюсы-минусы ну в принципе простейший взгляд на эти преимущества и недостатки позволяет увидеть что преимущество 1 система является недостатками другой в принципе достаточно очевидное решение практически тупое это взять их как-то их объединить чтобы остались одни преимущества и недостатки ушли собственно вот эта концепция на базовый называется лямда архитектурой принцип основан на том что вы делаете два контура то есть вот у вас идет поток ваших событий причем я тут уточню сразу что все понимали события может быть как события коллег стрима так и например лог изменений из вашей обычной вал типичной базы данных но на престол с исмо и скелет не так важно то есть это все как бы события дальше вы эти события for quite дублируйте в два места и у вас с одной стороны спидер быстрый уровень где у вас streaming считается считает всяких значений в реальном времени а с другой стороны у вас bitch in который считает что-то сложное но помедленнее вот а дальше в классической лямда архитектуре предполагается существование некоторого волшебного уровня который позволяет аналитиком кидать запросы к данным ну или системам кидать запросы к данным и чтобы вот этот волшебный уровень как-то определял откуда их брать из спидера или из батлера то есть вот так выглядела оригинальная концепция насколько помню twitter доделал там на шторме на прочих вещах вот ну а не это попытались сделать сделали про эту красивую книжку где они тактично не написали о проблемах которые у них возникли куча людей рванулись это делать и прямо лбом влетели в кучу проблем вот и давайте я вам сейчас попытаюсь проиллюстрировать в какие проблемы тут можно влететь очень болезненно и попытаюсь рассказать как мы их решали причем я тут буду рассказывать на практическом примере так понятнее а то вот я абстрактно говорю может и не понятно будет итак представим такую простую задачу сервис счетчик нам нужно вот у нас есть со это некоторое веб-приложение что им такое и нужно считать подземное количество например по дневной определенных событий просмотры объявлений просмотры телефонов просмотры объявлений пользовать уникальные посетители для данного пользователя вот такие вот штуки причем смотрите там вот у меня не спроста три точки это как раз означает что часть счетчиков мы знаем а часть частью у нас обрадуют это первый нюанс что изменится требованиям то есть дополнится второй момент фильтрация тут уже сложность считать нужно только людей без ботов и партеров причем вы понимаете что боты парсеры на веб-сайте их чистит одним способом боты и парсеры в в мобильных приложениях там другие подходы в счетчиках конечно пользователи хотят видеть живых людей изменчивость ну это вот три точки что могут добавить новые счетчики могут изменить фильтрацию например мы узнаем что какой то есть вот нет моего пропускали и тут весь алгоритм нужно поменять и в идеале вот поменять назад дальше скорость ну конечно все хотят real time если ваше объявление посмотрели только что вы вы в dealer хотите увидеть это тоже сразу то есть вот представьте вам звонят по объявлению на авито а у вас там счетчик просмотра телефонов еще не поменялся вы начинаете думать что что-то тут странное произошло вот то есть секунды секунду хорошо бы ну и стабильность потому что как вы знаете если кто-то работал со счетчиками google там есть такой лобовой подход когда копится сырые данные потом проходит день данные чистится и цифр просмотров она так хоп и падает то есть есть способы как с этим бороться например все цифры показывать умноженным на коэффициент но это очень примитивно все равно будут прыжки потому что какие-то объявления реально много смотрят а какие так только смотрят кто только парсеры и там реальности 0 просмотров так вот как видите куча проблем соответственно ну давайте попробуем эту штуку сделать по простому значит поднимаем какой энтин memory старридж пройдись с тарантул чего нравится что хоть можем поддерживать поднимаем какую-нить очередь событий и начинаем на этой очереди событие считать агрегаты вот смотрите вот снизу вот это вот летят события из свет серверов след слева до слева я дугу всех это слева а у меня мусор то есть события выбрасываются а.с. результаты агрегирования там по этим они копятся в нашем воображаемом одессе или в tarantul а вот ну как вы понимаете очевидный подход я думаю почти все свой счетчик делали сначала так соответственно вопрос что тут может пойти не так первая проблема вот смотрите вот в нашем воображаемом рейде си у нас у меня есть зеленый ряд и красный ряд просмотры для этих атомов и про объявлений и просмотра телефонов и вот решили добавить новый счетчик просмотры ну допустим профиля пользователя или всех объявлений пользователя и вот мы приходим к такой проблеме что этот получается счетчик с короткой историей те счетчики еще нормальные то есть пользователь когда пользоваться нашим сайтом он видит что у нас данные то есть но объявление смотрели а у пользователя как будто ничего не смотрели у него возникает ощущение что его как-то обманули вот потому что тут смотрите неспроста приведен пример про просмотра профиля пользователей просмотры объявлений пользователей в принципе можно сложить из объявление самих а вот просмотр профиля уже не сложить а если еще уникальные нужно считать то вообще сложно вот то есть короткая история очень серьезная проблема для но короткая история для новых агрегатов вторая проблема фильтрация тут собственно в чем суть стоит суть проблемы заключается в том что вы должны реализовать фильтрацию на основании тех данных которые к вам пролетают прямо сейчас а иногда для того чтобы отфильтровать события отфильтровать какую-то куку как плохую не недостаточной информации о ее проведении поведение прямо сейчас довольно часто нужно поизучать ну когда она возникла что она делала то есть она может на короткой перспективе выглядит нормально а вчера она например сделала 10 тысяч просмотров это как бы странно немножко для человека вот или сейчас она может быть что-то странное сделала но может просто человек самолет сел вышел и произошел как бы скачок пайпе но вообще-то всю жизнь он вел себя нормально то есть фильтрация это задача которая плохо ложится на streaming вторая проблема это сложная логика ну и третья проблема самая самая неприятная которое является совмещением 1 и 2 потому что ну как вы понимаете всю эту штуку пишет людям а люди ошибаются причем они иногда и просто так ошибаются иногда они невольно ошибаются иногда они просто не знают о каком-то нюансы внешнего мира и учитывают его неправильно вот у меня на картинке при при изображен сценарий как видите вот красная сетка и желтая сетка когда какое-то время работала неправильная фильтрация которая пропускала определенные действия как человеческие но на самом деле это были боты ну проблему нашли кому надо по башке настучали сделали правильную фильтрацию увидите желтенькую сеточку но событие вот видите в красном прямоугольнике сверху они уже порченые то есть там все плохо и если пользователь будет смотреть график он там будет видеть какую адские выбросы которые каток откуда-то потом пропали ну не хорошо в общем несолидно три проблемы давайте посмотрим как их может решить лямда архитектура то есть для того чтобы реализовать лямда архитектуру . как видите а поток событий для стриминга вот он у меня сверху тесто перестал сама наша и на моем либо за она как бы посередине и добавился второй поток который идет в батч player ну у меня там нарисовано vertica то есть я вообще люблю вертик у но может быть что-нить еще и так смотрите идея как это работает идет поток событий в по верхнему контуру и там в реал тайме работают счетчики которые осуществляют предварительный расчет для аккумуляторов вот видите они у меня там аккумуляторы без желтенькой рамки соответственно какой и вот у нас таким образом получается как бы первые предварительные данные они еще не вполне чистые но они уже есть то есть условно говоря 0 просмотров при звонке мы уже не слове при этом данный параллельно льются в бак слой в медленный там к и когда они туда долетают там происходит сложный расчет и избавь лайнера всплывают уже нормальные очищенные значений аккумуляторы в за определенные временные промежутки вот видите эти значки обведенные желтеньким это вот типу уже нормальные давайте посмотрим где это помогает первая проблема новые агрегаты то есть короткая история новых агрегатов как видите проблема короткой истории для новых агрегатов актуально только на так называемом периоде раскачки периоде между вводом расчета нас быстром слое и поступлением расчета из batch слоя поэтому технически если вы запускаете новый счетчик как бы ночью когда люди просыпаются им заходят на ваш сайт там вот вот этот разрыв между быстрым слоем и медленным вот видите между вопросе com и красной линии batch леера он уже срастется и дырки не будет потому что матч player если он у вас сделал нормально он содержит достаточную глубину сырых событий а и за счет того что вас сырые события вы новые агрегаты можете на живую там просто посчитать вот нужно вам просмотр профиля просто кинули соответствующий select грубой раз подняли данные хоп и у вас о новых счетчиков уже нормальная история дождитесь срастания и все отлично станет итак проблема с новыми агрегатами не проблема проблема с фильтрацией не человеческая активность она то есть смотрите как я вам уже говорил что нормально фильтровать на стриминге очень тяжело то есть отфильтровать все на стриминге тяжело но если мы можем себе позволить позволить отфильтровать на стриминге только явную жесть а потом дождаться до фильтра ван их данных из бачка то в принципе ничего страшного то есть в результате как бы базовой от фильтров капри приведет цифры ну в нужный порядок а потом ну вот точно как у гугла цифра уже зафиксируется на нормальном полностью правильном значении соответственно аналогичная история с исправлением ошибок в прошлом тут в принципе ответ такой же у нас определенные счетчики кривые так сотрите их и перезалейте из базы потому что вошла r позволяет все поднять да тут тоже возможна ситуация что вы можете обновить только то что дошло до барьера и у вас вот этот разрыв между спитфаеры мы батлером он будет но время пройдет они схлопнуться и у вас в прошлое потянутся уже хорошие правильные данные и а вот этой ошибки будут помнить только те люди которым наваляли за ее совершении но возникает другая проблема то есть одна из самых серьезных проблем которые и вызывало как раз неудачи при внедрении лямда архитектуры проблема с дублированием логике потому что если сделать лямда архитектуру в лоб просто мы делаем вот одно и то же действие на стриминге это одно и то же действие на бочонке то нам получается каждую логику нужно писать в двух местах на бочке relocker написать не сложно там с клиентом могут работать сложные конструкции а нас переносить ее на streaming там просто на уровне трансляция языков проблемы возникают и опять же многие вещи просто нельзя перенести на streaming но даже просто будем реалист и вот представьте вот вы видете проект и у вас два куска кода на разных языках которые должны делать одно и то же представите как весело там выкатывать новые версии исправлять ошибки застрелиться можно десять раз причем вот вот именно на уровне вот просто администрирование изменений потому что тут поменяли тут забыли и через некоторое время уже в этих простынях кода на из келли и на каком streaming или уже невозможно разобраться и понять почему они такие разные то есть они расползаются и все смерть проекту до свидания вот эта четвертая проблема это вот чисто то что лямда архитектура ряд проблем решает а вот эту штуку она приносит вот ну соответственно это вот я напоминаю основную идею как лямда должна работать так вот соответственно вот мы решили что нам нужно сделать сервис счетчиков по лямда архитектуре давайте вот посмотрим уже на физические цифры что мы в итоге получили вот это просто иллюстрация как может выглядеть это лиам до архитектуры я просто объясню классические документацию которую вы можете читать там может быть много слов про hadoop там всякие про apache технологии про шторм про hive и прочее я просто хочу чтобы вы увидели что на самом деле за лямда архитектуры нет ничего сложного спит layer например это может быть просто рейде сам не обязательно жесткий стриминговый сервис возьмите редис сервисы на кошке напишите и все супер или тарантул сделаете если вы любите соответственно в нашем случае ну а вид и у нас данных много черви счетчиков соответственно редис 64 ноды 064 сервера просто они там сгруппированы 1 1 и 3 терабайт оперативки для мастера соответственно столько же для слова ну и условно говоря изначально 64 можно поместить на одном мастере потом их можно там расселять расселять ну смотря сколько вы можете серверов выделить и насколько там оперативки многом то есть там проблема там с пропускной способностью сетевой карточки и так далее но в принципе ведь они в принципе ничего страшного мне это кажется i butler vertica мы любим верте q но может быть клика us дальше вот вот данные они могут наверно пугать ну просто мы обе ты у нас данных много если вы будете делать счетчик у вас там возможно будет поменьше вот то есть в нашем случае 14 рви raw 512 гигабайт оперативки на каждом и 50 терабайт данных это я говорю про данные который используется для счетчиков всего-то у нас конечно больше но для другого проекта может быть намного меньше опять же это может быть ли хаус это может быть довольно дешево то есть спирт lair батч player собираете такую штуку собираете другую штуку все работает один человек насколько я помню 5 месяцев об и все в продакшен ну как всегда phone 8 у меня айфона нет но я вот эту сучку штучку очень люблю яблочников вы возможно спросите вот я все к так классно рассказал а как же мы делаем фильтрацию как же как же вот в эту лямда архитектуру привнести грубую фильтрацию на спи двери чтобы не было скачка цифр то есть ну в общем да тут я думаю на картинке достаточно красиво иллюстрирована а очень простой ответ берёте и делаете еще один контур для архитектуры где bы делаете еще одну лямда еще одну ветку служащую для обеления кук то есть смотрите как делать сложные вещи внутри стриминга это вот способ довольно просто вырастить себе монолит который вас похоронят но сейчас же все знают что модно делать микро сервисную архитектуру поэтому как бы вот у вас сервис который агрегирует данные сервису нужно фильтровать сервису тяжело фильтровать сделать еще один сервис который будет помогать фильтровать соответственно это может быть просто in-memory база хороших кук ну мы cookies рассматриваем широко то есть для нас cookies с веб приложения и устройства но с которой мобильное приложение заходит это одно и то же в принципе соответственно можно сделать вот такую лямбду которая будет строить список белых кук в начале грубо по их поведению на потоке а потом актуализировать эти cookies уже исходя из более глубокого анализа там с машинным обучением с анализом истории поведения из хранилища то есть такая же схема видите cookies там определенные действия принимаются решения она типа нормальная копится копится копится потом для неё приходит информация из хранилище и смерти ки о том что либо эта cука ненормальная это хорошо замаскированный под или что вывод куку забыли вы подумали что она плохая но на самом деле она нормальная в ее seo добавьте еще чем белых хук вот вы возможно спросите вот одну для мы делали это же долго сложно а тут еще вторую к родителю какие для этого ресурсы нужны так небольшие в принципе это делается там полумесяц на это ушел тоже и если вы посмотрите на затраты то есть смотрите кука это строка вам не обязательно хранить саму строку в этом хранилище вы его можете хэш омск скала схлопываться int и хранить int и потому что ну коллизии вам тут не страшный но вы ошиблись там с одной кукой ничего страшного соответственно вам нужно хранить вот мы так взяли исходя из нашей всей истории группу посчитали 500 миллионов белых кук слыша устройств сколько это займет принципе 50 гигабайт хватает вообще более чем достаточно ну то есть 25 гигабайт на мастере и 250 гигабайт на слове то есть ну вот это может быть это может жить с другими решениями то есть это общее с современным серверами это просто копеечные объемы ну и опять же бачило и.л. в идеале это должен быть тот же матч player который использовался для анализа агрегатов по счетчикам вот предыдущем вот смотрите у меня тут важный момент написан что если в предыдущем месте там мог быть клика us здесь уже клика у скорее всего не подойдет потому что для многих анализов куб которых мы применяем нужны тяжелые join и к с которым у клика уса пока тяжело но в принципе там можно что-то накрутить но это уже будет не чистый клика us соответственно батлер как видите тут довольно тяжелый но это не копия это тот же самый большой эрэс в предыдущей серии ну и собственно вот основные выводы и решения как их можно решить то есть недостатки лямда архитектуры 1 главный недостаток врожденные лямда архитектуры это публикация логике соответственно чтобы на эту проблему не налететь и обними не разбиться во-первых логика должна быть простой вы должны четко определить что наша линда будет делать вот это второе соответственно если вы понимаете что с простотой вас не получается вам пришли бизнес требования которые потенциально и и усложняют постарайтесь не растить монолит а делать это как раз на микро сервисной архитектуре добавлять контуры лямды тогда можно играть база минут до вот кстати вот тоже не знаю заметили нет что вот этот хранилище белых хук мы сделали не на рейде seo на таран то ли потому что там вот коллекция с индексом она просто занимало меньше места элементарного оперативки там лучших сжатия то есть взяли вот другая задача взяли другую базу более модную году быстрые отлично никто то есть наследие на психику не давит соответственно микро сервис ность расширять микро сервисный важный момент что вы всегда должны быть готовы что ваш спидер вы вы должны быть просто у вас под рукой должны быть все кнопки чтобы ваш и spdr запросить его перетирание данными из butler а потому что ошибки будут вот просто железно будут и надеяться на то что вот сейчас мы запустим и все будет нормально нет вы всегда должно быть готовы какое-то окно какие-то счетчики или вообще просто вот ваша база и не выключиться и все потеряет и вы должны быть всегда быть готовы ее просто очень быстро и без боли пролить из барьера и последний момент что батлер должен поддерживать сложную логику чтобы вы могли там сделать действительно сложные вещи для того чтобы спасти данные сгенерировать хорошие спасаемые данные для спит леера и проверять гипотезы потому что если у вас бачелор будет тупой ну условно говоря не поддерживать много join автома конной функции всякие сложные штуки ну в общем вам тяжело будет а если он у вас будет хороший вам будет вот в такой идеологии вам будет житься очень легко и вывод одним человеком очень быстро это все сделаете и вообще запустить когда он все сервера придут соответственно всем спасибо давайте вопросы мне сказали мне сказали не давать право голоса пока чеку microsoft микрофон не принесут поэтому я пока помолчим добрый день еще раз реально такой вопросу было упомянуто фраза что вот эти агрегаты они пересчитываются ночью ночь в россии понятие такое которые abstract муж есть калининград а есть петропавловск-камчатский когда вы обе та ночь ну когда в москве а ночь но у нас смотрите у нас тут лесу это немножко проще мы все таки не по всему глобусу есть и он даже в россии есть несколько часов когда ну почти у всех ночь я не говорю она всегда именно вот так делается в некоторых случаях есть она не всегда пересчитывается рождение иногда чаще просто это вот ну это скажем так безопасно и окно которое есть если вы действуете в россии если по всей планете у кого-то будут данные может хикари веньки ну спасибо добрый день я справа справа от вас до дразни просто спасибо большое за доклад очень интересно вот меня возник такой вопрос как вы определяете глубину бачили эра то есть когда вы определяете что вот все вот эти данные совсем устарели и мы этих можем избавиться выключу все до свидания это вот очень кстати важное такое политическое решение на уровне бизнес требований то есть вы берете из просто с бизнесами продуктами подписывайтесь что мы все делаем но глубина на допустим год глубже и дети вот он и вот исходя под это дело вы уже подбираете железо объем и прочее то есть это обговаривается это очень важно зафиксировать что например год и не больше спасибо здравствуйте подскажите пожалуйста такой вопрос вот куда во vita вытекает то заветная труба в которую сливаются батч и streaming то есть как вы используете данные в какую б д видимо складываете а потом вроде непонятно непонятно разве было ну там заканчивается все трубой в трубу не осмотрели эта труба это на самом деле значок базы данных просто он набок положенный это база она она просто хранит вот это допустим все агрегата который есть это вот freddy's да вот он сам все и хранит а агрегацию выполняют сервисы на кошечки их можно прямо очень крутыми написать то есть игорь и вот тут в чём суть что и хранение данных и функции спит layers выполняет фактически одна и та же memory база либо радиус либо тарантул нечего будет сложнее то в принципе не нужно спасибо мне такой простой вопрос а как же с сервисным что же сервис был на первом слайде а потом мы про него ничего не смысле вот под бочком и сервис нагружать его постоянно сейчас я вот пойму что онa serving player ну это как бы сервис на горочке например или на php некоторые китайские запрос очки то что еще раз зачем нужен сервер player а чтобы кинуть конкретные запросы получить эти данные и поднять чтобы инициировать запросу условно говоря select a это мой diy аккаунтов views from грубая этом эти чтобы понять какие запросы нужно кинуть вот конкретно к тем данным которые в хранилище чтобы получить например счетчики поэтому в счетчике по юзерам ну это как бы мелочь она кажется очевидным но его надо сделать ну кажется не так что а вы считаете что если у вас вот в хранилище есть все данные то вас сразу ясиа агрегата и не не просто кажется что стирлинг layers это туда куда вы ходите за данными из butler а это не сервиса на кошечек нет смотрите ребята стрелочки посмотрите serving player он берёт туда никто не ходит из бочки эра туда никто не ходит там стрелочки нет вы должны ходить из dior ну и листвой пик куда уходит не знаю смотрите эта штука работает в другую сторону она берет агрегирует и дает уже результаты то есть вот я именно поэтому говорю что лямда архитектура она плохо подходит для это хок аналитики то есть вы говорите о посчитаем не вот чё такое сложное и она там как-то думает это очень плохо это пытались сделать но почти ни у кого не получилось эта штука считает конкретной агрегата вот но который вы требование зафиксировали вот она их умеет считать serving плеер умеет считать такой агрегат такой такой и она их выплевывает регулярно по запросу а вот произвольный креатив она не умеет это вот вы отдельно уже пошла ходите и запрашивать потому что иначе с и нас петлей ри произвольный запрос подержать невозможно к сожалению произвольные конечно нет но какой-то под цвет запросов точно может в этом его смысл сервер slayer и ночного клим просто 3 куда нибудь вот смотрите все вот когда проектировалась лямда архитектура и и хотели сделать именно такой чтобы там была свобода пациент запросов произвольное объединение всего со всеми на практике не очень работает очередь я вам сейчас рассказываю про про решения которого как молоток то есть ветер on out просто работает вы просто берете она так шарах победа то что вы говорите это отличная тема для исследований для разработки такая универсальная лямда которая и оттуда отсюда все соберет и классно нам покажет amazon пытается делать в redshift спектруме и в проекте осина то есть они пытаются это востребованная вещь но готовых стопроцентно рабочих решений я вам пока описать не могу смогу расскажу 1 раз я здесь здесь да если у вас там в вашем tarantul и лежит там счетчиков вас бывают какие-то проблемы с тем что вот их приходится обновить из вертите через какой-нибудь горный сервис особенно если на этих счетчиков еще выше здесь какая-то странная логика которая а на них нет логики вот смотрите это вот проблема исправления ошибки в прошлом но вот ну просто подумайте насколько их но там несколько миллиардов ну вот у вас запрос конвертики такой который вернет вам несколько там миллиард агрегатов он будет пуск ук работу 5 но есть и вот 5 10 минут в быть проливать вы просто несколько миллиардов поделить на 5 10 минут поиметь сколько там секунд и пай и с точки зрения транзакций в секунду и совсем немного для такой базы по одному так ну скажем так она она просто не актуально ну то есть и нет такой особо проблема то есть я понимаю чего вы говорите что если бы мы там делали реально сложные зависимости между счетчиками это была правда мне зависимость они все как бы одноуровнего не просто силы зла так поднимается и там просто вот окей вэлью просто заменяются ну там миллионы транзакции секунду просто простукивают и победа да ну вот как раз предыдущий дискуссии в этот сервер плеер который понимает в нем какие есть счетчики для каждого счетчика у меня есть код который агрегирует их из стрима и для каждого счетчика у него есть код который эти данные умеет sql запросом получите зверь тики и on demand то есть он demand исходя из окин окон доступности то есть бывает условно говоря стабильно допустим ежедневная перезалив к бывает кейсы но когда все плохо нужно спасать тогда прямо вот по щелчку и пошло лицо спасибо за доклад скажите объем вот этой сложной логики которые вам приходится либо дублировать либо как вы сказали микро сервисами использовать ее можно как-то интересовать сколько у вас не знаю количестве функции сложных объеме кодах в чем бы то ни было но я не готов то есть я не по я просто не помню потому что это вот сервис на год другой человек писал я вот просто не помню сколько это все микро сервисами вот сделано практически нету дубликатов стараемся избегать и соответственно вопрос если вам приходится делать не микро сердце дубликатами то для решения для проблем и пользуетесь ли вы какими то результатами которые ваш пример коллега сегодня про тестирование рассказывал андрей соседнем зале или вас никак не используя вы просто вручную сами использовали здесь какая-то автоматизация чтобы сводить на этапе разработки эту сложную логику к приводить чему-то единому ну скажем так я тут отвечу что сейчас мы стараемся бороться с этой проблемой не пропускает сложную логику на уровень вот стриминговых ирригаторов вот это вот мой ответ что действительно если туда напустить сложной логики то все опасности монолита они подстерегают ну и поверьте миллиарды счетчиков на стриминге хранящиеся в намимори базы и довольно сложно сравнить с миллиардами агрегатов из батлера это прямо отдельная задачка можно но сложно и асу и особенно сложно понять почему они не сходятся это прям смерть спасибо большое за доклад у меня вопрос про плохие cookies и пересчет уже с агрегированных данных на по 4 то есть на сколько я понимаю когда мы вдруг понимаем что какая-то кукла как бы до этого нам казалось нормальная сейчас там мы понимаем что это было не так у нас теоретически может быть достаточно большой объем как бы данных на барьере который мы должны пересчитать и более того такая ситуация может происходить как бы регулярно вот в итоге у нас как бы есть постоянный workflow то есть она постоянно обсчитывать новые данные которые оказались потери и в то же время нужно параллельно исправить ошибки которые мы как бы допустили за счет того что там про купите что-то не знали к тому моменту и как эти процессы уживается параллельно то есть грубо говоря кажется что там могут быть пике такие которые будут выезжать за пределы которые дает железо ну и она вашего просто вас прозвучал там несколько вопросов я на них всех могу ответить двумя такими словами а кому сейчас легко и нормально все считается то есть нормальная база поставленная нам бачи юрт даже vertica она вытягивает это ну они для этого и создан и потому что вот ну просто поймите как это делается вот там cookies вот просто есть таблица куб ну там вот тоже полмиллиарда есть таблицы в действие клик стрима ноута они просто joannites и агрегируются ну это не сложный запрос на самом деле ну это пугает конечно но поверьте нужно тут просто закрыть глаза и не бояться ладно тогда еще один маленький вопрос а какое то прогнозирование реально для этого сделать но то есть грубо говоря какую нам ожидать нагрузку связи с тем что мы должны исправить еще свои ошибки помимо того что считает данные которые сейчас поступают не ну можно конечно ну вот я поверьте это вот знаете как бы обратная связь с точки зрения поддержки этой системы там совсем небольшая нагрузка от именно вот этих перелетов задним числом понял пасибо спасибо за бывает хуже вещи спасибо этот доклад один такой вопрос небольшой а что используется для того чтобы хранится вот поток событий эта штука вко или просто там стриминга вояж и на мы называем ее и ее спи она сделана технологиями скилл соответственно есть доклад артема данилова был вчера прах хранилище для микро сервисной архитектуре и там есть ссылка на доклад антону сухого где он рассказывает о нашей шине вы можете использовать кафку кстати вот я вам интересную вещь расскажу то есть по опыту общения с этими людьми здесь мы вот от кафки отказались а я знаю людей которые от энеску отказались пользу кафки и вот чтобы у вас н из киева работала чтобы вы не говорили что я вас обманул и носки круто работает если размер размещение сервисов у вас отвечает кубер нить потому что он позволяет его шортить эскалировать и размазать то есть подбирать нагрузку вот горизонтально исходя из потока без это ставьте кафку без губерний составьте кафку похлопали похлопали и вопросы еще в давайте я счету тыл их буду в коридоре так что это были на бис аплодисменты спасибо за интересный доклад что вы думаете об и читал о чем и штаб хайпа и транзакций аналитика processing то есть когда у вас есть какая-то можем in memory база данных с диска вы хранилищем кто-то горизонтально масштабируется и у нее как операционные данные заливаете так и на ней же читайте анкету агрегаты и какую-то аналитику не совсем совсем уж тяжелую может быть эта мечта вы знаете у кого такое работает пачек на и пытается такое сделать что apache играет пытается такой вот вы зря про подчиняется ну короче уровня он пытается вы подождите нет смотрите это мечта и вот вот я вам задал вопрос я просто знаю на него ответ эта штука есть у paypal на сабха не они подняли единый супер компьютер который все транзакции льет в 2 хранения одной из них ролл сторож 2 кол им сторож 1 является о типе базы 2 является живой и мемориалов базой и поэтому если вы paypal если вы сумеете договориться за sap hana sap hana я я боюсь их чудовищны за это дело ценник вы это получите то есть услуга это мечта которая для отдельных счастливых и богатых людей уже стала реальностью возможно через год или два мы вам тоже чем такое расскажем что все забудьте про всю эту фигню все давайте вот will te pego лап одна база все проблема же решит часто это дорого очень спасибо из любопытства почему такая акция на патч игнайт у меня от скептической немножко отношения к apache hadoop узкому степа общее ко всем этим штукам не хода пуске ну условно говоря когда они доведут все до продакшен тогда и посмотрим могу быть не прав я люб я люблю решение которое вот-вот равен как молоток работ этот редис тарантула просто там сломаться ничему когда играет достигнет такого уровня стабильности знаете как и подход сервисов unix а то есть отлично делает одну простую вещь тогда и поговорим а сейчас не знаю ну все давайте теперь я пойду уже"
}