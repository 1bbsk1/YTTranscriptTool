{
  "video_id": "h2_OzqzEgmY",
  "channel": "HighLoadChannel",
  "title": "Тестирование аварий / Андрей Губа (Одноклассники)",
  "views": 346,
  "duration": 2347,
  "published": "2017-04-10T04:36:32-07:00",
  "text": "и так здравствуйте спасибо что остались послушать доклад на очень актуальной меньше тему итак начнем для начала о себе войти и 15 лет в компании уже почти 8 начинала как системный администратор далее возглавил отдела системного администрирования и сейчас я заместитель технического директора и в компании отвечаю за следующие направления системы администрирования api платформа эта команда которая решает самые сложные задачи в разработке и информационной безопасности немного компанией в цифрах вы видите здесь информацию начиная с десятого года сейчас мы достигли цифры примерно в 45 миллионов уников в сутки у нас порядка 8 тысяч серверов и по трафику мы достигли цифры в один терабит секунд интрига теперь давайте немного пофантазируем оба аварии небольшом в проекте допустим это проект на 200 серверов умер один веб-сервер ничего страшного занимаемся спокойно балансировщик выкинул его из ротации дальше отказала главная база пытаемся перейти на реплику не получается пошел downtime дальше пропало электричество на второй площадке дальше пропала сеть в основном the centre jabber сервер нашего любимого с нами больше нет коммуникация нарушена главный админ в отпуске на телефон не отвечает пятница вечер вообще все те аварийные случаи про которые сейчас рассказал по отдельности может быть и не фатальную но когда они случаются за короткий период времени когда появляются осложняющие обстоятельства такие как нарушенная коммуникация отсутствие самых сильных членов команды тогда решение аварии может затянуться эффект может быть более сильной и соответственно мы можем потерять много денег наша репутация может быть сильно пострадать и наши планы могут сильно нарушится поднимите пожалуйста руку те кто с хайло до занимался решением проблем на продакшн отлично то есть тема актуальна для нас самая крупная авария произошла в апреле 13 года проект был недоступен или частично не доступен практически трое суток это авария наделала много шуму как и среди обычных пользователей так и войти сообществе поднимите продал струкции кто читал статью об этой аварии на хабре спасибо очень приятно а теперь посмотрите на следующий график здесь количество инцидентов с эффектом для пользователей в месяц начиная с десятого года то есть мы видим что количество инцидентов снижается то есть она уменьшилась практически в 4 раза здесь хочется добавить что мы регистрируем все инциденты даже с небольшим эффектов для пульта авария 13 года тоже здесь где-то тринадцатом году соответственно если инцидент небольшое даже для трех пользователи он тоже в этой статистике есть но проект rose кодовая база соответственно тоже растет количество сотрудников растет количество серверов растет система усложняется но количество инцидентов снижается вот сейчас тоже инцидент произошел спасибо еще раз попробовать итак я расскажу о том как мы о проблемах узнаем вообще конечно проблемах можно сдавать от пользователей или от руководителя когда он позвонит лично поинтересуется что у нас происходит но я расскажу подробнее как это у нас происходит значит оперативный мониторинг это мониторинг который рассказывает о проблемах здесь и сейчас то есть проблема произошла бы сразу об этом знаем мы мониторим доступности корректную работу железо инфраструктуры кроме этого все наши приложения стандартным образом пишут статистику это позволяет нам следить за активностью пользователя то есть мы практически вариантами можем узнать о том сколько у нас пользователей в онлайне сколько они пишут сообщений сколько заливают фоточек причем вся эта статистика хранится но она не удаляется мы можем ту же самую информацию посмотреть на дату 2 года назад то есть мы и можно провести какой-то анализ продуктовый то есть мы это постоянно используем какие проблемы ну первое это новое оборудование модельный ряд постоянно обновляются появляются новые поставщики производители оборудование и не всегда можно имеющимися средствами провести оперативный мониторинг соответственно сейчас эта проблема решается так что до того как оборудование попадает в продакшен она тестируется на совместимый с нашими системами мониторинга далее но операционная система как дмитрий самсонов chiral своем докладе рассказывал о том что мы недавно перешли на новый дистрибутив соответственно когда мы переходили были некие сложности совместимости именно в системе мониторинга на пришлось некоторые скрипты переписать что-то подправить системе мониторинга в конечном итоге мы с проблему разобрались дальше следующая проблема проект растет сервисов больше данных больше график соответственно тоже этот скриншот в реальной странице оперативного мониторинга одной из нескольких десятков то есть представьте что команда мониторинга следит за этими графиками их много как мы решали эту проблему значит первая наша попытка это конечно приоритезация какие-то графики важнее какие-то менее важны удобный интерфейс но это проблему не решает соответственно попробовали автоматизировать процесс реакции то есть ускорить реакцию на инциденты им попробовали сделать такую вещь как семафор то есть это банально настройка срабатывание на превышение какого-то лимитом на графиках я вам покажу как это выглядит вот примерно так это выглядит то есть есть некоторые показатели за которыми мы хотим более пристально следить мы настраиваем триггера соответственно они срабатывают команда мониторинга получает нотификацию кроме этого системы активно пользуются продуктовые менеджеры им иногда интересно знать что стали больше дарить подарочков или стали больше слушать музыки дальше следующая проблема которая заключается в том что графики трудно анализировать когда их много системы очень сложно и мы попробовали для себя эту проблему решить тем что запустили свою собственную систему под умным названием smart мониторинг она автоматически обнаруживает аномалии на графиках она показывает взаимосвязь между этими аномалиями и решает проблему механики создания проблемных тикетов то есть он автоматически создает их и добавляет при начальную информацию это выглядит примерно так это скриншот реальная аномалий произошедшей и здесь видно что это видео сервис с ним есть некая проблема он стал работать медленнее это влияет на веб-сервера и причина этому это некое торможение баз данных то есть видео и само по себе эта проблема крови вайп суров еще влияет на некую базу дальше следующая наша система мониторинга кроме оперативно есть еще графики использования ресурсов но те кто видел как си примерно поймут о чем я говорю то есть в этой системе есть все production оборудованием и туда добавляется автоматически как только появляется в продакшене мы наблюдаем за использованием дисков памяти traffic of на интерфейса кроме этого для java приложений мы следим за java типом и скоростью работы горбач коллектором и s4 серверов собираем порядка 3 миллионов метрик эта система используется когда нужно что-то расследовать что-то пошло не так и можно пойти на сервере посмотреть как работала java в то или иное время следующая система это такое интригующее название у нее смысл следующий то есть какой-то момент времени мы мысли начали сталкиваться с проблемами когда начали узнавать что в каком-то большом кластере из нескольких сотен суров у нас есть две недели закончится места на дисках очень неприятно конечно у нас есть склад и резерв оборудование но не всегда но можно этим резервом покрыть большую потребность соответственно мы хотели узнавать о проблемах заранее значит эта система автоматическом режиме мы выкачивает информацию из системы графиков два раза в сутки и раз в неделю обязательно запускается режим аналитики который согласно установленным правилам проверяет все ли у нас в порядке то есть мы можем узнать что например на каком-то большом опять же кластере через два месяца при таком же тринити использование ресурсов этот ресурс закончится здесь можно видеть группа серверов конкретный сервер какие с ним есть проблемы и формула которая используется для подсчета то есть там свободная память минус той свободная память буфера кэша и так далее то есть вы можете посмотреть на форму и и справа видна конкретная задача в рамках которые это проблемы сейчас решается то есть каждую неделю мы прогоняем на аналитику если есть проблема создается сразу задача и админы занимаются ее решением нормальной ситуации напротив каждой проблемы есть задача дальше резервирование ну я не буду много рассказывать резервирование на холоде многие в этом специалист по силе и меня я просто приведу простые примеры для начала то есть это диск играет массиве резервный интернет-канал реплика базы данных множество серов одного типа в кластере а теперь как это происходило у нас самого начала то есть мы начинали как enterprises некий проект подход к работе и типы оборудования были взяли из небольшого проекта опять же на 200 серверов мы использовали матицу sqweel 100 раджа система крайне данных hp сервера hp и надеялись что это оказалось бы более надежное оборудование нам поможет но это перестало работать довольно быстро потому что объемы данных росли сложность росла и быстрая реакция не всегда помогала ну плюс к тому довольно уныло когда один из крупнейших проектов рунета ночью на два часа останавливается для того чтобы создать и индекс в какой-то базе или выкатить новую версию кода соответственно количество инцидентов росло практически там каждый день была какая серьезная проблема и мы решили в какой-то момент переосмыслить наш подход к обеспечению резервирования расскажу как значит первый тезис надежно и железо тоже отказывает если у вас есть один сервер то он может надежно может раз три года отказать ничего страшно если у вас 200 то наверно раз в неделю как проблема будет если вы с 2000 может быть чаще чем день что-нибудь случается соответственно нужно обрабатывать аварийное сценарии на стороне приложений то есть вот такой тыс для себя выработали и вот короткий список основных сценариев то есть это отказ связанных сервисов замедление возвращение в работы и старт бессвязных сервисов то есть все наши приложения обрабатывают стандартные сценарии отказа дальше управляемой деградации смысл следующее то есть если нарушена работа от какого-то большого кластера и мы не можем решить эту проблему быстро мы должны уметь контролируемая деградировать работу сервиса то есть сказать пользу на том что например в этом разделе сейчас показываются неполные данные отдавать не все данные с другой стороны если получается так что класс строку это заработал перестал работать совсем то мы должны уметь аккуратно его подключить показать пользу примерно такую страничку с парашютом и потом уметь включите сервис обратно но все это нужно проверять поэтому мы завели и у себя не большой зверинец это наша собственная разработка горилла помогает тестировать работу тех самых рубильников когда мы отключаем полностью сервис и проверяет работу сервиса когда недоступна его реплика в одном из это центров годзиллу мы пока еще не выпускали но смысл примерно должен быть вам понятен она должна будет автоматическом режиме проверять работу портала когда недоступен один из это центров и так тезис все данные должны иметь копии в разных дата-центрах это нужно для того всем понятно чтобы родной какой-то локальной аварии не был уничтожен весь сет данных включая бэкап и потому что аварии существенных у нас было несколько обратно перемотаем спасибо например горела основная резервная оптика сразу в один момент то есть мы площадку потеряно несколько часов дальше была ошибка проектирование электропитанию дата-центре это вызвало при росте нагрузки по электричеству к правлению изоляции и риску пожара то есть представьте что у нас есть несколько вечеров до 73 он сейчас может загореться то есть нужно все критично как-то переносить оттуда общем такая интересная история была дальше был ураган в москве очень интересная история произошла когда ураганом с какого-то соседнего здания кусок крыши сдуло и он попал на систему охлаждения до центра и она перестала работать то есть посадить на нам пришлось тушить резервное оборудование потом менее критично и потом самое критичное то используется ли пасхи дня увидели что половина портала не работает соответственно мы для себя поставили главную задачу проект должен работать случае отказа любого то центр эко это задача сильно приблизились какие сложности он приложении много они изначально не были готовы к поставленным задачам соответстви нужно переписать много кода плюс сложность заключалась в том что нас покинули некоторые разработчики которые из приложения начинали разрабатывать дальше нет готовых технических решений это означает что мы поставили задачу уметь обрабатывать аварийные сценарии у нас нет решения то есть в балансировке в система хранения данных пришлось решение искать или писать тестировать запускать продакшн и не хватает людей всем понятно что это значит обычно но обычно руководство очень быстро идет навстречу когда мы начинаем терять деньги в нашем случае было тоже сам то есть в конечном итоге проблему решили теперь я приду к рабочим инструментом опять же я не буду что-то вам советовать потому что в каждом проекте рабочие инструменты свои я больше расскажу о ней в контексте аварий и самое простое это доступ здесь все понятно должно быть есть доступ в продакшен можем что-то сделать нет доступа у картинку помнить с планом эвакуации вот значит должен быть аварийный доступ при чем здесь есть две важные вещи кацыв которые хотел вам рассказать это то что варин доступ действительно должен работать то есть это нужно проверять время от времени потому что инфраструктура меняется появляются новые сервисы и два месяца назад вы могли достучаться с аварийного там где пия надо код сервиса сейчас не можете почему то соответственно проверять и второе это безопасность ходит доступ и аварийный но он должен соответствовать всем политикам безопасности то есть с другой стороны он не должен быть сильно завязан на существующую инфраструктуру потому что случае аварии случиться опять эвакуация а теперь немного примеры из реальной жили вот в случае нашей большой аварии 13 года мы столкнулись такой ситуации что не можем попасть на все наши тысячи linux серверов по ssh кроме этого система централизованного управления которая могла бы эту проблему решить тоже перестала работать соответственно нам пришлось вручную вручную перекодировать нескольких серверов вот соответственно если у нас было лекарство сразу если вы все было сконцентрировано правильно мы не потратили бы очень много времени и так продолжаем систему управления как я уже упомянул может быть у кого-то есть паппет у кого-то есть какая-то панелька админка который позволяет всем управлять они должны работать во время аварии то есть вы не должны во время аварии заниматься починкой ваших систем управления должны с помощью их аварию устранять система мониторинга леса очень интересный момент который может быть все не сразу понимают пока с реальные проблемы не столкнуться смысл в том что система мониторинга во время аварии не должна показывать второе решение аварии кто составляет аварийный план кто в случае аварии если это необходимо предоставляет доступ и которых на момент решения аварии может не быть и кто отвечает за то что потом эти доступы закрыть кто координирует работы в отдельном дата-центре и как эти роли могут приниматься таком случае и кто отвечает за разбор конкретной аварии дальше чек-лист по установлению сервисов здесь все должно быть понятно у нас есть приятель ированный список наших сервисов то есть мы знаем какие сервисы чинить в первую очередь в том числе и варенье доступа и системы управления если все-таки они стали недоступны там есть ссылки на инструменты и инструкции то есть конкретно можно пойти посмотреть как вот сервис который только что починил можно еще и проверить на корректность его работы дальше что делать если что-то пошло не так потому что план есть конечно есть инструкции но персонал и что то может поменяться и пойти не так поэтому тот человек который занимается решением конкретные проблемы знает что ему делать есть что-то пошло не так а нужно пойти к координатору изложите суть проблемы и вместе с ним решить что делать дальше кроме этого обязательно есть тестирование всего функционала после решения аварий отдельные инструкции для чек который которого которым веселее всех в момент аварии это координатор он распределяя задачи он контролирует их выполнение он делегирует решение каких-то оперативных вопросов он обновляет оперативный план и и о происходящем информирует руководство и все команды которые участвуют решение проблемы у нас постоянно аварии происходит дальше отдельные инструкции на следующем на случай падения дата-центра ну здесь все просто если мы потеряли площадку у нас есть короткой инструкции как оттуда вывести трафик как трафик зависть и обратно и как проконтролировать что до того как заводится трафик что все хорошо есть также регламент взаимодействия с партнерами ну здесь принципе все понятно список контактов как мы с ними связываемся какие у нас уровне взаимодействия и в нашем случае то есть кто из нашей команды какие роли выполняет при коммуникации с партнерами значит наш план мы тестируем 1 квартал или чаще если нужно кого-то дополнительно обучить каждый раз мы назначаем отдельного нового ответственного который придумать сценарий мы обязательно привлекаем для тестирования и другие команды то есть сетевой отдел разработчиков для того чтобы не случае реальной аварии не терялись в том что происходит кроме этого мы фиксируем обязательно те проблемы которые результате тестирования обнаружили и сразу же появляются задачи на исправление этих проблем я рассказывал о том что мы фиксируем все происходящее вот это скриншот реального chateau тестирование плана на прошлой неделе когда у нас произошла авария то есть к резонатор придумал сценарий по этому сценарию начали работать люди и тут произошла реальная авария соответственно все переместились в другой чат вот примерно так сами аварии это тоже тестирование скажите просто какая авария является протестированный как вы думаете та авария из которой сделаны выводы та которая проанализировано я скажу немного о том как мы это делаем то есть мы все проблемы регистрируем в том числе и время также мы проверяем скорость работы команды мониторинга тем самым что мы мы фиксируем время оповещения когда людям сообщили о том что проблеме есть мы обязательно фиксируемый эффект от аварии хронологию суть того что произошло и обновляем этот сидит проблемой в процессе кроме этого мы устраиваем разбор аварии и дополняем обязательно ticket результатом этого разбора плюс в этом ti кити появляется список задач на исправление с конкретными исполнителями и сроками вот хронология аварии прошло во вторник а вот реально скриншот значит мы заметили проблему пытались ее починить откатились отключали включали накатывали это заняло достаточно во время в конечном итоге мы протестировали портал для того чтобы убедиться что все работает хорошо и того мониторинг должен быть точным надежным ваша система резервирования должны обрабатывать с основные аварийные сценарии рабочие инструменты должны работать во время аварии и это нужно проверять а план действий при аварии должен быть его нужно тестировать оттестирована аварию это то из которой сделаны правильные выводы читайте наши статьи notable следите за техническим выступлениями спасибо спасибо большое за ваш доклад у меня такой вопрос вот когда вы проводите посмертный анализ аварии вы делайте выводы как ты на будущее могли бы быстрее и чинить вот эти вот дельты между каждыми шагами как можно сократить или какие шаги можно исключить из починки чтобы авария быстрее чинилось спасибо спасибо denis него что значит как я уже упоминал результаты разбора то есть происходит разбор соответственно во время разбора мы анализируем все эти сложности с которыми мы столкнулись делаем из этого вэдэ делаем задач на исправление в том числе и это касается скорости восстановления если правильно понял вопрос скорость устанавливаем бы да да безусловно спасибо спасибо denis здравствуйте андрей спасибо за интересную презентацию я так понимаю аварии это некая такая большая проблема которая стала видна многим пользователям и длилось не которое длительное время да вот выгоде provari который бывает редко но учитывая от вас но много серверов вас видимо есть часто более мелкие проблемы которые решаются как-то оперативное да вот внутри поддержки вот в частности такой момент допустим где-то течет память как вы поймете что нужно покупать диски от вам пойти в разработку пойти в копайте там кони performance тестирования спасибо значит нас происходит следующим образом то есть все аномалии фиксируется это значит если мы с темой это ринге увидели что вас память усекла создается проблемный ticket дальше команда мониторинга в рамках своей компетенции пытается разобраться самостоятельно если эта проблема простая то есть эта проблема очевидна непростая дальше подключается дежурный разработчик который опять же определяет или он может проблема ученик сам или передать конкретную команду то есть ticket переезжает в конкретную команду которая занимается устранением проблем и а дежурный разработчик он на в рабочее время вот и лето такой 724 то есть у нас дижон разработчик дежурить неделю у от апдейт одобрит то есть и он же выкатывает апдейт и лагерь логично логично следить за тем что все после этого происходило хорошо еще вопрос о координаторы из из из каких сотрудников выбирается это из поддержки или из руководителей да я отвечу спасибо то есть по умолчанию это руководитель отдела системного администрирования далее это кто то есть им рядов то есть если нет руководителя дальше тут есть 7 рядов если никого из них нет талия наиболее опытные сотрудники и последний вопрос знают ли разработчики вашу схему развертки платформы да вот эти тысячи серверов как они связаны и как они взаимодействуют и смогут ли они в этом разобраться оперативно ну обычно это происходит следующим образом то есть дежурный разработчик это один из самых квалифицированных то есть он может суть проблемы понять то есть если он сам не может решить то он то есть эскалирует проблем usb подключает нужные ресурсы в конечном итоге проблема решается то есть не было никогда такого что он проблему не решат не отключит рога вопрос а вот сама схема этих тысяч извергов она известна разработки это кем она продумана кем она прорисована вы давно и это большие знания до таки очень специфично то есть хоть про структуры и большая но в ее построении используются стандартные подходы то есть все все эти 150 микро сервисов которые у нас есть они между собой общаются стандартным способом со стандартным способом пишут логи стандартным способом можно посмотреть про них статистику где опять же все примерно одинаково то есть да знают спасибо добрый день у меня два вопроса и 2 зависит от ответа на 1 в общем вы сказали что вас там когда-то центров y интрига до сказали что основа дата-центров там предположим 2 и ну как минимум ладно 2 простой вариант если я отвечу один то нормально 2 после это не вопрос на количество до центров в общем вопрос в том что работают ли они и они высказали что вы переводите в режим совместимости же приложения работали с двумя дата-центр это мульти дата-центров вот вопрос том что работать ли они всегда параллельно или у вас есть какой-то актив и пассив дата-центр там в случае аварии 1 вы все переключите на 2 или они все-таки работают в одно время все это центр мы стараемся что были равноценны да отлично тогда вопрос такой некоторые большие компании проводят не только там тестирование виртуальное что ахтунг все сломалось да давайте чинить там или что-то сломалось они берут идут или физически включают стива оборудование дата-центре чтобы сделать апдейты чтобы сделать что-нибудь там по сетям мы можем действительно что-то поломать центре вот я в это время листаю ответ подхожу нет нет не правильный ответ мы пока не занимались реальным тестированием то есть мы к этому вплотную подошли когда говорил что у нас есть большая главная задача я так же сказал что мы к ним приблизились вплотную мы пока не готовы выключает то центры но этот вопрос наверно полугода-года супер спасибо спасибо можно еще раз мы просто такой вопрос вот вы сказали что вы внедрили вот эту систему по контролю за ворами там выводы и так далее да а вот по каким метрикам вы понимаете что стало лучше пусть куда вы смотрите там за их количество отказов там всего сервиса в год или какие еще метрики есть до носа вначале показывала первый график и они просто его в excel и нарисовал то есть это один из инструментов анализа мы смотрим то есть во первых все инциденты регистрируются реально соответственно мы собираем статистику инцидент разного типа с железом софтом внешне инциденты когда нас партнеры подводит соответственно руководителей технический проводят анализ количество инцидентов и особенное внимание уделяется инцидентов с эффектом для пользования то есть это некий предмет отдельного разбирательства каждый раз и проводится анализ делаем выводы то есть но вот основное это работа с инцидентами то есть мы анализируем происходящее аварии из этого делаем выводы да но вопрос был про метрику может быть это аптайм там всего сервисы и ликуют его частью нет если говорить про конкретное слово аптайм то мы его не мере ну то есть у вас нет все это какой-то метрики по которой вы смотрите что услуга стоит что там лучше и лучше и лучше там не знаю метрика которые в среднем мире и там какой-то показатель завис на эту неделю месяца смотри сюда простой случай типа аварии происходят они не влияют или они быстрее чинится и поэтому суммарно все лучи лучи лучи и вы идете к светлому будущему мне кажется на вопрос уже ответил то есть используется опять же статистика из инцидентов мы смотрим на них и сделаем из этого выводы то есть какой-то отдельный там за графика графика аптайма у нас нет такого то есть есть статистика по инцидент помню спасибо вот какой вопрос это у тебя где-то вначале был график свет взаимосвязи причине следственный упал видеосервис что что зависит от него и чего произошло падение а вот да вот этот о каком как он был построен в что такое мы используем это это своя разработка используя для этого алгоритма то есть есть некий набор алгоритм я сейчас не буду публично озвучивать какие алгоритмы используем это автоматически страдает автоматически без практически в reel to me у команды мониторинга перед глазами есть экран на котором появляется изображение текущих проблем в графическом виде в таком вот так круто я очень только вот а второе вот ты узнал что нужно систем мониторинга которая сама не загибается пикапах вот-вот мать иху со времени нашли пришлось свой написать иова что у вас интересует именно так же это анатолий его знаю поэтому на ты к нему обращать анатолий вот ты про какую про оперативную или просто просто мониторинг не знаю на 10 тысячах что вас черт смертность территорию написали то есть свое безусловно и есть запись про который многие уже слышали за время конференции ну да есть свое есть у нас тоже пытались для записи не работать вот у нас работает круто здравствуйте можно еще один вопрос вы сказали что протестированная аварии это те из которых сделаны выводы а что если выводы неверны или они не могут быть сделаны в данный текущий момент времени то есть что происходит с такими авариями они повторяются нет дальнейший анализ какой-то производишь еще раз не слышали дальнейший анализ какое-то производится или что если она не повторилось или повторится ещё не скоро это бесконечный процесс то есть аварии всегда будут не будет такой ситуации что ничего не ломается той силе и вещь об авариях какого-то конкретного типа с каким-то сервисом или нет я имею ввиду продолжается ли расследование или она там на каком-то этапе останавливается и все очень через uid андре сироп обычная логика работает то есть если эффект от аварии небольшой но ресурсов на расследование потрачено уже безумно много мы останавливаемся то есть если рис очень большой или эффект очень большой ту мы будем копать пока не разберемся спасибо"
}