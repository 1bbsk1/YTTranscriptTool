{
  "video_id": "SeNq4CaIZ6U",
  "channel": "HighLoadChannel",
  "title": "Опыт и приколы эксплуатации сети одного видеосервиса / Сергей Митин (ivi)",
  "views": 1682,
  "duration": 3019,
  "published": "2023-01-19T05:55:16-08:00",
  "text": "добрый вечер еще раз представлюсь я сергей митин работаю в интернет кино the theatre эви работу уже достаточно давно занимаюсь развитием и эксплуатации сети ну и соответственно вот этот вот свой опыт приобретенный за много лет труда хочу рассказать немножко поделиться и рассказать самая интересная прикола как бы ни были надежные сервер и как бы ни были надежные базы данных как бы ни были надежно логика работы сервисов все это бессмысленно если сеть собрана из палочкой веточек но это в общем то достаточно очевидно тут опять же поделюсь каким-то прошлым опытом а вы наверно удивитесь но какое то время назад мы использовали у себя в сети коммутаторы d-link но вроде совсем не не enterprise и тем более не продакшен оборудование использовали мы правда не в продакшен сегменте использовали сегменте резервного контура пор управления но между тем то есть они полностью нас устраивали все было прекрасно с этими коммутаторами это был у них один маленький недостаток каждые восемь девять месяцев они утекали по памяти и требовали перезагрузки согласитесь не очень здорово даже для не основного сегмента сети еще одна иллюстрация вот для этих прекрасных картинок возможно вы помните была история когда один крупный федеральный оператор так удачно обновил софт у себя в сети и причем имя это были именно сетевики то есть все самое интересное аварии устраивают sti вики что он вырубил работу сервисов во всей западной части россии по моему на несколько часов вот как то так а к чему я клоню сеть должна быть надежным фундаментом для сервисов и нам нужно не сеть нам нужно в скалах гранитная скала ну стоит немножко отвлечься упустить стройность мыслей в архитектуре сети в планировании работ просто какое-то разгильдяйство и ты вместо сети получаешь клубок веревок который потом попробую еще раз путай ну то есть лучше учиться конечно не на своих на чужих ошибках но интерес не рассказывать про свои и так у себя в сети использовали причем в ядре сети мы использовали коммутаторы extreme extreme это не стиль жизни это название сетевого вендора использовали коммутаторы между ними устанавливается некоторое количество линков соединений объединенных в не так называемый link aggregation так называемый лак с использованием протокол велосипед в общем-то задумали хорошее дело пришло время обновления софта ну плановые обновления софта софт обновлять надо потому что регулярно устраняются уязвимости баги и прочее прочее прочее но это был особенный софт который мы накатили мы его накатили применили и произошло интересное событие был у нас link aggregation собранный из нескольких 10 гигабитных каналов в результате бага он развалился и вместо link aggregation получилось просто несколько 10 гигабитных каналов но с одинаковым набором милонов ну а что в этом случае произойдёт в этом произойдет чудовищный и адский шторм и этот шторм был еще более чудовищным потому что была не одна петля даже одна петля в 10 гигабит нам линки может наделать много чего плохого а тут разгон шел шторма в нескольких петлях в общем что он получил колоссальный он за аффект л нам тот сорт в котором эта неприятность произошла он за эффективном соседний сад как там потом рассказывали он даже за опек телка кого-то присоединенного к оператора связи кому мы присоединены были и они неосторожно не фильтровали прилетающий к ним хобот шторм в общем было весело продолжу свои экстремальные условия и так те же самые мои любимые коммутаторы они они меня точно любимые учитывая сколько крови они не попортили они были собраны стек а что такой стык это некая технология которая позволяет несколько физически разных устройств собрать в одно логическое устройство ну там в принципе все здорово полезная штука и программно-аппаратной архитектура экстрима подразумевала что есть уровень операционной системы есть уровень железа а между ними слой аппаратных абстракций так называемый опять же делаем хорошее дело решили в свою сеть сделать правильной красивой и настроить колите а сервис для трафика чтобы наиболее приоритетный трафик обрабатывался в более приоритет при применении этих настроек алгоритм работы софта на коммутаторах подразумевает что нужно выполнить перепрограммирование чипов но алгоритм был сделан так что перепрограммирование проходила шаг за шагом шагом было много процедура была достаточно ресурсоемкая потому что много чего перепрограммировать надо было чтобы в результате получили в результате циpкa был настолько занят взаимодействием с уровнем аппаратных абстракций что у него не было времени заниматься всякими глупостями как production обслуживание коммутатора то есть там обслуживать стек обслуживать или хиппи в общем все эти глупости у него развалились значит дальше то получилось вместо в степи link aggregation несколько 10 гигабитных клинков между ними что происходит снова мощнейший шторм который снова был мягко говоря неприятен ну и на прекрасном миндале cisco как у нас принято говорить хотя они говорят сиську но я буду говорить по старинке так как я привык обожаю этого вендора то есть я вырос на нем как и многие сетевики но и нацистские есть пятна значит в чем эти пятна заключаются тут вообще была прекрасная история наша инфраструктура в москве достаточно отказоустойчивого у нас 3 дата центра в каждом дата-центре как минимум пара коммутаторов ядра в каждом дата-центре свой border что такой border border это можете завтра который стоит на границе сети и обеспечивает связь между внутренней сетью и внешним миром в данном случае интернет ну и так нач в каждом соде свое ядро свои borders если из строя выйдет выйдет любой border для нас это не проблема оставшиеся borders абсолютно спокойно будут тащить всю нагрузку в том числе в член это частной большей нагрузки период когда сервис нагружен максимально в том числе даже на новогодние праздники в принципе мы можем потерять даже два бордера при этом мы сохраним связь с интернетом мы по-прежнему будем обслуживать все будет здорово ну правда в этом случае лучше уже не в члены лучше не на новогодние праздники вот но представить себе ситуацию что из строя выйдут все три бордера мы самонадеянно считали что такое невозможно то есть это невозможно сломать но как известно нет ничего невозможного для настоящих профи на дисках был один интересный баг но мы используем access control list и так называемые акулы и там был баг если я занимаюсь редактированием окц с листа и совершаю ошибку перезаписываю новое правило поверх существующего то всплывал баг который приводил крашу маркиза таро а чтобы этот краж танец и ровать нужно было сделать две две операции либо сделать шоу на этот акция слез где допущена ошибка либо сохранить конфиг значит все здорово инженер выполняет сетевой инженер выполняет какую-то задачу ему нужно внести изменения в access-list и да еще один момент что access-list и от ну там это был такой access-list который был одинаков на всех трех бог дырах он в одном месте в готовить себе конфиг в конфиге допускает ошибку ну как он же скопировал какую-то готовую строчку ее надо добавить в конец а он взял и забыл номер строчки поменять у него открыты три консоли значит заходит на 1 border применяет не смотрят что ему нему сказал заходит на 2 border применяет не смотрит что он ему сказал хотя он его рук обругал заходят на 3 border все здорово а что делает дальше любой cisco инженер когда изменяет конфиг если он не терял конфигурацию в результате того что забыл сохранить его он его тут же сохранять у него открыты три консоли и он по очереди сохранить сохранить сохранить значит но как я упомянул просыпается этот баг и прибор драк крошиться и так произошло невозможно то есть они все три reboot ну лись reboot занимает где-то 35 минут примерно и вот это вот был чудовищный стыд и позор когда мы на пять минут потеряли связь с интернетом ужас но и тут есть ещё одна вишенка на торте все эти увлекательные действия производились после 18 00 время вроде рабочие еще до но это вообще-то уже час наибольшей нагрузки то есть это уже высокая нагрузка на сервис ну и соответственно отгребли по полной как говорится было весело на свет соответственно так родились скрижали о том как надо конфигурировать чтобы не было больно несколько простых правил то есть если конфигурация потенциально опасно в обязательном порядке делаем review & review делает человека другой то есть один конфиг создал review сделал другой простая здравая логика configure оборудование без крайней необходимости в час и наибольшей нагрузки это по вечерам или выходные когда нагрузка на сервис и максимально ну не надо так делать лучше этого избежать если вдруг по каким-либо причинам действительно есть осмысленно необходимость о том что все таки надо что-то сконфигурировать то тогда у нас действует такое правило что инженер который понимает что это надо сделать он оповещает об этом дежурных он объясняет зачем это надо получает добро от руководителя но и соответственно опять же про пройдя через review если уж настолько нам припёрло как говорится ну давайте-ка грецов чен ын что-нибудь подыскать фигурирует всегда интереснее то есть конфигурировать в час наибольшей нагрузки вот следующее правило а сеть это сердце системы там я люблю сеть потому что я стиви как говорится поэтому считаю сердцем вот и вышло из строя сеть ничего ты не одни из сервисов ничего работать не будет поэтому когда мы планируем какие-либо сложные а значит потенциально опасные конфиге применяем новую функциональность существенно меняем конфигурацию там сетевую в общем делаем что-то опасное это делать только глубоко ночью но почему глубоко ночью а если вдруг мы что-то не учли и у нас что-то пошло не так и мы все-таки пользователи зацепили значит мы зацепим пользователь не так много как это было бы если бы там было днем ну или тем более в член и есть больше времени найти таблетку то есть и исправить ситуацию но и вот про поводу жертв копипаста есть свой клуб когда мы раскатываем конфигурацию на парные устройство пара коммутаторов доступа пару коммутаторов ядра то есть одинаковые конфиги то есть не надо катить везде то есть есть я накатываю на ядро новую конфигурацию я коммутатор ядра краж ну это чертовски неприятно но если я не крашу второй коммутатор ядра у меня сеть продолжат работать соответственно накатили на 1 пауза втереть в течение этой паузы внимательно смотрим за системой мониторинга есть и не видим ничего катастрофического ну там катим следующий конфиг точно на авто на следующее устройство которое стоит в этой паре итак нам нужно не сейчас кого то есть нам нужен гранитный фундамент для сервисов все компоненты сети которые так или иначе задействованы и вас в бизнес критиков но они почти все таки они должны быть зарезервированы при аварийном выходе из строя любого компонента возможны переходные процессы когда связность подрывается ну потому что по-другому невозможно но обязательно должно производить происходить самовосстановления и это принципиальный момент самовосстановления они восстановления руками и желательно чтобы эти переходные процессы занимали ну несколько секунд ну мысли простые здравые что с этим делать и так выход из строя любого компонента сети не должен привести к нарушению связанности и не должен привести где-то к деградации производительностью есть мы должны тащить нагрузку даже в член как мы это делаем но любой сервер должен иметь минимум два линка в разные коммутаторы доступа любой коммутатор доступа должен иметь минимум два ленка к разным коммутатором агрегации любой коммутатор агрегации должен иметь минимум двалин к коммутаторам ядра в соде должно быть не менее двух коммутаторов ядра и коммутаторы ядра между разными методами должны быть собраны в кольцо ну чтобы при разрыве общая связность между сотами сохранялась но мне могут сказать что это такое ветхозаветный eats кульный подход и модные ребята используют sdn и это software дефо нет network до используют классная штука но у есенов пока все не так просто с точки зрения стандартизации и работы в multivan дермой сети а у нас есть multivan дурная то есть нам on a vendor на сети работать конечно легко и приятно но нельзя себя ставить в зависимость от одного конкретного вендора поэтому принципиальная позиция что сеть у нас multivan дермы дозирования наша религия ключевые компоненты должны быть в количестве двух штук если по какой либо причине и объективным условиям конкретный компонент сети не требуют там дублях внутри дата-центра например пограничники borders у нас в каждом соде не по 2 штуки по одной штуке но они между собой если внутри сода в одном своде выйдет из строя другие подхватят нагрузку и все будет хорошо но в там где мы внутри сода не резервируем значит мы резервируем внутреннюю хардвар ную архитектуру конкретного устройства должно быть внутри железки 2 control plain 2 switch fabric 2 роутинг engine и линейной платы должны быть в четном количестве чтобы мы могли сделать симметричные подключения картинка хорошая мысль здравые то есть как всего этого добиться ну в целом вроде все очевидные просто все приличные сетевые вендоры умеют собирать так называемой стыковые конфигурации как я уже упоминал это когда несколько в физически разных устройств собираются в одно логическое устройство в стеке есть мастер есть slave если мастер выйдет из строя sleeve подхватит и его функции и все будет работать а далее потребители то бишь серверы просто подключаем а в разные физические железки все здорово но тоже так жили в старые добрые времена но как известно не всё так очевидно со стеками и дьявол деталях то что есть слив есть мастер это очень здорово но отсюда вылазят один неприятный момент это единый контра plain с одной стороны единый контра plain это хорошо потому что это упрощает конфигурирование с другой стороны из этого же плюса вылазит и минусы когда control plain единый далеко не всегда корректно и гладко проходит смена masters life когда какой-то когда мастер выходит из строя и слоев видит о том что мастер вышел из строя все здорово это не проблема это штатное поведение сети да у нас сломался мастер слоев подхватил его функция стал мастером forwarding пакетов не прерывался мы прекрасно все здорово но хуже другое то есть ну как известно в любом случае когда сломалась не до конца это всегда плохо и вот в ситуации если мастер по какой-либо причине сломался и не может выполнять свои функции но при этом опять же по какой-либо причине slave не понял о том что мастер сломался вот этот вот switch over переключения с мастера нас life не произойдет и получится дурацкая ситуация это не консистентные состоянии от очистись на этот сто процентов там реализация разного бога ра рода багов и как мы говорим о наведенного статического электричества то есть будем списывать все на него значит это нештатное состоянии но оно возможно и в этом случае мастер сломался не forwarded слоев был настолько скромен что не понял о том что мастер сломался его функцию не взяли в результате получается что astek черт побери ничего не делает он for вардинга мне занимается и главная функция сети нарушена то есть мы не передаем пакет и вот то что я вам сейчас рассказал это был реальный случай когда у нас стоял стояла пара коммутаторов доступа моей любимой циски они были собраны в стек и действительно мастер что-то с ним случилось мы под так и не поняли что с ним случилось slave не переключился forwarding отвалился все серваке которые были включены в эту пару мы потеряли неудовольствию всех сервисов и прочее что называется и пока дежурный инженер не подошел к проблемному коммутатору и не выдернул ему питания связи не было вот как только его отключил слой 2 все отлично как говорится теперь мы можем работать то есть все что связь сразу восстановилась и самое обидное он поставок он reboot нулся он вернулся белый прекрасный и и потом продолжил работать ну в общем мы к сожалению не докрутили эту тему и не поняли в чем был баг но вот так бывает и того как обсудили 100 kirova не имеет ряд неоднозначностей galaga родилась мысль что хочу чтобы было как в стеках но с разнесенным control plain ну здесь человечество тоже уже все давно придумала есть так называемые а многообразные технологии молча щас link aggregation у разных это к сожалению это не стандарт по крайней мере пока у разных вендоров своей реализации но а нечто подобное делают почти все основная идея здесь в том что так же как в стыковых конфигурациях мы имеем два независимых устройства и потребитель сервер физически подключается к разным коммутатором но в отличие от стеков где мы два разных устройств объединяем в одно логическое устройство с единым control plain прям логах мы имеем и физически и логически разные устройства да это усложняет конфигурированию то есть раньше мне надо было конфигурировать одну логическую железку теперь дверь но поверьте то есть сторицей воздается то что на при таком подходе разнесенным контра play надежность повысилась прям ну очень сильно есть коммутатор вышел из строя для нас это не проблема то есть вышел из строя вышел то есть мы прямо сервер продолжает передавать трафик с тем кто остался если вышел из строя link у конкретного сервера в сторону конкретного коммутатора это тоже не проблема есть куда трафик наливать вот все здорово но я бы по грех покривил душой если бы сказал что им лагов ские технологии такие всей себя белые пушистые и лишены каких-либо багов или недостаток естественного всех технологиях есть проблемы в том числе и вы им лавок и когда мы внедряли вы себя в сети благим и страх столкнулись с интересной штукой мы обнаружили что определенные типы multicast ого-го трафика образовывали петлю а для сетевиков петля это одно из самых страшных явлений причем не в книгу не то чтобы любой multicast в петлю уходил в им лаги а вот только определенный но потом когда ты ничего не понимаешь что есть говорите включая white shark из них и пакеты то есть все становится сразу понятно по снв или пакет и поняли что это вот этот вот интересный эффект вызывал multicast порождаемый протоколом с сдп не должно было так работать безусловно как говорится как мы выкрутились а мы просто взяли а если с ди пьета определенная multicast avaya группа мы просто взяли и на интерфейсах коммутаторов акции с листами запретили раз просто они не этого трафика он нам не нужен был в сети там он нам не нужен я даже не знаю откуда у нас черт побери вообще брался этот ssd п то есть но откуда-то брался вот мой вопрос за фильтровали все мы прекрасно то есть мы в дальше внедряем лаги отдалит проблему вендору вентер конечно же начал у нас убеждать о том что нет вы все у нас прекрасно вы просто неправильно читайте гайды неправильно configure ти потратили определенные силы на то чтобы вендору доказать что все-таки это баг они в конце-концов признали выпустили обновление софта всё стало хорошо по результату многолетней эксплуатации пожалуй можно сформулировать четыре главных бонуса повысилась во-первых повысилась надежность сети таких отвратительных и чудовищных аварии которые у нас возникали при использовании стыковых топологий с тех пор слава богу не повторялось надеюсь так и останется навсегда как говорится но никогда не повторится но по факту то есть стала надежней намного проще стало обновление софта чтобы обновить софт сейчас я могу любой коммутатор и сам лак пары отключить по периметру от всей остальной сети обновить на нем софт а потом в управляемых условиях вернуть его обратно убедиться что у меня и млад пора снова собралась вернуть там периметр его подключения все серверы про сервер работает все здорово как говорится трафик передается и дальше там проделываем ту же самую операцию со вторым членом обновление софта просто вообще отлично стала вот безопасно просто и без танца с бубном потому что со стеками там у нас был танец бубном еще тот предмет для отдельного доклады что называется стрельба по-македонски вот при авариях стала действовать намного проще ну согласитесь когда ты знаешь что из-за сети лежит на газ от гигабитный сервис этот стресс то есть ну про поверьте в я под таким стрессом был что называется когда происходит какая-то авария и мы подозреваем конкретно сетевое устройство мы можем совершить одно простое действие мы его просто можем выключить добиться нормализации работы сервисов а потом уже разбираться что с ним было не так ну а это естественно добавила спокойствие в действиях при авариях потому что там есть знаю что просто выключил восстановил и потом как говорится все будет хорошо сейчас топология сети соответствует ну если не лучшим то как минимум хорошим практиком то есть у нас явным образом сформировано три слоя слой доступа слой агрегации дистрибуции слой ядра между всеми слоями мы используем лак технологии то есть нигде нет единого control plain с непонятными переходными процессами между всеми слоями подключение по схеме крест то есть любое сетевое устройство подключается к вышележащим у уровню к разным физическим устройством соответственно потеря любого по любой причине устройство не приведет к тому что трафик прервется полностью то есть ну не возникли черной дыры вот но и самое главное что это все-таки разнесенный контру play и так вредные советы есть ты хочешь чтоб тебя ругала половина компании засунуть побольше трафика в один линк в один роутер в 1 сот ну понятно что так делать не надо мы будем балансировать нагрузку в москве вся бездна вся бизнес логика у нас в москве в москве у нас три дата-центра как я уже упомянул в которых стоят наши бизнес лойко и серверы и было бы здорово чтобы запросы приходили равномерно и примерно в равных пропорциях каждый из дата-центров здесь мы используем в общем-то никакого рокет сайенс естественно то есть используем dns балансировку и каждому сервису просто присваивается тряпичника и с учетом того что потребителей много трафик раскладывается достаточно равномерно нагрузка приходит достаточно равномерно днс dns балансировки могут быть нелинейные эффекты связанные с кэшированием в стях операторов с кэшированием на к и богами на пользовательских роутерах и прочее прочь пользователь роутер это вообще отдельная история ну и прочее прочее но между тем при dns балансировка в принципе одну треть на каждые 100 испечь его это абсолютно спокойно вы хотя говорю здесь никакого рокет сайенс простая dns балансировка а вот для балансировки внутри цодов мы используем один достаточно интересный подход мы используем так называемые расшаренные или как моих внутри себя а внутри себя называем сервисной api адреса а в чем здесь идея у меня есть группа серверов у каждого сервера из этой группы есть свой индивидуальной пир адрес который висит на физическом интерфейс нам нужно чтобы был расширенный айпи адрес у всей этой группы серверов и чтобы они abs принимали запросы и обслуживали что мы делаем мы помимо индивидуального айпи адреса который висит на физическом интерфейсе вешаем вот это вот сервисной печник на все эти серверы серверы из группы она lubeck а дальше мы делаем интересный финт ушами мы со стороны сети так или иначе поднимаем маршрутизацию то бишь роутинг сторону этого сервисного и печника в сторону каждого из индивидуальных айпи адресов серверов мы делаем так чтобы метрика стива я метрика в ядре сети было одинаково а в этом случае возникнет и если это включено в сети возникнет так называемая cm по этого iconsmall типов роутинг то есть сеть начнет делить нагрузку между вот этими направлениями но и тут вы мне должны сказать что все я вру ничего работать не будет потому что возможно неоднозначность прихода пакетов то есть и сеть начала делить ее представьте себе мы пытаемся установить и присоединения то есть один пакет придет на первый сервер а другой пакет придет на треть сервер остановится ли такой и соединяет нет конечно не установится как говорится это было бы правда это правда в том случае когда со стороны и сети используются так называемые терпеть от балансировка а вот если со стороны сети используется так называемая per flow балансировка когда мы при принятии решения о том как распределить нагрузку используем четыре параметра сурс айпи сур спорт destination айпи destination порт то есть flow вот тогда все выкладывается все просто замечательно то есть соответственно здесь уже эти flow раскладываются каким-то равномерным образом между серверами потребителями и здесь типе соединения может установиться и пакеты будут однозначно в рамках одного flow все пакеты будут приходить конкретному серверу вот эта методика дает просто замечательная балансировку то есть она вот ровненько ровненько разделяет нагрузку на ну когда на количество серверов включенных в эту группу плюс эта методика обеспечивает не только балансировку но и отказоустойчивость то есть когда у нас между сервером и сеть будет какой-то динамический роутинг и вы будете смеяться мой между серверами и сетью используем так называемый бюджете протокол барыги свои протокол дома внутри сети используем by джипе вот такие вот вот кто-то кто запретит от 1 вот соответственно когда идет сигнализация от сервера сторону сети что сервер умер в gps таисия сложилось маршрут стал невалидным все давай до свидания то есть на этот сервер нагрузку не направляем то все работает в общем достаточно интересный подход это не отменяет использование каких-то софтверных балансировщик of но чем лично мне нравится подход что софтверный балансировщик это дополнительный компонент который тоже является точкой отказа которому тоже надо каким-то образом обеспечить отказоустойчивость и прочее прочее прочее вот ну понятно что и те и другие методы они вполне работоспособны и имеет право на существование продолжаем про балансировку и так географический балансировать тоже дело хорошее нас страна большая и между москвой и владивостоком большие расстояния если вдруг по какой либо причине мы захотим начать показывать кино пользователю во владивостоке из москвы но это так себе идея потому что на больших расстояниях большие ртт то бишь 6 по-простому говорить большие пинги и высока вероятность потерь а вот большой ртт и потери пакетов это то что максимально эффекте эффективную скорость ищите педалада соответственно что нам надо сделать нам надо принести точку раздачи контента как можно ближе к конечному пользователю ну соответственно когда обсе проект начал интенсивно расти мы у себя внутри дискутировали покупать то что что сидя нужен сомнений не было то есть была дискуссия стоит ли купить услуги у какого-то коммерческого сидена которые тоже на тот момент уже были вполне себе работоспособна были либо строить свой мы выбрали путь строить свой но если вы посмотрите на эту картинку то есть география размещения узлов сидя она примерно соответствует географии покрытия услугами шпд понятно что на чукотке к сожалел при всем уважении к тем кто там живет на чукотке то есть там не так много людей и построить там узел два первых физически сложно по объективным причинам во вторых он будет неэффективен поэтому в общем то все это дело там разложилось по стране по географии там соответствии с п в целом у нас узлы от западных до восточных границ то есть от калининградской области до владивостока для сидена мы используем так называемый окей никас подход чем заключается одни и те же айпи адреса точнее сети мы анонсируем по протоколу bdp в сторону сетей операторов ну гдето протокол для взаимодействия на самом деле между разными сетями в интернете это основа интернета значит в результате сеть оператора видит маршруты до одной и той же сети из разных мест он видит несколько оператор видит много маршрутов а дальше срабатывает метрика внутри сети оператора и он выбирает наилучший а у некоторых операторов это метрика действительно расстояние до узла вот реально то есть один прекрасный оператор сделал метрику как расстояние и вот классно работает вот в результате в среднем по больнице сеть оператора действительно выбирает какой-то наиболее близкий и оптимальный маршрут что еще ну про размазывание нагрузки то есть бывает пятницу добрые бывают злые 1 1 из пятниц начиналось вполне себе ничего но в какой то момент мы поняли что что-то идет не так и это что-то не так выглядела как вдруг неожиданно и многократно выросла вырос трафик для себя мы пришли мы для себя внутри себя решили что этот и доз она потому что трафик вырос почти в два раза и естественно пошли проверять а мы вообще живы или ниже вы ну вскрытие показало что мы живы у нас есть один интересный очень индикатор видео просмотр и во всех наших проблемах сервисах всегда сразу деградирует просмотр вот если хоть что-то случилось то есть там мы косо косо на сервер посмотрели то есть видео просмотру идут вниз как говорится вот соответствие тут мы видим что видео просмотр не деградирует а трафик вырос то есть ну продал значит продолжаем жить сначала даже подумали что это на самом деле никакой не видоса просто что-то пошло не так мониторинг лед в общем чему вы жили да просто потому что нагрузка размазалась по географии то есть в сумме было много но нигде не было слишком много и у нас достаточно хорошее распределение по трафику между москвой и регионами то есть вот у нас в н.н. сейчас в москве обслуживается менее половины трафика москва почему в ней так много да потому что здесь в центральном округе народу но живет и все равно как не крути здесь всегда будет больше чем во владивостоке вот но и есть и уж там на то пошло то это на самом деле был не diedas это было интересное поведение 1 смарта вы платформы когда в смарт его платформе используются постеры постеры описываются в неком джейсен файле коллеги из тех кто управляет этим внесли изменения джейсон файл с описанием постеров выгрузили в админку этой платформы а эта платформа решила что прекрасный способ на все телевизоры по всей россии распространить это и они все кинулись качайте эти 20 30 килобайт ные файлы и создали вот эта вот нагрузку вот ну и про предсказания за долгие годы существования то есть мы имеем определенную статистику жизненный цикл сервиса то есть всегда максимум трафика это новый год в точнее не сам новый год и новогодние праздники потом спад после новогодних праздников трафика всегда меньше по мере того как растет световой день и улучшается погода люди меньше смотрят кино и трафик еще снижается летом уходим в дно осенью точнее в конце где-то последняя неделя августа начинается подъем и следующий новый год это следующий рекорд работает прекрасно и на основании какой-то достаточно длинной статистики можно простыми эмпирическими коэффициентами роста год году прикинуть а сколько будет сколь на какой трафик надо за ложится на следующий новый год а исходя из этого уже под этот трафик помощь не все что надо серверы сети все что угодно все было здорово пока не началась пандемия то есть пандемия нарушила эти законы природы на новый год 1922 понятно что возник новый рекорд но потом вы прекрасно знаете что весь мир из страну в частности отправили на сама изоляцию и самая жесткая самоизоляция у нас было где-то начало апреля вот тут законы природы нарушились то есть народу делать нечего сидят дома стали смотреть кино за что им большое человеческое спасибо как говорится вот но нагрузки добавила прям очень существенно но слава богу что когда мы умоляем инфраструктуру под новогодние праздники мы делаем это с определенным запасом и это позволило нам вполне комфортно пережить ну я близок к финалу то есть и того как сделать операцию на сердце бегущему марафон во первых осторожно а самое главное во вторых желательно так чтобы он это не заметил спасибо за внимание спасибо спасибо огромное ну что давайте вопросы о столько рук сколько рук спасибо большое за то клад мне несколько вопросов 1 почему используйте bgp внутри сети я так понимаю и ббп вот почему не лидер убийцы сказки который такой классный если спалить ну во первых почему не и герб и потому что он цесарки потому что это проприетарный протокол если уж говорить про эти протокол внутри сети то дисплеев который является стандартом и который будет работать на любых вендоров джипе внутри сети мы используем для одной специфичный задач вот навешивание вот этих вот расшаренных обидчиков да еще там понятно компоненты сети между собой там есть на нее смотреть они взаим коммутаторы ядра тоже победит и какую-то топологии имеют но вот это вот интересная фишка которую там один наш коллега придумал с навешиванием вот так вот серого тихо печников возникла мысль а почему не погиб он удобный то есть мы на сервере поднимаем а какой-то роутинг демон в частности bird со стороны сети настраиваем в депрессию все мы прекрасно она работает почему найти ответ простой а почему бы и нет но в целом гру что для в качестве протокола внутреннему египте протокола внутри сети это успев не герб и открыл потому что он siskin а у нас сейчас сеть не нацистов не все а cisco я просто поняла что все нации склонение был бы ты у нас чего настолько не было то есть во что мы только не вляпываюсь как говорится вот в том числе нацист как было много раз cisco протоколом классе но он проприетарный проприетарные протоколы должны умереть должны работать только общечеловеческие а еще один вопрос вот эта история с мастер slave когда они между собой не разобрались is life не стал новым мастером на каком протоколе или в каких технологиях она произошла еще раз на чем произошло это проприетарное цистин протокол с тыквой с насколько я помню он называется это достаточно старые железки были у нас но cisco 370 x это было стародавние времена когда мы были маленькие деревья большие то есть у циски диске есть протокол с тыквой вот вот он был он прекрасно заключу понятно еще последнее по поводу вот этой стоимости которая для всех становится равной если у вас одинаковое подключение которое дублируются все крестом то почему она и так не становится равным ведь вычисляется наименьший маршрут всего скорее до сервера и нужно любом роутере будет одинаковое расстояние до каждого сервера стоять вот это вот допустим смотрите на самом деле то сейчас небольшую тайну открою что l3 айпи протокол он начиная она он только в ядре у нас все что ниже это l2 собрана на им логах поэтому с точки зрения айпи это 11 брак с домен один вела и ничего боле а есть более красивые методы то есть вот это вот когда есть ядро под ним только l2 это нормально это работает но лично мне больше нравится тупо взрыв когда l3 начинается в стойке вот конкретный но по ряду причин то есть нам сложно очень сейчас перейти на такую топологию там надо перестроить всю сеть то есть как говорится не только сеть задействовать работой очень много людей как говорится но у которого на самом деле есть чем более полезным заняться вот как-то так понятно спасибо большое зима ну здравствуйте есть вопрос участие dns балансировки и бордеров а именно по части отказоустойчивости объединены ли бартера каким-то про таковым отказоустойчивость там пленарных братков что происходит во время выхода из строя одного или двух из них и как с этим поступать давайте почтим borders ни в какую кластерную конфигурацию не собираюсь каждый border живет в своей а резки вере так borders это одна из к наша 57 629 между бордюрами мы поднимаем би джи пи это в данном случае получаются а и bdp они друг с другом прекрасно взаимодействуют обмениваются маршрутом пересчитывают маршруты и так далее зачем что-то еще придумывать то есть но вышел у меня из строя border то есть мы перестану пересчитается таблица маршрутизации на остальных бордюрах чем проще тем лучше но не всегда еще вопросы добрый день спасибо большое за доклад андрей зубков вот прям по центру далеко-далеко правее offer прямо напротив прожектора до прошу прощения как греция сейчас и мальчик в прошлом сисадмин поэтому вы меня прям погрузили в мое любимое спасибо большое редко такие доклады встречаются один вопрос как в интервью одна книжка которую вы посоветовали почитать про сети чтоб 100 меня любимая книжка олефир олефир компьютерная сеть супер спасибо большое там много изданий было и сейчас наверняка они гораздо больше это то есть но лично у меня италию это любимая книжка спасибо за клад я человека сетей совсем далекий разработчик может быть вопрос мой будет глупый а вот где была просидел я правильно понимаю что у вас анонсируются одни и те же сети в разных регионах то есть все так в москве серверы владивостоке могут иметь 1-ой печник дар а насколько это типовое решение если если да как этому операторы относятся это на самом деле от типовое решение это полуха квай 1 4 который опять же сделали по принципу никто не стал а никто не сказал что так нельзя войти в 6-ой пейни koston заложен уже на уровне протокола ну где тут ipv6 как говорится никто чуть не хочет его пользовать вот google dns это и не каст то знаете у меня коллега она интервью когда сетевые инженеры приходят любят задавать вопрос где находится сервер четыре восьмерки вот и дальше он ехидно на них всегда смотрит как говорится и ждет отвергнут правильный ответ то есть он находится в москве но потому что это и не каст и отвечать будет сервер какой-то максимально близки но я надеюсь что в москве они еще остались то есть вот максимально близкий айпи не каст используют достаточно часто справедливости ради я скажу что все таки в сединах чаще используются либо dns балансировка либо какая-то l7 балансировка и этот подход полностью имеет право на существование и более того то есть успешные коммерческие сидена с этим работы но dns балансировки есть один интересный эффект под представьте себе что у вас вышел из строя узел и за сколько времени протухнет dns кэш у этого узла а вот когда айпи и microsd узел вышел из строя это со скоростью сходимости где пиво всем интернете ну то есть по оценкам там где то не больше трех минут что называется это займет ну а надо на практике то есть чтобы там где основные потребители это безусловно гораздо быстрее поэтому альпини к с одной стороны такой странноватый подход он в нем прелесть он простой он простой до безобразия то есть там не надо строить сложную бизнес-логику мне больше например для седанов нравится l7 балансировка когда есть умный l7 балансер который учитывает кучу разных факторов он замеряет там связь до этого узла связь до того узла и там до всех 50 узлов замеряет связь и принимает решение во вот этот вот конкретного вот этого пользователя классно обслужить там из узла в челябинске и давай туда его как говорится но это гораздо эффективнее чем и никас балансировка но что здесь нужно во-первых здесь нужно написать этот слов то есть вот задача непростая то есть я далёк от софт описание как говорится как вы понимаете но спинным мозгом чую что задача непростая во вторых чтобы это реализовать на каждом узле у меня должна быть одна и 1 4 сетка еще пардон 1 и пиво 4 сетка то есть я должен сделать уникальный айпишник на конкретном узле я должен этот уникальная печник анонсировать в интернет как я могу его нанси ровать в интернет по протоколу bdp в интернете провайдеры режут префиксы все префиксы длиннее слэш 24 а некоторые нищеброды кто себе не купил нормальный роутер начинают резать уже и более короткие префиксы значит соответственно мне на каждый вот этот узел надо по своей s2 4 сети и пиво 4 пространство закончилась его нету но там сколько там у нас узлов 29 30 лет и чтобы реализовать на и пиво 4 эту l7 балансировку мне надо сначала придумать где мне надыбать 30 слэш 2 4 сетей то есть скорее всего это теперь будет стоить каких-то абсолютно не разумных денег то что их можно купить сети на сайте вы 4 все плохо казалось бы выбор просто что вы тут мозг паритет устройте на ipv6 вот тут мы в другой упираемся как говорится но не хотят оператор по и пиво 6 работать то есть вот хоть ты тресни то есть как говорится то есть не все понимает что ipv6 когда-то надо будет интенсивно работать но почему-то туда никто не торопится вот понял спасибо спасибо еще есть есть вопрос можно по центру по центру напрасно центр то арестов константин скалы большое спасибо за доклад очень интересная вот в разработку пришел системного администрирования обожают такое у меня будет не по технику у меня про человеческие отношения вот когда была потеряна 3 ордера в течение там я так подозреваю 15 секунд а можно ли рассказать как проходил разбор полетов у спорте мы концепции непогрешимости и так далее вот эту всю как человек сознался такой в него был выбор ну там очевидно последовательность была но ваш вопрос на самом деле правильный каждый человек имеет право на шип я могу пояснить вот я искренне считаю что если у нас методологии процесс построен таким образом что мы рассчитываем что исполнитель не ошибется это плохой процесс который нужно менять любой ценой вот исполнитель разлом обоими руками важно не делать возможность накосячит он накосячит всегда вот это процесс плохой они исполнитель я я с вами абсолютно согласен где мои скрижали это бонус а где скрижали ну короче где-то будет скрижали то есть о том как как постараться сконфигурировать так чтобы не было больно да вы правы да да все так не нравится как наша беседа перешла в такой типично пятничную вечернюю да у меня тоже есть один вопрос то ответ на который я точно не хочу получать здесь это нужно где-нибудь сесть в отдельно в заведении я бы хотел подетальней порасспрашивать про то что тогда произошло с фейсбуком а вопрос который меня больше интересует когда будет тот момент когда случайно залетевший дятел таки положит весь интернет рано или поздно это должно произойти вот"
}