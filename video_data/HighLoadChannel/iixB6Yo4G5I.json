{
  "video_id": "iixB6Yo4G5I",
  "channel": "HighLoadChannel",
  "title": "Архитектура: история и будущее на примере ВКонтакте / Александр Тоболь (ВКонтакте)",
  "views": 22980,
  "duration": 3927,
  "published": "2023-04-28T06:21:29-07:00",
  "text": "Меня зовут Александр тоболь Я технический директор различных сервисов и вот буквально пару минут назад Лёша мне рассказал очень интересную историю говорит Слушай если ты не готов тебе нужно выучить буквально пару предложений начать с них Потом расслабишься и всё будет нормально только одна проблема Я не выучил поэтому начнём на самом деле с опроса и с довольно интересного опроса А есть ли в зале архитекторы люди которые занимаются архитектурой именно программного обеспечения большую часть времени Поднимите руки О ничего себе слушайте В моём докладе прозвучат некоторые возможно спорные утверждения возможно утверждения которые могут повредить вашу психику поэтому у вас есть ещё буквально пара-тройка минут для того чтобы уйти с этого доклада вот а я расскажу что нас сегодня ждёт в докладе Итак В начале мы поговорим вообще про то что такое высоко посмотрим как нас Интернет инфраструктура it привели к этому определению и посмотрим ретроспективу ВКонтакте прямо с 2006 года до наших дней как менялась архитектура И куда мы сейчас пришли у нас менялся не только логотип Да но у нас под этим логотипом ещё и менялось много чего внутри Итак давайте начнём с самого интересного с определения вообще что такое я вам буквальном 10 секун подумать что его придумал Олег Бунин и действительно этого определения никогда не существовало но Раз уж мы все на конференции Давайте буквально возьмём для себя 3 51 секунд для того чтобы настроиться на доклад и подумать А что для вас Значит вот для того чтобы вас всех не опрашивать я на самом деле прол коридор исследование в котором разе дили собрал некоторое среднее по больнице что highload - это когда большая нагрузка там у кого-то это 1.000 купс у кого-то 100.000 средне порядка 10.000 КПС это какая-то гарантия на ответ что там 90 перцентиль укладывается в какое-то время это очень быстро это очень высокое вола бити и в среднем люди называли Вы не поверите там четыре девятки пять девяток вот прям суперсервис который там не знаю 52 минуты в году лежит вот и это очень высокая утилизация ресурсов у вас постоянно загруженное оборудование звучали ещё слова это scalable distributed F tolerance сист Вот это всё звучали даже такие слова что High - это когда не вывозишь нагрузку вот если нагрузку шь это High Ну на самом деле очень легко написать такой сервис в котором вы не выводите нагрузку у вас всего 100 клиентов А вы её не выводите вот поэтому для того чтобы нам сегодня понять определение не существует любое определение которое вы дали для себя за это время в зале оно верное Но сегодня мы будем говорить что - это то когда стандартные средства индустрии на текущее время обработки хранения данных обработки запросов эксплуатации уже не работают И вот именно ради этого мы с вами приходим На конференцию для того чтобы найти ответы на свои вопросы когда стандартное решение вам не подходит хорошо а оп оп хорошо немножко о ВКонтакте у нас сейчас в секунду порядка 30 ой 3 млн запросов в Пике в секунду наш latency ответа AP меньше 120 миллисекунд лити У нас вот 9994 честно Посчитай с точки зрения определения High коридорного исследования мы тут вот по этому параметру ещё не дотягивает наша аудитория которой больше 100 млн они у нас постят в день 100 млн контента и смотрят 9 млрд и обеспечивают вот эту интересную нагрузку немножко посмотрим про структуру в 2006 году у нас было буквально 25.000 аудитории структура была Вот такая А в двадцать втором году у нас структура стала не сильно сложнее Вот и для того чтобы понять как мы эволюционировали оттуда сюда и что у нас поменялось Вам нужно будет вытерпеть весь доклад немножко про меня я порядка 20 лет занимаюсь коммерческой разработкой технический директор различных нагруженных сервисов первый раз выступал на лоде в 2013 вот смотрите такой ботаник Вот и тут пришли ко мне организаторы сказали нужно расказать ретроспективу я вот хорошо рассказываю технически сложные доклады вот Алексей не даст соврать технически сложные про меня пришли говорит нужно рассказать ретроспективу вот этого всего где я и где эта ретроспектива тут стало чуть интересно где-то в ране 200е красивую инфраструктуру ещё 2003 год Шёл на плюсах орма Да если кто-то представляет что это такой об Манга Когда у нас есть база данных там mysql ещё что-то это всё красиво конструкторами деструктора в плюсы превращалась я потратил всё время на разработку этой классной системы и продол бал всю логику следующий проект я сдал со спагетти кодом порядка 3 мб я в него смотреть просто не мог но я его сдал потому что там такая ситуация уже сложилось вот я никогда не работал архитектором поэтому довольно сложно мне верить сегодня про рассказ архитектуры и никогда не работал в командах где были архитекторы но с с двенадцатого года я занимался развитие развитием сервиса видео у нас получилась неплохая архитектура этот сервис видео сейчас саппортит все проекты VK это ВКонтакте Одноклассники какие-то проекты mail Group Вот и сейчас я с командой меняю структуру ВКонтакте самый главный вывод который я сделал за 20 лет что архитекторы не нужны Дело в том что в современном хайде тут конечно идёт определённое но но в современном хайлоу сложность систем объём систем и самое главное вариативность решений То есть каждый раз когда вы хотите что-то решить у вас очень много разных вариантов все которые могут предложить разные люди это невозможно уместить в голове одного человека Вот поэтому если кто-то принимает решение в одиночку перестаньте Это делать и начните работать с командой А конечно же перед докладом Я никогда не рассказывал про архитектуру я пошёл и Погугли Что такое архитектура это вот информация из Википедия я на самом деле от Института программной инженерии узнал что есть порядка 150 определений архитектуры я их все посмотрел Вы знаете для меня все одинаковые были абсолютно я мог сюда поместить абсолютно любой ничего бы не поменялось Вот поэтому я даже читать не буду кто-то Может быть увидел кто-то в Википедии посмотрит Вот Но самое главное что я понял что если есть определение у кото 150 вариаций всего остального то это какое-то социальное явление которое возможно только при создании группы лиц и вот это наверное важная история которую нужно учитывать очень важный вопрос архитектура и структура Если кто-то знает это фасад университета ти в котором Я учился и у нас был Преподаватель не последний человек в университете доцент заместитель заведующего кафедры который на каждой дипломной работе Если у вас было написано слово архитектура кот во чем архитектура отличается от структуры это действительно было очень интересно и вы знаете я знал что будет это вопрос можно было везде написать структуру и не попасть но я бы был не я если написал архитектура мы потратили где-то час и после этого я выяснил чем они отличаются давайте для того чтобы понять куда мы пришли начнём с определения структуры представьте себе У вас есть система к ней есть некое функциональные требования которые говорят мто нужно обеспечить части пользователя У вас есть различные сле есть Time to Market вы на конкурентном рынке Вам всегда нужно очень быстро развиваться Time to Market - это очень важно и у вас есть ограничения это с одной стороны бизнес это деньги Сколько у вас есть оборудования с другой стороны есть пределы вычислительной мощности процессора памяти у нас были раньше HDD SSD Вот это всё меняется он становится лучше но есть определённые ограничение по иоп сам того что можно ваша структура - это такая штука в которой вы обычно всегда помните что она должна быть надёжной масштабируемой И самое главное удобно в сопровождении и некоторый такой термин устойчивый Вот мы сейчас погрузимся назад буквально там на 30 лет для того чтобы понять откуда вообще все эти требования появились и почему мы всегда на лоде на вот этой вот конференции которая придумала этот термин всегда говорим именно про них Давайте немножко начнём с гипотезы об устойчивости дизайна или нашей структуры Это не я это Мартин фаулер придумал если мы вообще никак не поддерживаем нашу структуру и говорим Итак сойдёт и у нас есть какое-то количество разработчиков которые разрабатывают продукт В начале мы делом очень быстро у нас появляется продукт Потом мы начинаем тормозить потому что у нас очень много различных капас технического долга и там прочие истории очень круто если мы сделали правильную инженерию то мы со временем начинаем деть быстрее и вот есть очень плохая история про которую я в самом начале рассказывал прон когда мы на самом деле начали слишком много вкладываться в архитектуру и не смогли дойти до некоторой точки Когда у нас получился качественный продукт mvp с которым можно работать вот и Это история про ове Инжиниринг Почему у нас в действительности растёт вообще скорость разработки потому что в начале вы разрабатываете платформу у вас появляются база данных очереди какая-то скали емо ещё что-то когда вам нужно быстро запустить продукт вы потом в перспективе начинаете его делать очень быстро это наше идеальное решение если у вас набор а теперь давайте дадим Исходя из этого всего 151 определение архитектуры и Первое что я хотел сказать что это некоторый набор правил и процессов выработанных группой лиц по которым строится структура тут Наверное самое главное отличие архитектуры и структуры все те схемы диаграммы всё что вы рисуете стрелочки - это всё структура архитектура это наше некоторое внутренний набор правил по которым мы эту структуру строим очень ча явление мы предполагаем что для нас важно Это то круто если вы смогли его формализовать вот но при этом я понимаю что ВС что я дальше буду говорить докладе должно называться структурой А не архитектурой вот но я буду называть по привычке это архитектурой на самом деле это неправда Итого архитектура это на самом деле в первую очередь социальное явление Если вы это не документирование не документировании лучше фить и такт хите шени по которым вы меняете структуру это не структура Это не картинки Это не кубики но мы будем называть кубики архитектурой Ну просто все так привыкли и соответственно наши Главные требования - это Надёжность масштабируемости Time to Market и мы поговорим откуда они пришли вот самое главное понимать что идеальной структуры архитектуры не существует и всегда это компромисс компромисс между требованиями системе физическими ограничениями скорости разработки главное выбрать правильный компромисс тогда у вас будет успешный продукт и Давайте теперь посмотрим про как я и говорил про эро развития it индустрии и того как мы вообще здесь все с вами В этом зале оказались Давайте поднимем руку Кто родился в приб эру с восемьдесят пго по девяносто четвёртый год о отлично очень много народу А кто с девяносто пятого по двухтысячный таких уже меньше а с двухтысячного отлично совсем чуть-чуть хорошо и я расскажу как у нас двигалось развитие it индустрии вот вот с восемьдесят пятого с моего года рождения и до наших дней раньше я просто не мог смотреть на неё поэтому не знаю а на самом деле если что я это украл из книжки про развитие облачных вычислений у ли манче Вот и так совпало что это восемьдесят пятый год Ну давайте начнём значит с восемьдесят пятого по девяносто четвёртый компьютеры были только на работе это были внутренние заказчики они выглядели примерно так я программировал на БК 001 вот интернет был текстовый были мейнфрейм данта был предсказуемый Хорошо если раз в неделю обычно каждый день в 6 чав ушёл с работы тайм все работы провели всё отлично Всё работает главное количество сотрудников знаете сегодня 1.000 завтра по найму по чару смотришь завтра там 1.50 как бы ну нагрузки понятны в принципе а потом началась следующая ой-ой-ой Что случилось Ничего страшного того что мы забежали вперёд а началась следующая эра эра пузыря когда все эти интернет-компании вдруг стали 24 на 7 появились Internet Only компании которые должны работать появились там DSL модемы у всех компьютеры музыка видео вместо DVD стали распространяться по сети появились большие надёжные системы вы могли купить Прай серве который примерно так выглядел рост стал более непредсказуемый все стали туда вливать деньги надо брать больше серверов больше инвестировать в это всё в какой-то момент а произошёл взрыв А некоторый кризис доткомов вот в результате которого все компании развалились все поняли что будущее не за оборудованием что они все не окупаются Вот и наверное из Приятного Э высвободить большое количество свободного оборудования и многие ы mail.ru Поехали в Америку для того чтобы закупать освобождённые оборудование из этого вырос Мне кажется mail.ru Яндекс и многие наши российские сервисы именно из свободного оборудования взорвавшегося ы доко в целом прогнозирование было какое-то примерное сервера были вот такие потом началась Эра эффективного развития у нас появилась самае интересное коммодити железа мы начали понимать вот тут тут много чего написано потом прочитаете самое интересное важное с 2000 по 2003 год случился переворот поняли что Коди лье количество лучше чем какой-то mainframe то есть появились исследования в различных институтах mit бек которые говорят что есть алгоритмы решения что вам лучше много ломающейся они победили на самом деле какие-то Enterprise решения Ну ещё Java в том числе Вот и у нас произошёл бум а различных сервисов случился Web 2.0 получилась хорошая индустрия продуктов появился дуп всякие рекомендательные системы большие данные и наш компьютер который раньше был мейнфрейм который был каким-то сервером У Вас стоящим в офисе в дата-центре стал целым дата центром То есть дата-центр - это компьютер у вас скалится цпу память диски на уровне дата-центра Дант у ва выросла эффективность утилизации электроэнергии оборудования всего остального и случилось тот самый 20 который очень круто выстрелил при этом вместе с тем что у всех пользователей появились персональные компьютеры и потом случилась Эра КД компью года она считается Когда у нас появились требования к к высокой доступности Когда у нас инфраструктуры стали Деми и в целом у нас появи мир M рекомендаций ещ чего-то которому доступны все эти вычислительные ресурсы То есть у нас получился по сути безграничная поляна вычислительных ресурсов которые мы можем утилизировать Мы тоже стали утилизировать Давайте поднимем руку кто понимает почему будущее туманно Если вы учитываете законы мура и амдала о на самом деле я сейчас скажу буквально каждый десяты хорошо то есть буквально 10% знает что я расскажу закон Мура изначально выглядел как что каждые 2 года у нас увеличивается частота цпу ных ресурсов потом он перестал увеличиваться у нас 3 С5 4 ГГц стало пределом сказали слушайте количество транзисторов в камне увеличивается каждые 2 года в два раза потом Intel сделал правку что каждые 2 с половиной года увеличивается в два раза и к двадцать Пятому году это закончится Почему Потому что количество нанометров ширина дорожки по которой бегают электроны меньше чем сколько-то быть не может А ещё есть скорость света супер большие процессоры на кристалле быть не могут поэтому количество транзисторов тоже станет флэт А теперь закон нам дала он про параллельные вычисления он говорит что вы можете распараллелить всё что угодно но сложность ваших вычислений равна вашему самому короткому пути на вот этих параллельных вычислениях То есть вы всё рас параллели У вас есть самый короткий путь и быстрее него вы не можете неважно хоть у вас 65.000 процессоров вы всё равно будете работать за это время вот и следуя это мы можем сказать что мы на самом деле находимся в тупике буквально к двадцать пятому году по оценкам Intel Мы окажемся в ситуации когда у нас больше алгоритмически наши текущие алгоритмы не вывозят и железо нам больше не может дать ресурсов то есть мы упёрлись в в закон амдала по параллелизм и больше у нас по закону мура цпу не растёт и мы где-то находимся в этом месте и у нас на самом деле настанет Эра интересных алгоритмов параллельного вычисления наверное можно было сказать что всё туманно плохо Всё тлен в д ПМ всё сломается Но на самом деле есть квантовый компьютер пока всё что он умеет делать открывать классно криптографию различных мессенджеров https тем более Вот Мы надеемся что он сможет Нечто больше но пока мы понимаем что в д пятом году мы упр вычислительной мощности Нам нужно будет развивать алгоритмы параллельных вычислений следующее что хочется сказать развитие Веба Вы наверное слышали про Web 1 23 Давайте Я немножко расскажу как эти эры мача на it индустрию и желе и всего остального в начале был 1 ВС что мы делали мы просто читали все Поли интернет Мы просто его читали мы не не являлись создателем контента И когда у нас случилось именно Boom создани Интернет сайтов и всего остального случилась Эра we20 появились социальные сети различные ресурсы куда мы стали очень много поть Польши Да вот С3 случилась проблемка изначально Все думали что 3 это будет какая-то семантическая система которая рассказывает о связях объектов на которых ML сможет делать классный поиск и делать различные выводы Но на самом деле все перестали предоставлять свой контент в сеть все закрылись в районе 2.0 там различные ресурсы стали хранить контент только внутри вот и30 стал туманным и все обсуждать разли инго интеллекта пи истории умные помощники и так далее вот здесь яду такое ограничение его все путают Но вообще 3 - это семантик Web это вот который не случился когда все делятся данными со всеми и есть умные системы которые могут там всё искать а web3 в целом про распределённые вычисления и про них Мы ещё поговорим Итого что бы хотелось сказать что наши требования к надж росли если раньше там 10 лет назад для на профилактика сайта была норма сейчас когда у меня ночью Мой Банк с 0 до 2 ночи проводит профилактические работы меня прямо трясёт потому что я кому-то должен перевести деньги а сервис не работает раньше считалось что быстро поднятый не является упавшим а сейчас по вопросам в коридоре если у вас больше чем 52 минуты в год сервис лежит это уже катастрофа потеря данных помните мы копировали на флешке ещё куда-то можно найти тут пропало сейчас облако - это решение не дай Бог облако потеряет вы им перестанете пользоваться навсегда ни разу не теряла про масштабируемость различные сервисы набирают популярность все люди приходят эээ всё должно быть готово к масштабированию иначе вы просто не соберёте аудиторию Time to Market - Это история про конкуренцию вам нужно очень быстро всегда деплоить особенно если вы конкурирует с фейсбуком Инстаграмом мессенджерами и другими сервисами Вот и самое главное что Open Source В итоге победил мы все работаем на Open Source стеки Это означает что внутри одной корпорации замкнуться создать уникальное решение у вас не получится либо в это решение инфраструктурное орте либо оно погибает И вам нужно его заменить на какое-то Open Source решение про требования к архитектуре Мы же всё-таки про архитектуру Вот это всё есть некоторая книжка по построению какой-то модифицируемые развивающейся архитектуры в которой говорится что очень много различных Есть ли это там побили mity ещ что-нибудь их там эта Матрица гораздо больше может быть Рассчитано самое главное вам выбрать важное в этой архитектуре для того чтобы нам выбрать важное мы на самом деле посмотрели буквально на опрос наших пользователей увидели что 2 года назад у нас ВКонтакте пролежал 10 минут частично пролежал 10 минут но в субботу был фон не было никаких странных новостей мы получили порядка больше 100 новостей в СМИ куча жалоб пользователей и поняли что для нас Надёжность и масштабируемость - это супер важная история что с одной стороны мы должны быть надёжными никогда не ложится с другой стороны если к нам пришло много польз мы должны всю эту нагрузку так или иначе выдержать если посмотреть на количество кода Вот это количество кода в репозитория Google оно количество людей которые комит туда внутри Гугла У нас примерно выглядит так же у нас всё больше и больше коммитов и изменений в коде я могу сказать что мы за последние полгода Я посмотрел статистику У нас код bas вырос на на 16% то есть мы почти на 30% растём в год вот понятно что всем разработчикам главное удобство разработки то есть представляете за 30% в год То есть за 3 года мы там обгоняем весь ВКонтакте за предыдущие 3 года просто вот и это нам нужно для того чтобы знаете стоять на месте нам нужно очень быстро бежать А и Следующая история которая вот написана буквально пользователями они всё время жалуются говорят почините этот бак сделайте эту функциональность мы понимаем что качество продукта Time to Market и вообще скорость ответа работы всего этого очень важно для наших пользователей нам нужно это развивать и наверное ещё важная История это какие-то штуки которые доступны только сервисам у которых очень много ных ресурсов это удивлять наших пользователей это рекомендовать супер интересный Контент это создавать уникальные технологии типа там ещё чего-то основаны на основаны на огромном количестве ных ресурсов которые у нас уже есть а у нас на минуточку 20.000 серверов и ночью они все простаивают они могут или рекомендовать вам что-то или считать биткоины В общем что-то они делают самое важное для нас Самое важно Потому что когда мы лежим все утекают конкурентам и мы знаем что раньше данта это было нормально Сейчас 52 минуты в год какой-то частичной недоступность это катастрофа мы должны быть готовы к масштабируемости мы должны быть очень удобно для разработчиков быстро разрабатываться пользователи хотят от нас супер быстрых ответов и работы с большими данными и каких-то интересных решений на этом мы посмотрели всю историю развития инфраструктуры Веба и перейдём именно к тому как на это ложится инфраструктура ВКонтакте и тому представьте себе 2006 год это было довольно давно Я где-то учился в институте началась Эра 2.0 аудитория ВКонтакте было 50.000 пользователей и в это время за месяц до релиза консоль старта ВКонтакте выглядела примерно так вот здесь вот apch какой-то de Это буквально за месяц до старте ВКонтакте внутренняя архитектура была чистый МК Это был Debian Linux Это был AP mysql и PHP классическая ко получал некоторый набор IP адресов За каждым IP адресом находились порядка восьми nginx серверов которые были фронтами которые балансировать нагрузку между ещё сотнями PHP серверов сервисов Это стандартный наверное Lo баланси который у вас у всех сейчас есть но вы помните Это был 2008 год а случился 2009 год а мы купили домен mk.com аудитория была уже порядка 12 млн пользователей Да что ж такое-то Вот И в этот момент mysql перестал справляться он начал периодически падать коптить данные и мы поняли что Нам требуются некоторые решения тогда ВКонтакте их называл igns это некоторое решение которое объединяло данные и некоторую бизнес логику с ним если бы мы сейчас были в современном мире мы бы это назвали микросервисами У нас сейчас порядка 800 кластеров различных они реализуют какую-то логику сообщений фо рекомендации реклам тарго и так далее чем они отличаются они внутри хранят логику которая обрабатывает данные они хранят данные Обычно они написаны на c+ и на самом деле если говорить про текущий мир это микросервис с Database вот Database п микросервис - это движки ВКонтакте которые были придуманы в 2009 году Почему вообще Date так популярны наверное все знают lat стоит обратиться в кэш или один в память сколько дорого стоит по сети ЕС здесь смотреть цифры Есть разница буквально в тысячи порядков то есть сходить в кэш в диск супер быстрее чем обратиться в сеть Поэтому если у вас всё Там классно шардирование а если посмотреть на структуру то по именно хранению данных то изначально Это был binlog Right headlock как вам нравится все данные которые изменялись они всегда пишутся в binlog мы их Пим и это очень здорово мы всегда можем остановиться у нас был некоторый compaction который compaction который строил идеальный snapshot индекс который вам позволял это value это новый SQL это KV смотрите 2009 год ещё новый SQL даже нету это новы который строил вам снапшоты которые хранились или на диске в памяти если они вза или на диски потом у вас был некоторый ру индекс в памяти который вам позволял быстро реть и дальше у вас Да что ж такое-то что у ме сегодня с кликером Извините и вот здесь была кастомная логика которая обрабатывала все эти данные и был набор каких-то инструментов которые обеспечивали работу с сетью с репликации и так далее То есть по сути реально это микросервис с кастом логикой по обработке данных и база данных в чистом виде которая Кстати на самом деле Несмотря то что она в 2009 году была придумана она конкурирует с B3 с деревом если Давайте Вот кто знает что тако B3 илис дерево в базах данных о здесь кстати очень много почти 50% Молодцы вот в 200 году была придумана структура которая по лсм дереву это частичная часть лсм дерева но у неё есть свои плюсы и минусы и прямо это целый доклад но тем временем шёл 2011 год у ВКонтакте появились мобильные приложения сначала iOS потом Android аудитория уже была больше 30 млн и по поводу хайпа вокруг микросервисов 2011 год вот здесь вот на шкале ещё микросервисов вообще не существует А они у нас уже вовсю работают а понимаете как бы вот определения всё ещё нет и самая главная проблема которая у нас появляется представьте себе Вот у нас сейчас порядка 10.000 кэндо это вот энд серверов хардвар на которых работает ВКонтакте внутри на каждом сервере до 256 PHP воркеров там неважно чего это запущено и все они хотят прийти за данными ваш микросервис если аккуратненько это пересчитать то у вас 10.000 серверов по 256 коннекто если у вас даже маленький S tcp буфер дефолтный по 16 КБ то на вашем сер которые вы подняли Пусть он делает базу данных пользователей групп алике и так далее только 82 ГБ сот памяти это 2011 год 82 ГБ соно памяти есть далеко не в каждом сервере и внутри всей этой структуры у нас походу в PHP в backend порядка 250 млн запросов в секунду очевидно что нам нужен свой rpc и классным решением чтобы не иметь вообще сотны буферы по tcp не связываться с этим всем это использование когда у вас база данных имеет udp пор она получает request resp всю историю с кон контролом и так далее вы разрабатываете на уровне udp используете минимум памяти самое классное Что вы читаете из udp сокета одно поточно в базу данных запрос и Прочитав этот запрос вы используете Ту же самую память вы не перекладывается буферы Вот это на самом деле довольно Интересная история и Кроме того чтобы создать свой rpc протокол поверх udp Мы ещё и поняли что нам нужно пофиксить стерилизацию Все наверное знают стилизацию xml J который можно читать бинарный варианты гпа которые более компактные вот здесь видна картинка как стандартный пользователь с каким-то именем фамилии представлен в различных вариантах представления Ну и схем ная лизация Когда вы заранее знаете схему Она самое компактно это Например проба Но это был 2011 год мы придумали тогда Type langage он был сравним он даже был тогда сильно лучше профа мы его тели ня на проба в своё время но сделали фонс тесты сравнили со всеми конкурентами и поняли что если мы говорим про передачу польских данных то наверное это сравнимо с пробам А если мы говорим про передачу массивов А у нас в рекомендациях в контакте везде используются массивы мы по-прежнему быстрее всех имеющихся решений решение нативной лизации в Go проба msg пака и Джейсона у нас и меньше данных используется и мы быстрее стерилизуем и поэтому T ty назвали е Потому что он самый быстрый Он до сих пор самый быстрый Это буквально тест месячной давности мы его продолжаем использовать внутри инфраструктуры настолько классно Нам повезло что наш rpc на udp с Лем был использован для обращения к нашим движка к микросервиса к базам данных и это всё 2011 год когда мы вообще Про Сервис ш про какую-то историю не говорили 2013 год кру зарегистрировался ВКонтакте аудитория уже была в год и мы поняли что нам не хватает производительности слишком много логики и ВКонтакте стал в два раза быстрее благодаря чему благодаря тому что не самый быстрый PHP мы транслировали в c+ Plus и получили дополнительный прирост производительности Следующая история это важная веха это 2015 год уже на самом деле пятый год эры мьюн когда мы давно не работаем Нам железе мейнфрейм ещё чём-то у нас компьютер - это по сути дата-центр со скали емы цпу рамам дисками и так далее и мы поняли что наши пользователи перестают успевать смотреть ленту наши пользователи меньше 20% ленты смотрят она отсортировано по времени Это не очень интересно и у нас появились распределённые вычисления У нас появился Cloud компью и у нас появилась умная Лента умная Лента она таргетировать на счастье пользователя тогда это было просто время просмотра мы смотрели насколько вам интересно с в профиль переходить лайкать фить насколько вы недовольны ВС это вырасти нашу активность у нас выросло просмотру постов на 20% лайки на 40 Но самое интересное что у нас ещё и количество данных в дупе буквально петабайт в день для того чтобы всё это обеспечивать в целом мы стали покупать очень много оборудования но зато пользователи наши наверно стали счастливы Ну нам так Да что ж такое-то так они стали типа счастл 2017 го компанию Sun и распустил 2000 с по сотрудников компания Sun перестала существовать А в это время ВКонтакте Уже отдавал несколько терабит трафика терабит на самом деле 2 с половино там где-то рабита трафика но в 201 году уже некоторые операторы немножко крякали что вы у нас занимаете все каналы и мы поняли что нам нужно обеспечить cdn снизить нагрузку на трафик пользователи у нас уж хранили больше 100 питат картинок фоток музыка видео и так далее многи контент был популярный аномально популярный Контент это интересные мемы которые залетали какие-то треки и самые непопулярные треки - это вот видео музыкантов напротив зингера Если кто-то знает да вот самая популярная точка В Питере где музыканты играют вот эти видео были не очень популярны мы поняли что если самый популярный контент подвинуть поближе к пользователям то это решит проблему нагрузки сетей и пользователи станут счастливее вот эта карта седен узлов на самом деле здесь ещё не показана американ американский Континент там тоже есть некоторые узлы но в целом Мы в большом количестве узлов установлены у нас больше п узлов по всему миру многие в России СНГ но многие и за рубежом мы поняли что на контент придвинуть Как можно бли ближе к пользователю стандартные решения которые вам сейчас доступны например НС Как это работает у вас есть Main это родина Где у вас дата центр Где у вас хранятся все данные оно может быть распределён ещё как-то у вас есть различные пользователи которые приходят с некоторым IP адресом и говорят типа разрезов Вите мне там что-нибудь Sun vk.com мы сервера все раздающие cdn назвали Sun в честь Сана в 2017 году закрывшиеся не существующей больше компаний компания была классная Бирюзовая но к сожалению закрылась Итак мы все приходили к некоторым ДНС серверам которые нам возвращали завис от нашего IP какой-то геопленет сервер там упал то в течение ещё какого-то времени тля Это буквально минуты у нас всё было недоступно это было сложно балансировать и в целом GE DNS мы много раз пробовали использовать у нас пока нигде в продакшене это не работает следующее решение более популярное anycast Вы не поверите внутри Интернета можно один IP адрес анонсировать из разных операторов то есть буквально у вас один и тот же IP адрес доступен в Хабаровске Екатеринбурге и Питере таким образом ваши оператор выберет ближайший путь Обычно по количеству хопом и Единственная проблема что если ваш сервер в Хабаровске перегружен и вы бы хотели скинуть у вас нет никакого возможности потому что все пользователи Они находятся в Хабаровске они пойдут в Хабаровск там вы как Как ни крути даже если в Хабаровске у вас проблемы надо отдавать любой ценой это сложно масштабировать Что у нас получилось у нас было два типа Шей одни кэше - это КиШ которые принадлежали нам это наши дата-центры они имели огромное количество стыков с операторами это всегда четв там ростек вот это всё это были надёжные площадки которые мы контролировали и были операторы поменьше где у нас обычно был один оператор не надёжное соединение мы были не уверены для количества да что ж такое с VK cash А мы обычно использовали историю с any Custom потому что мы можем обслуживать сервера гарантировать качество и anycast был хорошим решением enic у нас работает на Москву Питер Самару Хабаров крупные центры которые прижат чисто ВКонтакте и операторские кэши мы поняли что их нужно масштабировать через bgp и расскажу чуть-чуть дальше как это работает стате себе У вас есть оператор ш Это небольшой провайдер у которого есть набор ашнико которые он обслуживает он обслужит различные подсети и говорит я вот Обслуживаю айпишник с такой-то маской айпишник с такой-то маской это всё мои пользователи мы этот весь bgp трафик заворачиваем к себе демоном читаем Что у нас там внутри от операторам Она довольно Громо нам пришлось прямо серьёзные алгоритмы для этого использовать это практически таблица морти зация такая же как в рауте только написанная у нас в коде Наго табли тогда когда к нам приходит пользователь говорит Дай мне какую-нибудь картинку музыку документ мы его айпишник отображая в ближайший по количеству хопом нашей внутренней инфраструктуры загруженности на самом деле серверов и всех внутрен говорим Слушай к тебе ближайший там будет Екатеринбург хаба или е что-то и возвращаем тебе урол на конкретный операторский кэш в котором скорее стопроцентно у тебя мачи маска ты получаешь туда контент но перед тем-то послать мы на самом деле считаем счётчик контента на в Хабаровске то есть мы считаем что это контент популярен в Хабаровске и только когда у нас счётчик превысил какое-то количество мы загрузили данные в Хабаровск и потом наши пользователи туда пошли во всех остальных случаях мы не перегружая канал на родину не перегружая си которых мы не уверены в которых каждая уборщ может махнуть шваброй и у вас нет этого сидна тем временем мы разгрузили сеть все операторы были счастливы мы установили некоторое решение которое сравн с Google Google с Google Global Cash на страны России и СНГ и шёл 2019 год у многих пользователей было очень много фотографий уже больше половины кбайта э фотографии со свадьбы с выпускного которые вы смотрите в раз в несколько лет но они для вас были очень важны и тёплые при этом параллельно ВКонтакте запускал суп вы могли пользоваться такси едой и так далее аудитория превышала 97 млв пользователей тут мы поняли что хранение то самое хранение на дисках в рейде стало недостаточно усто недостаточно не соответствовать рынку То есть если раньше мы могли хранить в рейде знаете какая проблема рейда да у вас если сервер пропал то у вас вообще Нет данных самое интересное что диски которые у вас очень долго работают 5 7 10 лет они все выходят из строя разом Не дай Бог вам потушить сервер десятилетней давности у вас хдд вообще не прочитаю вас рейд не спасёт при подъёме при остановке головок они почти ничего не прочитают то есть на самом деле если у вас сервер пнул которому уже очень много лет он на самом деле может ничего не прочитать и мы поняли что нам нужно перестать хранить в рейде На одном сервере и начать работать расми системами сае простое что мо придумать На одном сервере если серве пропал ваших фотографий больше нету мы стали распределять данные по нескольким серверам Обычно по трм для того чтобы Вы могли это прочитать Но это было немножко дорого для инфраструктуры и количества бюджетов и мы разработали довольно сложную интересную систему которую можно по куру посмотреть Если вы хотите если у вас вдруг есть ситуация что Вы много храните холодных данных то мы взяли допустим наме 2 ги польских фотографий 3 на5 например И посчитали по ним по вертикали горизонтали мы посчитали Да что ж такое-то кры И после этого мы посчитали некоторое решение с EV чт не вот здесь есть ссылка на статью и вообще на Как посмотреть и она вам позволяет восстанавливать в ряду порядка двух потерь дисков в вертикали порядка одного дисков Когда у вас Матрица горизонта диска Верка Кост что нужно потерять очень большое количество сегментов вот в определённом варианте чтобы вы не смогли починить почти во всех случаях вы можете чинить и при этом overhead у вас меньше чем например какое-то копирование у вас буквально 87 про на самом деле оверхед При этом Вы можете терять шесть случайных дисков если диски распределены то вы можете терять ещё шесть случайных серверов и четверть кластера и при этом восстанавливать там есть интересное решение оно конкурентное с многими решениями поставляемыми в мире вот ну Мы научились наконец-то хранить данные в системе не теряя их мы на самом деле думаем сейчас разрабатываем систему вообще отказа от файловой системы работа с диском с блочными устройствами Но количество разработчиков росло и мы поняли что намдо обеспечивать качество кода и у нас появился некоторый линтер свой который обеспечивает вы его можете с вго осно проектами у нас больше 8 млн строк кодов PHP и при этом Это количество строк кода Он обрабатывает буквально за минуту А если у вас какой-то МР реквест то Это буквально какие-то секунды при которых Вы можете проверить очень много вещей недоступность кода аргументы Вот это всё Мы добавили свой НР мы его засор сили мы продолжаем делать код лучше А тем временем шёл 2020 год появились новые продукты новые команды разработки который ВКонтакте уже была 97 млн пользователей в это время Хай вокруг микросервисов вышел на Плато наконец-то Хай закончился все сказали да микросервисы хорошо и мы решили к нашим базам данных на c+ добавили ещё го джаву и различные решения у нас появилось очень много решений на го это и распознавание речи по котороя сегодня дальше пойдёт это и некоторая трансформация изображений и работа с jav сервисами в том числе с мейми сервисами что е интересно чись kphp перестал быть транслятором PHP в Плюсы это стал некоторый кастомный язык первое что важно знать у нас больше 8 млн строк кода най PHP они транслируются в бинарник 2 Гб на плюсах больше 150.000 файлов на синтетических тестах от двух до 40 раз быстрее а в продакшене от семи до 10 и при этом есть успешные решения перевода крупных ресурсов реально на kphp с PHP на kphp которые стали действительно быстрее в 710 раз Если у вас есть PHP ресурс у вас мало оборудования вы хотите ускориться то в 710 раз мы вам Гарантируем и главное что это история Open Source можно приходить Но kphp кроме транслятора в это время стал Ещё и некоторым языком у него появилась строгая типизация она имеет Back compil поддержку в плагинах типа PHP ST от gid и так далее которые позволяют вам с одной стороны быть в PHP comp и запускаться на локальном сервере PHP а с другой стороны иметь строгую типизацию проверку мы поддерживаем PHP вплоть до восьмой версии у нас появилось там различные штуки с шаре най помнить параллелизмом рутина в PHP и даже поддержкой мы А если те всем воспользоваться можно уже воспользоваться А в скором времени у нас появятся дженерики представляете дженерики в PHP а шёл двадцать первый год уже одиннадцатый год Клауд компью у нас запускалось всё больше новых сервисов у нас появилось небольшое скромное Облако на 1500 серверов действительно ВКонтакте 1500 серверов - это не так много это конечно решение с куром Но самое главное мы в наш PHP добавили параллелизм у кого параллельные PHP Поднимите руки только сотрудники ВКонтакте и вот я ещё увидел какого-то чека я потом с ним поговорю один человек на самом деле поднял руку У кого параллельный PHP значит с точки зрения PHP мы добавили такую интересную логику мы разрешили делиться PHP на некоторые воркеры на этом сервере решили делиться в облако таким образом вы научились алоди свои вычисления на соседнее ядро или на соседнее облако в зависимости если соседнее ядро у вас ipc это вообще дёшево emory просто шарится и если это сеть то это передаётся в облако в соседнее за счёт этого нам удалось ускорить ленту в два раза Давайте представим что такое польс клиента У вас есть 500 постов пользователей у вас откуда они берутся все ваши подписки в среднем сейчас по ВКонтакте это Вы заходите с утра у вас минимум 500 постов мы их при ранком до 1.000 эту 1.000 ранжиру е различными ме алгоритмами там xgboost какие-то данные и так далее параллельно считается ваш Discover реклама блоки и так далее Всё это мёрт И рендерится за счёт паралельных вычислений и зако закона амдала нам удалось в два раза увеличить производительность дальше нам уже лизм не особо помогает вот Ну вот это получилось таким образом потом мы поняли что нам важно Time to maret разработка на всякий случай у ВКонтакте - это 3800 методов мы это всё автоматически с PHP кода умеем конвертировать в J и создавать СДК для всех клиентов iOS Android webs можете посмотреть J код и все эти Клиенты наконец-то у нас полная автоматизация случилась д Перм мы запустили Кто не знает что тако вот по можно прой но влом мыста контента в целом до двух раз на изображениях но на rpc всего на 2% но в целом очень хорошие цифры и мы со http2 переехали на hp3 ещё интересная штука которая нам стала доступна у нас были JPEG изображения Кто знает что такое wep примерно процентов 10 тире 20 это некоторый формат прогрессивный после пега который следует более сжатый при том же качестве на 30% меньше у нас уже были микросервиса на Go у нас было облако у нас было 600 пиб польских фотографий и по млн запросов за этими фотографиями пользователь приходил за этой фоткой попадал некоторый инс У нас есть своё решение в кэш по кэширования данных на инсе Если вы мимо кэша пролетели то вы попадали на некоторые Cloud решения в котором стояли или ГПУ или fpga которые вам могли из оригинального отказоустойчивого хранилища с конвертировать jp VP ещё и нужного размера если у вас разрешение экрана на телефоне какое-то мы вам могли это всё сконвертируйте на 15% при отдаче фото размер фото стал на 40% меньше вам всем стало легче Ну и вот это время трансформации пега и pga из оригинального размера и мы перестали хранить ресайз шёл август д первого астрологи его наименований этот банет называли некоторой чумой важно что с каждым годом количество видосов росло и в это момент поняли чтом нужно модифицировать нашу систему Мы из этого ди доса поняли мы быстро с ним справились это было не очень интересно стало интересно что если ваш кротик поражён каким-то вирусом или ещё каким-то Бат нето которые атакуют ВКонтакте нам было классно чтобы Вы всё-таки получили доступ к своей переписке и ко всему остальному здесь у нас появилось некоторое интересное решение что весь фронт который есть и BP считает некоторую информацию по пакетам отправляет в сервис который анализирует все эти данные и потом говорит некоторые правила назад кого нужно чить Но если вас Поли то всё что вам следует - это Директ JS Challenge вам предлагаем посчитать в браузере какой-то J скрипт потому что обычно те самые рауте с кротим которые принадлежат уже не только вам а ещё кому-то кому-то кто живёт на Водном стадионе в Москве да вот они на самом деле обычно не умеют JavaScript браузер и мы такие типа Давайте почитаем какую-то формулу на жава скрипте которая вет специи который може поть только вы и тогда мы Вам допустим доступ к кду Ну в крайнем случае если этого ничего не получилось Мы вам ещ копчу покажем она супер страшная эта Копча но можно с ней жить шёл 2020 год у нас появились новые требования к отказу устойчивостью в двадцатом году мы решили построить новый дата-центр мы поняли что у нас какой-то там история Да Нам нужна автоматическая отказоустойчивость мы выбрали некоторый сложный консенсус и с этим будем жить что ещ д втором году нам потребовалось немножко оптимизаций и для наших деся кэндо мы первое что мы включили Ну второе что мы сделали биндинг нума ядерный конкретный порт То есть если у вас есть что-то конкретный вывод мы увеличили на 20% Кати нашего энда То есть если раньше наш падал на 70% несь деградация по времени дальше Шторм всё плохо Вот теперь он падает на 90% утилизации благодаря чему всё что мы сделали Мы для наших нума ядер забиндить отдельный порт это проблема concurrency На На некотором сокете и некоторая проблема с тем что желательно поднимать внутри нума ядра все наши воркеры с Бинго на отдельный порт нам это позволило вот так вот улучшиться что ещ мы году мы у нас уже давно были микросервисы но мы решили дать доступ не только ptl rpc но ещё и по grpc к нашим сервисам и мы стали предоставлять различные сервисы типа видеозвонков ара и ещё чего-то на базе grpc внутри компании и даже за границами в год финальная архитектура ВКонтакте Ну на самом деле может быть не финальная у нас больше 100 млн Мау клиентов у нас 3 млн rpc у нас http 2 или 3 у нас больше 100 НН серверов с ксом которые поддерживают осные BP фильтрации ну вы знаете bpf На одном сервере там может до 20 млн пакетов легко фильтра нуть То есть это вы прямо одним сервером можете всю mic L7 атаку отразить Но на самом деле ратор сегодня расскажет лучше чем я дальше у нас порядка 10.000 серверов у которых есть свой http у которых есть PHP который ходит в кши и который ходит в микросервисы на PHP Go и при этом ещё есть в Cloud 10% у нас находится в клауде и это обычно jpc если смотреть про контент то когда пользователи приходят за контентом они идут на некоторые са сервера закрылся сервера остались Надеюсь что они останутся вот на которых которы некоторым решением которое по широва наш кастом доработке любим дорабатывать и на наша и так далее и даже выкладывать это в Open Source Вот и у нас есть кастомный ш в инсе который если кше нету ходит в данные если мы попались в операторе то Нас туда отправят только в том случае если данные на операторе есть там тоже будет с большим количеством каширования дивс по архитек контакте 200 бы лиен клиенты год то у нас просто стало больше клиентов наш трансформировался в у нас появились защиты отса наш pH трансформировался вш остался но протокол у него поменялся с на наш PC и если посмотреть на всю эту историю то видно что изначально был к потом появился появились микросервисы В 2009 когда их никто не называл микросервисами появился rpc своя сериализация транслятор в пятнадцатом появились ходу определённое вычисление в семнадцатом CD свои stpc микросервисы на разных языках Java и KP язык и после этого у нас появился PHP который есть у одного человека я его запомнил мы с ним поговорим И после этого у нас появилась http 3 возможность конвертировать фотографии на литу и различные истории с тизи досо Спасибо микротику Итого на самом деле архитектура за вот эти там 15 лет она развивается вместе с продуктом и под высокими нагрузками у вас всегда есть риск что какой-то сервис типа mysql начнёт коптить данные падать И вам нужно либо разобраться в My либо написать своё решение и обычно это решение примерно сравнимы нет готовых законов если у вас сервис не нагруженный небольшой Вы можете использовать каки стандартные подходы Но если вы cutting Edge сервис который конкурирует со всеми конкурентами вам нужно писать свои высоконагруженные решения и все что я сегодня рассказываю это просто пример их не нужно обводить Возможно это пример который позволит вам куда-то продвинуться но это не решение для вас когда мы получили какое-то решение нам нужно его эксплуатировать А например для того чтобы нам мониторить решение мы собираем 200 млн различных графиков с разных серверов пользователей всему остальному главная Интересная история что гранулярность - это 1 секунда и задержка 3 секунды То есть если у вас упал Продакшн Что случилось через 3 секунды мы уже знаем о том что у нас есть проблема И если вы хотите посмотреть про этот доклад то он будет на Московском хайде Вот написано что он принят Я уверен что он будет Поэтому ищите этот доклад У нас есть большое количество воч догов которые позволяют 200 млн графиков контролировать и все команды получают сообщения пуши нотификации У нас есть внешний мониторинг мы контроли да детектор и обращение в поддержку если у нас что-то не работает у нас есть огромные требования сй это скорость ответа батарейка сеть потребление Вот это всё и мы всё это контролируем и при любом эксперименте запуске а эксперимента новым функционала мы смотрим а не стала ли ваша умная Лента тратить больше батарейки пользователей и наверное самое важное что мы хотим сказать Иногда даже ВКонтакте с серверами может не справиться с нагрузкой задача деградировать обычно мы деградируем за счёт ленты и тут мы просто уменьшаем количество постов которые мы ранжиру если посмотреть наши вела ability У нас есть сервис который проверяет нашу доступность он тестирует все сервисы ВКонтакте основные которые вы можете видеть ленту видео а группы друзья и так далее и он говорит что мы недоступны Если хоть один из крупных сервисов недоступен и наши ла абилити Вот 9994 мы там порядка часа в год лежим и хотели бы лежать меньше но всегда эксплуатация это баланс тату Маркета если мы перестанем разрабатывать код мы вообще перестанем лежать Я почти уверен 99 99 Давайте перестанем писать код и всё станет классно но нет поэтому нам нужно очень быстро развиваться поэтому мы для наших разработчиков используем вот если вы пришли ВКонтакте вы написали код написали его буквально там за пару часов новую фичу вы прошли чек разработчика которы ттит буквально 10 минут А что у вас поменялось Что изменилось потом попали на сборку Вы не поверите но 8 млн кода ВКонтакте собирается за полторы минуты то есть порядка правда Это вам 80 серверов будет делать но вы через полторы минуты уже имеете некоторую готовую сборку и как это сделается можно посмотреть по некоторой ссылке а потом это всё пройдёт тесты мы Тестируем всё что у чего есть кадж пользователями больше 1% если пользователи по этому экрану функциональности пользо больше 1% это есть в код кавери что мы построили целый Граф настоящая картинка этого графа ужасного что вы Все переходите Вот и мы это всё Тестируем и все эти тесты они работают там на 30 машинах и буквально за 5 минут теперь наш бинарник и шный Если вы помните мы скомпилировать PHP в бинарник он 2 гиб 10.000 серверов Мы научились его деплоить за 7 минут мы это делаем госпо мы катаем в какой-то сервер он катает очень быстро внутри стойки потом катает внутри ряда ряд катает внутри дата-центра ну если вы знаете что та стандартный Да вот ва сплетня вы пришли на кухню рассказали друзьям если друзья эту сплетню не знают рассказали всем оны если они сплетню знают они перестают рассказывать то есть если я об этом знаю я не рассказываю не знаю рассказываю 7 минут нам это позволяет в целом Если мы с нуля хотим собрать весь наш бкд это 20 минут с автотесто с деплоя совсем на прот и 14 минут если у нас какие-то изменения у нас ещё есть 10 минут стейджинг то есть мы значит что мы всё собрали выкатили на стейджинг проверили 200 млн графиков что ничего не изменилось только потом идем и финальный вариант каждые полчаса мы катаем 10.000 серверов вы вчера пришли на работу написали новый код и через полчаса оказались в продакшене Вот правда я вас всех это но через полчаса в продакшене Я не знаю у кого вы ещё можете оказаться конечно бывают онные Но после того как запустили кодре крыть рубильника чтобы он не менял продакшн потому что у нас поезда и изменения каждые полчаса у нас уходит больше сотни коммитов различных людей они все закрыты рубильника мы их эти рубильники откатывается 200 млн параметров Итого у нас больше 200 деплой в день у нас постоянно крутится на пользователя минимум 100 а экспериментов и в день у нас 1.000 изменений рубильников По раскатке функционально различные разработчики крутят если посмотреть крупно то в году нам удалось запустить 230 крупных изменений это прям реально изменения с которые повлияли на пользователей наш Time to maret равен 5 часов я вам говорил про Полчаса это был плой Полчаса вы там пишите код заполняете какие-то чеки джира вам считаем полчаса и 4 часа у нас среднее время код ревю самое долгое что будеть кодре мы постоянно оптимизируем и на самом деле в год мы растём на 30% по кодовой базе помн что всегда эксплуатация и Time to Market - это баланс с одной стороны у нас есть лити 994 которые мы хотели бы сделать больше а с другой стороны у нас есть Time to maret где за 5 часов вы пришли на работу ВКонтакте и вы уже работаете самое главное что мы помним что архитектура должна защищать наш продукт разработчика Вы не должны ничего сломать Если вы что-то сломали вас автоматически откат бисек выкинут из репозитория ци что вы сломали все тесты и Вы не правы и вы будете дальше разбираться если говорить про архитектуру ВКонтакте всё что я рассказывал это была структура какие-то кубики устроены какие-то самое главное это мотивация и Мы помним что если мы хотим что-то поменять внутри структуры ВКонтакте самое главное - это зачем мы это меняем потом мы смотрим на какие продуктовые метрики это повлияет что у нас поменяется с точки зрения пользователя скорости ответов трафика и так далее никогда нет новый продукт новая технология Я хочу использовать react в новом продукте мы не знаем хорошо это или плохо поэтому к сожалению мы сейчас запускаем в vkui на базе реакта на старых продуктах Саб и оцениваем Профит всегда нам важна простота решения потому что архитектура структура - это про простоту чтобы все могли понять мы играем в долгую понимаем что мы готовы инвестировать в инфраструктуру структуру архитектуру и так далее чтобы в перспективе стать лучше и если вы пришли с нам с новым решением Говорите Вот это надо делать то нам нужно посмотреть варианты не существует решения что нет каких-то альтернативных вариантов и вот не те кубики которые я вам Рассказывал а именно вот это это текущая архитектура ВКонтакте которая накрыто некоторым социальным аспектом больше чем десятки разработчиков которые эту архитектуру поддержат и саппортит немножко про будущее Мы конечно хотим сделать к балансер bpf мы говорим про ко контро авторы Севе архитектуру но все наши разработчики они работают уже в се архитектуре они приходят комитет абы что и наш эн с этим справится и разберётся для вас вообще не существует серверов вы просто пишете код который вызывает функции Самое интересное это web3 и30 вот здесь вот я всё-таки потрачу времени хотя на меня Лёша уже смотрит что надо заканчивать представьте себе раньше Вы приходили на ВКонтакте Ну какие-то другие запрещённые сервисы Российской Федерации и загружали свою фотографи это фотографи этому сервису сервис не работает нет фотографии Представьте себе что мы на базе блокчейна ipf договорились что у вас есть распределённая файловая система которая поддерживается крупными вендора Вы один раз грузите фотографию в любой сервис она доступна во всех она постится если этот сервис не работает фотка продолжает вам принадлежать это какое-то невообразимое будущее но Представьте единожды постить вы доступны везде во всех соцсетях и это кажется тем самым будущем инфраструктуры когда ваш контент принадлежит вам а не сервису куда вы это залили А что было сегодня вы посмотрели определение высоконагруженные системы каждый придумали его сам я придумал за вас Вы посмотрели эру развития Веба ретроспективу ВКонтакте и посмотрели будущее для нас же важно растить доступность Э мы увеличиваем доступность данных масштабируемость Мы помним про Time to Market много инвестируем в ML упиралась помним про наших пользователей Ну и самая важная вещь которую вы можете запомнить архитектура - это социальное явление Не пытайтесь архитектуру придумать в одиночку а Надеюсь что никто здесь в этом зале не пытается придумать архитектуру нагруженного сервиса в одиночку и у каждого свой путь в зависимости от решения Мы помним что Open Source победил поэтому подключайте к Open Сорсо максимально опенсорс только лучшие проекты могут развиваться именно в опенсорс Ну и наверное важное мы сегодня решили открыть доступ к нашему распознаванию речи чтобы Вы могли делать классные игры сервисы И вообще если вам нужно распознавать субтитры к видео фото и так далее Это вам всё будет доступно абсолютно бесплатно Вы можете загружать файлы до 100 минут это работает в нашем облаке у нас облако простаивает там не полтысячи серверов уверен на всех в этом зале хватит У нас очень быстро ну и перед тем как вообще воспользоваться этой архитектурой вы сегодня можете сходить ещ на два доклада к Виталию шуму и Филиппу маковскому один расскажет как это устроено с точки зрения другой расскажет с точки зрения инфраструктуры потом вы можете просто прийти попробовать как это работает и добавить сервис распознавания речи с русского он работает особо хорошо на разные языки Вот поэтому Всем большое спасибо если вы хотите оставить фидбек по докладу и написать что я сегодня всё затянул Иша меня плохо выгонял со сны вот шите прямо сюда СБО ВКонтакте дружите как обычно я не успел с вопросами поэтому все вопросы на стенде ВКонтакте вы прослушали короткую версию онбординга в технические подразделения ВК давай селфи с полным залом который ты собрал друзья обратили внимание на логотипы над красотой Да Александр тоболь ВК обли обратили внимание L"
}