{
  "video_id": "q5mTwjC3FV8",
  "channel": "HighLoadChannel",
  "title": "События, события и ещё раз события / Антон Сухов (Avito)",
  "views": 1875,
  "duration": 2640,
  "published": "2018-01-16T12:25:11-08:00",
  "text": "меня зовут антон сухов я работаю ведущим разработчиком во vito и собственно занимаюсь разработкой системы сбора и обработки событий значит в стоим так ладдере хотелось бы коротенечко становиться о том какие существуют проблемы разбора событий касаемо micro series критик туры о том рассказать немножко о том как что выбрали мы в качестве решения что мы достигли подвести некие итоги для начала хотелось бы сразу заметить что производится под событиями если мы говорим про программистов снова себя волита мы подражаем некий джейсон с набором полей ну это может быть какой-то другой формат но для удобства 10 органично эту шутку это джейсон chic если мы говорим об аналитиках то есть на им мало интересует все эти наши фишки типа джейсон там или про табов еще что-то им надо просто сказать дай нам данные о пользователе нам об авторизации или дай нам данные о том какие продукты покупались и так проблемы то есть казалось бы мы привык то что нам говорят microserver текстура сделает ваш проект лучше если у вас был монолит он сделает его более гибким разработки у вас будет множество команд которые возьмут себе те технологии которые им нравятся то есть они там кто-то возьмет тебе питон это выберите php кто то еще что то возьмет но все это накладывает некие ограничения на сбор событий то есть у нас ухудшается коммуникацию есть у нас есть монолит у нас есть одна команда 1 кодовая база мы всегда знаем кому можно подойти и сказать ребята да пошли нам какое-то событие о том что делал пользователь есть у нас мика сердце и текстуры как это получается приходит аналитик говорит ребята я хочу знать там информацию о поиске время они горят ни слова больше мы тебе добавляем эту информацию события окей андрей всегда волен дальше он подходит в другим ребятам вред ребята я хочу узнать у вас данные о товаре они говорят ни слова больше мышц теперь все добавим сейчас все будет получается что одни делают это на php это делает это нога фото на питоне мы получаем дублирование кода мы получаем повторную реализацию одного и того же по сути у нас рано рано или поздно мы получим если не будем это дело как-то контролировать систематизировать и не под какую-то единую цену мы получим кашу чтобы всего этого избежать хорошо бы разработать некую систему сбора обработки событий которая должна учитывать несколько вещей в первую очередь она должна учитывать именно то что архитектура микро сервисная то есть события могут поступать с кучей источников это сайт мобильное приложение те же самые сотни сервисов которые разрабатываете 2 необходимая черта для системы это возможность агрегировать и обрабатывайте события но при этом обрабатывать события хорошо бы так то есть чтобы одно событие могло обрабатываться несколькими людьми то есть не просто аналитики как-то его обработали у себя и забыли возможно эта информация будет полезна и другим отделам другим командам и в идеале такая система должна это делать просто и ненавязчиво и так как же это можно решить все эти проблемы мы можем добавить документацию о том как какие у нас есть события в командах и в идеале каждая команда перед тем как что-то реализовать или аналитики как что-то получить они сначала зайдут читают документацию которую системы все пол напишем здесь на этого не будет никогда никто и она будет актуальная она не будет актуальна и мы можем ввести некий встречи синхронизации когда разные команды будут регулярных восемь раз две недели как-то состыковываться находить какие-то общие части верит что они планируют добавить что или а нибудь ведь что от них хотят аналитики потому что возможно это уже была кем-то реализовано или эти данные можно получить уже из других событий но это тоже проблема потому что конечно же у нас все программисты все экстраверт и все так и жаждут как бы пообщаться с коллегами место ушел пойти пописать код просто приходится утра и думает отлично мне сегодня тремито по три встречи 5 митингов ох поработаю мы можем добавить дикие регулирующий орган которые говорят говорить что есть вот такие то события где люди и если мы хотим добавить что-то новое мы должны будем платить от них некий оправ мы можем добавить единую базу мы можем сделать techradar которые граничат нам перечень технологий который мы можем использовать все это как так или иначе может решить и систематизировать все нашу как бы кашу которому получаем микроструктуре как мы это решили lavita сначала посмотрим что у нас было изначально у нас был большой монолитное приложение сайт который тепло сортом на порядка 60 серверов у нас был флинт в качестве транс системы для транспорта логов и было в manga в качестве системы для привычного хранения информации то есть события полились сосен через тот же самый флюенс который мы использовали для логов после как мы реализовали систему она стала более сложной то есть полностью от монолит естественно мы не успели избавиться у нас есть монолит часть микро сервисов мы сейчас продолжаем активно его распиливать надеемся то рано или поздно мы реализуем у нас есть fluent для сбора логов есть система имели стрим процессе на которую мы призовую альфа-версия через которую сейчас собираются события также осталось мунка в качестве привычного хранилища при переходе нами красивую текстуру ну как бы все эти события не шли параллельно то есть мы разрабатывали систему сбора событий также необходимо было разработать и внезапно внедрить не кися из идей для этого был выбран тем сити все это разворачивается в наших любимых докеров настраивается кубер нэйт сам все сейчас это есть поговорим ножка подробнее о том что же такое sp как ни странно эта система сбора событий она является единой точкой входа для событий всех наших сервисов авито представляет средства синхронизации между разработчиками и аналитиками так как я вначале упомянул что аналитика мне интересно все наши вот этот сейчас мы вам дадим джейсон чьих-либо вам дадим там csv и у это мало интересно и понятно + система позволяет получать данные в разрезе нескольких сервисов и больше оперирую бизнес понятиями то есть если мы говорим дай нам события пользователя по всем проектам системой sp позволяет это получить как это будет как это реализовано я попытка расскажу итак если ты коротко на слайде то это будет слайд с тремя главными блоками то есть мы видим что у нас есть аналитики разработчики некие потребители событий и три блока давать которых как вы объединены в один то есть нам нужен единый реестр событий нам нужно некий сервис коды генерации который нам позволит касс этот запах технологий как-то привести в порядок то есть у нас программист когда он будет реализовать новые события он не будет руками заполнять все эти поля мы после предоставив ему уже какие-то на борт и зеленых классов методов функции и у нас есть канал передачи событий то есть и такая некая единая шины для передачи событий что это присед в себя в разрезе авито и так у нас есть сайт у нас есть мобильное приложение то сессии посмотри на картинке юге нашего сайта он развернут на куче сидоров из вас как бы там ни в единственном инстансе а там порядка 60 штук есть мобильное приложение которое свои события отправляет используя сайт как прокси есть некие сервисы например авто teka которая живет отдельно есть некий внешний тулузы внутренние со всего этого дела надо собирать событие как я уже упоминал для пример сохранение использоваться манга и аналитики берут свой любимый набор скриптов и как бы постоянно пуля мангус собирают 8 раз в полчаса туда ходят выбирают нужные события обрабатывают их как говорилось был нас монолит используется флинт то есть для событий также стал использоваться флинт был на каждом сервере развёрнут локальный флинт все это агрегируется в некий флинт коллектор и потом попадает в базу естественно при таком подходе мы получили все те проблемы которых я говорил то есть когда у нас не было единого реестра событий у нас могли возникать коллизии в коде то есть кто-то поменял поле аналитики об этом не узнали либо добавили свежие поле не описали это не где аналитики об этом не могут не могли догадаться у нас при такой схеме увеличивается время доставки события от момента но как оно произошло до момента как его обработали аналитики потому что базу мы не можем дергать раз в секунду и проверять наличие нового сообщения это происходит там раз в полчаса для таких событий раз в час все это хорошо если событие никто ну и найти этого события условно говоря real-time не актуально ну прям допустим каких-то акциях и хорошо бы нам видеть о том как происходит событие в тех же самых кликов в режиме условно рядом выбрали вы в режиме реального времени то есть видно что есть у нас какие-то проблемы возникают в месте где события сама все производится то узнав узнать об этой проблеме мы можем только когда события проекту эту цепочку то есть полнотой знания о события узнали только аналитики чтобы это избавиться мы добавили первым делом реестр событий клика стрим сторож что он себя представляет он из себя представляет . синхронизации то есть у нас есть некий источник бизнес требования есть аналитики которые периоды бизнес сущностями говорят что они хотят получить есть наш реестр событий он позволяет нам уже сгенерирует код как бы о вавилон у нас три основных языка как бы сейчас добавился 4 java но изначально у нас был php питон его соответственно мы получаем набор функций классов и уже работаем с событиями посредством измененного кода какая там плюс то есть если у нас есть сгнили он ый класс то проверка валидности происходит на этапе сет cetera то есть мы передали какой-то пара это тут же об этом узнали в предыдущей схеме если у нас нет этого мы добавили более она была энтом стало с рингом проверим это только поставок аналитики не смогут сын продеть его из базы то есть мы получаем валидацию кода сняв и событий плюс данный сервис реализует у нас авто генерацию кода то есть мы автоматически получаем в конфликте страничку в которой перечислены все события все поля описанные их структуры и даже уже на этом этапе если аналитику заинтересуется каким-то событиям он может зайти в зеленую документацию который гарантированно актуально и посмотреть такие событий сейчас прилетают и так это что касаемо проблему микроархитектуры то есть мы более-менее синхронизировали все это дело привели в порядок весь разрозненный зоопарк техники и технологий но при этом у нас оставалось наследие то есть нас был флинт в качестве транспорта событий когда мы здесь написано нет такого что событий на чтобы этим праздную волос когда у нас монолит разделился на множество сервисов каждый из них получил какую-то свою настройку флинта то есть в одном вместе прописаны из клиента перешли свои события в коллектор в каком-то флинте это не прописано то есть когда мы стали менять флинт на другую технологию мы получили множество проблем то есть мы не знали в какое место посмотреть чтобы она что сказать что события от сервиса а переходит в сервис б потом к аналитикам были проблемы с тем что логе и события шить из руин то есть было мешанина если где-то ломалась допустим какое-то положение ломалась начинала слать stack trace использовать тот же самый флинт мы получали переполнение буфера флинт у нас могла свалиться системой почему было не понятно кто виноват это будет либо виноват тот кто продюсер события ли будут вы продюсер логе то есть по-хорошему этой все системы это разнести на два компонента логе отдельно события отдельно ну и как бы с лагами флюнс правов у нас на ура то есть блоги мы собирали слали все отлично когда мы захотели получить функционал динамические подписаться на части событий а не путайте весь поток который нас слет в мозгу легко и просто это сделать мы не получили возможно этом если покопаться и по настраиваться добавить сироп можно это реализовать но мы посчитали что нам проще заменить вот этот транспортное ядро сплюнь ты заменить на другую технологию в качестве этой технологии мы взяли и низкую new simple пью как бы почему потому что он уже представляет из себя распределенную систему уже как бы архитектуру заложено без единой точки отказа он легко масштабируется горизонтально написан на нашем любимом гол angie которому нас есть ника экспертизы как бы если мы захотим что-то подправить мы это сделаем если ошибаюсь friend нас написан был наруби с робби у нас все не так радужно править там что то мы не можем плюс он легко настраивается то есть сама по себе архитектура энеску она ориентирована на потребителя есть вводим партитуру которую заложили в е с т и транспорта событий это можно представить все по аналогии с самим сайтом авито то есть когда вы выставляете на продажу какой-то свой товар если бы все многих флинта вы должны были указать что я хочу свой товар продать вот только этому только этому человеку если мы говорим про миску 100 гр эхсон послу которым товар и потребитель опрашивает сайт говорит а где есть такие товары у кого можно купить и сам выбирает у кого это товар взять если мы говорим про миску то мы видим то же самую систему то есть worker опрашивает некий сервис топик и discovery in sq lookup который в которой он запрашивает дай мне перешли сюда серверов на котором хранятся события в таком-то то таких-то топиков получает данные список сыров он уже при необходимости к нему подключаться и выключить все эти события что нам это упрощает это упрощает администрирование то есть когда мы разрабатываем сервис мы просто кладем события к себе в локальную очередь и забываем про них дальше отдельно разрабатывается консьюмер потребитель который эти события будут у вас выкачивать дальше у нее есть печень слайдов про лису который я взял из их официального сайта то есть основы nsk это две вещи это топики и каналы то есть это те самые технологии которые позволят нам одно событие обрабатывать несколькими удобными нам способами то есть у нас есть топик которому можем подключиться используя разные каналы делаем один канал 2 3 канал делаем обработчик событий 2 3 то есть видно что у нас есть один топик есть три канала и 3 обработчика событий как же у нас будет привет события события попадая в топик она дублируется в каждый из каналов а уже из канала оно попадает к одному из потребителей таким образом мы можем событие щелчок допустим по объявлению автоматически обработчиком сложить в базу и тут же обработчиком который считает метрики посчитать метрики все это будет параллельно итак наша начальная схема примет вот такой вид то есть у нас добавились консьюмер и добавились какие-то обработчики теперь о том как происходит именно подписка на события в разрезе нескольких сервисов то есть в отличие от ребята в который поддерживал wildcard там или маски насколько кучи логине поддерживает но так как мы мышц программисты мы же можем это все добавить немного и мы получаем следующую схему то есть используем в качестве имен топиков принцип доменных имен у нас есть 3 сервера 3 хранилища на которых хранятся события указаны стопим именно топиков указаны у нас есть топик discovery носку lookup который мониторит это дело у нас есть то есть все эти серваке они регистрируются выноску lookup что важно если энеску lookup сломается не будет какое-то время недоступен по сути ничего страшного не произойдет то есть вам подымется через пять минут то концу мир узнает она и через сообщение через пять минут сообщение ставим степени к они пропадут они будут копиться капица в этой очереди и рано или поздно мы их разгребём естественно как бы не рассматриваю момент когда он совсем все сломалось и консилер после не может изучаться от этого как бы никто не застрахован и так у нас есть консьюмер который опрашивает паникой маски то есть он хочет получить авторизация пользователей от всех проектов и он получит авторизацию сайта и авторизацию с ios устройств с android у нас хранится только ники ивенты скролла то есть он их не получит таким образом от всего потока событий мы выбрали такую некую часть которую нас интересует более подробно можно почитать на сайте там них стали довольно хороший принципе есть куча паттерна в которой они предлагают как можно использовать энеску как это дело по тюнить чтобы у вас все было хорошо и так вот какая у нас получилась схема места вот такой казалось бы простой что же нам в этой схеме что дал он нам переход какие профит мы получили от перехода той казалось бы простой к схеме к этой в полтора раза росло сложнее схемы визуально мы получили следующее у нас снизилась нагрузка на мангу из-за чего то есть аналитики получили механизм когда они могут события благо обрабатывать в режиме реального времени не используя превышена хранилищу мангу они просто подписались какие-то события и сразу льются теперь вертик у если им это необходимо то есть нас нет уже необходимости всех сохранить хранить в мангу мы повысили скорость то есть сама по себе sq реализованный на год мы получили рутины всех голову рутины распределенность параллельность все то что за что мы любим год мы получили возможность подписываться на события по некому шаблону то что я показывал на предыдущих слайдах то есть место единого потока событий мы берем какую-то часть и выдаем это на данном этапе мы можем весь этот поток при направлять у нас он как бы написано такие некие плагины для удобства мы предпримем и трепет и либо складываем еще путь отдельно стоящую мангу потому что для чего это сделано естественно как бы у нас все это развернута в клубе нас кластере мы можем в до некой степени масштабировать нагрузку но при этом если каждый отдел захочет подписаться на одно и то же событие получить себе свою личную копию естественно вас увеличится сетевой трафик вырастет нагрузка такое мы тоже не потяни поэтому поверх этого у нас есть какие-то свои административные решения то есть мы уже читаем все события делим их у себя то по топиком и метательный мы можем выдавать то есть отдел приходит и говорит кайта команда что у нас есть там свой rabbit поднятый мы хотим получить туда такие какие-то события окей все мы это можем и мы получили гибкое масштабирование так как у нас сейчас все новые сервисы и в частности есть все это развернута в докер контейнерах кубер нет масштабировании мы можем достигать просто добавляя или уменьшать количество кодов поменяли циферку увеличили число кодов справились большим пиком событий если мы видим в монету по мониторингу of эти один из плюсов что и нас купи себе в принципе содержат уже механизм отправки статистике там 100 df мониторинг то есть если мы выбираем выноску в качестве транспорта системным автоматически получаем и мониторинг и админку увидят что у нас за счет трафик мы можем добавить новые коды справиться этим пиком увидели пик прошел слух кодов можно убавить все это динамичная и быстро если мы будем греть про флинт так у нас уже не получится и один из самых важных факторов мы снизили время доставки события то есть если раньше был механизм полинга базы раз полчаса то теперь есть механизм подписаться на события и получить его условно в режиме реального времени это то что мы уже реализовали то что сейчас уже работает то что сейчас мы пользуемся но в наших планах дальше как-то деду упрощать во-первых полу мы получили эту самую систему что у нас на секунду назад мы до сих пор не избавились от флинта целиком даже для доставки событий почему такое произошло одно из цель устояла при разработке системы сделать это максимально незаметно для разработчиков других сервисов потому что надо было его внедрить попробовать и при этом если бы мы сказали разработчикам так сейчас вы перестаете писать вафлю нет а быстренько себе пишите классики как 105 выноску они говорят но хорошо моди нас попробуем потом угри нет нет а давай сначала мы эту красивую school в перем суда кафку в капусте копье темно кафку естественно такое невозможно тусую возможно то конечно будет неправильно поэтому у нас был написан некий адаптер fluent in sq разработчики по-прежнему пишут во фрунт но только после локального флинта все это попадает уже в нашу единую систему сбора событий от этого тоже мы хотим в дальнейшем отказаться то есть сейчас мы ее внедрили она пошла некую проверку на боевых данных на боевых объемах данных мы удостоверились что в принципе она справляется со своей задачей поэтому теперь можно продолжить убирать флинт стоит задача сделать обработку событий добавление обработчика новых событий автоматизируем режиме то есть как это происходит сейчас нам надо у нас есть конфиг map и в конфу berlitz и часть работы с но приходится делать руками естественно хотелось бы поверх этого всего но вернуть админку где просто несколько щелчков мы добавили поля добавили ивенты поставили так по которому будет строиться топик ним и условно говоря через десять минут события polaris там есть и уже кто-то их есть продюсер тех событий уже при лесистом в мангале был прибит туда куда захочет аналитик или кто-то кому это надо ну собственно говоря у меня в принципе все то есть единственное что хотелось бы теперь сказать по поводу того что должно быть в ваших сервисах если вы захотите внедрить нечто подобное то есть у кого-то вместо go используется java и скорее всего не проще будет использовать там кафку для став для до 100 транспорта событий бесконечного что мы все тоже самое есть и там провели там soul keeper в принципе все это реализуется и другими технологиями то есть на доску свет клином не сошёлся но у вас должна быть обязательно . связи микро сервисов и как между собой так и между сервисами и аналитиками желательно конечно на первом этапе возможно это будет не дать на это кода генерация потому что каждое новое событие особенно если у вас потом будут сотни тысячи событий руками не напишите это авто документация ну и сиди скорее всего так и наличие у нас у вас уже будет везде внедрен ну и по максимуму все автоматизировать масштабировать выпуском говоря есть какие то вопросы будкам нету контакты мои указаны спасибо за внимание спасибо антон у кого есть вопросы поднимать руку меня зовут сергей вопрос вот несколько раз говорили что он где b является временным хранилищем и вопрос каким является целевым временного но почему то есть она во первых у нас в самоваре то летит куча событий и у нас спросят такой капот коллекции мы не можем хранить то как надо события хранить там за неделю на неделю там и три дня все события хранить постоянно мы не можем то есть вас в принципе унюхать нас там уходят там та ра байты данных по этому событию хранится в банке там в чине этом недели аналитики заливает его потом к себе в верте q ну для анализа как у них это потом странице верте ки я щас не скажу это лучше в этом у николая голову на вспомнить так от на хабре статья там уже сказано проверьте q это что касаемо там аналитиков если можно еще один вопрос ты нагрузку сейчас вот по вы говорили пологом сейчас в среднем у нас идет до миллиона запросов в минуту в пике когда у нас были проблемы у нас на каплю эстампов и мы обрабатывали и до 9 миллионов запросов в минуту то есть мы спокойно пропускали через эту систему это открыть люк под событием пусть это получить обработайте записать в моему маджестик лишь вопрос по поводу еще нагрузки вот то что мы разворачиваем всю эту систему в купер нить это позволяет нам избежать варианты когда у нас идет неконтролируемый поток событий которые нам просто будет захлопывать все и обработчики и базу и хранилище то есть выбирается позволяет настраивать таким образом что если ваш под будет есть в этом больше 3 гигабайт памяти он просто будет прибивать то таким образом получается что если кто-то конденсации к нашей системе начинает гнить поток но просто обрубаем его там уж просто круг им под как бы как один из механизмов того что это предупредит всю систему вполне себе может работать пожалуйста ваш вопрос здравствуйте меня вопрос про единый рез событий куда генерацию я так предполагаю единый реестр представляет собой некое декларативное писаницы пойду сам схемы да и из него генерируется на для java понятно классы для готом интерфейса для питона что-нибудь по самой к дегенерации я щас дарения результат процесса таскать не смогу скорее всего тоже сказать какая то либо всего класса путь на реализовано по всему лента либо набор функций которые просто передай мне стало строчку int там массив на выходе получше объект ну примерно представляю хорошо спасибо 1000 вопросы да здравствуйте за высокий и повторяю вопрос почему энеску и а не раббит примеру тот же как центральный на обработчик череде этих всех событий которые методики есть в принципе все то же самое что вы описали здесь в принципе то ногу с куном посетим во первых у него написан на корпусе сможет пopтит можем купить и его этот распределенная система то есть легко горизонтально масштабироваться в плане то при discovery и по нашему опыту который мы было выбито rabbit нас хуже справлялся с нагрузкой когда моего как-то прокатчики то объемы событий испании насколько дома проводили китая стресс-тесты у нас проблем с ним не возникало просто решили попробовать взлетела там у нас вопросами справа енот на картиночке на одной из слайдов была написана что энеску интегрирован внутри перф если память не изменяет нет картинки с кодом зачем это у него есть возможность интегрировать это упрощает разработку то есть там буквально добавляется две строчки и ваш сервис можно использовать как спасибо хранилище то есть упрощает тепло честно мне на допустим описывать еще один под упрощает именно само по себе конфликту конфигурирование администрирования то есть мы себя в коде можем использовать его как как хранилище новость клиенты консьюмер и подключается напрямую к фишкам либо все-таки с маленькой там просто меня не очень получается что после мы поверх этого хранилища можем реализовать еще еще какой-то некий функционал то есть мы бы здорово говоря артанис как у нас или использовать я просто могу представить есть когда мы решили хранилище и потом добиваем выноску чтобы событий не заводится вот в данном кластере когда мы используем коллекторы для энеску там это используется для чего то есть вот сюда мы сразу добавили тот самый так топика discovery то есть люди изначально когда мы в миску есть механизм когда говорит подцепить к носку lookup и скачай в себя но пока к этому топику тут пытаться мы сделали сервис который опрашивает in a skull cap и тут же себя хранит тут там просто получается меньше и меньше и меньше сущностей то есть по сути это тот же самый энеску но сейчас с небольшим наворотов как бы сделали wrapper для in искусств какой-то logixx с да да то есть небольшой как бы можно и отдельно его вынести но вот это просто она и так если мы будем посмотрим как рацию для фильма он так уже разрастается то есть у нас есть локальная диску есть и насколько лектор есть у нас кулак об которую сходить и еще какие-то сущности тут плодить для этого это просто добавить габаритам 10 точек кода и уже получили хранилищ спасибо так здрасьте я здесь до меня зовут сергей у меня большому счету организационный вопрос вы мельком 0 и что на события не подписываются какие-то административные сервисы то есть админы не следят за день метриками которые пересылаются по иску вот это поспать ну то есть есть какое-то событие да например пользователь кликнул мы заменяем но обычно полезно измерять частоту бликов что если она проседает значит наверно что то не так с положением и должны сразу и это у нас есть отдельно про мониторинг расскажет как путь после это тоже это статистика команда сыграет сейчас параллельно с в плане мои только у нас тоже есть дубляж с одних и тех же метрик после скулу сам psy позволяет он в себе может посади метрики о том что у него хранится топик с таким-то именем а вот что мы в этой имел положим этого топика это уже как бы будет вопрос здесь у нас топик будет там . clicks мы автоматом получим клики условного но клик или тамблер от этого уже carbonite админы закатываться не могут ли они будут мониторить именно я понял то есть и админ и админ и могут собирать то же самое метрику что у вас уже есть ну своим как это сборщик против дубляж окей лет использовать этот механизм либо еще как-то еще вопросы здравствуйте и спасибо за доклад такой вопросик вот уже говорили орбите роберт еще как известно теряет сообщение ты некоторых настройках вот нет ли такой проблемой в омске проблема будет если у нас нет концу мира допустим тут как бы вопрос следующим что нам по полностью реализовать отказу сущность ему вряд ли это возможно то есть мы можем настроить так что сказать события будет перед тем как будете не в памяти тут же спасаться на диск кубинец позволит нам даже там реализовать механизм что есть у нас под прибился он подымается и продолжит достать события из она будет не цена диски но таким образом мы потеряем скорости поэтому мы пока выбрали механизм что у нас большие собаки хранится в памяти то есть у нас если под прибивается мы можем поднять ее событие но как бы мы сатель на пошли на этот шаг чтобы получить прирост по скорости еще вопрос а я поясню на предыдущий вопрос из ты не против пожалуйста на самом деле войны scudo гарантия доставки один или больше то есть у нас клювом гарантирует доставку стопроцентное горит гарантию доставка не выбрать нельзя но мы просто сами для себя поскольку носку бернес там стоит если все остальное мы для себя выбрали во первых то то то что когда на скрине может доставить он доставляет он умеет держать память а потом записывать на диск после какого-то количества вот ну во-первых сделай так чтобы он побольше записывал в память а во-вторых мы не хотим чтобы у нас вот эти очереди локальные росли и поэтому мы все время там я не знаю только на какой стороне на стороне продюсеры на стороне консьюмер мы просто откидываем эти сообщения сами ведь мы как бы сознательно переходим с доставки один или больше до доставки 0 дайте что у нас тут целое целая группа экспертов еще вопросы антоном кстати дают микрофоны антон спасибо большое за доклад во время доклада вы сказали что остается задача разделения события и логов то есть в итоге получилось решите задачу вот в планах как бы есть семью механизм идите получается что но это надо просто лезть в эту кодовую базу всего этого сайта вида и там уже как-то про витю сессии сейчас там есть только там настройки флинта то надо будет составлять класс для отправки там либо просто продублировать код ну как бы все пути решения не есть надо просто это дело делать но так как это хорошо а сейчас как решается задача например нужен таковым событию бросить лук и отправить события данную обработку это как решается сейчас это все идут во фрунт то мажилиса быть идут с тегом т.е. по сути что событие что лог это джейсон с неким тегом то сейчас спас разное суэца разным трекам разные то есть у события там тех abs точки ивент . еще что-то логов там это abs . локс . шорты и дальше в во флинте они уже учатся в разные там разные реплики манги праздника лекция то есть в коде нужно получается и лук отправить и события отправить да тогда хорошо по сути как бы это можете как бы я одним и тем же и вот как это обрабатывать и что там что там это это джейсон у нас еще один вопрос здесь наверху направлении антон среди всей продвинутой обработки событий насколько было бы сложно реализовать следующий сценарий скажем last кого и вылью на о том же топике когда с определенным qualify котором массаж вытесняет предыдущий ну например последнее состояние никого сервиса если она не была доставлена консьюмер у следующая его вытесняет чтобы снизить уровень нагрузки вот подобная продвинутая обработка событий каким образом на этой системе бы вами решалась в принципе это добавлять еще один обработчик то есть по сути он должен обратить на по идее из интересуют обработчики уровня самой системы доставки то есть сам факт публикации независимо от модификации концу мира и продюсера discarded соответствующие это в принципе но в классических messaging system такие расширения есть даже длину для того же ребята как это если возникает потребность вот именно на уровне транспорта там есть ещё несколько подобных же сценариям настраивание очередей мы можем строить себе вот этот если не ошибаюсь из коробки у них это не поддержит механизм настройках но мы можем себе диск устройте себе в сервис а дальше скорее всего то есть это некий мрак строящей фильтр разработка то есть неё со уровня самой транспортной системы ну по сути да и у нас есть время на еще один вопрос"
}