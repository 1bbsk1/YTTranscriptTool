{
  "video_id": "itXPYgnyHYQ",
  "channel": "HighLoadChannel",
  "title": "TechTalk \"Как адаптировать легаси-приложение в условиях кратного роста нагрузки\"/ Андрей Иванов",
  "views": 72,
  "duration": 688,
  "published": "2023-10-06T07:26:32-07:00",
  "text": "приветствую вас на хайлот меня зовут Григорий Я дебриэл в компании euron и у нас в гостях Андрей из супермаркета Андрей Расскажи пожалуйста чем занимаешься Чем вы занимаетесь Что интересного делаете Я представляю компанию сбермаркет мы занимаемся доставкой продуктов людям экономия время и энергию для чего-то лучшего по факту нам в этом помогает наш it-система основной системой из которой взаимодействует наш пользователь до сих пор является Монолит на рубе который у нас прошел долгую эволюции от маленького приложения Который жил умещался на одном сервере на одной базе данных и сейчас мы выросли как и бэкэндами так и базами данных То есть у нас сейчас больше 19 реплик на только 23 который занимает почти 2000 ядер мы жили изначально на обычных железных серверах у нас был достаточно производительный сервер но он действительно был один и достаточно долгое время мы его нам хватало поскольку с самого начала у нас в культуру разработки было заложено что разработчик смотрит в свой код и он знает Каким образом этот код работает на проде в этом нам помогает apm мониторинг мы конкретно используем New relic то есть Он позволяет самому разработчику знать сколько запросов на какой-нибудь Поинт На какие методы идет и он тут же видит раскладку в зависимости от количества запросов скорость выполнения его как и в коде в самом так и времени выполнения запроса на базе В какой момент как вы поняли что ваша история вы за рамки одной базы что начали делать Ну здесь было наверное две вехи которые совпали по времени в одно в одно то есть мы переехали во-первых с железных серваков в Российской облако и соответственно Нам не пришлось немного поужаться в количестве ядер То есть если раньше мы могли заказать большой сервер и за достаточно дешево то нам как стартапу не было возможности платить за огромное количество ядер соответственно мы посмотрели нагрузку которая была и заказали соответствующий сервер параллельно с этим у нас активно шла разработка то есть активный рост и Мы практически два года подряд показывали рост больше X10 то есть от x17 до почти x 20 именно в нагрузках на бэкэнды соответственно в какой-то момент мы начали понимать что данных у нас становится намного больше и какие-то вещи мы уже не можем тащить на одном сервере у нас изначально было заложено что у нас есть реплика допустим реплика для аналитиков это изначальный исторический тяжелый запросы которые выполняются долго Поэтому их вынесли отдельно и были реплики Ну Для обеспечения отказа устойчивости в какой-то момент Наверное это было как раз связано еще с появлением квида в нашей жизни у нас начало начала расти нагрузка практически плюс 20 процентов каждый день а соответственно Мы очень крепко взялись за оптимизацию нашего кода и по сути каждый день мы оптимизировали и выигрывали 17 процентов производительности а нагрузка каждый день росла еще больше и мы поняли что все пора нам учиться работать с репликами для этого в самом языке Ruby это есть прекрасная возможность то есть создать просто еще один пул коннектов к базе и мы постепенно начали учить предложения работать с двумя пулами соответственно разработчик сознательно мы выбрали несколько самых тяжелых запросов которые мы смогли вынести на базу это практически треть нагрузки на мастер у нас сняло сразу Единственное что конечно это все произошло у нас не с первого раза не всегда легко один раз мы даже разломали пастор поскольку запросы которые считались что они будут ridonly почему-то попали в readride реплику мастер устала плохо Мы некоторое время собирали данные по двум кластерам в один снова это позволило нам пережить скачок нагрузки и хватило для того чтобы обслуживать клиентов которые у нас сейчас есть но как говорится мы растем И помимо того что нам пришла нагрузка в виде людей которые понимают что теперь надо заказывать как-то онлайн продукты потому что самим невозможно сходить в магазин мы росли еще и экстенсивно Выходя на новые города открывая новые магазины количество данных в системе начало расти еще больше потому что помимо того что у тебя появляется новый магазин у него свои новые цены свои новые истоки их надо обновлять и мы начали сталкиваться с тем что у нас начали отставать реплики у нас перестало хватать производительности самой реплики Да это вот знаешь такие типичные не проблемы а истории когда стартап начинает развиваться и ему перестает хватать одного сервера базы И там начинается история что ну Окей давайте сделаем ряду он ли реплики и рейд запросы на эти реплики потом в какой-то момент приходят понимание что база уже несколько терабайт и в ней реально много данных и она просто не успевает реплицироваться начинается какая-то история с шардированием и наконец такая финальная Вишенка на торте когда запросов на запись становится настолько много что они уже все не успевают обрабатываться одним даже мощным мастером и у нас становится такое классное решение базы данных какой-то из вариантов Если же вы уже дошли до этой стадии Когда у вас мастер не справляется с количеством запросов на запись да и это как раз был тот самый кейс у нас в тот момент база была ну по современным меркам очень маленькой То есть она была там ну порядка полу терабайта она полностью умещалась в оперативку но если говорить про запись то это всегда как минимум 4 операции запись на каждую транзакцию а система наших импортов была в том что мы должны Каждый каждый полчаса на каждый магазин больше нескольких миллионов записей апдейтить соответственно нам пришлось думать что делать с этим объемом данных первое что мы пришли к разработчикам и сказали что теперь мы тротлим импорты на что они очень были не рады но и пошли параллельной работе Что начали делить данные на меньшие Бачи соответственно базе становилась лучше но все равно из-за того что как нагрузка на мастер так и на одну реплику росла все же база начинала разъезжаться немножко что мы сделали мы начали использовать уже более продвинутые инструменты мы смогли внедрить у себя такой инструмент как прокси SQL этот инструмент по сути позволяет спрятать базу данных за обратный прокси как Допустим backendl мы прячем иксом точно так же базу данных мы спрятали за этим прокси это позволило Нам очень четко управлять запросами к базе Мы понимали что теперь у нас реплика хоть и приложение думает что она обращается к одной реплике на самом деле реплик стал целый пул соответственно количество запросов на каждую реплику в стало достаточно ровно распределяться между всеми репликами Вот Но самое главное что мы получили с помощью этого инструмента что мы смогли выносить запросы на реплику без необходимости писать код то есть эксплуатация смогла влиять на работу приложения самостоятельно соответственно мы могли собрать дам и сделать срезы Какие запросы чаще всего выполняются Какие запросы поедают Больше всего времени самые длинные И что самое главное с чего мы начали выстраивать именно процесс выноса реплик то есть не какой-то разовый когда разработчик посмотрел Ой мне вот это кажется надо вынести А это как раз то что мы начали отслеживать А сколько всего вот такой тип запроса потребляет место на Мастере соответственно если у нас запрос очень долго выполняется но он выполняется там раз в сутки нам он не особо интересен конечно в каких-то случаях надо оптимизировать чтобы там выполнение конкретно этого запроса было быстрее но на общую нагрузку на базу он не влияет а также не особо влияет самое высокорейтовые запросы если они выполняются моментально они покрыты индексами то они для нас тоже не интересны нас интересует середнячки то есть которых много и они выполняются Долго но долго не конкретный запрос а именно общая сумма всех запросов соответственно мы взяли эти запросы и пошли по ним по порядку от самого долгого к самому медленному здесь надо понимать что не все запросы можно вынести если запрос допустим содержится внутри транзакции то мы саму транзакцию сломаем если вынесем его на реплику да это получается такой умный прокси человек который играет в паре с приложением и такая конструкция действительно уже позволяет отойти от какого-то Пула коннекции в приложении когда мы знаем что вот это мы ходим на первый мастер это на второе то мы ходим на реплики и делать логику балансировки уже за прокси А ну что ж я надеюсь вам понравился этот 10 минутный сэмплер семплер всего того что вы можете Обсудить с Андреем на стенде Сбербанка Куда Мы вас приглашаем мы Прощаемся с вами и с прошлого и ждём вас в оффлайне во плоти на конференции Приходите к нам"
}