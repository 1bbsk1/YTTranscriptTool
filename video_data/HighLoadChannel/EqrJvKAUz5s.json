{
  "video_id": "EqrJvKAUz5s",
  "channel": "HighLoadChannel",
  "title": "Гетерогенные сервисы для highload проектов / Игорь Мызгин (Webzilla)",
  "views": 118,
  "duration": 2110,
  "published": "2017-04-22T14:47:31-07:00",
  "text": "я немножко скажу как я субъективно вижу что происходит в рунете и в интернете расскажу пару кейсов немножко поговорю о том что мы видим в проектах совершенно разных расскажу что мы для этих проектов делаем а пусть лет я попробую поотвечать на ваши вопросы ну или попробую там технично с лица позвать своих коллег экспертов чтобы они ответили на этот вопрос на который не отвечая соответственно что происходит в интернете почему рунет это не интернет вот почему давайте посмотрим на интернет я зашел на fine ass я хуком и посмотрел текущей капитализацию facebook linkedin и крупных сайтов которые капитализированы и существует на бирже их сотни есть nasdaq есть а им в лондоне есть другие площадки где люди торгуются и соответственно большие интернет-сайты они не парятся сколько денег они зарабатывать на аудитории они не парятся сколько денег им обходится обслуживание одного запроса а не парятся каким-то глобальным показать мне сколько у них аудиторе сколько там сотен миллионов человек зашло и вот как бы им там company который стоит из 10 человек и там x от виртуальных серверов на амазоне которые не зарабатывать ни копейки продать за 10 миллиардов долларов не за гуглу например соответственно источник жизнью источник денег на который происходит все это счастье обычно это инвестиции фондов это айпи а с по соответственно этом все крупные игроки живут но и не крупный там всякий star также которые мечтают стать крупными что происходит в россия я выписываю опять же капитализацию индекса как ты заяц умерла из-за нас вопросом а какая компания может быть 3 публичной компании которая зарабатывает там в интернете rambler ну да наверно но я даже не стал искать его капитализация тоже большая очень маленькая и соответственно в рунете есть такое понятие как ценовые ножницы суть какая берем любой сервис он оказывается у сервиса своим потребителям потребители ждут что функционала бы становиться все больше и больше потребитель становится все больше и больше сам contex становится тяжелее и тяжелее технологии становятся больше и больше а user денег больше не платят соответственно поступление от юзера они не растут то есть вот простой пример берем там службу знакомств заходим на любой российский дейтинг и мы видим модель доклад freemium а то есть регистрируйся бесплатно ну там что-то отдельно ты можешь платно подарочки выделить себя поднять анкетку там покрасить себя бал дом еще что-то сделать заходим на типовой американский дейтинг и мы едем variants of scripture 1990 долларов месяц у тебя сапко скрип шин там типа 3990 тебя премиум-аккаунт принципиально разные мотивации соответственно монетизации счет чего живет ruined большинство разных сайтов кроме того что тратит деньги инвесторов частных за счет того что они зарабатывают на свою аудиторию на своем сервисе в прямую не впрямую по разному соответственно этим runet отличается принципиально от интернета большому сайт в рунете нужно сильно думать как он зарабатывает деньги большому сайт в интернете нужно думать о правильности просили за и investor relations соответственно парке saw is run эта кнопочка работа есть такой сайт имхонет ну все я наверно видели все его знают определенное время назад в нем сложилась такая ситуация что накопилось 8 10 с небольшим серверов архитектуры сервис в принципе более чем классический есть фронты есть бейтесь отдельного дата провайдеры с одним маленьким исключением весь свой собственный к метатель движок который как раз вот и заставлять ноу-хау имхонета и есть коммерческий департамент есть дизайнеры они соответственно постоянно что-то хотят то есть функция функционал сайта меняется динамически здесь из того как придумают коммерсанты как продавать аудиторию и аудитории есть такая внутри уже там любимая поговорка что надо бы код переписать но идут годы маркетологи приходит с концепцией срочно вчера вешаем новую фичу а завтра тянем новый дизайн и получается такой слоистый пирог в котором исходный код тянется с 2007 года соответственно менялись люди которые вы писали сменяясь архитекторы менялась имплементация этот код пережал с хостинга на хостинг он динамически дорабатывался но не упорядочил суда есть я пытался найти там на картинках и там в яндексе и в гугле каком такую шанхай ну не смог найти иисуса рацию поэтому представьте шанхай соответственно мы посидели посидели посидели подумали посмотреть что у них происходит то есть основные их проблемы вот это это их код который приводил к такой неприятные вещи как и for super было четко приди на кодом настолько тоже нехай label цена работать через 1 плюс тот хостинг на котором они были он был лучше тем чем те хостинге которые они были до того но он тоже не если существен ограничение тока физической оборудованием и притом мегабит полосы 6 евро неприятно и используясь из-за этого внешней сидел провайдера с которыми интеграции было весьма неочевидной соответственно и какие ещё у них были проблемы опять же да напоминаю код я честно говоря не программист вот ни разу не программист я четыре года был в армии я умею ругаться матом я хорошо умею ругаться матом когда я обсуждал с ребятами чё вас происходит внутри я узнавал новые физиологические конструкции в области русского мата то есть вот я покатил свою лексику то есть я думал что я уже нельзя удивить можно соответственно так как у них код определял инфраструктурой инфраструктура была вот 86 серверов среднем теле зация в пике было 30 а на некоторых железках 3 4 процента по циpкa например утилизации славе так получилось да и нас 2014 год пришло марки искал знаете у нас тут у как тут крымнаш сочи олимпиада мы плохо продаем надо уменьшить ресурса расходы на сайт широкий но есть проблема как раз были в попытке очередные переписать код подумать об этом оценили трудозатраты решили за 2014 года процесс отложить больше нужно менять что-то с хостингом принципиально соответственно полтора месяца думали тестировали придумали следующее что в итоге получилось бренды уехали на виртуалке причем там качество букетов динамически менялась в процессе переезда там в разы в итоге остановились на разумном количестве соответственно adata провайдером из консолидировали на крупных машинах что смогли и что важно и сделали мы перенесли все таки хранение статике хранение медики с серверов на облачное хранилище код пришлось минимизировать но слава богу на 1 мизеры там сугубо в нескольких местах поэтому была не такая боль и интегрировали в нашей сидел раздачу контента такими нюансами мы столкнулись первая проблема которому видели это стык физической инфраструктуры и виртуальной инфраструктуры мы почувствовали на себе что каждый хоп каждый там изменения пакета каждый там сотой доли миллисекунда условно говоря дает нам проблема с производительностью следуй управлюсь этого мы столкнулись облачные сервера имеют по мегабайт на тарификацию трафика вас что-то 4 плохо из-за этого у нас было ограничение мы не могли виртуализировать фронтэнда которую виртуализировать лучше всего с их кодом их дата провайдера было неразумно виртуализировать потому что на машинах фактически утилизировать больше чем по 200 гигабайт памяти и делать из а там 200 56 г б у просто не было и нет смысла экономического но несмотря на все это что мы получили итога бюджет на хостинг уменьшили там почти в два раза ну ладно 1 и 7 раза 80 + превратилась там ну в 30 + смогли начать использовать сидел для сдачи статике они раньше использовали сидела но эти сидел был отдельно а хостинг был отдельно сейчас на все стало вместе и стали использовать в инспекцию для хранения контента с таким образом сняв головную боль а что будет если вот это вот зверюга supermicro с 20 четырьмя sata шины с дисками как то умрет тоже как бы такие кейсы уже были было грустно и больно и на некое время опять отложили проблему а давайте перепишем код то есть эта проблема она не ушла она вечна но слава богу она уехала на пятнадцатый год 15 м году же можно быть на ней думать другой кейс есть такое облачный новый статус джеффу толк здесь все гораздо лучше с точки зрения legacy кода потому что у людей уже был опыт они сделали раньше такой messenger как vip и они изначально проектировали новый messenger под возможность газа цельному что берни подобной платформу соответственно что кайфа толк фоток это такой помесь условно skype с дро боксом который позволяет с одной стороны быстро общаться на любых девайсах продолжает от бесед с одного девайс а другой с другой стороны позволяет в этой же среди обмениваться любым в том числе тяжелым контентом между общающимися людьми соответствие для них мы собрали гетерогенную структуру который состоит из физических cisco со для кому подсоединения пользователей к самой системе из виртуализированных фронтов и блокировщиков нагрузки из физических серверов индексации управления сессиями и из интеграции с облачным хранилищем чтобы хранить там много в будущем еще больше материала кнопка с чем мы столкнулись несмотря на то что кот был писан но с нуля и люди понимали подо что они пишут опять же мы столкнулись с проблемой как нам поженить на быстром 10 гигабитному верните виртуальной машины и физически машины соответственно был нюанс с тем что балансировки нагрузки должны зависимости от необходимости создавать удалять фронты так как это не чистый старт менеджера такой гибрид is a passenger с облачным хранилищем то нужно формировать url и и отдавать их наружу из облачного хранилища соответственно у них собственная система авторизации пользователей их аутентификации и 3 фикации за премиум вещи и сразу было желание использовать сидел для раздачи тяжелого контента проект стартанул от фактически не очень давно в продакшене за октябрь они удвоили количество файлов раздаваемых тоха минимумах внутри между абонентами здесь не сказано эта цифра но насколько я помню по моему 800 тысяч объектов штампом в день передавалась между пользователями соответственно росту сейчас тут у них вот происходит от месяца к месяцу с неким мультипликатору это происходит у них в том числе потому что у них они тоже прошли вот такой пол этап со скрипом с другими своими продуктами о чем мы слышим когда мы общаемся с любым проектом в концепцию ребят дать им сделал вам хорошо все говорят давайте перепишите за нас наш код соответственно так как мы сегодня на хайло ведь мы проговорим про идеальный мир к сожалению идеальный мир бывает только на конференции в жизни все бывает гораздо печальнее печали супер в том что например в любой системе что-то должно масштабируется горизонтально а что-то может масштабируется только вертикально то есть какой-нибудь там найдется какой-нибудь там какая то база данных которые мы нельзя как бы там размножить вот так а придется про суд набивать памятью процессорами там дисками опять же сетевая сетевые задержки то есть бывают вещи которым более менее все равно между двумя частями системы какая задержка их можно сам сильно виртуализировать между ними поставить виртуальной switch накрутить что-то и не можно обойтись и который вот ты просто 2 железки и задала свеча разносишь подносы чан и у тебя падает в два раза производительность двери ты начинаешь мучать с этой проблемой опять же хай-лоу холодам но регулярно мы сталкиваемся с проблемой что люди хотят видеть гарантированный об стены гигабайт хранение в блочном хранилищу то есть вроде как вот мы сейчас говорим тут вот что нельзя так делать но она есть а попутно люди хотят и поговорить об эластичности чтобы она была или например вот нет у нас есть минимум там два проекта мы точно насчитали которые хотят видеть foldable которые хотят иметь возможность сами определять через какой своих uplink по какой по какому провайдеру тому что они будут отдавать соответственно такое разумное пожелание высокой доступности она когда обычно слабо коррелируется public клаудом ну и стандартный просьба от всех людей можно лесу это получить дешёвый побыстрее и вот например нам берем там мы хотим многое о псов на гигабайт эти гигабайтов должно быть терабайта соответствии с классическим трп разный подход о да сын faber чену flasher смц hp миллион денег это все как бы слегка не коррелируется вчера бесплатно потому что вспоминаю первый слайд архитектору инфраструктуры нужно очень четко понимать что эта архитектура обслужит абстрактный миллион пользователей и на этот миллион поиск от потратит условно где абстрактных там 100 долларов потому что маркетологи планируют на этих поисках заработать там слон горе до 200 надо еще самим поесть и вот эти ограничение от коммерческого подразделения от маркетинга от финансистов они очень сильно привет архитектуру между тем как правильно к режиму вот вчера и нет бесплатно соответственно на все эти пожелания у нас наш субъективный выход когда мы начинаем говорить с людьми томов что про puns так что про физически сыром и получала самом деле одни и те же вопросы а можно ли вот все что вы говорить о к этому добавить вот эту вот какие предыдущий пункт я показывал соответственно в эти вопросы мы слышим многократно соответственно мы сами тоже там посмотрели рынок и клиента посмотрите говорят ребята да как будто у public провайдер да есть amazon xrx у них в принципе есть возможность совместить виртуальную среду с физической с рядом ограничений а больше никого нету и мы решили так как вопросы задают одни и те же от совершенно разных людей называть это все счастье гетерогенным решением почему гетерогенна не гибридно тоже существует гибридно облака это смесь паблика справа там чтобы не путать правит public интеграцию между он house право ты public клаудом с с крещением физики с виртуалок и мы будем называть скрещение физик с virtual то гетерогенным решению как она устроена то есть наш имплементации public ладно пан стойки как он сказал сегодня мой коллега из-за нутаникс а openstack это с одной стороны плохо другой стороны хорошо но вот мы с ней не согласна что это плохо мы считаем что это очень неплохо но он был прав нужны люди которые вы сильно допилят стива среда обычно есть такая боль что есть люди которые рулит сетями в облачной среде есть люди которые рулят внутри площадке есть люди которые рулят тем как площадка взаимодействуют с внешним миром с провайдерами и когда это разные люди то начинается полное непонимание полная боль здесь нам хорошо в тем что у нас это почти одни и те же люди но среда на общий поэтому ею рулим поэтому мы достигаем гибкость и сетевых конфигураций опять же классические руке услуги аренды оборудования то есть вещи которые экономически нецелесообразно виртуализировать или вещи которые слабо виртуализировать их проще поставить на физике и опять же там вещи например типа вы для сетевых железок как вот например случаев о толка кластер из двух cisco asa сделать на виртуальной среде функциональный аналог будет намного дороже и намного менее производительным что самое интересное то что у нас есть сквозной мониторинг всего этого счастья за счет чего мы и там вообще можем назвать решению что мы можем мониторить что происходит и физической виртуальной среде соответственно это все построенного подтёки то есть что было раньше раньше без опыта так все было плохо как мы знаем степан степан а теперь все хорошо но как будет рассказывать мой коллега мы добавили вот эти pixiv то сейчас я об этом скажу подробнее как говорил мой коллега будет говорить через полтора часа openstack эта вещь который работает все но все работать недостаточно хорошо мы openstack года полтора немножко провели напильником сделали ряд улучшений выпускники например мы сделали слева написано то что нужно сделать чтобы появилась виртуальная машина среди а понс т.к. мы делаем один из степи запрос crystals то есть мы сделали такую апина допил то есть когда типовые последовать и действий с разными под системами области к мы заменили их на один такой фактически оля шорткат в кавычках что еще мы сделали хороший суп на стоякам но это не все что он хороший способ сделали просто как пример мы сшили наш сидел с нашим up in стеком так как мы сами там мои коллеги написали сами сидим-то для нас это было чуть проще чем интегрировать чужой сидел к чужому 140 мы допилили напильником ftp доступ к лифту и мы теперь говорим что она работает надежно и стабильную по просьбе наших клиентов мы допилили существует возможность обеспечение в контейнерах свифта version насти документов но мы это тоже немножко допилили а попутно у нас получилось что мы смогли сделать механизм записи защиты от удаления в компьютер и мы пошли по пути то есть вот все не там читать им будет рассказывать как надо пилить квн мы пошли на что по-другому тема допилили физику ssd дисками то есть у нас все живет на ssd весь компьютер и за счет этого мы гарантируем что у нас все таки не будет тех проблем у которых чуть будучи духе рассказать в к время с точки зрения производительности ввода-вывода но и например у нас две основных сейчас площадке sapan стеком автор амстердам и даллас соответственно в механизма пластик есть механизм синхронизации данных вот и вся такая что у нас есть там только один поток передачи данных мы же сделали механизм многопоточных с управлением с контрольной производительностью как она все проливается что еще мы добавили выпал снег но это уже на самом деле не добавили в понс так мы можно сказать мы отказались от даже бардового понс т.к. почему потому что у нас есть свой собственный dashboard мой веб силы ком в котором уже внесли и управлению контентом и управление сетевыми услугами там управление api адреса my location пи адрес оф как на физика кто virtual koch управление классическими услугами нашего хостинга управления аренда лицензии и мы планируем туда скоро впилить все управление нашим седину то есть за счет того что есть единой консоли управления всеми сервисами которые за действия в этой гетерогенной инфраструктуре у нас не возникает проблема когда у нас есть условно ввела начинающейся физики оканчивающиеся на виртуалку и где какие api адреса почему у нас это получается вообще а кто такие мы потому что вот сады сорвалась окна в сумках что-то нарисовано слева томата я разработчику со нагруженных систем с другой стороны я понимаю что хочется сказать кто мы мы это x5 холдинг в который входит в обвела которая красной словно ваши в сумках который является провайдером хостинга также ряд других компаний как-то например как она быстро все появляется universe он сидел то есть мы пропустим того позируя врут в северском компания пи транзит который является нашим провайдером связи наша компаний который является нашим разработчикам что такое выразил в цифрах это 5 дата-центров это хороший связано 7-1 провайдеров больше 1000 стойка управлений в которых больше 18 тысяч серверов по трактора by the capacity который в принципе наполовину утилизировано полторы тысячи клиентов и тысячи стыков с локальной провайдеров в 13 сетевых точках присутствия по миру сад ясно кто наши клиенты на ком мы в том числе тренировались вас клетки совершенно разных отраслей начинали мы на одних класс клиентов потом продолжение других с точки зрения насколько нам выносит в хорошем смысле мозг админы по задачам надежности доступности скорости отличаются две категории клиентов первое это фриксио форекс со сми так вот вот тут еще десяток разных красивых logo там еще там четыре пять десятков чуть менее менее известных logo второй класс клиентов которые любят выносить мозг хорошем смысле за надежность за производительность за минимум денег одновременно это игровики вот они попадают в категорию быстро надежно качественно бесплатно вчера просто то есть например enterprise клиенты они ребята такие они один раз подпрыгивают очень сильно хотят потом очень долго жуют внутри себя чего не хотят формализует формализует формализует потом они один раз мигнет потом все они расслабляются вы знаете что enterprise клиент этот клиент так вот spore и всплесков вы подумали об архитектуре мой подумай о миграции даже она существует обслуживать задачу ребята там из форекс ребята из игры веков у них вот нету отдельных всплесков активности у них всплеск поднялся активность не пропадает то есть они там функционируют чем-то постоянно соответственно там бывают случаи там не знаю а вот мы видели что у нас течении 40 минут в ночь с субботы на воскресенье на 5 процентов меньше было утилизации фронтам по количеству поезда вот наши графики объясните и начинается разбор полетов почему выясняется что там один из наших провайдеров и один из их и один из шапошников российских меняли стык там в амстердаме чет там была технически работа и ним и не а не ново разбираемся и тут люди которые любят задавать много вопросов о качестве сервиса за минимум денег уже по даже последний слайд и даже успел дойти до последнего слайда раньше чем мой счетчик соответственно что я хотел сказать что несмотря то что конференция зывается highload и напишите нам письмо сабжу highload мы сделаем хорошие коммерческое предложение жизнь она к сожалени такая хорошая как мы видим и слышим со сцены потому что у нас есть два существенных ограничений а деньги и то что мы уже у нас следовали в рамках своего проекта соответственно вот этих два фундаментальных ограничения они сильно делают не только текстуру которую надо от архитектуры который получается и в рамках того чтобы эта архитектуру получалось более-менее оптимально мы потренировались на разных клиентах совмещать виртуальные мощности об инс т.к. с физическими железками с физическими услугами с сединам ну дай бог скоро adidas protection и посмотрев пообщавшись с коллегами мы понимаем что данный опыт может быть интересен многим потенциальным нашим клиентам и они у я попытался не вам рассказать соответственно у нас даже есть во время на наши вопросы так зачем я была плохая презентация если нет вопросов нет вопросик есть даже да ни один значит надо не пахали простых спасибо за доклад я так понимаю вы любите очень до того представь том числе свифт и а соответственно мы сталкивались во время с проблемы хранения свифти кучи мелких файлов да там десятки миллионов файлов у нас буквально недавно был кейс с клиентом о котором более подробно может рассказать николай 2 сон сегодня будет как раз рассказывать о том как оптимизировался свифт под разные виды нагрузок . он не предназначен для высоких нагрузок у нас был кейс когда нужно было быстро быстро свист положить условно говоря x миллионов мелких файлов а потом их произвольно читать вот николай об этом кисть будет рассказывать насколько я знаю я тогда уточню просто изначально собирался тема видимо теперь придется да ну а дальше понимаете а все-таки моя функция это общение с клиентами на вопросы там коммерчески угла коммерчески а все-таки по поводу пластика его архитектуре его enter раз упоминается вас есть коллега георгием есть коллега николай где-то там которого наши перепроверяет презентацию prezi делать dance сходить и он более подробно более технично расскажет то есть дамы с этим скисли столкнулись дамы столпились даже ну там одного из клиентов которым мы недавно делали тестирование миграция к нам вот у него у клиента была задач условно говоря его вычислительные ресурсы генерируют миллионы небольших рисунков эти миллионы больших и судачков нужно положить в контейнер свифт а потом поняла обеспечить произвольный доступ и столкнулись с тем что условно говоря миллион по килобайт у копируется в swift на порядок медленнее чем там миллион килобайт это у нас гигабайт вот если кидать один файл гигабайт то существа за сосется на порядок быстрее чем миллион по мегабайту и об этом как раз николай расскажет хорошо спасибо будем учитывать лучшего помочь пара вопросов там говорилось про 5 дата центров в россии что то есть в россии мы планируем организовать во-первых седину злое потому что у нас есть довольно большое количество трафика седин которая терменируют в россии и мы планируем в россии развернуть нашу pan stik мы в россии не планируем делать colocation и аренду физики но мы верим что нам удастся поставить россии платформ и сделать так чтобы нашего pan stik у слова говорят мы стремимся к чему то есть сейчас у нас есть даллас у нас есть амстердам между ними в есть возможность там переноса репликации лена хранения с их те сейчас мы дотягиваемся свифту навесил constic мы дотягиваем на сингапур и 4 площадка у нас вот интересно что будет быстрее индии ли россия но в итоге мы планируем вас везде будет по имплементацию понс т.к. есть ли какие-то планы по выработке решений в аспекте законов а персональных данных в россии и вот этих сопутствующих вопросов да ну смотрите в запись 152 фаза 242 97 фаза дело с 9 фазы 3 ссоры подготовку же живется смотрите об этом я могу говорить очень долго я буквально позавчера в питере делал 40-минутный доклад на эту тему с 20 минутный потом секции вопросов-ответов у меня простить моя пресса вы ко мне подойти вам пришлют это отдельный разговор он вот это может сейчас запустить на прессу и еще 40 минут говорить о персональных данных и нюансах это прекрасная тема 242 фазе меня умиляет он велит не только меня но его поправят и почему это тоже подойдите расскажу это просто ник хай-лоу домику поставь нет никакого отношения и последний вопрос а сопутствующий сервис и планет добавлять ну там например load balancer и в таком духе как аппаратно или как виртуальное или вот тогда все равно лишь бы были надежные высоко надо вот так мой коллега кивает у сильных головой войдут хайр в год да да да и рвется и микрофон вот balancing на уровне тисе пи х ттп очевидно будет честно говоря просто не было от запросов поэтому функцию не выкатывали в конце таки она есть и работает отлично там фактически контролируемые конце т.к. ho прокси который тщательно следит за инстансами то есть он стоит следить за тем чтобы конфигах a proxy был актуальным вопрос следить за тем чтобы она ловит балансе лодзь автоматическое scalability тоже есть но в настоящий момент в upon стеки она считается technological превью не основная технология с учетом скорости развития понс т.к. я думаю в следующем релизе это будет уже стабильная версия следующий релиз это несколько месяцев игоря я правильно понимаю что вы в том числе являетесь автономной системой и участвуйте в маршрутизации на уровне bdp да у нас это мой взгляд тоже помогает использовать такие смешанные методы и дело в том что трафик гонимый через вас он более оптимизированным такое костное подозрение что в принципе на уровне российского трафика выявляется где-то наверно 1 входите где-то в 6 шестерку вот этого внутреннего хождения . ну где-то вот в десятку точно есть вот это вот ваш этот доклад о том что вы используете для клиентов часто комбинированное решение тут облака тут физика она может быть еще не знает вопросы некоторые добавления такие костные предположения о том что поскольку вы влияете на маршрутизацию на ваших системах это делать может быть даже правильно такой не здесь ситуация такая что мы стоим в перелива пленки у всех their полтора то есть это r1 который российский то r1 мы в пире с ростелекомом с ттк с фиксацией большой тройки вот я не знаю в перелив папку или покупаем ли morethan но опять же этот вопрос с нормальным тир-1 мы совсем у но мы покупаем суть всех тир-1 авто это это entity эта we raise на та та та это делала это цель я просто ищу 7 не помню да плюс я как говорю у нас да есть несколько клиентов которые от нас получается ул treble и сами определяют своем шпинату зависимости от"
}