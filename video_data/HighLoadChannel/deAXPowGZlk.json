{
  "video_id": "deAXPowGZlk",
  "channel": "HighLoadChannel",
  "title": "Ребаланс в распределённой базе данных.  / Владислав Пятков (GridGain)",
  "views": 486,
  "duration": 2192,
  "published": "2023-01-19T05:55:16-08:00",
  "text": "всем привет меня зовут петров владислав значит есть компании гардиан сегодня поговорим про ребаланс или как набрать состоянии угла до состояния кластер а вот немножко себе кратко буквально в двух словах я уж там пять лет работаю в компании great game все это время занимаюсь разработкой распределенные базы данных три года я был в команде поддержки занимался поддержки конечных пользователей ну и последние два года уже на самом деле больше пока готовился этот доклад уже три года занимаю позицию в команде разработке ядра пишу фича мы как компания продаем этот продукт который называется great game одноименно с именем компании это собственно распределенная база данных основанная на таком открытом продукте к коробочек знает и вот я на самом деле все чтобы ты зачем я буду говорить все касается apache играет равной степени как игритт game и вот что такое пачек найтись кто-то не слышал это такая штука которая умеет es que el умеет распределенные транзакции распределенных ранения обработка данных компьютерных всякие разные и по всем этим критериям ее можно отнести к таким продуктам как и распределенные базы данных и вот мы о чем поговорим сначала мы определимся что же такое ребаланс когда вам происходит для чего нужен потом поговорим о способах реализации как именно данные передаются между узлами какие там есть сложности далее поговорим об оптимизации которая позволит существенно ускорить время ребаланса который называется исторический ребаланс и в конечном счете на основе всех этих найдем попытаемся сформулировать что же делать нам как администратору база данных чтобы как-то подготовить наш кластер край балансу и не чтобы нас не передка не превосходила всяких артефактов когда начинает 3 баланс то что ребаланса на самом деле процесс который мы не планируем он там базе данных как то происходит и вот начнем с определению ребаланс это который процесс который приводит распределение данных к равномерному по всем углам вот предположим на картинке слева данные были как-то не равномерно расположены нам нужно процедура которая эти данные расположен равномерно ну или хотя бы постарается это сделать ладно пусть она старается но данные в базе данных обычно позиции ниже не просто как данные а какими-то участками эти участки обычно называют спарте цементом или шар даме можно называть по-разному ну что такое то нужно низкого уровень группировки данных это способ их распределения там и многое множество другое там партиции бывает динамические бывают статически если мы там создаем их при при configure им то есть говорим что это таблица у нас раздельно там на 5 партийцы допустим ли на 125 или динамически когда по внутри внутренним потребностям системы система сама понимает что нужно увеличить число партиций или уменьшить но для нас главное что эти партиции перемещаются целиком между узлами вот так мы можем идти портится представить над всеми что-то нас есть лесенка набор партиций а собственно колонки это наши узлы или ноды распределенные системы и теперь поговорим о триггерах вот был была система в нее добавили узел чистый допустим нам не хватало ресурсов на этот чистый узел должны данные какие-то переехать по кому-то законом и эти данные выбираем ну и должны переместить результате получить распределение близко к равномерному другой trigger несколько то степени обратный узел удаляем из топологии получаем систему из двух узлов ну и понятно недостающие портится на них тоже нужно распределить чтобы фактор рипли catia не менялся отлично в системах с динамическим позиционированием мы например получили большую партицию эту большую партию тоже нужно разделить и соответственно приблизить приблизиться к равномерному распределению данных и снова то мне как-то переместить на другой узел ну и для полноты картины портится в динамических системах обычно как распредели разделяются так и могут сливаться в одну если они довольно маленькие и вот теперь передем к случаям как когда и как ребаланс происходит какие разновидности бывают вот например у нас была система некоторое время мы туда добавили пустой абсолютно узел тут у нас вариантов нет нужно перемещать все данные то есть все данные на чистый узел как бы они будут переезжать но бывает и другая ситуация когда у нас была система некоторое время она жила обновлялась какие-то транзакции поддерживала и вот в этой системе узел выпал по какой-то случайности не знаю свет отключили потом администратор замечает это и быстренько бежит этот узел включает он включён в топологию система его принимает он начинает работать а как как он должен работать у него их данные отстают потому что во время того когда топология было из двух узлов было нагрузка поэтому в этой ситуации очевидно что есть какое-то какое-то преобразование если бы мы эти вот данные эти операции на грани на этот узел то мы могли вот его состоянии догнать до состояния кластер а вот это второй случай когда в принципе узел нас не чистый он содержит данные и когда мы говорим о или балансе надо отметить его сходство с процессом репликация знаете почему вот потому что и то и другое перемещает данные но когда мы говорим о репликации обычно говорим что данные перемещаются во время операции а когда говорим о не балансе эти данные уже были в системе они просто распределяются равномерно то есть вот такая разница в текущий момент мы реплицирует данные а если там по каким триггером системы и с целью эти данные расположить равномерно там происходит ребаланс ну иногда репликация по своему устройству она позволяет сама делать ребаланс в каких случаях вот когда у нас перемещается do the data entry у нас есть какая транзакция допустим в ней данные реплицируются на несколько реплик когда эти самые записи перемещаются мы подспудно предполагаем что перемещение она будет осуществляться тот же момент когда эта операция происходит но вот если у нас перемещаются какие-то logo рекорды мы полагаем мы можем отложить этот перенос то есть например на несколько реплик мы можем перенести себя на синхронно данные она другие в качестве для того чтобы их перенести мы будем использовать лак вот например как это происходит в протоколе ровд такой самый наверное сейчас если известны ровд это вообще протокол консенсуса протокол репликации logo и в чем его основная особенность в том что это это протокол перемещение записи логово во-первых в том что он основан на консенсусе то есть только несколько реплик только большинство мажоре ти реплик должно сказать да мы эти обновления приняли остальные мог ничего не сказать они могут быть неактивную на них могут быть какие-то проблемы вот в данной ситуации две реплики сказали окей мы данные обработали а текущая операция 1 отстала это не проблема для роста он не ждет ответа от 3 реплики он продолжает работать на нее данные переедут потом когда-то когда операция уже закончится ровд на самом деле предполагает он без технического описания просто предполагается он проходит что иногда намного может и не хватать иногда бывают такие ситуации когда узел либо нужно очистить лучшим мы можем перенести snapshot мы можем переместить данные полностью но после переноса вас на фото это заняло некоторое время нагрузка у нас на систему шла мы все равно должны переместить logo риккардо те тот самый лоб будет перемещен позже то есть нагрузка будет отложено теперь поговорим о реализация все что мы раньше говорили да понятно у нас есть ребаланс цели тоже есть на даный нужно переместить но переместить их нужно под нагрузкой потому что мы не можем обрываться процесс довольно долгим и даже если каких технически окна закладываем то никаких окон на самом деле в конечном счете не хватит с ростом нашей базы данных будет линейно расти просто с объемы с ростом объема данных будет линейно расти временно ребаланс и когда мы будем продолжать дальнейший диалог мы будем говорить уже а портится как как в общем анодах мы немножко абстрагируемся о том что да но дух там куча партиции скажем что вот у нас портится однопартийца перемещает данные на реплику понятно что мы можем перейти к такой абстракция незачем рисовать всю эту лесенку и вот есть partition нам нужно перенести данные что можно сделать данные я напоминаю меняются ну давайте попробуем скопировать их что делать с нагрузкой эти данные они же поменяются во время переноса давайте заблокируем и и будем переносить данные и сразу поймем что какое-то плохое решение потому что коль скоро этот баланс является каким-то внутренним процессам системы нагрузка это внешний процесс который тормозить нельзя нам не понравится такая система которая вдруг latency которая вырастает разрешим обрабатывать данные пусть данные обрабатываются и в тот же момент мы пытаемся переместить его и перенести их ребаланса мы их только только перенесли только-только скопировали какие-то data entry на соседнюю ноду как вдруг они изменились и нам снова приходится их перенести переносить таким образом процесс затягивается мы уже не знаем сколько конкретно данных нам нужно перенести чтобы получить актуальное состояние на отстающим узле постоянно мы их перемещаем они обидятся опять перемещаем ее снова в системе под нагрузкой в такой ситуации мы можем получить такой вот бесконечное отставание одного узла а нам бы хотелось как-то ограничить сказать вот теперь ребаланс закончен на верное решение желание понятно ну давайте попробуем обрабатывать эту нагрузку и на реплики на которую идет ребаланс тоже согласна до что мы можем в принципе переносить данные и на ту то там на ту ноту которой этот ребаланс обрабатывать получается два потока один поток идет на ребаланс второй поток на обновление которое мы пускаем так же как на реплики все вроде идет нормально но здесь нам понадобится версия записи потому что два потока через конкуренция мы всегда можем принести одну и ту же запись в партицию чем больше версия тем мы вводим эту версию эта версия инкрементальный растет постоянно чем больше версия тем лучше поэтому если у нас так получилось что апдейт пришел раньше чем ребаланс мы пишем апдейт балансирующая запись приходит у неё версия меньшему и откидываем все вроде просто и работает но на самом деле не очень потому что есть удалению вот как обрабатывать удаление все мы понимаем что когда запись удаляется хочется место наверное очистить на такой может быть обывательский подход очистка место не принесет нам ничего хорошего потому что очищенными записи уже нет версия вот пришло у нас удаление на реплику на которой идет ребаланс это удаление отработала в холостую поскольку ребаланс еще не донес до я запись и запись приехал ли балансом получили рассинхрон то есть обработка удалений должна быть какая-то специфическая либо либо что-то еще ну а что еще давайте предположим что мы удаление сразу не обрабатываем на реплики мы их накапливаем до завершения ребаланса накапливаем где-то в очереди все хорошо у нас есть версия заключением того что понятно что эти удаления будут применены после ребаланса и вроде как портится станет континентом состояний однако мы получаем некоторый дисбаланс из-за накопления данных на реплики на которой идет ребаланс тем более тем более что этих удалений может накопиться довольно много за время ребаланса это часы на самом деле в реальных системах ну ладно больших может быть где-то ребаланс идет и и десятки минут в больших базах это реально часы ведь куда лень накапливается много удалением где-то должны хранить какой-то резерв наверно иметь и вот если мы храним в памяти и то с большой вероятностью приведет нас thought of memory в конечном счете если храним на диске танос пояснив тон девайс если вы решение не хранить вот эти удалению есть конечно все давно придумали там стоуна то есть давайте запись не удалять давайте просто помечать как удаленно и вот в этой помеченные записи уже будет версия если в записи есть версия то мы можем использовать тот же самый закон это тот же тот же самый подход то есть обновлять на большую версию ну и отлично там стороны у нас там хранятся в коне и в конечном счете мы должны тоже будем удалить но удалить мы их можем после ребаланса там неизвестно когда чистка от там стонов может быть фоновая the sky какой-то отдельный она нужна только патчи для того чтобы комп артизи ровать пространство на диске не для того чтобы с темой так работает она отлично распознается матом стоуна в select ах вы удаленных данных не увидите если все работает правильно вот это все хорошо но ребаланс идет часами и это плохо и в некоторых ситуациях и в реальности мы не можем себе отказать в том чтобы проводить ребаланс мы не можем ждать технического окна и хочется его очень сильно хочется его ускорить давайте поговорим про ускорение и вспомни тот самый сценарий с которого мы начинали помните когда у нас в какой-то узел вышел из-за проблем из-за каких-то этот узел администратор там в течение скажем часто там 10 минут подбежал или стартанула включил в сеть почему-то выдернули из сети например и он начал работать входят в топологию на нем есть данные как мы можем с ним поступить мы можем конечно его очистить и туда эти попытаться все удалить завязанного идея плохая хочется его состоянии догнать до состоянии кластерную понимаете что очистка это еще один дополнительный процесс который там данный должен перед 6-ти вычистить здесь к она сработает быстрота manling но хотелось бы избежать для того чтобы все это проделать нам нужны операции а где эти операции которые производились над системой у нас транзакционные система транзакционных системах вообще говоря всегда хранится лог упреждающий то есть так называемый во вал архив right ahead лак те , те операции которые еще не попали в пречистенке на хранилища через чекпоинт они хранятся на ствол архиве и вообще-то нужны для каши recovery когда у нас узел там чем-то сложился ну или узел либо с база данных сложилось мы и поднимаем и лог там последние за комичен транзакции накатывается ровный из вала но эту штуку мы храним довольно долго она обычно не чистится она же не индексируется это просто какой-то файл возможно разбитый на сегмента мы можем взять данные оттуда там же там же те самые операции которые произошли над the partition просто переместить их на друга и на другую ноду и там накатить отлично мы знаем где их брать если они в конечном есть но они там есть и вот ситуация у нас есть два узла один отстаёт от другого у них есть счетчики обновления счетчику 1 11 другого 5 разность 6 нам нужно переместить как раз 6 записей logo мы перемещаем не нас нас твое счастье partition синхронно проблема вала в том что он довольно сложно читаемый у него там просто наверх записываются записи и нам сложно найти там 10 записи 50 тысяч а чтобы это сделать нам придется прям прочитать весь вал поднять его с диска он там где-то на диске лежит даже не знаем его tranque этим обычно по размеру если там эти записи мы не знаем чтобы это реализовать мы можем вставлять с какой-то периодичностью якоря эти якоря можем хранить в памяти и это еще раз ускорит способ поиска нужного нам интервала в внутри архива наиболее подходящий интервалы ищем нам нужен интервал размер которую я я как которого меньше чем 5 вот который искомый от 8 нам не подошел а тройка подошла да это будет чуть чуть больше чем необходимо однако на кота записи волованы 9 импотентом если мы эти записи уже накатывали то они просто про игнорируются ну а новые наконец мы но мы нашли способ выбора наиболее подходящего интервала не читая вал целиком ну и давайте ещё представим всегда ли этот исторический ребаланс полезен можно представить себе такую ситуацию она на самом деле очень не искусственная наверное я может показаться однако ее очень просто представить мы храним в базе данных какой-то счетчик этот счетчик постоянно инкрементироваться в результате у нас одна запись и куча очень много записей в воле много апдейтов все эти апдейте они делают вал большим для перемещения этой записи для для накатывала нам придется переместить вот сколько здесь 1105 учиться 6 обновлений 6 записей архива на вал архива нам придется переместить это плохо потому что на все 1 запись здесь хорошо работает понятие порогового значения например выберем порог в 10 да вот 10 записи решим что переносить проще прям записями 50 возможно проще записями стала наверное тоже легче тут порог работает хорошо ну вот когда мы говорим tom petty 1000 записей удобно ли переносить 5000 записи целиком либо лучше здесь воспользоваться именно куском много мы не знаем то есть нам нужен какой то еще подход ну давайте вот этот подход у нас тоже вытекает на самом деле автоматически просто мы ужинаем размер logo которую нужно перенести просто разных аккаунтов посчитаем разных счетчиков обновлений то есть вот здесь мы видим да у нас каунтер на отстающая partition аккаунтор на текущие это 7 у нас одиннадцатом 4 до 1 снасти а иначе бы нам пришлось перенести 8 записей понятно 4 меньше 8 принимаем решение в пользу исторического ребаланс и переносим 4 записи архива иначе нужно переносить партицию целиком там придется приносить 8 записей такая простая обрести к но позволяет оптимизировать количество данных для переноса что немаловажно потому что мы тут работаем сетью они перенос для нас критическую роль играет когда мы все это обсудили давайте поговорим как ребаланс влияет на кластер и почему важно иногда понимать важно уметь настроить правильно кластер вот простая ситуация у нас есть кластер в этом кластере очень много данных терабайта мы не хотим их очищать мы не хотим чтобы у нас был полный ребаланс мы хотим чтобы вывести баланс был исторически здесь пример здесь 8 конечно записи в реальных задачах гораздо больше мы просто не можем себе позволить потому что ребаланс будет занимать часы и предположили что у нас этот исторический ребаланс работает и вроде бы он работает по нашим вот этим эвристик am нашей листика говорит что там 4 записи перенести лучше чем 7 то есть приносил up a logo нет потому что мы дали мало места для вал архива так получилось вот если так получилось то никакого конечно исторического ребаланс тут не будет будет полный и полный ребаланса всеми вытекающими удаление и перенос partition решением в этой ситуации будет правильная настройка вал архива и правильно ее можно настроить только исходя из вашего war клода если ваш war плод такой что он генерит например там один гигабайт данных в минуту и мы предполагаем что администратор скажем за 10 минут добежит до сервера его перезагрузить случае чего наш вал архив должен хранить по крайней мере 10 гигабайт данных нельзя делать в чрезмерно маленьким если мы рассчитываем на исторический ребаланс и второй момент здесь мы уже выбрали исторический ребаланс мы нам понравилось это решение отлично работает отлично себя показал а мы решаем данные переносить исторически но в это время идет нагрузка нагрузка идет идет а данные мы еще все еще не перенесли этот ставишь она дана то ли медленно это лечо там с ней она так медленно и эти данные потребляет а вот но мы же должны хранить чтобы ими ей передавать мы должны заблокировать участок архива важно что понимать раз мы его заблокировать значит мы его не можем удалять и это опять дополнительный набор набор данных которые мы должны хранить и если в это время идет нагрузка вал архив продолжает расти он растет растет и в конце концов но spice лишь план девайс поэтому большой архив это хорошо но если ваша система позволяет задавать жесткие лимиты для архива это тоже хорошо задавайте их потому что архив не должен передо стать мыслимые границы чтобы мы потеряли но да и опасность такое но такого сбоя она куда более очень велика чем опасность сбой этой ноты на которую мы производим ребаланс потому что это источник данных это то надо которая хранит сейчас актуальную копию данных такая ошибка может может привести к потере данных на кластер если нас нигде больше они хранятся это нам надо вышла по на успешное в тон девать все тут нужно вмешательство администратора больше у нас классе тренировать способен и немного поговорим об обработке сбоев ребаланс должен быть таким процессом который который нельзя убить ну вернее который не может убить кластер он может он как-то измениться но кластер он не должны убить и хорошо бы еще и не влиял на его работу потому что конечно и пользователи они беспокоятся своих данных им это а нет ни инициировали не хотели чем там и так разбой источника пока портится еще есть пока данные где-то на кластере сохраняться сбой источника приведет к новым триггеру ребаланса к новому планированию это будет обработано новое планирование за планирует новый баланс и новый ребаланс пройдет так как пройдет скорее всего успешно ничего страшного ошибка на приемнике во время исторического ребаланса ну например вот как прошлый раз ему передавать из предавались логин а тут вдруг тот кто передает данные отказал говорю слушали у меня больше нет этих логов я не могу тебе передать ну это какой-то отказ сбой либо он не может их записать потому что у нее какая-то какой-то коробке на ну он выходил в принципе по сбору там как-то он пытается такая предзакатного эти данные не проблема мы всегда можем откатиться в этой ситуации к полному ребаланс просто очистив хранилище полный ребаланс всегда нас спасет потому что здесь не может быть ошибок но это просто чистая копия не ошибка на приемнике во время полного ребаланса это такая одна из каких-то системных ошибок который вообще никак обработать уже нельзя однако можно сохранить кластер в такой ситуации если у вас есть какой-то реализован кто-то watch dog какой-то сервис обработки не обрабатывает исключение нужно это надо просто исключить выключу из бластера без нее мы работать сможем вот с ней работать будет туговато потому что она не позволяет совершить ребаланс не может обрабатывать данных да еще и тормозит мастер и в заключение хотелось бы акцентировать внимание на том почему тут в общем то теперь мы знаем что такое ребаланс что он бывает во всех распределенных базах данных все эти распределенные базы данных должны с ним как-то мириться в этом latency настрою этого и не что немаловажно знаем триггере того ребаланс когда его можно ожидать мы поговорили о способах реализации можно конечно заблокировать данные в на момент переноса можно отложить перенос да ну заблокировать это плохо можно отложить перенос данных то есть перейти какому-то аналогу ванильного raw то это тоже хорошо в этом случае это работает но в этом случае процесс может быть не сходящимся потому что мы постоянно будем при нем переносить обновляемые данные мы не знаем когда наш узел перестанет отставать а можно обрабатывать процент нагрузку параллельно но тогда у нас вот эти вот версии и там стоун и нам понадобится так как я рассказывал исторический ребенок сможет существенно ускорит работу на и баланса но его надо настроить и коль скоро мы понимаем всю специфику ребаланса мы мы должны правильно настроить свой кластер правильно подогнать размеры вал архива и тогда конечные пользователи системы будут спокойно спасибо за внимание кто хочет немножко там углублено я staat ссылки оставил ссылки здесь на apache и гноить вот в правом верхнем углу в углу как раз описание ребаланса ну и там мои контакты если у кого-то останутся вопросы уже после какие-то развернуты можете мне на емейл написать я готов ответить ну и сейчас готов к вашему просто спасибо за внимание черт спасибо большое но сперва работа отнимает вот эти вот все фразы что задавайте вопросы если чего может пасть потом это мы готовились за сказать но да и так вопросы вот вопрос есть 1 прям вижу да привет спасибо меня зовут михаил спасибо за доклад вопрос по историческому или баланс и вот и сказал что я тут я тут я я слышу окей а ты сказал что вот этот вал что он ограничен да и если мы не блокируй если точнее мы блокируем записи когда-то которые пытаемся перенести сверху у нас закидываются новые потому что мы не убирали нагрузку на базу данных и собственно с конца он будет удалять записи ну чтобы сохранить свой развить максимальный и в конце концов может начать удалять записи который мы пытаюсь сейчас перенести а если их просто скопировать память положить рядом и уже их потом переносить дальше продолжать удалять с конца ну или сначала спасибо можно скопировать а сколько скопировать то есть тут у нас опять же задача того чтобы какой-то дополнительный объем памяти иметь правильно вариант валидэ можно скопировать спасибо еще вопросы так вот ведущий всех побед на да привет по себе большой за доклад было очень интересно вопрос такой как я понял исторический баланс он опираются на вал вот и хочу спросить не может ли сам вал стать точкой отказа то есть ними может ли он быть не консистентной допустим на какой-то нодди из-за этого ребаланс пойти не так и если может то как он загоняется спасибо вал может быть . отказа в прежде всего потому что volta служит задача хороши recovery то есть что значит он может быть и . отказа значит что на вал мы не можем рассчитывать стопроцентно его может не быть он может удалиться в любой момент по каким-то причинам если он удалился то мы откатываемся на полный ребаланс а вот если в воле ошибки то надо ошибки искать его чинить простая и я не представляю что значит базу данных у которых более ошибки значит она и на краш recovery так восстановится вообще вал кладутся как бы checksum и каждая запись прежде чем прыгну когда он почитается и звала она еще на бинарном уровне чиксу на свою сравнивать если там чтоб не так она ноту служит каждая нее такое мы накатывать не будем разбирать сами а можно еще вопрос вопрос такой вы сказали что для синхронизации ну вот так при балансе используется в версии записей до насколько понимаю там для всех ребрик должно быть сквозная нумерация то есть какой то вы сейчас выгнать и делаете ну у нас доверьте есть нумерация reverse для entry для записи версия для записи нет как она между узлами синхронизируется этот момент а ну на discovery узлы вернее на сборе топологии в каждый узел там внедряется какой-то указатель который делает его уникальным ну и порядка растущему то есть генерация последовательности она всегда должна быть до любой кластерной системе вот у нас там есть алгоритм который на сборе топологии каждом углу дает какой-то там ордер версию этот ордер он имеет плат версию записи в конечном счете то есть каждую запись скажет с каждого узла в принципе можно сравнить но нари балансе на самом деле у нас не встает задача сравнивать версии с разных узлов обычно когда мы партицию тянем ее тянется одного узла нас там задача сравнить запись с того же узла просто над двумя потоками приходят они могут конкурентов получить не норы с разных углов а с разных потоков 1 через ребаланса другой получается добрая вторую реплику со второй репликой отвечу с 3 ну условно говоря мне или данные с одного угла сравнивая при балансе обычно но если в темных партийцев просто порою на конкретный реплика может горам до разберем конкретную время реплику просто надо обновлять разным потоком спасибо если еще вопросы а так сейчас подойду саша привет спасибо большое за твой доклад у тебя вот был слайд где было написано о том что если происходит ошибка при ребаланс тенге то надо выводится из кластера и тут очевидно существует опасность того что каскадная огромное количество узлов будет выведена из кластера и но мой вопрос собственно что какие методы используются для предотвращения такой ситуации но если мы говорим про ребаланс и ребаланс обычно происходит там на каком-то конкретном конкретном узле который наверное раз на него приходит приходит 3 балла значит данные где-то еще есть кажется что для ребаланс от не очень актуально но вообще да если везде происходит ошибка туда это скорее всего никак тем более непредвиденно скорее всего может привести к катка одном отключению надо будет разбираться но это не задача уже ребаланс то есть вы понимаете что такое не обрабатываем исключение лучше уж гасить узел чем продолжать работать с непредвиденной какой-то ошибкой гасим кластер разбираемся что думаете не было такого поддержки три года вижу еще две руки спасибо большое за scanspeak транзакции произошел давайте такой вопрос допустим надо упала и допустим там администратор не помешало сразу вы там здесь минуты и понять и там допустим выходные лифтинг почему-то отвалился и накатил накопился талантом не знаю напал терабайта допустим и потом все же там через какой-то день обратно подключаем это наду и получается вот эту island будет полтора то накатываться и то есть будет паки робкой и вопрос то есть можно кит комбинированный под водой из позови чтобы каком-то случае и слона накатался а допустим другом случае мы полностью opel на который этого как мы используем комбинированные подходы в центре выбора того будет ли перенос ствола или нет стоит вот это вот обрести к сейчас вот этого turistico то есть если у нас размер вала он больше чем полный размер партийцы то мы переносим портится иначе меньше в общем случае для ноды делается такой выбор для каждой партийцев каждая портится может и ребалансить либо полностью либо вот через вал то есть это есть комбинированный подход что пасе большое спасибо есть время на последний вопрос отлично привет спасибо за доклад вопрос про якоря очень простой конфигурационный параметры тот он меняется в онлайне в оффлайне или же мы можем там я не знаю менять только после ребута всей системы в какой системе ну вот смотри у тебя раз лапочек знает не меняется вообще то есть у тебя всегда якоря выставляются с каким то определенным до в а пачек найти река и якоря восстанавливает выставляют на каждом чик pointer описывается вал если чекпоинтам происходит чаще и он был он там по своей системе происходит у ее горячее от ставится чаще в общем есть некоторая зависимость с количеством грязных страниц и выставлением этих дикарей я думаю это правильно потому что мы потому что это как бы связано с и генерация архива с частотой обновления супер спасибо огромное на этом"
}