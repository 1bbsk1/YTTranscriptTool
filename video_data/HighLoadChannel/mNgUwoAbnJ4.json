{
  "video_id": "mNgUwoAbnJ4",
  "channel": "HighLoadChannel",
  "title": "Про историю и будущее поиска / Андрей Аксенов (Авито, Sphinx)",
  "views": 2880,
  "duration": 2962,
  "published": "2023-04-28T06:19:52-07:00",
  "text": "в абстракте было обмана написано что он про историю и будущее поиска но понятное дело про будущее будет там полторы полтора слайда фантазии в конце и это только в том случае если да я до них этих полутора слайдов в принципе доживу посмотрим что там раз рассказывать в этом вашем так тормозит все значит системы всё не синхронно с точки зрения наблюдателя но и уменьшил консистенции достигается и все три экрана вроде В итоге показывают одно это хорошо так вот что там рассказывать-то Понятное дело что как только доклад про историю поиска то видимо основной трикаут из этого доклада для всех присутствующих должен быть такой когда сдаёте корпоративные Ну вот обратно значит когда увольняетесь и улетаете куда-нибудь а то очевидно надо историю поиска затереть и особенно внимательно относиться к порнотабам в этот момент потому что Ну мало ли что произойдёт Вот наверное такие ожидания у всех кто не пришёл на доклад Возможно у половины тех кто пришёл примерно такие же здесь была ещё одна смешная шутка про пост версия альтовиста вот я ее к несчастью забыл неожиданно для себя хоть подготовки Я понял что не то чтобы у меня как бы три минуты есть на то чтобы вот там грубо говоря осветить все вопросы про историю так называемого поиска полнотекстового и остальные 47 минут можно тратить чисто там на диалог интерактив смешные игры в бутылочку и в целом работу с залом а наоборот как обычно времени не хватит Именно поэтому я его бездарно тратил Почему так потому что ну в общем-то полнотекствый поезд на мой взгляд это грубо говоря такая особенная база данных со своей какой-то странной э забавные спецификой То есть это мало того что база данных какая-то внизу особенная база данных со спецификой повторяюсь не совсем такая как и но и ещё просто полная гора разнообразной магии которая вдобавок вот этого слоя база данных в этом самом полно текстовом поиске есть таким образом вместить всё это и в 30 минут и в 50 и даже в 4 часа это некое такое занятие совершенно бес но я тем не менее как настоящий тупой ноупорный спортсмен все равно попробую вот ну и понятное дело тут некая личное видение истории там за последние несколько десятков лет я естественно из этой всей истории выбрал наиболее интересующие почему-то лично меня штуки если у Вас как бы радикально другое видение но видимо вам надо делать Просто следующие доклад такой же и как бы это конкурировать не конкурировать А панировать ну в общем перерегиваться с моим посредством переписки это полагается о чём непонятно всё ещё что значит поиски как немного конкретизировать а каких скитаниях вечных поиска внезапно понял что доклад надо можно было назвать от трёх слепых мышах потому что в принципе по большому счёту до сих пор тыкаемся как слепые мыши во всех интересных в них трёх больших кусках поиска вот значит они типичный поиск любая по поисковая система она стоит на трёх китах но киты слепые из палочками а матчи лингвистика и ранжирование матчем - это то как мы документы куда-то там сохраняем как-то там достаём лингвистика - это процесс который в общем-то и при э сохранении этих документов и А ранжирование нам даёт Ну дополнительно обрабатывать тексты как-то более интересно чем просто значит буковки сохранить и ранжирование про них и попробую поговорить как оно значит из глубин веков до наших дней развивалась доклад обзорники хардкора не будет все как обычно мой любимый образ из голубых из глубины веков он был вот такой это как первая гутенбергская Библия поэтому я не уверен если там в хвосте конкорданс который по каждому ключевому слову показывает на нужные странички Вот но неожиданно в этот раз я какие-то другие ключевые слова ввёл в Google и поэтому нагуглил Другую картинку под названием самой первой поисковой движок мира и неожиданно Это примерно Вот такая вот картинка получилась это библиотека минус седьмого века до нашей эры на глиняных табличках и что характерно уже тогда человечество изобрело ключевые слова потому что дескать по автору группировать все вот эти вот книги было недостаточно хорошо они были сгруппированы Passat jumpton то есть практически древние вавилоняне или кто там с Турбо не пал А шурбанил ассирийцы ничего не знаю про историю а на мой деле концепты внутри своих креативов доставали из них ключевые слова и натурально это библиотека была организована по концептам по ключевым словам причем мало того что по ключевым словам который не просто встречаются а которые так сказать хорошо описывают что же там внутри вот так вот выглядело значит самый первый поиск в мире ну причем Я думаю что это не они придумали то есть был Ещё какой-то более ранний вариант там версия 0.1 Вот но просто уже не сохранилось вся глина разбилась а Понятное дело что Ну не первый Да но компьютерризация у этого поиска в принципе не очень и нам интересно в рамках всё-таки it-конференции как вот это всё на компьютере укладывается на компьютер оно укладывается примерно так значит вот типичная картинка вот типичный отсчёт с типичным воспринта типичных разработчиков где-то около поиска какие-то графики какие-то эти заусы тизаурусы э заголовки а что если мы проиндексируем только заголовки о чём и поищем по всему подряд а что если простым А что если не простыми слова все знакомые вот натурально не знаю занимается ли хоть кто-нибудь информационным поиском в зале Надеюсь что хоть один человек есть но слова-то вроде все знакомые не дадут мне коллеги наверное соврать блок precision Run прикол ncg только почему-то нету А так в принципе всё нормально значит внимание вопрос какого года вот этот вот спринт в котором в принципе есть всё что-то промаченко ну есть разные виды мачетинга тайтлы отдельно фулте кстати отдельно лингвистика довольно развитая там если вглядываться упоминаются и там и стриминги ты заурсы и синтаксические анализ предложений разваливание их на подлежащее сказуемо и так далее и тому подобное Это довольно серьёзная лингвистика Ну и конечно наши две стандартные любимые метрики присяжен рекол все понятное дело уже чувствуют крысу и предполагают что Видимо это ну как бы если предыдущая библиотека была минус седьмого века до нашей эры то наверное гипотеза должна быть что -3 века до нашей вот этот скриншот Если хоть кого-нибудь хоть какая-нибудь гипотеза Сколько лет прям Пальцем в воздух или там куда получится 18 век но шрифты были все же лучше в 18 веке более витиеваты еще гипотезы подсказки из 20 века 68 практически идеальное попадание значит редактор по имени колер написал Вот такой отзыв я уверен на этот скриншот что слова все красивые но как бы фамилия у тебя салтон это скриншот из работы с сатана шестьдесят пятого года шестьдесят пятого более того у него там еще есть ссылки есть на предыдущие работы про А значит поиск по так сказать ключевым словам и вот это вот это отчёт уже работающий готовой системе полнотекстового поиска натурально все ключевые слова знакомые вот вообще всё то же самое Чем мы сейчас занимаемся выглядело Это примерно так это не салтон это хрен знает кто шкаф на котором всё это запускалось выглядят вот так IBM 7094 или что-то в этом роде надо понимать что шкаф за которым Сидит мужик это не компьютер Это клавиатура компьютер - Это всё что за вот эти вот бобины и не только этот ряд весь задний ряд это тоже компьютер компьютер - Это довольно мощный 100-200 тысяч кило флопсов то есть иными словами если таких компьютеров поставить 30.000 штук то получится ровно одна мобила вот Ну в принципе как бы Прогресс какой-то есть И в этот момент закрадываются определенные подозрения то есть в момент когда все вот эти вот слова весь этот отчёт спринте проделанным в области полна текстового поиска на одной тридцатитысячной мобилы мы начинаем подозревать что наверное что-то отличается наверное что-то всё-таки за эти 60 с чем-то лет изменилось кстати э в момент когда я считал что с пятьдесят седьмого года по шестьдесят пятой произошло 65 лет а шестьдесят пятого пятьдесят седьмой Ну там работа пятьдесят седьмого шестьдесят пятого складываем вместе симметрично получается Я думаю это что-то значит но я пока не понимаю что конкретно это значит Ну видимо именно в этом году надо было делать Этот доклад в следующем уже такая нумерология не получилось бы что же поменялось за примерно 60 лет 57 65 какая разница округлим до примерно 60 поменялось Я думаю конечно же рептилоидами с планетой не беру больше некому Вот и немного ещё АйТи учёными но тем не менее три части матчи лингвистика ранжирование часть номер один промачиваем что я считаю значит самым важным достижением в матчанге за последние Ну 60 лет с чего всё когда-то начиналось Понятное дело что когда у тебя одна тридцатитысячная мобила Ты экономишь каждый пик и системы и 60 лет назад и 40 лет назад и даже 20 лет назад ещё 20 лет назад Они вообще говорят на такие сложные концепции как Где находятся слово в документе И тем более как найти точное совпадение фразы не замахивались вообще поначалу всё ограничивалось много чего ограничивалось булевым матчингом не координатным так называемым поиском когда ты в индекс не сохраняешь ни черта кроме просто факта наличие ключевого слова в документе и работала некоторое и в какой-то мере работала структура которая называется красиво сигнальчай файлз но на самом деле по большому счёту просто тупо битва по каждому ключевому слову мы храним натурально бит моб который показывает Есть ли этот ключевик в документе или нет Вот черт знает когда думаю не очень важно придумали естественно концепцию и под названием так называемые инвертированные файлы для позиционных так называемых координатных индексов по большому счету у тебя другого варианта нет но естественно б деревья придумали еще раньше и блин Забавно опять-таки вот один из интересных таких выводов личных одной из интересных открытий лично для меня в ходе подготовки доклада было в том что люди разные штуки изобретали вообще не в тот момент когда я предполагал что они их изобрели что-то оказывается изобрели натурально там на 30 лет раньше чем казалось а что-то наоборот на 20 лет позже чем мне казалось то есть мне казалось что это уже как бы известный факт последние полвека а фиг вам его избрали оказывается 10 лет назад довольно неожиданно так вот это один из этих интересных фактов довольно много времени ещё 20 лет назад Вот буквально там В девяносто восьмом году А вышло некая такая программная работа которая так сказать в гроб вбила что Пацаны ваш индекс полнотекст твой есть единственный метод нормально хранить и этот метод называется инвертированные файлы так называемые когда к каждому ключевому слову есть ассоциированный список документов хорошо пожатый и список позиций тоже хорошо пожатый не играйтесь в эти ваши долбанные сигнатурные файлы не играйтесь в эти ваши долбаные бы деревья как бы вы их не устроили ни в коем случае не складывайте соответственно полна текстовый индекс в базу данных чему нас учит пост говорит это работает неэффективно только в девяносто восьмом году Как ни странно эти игрушки более-менее окончилась как когда вышла программная работа которая всё это померила И относительно убедительно доказала что дескать так делать нельзя и инвертированные файлы во всех натурально разумных случаях побеждают тем не менее что сейчас вот большой Прогресс за первые 30 лет это мы отказались от битмапов и придумали так называемый инвертор файлс А сейчас вдобавок к просто банальным инвёрнут файлс инвертированный файл я повторюсь это просто довольно тупая штука это просто список растущих номеров документов до kyd 1 3 5 17 29 и так далее для каждого ключевого слова с этим работать в принципе можно для небольшой коллекции в один тире 10 млрд документов в для большой коллекции с этим работать как ни странно уже опять нельзя то есть на масштабе веб-поиска к несчастью приходится изобретать ещё всякие приколюхи вдобавок к этим самым базовым инвертированным файлам Но это в целом важный момент подавляющая часть поисков в мире она всё-таки не масштаба всего Веба веб поисковиков и ну не так много понятное дело там Google понятное дело бинка понятное дело 17 разных китайских поисковых машин про которые мы ничего не знаем потому что не владеем языком Яндекс в конце концов и так далее Вот Но их довольно немного сильно больше поисковиков они как бы в эти проблемы не упираются соответственно де-факто стандартное решение это инвертированные файлы просто списки иногда для ускорения всякого бы дополнительная акселерирующий битмапы в этих инвертированных файлах иногда в эти инвертированные файлы по каждому ключевому слову для того чтобы супер быстро вынуть из индекса самые лучшие результаты кладут дополнительные дополнительные топ к результатов по той или иной метрике возможно по некому статическому ранку есть у вас миллиард вхождений слова я думаю что как бы в современную эпоху у нас будет миллиард вхождение слова в все страницы всего Веба это кажется уже э однозначно вот и это много Ну давайте топ 1.000 по какому-то статическому ранку возьмём и положим в начало этого списка на миллиард документов Вот так оно в принципе и работает а это изобрели и повторюсь за последние Ну грубо говоря 20-30 лет там 20-25 лет до этого методика хранения была какая-то другая ещё один момент который мне кажется Ну придумали его наверняка давно Но более менее популярен становится только в кавычках в последнее время и то не вполне все Спаси Господи чуть менее чем все движки которые на рынке есть они скорее всего Как будут по умолчанию скажем так мочить документы либо чтобы было Хоть одно ключевое слово То есть через Вульф оператор ор либо через оператор end Ну вот практически там с 90-процентной гарантией движок будет работать именно вот так это опять-таки как нам рассказывает веб-поиск не и очень хорошо потому что извините если пользователь задаёт запрос из пяти слов в которых одно супер редкое какое-то то документы в которых совпало три относительно частых слова в виде идеальной фразы Ну наверное надо вернуть но и документ в котором совпало только лишь одно суперредкое слово тоже надо вернуть но документ в котором совпало два предлога возвращать не надо вот это штука которая так называемая в это Дент мягкий кворум она придумана тоже относительно недавно ну то есть 20 лет назад я подобных работ не видел и тем более применение в бою теперь естественно поисковики все подряд работают уже давно примерно так все остальные когда-нибудь Я надеюсь подтянутся спойлер мой маленький карманный поисковичок пока еще работает не так вот но это не я виноват а соседняя группа качества и ранжирования по так как технологию мы им честно говоря давно обеспечили они в принципе это могут внедрить движок позволяет А вот пользователи пользуются не все второй гигантский Кит в части матчанга которая опять же произошёл как мы эти данные которые Ну матчем списки документов в списке позиций и так далее как мы их физически сохраняем на диск И я как бы регулярно люблю запустить шарманку о том что сжать это важно Вот опять её запускают натурально первые 40 лет первые 40 лет ещё в 2000 условном году было модно хоронить индексы жать их каким-нибудь дурацким битовым кодом типа хаффмана голомба и так далее чем он дурацкий сейчас значит расскажу и покажу на пальцах на палочках и квадратиках на а далее Что интересно человечество чуть менее чем 100 лет назад придумала нехит нехитрый кодек под названием very был int это который по байтовой и в котором верхний битый что-то значит продолжаем высчитывать из потока или нет А я потратил натурально часа два или три просто пытаясь его датировать и не смог Ну то есть когда первая вот упоминание про нехитрые довольно метод его ж когда-то кто-то изобрёл всё-таки ну его как бы не в 1812 наполеон-то придумал вот ну я не смог В общем лучшее предположение которое у меня есть это то что Честно говоря вот этот very был int можно натянуть некоторым образом на Такое сильное упрощение довольно более сложного сильно более сложного даже пожалуй куда хаффмана А кот хаффмана оказывается придуман аж 70 лет назад ровно в пятьдесят втором году Ну наверное и в реале был тогда же примерно придумали А тем не менее за последние примерно 20 лет опять-таки в основном с подачей веб-поисков но неожиданно в 2006 году победный метод придумали по колоночной базе данных про которую никто не знает потому что она называется не Клик Хаус аммония ДБ Вы что-нибудь слышали про монит ДБ или мать её X100 это там по-моему предыдущая версия системы называлась тексту или следующая Ну скорее всего Нет ну не клика уже Поэтому тормозит но тем не менее а основной турбо-мед сжатия всякого воздержание по колоночных данных в базах с одной стороны и неожиданно физического сжатие индекса в полнотекстовых системах с другой стороны придумали вот там и тогда и довольно недавно по историческим меркам буквально вот 15 лет назад Кроме того значит опять-таки лет семь назад вышла программная статья которая рассказала всему миру о том что если вы вот это вот э вот этот хитрожопый Метод имплементируйте при помощи векторных сссней инструкций то он вас будет работать в 100.000 раз быстрее всего остального и станет де-факто стандартом Вот но на самом деле А естественно проприетарные реализации ровно такие на ssk и так далее Ну по-моему они где-то с 2008-2009 года уже минимум есть а то раньше просто Они всегда жили В дебрях э соответствующих компаний этим занимающимся там грубо говоря внутри Яндекса было внутри Гугла было и т.д и тп потом постепенно и А на публику проникла это ещё один интересный момент как ни странно опять-таки такой небольшой интересный На мой взгляд кусочек Как ни странно Э Поиск такой фронтир Ну понятно дело э у Гугла большая капитализация есть еще запатентовать где определенные работы и определенные техники проникают в публичное пространство Ну довольно не сразу с задержкой в несколько лет здесь прям наглядный пример сибреализации супер значит эффективного кодека для сжатия кишок данных они вот протекли там лет через 9 только на пальцах в чем суть попробую потратить на это все-таки одну минуту типа почему это важно как известно компьютер А и B А работает байтами б ужасно не любит условные переходы Вот это в принципе всё что надо знать про компьютер современный если бы хотите знать про него одну вещь байты у меня тут обозначены квадратиками а условные переходы палочками внутри этих квадратиков так вот хаффман который довольно прикольный битый кот он битовый он требует чтобы вы поток сохранённых куда-то цифрах декодировали по одному биту за раз или по два или три или по пять каждый раз переменное число бит и каждый раз видвление это возможно довольно агрессивный метод кодирования Но это к несчастью в том смысле что жмет хорошо но это неэффективный метод кодирования за счет того что ветвлений получается до этой матери и соответственно работа с памятью ведется блин битами это тоже неудобно для компьютера соответственно War int который всенародно любимый был до вот этой вот группы методов А про которые если кому сильно интересно любое количество деталей я расскажу в кулуарах а основан на том что делаем мы тоже самое только всегда кодируем в поток по одному байту за раз и это получалось там ну не то что в 100.000 раз натурально кратно быстрее чем А кот хаффмана а вот это вот группа методов с четвёртого пятого шестого года ещё вдобавок говорить смотрите вот у нас вар-инт Мы каждый байт должны прочитать из потока посмотреть наверх и заветвится А давайте этого не делать давайте мы все длинные следующих четырёх значений например положим в поток одним байтом ма сказать такой И после этого Прочитав этот байт ветвиться больше не будем декодируя следующие много байт это работает ещё более эффективно При таком же коэффициенте сжатии Ну и дальнейшее развитие А давайте положим такой один маркерный байт на не там четыре значения А на блок из 128 значений вот Казалось бы какие-то довольно тупые техники тем не менее натураль- это прорывной результат они очень сильно ускорили э ну собственно и построения индексов и поиска-то Да это ТТ и ТТ один из прямо прорывных результатов Хотя Казалось бы банальная возня с битиками и бантиками и просто так сказать оптимизация ответвлений довольно забавно Что за цифры мы вот таким образом кодируем что там лежит а здесь Изменений нет последние 60 лет Понятное дело что от бедности мы туда клали только номера документов 60 лет назад и 20 лет назад мы туда тоже не ну 20 лет назад туда уже всё-таки начали класть позиции а 30 лет назад плавали номера документов 20 лет назад все научились в индексе класть позиции не только номер документа 1 2 3 но и тот факт что слово упомянутая в нём встречается На позиции номер 17 и вот это вот соответственно в индексе типично и хранится далее опять-таки определённый разрез между такой Квантовый скачок если угодно между веб поисковиками и всем остальным почти всему остальному на самом деле к Каждому каждому вхождению какого-то слова в какой-то документ не какая-то дополнительная информация не нужна вот этих вот док ID и позиции внутри документа достаточно совершенно для того чтобы решать все практически задачи и ранжирования и лингвистики и т.д Ну а тем веб поисковики в стремлении выдавливать все возможное качество вроде бы все еще раньше Точно складывали думаю сейчас это все еще так Еще вдобавок рядом с каждой позицией складывать дополнительную информацию типа А какого был цвета шрифт размера какая-то это была часть документа А может быть классификатор показал что это общий на миллион страниц header Footer А может быть классификатор показал что это прям ядрёный мясной контент вот те самые 140 байт цвета которые в мегабайтной страничке скачанной twitter.com конечно же собственно нам и важны вот это вот всё такой пайлот он опять-таки в некоторых редких случаях есть веб-поисковики если что считаю случаем редким их подчёркиваю мало это случай гигантский который которыми мы все повседневно пользуемся но тем не менее это штука относительно редкая если считать грубо говоря там инстансы всех поисковичков мира Вот соответственно здесь пока стандарт - это вот документ а ID и позиция и конвергенция с базами данных определённая датировать В какой момент произошел прорыв не могу но по личным ощущениям давным-давно было такое был была такая концепция я ей точно страдал Но кажется не я один что поисковик это некая такая отдельная штука на шлепка вдобавок к уже существующей какой-то базе данных основной в которой хранятся документы Ну понятное дело постепенно стало ясно что это не так что иногда э твой поисковик - это и есть твоя база данных и тут Конечно все такие ну владельцы терабайтных кластеров и ластики такие тру тру конечно ещё не хватало эти терабайты дерьма дублировать два раза им как бы только в одном месте место и это место естественно сегодня и ластик другого нет большой сдвиг парадигмы здесь соответственно в чем если раньше получается что у тебя полна текстовый поиск по большому счету головой болел Только вот за эти там тексты токенизацию ранжирования всю вот эту черную магию то сейчас поиск и уже довольно давно последние натурально лет 20 минимум честно говоря должен еще и себя косплей такую маленькую А лучше большую и интересную распределённую и т.д и ТТП Слава богу хоть распределённых транзакций и без распределённых джойнов 100 млн на 100 млн таблиц а но тем не менее базу данных и естественно как без джисона внутри этой базе данных Все любят gson а какие здесь свежаки за последние 5-10 лет покопавшись в голове я как ни странно понял что они есть какое-то добавление которое пришло которое На мой взгляд интересная которая прямо может обеспечить некий Квантовый скачок и прорыв это БГ возможность эффективно складировать в поисковики а просто банально массивы флотов или углублённых флотов Почему Потому что не рассечки рейтинге и вектора которые ты в эти массивы флотов складываешь Ну возможно чуть позже про это еще расскажу Итого первая часть из трех пол доклад как обычно прорывы за 30 лет научились хранить ловчее избавились от детских дурацких замесов Давайте в Б3 всё сложим и научились сжимать это сильно лучше Казалось бы ничего особенного но честно говоря эффект термоядерный современный индекс - Это значительно более эффективная штука чем было раньше часть вторая из трёх лингвистика значит первая картинка которая меня в Гугле за 10 минут до начала доклада выпало она выглядит вот так дескать как бы что-то видимо ассоциация коллективного бессознательного на лингвистику это нечто вот такое то есть э специалисты так сказать которые одновременно говорит на этом 40 разных языках и всё такое и языки в принципе тоже мы все узнаем Значит на самом деле лингвистика конечно же выглядит вот так а не как на предыдущем слайде это значит товарищ зализняк у которого основная деятельность в общем-то была непрознание 40 разных языков А про исследование древнерусского Вот Ну а соответственно корпусные лингвисты обычно выглядят вот так вот то есть тут некие есть разрыв между ожидаемым и настоящим образом как это всё устроено мало что знаю про лингвистику поэтому цинично Ну шутка что-то знаю но не честно говоря не очень много поэтому цинично набрасываю что основа не изменились натурально за 60 лет концепция под названием Давайте как-то развалим текст на Отдельно взятый ключевые слова на отдельные токины придуманы естественно ещё 60 лет назад все концепции под названием отдавайте что-то сделаем с этими словами давайте возьмём словоформу Давайте приведём её к корню Давайте протекаем части речи давайте сделаем wordsensityation то есть снятием анемии по-русски давайте давайте всё это Придумано натурально если занырнуть вот в эти статьи древних в шестьдесят пятого года в какой-то мере упоминается Как ни странно другое дело что вычислительных мощностей постоянно не хватало и всё это и достаточно просты на э-э в данный момент какие-то штуки они рождались натуральному в муках хронология Вот такая залезняк свой словарь бумажный причём и изготовил 77 году я не знаю насколько это прорыв в мировом э так сказать искусстве подобных словарей Это натурально был прорыв для русской морфологии потом постепенно его оцифровали и по большому большому счету на этом словаре в его цифровом виде вся любая современная русская морфология я основывается На чем основывается английская Не знаю думаю на каких-то аналогичных словарях примерно тогда же 40 лет назад придумали начали придумывать так называемые стеммеры для английского языка стеммер это всего-навсего штука которая берёт ключевое слово и хвост у него отрубает так чтобы э падежи снять бежать бежавшие бежал вот этот хвостик э убирается и возможно заменяется чем-то другим Как ни странно Здесь тоже Прогресс какой-то довольно медленный Ну то есть вот всенародно опять-таки любимый на данный момент сноубол так называемый микро язычок который сам по себе никому не интересен но который используется как язык описания для натуральной там 40 разных простеньких стеймеров для чуть менее чем всех интересных языков на планете а 20 лет обратите заняла дорога от того чтобы придумать первый таймер портеровский э-э Ну как бы сноубол тоже господин Портер придумал А 20 лет от того чтобы придумать первый стеммер для английского языка и для того чтобы немного это все дело обобщить и сделать некую коллекцию стеймеров которые сегодня еще через 20 лет натуральный весь мир пользуется Как ни странно вроде бы кажется что задача такие нехитрые но как бы решались поменьше меня они почему-то годами десятилетиями Я полагаю не хватало не вычислительной мощности ни к корпусу объёма корпусов данных и соответственно Ну возможности людей даже сидеть и всё это обрабатывать Сегодня 22 год ситуация примерно такая морфология На мой взгляд скорее решённая задача чем не решённая хотя всё равно естественно постоянно ошибается Потому что постоянно полагаются какие-то новые слова неологизмы входят в строй какие-нибудь просто там название брендов Спаси Господи кто знал в 1078 году как там правильно склонять крейси это невозможно даже предугадать было это и часто предлагать невозможно вот а склонять надо и правильно или монетизировать тоже надо поэтому процесс такой с постоянными улучшениями А вот это вот работа с отдельными ключевыми словами которые стемпинг климатизации т.д и т.п это если мы задумаемся на секунду по большому счету работа некая модель если в терминах марковских моделей это работа модели первого порядка мы смотрим только на сам токен ни на что кроме этого долбанного токена мы не смотрим но слово-то всегда важно в контексте в каком-то вокруг него стоящем так вот опять интересный момент ещё 20 лет назад был всякие резиночки по поводу того что давайте рассматривать язык не как модель первого порядка А хоть какие-то энграммы э смотреть и там хотя бы на пары слов возможно тройки слов марковскую модель строить короче ну не примитивную первого порядка А хотя порядка второго третьего опять же Google emgrands через какое-то время сделали и т.д и ТТП но тут странный такой момент как ни странно э на мой взгляд Ну мне кажется интуитивно я не специалист но мне кажется в этом есть дополнительные положительные сигналы для ранжирования и за 20 лет могло бы стать стандартом пока не стало ну то есть де-факто стандарт это на данный момент это все равно работа с отдельными токенами взяли текст развалили его на отдельные слова особо не глядя там сделали морфологические гипотезы и в принципе этого достаточно сегодня опять же уже всем доступны натурально всем доступно там на гитхабе в три клика можно скачать довольно клёвый Э не не сетевые модели языка которые тебе по сравнению с моделями двадцатилетней давности ещё больше интересной информации дают Ну то есть там э модель двадцатилетней давности тебе давала по паре слов допустим какую-нибудь вероятность Э что это пара слов случается или условно вероятность появления одного слова после другого современная нейросетевые модели чуть ли не Войну и мир способными реализовать и её точно предсказать то есть грубо говоря если ты возьмёшь э простые если ты возьмёшь Войну и мир вымораешь из неё каждая пятое слово то скорее всего гптшечка которая Войну и мир помнит наизусть И вообще всё помнят наизусть она скорее всего тебе пропуски восстановят кажется анализ текста при помощи настолько могучих моделей должен давать какой-то сигнал Ну вот местами дает местами уже дает об этом за следующие две минуты скажу но он пока не повсеместный вот эта штука пока повсеместно не проникла повсеместный поиск сильно более тупой и на просто токенах просто со стеймерами или монетизацией и так далее В общем есть еще над чем работать и решают Понятное дело что тогда что сейчас дата с этой нельзя построить словарь на 30 слов словарь должен словарь Железняка - это 150.000 корней и Если я правильно помню чуть ли не 10 млн возможных словоформ которые из него легко генятся Ну нейросетевые модели ещё более злые вещи Вот соответственно ещё один момент который был какой-то был таким и остался наверное этот слайд вообще стереть надо Ну ладно уж а очевидные два подхода что мы делаем с этими токенами за 60 лет не изменились вовсе Ну то есть подход номер один Мы заменяем каждый токен его нормальные формы все слова мы Превращаем в нормальные было Мама мыла раму стала мама мыть рама именно эта складируемый в индекс именно в такое Превращаем запрос это вот подход номер один подход номер два мы взрываем запрос и с разными весами которые на слайде не показаны разные словоформы ищем естественно в этот момент у нас сложность поиска взрывается на число словоформ запрос становится точнее Но э-э работает кратно дольше Честно говоря ничего не изменилось ничего интересного не знаю зачем я теперь не знаю зачем мне этот слайд вставил Какие прорывы в лингвистике за 20 лет по моим личным ощущениям вот эти На мой взгляд то что раз в несколько лет становится доступно общедоступно на халяву то есть даром некая интересная штука которая раньше до этого человечеству в целом доступно не было Ну то есть условного Гугла допустим был морфологический словарь ещё В девяносто третьем Хотя Google ещё не было В девяносто третьем А у остальных не было сейчас это не так 20 лет назад общедоступный стриминг который дал хоть какие-то площадке морфологии 10 лет назад а вот про который я так понимаю никого кроме русского человека не знает но который довольно недурно лиматизирует А сильно меньшим количеством ошибок морфологию обрабатывает чем стеммеры А вот и соответственно где-то 15 лет назад Facebook Fast текст выложил который соответственно Ну грубо говоря некой морфологические задачи позволяет решать на ещё более новом качественном уровне и с того момента опять определённые прорыв кроме фаст-текста-то ещё много чего выложили всякие гптшечки вот это всё но подчёркиваю в разрезе поиска это пока ещё не де-факто стандарт поэтому про них тут почти ничего плюс всё-таки Ну за 20 лет сильно большие корпуса сильно больше вычислительных мощностей если 20 лет назад Я вообще как бы ума не давал Как мне снять как мне например подойти к задаче снятия анемии в одну свою каску из-за недельку две но 35 пристойной прототип который будет пристойно снимать эту самую анемию то сейчас отчётливо представляю как это делать отчётливо представляю где брать пусах готовые библиотеки утилиты и т.д и т.п Обратите внимание я это я программист на c++ я не про пустой лингвист Я ничего про это не знаю почти Ну правда работаю в нише давно облегчила короче ситуация Итого Что изменилось за многие годы в части лингвистики сильно больше мучного стечь сильно больше данных сильно более качественные модели Как ни странно базовый уровень вырос не сильно берём поисковик сегодняшний какой-нибудь халявная пинфорсный или даже корпоратив внутри корпоративный ни черта не изменилось 20 лет назад на него просто не было морфологии А сегодня он умеет стэмить наконец-то потому что для этого надо подключить опенсорство библиотеку образца 2005 года на этом наконец-то все научились иногда отдельно взятые э движки системы решений и так далее умеют больше де-факто стандартным к несчастью это пока не стало часть редьки ранжирование Ну тут как бы слово ранжирование вызывает разные дурные ассоциации Вот Но мне почему-то в голову пришла именно такая ранжирование оно в целом какая-то такая задача которая воспринимается как магия и во многом наверное этой магии и является магия она является потому что ну аналитически она не решается никак до сих пор и я сомневаюсь что будет решена даже нейросеточки которые мы тренируем которые кажется уже сейчас э находятся на пороге более лучших предсказаний релевантности чем 20.000 сейчас эсэров Ещё немного и кажется они начнут этих асессоров так сказать рвать в конце концов эти сеточки стоят на живых людях а не наоборот аналитического решения задач скажем так нет нейросеточки это гигантская статистика не более того Именно поэтому ощущение магии да магия там внутри изрядная магия вся эта происходит последней грубо говоря лет 10 первые 40-50 лет человечество натурально как слепые котята считала два простых числа и из этих двух простых чисел собирала две простые формулы натурально 50 лет назад придумали пару нехитрых описывающих текст циферок очень тупых очень тупых берем весь текст развалим его на отдельные уникальные слова кидаем все слова в мешок вот так этот мешок трясём Надеюсь кликер от этого не откажет то а так этот мешок трясём и смотрим на статистику то есть смеси того этого рагу которая у нас получилось сколько у нас таких слов и таких слов в отдельном мешке в отдельном документе сколько их у нас во всей нашей коллекции документов во всём нашем корпусе который мы используем две вещи две TF - всё равно frequency частота Термина в документе и idf invers докими фрикнулись обратная частота степень разности ключевого слова от нуля до единицы ноль слово встречается вообще везде один слово уникальное встречается в одном документе на всю коллекцию 50 лет подряд все по большому счету ранжирование которое в мире было и более того во многом все ранжирование которое есть до сих пор считается из этих двух долбаных чисел по большому счету в 94 году Оказывается мне почему-то казалось что еще в 75 но оказывается путь от TF idf до двух конкретных вот этих параметров до конкретной нехитрой формулы которая даёт настолько хорошие результаты что используется везде повсеместно до сих пор занял 30 лет про кого не 30 там 20 лет практически семьдесят второго семьдесят шестого по 94 это вот тоже у меня глаза раскрылись честно говоря Вот так я думаю очевидно в семьдесят втором это придумали в семьдесят шестом но день в восьмидесятом это формула родилась Хрен там В девяносто четвёртом был очень удивлён Ну такой странный конечно э открытие тем не менее формула оказалась настолько удачная что её натурально все подряд используют до сих пор для ранжирования Найдите ее на этом слайде вот тут три нехитрых формулы совершенно они к несчастью математические Поэтому в тех местах где программист пишет X там написано что-то типа C от VD Но это просто к несчастью такая диссертационная нотация где вместо X Ты должен написать Вот это вот вот это вот иначе диссертацию ты не защитишь А если вдуматься то конечно формулы простые кто из них бм-25 ну как бы довольно сложно угадать не специалист у Да и специалист честно говоря с первого э раза запутается Вот но как бы победила именно формула номер два которая bm25 очень похожий на неё другие формулу которые на немного других принципах но собираются из тех же грубо говоря из других ментальных моделей текста и запроса Но которые Ну блин на тех же переменных они вот не победили победила бм-25 и натурально держится 30 лет 46 лет мы делаем всё ранжирование на двух переменных с двумя передаточными функциями F и ж 46 лет tfdf модели и умирать особо не собирается Но тем не менее победное поступили и большие шаги тут понятно дело что формула ранжирования в которой внутри есть bm25 Это bm25 и ещё 10 примерно таких же формулах которые на самом деле выглядят зубодробительно Но на самом деле считают какую-то простую штуку той или иную сумму чего-то там а Это понятное дело что вручную коэффициенты там подбирать нельзя надо как-то машинным образом оптимизировать после этого какая разница насколько много сигналов и насколько сложная у тебя оптимизационная процедура если ресурсы позволяют вот ресурсы стали позволять значит Победная поступь машинного обучения где-то с десятых годов последние 10 лет и неожиданно отдельный момент который ве поиски закатывается под ковёр А во всех остальных поисках Нет надо понимать что это всё вот эти все приседания они все про ранжирование по тексту про э осмотр текста запросы и текст этот документа а самые сильные сигналы не обязательно текстовый даже вообще смотрим вот на эти текстовые сигналы там ну BM 25 Ну максимальное общее э совпадение между запросом документом какой-нибудь даже векторные бединг смотрим на них это что самый сильный сигнал в вашем товарном поиске ответ нет в вашем товарном поиске два самых сильных сигнала Это цена и расстояние Скорее всего я честно говоря не знаю какие у нас два самых сильных сигнала Ну а текст релевантность в целом важна но дополнительные атрибуты тоже важны Ну соответственно Что изменилось за 20 лет 20 лет назад все считали плохой бм-25 и часто ещё вдобавок ошибались в реализации этого самого бм-25 а где-то 10 лет назад мы к от формула к в которых там 10 каких-то странных сумм перемножений и т.д и т.п перешли и подборка коэффициентов в этих формулах чуть ли не вручную перешли к закидыванию этого всего в эмма-систему вот ну и в двадцатом году Мне кажется эмэль постепенно ползущим образом победил и где Везде где ранжирование минимально важно хотя бы обязательно вырастет той или иное или решение а со всеми вытекающими моментами метрики правда кстати Обратите внимание присяжен реколы по верхней к построен 10g за 60 лет не изменились совсем тем не менее опять-таки тут Веб на фронтире и впереди планеты всей некие прорывные статьи которые мы считаем и от наших разных друзей и от Яндекса про гптшечки и Берт очень-очень в этом плане интересны и возможно Именно они определят следующие 30 лет рано или поздно потому что грубо говоря в 2020 году в начале года у Яндекса 95% качества давала 1000 фичей разных которые там за многие годы росли и именно эта тысяча фичей заваренных машинным обучением она вот ну давала тут довольно высокий уровень качества в веб-поиска которому мы привыкли а не один конкретный сигнал из этой тысячи не было определяющим А в 2020 году такой сигнал Нашёлся это как раз некая штука которую научились считать при помощи Берт сетей и их собственной реализации под названием Я Ti Или как это правильно читать не знаю думаю Яндекс подскажут Ну и понятное дело датасеты всё ещё решают к несчастью в дебре нейросетей У нас тут времени нет уже давно Итого много-много прогресса есть определённые фронтирные свершения кажется человечество подходит к тому чтобы некий де-факто новые супер нейросети сигнал придумать который так сказать станет новым де-фактом стандартом После значит коронованного 30 лет назад бм-25 общий такой Гигиенический уровень но сдвинулся не сильно все научились в бм-25 хотя бы а вторая планка гигиенического уровня сдвинулась довольно хорошо все вместо бм-25 и дурацких окон научились хотя бы простенькие модели варить Ну и есть определённые э свершения на фронтире которые может быть проникнут в общем ещё конечно же у меня семь секций но нет На самом деле две и буквально по одному слайду ещё я бы отметил отдельно Что за 20 лет человечество как бы Выяснилось что поисковой intent бывает разным Мне кажется такого понимания 20 лет назад не было сегодня все довольно чётко понимают что поиск цветов - это одно а поиск торрентов это в принципе похожая вещь что немного другое поиск по логам этот редьки поиск такой такой такой очень много разных скажем так под видов поиска можно придумать они все немного отличаются у них у всех своя специфика Но слава Богу Под каждый из них свой абсолютно уникальный движок строить не надо только под часть потому что принципы абсолютно одни документы поля атрибуты и так далее Все Везде одни единственное что у вас матчинга и так далее лингвистик тоже одни и те же другое дело что иногда при поиске пологом вам лингвистика-то и вообще не нужна Вы точное совпадение ищете какого-нибудь cloudf плеер Рей ID а местами нужна Когда вы ищете по плохо написанному человеку читаемому сообщению всё равно даже там бывает нужна лингвистика и по большому счёту среди вот этих общих подходов несколько меняются только Ну целевые метрики потому что в реалтайм обслуживание рекламок ваша самая важная метка - это то что вы успели эту долбаную рекламку отдать за 10 МС Если вы её не успели Ну всё вы её не показали А в веб-поиске очевидно Вы можете и 30 себе позволить миллисекундное обслуживание запроса вам важно качество повысить Итого а в чем тут некий прорыв На мой взгляд 20 лет назад мне кажется человечеству в целом если угодно плохо понимала что невозможно построить единую систему которая молча придет и исправит все некий такой супер универсальный поиск который будет решать абсолютно все задачи и абсолютно все будет решать достаточно хорошо Сегодня мы Это скорее понимаем Сегодня мы это мы умеем строить нужные кастомизированные системы но там где-то не сильно важно всё равно Мы конечно же берём и ластик или после и пользуемся Так что называется встроенными значит доступными на возможности Ну в принципе это нормально и те самые минус 4 минуты Ну ладно меньше мысли вслух под названием А что нас в будущем-то ждёт по поводу всего этого в связи со всем вышесказанным Ну я всё рассказал честно говоря пытливый ум мог выписать заметки Мне кажется что возможно может быть появится ещё какая-нибудь прорывная лоу левел техника как битики и байтики укладывать по сравнению с современными Но кажется что тут уже почти что все что можно выжили Мне кажется что если мы выжмем из этого еще там дополнительные условные даже три раза производительности то это уже не будет иметь никакого значения потому что matching достаточно быстро и все основное время ты по большому счету тратишь на ранжирование на обработку дополнительных атрибутов и вот это вот всё может быть сменятся де-факто стандартный вот этот вот Нижний Гигиенический уровень то есть 20 лет назад Гигиенический уровень Это был просто боли в список в этом документе есть это слово сегодня Гигиенический уровень это есть ещё и позиция может быть через 20 лет в гигиеническом уровне Будут ещё какие-нибудь беграмы обязательно у всех будут биграммы или там Топки или стейта ДТП отдельный интересный прорыв это вектора векторные поиски когда мы Превращаем весь документ весь запрос и так далее Вектор М бедингов И после этого делаем стандартный топ-кей поиск по этому делу Это очень тяжело считать если в лобешник и так далее а всякие клёвые индексы типа фаисты из Канна позволяют это делать быстро и натура тысячи раз быстрее чем в лобешник и дают неплохой полноту и качество но достаточно они тяжеловесные поэтому непонятно станут ли они де-факто стандартом или нет Я думаю что нет я думаю что ключевые слова с нами всё-таки всерьез И надолго и в лучшем случае будет де-факто стандартом смесь ключевых слов и векторов а-а Как ни странно мне кажется есть определённый э-э простор для абсолютного прорыва в лингвистике и ранжировании мы пить 50 лет ранжируем функции из двух переменных TF idf под названием bm25 уже давно назрело значит э ситуация Революционная для нового короля для новой де-факто стандартной модели что это будет за модель Я не знаю может кто-то придумает третью переменную А может быть Google выложить свой стандартный суперранжирующий Берт и все себе внедрят его вообще все И будет именно так Кроме того я опять-таки считаю что полна текстовый поиск чем дальше тем сильнее будет сращиваться с базами данных и дальше тем меньше будет Вот это вот различие между так называемой базой и так называемым поиском возможно в итоге пройдем к прям вот пониманию того что ну у нас есть там база данных X которая плохонько умеет нам в полно текстовый поиск но зато хорошо умеет в транзакции и есть база Y которая плохонько умеет в транзакции но как ни странно умеет а но зато прекрасно умеет полно текстовый поиск Чёрт его знает процесс это всё довольно медленно и вот если бы Прогресс ушёл вперёд то везде бы уже давно была монго тебе А ещё у нас пока Oracle не вымер и это конечно чудовищно В общем медленно но верно Мы резюмируем медленно верно индустрия двигается вперёд местами Мне кажется она двигается чудовищно медленно тут опять про бм-25 и 46 лет и иногда случаются перевороты вот ну двигается медленно как бы она наверное вот так вот а получается перевороты наверное вот так иногда случаются какие-то прорывы с бм-25 сбертами с пфд и так далее и тому подобное которое каждый раз меня заставляют вспомнить стих это который стих существительное а не которая стих глагол снятие омонимии в глазах канатоходца мирзыбок и неуловим и он вот-вот перевернется и ты перевернешься с ним это всё Андрей Спасибо большое три минуты и три вопроса есть вопросы комментарии рекомендации возьми меня на работу хочу ребенка страшно страшно Но хорошо тебя просто хотят в кулуарной зоне тебе традиционно призы от конференции друзья обратите пожалуйста внимание на место в программе Андрей Аксёнов да Спасибо тебе большое У меня встречный вопрос чего я не рассказал из того что вы ожидали Услышать по абстрактам А это Уловка это способ остаться еще на полчаса сейчас перейду но тем не менее"
}