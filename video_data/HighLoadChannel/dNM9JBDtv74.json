{
  "video_id": "dNM9JBDtv74",
  "channel": "HighLoadChannel",
  "title": "ML on the edge / Андрей Татаринов (Epoch8)",
  "views": 1502,
  "duration": 2196,
  "published": "2019-12-05T13:05:08-08:00",
  "text": "меня зовут андрей татаринов я верю что будущее за машинном обучении и поэтому пару лет назад я собрал команду назвал эпоха 8 и мы занимаемся заказной разработкой представляем разные услуги в области разработки решений типовые задачи с которыми мы сталкиваемся там два трека у нас два трека это разные кастомной себе нлп в основном пар классификацию текстов похоже на то что показываю ребята предыдущем докладе семьи стандартная задача это взять изображение понять что на нем нарисована понять где находится машинка котик собачка на изображение в конкретном месте извлечь какие-то дополнительные признаки изображения и все это хорошая задача и все это задач которая хорошо решаются нейронными сетями но у нас есть специфика наших задач она в том что нам все задачи нужно доводить до внедрения то есть мы не заканчиваем задачи на исследовательской части всегда когда к нам приходят как всегда когда у нас появляется задача ее нужно внедрить и внедрение это тяжело почему потому что опять таки как было на предыдущем докладе первый вопрос если у вас cpu нужно ли к полу давайте разберемся обязательно ли использовать видеокарты для внедрения скобу все сложно это дорого если у вас облачный сервис мобильное приложение или еще что-то вы хотите делать inference в облаке то по самым дешевым прикидкам в самом дешевом облаке вы будете тратить порядка 1000 долларов в месяц на один сервер с видеокартой если вы хотите закупить сервера самостоятельно класса консьюмер грей то есть игровые машины то самое дешевое на что вы можете рассчитывать это тоже примерно 1000 долларов и огрести кучу проблем с нестабильностью железа и сложностью в эксплуатации но на самом деле альтернативы есть перед тем как поговорить про них давайте разберемся вообще почему почему считается что нужны нужные видеокарты для работы с ней растя me ну естественно там как только вы начинаете любой q как только вы входите в любой курс машинного обучения вам говорят для прохождения этого курса вам потребуется видеокарта и от действительно так вот возьмем стандартный любимый пример для бенчмарков например у энви дереза на 50 это модель в которой пятьдесят миллионов параметров ай 30 миллионов параметров это 30 миллионов весов к которая как-то налить у них 50 слоев и прямой проход это там 4 гигов labs операции для маленькой картинки 224 на 224 и в процессе обучения вы делаете прямой проход обратный проход градиентный спуск поэтому 30 или миллион мерному пространству миллионы раз а может быть и там сотни миллионов раз и действительно эта задача которая без без какого-то без какой-то аппаратной поддержки решаются очень тяжело ну то есть вы будете ждать годами для обучения но что очень важно при внедрении вы не делаете обучение для для того чтобы внедрить и применить модель к конкретному запросу конкретного клиента вам нужно сделать один inference один раз это все еще очень много это это миллиона операций но это гораздо меньше это катастрофически меньше чем количество ресурсов требуемых при обучении самое банальное что можно придумать это запускать не цпу да перед тем как поговорить запуска когда мы запускаем sinful почему мы можем хотеть этого делать например вам нужно внедриться в существующую инфраструктуру например у вас есть hadoop кластер на тысячу ядер в котором уже много процессор най мощности и нет видеокарт или у вас много локальных инсталляций например у вас много точек на которых стоит по одной видеокамере каждую из которых надо обрабатывать и ли у вас какой-то закрытый контур в которой нельзя просто так взять и принести новое железо перед тем как говорить дальше давайте разберемся как правильно думать про производительность видеокарты и процессора очень важное понимание про видеокарту это то что видеокарта это очень много очень маленьких лидеров то есть это тысяче ядер каждый из которых не очень производительно они плотно упакованы могут работать параллельно у них очень хорошая память и видеокарта работает эффективно на вычисления в тот момент когда вы можете распараллелить свою задачу и эффективно наполнить ее работой как правило эффективная утилизация видеокарты достигается когда вы обрабатываете больше чем условно одно изображение за раз например вот такой небольшой оффтопик nvidia всегда когда показывают результаты бенчмарков своих видеокарт или своих способов ускорения всегда говорит а вот этот бенчмарк по производительности мы сделали с окном latency таким-то то есть когда идут запросы один за одним они ждут какой-то там 7 или секунд пять десять миллисекунд почуют запрос и отправляют их на обработку потому что это эффективнее чем делать обработку по одному по одному кадру за 1 цикл наоборот циpкa это очень мало тон 48 там 610 я der которая достаточно которая гораздо мощнее чем каждое ядро видеокарты и даже одно изображение которое вы подаете на вход полностью утилизирует весь процесс а из там все очень просто подали 10 изображений стала в 10 раз дольше вот например для того чтобы появилась интуиция там 1080 это видео карта консьюмер great для того что ставится официально ставится в видео цен в дата-центры класса в столб и стоит он так далее цифры еще интереснее разницы наверное видеокарте между почём размера 1 размером 5 нет это значит что если вы обрабатываете изображение по одному вынет вы вы не до нагружаете ресурсы используете его неэффективно рядом видно что процессор там ту ту же самую сеть исполняет медленнее но в целом приемлемо медленно то есть стоит 120 там 100 200 миллисекунд на один кадр это окей если вам нужно обрабатывать один кадр в секунду несколько то есть мы там на ис-7 обычно мере сказал соответственно как внедряться то есть хорошо вы вы решились что все хорошо мы будем просто без видеокарт запускаться на том что на том железе которого на стоит хорошо если вы контролируете свое окружение то есть вы там нам комфортнее всего работать с джокером завернуть туда весь наш исследовательский стек и запускаться вот запускать то же самое что запускалась в исследовательском окружении тогда все просто проблем нет если но бывают ситуации когда вы не можете контролировать окружение полностью там несколько вариантов например внедрение в hadoop не класть его там мапри десны ну то есть в целом ходу пока система не предполагает что у вас на каждой ноге докер вы туда скачали огромный и образ там так далее они говорят задайте нам by на ней без зависимостей мы его запустим или у вас например винда или еще какое-то там окружение с лимитированным лимитированное к изменению мы в этом случае обычно пишем монолит нога ланга легко писать легко легко описать те части которые нужно сделать для обертки легко внедрять но есть нюанс про который вот очень внимательно нейросеть это не только вот блок с весами не только не только 100 мегабайтов не только файлик на 100 мегабайт очень важно понимать что вот минимальный минимальный запуск нейросети это три стадии эта стадия при процесса подготовки результат подготовки данных к обработке собственно применение вот вот эта интересная часть с применением нейросетей и обработка результатов и это важно потому что при пост-процессинг а это код который написан который не'бан длится вместе с нейросетью бывают разные случаи например если мы занимаемся просто классификации картинок то при пост-процессинг тривиальный при processing взять картинку преобразовать ее там не знаю в нем парень просто в массив в определенном формате передать передать черной коробке получить одно число по справочнику ну или там несколько чисел применить немцов макс взять максимальная и по справочнику найти там нужны класс все тривиально это легко перенести на на любой язык но бывает сложнее например есть экстремальные примеры даже не аксиальный типовой пример в задачах компьютерного зрения детекция объектов там классические архитектуры которая это решают там fast rc н.н. yolo но весь класс ssd дикторов они не умеют работать с произвольным количеством с неопределенными выходами если вам не если вы не знаете условно сколько машинок вы хотите найти на изображении нейросеть она может отдавать только выход определенного фиксированного размера на всегда отдает там некоторое количество баландин баксов гипотез баландин баксов с дополнительными атрибутами и код который занимается иногда она дублирует то есть нормально получить 15 предсказаний наложенных друг на друга для одного и того же объекта с чуть-чуть внесенные и для того чтобы расшифровать результат работы вам нужно написать код вот чтобы вам было страшно вот так не надо читать это от просто ужасаться это кусочек кода интерпретации phaser оценена и тут возникают нюансы у нас был анекдот из жизни разрабатываем модель для распознавания объектов в мобильном приложении в исследовательском контуре все хорошо ну там мы мы там решаем проблему детекции определенного класса в исследовательском контуре все классно мы знаем что при хижин прикол то есть что мы находим примерно 80 процентов объектов этого класса на изображении заказчик проверяет в интегрирован интегрированную модель мобильное приложение и говорит нет у вас ребят ничего не работает деталь к общине находятся начинаем разбираться разбирались долго выяснили что мобильный разработчик момент внедрения модели внес улучшение в интерпретацию результатов и обрабатывал вне три с половиной тысячи баландин баксов гипотез которых в которых были правильные ответы а только топ 100 по уверенности которая реально оттекает кучу результатов большая проблема мы ее пытались решить и выяснилось что проще всего решить ее так научить дата-сайентистов писать на свете потому что чтобы локализовать ответственность в в одной команде ну чтобы можно было чтобы они контролировали тот способ которым модель внедряется в бой ну и того на цпу внедряться хорошо можно этого не страшно это не страшно если помнить про вопрос портированием тут еще вот в в разряде запускаться на цп у меня в голове идут несколько тем про которого стоит просто упомянуть что когда-нибудь в не очень далеком я надеюсь будущем нас всех в дата-центра будет ждать счастье там им-то вы наказ кейт лейки которая с новым классным расширенным набором инструкций обещают performance на inference не хуже чем видеокарты просто сами по себе но стоят они там по моему десятки тысяч долларов и рядом с запуском на циpкa у меня идет запуск на комодики железе это тоже работает то есть на на на любых радианах если я не ошибаюсь есть порт атаки и dance of low там tensorflow раком на на всех быть на большинстве бытовых intel hd видеокартах работает окон война это разработка от этого дальше у каждого в кармане я телефон на большинстве большинстве телефонов большинстве топовых телефонов айфонов начиная с семёрки во всех флагманах на андроиде есть сопроцессор предназначенный для inference а не раз тень на айфонах и рыбаки которые управляют запуском которые управляют доступом на айфоне это кармель на андроидах это nn api которым никто не пользуется практически потому что он сильный все пользуются и мальки том который в свою очередь используют end of low light как они сравниваются если вы работаете на ios ну наверное проще работать на карамель карамель это вот как в любимом apple стиле ограничивающая вас технология далеко не все далеко не любую нейросеть можно спарте портировать в карман это было очень сильно заметно на карамель 1 и 2 карамель 3 который вот сейчас объявили буквально две недели назад если я не ошибаюсь расширил набор операции они обещают что большинство state of the art моделей портировать маркируются но если вы умудрились сконвертировать модельку armelle то оно гарантированно работает ускорением это очень важно м л хит он такой он вот как как им экосистемы android он разнообразный и ставь лайк без без ограничений практически без ограничений принимает себя любой tensorflow граф и говорит да хорошо мы его запустим но иногда выясняется что некоторые операции не поддержаны в напе не поддержаны на ускорителе и программа и и его и и и ваши inference без предупреждения сваливается in school это может выглядеть так у вас кусок вычислений прошел на ускорителя промежуточные данные скопировались это процессор прошел чуть чуть inference на процессоре скопировались обратно на ускоритель продолжили то есть на форумах прям есть типичный набор вопросов почему при подключении на на pi model на начинает работать медленнее потому что н н api оказывается не имеет драйвер к вашему конкретному устройству на удивление быстро работают ускорители в мобильных приложениях мобильных устройствах на iphone x наш любимый рис на 50 инфе лица примерно за 100 миллисекунд или 120 что такое это медленнее чем desktop видеокарта но опять это приемлемо для целого класса задач но важно понимать что мобильное устройство это не сервер и есть ограничение если у вас inference если вам доступен inference 100 миллисекунд это не значит что вы можете бесконечно долго делать inference 10 кадров в секунду батарейки греются горят быстро быстро разряжаются устройство греется операционная система начинает тротлинг то есть это это выражается в том что буквально там через там десяток минут работы приложения она начинает тормозить почему потому что операционная система ограничивая доступ к ресурсам и тот же самый набор вопросов связанных с портированием на на плюсы galant там и так далее еще очень важно понимать что на мобилках медленный процесс а то что сама нейросеть работает быстро не обозначает что весь pipeline работает быстро например плохой идеей будет переносить большую часть постпроцессинга на процессор вот интересно пример на как ли вообще в целом считается хорошим подходом для детекции сложных объектов например клеток или домов на на снимках на спутниковых снимках использовать архитектуру и нет там смысл такой мы для каждого пикселя изображения определяем класс к которому относится этот пиксель то есть на выходе вы выпадали на вход картинку на выход получили картинку с глубиной в количество классов которые вы хотите предсказывать и доктора пока пиксельная разметка после этого вы можете делать какой-то интересный пост-процессинг например найти в связанные компоненты на картинке то есть вот области закрашена одним цветом искать а вот эта вот область закрашена одним цветом цвета машинки значит вот боуден бокс по машинке вот такой если попытаться такое фактуру перенести на на мобильное устройство будет плохо потому что вторая часть связана с поиском компонент и постобработкой будет работница по тяжело медленно батарейка говорит час линии и того на мобилках можно запускаться локально над стараться сделать так чтобы пост-процессинг при processing был легкий вся работа была нас о процессоре и надо научить от санти став писать мобильное приложение хотя бы часть есть интересный есть ещё интересный аспект связанные с внедрением условно в чистое поле то есть сценарий выглядит так есть какая-нибудь гениальная бизнес-идея например реальный случай давайте поставим видеокамеру давайте в каждый bowling напротив экрана повесим видеокамеру и будем распознавать счет на табло почему так потому что это самый простой и дешевый способ за интегрироваться со всеми системами к которым уже утерян доступ или хозяева не хотят пускать вас в свой контур реально проблема понятно что ни в одном боулинга напротив ни одного экрана не сменить не стоит сервер который может обрабатывать и при этом часть работы нужно делать локально нужно видеокамера нужен захват видео с нее и обработка этого видео то есть можно конечно кусочек отнести в облака или куда-то в центр но все равно останется часть железа которая работает вот локально и и хорошо бы удешевить стоимость вот вот этого решения которой стоит вместе такой интернету в kings ii до закупить десктопы дорого десктопы это опять 1000 долларов и при этом в него просто физически сложно воткнуть много устройств мы за последнее время вот буквально за последние там по моему в течение последнего года начал появляться класс устройств очень интересных которые кажутся решают именно эту проблему первым на рынке из достойных появился google с чпу и вслед за ним деться на на тэн виде этап это вообще это короче если можно работать на это пун надо работать на эту штуку с ним пара проблем есть первое что их компилятор моделей находится в облаке то есть вам физически надо свою proprietor ную модель за которую заказчик заплатил в деньги и которого очень боится что это его единственная айпи она утечет куда то вы его загружаете в интернет оттуда получаете оптимизированную модель под устройство и это нельзя заказать в россии физически нельзя там течение первых двух недель после того как их анонсировали я попытался заказать в россию не заставляли я пытался заказать через европу в европе товарищу пришел пришла анкета от экспорт control как на товары иного назначения и рад сказал нигде нет вот но по всем бич марком бьёт всех бьет десктопной видеокарты вот такая вот малюсенькая штука очень круто из доступного нам следующий классный кандидат это джадсон надо вот если очень грубо то это разбери пай с видеокартой маленькой видеокартой на 100 на 100 ядерно 120 лидер но как бы полноценно полноценный по полноценные рабочие машины с ним есть нюансы связанные с производительностью вот мы только начинаем с ними работать поэтому мы еще поэтому я еще не могу точно сказать в какой из категорий мы есть если если делать наивный inference того же резне то то легко получить 12 кадров в секунду в обработке если пойти на шаг дальше воспользоваться nvidia runtime am сделать конверсию в p16 убедиться что нет потери качества и запустится 1 по 16 можно ускориться в 10 раз там можно еще конвертироваться в and 8 надеяться что все будет продолжать будет хорошо я кстати не устойчива к конверсии в and 8 там остальные модели чуть лучше но опять это это теоретическое знание от меня-то будет чуть чуть получше если как invideo по хардкору написать на тендер рты на си плюс плюс pipeline ференца буде 65 fps разница катастрофическая но очевидно что потенциал есть джексоном классно он очень дешёвый эта штука стоит 100 баксов причем что интересно меня табличка нет если сравниваться то вот вот эти вот фпс и там средний фпс и можно получать на процессоре на курае 7 примерно тоже самое можно получить но core i7 стоит 300 баксов без обертки просто сам процессор эта штука стоит 100 баксов полноценно как как полноценное устройство на нем достаточно легко разрабатываться это там наш любимый ubuntu tensorflow все хорошо памяти достаточно процессор слабый ну к этому привыкли еще на мобилках в целом как бы кажется что для нас это сейчас лучший выбор и есть еще серии анекдотов там вещей которая хочется упомянуть которыми мы не работали но это мне кажется забавным есть класс устройств которая стоит под 10 долларов вот например хорошего пример там это кипятка 210 их не очень много их там две три штуки разных типов но они есть они супер маленькие то есть это это процессор который может принять в себя модель машинного обучения размером 6 мегабайт они заточены под inference ввп 16 то есть это в половинной точность точности флот поинта и вот в половинной точности туда кажется пролезает той не ела с плохим качеством но 30 fps ами в моей голове это эта штука подойдет если в понятие в каждую камеру видеонаблюдения или в камеру который распознает лица или там еще что-то которая делаю предварительный отсев то есть чтобы не гадать 24 часа видео в облако а чтобы дать только те кадры где скорее всего есть то что нам интересно и еще есть интересная штука мы с ней поигрались мы правда поигрались мы портировали нейросеть у меня есть модель который распознает котика через видеокамеру зачем это нужно я не знаю но можно если захотите знаете можно портировать нейросеть на какой-то из фреймворков которой работают прямо в браузере у клиента используют ускорение любой его видеокартой которая поддерживает opengl и это прямо реально работает быстро с единственным ограничением всю нейросеть kotova вы подготовили надо будет скачать но работают и того следующий раз я надеюсь когда вы задумаете запускать что-то в бой то может быть перед тем как задать вопрос не пора ли нам закупить кластером пью может быть вы найдете какой-то из вариантов которые устроить который вас устроит спасибо так меня можно включить с этого микрофона давайте по задаем вопросы а привет этот антон у меня несколько вопросов самый наверное такой интересный это аппарат когда портирует любую нейрону сеть по принципе любой задачи to copy the vision international и остался сталкиваешься с проблем во первых места ну просто допустим большим он просто сейчас много хороших моделей который дает хороший интер интерес они они висят но ну гигабайты лобода да и минус гигабайт на устройстве пользователя конечным это печально до соответственно во первых как с этим боретесь ну здесь как бы бороться то есть во первых мы еще то есть в тот момент когда мы запускали на мобилках мы просто выбирали модель объем который приемлем для скачивания то есть вот мы выйдем и такие подумали с продакт менеджером и решили что ну наверное 100 мегабайт на скачивание ну наверное окей и сделали всё нарезать 50 вот если бы нам сказали нет нам нужно не больше 20 окей как бы деградации об архитектуре вот с контролируемым ухудшением качества а вот еще один такой вопрос когда пользователю не нужно сделать что-то с компьютер vision к примеру включить вот эта модель должна в грузится да она должна там не знаком с компилируется здесь этого как мы запаковали там подобно над трать время как закрыл приложение чак открывать прошла и заново это все дело порождается за нет не корректное утверждение номер знали висит в памяти ну то есть это просто не в россии достаточно тяжелые проблемы ну то есть это это это разные это разные под системы то есть опять вопрос наверное в звучит так есть ли задержка на 1 inference наверное таро вот если честно именно на этот вопрос так по отсутствию жалоб со стороны клиентов вот вот и мы сейчас наверное говорим про мобильное приложение в первого числа по отсутствию прямо не было такой проблемы то есть люди чтобы все инферн с первых будет там через две через три секунды только потом нормально а я не могу подтвердить что он через две-три секунды ну к примеру вот сейчас сейчас я не могу подтвердить то есть у нас нет внутренней у нас не было такой внутренние проблемы чтобы там на тестирование приложения вы сказали первую через медленно мы если честно об этом даже вот это хороший вопрос но судя по отсутствию проблем кажется что там все не так плохо окей понял а второй это скорее больше организационный вопрос вот как тяжело было дата сайтом перейти на более прикладной прикладные вещи иногда надо было уже писать не просто research и там или там допустим бэкон а переходить на клиентское устройство ну это было на самом деле осознанное решение почему потому что нет на самом деле они сначала говорят нет мы и хотим я такое говорю да хорошо вот заказчик вот я мобильный разработчик запустить сам и они что делают они пишут инструкцию по интерпретации результатов мы две недели с с разработчиком со стороны заказчика отлаживаем эту инструкцию отлаживаем инструкцию отлаживаем его код концы этих 2 недели они фактически уже пишут этот код потому что без этого невозможно отладить и после этого как бы следующий логичный шаг давайте просто сами напишете уже он не очень сложный то есть они как бы нет проблемы то есть они понимают они понимают зачем это нужно например я сейчас всю жизнь больше часа пишет о нет да да да да да да допустим там на объекте всем приходится писать или наш свифти или там допустим на android java ну как бы ну да да но как бы человек с мозгом способен обучаться да и такой вот такой уточняющий вопрос а от этого качество сильно пострадала или там то есть что во что уперлась вот именно вот проблема того что надо надо садиться портировать на разработку на конечном ничего не мной наоборот качество качество интегрального pipeline а выросла потому что он наконец-то стал соответствовать исследовательскому окружению качество моделей нового но качество в исследовательском и кружевом времени сколько это заняло командирование на самом деле сейчас уже легче будет то есть как бы если мы сейчас заново решаем такую же задачу то мы просто будем сразу же то есть там можно сделать bootstrap силами мобильными по мере разработки сказать напиши мне код который оборачивает вот это все вот сюда картинка приходит вот здесь результаты алгоритмическая часть связанная с интерпретацией она она ничего не не очень сильно зависит от языка это как бы тоже самое как написать это на питоне но не использовать мумбай и все библиотеки то есть надо просто в чистых массивах написать вот вот эту интерпретацию да надо ее написать как бы она это опыт пост алгоритм в другом синтаксис спасибо ши ещё вопросы есть вопрос руки поднимаете чтобы мы увидели так есть у нас микрофон пожалуйста чтобы наслышались очень круто на мобильных телефонов они была идея сделать какой-нибудь кластер из мобильников а может быть они покажутся дам дешевле энергоэффективнее но нет кластер на мобильник яркой телефоны горят вообще вот это вот важно не ну можно же взять какой-нибудь там взять с него снять корпус до положить его в жидкий азот банк куда-то выпускать лежит может быть да да да да да да то есть я на самом деле на самом деле это хороший вопрос и и сейчас вот это вот тема вот буквально там не знаю полгода-год она идет google есть задачи связанные с машинным обучением которые не связаны с использованием телефонов просто потому что они эффективны и как вычислить или иногда это приватность иногда это там локальность данных там еще что то и вот до обучиться до тюнить твою модель условно там под твой способ исправлять опечатки это double google затем экспериментирует у нас необходимости до обучать и или там inference и на кластере из айфонов делать не было даёт ещё такой вопрос а вот какой нибудь может быть из практики самый интересный кейс вот машин лендинга какой-нибудь чтобы который можно рассказать дальше ну вот смотрите они как бы опять все интересные смешной кейс вот про боулинг смешной это это честный кейс мы прямо вот сейчас на джексон ах это запустим а вы говорите в микрофон в трансляции нас не слышит ещё вопросы есть давайте 321 все спасибо тогда андрей татаринцева"
}