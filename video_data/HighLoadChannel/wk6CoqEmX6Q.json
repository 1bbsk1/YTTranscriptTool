{
  "video_id": "wk6CoqEmX6Q",
  "channel": "HighLoadChannel",
  "title": "Опыт построения и эксплуатации большого файлового хранилища / Даниил Подольский (GitInSky)",
  "views": 986,
  "duration": 2822,
  "published": "2017-07-30T00:47:31-07:00",
  "text": "а итак даниил подвойский из компании японской будет рассказывать о том а что в принципе каждый инженер должен сделать своей жизни после того как он родил ребенка посадил дерево и построил дом это сделать свое файловое хранилище вот поприветствуем даниила сейчас показывает его слайды слышно ли меня так 1 1 зеленый горит поправить 1 1 2 отлично так гликирование не дали это значит доклад мой называется опыт построения эксплуатации большого файлового хранилища что это за звук большой файловое хранилище мы строим эксплуатируем последние три года значит куда давить на самом деле в тот момент когда я подавала тезисы доклад назывался ночью через лес опыт построения эксплуатации бла бла бла но программный комитет просил меня быть посерьезнее тем не менее на самом деле это так рад доклад ночью через лес этот образ придумал коллега чистяков для конференции стачка и там мы делали доклад про ночью через лес он раньше был нет ни франции был про докер а я вот про новую sql базы с тех пор как я хожу на конференции мне стало понятно что слышит я хочу не истории успеха а истории вот этого ужаса и кошмара который нас всех ждет на пути к успеху потому что успех чужой мне не знаю мне ничего не даст ну то нет конечно если кто-то уже что-то сделал само знание о том что это возможность делать уже помогает мне двигаться но в реальности я бы хотел знать где там минные ловушки еще один кайфовый образ это все вступительное слово к докладу еще один кайфовый образ книга пикник на обочине там есть научно-исследовательский институт которые следует эту зону у них есть летающие боты у них есть автоматические маркеры у них есть то и это у них есть роботизированные системы а есть сталкеры который бродит по этой зоне просто так раскидывая гайки туда-сюда так вот так уж получилось что мы работаем в том сегменте где востребованы сталкеры в они научно-исследовательские институты мы работаем в том сегменте где highload наиболее важен мы работаем в сегменте нищебродов highload это вообще-то для нищебродов потому что взрослые парни просто не допускают чтобы нагрузка на сервера у них превышала 30 процентов вот если вы решили что ваш flow watermark 70 процентов вы значит во первых у вас начался халат во вторых вы нищеброд и так для начала что такое файловое хранилище и зачем она вообще может оказаться в нашей жизни файл файл и кусок информации это его официальное определение это кусок информации снабженный именем по которому этот кусок мамы на кусок информации можно извлечь извлечь почему то есть это же не единственный на свете кусок информации которые снабжены минимум почему файл отличается от всех прочих потому что файл слишком большой чтобы обращаться снимать так как с одним куском ну смотрите если вы хотите поддерживать например 100 тысяч одновременных соединений это не так уж много и у вас и отдаете в файл размером в 1 мегабайт это означает что если вы хотите обращаться с файлом как с одним куском информации вы вынуждены загрузить все сто тысяч файлов по одному мегабайту в память это 100 гигабайт памяти невозможно соответственно вам придется что-то сделать или ограничить количество одновременных соединений и для корпоративных применений это например нормально или обращаться с файлом так как будто он состоит из кусочков информации из из из из отдельных мелких кусочков из чанков слова chance будет употребляться в докладе дальше краеугольный камень здорового питания было зачеркнуто да так вот файл это краеугольный камень сегодняшнего обмена информацией на все а потом за да то есть мы все что можно оформляем как файл просто по привычке потому что до недавнего времени никаких средств хранить информацию иначе как файлов на диске у нас не было об этом тоже будет позже почему на самом деле сегодня этот подход блок срабатывает и лучше бы от него отказаться но пока не удается файловое хранилище и соответственно место где файлы хранятся а на самом деле есть следующий слайд файловое хранилище даже может быть более важно что файлы там хранятся это то место откуда к файлам предоставляется доступ откуда они отдаются что такое файловое хранилище мы поняли что такое большое файловое хранилище эксплуатируя большой файловое хранилище я обнаружил что это не характеристика самого хранилища вот например у васи пупкина есть архив подросткового кино на пять петабайт это большое файловое хранилище нет почему потому что она никому не нужна потому что пар вася пупкин не может смотреть все пять петабайт одновременно он смотрит один маленький фильм ну там есть еще несколько характеристику этого пушкинского хранилища например если он его потеряет он расплачется и скачает все заново из интернета много файлов то есть можно было бы предположить что если много байт то это небольшое тогда может быть много файлов это большое нет много файлов есть хранилище в которых очень много записей у нас есть база в которых миллиард строк и тем не менее они не являются большими почему потому следующий следующий пункт потому что если у вас есть миллиард строк в базе у вас есть удобные и надежные средства управления этими строками для файлов такого средства нет то есть все наши активные используемые сегодня файловые системы иерархические это значит что для того чтобы выяснить что у нас происходит по файла файловой системе нам надо пройтись по ней все имеющиеся каталоге открыть их почитать иногда у нас нет даже обычно у нас нет индексов на каталоги поэтому мы вынуждены прочитать его от начала до конца найти нужный файл ну вот это вот все все все это себе прекрасно представляют так вот большое это описание ситуации в которой вы оказались со своим файловым хранилищем они собственно самого хранилища а очень часто и это будет любимый фокус можно превратить в большое хранили файлохранилище в нормально просто перенеся его на ssd там значительно большие abs of и стандартные средства управления информацией который используется на файлу систему начинают работать достаточно быстро чтобы управление таким файловым хранилищем не представляла собой проблему следующий слайд парадокс файлового хранилища вот с точки зрения бизнеса эти файловые хранилища незачем не нужны когда у вас есть довольно большой проект который там отдает принимает от пользователей файлы отдает пользователям файлы там показывает пользователям рекламу все в общем понятно да а вот почему примерно половину бюджета проекта составляют какие-то железки невнятные а на которых лежат какие-то невнятные байты это бизнесу объяснить нет бизнес в принципе понимает зачем это но на самом деле файловые хранилища не нужны в бизнес требует бизнес требованиях никогда не будет написана хранить файлы бизнес требованиях будет 1 писано отдавать файлы на самом деле существует есть tz в котором будет написано хранить файлы и тот же на систему резервного копирования на самом деле это тоже вранье когда мы хотим систему резервного копирования мы не хотим систему резервного копирования мы хотим сделать систему аварийного восстановления то есть мы опять хотим файлы читать они хранить к сожалению создатели файловых хранилищ этого не понимают может быть только создатель ис-3 додумались до этой простой вещи все остальные аккуратно вот их интересуют чтобы файлы были сложены чтобы они ни в коем случае не разрушились и если опасность и об этом будет позже но я скажу сейчас если опасность разрушения возникает то надо прекратить всякую деятельность ни в коем случае не отдавать файл который может быть побит или ни в коем случае не загружать новые файлы если у нас есть опасность разрушения имеющейся информации странный ну то есть то есть да не понять это традиционный подход но тем не менее к тому чем мы занимаемся он отношение не имеет а вот тем не менее файловое хранилище вещь необходимая потому что то то есть вот никак весне не обойдешься она не нужна но и вот как то вот нет без нее никак потому что файлики все-таки должны где-то быть чтобы ты мог их отдавать значит основной источник опыта общения с файловыми хранилищами в моей жизни это проекции top ru setup.ru это массовый хостинг с некоторыми фишками там сайты генерятся по шаблону соответственно шаблон пользователь его заполняет нажимаем сгенерировать генерируется от двух десятков до двух сотен файлов они все складываются хранилища пользователи загружают картинки пользователи загружают разнообразные другие бинарные файлы в общем это неограниченный источник вот этого вот всего на данный момент в хранилище сетапа лежат 450 миллионов файлов поделенных на полтора миллиона сайтов это довольно много и собственно как как мы дошли до жизни такой обед это видимо все-таки основное основное содержание моего доклада как мы двигались к тому что у нас там есть сейчас значит 20 миллионов файлов в сутки это объем обновления сетапа сегодня в пике это само по себе на то есть 20 миллионов это уже много первый раз вы столкнулись с проблемами когда у нас было шесть с половиной миллионов файлов тем не менее в 2012 году файловое хранилище сетапа было организовано очень просто генераторы контент опубликовали на два сервера с целью буса обеспечения отказоустойчивости синхронизация если у нас один из этих серверов помирал мы выбрали другой такой же копировали ир сен com с одного на другой весь всю эту массу файлов и все у нас было хорошо для горячего контента у нас использовал ssd тогда еще hetzner это все песни извините это к вопросу о том что мы именно сталкеры да то есть если такое смешное место это такая зона мы оттуда выносим время от ремни ведьмин студень продаем его на черном рынке и с этого живем вот соответственно в чем мы увидели проблемы когда эту с этой схемой познакомились в 2012 году в тот момент когда у нас один из дублирующих серверов помер мы некоторое время подряд живем без файла вера соответственно пока идет arsenka мы вынуждены молиться и трястись от страха со статистикой по файловой системе тоже в общем проблемы и слепой ssd все еще это удавалось собирать по 60 гигабайт нам 64 гигабайта тогда были ssd в his не вот и никаких других то поезд hdd мы очень быстро к весне 12 поняли что мы не знаем что у нас лежит на диск и никогда не узнаем правда тогда мы еще не думали что это станет проблемой летом 2012 года у нас сдох очередной сервер мы привычным движением заказали новые запустили р sing и он никогда не закончился вернее никогда не закончился скрипт который запускал арсен в цикле пока не обнаруживал что все файлы скопированы оказалось что файлов уже достаточно много что обход дерева занимает шесть часов что копируются файлы плюс к этим шести часам еще двенадцать из за 18 часов контент успевает изменится настолько что реплика наше не актуально ночью через лес то есть никто не ожидал что эта палка угодить нам в глаз ну так вот то есть часто это очевидно а тогда мы были очень удивлены типа как же так тогда же ваш покорный слуга придумал сложить файлы в базу почему он это придумал откуда вообще взялась эта идиотская идея потому что все таки мы собрали какую-то статистику 95 процентов файлов были меньше 64 килобайта 64 килобайта это даже при ста тысячах одновременных соединений вполне подъемный подъемный размер чтобы обращаться с этим как с одним куском остальные файлы были спрятаны в базу большие файлы были спрятаны в blu бы там позже я буду говорить о том что это самая главная ошибка этого решения буду говорить почему мы тогда верили в мастер мастер репликацию поэтому мы написан ну а в паз грисельда это все было сделано в пастбища вот в пожгли си нет никакой мастер мастер репликации поэтому и нигде нет на самом деле ни в одной особой нет мастер мастер репликации поэтому мы написали свою которая учитывала особенности нашего контента и обновления этого контента и соответственно могла функционировать нормально вот и весной 2013 уфа какой странный год у меня там написан весной 2013 года мы наконец столкнулись с проблемами того что мы понаписали файлов стала к тому времени 25 миллионов оказалось что при таком объеме обновлений который к тому времени происходил в системе оказалось что транзакции занимают существенное время поэтому некоторые транзакции которые были покороче успевали закончится раньше чем начавшиеся раньше но более длинные в результате авто инкрементный счетчик на который мы ориентировались в нашей мастер мастер репликации оказался с дырочками то есть некоторые файлы не когда наш мастер мастер репликация не видела это был большой сюрприз для меня лично я пил три дня вот потом наконец придумал что надо просто всякий раз когда мы запускаем мастер мастер аппликацию от последнего этого счетчика отнять сначала приходилось отнимать тысячу потом 2000 потом и 10000 когда я приду когда я вписал в это поле 25000 вот я понял что надо что то делать но пока тому моменту я не знал что делать и мы наткнулись опять на ту же самую проблему контент менялся быстрее чем его синхронизировали оказалось что вот это наша мастер ремастер репликация работает довольно медленно и работает медленно на самом деле не она работает медленно вставка в позу из особенному медленно работает вставка в глуби поэтому собственно в какой-то момент ночью когда количество публикаций уменьшалась радикально база сходилось но днем она все время была немножко не пренсе стэнд на я чуть-чуть вот это наши пользователи заметили это следующим образом они загружают картинку они хотят ее тут же увидеть а ее нет потому что загрузили они ее на один сервер а а запрашивают они через round robin с другого вот ну пришлось научить наших наши сервера заменить бизнес-логику пришлось доставать картинку с того же сервера на который мы ее загрузили это аукнулось нам потом проблемами с душем сервером когда сервер сдох надо лезть туда где вот этот вот rutor и менять ему параметры рутин га в общем осенью 2013 года файлов стало 50 мегабайт и тут мы наткнулись на то что наша база в общем-то не тянет потому что там довольно сложный был джоин для того чтобы отдать пользователю именно последний файл в смысле последнюю версию того файла который он загрузил и у нас перестало хватать наших 8 ядер то с этим делать мы не знали но олега чистяков нашего решения коллега чистяков на триггерах сделанном из материала east view то есть по запросу файлик из мухи в который был долги join переезжал в отдельную таблицу откуда в дальнейшем у запрашивать собственно понятно как это было устроено и это все тоже стала работать прекрасно репликация мастер мастер к тому времени уже работала очень плохо но вот этот костыль и км и подкасты лили и все вроде бы было хорошо до весны 2014 года вот 120 миллионов файлов контент перестал помещаться на одну машину в основе взять машины в которых было бы больше чем четыре по четыре три винта тогда еще по три четыре по три тайвин то нам не удалось поэтому было придумано следующая мелкие файлы остались воскресе крупные файлы переехали на левый fs тут же выяснилось что лево fs довольно медленно и хранилища мы тогда же попробовали разные другие кластерные хранилища они все довольно медленные но ещё оказалось что ни одно из них не транзакционные сюрприз правда мы же не то есть зачем нужна транзакционные файловая система транзакционные файловая система нужна для того чтобы изменить весь сайт целиком если пользователь опубликовала новый сайт он не хочет чтобы он в течение получаса по файлик у менялся он хочет пусть он даже будет публиковаться полчаса он пользователь хочет чтобы сайт был опубликован весь целиком новый ни одно из тестированных нами хранилище ту функциональность и нам не дало мы могли бы ее реализовать на пасек совместимых plaster на файловые хранилища с помощью 7 линков ну как это делается на стандартной файловой системе но оказалось что ни одно из них не обеспечивает нам достаточного количества iops of чтобы отдавать наши 500 запросов в секунду поэтому мелкие файлы остались в адресе а вместо больших файлов из ссылок на и и блобов оказались ссылки на лево их из хранилища и все опять стало хорошо ну то есть сейчас понятно что этому еще один такой костыли кпп от пихнули под нашу систему но тем не менее все стало хорошо на некоторое время 2015 год начала налево fs кончилось место и там уже было 400 миллионов файлов видимо пользователя наши все время наращивает обороты у нас 0 фас кончилось место мы решили что на добавить парочку над мы их добавили некоторое время подряд баре balancing шоу потом он как бы закончился и оказалось что мастер ну да теперь не знает где у нее какие файлы лежат ну хорошо мы отключили эти две новые ноты может быть оказалось что теперь она совсем ничего не знаю значит мы подключили эти две новые обратно до обратно и оказалось что теперь и balancing не идет вообще то есть вот вот вот так вот мы наняли той с ним и технический руководитель команды set up нанял переводчик с японского и мы позвонили в токио 8 утра поговорить с компанией rakuten компания роаккутан поговорила с нами час и простить меня я буду ее ругать потому что вот так со мной еще не обращались моей жизни я двадцать лет в бизнесе компания окутан поговорил с нами час выслушивал внимательно наши проблемы почитала наши багрепорт и в их трекере после чего сказала знаете у нас закончилось время спасибо извините вот и тут мы поняли что к нам пришел пушной зверь потому что у нас 400 миллионов файлов у нас с учетом репликация фактор три двадцать с лишним терабайт контента на самом деле там не это не совсем так но но вот реального контента там сейчас где-то 8 терабайт вот ну вот что делать поначалу вы паниковали мы пробовали расставлять по ходу метрики мы пробовали понять что вообще происходит коллега чистяков ниц не спал ночами мы пытались выяснить не хочет ли кто-нибудь из питерских ирландии став потрогать это палочкой питерские роланде 100 казались разумными людьми они не стали трогать это палочкой тут возникла идея что хорошо ладно пусть пусть но есть же взрослые парни у взрослых парней парней бывает схд мы берем схд и на эту исходя кладем все наши файлы и ведём себя как взрослые парни мы согласовали бюджет мы пошли в hetzner нам не дали схд более того когда мы придумали что мы возьмем там 9 дисковую машину поднимем на ней а из casino free везде askozia чипа внутренней гигабитной сети про бросим это все в наши отдающие сервера поднимем там вот старая хранилища на полюсе в том виде в каком она была даже посчитали вроде к в 128 гигабайт мы еще полгода влезали 128 гигабайт оперативки оказалось что вот эти 9 дисковые машины невозможно подключить к нашему кластеру по внутренней сети потому что они стоят совсем в других стойках ночью через лес никто не ожидал из нас что это я бы попадется нам под ноги а и никто не ожидал что мы все таки в зоне вот что это очередная давилка короче по общей сети he's мира эта идея не проканало вот это вот с в вас сквозь исходя из kasia там слишком большая была late in se у нас развалил разваливалась кластерная файловая система так 2015 год весна 450 миллионов файлов налево fs место закончилось совсем ну то есть вообще про путан даже не стал stabber заговаривать спасибо вот но ужас то вот тот самый тот самый ужас где тут вот да вот это вот вот это вот самое страшное что с нами случилось весной 2015 года все было хорошо еще вчера а сегодня диск sata решена в после сервере поднялся до семидесяти процентов и уже не опустился вниз а через недельку он поднялся до 80 ну и сейчас вот до переключением на наш последний вариант он врет все время держался от 95 до 99 процентов это означает что во-первых диск скоро умрут во вторых все очень плохо в смысле скорости отдачи и скорости публикации и тут мы поняли что надо как-то уже сделать шаг вперед что и на если мы то есть ситуация такая что нас этого проекта выгонит все равно после этого проект закроют потому что эти 450 миллионов файлов это и есть собственно весь проект в общем мы поняли что надо идти ва-банк про решение которое мы разработали весной 2015 года и внедрили которые сейчас функционирует я расскажу чуть позже а пока чему мы научились за то время пока все это эксплуатировать эксплуатировали во-первых отказывал стой чегось отказывал устойчивость это не от хранить файлы никому не нужна если знать что у тебя надежно спрятана 450 миллионов пользовательских файлов если пользователи не справляется получить к ним доступ файловый кеш ну я к нему надо сказать относил стать слегка с презрением пока не обнаружил что есть ли у тебя действительно высокая нагрузка ты никуда не денешься ты должен хотеть спрятать в память томку цвет не такой уж и большой на самом деле хотят составляет около 70 тысяч сайтов и около 10 миллионов файлов то есть это все в принципе если бы в честере можно было взять задешево машину стара байтом оперативной памяти мы бы все это спрятали в память и у нас опять бы не было проблем если мы были взрослые мальчики они stalker с помойки распределенные системы хранения если у вас есть деньги на большую схд которая сама по себе умеет синхронизацию с другой такой исходы когда я последний раз проверял шоссе стоила около 50 тысяч евро вот и иные диски туда какие попал не вставишь вот может быть сейчас это стало дороже а может быть подешевела на этом рынке много игроков и been killed packard эссекс они секс и msi извините тем не менее дешевые решения мы пробовали их несколько вы пробовали цех мы пробовали вот левого fs еще парочку каких-то названий которых я не запомнил если ваше хранилище заявлены как ивент шёл consistent это значит что вы попали в проблемы есть ли и ваше хранилище заявленных от стран консистенсии вы опять то есть в какие проблемы попали совершил консистенция рассказывал только что в какие проблемы попадаем so strong консистенция мы туда запихиваем файл он почему-то не запихивается и обламывается пользователь повторяет попытку публикация ложится в очередь через некоторое время учить разрастается через некоторое время оказывается что ваш файловое хранилище легло под той нагрузкой которую создают пользователя которые все время как обезьянки тычут в кнопку в корпоративной среде можно было бы объяснить им что так делать нельзя но они заплатили деньги они хотят тыкать кнопку отдельные проблемы оказался и balancing даже в тех системах где то есть первое что люди научатся делать системы цех они мучаются прикручивать и море balancing чтобы в таиф этот проблем когда в тот момент когда он нужен он не съедал сто процентов пропускной способности диска и внутренней сети а ещё она тормозит вот все те распределенные системы хранения которые мы попробовали они все страшно тормозят ну всмысле на них и укусов меньше сотни если это пассив система это вообще приговор если это объектно и хранилище то это может быть хорошо если вы у нас клево fs был пристроен varnish чтобы самый популярный контент все-таки отдавать из куша они запрашивать каждый раз оттуда чем еще мы научились тому что мелкие файлы это не файлы мелкие файлы это записи в базе данных с ними можно вот все еще что можно забрать из хранилища одним куском и одним куском отдать пользователю это все не файл это все очень удобный контент с которым можно обращаться после вскоре блок никогда его не используйте вдруг то есть можно было бы прочесть об этом в документации но я почему-то не прочел подрезку el blog не попадает в стандартную репликацию разгрузку или не в такую не всякую то есть вот реально это таблица 6 после сна и внутренняя в которой поделенный на кусочки лежит этот самый файл но эта таблица не попадает в репликацию никогда ну то есть может быть они исправят эту проблему но вот это то на что мы наткнулись уже когда у нас лежали четыре терабайта данных в этом самом пост грехи и что-либо делать было поздно реализация view вот это оказалось серебряная пуля то есть если у вас начинает подтормаживать на joiner на циpкa база то материализация view может вам помочь этого очень легко там 34 триггера и все у вас хорошо главное не забывать проверяйте что данные изменились и соответственно данный материализованные вьюки надо бы инвалидизировать самописная репликация я и очень гордился пока не обнаружил как она работает не надо не пишите сам описанную рипли ко мне говорили об этом стоит начиная с 2012 года 2012 осенью на highload я все это рассказывал с большой гордостью вот про то как прекрасно у нас работает файловое хранилище на пост грехи и мне сразу сказали что будет плохо я знал что будет плохо я не знал что так скоро значит отдельно конечно когда мы начинали со всем этим возиться мы знали что взрослые мальчики вроде vkontakte никогда не удаляют файла они только помечают их как удаленные мы подумали что нам даже помечать их как удаленные не надо у нас все будет хорошо и так пока к нам не пришел роскомнадзор и не попросил удалить вот это это это и это вот мы удалили сайт удаление сайта мы предусмотрели но оказалось что в поисковиках прячутся прямые ссылки на контрафактные картинки поэтому дальше начался реальный кошмар вот и опять привет компанией арлангур опутан java лучше чем орландо не в смысле что сама java лучше чем or long а в смысле что java программист и не достаточно сообразительна java программист и парео полезли бы в эту клоаку и починили бы нам может быть левый fs почему мы не научились мы не смогли научиться большой отказоустойчивые сходи у заказчика просто нет такого бюджета и мы его понимаем а даже если вы заказчик нашел бюджет ему пришлось бы найти бюджет на переезд из hetzner а потому что если ничего такого не дают распределенный пасек совместимые файловые системы вообще пусик самого пасек совместимая файловая система это система которая обеспечивает случайный доступ фактически она этим отличается от объектного хранилища случайный доступ на запись в случайный файл по файлу это не зачем мне нужно под наша задача поэтому вот это вот cfs она не нужна но собственно и и никто и не использует она в битах так теперь про то как мы решили проблему и будем надеяться решили ее на ближайший год как минимум а может быть и на 2 а потом конечно начнётся по новой вот мы взяли кластер но и новые скольку вот мы взяли aerospike но вообще-то можно взять любую взяли и распарить потому что у него прекрасные показатели полотенце он в прошлой осенью сделался бесплатные поэтому мы вот им воспользовались мы разделись сами рубим файлики начинки потому что и каждый чанг мы храним отдельно отдельной строкой в базе данных и отдельно его извлекаем когда он нам нужен мы написали versio не рование вот ту самую транзакционных фактически это модернизация идея 7 линков то есть у нас есть некая транзакция найди которая относится все файлы под нее загружены а потом в таблице с сайтами меняется меняется ссылка на транзакцию самописный дедов ну если уж мы сами режем на чанки то мы можем посчитать для каждого чанка ш1 мы можем этот ш1 хранить в базе как ключ доступа к этому танку в общем де-dat уменьшил нам вот это у нас так извините я расскажу подробнее у нас был дедом он был на файловом уровне мы считали сумму для файла и соответственно дублировался файла хранились в базе только один раз использование бачан кого воде до по уменьшило нам объем базы с 8 терабайт до 6 это не это чистый объем данных без рипли без репликации самописные транзакции но тут понятно что в распределенных базах данных нет транзакций вот нам пришлось написать свои которые вот чисто под ту задачу которую мы решаем ну и понятное лозы 4 сжатия мы туда прикрутили так вот это работает я не озаботился подключить свой ноут соответственно графике я вам не покажу но сейчас это все в бою сейчас это все хранит в себе те самые 450 миллионов файлов полтора миллиона сайтов отвечает отвечает на 600 запросов в секунду и это ну то есть она при этом не загружено она может больше и принимает вот 20 миллионов апдейт в сутки до 8 серверов в этом кластере 8 и это связано с тем что у нас на меньшее количество серверов не помещается контента у нас репликация фактор 3 у нас там диски 2 по 3 мы очень смелые ребята поэтому они стоят у нас в 10 вот соответственно мы имеем 8 серверов по 3 т на данный момент каждый сервер заполнен уже на 66 процентов соответственно мы собираемся взять еще два и подключить в тот же самый кластер и дождаться окончания ребаланс инга вот полторака запросов в секунду это то на чем я тестировал версия при этом не в кластер из восьми машин понятное дело а в правил производительность моего 1 устава сервера но я решил что полторака это нам это вдвоем в два с половиной раза больше чем у нас сейчас есть поэтому я на этом месте продолжать тестирование не стал но все-таки ночью через лес мы очень гладко с этой нашей на у из quelle базой переводили пользователей и все было очень хорошо пока не оказалось что мы залили по 2 терабайта на каждый сервер тут тут то есть фантастика год plaster еще пять минут назад работал а вот он уже не работает вот он отдает 500 ошибки вот он дает 404 ошибки вот она дает 403 ошибки не работает публикация в логах очень странные сообщения я не смогла найти такой то сектор вот спасибо спасибо поддержки распайка я не спал всего одну ночь на утро они мне ответили что за дела осталось пять минут и учти еще вопрос да так вот я все таки да говорю значит оказалось что нельзя сконфигурировать один файл хранилища внутри вот низкоуровневого распайки больше чем 2 т можно при этом и сконфигурировать несколько сейчас на сун конфи сконфигурирован на три файла по одному т но вот первый наш кластер ac повязки был сконфигурирован на один на каждый ноги по 1 один файл на 3 т вот как только мы добрались до 2 т она немедленно все прекратила функционировать и причем очень неприятным образом мы потеряли некоторое количество загруженного канта пользователями контента которого на старом стороже уже не было она новая он уже обломался это вот ну да это вот та самая ночью через лес то упало в овраг ты лежишь в кризис вот ты не видишь небо ты не понимаешь куда вылезти адские твари уже воют restart 1 но до занимает три часа потому что она строит в памяти индекса на вынужденность читать с диска все данные 100 мегабайт в секунду с простой sata sata диск ну и ребалансить мы проверили это мы вырубили одну ноту стер лишние данные включили и обратно 1 но до полное восстановление бластера при замене 1 надо занимает 60 часов это означает что никакого рипли пишем фактора меньше трех мы себе позволить не можем все спасибо у нас три минуты давайте быстро такой вопрос почему вы идете таким сложным путем вы же можете просто добавить ноты которые ну про шары горизонтально условно говоря похожу добавлять файлы просто на новые ноты в новой кластер отказоустойчивость как мы обеспечим на новых но до поры не добавлять да разумеется парами то есть у нас две ноты они заканчивают болеем кластер еще из двух нот 8 серверов мы возьмем 16 нет еще два нет то есть 2 2 но до закончились но которой работали пластики добавляем еще две и новая файл или он туда потом еще 2 еще 2 ну значит во первых одновременно вышедшие из строя две машины это не такая уж и редкость в детстве во вторых это все-таки то есть чем отличается raid5 от рейда который до 1 ну три три машины пусковую от рейда 10 да нет мы то есть в тот момент когда этот вопрос был актуален мы считали и оказалось что дешевле все таки вот так как мы придумали смотрите еще в январе 2015 все отдавалась с одной-единственной под глистной базы 95 процентов контента и все остальное отдавалась и слева fs если бы лео fs не поломался мы бы вообще ничего не трогали у нас все было отлично но их можно было вообще не использовать используется очень простые схемы репликации там мы видим а не достаточно умны оказались внутри снят мы не использовали достаточно простых стрим репликации мы использовали то о чем я рассказываю ну простите у вас есть опыт эксплуатации двадцати терабайт два хранилища не в смысле что я хочу вас унизить а в смысле что когда вы начинаете двигать вот когда вы начинаете с этим всем иметь дело я боя все понимаю о чем вы говорите можно не доводить до двадцати терабайт можно делить на более мелкие кусочки и держать их на этаже больше о большем количестве серверов это же бизнес часто проекта решает как на как и на что выделять деньги его голове же от администратора что что нам нужно ну ладно это вопрос уже да это известно всем такой вопрос вы не думали хранить здесь вы не думали просто разделять ее положить все файлы например на ис-3 и там провели калькуляцию нет мой перед с 3 можно воткнуть не знаю какой джин x для кэширования сколько будут сколько стоит такой трафик 600 запросов в секунду на ис-3 переднем engine для каширования и все этой самые частые данные будут лежать в кэш engine.exe и вытаскиваться и со стритом да мы считали с 3 мы считали стримы во-первых выяснили что с 3 это объектно и хранилища соответственно перед ним нам все равно нужен пост бриз для обеспечения трансакционных ти пусть даже ручной и мы ужаснулись тому сколько денег это будет стоить 2 вопроса если можно рассказать про спайкой свою настройку да да вот рассматривали в реакции с по-моему он умеет то же самое только уже из коробки первый вопрос а второй вопрос как вы решаете если у вас backup такой понять как бы к пыли вы полностью значит надеетесь на свое решение на первый вопрос и ответ нет мы не рассматривали может быть потому что в январе даже в феврале наверное 2015 года мы впали в панику может быть если бы у нас было полгода на проверке в вариантов выбора смотрели все это но так вот что мы имели в руках то мы сделали вот а второй вопрос значит нет никаких бэкапов для 8 машин по третей не бывает вот мы надеемся на versio не рование на самом деле в проекте был бекапный сервер а вот и он сдох вчера 45 секунд на нем спасибо такой вопрос а рыба лансинг реально уже проводили в бою но добавляли новые ноты как раньше вас меня за 60 часов где нет такой деградации скорости отдачи при этом но диск сатурн чтобы он прошел бушере balancing прошел за 60 часов они за 240 мне пришлось очень сильно подкрутить настроечки и старейших дисков было 100 процентов это был последний вопрос спасибо оси большое спасибо заднем это вот"
}