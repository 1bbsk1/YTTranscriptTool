{
  "video_id": "-9miX-E4-qM",
  "channel": "HighLoadChannel",
  "title": "Как пережить спринт в 1 год и переехать в Clickhouse / Денис Трайжон (MarketGuru.io)",
  "views": 648,
  "duration": 3052,
  "published": "2024-10-29T02:59:00-07:00",
  "text": "поприветствуем следующего докладчика Всем привет Меня зовут Денис я из продуктовой it компании макет Гуру А тема тема моего доклада как пережить спринт в оди год и переехать в кликхаус Ну поехали немного обо мне разработчик больше 10 лет опыта темт техлит Ну в общем было много разного а специфика работы Значит у нас продуктовая it компания Мы работаем в сфере B2B то есть наши клиенты - это продавцы на маркетплейсах соответственно из них это 20.000 клиентов в месяц 6000 в день Ну нагрузка большая не ахова дальше интереснее 60 млн товаров в сутки сть то есть 2 млн в час больше полутора миллиона поисковой выдачи в минуту Мы пропар и Ну раньше было 4 уже 5 ТБ в день с wiber наш продукт Ну тут всё классика жанра аналитика товары отзывы рейтинги статистика продаж упущенной выручки и так далее О чём будет доклад доклад будет немножко про технические вызовы с чего всё начиналось как мы к этому шли на какие Грабли натолкнулись в процессе ввода в эксплуатацию кликхаус Ну и выводы чего не будет не будет низкоуровневое математики очень сложных кейсов э там Яндекс и всё такое предпосылки Ну собственно товары на маркетплейсе есть товары это они выдаются в категориях у них есть свои бренды продавцы собственно вот эта поисковая выдача то есть пользователь вводит там например iPhone и ему выдаётся собственно какие-то товары А на Нижнем уровне они состоят из размеров со складов остатков с ценами и соответственно рейтинги довольных пользователей всё это надо собственно собирать Как собирать у нас На горизонте Двадцатый год это было когда-то давно продукт на стадии МП то есть что это значит что самые дешёвые виртуалки У хостера потому что деньги не резиновые ставим просто в каждую виртуалку Ну и заливаем туда данные как заливаем парсеры всё это дело собирают И вот так вот на в каждую базу данных на каждый день всё это кладётся Вот такая у нас функционально шардирование Даше у нас естественно никаких аналитических запросов на литу не будет у нас пог для этого у нас есть воркеры которые подсчитывают заранее всю эту статистику и кладут туда же к этому всему мы добавляем просто приложение которое смотрит туда же и отдаёт это всё пользователям в браузер реализация тут тоже всё очень простенько знат таблица фактов вот такой небольшой эмл од строчка в день товар ВС остальное засовывая просто в эту строчку в JB Ну что мы имеем Ну тут всё очень простенько огромные объёмы апдейт и инсертов не знаю какая это цифра в день получается Ну иногда жёсткий диск говорит До свидания и с этого всего вытекает следующее умирающий автовакуум потому что это пагс и в нём есть J B который очень хорошо работает из этого вытекает то что у нас раздувается таблица которая не может сама себя чистить Ну и тут собственно почти Приплыли немножко мы с этим боролись боролись А что А что делать а делать надо то что нужно бизнесу нужно выпускать новые фичи нужно их успевать посчитать А времени нет время у нас ограничено 24 часами и мы уже как бы уже не укладываемся на выпуск новых фичей Ну собственно катить перед собой это уже немножко невозможно весь этот велосипед что делать искать решение ну почитали в интернетах для таких задач хорошо подходят колоночные базы данных ну и соответственно вот кликхаус на слуху надо пробовать подумали как его а делаем маленький такой сэндбокс для того чтобы всё это потестить что да как естественно просто вместо гресса сём кликхаус и думаем что всё будет хорошо а справочники оставляем также в пассе залили проверили результаты просто отличные 1-2 секунды на любые аналитические запросы по этим данным Мы очень были довольны этим проблем естественно не нашли вот ну и собственно принимаем решение что надо собственно двигать дальше теперь вот мы такие думаем что сейчас ВС заработает И вот будем в Радужном мире первая неделя прототипа наша команда счастливые и довольны но как бы можно было сказать что они жили долго и счастливо но у нас собственно наш доклад сприн в о год немножко юмор немножко не юмор было и не смешно поставили задачу естно в жире что было за этот го всех процессов Перестройка всех схем Перестройка всех заливок Ну в общем полная тотальная Перестройка несколько раз по кругу Ну и естественно грабли переходим собственно к нашим граблям интеграционные это собственно грабли о том как мы пытаемся заливать данные в кликхаус И на что можно напороться как заливать Ну понятно надо вставлять много данных в базу в паре у нас конкурентные запросы они чами туда что-то вставляют всё более-менее нуха естественно этого не любит ему нужно побольше но пореже ну блин почитали доки есть буферные таблицы огонь берём пример из доков вот такой вот Ну естественно всё классика всу как это дела чувствует себя на таком небольшом бита во-первых большой поток инсертов бо это 2л в минуту скидываются на диск И почти одновременно естественно утилизируют в этот момент ВС цпу которое они могут по утилизировать и прикол в том что входящие инсерты ставятся на паузу это же буферные таблицы и парсинг останавливается помним про то какой был единорог у нас там в начале вот мы такой вот и получаем что делать решение ну во-первых нужно попробовать например подобрать параметры под ваш поток данных может быть из этого что-то выгорит ну мы немного например расширим оперативную память Либо попробовать асинхронные инсерты Ну немного так такое себе А хорошо подойдёт Если вы небольшие пачки например инсерта селекта вставляете ну для нас не подошло потому что ну было как-то всё равно морально неспокойно надо попробовать что-то другое и Вуаля мы нашли кафку не знаю почему мы сразу её не нашли вот что это даёт результаты парсинга складываем парсинг в этом не тормозит он вообще никак с базой не контактирует в кафку равномерка всё это заливается и о в кликхаус есть движок кавка отлично то есть руками не надо ничего переливать Ну давайте пробовать это такой простенький пример то есть табличка с кавка движком материализованная вха и всё это просто селе звёздочка Погнали в таблицу и могли возникнуть проблемы во-первых Ну такая слабенькая проблемка то что это во-первых не бесплатно и вс это весь этот балет на него нужно цпу этим немного Тяжело управлять потому что там таблицу или перестраивать или матюх надо перестраивать Ну тут всё в общем-то по классике жанра и самое мале немного совсем неприятное это то что у нас утя памяти на дистанции 3 дня и она где-то внутри кака движка на огорчило потому что приходит килер и убивает кликхаус вместе со всеми кто там жил блин как так вроде продакшн риди вс всё должно быть хорошо Ну мы начали искать и нашли очень хорошее решение под эту задачу это называется здесь представлена небольшая схема как его можно использовать Понятное дело парсеры и другие воркеры пишут в каку Тут всё понятно - это такой который ставится на кавка топики и собственно он и переливает эти данные в кликхаус всё просто шикарно во-первых не надо писать свой велосипед он уже написан за нас одно приложение может писать в том числе и на разные шарды оно это поддерживает это если у вас несколько шардов поддерживается автосхема это вообще практически бонус не надо ручками писать каждую стро параметры трешхолд Вот пример вот чтобы несколько потоков например в кликхаус ходило а не один чтобы например буфер сайз на количество строчек можно настроить где-то чуть меньше где-то чуть побольше и прямо вот большие пачки улетают из свки в кликхаус всё шикарно есть маленькие нюансы но плюсы гораздо перевешивают минусы тада отлично переходим к следующему типу это эксплуатационные грабли мы можем тут столкнуться Ну понятно в процессе использования либо с дефолтным значениями Либо со справочниками Ну потихоньку по порядку дефолтные значение а всё очень прозаично строки инты буны Да всё что угодно мы на радостях прописываем default value потому что ожидаем ожидаемого поведения но маленький нюанси эта штука работает только на инсерта а второй маленький нюанси только на инсерта на инсерта которые идут через хттп а Син он заливает через tcp сот и естественно наши дефолтные значения превращаются в тыкву Ну как бы понять простить и отпустить потому что на это повлиять нельзя но держим в уме что наши дефолтные значения превратятся в дефолтные значения которые кликхаус дефолтные значения и всё будет шикарно играем по правилам игры следующий кейс это работа со справочниками во-первых для чего Запроси к нему нужно кусочки каких-нибудь справочных данных подклеить в этом случае ну во-первых недёшево и желательно чтобы он поместился в оперативную память иначе Финита под запросом будет медленно и где-то будет даже сильно а где-то не сильно в памяти Желательно чтобы эта штука умела поместиться если мы будем пробовать какой-нибудь кше в памяти Ну и у на есть данные которые из наших Там воркеров они укладываются про в в in Memory просто актуальное значение всех данных но так как они у нас в редисе есть Ну что далеко ходить а в доках мы нашли R Dictionary шикарно поставили и какие мы видим маленькие нюансы первый нюанс а разработчики кликхаус думают ну раз они и так в памяти но Значит мы другие варианты кэширования на строни кликхаус поддерживать и не будем поэтому для этого кри поддерживаются только Директ запросы это ну ок Ну почему бы нет и второй момент что для конвейера обработки запроса Вот такая вот сессия работы с редисом онана поточная То есть если у Вас например 1000 каких-нибудь строчек в селекте И вам надо по каждой сделать Г ну Приплыли потому что он по каждой будет один за одним посылать запросы в редис Эх опять 25 Какие решения ну самое простое ито интенсивно словари не использовать второй момент что даже если всё шикарно работало на миллионе таких гетов мы бы всё равно по сети Ну наверно что-то долго делали Ну либо обращаться к этому ресу из ваших алике всё тут просто Либо например использовать просто пото с это это было неизбежно Но для тысяч айдини ков работает шикарно быстро просто задорно погнали дальше А тут как бы материализованные в юхи подкрался так сказать незаметно северный волк Всё просто ну допустим наши инсерты по миллион строчек идут на таблицу самая классика жанра на неё повешена несколько матв и каждая из них что-то скидывает в какие-то свои таблицы где-то какие-то Преда скидываются где-то почти чистые данные первая проблемка например сложные агрегации какие-нибудь там г by или там куча например агрегатов которые мы решили посчитать каким-нибудь там где-то выпендриться может захотели вторая второй момент - это подзапросы ну мы вот там например ином решили что-нибудь выкачать Да а внутри нашей матюхин на в ин попадают Рандомные айдини из вообще из любых гранул Ну понятное дело это будет работать медленно и например несколько Джой Ну собственно натягиваем это всё выпускаем в продакшн и Ну в общем нам может долго обойтись такая заливка Какие могут быть решения решение первое никаких сложных агрегатов тут всё просто никаких под запросов ещё проще ну и желательно никаких джоновна оно работает всё прекрасно но спустя 9 месяцев мы обнаружили вот такие параметры а parallel View Processing единичка что эта штука даёт а матв могут обрабатываться параллельно не знаю почему-то по умолчанию они все обрабатываются последовательно и нужно собственно провести какой-то раскопать где-то этот параметр а второй параметр Немного более спорный - это оптимизации на инсерты Например можно повысить поток записи отключив эти оптимизации которые делают убирают дубликаты В некоторых случаях Это поможет в некоторых случаях будет такое спор ненько Ну теперь следующий кейс работа с таблицей связей это просто там классика жанра таблица m2m понятное дело у нас тут есть помидоры огурцы Ну и Понятное дело что по помидорам поиск работает шикарно быстро и задорно А вот по огурцам как-то медленно что делать мы находим встроенное решение это проекции например то есть это доб таблица внутри Клик Хауса под капотом нашей основной таблицы если мы встречаем например ключ который входит в проекцию запрос пере адресуется к этой вложенной таблице тут тоже вроде ВС прекрасно удобно то что одно именно на все эти две таблицы то есть в запросах вы используете просто одно и тоже имя А какие идут проблемки первое не работают оптимизации сжатия к примеру То есть если вы в основной таблице написали нужные вам кодеки их не будет вторая проблемка то что у этой встроенной таблички и без вариантов из этого вытекает интересный момент что А если вы всё-таки используете какой-нибудь replacing ver collapsing или другие с плюшками то у вас начинают расходиться то есть либо у вас не будут убираться дубликаты и надо будет что-то придумывать к этому А либо где-то может поплыть статистика какие мы нашли решения первое очень простое никаких встроенных проекций делаем просто брутально отдельную табличку просто с именем projection Матюха переливает на неё данные которые мы сём в основную таблицу тут тоже всё классически там просто селек звёздочка если у нас шардирование сикер который просто в отдельном потоке из того же топика Кафки заливает в эту проекцию данные следующее следующее это у нас Космическая наша Одиссея А кто видел Вот такое в космосе время выполнения запросов да а что делать делать правильно их надо пристрелить тут тоже ВС ожидаемо но этого не было бы в докладе Да потому что иногда Нельзя просто взять и пристрелить какие у этого есть причины первое конвейер запроса берёт данные блоками Это первый момент второй момент ky если вы его запустите сможет убить запрос только между блоками обработки Вот пока вы бло е обва что что-то выстрелит и третий момент что если вы перешли в финальную стадию агрегации агрегатов то там кри тоже уже не работает и соответственно запрос просто с аксом уже выполнится а потом уже формально его типа пристрелили ну если выполнится Какие решения тут попроще и Ну например Мы можем поставить треды превентивно например прямо на все запросы пользователей четыре треда чтобы немножко наши запросы не гуляли по по ядрам и не не делали плохо другим а может быть поможет В некоторых случаях вот этот вот Max execution Time на например там 42 секунды а Настоятельно рекомендую использовать дефолтное значение блок сайза и всё-таки от него Не отступать Потому что когда ваш блок будет Ну например там какой-нибудь мегабайт Вот пока этот Мегабайт не кончится у вас Skill quy Не отработает ну либо поставить пси Но про него Чуть подальше Ну понятное дело в любом случае хорошо помогает Например если у вас там какая-то нативная инсталляция либо рестарт Клик Хауса теперь переходим к со запросу Мне было интересно какой самый простой способ положить кликхаус Как вы думаете есть идея ещё проще Ну например Select почему бы нет просто делаем много параллельных селекто Ну очень много надо делать Ну например 20 штук что мы получаем прод в полку лапт у нас там в миллионы лет чтение с дисков и Прогресс почти нулевой вот а запросы там остаются в этой тайной комнате навсегда Какие решения мы для себя выработали неожиданно надо ставить лимиты и квоты на запросы это очень сильно помогает за счёт чего например превентивно на сеттинги в users xml Ну вот Т Ma 4 чтобы вот все подряд делали только использовали четыре ядра либо на сеттинге в каждом кри Если вы их с то есть мы хотим чтобы запросы отрабатывали Ну например до 2 секунд если оно быстро отрабатывает это конечно хорошо если их конкурентно много запросов Ну допустим до секунды допустим до двух мы Вот выбрали какое-то время и под это время методом тыка что ли определяем Какое количество ядер у нас на инсталляцию Клик Хауса есть и какое мастре мы туда будем вставлять соответственно от э этого параметра ЕС немножко поиграться Томы вопервых все как бы но при этом они утилизируют ядра по отдельности и меньше друг другу мешают просто ну либо вот собственно инструмент которым мы пользуемся мы его нашли и очень сильно довольны это базовая схем Куда этот чи прокси можно вставить то есть эта штука работает по принципу того же самого инса то есть реверс прокси ставится между приложениями и собственно кликхаус или кликхаус которые у вас там есть с репликами без реплик что эта штука даёт Первое это можно управлять конкурентностью запросов Какие Какое количество параллельных будет отыгрываться временем пребывания в очереди То есть если за очередь уже там долго лежит запрос Ну может быть он уже там и не актуален потому что у нас же всё-таки сла есть какие-то а третье бонус пси отстреливает запросы которые либо превышают время жизни в очереди либо например время работы в самом кликхаус Ну собственно он сам посылает этот авто Kill quy умная балансировка если например какая-то из реплик выпадает он на неё уже не шлёт запросы рассылает их на на живые кэширование бонусом идёт То есть если вы там например часто получаете одни одну и ту же аналитику Ну можно её в редис загружать А дальше будет очень быстро контроль метрик потому что там pros формат поддерживается и в любой случай можно на технические работы отправить кликхаус всё это дальше раскидывает на другие реплики А вы там например апгрейды тада Отлично Теперь оптимизационные грабли - это вот прям Самый сок когда всё хорошо работает а нужно например что-то срезать косты или ещё что-то ну начинаем где-то по под ноля надо например вот собственно кодеки настройки и тому подобное кодеки тут как бы Всё достаточно просто это колоночная база данных и она поддерживает по колоночные сжатие Ну соответственно можно их накидывать при создании хоть альтеро кодеки общего назначения могут быть могут быть специализированные под разные типы данных ну и их можно комбинировать это тоже бонус но диски не резиновые и естественно из того что у нас было мы выбираем по максимуму например зст 13 он там прям супер пушка бомба показал себя вот остальные плюс-минус тоже неплохо что-то там делали под что-то они подходят вот мы в доках нашли что нашли так и используем для лотов остального вот собственно табличку мы такие сделали и всё пушка бомба Огонь выкатили Всё радуемся дальше делаем какие-то фич проходит 6 месяцев и скидыщ очень долгие мержи очень много парто мы пытаемся увеличить лимиты там например в background по size Но это вообще никак не помогает И наши селекты просто умирают ищем причину Ну собственно Вот она причина что делать тушить пожары начинаем Естественно с себя Потом с базы выпиливаем Все странные кодеки совсем они жрут цпу ставим что-нибудь дефолтное дальше опять тушим себя и делаем рес кодеков потому что ну надо что-то делать на дефолте далеко не уе потушили единственный кейс когда он что-то там лучше это если идёт постоянный инкремент или декремент в винтах Ну у нас такого нет а горила с фпц у них всё тоже самое если тоже постоянные маленькие одни и те же инкремент декремента ну где-то флотах у нас этого тоже нет ПЦ Вроде бы он лучше чем Gorilla Но на вот прямо идеальное последовательности там до 10 раз но это вообще всё это не наши кейсы нам это не подходит и не нужно выпиливаем дальше такие маленькие законы и правила которые мы для себя выработали для коротких строк просто дефолт Ну понятное дело там будет л4 к примеру для длинных строк можно поставить за st3 чуть-чуть где-то оно будет поком пакт нее по хранению главное не переусердствовать если у вас там одни строки в таблице то не надо всем лепить за st3 будет плохо для бинарных или булевых тут просто деф вс прекрасно там не надо ничего делать для никаких специализированных Коков Просто деф теперь для отсортированных повторяющихся последовательностей Ну наме 11 там потом 444 4242 и так далее Дета Казалось бы но он хорошо к этому подходит прямо очень срезает косты для рандомных интов увы и ах сае лучшее чучу срезать чтото т64 Вот но он тоже себя он просто чуть хуже себя проявляет чем дефолт чуть-чуть процентов на 5-10 внедрили и маленький моменти - это вот этот вот ТЛ есть у нас волшебное слово Деол которое мы вставляем вот в эти инструкции кодек А что это даёт в зависимости от того как мы это потом выставим в настройках партиции У нас этот дефолт по дефолту например lz4 А в какой-то момент времени мы можем Всё пережать на з3 и соответственно ещ увеличить компрессию примерно в два раза и соответственно мы делаем этот тре компресс что вот первый месяц тут текущий у нас парсинг идёт и там всё меже быстро потому что там просто lz4 А вот когда эти данные уже предыдущий месяц который уже никаких инсертов нет И межей там нет Вот тогда мы их финальном за std 3 отправляем в архив что это Дат Ну понятное дело количество строчек и сжати по А вот на диске мы таким образом вот на такой просто партиции из какой-то таблицы мы собственно срезали косты считаю больше чем в два раза то есть в сжатом виде оно получается гораздо эффективнее понятное дело в разных таблицах это ситуация будет немножко по-разному где-то у вас Чем больше рандома тем ситуация будет Похуже чем больше повторяющихся или е каких-то значений с какими-то инкремента там будет пошир ситуация Ну мы были бы не мы Если мы в них не вляпались особенно если у нас ничего не тормозит если непонятно что не тормозит если Непонятно из-за чего Это не тормозит А вот в какой-то момент времени у нас не тормозило буквально всё потому что кликхаус не тормозит так что же мы там натворили поставили 40 трев но у нас 40 ядер Почему бы не поставить 40 трев Пускай все используют чем быстрее тем лучше поставили Max Block size на 10 Мб ну чем больше блок Ну мы там прочитали в документации чем больше блок тем как бы быстрее он поработают агрегации на этом блоке не будет как бы прерываться на следующие блоки Ну типа быстрее Аа ну тут более-менее по документации то есть background Pull size мы поставили чуть меньше потому что у нас там ядер было в принципе не резиновое количество но мы добавили concurrency 2 ну чтобы он одни и те же ядра использовал для разных мержи и мутаций вот ну и F po size мы вообще не знали какое хорошее значение просто поставили 36 почему-то не 42 какие варианты мы выбрали более оптимальными и более рациональными maxs примитивно в пять это компромиссный вариант ядер там 40-80 где-то в том районе у кликхаус вот мы на каждый запрос превентивно п если никто не передал какое-то специальное значение в settings вот он использует только п блок сайз мы вернули просто к дефолтном значению которое себя хорошо показывает и больше его не трогаем просто на него дышим 65.000 Si выставили в 24 треда Ну то есть от 40 или 8 ядер суммарно Вот мы там где-то 30% зарезервировать для мержи Пускай себя используют но кокун поставили один к сожалению сколько мы не наблюдали на conc 2 там начинается какая-то лютая дичь то есть либо у них какая-то борьба происходит либо ещё что-то в общем не прогнозируемые мержи которые улетают куда-то там за 1.000 секунд вот поэтому на единички всё вот прогнозируемое на зарезервировано количестве ядер всё простенько 128 F Pull size Ну чтобы быстро обмениваться партами с другими репликами реплик у нас не не очень много их там всего четыре было но в общем оно тормозило на 128 всё хорошо быстренько в эти парты перекидываются между ними репликация четвёрки для мы оставили такими же они хорошо соотносятся с 24 то есть из че трев мы четыре резервирует успехи во-первых нет борьбы за цпу всё всё прогнозируемое стабильно работает никаких зависаний там всё пристреливать парв между репликами быстрый и меже прогнозируемые ну и соответственно ба стартует быстро там е были дополнительные параметры Но их сюда не ВС почти дефолтные мы их там чуть-чуть задрали потому что в каких-то местах у нас большое количество Патов возникало и это было нормально так так и задумано и собственно маленькие парты до 100 мб там тоже есть отдельная настрое они хорошенько мется Вот в этих своих четырёх трех кейс это нужны корректировки данные меняются где-то на маркетплейсах что-то поменялось где-то они нам вообще какую-то лютую дичь отдали нам теперь надо переделать перепилить А это всё числовые данные по которым нам нужно считать статистику и естественно За любые дни это может залететь и как бы что-то с этим делать Ну понятное дело alter Del Это не наш вариант Потому что это не прогнози и медленно пометка будет расходиться Ну мы остановились на мы увидели там умножить на типа всё Панацея Всё огонь Ну Пацан сказал пацан сделал естественно Мы накатили это в продакшн считаем первый день вообще всё шикарно норм Второй день уже начался какой-то скрип самое странное что у нас остались старые данные Почему Почему Почему их вообще в два раза и как это вообще могло произойти Ну ничего лучше чем пойти в исходники мы не нашли пошли в исходники обнаружили Вот примерно такую конструкцию она выглядела по-другому они там их уже немножко перепили а кратко Это что значит что для верши колан он считает количество плюсиков и минуси и если они хоть чуть-чуть не совпадают о вообще ничего не собирается мержить если бы мы знали об этом сразу а правки у нас идут в 90% данных а это примерно 20 млрд в день и вот собственно тут мы сразу такие X3 Ну и вот собственно наши инсерты очень сильно тормозят и соответственно кон Мы также на мусорку выкидываем А что делать первое не использовать си -1 вообще ничем не помогает второе партия на день что-то там наделали вот на один день мы её ставим подзапрос простой вот он только проверяет мы туда уже что-нибудь записывали или нет то есть самые минимальные косты вот этот вот пред Запроси и соответственно целиком удаляем сразу целиком всю партицип Ну и мы доль просто эти 10 и старое выкинем Ну и собственно актуальные партиции просто храним в отдельном справочнике тада и у нас всё хорошо стало работать Ну теперь к релизу на продакшн Да погнали Так что могло пойти не так первое Ну в деве это всё работало и даже быстро зелёным помечено то что всё вообще шикарно и чи прокси сделали и мастре по цпу всё отлично а запросы выполняются 40-50 секунд идём дебажить из рекомендаций вот собственно два примера из документации они шикарно себя проявляют первый это explain который нам показывает количество Патов и гранул визуально очень удобно а второй - это уже через Click House client в самом конце Там очень большой список логов особенно если Трейд сделать а Самый сок это он в самом конце лапт 15 секунд и мы имеем 90 млн строчек на 2 ГБ Казалось бы цифры небольшие но прикол состоит в том что запрос не оптимизирован то есть мы заранее не знали когда мы делали на деве что это в это может вылиться потому что на деве нет никакой никах параллельных пользователей в общем-то ничего один запрос выполнили сколько там не было там гигабайтов он за о секунду те выплюнул или за полсекунды и всё всё шикарно Какие решения первое проверяем каждый запрос вот через Наши или чере Через второе следим за тревожными маркерами то есть это количество парв Кост Кост Кост Воса перед прокш делаем полное нагрузочное тестирование тада И вот теперь всё хорошо какие выводы Первое - это инструкция по использованию кликхаус и второе - это собственно сами выводы если много данных то лучше заливать сразу в кафку Если есть возможность поставить сикер соответственно через отдельный инструмент не через C Engine это всё гоним потому что ну например на шардирование это eng засовывать например в distributed Click в distributed таблицу туда засовывать а си хорошо себ проявляет если менять надо настройки Обращаемся к профессионалам Если нужны справочники пользуемся внешними например греми либо например натравлю на тот же самый кликхаус Ну там где это нужно если использовать надо Ну приходится материализованное вху Ну избегайте каскадов джоно сложных агрегации и всего остального пото что это ВС очень за блочит и если у вас в эту таблицу идт большой поток инсертов Ну Филя он будет не такой большой Если нужны проекции не делаем проекции делаем отдельные таблицы подбираем кодеки Именно под характер данных то есть после орба смотрим Какая получается последовательность чисел в той или иной колонке и если под этот характер вам это подходит значите Кок если не подходите простоте дропаем целиком если нужно что-то очень быстро желательно поместиться хотя бы в оперативную память тогда вы можете использовать inem справочники Ну на ТО на основе того же хау или Паг Вот и обязательно естественно делаем для любого запроса нагрузочное тестирование чтобы потом не делать лбк на проде и любые запросы пропускаем через пси это вот очень сильно нам помогает шардирование Может тоже очень хорошо ускорить многие запросы но надеюсь об этом на SA H 24 Спасибо Спасибо Денис в трансляции был вопрос использовали ли вы Profile guided optimization из То есть оптимизацию под конкретную нагрузку по данным снятым с эксплуатации Хауса Так мы немножко бенчмарком пользовались профайло мы смотрели как собственно показывают себя execution quy то есть уже Лог выполненных запросов сколько там например парто и как это читается Как выполняется Но это было уже где-то в середине я на самом деле не знаю пго вроде как требует пере сборки кликхаус вы свою сборку использовали мы используем просто обычную сборку образ Ну собирают просто из Докера и то есть ванильная сборка хорошо Спасибо Давайте перейдём к вопросам из зала вот уже микрофон есть Здравствуйте спасибо за доклад очень интересно у меня два на самом деле вопрос Первый вы сказали что у вас обновление данных 90% это за день за год или за всё время это в день за месяц в день за месяц а ниже месяца всё уже Ну получается предыдущие месяцы Они уже не претерпевают кардинальных изменений А вот каждый Да каждый день за вот эти текущий месяц могут быть корректировки вообще где угодно на чём угодно понял спасибо второй вопрос про справочники которые в постгрес я правильно понимаю что справочники нужны и там и там и нужна какая-то синхронизация вот есть ли у вас этот процесс Какие используете методи мы частично используем какие-то справочники в постгрес какие-то у нас остались так сказать leg ещё пока мы использовали пагс какие-то мы начали делать на кликхаус и там в принципе оно нормально если там например нет большого количества апдейта Но вот один справочник он к сожалению мы его сделали плохо и сейчас он немного добавляет дискомфорта потому что там очень большое количество апдейта остаётся всё равно не помеченных там например 70 парто и это тормозить запросы Ну в общем мы от этого отойдём Скорее всего перепили как-то на на оптимизации А так да есть большая часть на постгрес там например Это бренды продавцы ещё что-то А вот эти справочники они Разве для данных которые хранятся в кликхаус и для аналитики они не нужны они нужны просто они там например название бренда То есть это не какая-то ни какие-то аналитические данные мы их просто подклеим когда уже нам надо спасибо А Денис Спасибо за доклад у меня такой вопрос Если я правильно понял у вас сервис Сначала был на постгрес а потом начался процесс миграции на кликхаус вот в процессе миграции постгрес продолжал существовать Если да какой командой поддерживался у вас получается параллельно две СУБД существовало Как вы командные ресурсы делили при этом очень хитро мы не делали никакие новые фичи на токо на том что было уже в запасе как было тяжело Все ждали Когда будет релиз Да вы в итоге смогли полностью от отказаться или д справочники остались ть из исторических вещей перетекают в кликхаус понятно На ВС это у вас ул год да а планировали спринт Поня Спасибо можно тогда я сразу в догонку эксплуатационный вопрос задам а резервное копирование в кликхаус настроили Ну да но не сразу не сразу У меня такой вопрос Спасибо за интересный рассказ т вопрос про приоритизации запросов используете ли вы её или может быть сам как он пси умеет это делать ну то есть очередь из быстрых запросов она прямо мгновенно проходит не стоит а тяжёлый запрос там в другой очереди или может быть они динамически за там нет таких настроек чтобы приоритет на кри был и это на что-то офек но впрок можно сделать разных пользователей с разной шириной вот как бы конкурентных запросов и на те запросы которые медленные и соответственно Там пробуксовки могут быть мы там собственно ограничиваем эту ширину чтобы не ушатова собственно кнд А те например запросы которые очень быстро пролетают мы там в принципе можем чуть пошире сделать количество конкурентных и соответственно Они быстро проигрываются быстро освобождают ядра чтобы медленные продолжали что-то делать а быстрый медленный определяется от того пользователя Ну короче от очереди в какой он запи на этапе нагрузочного тестирования на этапе подготовки фичей каждые запросы тестим проверяем и дальше видно что запрос медленный Ну он идёт сюда запрос быстрый он идёт туда Спасибо функциональность прокси то есть сервер кликхаус содержит се вообще всё там там бенчмарка встроенная клиент встроенный Почему прокси или он прям тоже есть встроен а там нет очередей то есть там Можно например настроить Max concurrent quy на уровне кликхаус но если у Вас например апликейшн посылают ещё какие-то запросы они просто идут лесом просто можно попросить наверное разработчиков кликхаус встроить Ксю внутрь сервера чтобы хотя бы бинарка сервера если он уже база данных он уже хранить данные умеет и очередь там написать вроде как это ну не Rocket Science Мы мы не пытались но чи прокси уже в принципе больше года существует и если пор команда решила не встраивать наверное на это есть причина может они просто ещё не решили не встраивать А может у них есть более сложные приоритетные задачи действительно пытается сдать на Денис Спасибо большое за доклад очень интересно всегда послушать когда реальный кейс Как реально разбирали про дете вопрос Вы там показали что кодеки вы перекомбинация за меся данные поранили один там второй замм сильне сильне вопрос хороший тут какой момент помимо л4 у него там допустим показатель единичка опорное значение мы идём собственно в сторону зд и дальше 1 2 3 4 5 до 22 эффективности сжатия график То есть можно сжимать но мы увеличим время почти каждый раз в два раза и Профи мы от этого с каждым уровнем ну плюс 1% плю 2% и ради чего А насчёт выборки вообще ничего не страдает по именно раз сжатию то есть когда эти селекты проигрываются у з и у4 примерно одинаковый одинаковое время выполнения по жати Поэтому в общем-то ничего не теряем А и нет рационального смысла Здравствуйте это спасибо за доклад А я просто чуть-чуть опоздал если у вас 90% апдейта А зачем кликхаус потому что по вот тем миллиардам строчек которые собственно и апдейт а нужно статистику считать на постгрес Я не уверен то есть статистика считает на ста на статических данных а если идёт изменение данны у нас как бы есть и те данные которые и не меняются Но вот вот в том месте свет клином сошёлся что они меняются и приходится с этим жить заливаем парт убираем старый парт всё отлично а потом на это сказали вас суточный Да вот за день мы его укомплектовали на следующий день мы просто комплектуем новый вместо него А старый потом выкину то есть на следующие сутки данные не меняются данные могут поменяться вот те которые в этих 30 днях это вот за день меняются данные просто в Рандомные данные на интервале 30 дней Ну а у вас парт только суточный и вы Мочите это суточный прям в суточный парт и потом вот в одном парте сразу днях меняется Дан токо формируется он за день а внутри этого парта хранится данные за месяц то есть он также как бы за месяц но мы его один день формируем и сразу целиком выкидываем чтобы не было Альтер делив и всего остального хорошо спасибо Скажите а вот как вы агрегат считаете и сравнивали ли вы Ну вот вы сказали насчёт проекции сказали насчёт внешних Ну то есть вы считаете агрегаты отдельными таблицами сравнивали ли вы варианты когда агрегаты считаются где-то во вне Ну в том же поре например да затем уже готовые агрегаты в виде обмена секции переливаются ха ики вке вот Тай сложный вари может мы не додумались просто а агрегаты У нас есть например те которые для справочников это просто арг Максы чтобы просто лаа А вот для статистики мы их не считаем то есть мы подготавливаем данные чуть-чуть укрупнене какая-то суммарная Ну вот например из этих 60 млн товаров с какой-то уже небольшой укрупнённой стай именно за день потому что минимальный шаг день для статистики туда-сюда сюда А дальше уже идёт просто реалтайм запрос на другой табличке которая уже прямо по шардам распиле и шарды занимаются обработка этого запроса на литу То есть получается у вас дороже считать агрегат да чем чем отдать чуть быстрее по ранее рассчитанным агрегатам пользователя дороже не дороже тут момент такой что мы бы с радостью их и делали У нас например первая версия была вообще все предпочитает с агрегатами мы фоновые ворке считают прямо в кликхаус вставляют и мы оттуда просто сели лимит осет А вот вы сказали про агрегаты на день всё-таки агрегаты считаете да то есть избавляется тци это было да Ну мы укрупнение се от большого количества строчек укрупнить делать агрегацию таблицу потому что пользователь может натыкается потеряют смысл они не подойдут предпосевной из кластеров да а зачем а для скажем так дополнительной надёжности были сценарии Когда например три реплики не справлялись и мы например одну отправляли на техническое обслуживание там например диски уже тютю и соответственно Вместо неё была всегда четвёртая запасная чтобы три в ряд всегда стояли и обслуживали стабильно потому что потом начинается просадка запросы дольше выполняю проигрываются пользователи немножко негодуют это такой такой дополнительный резерв ясно и ещё ну комментарий даже вы про кавка таблице так негативно отзывались Может у вас там память текла Да текла Ну она не мгновенно текла просто типа за 3 дня Это старая версия наверное Может Попробуйте новую и вот вы сказали что кавка таблица Обязательно должна при шардирование в distributed писать это же не обязательно Ну в реплику писать в 3 всё льно будет ну надо ж на этапе шардирование тогда хотя бы процент н поставить чтобы Матюха переливалась речек чтобы Гештальт закрыть вроде бы на современных версиях не ТЕТ но у нас и нет желания к этому возвращаться потому что с кавка и сикера всё-таки больше гибкости и вариантов использования Ну вы же там тоже в Син настраиваете куда что писать Ну там проще там проще просто Либо на технические работы что-то отправить временно либо перенастроить поток это всё равно не надо делать Create или Drop Table чтобы перенастроить что-то Понятно спасибо спасибо Сейчас самое главное нам нужно выбрать самый полезный самый интересный самый хороший самы какой-нибудь ещё мне за запомнились вопросы вот от Вячеслава Отлично спасибо Вячеславу вручается подарок от партнёра Газпромнефть А вам тоже вручается приз от конференции Спасибо за участие и Давайте поблагодарим спикера Спасибо"
}