{
  "video_id": "wNpzD_1SRn8",
  "channel": "HighLoadChannel",
  "title": "Как эффективно ранжировать весь товарный Рунет / Айдар Гилажев  (Яндекс)",
  "views": 361,
  "duration": 3067,
  "published": "2024-04-17T00:59:37-07:00",
  "text": "Итак Встречайте Айдар глаже из Яндекса Всем привет Меня зовут Айдар Я из команды товарного поиска буду рассказывать как эть ве товарный для начала обо мне товарного поиска в Яндексе работаю с чего года и больша часть своей карьеры на самом деле занимаюсь поиском разных сервисах О чём доклад для начала мы поговорим в целом о том как может быть устроен поиск Это чтобы выровнять контекст Ну потому что как правило там какие-то специфичные термины но суть одна и та же в разных компаниях разных сервисах дальше поговорим пром специфику поиска Что начинает в НМ меняться как только мы добавляем вот эту приставку E Ну и в конце поговорим о том как мы замеряем наши успехи поговорим про метрики Итак для начала Про Сервис Давайте представим что вы решили купить iPhone 15 самый последнюю модель зашли в поиск Яндекса тут увидели прекрасный табик товары Нажали на него увидели карточки товаров Айфонов пятнадцатых дальше наверно перешли в эту карточку узнали все нужные вам характеристики увидели фотографии увидели отзывы рейтинги и конечно же увидели блок с ценами тут Важно заметить то что у нас в блоке с ценами представлены крупные игроки это azone wildb Яндекс maret но есть и магазины а не такие большие но в этом суть нашего сервиса собственно суть утп то что мы даём возможность выбирать У нас есть разные игроки А что ещё интересно заметить про наш сервис то что он высоко нагруженный с Маря нагрузка свыше 10.000 rps он старается работать довольно быстро для веб-поиска мы стараемся укладываться в 300 мсн девяносто девятой квантили у нас довольно большая база миллиард документов и при этом на самом деле мы не только Работаем как отдельная точка входа для пользователя у нас ещё есть такой пайплайн то что наши данные используются в других точках Яндекса это ранжирование Веба и реклама Итак давайте начала поговорим про то как может быть устроен поиск чтобы это проговорить Надо наверно начать с постановки зада зада звучит так есть текстовый запрос есть-то Кост и Наверно это не какие-то просто Рандомные результаты это хорошие результаты Ну например Потому что по запросу iPhone 15 можем показать вообще что угодно это будет стёкла чехлы наушники очень много аксессуаров и Вероятно это не то что надо показать именно по запросу iPhone 15 скорее всего пон именно про смартфоны смотрите у нас изначально получается есть уже слова про список про порядок То есть у нас возникает задача перепоя данных это задача ранжирования а в итоге получается вот такая очень примитивная схема У нас есть какая-то наша большая ком база есть стадия ранжирования есть итоговый ответ там две-три карточки которые пользователи видят и как вы понимаете эта схема она очень странная потому что ну нельзя ранжировать миллиард документов Это очень дорого по железу это ни в какие тайминги не влезет понятно что в реальности схема устроена не так в реальности У нас есть отдельная часть которая занимается тем что мы готовим документы уже хоть как-то подходит для ранжирования эта стадия называется кандидата генерация тогда схема получается чуть сложнее Опять же эта схема далека от реальности потому что в реальности Мы живём в мире где данных много на одну машинку они не лезут и на самом деле у нас будет какое-то шардирование то есть мы нашу большую базу нарежем на такие маленькие кусочки и уже стадия кандидата генерации стадия ранжирования будет работать над маленьким кусочком как только мы придём к этой идее у нас возникает ВТО такое белое пятно в этой сме потому что вот есть ранжирование внутри шарда и у нас таких шардов может быть много и есть в итоге р который ответ получает понятно что здесь должно что-то быть наверно мы должны как-то результаты объединить наверно это будет какой-то очень понятный подход в стиле просто берём лучший набор кандидатов в этой стадии возвращаем наверх у нас появляется стадия слияния и вотт этой смехом подумать что у нас наверное на стадии слияния набор документов вероятно по структуре будет отличаться от набора документов которые мы получаем на стадии кото генерации в рамках шарда наверное можно здесь добавить ещё одну ранжирование возможно можно использовать какие-то более тяжёлые фичи и можно поиграться топами ранжирования ближе к генерации и вот на уровне слияния и получить в итоге конфигурацию которая даст Ну в реальности Так оно и есть это гипотеза верна То есть у нас а есть более простое ранжирование которое назовём Лайт ранжирование есть ранжирование более сложное а более тяжёлое которые назовём хард и оно работает уже после слияния А давайте теперь приземлимся ную пока что схему к реальности на наш сервис у нас получается такая история есть очень много товаров миллиард миллиард товаров А есть 16 шардов с каждого которого приходит 250 документов а есть стадия слияния и ещё одно ранжирование там у нас 60 документов но в конце у нас будет пользователь который увидит первую страницу десяток товаров из них наверное ему там приглядеться там два-три документа А что мы видим Мы видим на самом деле воронку очень много документов чуть меньше потом на два порядка меньше и в конце совсем чуть-чуть Вот это очень важно осознавать А все наши оптимизации или какие-то продуктовые внедрения они должны быть сделаны именно с учётом того что у нас есть вот эта воронка а схема которая я показал это на самом деле очень сильное упрощение поиск устроен гораздо гораздо сложнее Ну например Потому что у нас есть богатый русский язык в нём есть отпечатки аббревиатуры синонимы есть расширения есть на самом деле например пайплайн доставки данных Мы хотим как-то шарды из нашей базы строить доставлять наверное хотим ещё делать Контр быстрой доставки данных очень много всего на самом деле есть в поиске Вот про это всё я говорить не буду будем говорить именно про те кубики которые показал на схеме раньше будем как бы зуми в них и уже про них говорить Вот давайте начнём с части про кандидата генерацию первое что надо осознать какие нам свойства от этапа ка генерации требуется Давайте опять же возьмём запрос iPhone или iPhone 15 но для начала просто возьмём пнем нашу базу что будет греем базу по тайлу видим что у нас находится там порядка 7 млн товаров по запросу iPhone А по запросу iPhone 15 там Т сотни тысяч а в схеме которую я показывал раньше было 4.000 кандидатов на самой первой стадии А что это означает то что ну кандидат генератор должен быть уже довольно Хорош он не может вернуть какой-то Рандомный набор документов потому что у нас все последующие стадии это уже исправить не могут вот такое важное требование и теперь вопрос Как нам это реализовать я тут хочу немножко ещё закопать в детали расскажу про инвертированный индекс это на самом деле не совсем классический инвертированный индекс как в книжках он с некой спецификой полезно на самом деле про это проговорить Ну наверное Вы знаете что такое инвертированный индекс Я просто кратко напомню абстрактно - Это такая таблица в столбцах которой находятся айди ники документах в строках которой находятся термы термы - Это для упрощения такие куски запроса ЕС у нас с iPhone 15 терма будет iPhone и 15 Ну есть ещё бигра iPhone 15 а на пересечении строк и столп сов находится некие числа которые говорят о том насколько данный документ по данному термо будет полезен Ну это число обычно называют релевантность дальше я раскрою как его можно посчитать А давайте представим что О'кей вот есть такая таблица она как-то там реализована физически на уровне шардов там какие-то понятные технологии Как нам сделать поиск а делаем так берём исходный запрос бьём его на термы А дальше смотрим Какие документы вот просто можно представить что мы таким итератора Идём по этой таблице по строкам смотрим Какие документы набираются если есть грама то в случае iPhone 15 берём её как есть Если же это не грамма а есть какое-то пересечение по отдельным терма Ну берём отдельный термы и суммируем например берём релевантность по 15 релевантность по iPhone суммируем получается такой какой-то докумен из них мы бем топ 250 По диту релевантности что важно заметить свойства получается то что у нас эта релевантность не считается в ран тайме она считается на этапе индексации То есть она офлайн релевантность всё это работает довольно быстро потому что мы сделаем только ИЖ очень эффективная конструкция Иране эта пония документов для конкретного терма и таким образом Это очень хорошо лечит проблему на самом деле частотных термов если Терм встречается в каждом десятом документе любые другие подходы вида Давайте там считать слово позиции как-то пересекать они будут либо дороги по времени Либо мы будем резать ть как-то наш итератор в рамках Вот вот этой Роки и всё равно не найдём нужный документ А если мы сделаем это всё в оффлайне вот эта гипотеза работает получается довольно хорошо собственно про релевантность Как ни странно очень хорошо работает подход тфф точнее что-то похожее на тфф давайте мы посчитаем опять же кратко напомню частотность терма внутри документа частотность терма по всей коллекции если Терм довольно часто встречается соответственно скор немножко вверх бустится если Терм сам по себе супер частотный по всей коллекции Ну скоро немножко падает с какой-то там скоростью А И вот очень важное свойство которое очень на а полезно для внедрение Именно таких продуктовых штук а не для текстового поиска как есть Это атрибуты Вот давайте представим что у нас есть задача не просто искать что-то по тексту искать по тексту плюс какой-то фильтр на магазин например хотим найти iPhone 15 в каком-то магазине Как нам это реализовать очень просто Давайте представим что документ - Это не просто набор текстов документ - это ещё и набор неких таких полей мы их называем атрибуты соответственно берём наш инвертированный индекс кладём отдельную строчку на каждый атрибут например атрибут магазин тоже его вводим и соответственно уже изначально кандидата генератор возвращает все 250 документов которые и хороши с точки зрения релевантности и они все проходят фильтр на Вот это требование на атрибут Если же мы бы делали какую-то логику на фильтрации вне кандидата генерации у нас бы там проце документов мы бы получается потеряли вот эту квоту ранжирования использовать бы её неэффективно Поэтому такие вещи надо делать именно на уровне Като генерации а буквально пару слов про то как ранжировать Ну понятно есть стандарт индустрии Это градиентный бустинг в нашем случае кат Boost используем ранжируются на очень понятные группы текстовый фичи в последнее время это Нейро сеет Трансформеры Бер есть кликовые фичи но это какая-то история взаимодействия пользователя с конкретными документами есть документ нае фичи в случае ком поиска опять же понятно там скидки промоакции цена и довольно полезная группа фичей визуальной фичи это семантическая близость картинки документа запросу зачем она нужна Ну например часто магазины пишут что вот это какое-то платье тайтл платье а цвет не указывает но любой разумный человек смотрит на фотографию плать понимает что крас если запрос красной плате мы такой документ вытащим На самый верх а тексто бы здесь совсем не помогли нам да как это всё реализовать и положить на нашу схему но в случае кликовые и документ чей кажется всё работает понятно это какие-то в Stage тут нет Большой разницы делаем это вот на стадии или ранжирования и там и там что 4000 документов документов работает дво быстро Понятно Если говорим про тек фичи Ну визуальных тоже это касается первой стадии ранжирования будем использовать такой двухбашенный подход мы берём текст запроса шлём его в некий сервис там скорее всего Бут юшки они там прокрут посчитают запрос ибет Вектор чисел который представляет собой текст запроса документной ибет мы будем доставать опять же из нашего жа и в итоге в конце сделаем приумножение перемножение получаем вот такую очень мощную фичу которая хорошо масштабируется там на тысячи документов А если мы говорим про Хард ранжирование здесь можно разгуляться мы на самом деле можем себе позволить слать все тексты документов как есть и запрос как есть в наш сервис с дюшка в сервис ртов Ну потому что он там прокрути не знаю там Пару десятков миллисекунд получится очень классный скор очень хороший Импакт на качество ранжирования а при этом по времени Ну мы се можем себе позволить 20 миллисекунд это не то чтобы будет замедление О'кей теперь собственно что меняется как только у нас появляется специфика икома первое что появляется - это региональность давайте представим что вот например мы ищем фотографии котиков и тут Наверное понятно что если поез из Челябинска нашёл фотографию с котиками где-нибудь в липской области но он не шибко расстроится потому что он искал картинку мы его задачу решили Если же у нас появляется ком поис где есть транзакции всё на самом деле не так потому что ну пользователь увидит например плашку то что товар недоступен в продаже или увидит там то что товар доставляется не знаю 60 дней из-за границы он расстроится уйдёт в другой сервис это не то чего мы хотим О'кей У нас есть проблема Давайте её решать Давайте попробуем а пронести её в команду имей инженеров име инженеры скажут О'кей не проблема мы её решим А понятно как Давайте напий данный документ находится в регионе пользователя или не знаю сроки доставки на майним примеров датасет соберём обучим зашиваем в прот запустим А и увидим что всё работает плохо Почему работает плохо Ну потому что например пользователи из Челябинска можем уже на этапе Като генерации набрать каких-то классных популярных документов из Москвы они действительно хорошие но они из Москвы они из Челябинска и получается что у нас кво ранжирования используется довольно неэффективно что делать Ну давайте да вспомним логику ронки Чем раньше мы начинаем делать какие-то изменения выдачи тем получается эффективнее Давайте добавим учёт регионов на этап кандидата генерации то есть положим отдельный атрибут именно про регионы на документы и соответственно будем идти не просто с текстовым запросом будем идти с пользовательским регионам А что в итоге получается В итоге получается что всё работает эффективно нет потерь в ранжировании единственное что надо договориться про региональность потому что это такая штука есть там области есть города есть улицы но мы решили что у нас хорошо работает приближение на уровне городов их там несколько сотен и индекс это этого не супер сильно распухает соответственно надо ещё понять что делать если у нас товар доставляется например по всей России ну делаем так на самом деле мы работаем не с регионами А с иерархией регионов и пользователь идёт наш поиск не с своим регионом он идёт с регионом плюс регион выше выше например Челябинск челябенская область федеральный округ и вся Россия и тогда мы можем Этому пользователю показать товар с доставкой по всей России очень Может быть там будет тот товар который надо было найти а О'кей ещё одна особенность про модели товаров весь наш опыт показывает то что пользователи очень любят разнообразие любые внедрения с разнообразием сильно красят на метрике всё получается хорошо и то же самое на самом деле с разнообразием по моделям А что я имею в виду если мы покажем три-четыре одинаковых Айфона одной одно и той же модели на разных магазинов это будет не то же самое если мы покажем такую одну красивую обогащенную карточку именно Айфона 15 как модели а внутри уже будет магазины вот вопрос как это реализовать то есть мы пришли к идее что пользователи ожидают что у нас будет товары одной модели представлены в виде карточки Давайте её реализуем Ну надо делать видимо какую-то группировку вопрос когда её делать я вот На слайде иллюстрировал что например у нас есть четыре товара четыре разных магазина но две модели в итоге получа 50 этой квот ранжирования использу неэффективно мы е как бы потеряли А могли бы что-то полезное показать пользователю Как делать Давайте опять же подумаем что наверно надо делать как можно ближе к кандидата генератору но тут Важно вспомнить то что мир устроен довольно сложно не всегда можно выделить модели на товары бывают такие товары их тоже с ними тоже надо как-то работать Ну давайте представим что у нас изначально есть два типа товаров на этапе рации это товары без модели товары с моделью дальше давайте сделаем атрибут и сделаем так чтобы наш генератор возвращал какой-то такой фейковый документ на товар у которых есть модель Этот документ будет один ну а товары без модели возвращаем как есть потому что ничего лучше мы сделать не можем если попробовать такую штуку реализовать зать Мы уся в то что не можем себе на такие простые вопросы а какая будет какой будет регион у модели и что будет в случае если ну у нас есть такая история то что товары заканчиваются как вот это свойство поддержать в модели непонятно на самом деле Понятно Мы добавим ещё одну стадию стадия будет выглядеть так ну товары без модели они остаются товарами без модели потому что ничего лучше не сделаешь а товары с моделями у которых Вот есть такой фейковый документ Модель которая Одина на все такие товары он превращается в настоящий документ который уже можно аранжировать у этого настоящего документа будет какой-то магазин А значит будет какой-то регион Ну если у нас не получается превратить фейковый документ в какой-то товар внутри этой модели внутри региона значит в принципе всё закончилось в продаже ничего нет ничего страшного Мы такой документ выбросим вместо него какой-то другой полезный добавим в итоге никаких Потер не будет е один интересный аспект сортировка по цене тут Важно заметить то что в принципе В каком-то смысле поиск толерантен к ошибкам понятно что у нас есть несколько стадий ранжирования и каждый из них улучшает Исправляет ошибки другой А и В целом если какой-то трш пролез но он будет где-то там внизу болтаться Поли он будет не сильно новить возможно он до него даже просто не долистать ничего страшного а но в случае сортировки по цене не так если какой-то трш пролез в наше ранжирование то мы со соотвественно его в самый верх переста например для Айфона пго там какой-нибудь чехол за 1 руб мы самый верх покажем пользователь довольно сильно от этого расстроится это будет прям супер сюд А что делать Ну опять же идём нашу команду и mail инженеров Они говорят Мы знаем что делать давайте мы обучим отдельную модель фильтрации давайте мы соберём датасет заем впро А ну да ещ важно наложить эту туку схему У нас есть две стадии ранжирования и непонятно куда добавлять вот эту самую фильтрацию если мы делаем на уровне Хард ранжирования у нас всего 60 кандидатов и продуктово очень странно делать сортировку по цене на шестидесяти кандидатах это довольно Странно Если же мы делаем это на этапе вот этого самого ранжирования у нас получается 4.000 документов в принципе 4.000 документов продуктово уже звучит довольно хорошо что получается получается что у нас нету на самом деле никакой стадии хар ранжирования мы просто делаем слияние возвращаем как есть документ сортированный по цене а вместо й ранжирования работает этап фильтрации по релевантности а О'кей реализовали зашили запустили эксперимент и довольно быстро по жалобам понимаем то что пользователя есть такой характерный паттерн они вот нашли какой-то классный товар они запомнили его цену дальше думают а может есть товар похожий но дешевле переключает режим сортировки по цене и видит что товар пропал Или наоборот какой-то товар появился Почему Такое может быть Ну потому что изначально в нашем дизайне мы говорили только про фильтрацию в режиме сортировки по цене а на самом деле такая же логика должна быть и в ранжировании по релевантности если мы такое не сделаем у нас товара будет либо появляться либо исчезать это будет очень юд пользователь будет такого раздражаться то есть выдача должна быть консистентной Ну и Да очень интересный такой Аспект то что любят учить модели смотреть на метрики и вот тут Э довольно забавно то что э разные метрики они скорее характерны для разных аспектов поиска если мы говорим про сортировку по релевантности нам скорее важна полнота потому что нам важно хороших документов набрать если что-то некрасивое пролезла но оно будет где-то внизу болтаться в принципе ничего страшного А в случае сортировки по цене наоборот Нам очень важна точность ошибки мы э не хотим себе прощать а и да и про метрики Ну жизнь устроена Так что как правило а есть метрики которые очень интересны бизнесу например дау за ними следят стейкхолдеры но вряд ли дау - это та Метрика которая интересна разработчикам просто Потому что если хороший разработчик обучит классную модель зашивает её в прот вряд ли он поменяет дау Это довольно сложно а то есть эта Метрика какая-то очень далёко от разработчиков но есть метрики попроще например метрики конверсии то что у нас действие завершилось каким-то успехом это уже то что более-менее чувствительно к работе разработчика он може на такие метрики смотреть Ну и наверное мы ожидаем то что рост конверсии сопровождается ростом Да ну потому что вроде как хорошие действия должны сопровождаться тем что аудитория сервиса растёт и есть совсем а метрики простые вроде количество кликов то скорее будет а там сверх длинных кликов кликов длиннее какого-то порога вот эти метрики разработчики с радостью растят они могут за один день такие метрики вырастить или там сломать испортить но при этом мы считаем что если мы делаем всё правильно Если метрики устроены разумно если мы живём в границах применимости этих метрик они устраивают такую иерархию то есть хорошее внедрение оно растит не только клики оно и растит конверсии Ну и наверное в какой-то перспективе оно растит аудиторию дау я вот проговорил про онлайн метрики которые полезны для такого сервиса на самом деле реальность такая что онлайн метрик недостаточно почему Ну потому что например Мы хотим команду расширять хотим проводить кучу экспериментов И на самом деле в наше а пространство очень много экспериментов не влезет Либо мы будем копить какие-то очереди запусков либо у нас эксперименты будет на какой-то маленький процент и тогда ответственно будет Вопрос с чувствительностью и приём кой жить так довольно тяжело с другой стороны Наверное мы хотим ещё задуматься О таргете использовать какой-то Таргет для обучения моделей опять же я упомяну что у нас есть такие User engagement метрики это конверсии или клики Почему бы не учиться на них ну в реальности история такая то что конверсий нам как поиску не всегда хватает их прям не супер миллиарды кликов конечно завались их очень много но учиться на клики себе дороже потому что очень легко прийти к ситуацию когда такая модель она показывает какие-то карточки по которым реально очень много кликов но за карточкой нету целевого действия нету покупки и это может приводить к каким-то странным эффектам когда магазины партнёры начнут оптимизировать что-то не то они начнут там какие-нибудь кликбейтные надписи добавлять на картинке там в общем что-то сомнительное произойдёт лучше так не делать А ну так получается что мы с двух сторон приходим к идее что нам нужны офлайн метрики Окей Что такое хорошая олайн Метрика олай Метрика должна как-то моделировать пользователя как это сделать Наверное мы хотим декомпозировать поведение пользователя на конкретные аспекты и ну в нашем случае Какие аспекты приходят в голову самое первое - это релевантность соответствие текста запросу Ну очень понятно берём пишем инструкцию для асессоров заводим асессоров и чем больше у нас бюджет тем наверное больше можем такую разметку добывать Но вот тут я на слайде пытался продемонстрировать Как выглядит такая инструкция фрагмент инструкции для асессоров следующий Аспект - это популярность модели но тут история такая то что не все Айфоны полезны по запросу iPhone если мы найдём в продаже iPhone восьмой он будет супер релевантный он вот по инструкции выше все критерии пройдёт но вряд ли это то что сейчас пользователи готовы покупать что делать Ну давайте возьмём с каким-то скользящим окном посчитаем что в реальности пользователи покупают внутри данные категории взвесим там каким-то коэффициентом получим вот такой СР Аспект популярности модели ещё отдельный Аспект про ugc про рейтинги Ну если пользователю полезно понимать что его выбор какой-то обоснованный если он видит что у данного товара высокий рейтинг Наверное он делает всё правильно соответственно Давайте занесём Аспект рейтинга Ну просто вычтем от какого-то среднего значения тоже в зависим на какой-то коэффициент будет Аспект рейтинга Ну и конечно же пользователи смотрят на цены как правило они смещаются в сторону дешёвых цен какие-то супер дорогие товары неинтересно супер дешёвые товары тоже на самом деле выглядят подозрительно тоже можем по по нашим доступным данным посчитать какие характерны средние цены для данной категории и получить вот такой вес и отдельная особенность которая отличает именно нас от маркетплейсов - это Аспект наличия потому что мы получаем данные от партнёров и если после клика на товар который был классный с хорошей ценой пользователь увидел что товар недоступен он довольно сильно расстроится Мы отдельно заводим такой Аспект тоже делаем с помощью асессоров снимаем скриншоты асессор смотрит говорит что вот шоте товар в наличии А но очень похоже на релевантность Единственное что наверное релевантность можно закро потому что это данные Новика наличие Каширова нельзя Это такая относительно дорогая Метрика надо каждый раз заново проверять А ну и да как нам теперь это эти аспекты превратить в что-то реальное Ну аспекты по аспекты а посчитали по документам Теперь мы почитаем скор самого документа на самом деле тут логика такая что наверное самое простое что приходит в голову - это Линейная комбинация но в принципе так оно есть за исключением аспекта вот наличия товара аспекта он Сток потому что мы считаем что все наши усилия по росту релевантности или там наличие рейтинга и чего-либо в Метрика ничего не стоит если мы товар не показали если мы товар в реальности нельзя купить если он недоступен поэтому он Сток идёт как множитель это очень сильная ошибка очень большой штраф мы прямо заем все наши усилия вот Ну соответственно если мы говорим про скор всего запроса Ну мы берём там некий такой подход некий Дискаунт за позицию чем раньше документ тем его больши вес А вот Единственное что надо на проговорить то что я проговорил про то что у нас есть какие-то веса ещё там есть другие свободные параметры их надо как-то разумно подобрать как это делать ну делаем так как у активного сервиса У нас очень много а экспериментов накопленная история про каждый а эксперимент мы знаем это был было какое-то хорошее улучшение или наоборот это было ухудшение в принципе то же самое можем теперь проделать такой реплей для оффлайн метрики сделать вердикт но утверждение такое что хорошие оффлайн и онлайн метрик они должны быть согласованы собственно берём вот такую корзину а экспериментов прогоняем по какой-то сетке веса и делаем так чтобы у нас согласованность была довольно хорошим там по какой-то функции Ну это опять же некое упрощение но подход базово такой а что в итоге получается у нас есть логи нашего сервиса Мы из них достаём фичи достаём запросы документы дальше отгружаем Вот этот пайплайн с асессора получаем разметку что здесь довольно удобно то что мы этой разметкой Можем сами управлять например мы захотели сегодня заниматься сильно больше с запросами про электронику Ну соответственно берём и отправляем на разметку сильно больше запросов про электронику а не какой-то там честный сэмпл Или например про Fion И последнее что хотел проговорить это на самом деле Клико добавка наверное все почувствовали что есть какая-то недосказанность недосказанность такая то что мы пытаемся с помощью офлайн метрики промодель вводим какие-то аспекты А это довольно сложно Ну мы не можем придумать все аспекты Потому что люди устроены довольно сложно с одной стороны с другой стороны у нас есть бездонный источник данных это клики и довольно странно их не использовать что можем сделать сделать такое берём наши логи обучаем по нашим логам модель на факт клика Не клика получаем такую модель которая предсказывает Клик А теперь берём нашу итоговую разметку для полу аранжировать метрики потому что ну мы обучаемся под онлайну должны расти с одной стороны с другой стороны в такой конструкции мы не сильно отходим от идей заложенных в офлайн метрике и получается мы явно управляем тем какой поиск мы строим и да резюме доклада в начале Да я проговорил про то как можно строить поиск дальше мы прошли вот по конкретным проблемам это регионы в поиске модели товара сортировка по цене проговори протри вчем Ну вая добавка можно было спасибо большое адар спасибо большое за доклад У нас сейчас будет сессия вопросов ответов и тебе надо будет в голове держать автора лучшего вопроса То есть ранжирование у себя построй Вот потому что мы автору лучшего вопроса подарок вопросов смотрю много я тогда свой вопрос задавать не буду так кому-то приносите микрофон первый у нас будет Денис здравствуйте А у меня вопрос пробовали ли применять Яндекс gpt для задач поисковых сценариев Аа ну конкретно вот в этой части про ранжирование Яндекс gpt не сильно применим Ну потому что у него там тенси несколько секунд и довольно сложно и в оффлайн части а Конкретно вот того что я говорил тоже не используем но эксперименты идут довольно активные могу так ответить Спасибо Здравствуйте а скажите вот данные от партнёров загружаются в базу С какой периодичностью А это очень хороший вопрос У нас есть получается два контура доставки первый контур доставки он берёт из данных по партнёрам из так называемых фидо строит вот эти индексные фа на которыми уже поднимается вот вся вся эта история с кандидата генерацией с ранжирования такой контур наверное имеет скорость там доставки порядка десятки часов есть контур быстрый он данные у него KP обновление данных там 5 минут То есть если партнёр нам присылал что у него цена поменялась или товар недоступен мы такую информацию досм за 5 минут спасибо то как раз хотел узнать что парт придётся огромные складские запасы держать чтобы не вылетать из поиска всё понял спасибо А добрый день спасибо за доклад А мне можно два вопроса один по поводу фасет поиска Вот всегда очень интересно как вот делается потому что разные товары для разных товаров разные вот эти вот вещи более релевантны а второй вопрос всё-таки Как вы придумываете вот эти вот интересные штуки в поиске кто это бизнес к вам приходит то что вы рассказывали одно дело алгоритмы иннер подол сказал мне нужно вот это вот это а вот что нужно вот это вот это вот Кто придумывает Ну то есть конечно да одно это по логам вы там смотрите А вот Ну где-то должно быть какое-то что вот а вот здорово было бы вот это ещ сделать вот кто это придумывает СБО Да я понял про первый вопрос Да я понял у насва поня длинная история но она вот даже если её фиче катить в доклад с трудом лезет это первое а второе там на самом деле есть у нас идея как сделать эту часть заметно лучше и хотелось бы уже рассказывать историю какого-то успеха историю вот изменения Ну и если кратко то опять же чем раньше начинаешь учитывать категории и параметры тем всё работает лучше Ну идея вот ровно такая как можно раньше всего нести вот Ну детали могу наверно отдельно рассказать вот а нас второго вопроса Ну история такая что Есть проекты в них есть тех Лиды и продакт менеджеры и как правило практика показывает что хорошо работаеть когда вот именно Тандем и каждый Тандем каждый участник тандема довольно сильный то есть продакт-менеджер приходит приносит какие-то идеи техлит говорит что вот это стоит довольно Дорого это дёшево а Давайте попробуем ещё вот это То есть у нас вот такая пара И если кто-то в этой паре Ну недостаточно силён получается второй участник берёт на себя чуть больше функций обычно так получается а вот ещё участник есть Добрый день спасибо за доклад У меня вопрос по поводу метрики вот этой о Stock кажется что Она довольно Критична вообще для объёмов выдаваемых результатов поиска и если там товаров нет то и вообще нет смысла считать все остальные метрики и что-то делать дальше Есть ли какая-то Вот этапность ранжирования в зависимости от этой метрики Как решается вот эта проб Да я понял ну ском довольно тяжело потому что не сечас у нас онк это вот Метрика Мы каждый день замеряем сигнал там по кому-то кро строим графики У нас есть информация просток а дальше Конечно мы пытаемся с помощью сделать так чтобы Ок держался на каком-то хорошем у сделать Ну мы пытаемся по максимуму использовать данные от партнёров если вот они говорят что вот эта информация скорее скажет о том что товар может быть не в наличии например информация о нём довольно долго не обновлялась или там какие-то другие фичи мы пытаемся это соответственно в наше ранжирование затащить Вот Но как правило так как здесь не супер какой-то большой потенциал я говорил про вот свыше 95% это делается как раз вот на последнем этапе Там ближе к ранжирования который называется Хард Ну потому что в принципе этого хватает И там какие-то фичи можно напилить не супер дорого а много вопросов Прошу прощения Привет меня зовут Айгуль Спасибо за доклад Вопрос такой ты упоминал что вы используете Трансформеры Ну двух башен архитектуру вот вопрос следующем первый вопрос На какой Таргет вы обучались ну и если можешь прокомментировать а второе Вот ты говорил что вы учитываете модель товаров Ну то есть что если там iPhone пятнадцатый столько-то гигабайт то давайте сгруппируй все такие товары вот Ну соответственно типа как как это учитывалось при обучении разных моделей вот да спасибо а ну про трансформера история такая то что когда вот мы говорим про двухбашенный подход А мы на самом деле используем Таргет только как релевантность то есть у нас там есть какая-то многомиллионное разметка ну Ну я как говорил Здесь удобно жить с оффлайн релевантность Ты каждый день что-то замеряет там за несколько лет Копится какая-то история её не жалко отдавать под обучение Трансформеров Чего угодно а это вот про первый этап ранжирования на втором этапе ранжирования когда мы отдаём все тексты как есть мы можем в принципе разгуляться и сделать несколько голов у нашей модели То есть она может не только релевантности предсказывать но и клики заказы что угодно можно такие задачи решать Вот там ещё какой-то вопрос был дадада да как как агрегировать информация про разные товары внутри одной модели при обучении разных формул Всё я понял Да это тоже на самом деле разумный вопрос потому что А если мы ранжиру вот такую одну карточку честно заглянуть в её состав всех документов А у нас их может там три сотни Это довольно накладно с точки зрения реализации мы на самом деле к этому движемся про это можно рассказать но сейчас мы скорее используем такой подход что у нас есть некая история про то что обычно лежит за этой карточкой насколько Там хорошие документы и вот мы используем такой некий исторический агрегат А честно в ран тайме заглядывать содержимое карточки Ну мы не можем то есть мы можем там два-три оффера посмотреть два-три документа Прошу прощения но посмотреть прям все три сотни Это очень дорого но возможно и не надо но пока вот сделаем пока что вот такой подход но он будет меняться Спасибо а Да привет Я хотел тебя спросить На прошлом слайде ты показывал формулу Как вы считаете Таргет и вот там была Константа или функция Вот Click Boost и я хотел бы чуть подробнее узнать как она устроена если ты конечно сейчас можешь это рассказать А да да Да дело в том что мы учимся Ну то есть я говорю В целом модель скорее про факт клика Не клика ну точнее сверх длинного клика и не клика Да там дольше 12 секунд Ну просто надо как-то отмерять фал совы клики от настоящих с одной стороны с другой стороны если пользователь завис на на каком-то сайте совсем долго или не супер долго возможно вот между этими двумя ситуациями Есть какая-то разница и довольно странно сигнал не используется поэтому мы используем такую функцию которая делает какое-то насыщение чем Поль дольше проводит времени тем вес добавки будет сильнее если он там зашёл на там 2 минуты и тут же свалил наверное Чуть поменьше надо брать вес такой добавки вот идея такая спасибо спасибо за доклад вопрос в догонку к поставке данных на каком этапе дружат данные которые долгие с теми которые оперативными типа цены и наличие на самом Нижнем этапе прямо на этапе вот када генерация документ в документ обновляют постоянно да да Ну в этом собственно Идея то есть если мы понимаем что у нас документ условно протух его нет смысла показывать мы его на самом первом этапе забракует хороший документ у нас не будет потерь Так вопросы ещё есть А да Привет Саша ВКонтакте Спасибо за доклад я правильно понимаю что на приравнен й ранжирование так можешь ещё раз вопрос задать я перевою ты ты упоминал Берт у вас вот на при ранке используется что-то что-то вроде Берта Да да да вот хотел и ты много говорил про то что часто неэффективность происходит из-за того что вот на этом Лайт ранжировании Э мы фильтруем каких-то хороших кандидатов они не попадают потом на хард ранжирование Ага и вопрос в том как ты считаешь не было ли Ну не будет ли эффективно заменить вот этот эту Берт подобную модель на какую-то мультимодальный мультимодальный инкоде Ну тут вопросы Я кажется понял да Про что вопрос тут скорее вопрос в эффективности То есть когда мы хотим считать там 4000 документов само деле больше Это упрощение делать какие-то сложные Трансформеры мы себе позволить не можем то есть мы можем что-то вот типа взять быстро посчитать коч сно расстояние достать и один раз прогнать на запрос но честно прям слать какой-то батч он в некие тайминги не влезет спасибо Так а ещё вопрос У тебя прямо это ты в голове ты ранжиру ешь эти вопросы Слушай а а вы с читерами боретесь кто Ну два раза один и тот же товар закидывают чтобы ну как бы два товара мого магазина были в ранжировании Вот смотри тут были люди которые по два вопроса задавали Вот это захотели пода получить ещ один вопрос использовали ли вы какие-то векторные индексы увидел типа Давайте использовать не нативный индекс а искать по семантической близости Да Дада возможно прямо в онлайне аранжировать по Ну на самом деле мы про это активно думаем просто текущая конструкция Она работает довольно хорошо то есть вот если задаваться целеполаганием Зачем какие-то компоненты менять именно эта стадия работает прямо очень хорошо закапывать вот на переезды которые ты говоришь Ну пока что смысла нет но про это думаем Угу так вот рука там рядом с тем человеком у кого забрали микрофон только Спасибо за доклад А вопрос про тестирование вы говорили что с помощью а тестирования подбираете веса для параметров использу разжеванием повысим ещё нет Там на самом деле гораздо сложнее Я могу попробовать так рассказать Ну потом Пожалуйста скажите отвечать на вопрос или нет У нас есть набор а экспериментов такая корзинка про каждый мы знаем насколько это эксперимент плохой или хороший Ну там грубо плюс о мину о но ещё есть там какой-то confidence там ма не посчитай дальше мы делаем такую функцию которая условно считает некий скоро согласованности Ну что типа вот такого тангенса соответственно у нас есть возможность посчитать офлайн оффлайн метрику и тоже офлайн метрике будет потенциально какой-то скор либо в плюс либо в минус тоже с каким-то конфиденс дальше мы пытаемся считать вот где у нас есть успешные корреляции меду эти д подходами им коэффициенты в оффлайн метрике по какой-то сетке Ну там опять же немножко сложнее но можно ть что мы по какой-то сетке прогоняем Вот все наши коэффициенты это делается Ну в оффлайне недорого посчитать и дальше берём В итоге самую согласованную версию где вот кривая выглядит наиболее красиво вот такой подход Ну то есть я правильно поняла что в каждом тесте подбираются по нескольким параметрам разные веса и Угу Всё поняла Спасибо так вопросов больше есть так раз раз спасибо за доклад хотел уточнить одну вот вещь Вы сказали что вы делаете фейковые документы вот эти фейковые документы они на каком этапе живут они в индексе индексы раздувают или в ран тайме и какая их жизнь да Это хороший вопрос на самом деле этот подход далёк от идеала мы тоже хотим это место переписать фейковый документ действительно живёт в индексе он занимает там место а ну то есть если у нас там условно 100 млн моделей мы вот нагрузку к этому миллиарду добавляем ещё 100 млн моделей пока что реализовано так но в целом можно лучше Понятно спасибо ты вот говорил про миллиард весь товарный енет у тебя сейчас количество вопросов тоже будет близится к миллиарду потому что я уже забыл чей вопрос мне больше понравился Надеюсь ты помнишь да да Так вопросы ещё есть давайте усложни ему задачу пусть покажет как классно он умеет ранжировать большое количест данюи есть класс Да у нас ещё есть минут три на вопросы ни в чём себе Не отказывайте а телепортируются микрофон туда пожалуйста как-нибудь а почему миллиард весь товар най рунет Почему не миллиард 50 А да это кстати хороший вопрос Когда мы технически задались вот таким вопросом Какое количество документов мы хотим держать в индексе там полмиллиарда 2 млрд 10 млрд мы на самом деле поняли что технический рост индекса - Это вообще не то что надо делать это Тупиковый путь нам надо понять какие документы товарные документы самые полезные то есть мы взяли нашу аналитику и поняли что вот в этом месяце такой-то документ там с такого-то конкретного сайта реально люди покупают например там сзона допустим и значит если он действительно часто в поиске задаётся его люди реально ищут Наверное он должен быть в нашем индексе то есть мы придумали такую метрику полноты и там достигли каких-то там показатель что вроде больше 80% вот ну то есть весь товар рне там десяток 20 20 млрд документов А нуно в принципе вот то что мы набрали миллиард оно технически нас не сильно ограничивает но это уже довольно хорошие полнота свыше восьмидесяти документов которых скорее всего ты будешь просматривать короче сначала вы отбирается элиту которая идёт в индекс и это это Элита всего лишь миллиард то как это Золотой миллиард Да получатся есть да у нас встречи так называются Да понятно вопрос да спасибо Меня зовут Марк .ru я хотел про регионы спросить вы говорили что вы их тоже добавляете в индекс а как вы То есть вы с ними прямо как с токенами текста работаете То есть у них есть какая-то релевантность да то есть как вы потом это всё учитываете А вот дада да про про атрибуты полезно проговорить они работают именно как фильтр и их задача сделать так чтобы кандидаты генератор вернул весь топ но этот весь топ прошёл фильтр То есть у них нет никакого скорой релевантности у них просто есть вот значение то что у этого документа выставлен какой-то атрибут понял Понял ну и соответственно там уже иерархия вложенность и да да да да да Всё спасибо Так всё закончилось не закончилась вон рука Что у нас по времени у вас есть минута задать вопрос и чтобы он ответил А вопрос в том сталкивались ли вы с проблемой по-разному составленных каталогов от партнёров А например кто-то передаёт название без атрибута кто-то с атрибутом брендом кто-то передаёт все атрибуты кто-то нет достав это пробле и Если да то как вы с этим боретесь Да тут есть два аспекта первый Аспект если партнёры сознательно или случайно пытаются нас Заха мы такое контролируем То есть у нас есть оффлайн процесс который называется СКК там Служба контроля качества мы прямо Валиди то что этот товар реально на сайте есть то что его реально можно положить в корзину можно заказать то есть мы проверяем что данные настоящие й процесс второй процесс то что партнёры иногда несут совсем что-то странное например они могут там какому-нибудь платью навесить title iPhone Ну случайно скорее всего ошибка вот в этом случае Скорее это что-то случайное не зло намеренное Но это как бы портит качество выдачи мы такое пытаемся тоже явно коммуницировать то что Вот ребята У вас есть какие-то баги пожалуйста их Исправьте А вот ещё у нас есть такие рекомендации Как хорошо заполнить тайтл Как хорошо заполнить Там дескрипшн документа Если вы будете следовать им наверное шансы что качество поис возрастёт будет побольше Вот примерно так выглядит коммуникация Да спасибо Так тебе нужно время твоей email модели чтобы аранжировать автора лучшего вопроса А мне вот понравился вопрос про то какие у нас таргеты у Трансформера Если не ошибаюсь Айгуля имя было хорошо Так у нас есть Два подарка первый подарок для автора лучшего вопросов Большое спасибо за вопросы и вообще Спасибо вам всем за такую активность это прям очень круто и у нас есть подарок личной для тебя за то что ты сделал такой классный доклад и внёс вклад наше сообщество большое тебе спасибо подавай ещё Давайте похлопаем Айдару"
}