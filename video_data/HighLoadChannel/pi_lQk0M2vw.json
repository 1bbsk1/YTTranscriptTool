{
  "video_id": "pi_lQk0M2vw",
  "channel": "HighLoadChannel",
  "title": "Как мы построили сервис распределённых очередей в Яндексе / Василий Герасимов",
  "views": 3104,
  "duration": 2993,
  "published": "2020-04-27T13:28:24-07:00",
  "text": "я как уже сказали василий герасимов разработчик системы распределенных очередей в яндексе называется индекс мисочку или сокращенного mq я уже более 10 лет разрабатываю разные проекты на си плюс плюс где-то последние полтора года занимаюсь распределенными очередями а них там и сегодня и поговорим наша система уже где-то на протяжении года достаточно успешно и активно используется внутри яндекса совсем недавно она появилась она стала доступной для каждого из вас в яндекс облаке доклад будет состоять из трех частей сначала мы вообще определимся что именно мы разработали вторые вторая часть это архитектурные моменты это какие-то основные моменты по архитектуре проблемы и что самое главное как мы успешно их решили и третье собственно какие-то фишечки самого процесса разработки каждому из вас может быть полезно поехали основные понятия представим что у вас есть два сервиса два микро сервиса или не очень микро сервис а и сервис б они оба подняты своими эстонцами на каких-то машинах здесь я по три нарисовал и сервиса of service б нам нужно передать какой-то сигнал киты сообщения как это можно сделать первоначально наивный такой подход мы просто запоминаем в машинах из сервиса а адреса машин сервиса b и напрямую просто им отсылаем то что хотим передать как мы видим здесь на слайде достаточно много стрелочек а именно квадратичной зависимости от количества машин и это может оказаться не очень удобно например при этом увеличение нагрузки буфер может просто переполнится и сервис начнет выходить из строя для того чтобы избежать многих проблем посередине вставляют очередь сообщение тогда как вы видите стрелочек гораздо меньше уже линейная зависимость у нас все сет все машины сервиса а просто отсылают свои сообщения в очередь очередь их надежно сохраняет подтверждает запись и тут все а уже может дальше заниматься там какими-то другими вещами не копить ничего сервис беспокойно вычитывает и так очередь от специальный сервис для надежной передачи сообщений это дает нам много плюсов например вы можете уменьшить или увеличить количество инстансов и очень просто это сделать появляется отказоустойчивость системы если очень сохранила сообщение на их никуда уже не тянет если это worker и упали мы можем все равно и их обработать сообщение пиковые нагрузки нам тоже теперь не страшны и плюсом получаем более явно такую декомпозицию системы вот несколько примеров например в яндексе есть видео робот одна часть этого робота она обходит интернет получает кита ссылки на видео и просто просто сохраняет их в очереди соответственно вторая часть уже фоновом режиме это видео обрабатывает сохраняет надежно чтобы оно было доступно и тогда уже удаляет сообщение из очереди второй интересный пример известный наверное всем кто пишет на питание framework для асинхронных задач севере и он как раз внутри себя используют для постановки задач очередь соответственно вторая часть система worker они просто берут из очереди эти задачи и выполняют все надежно и масштабируем а теперь поговорим о основных свойствах ими наших очередей wine клуб во-первых нас на shape совместим с api известных очередей amazonas queens например это означает что мы легко можем работать c лари потому что они поддерживают amazon нашей очереди persistent ные то есть чтобы чтобы подтвердить что подтвердилось отправка сообщения мы должны записать это на там необходимое число реплик после этого сразу же после этого мы можем даже потерять весит это центр плюс одну машину и все равно сообщение будут доступны наружу вот нашему сервису б кроме этого нашей очереди полностью управляем является то есть яндекс берет все хлопоты по поддержке мониторингу работоспособности системы нас поддерживается паттерн can консьюмер это означает что для одного сообщения в нас как бы она рассчитана на одного worker а есть противоположный скажем так паттерн когда квартира могут подписаться на сообщения тогда всем будет приходить уведомления нашей очереди неупорядоченные стандартные есть еще фифа очереди который поддержит частичный порядок но соответственно что это значит что они неупорядоченная это значит что все worker работают параллельно никто никого не ждет для того чтобы получить это сообщение просто получает то которой можно получить есть разные системы очередей некоторые из них ну вот наподобие кафки стремятся оптимизировать свою пропускную способность то есть там через них можно довольно много мегабайт прокачивать наши же следят за в танце операции это значит что на каждую из операции в очереди будет будет длится порядка 100 миллисекунд интерфейс довольно простой состоит из основных трех методов сэндвич для записи россии вписать и соответственно читающая страна должна явно после обработки вызвать били тысяч то есть если она этого не сделала мы получаем сообщение там другим гарниром еще раз об этом сейчас поговорим вот кратко цикл жизни каждого сообщения сначала появляется продюсер он вызывает цент соответственно очередь кладется общение в себя подтверждает запись дальше в общем-то продюсер больше никак не участвует в цикле жизни сообщение после этого у нас появляется консьюмер вызывает метод высев тогда сообщение которые он получает скрывается от остальных worker of на специальное время называемая визе билете тайм-аутом это время как предполагает api это время на которое которого должно хватить на обработку концу миром сообщения можно его на самом деле счет другим api другими методами продлите если не хватает но об этом мы сильно не будем говорить после этого консьюмер после обработки вызывает явно дэвид массаж по специальному inblu который был передан россии вам ему вот собственно вся схема дальше про архитектуру во-первых индекс мисочку построена поверх индекс do the bass pro index.dat обойтись вам вчера рассказывал мой коллега сергей пучин надеюсь вы все поставили хорошие оценки в докладу индекс мисочку это распределенная отказоустойчивая база данных который поддерживает эти транзакция с ней можно говорить на диалекте ce que el мы это называем воитель индекс квиллингу и одно из самых важных свойств она строго консистентная что это значит это означает что несмотря на то что база данных распределены там может могут быть разные реплики там не всегда они все и синхронизированы но средствами базы она не позволяет никогда прочитать киту устаревшие данные это очень важно это позволяет нам например в ответ на россию message не выдавать одно и то же сообщение до истечения визе билете тайм-аута двум разным маркером архитектура кратко у нас есть поверх сервиса балансиром ты там получает запрос по протоколу http в tps дальше перенаправляет на одну из машин сервиса там стоит им джинкс который терменируют htp соединение и дальше пробрасываем запрос выемку серверов сервер общается с базой сердце сервер чтобы выполнить задание соответственно вызова api он общается с базой что-то туда записывает и дальше отвечает клиенту очередь так как я говорил что сообщение в ней независимый не никто никого не ждет очередь у нас сортируется то есть на самом деле это shard это такая же под очередь но интерфейс на несколько шагов очереди они объединены в одну чтобы клиент не думал о том чтобы ему там масштабировать средствами клиента это все делается на нашей стране тем больше шортов тем выше мы можем обеспечить пропускную способность очереди потому что все шарды полностью параллельны и так очередь это набор шар дав так как мы разрабатываем ся на базе данных поверх базы данных shard представляет собой набор таблиц в базе данных для того чтобы выполнить какую-то операцию над очередью мы делаем запрос в базу на транзакцию и после этого выдаем пользователю результаты казалось бы у нас есть ну в общем то все кубики все компоненты которые позволяют нам построить наш вот этот интерфейс очереди это балансиры джинкс база данных что угодно но такое решение просто так вы не заработает почему тут есть два варианта наивного решения 1 что запрос поступает в наш сервер дальше мы делаем какую-то транзакцию в базу например там ищем по базе там какое-нибудь сообщение которое мы можем выдать читающей стране и дальше соответственно там делаем транзакцию и выдаем но это будет достаточно неэффективно то что база данных не очень-то любит например сканирующих запросов поэтому производительности тут большой получить не удастся можно пойти другим путем мы допустим на военке сервере располагаем какой-нибудь ешь где вот эти вот сообщения которые мы можем выдать например как то ссылки на них складируем и соответственно чтобы не делать сканирующих запросов мы выдаем пытаемся выдать из кэша но как видно на схеме каши между собой подружить довольно сложно они располагаются на разных машинах их надо как-то синхронизировать если этого не делать может оказаться что сообщение который мы пытаемся выдать она например уже кому-то выдано то есть мы получаем как бы лишнюю транзакцию которая не превращается в какой-то нормальный результат см и в итоге должны делать еще одну да если мы наткнулись на гонку то есть такое решение у нас не заработает как можно решить эту проблему довольно очевидный подход мы можем сказать что а давайте считать что у нас есть какая-то одна машина в сервисе и она ответственна за выполнение всех запросов по данные очереди стеб над новым мастером тогда появляется еще какой-то слой прокси собственно это все в амку сервере там прокси мастер это все там в одном бинарники можно расположить но это неважно появляется свой прокси который знает где находится мастер по какому-то признаку ищет его и переправляет ему запрос и тогда мастер может легко сделать себе какой-то кэш его синхронизирует не нужно он уже находится на одной машине и получаем профит получаем такую оптимизацию производительности все транзакции которые мы шлём в базу они выдают нам какой-то хороший правильные результаты который нужен конечному пользователю и так мастер дает нам согласовано каширование информации кроме того если нам вдруг понадобится мы можем например все полученные запросы от мастера взять интро как-то переупорядочить или там предположим если мы загружаем кэш то мы можем немножко подождать и потом и потом их выполнить все на одной машине на мастере то легко сделать кроме этого мастер это идеальное место для сбора пользовательских метрик ну например метрика количество сообщений в системе если мастер с ними работает ему легко вы дать пользователю эту метрику кроме того как я уже говорил мастер это не такая прям вот сто процентов обязательная штука но эта оптимизация производительности не обязательно в том плане что как мы видели есть наивное решение которое тоже будет работать могут работать просто плохо где у вас есть мастер там появляются конечно же проблемы проблемы характера что есть какие-то переходные моменты в системе когда там мастер система решает что мастер должен быть на другой ноге мастер на думы выбираем средствами vdb она может перевозить их кроме того если сеть например разделиться на две части то мы получим 2 какие-то не за два кита независимых пускай системы в которых каждый каждый мастер будет думать что он единственный как же решать эту проблему мы тут возвращаемся к одному из основных свойств vdp это строго консистентной sti мы считаем что состояние очереди которые мы получили из базы данных она всегда является верным что это значит если вдруг допустим мастер в ответ на читающий запрос он взял и поменял в базе там поле визе билете тайм-аута и мы видим что это сообщение выдавать нельзя то это значит что так оно и есть мы не можем это то мы не можем полученное сообщение выдать пользователя но мы здесь хоть и получаем лишнюю транзакцию мы здесь тем не менее остаемся консистентными в том плане что api очереди соблюдается нас по-прежнему нету двух лидеров получивших одно и то же сообщение до истечения визе билете тайм-аута на самом деле ситуация как вы все успели заметить похоже на наивное решение есть два каша и вот мы их тоже пытаемся подружить но отличие здесь в том что два мастера это достаточно маленькое время такой может быть переходный период поэтому да получается небольшая деградация системы но она быстро восстанавливается следующая проблема с которой мы столкнулись это то что несмотря на то что мы добавили мастера система все еще не очень эффективно почему потому что на каждый пользовательский запрос мы делаем минимум одну транзакцию допустим нам пришло три запросы нас and мы сгенерировали там три транзакции на запись конечно войти by достаточно производительная но если эти транзакции организовывать в один и тот же shard базы будет страдать она эффективность и тут понятно и решение мы можем однотипной транзакции например транзакции нас and транзакции на россии в транзакции наделит объединять в один бачек пачку и отправлять их вместо 3 за 1 одно с тремя сообщениями есть тут есть еще проблема что можно наивность делать как мы берем допустим 3 стенда откладываем их во временный буфер и потом в этот буфер там допустим с частотой 1 в 100 миллисекунд будем обращаться и скидывать базу то что накопилось то наверное решение но он очевидно что это даст нам в среднем 50 миллисекунд задержки как я говорил наши очереди они минимизируют light in se операции поэтому многим клиентам может быть очень важно чтобы задержка была какая то не небольшая тут в голову приходит идея из алгоритма легла то алгоритм используется для передачи данных по такой ли типе сейчас я вам расскажу коротко о них вот у нас есть временная ось нам приходит запрос опять предположим это сэндвич на одно сообщение мы понимаем что сейчас у нас нет никаких транзакций в базу и просто берем его как есть и получаем там матч из одного сообщения отправляем его в базу и ждем подтверждения записи камер транзакции в это время очевидно могут для там нагруженных допустим очередей прийти еще какие то сообщения но так как у нас есть транзакция в базу который еще не подтверждена мы берем и складируем вот эти вот новые сообщения в свой внутренний буфер то есть пока просто ждем на картинке три сообщения мы накопили то есть уже как получается некий патч после этого нам приходит уведомление что транзакция закончена то есть снова транзакций в системе неподтвержденных нету и мы руководствуясь этой логикой берем и из bocha точнее из накопленных буфере сообщений делаем batch отправляем в базу также стоит еще отметить что мы можем сформулировать там какой-то максимальный размер батя например там н сообщений если мы уже в буфере накопили эти н сообщений то дальше нет смысла ждать мы можем отправить вторую параллельную транзакцию это дает нам для очередей с низким рейтинг ну очевидно вообще никакой задержки да потому что если очередь пишет раз в час мы просто берем и отправляем без всяких задержек в базу записи для очередей с высоким рейтинг мы получаем существенное улучшение потому что все очень эффективно по чуется там пусть пусть даже какой там первый запрос у нас улетел не до заполненный потом это алгоритм очень быстро адаптируется для людей со средним рейтом мы получаем дополнительную задержку это задержка у нас будет не более времени затраченного на выполнение одной транзакции потому что запрос может прийти сразу приставок транзакций началась тогда этот новый запрос 2 он будет ждать кроме этого если клиент пишет какими-то пиками нагрузки например раз в час там сразу по 50 сообщений например то такой паттерн нагрузки он не ведет под поттер нагрузки на очередь такой не ведет к пиком нагрузки в базу данных плюс мы можем теперь эффективно ограничить количество одновременных транзакций потом данной очереди в базу что тоже нам добавлять надежности дело за малым нужно теперь эмпирически как-то подобрать собственно какой же оптимальный размер бача и какое оптимальное количество параллельных транзакций в базу у нас будет но тут надо просто взять каким-то с разными соответственно значениями пострелять и получить значение лейтон сие пропускной способности очереди если мы при дальнейшем увеличении размера бо чая или количество параллельных транзакций уже никаких улучшений не получаем ну это означает что мы достигли уже каких-то оптимальных значений я померил вот такие получились результаты на 1 шард количество fps до ботинка было в шестьдесят пять раз меньше количество fps после боатенга то есть мы получили хороший видишь также сократилась и время пользовательских операций что тоже важно клиенту ну и дополнительный довольно-таки приятный бонус уже не для клиента для нас самих то что количество параллельных транзакций то есть нагрузка на базу она также уменьшилось то есть все вот эти три параметра а не улучшились время наверное многие догадались время транзакций она сократилась потому что при уменьшении числа параллельных транзакции они просто меньше друг друга ждут кроме того транс может быть следующая ситуация что допустим у пользователя есть там несколько очередей 1 например тестовая другая там production можно даже так и мы хотим как-то устранить их взаимовлияние что это значит допустим у нас есть какой-то несколько продакшн и сервисов и один из них сломался ну например какой то ну по причине например ошибки он начал генерировать много очень читающих там запросов соответствием нагрузка на очередь увеличилась и мы ее хотим как-то ограничить у нас есть мастер то есть место где мы можем посчитать рейд запросов поставили творит лимитер и соответственно не даем исключаем взаимовлияния очередей кроме того есть небольшой нюанс сроить лимите рам так у нас очереди есть по сути едва сервисы на которых используют это читающая страна и пишущая то мы поставили отдельные рейтинги терры на разного типа операции с например если один сервис начал генерировать очень большую нагрузку на чтение много вызовов rasif message to это то так у нас отдельные отдельные лимиты на это мы таким образом все еще дадим не не испортив шим у тебя сервису который записывает на сообщения все еще дадим их записывать дальше тут про разработку мы понятное дело что система мы рассчитываем по крайней мере что она будет выдерживать какие-то нагрузки поэтому нужно разумеется нагрузочное и тестировать тут мы подошли к этому тестированию следующим образом вы организовали отдельный сервис который ну с очень простым кодом он просто вызывал там разные операции на очереди right with david и все дождь он как бы ничего не проверял никакие light инси инси мы можем посмотреть на графиках и для того чтобы узнать какие проблемы с с нашей системой будут возникать в результате нагрузки мы их узнаем от наших стандартных мониторингов как раз отладили мониторинге заодно да получается и написали тестовую нагрузку такими простыми способами но это еще не все это не вся нагрузка которую мы хотим иметь там в нашу словно тестовый кластер нам нужно быть уверенными что уже не не пиковые нагрузки будут работать не только они но и нагрузки от типичных пользователей то есть это как правило там очередь не очень большим рейтом но для них важно например что будет соблюдаться низкие light in se операций тут мы сделали еще один сервис который генерирует просто не очень большую но типичную для клиента нагрузку кроме того это позволяет нам мерить по вот этой модельные нагрузки некоторые метрики дальше какие важные эксплуатационные метрики у вашей системы ну во-первых мы верим ошибки которые мы выдаем пользователю разумеется по вине сервера пятисотке по кроме этого есть специальные метрика время между тем как сообщение записалась и reader его уже смог вычитать то есть по модельные нагрузки мы это можем отследить мы за этим тоже следим модельная нагрузка здесь нужно потому что просто reader мы не можем следить следить это на всех очередях потому что зависит от ридера когда он начнет вычитывать там данное сообщение на пути мысли worker of у него мало то вот эта вторая метрика она может возрасти поэтому нужно именно нужен какой-то сервис который моделирует ожидаемую нагрузку кроме того верим не успеете транзакций потому что они как правило являются причиной собственно вышеназванных 5 соток отдельно про зависание запросов нас сервис он внутри асинхронные соответственно очень легко написать такой код который возьмет ее никогда не ответит пользователю может быть пользователь ты переживет но это влечет в нашей системе каким-то потерям ресурсов соответственно мы не хотим чтобы такое чтобы такое происходило поставили таймер который удаляет ресурсы и сразу же а летит нам что произошло что-то очень плохое пользовательские метрики это метрики которые за которыми должен следить пользователи ну потому что мы конечно стараемся мониторить все но что-то зависит именно здесь не пользователя и нужно фиксить именно его стране вот пользователи как правило смотрят на размер очереди на число попыток прочитать сообщение что это значит если пользователь много раз учитывает одно и то же сообщение то скорее всего у него одна из двух проблем либо его worker и падают и не успевают вызвать билет либо worker и не успевают обработать сообщение за требуемой визе билете тайм-аут тогда я просто получают другие соседние worker и и это тоже на самом деле проблема есть отдельный график числа невидимых сообщений возраста самого старого сообщения в очереди они тоже разуметь помогают понять насколько очередь хорошо обрабатывается ну внутри яндекса как я говорил нас много клиентов мы опрашивали их что им нравится не нравится и многие сказали что система очень удобна тем что мы как бы кроме того что понятный и не надо им самим настраивать она еще предоставляет многие важные метрики и графики по которым сразу же можно разобраться с состоянием сервиса ну соответственно в облаке эти графики мы тоже предоставляем пользователям кроме того еще хотел сказать про эффективную работу с лагами ну наверное части из вас это покажется довольно таки очевидной вещью но нас система высоко нагруженная чтобы там смочь эффективно разобраться с лагами мы прокидываем некий request айди сквозные и там после каких-то неудач мы можем быстро разобраться что произошло с конкретным запросам но при этом так как опять система высоконагруженных много логов таким образом не запишешь и мы применяем подход сам полированием то есть допустим мы от request айди рассчитываем некий хэш и в одном случае из ста настроить и можно как угодно все записи в лог вот с этим вот request айди они осуществляются на подробном уровне то есть если что-то произошло у нас в принципе почти наверняка будет хотя бы один запрос по которым мы можем достаточно подробно посмотреть там все его события которые нас интересуют и разобраться в проблеме спасибо доклад подошел к концу ваши вопросы пробуйте тест день добрый спасибо за доклад такой вопрос как обстоит с реактивностью как для получателей так и для поставщиков сообщений в очереди не совсем понял что значит реактивность в плане обеспечения wordpress ограничения входящих данных в плане чтобы потребитель получил 100 кусков может переварить то есть до 6 м может скоро только столько лицам и я понял вопрос да это собственно очередь как раз для этого сценария и придумано на самом деле у нас как мы выяснили в очередь отправить можно достаточно много запросов скорее всего вам хватит ну хотя при этом мы все равно ограничиваем rightly эмиттером а читающая страна очереди она как бы сама вызывает россии в сочи поэтому тут ни какого бока прошу не надо а насколько она может обработать а настолько ну как бы вызовов сделает потом удалить сообщение потом начнет снова делать новые вызовы ну то есть тут она скажем так не будет проблем именно потому что вот такая пика она активно идет о нашу систему читающая страна сервиса ответила на вопросы здравствуйте спасибо за доклад вопрос такой почему не м куб и повторили пиарь конкурент хороший вопрос у amazon спс ну достаточно старая проверенная api и одно из следствий то что есть тот же севере кроме того огромное количество библиотек на всех языках программирования которые позволяют уже без там каких-то дополнительных телодвижений is connected с очереди начать с ней работать есть например даже я не помню как называется библиотека которая позволяет в очередь бра записывать нам это очень большие сообщения она там провязана с другим сервисам там с 3 соответственно вот как бы даже такие составы составные библиотеке есть что конечно очень удобно наши наши внутренние клиенты они метро очень активно используются лари потому что просто мы проскочил очередь и мунирович помимо переменные не использовать утвержденный по сути отраслевой стандарт м куб или хотя бы не поддержать там технические какие-то ограничения или просто там не верите в него лук мы мы как бы мы сделали так скажем так можно было сделать немножко по другому не вижу тут проблема хорошо пасибо василия сзади справа вот там справа упаси вода clans мне любопытно на каком решение или алгоритме у вас построены выборы вот кого ремастера лидер алекс понял вопрос на свадьбе есть для этого средства это да это все наши разработки у нас есть специальная сущность которая словно следит там за ресурсами где что лучше поднять и она выбирает где лучше сделать мастера используется какой-то там промышленный алгоритм типа ровд или какой-то ваш собственный не совсем вот это вот этот компонент системы допотопными алгоритмами гарантируется что он будет один и он тоже контролируется что он сделает какие-то изменения там не более чем в одном экземпляре соответственно таким образом он может непротиворечиво нам обеспечить мастера то есть вы опять изобрели что-то свое что то что то свое и опять изобрели опять спасибо здравствуйте спасибо за доклад у меня такой вопрос есть какие-то технические ограничения по количеству одновременных подключений к очереди по количеству сообщений в очереди ну скажем так вообще любая очередь она не создана например для хранения данных это или нет таким образом конечно предполагать что вы вычитывать эти сообщения мы просто возникают ситуации что могут отвалиться все слушатели и она начнет не начнет поступать много сообщений и вот какое то техническое ограничение существует а нам на каком-то количестве там 10 тысяч двадцать тысяч отвалится или она будет их принимать до какого-то порога какой-то пару ну скажем так миллионы вы сможете записать отлично а по одновременным подключением по одновременным подключением тут скорее вопрос там со стороны engine ксо которой изображен схеме с архитектурой ну в принципе у нас достаточно мощной машины кроме того их больше чем одна поэтому мы ни разу не упирались в какие-то такие лимиты добрый день и спасибо за доклад подскажите там говорили что пакуйте сообщения там то есть количество общений и естественно по размеру пакета этого на это именно то есть размерную сообщение имеет значения какой будет показать то есть если будет слишком большие сообщению того как это будет поставить да у нас sharp устроен так что у нас именно данное сообщение записал в отдельную таблицу де-факто мы пакуем по количеству сообщений разумеется стараемся обеспечить чтобы транзакции у нас если вы слышали вчерашний доклад сергея пучина там говорилось о том что величина данных транзакций над не больше 50 мегабайт это мы конечно тоже соблюдаем у нас есть вот эти нашей очереди они не рассчитаны на большой размер сообщений максимум что можно вот вообще записать это 256 килобайт но по нашему опыту клиенты пишет не такие большие сообщения как правило они существенно меньше по размеру когда вы считаете там что запомнилось bbs это значит некое количество средних там сообщением соединенных по вашим параметрам то есть ну например на знает сока максимальное сообщение для ети что она запомнила препарат править . да ну на самом деле у нас получилось все достаточно просто эмпирическое число оптимального размера батя у нас оказалась таким что мы не привести не превысим этот лимит 50 сообщений по api то есть все даже если все сообщения будут длиной 56 килобайт мы все равно уложимся поэтому у нас немножко упрощается но вообще да конечно за этим же легко проследить спасибо теперь вот тут справа за телевизором да я мне не вопрос я менеджер сервиса вместе с василием мы работаем на рынке и как-то прокомментировать вопрос про мкп да мы конечно ну не то чтобы не верим мы рассматривали когда планировали делать очередь какой на вообще взять интерфейс свой проприетарным придумать затем куб м м клуб и версии который взял либидо или более свежий 10 вот и леску с но силу скорее того что во-первых облака облака да то есть у нас есть некий интерес чтобы люди могли 100 мс амазонского сервиса left in шеф просто переехать подменив and point и такие кейсы мы уже видим это достаточно полезно да это первое во вторых вот как новинского василий интеграции с 3 тоже мы видим кейсы когда люди большими сочи шлют и у них прямо вазовской либо они берут экстренный клиент пересылают сообщение больше чем 200 56 к вот ну и то что мы сейчас сделали это некая точка на пути мы конечно думаем про расширение если мы увидим достаточное количество интереса там и м куб и будем поддерживать и расширять puffy чем у меня такой вопрос насчет получатели они поддерживают и push & pull модель получения сообщений о нет только пруд получатель должен явно вызвать метод rasif массаж соответственно там есть два варианта либо получатель там выставляет значение что он там не должен ждать тогда если в очереди есть сообщение они выдаются если нет тогда не выдаются пустой список приходит есть еще второй вариант когда получатель говоришь ты я готов подождать и там предположим 20 секунд тогда если за пишущую страна за эти двадцать секунд что-то запишет при простом случае если ты единственный читатель то он их получит эти сообщения то есть там нельзя сделать так чтобы держать соеденение постоянно выгребать вот эти сообщения ну и скажем так сообщение и же вот именно в нашем океане рассчитан на то что их будут обрабатывать соответственно мы какое-то время на обработку читателям сообщение с террасы не модели уведомления у нас модель что читатели то там условный worker который что-то будет делать дальше там фоновом режиме и ну и опять-таки вот как был вопрос про абак-пресс если читатель будут уведомлять то может тоже возьми дополнительной проблемы если получить слишком много сообщений то есть постоянно то есть читатель должен постоянно прошивать очередь но он обычно происходит так читатель это вот worker он посылает красив message говорит что я готов ждать там столько то максимальное значение выставляет соответственно он там просто ждет когда сообщение появляется он их получает обрабатывает и снова как бы повторять цикл он должен сам до сказать что он освободился и готов обработать новые сообщения от когда он был светом готов ждать столько то что очередь делает свой д.б. она и и туже постоянно начинает долбить типу дай мне дай мне сообщений ну у нас есть мастер соответственно он всегда знает когда появилось новое сообщение в очереди поэтому принципе долбить не обязательно а как он от white зная что при устного сообщения он узнает этот клиентов клиент вызывает send message соответственно так как все запросы обрабатываются мастером ровно и тот же мастер обрабатывает send message после этого он узнает что в очереди появилось новое сообщение можно дать то есть получается су-100 сообщения найдет не как бы не совсем через буквой д б а при обработке любого вызова api центр осилит мы обращаемся в тебе на стенд и до лета понятно там нужно явно что изменять на ридеров тоже потому что мы должны записать в базу надежно тот факт что визе били тайм-аут поменялся то есть через white by это все конечно проходит то есть мастер может в любой момент взять и отключится допустим там что-то случилось с данной машиной и мы ничего не потеряем в том плане что новый мастером у него есть способ получить через во все свое состояние понятно спасибо все спасибо большое аплодисменты спасибо большое назначь пожалуйста лучший вопрос мне больше всего понравился вопрос самый первый пара опыт прошел кто это был подойдите есть спасибо"
}