{
  "video_id": "pfTB1Txsgoc",
  "channel": "HighLoadChannel",
  "title": "Архитектура ленты и рекомендаций ВКонтакте / Андрей Якушев (ВКонтакте, VK)",
  "views": 1317,
  "duration": 2749,
  "published": "2024-04-17T01:10:18-07:00",
  "text": "сейчас будет один из самых На мой взгляд лучших и интересных докладов опять же для меня как для сотрудника другой соцсеточки Одноклассников не будем долго делать никаких подводок Встречайте Андрей Якушев будет рассказывать про архитектуру рекомендаций и ленты ВКонтакте Всем привет тема на самом деле гигантская у нас всего 40 минут а то и меньше поэтому мы где-то полетим это так помоги пожалуйста ракета не взлетела отлично чуть-чуть вперед убежали поехали ВКонтакте всем знакомый продукт очень большой 52 миллиона пользователей ежедневно заходят 2 миллиона активных авторов это только сообществ если сюда добавить пользователей то несколько раз больше продукт растет каждый год в этом году у нас фантастический рост плюс 40 процентов все классно но это все создает офигенный челленджи с точки зрения хайлода Так куда мне направлять заедает с точки зрения хайлоуда что мы имеем мы должны ранжировать ленту на 11 тысячах РПС в секунду у нас время ранжирования 600 миллисекунд 50 перцентиль не очень маленькая но не так смертельно процессе построения рекомендации мы делаем порядка 20 тысяч сетевых запросов в разные хранилища которые у нас в ранжировании используется в количестве 45 штук как мы дожили до этого но у нас было некоторые Этап развития что самой лентой рекомендации что у меня началось вообще машинное обучение такое серьезное ВКонтакте в районе 2014 года антиспам первая рекомендации друзей видео потом мы стартовали за разработку ленты рекомендации поняли что нам нужно Big Data нужна продуктовая аналитика Ну и вообще какая-то платформа для рекомендаций где-то тут я стал этим летом начиная с разработчика и получил плюс 5 килограмм к своему весу в двадцатом году мы начали Эта система масштабировать запустили наверное рекомендации всех возможных вещей которые у нас есть на платформе часть передали другим командам Но и сфокусировались на развитии ленты именно главного нашего продукта и r&d вокруг рекомендательных систем здесь я стал директором получил Еще плюс 5 килограмм и вырастил собственный пульс примерно на десятку может быть это не связанные вещи небольшие напоминание Как выглядит Лента в ней показывается записи тех кого вы подписаны ставки и рекомендации реклама блоки мы подробно поговорим про первые пункты остальные два за скопом но просто чтобы понять аукцион блоков это некоторые каждый блок это некоторые отдельные сервис который предоставляет рекомендации которые может жить как-то инфраструктуре ВКонтакте так и на каких-то других инфраструктурах о чем мы поговорим научимся строить хронологическую ленту научимся ее ранжировать рекомендовать и потом покажем как это все вместе работает для пользователя чтобы немножко так сгладить наше достаточно большое время ответа пойдем по порядку Здесь тоже что мы хотим от хронологии Мы хотим показать записи о тех на кого подписан пользователей Мы хотим иметь какие-то минимальные задержки до появления новых записей Ну и на самом деле мы хотим показывать не бесконечную ленту а там 5000 записей ниже все равно мало кто листает Ну и с точки зрения последовательности как бы алгоритма выглядит Все просто приходит пользователь мы достаем список тех на кого он подписан это порядка 300 авторов достаём айдишники записи этих авторов в каком-то хронологическом порядке Ну и с этими пятью тысячами яичниками уже занимаемся рендерингом постов которые даём на клиента на клиенте где пользователи видят свои долгожданные пять записей на первой странице есть под капотом используется много движков хранилище это всё в основном Киев город же которые персистан Киеве или сторджи которые хранят данные преимущественно в памяти интересная часть находится где-то здесь как мы вообще формируем список вот этих айтишников записей от заданных авторов нас здесь используется кэш в котором храним предыдущую сессию пользователя предыдущего хронологическую ленту делаем запрос него достаем дату последней записи и идем в основной движок где грузим записи которые появились начиная с этого момента времени мерджам эти два кусочка записей обновляем кэш и возвращаем назад пользователю переходя к этапу рендеринга Что такое движки мы поговорим чуть позже но вообще зачем здесь нужен кэш Дело в том что 5000 записи достаточно тяжелая структура даже И если мы пытаемся работать без кэша то грузимся каждый раз эту большую структуру у нас не выдерживает сеть и мы скажем вот эту нагрузку на сеть переносим с нашего основного хранилища на десятки тысяч серверов мкша и за счет этого Выживаем всё просто тут я начал говорить про наши движки что это такое есть отличные выступления или щербака можно его послушать отдельная большая Тема что для нас важно сегодня по сути движки это такие нано сервисы уже не базы данных но еще и не микросервисы они соединяют какой-то в себе задачу хранения данных и самую простую элементарную бизнес логику мы как раз много примеров сегодня такое логики рассмотрим движки однопоточные все данные хранятся в памяти и много всяких трюков используется для того чтобы это работало в онлайне под нашей нагрузкой собственно говоря движок News он реализован вот на этой концепции парадигме по сути представляет из себя набора два типа машин хранилище где мы шардировано храним данные всех авторов это выглядит как некоторые просто очередь событий Каждый раз когда вы что-то лайкаете или пишите пост в это хранилище отправляется информация о том что какой-то пользователь написал такой-то пост и над этим хранилищем уже есть мёрдж против нашей терминологии это какие-то по сути реплики абсолютно одинаковые машины когда мы приходим в этот движок запросам он со списком авторов Чьи новости Мы хотим отобразить в Ленте это профессиона разбивает на кусочки этот список и э на подписки авторов и отправляет на каждой сторож запрос Дай мне данные от этого небольшого количества авторов потом всё мёджеты возвращает а достаточно типичная схема для ВКонтакте и диагональный запрос достаточно тяжёлый движок порядка 60 серверов суммарно используется здесь для того чтобы держать нашу нагрузку а вроде собрали хронологическую ленту Но на самом деле на уже начинаем сталкиваться с такой интересной может быть задачей что популярные авторы укладывают хранилище речь Вот про те персистент пмц персистент мкши где мы храним данные о не знаю поле в возрасте каких-то анкетных данных об авторах нагрузка которая создают там авторы миллионники настолько большая что даже если мы эти данные прокашируем кш нашим самым скажем так быстро мы хранилище он всё равно будет падать этот шард с этими данными будет падать Хотя вроде он работает ну максимально быстро из него выкинули всё что только можно отвечает за одну миллисекунду и держит на одной ядро РПС 20.000 запросов но не хватает нам этого ну и здесь решение понятное что мы можем прокашировать эти данные в нескольких копиях мкша но мы не хотим для каждого пользова на платформе дублировать данные в несколько шардов Мы хотим делать Это только для буквально нескольких тысячах тысяч этих очень популярных авторов но и встает в интересные тоже Задачка Как зная только лишних пользователя понять что он вот такой супер популярный супер создает супер высокую нагрузку для этого у нас есть ответ на каждое из наших машин из тысяч Наших воркеров мы поднимаем локальное тоже Мэм кэш под названием config Engine как нетрудно догадаться там хранятся конфигурация системы которая влияет на работу бизнес-логики платформы Ну и вот этот вот списочек суперпопулярных авторов Итого мы просмотрели научились строить технологическую ленту посмотрели Какие движки используются поняли что нужно кэшировать и решили проблему популярных авторов все это дело у нас работало в шестнадцатом году порядка 400 миллисекунд вот основная проблема в том что это был Монолит и одновременно с построением ленты мы грузили рекламу аукцион блоков и на самом деле ходили вот большое количество движков Давайте научимся это ранжировать Но на самом деле Зачем нам нужен ранжирование Зачем нужны алгоритмические ленты контента много и пользователей не успевают все просматривать и в хронологической ленте на самом деле теряется контент важный для людей А вот это понятие важности оно сугубо персональное и чтобы как бы видимость важных для людей вещей была повыше мы пытаемся это дело как-то ранжировать собственно говоря что хотим от алгоритмической ленты хотим всегда показывать не просмотренный контент хотим ранжировать по релевантности чтобы это не значило Ну и принципе когда мы сели за эту задачу в шестнадцатом году что мы чем мы обладали ВКонтакте у нас было 25 миллионов активной аудитории ежедневно у нас была уже огромная кодовая база больше миллиона строк собственной политарные технологии кие PHP свой сетевой протокол РПЦ TL своей базы данных движки но все выглядит что Open Source сюда не очень легко втаскивается Но и так просто с наскоку решить ранжировать ленту не получится все еще усложнялось тем что у нас была небольшая команда Ну и опыта в команде с работы с бигдатой и Элем такого масштаба скорее не было но нам надо было запуститься за 3-6 месяцев Сейчас мы как раз рассмотрим как в этих условиях можно найти научиться ранжировать ленту мы справились кстати За 5 месяцев где-то так в принципе вот Мы научились строить хронологию и где-то после того после Тапа где мы получаем античники новостей но нам надо научиться их ранжировать добавляем сюда вот такой этап который состоит тоже из нескольких под этапов фильтрация новостей которые вы видели и куча всего связанного с ранжированием Проект подробнее поговорим а давайте начнем с простой задачи научимся хранить те записи которые пользователь просмотрел но Заодно познакомимся детальнее с особенностями и прелестями наших движков во-первых у нас системе есть холодные пользователи и горячие и все нам важны холодные практически ничего не смотрят горячие смотрят там тысячи постов и так оказалось что нам не критично нам критично потерять данные об их просмотров там давности Два двух-трёх дней потому что сразу же приходят жалобы и ну пользователи думают что платформа сломалась и в противовес этому нам не критично потерять данные просмотры буквально там 10-15 минутной давности пользователи Ну это Воспринимает как совершенно нормально и естественно Ну первое решение - это давайте всё загрузим кэш он распределенного хеша таблицу в которой напомню есть ограничения на количество памяти которые выделяются либо на количество ключей которые она содержит и если один из этих параметров Скажем мы вылезаем за него то срабатывает лру эвристика и все старое выкидывается безбожно в никуда из этого хранилища Окей первый вариант решения давайте сделаем ключик в котором до каждой событие просмотра юзер пост значение единичка если пользователь видел этот эту запись делаем мы понимаем что данные о старых холодных пользователях теряются потому что данные горячие пользователей заполняют всю таблицу и вытесняет всё через л.ру ну и плюс получаем очень большой оверхед на хранение одной записи другой вариант Давайте хранить для пользователя список в этом мкш постов которые Я видел но и как-то его там обрабатывать но в этом случае когда нам прилетает событие просмотре нам придется эти данные грузить на наш как бы PHP worker что-то с ними делать и записывать обратно А это немножко противоречит как раз концепции наших движков но и создает какую-то абсолютно избыточную нагрузку на сеть и как раз вот для таких вещей мы занимаемся разработкой собственных движков нано сервисов которые позволяют нам какой-то Элементарно абстрактную логику выносить вот с основного горкера на другие машины например в данном случае мы можем организовать циклический буфер в котором будем хранить фиксированное число записей на пользователя и когда получаем событие просмотре записи то либо дописываем начало этого циклического буфера события либо если вы пользователь уже видел этот пост но события соответствующий элемент в эфире передвига ем куда-то в начало добавляем всякие эвристики чтобы это работало по эффективней например не создаем избыточную нагрузку на жесткие диски в шестнадцатом году это была проблема и Вуаля мы создаем движок который в целом держит памяти 10 терабайт данных Окей Идем дальше переходим к интересному ранжированию в целом у нас хронологическая Лента содержала 5000 записей Но на самом деле пользователи так глубоко не смотрят и мы можем здесь немножко сэкономить ранжировать 500 записей вместо 500 и добавить этап при ран который будет через различные евристики нам помогать отбирать вот эти вот 500 наиболее релевантных записей для пользователя обычно в машинном обучении обычно в машинном обучении здесь используют какие-то легковесные модельки легковесные модельки работающие по очень ограниченному набору фичей но и у нас такой подход работает в ряде систем но в Ленте он не работает то ли из-за очень сложного продукта Я очень разнородный контент показывается в Ленте признаки слишком слабые Но несмотря на несколько попыток у нас не получилось побороть систему квот и наборы в листик которые отбирают как раз те самые заветные 500 записи для пользователя квотирование нам позволяет для задать какой-то выделить фиксированный объем для юзер контента который по статистике самые важные самые ценные для пользователя с другой стороны ограничить объём рекомендаций Ну и позволяет ещё добавить некоторую персонализацию под пользователя а эври Киев которые работают внутри каждой квоты базируется на всем хорошо известном алгоритме Round Robin который группирует посты от одного автора в бакеты и мы набираем примерно одинаковое количество записей от каждого автора На этом этапе очень интересные свойства у этого ран Дробина вообще в рекомендательных системах Идем дальше получили 500 записей нам надо научиться их ранжировать ранжирование работает на базе признаков каких-то данных на основании которых строится прогноз нам это все надо делать в онлайне Каждый раз когда пользователь открывает приложение потому что ну он не будет ждать пока мы что-то там в оффлайне посчитаем ему отдадим и у нас есть два очень тяжелых этапа экстракт фичеры с трансформ фьючерс в рамках которых мы грузим данные из всех возможных движков потом их подготавливаем в какой-то вид который пригоден для машинного обучения и на этапе скорпост уже гоняем машинку что это за признаки Но на самом деле самое эффективные признаки это разные в рекомендательных системах этого разного рода счетчики количество лайков комментов которые собрал пост или сколько раз пользователь при взаимодействовал с тем или иным автором но и конверсии отношения между этими счетчиками там например конверсия просмотра поста в количество лайков которые он собрал все эти вещи можно представить в виде векторов в виде массивов чисел либо dens вектора либо спас Вектора в которых у нас куча ноликов чуть-чуть единичек и мы их можем хранить Чуть более эффективно в памяти вот простая штука Она обычно считается Когда у вас есть инфраструктура bigdata на где-то бедате либо продуктовой аналитики и данная экспортируется в продакшн раз Ну с какой-то задержкой которую вы можете себе позволить раз в сутки раз 15 минут вот у нас достаточно понятная штука но у нас на тот момент не было в полной полностью готовой инфраструктуры bigdata и аналитики для того чтобы эти признаки считать поэтому мы стали выкручиваться в целом Наверное хайлоут это вообще про то когда вы выкручиваетесь из ситуации но и у нас были движочки поэтому мы пришли к концепции затухающих счетчиков Когда у вас автоматически без вашего участия данный число взаимодействий пользователя с автором но постепенно затухает во времени если пользователь с ним много взаимодействует то этот счетчик скажем все время поддерживается на определенном уровне А если пользователь теряет интерес к автору то этот счетчик постепенно затухает очень простая концепция её можно реализовать в онлайне Вроде выкрутились да но оказалось что Для нас это очень крутое решение потому что до сих пор оно в продакшене и крутым оно является потому что мы очень быстро Мы научились в онлайне очень быстро поддерживать все наши счетчики каунтеры в актуальном состоянии мы потеряли интерпретируемость данных потому что дома появилось какое-то сглаживание затухание для машинного обучения Все равно оно одинаково классно работает Что с честным счётчиками что такими затухающими Вот Но мы получили взамен интерпретируемости то что эти данные обновляются но считайте что в Реал тайме в онлайне а Пару слов про ранжирование ранжиру мы по релевантности Что такое релевантность Никто не знает Каждый интерпретирует Это по-своему для нас мы пришли к фреймворку много критериальной оптимизации в рамках которого вот разработали какую-то свою функцию потерь У нас есть доклад чисто про ML часть вот об этой штуке кому интересно можно послушать важно что в процессе нам Пришлось сделать свой форк буста который обучается у нас на спарке на терабайтных дата сетах в которых у нас порядка тысячи признаков и 10 таргетов и как раз вот из-за этого мультитаргета нам пришлось реализацию пропачить и чтобы она работала быстрее Но одно дело научить это обучить модель в оффлайне другое дело научиться империть предсказывать в онлайне и в 2016 году у нас не было возможности просто затащить эту библиотеку в Production потому что это какая-то сторонняя библиотека написанная непонятно как непонятно кем А у нас здесь 25 миллионов пользователей тысячи серверов но и риски от того что что-то пойдёт не так были расценены админами какой очень высокие поэтому мы опять же пошли каким-то другим путём путём кодогенерации которая до сих пор у нас в продакшене в котором мы по заданной модели машинного обучения У нас это ну деревьям ПХП код который точь-в-точь реализует логику этой модели PHP код мы пилируем все транслируем c++ код c++ Quad мы уже компилируем и отправляем на продакшн э-э собственно говоря где у нас крутится основной основная бизнес-логика такая схема но все это нам позволило достаточно быстро и без лишних споров развития инфраструктуры научиться ранжировать поэтап по стран к особо нечего добавить куча бизнес-логики которую вы не можете напрямую оптимизировать в машинном обучении но которое важна разнообразие актуальность свежесть контента много всяких таких параметров рек систем решается тут Ура Мы запустились 5 месяцев и начали развивать систему новые признаки обучения кучи экспериментов продуктовые запуски мы росли росли росли выросли Да очень большого состояния при котором у нас Лента работала за 1200 миллисекунд в среднем вот звучит страшно Но мы на самом деле подстелили соломку для пользователей выглядело все не так ужасно об этом позже но проблема с которой нам надо было работать причина Ну на самом деле система eml это задача куча данных грузим куча последовательных операций Ну и чего же греха таить много не оптимального Legacy кода который накопился но уже примерно за 15 лет за 10 лет существования платформы и тут мы начали прокачивать нашу систему куча фантастических запусков от Кирсанова про ПХП про КПП корутино и статической типизация векторные операции профайлинг все это оказалось У нас под рукой мы сделали куча оптимизации что-то даже сработало но пошли дальше добавили уже настоящий параллелизм систему И помимо корутин у нас появились воркеры это возможность часть вашей задачи просто запустить на соседнем ядре соседнем процессе вот до этого у нас все было однопоточным возможность занести нагрузку ВК Cloud куда-то там далеко отложенных задач все это позволило очень реализовать все возможные вычисления каких-то только можете мечтать Ну и надо сказать что чему это привело к тому что мы ленту стали с одной стороны ранжировать параллельно по данным на нескольких ядрах сервисы часть сервисов внесли Вот в это самое облако Ну и все все это вместе нам дало ускорение два раза результат который мы не верили в начале но спустя полтора года работы это выглядело просто как фантастика всё можно можно радоваться Мы научились ранжировать ленту при 11000 rps но и что мы еще узнали познакомились вот с движком soulnews циклическими буферами поняли Как считать признаки когда у вас нет Биг даты кодогенерация Когда у вас нету возможности линковать библиотеки но и поняли как что можно распараллелить и на текущий момент Лента именно eml часть добавляет 30% к времени работы относительно хронологической ленты на самом деле не так страшно как может показаться 30 процентов это не 60 не 70 и скорее всего намекает на то что точки оптимизации надо искать в каких-то других местах не очень связанных Давайте научимся рекомендовать что мы хотим от ленты рекомендаций Мы хотим показывать Мы хотим на самом деле рекомендовать авторов а не записи потому что авторов у нас Ну вот 2 миллиона а записи только в двадцать втором году было сделано 6,3 миллиарда на платформе и во-первых это вычислительно проще во-вторых с точки машинное обучение на авторах работает гораздо лучше чем чем на таком количестве записей которые еще очень живут живут на платформе очень ограниченное количество времени мы хотим делать фокус на качестве а не свежести ранжируем Поэтому в два раза больше заботимся о разнообразии новизне авторов то есть делаем высокую ротацию к системе Ну и как ни крути Когда вы работаете с рекомендациями вам нужно заниматься контентом анализом понимать какой контент кликбейт трек всех возможных форм и направленности но про это мы сейчас не будем разговаривать на самом деле что построить ленту рекомендации Мы практически всё уже умеем мы умеем загружать записи пользователи умеем ранжировать нам надо научиться рекомендовать авторов для пользователей тут небольшое теоретическое отступление праймбидинги про то как сейчас работают рекомендации для каждого объекта в системе мы строим некоторые его векторные представления в многомерном пространстве это каждый объект это точка в 64 100-мерном или 512 мерном пространстве и особенности этих точек в том что похожие объекты которые можно рекомендовать друг заместо друга Они находятся рядом в этом пространстве Ну и задача рекомендации сводится к тому что вам надо по точке найти ближайшие к ней точки в этом пространстве вся эта магия строится на основе анализа всех данных которые у вас есть берётся целиком матрицу подписок лайков контентные матрицы и что-то с ними делаете и благодаря вот этим имбидингам получается даже построить интересные эффекты интересные штуки например по аудиоконтроль анализу звучания песни и понять кому она может быть релевантна либо по ID сообщества предположить какие-то товары могут быть релевантные его подписчикам либо ещё более интересные по idшнику Ну пользователя вашему другу предположить Какие Вам могут быть интересные темы а методов построения бедингов очень много мы сегодня про это не будем говорить важно что они позволяют достичь разных эффектов одни методы например группируют сообщества по географии и выделяется кластер сообществ про город Киров детский Киров ревизорро Киров речные прогулки повязки город Киров и так далее а другие методы наоборот группируют по контенту и выделяется авторынок Самара авторынок Тюлень Тюмень и так далее ну и важно что это очень тяжелый процесс вычислительно тяжелый halload в мире Big даты и тут уже скорее всего вам без ходу не обойтись поэтому мы развиваем да вот тут поэтому мы сделали Всё это инфраструктуру и здесь пара чисел про один из самых тяжёлых процессов у нас это построение рекомендации друзей в рамках которого мы 300 получаем на вход матрицу Дружбы 100 миллионов на 100 млн пользователей в процессе вызываем 30 миллиардов раз функцию предикт у легковесной моделькистан Для всего этого дела резервируем 10.000 ядер CPU 10 ТБ оперативки ждём 16 часов Всё это для того чтобы пользователи для каждого пользователя найти список рекомендованных возможностей Так ну еще небольшая вот здесь пример как раз этой оффлайн инфраструктуры который у нас обрабатывает каждое действие пользователя Каждый раз когда кто-то создает контент У нас у нас стартует много-много разных вещей под капотом аналитика ходуп запись в куче разных движков контентный анализ но и самое важное для нас здесь это различные задачи на пересчет рекомендаций Вот это важный момент что как раз рекомендации строятся у нас в оффлайне в онлайне мы их пересчитываем не раз в день А примерно Каждый раз когда получаем какой-то новый сигнал от пользователя а это оказывается критичным для продукта Особенно для новых или малоактивных пользователей которых мы хотим Ну как можно быстрее завлечь на платформу и дать им максимально крутые рекомендации Ну и на самом деле пользователи тоже не готовы ждать Сейчас 23 году люди заходят на платформу и сразу ожидают увидеть какой-то максимально Крутой и релевантный для них контент вот мы это делаем за счет вот этого пересчёта рекомендаций когда получаем информацию от пользователя с одной стороны с другой стороны это нас заставляет использовать определенные методы рекомендации каким-то определенным образом построить нашу архитектуру вернемся к нашей картинке напомню Мы хотим порекомендовать авторов для пользователя чтобы сформировать ленту с построение рекомендованных авторов во многом похоже на скажем тоже ранжирование ленты у вас есть список кандидатов в рекомендации вы их ранжируете постранжируете что-то с ними делаете но на выходе для пользователя Допустим мы хотим получить 300 рекомендованных авторов Вот и сами эти кандидаты вы их можете искать через кучи разных методов я собственно говоря хотел бы рассказать Вот про тот который у нас работает в онлайне когда мы пересчитываем получаем новый сигнал от пользователя и запускаем этап пересчёта как это выглядит выглядят что у нас есть пользователь У нас есть некоторая история его активности на платформе то что он то что ему понравилось или не понравилось На что он просто кликнул все это различные действия которые цены полезные в рекомендациях и которые надо хранить храним мы их вот в том самом движке солоньюс который с вами до этого детальнее рассмотрели и эти сиды эту историю активности пользователя мы дальше какой-то магии машинного обучения обрабатываем и каждого пользователя на выходе описываем Ну примерно 10 векторами который вот получается на базе анализа около тысячи Седов а почему 10 имбидингов Но потому что у пользователей ни одна тематика интереса а много разных и как раз каждый из них каждый теме интереса соответствует некоторой этот имбиленг в процессе мы здесь вынуждены грузить вот бединги вектора для тысячи сидов Вот И делать еще какую-то гонять какую-то машинное обучение тяжелый процесс но тоже продуктовый он себя оправдывает дальше обладает теми 10 бензингами делая запрос какой-нибудь движок хранилище которое умеет находить по точке ближайшей к ней объекты многомерном пространстве мы собственно говоря формируем набор авторов как кандидатов которые отправляются в рекомендации в качестве такого хранилища Ну можно использовать хорошо известный мальчиком фаис но у нас похожее решение разрабатывают в шестнадцатом году в фаист вышел позже поэтому у нас здесь есть движок с интересным названием вот собственно говоря Мы научились искать кандидатов в рекомендации и потранжировать мы их можем примерно так же как ранжируем записи в Ленте по тем же самым признакам только уберем признаки связанные про с постами оставим только признаки пользователей автор но есть некоторые деталька что в Ленте у нас само важную роль играли признаки она копленной истории взаимодействия пользователя с ну с автором с тем на кого он подписан А это истории для рекомендованного автора нету мы еще хотим достаточно активно ротировать Ну и нам надо здесь придумать какие-то новые признаки которые на человеческом языке звучат как этот автор похож на тех с кем пользователь взаимодействовал раньше считается это через скалярные произведения между имбидингами Ну и тех признаков Мы хотим считать много для разных типов взаимодействий для разных имбидингов тоже достаточно тяжелая задача Если вы даже возьмете Вот 1000 кандидатов загрузите их вектора на ваш Back это будет уже 256 килобайт пользователя будет описывать 10 векторами вам надо будет 10 тысяч раз посчитать скалярное произведение между ними достаточно тяжело но мы вспоминаем что у нас есть свои движочки которые мы можем нано сервисы которые мы можем честь нашей логики выносить собственно говоря это мы сделали тут доработали несколько из них и перенесли всю математику с нашего основного worker вот куда-то на другой кластер вроде бы не сэкономили по цпу Да в системе просто часть нагрузки вынесли другой кластер но зато как минимум сэкономили по сети и не гоняем данные все на этом мы сделали ленту рекомендации научились рекомендовать авторов много всего узнали воспользовались ранее разработанными движками чуть-чуть его доработав но получается что этот процесс все равно очень долго работает 99 процентиль благодаря вот инструменты Саши Кирсанова про который он вчера рассказывал мы знаем что Лента работает 2 секунды и грузит аж 3 мегабайта данных в процессе построения тяжело Давайте подумаем что ещё мы можем сделать чтобы это все работало более симпатично для пользователя идем к системе какие у нас есть каши когда пользователь открывает приложение во-первых его встречает на первом месте пост предыдущей сессии которую он уже не видел и пока пользователь читает этот пост мы успеваем отранжировать Ленту И это трюк который нам позволяет значительно повысить качество продукта с одной стороны с другой стороны Ну не так сильно заботиться о скорости ранжирования но одновременно с этим мы делаем запрос на сервер за умные ленты тут нас встречает какой-то очень быстро живущий кэш на 10 минут который защищает нас от избыточно-частых ранжирований если этот Конечно не срабатывает запускаем ранжирование тут важный продуктовый момент что мы одновременно ранжируем и записи тех на кого пользователь подписан и топовые записи из ленты рекомендаций которые у нас хранятся в кэше одновременно ранжирование и того и другого позволяет добиться большей персонализации потому что кому-то использовать или скажем и предсказываем что рекомендации более нужны кому-то менее нужны Вот но встает вопрос о том чтобы поддерживать этот кэш в актуальном состоянии он нас поддерживается скажем вот этим вот независимым сервисом ленты рекомендаций которые по разным триггерам запускает пересчёт и обновляет пойдем дальше можем еще один кэш предложить и Кэш постов юзер пост Score здесь особенность в том что если пост какой-то какая-то единица контента у вас покрутилась системе некоторое время вы ее допустим 2-3 часа в ее отранжировали для пользователя посчитали Score то скорее всего в течение дальше следующего часа этот скор никак не изменится и вы можете здесь тоже сэкономить ресурсы и не ранжировать повторно именно этот конкретный пост для этого конкретного пользователя а переиспользовать скорость предыдущего раза это тоже достаточно снижает нагрузку запускаем ранжирование Ну и в момент когда мы это ранжировали начинаем формировать ленту нам успевает ответить сервисы аукциона блоков сервис рекламы которая у нас крутится вот в этом облаке где-то там далеко и мы собственно говоря формируем ленту для пользователя и того Подводя черту рассмотрели наверное основные абстракции необходимые для построения ленты решили типовые проблемы попутно прокачали инфраструктуру вообще всего ВКонтакте Ну и заполяра заполировали все в конце кашами Много чем осталось за кадром тут вот ссылочки на разные выступления которые могут быть интересны и глубже раскрывают те или иные аспекты работы системы Я же заканчиваю Всем спасибо Андрей огонь доклад Большое спасибо Мы немножко чуть-чуть совсем перебрали по времени поэтому у нас будет только два очень быстрых коротких вопросами очень быстрыми коротками ответами и будете ловить Андрея в кулуарах дискуссионной зоне Поехали Так у кого первый микрофон Тогда Андрей Спасибо большое за доклад совсем коротеньких вопроса Первое это какое для нас сессии юзера используется для рекомендаций То есть За какое время последние события используются и второе Как вы решаете проблему холодного старта для пользователя ага но на самом деле мы ограничиваем не по времени вот это данные которые о действиях пользователя по количеству Вот потому что там несколько максимальное значение вот порядка 1.000-2.000 но на разные типы действий но там в среднем какой-то меньшее количество значений проблема холодного старта для пользователей решается за счёт контентного анализа и алгоритмов рекомендаций на базе Ну анкетных данных пол- возраста гео и в принципе практически всё что вам доступно в этот момент вот так ещё Да да здравствуйте Спасибо за доклад хотел спросить а как вы решаете и встречались ли вы вообще с проблемой информационного пузыря встречались да как раз э-э Ну с одной стороны мы её решаем за счёт того что пользователю домешиваем рекомендации и показываем различные вставки с другой стороны в самих рекомендательных системах вот этот метод который описан когда мы формируем бединги под пользователям мы там делаем разные рандомизации и э-э скажем используем какие-то ещё подходы для того чтобы получить хотя бы несколько векторов отличных от ярко выраженных текущих интересов понял спасибо Ну всё третий Ну всё последний вот этот третий хорошо меня слышно Андрей привет Спасибо что за доклад мне наверное будет банальный вопрос но очень любопытный вопрос тестирования и все эти пирамиды Да именно как отдельных сервисов так и интеграционного тестирования и при установке цели Да и кипя метрик что является что система рекомендаций работает корректно как вы это определяли то что для вас было критериям качества инвалидность работы системы хорошие сложные вопросы про тестирование интеграционное тестирование оно в основном используется для оценки работоспособности системы а работоспособность э всего машинного обучения на этом объёме данных с таким большим разнообразием вручную не протестировать здесь у нас очень большое количество различных э-э графиков метрик э-э показателей которые говорят о том что что-то работает хорошо или плохо самый банальный это даже число просмотров постов либо там оценка калибровки Модели там количество скорб делённое на количество лайков А вот любопытно а не проводилось ли у вас такое что собиралась группа людей заводились новые аккаунты да то есть ну и они регистрировались подписывались и дальше там со временем за неделю за две вы смотрели как для их пользователи работают рекомендательная система Да мы время от времени такое упражнение проделываем а-а и ну находимся всегда какие-то точки улучшения вот этот график это не на холодный старт не так просто решить на таком многообразии контента спасибо понятно тоже есть я про это могу лично уже всё спасибо за вопросы ещё раз благодарим Андрея тебе нужно выбрать тебе нужно выбрать двух лучших блин Слушайте все вопросы Давайте про холодный старт и протестирование и тебе ещё нужно подарок подарить даже где подарок нас хелперы пропали Ну сейчас значит подарим вне сцены"
}