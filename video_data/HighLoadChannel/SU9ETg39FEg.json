{
  "video_id": "SU9ETg39FEg",
  "channel": "HighLoadChannel",
  "title": "Архитектура хранения и отдачи фотографий в Badoo / Артем Денисов (Badoo)",
  "views": 3611,
  "duration": 3172,
  "published": "2017-04-22T14:48:19-07:00",
  "text": "всем привет меня зовут артем и я работаю в компании баду сегодня мы с вами будем много говорить про фотки но прежде чем мы к самим фоткам перейдем у меня будет два маленьких предисловия первое это что же такое баду вдруг то не знает водой это крупнейший в мире сайт знакомств на данный момент оно зарегистрировано порядка трехсот тридцати миллионов пользователей по всему миру но что гораздо более важно в контексте нашего сегодняшнего разговора это то что мы храним около трех петабайт пользовательских фотографий каждый день наши пользователи заливают порядка 300 миллионов новых фотографий или нагрузка на чтение составляет порядка 80 тысяч запросов в секунду как достаточно много для нашего бренда и с этим иногда бывают трудности об этом я расскажу позже это первое объявление второе объявление то что вы наверное заметили что нас будет два доклада первый доклад мой в нем я расскажу про дизайн эта система которая хранит и отдает фотки в целом и проведу на нее такой взгляд с точки зрения разработчика о том как она развивалась будет краткое ретроспектива где я основные вехи обозначу но уже более подробно буду говорить только о тех решениях которые мы сейчас используем и второй доклад оон будет от моего коллеги дмитрия он будет полностью создает очень на аппаратной составляющей хранения фоток и если вдруг мой доклад покажется вам местами капитанским и простым не переживайте и не расстраивайтесь там будет много хардкорного хардкора и технических деталей вы не соскучитесь вот а теперь давайте начнем как я уже сказал это будет ретроспектива и для того чтобы ее с чего-то начать давайте возьмем самый банальный пример у нас есть общая задача нам нужно хранить соответственно принимать хранить и отдавать фотографии пользователей в таком виде ну задач общем мы можем использовать что угодно можем использовать какой-то современный облачный сторож можем например использовать коробочное решение которых сейчас тоже много можем например просто защита пить несколько машин в своем дата-центре и поставить на них большие жесткие диски и хранить фотографии там баду исторический и сейчас и тогда на то время когда это только зарождалась она живет на собственных серверах внутри наших собственных dc поэтому для нас этот вариант был оптимальным мы просто взяли несколько машин назвали их fotos нас пойдут получился такой кластер который хранит фотографии кажется чего-то не хватает для того чтобы все это работало нужно каким-то образом определить на какой машине каких графии мы будем хранить и здесь тоже не надо открывать америку мы добавляем наше хранилище с инфой пользователях какое-то поле это будет ключ хардинга ну в нашем случае мы назвали его play сойди и вот этот айди место он собственно и указывает на место в котором хранятся фотографии пользователей мы составляем карту на первом этапе это может сделать даже руками в которой мы говорим то что фотографии этого пользователя с такими с таким плей сам будут приземляться на такой сервер благодаря этой карте мы всегда знаем когда пользователь загружает фотографии где ее сохранить и знаем откуда ее отдать это абсолютно тривиальная схема но она имеет достаточно существенные плюсы первое это то что она простая как я уже сказал а второе это то что с таким подходом мы можем легко горизонтально масштабируется просто доставляя новые тачки и добавляя их в карту больше ничего делать не надо так оно какое-то время и было у нас это было порядка 2009 2010 года даже на 2009 2010 же поздно доставляли машина доставляли машины и в какой то момент мы начали замечать что она обладает определенными недостатками какие недостатки в первую очередь это ограниченная вместимость то есть мы на один физический сервер можем записать не так много жестких дисков как нам хотелось бы и это с течением времени и с ростом до сета стала определенной проблемой и второе это не типичная конфигурация машин поскольку такие машины тяжело переиспользовать каких-то других кластерах они достаточно специфические то есть они должны быть слабые по производительности но в то же время с большим жестким диском это все было на 2009 год но в принципе эти требования актуальные на сей день у нас ретроспектива поэтому 2009 все было плохо с этим совсем и последний пункт это цена то есть цена тогда была очень кусачее и нам нужно было искать какие-то альтернативы то есть нам нужно было как-то лучше утилизировать и место в дата-центрах и количестве непосредственно физические сервера на которых все это размещено и наши системные инженеры начали большое исследование в котором пересмотрели кучу разных вариантов они смотрели и на кластерные файловые системы такие как поле surf и люстра там были проблемы с производительностью и достаточно тяжелой эксплуатации отказались пробовали монтировать весь dataset по нфс на каждую точку чтобы таким образом как-то смасштабировать все не тоже плохо за пробовали разные решение от разных вендоров и в итоге мы остановились на том что мы стали использовать так называемые стоящая networks это большие сходи которые как раз ориентированы на хранение больших объемов данных и они представляют из себя полки с дисками которые смонтированы на конечные отдающие машины по оптике таким образом мы имеем какой-то полу машин достаточно небольшой и эти исходы которые прозрачно для нашей отдающий логике то есть для нашего engine.exe или кого-то еще обслуживают запросы за этими фотографиями у этого решение были очевидные плюсы это схд она ориентирована на то чтобы хранить фотки это получается дешевле чем мы просто сетапе машины с жесткими дисками 2 2 плюс это то что вместимость стало гораздо больше то есть мы гораздо меньшем объеме можем разместить гораздо больше 100 раджа но были и минусы эти минусы обнаружились достаточно быстро с ростом числа пользователей и нагрузки на эту систему начали возникать проблемы с производительностью и проблемы здесь достаточно очевидный любая схд которая предназначена для того чтобы хранить много фотографий в маленьком объеме она как правило страдает от интенсивного чтения это на самом деле актуально для любого и облачного стороны чего бы-то ни было сейчас у нас не существует идеального стороны который был бы бесконечно масштабируемым и в него можно было бы все что угодно запихиваете он бы очень хорошо переносил чтение особенно случайные чтение как в случае с нашими фото с вами потому что фотографии запросто непоследовательны и это очень сильно affected их performance даже вот по сегодняшним цифрам если у нас выпадает где-то больше 500 рпс за фотками на машину который подключен сто раз уже начинаются проблемы и это было достаточно плохо для нас потому что число пользуйте растёт всё должно становиться только хуже надо это как-то оптимизировать для того чтобы оптимизировать мы в то время решили ну очевидно посмотреть на профиль нагрузки что что вообще происходит что надо оптимизировать и тут все играет нам на руку у нас соотношение я в первом слайде уже говорил у нас 80 тысяч запросов в секунду на чтение при всего 3 play миллионов а плода в день то есть эта разница на три порядка ну очевидно что оптимизировать надо чтение и практически понятно как есть еще один маленький момент специфика сервиса такая что человек регистрируется заливает фотографию далее начинает активно смотреть других людей лайкать их его активно показывают другим людям потом он скажем находит себе пару или не находит пару это уж как получится и и на какое-то время перестает пользоваться сервисом в этот момент вот когда он пользуется его фотки очень горячий они востребованы их просматривать очень много людей как только он перестает это делать достаточно быстро он выпадает из показов другим людям таких интенсивных как были раньше и его фотки практически не запрашиваются дать у нас очень маленький горячий datasette но при этом за ним прямо очень много запросов и совершенно очевидным решением тут на прошиться добавить cash cash дрю все проблемы наш решит что мы делаем мы добавляем перед нашим большим кластером со сторожами еще один сравнительно небольшой которые называем фото каши это по сути просто каширу ющий прокси как это работает внутри вот есть наш пользователь сторож все как раньше что мы добавляем между ними это просто машина с физическим локальным дискам который быстро это ssd допустим и вот на этом диске хранится какой-то локальный кэш как это выглядит водитель посылает запрос за фоткой индекс ищет ее сначала локальном кэше если нет то делает просто прокси ps на наш торт скачивает фотография оттуда и отдает ее пользователю но это очень банальное прям не понятно что внутри происходит работает это примерно так кэш логический разделен на три слоя когда я говорю трясло это не значит что там какое-то сложная система нет условно просто 3 директории файловой системе это буфер куда попадают только что загружены из прокси фотографий это горячий кэш в котором хранятся активно запрашиваемые сейчас фотографии и холодный кэш куда постепенно фотографии выталкивается из горячего когда к ним приходит меньше request of чтобы это работало нам как-то надо менеджер этот кэш надо переставлять фотографии в нем и так далее это тоже очень примитивный процесс индекс просто на каждый запрос пишет на ramdisk access лук в котором указывает путь до фотки которым сейчас обслужил относительный путь естественно и то каким разделом оно было обслужено то есть там может быть написано там фото 1 и дальше или буфер или горячей и cacilie холодный кэш или прокси в зависимости от этого нам нужно как-то принять решение что делать с фоткой у нас на каждой машине работает небольшой демон который постоянно вычитывает этот лак и у себя в памяти хранит статку по использованию тех или иных фотографий то есть он просто собирает там ведет счетчики и периодически делать следующие активно запрашиваемой фотографии как за которыми приходит много request of он двигает горячий каждый его они не лежали соответственно фотографии которые запрашиваются редко и стали запрашиваться реже он постепенно выталкивает из горячего каша в холодной и когда у нас в каше кончается место мы просто начинаем все удалять из холодного каша без разбору и это кстати хорошо работы для того чтобы фотка сохранялась сразу же при проксирование в буфер мы используем директиву праксис тор и буфер это тоже ramdisk то есть для пользователя это все работает очень быстро это что касается внутренности самого каширу ющего сервера остался вопрос тем как распределять request и по этим сервером то есть у нас допустим весь кластер там из 20 сторож машин и 3 каширу ющих серверов так получилось нам нужно каким-то образом определить какие request и за какими фотками и куда приземлить самый банальный вариант это я не знаю round robin или случайно это делать это очевидно имеет ряд недостатков потому что мы будем очень неэффективно использовать кэш в такой ситуации запросы будут приземляться на кита случайные машины здесь она запиралась на соседей и и уже нету и работать это будет если будет очень плохо даже при небольшом числе машин в классе нам нужно каким-то образом однозначно определять на какой сервер приземлять какой request есть банальный способ мы берем хэш от орла или хэш от нашего ключа хардинга который есть в руле и делим его на цело на количество серверов будет работать будет то есть у нас стопроцентно request например за каким-то экзампла url всегда будет приземляться на сервер с индексом 2 и конечно будет постоянно соответственно утилизирован как можно как можно лучше но возникает проблема с ришар дин нам в такой схеме при sharding я имею ввиду изменение количества серверов предположим что наш кошелек кластер перестал справляться и мы решили добавить то еще одну машину добавляем у нас теперь все делится на целый не натрий она 4 таким образом практически все ключи которые у нас раньше были практически все url и теперь живут на других серверах весь кэш 1 инвалиде рвался просто моментом все запросы повалили на наш кластер старриджа и ему стало плохо отказ в обслуживании недовольной полиции так не хочется делать это наряд там тоже не подходит таким образом что что мы должны сделать мы должны каким-то образом эффективно использовать кэш постоянно приземлять один request на один и тот же сервер но при этом быть устойчивыми к ришар дингл и такое решение есть она не то чтобы очень сложно и называется consistent кэшем как это выглядит мы берем какую-то функцию от sharding ключа и раз размазываем и и все ее значения night на окружности то есть в точке 0 у нас сходится и и минимальное и максимальное значение далее мы на и той же окружности размещаемся наши сервера примерно таким образом каждый сервер определяется одной точкой этот сектор который идет до него по часовой стрелке он соответственно обслуживать с этим хвостом когда нам приходят запросы мы сразу видим что он про запрос а у него там хэш такой ион обсуждается сервером 2 там за просьбой с сервером три и так далее что в этой ситуации происходит при ришар ринге мы не инвалиды ruim весь кэш как раньше и не избегаем все ключи а мы сдвигаем каждый сектор на небольшое расстояние таким образом чтобы в освободившееся место условно говоря в лес на 6 сервер который мы хотим добавить и добавляем его туда конечно в такой ситуации ключи тоже съезжают но они съезжают гораздо слабее чем раньше и мы видим что например два наших первых ключ они остались на своих серверах а поменялся каширу ющий сервер только для последнего ключа это работает достаточно эффективно и если вы делаете ну добавляете новый хостинг и ментально то большой проблемой здесь нет вы по чуть-чуть добавляйте добавляете ждете пока кэш опять наполнится и все хорошо работает единственный вопрос остается при отказах предположим что у нас какая-то . вышло из строя и на мне очень бы хотелось в этот момент перри генерить эту карту инвалидизировать часть каша и так далее есть например machinery бухнулась да а нам надо как-то обслуживать запросы мы просто держим на каждой площадке один резервный фото кэш который выполняет роль замены для любой машины которые сейчас вышло из строя и если вдруг у нас какой-то сервер недоступен трафик идет туда у нас при этом естественно там нет никакого каша то есть он холодный но как минимум запросы пользователей обрабатывается если это короткий интервал то мы совершенно спокойно это переживаем просто больше нагрузка на 100 речь идет если это интервал долгий то мы уже можем принять решение о том убрать этот сервер с карты или нет или может быть заменить его другим это по поводу система каширования давайте посмотрим на результаты казалось бы ничего сложного здесь нет но вот такой способ управлений кашам дал нам heat-ray порядка 98 процентов то есть из вот этих вот 80000 request of в секунду только 1600 доходит до 100 раджей и это совершенно нормальная нагрузка они спокойны это переживают и у нас всегда есть запас мы разместили эти сервера в трех наших dc и получили три точки присутствия прага майами и гонконг таким образом они более-менее локально расположены к каждому из наших целевых рынков и в качестве приятного бонуса мы получили вот каширу ющий прокси на котором цепью на самом деле простаивает потому что для дачи контента он не так сильно нужен и там с помощью engine.exe а мы реализовали очень много утилитарной логике например можем экспериментировать слепили прогрессе в джипег это эффективные форматы современные смотреть как это влияет на трафик принимать какие-то решения там включать для определенных стран и так далее можем делать динамический ресайз или кровь фотографий на лету это хороший узкий когда вас например из мобильное приложение которое показывает фотки и мобильное приложение не хочет тратить сепию клиента на то чтобы запросить большую фотографию и риса идите и потом до какого-то размера чтобы запихнуть во вьюшку мы можем динамически просто указать в ур ли какие-то параметры viewport условные да и фото кэш сам отрицая сеть фотографию как правило он подберет тот размер который у нас физически есть на диске максимально близкий запрашиваемую и задонске или того о конкретных уже координат и также мы можем добавлять туда много продуктовой логике то есть мы например можем по параметрам урла добавлять разные water марки можем например блюрить фотографию размывать или pixel и заводь это когда мы про хотим показать фотографию человека на не хотим показать его лицо это хорошо работает это все реализовано тут что мы получили мы получили три точки присутствие хороший hit rate и при этом у нас не просто его и цепью на этих машинах он теперь стал конечно важнее чем раньше нам нужно ставить машина сибиу посильней но это того стоит это что касается отдача фотографий здесь все достаточно понятно и очевидно я думаю что я не открыл америку так работает практически любой сидел и скорее всего у искушенного слушатель мог возникнуть вопросов почему просто не взять и не поменяйте свято населен было бы примерно тоже самое все современные сидена это умеют и здесь ряд причин первое это фотки это один из ключевых компонентов нашей инфраструктуры и нам нужно над ними как можно больше контроля то есть если это какое-то решение у стороннего вендора и вы не имеете над ним вообще никакой власти вам будет достаточно тяжело с этим жить когда у вас большой datasette и когда у вас очень большой поток запросов пользователей приведу пример сейчас на своей инфраструктуре мы можем например в случае каких то проблем или подземных стуков мы можем зайти на машину поди божиться там условно говоря мы можем добавить сбор каких-то метрик которые только нам нужны можем как-то экспериментировать смотреть как это повляет на графике и так далее то сейчас собирается очень много статистики по вот этому каширу ющ ему кластеру и мы периодически смотрим на нее и долго исследуем какие-то аномалии если бы это было на стороне сидена это было бы гораздо тяжелее контролировать или например если происходит какая-то авария мы знаем что случилось мы знаем как с этим жить и как как это побороть это первый и второй вот он тоже из скорее исторические потому что система развивается уже давно и много разными бизнес требований на разных этапах они были и далеко не всегда не укладывается в концепцию сидена и пункт вытекающие из предыдущего это та что на фото к шагу нас много специфической лайки которую далеко не всегда можно по запросу например вряд ли който се день будет по вашему запросу добавлять вам какие-то кастомные вещи например там шифрование орлов если вы не хотите чтобы клиент мог что-то менять хотите сгенерить url на сервере зашифровать его потом отдать сюда и так далее какие динамические параметры то есть какой вывод напрашивается в нашем случае сидел это не очень хорошая альтернатива а в вашем случае если у вас есть какие-то специфические бизнес требования то вы можете совершенно спокойно реализовать то что я вам сейчас показал сами и это при похожим профили нагрузки будет отлично работать но если у вас какое-то общее решение и задача не то чтобы очень часто вы можете совершенно спокойно брать седин или если для вас гораздо важнее время и ресурсы чем контроль и современные стен умеет практически все то о чем я рассказал вам сейчас ну за исключением там плюс минус каких-то фич это по поводу отдачи фотографии давайте теперь немножко переместимся вперед в нашей ретроспективе и поговорим про хранение 2013 год шел к ширу ющие сервера добавились проблемы с перфомансом ушли все хорошо data set растет на данный момент вот не надо на и на 2013 год у нас было порядка 80 серверов которые подключенных сторожем и порядка 40 каширу ющих в каждом dc это по пятьсот шестьдесят примерно петабайт доколе терабайт данных на каждом dc то есть около петабайт а в сумме и с ростом datasette а начали сильно расти эксплуатационные издержки в чем это выражалось в этой схеме которая нарисована с самым с подключенными к нему машинами и кашами очень много . отказа если с отказом каширу ющих серверов мы раньше уже справились там все более-менее прогнозируемые понятно то на стороне именно 100 раджив все было гораздо хуже у нас есть во первых сам storage area network который может отказать во вторых он подключен по оптике на конечные машины могут быть проблемы с оптическими картами свечами их не так много конечно как самим самым но тем не менее это тоже . отказа далее сама машина которая подключена к сторожу она тоже может выйти из строя это у нас три точки отказа далее помимо . отказа это тяжелый maintenance самих 100 раджей потому что это сложная многокомпонентная система и системным инженером бывает тяжело и последний самый важный пункт если в любой из этих трех точек произойдет отказ у нас есть не 0 вероятность потерять данные пользователя поскольку может побиться файловая система предположим да у нас побилось файловой системы и восстановление идет во первых долго это может занимать недели при большом объеме данных а во-вторых по итогу мы скорее всего получим кучу непонятных файлов lost & found которые нам потом нужно будет каким-то образом смочить на фотографии пользователей и мы рискуем потерять данные риск достаточно высокий и чем чаще такие ситуации случаются и чем больше возникает проблему со всей этой цепочки тем этот риск выше с этим надо было что-то делать мы решили что надо просто резервировать данные это на самом деле очевидное решение хорошие что мы сделали так выглядел наш сервер который подключен к 100 раджу раньше это есть один основной раздел это просто блочное устройство которое представляет на самом деле mount на удаленный сторож по оптике мы просто добавили второй раздел обстреле рядом второй старт благо по деньгам это не так дорого и назвали его в backup разделом он тоже подключен по оптике тоже на той же самой находится но нам надо как-то синхронизировать между ними данные здесь мы просто делаем рядом асинхронную очередь она не очень нагруженный мы знаем что у нас мало записей очередь это просто табличка в морской лев которую пишется строчки там типа надо за звук опять вот эту фотографию при любом изменении или при оплате мы копируем с основного раздела на backup асинхронным дубровский in the background маркером и таким образом мы имеем всегда 2 консистентных раздела даже если одна часть этой системы в этой строим мы всегда можем поменять основной раздел бекапом и все продолжит работать но из-за этого сильно возрастает нагрузка на чтение потому что мы постоянно еще еще помимо клиентов которые читаются основного раздела потому что они сначала смотрят фотографию там там же она более свежая а потом уже ищут на бэкапе если нашли night vision express и делает еще и плюс наша система backup а теперь вычитывается основного раздела это не то чтобы было узким местом но не хотелось ведь нагрузку по сути просто так и мы добавили третий диск который маленькие ssd и назвали его буфером как это теперь работает пользователя платит фотку на буфер далее кидается ивент в очередь на том что я надо раз копирует на два раздела она копируется и фотография какое-то время допустим сутки живет на буфере а только потом оттуда пор житься это здорово улучшает юзер экспириенс потому что пользователь заливает фотографию как правило за ней сразу же начинают идти request и или он сам там не знаю обновил страницу зариф решал и еще что то но это уже зависит от приложения которое делает apple о или например другие люди которым он начал показ они сразу же за этой фоткой послать реквест и в к шею еще нет первый запрос происходит очень быстро то есть по сути также как фото каша медленный сторож не участвует вообще в этом а когда через сутки она будет спор жена она уже либо за к ширава на нашем на нашем кашель ищем слоя либо она уже скорее всего никому не нужно то есть юзер экспириенс здесь очень здорово подрос за счет таких простых манипуляций но и самое главное мы перестали терять данные мы скажем так перестали потенциально терять данный мышцы мы их особо и не теряли на опасность была но мы видим что такое решение она конечно хорошая но но слегка похожа на сглаживания симптомов проблемы вместо того чтобы решить ее до конца и проблемы здесь некоторые остались во первых это . отказываетесь самого физического hasta на котором вся эта машинерия работает она никуда не делась во вторых это остались проблемы с ценами остался их тяжелой maintenance и так далее это не то чтобы было критическим фактором но хотелось попробовать как-то без этого пожить и мы сделали третью версию по сути вторую на самом деле версию резервирования как это выглядело это то что было основные проблемы у нас с тем что это физический хост мы во-первых убираем сан и потому что хотим поэкспериментировать хотим попробовать просто локальные жесткие диски это уже там 2015 2017 год и на тот момент ситуация с дисками и с их вместимости в один ход стало гораздо лучше мы решили что почему бы нет почему бы не попробовать и далее мы просто берем наш backup раздел и переносим его физически на отдельном машину таким образом мы получаем вот такую схему у нас есть две тачки которые хранят одинаковый datasette они резервируют друг друга полностью и синхронизирует данные по сети через асинхронную очередь в том же самом mais quel и почему это хорошо работает потому что у нас мало записи то есть если бы запись была соизмерима с чтением возможно мы бы получили какой-то сетевой overhead ее проблемы записи мало чтения много это этот способ работает хорошо то есть мы достаточно редко копируем фотографии между двумя этими серверами каким образом это работает если чуть чуть детальнее посмотреть оплот балансировщик просто выбирает случайной хост из пары и делает оплот на него при этом он естественно делает half чайки смотрит что машина не выпало то есть он оплатит фотки только на живой сервер а потом через асинхронную очередь это все копируется на его соседом сопло там все предельно просто задачи чуть-чуть сложнее здесь нам помог лу а потому что на ванильном engine.exe такую логику сделать трудновато бывает мы сначала делаем request на первый сервер смотрим если фотография там потому что потенциально она может быть залита например на соседа а сюда еще не доехал а если фотография там есть это хорошо мы ее сразу отдаем клиенту и возможно каширу им если ее нет мы просто делаем запрос на соседа и оттуда и и гарантированно получаем таким образом опять можно сказать могут быть проблемы с перфомансом потому что постоянно и round trip и фотографию залили тут ее нету мы делаем два запроса вместо одного это должно медленно работать в нашей ситуации это работать немедленно у нас собирается куча метрик по этой системе и например вот хитрый на условный хитрый дат такого механизма он составляет около 95 процентов то есть лак вот этого быка по он маленький и за счет этого мы практически гарантированно после того как фотка была загружена забираем ее уже с первого раза и никуда два раза не ходим таким образом что мы еще получили и что очень круто раньше у нас были основной backup разделы и мы читали с них последование то есть мы всегда сначала искали на основном потом мы бэкапе это был один ход теперь мы утилизируем чтение с двух машин сразу то есть распределяем запроса round robin нам в небольшом проценте случаев мы делаем два запроса на зато в целом у нас теперь в два раза больший запас по чтению чем был раньше и нагрузка прям здорово снизилась и на отдающий машины и непосредственно столь же которые у нас на тот момент тоже были что касается отказоустойчивости собственно поэтому и боролись в основном с отказоустойчивостью здесь все вышло шикарно одна тачка выходит из строя никаких проблем с тем ный инженер может даже не просыпаться ночью подождите до утра ничего страшного не будет если даже при отказе этой машины вышла из строя очередь тоже никаких проблем просто лак будет копиться сначала на машине а потом уже на живой машине а потом уже добиваться очередь а потом уже на ту тачку которая войдет в строй через какое то время то же самое с минтрансом мы просто выключаем одной из машин руками вытаскиваем из всех пулов не и перестает эти трафик делаем какой то month and что-то там правим после этого возвращаем ее в строй и вот этот backup загоняется достаточно быстро то есть за сутки downtime одной тачки он загоняется в пределах пару минут это прям сильно мало с отказоустойчивостью еще раз говорю здесь все круто какие можно итоге подвести из вот этой схемы с резервированием получили отказоустойчивость уже сказал простая эксплуатация поскольку на машинах локальные жесткие диски это гораздо более удобно с точки зрения эксплуатации инженеров которые с этим работают получили двойной запас по чтению это очень хороший бонус плюс к отказоустойчивости но есть и проблемы теперь у нас гораздо более сложная разработка каких-то fitch связанных с этим стало потому что система стала на сто процентов винчи или consistent мы должны скажем в каком-нибудь background джоби постоянно думать а на каком сервере мы сейчас запущены а точно ли здесь есть актуальная фотка и так далее это естественно все завернутого бердские для программиста которые пишут бизнес-логику это прозрачно но тем не менее этот большой сложный слой появился но мы готовы с этим мириться в обмен на те плюшки которые мы от этого получили и тут опять же возникает некоторый конфликт я вначале говорил что хранить все на локальных жестких дисках это плохо а теперь я говорю что нам это понравилось да действительно с течением времени ситуация сильно поменялась и сейчас в этого подхода появилось много плюсов мы во-первых получаем гораздо более простую эксплуатацию во вторых это производители и потому что у нас нету вот этих оптических контроллеров подключения к дисковым полкам там огромные машинери это просто несколько дисков которые вот конкретно здесь на машине собранные в рейд но есть и минусы это приблизительно в полтора раза дороже чем использовать сан и даже по сегодняшним ценам поэтому мы решили так смело не конвертить вечно весь наш большой кластер в тачке с локальными жесткими дисками и решили оставить гибридное решение то есть половину машины нас работает с жесткими дисками не половина где-то процентов 30 наверное а оставшаяся часть это старые тачки на которых раньше была первая схема резервирования но мы просто перри монтировали их поскольку нам не нужно не могут данные ничего мы просто переставили mount с одного физического hasta на 2 и у нас появился большой запас почтением и укрупнили то есть если раньше мы на одну машину монтировали 140 теперь мы на одну пару монтируем там четыре например и это нормально работает давайте подведем краткий итог того что у нас получилось за что мы боролись и получилось ли у нас есть пользователи 330 миллионов целых у нас есть три точки присутствие это прага майами гонконг в них расположен к ширу ющий слой который представляете сеточки с быстрыми локальными дисками с ssd на которых работает простенькая машинерия из engine.exe его exo слова и демона питоне который все это обрабатывает и манжет кэш при желании вы в своем проекте если для вас фотки не так критичны как для нас или если трейдов контроль против скорости разработки и затрат ресурсов для вас в другую сторону тогда вы можете спокойно заменить его сединам современности на это хорошо делаю далее идет слой хранение на котором у нас мы мастера из пар машин которые друг друга резервируют асинхронно копируются файлы с одного на другой при любом изменении при этом части этих машин работает с локальными жесткими дисками часть этих машин подключены аксоном и с одной стороны это удобнее в эксплуатации и немного более производительно с другой стороны это удобно с точки зрения плотности размещения и и цены за гигабайт это такой краткий обзор архитектуры того что мы получили как это все развивалось еще несколько советов от кэпа совсем простых во первых если вы вдруг решите что вам срочно нужно все улучшить в вашей инфраструктуре фоток сначала поверить и потому что возможно ничего не надо улучшать приведу пример у нас есть кластер машин который отдает фотографии из attachment of в чатах и вот там до сих пор работает схемы с 2009 года и никто от этого не страдает всем всем хорошо всем все нравится для того чтобы измерить сначала нарежьте кучу метрик посмотрите на них и тогда уже решите чем вы недовольны что надо улучшать для того чтобы эта мерить у нас есть классный инструмент который называется пемба он позволяет собирать статус engine.exe очень подробную на каждый request и коды ответов и распределения времен все что угодно у него есть бензин gigo всякие разные системы построение аналитики и вы можете потом все это красиво смотреть сначала померили потом улучшили то ли чтение мы оптимизируем кошон запись шарниром но это очевидный пункт далее если вы старайтесь только сейчас начинаете строить вашу систему то гораздо лучше делать фотографии как файлы имеют обл поскольку вы теряете сразу же целый класс проблем с инвалида ции каша с тем как логика должна находить правильную версию фотографии и так далее допустим фотку залили потом мы ее повернули сделать так чтобы это был физический другой файл то есть не нужно думать что вот я сейчас там сэкономлю чуть-чуть место запишу в тот же самый файл там поменяю версию это всегда плохо работает с этим потом много головной боли следующий пункт про орисой знали ту раньше мы когда пользователь платит фотографию нарезали сразу целую кучу размеров там на все случаи жизни под разных ентов и они все лежали на диске теперь мы от этого отказались мы оставили только три основных размерах ну условно говоря маленький средний и большой и все остальное мы просто да он с гелем из размера которые стоит затем которые нам спросили viewport просто делаем dance кирилл и отдаем пользователю сыпью каширу ющего слоя здесь получается гораздо дешевле чем если бы мы постоянно периге не реале эти размеры на каждом 100 orgy допустим мы хотим добавить новые да это дело там на месяц прогнать везде скрипт который бы все это аккуратно сделал при этом не положил кластер то есть если есть возможность сейчас выбирать лучше делать как можно меньше физических размеров но чтобы хоть какое-то распределение было да там скажем три и все остальное просто риса ездит на лету с помощью готовых модулей а сейчас все очень легко и доступно инструментальная синхронный бэкапы это хорошо как показала наша практика такая схема здорово работает отложенным копированием измененных файлов и последний пункт он тоже очевидно если в вашей инфраструктуре сейчас таких проблем нет но есть что-то что может сломаться она обязательно сломается когда этого станет чуть больше поэтому лучше подумать об этом заранее и не испытывать потом проблем у меня все о не туда говорят что за хорошие вопросы будут три футболки спасибо за доклад а скажите пожалуйста вот вы рассказывали то что вас стараешься таки могут меняться и вы кто меня есть фотография тасс я так мимолетность 1 2 вы отдадите старую до или же там через пять секунд аргумент разруливаете я бы фотография давались именно та версия которое надо спасибо за вопрос очень хороший вопрос давайте на него отвечу на самом деле все немножко сложнее когда мы делаем такие request и на один хост а потом на другой поскольку у нас фотки не имеют обл у нас есть этим ряд проблем и при изменении фотографии мы обязательно то чем все файлы на диске на какой-то там текущем time и сохраняем этот интайм в базу и при генерации орлов мы передаем в них грубо говоря вот этот чек сумму да ну там steam фотографии и мы проверяем не только наличие или отсутствие фотки а еще и м time на диске то есть запрос пришел на 1 хвост он может ответить не просто есть нет а есть нет и фотография старая в таком случае мы тоже идём на соседа и отдаем ее оттуда вот это краткий ответ а не подскажите а почему в качестве очереди в база они там ребята и всякие другие мои giochi во-первых потому что здесь нету больших проблем с перфомансом зато есть транзакционных и транзакционных при записи очередь для нас иногда бывает удобный во вторых потому что у нас есть огромная ферма в моей цели мы умеем и и хорошо готовить вот такие такие ответы меня вопрос с 2 пока каширу ющий прокси с моей точки зрения довольно странное решение как парсить логе смотреть за ковшом вручную соответственно вопрос пробовали ли вы squid в роли reverse proxy что проблеск вид кальмар я к сожалению и даже не знаю чем вы говорите это явно ешь такой кальмар но вот конкретно продукция этим названием нет второй вопрос судя по вашей схеме скорее всего у вас есть такое допущение что пора серверов обязаны быть одинаковыми в том числе по дискам внутри вот если это так то какие ваши буду действий в дальнейшем когда диски будут по-разному мусоре ци без и со временем в таких дисков не будет продажи смотрите как правило это скорее на самом деле вопрос к следующему докладчику но я на него отвечу с точки зрения программиста с точки зрения программиста здесь ситуация такая то что мы тщательно следим за консистентной между двумя этими тачками и вот по поводу запаса по железу я вам сейчас не могу сказать то есть я думаю что в 21 веке мы можем как-то решить проблему со взаимозаменяемыми жесткими дисками одного размера думаю что это возможно а по поводу консистентной sti мы очень внимательно за ней следим и у нас есть такая метрика как то есть мы не можем померить да какие фотки есть на этом сторону а какие нет там десятки терабайт и каждый раз их сравнивать это безумие поэтому мы просто мере им количество занятых ой нот на этом 40g и на соседи и поэтому количество мы можем косвенно определить что там есть проблемы с консистенции или нет и вот мы стараемся держать эти define од как можно более низкими но в идеале они всегда стабильно но там есть какой то маленький процент после которого мы начинаем уже research и дают какие там проблемы надеюсь что ответил на вопрос спасибо за доклад у меня на одно дополнение это по поводу того что писать на летающую чем он хорош мы с мы сильно экономим на на том что они не храним лишние варианты которые но до изображений которые вообще не использует да да да это хорошее это хорошее спасибо это хороший point потому что мы spur жив все вот эти вот старые размеры которых было много сэкономили очень много мест и два вопроса я так не мог бы скорее всего себя не решайте вас не задачи не хранить два одинаковых изображения не такой дедупликации мы не занимаемся в этой второй вопрос и если у вас задачи фильтрации изображений ну там пикантные запрещено есть конечно такая задача есть это делает этапе да ты идешь это делается уже после того как произошел apple ios просто фотография какое-то время не показывается другим людям она очень быстро проходит и ручную модерацию и так же у нас есть штуки с искусственным интеллектом ну с нейронными сетями имею ввиду которые это все делают на лету то есть у нас есть несколько этапов до можете подойти подробнее тут есть человек отвечает за модерацию она вам расскажет как точно это делается но мы делаем пост проверку уже файлов поставкой они были залиты но в бизнес лайки и хочу никому не показывая всем можно вопрос увидит но другие нет артем здесь здесь здесь нет нет нет отправить вот вот вот так что все войны будьте добры подскажите пожалуйста какие исходы вы использовали то есть скажем так насколько они были промышленные потому что современные вендоры там mensen это abs угон они все позволяют делать репликацию между двумя из ходы вот без промежуточной прослойки да без той очереди которые вы писали сами в принципе в масть обычная мастер своего рипли catia на мастер мастер мастер мастер скорей ну насколько я понял из слайдов репликация было односторонняя не нет пользования из ходы или нет в случае использования я провинился с ним кстати вы когда из показывали очередь репликации она в одну сторону или в 2 все-таки смотрите вы абсолютно правильно сказали когда мы используем первую схему с двумя болтами на одной машине очередь односторонне всегда все копируется с майна на backup если мы используем эти две ну вот схемы с двумя машинами то это мастер мастер поскольку разные фотографии даже одна и та же фотография может меняться параллельно в двух местах то есть мы за этим следим и мы просто под общим лаком каждую фотографию меняем ну то есть скажем где берем лук на озеро и на фотку поэтому у нас нет проблемы с риск античными но тем не менее это мастер мастер по а почему тогда отказались от сходи в пользу именно хранение на машинах знаете я просто не хочу спойлерить доклад коллеги все понятно к чему это скорее да вопрос следующий докладчик мы не до конца отказались у нас сейчас такой гетерогенные решение по лестнице и по своей другое добрый день узнаете такой вопрос соответственно когда вы говорить об очереди вы вне заносите что та картинка кладется в буфер теперь простая ситуация картинка были глав буфер в не знаю попало не попала в очередь на машинка упала как вы в таком случае считаете то есть такую точку отказа ну я бы не сказал что эта точка отказа это скорее проблема с тем что у нас может остаться мусор на буфере в какое-то время мы просто с одной стороны можно сказать что здесь можно придумать сложную машинерию точно такая же ситуация будет если картинка уже загрузилась мы например мы ее правим дома сидим рядом темпа вы и файлы но не успели хну внуть в настоящее мы это решаем как правило организационно это редкий случай если мы например видим что с местом на буфере беда мы можем их просто служить но то есть это на уровне приложения что человек увидит вот там я не за я зашел за lil phat куда она пропала ну как повезет смотрите у нас хорошо то есть я имею ввиду что здесь очень много точек в которых что-то может пойти не так поэтому это сильно зависит от контекста предположим что вы залили фотку файл уже сохранился но он сохранился с темпом именем дальше у нас по любому есть двухфазный commit между базой и файловой системой но мы стараемся его минимизировать тем что мы сначала все сохраняем в темпа и файл и потом я не помню что мы делаем сначала честно делаем м в или к митин транзакцию но вот в этом окне да может быть рейс но это очень редко ситуация и это каких-то проблем не приносят хорошо пассива здравствуйте удаление реализована таким образом когда пользователи удаляется или фотографии удаляется пользователем или модератор помечает ее как удаленную и так далее во всех этих случаях мы просто пишем инфа об этом в отложенную очередь который кстати тоже на майской и и потом там через две недели или через месяц в зависимости от того насколько ну какие параметры условно говоря да там если например на фотографию пришел абьюз request какой-то от полиции там или ещё от кого-то там и пурджем ее мгновенно отовсюду и более того мы даже сбрасываем с неё кэш то есть мы из каширу ющих серверов тоже отовсюду и и удаляем а если это просто пользователь удалил сам там и там какое-то время этот хвост держим неа нет имеется виду не запрос а слушайте это какой-то уже в сфере вот legal начинается разговор я не могу так сказать про и так как как решается с точки зрения юридической это добрый день меток вопрос я пришел с ним попасть во что-то пропустила а водку договорились что балансировщик он выбирает на к серверу и загрузить фотографию а таким образом именно загружает вот прилетел файлики он такой серверу на удержит файл каким образом файл то доставляется это балансировщик почтите файл естественно доставляется это балансировщик и знаете я могу запутаться короче вот как лвс такой же балансировщик то есть он просто роу тит request и куда надо и он приземляется нужны серый просто 6 теперь положите на северах есть ты принимаешь сестра конечно конечно там почти всем работе там работает там работает engine.exe php-fpm это сразу вопрос вот вы рассказали про то что идет проверка джинсовое то с какого сервера фотография есть там или не какие-то грабли или просто 25 не понимаю нутра физкультурой трой двух углов и все без всяких вы смотрите потому что вот в начале задавали вопрос по поводу того что фотки не имеют обл поэтому у нас может быть одинаковый файл но с разным физическим интаймом на диске поэтому мы делаем туда запрос там булочная логика смотрит стад и отдает нам либо с двухсотым кодом либо с 300 кодом ответа спасибо графе спасибо за доклад я хотел спросить про риса и стали ту когда у вас происходит решаясь на 3 получается три фотографии потом еще n количество фотографий под основные размеры они как-то группируются или одни обозначается суффиксами там 75 на 75 она там сто на сто как они как тему понимаю что это в стек фотографии от одной фотографии вы имеете ввиду схему именования файлов ну просто вот это схемы и именование или где-то просто схемы или где-то просто с кем именование файлов с теми хорошо спасибо и второй вопрос а вот три слоя кэша когда происходит переход из одного слоя сша в другой слой каша копируется весь стек фотографии этих нет копируется только файл то конечно же там у файл его то есть у нас может быть два разных размеров фотографии одна из них очень горячие востребована другая никому не нужно так опа получается допустим 100 на 100 может лежать в горячем хэши а 200 на 200 в холодном или вообще не увидели же для рафтинга да спасибо там просто по именам файлов пожалуйста вернемся к синхронной очереди давайте вот тут задавали вопрос что будет там если будут там какие-то гонки у меня такой продолжим этот вопрос а что будет если асинхронной очередь допустим ну допустим базу с этой очередь уйдет вместе с диском то есть совсем у нас есть резервирование естественно словно очередь тоже резервируем асинхронной очереди конечно есть регулирования тоже при помощи асинхронный очень второго порядка нет твари помощи майской не репликации 100 всем спасибо ну то есть какой-то процент фотографий которые только что модифицировались мы безусловно при этом можем поменять или по коробке но это очень малый процент по сравнению с рисками которые есть если мы потеряем все у нас роба есть репликации если вы об этом то есть вряд ли будет большое расхождение между репликами сразу может потом поговорить подробнее я здесь здрасте спасибо за доклад я бы хотел задать три коротеньких вопроса как бы во-первых какую файловую систему вы используете просите это вопрос следующим даже не хочу спойлерить ладно дальше тогда следующее каким образом вы монтируете то есть я понимаю что есть большой сторож хранилище но вы каким-то образом отдаете на ноды раздающие то есть где applications that girl singing стоит какую все-таки файлов стиву используйте или как вы монтируете сделал я понимаю что это может быть либо исказили какую технологию вы используете то есть сам это хорошо но это одно общее слово ничего не говоря согласен да потому что я программист но то есть я понимаю извините что-то говорю просто лучше дождаться следующего докладчика этот образ ему просто хорошо и не последний тогда вопрос из коротеньких какую иерархию файлов вы поддерживаете то есть она у вас идет в на валку в одну дерек больно пытаемся там по выставке под понял-понял можете передавать хороший вопрос поддерживаем такую что у нас нарезанное ну с одной стороны нам здесь нужно обеспечить ситуацию чтобы у нас не было куча файлов одной директории это я немножко расширяю вопрос но с другой стороны не потратить слишком много енот на то чтобы сделать огромную армию поэтому у нас по моему порядка четырех уровней вот у нас есть этот placa de ключ хардинга вы просто от него циферки вот так отрезаем там первые четыре да это как бы дерево а дальше просто по юзера и директорий лежат иерархия первые четыре уровня это просто для того чтобы файловой системе их разложить широко а потом дальше юзера иди и еще одна директория просто по остатку отделения на фото иди чтобы когда у нас есть юзеру которого там три тысячи фотографий чтобы не было каких-то проблем спасибо за доклад у меня вопрос по поводу шарди рования вот вы рисовали круговую диаграмму где вы частично шар де равале данные со всех нот вот хотел бы узнать если жади рания презирайте происходит то нет ли у вас перегрузки для тех пользователей которые наиболее востребованными в тот момент я понял вопрос на самом деле ордера не происходит не появится резину можно сказать что боюсь ради один юзер всегда будет приземляться на один сервер и вы имеете виду ситуацию когда это какой-нибудь там я не знаю джастин бибер да это скорее всего больших проблем не создаст во первых потому что у нас другая специфика то есть у нас сайт знакомств они стали сеть допустим и людей показывают там каким-то образом друг другу последовательно да и большой нагрузке то есть ты не можешь особенно сильно выбирать кого тебе покажут это автоматически все подстраивание происходит горячих польза для особо нет вот и к чему спасибо есть популярный но это не настолько критично там 80 fps поэтому не может он быть таким популярным что положите the cash здравствуйте у меня такой небольшой вопрос у вас есть две реплики что произойдет если на реплики откажет полностью диск с потерей данных 0 вообще все реплика откажет как вы будете замену вас но я понял я понял но это общий вопрос он на самом деле не только в этом контексте я везде мы просто поднимем новую тачку дали включим ее в работу так чтобы на нее там есть как бы два режима есть боевой режим а есть просто вернуть чтобы на нее догоняли сиф и до включили запустили р sing на нее начинают загоняться те div и которые сейчас есть этот пирсинг который был просто с апдейтом запускаемся сколько но это конечно будет занимать очень много времени при этом как сильно деградирует производительность при наличии одного сервера смотрите я изначально сказал что у нас не было вот этого двойного и он и чтения которое потом появилась за счет того что у нас два сервера поэтому когда один выходит из строя performance особо не аффект лица конечно есть вероятность того что 2 в этот момент если это окно длинная до выйдет из строя но это не такая большая вероятно то вроде спасибо за вопрос если вам нужно пример модифицировать картинка таким образом чтобы на ней наложить водяной знак или надпись какой-то сделать это как лучше делать на лету или все-таки хранить отдельно картинка лучше это сделать на лету потому что потом вы будете рвать на себе волосы когда вам нужно будет поменять вот водяной знак насколько это быстро насколько накладно на процессор наши конкретно наши метрики показывают что мы безусловно стали потреблять больше сыпью но он все равно не стал узким местом по сравнению с диском на этих машинах все хорошо спасибо друзья давайте тогда мы дальше продолжим вопросы задавать ребята рывку арклей приходите на стенд сейчас минут пять передохнем мы продолжим слушать про фоточки"
}