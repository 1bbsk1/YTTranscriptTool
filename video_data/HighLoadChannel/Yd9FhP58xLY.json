{
  "video_id": "Yd9FhP58xLY",
  "channel": "HighLoadChannel",
  "title": "Строим сервисы на базе Nginx и Tarantool / Василий Сошников, Андрей Дроздов (Mail Ru)",
  "views": 3205,
  "duration": 1938,
  "published": "2017-04-09T09:39:42-07:00",
  "text": "вася сочников и андрей дроздов долгое время то что мы делали в тарантулом было достаточно эзотерическим только потому что не было людей которые бы вообще могли это как-то применить или объяснить как это можно применить и вот наконец то такие люди появились они не просто взяли какую-то историческую базу данных хозяин они что-то полезное и если вам вас вообще эта технология интересуют это люди которым вам нужно идти вам нужно идти не ко мне потому что я какой-то хардкорный гекко который вообще не понимает человеческих проблем и я с трудом разговариваю на такие проблемы но на таком языке телеги попробовали сделать что то что они сделали поженили тарантул сын джинсам сделали из этого какую-то полезную штуку вау это сделал не смог поэтому строим сервисе на базы и джинсы тарантул спасибо копится думаю это было неплохое представление ну что ж как он сказал мы сегодня будем говорить о том как строить сервиса на базе james & tarantula это первая часть этого доклада я вас познакомлю чуть больше с ops 3 модулем 2 часть андрей мой коллега расскажет о том как реализован в tarantul и sharding и третья часть мы вас мы вам покажем отчет о том как и рпс какой latency и какие показатели мы смогли из всего этого снять на реальных данных немножечко фактов о tarantul & tarantula это не только база данных как знают многие на это еще потрясающий гифки и мощный апликэйшен сервер будем также присутствует sharding в нем есть мастер мастер аппликация синхронный в нем есть вал в нем есть снапшоты думаю многим из вас известно эта схема у нас есть некий хоровод демонов который пляж от между кэшем сторожем и веб-сервером и решая какие-то практические бизнес-задачи вы всегда сталкиваетесь проблемы не только бизнес как взять ту или иную логику ну и с вопросом какой framework используясь какие протоколы используясь как правильно синхронизировать кэш и сторож что с этим всем делать поэтому поскольку тарантула из прикол прекрасные свойства а именно прикрывшись и я подумал почему сразу не ходить из тарантула в яндекс и как у нас изменится эта схема а изменится она ровным счетом так у нас уходят набор разно сорных демонов на возможно же разным языках и у нас уходит хэш поскольку тарантул изначально все-таки база данных и схема наша будет выглядеть так она не немножечко подробнее и джинкс принимает данные от клиенты клиент может быть обычным браузером может быть каким-то вашим демоном кладет эти данные в tarantul на выполнения после чего получает ответ именно как апликэйшен 100 евро то есть вы достаточно просто можете написать любой функционал на лола я специальные из этой схемы не убрал сторож по той маленькой причине что многие любят использовать тарантул каким-то другим сторожем предположим майский элем либо чем то еще поэтому сторож в этой схеме оставил потому что он может быть а может и не быть а может быть сторож будет сам тарантул немножечко оба стрим модулем up stream модель как и говорил базируется на had a to pay джейсоне джейсон доставляется в appstore модуль по средствам post запроса это строго структурированный объект где вы говорите ими метода говорите какие параметры и говорите индификатор сообщения похожим джейсон рпц и соответственно имя метода которые вы указали и параметр который вы указали они пойдут по преобразуется асинхронно в massage pack тот который понимает ранту уйдут в тарантул и произойдет вызов этой функции после чего вам вернется результат тут есть несколько интересных моментов и джинкс песни читать партию джейсона он но этот модуль его сильно нет тормозит чем о чем мы вам покажем дальше вот а теперь реальный успех когда мы все это начали возник вопрос а действительно это так здорово это решает проблемы с которым мы сталкиваемся и поэтому мы задумались какую задачу решить естественно поскольку мы люди амбициозные мы решили задачу решить весьма сложную и взяли википедию википедии базируется на майские она вся кашированная перека ширвана вот но мы решили взять не всю википедию а только сос самую сложную часть а именно граф категории что такое граф категории граф категории википедии это статьи термины и любые другие определения на которых вы видите ссылки внутри википедии то есть предположим типичный случай граф категорий вы начали читать о египетском фараоне закончили читать и ложки казалось бы как вы перешли туда но это вопрос мы тоже сегодня ответим а теперь а миграция как это происходило мы написали несколько демонов которые смазки йеля брали данные по википедии преобразовывали их джейсон и халевин джинкс engine кспеха of тарантул после чего тарантул хординги эти данные распределял в том что википедия все точно большая и в чем главное преимущество этой схемы допустим мы загрузили только русские сегмент википедии мы бы захотели загрузить английский все чтобы надо было сделать здесь это просто добавить еще несколько машин вот и конечной архитектуры для википедии выглядят так то есть у нас есть некоторые клиенты которые обычным ajax уходят в джинкс который ходит соответственно втором тут берет данные возвращает их клиентам я упоминал ложки фрау не давайте поищем связь вместе вот здесь url и есть на нем сейчас ведь этот сервис вы можете зайти что-то ввести и он нам построить граф категории и вы поймете насколько это интересно то что связи там можно неожиданной найти вот и еще ко всему прочему пока вы ищете там есть несколько нюансов надо писать булку с большой буквы в поиске и соответствовало разделять не пробелы манеж ним подчеркиваем вы просто не успели будет допились потому что на весь этот проект мы потратили с андреем но один день выходных то есть это восемь девять часов что доказывает что это тело действительно просто что очень просто данные которые уже как бы лежат устоялись которые сделаны через избыточность как мы с келли можно легко просто и компактно разместить в tarantul ну для тех кто не вышел интернет знаю здесь интернет не очень хороший вот это пример того grape который будет там представлена но то есть как видно археологические артефакты вот рассказ продолжим андрей спасибо вася а можно мне clicker привет меня зовут андрей дроздов я расскажу вам о том как устроен шарден примите в зале руку те кто сталкивались шар лингам по работе то увидим очень много людей это хорошо я думал славная идея понятна когда мы сталкиваемся с такой ситуации что хранить данные на одном компьютере невозможно мы прибегаем корзинку и в нашем случае мы еще используем engine хотят абсолютно не обязательно то есть тарантул может быть абсолютно самостоятельным в этом плане но это удобно и 1 понятие с которого бы я хотел начать эта зона и избыточность представим что у нас есть несколько дата-центров один предположим находится в москве а другой в красноярске шар тенге в таранто ли реализовано разбиение на зона для того чтобы это было удобнее использовать и соответственно из этого тикать на эти избыточность на данном слайде мы видим пример система в которой из 2 зона избыточность равна двум то есть каждой зоне данные дублируются это очень удобно для и к надежности ключевое понятие которое нам понадобится чтобы разработать свою систему шарлин га это шар функция то есть то функция которая на основе полученных данных решает а на каком же из серверов мы должны выполнить операцию для этого мы будем использовать консистентной хэширование если вот в двух словах мы представим себе некую окружность и все наши сервера отобразим на нее и когда приходит нужны новые данные мы выбираем некий признак по которому мы эти данные также отобразим на эту окружность и соответственно мы попадем на нужный нам сервер чем это удобно что если вдруг в во время работы нашей системы что-то случается например сервер выходит из строя то нам не нужно перехешировать все таким образом это повышает надежность помимо этого в шарден ганашем реализована система мониторинга так сказать гости flake мониторинг что это значит как только какая-то но до выходит из строя система может понять что что то случилось не так каждый узел сканирует все остальные узлы и узнает у них их видение мира таким образом если 1 года вышла из строя одна другая узнает об этом и эта информация распространиться по всей системе как только все узлы будут знать что какой-либо другой узел вышел из строя они его исключат таким образом система продолжит корректную работу поясню на примере предположим у нас есть два сервера тарантул мы объединили их в sharding и начали работать сейчас мы видим процесс инициализации то есть в поле таймс там у нас начальное значение и вот началась штатная работа никаких ошибок нет все работает корректно все классно как вдруг представим себе один дата-центр взорвался пошли ошибки 1 но до пытается достучаться на 2 но что то не так однако пока мы еще не уверены этой разумеется задается некоторым порогом то есть можно выбрасывать ноту сразу при возникновении ошибки но предположим нашем случае мы дадим 10 попыток число предельная достигнуто кластер понял что пора исключить воду и система продолжила свою работу теперь когда у нас есть джорд функция и мониторинг мы можем попробовать пошарить какое решение приходит сразу это однофазная операция то есть к нам приходит запрос мы выбираем узел на который мы хотим page or did наша данной выполнит там операцию идем на него и собственно выполняем однако мы можем работать не только так но об этом чуть позже сначала я расскажу о том как это выглядит если вы будут будущий пользователь тарантул или может быть нынешний хотя много народу пользуются тарантулов просто вот интересно ну есть уже это хорошо так вот вы сконфигурировали sharding и вы не общаетесь с каждым сервером отдельно то есть вы работаете с шар дом совсем кластерам так как будто вы работаете с одним тарантулом это некотором смысле очень удобно то есть вы просто выполняете операции не задумывайтесь однако если же вы все-таки мы хотим использовать нашу систему более сложным образом например нам понадобится транзакций тот на фазной операция нам уже не подойдут поэтому был реализован двухфазный протокол что это значит когда приходит запрос для начала мы находим все шарды на которых операция должна быть исполнена после чего мы рассылаем все данные и только когда мы удостоверились что все данные все операции были разосланы по нужным шердом начинаем их исполнять кроме того нашим шарден где реализован бочанг то есть мы можем исполнять несколько инструкций сразу ну реализовано это через несколько буферов которые создаются мы заполняем операциями и все так же через очередь отправляется по шар дам вот так выглядит двухфазный протокол соответственно в коде то есть если вы будете использовать достаточно сказать кубе генку and точно так же как с транзакциями и выполнять соответствующие операции обращу внимание что помимо того что мы передаем то пол с данными который нам нужен для операции нужно еще придавать уникальная де операции это нужно для некой consistent насти относительно всего кластера то есть это может быть какой нибудь ну номер транзакции в реальных системах это такой индификатор можно найти вот так выглядит конфигурация то есть если вы не знакомы с тарантулом вы можете установить из репозиториев сам тарантул модуль хардинга модуль engine ксо и сконфигурировать его следующим образом вы просто указываете сервера логин пароль избыточность и порт все это представляет собой просто влажную таблицу после чего вы это передаете модули шаринга он запускается и все работает вот представим что вы написали с вами такой модуль шарден когда разумеется он полностью написан на lua хорошо было проверить ну нам понадобилось придумать тест который бы достаточно строго на реальных данных оценивал как работает наша система тоже так бывает что придумаешь benchmark и но совершенно не отражает реальной жизни хотелось хотя бы как это к этому приблизиться так реальные условия нас есть 8 серверов четырехъядерных 60 четырьмя гигабайтами оперативной памяти и мы планируем гонять по ним 100 гигабайт данных с избыточностью то есть на самом деле 200 и размер одного запроса будет два килобайта то есть такой большой джейсон и мы захотели узнать насколько сколько из такой системы с такой архитектуры можно будет выжить производительности рпс архитектуру нас выглядит следующим образом нас есть два слоя один из них слой данных там где сторону а второй слой бизнес-логики то есть об сервера и те и другие являются тарантулами плюс аb слоя находится и джеймс который непосредственно взаимодействует star антон и в свои приложения приходит нагрузка обсерверов выбирают куда именно мы хотим шар лить и отправляет операция исполняется все достаточно тривиально помимо этого у нас есть две зоны это первые 2 сторож и 3 4 между ними настроена репликация теперь когда мы придумали наш тест хорошо бы решить а что есть хорошо и что есть плохо для этого у нас есть четыре метрики и помимо этого тест с отключением всех мастеров то есть он просто берем какой-то момент вот под нагрузкой включаемся мастера и после этого проверяем что на сетевом уровне не было ни одной ошибки что на уровне что ты по протоколу были только 2 сотки никаких 5 соток что сам тарантул и выдал никаких exception of low и помимо этого нас есть тест на апдейты нам нужно чтобы количество запросов которые мы послали соответствовал количество байтов и джинкс и которые там произошли что вполне логично первый тест тест почтения мы использовали возможно некоторым известную утилиту для нагрузочных тестов выключался он очень простых условиях мы просто создавали некую нагрузку и пытались читать из множества данных ну порядка 100 миллионов записей предположим а пользователь на графиках видно что нагрузка на 1 observer измерения проводились именно на нем достигает чуть больше чем 20 тысяч запросов в секунду всего в системе таких у нашей было 4 соответственно общая производительная система была порядка 80 на верхнем графике изображена задержка то есть единицы и мы видим что средняя не превышает 50 секунд максимальная не превышает 120 второй тест был тестом на запись мы тоже плавно повышали нагрузку и пытались инкремент некоторые записи в тех же данных которые мы считывали в предыдущем тесте то есть просто мы указываем никитич ник и там происходит + 1 в этом тесте производительность достигала соответственно порядка сорока пяти тысяч запросов в секунду поскольку у нас 4 от сервера или ты носил было меньше 30 миллисекунд и последний тест самое интересное то отключение узла он полностью повторяет тест на запись 100 лишь разницей что в какой то момент мы берем обрываем все мастера что происходит выключается мастера система об сервера понимает что что-то пошло не так все запросы переключаются на реплики и система восстанавливается продолжают работу вот на графике очень явно виден пик это момент переключения и соответственно время переключения меньше пяти секунд я не знаю хороший или плохой показатель тем не менее наверно хорошо после этого мы получили цифры в 80 тысяч запросов в секунду на чтение и порядка 40 на запись потом мы решили что надо выжать максимум и унесли джинсы с об слоя и посмотрели что получилось в итоге итоговая производительность составила 100 тысяч запросов в секунду на чтения 47 тысяч запросов на запись и тест на отключение мастеров показал такую же производительность из не было потерями в производительность несмотря на то что мы отключили какой через серверов и время восстановление latency было меньше пяти секунд на этом наверное все то что было разработан разумеется можно попробовать по использовать есть пример на гитхабе вот этой маленькой система с википедией ее скорее удобно использовать как демку для того чтобы попробовать что-то свое разработать с индексом и шарден гам ну и ссылки на шар ты с модуль тоже здесь представлена андрей дополнить клиенты это был явных станка и пункты и забыла то уточнить не второе у было тысячу параллельных коннектов которые были непостоянны во время всего тестирование которые могли еще при этом прерываться что очень важно собств до вопроса давай видимо кричать придется мало слышал все большая василий андрей тест в таранто ли было ли настроены persistent и как как она была настроена под вы имеете году дисковый психология стиле вы строгой memory stick нет написали на диск то есть это был микс то есть все было в памяти но разумеется работали экологию и все записывал не все было полностью в память я правильно помню предыдущий докладчик рассказал что пока запись на диск не произойдет клиент ваш тест соответственно не получают ок то есть запрос не считается выполненным тут вашим случилось по такой схеме сложным в общем data cable ну отдельную тему мутные у вас получилось что запись быстрее чем чтение это дело в том что сейчас я покажу я жена ожидал такого вопроса она не быстрее просто чтение в два раза больше нагрузка и тут кажется что лейтон все больше хотя на самом деле вот тут мы доходим до 21 до 20000 рпс а в записи до 12 это вы на нижней жирдяя вот на верхнюю смотрю и вижу число 20 я думаю это миллисекунды почему-то да это правильно это задержка но разумеется чем нагрузка будет выжить им больше будет задержку до не получается у нас на записи максимальной вот линии 20 а если открыть чтение там будет максимально по моему попробуйте предыдущий слайд вот там там порядка стопы нет все верно если бы мы и чтение и запись разгоняли то одинакового количества рпс то вы бы были правы и это было бы справедливо тут нагрузку на чтение почти в два раза больше чем на запись логично что в танце от этого зарастают пожалуйста коллеги спасибо за доклад но честно вам скажу я немножко не понял к чему были картинки почему потому что это простите какие-то сферический конь в вакууме честно скажу так вот прямо вот объясню почему вы не сделали никакого сравнения с вашими что называется ближайшими хотя бы как грится коллегами то есть я вам навскидку могу сказать что в шар денги у нас богат допустим тот же манга да и посмотреть как он это делает с тем что хранить как хранить и обеспечивают не завершил конце снасти которого вас там тоже якобы ну какая то есть это все таки или птиц от яндекс да и в конце концов посмотреть еще на ситуации то есть вот я насколько понял вас тестах там все было хорошо а вот давайте мы внутри dc допустим подёргаем и посмотрим как машина к вам будут устанавливают у меня есть три единственного тарантула да допустим они у меня в sharding это соответственно все здорово реплицируются две зоны вот прекрасный теперь между ними данные но между ними лим ушел произошли какие-то операции записи и туда и туда на а теперь меня вопрос а дальше них появился кто как будет избираться кто теперь мастер тыс у кого данные важный у кого как бы мастеркопия и как это делать то есть понимаете более того в условиях россии то есть яндекс об этом чуть четко говорит что если мы с вами сделаем два dc1 как вы говорили в красноярске да а другой где-нибудь там допустим еще где-нибудь да скорее всего из москвы до них канал у нас будет с вами один и тот же мы в россии тут по-другому не бывает и когда этот канал ляжет а потом вернется будет очень интересно посмотреть да то есть как бы что что-то давит наверно ответ на две части разобьем первая часть насчет сферического коня тут я не согласен во-первых дима сегодня доклад вел о сравнении тарантула со своими конкурентами вот это 1 2 насчет насчет три join a это был совершенно конкретный прототип под реальная задача это реальная задача реальные данные которые будут использоваться соответственно но вы совершенно справедливо сказали про риджуэй мы про этот вопрос думаем и если кому-то понадобится действительно ли join и возвращение нодов кластер то мэтт реализую то есть мы вы не думаете что мы об этом не знаем действительно вы правы вы правильно заметили это да но начал говорю сферических коней соизмерима было испытание других систем и которых частности дмитрий сегодня сообщил но спасибо вот те цифры которые были с утра там был и были четко описаны как греться в инженерном подходит условия теста мы берем столько-то значений таких-то вот таким то бенчмарком понимаете у вас здесь ситуация какая но ну примерно так я сейчас возьму инстанции жюри разверну туда там не знаю там джумлу и получу 55 рпс of все я развернул моду получил 55 рпс of я молодец мне скажешь подожди но ты была сравнил так в амазонии развернули выезжали развернул скукой сколько рпс от получил или ты развернуть шуму или drupal и скука рпс а получил я бы сказал да окей я могу как бы сравнивать понимаете и вот с утра это было сравнение здесь же у вас просто разгоняем чтение до скольки то там рпс of окей и ты его получаем цифру сколько вы может выжить с текущим машин я понимаю я просто потому что но может быть я не понимаю где задавайте долго заднего потратили но тем не менее вам встречный вопрос смотрите мы запустили тест получили сколько мы можем выжить скал с кластера так делают абсолютно все в чем здесь проблема ну хорошо смотрите мы с вами поставили в этот тест выжили из кластер а скорость x теперь у меня вопрос если мы заменим тарантул на мангу мы выжмем x или x умножить на 2 или x делить надо не могу ответить на этот вопрос мы не ставили целью сравни с каким-то другим решением мы предлагаем это решение сравнивая нос дима ними мы лишь решение и реальные цифры на реальных данных спасибо все-таки я вот коллега задал вопрос я ответа такими уж о что будет если кости распался стало два мастера запись вова произошла они потомка них появился и как это соберется вместе на данный момент реализован так что если мы но дурь из кластера исключили то мы пока не поднимаем вот о чем я говорил на вопрос коллеги что если действительно ли джон понадобится то мы его запилим и от проблема будет решаться сейчас этому пока нет это больше прототипы исследования того как можно сделать шарфик на таран то ли еще вопросы мне это будет садись если нужно вот это шар ник добавить еще допустим 20 мастеров как будет проходить перестроение какие вас убить одно создать с боку рядом другое или все в оффлайн угнать ее через восемь часов поднять или там как как это работает и какие примерные времена новый какой-то задачу решали наверно смотрели тоже момент насколько я понял вопрос тоже про про добавление серверов в шар кликнув кластер да вот только что я на этот вопрос отвечал что пока это не реализовано но если это будет необходимо кому-то из я при перестроении кашалот вы говорили про хэш то что данные шарится похожу и вот когда яся добавляется сюда посему пара-тройка серверов то получается что данный лежащий на одном шар дети придут ну лежать на другом вот моя продал британии про пропадания обратно в имейте ввиду что 1 1 шард который вот мы поднимаем он должен как бы догнать всю систему чтобы состава корректно нет ну предположим простейшие функция растут и да допустим есть там пользователи дилеры 10 серверов я делю из неких на 10 по остатку кладу куда потом добавляю еще 10 серверов начинаю делить на на 25 посадку кладу предположим дальше от балды вот и получается что мы не а вот половина данных висящих наш радист номером 5 должна перейти на шарфе номером 6 вот-вот перестроение вот это и есть операция и джо и когда мы добавляем в кластер воду вот подобное предстояние могут иметь место и реализовано я немножечко выясню слова андрея поскольку сейчас шарден как он сказал было только чтобы посмотреть как это реализовать пощупать таскать на себе все и он должен сейчас используется только как сын мэмри движком без создать берти станции таким образом мы можем либо гнусью систему вот ну как я и сказал если это фича кому-то реально понадобится продакшена пили и запилите и это не проблема тут очень важен фидбэк не только наши наши видели но его что же мы-то знаем как это сделать но вопрос нужно ли вам сейчас мы лишь показали что есть sharding он работает дает столько попугаев несмотря на вопросы залу носитель настолько дает если вы хотите сравнить с мангой без пожалуйста мы можем исходники выложить они уже выложены сравниваете скрыть простому что вопрос по поводу трех слоев которые в итоге пришли а какой из слоев итоге оказался бутылочным горлышком и джеймс слой шарден га или в итоге строить вот первый слой который слой сторону там те тарантулы которые исключительно хранят данные слой applications серверов это те тарантулы которые получают запроса там дженкса решают где мы должны проводить данную операцию то есть ну вот куда именно пойти с точки зрения шар zynga плюс там если это какая-то реальная задача может быть расположена эко бизнес-логика и третий слой вот в итоговой схеме это непосредственное джинкс и которые по тарантулы моего протоколу могут входить в эти самые сервера и доставит о них запрос а с другой стороны клиент давайте немножко перефразирую вопрос если я хочу и вот вас получилось 100 тысяч беседа да если хочу 120 какой слоев мне нужно добавлять машин в ну либо вам любого вы не можете вы во время нашего эксперимента вы не выяснили в какой конкретный я могу ответить здесь конкретно есть соотношение сколько как говорится тарантулов переживет один индекс такое соотношение есть сейчас по моему но один индекс разбирает 3 тарантулов таким слов таким образом если вы хотите увеличить превратились на x должна была ведь одну один engine все те не справляются допустим и еще три тарантула потому что один это будет мало 2 тоже вот и все это можно сделать в рамках соответственно но либо и любви имеющихся машин либо новых вот тремя торрентами подразумевается первый верхний слой а вот средний слой вообще не влияет и как средний ринита розария it сзади находится тарантула который исключенной persistent ностью то есть которые пишут свои данные апликэйшен сервера это то где находится логика приближены к данному то есть ло функция вот таким образом на не на этот слой на 2 на него нагрузкой титанической джинсов кладется под по той причине что это все-таки web request of может быть дофига connect и рвутся не роудса конечно engine сумки половиц имя connection имел тарантул тяжеловато немного справиться вот и поэтому вам придется расширять как минимум 2 уровень если допустим у вас и джинсы справляются если engine x они справляются то 2 и 3 а если уж не дай бог стороны не справляются то прирост ищут сторож влепить погибать если на этом ваши вопросы закончен и давайте поблагодарим за хорошее выступление наших коллег"
}