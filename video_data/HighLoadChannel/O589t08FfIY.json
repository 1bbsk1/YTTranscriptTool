{
  "video_id": "O589t08FfIY",
  "channel": "HighLoadChannel",
  "title": "Как выкатить в highload production сервис рекомендаций с BERT-like-моделью/ Марк Паненко (Работа.ру)",
  "views": 744,
  "duration": 2820,
  "published": "2023-04-28T06:19:52-07:00",
  "text": "Добрый день Уважаемые коллеги Здравствуйте как мне уже представили меня зовут Марк паненко Я работаю в rabota.ru работаю тимли Дом руковожу небольшим отделом машинного обучения собственно работа.ру я думаю из названия уже понятно что мы занимаемся вакансиями резюме на нашей площадке соискатели еще для себя работу работодатели ищут для себя сотрудников Так пару слов о себе в данный момент как я уже сказал Тим Лидер работа рун ранее работал в компании из Калифорнии Клер Скейл такой консалтинг который занимается чисто и wstecom То есть все машинное обучение в этой компании делал я кучу проектов успел поделать и помогал ребятам из стартапа из компании labs занимался архитектурными вопросами собственно помогал на том же www строить различные архитектурные решения для eml-проектов постоянно выступая на разных конференциях может быть кто-то даже уже видел собираю бумажные книги по машинному обучению все которые ногу выходят стараюсь покупать и люблю природу Ленинградской области стараюсь это уже выезжать так доклад у меня будет разделен на три крупных части вначале целеполагания Да зачем мы будем обучать вообще свою модель и почему именно с нуля как мы это будем делать и какие необходимые шаги именно в нашем конкретном случае понадобились для того чтобы это все взлетело дальше расскажу про процесс обучения дистилляции и какие варианты архитектуры можно построить на такой системе вначале расскажу про все там детскую игрушечную архитектуру потом поговорим об архитектуре которая уже можно использовать с определенными ограничениями и в конце архитектура которой мы вот пришли которые практически с какими-то доработками сейчас у нас используется Так ну начале вообще зачем нам Берт подобные языковая модель назвали мою кстати charbert Ну Бер подобное и чарт домен на Чар текстом много резюме много вакансий на площадке их могут быть там миллионы или сотни тысяч Все они уже размещены в сети Ну и как обычно пользователи с этим работают не заходят куда-то и начинают искать если говорить про то какие у нас профессии находятся у нас в основном это рабочие профессии так называемый синий воротнички это наши специализация Если вы в Москве например хотите работать водителем такси Да вы вводите в поиск водитель такси получаете себе там 3000 вакансий А если в обратную сторону то может и 10 тысяч резюме Да если вы работодатель соответственно нужно все смотреть и попытаться найти там ту единственную которую вам нужно то есть с этим работать довольно сложно в таком контексте поэтому нужны сервисы которые за вас проанализирует большую часть информации там сделают вам подборку сжатой удобной там помогут вам найти то что вам нужно сразу расскажу про архитектуру то есть это Ну скажем так уже наверное классический вариант для дистиллированных моделей три слоя энкодера виртовых с размерностью 384 и 200 тысяч размер словаря то есть вот такого архитектуру Мы в конце для языковой модели хотим получить Ну сразу пример на котором я покажу и зачем вам это вообще понадобилось Да и что мы получили с точки зрения вот Фил Маск задач когда мы одно слово маскируем и заставляем нашу языковую модель его предсказывать мы так и обучаем Да в процессе и собственно так потом мы проверяем мы для примера Я здесь привел две известных реализации Берта Роберта ди Павлов и Берта материнку Ну и вверху видно нашу модель в самом верху таблички У нас есть фраза которую мы маскируем удостоверение категории C собственно это случайные тексты взяты из нашего собирался из вакансии резюме и там различных текстов из этого домена Собственно как и писали из Ди Павлов они обучались у них очень много новостного домена и поэтому новостной контекст он принципе превалирует Берт мультилинговол но на таких примерах работает не так хорошо предсказывает там предлоги точки Ну собственно наша модель предсказывать то что нужно потому что собственно на этом домене обучалась права водителю удостоверения Ну вроде как хорошо да Давайте еще пример посмотрим такой попробуем предсказать требуется аккаунт менеджер а Роберт нью- А да Берт мультилингол команда менеджер и тоже тире Ну и мы там Project product аккаунт то есть опять же угадали Ну понятно тренировались на своем домене самый интересный пример и собственно тот из-за которого мы решили что все-таки модель нужно тренировать свою Это пример с рабочей профессии самое простое слово просто профессия слесарь Посмотрите вот как новостной домен на рабочую профессию отреагировал да то есть сами Представьте вспомните новости в которых встречалось слово слесарь наверно как слово сожитель или еще что-нибудь мало в новостях встречается в позитивной коннотации А нам Это совершенно не подходит потому что у нас очень много рабочих профессий мы не хотим чтобы они были в негативной коннотации как-то группировались какими-то другими плохими словами не знаю там негативного описания и так далее вот собственно на задачу Фил Маск мы вот такие вещи проверили посмотрели решили что точно будем тренировать свою а также проверили какие у нас получается вектора У нас есть датасеты в которых там различные целевые различные цели Да там где-то Мы хотим предсказывать мэтч вакансии резюме отклики да то есть по факту нас есть отклики есть разметка где-то У нас есть просмотры где-то У нас есть там добавление в Избранные и так далее плюс мы можем делать еще разные синтетические вещи например группировать вакансии резюме по похожести и собственно на этих дата сайтах можем проверять уже готовые модели мы проверили достаточно большой пул моделей в том числе и там лазер лапси там то чем в основном сейчас занимаются многие компании по проверяли различные дистиллированные модели Вот но пришли к выводу что надо все-таки делать свою как раз расскажу про процесс как мы это делали свою вот и еще один такой Point Почему свою вот можно же все понимают что можно взять любую дистиллированную модель не подходит к нам с точки зрения домена Давайте добычи мы на нашем домене и все вот но есть еще одна проблема Проблема с такинизацией если мы возьмем практически любую современную модель она достаточно сильно дробит HR текст То есть если у нас например состоит из там 500 слов описание вакансии со всеми остальным мы ее разобьем там на 2000 и придется нам комнатам форме хоть и уже никакой скорости мы Говорить не будем а если мы будем обучать на своих текстах правильно подготовленных свойственизатор собственно мы можем попытаться эти 500 слов плюс-минус 600 700 токенов обернуть Да и сказать Что окей 512 нам Хватит там даже если чуть подрежем ничего страшного много не потеряем вот собственно поэтому нам Понадобился свой токенизатор Ну а раз учить с нуля и учить таки не завтра тогда выбрали BP Я дропаут здесь классический пример из статьи Чем отличается бпи и BP е дропаут напомню что bpe сворачивает слова пытается свернуть Да в один токен в зависимости от встречаемости БП е дропа Вот ещё иногда разбивает их на под слова таким образом все слова которые часто встречаются в одном контексте они могут встречаться как целиком так и под слово и если оно встретится потом в классический пример водяной вода водный да то есть вода мы часто знаем что такое водный может разбиться на другие токены в классическом такого не произойдёт как раз таки из-за добавление дропаута Ну и опять же картинка из статьи как распределены у нас вектора векторном пространстве для классического бпи для БП дропала то есть видно что они более равномерно распределены Ну и авторы отмечают чтобы большинстве задач превосходит собственно там в статье указывают на каких конкретно задачах работать с редкими токенами Ну и устойчивость к шумным данным для нас на самом деле тоже очень важно потому что опечатки и какие-то неточности неправильно написаны встречаются сплошь и рядом вакансии резюме плюс мы еще этой моделью с помощью этой модели работаем с сообщениями с отзывами и так далее там вообще сплошь и рядом такие тексты Ну переходим к процессу обучения дистилляции мы для обучения собрали большой мультиязычный корпус почему мультиязычный Ну понятно вакансии резюме Русские английские могут быть да это сплошь и рядом вот помимо этого я как я уже говорил нас рабочей профессии у нас встречаются языки из ближнего зарубежья помимо самих языков часто встречаются различные фразеологизмы Да вот мы войти привыкли к англицизмам сплошь и рядом резюме везде там не разработчика девелопер и так далее То есть рабочих профессиях фразеологизмы могут быть из языков ближайшего зарубежья поэтому мы их тоже включили в этот изначальный корпус и обучаем такую условно большую заковую модель То есть это такой тоже классическая размер наверное из Роберта Бейс 6 слоев 768 размерность и далее тоже классическим подходом дистиллируем из неё нашу небольшую модель архитектуру которую я изначально показывал То есть как классический Да ну просто предсказуем распределение токенов Мы постараемся его повторить наши модели в принципе уже можно использовать уже мы получаем неплохое качество уже работаем с собственным доменом но хочется больше во-первых есть куда улучшаться есть возможности еще поучиться во-вторых есть тексты изначально мы берем параллельные корпуса на разных языках на тех тесных которые у нас были в нашем корпусе чтобы у нас и токены встречались и собственно модель могла с этим работать и посматривая на статью от центра трансформер это ребят которые делали с трансформер реализуем тоже такой же подход берем текст на русском языке и на других языках там русский английский немецкий и так далее и пытаемся приблизить представление этих текстов берем Вектор с последнего слоя и пытаемся собственно приблизить представление используя обычный тот продукт собственно обу находимся на такой Лось и получаем модель которая может например вакансию на английском языке и такую же на русском поместить в пространство которым они будут находиться близко тогда мы будем подбирать на резюме разработчик вакансии там на английском на любых других языках которые у нас есть выборки собственно там используя простые методы вроде Canon Ну и немножко дальше мы еще пошли решили Раз уж обучать Так давайте еще попробуем адаптировать домен как я уже говорил у нас есть разметка У нас есть автоматическая разметка на матчи различные То есть например вакансии резюме подобраны пользователи они откликаются на определенные вакансии соответственно отклик это достаточно сильное действие мы можем сказать что это вакансии подходит резюме А есть еще следующий этап после отклика его могут подтвердить или удалить да то есть подтвержденные это еще более сильный сигнал соответственно мы подбираем эти пары вакансии резюме и у нас уже появляется неплохая разметка она уже целевая для нас плюс тексты естественно полностью из нашей области дальше берем получаем похожие вакансии у нас гипотеза была какая что если пользователь откликается на две вакансии то они наверное похожи А если таких пользователей которые одновременно откликнулись на эти две вакансии много то они точно похожи соответственно подбираем пороги подбираем такие вакансии и у нас появляются разметка похожие вакансии плюс-минус то же самое можно делать похожим резюме То есть у нас есть отклики от работодателя приглашение соответственно если то же самое если работодатель приглашает людей с определенными резюме и Они часто встречаются вместе да то они собственно тоже похоже получаем эту разметку плюс еще случайным образом мы сделали 15 процентов вот подбирали не сильно долго этот процент переводим их на те языки которые у нас есть в изначальных данных вот для того чтобы еще улучшить работу с иностранными языками и собственно то же самое что в прошлый раз берем данные прогоняем через модели и дистиллируем на близость представление собственно получаем финальную модель с которой мы дальше работаем какими он обладает преимущественно во-первых она достаточно небольшая То есть она может работать быстро по сравнению с другими архитектурами она во-первых у неё меньше словарь во-вторых меньше размерность меньше слоев со всех сторон она существенно меньше достаточно быстро даже на CPU Она позволяет выполнить нашу задачу и держать нужно нужную нагрузку Да но на GPU вообще получается на самом деле с большим запасом Вот и надо как-то ее проверить Особенно для того чтобы показать вам да была идея такая Давайте посмотрим на те векторные представления Ну а посмотрим через что естественно через юмор да то есть Через что наверно еще можно посмотреть не скажу что Это какой-то точное доказательство того что все хорошо вот Ну по крайней мере гипотеза была такая если тут будет все хорошо то попробуем посчитаем метрики если там будет все хорошо будем использовать что получилось вот смотрите синим с вашей стороны вправо верхнем углу отмечены профессия бухгалтер Да они кластеризовались В отдельной области как получается директора то есть вакансия название писание различные параметры они поддаются в модель собственно векторное представление мы потом через юмапы прогоняем Собственно как видно бухгалтера кластеризовались В отдельной области дальше посмотрим водители а водители вот внизу они такие бледно-зеленые у них отдельно кластер достаточно далеко от бухгалтера Мне кажется логично Да что водители бухгалтеры близко в принципе находиться не должны А вот если посмотреть другие профессии например менеджер по продажам и продавец-консультант профессии похоже должны быть близко они на самом деле близко и расположены добавляем сюда продавец кассир Они вообще перемешались с ними добавляем просто продавец то же самое перемешались менеджер по работе с клиентами достаточно близко Да тоже как-то кластеризуется находится рядом вот ну и собственно повара охранники грузчики кладовщики Вот кстати рабочие профессии Обратите внимание что грузчики кладовщики комплектовщики Они находятся в правой нижней части экрана вот там синий красный темно-красный и Наверное это какой-то бледно желтый цвет собственно эти все профессии на самом деле часто в обучающих данных заменяют друг друга То есть когда мы начали разбирать кейс почему Например грузчик и комплектовщик может находится рядом Мы обнаружили что действительно человек с резюме комплектовщик откликается и на вакансии грузчиков и на вакансии водителей на вакансии курьеров то есть с точки зрения нашего домена это абсолютно нормальные правильно И вот если сейчас взять это двумерное представление напрямую как оно есть взять конкретное резюме например Давайте возьмем грузчика у нас попадет в правом нижнем углу в какую-то точку и просто подберем ближайших соседей скажем Давайте заберем там 50 ближайших соседей Все мы уже получим там систему мэтчинга основанную на контенте в которую попадут в основном грузчики но там будет и кладовщики и другие профессии которые потенциально интересны Этому пользователю то есть принципе с этой модели уже можно строить какую-то систему Ну и собственно какие варианты систем Я предлагаю строить вначале будет супер игрушечная Вот такая архитектура для версии не знаю полезно вам на такое посмотреть или нет но может быть кому-то будет полезно в трансляции то есть в чем идея берем API самый простой любое которое там знаете умеете ласков остапе что интересно берем какие-то предобработки вакансии и резюме отправляю модель считаем там косинус до product Выбирайте что вам больше нравится и отвечаем Просто у нас Score балл который означает расстояние вакансия от резюме Но если у нас десятки или там сотни вакансий например десятки сотни режимы то можно вот это сделать там сколько надо 20 минут 15 Чтобы это сделать И собственно проверить запустить и начать работать да у вас там стартап вы не хотите там что-то долго делать быстро буквально не знаю Если еще на сервисах amazon там лямбда функции Напрямую это все поднять можно за 7 минут сделать она еще будет нормально нагрузку держать как только у вас появляется какие-то адекватные количества вакансии резюме тысячи да то вам нужно полный матч делать тысячи на тысячи То есть вы уже будете несколько дней это считать матч каждый раз на каждую новую вакансию и все уже такой архитектуры дальше Не выжить поэтому вот что мы предлагаем на самом деле вот то что мы предлагаем оно у нас реализовано в некоторых сервисах то есть там где Не предполагается Real Time работа и где нагрузка не такая высокая вот работает такая архитектура расскажу поподробнее то же самое есть какой-то сервис API вот в левом нижнем углу экрана у нас есть растопим в этот раз топи приходит запрос от пользователя в котором есть информация например ID вакансии и описание вакансии У нас есть сервис который находится посередине которая называется в поиск похожих векторов он на базе и каиндекса собственно вся архитектура я ее назвал Каин индекс потому что на базе поиска ближайших соседей через быстрый поиск через систему быстрого поиска иерархического там фаиса нойпедран да там все про них знают собственно они позволяют быстрое время обрабатывать большое количество данных и находить ближайших соседей быстро и с нужным вам качеством скажем так в этом же сервисе Для чего Ну если мы будем вектора получать да то есть будем прогонять туда загружать нужно наверное все-таки где-то сохранить чтобы потом каждый раз не викторизовать на каждый запрос Поэтому в качестве хранилища используем радист смотрим в правый верхний угол это микросервис который называется victories Sense Service Собственно сам сервис Модели там крутится языковая модель я отдельно отметил если артефакт storage то есть не забывайте пожалуйста версионировать модели Это очень важный версионировать данные для того чтобы потом не напороться на какие-то ошибки в продакшн и Я рекомендую мальфом Мне нравится Мы везде используем во-первых действительно во многом уже взрослое решение во-вторых удобный интерфейс И самое главное когда к вам придет заказчик и скажет А если у нас какой-то бесшовной интеграции ссылочка на веб-интерфейсе там Продакшен версия все отлично собственно этот микросервис Он выполняет функцию векторизации есть два контекста мы должны викторизовать все что у нас есть базе и отправить сервис Которому будет работать кайнен индекс и второй вариант мы должны по одному запросу викторизовать и искать соседи в этом индексе собственно вот эти две функции Он выполняет для того чтобы из базы данные перетащить в индекс мы используем airflow У нас есть несколько баз данных одна основная вторая промежуточная для того чтобы правильно учитывать что обновилось что нет потому что не все обновления Да мы то есть мы не все данные из вакансии резюме используем Зачем нам на те данные которые обновились но нам не интересно зачем нам собственно викторизовать заново поэтому две базы получаем оттуда данные бочами получаем Вектора загружаем в индекс к нам приходит запрос в резюме в рекомендации мы идем в микросервис skyenindexом проверяем Можем ли мы по ID 1 то есть ли у нас уже Вектор в радиусе если есть отдаем соседи Ну то есть мы сделаем поиск индексе отдали сосед если нет идем в микросервис с моделью викторизуем то что нам передали обратно возвращаемся в каиндекс и получаем соседей то есть мы получаем ответ всегда и всегда можем что-то порекомендовать это система рекомендации Ну или система matching А это по сути контент без системы То есть у нас есть контент вакансии есть контент резюме Она позволяет решать Ну вот так сразу можно сказать четыре задачи это мэтчинг вакансии резюме в обе стороны да то есть для резюме можем подбирать вакансии для вакансий резюме это похожие вакансии похожие резюме Ну собственно если зайти на сайт и работа.ру и многих других то есть это очень важный части во-первых рекомендации по вашему резюме во вторых Когда вы открываете любую вакансию или работодатель открывает любое резюме ниже там есть посмотреть похожие то есть вещь очень важная очень помогает растить те бизнес метрики которые нужны зарабатывать деньги которые нужны Какие плюсы у этого сервиса Ну не сложно то есть принципе видно что реализация достаточно простая он достаточно стабильный работает достаточно быстро даже на больших объемах данных можно работать псевдоре алтайме то есть нам может прийти человек создать новое резюме мы его викторизуем и отдадим мы не можем быстро получить вакансии да то есть нас все равно какой-то период должен быть Для того чтобы мы в индекс загрузили новые вакансии мы будем делать слишком часто собственно мы будем ресурсы неэффективно использовать поэтому там выбирается какой-то оптимальное решение в нашем случае это три часа и у нас есть запаздывание в три часа Вот и собственно Это только контент без системой как-то в эту архитектуру прикрутить еще что-нибудь например коллаборативку сложно поэтому берем часть этих компонентов и вместе с ними добавляя новые строим другую систему она называется even driven Ну то есть по сути это потоковая обработка данных она основана уже на кафке и собственно позволяет работать и реалтайм и достаточно быстро и разные модели применять Давайте расскажу подробнее То есть у нас есть зелененьким отмечена часть приема событий У нас есть API в нашем случае это тоже rest с точки зрения удобства но это не принципиально это пишут в топе Кафки дальше из этого топика читает два консилмера направление стрелок это направление взаимодействия не направление потока данных да то есть это Стрелка показывает кто взаимодействие с каким сервисом то есть Вот например Мы видим что микросервис который называется части экрана он читает из топика Кафка топиканс соответственно из этого тупика читает два сервиса один верхней части это Spark streaming который небольшой периодичностью раз пять минут бочами забирает все эти события они нам дальше нужны для переобучения моделей для аналитики собственно он триггерится с помощью апачнифи забирает данные складывает их в datalk на ходубе и дальше есть еще один pipeline переобучение модели собственно переобучение модели построена вокруг airflow airflow также периодически запускается с помощью тоже с парка И apachine кстати не просто так потому что это были по требованиям безопасности да то есть если у вас таких требований нет можно его выбросить из этого собственно он запускает джабу которая готовит данные для обучения и запускает pipline перетренировки модели с помощью складываем в нужные артефакт storage и дальше триггерим жопу которая заливает новую модель Собственно как модели обновляется но мы как уже говорили хотим чтобы модель могла использовать какие-то Real Time вещи то есть вот в идеальном мире мы хотим чтобы пользователь например Начинал работать с какими-то вакансиями этому понравились Давайте вот реальный кейс расскажу очень много продавцов в принципе у нас это один из основных Одна из основных групп пользователей и вот очень сильно отличается продавец который хочет продавать например модные товары в бутике от продавца который хочет там продавать продукты в Пятерочке Ну то есть это прям совсем разные профессии хотя и там и там резюме продавец и контент системы Мы скорее всего больше продавцов в Магнит Пятерочку ленту и так далее порекомендуем Потому что их просто больше то есть их тысячи а продавцов бутиков мало Мы хотим чтобы в тот момент когда пользователь начал дизлайкать вакансии продавцов в какой-то Ритейл мы начали меняться мы начали менять рекомендации как только увидели от него еще и лайки например на продавцов в бутике мы начали их много показывать даже если их мало на площадке там 7-8-10 до таких вакансий но если мы все поднимем в топ скорее всего человек будет счастлив и там нашу метрику поднимет который мы хотим оптимизировать Вот а если нет то скорее всего напишет плохой отзыв и удалит приложение поэтому Для нас это важно мы позволяем таким образом пользователям управлять вообще своей платформой под себя перестраивать платформу То есть когда вы видите рекомендации вот если Вы заходите нетфликс там Яндекс музыку и так далее вы сами под себя настраиваете сервис Когда вы зашли через месяц пользования это два разных сервиса хотя там технически может быть ничего не поменялось на обновление не проходило вы своими действиями скажем так настроили эту рекомендательные системы им пользуетесь И поэтому очень часто сложно спрыгнуть с этого сервиса на какой-то другой потому что там уже все хорошо там уже меня знают мои интересы А тут заново всё и какая-то странная подборка и так далее собственно Это для нас важно Давайте расскажу как выглядит Процесс получения рекомендации тоже интересно сделали У нас тоже есть растопи и туда приходит пользователь и говорит Дай мне пожалуйста рекомендации для вот такого User ID плюс какой-то информацию например его резюме если они есть мы сразу записываем этот запрос в радиус и в рейдесе при переобучении модели у нас для всех пользователей у которых есть набор целевых действий пред сгенерирована 5-10 предсказаний в зависимости от платформы которую мы готовы отдать сразу то есть Мы записали и сразу же пользователю отдаем 5 предсказаний которые он может сразу показать на время 2-5 миллисекунд зависимости поэтому мы и сейчас переделаем Моби движение в котором на главной будет рекомендации потому что вы можете зайти и вам не нужно ждать тогда в часы пик там даже там пол секунды это может быть заметно будет думать что подлагивает и удалят потом а Здесь мы получаем рекомендации сразу же пока вы их пролистаете как раз пройдут эти пол секунды за которые модель отработает и вы получите там остальные свои 200 300 зависимости от того какая модель Ну и собственно параллельно с тем как мы отдали рекомендации зарядиться у нас в топе Кафки записался запрос на то что нам нужно порекомендовать Этому пользователю какие-то вакансии собственно мы этот топик в потоке читаем с помощью сервиса это сервис от компании Робин Гуд достаточно удобный неплохой с бетон реализации поэтому его легко использовать мне без минусов Конечно вот но в принципе все минусы там с небольшой долей скажем так нервов и ночного кофе можно победить этот сервис читает запрос на рекомендации определяет В какой из моделей он может его отдать во все сразу или в каких-то моделях например нет пользователевые действия мы только по контенту Работаем или например нет резюме но есть действия тогда только коллаборативка или то тогда обе модели в зависимости от того какие модели пишет в нужные топики это собственно топик в который пишется запросные рекомендации его уже читают другие микросервисами которые собственно генерит предсказания сгенерированные предсказания пишутся в Кафка топик который называется рекомендует и собственно оттуда уже попадают в радиус в этот момент пользователь который присылал запрос может зайти в apip запросить предсказание если они готовы собственно получить все оставшиеся предсказания какие здесь явные плюсы Ну во-первых мы можем учитывать как я уже говорил все действия пользователя которые он только совершил во-вторых мы можем начинать работать с пользователем как только он совершил необходимый набор действий то есть новый пользователь появился на площадке только зашел и просмотрел 5 резюме у нас стоит 5 просмотрел 5 вакансий После этого мы можем начинать ему какие-то уже не очень хорошие но Давайте рекомендации за счет того что мы работаем все время принимаем события С небольшой задержкой после просмотра 5 вакансий это в принципе нормально кейс человек 5 вакансий Может там открыть просмотреть если что-то интересно и начать получать свои предсказания во-вторых сервис основан на сервисе Кафка Я думаю что здесь на платформе на площадке очень много говорят о Кафки о плюсах и минусах вот я наверное минусах говорить не буду мы вообще этот сервис брали как сервис в сберклауде и не столкнулись с теми болями с большей частью которая там сталкиваются все остальные Вот наверное еще столкнемся в будущем но пока только положительные результаты собственно за счет того что сервис вокруг Кафки Вы можете работать с большим объемом данных то есть вас может быть в час пик прилететь там 100 тысяч событий Да вы их не разгребете моментально немножечко по деградируете по времени на сервис не упадет Вы можете отработать окном эффективно загрузить свои ресурсы и там эффективно использовать свои GPU там все наверное знают что несколько раз быстрее работает собственно Но если вы будете пихать по одному предсказанию то будет даже медленнее чем если как раз окном и пачкой то как раз и получите свой прирост скорости Ну собственно подобные модели которых мы говорили которые здесь кстати тоже представлены Агенте тоже самое то есть не всегда можно в GPU особенно в современной в котором там может быть 80 гигабайт памяти Да если вы будете грузить туда по одному примеру вы не всегда получите нужный прирост А вот если вы пачкать загрузите все 80 гигабайт и обработайте их сразу там например там 100 запросов в окне то получите нужны прирост здесь как раз вот этот скажем так с одной стороны большая нагрузка работаем эффективно маленькая нагрузка мы можем себе позволить немножко работать неэффективно вот ну и собственно все о чем я сказал свел в такую табличку Особенно для тех кто будет потом смотреть эта презентация отдельно будет наверное полезна это мои оценочные суждения по поводу этих архитектур Понятно сервис только для plc только для того чтобы что-то попробовать проверить может быть в тесте прогнать на каком-то маленьком объеме данных архитектура которая Я назвал кайн индекс она в принципе очень плохая она у нас работает но не позволяет работать реалтайм и чуть-чуть хуже работает под большой нагрузкой Ну и венгривен на основе Кафки она классная работает быстро но и посложнее реализовывать и ладно Еще реализовывать самое главное потом поддерживает то есть нужно будет поддерживать тех разработчиков которые реализовывали чтобы они все время оставались у вас команде И вы не потеряли эти навыки потому что большие сложные сервисы как во многих компаниях особенно там я часто сталкиваюсь Когда собеседой людей что мы все сделали все внедрили но сейчас не знаю работает или нет то есть после того как люди уходят часто уходят эти сервисы чтобы этого не произошло здесь нужно быть внимательным на этапе проектирования особенно там Если вы архитектор Подумайте заранее как вы будете отыгрывать в этом плане Ну и на этом У меня все можно оценить доклад по QR коду Ну и собственно Добавляйтесь пишите Telegram ОДС и моя почта Всем спасибо ваши вопросы Спасибо за доклад ты говорил что помимо что модель Это только один из вариантов вот в ивент архитектуре я правильно понимаю что можно параллельно подключить к лаборативную фильтрацию или какие-то другие И что это в целом такой универсальный для того чтобы можно было не только вакансии резюме например в иконусе Да да все верно то есть модель это основа контент системы Когда нам нужно викторизовать что-то на основе контента но в такой архитектуру Вы можете добавить любое количество микросервисов то есть вот здесь их два Да Фауста LS на основе собственность модели Да это коллаборативка классическая Агент соответственно работает с этой модели про рассказывал хотите добавить еще что-то можно добавлять еще то есть например вот в эту архитектуру очень легко встроить модель второго уровня Если вы хотите дальше свои рекомендации переранжировать например вы рекомендуете товары в маркетплейсе Да и вам важно не просто правильную подборку сделать вам У вас есть цена То есть вы можете показать или один товар или другой собственно дальше Вы можете обучить модель правильно правильно обучить модель для того чтобы она предсказывала вероятности например покупки Да вот или вероятности перехода Если вы продаете переходы собственно и дальше умножить на ценные получить там от ожидания своей прибыли собственно и показать тот продукт который мы ожидания больше просто добавляете после Кафка топикан резался еще один микросервис который прочитает из него сгенерированные уже все кандидаты и переранжируют их зависимости от того что вам нужно хотите еще еще добавлять можно кандидатов больше власти подключить оттуда кандидатов подтащить или векто или прям поисковым запросам то есть вариантов здесь очень много очень легко расширяется Спасибо Ну все зависит от ресурсов понятно да то есть и от того как это вообще оценивать если мы говорим про один там современные одно ядро современного ципул то это в районе 100 150 миллисекунд на один запрос единичный соответственно пачкой будет также Да зависит примерно также плюс-минус зависит от того сколько у вас памяти То есть если вы можете память засунуть сразу 100 да то есть уже не 150 а может быть 200 миллисекунд будет на 100 соответственно разделите Спасибо пожалуйста Привет Спасибо за доклад вот здесь на графике есть часть про тренд paypline и Sei Job у которой выкатывает обновление модельки можешь подробнее рассказать по какой логике выкатывается новое обновление И как вы решали проблему например с тем что у вас там где-нибудь Трейдинг посыпался и кривая модель Да хороший вопрос Спасибо Давайте тогда подробно расскажу про вот этот Весь pipline который таким светло-розовым цветом помечен то есть мы собираем все события которые нам интересны дальше с помощью Спарк джаббых обрабатываем и складываем с определенными партициями Train pipeline который у нас работает для тренировки моделей он собственно с определенными ограничениями забираете данные из ходу и тренирует модель дальше vrflow есть цикл оценки То есть у нас есть целевые метрики которыми Мы работаем И мы понимаем что мы не можем выбиться за определенный то есть мы можем быть чуть хуже чуть лучше но сильно не можем То есть у нас если мы попадаем скажем так в нужный нам интервал выкатка происходит автоматически если не попадаем то летит Alert и надо выкатывать руками вот то есть Пока решили так что-то более сложное не придумали Ну как не придумали в принципе придумали надо посмотреть наверное на дисперсию данных Да надо посмотреть на дисперсию оценок модели и так далее Вот но самый железный вариант это если все хорошо все хорошо если что-то плохо срочно зови разработчика Поэтому пока так ну и собственно после того как прошла проверка если проверка пошла происходит загрузка Ну дальше они так далее если не прошла то падает Дак в Air Flow и собственно шлет на то что что-то пошло не так Ну а дальше поднимает новый под в кубери и меняет раз Да спасибо за доклад А можете показать пожалуйста в цифрах результат то есть как было до этого 5 Лайна Как стало потому что я первый вариант 2 3 варианта как допустим выросли оборот пользователя и так далее матча между работодателем искателем Ну тут все зависит от того еще как внедрена модель да то есть мы строим сейчас сервисы практически с нуля то есть изначально там все строилось вокруг поиска в rabota.ru и собственно те контексты в которых мы применяем сервисы не везде мы можем тесте померить старый метод снова То есть часто это прям новый функционал который мы проверяем на метрике например как вообще пользователи но функционал реагирует Вот Но есть части где можем померить и например предыдущая архитектура на основе вот Каин индекса она у нас работает в том месте где вы обновляете резюме либо первый раз создали резюме и у вас всплывает попап в котором показывается рекомендованные вакансии либо при каждом обновлении всплывает папа собственно показывает там раньше там как раз из поиска был такой на правилах системы рекомендации сейчас наш собственно там в обтесте мы мерили отклики целевая Метрика была количество откликов на пользователя Вот и собственно мы там примерно на 30 процентов смогли улучшить В итоге вначале там 10-15 докрутили докрутили и дошли до 30 процентов после этого старую систему отключили и начали работать уже полностью с новой сейчас происходит вторая операция У нас еще есть сервис по рекомендации скилов Он кстати работает на основе этой модели плюс графовая база данных и собственно раньше скилов на площадке было мало в резюме было в районе 10-11 процентов вакансиях не было вообще мы сейчас включили в резюме сейчас в районе 50 процентов скиллов и вакансиях в районе 30 вакансия включились совсем недавно ждем что они стильно вырастут потому что вакансии время долгоиграющие то есть они переопубликовываются и каждый раз дополняются поэтому новая модель сейчас тоже поедет уже с использованием скилов при викторизации поэтому Надеемся еще приподнимем то что связано с рекомендациями на основе этой модели здесь сложнее потому что эта модель это архитектура сделана для как раз полностью изменение Flow работы то есть вот сейчас я надеюсь к Новому году выйдет обновление мобильного приложения в котором уже все это будет готово и собственно сейчас уже вышло обновление в котором эта часть работает но она работает Stories У нас есть приложение такие Stories там рекомендовано вам можно зайти посмотреть вот там оттуда рекомендации они работают достаточно хорошо но не все пользователи заходит То есть просто пользователь который туда перешел их очень маленький процент Мы хотим чтобы все видели эти рекомендации поэтому главное будет в принципе страницы и рекомендации и вот как вырасти там уже будем смотреть будем версии приложения сравнивать пока сказать не могу что именно по цифрам хорошо спасибо еще один небольшой вопрос Вы сказали Очень сложно разрабатывается вариант архитектуры номер три очень сложно это сколько по времени Ну это не очень сложно все зависит Наверное от того кто делает Каким составом насколько мотивированные люди мы вот эту архитектуру делали наверное месяца 4 скажем так в три активных разработчика и два им помогающих Вот поэтому но при этом там из этих троих ранее с Кафка работал только один да там с какими-то другими сервисами тоже кто-то один То есть много знакомились с технологиями если технологии знакомы Я думаю что за месяц тут вполне можно сделать Кстати по поводу технологий вы использовали какие-то верхние уровни вы не знаю штуки которые позволят выкатить вот эти сервисы мгновенно Ну чтобы ускорить разработку у нас настроен в принципе в компании принят такой общий до Flow выкатки сервисов у нас естественно cubernet часть инфраструктуры в области части инфраструктуры свои сервера есть хэллоунвейв это такая библиотека которая разработали наши дропса Она кстати в открытом доступе можно посмотреть Вот как раз верхний уровень настройка над классическим холмом который более верхний уровня позволяет тепло идти разворачивать эти микросер Спасибо"
}