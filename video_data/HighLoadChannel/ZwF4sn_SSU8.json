{
  "video_id": "ZwF4sn_SSU8",
  "channel": "HighLoadChannel",
  "title": "Женские сети: как нейронные сети помогают в индустрии красоты / Артем Просветов (CleverDATA)",
  "views": 493,
  "duration": 2543,
  "published": "2019-05-14T15:02:09-07:00",
  "text": "итак я сегодня я хочу вам рассказать про то как нейронной сети помогли нам в индустрии красоты сначала пара слов про нас я представляю компанию клевер дата и наша компания занимается разработкой различных платформ по работе с данными которые помогают объединять данные из разных источников в периметр например дан с сайта сданные серым и удобно с ними работать другим нашим интересным решением является berza данных которая поражает уже своими масштабами и регулярная аудитория этой бирже порядка 100 миллионов людей ежедневно на этой бирже можно покупать данные продавать данные обмениваться данными и за счет этого улучшать какие-либо бизнес-процессы и конкретно про наш случай который я хочу рассказывать к нам обратилась маркетинговое агентство из великобритании которая специализировалась на брендах бьюти-индустрии и нас попросили оптимизировать маркетинговые коммуникации с аудиторией нескольких брендов для начала у нас было всего три бренда у этих брендов было четыре канала коммуникации это сайт email и социальные сети типа фейсбук и adwords и для того чтобы решить эту задачу нам необходимо было разрешить целый ряд вопросов во первых нужно было понять что предлагать человеку во вторых нужно было понять когда это предлагать в таком канале это предлагать определить нужен у нужна ли человеку предоставлять скидку и тем людям которым скидка не нужно конечно и и предоставлять не следует а также определить частоту коммуникации для того чтобы решить все эти вопросы конечно нужно было разработать целый ряд предсказателях и рекомендательных моделей и как раз я начну с того что расскажу про наш опыт разработки именно рекомендательных моделей традиционно когда начинают работу с рекомендательными системами обращаются к классическим методам и коллаборативный фильтрация является одним из самых классических методов которые в первую очередь приходит на ум в этой задаче суть коллаборативный фильтрации достаточно простая мы строим матрицу предпочтений где по строкам расположены люди а по столбцам расположены продукты если человек купил какой-то продукт у него возникает там один раз купил от единица если дважды купил-то двоечка и так далее притом можно рассматривать модификации можно рассматривать несколько раз он купил сколько раз он его посмотрел или какие то другие метрики главное то что мы можем найти близких людей к каждому человеку по его группе и усреднить данные для этих близких людей в матрице предпочтений и благодаря этому можем найти рекомендации для этого человека то есть человек скорее всего захочет обратите внимание на тот продукт на которую обратили внимание похоже на него люди по покупкам у этого подхода есть серьезный минус на больших масштабах он становится неэффективной потому что необходимо рассчитывать матрицы о расстоянии каждого человека скажем человеком и это возникает уже сопряжено со сложностями то есть матрица расстояний там миллионов людей будет большой размерности в больших масштабах традиционно обращаются к алгоритму а л с который сводится к тому что матрицу предпочтений раскладывают на 2 матрицы матрицу людей и матрицу продуктов это можно сделать параллельно и очень быстро в принципе это делается пошагово в каждом на каждом шагу фиксируется одна матрица и подбираются оптимальные viso в другой матрице и что очень удобно просторе так сказать человеку знать его предпочтение по продуктам достать достаточно легко для этого не нужно обращаться к информации о всех других людях еще одним альтернативным подходом в коллаборативный фильтрация является просто транспонирование по сути матрицы предпочтений и растения тоже самой задачи уже не на людях она продуктах тогда виктора получаются более высокой размерности и результаты становятся более устойчивыми их можно перечитывать резы обновлять эти рекомендации можно реже и они в принципе меньше подвержены изменению со временем но еще один есть метод это классические ассоциативные правила которые по сути из представляют из себя некую статистику того как товары попадают рядом друг с другом в корзины метод очень простой поэтому вычисляется очень быстро и в качестве его результата получается красивый граф наглядный который показывает как товары друг с другом связаны такой граф любят рассматривать представителей от бизнеса находить в нем что-нибудь какие ним какой то смысл и объяснять для себя какие-то феномены но у этого метода есть очень серьезные минусы во-первых он далеко не всегда может дать нужное количество рекомендаций потому что может быть не наберется по статистике для конкретного продукта каких-либо рекомендаций связок с другими продуктами вот и для людей мы можем рекомендовать только новые товары которые они еще не покупали во всех этих методах есть целый ряд таких недостатков которые хочется исправить они например не учитывают состав продуктов может быть в какой-то индустрия это не имеет значение но ведь индустрии становится важным какой состав у конкретного продукта может один из покупатель обращает внимание на продукты с алоэ человеку нравится запах алое он готов с большей охотой покупать продукты запаха мало и конечно матрицы предпочтений не отражает эту информацию кроме того в бьюти-индустрии нередко важна последовательность продуктов которые применяются и для какой например кожи применяется продукт если человека кожа сухая он сначала избавляется от этого недостатка после этого может применять продукты для нормальной кожи то есть нам хочется добавить намного больше информации чем это есть и возможность делать в матрице предпочтений классической в таком случае хорошо обратиться к нейронным сетям и в частности к рекурентные нейронным сетям которые себя прекрасно показали на задачах работы с текстами и задачах в которых важны последовательности и главный временной последовательности в таких неровных сетях помимо входящего вектора учитываются прошлое состоянии ячейки новый тег сетей есть еще более модная и современная и широко используемая модификация под названием stm ячейки estm сети которые обладают ячейкой памяти и они могут забывать информацию записывать информацию воспроизводить информацию благодаря этому они могут воспроизводить очень сложные последовательности действий итак если мы натренируем нерону сеть в которую в качестве им беден гав добавим информацию о продукте мы уже получим весьма неплохое решение но мы столкнемся с следующей проблемой такая нейронная сеть должна будет по сути размечать по одной ячейки всю матрицу предпочтение это очень долго матрица предпочтений может быть гигантской и эту проблему надо решать для того чтобы решить эту проблему традиционно обращаются к архитектуре dsm можно переформулировать проблему рекомендаций в формат подбора наиболее релевантного релевантной выдаче к примеру у нас есть несколько документов и запрос пользователя и мы хотим найти ранжировать эти документы по запросу модель dsm позволяет натренировать отдельную нейронную сеть для пользователя отдельную нейронную сеть для продуктов и в качестве финального слоя сделать нейрон который будет воспроизводить косинус на и расстояние между теми тендерами которые выходят из 1 2 нейронной сети в чем преимущество такой нейронной сети мы можем после ее тренировки раскурить всех людей по отдельности с помощью одной неровно city про спорить все продукты по отдельности с помощью другой неровности а потом быстро и эффективно найти косинус на расстояние между полученными векторами уже на своей стороне это будет параллельно это будет в разы быстрее итак короткие выводы этой части следующий что нейронной сети в принципе позволяют сделать чуть больше чем другие классические методы и если их адаптировать под конкретную ситуацию можно ожидать того что они дадут лучший результат другая тема с которой мы столкнулись связано с тем как воспринимать последовательность действий человека с этой проблемой сталкиваются в целом ряде классических задач например это предсказание оттока это детекция анти фродо это предсказание просто поведения человека как хорошо закодировать поведение это одна из сложных задач и ее можно решать по разному типичный лог событий которые мы могли использовать выглядит следующим образом обычно большинство из нас получают email рассылки в этих имел рассылках стоит так сказать пиксель который фиксирует все нашей активности с этим емейлом к этой информации не принято обращаться но на самом деле оно где-то остается и ее можно получить при желании соответственно все даже нет открытые письма но в которых не было клика все эти события фиксируются это кайлок может выглядеть следующим образом когда есть различные люди и в потоке хронологическом они кто-то открывает письма кто-то кликает на ссылки кто-то отправляет письмо в черный список и описывается то есть это не структурированная информация а для того чтобы применять методы машинного обучения хорошо бы перевести ее в четкую структурированную форму как это можно сделать есть целый ряд подходов и один из подходов это посадить да это санте ставили эксперта который изобретет несколько признаков множество признаков которые будут отражать характеристики поведения человека после этого навесить модель предсказательную и получить хорошие предсказание но если мы можем обратиться к модным современным методам машинного обучения с помощью нейронных сетей почему бы это не сделать с помощью де пленник которая заменит по сути экспертную работу естественно в последовательность действий человека можно ожидать важность того такие события предшествовали его действием и тоже должны хорошо сработать рекуррентной нейронной сети и а вот ставим ячейки но хорошо бы их с чем-то сравнить и для того чтобы их сравнить мы можем обратиться к другому ответвлению нейронных сетей tver ночным нейронным сетям который прекрасно себя показали на распознавание изображений и которые так сказать отлавливают паттерн и возможно это сыграет свою роль в задаче мы подумали теперь пусть архитектура нейронной сети нам известно но какую структуру это нейронная сеть будет иметь если мы будем обучать нейронную сеть на каком-то целевом действие это не ровно сеть будет знать о будущем запомнит эти знания о будущем и мы ее далеко не в ее результаты далеко не во всех моделях сможем применять если мы не будем это делать аккуратно иначе может перед текане информации произойти дата летишь и они будут переобучаться хорошим подходом является авто кодировщик который не тренируются над заливом действие структура авто кодировщика выглядит следующим образом есть ни одна не ровная сеть которая изменяет размерность входящего тензора уменьшая ее и другая неровно сеть которая обратна развертывает тензор в изначальный формат вот это бутылочное горлышко по сути может быть закодированным представлением входящего тензора а мы можем в процессе обучения неровности требовать чтобы входящий тендера исходящий тензор были максимально близки друг другу чем ближе они тем лучше неровно сеть научилась обобщать этот входящие тендер и потом его разворачивать обратно ну что ж мы будем подавать на вход такой нейронной сети как в качестве тендера примеру давайте закодируем поведение человека в таком one hot стиле последовательно человек сначала получил письмо открыл письмо кликнул на ссылку затем зашел на сайт потом прошло какое-то время он снова открыл письмо опять зашел на сайт и совершил покупку вроде информации об этом есть но проблема в том что для разных людей требуется разное количество действий мы должны это учесть мы можем дополнить до какого-то фиксированного размера тэн тензора нулями недостающую матрицу размер этого тендера это отдельная история и мы для начала выбирали этот размер равным 128 событиям после ряда экспериментов мы поняли что мы себя переоценили такое количество событий совсем не играет большую роль и можно ограничиться в 60 четырьмя событиями а то и 3 стих двумя но здесь не учитывается время нужно учесть время между событиями потому что это важная информация для сети важной информации для моделей и эту информацию нужно передать как-то в на софт энкодер можно учет времени сделать следующим образом мы разницу между времени между событиями во времени найдём разницу затем ее логарифмируют а затем поделим на максимальное в принципе это вполне оправдан и потому что распределение вот эти разницы между событиями представляет из себя блок нормальное распределение и мы переходим к такому диапазону сначала оплот нормального к нормальному а потом от нуля до единицы переводим что очень нравится нейронным сетям они в этом диапазоне лучше всего работает скажите всем ли понятна разница между чем брать на последнем событие не очевидно но разница между по последним событиям хорошо брать между этим событием и текущим моментом итак все готово начинаем тренировку наши неровной сети сделаем архитектуру авто кодировщика на основе ласты мячик сверточек нейронных сетей а также сделай некую гибридную архитектуру которая представляет из себя сначала слой карточных нейронных сетей а потом stm ячейки по метрики качества баннеры кросс энтропии мы видим что побеждает свёрточная неровной сети и гибридная модель и это в принципе нам может сказать то что похоже важнее не временная разница между событиями а намного важнее сами паттерны поведения а человек все-таки существо шаблонное но это ведь метрика качество отражает как хорошо мы кодируем тендер и потом его раз кодируем это в принципе не то метрика качество которое нам нужно в нашей задаче мы хотим эти закодированные признаки использовать в других моделях и по сути это так как у нас практически бинарные вектора у нас разница между векторами он предсказывает что тендер на входе и тендер на выходе будет курс баннеры лосс близки так вот добавим эти признаки в какую-либо модель мы взяли модель который предсказывает покупку человека в ближайшее время и мы увидели что качество этой модели улучшилась притом качество модели если мы добавили признаки от свёрточная нейронная сеть и поднялось выше чем качество модели при добавлении просто stm кодировщика если говорить о гибридной модели то ее качество очень близко качество свёрточная нейронная сеть и однако дисперсии ошибок больше 100 в принципе им тоже можно объяснить для себя те это идею что модель как бы более сложное поэтому дисперсия ошибок у нее шире для того чтобы посмотреть разницу было был ли какой-то прирост качества от добавления этих новых признаков в предсказательную модель что посмотреть рок кривые и вот базовая модель синеньким обозначена расширенная модель с добавлением новых признаков обозначена желтеньким и прирост качества примерно на 10 процентов наблюдается если посмотреть по значимости признаков то доминируют признаки от авто кодировщика вот они заняли самые самые высокие рейтинги конечно не обошлось и без признаков которые сделали на основе экспертного знания там например количество кликов или количество событий но как мы видим все таки основную роль играют именно признаки связанной с поведением здесь это такой же классификатор который предсказывает конверсию в ближайшее время и если мы спроецируем полученные признаки спроецируем признаки от авто кодировщик of на двумерную плоскость с помощью метода ты сам я мы увидим разницу между поведением людей которые совершают покупку ближайшее время и теми которые не совершают красными точками обозначены те люди которые совершили покупку в ближайшее время синими те которые не совершали мы видим что есть концентрация красных событий красных точек есть зоны где в основном доминируют синие точки а есть прям такие зоны где нет ни одной красной точке это просто зоной обреченности в которых уже неважно что то посылать людям их лучше не трогать итак на этом этапе их как у такие выводы хочется обозначить во-первых вывод что возможен перевод неструктурированной информации в структурированную для применения методов машинного обучения через нейронные сети и если такую закодированную информацию добавить в классификатор то можно ожидать прироста качества и и что мы получили мы получили следующую архитектура у нас есть данные о продажах есть данные о рассылках есть данное поведение человека на сайте мы на этих данных прогоняем рекомендательную систему устроим ряд предсказатель ных моделей и в принципе получаем для каждого человека персонализированные компании которые говорят что рекомендовать да говорят скидку если нужно говорят нужен пробник человеку или нет в общем то все что нужно только осталось получить одобрение маркетологов значит мы на определенном этапе генерировали каждую неделю набор таких компаний эти компании выглядели как фиды такая вот лента фидов и маркетолог их просматривал большинство из них он запускал какие-то если он говорил что такой продукт не надо использовать в маркетинговых коммуникациях совсем мы естественно это принимали на карандаш и исключали этот продукт из наших рассылок в итоге мы наработали достаточный уровень доверия но пришли к следующей проблеме если мы перейдем к формату сел в драйвинг маркетолога который не будет уже эти просматривать рассылки а будет самых запускать в автоматическом режиме мы должны найти оптимальное время для этих рассылок у нас нет таких же соображений априорных как у маркетологов которые запускали эти рассылки которые это делали там свою свое рабочее время но мы можем использовать данные о том как человек открывал письма как он заработал с письмами и найти для каждого человека персональное время когда я с ним лучше всего общался поэтому мы как раз занялись этой проблемой как найти оптимальное время для рассылки конечно мы обратились сначала к источникам другим и на стуле что чаще всего обращаются в этой задачей к статистике по рассылкам различным определяют для каждой аудитории когда чаще всего открывают письма в какой день недели в какое время чаще всего открывают письма и в это время делают ковровую бомбардировку всем получая какой-то прирост к open rate у мы решили пойти другим путем и для начала по исследовать данные в принципе мы заметили то что если посмотреть распределение открытие писем по времени то это распределение состоит из трех пиков люди читают письма утром в обед и вечером в принципе это очевидно для этого не надо было проводить особый анализ но мы можем для себя определить что в принципе если будем решать задачу классификации в этих трех окнах то это уже будет какое-то решение но проблема заключается в том что большинство писем открывается тогда когда их люди получают у нас всех есть современные устройства есть push-уведомления и люди открывают письма тогда когда их получают или в течение нескольких часов после этого соответственно если мы будем просто тренироваться на том когда люди открывали письма мы получим тоже распределение которое отражает когда они получали эти письма естественно такая такое решение задачи нам не нужно нам нужно все-таки найти для каждого человека какое-то персональные для него время что можно сделать после ряда экспериментов мы пришли к следующему подходу мы возьмем только те открыть и писем которые были спустя 24 часа после рассылки их человек открыл на следующий день то есть он открыл их не потому что ему пришло письмо или уведомления а потому что уже время было для него комфортно и когда он читает почту мы подразумеваем что это делается в таком режиме соответственно вот человека получается например пара открытий в обед пар одно открытие вечером мы случайным образом сэмплер у им из этих открытий одно и получаем разметку далее строим признаки по открытием тренируем классификатор и что важно мы строим признаки по открытием уже не только тем которые были сделаны спустя 24 часа а по всем открытием включая и открытие сразу после рассылки и после этого тренируем классификатор и можем с помощью этого классификатора разметить всех людей даже тех у кого открытие спустя 24 часа не было значит мы находим оптимальный интервал утра обед или вечер для человека а затем можем усреднить все его открытия в этом оптимальном интервале и найти тот оптимум который даст нам наилучший результат когда человек читает письма да если посмотреть распределение рассылок писем они конечно выглядят таким образом рассылки были в основном утром потом немножко побед и немного вечером распределение которые мы получили для оптимального времени рассылок для каждого человека выглядит по-другому это тоже 3 компонентное распределение что естественно но центр тяжести этого распределения смещен к вечерним открытием больше всего люди открывают вечером письма и так мы получили достаточно комплексное решение для того чтобы делать персональную рассылку через емейл и если люди хорошо реагируют на емейл и мы можем задавать стратегию и с ними коммуницировать посредством рекламы через фейсбук или использовать их через слуг и лайк в фейсбуке это дает прирост метрик качества и в конечном итоге для маркетолога наш инструмент выглядел следующим образом это была некая доска результатов без борт с тем сколько было писем разослано и какой результат они повлекли сколько людей с конвертировалось и какую прибыль получила компания теперь выглядели намного лучше чем те метрики качества которые получали маркетологи и с помощью ручного труда придумывая каждый раз одни и те же рассылки исходя из текста про sonic там как какой продукт надо разослать потому что его больше всего на складе аудитория не было восприимчив более восприимчива и она не была так утомлена одними и теми же предложениями то есть они получали каждый раз что-то новое каждый человек получал персональное предложение на этом все спасибо за внимание я буду рад ответить на ваши вопросы раз спасибо большое за доклад очень интересно у меня как бы две части одна наверное я может быть пропустил докладе по поводу объема данных которые вот . для обучения нейронных сетей все-таки нужно довольно много и 2 в момент по поводу признаков вот то что вы говорили что могут быть признаки которые явно не задан изначально там цвет запах там для какой кожи там и так далее я так понял что вы все вот это как бы доверили определению ну чтобы не нейронная сеть сама вот это находило да вот эти вот плотины и признаки но как бы может быть часть таких признаков можно находить явно как бы на все наверно потому что какие-то вещи там ну не знаю может быть рекламной кампании что который вот сейчас идет привлекает человека к этому продукту это как бы наверное лучше сеть чтобы находила вот какие-то признаки которые заранее известны по продуктам вы не пробовали добавлять просто как явные признаки мы если пробовали то к чему это привело сначала пробовали действительно явные признаки ее навешивали даже классификатор поверх а вот эти классических рекомендательных моделей получали более менее результат но с чем что у нас было на руках у нас на руках был большой корпус рекомендации бьюти-блоггеров и мы могли на этом корпусе натренировать им бединге по продуктам по этим мб-1 у продуктов было текстовое описание и в поэтому корпусу рекомендации блогеров мы по сути тренировали при тренировали нейронные сети в этом митинге которые шли на вход для дальнейшего обучения и за счет этого получилось качество лучше так это был первый по поводу объема объема данных к объем данных значит у нас было в начале три бренда из них один самый большой был у него аудитория была порядка двухсот тысяч человек в сумме это действительно не так много как бы по если смотреть по отдельности все бренды но за счет того что мы их на них тренировались в совокупности мы получали прирост качества и мы могли уже тренировать неровно сети нам хватало тренировочной выборки до спасибо за доклад я насколько понял вы использовали селянки как в качестве средства сокращение размерности по сути для данных of сравнивали ли вы of thine кадр с какими-то классическими подходами там типа писей писей хороший вопрос дамы в качестве классического encoder там писания или тоже itsm и конечно можно было использовать но это эти признаки нужно было бы попробовать да мы не сравнивали напрямую спасибо за доклад а вы не пробовали пойти дальше то есть связать рассылку и-мэйлов не с переходами собственно с покупка или сколько с какой-то до чем но для замкнуть цепь да то есть условно чтобы подобрать подобрать фермер рассылки так чтобы наиболее вероятный превзошло покупка подобие мы начинали работать в этом направлении и если все сложится удачно я надеюсь мы как раз в этом направлении и пойдем дальше спасибо добрый день скажите о какой стойка технологии вы используете мы используем python keros тензор flow данные хранятся у нас спас гресь и этого хватает для небольших брендов вполне спасибо а можете еще прокомментировать вот такую сторону как вы ко мне not коммуницировали грубо говоря продавали ну заказчику результаты в частности вот эти вот признаки которые строят авт энкодер да это же их человеческим языком по не понятно что это что определил алгоритм причем он там через два месяца будут другие данным может быть другое что-то определит как насколько это ну люди бизнес-аналитики воспринимают насколько сложно там это продается и для или нет может быть на наше счастье после того как мы начинали рассказывать про те технологии те алгоритмы которые мы используем бизнес как то на ум начинал доверять и уже не вдавался в детали то есть конкретно что под капотом не особо интересовало бизнес интересовало чтобы были хорошие результаты а вот хорошие результаты определяются на тестирование и мы организовывали достаточно долгое время тестирование и проверку качества нашей компании и классические компании брендов после того как мы показали статистическую значимость прироста качество уже дальше убедить бизнес не составляло труда спасибо какой самый интересный вопрос был вот вопрос от туда поток вопросов оттуда был самый интересный да да конечно подождите задайте вопрос микрофон нужно для трансляции спасибо за доклад и такой образ получается учитывая что вы обезличивание а вот эти вот явные факторы значит ли это что по сути подобные но нейронную сеть можно использовать для любых товаров для любых рекомендаций абсолютно верно его не и я вам расскажу такую же историю раз у меня есть время еще немного мы попробовали нашу рекомендательную систему с минимум маму модификации применить к задаче рекомендации билетов билеты по направлению каком-то куда люди путешествуют вообще это был очень смелый эксперимент потому что рекомендация билетов совсем не очевидно область где могут сработать рекомендательные системы если у человека бабушка в определенном городе он не будет летать в соседний город потому что этот город ближе или например для него то есть его сложно будет убедить слетать в какой-то другой город поднимает тех направлений в котором привык летать но оказалось что это нейронная сеть сработала нас на уровне то есть там конечно пришлось модифицировать и и там размерных внутренних слоев но некие связи по тем людям которые летают по тематическим направлениям например там исторические туры в европу или туры по архитектуре что посмотреть она стала стала отлавливать и давало результат и примерно на том же уровне что и рекомендательная система которая была специально разработана под этот случай и которая разрабатывалась несколько месяцев спасибо еще один вопросик не совсем к презентация к началу вступительному слову вы говорили про биржу данных которые у вас есть да надо хотя бы пару слов сказать и как партнеров вы привлекаете с работой с этими данными и данные партнеров если у партнера бездарные то есть как вот нужно данные на биржу представлены уже несколькими десятками поставщиков и эти данные отражают эту информацию которая например человек о себе оставил на сайте если он заполнял формы или просто для кого-то интерес человека к определенному классу сайтов например посвященных медикаментам тоже полезен эти данные активно используются в частности автоиндустрии потому что они помогают детектировать тех людей которые имеют интерес к автомобилям эти данные используются для маркетинговых коммуникаций и были эксперименты по использованию этих данных в целях улучшения скоринга банком автово это же эти эксперименты показали прирост качества если вам интересна эта тема пожалуйста обращайтесь пишите нам мы будем рады с вами начать сотрудничество спасибо"
}