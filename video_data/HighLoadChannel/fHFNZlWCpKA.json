{
  "video_id": "fHFNZlWCpKA",
  "channel": "HighLoadChannel",
  "title": "Gobblin как ETL-фреймворк / Иван Ахлестин (Rambler&Co)",
  "views": 1318,
  "duration": 2793,
  "published": "2018-08-16T04:59:44-07:00",
  "text": "будет моя презентация прежде всего я скажу что такое цель для тех кто еще не знает что это такое за понятие что такие за буквы расскажу кто мы такие чем сейчас занимаются рамблер как и почему мы искали свой собственный новые это ли решение почему гоблин оказался действительно крут почему даже если вы можете уже может быть уже используете своем продакшене есть дополнительные кейсы которые вы не знаете и вам стоит узнать и как с ним дальше жить и делать при этом всем вещи значит прежде всего это аббревиатура шифроваться следующим образом это экстрактор формула от большинство задач который выполняет своих сидя на своих задачах это данном случае это ели вы загружаете данные трансформируете wear дайте какой-то точку назначения соответственно я буду говорить о гоблин как mapreduce отель фромборке который позволяет организовывать pipeline и при помощи свой срок своих собственных абстракций и инструментов значит теперь коротко расскажу о rambler многие считают трамблер это поисковик издавать далеких дремучих 90-х до мы ушли с поискового рынка но на самом деле мы остались гигантским видео холдингом газета лента же же чемпионаты и очень много других крутых ресурсов все это мы мы вместе всем занимаемся поддерживаем нам все это нравится значит конкретно чем мы занимаемся в свои модели в своем отделе машинного обучения что у нас происходит за сценой прежде всего машинное зрение совершенно недавно мы сдали проект по классификации аудитории кинозалов кинотеатров поэтому если вы посещаете регулярных кинотеатры у вас скорее всего посчитали и показали свою рекламу это все наша заслуга я камер с рекомендации когда вы заходите на сайте какого-то магазина мы уже для нескольких партнер в том числе к вырабатываем отдельный полицейские рекомендации на основе нашей инфраструктуры и наши наших технологий также мы занимаемся кто такой кто знает что такое понятие rtb сразу поймет а так я вам скажу мы участвуем в real time аукционах по пери торговки рекламы как со стороны хозяев рекламных площадок и так и со стороны предоставляя нашим рекламодателям наилучшие ресурсы и оптимальные для того чтобы не будь ударенным аудиторий рекламы так кроме этого мы делаем единой рекомендации на основе нашего браузерное расширение совершенно недавно вышел проект рамбер старт для которого мы делаем подборке это некоторые такой налога индекс дина вот мы менее конечно популярны но если что пользуйтесь нам пока не нравится активно используем внутри и всем рекомендую значит теперь к технической части что мы наш отдел представляет себя изнутри прежде всего вся наша основная техническая база это free bsd 10 до мэр на единственно кто занимаемся машины рингом и bigdata на классических и миксах и наверное точность них никуда не уйдем нам нравится за дзд false в качестве основы для ходу по у нас основной джентельменский набор с ним связанный калдарский дистрибутив spark 2 hive т.с. естественно кафка для доставки данных плюс все это склеена виде сиси диета дженкинс и рф low как scheduler задач графит мониторинг консул это сервис discovery и хранилище глобальных настроек кластерных и solstic в качестве оркестра таро и глобального накачка наш основной разработка деле за примера следующем пропорциях то есть у нас в основном разработчики математики разработчики они любят использовать питон 2 наше pipeline использую старую питона новый проект и пилится только на третьем кроме это у нас есть большое количество интеграции как страницах ой вам так из аспаркам плюс это для этого для ходу по напишу некоторые кастом решений все это реализуется на небе а также некоторые бренды особо критично нас тоже реализованная так теперь перейдем к нашему основному pipeline у для того чтобы понять насколько критично для на доставка до но когда данных и насколько она потом влияет в конечном итоге результат нашей работы я хотел как раз о нем рассказать именно абстрагируясь о каких-то там мелочи и детали и прежде вот это вот облачко это та множество источников данных которые к нам пишут в то либо это могут быть как наш любимый каталог rambler top100 это могут быть некоторые наши внешние партнеры которых на вот как собственные счетчики так и вижу какие-то данные например картинки как те же самые кинотеатры кинозалов все это агрегируется в нашу ковку мы на самом деле очень долго бились чтобы все наши поставщики единым образом поставляли туда данные они каким-либо другим способами входи fs оля углам и всем прочим слава богу 2 где-то полтора года назад мы это окончательно проблему решили дальнейшем наш капот хдс решение раньше это был камень теперь это гоблин в печи ты данные бочками то есть посуду лером складывает это хэдов с тем образом уже которым готовы потреблять их уже конечные потребители например уже про трансформер из конкретных строчек например tsv логов в джейсон и к ли какие-то более высокоуровневые структуры которые может подхватить hive или spark в нативном виде дальнейшем все эти данные обрабатываются задачами которые сша контроля за через р флотом загружается питания чей загружается pipeline и они используют свой основе хайфы spark они обсчитывают эти данные говорят какие то фичи например на основе логов показав варятся например ваши половозрастные характеристики конкрет связаны с конкретным айтишником ваш сегмент пользовательские для показа рекламы конечность это складываться в pdf с до или или уже напрямую виде конкретных данных для брендов складывается и хранилище то есть когда вы пример скрываете страничку с магазина и к вам загружается страничка блок с обувью это значит то что было до звона наш backend с просьбой для данного конкретного пользуясь кадиш ника выдать предпочтительного рекомендации на основе тех кто то кого то кого то и у него вектора пользователя вот мы это быть довольно быстро даем вот значит к чему я все это рассказываю любой сбой в доставке строк из источников на уроках на уровне кафки так на уровне камень у гоблина автоматически ведет к снижению качества или а неразберихи в данных то есть у вас перестает жениться данные модели перестают вообще отражать реальное состояние вещей поэтому наиболее критичными не терять эти данные каким образом дальше расскажу о нашей кафки примерно нагрузка это ежедневная 6 тобой в день исходящий нагрузка 23 3 байта в день у нас порядка 30 п 30 с плюсом поставщиков который пишет разрешено различных форматах пять нот и нам всего этого мало и она рассказывает парковку дальше не буду просто отошли у вас к весеннему доклада на booking.com мой коллега в чем выборнов очень круто рассказал про кафку ее эксплуатации условиях повышенной нагрузки рекомендую ознакомиться и посмотреть там очень многие грабли и тонкости подводный моменты расписаны с которыми сталкивались а дальше мы просто придем уже как картелю так вот наш старый pipeline ну вы скажете вроде бы как бы работает все круто но я вам скажу что на пса плен моментов все стало не так вот и почему наш каталог rambler top100 на какой то момент времени произошла в него к радикальной модернизации он был переписан буквально с нуля вот на новое на новый стек результате не рассказали переходите с ваши древни кафки 08 на десятку все круто быть были бы радость это сделать но все вещь там что наш старый консилер не умеет работать с кафкой 10 он просто банальный мэр у прежнего разработчиков linkedin прекратил это поддерживать это решение начал переписать но бы естественно и как вы догадались это гол лена кроме этого у нас уже совершенно достал что не было в камин предусмотрено никаких фэг решений то есть не было нормальных защиты от потери данных то есть если единожды данные были прогнаны через pipeline какой-то объем вот и например эти данные были него лина или ваш конверта просто сломался эти данные так эти данные могли быть потеряны навсегда если ихний заранее не почесаться и а куда ты ходил и отложить кроме этого мы работали практически вслепую мы не могли понять иногда до самого конечного разве на например почему он эти данные него ли ну дак уже когда там упал sparkasse и хоровой pipeline у могли понять что вот данные что ты с ними не то какие то дополнительные поля вставили что-то наши поставщики перепутали или еще проще и уже начали решение довольно было плохо мониторе лосьона пришло приходилось часто просто пошутил гулянками вывод камин поскольку нормально интегрирует туда графит графон клиент не было возможности ну как бы зачем заморачиваться вот поэтому мы начали выработали критерии для нового отеля включили того то что мы любили в камеру и и ждали от нового решения однозначно то есть маст хэв прежде всего это играть ли wynn's это вообще отдельная тема что особенно связанная с кафкой как доставить ваше сообщение единожды по назначению при этом не потеряв и не продублировал его в особенности в условии всяких отказов кластера его с его сплит brain об резкого повышения нагрузки о чем это на самом деле тема отдельного доклада вот камеру так и на чего удовлетворял этому решению кроме этого мы обязательно должны были сдавать сов на наши бизнес-логику как минимум мы хотели протестировать наши все приходящие записи вот таймс темпу там стенд мог быть зарыться в приходящий строки я в совершенно неожиданных местах в совершенно разных форматах так unix timestamp этом какое-то strftime строка очень много всех всякой сел веселых вещей плюс возможность реализовать какие-то кафтаны и декодеры то есть вам приходит с востока например на выходе джейсон может быть какой нибудь про табов еще что то все что вам придет голову нужно иметь возможность сделать плюс адекватно покрасить это не только мы понять на каком моменте произошел сбой когда мы начали нас по начали поступать ошибки чтобы резко тушить pipeline и понимать что же что же происходит с ними данный которым приходит кроме этого новое решение однозначно должна была переваривать всю нашу нагрузку которая снижаться точно не собирается а еще я неплохо было бы иметь большой запас для того чтобы у вас в дальнейшем все это ленина отмасштабировать естественно решение должны быть картах и dfs компетенции оно должно быть уже об как на использовано в продакшене и иметь возможность реально fault tolerance решение да то есть поддерживать нормально традиционность то есть запись в конечную точку назначения уже ведь в виде транзакции за один раз максимально ну плюс активное сообщество не хотелось бы как скамью для долгое время патчить и поддерживать решение которое собственно уже умерла и не иметь неважно какой вопрос проконсультироваться или получить бесплатно большое количество фич которым можно конечно в дальнейшем использовать так в результате мы сравнили коробочное решение библиотеки с точки зрения коробочных решений естественно на первом этапе ага блин почти по всем этим пунктам выиграл домой сначала только сомневаюсь насчет пункта real-time а вот как оказалось на самом деле зря потому что есть нет не особо афишируя фича стриминга и они немножко дальше расскажу кроме этого смотрели два папы два таких рядовых решений как к вкусу pdf с connect и этой light да на самом деле два последних оказались винта специфичными штуками которые сделаны для внутреннего потребления вот поэтому квадрата что их выложили большое количество проблем там казалось нереализованные для нас неподходящие вот некоторые спросят почему начали использовать apache + вроде бы как бы он тоже умеет и кафка и в дпс что-то пишет она сама ляпать в лимон как бы надеяться на самом деле некоторыми кого нету сам и кафки вот и как отходи fps это побочный эффект а кроме этого он отстоит full то есть по нормальному сохранить как независимо от ступы мониторить их и смотреть чуть дальше будет ну как бы такая как эта проблема поэтому до женщин они начали в сторону смотреть кроме этого мы подумали ну если так много всяких и к решения может быть мое дело что за свое лучше у нас большое количество приложений уже я написано spark стриминге поэтому хорошо хорошо я снова технология основная затычка заключается в том что грег эванс очень довольно проблематично то есть большими-большими на тягами можно реализовать на spark стриминге вот поэтому ну как учитель засомневались насколько это бестактен вообще использовать большое количество питонов ских клиентов у нас отпала сразу потому что наш топике они просто не переварит предательство там жуткое и кроме этого возникла довольно экзотическое предложение использовать шланг в качестве в библиотеке есть решение такой как брод наносим круто кроме того что это и long вас фактор этого решения у нас наши модели равен единице естественно мы его отклонили поскольку как было яви у нас большое количество разработчиков компетенции уоттс с ерлан вам довольно натянуто так значит эта картинка рассказывает о том как выглядит гоблин когда вы stalker первый раз это большое количество очень красивые детали и очень красивой документации но непонятно какая то компоновать ну как бы в pes первый взгляд эти глаза очень быстро сбегаются может вроде бы мне ты это и это и это и это пугаться этого совершенно не стоят вот нужно просто разобраться значит что говорят сами разработчики прагу блин они говорят что это на самом деле универсал дед инглиш на фреймворк то есть она сама некоторые такой швейцарский армейский нож который умеет из различных источников да это будь то реляционные не реляционной базы данных какие-то кластерной файловой системы все hadoop не верьте на самом деле у меня что может и обратно тоже выгружать и соответствующие коннекторы которые позволяют работать по плану по плану в обратную сторону от все это можете развернуть и сделать выгрузку например в канте уроков к базы данных или мускул вот есть конечно не выдержит и нужно опять же метит все готовить так вот это не диаграмма рассказывает прежде всего о внутреннем устройстве гоблин о том как он устроен на базовом уровне самый нижний уровень рассказывает о уровне деплоя до гоблин прежде всего может работать не только mapreduce решение и токмо придет решение он может работать как рака локальной локальная при кишан должен направляться на к ногами на коленях локальной файловой системе на ваши данные гоблин он вкладывает в душе при этом папочку естественно работает как классическая ходу каприз приложение умеет работать связке с ярнам плюс есть неофициально такой об агадире мастер фича streaming оно какое-то время была сырая сейчас они добавили закрыли еще париже она уже уже вполне используем в частности возможность хранение выступов дальше над ними дед уровень выполнения которых связан по методу лир лаунчер то что запускать более с к вороне абстракции в частности например для локального исполнение этого колледжа ласки и любви для ходу по это квартиры его собственные дальше work юниты как основное на идее с исполнении гоблина и на ней высоко уровне структуры тип источник экстрактора conver сторона грустно все подальше кроме того два основных толпа но который все строят этом это фтор goblin style система позволяет хранить собственное состояние всей всех этапах фича и сохранять их для повторения и повторения транзакции если что-то пошло не так кроме этого все уровня система мониторится есть очень развитая система как event of the queen инструмент интерфейсов для всех ваших основных классов об этом немножко тоже могу потом рассказать в топик так вот это на самом деле если что вам нужно знать о гоблине до поскольку это вроде бы mapreduce решение да она по вто копирует собой все это методологии только добавляет как бы собственной фракции пережить его сурс да это то что работать на стороне вашего локального драйвера и позволяет вам разбить ваша ваши истории источник например это будет будь то вы в сайт вам он разобьёт ваш веб-сайт на какое-то количество мочи страниц которые нужно будет например скачать дали призыв из он вам разобьется это на коне конечное количество блоков для в рамках которого сделать каждому work you need присвоит этому варки нету где кьютер который будет дальше его обрабатывать дальше он пройдет все стадии pipeline а вот по как экстракта конвертер quality checker форк оператор в райтер я бы во всем сейчас этом отдельно детально расскажу и запишет это точку . назначение например кфс кроме одного обычно почему-то все упускаю выпускают на саму тоже критично важно все это стоит торт то есть дата publisher ресурс всегда связаны все это начинается не с того момента с момента последнего печи датах успешного гоблинами обалденную документацию в частности того что если вы откроете его танки хоп репозитории либо 1 резать dogs сайтик там есть конкретно степ бай степ решение например в кипятись вечер кафка таху dfs тома песнях есть пример используя архитектуры нужно проблема в том что все эти решения столько базовые примитивные с коробочками . они подойдут как для простых кейсов типа возьми положи вот поэтому для полного понимания вам все равно придется заглянуть код и посмотреть конкретно реализацию о некоторых вещей для того что сделать свое сейчас об этом я тоже расскажу изнутри гоблинский pipeline выглядит как обычный дрова property-файл то есть вы условно накидываете в сети классы которые будут например ресурс класс в казахскую указывает раковка simple сурс это означает то что основе данных будет происходить из кафки она ответственна за экстракт данных причем этот экстракт будет например байтах дальше параметры самого jobo всех блоков исполнения ночи много всякой со всякой получаете целый портянка который описывает ваша задача давайте остановимся именно на этапе трансформа и что происходит конкретном матери вот что нет гоблин с коробки обычно вам предлагает вот такой вот простенький pipeline да где у вас например есть кафка simple сурс он генерится чисто под потоки сверх байтов все круто а дальше он пропускает вот эти три важных блока такие как конвертер который преобразует данные quality cure for ну как бы все это не нужно в raider записанный на диск причем считай что данный например запишет их разобьет соответствие тому с тем как данные пришли ему на момент запуска самой джо бы то есть как бы ни о чем то есть мы получаем экстракт без всякого трансформа прямая загрузка уже в конечный источник то есть это рав решение которое вам мало пригодится почему все так плохо спросите уровень результате его неужели гоблин вообще ничего не умеет нет конечно все он умеет но вся вещь том что основной разработчик linkedin в основном спалит формата вр а вот это крутой формат стерилизация данных вот который позволяет описывать разветвленной структуры связаны все это нет информации но если у вас есть старый граф pipeline который не умеет с этим работать которые вы не хотите перевязывать ваш бизнес станет на довольно долго да пока вы будете переконвертировать все ваши таблички весь ваш pipeline общем это доставляет этого отказались и решили пойти другим путем реализовывать собственные блоки да вот условностью это выглядит вот так вот так выглядит то решение которые вам предлагают гоблин с коробки вот рядом pipeline ваши мечты и основали чем отличается только в том что нужны костанай блоки реально костанай блоки те которых нету всегда нужно что-то стоп что смотреть что-то доделывать что-то досматривать поэтому из теста мы решили сделать некоторые такой домашних umbreon это на самом деле никакой не for гоблина это некоторые аддоном extension до который позволяет на основе публичного api гоблина реализовать стильный классы конвертеров экстракторов колите checker of что на самом деле на избавляет от большое качество глоба головной боли связаны прежде всего 1 старый камю мы регулярно патч или то есть у нас был форкс долгой истории очень много логики которые уже давно разошлась даже с моим лайнам который тоже какого-то моменту устарел это представляло жуткое количество головной боли а здесь как бы авт вот он отдельно в атомарные решение 11 пакет крутым его в чем смотрим раскатываем плюс мы овладели покрыла мини-тест и мы смотрим за то что наша логика выполняется это мне все хорошо для тех кто хочет реально попробовать всего блины или начать какие-то что от 1 штуки мы выложили специально вот наше решение это как раз вот этот пакетик с набором castano классов если вы захотите стартовать попробуйте я думал вас все получится так давайте теперь становимся конкретно на сущность тогда если понятно я вам рассказал про sur sa задачи это разбить вашу большую задачу на work юниты дальше редко александра македонского принцип разделяй и властвуй закажет окна когда каждый варки нет запускается отдельный task экстрактор достала до столом данные сказки и условно это была какая-то быть строка она прилетела конверт он понял что хайд валидный джейсон и и преобразовал вот и давайте посмотрим как реально реализуется конкретный конвертеры обычно converter это вот как раз та штука которая позволяет вам состыковать множество различных разрозненных форматов между собой для того чтобы записать например в pdf с уже то что будет готова например для использования хайлом или как-то 10 реализовать карл картинки который к вам пришли например бы 64 вы хотите например перевести друг другу бинарный формат соответственно вам нужно ли сдавать вот такой примитивный до русский интерфейс дословно конструктор трансформер схемы то есть не то и нас трансформерами этой информации и трансформер самой строке в чем трансферной строки возвращает итеративно это на немножко нужно обратить внимание это подбор не и побочные эффекты всякие сейчас вам объясню как это выглядит на практике вот на практике это выглядит вот так да у вас есть некоторый сурс до который генерит вам данные эти данные не какой-то определенный тип это может быть кружка текстом и сырые байт и это может быть в некоторых x тракторах авра это может быть джейсон это все что угодно джейсон деревья плюсом эта инфа который описывает структуру этих данных для дальнейшего преобразования transform и дальше выстраивается целая цепочка из n конвертеров на входе каждого конверта ради 1 в прострочка подразумевается я от 0 до n виде возвращаемого оператора выходных значений естественно весь мой план должен такого втыкалась как по типам метаданных пока под символом самих данных и так давайте который собственно запишет эти данные как бы бренд всё круто и сразу к вам приходит мысль блин а может быть я режу какую дополнительную руку в конвертере буду сразу обращаться какой-нибудь внешней базе правда что-нибудь joy нить какой-нибудь индекс справочник ли считать сразу каунт и нет на самом деле гоблина трансформеры поддразнивая что они вообще стоит лес они должны быть функционале чистой чистыми и расскажу сейчас почему вся бишь в том что вот когда мы столкнулись мы начали это принялся подразумевается в документации вы не знаете каким образом будет запущен ваш код ваш converter обычно он является multithreading соответственно каким-образом муляжи в кластере каким образом scheduler его запустит вы не знаете если у вас какие то есть разделяемые ресурсы вы не знаете каким образом они будут использованы как может придти привести к большим проблемам даже если вы все это учли мы столкнулись таких такой штукой да например когда я бью ли контейнер составите к декларации да так что вот этот внутренний бессоновский объект который я сразу за ним оказался изменяем эмулируемым в результате есть наш pipeline оказался испорченным наши конверты начали паспорте данные делать дикий shuffle а за этим нужно следить и не допускать такого то есть любой ваш transform должен быть как бы функцио начисто то что на входе на выходе ну естественно вы можете какие-то использовать какие-то справочнике данных но если их объявите например в самой жопе на этапе инициализации теперь расскажу в следующем блоке такая штука как валите checker довольно прикольно да вот вы круто трансформеры данные они у вас прошли я и так трансформа и теперь вы хотите на них посмотреть пристально сможете ли вы их дальше использовать условно к вам пришли какие-то уже готовы драконовские деревья раньше вам лили кассандре в одном формате теперь в другом о поставках и ваш первый раз сломался pipeline вы поняли что так дело дальше не пойдет говорите разработчика пишем не то что должно мне приходить в digital схемы или например расскажи какое количество полей фиксер она не должна прийти делаете некоторую политику которая говорит да или нет этой это этой скань как к каждой конкретной строки на выходе вот и дальше есть несколько вариантов решения того что делать этой строкой например который не пошла validation прежде всего она может быть как дальше пропущен она просто мониторинг будет описано что строка не прошла валидацию да и соответствующе метрика будет а в виде ошибок эта строка может быть просто оправдана дивно либо от redirection а например в отдельных офисную папку значит это дает на самом деле большое количество возможностей с восстановлением всех этих записей и по ней пониманием того что происходит вот в результате агрегировать html на говорил то что у нас получилось у нас получился в качестве экземпляра такой вот джейсон pipeline да когда она у нас был кафка simple сурс он байт и дальше мы эти байты период переводим тагирова на стоим с темпом запись где вырос есть уже готовы джейсон структуры внутри запись записано пилотов виде лонга мы проверяем наш адрес он на соответствия схеме если круто пропускаемого дальше удача записываем его ходе в зависимости от того там темпа который был записан запись он будет там записано положим в нужную папку поцелованный зарегистрирован поверх айви вот теперь расскажу о файл бабки сида допустим у вас большое количество данных не прошло валидацию оказались чем-то не тем на этапе так вот на этапе конвертера да они например не смогли атр оформиться или тек валили quality & чекеры хват отправил в трэш или например в какую папку одни на совершенно случайно забыли удалить данные в кафку или какой-то внешней поставщик был рассеян и положили вам данных и dfs вам эти данные нужно заново запустить ваш pipeline уже сейчас естественно после полного если расследования инцидента да чтоб конкретно прошу то ли это была проблема конвертора то ли это была проблема данных вы натравливать слова на вашу логику гоблинам но уже немножко другом виде вот и также публикуйте в рамках снова попали как это работает мы просто меняем наш input на ходу qte в текст он тут пишем ему адаптер потому что текста это ходу пуске тиф и дальше в рамках того же там у старого pipeline а мы можем наши данные довести до финала круто это позволяет нам избавиться от использует одну и ту же самую логику ту же самую логику и при этом будь уверена что все наши данные теперь уже наконец-то доехали до потребителя вот что это дает кроме того какой дополнительный эффект вот наша старая платформу да если посмотреть кафка терех и dfs дано направленный pipeline вот а гоблин позволяет делать в следующую штуку поскольку у них большое количество коннекторов 1 мы уже сразу использовать ходов-выходов разрешения мы можем сделать реплей для того чтобы они какие-то части наших данных записать в чужую кафку либо свою собственную кафку на какой какой-то другой топик кроме этого гоблин позволяет в режиме стриминга либо патчами записать эти же данные уже но уже подготовлен его любимым ваши базы данных под будто под газ будь то редис кроме этого мид крутую на темноты грацию с кайлом то есть вам доступно все типы данных ну например такая штука как паркет или уорк для использования напрямую из гоблина но только через адаптер хайфу круто как итоге в каких случаях стоит смотреть в сторону гоблина прежде всего однозначно это кафка тоха dfs да если вы занимаете доставкой данных и дальнейших архиваций у вас hadoop но вокруг какая-то своя уникальная система своей уникальной системы то есть вас много карстовых решений которых которые непонятно как поступать ступица но данный нужно доставлять hadoop и дальних для дальнейшей обработки если сборы доставкой данных стали ваших главных болью ходу гоблин позволит решить покрытие там метрики метриками мониторингом и при этом всем нужно что-то особенное так дам вам ссылки на документацию гоблин недавно совершенно переехал в об очистке инкубатор соответственно у него немножко инфраструктура поменялось вообще за последние полгода появилось много этих вещей с которыми мы намучились а не повис например джейсон сурс ну и плюс ссылка на сообщество вам там всегда ответит там хорошие вот ваш вопрос и да умеет имеет ясно и еще вопрос и он то есть получается очень выгоден если у нас много поставщиков данных и они все они могут гарантировать какую цену ассистент когда что каждый раз вас будет что-то да например с орками паркетом почему бы не стала рекомендовать не рекомендовал бы это делать поскольку это формат и уже который подразумевает полностью спас бас упакованы бочче большие они уже задач значит заточен на большие данные а гоблин например если у вас был он уже по ним принимать данные сети частями до вас получится просто мешалкой большое количество маленьких паркет файликов это не имеет смысла вот желательно как бы сначала получить например fitch данных при за сутки до потом на него натравить гоблин вы получите он вам фетт агрегирует положит например паркет до или сделать например самый через hive но как было попроще все таки а в чем была проблема чтобы не используем газ парк для конвертации сказки основная проблема в том что пример spark streaming да у него прогул правде включается то что он хранится снова о ступы по умолчанию на стране кафки в нашей кафку регулярно штормит за высокой нагрузки из-за того что поставщики в тонны в виде пишет данные до плюс наши админы когда бывает бывает аварии в случае если у вас ваши ваши уставу пропали они пропадают вот вас нет варианта обеспечит ли когти вас у вас получится либо потеря данных до либо дублирование с тренингом это большая проблема то есть нет есть конечно всякие обходные решения но это не для продакшена ясно спасибо добрый день по себе за доклад у меня такой вопрос если у нас появляется новый очереди в каске в которую например этот час он и к нам приходит сколько времени разработки нужно для того чтобы поставить новый pipeline который будет выгружать от кафки да там уже позиционирование красивый формате 15 минут обычно если у вас есть уже готова формат вы быстро накидывать этот конвертер реализует интерфейс сразу по накидываете тесто для него вас уже ваше обычное приходится какие-то задокументированные данные дав приходит таком-то формате я хочу туда-то туда-то туда-то либо у вас уже есть какой-то готовый конвертер обычно елены формат ивой повторно используется вы инкрементировать эту схему до вас приходит например быть строка вы знаете что там находится та та та та та та я и например преобразовать джейсон авра или еще что то в вот проверяете пишите и не тесты на основе ваших примеров прогоняйте себя и сиди и круто она проходит тест и выкатили на кластер все она работает плюс прописали естественно ваш топик для фича некий второй вопрос как используется streaming тусить если мы кладем данные на выходе fs.to наверное это все таки кладется какими большими блоками до на самом деле string найдется периодически большим какими-то блоками да получается что угодно президента запускается на кластере по мере поступления данных и обработки кого-то качество батю никто . он по вступлении на отсечке регистрирует данные уже переносит их рабочей области не как стать джова область до в которые он пишет те данные которые к нему приходят по мере работы вот в чем тогда преимущество между использованием гоблину в таком режиме и например запуском спарка с какими-то микро бочками с должны большим микро знаете на самом деле streaming это обычная по большей части в обычно фича для гоблина это не его как бы изначально специализация но как бы просто прикольная фича которые он спасибо вопрос смотрите какой у вас на схеме есть то что вы пишите в несколько баз данных соответственно распределенная транзакции да естественно насчет насчет баз данных а вот здесь у да вот чуть более подробно что произойдет если и близости и нет из естественно когда вы пишете дан данные в базу данных вы либо и должны контролировать количество worker of которые работаю вас на кластере постоянно чтобы банально его не задавить либо использовать гоблин уже в символ not режиме до какого-то демона и он уже последовательно выполняя данные переносит эти данные другом режиме с другой нагрузкой вот то есть когда вы запустите запустите матчевую обработку например в полной мощностью до естественно у вас быть обычно кластеры виде содержат вот в то же самое с поясом это просто регистрирует по большей части то что ваш имеющейся логика до которые трансформеров может быть повторно использованы без какого-либо унификации вы просто как вы набираете такой конструктор вот и его используете уже просто другому немножко киеси например как уже пост пост об гори гори гори горячий доставки данных например из кафки спасибо большое за доклад у меня такой вопрос имеется 1 шт интеграции с таким базам данных и house например и опыт над не откликался у нас интеграции не было и как бы у нас проблемы нету потому что как бы данных попадает нас из кафки напрямую клика усиленно занимается наш одиним пир у отца и вообще на самом деле у ходу привлекался несколько разных задач и поэтому нет мы не занимались не женились спасена спасибо за детали интересно а может ещё поделиться если такие случаи были где вот это вот ну как инструмент гоблин не применим в каких либо задачах либо вы прям полностью взяли и все на нем не рвутся естественно ком на самом деле часто нишевых инструмент да он предназначен чисто для отеля он говорил означает то что то что он позволит вам в грабит данные из конкретного источника распределенную только от чистую х трансформер сделать какие-то нетривиальное преобразование да и не говорю там какой-то matching воины еще все прочее что вы хотите почитать каунт обычно что хотят от streaming приложений когда обрабатывает потоки нет это реально она в . на выходе 0 n1 в . на входе на 1 он на выходе ничего большего от него ждать не стоит то есть он превосходен именно вот в конкретный случай да например как отдыха dfs он вообще бог в другие решения но надо смотреть потому что используйте то сумму и у вас ваш опять же к системам дальше какой-то развитие по нему будет или и вот вышел задачу решили и все то еще на этом месте такого самолета но я думаю да скорее всего также на этом месте замрет хотя опять же у нас часто возникают задачи пример поэтому прилива данных между кластерами гоблина есть встроенная disciple та штука на самом деле которая в ходу объявлена де прикиды по моему потихоньку отмирает мы можем реализовать при помощи гоблина вот плюс говорю поиграться подкинуть кому-то данные парочку по данных таки всякий файл бакс случае до либо workaround на тему такой мультитулы хорошо иметь под рукой как просто как инструментарий того же самого администратора дали divorce а можно добрый день спасибо большое за доклад вы упоминали гоблин как фреймворк который позволяет за грабить из разных источников до при этом он у вас грабят только из кафки нет на самом деле может грабить и сходив в саму сразу боевом вывоз тбо случае сразу использовал как из ходовых pdf с а кроме этого говоря если вы посмотрите документацию нас небо просто таких типов когда нам нужно грабить web-сайты да но открываете там стандартный gm пол да пожалуйста графику grub свои кипеть и он вам будет лечить статьи вкладывать я понимаю вопрос несколько в другом вторая часть поставку данных непосредственно в кафку он у вас не решает поставку данных какую нет нас нет такой тему то есть мы как бы являемся конечно конечно обычно поставщиком либо вам опять же из нашей же кафки все это выбирается по как бы другими хорошо спасибо спасибо за интересный доклад два вопроса про экзо клеманс зачем она нужна и как оно реализуется игры cliff'a once ну смотрите quick эванс это прежде всего гарантия того что идеи сообщение которое приходит из кафки точно в единственном экземпляре попадёт к вам хдс и в конечно это обычку который будет обработан хайло ни больше ни меньше чем один раз как это организуется гоблин поддерживает так называемой транзакции то есть по мере джо бы он ждет обработки все съедобные у него есть стоит хранилище результаты каждого worker а сейчас результаты каждого worker пишется в отдельную как каждого work with a пишется в отдельную папку по результатам выполнения всех his job драйвер принимать решение анга мы завершились и быстро переносит эти данные заду транзакцию в конечную точку назначения то есть это почти y le mans вот нет и дома и взять эванс не гарантирует нас она нас на самом деле мы начинали еще с 8 кафка и плюс там нужно тоже рассматривать ее функционал то есть и включая и на клиентском и на серверном уровне мы пока то глубоко не заморачивались вот нет мы пока еще не рассматривали случай второй вопрос про говоришь нам важен real time если я все правильно понял то гоблин читает и сказки каким-то давай это богачами сколько времени проходит от того как запись попала в тот и кафки до момент когда она попала в ну на же стандартный бы сейчас получасовые соответственно вот такой пучок полчаса 4 час до фан-арт стандартный г вот вот если необходимо то есть возможно сделать были часты печи да но так что например стримингом писать нас возможность ну не было пока необходимости спаси большое спасибо за доклад хотел бы уточнить и вот вы сказали то что гоблин он исключительно простой то есть одна строка на в ходе и 0 н на выходе при этом какие у вас source и какие источники то есть могут ли они быть у вас большие например сжатые какие-то данные где нужно там снять декодирование какой-нибудь разжать их как какой объем то есть может прийти к вам несколько мегабайт запросто у нас некоторые тофик и топике в том числе частности для кинотеатров на улицу картинки соответственно иногда услышан ахаххаха hd разрешения десятки мегабайт на строчку переваривают нормально с этим проблем нету выйти source и режете как-то или он идет единым массивом осмысленно там обычно для обработки смотря уже на уровне кафки они уже порезан и да то есть там есть несколько партиций в каждой партиции там есть собственные диапазоны в рамках которых через поиск происходит выборка каждая строчка да это терре картинка если нужно как-то рисовать картинку нет это естественно concorde каждая картинка на то можно неделимое transform они проходит проходит один раз если нужно transform обычно там все-таки raw вот мы эту картинку просто условно складываем в другое место да и подписываю вот то есть большие так баш ваши большие данные на строчку там типа порядка сетки мегабайт от не проблема спасибо большой брат эту прямую спасибо за доклад"
}