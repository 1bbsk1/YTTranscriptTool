{
  "video_id": "nrm7_LSRDSI",
  "channel": "HighLoadChannel",
  "title": "Потоковые алгоритмы в задачах обработки больших данных / Виктор Евстратов (Segmento)",
  "views": 706,
  "duration": 1355,
  "published": "2017-04-07T15:17:09-07:00",
  "text": "да меня зовут виктор я to the seine тест компании сегмента где мы занимаемся таргетированной рекламы в интернете возможно вы видели что-то такое может быть что то из этого показывали мы вот сначала я немного расскажу об одном из способов котором реклама попадает на ваши мониторы этот способ называется real time getting то есть по сути это аукцион на котором каждый показ рекламе в интернете разыгрывается и компании нашего профиля делают ставки исходя из каких-то вот данных которые у нас есть о вас понятно что рекламу интернете много и на каждой странице может быть по несколько баннеров и вот каждый заход кого-нибудь на такую страницу вызывает создание нескольких аукционов таким образом понятно что это что-то очень нагруженный вот мы например обрабатываем более двух с половиной миллиардов запросов в день и остальные числа у нас тоже какие-то большие делаем мы это с каким-то железом неважно как собственно часто ставятся какие могут задачу ставить наши заказчики они могут хотеть просто чтобы люди смотрели их реклама могут хотеть чтобы люди посещали их сайты могут хотеть чтобы они как-то хорошо посещали их сайт и много там ходили смотрели страницы или может быть даже что-нибудь покупали понятно что самый тривиальный способ участия war тебе мог бы быть это взять просто все деньги заказчика разделить их на количество кликов которые он хочет и просто с такой фиксированной ставкой пытаться всегда победить и выполнить все цели но кажется это не должно работать и скорее всего так вы просто останетесь без клиентов поэтому хочется как-то по-умному все-таки воспользоваться например какой то информацию о пользователях например вот есть базовая информация это операционная система браузера все что приходит с заявкой на запрос запросом на участие в аукционе ну так же мы можем что-то сами научиться находить в каких-то открытых источниках считать что-то на основании поведения перемещение пользователей в интернете или где-то купить если это где-то можно идти еще я буду рассказывать о том как во всем этом могут помочь потоковые алгоритмы собственно это алгоритмы которые позволяют нам условиях наших больших данных что-то все-таки посчитать при том что память которую съесть сильно меньше вот я предлагаю использовать их в таких кейсах как например качеств оценка качества прогнозов которые дает нашей системой или онлайн оценка каких то характеристик трафика собственно хотим мы это делать все прямо сейчас и в этом потоковый алгоритмы нам очень помогут потому что они как раз позволяют делать онлайн я буду рассказывать об алгоритмах оценки медианы и распределения характеристик трафика на самом деле если у вас есть какие то другие любимые алгоритмы и приложения возможно вам повезет и вы сможете найти их какие-то потоковые версии решить свои любимые задачи на больших данных этого человека уже много раз наверное вспоминали здесь это эндрю яндон автор одного из самых успешных курсов по машинному обучению и мне в его курсе запомнилась то что он всегда очень подробно и хорошо объясняет математику и все ее понимают возможно поэтому среди моих знакомых дата-сайентистов так много людей с психологическим образованием поэтому я возможно тоже буду некоторые вещи объяснить слишком подробно но надеюсь понятно например распределение что такое распределение это такой математический инструмент который позволяет нам описать некоторую случайность например рост человека который зайдет в это помещение следующем вот тут вот представлено распределение ростов мужчин и женщин но возможно эти картинки могут быть понятны и могут быть не очень понятными могут быть красивыми но возможно хочется иметь что-то более конкретное например каким-то образом оценить наиболее вероятный исход события в этом нам может помочь средние собственно можно догадаться что это как вы считаясь средний рост вроде понятно но на самом деле это и не единственная вещь которая нам может помочь в этом например есть медиана вроде бы никакой разницы по определению медиана это вот если вы мужчина ростом 170 три сантиметра примерно то значит то что примерно половина мужчин ниже вас примерно половина мужчин больше вас выше соответственно здесь на картинке не видно никакой разницы и зачем это нужно считать и явно сложнее но ну вы поняли да нет не поняли это старый фильм монти пайтон он про карликов путешествующих во времени это может все испортить вот и диана и вот средние со средним произошло что-то плохое просто отвратительно так что понятно что средние в некоторых ситуациях ведет себя плохо и лучше бы нам считать медиану она как-то больше похоже на то что мы ожидали увидеть собственно где мы все это применим я предлагаю следующий кейс знаете был такой известный случай в волмарте это крупная сеть магазинов америке проводили исследования пользуюсь корзины покупателей и обнаружили всякие интересные закономерности одну и самые известные из них стало то что часто если человек покупает подгузники то он покупает и пиво и поэтому теперь пиво которые нужно продать стоят где-то рядом мы попробуем узнать что-нибудь подобное мы попробуем увидеть то что у ваших пользователей есть какие-то интересы и у пользователи которые везут тебя хорошо интересы выглядят как то по другому то есть вот допустим мы умеем их определять мы умеем можно формализовать эту задачу как составить портрет целевой аудитории какой-то рекламной кампанией как это примерно мы как это примерно должно выглядеть вот приходят какие-то люди которым нам нужно показать рекламу и мы с какой-то наши базы данных в которую мы заранее положили их интересы каким-то способом подсчитанные достаем их после чего хотелось бы сделать следующее взять интересы вообще всего потока который идет с аукционов и каким-то образом сравнить с интересами тех людей которые в итоге совершили что-то хорошее например что то купили на сайте с товарами для детей ну немного подробнее в двух словах про интересы будем называть интересом пару из тематики и уровню вовлеченности человека в эту тематику ну и вот есть какие-то разные люди да и видно спортивные автомобили интересно неинтересные и так далее но понятно что в нашей системе мы храним это в каком-то более абстрагирован ой форме более удобные виде каких-то чисел будем считать то что 0 это полное отсутствие интереса единицы это очень высокий интерес и все что между это какие-то переходные стадии вот я рассказывал про распределение и тут кажется логичным попытаться его оценить потому что приходят какие-то люди они несут скобы какие набор чисел олицетворяющие их интересы и хочется наверное посмотреть на распределение этих чисел ожидаем увидеть что-нибудь вот в таком духе например интерес бары и рестораны во входном трафике которые идет с аукциона может выглядеть и примерно так и выглядит что означает что в целом мы видим людям люди пытают умеренной интерес к этой теме и вот есть некоторые не очень большое количество людей которые интерес к этой теме высокий интерес и наоборот в общем есть например вот три такие тематики видно что в всем в сыром трафике распределение тематик похоже это оно какое то такое что люди питают умеренные интерес ко всему но давайте теперь посмотрим на срез аудитории которая ведет себя хорошо что-то покупает или много ходит на сайте или еще что нибудь в этом духе здесь может быть не понятно и непривычно смотреть на такие картинки над серым отмечен на то как он выглядел раньше цветным отмечен то как на выглядит теперь можно заметить то что появляются определенные ожидаемые перекосы например вот товаров для детей раз это был сайт рекламирую на котором продаются товары для детей но также можно увидеть неожиданные вещи похожие на тот случае с пивом подгузниками вот то что барам и ресторанам у людей наблюдается перекос интереса в сторону высокого интереса и вот по каким то причинам интерес к спортивным автомобилем упал кто его знает почему это произошло но в любом случае это какая-то информация которой мы теперь можем пользоваться что таким образом можно получить но вот например такой визуальный tool для оценки и сравнения трафика с различных платформ или там интересов в целевой аудитории но например также эту систему можно использовать как ядро для какой-то решающий система которая будет автоматически подключать отключается интересы по каким-то сегментом и вообще много чего можно придумать это могут быть не интересы это может быть создадим или что-то другое что умеете узнавать про своих пользователей собственно теперь я немного расскажу про то какие алгоритмы здесь можно применить для того чтобы решить эти задачи для начала я расскажу о том что все знают как посчитать медиану с помощью сортировки и все знают что такое сортировка все ее как это умеют делать это определенно плюс и работает она точно то есть предлагаются просто отсортировать их 1 последовательность чисел и взять центральные элементы но как мы знаем мир не идеален и мы не можем просто взять и все отсортировать потому что нам просто не хватит на это памяти поэтому приходит на помощь алгоритмы и желательно алгоритма с двойными фамилиями с одной фамилий может не хватить например для начала я расскажу про алгоритм который не помогает нам решить тот кейс который описывал он сразу может быть полезен потому что и мы нашли для него определенное применение потому что этот алгоритм позволяет сутки за конечную память среди очень большого количества чисел найти их медиану это алгоритм монро патерсона вот эти замечательные люди для нашего сделали у него плюс в том что он дает точное решение дело это за конечную память но минус у него в том что при он требует размеров выборки и в зависимости от знания размерах выборки и в зависимости от размеров в этой выборке он требует совершить иногда более одного прохода по данным что мы в общем-то не хотели бы часто делать например если мы хотим его узнавать что-то онлайн то нам это точно уже не подходит потому что мы не знаем ни размера выборки и не можем заново по ним пройтись вот собственно но тем ни менее из двух алгоритмов которые рассказываю этот можно хоть как то писать на пальцах поэтому я попытаюсь это сделать для его самого простого случая с одним проходом пусть у нас есть вот такая последовательность и 9 чисел и нам доступно 5 ячеек памяти явно не влезает уже еще и две из них мы займем под какие-то счетчики они нам понадобятся теперь точно не вылезает ну сначала будем делать что-то очевидное предлагается что у нас есть два счетчика и оставшиеся в оставшихся ячейках памяти мы будем поддерживать какой-то порядок поэтому просто пока что идем по последовательности и записываем число вот у нас было 13 ей двойку мы запихнули между единицы и тройкой чтобы сохранить порядок ну вот теперь у нас уже кончились ячейки и непонятно что делать но алгоритм манова патерсона предлагает следующим действием посмотреть что если еще чуть и уравновешен это вытеснить какое-то из чисел не важно какое то есть мы находим то что двойку мы хотели бы запихнуть между единицей и старой двойкой и понимаем то что единица тут больше не место и вытесняя моё счетчик клементе моего то же самое происходит вот в этом случае вот и суть в том что мы постоянно пытаемся поддерживать баланс между счетчиками в зависимости от того где больше где меньше в ту сторону и вытесняем данные вот каким то таким образом мы можем вот прийти к такой картине что все числа на свете закончились у нас все еще к все урона и силосе середине находится какая-то отсортировано я последовательность после чего мы можем взять и поверить то что только это действительно медиана но к сожалению мир не идеален и собственно есть нехорошие случай когда медиана сразу же может попасть в один из счетчиков и очень глубоко туда попасть и мы тогда я уже никогда не найдем вот как раз чтобы такие случаи нивелировать на самом деле алгоритм монро патерсона и сложный и работает не за один проход как я вам показал но если вы хотите узнать как то там в конце будут ссылки на статьи можете их почитать вот собственно возможно я слишком подробно все рассказываю и все равно ничего не понятно и заверни эндрю теперь я расскажу про второй алгоритм тут тоже двойная фамилия возможно он нам поможет и да он гораздо лучше очевидно я не буду рассказывать его уже совсем никак расскажу только общую суть принципе оно не изменилось у нас есть конечная памяти у нас есть некоторое количество ей детей которые мы используем под счетчики также мы на все оставшиеся памяти созданием какую-то сложную структуру данных которые будет более умные чем просто отсортированный массив будет хранить какие-то данные о том сколько у нас уже пришло отсчетов с таким-то значением мы что-то еще в этом духе и полному их отфильтровывать чтобы поддерживать заданную точность до алгоритм к на гринвальда не гарантирует нам что мы найдем точный ответ но нам обычно и не надо мы хотим иметь ответы с какой-то допустимой погрешностью собственно он нам позволяет это сделать и зато он нам это в отличие от предыдущего алгоритма позволяет сделать за один проход ну и это замечательно этом этим алгоритмом мы воспользуемся собственно также его плюс том что он оценивает не только медиана сразу же и еще какие-нибудь порядковые статистике какие вы захотите и таким образом он позволяет оценить распределение собственно вот ссылки на статьи наш отдел аналитики очень любят питон hadoop но вот какое то время назад надоело делать отчеты которые по часу считаются у нас появился spark собственно мы начали экспериментировать со всякими потоковыми алгоритмами придумывать как можем их применить вот это у нас работает на статьи забей за кафки sparco вот так вот спасибо получилось быстро могу ответить на вопросы александр компания будут спаси большой это за доклад очень интересно а вы не могли бы немножко раскрыть тему вот у вас происходит кто-то веб запрос на вашу систему на тоже получить баннер да и вот собственно дальше что происходит вот здесь участвует бы давкой bass spark и прочим то есть использовали в эту под систему для того чтобы выдать принять решение о том чтобы выдать тот или иной баннер конкретные сейчас вот наши эксперименты привели к тому что у нас есть что-то похожее на этот тур с помощью который у нас смотрят какие-то люди и придумывают нужно ли включить какой-то сегмент она еще не совсем так шитов систему и автоматизирует что-то но пользу уже приносит вот вопрос задам топосе большой за доклад в какой-то конкретной все-таки задачи нужны вот эти механизмы связанные с оценками распределение ведь насколько я понимаю ключевые задачи которые вам нужно решать это выбор правильной аудитории которые похоже там может быть на кого-то то есть там больше должны быть удачи там классификации решаться или чего то еще мрамора вот как раз я предложил такой кейс который позволяет чисто визуально оценить то что целевой аудитории присутствуют какие-то интересы вот и таким образом найти сегменты это такой способ найти и первоначальный жесткий таргетинг внутри которого уже будет происходить как это более умная классификация там заказчики начали приходится говорит нужны конкретные какие-то интересы ли вы собственных отбирать или способом ну скорее он нам говорит какие интересы нужны на них мы сразу таргетируемся потому что интересы у нас есть но также мы ему потом заказчику можем сказать то что мы обнаружили что вот у этих людей если у них есть еще дополнительные вот такие интересы у них выше шансы конверсии и давайте мы на них тоже таргетироваться будем так спасибо за доклад у меня интересует немножко вопрос в сторону от основной тематики у вас большой поток данных и меня интересует с помощью каких инструментов вы сохраняете сырые данные и с помощью каких вы производите аналитику это на самом деле лучше выпуск просить наших программистов потому что чукча дата-сайентистов но я не хочу соврать но у нас есть кафка например в нее отправляется что-то из нее уже пишутся в gdfs примерно так а может ещё рассказать здесь про некий цикл собственно внедрение моделей как у вас сделано от разработки собственных модели до внедрения тоже есть какая-то идея это саммите ес ну то есть вы придумываете как она потом уходит production ну у нас появляется какая-то гипотезу например о том что хотелось бы научиться считать интересы пользователей или определяется с дом и мы пробуем какие-то статьи найти то как это кто-то делал что-то инкрементируем там составляем ся этот ассеты выкачиваем и захотев с идем вайпа это ноутбук что-то там делаем если получается построить какие-то интересы иесус дым например которые подобно хорошо тянут как фичи в наших предсказаниях то мы уже пытаемся рассказать программистом как их внедрить вы просто передаете ему грубо говоря ну не какую-то методику то есть документ как именно это нужно их реализовывать ну дай когда уже проверим всякие метрики все остальные хорошо а как вы потом контролируется что они реализовали именно то что у вас на стенде получалось но на самом деле это хороший большой открытый вопрос но мы пытаемся обычно например мы их просим мы составляем какой-то общий datasette на нем делаем прогнозы или что-то еще и или там рассчитываем и интересы и потом мы это делаем независимо друг от друга и сравниваем ся но ведь они работают с потоковыми данными с новыми данными которые вы раньше не видели как вы сравниваете так я говорю ну то есть совсем по живому у нас не получается у нас еще нет bkt синга но мы об этом мечтаем но в данном случае мы именно выкачиваем какую-то фиксированную выборку и работали они вместе одно и то же выкачивается и проверять спасибо спасибо за доклад меня зовут михаил у меня вопрос по второму по минутам алгоритму было сказать что можем оценить статистику за один проход заданной точностью а где здесь трейдов чем за это платим памятью или я могу сказать что хоть с точностью до 10 минус 16 и за 1 порно удар на это платятся памяти то есть у вас есть какая-то память которая вам доступна и в зависимости от нее можете получить различную точность до но лучше почитать статью если какие то оценки на количество памяти завести точностью до есть удалил со слайдов а нетленные там логарифмы я подумал кто захочет на это смотреть на взгляните на ганновер кароч на цифры какой-то жест получалось там реку объем памяти нотки потоках есть ну точно не скажу все просто хорошо работает и это получило спасибо вам большое за доклад"
}