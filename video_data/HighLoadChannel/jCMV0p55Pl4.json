{
  "video_id": "jCMV0p55Pl4",
  "channel": "HighLoadChannel",
  "title": "Расчет факторов в Антифроде Яндекса / Андрей Попов (Яндекс)",
  "views": 1946,
  "duration": 2727,
  "published": "2020-04-14T11:11:35-07:00",
  "text": "ну привет меня зовут андрей я работаю яндексе руковожу разработка анти фродо индекса и тем моего доклада расчет факторов антифоны индекса быстро удобно выразительного мне тут сказали что люди предпочитают использовать слово фичи поэтому везде в докладе я буду упоминать его ну вот название стал старая ну вообще что такое анти фрод но проще всего показать вот эта система которая защищает пользователей от какого-то негативного влияния на сервис по негативным влиянием ними виду какие-то целенаправленные действия которые могут ухудшить качество сервиса и соответственно ухудшит пользовательский опыт взаимодействия с ними это мог быть как достаточно простые тупые парсеры и роботы которые ухудшают наши статистике либо это может быть какая-то цель направленность сложная мошенническая деятельность но второе понятно определять сложнее и интереснее чем вообще более цифру одну пару примеров это например какая то имитация действий пользователя этим заниматься такие ребята которые мы называем черное seal это те кто не хотят улучшать качество своего сайта улучшать на нем контент вместо этого они пишут роботов которые ходят на на поиски яндекса кликают по их сайту и надеяться что из-за этого их сайт поднимется наверх куда-то но я напоминаю что на всякий случай напоминаю что такие действия противоречат пользовательскому соглашению и могут привести к серьёзным санкциям со стороны яндекса ну или вот например накрутка отзывов о такой отзыв можно увидеть снова пластикового на организацию который составит пластиковые окна на картах ну за этот отзыв она же самое заплатила окей но если смотреть на верхнем уровне то архитектуру этих рода примерно выглядит вот так то есть это некоторый набор 40 событий это какой-то anti-flag система анти фрод черный ящик в данном случае желтый и на выход из него генерируются какие-то размеченные события яндексе достаточно много разных сервисов все они так или иначе ну карамель большие из них так или иначе сталкиваются с разными видами фродо например некоторые из них это поиск марки дзен элькар но кроме этого буквально десятки других где мы были в команде в компании еще года 2-3 назад ну то есть у нас был каждая команда выживала под натиском фродо как могла на генерировала какие-то свои свои команды эти фродо своей системы которые не всегда работали очень хорошо были не очень удобны для взаимодействия с аналитик но для взаимодействие аналитиками и главное они были очень слабо про интегрированы между собой я попробую сказать как мы это решили созданием единой платформы так зачем же нам нужно дать единая платформа ну перри использование опыта и данных централизация опыта и данных в одном месте позволяет быстрее и более качественно реагировать на крупные таки крупная такие как правило бывают кросс сервисными так или иначе 1 какой-то единый инструментарий люди люди имеют привыкли инструменты которыми они привыкли они работают быстрее и понятно что скорость подключения если новой сервис мы запустили какой-то новый сервис который сейчас на которой происходит активная атака мы должны достаточно быстро подключить к нему качественный анти фрод также вот сказать что ну мы мы в этом плане не уникальны все крупные компании так или иначе сталкивается с подобными проблемами и все те с которыми мы коммуницируем как-то взаимодействуем так или иначе приходит к созданию какой-то своей единой платформы расскажу немного про классификацию того как мы по крайней классифицировать фродо это может быть какая-то оффлайн системы которые считаются часы и дни эти же считают какие-то тяжелый оффлайн процессы например сложной кластеризации или сложные переобучения какого-то хитрого машину обучения это да пра пра про эту часть ее практически не не не буду касаться в этом докладе есть какая-то не риэлтор часть которой работать за единица минут это некая золотая середина меня достаточно быстро реакция достаточно тяжеловесные методы в первую очередь я здесь остановлюсь в докладе на ней но также важно сказать что мы используем данные с этапа на уровне выше на этом этапе ну и также есть какие-то онлайн-части это она части нужны в тех местах где очень критично отсеять фрод еще dota до того момента когда мы приняли какое-то событие передали его пользователям ну понятно что здесь здесь то что требует максимально быстрой реакции здесь опять же мы sperry используем данные и какие то какие то алгоритма шины обучение посчитаны на более высоких уровнях о чем вообще кратко расскажу в докладе то есть расскажу как вы устроены у нас это единая платформа я расскажу про язык описания фичей и взаимодействия системой и расскажу про наш путь к увеличению скорости то есть как переход из вот этого второго этапа в 3 ну если что я не практически не буду касаться собственно методов машинного обучения в основном я буду рассказывать про платформу которая создает в чьи для для машинного обучения которым потом уже используем кому это может быть интересно но очевидно что тем кто пишет anti-fraud или борется с мошенниками кстати интересно сколько людей таких залива поднимите поднимите руку кто будет болеть с мошенниками ну вообще я удивлен достаточно узкой области ожидал меньшего количества день ну вот вам это будет интересно возможно одно также тем кто просто запускает какой-то поток данных и считают какие-то фичи запускает м л так как мы по сути сделали достаточно общую систему может быть что-то из этого он будет интересно какие системы требования но и достаточно много но вот некоторые из них это большой поток данных но мы оборваться он более сотни миллионов всяких событий за пять минут это полностью конфигурируемые фичи и декларативные язык описания факторов ней свечей обещал говорит в чьей окей новый от конечно крост отсек за клеманс обработка данных которые нужно для некоторых из сервисов удобная инфраструктура как для аналитиков которые подбирают итоговой фичи обучают собственно модели и подобные так и для разработчиков которые поддерживают эту систему ну и конечно скорость дальше расскажу про каждый из этих пунктов по отдельности я не могу рассказывать соображений безопасности про какой-то реальный сервис давайте я вам представлю новый сервис яндекса и на самом деле забудьте не цитируйте меня пожалуйста здесь но это какой-то выдуманный сервис который я придумал для для показа примеров пусть это сервис который на котором люди заходят имеют базу всех существующих книг они заходят ставит на и не оценки от одного до десяти и злоумышленники хотят влиять на итоговый рейтинг чтобы их книги покупали ну все совпадения с реальными сервисами естественно случайно ну рассмотрим в первую очередь не real-time версию для этого так как ну с первом приближении кажется онлайн здесь конкретно не нужен большие данные но яндексе есть классический способ решения проблемы с большими данными давайте использовать но придется мы используем конечно же нашу собственную реализацию придется называть войти кстати о максим ахмедова сегодня будет вечером рассказ про нее обязательно приходите вот ну вы можете использовать ваш реализацию либо реализация вроде hadoop почему вообще не используем сразу online version а банально не всегда нужно и как бы она может осложнять пересчет в прошлое если мы добавили какой-то новый новый алгоритм новые фичи и мы хотим часто пересчитать данные в прошлое чтобы изменить какие-то вердикты по ней сложнее используя тяжеловесные методы но думаю понятно и она может быть по ряду причин более требовательным присутствуем вот я если мы используем определено получается примерно такая схема мы используем некий мини bathing разделим на максимально маленькие куски бачей которым можем сделать данном случает например одна минута но текст работы мне придётся узнать что меньше этого размера наверное уже возникает слишком большие в архиве самой системы и она не сможет справляться обработку 1 минуту за одну минуту словно дальше мы над этим набором бачей запускаем как какой-то набор радиусов и получаем размеченный batch ok для наших задач нам часто возникает необходимость читать точные значения печей например если мы хотим посчитать точное значение количества книг который прочитал пользователь за последний месяц то мы будем считать это это значение за каждый матч и на должны хранить в каком-то едином месте все всю эту собранную статистику и дальше удалять из нее старое значение и добавлять новые почему не использовать почему не использует методы приблизительного подсчета но короткий ответ на их тоже используем но иногда в задачах анти фродо важно иметь именно точные значения за какие-то интервалы например разница между двумя и тремя прочитанными книгами может быть весьма существенна для тех или иных методов окей но как следствие нам нужна какая-то большая история данных которых мы будем хранить эти статистике давайте попробуем это в лапту 100 у нас есть одна минута есть какая-то большая старая история выпускаемые на вход радиусу и на выход выдаем обновленную историю и размеченный лог ножничные киты данные ну тексте опять же те кто из вас кто работал придется наверное знают что такое может работать достаточно плохо если история у нас может быть сотни а то там тысячи десятки тысяч раз больше чем самбо что такой процесс нет может работать пропорционально размеру истории они размера bocha ok заменим это на некий кивали storm но как бы это опять же наша нашей собственной реализации кивали хранилища но она хранится ну как хранить данные в памяти наверное ближайший аналог это какой-нибудь родис от но у нас тут получается небольшое преимущество что наша наша реализация то кивали сторону очень сильно про интегрирована с но придется мы кластером а придется на которых мы то запускается то есть там получается удобная транзакционных удобная передача данных между ними вот но общая схема что мы в каждом джоби и товаре дьюса будем ходить ходить в этот кивали сторож брать с него данные обновлять их и записывать обратно после формирования итоговый вердикт а у нас получается история которая позволяет обрабатывать только необходимый ключей ну и легко масштабируется расскажу немного про то как мы конфигурируем фичей ну простых счетчиков бывает часто недостаточно для поиска мошенников нужны достаточно разнообразные фичей нужна какая то умная система по их конфигурируем и как этом на удобную систему ну давайте разобьем на какие-то три этапа экстракт где мы изымаем данные для данного ключа и слога это какой-то мальчики мы сливаем эти данные с со статистикой который находится в истории ну и где мы build где мы в итоге формируем итоговое значение фичи например давайте посчитаем процент прачечных детективов пользователя допустим вы считаете что слишком высокий если пользуются считать слишком много и детективов том какой то подозрительно никогда не понятно что от него ожидать ой ну тогда экстракт это будет изъятие количество детективов который пару пришел пользователь в этом матче мерч это какая-то взятие всех детективов за месяц для всех этих данных из бачей за месяц и build это какая-то сумма после этого мы тоже самое делаем для для значений всех книг который он прочитал и в итоге получаем деление окей а если мы хотим посчитать например разные значения например количество различных авторов которые считают пользователь вообще ну тогда мы можем взять количество различных авторов которых ну раз различных авторов которые прочитал пользователю этом бычьи дальше хранитель некую структуру где мы делаем ассоциацию из авторов последнее время когда пользователь его читал таким образом если мы снова встречаем этого автора у пользователя мы обновляем это время если нужно удалять старые события мы знаем что это удалять ну и для этого для подсчета итогов вич и мы считаем просто количество включений окей но проблема в том что в шумном сигнале в таких печей по одному разрезу бывает недостаточно нам нужна какая-то система по склеиванию их джой ну по сути этих вещей из разных разрезов давайте например введём вот такие вот разрезы пользователь автор или жанра и тогда посчитаем что-нибудь более сложное например средний лояльностью автором под лояльностью здесь имею ввиду что автора пользователи которые читают автора они читают практически только его и почему это среднее среднее значение она достаточно низкая прочитанных в среднем авторов у у пользователь которого читают ну это на самом деле может быть потенциально сигнал про то что конечно это может означать что автор просто такое что все вокруг его фанаты все кто его читают и читают только его она скорее всего это может также значить что сам автор пытается как-то накручивать систему и создает этих фейковых пользователей которые читают его ну давайте давайте попробуем это посчитать то есть как бы посчитаем хочу которую считать количество различных авторов за какой-то большой интервал здесь например вот это значение второго и третье значение они нам кажется подозрительным но слишком слишком мало окей тогда посчитаем какое то среднее значение по авторам который связан и за какой-то большой интервал и тогда здесь среднее значение опять же достаточно низкая то есть три этот автор нам кажется почему-то подозрительно и мы можем обратно это вернуть пользователю что понять что конкретно этот пользователь ведет себя ну как бы имеет связь каким-то автором который нам кажется подозрительным понять что само по себе это не может являться явным критериям что что как бы этого пользователя надо фильтровать или что-то такое но это может быть один из один из сигналов которым можем использовать а я как это сделать в парадигме муп радиусов но это давайте сделаем несколько последовательных радиуса по зависимости между ними у нас получается какой-то график огров радиусов он влияет на то по каким срезом мы считаем фичи какие join и вообще допустимое количество потребляемых ресурсов но больше эдиуса больше и больше ресурсов очевидно ну и light in sea front cut окей ну давайте построим вот такой например гав с разобью на 2 этапа радиуса который у нас есть на первом этапе мы посчитаем параллельно разные радиусы по разным разрезан вот наши пользователи автор и жанра и нам нужен какой-то второй этап где мы соберем фичи из этих разных радиусов и примем итоговый вердикт опять для патча следующего мы делаем аналогичные действия при этом у нас метеозависимость первого этапа каждого патча от первого этапа прошлого и второго этапа от второго этапа прошлого здесь важно что у нас нет вот такой вот зависимости то есть нас фактически получается конвейер как будто первый этап следующего патча может работать параллельно со вторым этапом 1 бача окей ну а как в этом сделать вот этот 300 1-ую статистика которая приводил выше как бы если у нас всего два этапа ну очень просто мы можем считать первое значение на первом этапе бача н второе значение на первом этапе бача n + 1 и итоговое значение считать на втором этапе баччан + 1 таком таким образом то переход между первым этапом он будет но фактически какой то какой то стану там будут может быть не совсем точной статистики для патча импульс один но обычно этого достаточно для подобных расчетов окей ну имея все вот эти вещи можно строить более сложный фичи из кубиков например какое-то отклонение текущие оценки книги от средней оценке этого пользователя или доля пользователей который стоит сильно позитивно или сильно негативной оценки книги тоже подозрительно и лет средняя оценка книг во пользователям которых большая наценок за различные книги но этот такая может более точно более точное более справедливая оценка с какой точки зрения окей ну к этому добавляется то что мы называем отношения между событиями часто у нас в блогах или в данных которым посылается появляются какие-то дубликаты то есть на это может быть либо технические события либо робот на и поведения мы такие дубликаты ну так же обнаруживаем или например какие-то связанные события например у вас в системе показываться рекоммендации книг и пользователи кликают по этим рекомендациям и чтобы статистике итоговой статистике которое влияет на кого нет ранжирование не портились нам нужно следить за тем чтобы если мы отфильтруем пока что мы должны фильтровать и клик по текущие коммендации ну так как поток у нас может приходить неравномерно то если нам придется сначала клик мы должны его отложить до момента пока мы не увидим показ и не примем вердикт по на на его основе расскажу немного про язык описания этого всего здесь можно не вчитываться это чисто для примера но в целом как бы начинали со такого что у нас были три основные компоненты это первое описание единиц данных в истории вообще говоря произвольного типа это какая-то фича нула был число и это какое-то правило то что называем правило это набор по сути набор условий на этих вещах и ну чего он тащил ну как бы это было у нас три три отдельных файла как проблема в том что одна цепочка действий разнесена по разным файлам с нашей системой нужно работать большому количеству аналитиков им это неудобно у нас язык получается императивный в том плане что мы описываем как посчитать какие-то данные они декларативные в том плане что нам нужно посчитать то есть тоже не очень не очень удобно в этом легко допустить ошибку достаточно но и высокий порог входа новые люди приходят они не совсем понимают как стать им вообще работать ну решение давайте давайте сделаем свой dsl да и так более понятно описывать наш сценарий он проще для новых людей более высоко уровня вдохновением брали на самом деле еду я думаю много как каждый видите свое вдохновение на в целом брались таких вещей как иск велиал к мисс sharp ленка подобное вот приведу пару примеров те которые я приводил выше то есть как бы процент посчитанных детективов мы считаем количество прочитанных книг то есть группируем по пользователя считаем каунт прочитанных детективов это к этому условию мы добавляем фильтрацию и если мы хотим посчитать итоговый процентам и ну просто читаем райт ну как бы все просто все просто понятно интуитивно или например если коли мы считаем количество различных авторов то мы делаем группировку по пользователю задаем distinct автор к этому можно добавить какие условия например окно как расчета или лимит на количество значений которые мы храним но из ограничения памяти от и в итоге считаем какой-то каунт количество ключей в нем или вот средняя лояльность при которой говорил то есть опять же у нас по счетам сверху какое-то выражение мы группируем по автору и задаем какое то среднее значение среди среди этих выражений потом сужаем это обратно обратно к пользователю ну к этому потом можем добавить какие-то условий фильтрации фильтр у нас может быть там что например лояльный слишком высокая и количество процент детективов там между 80 и 100 оки что мы под капотом использованную под капотом мы используем там самые современные технологии напрямую 70-х годов такие как flex бизон может быть слышали ниган генерирует код ну вот у нас с кодом проходит через это наш эликсир который сгенерируем flex и пастер который сгенерируем бизон эликсир генерировать какие терминальные символы слова в языке pasar генерируется токсическое выражение из этого мы получаем какой-то абстрактно синтаксическое дерево с которым уже можем делать преобразование и в итоге превращаем это в низкоуровневые файлы которые понимают систему пару мыслей что в итоге но это это не сложнее чем может показаться с первого взгляда нужно потратить много ресурсов на это нужно продумать такие мелочи как приоритет операции крайней случае подобное нужно изучить редкие технологии которые вряд ли вам пригодятся в реальной жизни но если вы не пишете компилятора конечно но в итоге это того стоит то есть если если у вас как у нас большое количество аналитиков которые часто добавляются часто приходят аналитики с других команд то в итоге это дает существенное преимущество потому что работать им становится проще немножко расскажу про надежность некоторые сервисы требует отказоустойчивости такие как раз дацик за клеманс обработку ну думаю всем примерно понятно нарушение может вызывать расхождение статистика и какие-то потери в том числе денежные наше решение для для mapreduce для определить задачи что мы считаем данные в каждом 3-м не так и на одном кластере синхронизируемых на 2 вот например как бы мы здесь вели себя то есть у нас есть какой-то лидер flower есть какой-то message broker допустим можно считать что это условная кафка хотя конечно же собственно реализация вот ну как бы мы мы доставляем доставляем наши bacino оба кластера мы запускаем на одном из этого лидеры набор дьюса в принимаем итоговые вердикт и обновляем историю и результаты передаем дальше сервису обратно в этот message broker окей ну раз какое-то время мы естественно должны это сделать репликацию то есть мы собираем какие-то снапшоты собираем к этому какие-то с czech логе то есть изменения за каждый матч это это синхронизируем на второй кластер flower и также поднимаем историю которая находится таком горячем горячем состоянии напомню что здесь история у нас хранится в памяти таким образом если один дата-центр почему-то становится недоступным то мы можем достаточно быстро с минимальным лагом переключиться на 2 классе а я почему не считать вообще на двух кластеров параллельно ну как бы внешние данные могут на двух кластеров отличаться внешние данные поставляют нам могут какие-то внешние сервисы ну как бы что-то к общей внешней данные но это это что то что сводит вот этого более высокого уровня поднимается на то есть какие-то сложные просто лизации подобное ну либо просто какие-то вспомогательные данные о кей нам нужно согласованное решение если мы будем параллельно считать вердикт использованы разных данных то и и периодически переключаться между результатами двух разных слов согласованность между ними будет сильно падать ну и конечно же экономия ресурсов так мы в каждый момент времени используем ресурсы степенью только на одном кластере что со вторым кластером ну как когда мы работаем он практически простаивает давайте использовать его для его ресурсы для полноценного при провода полноценно преподами этот подразумеваю действительно ну фактически полноценную из столицы я который понимает тот же поток данных работать в прям в принципе в тем с теми же объемами данных и так далее в случае недоступности кластера мы меняем эти инсталляции с правым при продам таким образом при провод у нас какое-то время лежит ну ничего страшного окей ну и одно одно преимущество которым можем так иметь можем на природе считать больше печей почему вообще нужно потому что понятно что если мы хотим какой-то объем большой объем фичей считать то нам часто не нужно все их считать на проводе на про диму считаем только то что нужно для получения итоговых вердиктов но при этом на природе у нас такой как бы горячий корж больше большой самых разнообразных вещей случае какой-то атаки мы можем его использовать для одну закрытия проблемы и переноса соответственно этих вещей на провод ну так же мы обучаем модели на при прадеда ну на то что сказал переносим фичино про окей ну и также к этому добавляется преимущество бэк-ту-бэк тестирования то есть мы все выкатываемся сначала на припрут сравниваем полностью любые отличия ну и когда таким образом не ошибемся при минимизируем вероятность что можем ошибиться привыкать кино про войну немножко планировщик скажу понять что у нас есть какие-то машины которые запускают задачам определюсь и это здесь некие worker и worker и регулярно синхронизировать свои данные в какой-то в кросс dct то базу это ну мы просто состояние того что они успели посчитать на данный момент случае если worker который это делает данный момент он становится недоступен to our hero пытается захватить ловко забрать состоянии перри подняться с него и продолжить продолжить работу продолжить ставить задачи на этому придется мне что здесь в случае перри поднятия этих задач они некоторые множество из них может перезапустится поэтому здесь очень важное свойство для нас это эдем патент ность это возможность перезапускать каждую операцию без последствий а то есть весь код написан так чтобы чтобы это нормально работала окей ну как мы немножко расскажу прыгать за клеманс мы выносим вердикт а согласовано это очень важно мы используем технологии которые дают нам такие гарантии и мониторим естественно все расхождений сводим к нулю даже когда кажется что ну как бы это уже сведена периодически возникает какая-то очень хитрая проблема которую мы не учли как я расскажу очень кратко про инструменты с которым мы используем но поддержка множества мотив родов для разных систем ты достаточно сложная задача то есть буквально десятки и десятки разных сервисов и как бы нужно какое-то единое место где видно состоянии их работы в данный момент вот например наш наш командный пункт детей можно видеть состояние кластеров которые мы сейчас работаем переключить их между собой выкатить релиз и подобные или вот например даже борт проблем где ну где где мы сразу на одной странице вижу видим все проблемы проблема всех этих родов разных сервисов которые к нам подключена здесь вот видно что у нашего сервиса книги при провода сейчас явно что-то плохо но сработает мониторинг и дежурный будет смотреть пакет что мы мониторим но очевидно лак системы крайне важен очевидно время работы каждой отдельной стадии ну и конечно же фильтрации отдельных правил то есть такое одинцово и требования ну какие-то возникают какие-то сотни графиков какие-то даже борды и вот например на этом даже партии видно что контуру сейчас сейчас было достаточно плохо то есть тут какое-то существенное существенно лак набрали я расскажу немного вот про про переход в онлайн часть доступны хотели проблема в том что лак в полноценном контуре может достигать единицы минут это в контре нам определи некоторых случаев нам нужно банить ног обнаруживаете мошенника быстрее что это может быть например в нашем сервисе яндекс книги добавились какая какие-то возможности покупать книги у нас появился какой-то новый вид платежного фродо на него нужно реагировать быстрее тогда возникает вопрос как перенести вот всю эту схему в идеале максимально сохраняя язык взаимодействия ну привычный язык для аналитиков окей ну попробуем перенести его в лоб то есть допустим у нас есть какое-то какой-то балансир с данными от сервиса есть какое-то количество worker of на которой мы сортируем данные балансира есть какие-то внешние данные которые мы здесь используем здесь очень важны и набор вот этих вот историй напомнить что каждая каждая такая история у нас разная для разных радиусов потому что ну как бы в ней разные ключи окей ну как в такой схеме может быть возникнуть следующая проблема то есть допустим у нас на worker пришло пришло два события в таком случае у нас при любом шарди рование этих маркеров какой мы не сделаем может возникнуть ситуация при которой один ключ попадёт на разные варки рану данном случае это был вот автор толкин попал на попал на 2 бургера в таком случае мы из этого key value стороны считаем данные на оба worker из истории мы его как-то обновим по-разному и возникнет гонка при попытке записать обратно ну какое решение давайте сделаем предположение что раздели чтение и запись можно ну как бы что что запись может происходить с какой-то небольшой задержкой обычно это не слишно важно по небольшой задержкой здесь подразумевая какие-то единица секунд это важно в частности по той причине что наша реализация the key value стара она занимает больше времени на запись данных чем на чтение окей ну давайте да значит будем обновлять статистики с отставанием обычно в среднем это работает более менее хорошо за счет того что мы будем хранить какое-то кашированные состояние на машинах и та и другая вещь возьмем ну как бы для для простоты сольем вот эти разные истории в одну и page радировал мы и по типу разреза и собственно ключа этого ключ у этого разреза с какая-то единая история ведь тогда опять же мы добавим балансер добавить какие-то машины читать или которые могут page ради раваны как угодно например просто по нагрузке они будут просто читать эти данные и принимать итоговый вердикт и и возвращайте в балансиру в таком случае нам нужны набор машин писателей которым напрямую будет управляться эти данные эти писатели будут соответственно обновлять историю но тут все еще возникает то проблемы который описал выше давайте тогда немного изменением структуру писателя сделаем так чтобы он был паша диван одинаково с историей также по типу ключа и значению ключа в таком случае когда ну и ваши родирование такой же как истории тонус не возникнет проблема которая писал выше но в таком случае у него меняется но его предназначение он больше не принимает никакие вердикты вместо этого он просто принимает апдейты из ридера и правильно принимают смешивает их и правильно применяет их к истории ну понять что здесь нужна какая-то компонента координатор которая распределяет эти апдейты между лидерами райдерами окей ну да так это к этому добавится что нам нужно поддерживать конечно же актуальный кэш маркерах нам нужно ну в итоге получается отвечаем за какие-то сотню миллисекунд иногда меньше и обновляем статистике за секунду в целом это работает хорошо для сервисов этого достаточно оклик как к подвиду итог что он вообще получили мы получили что аналитики стали делать работу быстрее и одинакова для всех сервисов это повысило качество и связанность всех систем можем пересели на переиспользовать данные между этих родами разных сервисов и новый сервис и получать качественное тихо и быстро и опять пару мыслей что если вы будете что-то подобное писать сразу подумайте про удобство аналитиков продавца поддержки и расширяемости данных систем делайте все что можно конфигурируем им это понадобится это что иногда cross that exactly валанс бывает сложно достичь но можно даже если вам кажется что это уже достигли перепроверьте может вы ошибаетесь окей спасибо за внимание пожалуйста пожалуйста вопросы спасибо за доклад я хотел бы задать вопрос поводу клика уса можно ли было бы использовать для извлечения pitch и call с местного придется в данной конкретной схеме мы этот кликал что уже рассматривали и рассматриваем все дальше но там потенциально могут быть проблемы с со скоростью записи некоторых наших сценариях и ну как бы с объемами данных которым там храним на самом деле говорю что здесь есть возможность использовать его скорее всего так или иначе в данном случае нам это пока не сильно нужно было но мы таки возможности тоже смотрю привет спасибо за доклад хотел уточнить не услышал в докладе по поводу того как работает система она выдает true force на попрад решению ли это скоринговые решения ну смотри на 1 на самом деле все окей хорошо но ты-то на самом деле практически на все что можно сказать тут будут исключения так или иначе то что я говорю что сервисов много и задачи у них сильно отличающиеся но в основном мы принимаем решение true force по каждому конкретному событию и так вот такой вопрос той истории про продажи индекс книг скажем так у нас есть заблокированный платеж же яндекс книг к обратился пользователь при ручной проверки его а прогнули модераторы будет ли в онлайне учти но соответственно правила потому что этот пользователь легальный он подтвердил обновляться статистики да да да у нас есть такие системы то есть конечно при ручной проверки она потом обновляет статистике после этого мы это учитываем и последний вопрос мужик тему вот у нас прошел платеж он прошел считался как легальный при онлайн расчете будет ли отрабатывать оффлайн или нир real-time для него это так скажем для того чтобы избежать чардж backup дальнейшем и превентивно может быть вернуть этот платеж окей ну да опять же значились от каждого конкретного сервисы могут немного отличаться но в основном да мы обрабатываем такие данные и waffle они тоже имея более полную информацию имея возможность запускать более сложные методы там да то есть как то из изменяя в том числе вердикты которые мы приняли спасибо за доклад я представляю компания которая разрабатывает анти фрод в сфере такси но в том числе вот и мы делаем зон тиф рот после вашего и вот собственно вопрос почему вся вот эта вот история она не работает в такси потому что там реально обычные таксисты обходят все эти сложные технологии опускают парке за нам на 150 200 тысяч за наш в реальных денег но просто просто чем связано это сейчас а anti-fog после такси это это вы про какой-то яндекса яндекс такси нет в смысле а что что занял после у вашего анти фродо на мы чувствуем а делаю осмысляет какой-то банка скида небанковские много темные какой то мы и делаем ну да интеграцию с выплатами платежами и прочее в том числе и платёж anti-fraud том числе и сам фрукты самих таксистов то есть они обходят всю систему и опускают реально деньги окей ну давайте этот не буду не буду что-нибудь врать потому что нам конкретно наша команда пока именно антиподами яндекс такси не занимается но в целом мне кажется что если там есть пространство для улучшения мы этим будем работать а есть может контакты да подойдите ко мне а потом я смогу сказать команд спасибо за доклад такой вопрос вы хранить историю в памяти да не бойтесь что при увеличении количества вещей и усложнения их вы просто за рамки памяти у тети то при будет слишком дорого хранить мой объем вот этого хранилища мы используем ну наш собственно реализации кивали стара она конечно по шарди раваны тоже то есть она это все действительно ни одной машине хранится в памяти у нас там история может занимать какие то сотни терабайт которым мы хотим храним на каких то там около за сотни и сотни машин и что-то такое такого порядка но и достаточно да она у вас есть такие ресурсы да хорошо и раз вы используете централизованное решение да это круто как вы видите ли вы некоторые репозиторий фичей который может кросс продукту вокрус аналитически на джерси он и так далее как потом для них доступна для этих аналитиков чтобы дантистом чтобы они тут их выбирали этом мы храним это действительно репозитории аналитики из разных сервисов могут соответственно переиспользовать какие-то данные если какие-то посчитанные фичи если им это выгодно ну естественно храним историю изменений подобное то вы просто выводит хранители но давайте репозитории там с визуальную часть это просто какой-то систем контроля версий на спасибо андрей спасибо за доклад такой небольшой вопрос про продакшен и пре-продакшн вот получается что там с одной стороны говорит что препарат в случае production случае падение можем заменить на припрут но потом дальше говорит что на самом деле на природе может там другие фичи и другим классификатор как сесил тянуть то есть отчет да и просто спасти poссию там на но на самом деле они не совсем другие я бы сказал что это просто над множество этих речей почему это важно потому что те фичи которых нет на протянулись на природе в случае возникновения какой-то таки мы можем xp использовать перенести на провод для того чтобы эту атаку поймать но в целом да мы говорим что это некритично что если там дата-центр недоступен пару часов то препарат может эти эти пару часов полежать обычно это обычное нормальное состояние ответил в целом бля ok к каждой можно еще один во продолжение детских историй вы говорили что вы вычисляете ошибки страны поведения пользователя в логах вы используете какие-то классификаторы и вот дополнительный вопрос вы как-то контролируете окно за который в этот странное поведение нелегитимной пользователи учитывайте чтобы допустим если пользователь год назад ошибся случайно и сейчас не вы лидировать его как нелегитимного да конечно у нас есть у нас есть окна его там приводил примеры что мы считаем какие то какие то фичей там например за месяц или за какие-то другие интервалы просто пример если мы считаем пользователя каким-то мошенникам или что то такое то естественно у него есть какое-то время через которое но этот вердикт попадет мне здесь оклад вопрос с 2 1 - тема про от кучу данных из кучи продуктов янык сада были ли деле вот фактически используйте она не только для антипода сборкой другой статистики да то есть принципе судя по описанию возможностей у нее такая есть до постройки это статистике ну несчастным потом мы все есть и второй вопрос что мы как не решили проблему естественного изменение поведения пользу служивого просто какой-то ну если мы постараемся пример по книге до вышла новая книга к это у нее прям огромная компания люди ломанулись и все эти статистике полетели да просто потому что вот внешний фактор и он буквально вот тут же там сотни пользователей заблокировались потому что до систему было поведение не нестандартные вот что таких ситуации делать пока первый вопрос про то используем ли мы в других системах на самом деле да то есть таким таким если точно есть и у нас есть как минимум один контур которого общение как не связан с анти фродом но использует нашу систему но и потенциально возможно будем расширять по необходимости прямо сейчас в компании еще достаточно этих родов которые хотели бы к нашей системе подключиться поэтому пока мы очень сфокусированы на них второй вопрос про внезапное изменение поведение но вы конечно такие проблемы есть естественно все все подобные там фильтрации то что мы делаем все вот эти бизнеса в метрике они мониторятся дежурные они предупреждаются они изучают эти проблемы и в случае необходимости отключают плохо сразу работающие фильтры или или как ты их меняют но вообще скажу что-то чисто за счет нашей процедуры принятия конкретных фильтров конкретных методов такое происходит не сильно часто на происходит конечно да то есть но в таком случае мы как-то откатывает наши вердикта меняем их на друга и на дно кпп и меняем их значения и улучшаем как-то наши методы отключаем случае необходимости фильтра здесь вопрос про dsl показание цель выбор сети получается дерево что дальше с ним происходит допустим как вы определять что можно сделать грубой автор торгуем грубое жанр это систематически проверки вы его что это потом компилируется как как отправляя но радиус условно компилируется сейчас не буду искать в общем не совсем компилируется я его говорю что нас вначале был подходом словно с тремя джейсон файлами каждый из которых там не скоро у него описывает в деталях что мы хотим посчитать вот этот dsl как раз транслирует в эти три файла то есть эти файлы на низком уровне уже система легко понимает вот и соответственно у нас тут пока нет какой-то внутренней интеграции прямо в самку то есть мы вот в эти компоненты транслируем и ну как бы синтаксическое дерево в принципе из него примерно кажется более менее понятно как сформировать вот эти этой файл отдельных не и конечно да у него бусин тактическая ошибка она это да то есть но она она она не скомпилирует ссорились не соберется то есть они запустится поэтому ну как бы она не не не там не не попадется по такшина и на и он не потратить на слишком много времени считается кать временно вопросы закончилась но вы можете продолжить задавать вопросы андрею в дискуссии о не зоне доцент а секундочку мы еще не собираемся пожалуйста скажите кто по вашему мнению задал самый интересный вопрос вот человек синим пожалуйста вставайте он получает у нас приз за самый лучший вопрос андрей также мы хотим поблагодарить вас за все ваши труды и в код спасибо вам большое спасибо"
}