{
  "video_id": "kokWLDh8GmY",
  "channel": "HighLoadChannel",
  "title": "Hazelcast, который смог: как получить молниеносный ответ на сотни запросов в секунду / Прохор Крылов",
  "views": 379,
  "duration": 2289,
  "published": "2024-10-29T03:07:39-07:00",
  "text": "Давайте начинать всем Ещё раз привет Меня зовут Прохор Я занимаюсь эндом самокат тех и сегодня расскажу про наш опыт внедрения и использования хозл Каста какими проблемами при этом столкнулись Давайте начинать сначала немножко расскажу про компанию самокат тех мы делаем Ритейл реального времени для быстрой доставки маркетплейса и логистики То есть у нас не только всем извест сака но и другие продукты поэтому мы можем мериться и миллионом заказов доставленных за какой-то период времени присутствием различном количестве городов с технической точки зрения у нас больше 500 микросервисов которые написаны на разных стека в том числе на котлине если про gvm говорить и на Джаве немножко rps достаточно большие для тех задач которые мы решаем и той области бизнеса в рамках которой работаем стараемся делать так чтобы пользователи получали ответ на свои запросы достаточно быстро вот О чём сегодня поговорим Сначала посмотрим на решение типовой задачки обработки запроса и как кэш может помочь нам в этом и где он на каком этапе может использоваться какие варианты на рынке будут как внутри той или иной компании можно принять решение о выборе инструмента не брать же всё подряд и опыте использования нашем а в конце рассмотрим то какой Какой вывод мы для себя сделали Вот тут я немножко Хочу уточнить что это наш опыт с понятными входными данными в рамках ограниченных средств команды которой мы это делали в условиях развития в условиях быстрорастущей команды и что это будет всё-таки специфично по gvm разработка по gvm Вот давайте же начнём Из чего состоит Типовая обработка запроса на примере в микросервисной архитектуре и проследим за увеличением сложности посмотрим на разные подходы и оценим варианты на рынке для начала всё как правило начинается с какого-то запроса пользователя с мобилки с Вебка на наш кнд и походам может быть в БД или стороннюю систему в стандартном кейсе никаких сложностей обычно не возникает мы получили запрос сформировали ответ выдали потом становится всё больше запросов какие-то данные мы не хотим за ними ползать каждый раз в баз данных добавляем кэш всё ещё работаем в рамках одного инстанса и проблем пока ещё не возникает потом у нас появляется балансировщик несколько инв сервиса и Кэш внутри каждого из них а кажется всё должно быть в порядке но не так-то было Потому что если мы столкнётся с каким-то запросом Когда у нас одни и те же данные должны вернуться с разных инстан сов но балансер раскидывает запросы так Так что они попадут на разные поды а кэш там не одновременно инвалиди и поэтому мы получим невалидные данные в нашем случае наверное не очень хорошо увидеть в приложении самоката обновив страницу молочко по разной цене вот поэтому приходит к какому-то вынесенного кшу когда он вообще стоит рядышком и все инстансы сервиса ходят в одно место Таким образом мы решаем эту проблему альтернативой может быть распределённый кэш про который как раз один из вариантов холка буду рассказывать когда внутри каждого инстанса поднимается уже инстанс самого кэша и на кластере сервисов они объединяются в какую-то единую группу в рамках которой хранятся данные а вариантов обычно много могут быть какие-то более арогантный вещи например вынести кэш с данными на балансировщик но это уже реже используется и всё-таки в настоящее время выбирают между каким-то Лан серверным вариантом либо распределённый кшм при этом нужно помнить что требования инфраструктуры могут сказаться на выборе инструмента способ развёртывания где вы это делаете в клауде на своём железе сами это обслуживает но и про плюсы минусы потому что не для каждой задачи подходит любое решение Давайте посмотрим на варианты на рынке и начнём с самых популярных например дис поднимите пожалуйста руку Кто пользовался Кто знает У кого в компании используется Ну вот очень много обычно первое С чего начинают это с него и проводя собеседование часто встречаю людей горят что Компани больше ничего не надо Он позволяет хранить данные в памяти использоваться как ш как streaming eng и mess брокер потом уже идут как бы такие не такие популярные вещи как например лка Это целая платформа которая предоставляет все те же возможности написано на две и хорошо интегрируется см приложениями очень похожее решение они позиционируют как распределённая база данных лат нить данные в кше ещ есть куча решений например Тарантул предыдущий докладчик В этом зале рассказывал про него очень обстоятельно тоже в принципе может для этого использовать своей документации они это заявляют ну и даже Касандра которая по сути но SQL база данных Может с этой задачей справиться и помогает её решить Ну и в том числе другие варианты Однако когда мы приходим к конкретному выбору мы можем даже провести сравнение у себя что нам лучше подходит но приходит всегда к выбору из двух решений которые скорее всего максимально подходящие если взять например самый популярный в данном случае дис илка который широко используется в разработчиков то можем их так быстренько сравнить и как бы сделать выводы что плюсами редиса является Широкая распространённость он достаточно просто в использовании куча библиотек написано на разных языках для работы с ним имеет поддерживает сохранение на диск таким образом можно его использовать как базу данных даже он достаточно сложен с поддержкой масштабированных когда вам нужно именно отказоустойчивого решение и потребляет много железов при специфичной нагрузки вот л Cast поддерживается отлично на уровне jvm он на Джаве написан всегда можно провалиться в его реализацию посмотреть как он работает а поддерживает из коробки дит режим и мультитрейд архитектура достаточно производителем наши фонс инженеры его тестировали на разных наших решениях и у него достаточно большой запас по производительности для наших задач При этом он не так распространён а есть определённые ограничения коммьюнити версии которые мы тоже рассмотрим Ну и под капотом хоть мы можем посмотреть анализация достаточно сложна она и если надо что-то понять как он работает может быть это в документации недостаточно подробно описано будет сложно вот в любом случае нам нужно что-то выбрать мы потом несём за это ответственность поэтому как правило нужно подходить к выбору осознанно Ну и в нашей компании критериями послужило то что у нас был у разработчиков опыт работы с этим инструментом как таковая отсутствовала команда инфраструктуры и в условиях договорённости если компания команда инфраструктуры брала на себя ответственность поддерживать то или иное решение они должны были знать как его разворачивать масштабировать в тот момент не было достаточно времени чтобы они могли с тем же редисом это сделать поэтому тут больше подходил он в режиме позволяет работать чисто внутри приложения вот ну и требование минимального времени ответа когда тебе нужно всё только из памяти достать часто важно поэтому мы посчитали что нашли для себя это решение выбрали его в своё время и начали использовать здесь я хочу немножко остановиться на деталях и рассказать о том какие тонкости мы учитывали и стоит всем учитывать при выборе этого решения что скорее всего вы не будете брать Enterprise версию потому что её сейчас уже нельзя купить но во-вторых она достаточно дорогая поэтому Open Source наше всё и его комьюнити версия то как HC позволяет работать это C серный режим Когда мы ставим ин лка и ходим к нему по сети с помощью того или иного клиента либо Т режим Когда он встраивается в инстанс нашего приложения скорее всего понадобится делать какие-то доработки руками Потому что м как мы посмотрим дальше коммьюнити версии не всё предоставляет из коробки Ну и сложность конфигурирования на той или иной экосистеме тоже доставляет некоторые неприятности вот а ещё момент нужно помнить что за нейминга достаточно много прячется если погуглить начать как пытаться исследовать эту историю то можно увидеть что в интернете достаточно много разных статей написано прол C Memory datag platform Jet что это вообще такое на самом деле это всё разные версии либо comunity либо Enterprise Edition То есть например в третьей версии там был HC и HC Memory datag который был платный потом в четвёртой версии добавился jeet он тоже добавлял дополнительные возможно Ну а в пятой версии текущей актуальной они назвали ВС Это лка спфм запихнули в одну коробочку То есть тут нужно просто для себя всё это структурировать и разложить нужно помнить про Open Source Enterprise ограничения что самые вкусные интересные фичи есть только в Enterprise версии например монитори ра управление Лин апдейты Ну и так далее Вот и самое важное что есть в Open Source версия это кластерный режим Low latency Data Access и возможность работать внутри Java приложений широкий выбор структур данных Enterprise версии поддерживает всё то же самое что Open Source Но вот как я говорил добавляет наиболее важные из них как нам кажется это возможность сохранять данные на диск мониторить кластер предоставляет Rolling updates при выгод новых версий и репликация данных между DC а также хранилище высокой плотности Я много раз упоминал про Edit режим Давайте же чуть подробнее посмотрим что это если вы использовали другую терминологию то это всего лишь возможность поднять ин внутри своего приложения и после этого все инстансы сервисы объединяются в такой кластер и сделав пут на одном из инв сервиса в кэш мы можем быть уверены что эти данные после окончания выполнения операции будут на другом инстанс доступны и у вас не будет проблем с их получением Так давайте же переходить к нашему опыту использования и здесь мы максимально подробно постараемся всё посмотреть С какими сложностями мы столкнулись вот для начала немножко аген ды как он у нас используется это 50 с лишним сервисов которые обрабатывают пользовательский запрос о участвует непосредственно запросов клиентов нашего мобильного приложения в кше у нас достаточно разнородные данные хранятся это могут быть как несколько мегабайт десятков так и до гигабайт доходит Ну естественно это не один не единым куском А в сумме на все ключи конкретного кша мы используем режим про который я рассказал и дальше тоже мы его затронем и что это только для потому что у нас был с ним опыт первоначально но и в других стех ребята используют чаще потому что они не могут в режим его внедрить у себя как зависимость подключить а также мы очень много вложились в общие библиотеки которые помогают с халом работать достаточно просто с точки зрения продуктовых команд когда не надо читать всю документацию а достаточно лишь коротенько ми в нашем в нашей библиотечка в тла Изучить и подключить А дальше уже при необходимости начинать конфигурировать и разбираться подробнее кажется всё круто на первый взгляд очень радужно радуги единороги и мы начинаем это на разных кейсах использовать например хранение попыток логина пользователя возможно какие-то Локи доступные заказы склады рекомендации пользова данные витрин каталогов категорий товаров цен и возможно технические вещи вроде фича флагов после этого получая некоторые или сталкивалась с некоторым набором сложностей наступая на всевозможные грабли уже не кажется что всё так красиво как казалось первоначально вот первое с чем можно столкнуться это развёртывание холка у нас используется кубернетес для сервисов Сейчас мы посмотрим какие трудности могут возникнуть в этом вопросе в чём проблема Проблема в том что подняв несколько подов одного и того же сервера сервиса они друг друга не обнаружат холка не увидит соседние инстансы самого себя и не объединится в кластер он конечно начнёт об этом ругаться в логах и вся магия не случится что же можно сделать Ну тут надо остановиться нем разных версиях и если вы например использовали л или используете до пятой версии то нужно добавить отдельную зависимость в лка кубернетес которая добавляет возможность ему помогает ему обнаружить себя внутри Куба вот при этом при конфигурирования не забыть задать настройки ДНС и установить настройки обнаружения внутри сети вот начиная с пятой версии эта зависимость уже из коробки предоставляется и подключается одной библиотекой поэтому нет необходимости заморачиваться об этом но конфигурировать всё равно придётся После этого мы в логах Увидим что инстанс и мемберы лка сложились в один кластер всё начинает работать и мы можем с ним взаимодействовать как и планировали следующая проблема - это обратная совместимость с чем тут сложность а сложность в том что храня какую-то структуру данных в кэше мы постоянно её модифицируем обновляем добавляем Новые поля добавляем поля которые могут быть типом класс может быть каким-то примитиво удаляем поля и а при этом раскатываем наши сервисы через Rolling update через какие-то другие способы когда у нас в один момент времени несколько версий инстанса м работают Поэтому нужно уметь так строить миграцию что добавляя вот новую структур например в версию 2 New Field Когда уже был в первой версии Old наполнено нужно её делать сначала опциональный здесь может возникнуть сложность с тем что при использовании котлина по умолчанию все филы они not это его фича проверка в тайме и поэтому у нас при выгод такого возникнет ошибка что ты мол пытаешься not N нал записать значение в not переменную потому что всё ещё нету этой информации для филда при репликации данных и выкатки нового инстанса а тут надо помнить что не только репликации на новую версию нужно поддержать но и репликацию обратно потому что можно раскатить сервис понять что там есть баги он объединил в кластер Но нам нужно его всё равно откатить и поэтому есть некоторые подходы например написать свой стерилизатор представляющий дефолтные значения Ну задача реализуемая но не самая тривиальная без дополнительной разработки можно поиграться с переименованием кэша или с добавлением поля как N и с дальнейшим пингом вот через перенова кша - это по сути добавление нового но нужно помнить что необходимо наполнять кэш и прогревать его перед вводом пода в балансировку вот через ин это добавления в структуру кэша сначала значения как опционально потом выкатываем релиз дожидаемся наполнения значения нового по всем ключам делаем поле обязательным следующим релизом для заполнения и уже вторым релизом выкатываем и фиксируем первоначальную задачу это всё равно не для всех случаях работает у Вас например может быть кейс когда вам нужно откатиться сразу на две версии И к сожалению тут вы всё равно словить ошибки но нужно об этом помнить и использовать тот подход который вам подходит вот а похожие кейсы как я и говорил достаточно стандартным способом решаются это добавление удаления нового класса может быть удаление кэша его класса добавление поля где этот тип класс Ну и так далее мне помогает думать что мы работаем со структурой данных в кше как с миграция базы данных то есть мы не сразу приходим к целевой версии а постепенно несколькими двумя-тремя иногда больше релизами добавляем новое значение в структуру потом удаляем старое если это необходимо таким образом делая коротенькие итерации при этом скорее всего из-за проблем с коммьюнити версии и стандартной стандартным реализатором у вас возникнут проблемы именно с с обновлением структур данных поэтому мы пошли в другой стерилизатор крио подключили его через библиотечку subzero но что-то как-то совсем не учли что мы используем вообще никакую не мажорную версию а версию 01 и наивно полагали что обновив с на 011 всё продолжит также успешно работать но ребята там у себя что-то сломали и поэтому мы это на тестировании словили нужно это помнить и не проверять всё не забывать всё тестировать и проверять таким образом достаточно много документации нужно изучить чтобы все подобные кейсы покрывать и у себя в проектах не натыкаться на подобные проблемы мы собрали в конфлюенсе ряд примеров когда можно столкнуться с той или иной проблемой и описали Как из неё выходить когда в команде или продукте очень много подобных задач и часто меняется структура данных следующая проблема с которой вы скорее всего столкнетесь это обновление уже существующих разработанных приложений проблема стандартная только поддержка в интей версии и попытавшись раскатить новый инс внутри кластера лка с новой версии зависимости мы увидим такую ошибку что вот ты тут Обнови потому что это я не могу обновить потому что это только проблем и тут можно найти способы сейчас я про них расскажу первый - это через разные именно кубернетес деплоймент когда мы не просто новый инс новую версию сервиса поднимаем а поднимаем в отдельном деплоймент который вообще никак не объединится в кластер хозл Каста уже существующего версии сервиса надо понимать что если вы сразу на него начинаете подавать нагрузку то там может просто не быть тех данных в кэше которые есть в предыдущем инсе или они могут также разъехаться То есть Не сработает вот э вот инвалидация одновременная но способ возможный также постепенное раскатывание с выключением старых версий сервиса канареечный релизы упрощают эту задачу то есть не нужно там в конфиг лезть поднимать новый деплоймент кубернетес внутри какого-нибудь инструмента вроде arg CD который у нас используется можно это делать в рамках ко наречных релизов меньше ручного вмешательства но ограничения остаются теми же а остановка через выключение по сути сервиса если у вас продукт по рассчитан на операционную деятельность какую-то и например там у него есть окно технологическое ночью то можно ночью в принципе выключить все сервисы в ноль поднять новую версии сервиса которые снова версии хозл Каста работают и у вас никаких проблем с репликации данных внутри разных версий Хала не будет он не будет ругаться на то что мне нужен энтерпрайз Ну и наверное есть какая-то собственная реализация которую можно написать потому что Халка предоставляет а для этого но это тоже непростое решение мы пока До него ещё не дошли в рамках всех текущих продуктов нам помогали те или иные варианты которые я ранее рассмотрел следующая сложность - это мониторинг кластера всё знакомо этого нету в коммьюнити версии поэтому мы идём и читаем документацию смотрим Что есть Разная статистика по примитивного по его примитива которую можно получить А что можно статистику чего можно получить можно получить статистику маповская строить графики на размер кэша на среднее время выполнения операции и на количество операций с Шом всё это можно сделать самому только нужно немножко поработать следующее с чем скорее всего придётся столкнуться это предоставление API для менеджмента кэша получения всех текущих Шей внутри каждого сервиса подключение получение значений по ключу и так далее в чём проблема Проблема в том что этого опять нет й версии а нужно получать статистику по шам менеджмент под систему сделать Ну и какие-то расширенные возможности там например очистить ве весь кэш для решения предыдущих проблем обновления версии Хала либо поддержки обратной совместимости hazelcast предоставляет AP для этого есть Local Map statistics который можно за использовать и внедрить А у себя для настройки опишешь У нас есть собственная обёртка над работа с Шами она во-первых помогает ускорить подключение в продуктах но также помогает сделать какой-то опи например вот такой вот когда можем получить все кэши получить кэш по имени инвалиди его получить все ключи и каким-то образом управлять данными которые там лежат например с тем же редисом было бы всё проще у него по умолчанию ipi уже есть а здесь всё лежит внутри памяти наших приложений Поэтому нужно будет доработать но опять же в Прай версии это было бы Таким образом мы собрали целый Спектр граблей которые с которыми скорее всего те или иные сервисы продукты могут столкнуться в зависимости от того какие вы задачи решаете если у вас в коше лежит какая-то структура данных очень небольшая вы её не будете изменять может быть у вас есть возможность для простой скорее всего вообще никаких проблем не будет но если у вас большая Ну такая относительно большая нагрузка вам при этом Нельзя терять данные то возникнут сложности и нужно как-то уметь с ними работать таким образом можно сделать выводы что в комьюнити версия холка вполне же не способна нужно только уметь с этим работать а Кроли Нора может быть глубока и обязательно понадобится какой-то для управления который самому нужно будет сделать понятное дело мониторинг который также нужно будет доработать Ну и помнить о том что самостоятельно нужно делать выводы и брать на себя ответственность Мы в своё время взяли хозл Каст но недостаточно подробно что ли изучили его документацию не оценили Какие трудности в перспективе у нас возникнут и поэтому Возможно не стоит в моменте зацикливаться на каком-то решении и Если у вас есть возможность переделать это произвести но нас было время мы смогли научиться бороться со всеми сложностями реализовали те или иные вещи которых не хватало и коммьюнити версия в принципе помогает нам жить спокойно и отдыхать вне рабочие часы Вот Всем спасибо рад был поделиться своим опытом Прохор Спасибо тебе за замечательный доклад ты смог тебе маленький презент от организаторов и от Газпром нефти и у нас есть вопросы Из зала пока несу микрофон а если ты бы знал вначале пути что о надо сделала ты вот туда вылез в эту круч Нару ой боюсь да боюсь сказать что у нас особо не было выбора Мы тогда были очень маленькой компании все разработчики помещались в одно помещение и у нас как я и говорил был опыт сходил кастом а сделать нужно было как всегда вчера поэтому мы его взяли и сначала он решил все задачи отлично но потом с годами мы наталкивает или иной сложности о стандартно проблемо масштабирования как понимаю а сейчас что бы ты предложил в качестве альтернативы такой э очень очень похоже альтернатива - это apch который предоставляет много тех вещей из коробки даже В бесплатной версии и у нас вопрос из зала Спасибо за доклад Камиль тика Вопрос такой в принципе я так понял уже немножко его озвучили звучит так что внедрение этого решения много дороже чем даже какой-то прожорливый редис и обучение разработчиков ресу можно сказать так не было ли мыслей не знаю вернуться к ресу и Действительно ли Ну окупается ли это решение мы да спасибо за вопрос Мы очень быстро росли и сделав первую версию приложения по gvm когда мы и так его с питона переписали У нас вот не было времени так не ю немножко откатиться и ещ раз всё переделать вот тем более все эти ве дали как правило потом у нас скорее всего в начале были сбои потому что мы как бы немножко закрывали на них на глаза сейчас стало всё Это сложнее и нам пришлось учиться работать с тем что есть и внедрять описывать вот эти все подходы а для обновления там структуры данных для раскатки на новых версиях А уже существующего кода вот ну в перспективе Да у нас в компании уже и ris также используется То есть как я и говорил в конце не надо затачивать на что-то одно а всегда смотреть по сторонам мо короткий Вопрос такой стандартный контрибьютор до этого в руки не дошли Вот Но вообще есть направление в компании в рамках своего аккаунта в гитхабе развивать наши наработки и скорее всего Мы выложим то что сделали для того чтобы можно было другим это использовать как раз о ЧМ я всём рассказывал а с радостью будем ждать когда вы это всё-таки выложите Мы тоже что-нибудь открыта и у нас следующий вопрос пока не микрофон А сколько времени у вас занимает выкатка новой версии в куп а выкатка новой версии сервиса или что Ну вообще я как раз на эту тему тоже доклад уже рассказывал про платформенный подход и в своё время мы довели Ну как бы выходку нового сервиса до продакшена там до 1-д дней К сожалению с ростом компании у нас появилось много новой бюрократии и поэтому этот путь сейчас как увеличился это тема отдельно обсуждение корах и у нас следующий вопрос Спасибо за доклад Вопрос такой значит у нас скажем десяток сервисов и получается у нас столько же будет нот в нашем распределён кластере сервис хранить в себе копию данных и вот нужно нам это 10 нот или не нужно это же бесценная память сервисов и ну и я бы ещё добавил к минусам такого решения что Ну в целом если мы сравниваем например с тем же рельсом что мы прикола себя к два стеку я например очень люблю jav но кто знает что будет завтра а совершенно согласен да Сейчас я отвечу на оба вопроса Первый про то что в каждом инстанс хранится полный слепок данных на самом деле л Cast предоставляет много примитивов данных и вот то что вы говорите это так называемый реплицируемый кэш когда на каждом инстанс всё хранится он ещё позволяет точно так же создать распределённый кэш Когда вы можете конфигурировать сколько данных на каждом инстанс будет храниться и он при запросе на свой инс если не найдёт их у себя в кше он пойдёт в соседний вот поэтому мы не храним весь слепок на каждом но при этом нам нужно хранить бэкап других инстан сов на этом поэтому а объём данных всё равно один Просто он распределён таким образом вот А по поводу второго вопроса А ну мы знали на что идём Да это как бы понятное ограничение И если мы вдруг начнём переписывать на каком-то другом стеке скорее всего у нас не только проблема с этим будет ну и всё-таки первый хотелось бы дополнить это же получается наша бесценная память наших сервисов тут если мы например используем дис мы можем выносить это куда-то на отдельный сервера улучшать масштабирование А здесь мы потребляем свою бесценную память Да но мы и Профит получаем потому что в редис это сходить по сети куда-то А тут мы в хип сходили Это совершенно разные временные затраты друзья это отличная тема для дискуссии многие к вам соединяться и от меня Ремарка Java вечно и у нас следующий вопрос Спасибо большое за доклад а интересует такой момент Вот есть же варианты использования л Каста как отдельно стоящую и как инстансы внутри сервисов замеряли ли разницу именно накладных расходов по тому же цпу понятно что по памяти будет использование Да потому что мы внутри пода храним А вот именно по цпу то есть накладные расходы на поддержку самого хазел Каста внутри инстанса сервиса то есть какая разница между обычной работающим приложением из хозл кастом Да спасибо за вопрос У нас как мы и говорил первоначальный лка выбран был на основании опыта сотрудников и мы особо его там нефос тестирование не проводили потому что знали что для этих задач он справится на основании опыта в дальнейшем с учётом появления у нас перфоманс инженеров sre инженеров мы проводили тестирование и сейчас проводим тестирование запаса сервисов с его использованием и они измеряли Сколько памяти он начинает выжить при увлечении объёма данных и так далее я вот не помню чтобы СП какие-то проблемы были Вот честно Ну то есть даже в процентном соотношении он незаметно не не скажу цифры да точно измеряли но я не помню чтобы там нам указывали что вот у вас тут скорее всего потребуется больше их запрашивать поэтому скорее всего незначительно Окей спасибо я что-то сечас заметил что у нас уже второй или третий доклад где Старт успешных технологий началось с того что в команде был Тото тот кто это когда-то видел это прямо сегодня Тема и у нас следующий вопрос Здравствуйте меня зовут Олег Подскажите наверное два вопроса Первый Как ведёт себя Если разваливается кластер особенно если у вас распределённый кэш вот особенно если он разваливается на два более-менее стабильных куска То есть у Вас например есть два цода в половина хостов в первом цоде половина во втором как он определяет и у вас соответственно сразу начинается расхождение данных между этими цода Вот это первый вопрос а второй связан с первым А друзья Давайте по одному вопросу спикеру неудобно давай Да ничего Ну ладно давайте а второе с производительностью То есть когда у нас идёт например обновление кэша там же нужно обновить по сути все э как это все инстанции соответственно в этот момент что делает hazle Cast он не отдаёт кэш или что происходит Спасибо сейчас да так Ну сначала на первый вопрос отвечаю о м так разные дата-центры мы во-первых не работаем в разных дата-центра у нас Если есть какой-то инстанс то только в одном дата-центре Вот и вся нагрузка идёт на него но при этом Split Brain про который вы спрашивали всё равно может случиться даже в рамках одного ДЦ То есть когда у вас Да действительно сервис работает вы подняли ещё один каким-то магическим образом не увидели что он там в логе начал километровые стык трисы выкидывать он до создаст сервис он его поднимет Потому что это не является ограничением при раскатке То есть он чек пройдёт холка есть да вот но он действительно может поднять два независимых и тут мы столкнётся с проблемой что ну у нас может конечно разъехаться да это правда он начнёт об этом влоге сигнализировать но можно сделать так что у тебя сервис просто не будет подниматься При этом если он какие-то сложности начал обнаруживать Ну именно обнаружение если вопрос репликации данных то тут сложнее уже То есть когда он с репликации столкнулся там не получится Это при старте остановить расска нет Там скорее вопрос про то какой он будет считать мастером То есть у вас работающий кластер который развалился на два под кластера и каждый себя счита Т мастером Вот как он поведёт себя в этой ситуации вот а там Дело в том что ему не надо знать мастер То есть он как бы определяет какой-то который Ну всех учитывает остальных Да членов кластера но он не является точкой входа например только для какого-то рода там запросов вот на любой инстанс можно отправлять одинаковые запросы друзья это отличная тема для отдельного исследования Я думаю тему будете продолжать И у нас следующий вопрос да Прохор Спасибо за доклад Меня зовут Елена У меня вопрос такой на вашем опыте да то есть А есть ли какие-то ограничения по объёму кэша По числу записей в кэше когда кэш становится медленной когда предел за который не стоит выходить а тут зависит от того как вы разворачивается и где потому что репликации между инстанса хозл Каста тоже всё-таки занимает какое-то время я вот как раз не ответил на второй вопрос молодого человека по воду производительности сейчас заодно отвечу что действительно когда мы долгую вставку какого-то большого объёма данных делаем то ну другой инс он как сути лочи он не может ничего считывать но тут у лка есть как бы свои там ну точнее может считывать он не может как бы он начинает замедляться при этом Вот и поэтому жёсткие тайминги ставят для него вот по поводу объёма данных ну у нас там десятки гигабайт лежат но мы ещ Пока не перешагнули за 32 гиба когда это уже кажется Ну и другие проблемы начинают виртуальные машины вызывать Вот то есть на таком объёме данных проблем нет но нужно понимать что я не говорю что у нас есть ключ и против него 10 Гб лежит н да это ключ значений где значение там ну до нескольких метров это Окей работает но суммарно по всему кшу на все эти ключи с их значениями это гигабайты Да и проблем не возникает спасибо спасибо за вопрос Спасибо за подробный ответ и у на во Да я хотел спросить вот вы много говорили про Прай версию вы не считали что было дешевле купить рй с самого начала или доливать Open sce опции купить рй не было тогда и тем более нет сейчас вот но первоначально не предполагалось даже в перспективе покупка Прай мы знали к чему к чему это может привести Ну и были готовы к доработка Спасибо что ты сделали иначе ты бы не упал Здесь нам докладе Спасибо большое и последний вопрос Выбери лучший вопрос который мы отдадим белый пакетик СПО Да молодой человек вот про 10 инстан сов спрашивал да два два было вопроса спасибо Прошу выйти к нам на сцену Прохора который смог Спасибо всем спасибо"
}