{
  "video_id": "T-wzoCJoF3s",
  "channel": "HighLoadChannel",
  "title": "Нейросетевые рекомендации сообществ ВКонтакте / Любовь Пекша (ВКонтакте)",
  "views": 797,
  "duration": 2379,
  "published": "2023-01-19T06:59:52-08:00",
  "text": "всем привет я любовь и уже около трех лет и занимаюсь рекомендациями и на данный момент руковожу ориентир разработкой в группе remedy и рекомендации вконтакте а сегодня мы поговорим о том как мы имплементировать линейра сетки в рекомендации во-первых обсудим требования потом о том как мы выбрали изначальный алгоритм о том как сделали так чтобы он был рабочим и как реализовали это вроде а сообществ вконтакте а сообщества один из самых ключевых продуктов наша нейросеть нашей соцсети и у нас есть миллионы потребителей контента миллионы авторов контента сечь процентов content can set time spent a сосредоточено вокруг сообществ зачем рекам зачем рекомендовать сообщества ответ довольно очевиден потому что мы увеличим обреченность пользователя и у нас есть две точки входа в рекомендации это главная лента и это раздел сообществе тот факт что мы показываем рекомендации в главной ленте означает что у нас нет большого пространство для экспериментов потому что если мы покажем нерелевантные рекомендации пользователь идет из главной ленты 11 очень плохо перейдем непосредственно к рекомендациям во первых у нас есть источники кандидатов их несколько потому что разные источники кандидатов улавливать разность именно о пользователе и работают за разное время источники кандидатов отбирать порядка 10000 кандидатов для рекомендаций которые потом отправляются на ранжирование и уже ranker а мы получаем список отсортированных сообществ которые даем пользователь отлично в чем же наша задача наша задача сделать источник candidate of long-term интерес что это означает представим себе пользователя который интересуется программирование ну а вчера внезапно залип на котят с точки зрения рекомендации порекомендовать вам подписаться на котят это хорошая идея но нам важно не забывать что пользователь все ещё интересно программирование почему вы решили поправок не раз этим во-первых это мощно лучше разработки теплер надо всё такое и во-вторых это гибкость как с точки зрения данных так и с точки зрения требований к системе здесь стоит немного поговорить о том какой опыт был у нашей команды мы занимаемся рекомендациями и рекомендации это на треть runtime код который работает на бэг-энде мы должны обеспечить всю логику от отбора кандидатов до ранжирования и большую часть этого кода это backend треть эта работа с бедра то стейка стеком всякая кавказ тренинги и так далее еще во треть приходится на алгоритмы причем word сосчитала этих алгоритмов это какие-то spark job-mo придется что имеет работать большими данными собственно опыт с не растя минос был не такой большой и по большей части это был опыт не связанные с персональными рекомендациями например лупи и давайте поговорим о требования которые мы выдвигаем к алгоритму начали о тех требований которые продиктованы нашей суровой реальностью немного терминологии embedding это вектор embedding пользователя вектор пользователя это синонимы скалярное произведение мы можем считать как меру близости двух векторов чем выше скалярное произведение тем более близки электорат друг другу и а н н а прочь имеет сервисный барс это алгоритм для быстрого и эффективного поиска ближайших викторовка сада на у нас работают на скалярное произведение и как вы могли догадаться первые наши требования быстрый поиск кандидатов быстро поиск кандидатов через войны и для нас как разработчиков алгоритм это требование означает что релевантность между пользователями вектором пользователей и вектором сообщества это скалярное произведение теперь поговорим о данных во первых у нас есть данные сообществу сообществе нас очень много 500000 это лишь верхушка айсберга который мы используем непосредственно в рекомендациях но тем ни менее данную нас все равно очень разряжен нас есть в обществов которые встречаются в свете всего 15 20 раз и о сообществах у нас есть много данных например локации какая-то сложная информация название тематика постов нас есть пользователи нашим алгоритмом мы строим рекомендации для 100 миллионов пользователей и так же у нас есть просто информация о пользователе и сложно например им бединге интересов по другим доменом и самой важной информации который он свистит историю взаимодействий мы берем историю за последние шесть месяцев и хотим учитывать все разнообразие bk который у нас есть исходя из того объема данных которых есть возникают еще одно требование который называется baku fires мешок айтемов она заключается в так что мы хотим учитывать уникальную историю для пары пользователь сообщества и для этого нам нужно как-то просуммировать на 6 дек также мы хотим чтобы финальном процессе наша история была не упорядочена и для того чтобы как-то просуммировать наш век мы используем затухание во времени еще одно требование последнее на сегодня это вычисляем avoiding пользоваться им банки могут быть вычисляем и и обучаемым обучаемым биллинге и боинги сообществу они обучаются вместе с моделью после того как модель обучилась этим бединге фиксированы а вычисляемый мы можем вычислять используя историю пользователя и его метаданных почему история поль интересы пользователя меняются часто а группы более стабильно плане тематика поэтому можем реже приучать модель но чаще или перец более того мы даже можем питере чего практически в реал тайме раз 15 минут я сути запись также позволяет нам скинуть место на видеокарте 100 миллионов м поединков пользователя очень сложно запихнуть на одну видеокарту и нам бы пришлось параллелей и так далее а очень не хотелось тратить на это время и главное преимущество это cold start такой подход позволяет делать рекомендации для пользователя с маленьким количество взаимодействий лучше исходя из наших технических требований мы можем стать первый прототип нашей архитектура и скалярное произведение как скоро релевантности i'm wading пользователя вычисляю щийся агрегируются магии из его истории и bay wing сообщества которые обучаются а теперь поговорим о том чего мы действительно хотим от нашего алгоритма во первых это лунг тер как я уже сказала ранее во вторых разнообразие или доверся это означает что мы хотим что на наши кандидаты учитывали разные интересы пользователя в списке кандидатов на самом деле это сложно и требование исходя из технических требований потому что если у нас есть 100 похожих сообществу их виктора будут практически идентичны и они все сто разом приняли дуются пользователям также мы хотим использовать еще разнообразие метаданных которых у нас есть и хотим бороться цеплялись басен в паре тибальт это когда популярные айтемы рекомендуется чаще чем обуславливает их популярность и расскажу об этой проблеме и немножко подробнее на слайде представлена совместная встречаемость мультфильма история игрушек с другими фирмами под это сету movie лэнс и забери пользователи посмотревший истории игрушек самые популярные фильмы оказываться не истории игрушек 2 афоризм star wars и так далее и самая правая колонка это футляре тиран популярность фильмов по datasette у чаще всего встречается те фильмы которые просто популярны и исходя из классической постановке задач по истории пользователя предсказать что еще посмотрит пользователи для пользователя посмотрев шива только история игрушек порекомендовать посмотреть и звездные войны это нормально и в некоторых случаях это так например если мы делаем рекомендации для инопланетян но в нашем случае мы хотим считать что пользуйтесь знает о популярных а этом их из других источников и поэтому здесь важная ремарка поджарьте боятся то зачастую проблема не только алгоритмов но и метрик и об этих метриках мы сейчас поговорим наша основная онлайн метрика это количество подписок из рекомендаций также важно отметить что это среднее количество подписок из рекомендаций для пользователей потому что нам важно делать рекомендаций качественно для большинства пользователей и наша основная оффлайн метрика которую мы вычитаем до того как запустить эксперимент это recall доли релевантных поэтому в которой были бы рекомендованы пользователя и вообще говоря прикол можно вычислять по разному в зависимости того что мы считаем релевантным альтами первый подход самый классический который мы называем вечерком заключается что что давайте возьмем историю нашего пользователя и отодвинем ее на пару дней назад и посчитаем рекомендации и посмотрим насколько хороша наша модель предсказывает на что подписался пользователь в последние несколько дней и это действительно хорошая метрика потому что угадает на что подпишется полете ли это почти почти то же самое что порекомендовать ему на что-то подписаться минусы это метрики в том что неравномерного пользователя не будут представлены только тени жира которые на что-то подписались последние дни она не учит этот факт что мы хотим long-term источник кандидатов и патриархи боится мы по истории игрушек рекомендовали бы и звездные войны а что же мы сделать чтобы вы видите эту проблему мы придумали наш recall он заключается в том что в качестве релевантных поэтому будем использовать а этом из истории часть из них мы будем маскировать часть из них не маскировать в часах часть преимущество во первых равномерность вы пользователей для того что пользуется попал в эту метрику ему достаточно быть представленным куда-то сети хотя бы 1 одним а это это лаг чо потому что мы угадывание не тона что пользователь подключится завтра а то чтобы было интересно месяца назад и менее подвержена повторите боюсь потому что для пользователя который посмотрел только история игрушек мы должны быть должны уметь угадывать историю игрушек а соответственно мы будем рекомендовать ему фильмы похожие на историю игрушек например историю игрушек 2 ну а здесь важно отметить что вы должны быть уверены что у вас история игрушек похоже на историю игрушек 2 поэтому отдельно нужно придумать метрику измерять качество mb динка фантомов я не буду рассказывать про нашу метр а потому что во-первых оно сложное во вторых очень сильно зависит от вашего домена рекомендаций скажу что используйте всю ту информацию которая у вас есть оба шемар и темах например у нас это локация контент тематика сообщества ну и как обычно считается метрика качество им берингов это какое-то расстояние между двумя сообществе например и мы берем для случайно сообществ строим топ ближайших к нему соседей и смотрим какое расстояние между исходным сообществом и его ближайшим соседями и чем меньше тем лучше это метод заключается в том что мы вообще-то и придумали и основная проблема почему мы стали и придумывать вообще говоря это проблема оффлайн и онлайн корреляция мы улучшали нашего флан митраков вечерком в плане но не видели прироста в онлайне надо было что-то с этим делать мы придумали эту метрику и улучшая эту метрику мы действительно стали улучшать онлайн рекомендации так что победителей не судят отлично с требование разобрались давайте уже перейдем к архитектуре наш продавай билайн этого что век идея заключается в том что давайте считать сообщества словами а историю пользователь предложения будем строить вектор сообществ как слова история пользователя вектор полиция это соединение ликторов его сообществ весами да у нас несколько источников кандидатов у нас есть и более классический как матрица факторизация но мы используем именно этот как бизнес который мы сравниваем всего в плане потому что он удовлетворяет нашим техническим требованиям менее подвержен поджарьте барсу объекта пользователя посмотрев шива истории игрушек это вектор истории игрушек он лучше справляется с редкими а это мими а у нас очень сильно раздражены и данные и самое главное преимущество он работает приносит нам приростов роли для того чтобы получить вектор пор не тебя нам надо как-то усреднить его историю с какими-то лесами разного фидбэка у нас много а надо получить его один вес и давайте попробуем найти вес типов фидбэка вес лайка вес клика для этого мы мучились супер лёгкой модель в которой всего несколько параметров обучаем их вес и вес типа лайка места типа клика вектор сообществ мы зафиксировали world веком и поталь вектор пользователя вычисляли и пытались угадать по типу как включили кол на что подключиться пользователь в будущем и мы получили довольно интересные данные самый высокий вес оказался у джона это вступление в сообщество а самый низкий вес способ скрип шина может спросить в чем вообще разница разница в том что join это именно вы нажали на кнопочку этот байк который умеет затухать a subscription это просто факт подписки и это так настолько слабой что мы убрали возвра коммендации полностью и не потеряли в качестве и если вам вдруг интересно остальные тип фидбэка распределены как после joy на самый высокий like потом коммент клик на пост открытие сообщества и всякие другие типы кликов просмотр видео и так далее отлично перейдем к прототипу нашей архитектуре мы решили взять архитектуру который называется камень камере control a multi in france 3 команды шон и статьи от али-бабы двадцатого года в чем заключается идея для того чтобы обеспечить дайвер у нас четыре поединка пользователя которые называются авторами головами и теперь релевантность у нас считается не просто как скалярное произведение а как максимум скалярных произведений между этими четырьмя головами и им поединком айтемы и для агрегации нас используется механизм внимания причем в довольно простое в форме вес поэтому мы учитываем используя только м поединка самого этого ритма в плюс это решение она довольно простое нам нужно заменить всего два кубика в нашей изначальной архитектуры во вторых она обеспечит нам довершить и также это продуктовая статья что немаловажно для нас и также заранее скажу что наши итоговой кандидаты это то 500 ближайших кандидатов к каждому не зови кторов пользователя главная проблема этого подхода в то что он не работает если работает значит надо чинить и начали попробуем починить архитектуру главная проблема была в том что у нас очень раздражен данные и бединге для редких ой томов не сходились и мы много чего пробовали обучали там по несколько дней пытались что-то там фризить экспериментировать с оптимизацией но все равно мы работала хуже чем vertu век поэтому просто взяли инициализировали наши виктора world веком и потом до обучали наши ректора это работает получилось примерно также как во рту век но работа с одним условием что у нас только один вектор caretero а если у нас несколько векторов пользуются это оказало оказалось что они все слишком близки друг другу решение тут тоже простое давайте просто сделаем штраф на близости викторов пользователю друг другу стало хуже и стало хуже из-за того что у нас были ненормативных головы то есть они никогда не выигрывали в морщинами скалярных произведений из-за этого не до обучались поэтому давайте сделаем хедер pout то есть во время обучения мы будем отключать next из голов чтобы остальные могли до обучаться отлично теперь работает лучше чем что век на онлайн несравнимо но приростов нет ну и классно по крайней мере мы смогли хоть что-то сделать давайте попробуем дальше для того чтобы наши элементы нашего наших викторов не улетали в нем саном же как-то регулировать нашим бединге в артеке стандартные используется нормализация но мы попробовали использовать лернер этот подход часто используется в анапе например уже аппетиту и это действительно сработало причем у нас сильно выросли метрики сильнее чем мы могли вообще даже ждать и если вдруг вам захочется имейте нормированные виктора вы можете после на плеер норма сделать нормализацию и качество будет такой же как если бы вы просто сделали лернер еще одна проблема которая была в нашей листик туре attention смотрит только на одну группу из истории что это означает что я себя два пользователя для каждого из которых мы хотим сделать два вектора интересов для первого объединить кошечек и собачек 1 2 разделить но из-за того что наш о том что смотрит только на одну группу из истории мы получаем что для первого поезд занимательная информация собачка проблема заключается в том что на штендер только на одну группу из истории когда определяет вес и за это они так не понимает что в первом случае ему нужно разделить кошечек собачек а во втором первом случае объединить во втором разделить и решение здесь радикальная не используется такой attention даже если вы делаете только один вектор пользователя на самом деле такой подход не сильно лучше усреднение с каким-нибудь и сами потому что ну что может all green достать и самого вектора сообщества в лучшем случае вы сделаете чуть чуть лучше чем усреднение а в худшем случае ваша модель достанет популярный боец из мб1 до сообщества окей что делать кажется идея простая давайте просто будем смотреть на историю пользователя в момент когда мы определяли без если вы хотите так делать используйте и кику areas of attention is трансформера опять же идеи из нлп и кратко терминология кейс это вся история или например предложение аквар это сообщество вес который мы хотим вычислить случае с предложениями это слово контекст которого мы хотим понять предложение но если посмотреть на этот слайд можно увидеть одну проблемам стрелочек очень много их м квадрат и кажется что в отличие от предложений взаимодействие между сообществами в истории пользователей не такое сложно особенно учитывая тот факт что она у нас неупорядоченная кажется что можно как-то простить и мы придумали такую идею и мы стали использовать какую-то агрегацию историей пользователя а в качестве квари стал потенциал и что у нас получилось мы используем history игры гершон как лари и мы решили принять историю сами как мы это делали во рту веке мы пробовали разные варианты но с пенисами работают хорошо также мы добавляем метаданные пользователя к нашему кларе на самом деле это основная причина почему мы так делаем потому что несмотря на то что в типично kia quoris of attention н квадрат работает это не сильно дольше но для того чтобы учить место данные пользователя в таком отеле не нам бы пришлось конкатенировать историю политика к каждому элементу а это как-то слишком дорого и стильно и если сравнивать эти два подхода без метаданных пользователей то они то эти печеньки и кларисса потому что он работает чуть лучше чем вот такая от агрегация но когда мы добавляем метаданные пользователя вот такой алгоритм работает также как и эти печеньки и кларисса по танцам и раз уж мы начали копировать архитектурой трансформера мы можем также красиво добавить рихито лс и несколько линейных слоев итоге по архитектуре красно классно работает предали цивилизация лернер в качестве регуляризации найм бединге и внимательно относиться к вашему attention этот подход позволил на получить плюс 11 подписок из рекомендаций для нас это супер круто но можно лучшим давайте мы пограммы разобраться в том как мы обучаем нашу не розетку это чтобы обучать не раз сетки нам нужны торги то во первых нам нужно позитивом и позитивно как и в метрике мы берем из истории часть из них мы маскируем а часть из них не маскируем и также нам нужно негативы изначально мы брали их случайными случайной группы но такой подход можно улучшить усложнив негатив сэмплинг сэмплирование негативов во-первых сэмплировать пористый от сета так чтобы популярны а это мы сэмплировать для чаще здесь есть параметр альфа принять в равна нулю это равномерное сэмплирование пре-альфа равную единице сэмплирования чисто по чистоте а можно на сэмплировать несколько негативов но оптимизировать только худший полосу и просто сделать несколько негатива все их оптимизировать и здесь самый оптимальный вариант это скомбинировать эти варианты или хотя бы несколько из них теперь поговорим о лосе лосс в отличие от оффлайн метрики должен быть оптимизированы для то что мы могли обучать нашу не розетку немного терминологии позитив это чисел k предсказание для позитивного таргета негатив предсказаний для негативного таргета я рассмотрю 4 лосса сегодня хотя мы пробыли очень много разных вариантов первое этого point вас кросс энтропия идея заключается в том что мы отдельно и максимизируем позитивное предсказание и отдельные минимизируем негативное предсказание можно придумать несколько улучшений а например не максим не минимизировать до конца и негативное предсказания только до какого-то порога и так же помните у нас есть в сагру так мы берем позитивы из истории vesa для позитивов у нас тоже есть и мы можем домножить ту часть лосса который отвечает за позитив на этот вес и это работает очень круто стоит сказать что именно этот раз мы используем в нашей не розетки хотя в других домена где я пыталась использовать эту архитектуру выиграл мне этот вас и как раз таки я думаю что связано с тем что у нас очень серьезное влияние имеет вес позитива и это единственная point вас архитектура который мы используем в остальных подходах мы тоже можем нажать на вес позитива но этот я за будет неявно влиять и на негативы и возможно это пор смотрели хотя и детального расследования я не проводила а что еще мы пробовали часов макс просто на трапе я и грубо говоря мы максимизируем позитив при условии негатива а проблема заключается в том что благодаря параметру не gc мы могли бы влиять на нашу оффлайн метрику но спрос на тропе и если у нас уже высокое предсказание для позитива то через параметры негатив саклинга мы не сможем взять на на нашу модель и улучшить наши оффлайн метрики это модели работ этот вас работает лучше только если мы используем много негатива и видимо это связано с большей стабильностью и можно также придумать несколько улучшений из компьютере жена crimes of max и арк весь еще один раз который мы попробовали это не pr лосс из классической матрицы из классического подхода матричной факторизации и делали час в том что мы максимизируем разницу между позитивным и негативным предсказанием проблема заключается там что позитив у нас не случайно а негатив от случайной и довольно быстро вы достигнете ситуации когда среднем разница между позитивным и негативным вас будет больше нуля тогда ваши модели просто достаточно до можете эту разницу на какую-нибудь константу чтобы минимизировать лосс и поэтому просто у нас начинают вылетать бесконечность элемента викторов и такой подход не работает без сильной regular соци в названии by pr это бойся а бойцовская в этом модели бойцовское объяснение l2 регуляризации то есть даже название этой модели есть регуляризация последний подход на сегодня в росах это триплет вас он тоже максимизирует разницу между позитива и негатива но до определенного порога и он работает хорошо только при условии если позитив и негатив мы будем считать не через кляр на произведение о через бухту этого расстояния на самом деле это довольно необычно подхватывают тоже посмотрела и статьи про нлп и stand at берто но действительно работает видимо из-за того что f3 давай расстояния в какой-то мере является регуляризации name бединге минус надо подбирать гипер параметр и чуть менее эффективно работает по времени работает при большом количестве готти вов и так сэмплинг крутой некий десантник это круто негатив сэндоу нам + 6 подписок из рекомендаций полос а могу сказать что есть много хороших лоссов но лучше и зависит от того какая у вас задача какие у вас данные и последнее что я хочу сказать что оптимальный параметр и нигде зависит от вас а то есть вы не можете взять и зафиксировать параметры негатив сапога а потом выбирать лучшие лоз если вы хотите выбирать ласково нужно выбирать совместно с параметром негатив самсунгах как и говорил ранее им были game и придание целе зиру и давайте расскажу о том как мы обучаем этим был бы передать во первых во рту рек мы обучаем его не на всей истории пользователя за несколько месяцев она сессиях которые длятся обычно не больше 3 часов а почему так потому что в течение полугода у пользователя может быть огромное количество интересов но в 1 сессия это обычно один или небольшое количество и за счет это у нас получается лучше ботинки для сообществ а во-вторых мы улучшаем нашем рейтинге за счет контента обучаем еще одну модель в которой по контент и информации сообщества описание заголовок тематике последних постов пытаемся приблизить вектор полученный world веком и отдалить случайный вектор а тут может возникнуть вопрос почему бы нам просто не взять и не добавить нашу информацию о сообществах в нашу основную модель и не делал дополнитель модель на самом деле ответ тут просто и потому что хуже работает когда мы стали использовать вот такие при допущенные контентом и биллинге оказалось что наш лосс вырос то есть с точки зрения модели когда она обучалась она стала хуже то есть как бы формально нашлось не идеальный выбран но тем не менее по оффлайн метрики мы стали лучше и у нас выросло благодаря при обучении с помощью контента на 3 процента подписки из рекомендации лично с модели мы закончили теперь надо все это тащить в рот первых нужно настроить соседи все это у нас работать с помощью эльфа берем его диска дупа запускаем спарка фскн утробу которая из сырых логов стоит нам datasette и и потом в докере в докере spider чем счетам виктора виктора сообщества кладем войны виктора пользователей кладем факелы как это работает непосредственно в роли у нас поступает запрос на рекомендации и мы идем в кэш если в каше есть недавний рекомендации мы просто берем и отдаём их фрод если в кэше нет недавних рекомендаций мы идем на шкивы или старриджа достаем вектор пользователя делаем запрос войны и отдаем этот список в рекомендации зачем нам нужен кэш ну собственно для того чтобы не делать слишком много и слишком часто запросы к а н н у один раз когда вы листаете вашу ленту рекомендация может несколько раз встретиться блок с рекомендациями соответственно так мы улучшаем стабильности работы нашего production так можете спросить вы получили такой классную модель вырастили подписки но от неужели все ради чего вы это делали нет мы можем улучшить несколько других сервисов наших наши соцсети во-первых семинара это похожее сообщество к нашему есть он просто к нашему а н н и делаем запрос не лектора пользователя а вектором исходного сообщества для которая хотим найти похоже во вторых простые интеграции для тех доменов где авторами контента являются сообщества мы можем сделать схему очень похоже на ту съем которую на есть у нас только здесь и войны мы загружаем не все сообщество а только те сообщества которые публикуют контент нужного нам типа более сложной интеграции мы можем наш вектор пользователь использовать как новый мята дату и вектор сообществе как новый метод у пользователя таким образом наша суперкрутой модель позволила нам растить не только метрики сообщества метрики других доменов на 6 мы получили плюс 20 процентов прироста из рекомендаций и хочу дать советую для тех кто как мы занимался более классическими рекомендациями но вдруг захотела сделать de fleur ник это во-первых обратить внимание на ваши оффлайн метрики это особенно важно для теплер надо потому что это очень мощной моделей и вы можете переобучать вашу метрику проблема fly онлайн корреляция чаще встречаются с deep лингам вторых будьте внимательнее к вашей архитектура вам нужно действительно понимаете что происходит трек не бойтесь грязных кругов таких как преданный зале зация и последнее что я хочу сказать что это сложно это действительно сложно это не первая попытка нашей команде сделать ниро сетевые рекомендации и стоит отметить что для того чтобы их сделать успешными нам пришлось обратиться к другим сферам deep learning для того чтобы имплементировать лучшие подходы собственно без глубоких понимание том как работать и пленок очень сложно сделать хорошие рекомендации на этом все спасибо за внимание супер спасибо большое и мы давайте зададим вопросы в зале понимаете руки к вам подойдут барышни вот я выгляжу там вот молодой человек прям сейчас куклу верник практически отличный вид спасибо большое за доклад очень круто смотри у меня будет такой вопрос каким образом вы решаете проблему холодных пользователей которые еще не оставили вообще никакого фидбэка либо оставили фидбэка очень мало каким образом обсчитывается рекомендации для них спасибо спасибо вопрос но во первых у нас есть создаем то есть мы строим рекомендации там полу по возрасту по локации вот этот все самом крайнем случае нас даже есть маркетинговые топы если водится в общении справный какая себе информация во вторых у нас есть симилара то есть если пользователя стал к это фидбэк вы сразу runtime ищем похоже на нато собственно которой он стоял фидбэк похожий и сразу их рекомендуем таким образом мы сможем быстро реагировать на последней недавно отставленный фидбэк пользователя ну вот дальше уже идут более сложен и классические подходы когда пользуется оставил какое-то количество фидбэка эта модель позволяет эту модель мы используем когда у нас есть хотя бы хотя бы два разных сообщества от пользователя можно я пока вот мучиться и найдем кого не чувствует эти вот вот молодой человек перед этим у микрофончик пожалуйста я покажу этом вопросе такой немножечко не в тему а вот в случае если у человека были какие-то рекомендации вы ему уже сформировались список да а потом подмешивать или вы ему другие из других сфер рекомендации чтобы по нему как-то немножко расширить его зона интересно это задача называется exploration и но она решается немножко отдельно собственно прикол в том что у нас есть несколько источников кандидатов эти кандидаты могут выполнять разные цели в том числе ну сочи кандидатов для лучшего expiration тоже пользуется находил новые интересы у нас тоже есть отдельный модель и путь спасибо и спасибо за доклад хотел уточнить момент вы тут решайте такую задачу в ранжировании матч инга типа да такую метрику энди сиджи не пробовали но используется в оценивании качестве ранжирования у нас тут преклов то что мы отбираем какой-то список рекомендаций они все идут на ранжирования и ranker не знают на какой позиции нам отдавала этот источник иди до тоф то есть именно честно меня нам необязательно имеет мерить метрики которые учитывать позицию с виду клики как-то словно для gain а вот так вот с виду вот вы писали там метрику там им классом почитать с виду тысяч жену мы по факту но мы просто берем таргет и из истории она ваша мерить м10 жена мы как бы нужно позиция у нас как бы но нет этих позиций не ту позицию вы считаете на количестве на подписке плюс лайки + клик она типа использовать качестве появится отсортировать по лесу до вопросы начали хорошо и вот сюда молодому человеку микрофончик поднимаете руки если есть вопросики а потом а потом вот туда вот спасибо за доклад а еще такой вопрос когда вы сообщества рекомендуете вы учитываете от какую-то активность самого сообщества или уже количество подписчиков то есть не порождает ли это проблему это популярен скорее вопрос о качестве самого сообщества метрики какие у ты оттуда забираете но эта информация сообществом и учи там разную вообще говоря мы не учитываем информацию об активности сообщества в когда мы строим рекомендации чтобы нас не было никого там папуля реки бойца на всякий случай но в ранжировании эта информация учитывается то есть ну и собственно в ранжировании учитываться только вот как раз таки более простые фичи о сообществах такие как там city or и различной активность рост активности подписки очень много всего очень разной информации амид информация о сообществе как часто обновляется bluetooth и стремится виду она постоянно пополняется вот в этом вот риан жирование по-разному есть ну как бы разные источники фичей так сказать для ранжирования на есть и те которые runtime а что спасибо давайте еще вот последний вопрос молодой человек и потом мы в город перейдем дойти до спасибо за доклад вопрос такой как часто вы перед заваривайте рекомендации и используйте любые real-time новые данные для этого то есть во многих своих системах сейчас есть такая штука что ты допустим дал какой-то фидбэк какой-то отдельный ой там и уже через 10 секунд через двадцать лайфхаки про уже рекомендаций злить его перри завариваю то есть другая вертикаль подключена как делаете ли вы такое или планируете мы делаем это с помощью у вас почте то есть вы оставляете бег на какой-то на какой-то группой мы просто берем последние группы на которые вы оставили позитивный фидбэк и рекомендуем похоже на них мы обновляем виктора для пользователей 1 раз в день и переобучать за новую раз в 10 дней и обновляем виктора для пользователей только для тех которые но оставили кончик back но но когда мне при обучаем там уж какая то эта информация о полить их измениться соответственно там чуть больше менять рекомендации мы также экспериментируем сейчас разрабатываем работу ну вообщем inference викторов пользователя почти в реал тайме то есть у нас будет собираться в batch пользователей которые недавно ставили фидбек и они будут отправляться через гркц гу потом то доронин и x и все такое на во первых это еще разрабатывается во вторых не мной разрабатывается тут я наверное не буду говорить о каких-то подробностей"
}