{
  "video_id": "LWEoJ04p-fQ",
  "channel": "HighLoadChannel",
  "title": "Необходимость и боль перехода с IBM MQ + RH Fuse на Apache Kafka + Apache Camel / Александр Крылов",
  "views": 784,
  "duration": 2674,
  "published": "2023-04-28T06:10:46-07:00",
  "text": "Доброе утро коллеги знаете что всегда где-то есть утро поэтому оно может быть и добрым Почему бы и нет сегодня как мы с вами поняли я расскажу про определенную боль перехода с вендерлочных решений на опенсорсные вообще зачем Как где И почему но прежде Давайте с вами познакомимся во-первых я из компании Росгосстрах лидделов со паузка Росгосстрах Если вы думали что в росгосстрахе Как таковой идти нету несмотря на небольшое подразделение вы ошибаетесь нам более чем есть о чем рассказать и поделиться вам собственно я являюсь как и сказал коллега вокалиста метал банда террорин сайт для тех кто любит потяжелее преподавателям в нетологии по направлению системного администрирования и ML Ops по Data профессии дата инженер помимо этого разработчика модуля для ребрейна по ха прокси Ну и спикер Как вы поняли различных конференций credention вы видите собственно на экране в конце они будут продублированы если кому-то будет Интересно добавится потом еще в том числе пообщаться О чем я поведу свой сказ мы все формулируем и Сначала я расскажу в целом что как было мое участие непосредственно и роль которую я принимал в данной активности далее я расскажу про определенные первые итерации попытки ухода от одного из решений и собственно путь прихода к тому что мы получили сейчас фактически про промежуточную некую архитектуру После чего мы с вами затронем и поговорим о том какая архитектура мы видим уже прям совсем в целевую Как вы понимаете всегда есть куда идти дальше на месте мы не стоим начнем с исторической справки прежде всего Прежде чем перейти непосредственно к тому что вы видите на слайде скажу следующее я с точки зрения человека который присутствует на этом проекте являюсь тем кто помогал с организационной процессуальной точки зрения планировании по переходу и соответственно тем человечком который в том числе как сам так и с моей командой А у меня команда в подчинении с 12 человек по различным проектам переносили постепенно всю нашу радость про которую я сейчас и расскажу собственно про что пойдет речь про интеграционную платформу и как вот предыдущий коллега также вещал здесь будет все немножко про тоже но с другой точки зрения и с другими технологиями собственно речь пройдет про интеграционную классическую интеграционную платформу в виде шины данных где есть некие системы источники системы получателя и некая шина но Давайте чуть в конкретику что по задачам представляла наша шина данных ну и де-факто представляет по задачам сейчас это система которая отвечали за транспорт фактически транспортировка различных сообщений пакетов от системы по данным в случае где было необходимость передавать эти самые данные между системами с каким-то возможным преобразованием семантики Ну то есть фактически это внутреннее содержание чего-либо по на уровне пакетов завернутых во что-либо это может быть какие-то архивы Джейсон и все что угодно Ну и само себе расположение в данном случае расположение имеется ввиду когда вы используете данные в каком-то бизнес-процессе и берете из одной системы и переиспользуйтесь другое ну то есть такую задачу У нас также решает это что здесь можно сказать весь подход с точки зрения классики и теории он базируется на классическом Соу то есть сервис сориентит архитектор Собственно как я и сказал помимо текущих задач по связям Это передача данных из одной системы в другую И как вы можете увидеть на слайде здесь варианты этой связи они могут быть совершенно разные и в общем-то в этом ничего такого нету у нас могут быть как классика в виде системы источник шина система получатель так и более сложные структуры когда подключается чуть большее количество адаптеров шины которые выступает в качестве посредника передачи различные информации Ну и есть еще один менее стандартный подход но в том числе подходящий под классификацию со это использование в одной системе функционала другой Ну то есть если мы говорим исключительно про данные и про какую-то информацию это одна история а функционал это немножко глубже условно для того чтобы вам в системе посчитать предварительный расчет того же самого КАСКО у вас пройдет сквозняком с точки зрения транзакционности по нескольким системам на уровне request responso это самое сообщение и потом вернется то есть речь про это как наша шина с точки зрения процессуальной архитектуры выглядела есть некая система потребитель сервисов есть транспорт Ну собственно у нас аббревиатура это РГС апин так вот сложилась исторически назовем это так соответственно есть некая виртуализация конкретика будет чуть позже с точки зрения архитектуры решений присутствует свои адаптеры потребителя и адаптеры источника при этом есть по сути инфраструктурная часть составляющая которая на каждом этапе делает какой-то свой Ну в общем если правильно сказать есть там брокер есть адаптеры и на каждом этапе соответственно свой инструмент если обобщать то с точки зрения процесса оно получается так приложение хочет взять какую-то информацию или передать другое приложение далее это все попадает на транспорт транспорт у нас взаимодействует с брокером очередей брокер очередей кладет свою кубышечку в нужную очередь Согласно логике после чего это переправляется по транспорту в систему получателя если совсем облегченно чуть позже будет побольше конкретики как это выглядит с точки зрения архитектуры У нас есть некий инициатор в роли инициатора может выступать как система так и человек например тестировщик которому нужно протестировать домовую доработку по какому-нибудь адаптеру шины там не знаю маппинг полей добавился или что-то еще далее У нас Собственно сам транспорт в нашем случае транспорт от IBM им Q и здесь имеет смысл сказать для тех кто не знает что это за инструмент объем Q это некое по которой используется в качестве брокера очередей то есть У нее есть месседж аренд middler И причем Это решение класса действительно именно Middle VR и это решение позволяет управлять собственно этими самыми очередями Посредством передачи из одной системы источников другой как мы с вами понимаем самая BMW много чего не умеет это достаточно громоздкая большущая коробка Раньше она называлась сфера потом она называлась в общем со своей жизненной цепочке она несколько раз меняла название Такая же история будет про другой инструмент который я чуть позже скажу собственно в нашем конкретном случае это просто голый брокер очередей который занимается приемом первичного входа транспорта с систем источников либо инициаторов запросов следующий компонент который у нас присутствовал это Red Hot Fuse и Пожалуй это один из самых таких громоздких компонентов в данной цепочке здесь есть несколько аббревиатур с которой Возможно вы в типичном таком лексиконе брокеры очередей могли не слышать этот IQ я чуть позже скажу про что это имеется ввиду А так по факту это именно мессенджмент внутри которого в том числе присутствует те самые адаптеры в нашем случае эти были адаптеры на базе классической Java если не изменяет память Изначально этот jdk 8 впоследствии 11 Ну и собственно он также отвечает за самый все вот эти самые интеграции то есть mqtransporthed Fuse по сути все остальное в том числе и логика и здесь важно заметить еще одну интересную хитрость В некоторых случаях нет определенного рода системы ходили минуя транспорт То бишь обеимм Q напрямую во фьюз это было связано с особенностью архитектуры мы это также еще неоднократно проговорим Именно поэтому вы видите вот эту дополнительную стрелочку собственно я пояснил чтобы у вас не было вопросов Почему именно так что касается Red had Fusion для тех кто не знает у него также достаточно длинный путь в плане именования то он был jbos сам то он был Джейд Босс фьюз потом фьюз в итоге он стал в общем вы можете встретить большое количество вариантов аббревиатур смешанных между собой но по факту сейчас это собственно что в нем присутствует с точки зрения архитектуры есть инструменты обработки на базе собственно девелопер студия есть системное управление так называемый jbos оператор Network и есть достаточно большое количество компонентов для парсинга например обработки таких стандартных вещей как суап xml rest Full http в качестве компонента у него выступает Апач оси XF дальше обработка преобразования данных шаблона ин в данном случае у него под капотом находится Как вы видите Camel Ну и Собственно сам дополнительный брокер сообщений который Как показывает практика не всегда применяется из коробки а используется что-то внешнее как собственно и было в нашем случае в виде обеим Q и Есть еще такая интересная штука которую я намеренно выделил красненьким этот take you что это такое фактически это промежуточный сервис который управляет жизненным циклом сообщений Ну то есть его очень грубо можно назвать некими мозгами Но де-факто это просто определенный сервис который знает По какому алгоритму из какой очереди какое сообщение брать и куда класть его в другую сторону например в какой-нибудь адаптер который впоследствии пойдет в целевую систему я это условно назвал тыкву и вы чуть позже поймете почему это аббревиатура использована именно в таком контексте соответственно внутри под капотом здесь присутствует апачков Идем дальше ну и соответственно конечная цепочка элемент цепочки это система мастера Святой система получатель в том или ином виде причем Обратите внимание что у нас переключение между системами внутри происходит по обращению к транспорта Как сами инициаторы так и непосредственно Fuse это то о чем я говорил в части переиспользования Ну то есть использование одной системы функционала другой системы это вот именно про это поэтому здесь выглядит цепочка именно в таком Ключе с точки зрения цифр чтобы было понимание С чего мы это начинали Когда у нас интеграционная платформа появилась как класс на старте это было 13 систем инициаторов три транспорта по инвайерментам То есть это прод предпротест 88 потоков и 7 мастер систем в принципе объемы не такие большие но это вот тот самый Старт который был проделан примерно за год какая была на тот момент времени нагрузка примерно 7000 запросов в минуту напротив примерно 8 миллионов запросов в сутки за день Ну да в сутки собственно Циферки понятно что небольшие по сравнению со многими другими компаниями которые здесь выступают но как я и говорил Это определенный Старт с которого Мы начинали Теперь давайте поговорим немножко про Команду в любом случае когда подобные инструменты появляются в компании команду тоже тут нельзя убирать присутствовали следующие компоненты следующие люди это аналитики потоков данных разработчики этих самых потоков и архитекторы ну и по факту получился процент что когда появляется новый поток он приходит на архитектуру согласуется на базе бизнес требований бизнес требования формируется в техническое задание техническое задание уходит на разработку разработчик все это делает реализует первая версия кладется тестируется если все хорошо это все вокруг себя обвешивается с точки зрения версионирования Ну и собственно дальше уже это все идет по циклу пока тот или иной поток по каким-то причинам Ну условно не будет диплейтен и не нужен например там вдруг какая-то система мастер до свидания скажет И поэтому поток станет перестанет быть нужным это что касается команды развития безусловно команда поддержки также необходима и нас был бедный один администратор платформы который все это тащил как вы понимаете это Пускай горлышко Ну и безусловно у нас присутствовала определенная архитектура решения это именно сам архитекторы потоков которые способствовали определенному составлению стратегии Vision развития данной платформы а теперь давайте поговорим про ресурсы учитывая Все выше сказанное проблема ресурсов заключалась в том что учитывая очень нестандартную громоздкую систему которая стала интеграционной платформы мы получаем крайне уникальную компетенцию по которой на рынке очень непросто найти специалистов это определенные редкие шивы которых Если находишь то они будут стоить очень хорошо в копеечку также люди становятся заложниками стека Ну это очевидно потому что во-первых это venderlock во вторых это Enterprise конкретное решение по которым в принципе в России не так много специалистов в том числе сертифицированных ну и соответственно люди потом если там занимались например только фьюзом они еще как-то могут перейти во что-то собственное тот же самый кэмел у них есть понимание А вот себе м-кью несколько сложнее это крайне специфическая система и связи с этим все свои последствия но и утечка если у нас уходят люди на подобные системы при этом не задокументируют что-то Или вдруг что-то не дорасскажут вы понимаете Какие могут быть из этого последствия в том числе тот самый бедный один администратор который все это поддерживал Ну вот собственно я перехожу к поддержке архитектуры Здесь также идет уникальная компетенция и мало специалистов на рынке которые действительно могли бы понимать куда как под капот залезть чтобы пофиксить ту или иную проблему Это не я это все моргнули просто далее собственно по развитием платформы Здесь тоже определенные сложности были поскольку компонент не один и все компоненты громоздкие и по ресурсной составляющей развивать сложно это распределенная инфраструктура в том числе где Возможно это сохранение drp то есть отказоустойчивость минимальное дублирование два соды и так далее и сложно такую громоздкую вещь в том числе масштабировать Ну и как мы с вами понимаем санкции ведерлог никто не отменял понятно что изначально когда все эта История начиналась об этом никто не думал но реально немножко вносит коррективы Давайте теперь поговорим про рефакторинг и иные способы решения проблем по которым мы пошли и почему собственно Так в том числе пошли проблемы я Обозначил Давайте теперь расскажем как они решались первая проблема которую мы начали решать Это история вокруг ресурсов То есть это уникальная компетенция заложники стеков утечка кадров Ну то есть в том числе по сути Здесь идет соотношение инструментов и соотношение людей решение Оно хоть и далось не сразу по сути перейти на более что-то популярное желательно опенсорс специалистов которых по компетенции намного больше на рынке и это развяжет руки Это поможет людям научиться болеть и уходить в отпуска они бедными сидеть постоянно И поддерживать все это вот И с чего мы начали начали мы безусловно с Red had фьюза здесь есть определенные ссылочки там можете их пощелкать это определенные статистики потому как можно какие вещи разделять периодически Они здесь будут появляться если кому-то интересно потом могу в том числе скинуть у фьюза достаточно много аналогов и безусловно нужно подходить с точки зрения здравого смысла мы составили Опрос лист мы составили критерии под них в итоге подошел такой пул инструментов но этот пул достаточно быстро расстрелился по причине как минимум того что нам не подходит Облачное решение а ряд из них это облако и до свидания дальше мы начали смотреть Ага кэмел адаптеры в принципе Понятно rabbiting you и вот мы столкнулись с определенной проблемой оказывается учитывая специфику наших адаптеров для того чтобы перейти на репетингю их бы пришлось рефакторите переписывать от слова совсем то есть 88 потоков Палите быстренько Перепишите в достаточно Короткие сроки Ну и как вы понимаете Мы в том числе отстрелили Именно из-за этого посмотрев внима она на особенность Spring bootа у нас ранее напомню был чистый Java и в том числе технологии использования батчинга в springe соотнеся ее с кремом мы поняли что это тот самый результат который нам подходит по всем нашим критериям собственно это и случилось Мы выбрали Camel в качестве сервера приложения брокера очередей именно в части адаптеров и соответственно спрингбут технологию Spring Boot butching для использования непосредственно сервисов для написания этих адаптеров Ну и померив замерив сколько по времени Это примерно займет в совокупности примерно 40 человека часов на адаптер с нуля как мы с вами понимаем есть нюансы это новый адаптеры но у нас есть 88 адаптеров которые также нужно переписать они по заметке времени вышли несколько больше если не изменяет 580 человек часов это чуть позже также будет но тут как мы понимаем нюанс нюанс с точки зрения утери компетенции Возможно где-то не задокументировано подводные камни и все радости бытия когда мы переходим с какого-то Legacy на что-то новое какие у нас Циферки получились как я сказал 40 человека часов на адаптер с нуля 80 человека часов 1 адаптер сюза на Camel 4 человека часа с точки зрения переписания сиди пайплайна Это для нуля если мы что-то новое делаем если мы переносим это несколько больше потому что пайпы там оказались не шаблонизированы и со своими нюансиками и того перенос текущего функционала с точки зрения потоков данных занял у нас полтора года соответственно при постоянной здесь важный нюанс при постоянно меняющейся команды То есть это та самая текучка про которую я говорил и тут у нас появилась очень интересная вещь тот самый условный тейк-кью про который я говорил был выделен в отдельный маленький микросервис на базе Spring bootbat щенка и этот самый инструмент у нас опубликовано lot balancere в виде F5 ранее Это был haproxy и по сути нам стало проще чтобы это был просто маленький сервис который занимался взятием Тех самых нужных месседжей из брокера и укладывание его в нужный поток данных видео адаптера и в итоге какую архитектуру получились с точки зрения интеграция которые стала на место фьюза Ну во-первых очевидно как я сказал это Camel со спрингбутом с точки зрения самого адаптера Ну и посмотрев быстро с точки зрения скорости обработки передачи Дана совокупная утилизации мы увидели что все это радость Хорошо живет в контейнерах просто так ставить Standalone нет смысла Их достаточно много они могут спокойно жить name Space поэтому здесь кубернете в качестве регистратора нам помог важно сказать что это не классический ванильный cubernetis поскольку у нас есть гиперконвергентное решение но танец у которого есть нативный карбон в качестве менеджерелки кубера мы используем его потому что гибко модно Молодежная и вкусно с точки зрения опубликованной API мы можем типлейтами просто Нарезать и по кнопке нарезать эти самые кластеры кластера на им спейсы в кластерах и так далее ну то есть как вы понимаете При грамотном подходе здесь у нас и инфраструктуры скот и YouTube и потом можно еще легли от опса прикрутить и будет вообще просто пушка вот собственно по факту мы проблему решили причем Мы решили сразу несколько ее аспектов и по ресурсам и по уникальной компетенции и по заложником стеков и по утечке кадров в случае причем получилось не только с точки зрения того что кадры могут утекать из-за заложников стека и всего предыдущего вышесказанного но и в том числе того что команда оказалась более сплоченной такое тоже бывает какую следующую проблему мы стали решать Но нам нужно решить проблему развития платформы да Мы перешли на что-то новое Но это что-то новое нужно развивать А у нас на секундочку на подходе за спиной еще большое количество новых систем новых потоков и нужно понимать Вектор развития платформы с точки зрения как технологии так и процессов занесения новых потоков Ну и те же самые ресурсы на уровне элементарного Hardware то есть перформанс а в нашем случае самым таким очевидным решением стал рефакторинг этих самых адаптеров Да мы их перенесли мы их сделали Но для того чтобы все было несколько вкусней с точки зрения в основном они касались ресурсов Мы решили провести рефакторинг этих самых адаптеров как вы видите здесь есть интересная схемка это пайпс фильтра с патч Camel То есть это варианты US кейсов при которых может использоваться Camel Ну как бы здесь к кому-то может быть это очевидно но почему-то некоторые Это почему пускают и соответственно если мы рефакторинг все что у нас на базе Spring быта то мы это делаем с точки зрения микросервиса нашего tq так и собственно адаптеры к маловые которые у нас уже крутятся в кубе ли факторим в рефакторинг и в общем-то этот процесс также звонил не один день как мы с вами понимаем где-то refactory проходил достаточно быстро причем действительно быстро то есть Это буквально несколько дней где-то он там немножко затягивался конкурирующие приоритизированные ресурсы по проектам и прочему И постепенно все это делалось и тут нежданно-негадана мы все хорошо рефакторим рефакторе у нас появляются новые потоки и появилась новая проблема причем пришла она откуда мы просто не ждали кто-нибудь отгадает В какой стороне у нас стрельнуло из всей этой радости Есть у кого идея коллеги нет именно мы взяли наше bmmq или просто его задасили то что мы получили мы а три Факторе нашего адаптер У нас их стало больше они стали работать намного быстрее то есть messengine 3 квест там но он достаточно большое количество раз стал меньше сказал до свидания Ребят я полежу и Как вы понимаете это не просто Шутки у нас продакшн так ложился по несколько раз на дню Ну то есть действительно было очень больно И что мы в итоге получили решая одну проблему мы получили что мы забили ресурсами нашей MQ А на тот момент времени мы еще из лицензию платили А лицензия bmm Q считается по ядрам причем по ядрам не виртуальным А по ядрам физическим если вас обеим висит например на большущий военный ферме то вы будете оплачивать всю ферму и вот представляете расширять лицензию по ядрам на всю ферму Ну это в общем-то много учитывая то что может быть и сексом за тысячу Ну и убедились в том то что MQ при таких объемах и его увеличения он достаточно нестабилен И причем Сколько бы мы оперативной памяти не докидывали у нас все равно все утекало в утилизацию ядер а мы стали думать и придумали некую промежуточную архитектуру которую мы продолжили решать в части развития платформы мы поняли что обеим Q надо менять И явными претендентами причем важно сказать что только в части функционала потому что как мы с вами понимаем MQ это широкий инструмент и он может быть использован как и по адаптером с точки зрения адаптеров потоков данных так и с точки зрения брокера очереди Ну то есть достаточно широкий инструмент Именно поэтому он попал сюда на Версус Ну и собственно Кафка как мы с вами помним из моего рассказа ранее Rabbit MQ нам попросту не подошел Потому что есть специфика внутри разработки именно в коде по адаптерам поэтому выбор стал достаточно быстро очевиден если кому интересно повторюсь здесь есть альтернативы их сильно шире Ну больше я имею ввиду но я ранее критерия говорил с точки зрения облаков и прочего что нам не подходит поэтому здесь такой выбор быстрый и стал соответственно выбрана была Кафка и что мы в итоге получили имеем текущий момент времени у нас есть система инициатор У нас есть соответственно тестировщики которые напрямую к нам ходят У нас есть микросервис опубликованный на лоа от балансери который держит достаточно высокую нагрузку по сравнению с тем же haproxy никто не говорит что плохой Он прекрасный но F5 нам просто как корпоративное решение подошло в данном случае лучше ну и соответственно пока у нас переход архитектура мы вынуждены одновременно хранить старый брокер очередей И постепенно затаскивать новый все остальное с точки зрения мессенджмента интеграции в виде потоков данных остается точно так же ну и соответственно мастер системы точно так же и мы ненароком решили еще одну проблему это санкциями darlock помимо проблемы развития архитектуры и в общем-то это хорошо сами того не понимаем мы это сделали теперь что нам предстоит сделать в данный момент времени во-первых нужно полностью перейти на Beam Q накавку и немножко поменять процесс разработки о чем здесь речь когда мы перерабатываем достаточно большой Пласт инструментария с точки зрения техники в некоторых случаях этот случай попал под этот нужно немножко изменить процесс разработки с одной стороны казалось достаточно неплохо выстроенный процесс но мы добавили несколько уточняющих момен верхней части архитектуры проработка потоков данных в том числе добавили обязательный критерий указания по бизнес-требованиям Какая нагрузка планируется Первое Это необходимо для того чтобы можно было примерно понимать масштабирование кластера cubernakes на базе которого все это радость живет ну и второе чтобы не попасть в ту же самую ситуацию с DDoS сам И постепенно наращивать ресурсы в том числе на нашем брокере очереди Ну и процесс соответственно поменять с одной стороны было чуть сложнее но он начал меняться быстрее Ну и переход семью на кафку конечно это еще дополнительная боль которая в текущий момент времени у нас еще в процессе Какую целевую архитектуру мы видим понятно что у нас оставалось так или иначе проблема с точки зрения поддержки архитектуры именно в части поддержки по команде ее сложности уникальные компетенции мало специалистов на рынке и мы в итоге видим целевую архитектуру которая выглядит Таким образом у нас есть инициатор У нас есть наш самый tq который занимается тем что я уже рассказывал неоднократно есть Кафка в виде транспорта есть потоки данных на базе а пачками была со спрингбутом внутри соответственно докер контейнера на базе cubernetis карбон на базе Ну танец немножко громоздко но тем не менее это очень удобно с точки зрения как я сказал того же самого инфраструктуры CD поддержки этого инструментария безусловно есть некий мастер системы теперь по этапам учитывая предыдущий опыт определенную боль все вот того что я рассказывал ранее а ее было немало как под досу так и как я сказал постоянно изменяющиеся команде мы оценили примерно ресурсы мы взяли планирование с точки зрения перспективы разбиения по кварталов и соотношения цели компании ну и соответственно просто слона теперь кушаем по частям как собственно делали ранее это в части перехода с фьюза на кэмул переиспользуя тот же самый опыт с поправками на ветер и какой результат мы получили в текущий момент времени у нас 159 потоков в продакшене используется 30 систем между которыми все это радость происходит нагрузка в среднем идет в части фьюза нас Планк 3000 запросов в минуту здесь важная поправка мы от фьюза полностью отказались в части потоков данных по бизнес операциям как маленький рудимент который остался Еще скажем так до конца его нужно еще вырезать со своими корнями это часть доставки сообщений с Планк по ряду систем где это интеграция еще есть целевую систему просто будут писать напрямую в Планк и Fuse отстреляет уже полностью соответственно 4 миллиона запросов в сутки это только спланк Да вот так вот интересно мы были на той цифры но тем не менее она осталась и все те потоки в том числе новые это 10 примерно 10 тысяч запросов в минуту Пока мало и 4 200 тысяч запросов в сутки пока мало это не пиковые нагрузки это нагрузки такого стандартного дня в Пике она может достигать до 6 миллионов сутки спокойно ну и соответственно в минуту также будет порядка 22-33 тысяч и того что мы реализовали у нас появилась новая архитектура интеграционные шины был выстроен новый процесс разработки понятен Вектор развития платформы Ну там понятно с промежуточным результатами и какими-то такими определенными ретроспективами того чтобы постоянно грамотно выравнивать направление вектора развития ну и внедрены достаточно распространенные технологии которые помогают нам с точки зрения компетенций достаточно просто людей отпускать в отпуск у нас уже не бедная один администратор А мы перевели компетенцию с точки зрения поддержки в подразделения в части себя сиди по шинам а до этого я напомню один человек держал всю инфру все настройки и в том числе делал с нуля CD по адаптером шины мы все эту Радость переняли в себя Ну собственно в том числе поучаство во всех этих переносах созда новых пайпов переписания старых пайпов в шаблонизировав их безусловно Ну потому что как иначе и собственно Это дало Нам тоже достаточно большой Профит потому что ну все научились убрали узкое горлышко Ну и в общем-то определенное счастье возникло в головах но мы понимаем что это еще не последний этап и учитывая то что целевая архитектура еще не реализована в виде полного ухода от MQ до Кафки нам что еще нужно сделать И на что вам в том числе при подобных итерациях перехода не только в части интеграционной шины но и перехода с одного инструментария на другой стоит учесть первое учитывайте текущие ресурсы и развитие платформы у вас всегда должен быть определенная планирование с точки зрения релиз ноут конкретных компонентов любой платформы с точки зрения того инструмент он вообще живой Или он там через полгода за депликетится или вообще перерастет во что-то другое купит какая-то другая компания и это штанишки что-то другое перестали быть опенсорством и так далее за этим Нужно следить постоянно дальше планируйте нагрузку не на общую систему она каждый ее компонент это важно Ну по крайней мере вы не столкнетесь с тем же самым дедосом который получился у нас Ну и как вы понимаете помимо планирования нагрузки на Каждый элемент системы важный Аспект До которого мы по определенным причинам из-за отсутствия ресурсов не пришли сразу это нагрузочное тестирование причем нагрузочное тестирование как на всю систему так и на каждой ее компонент чтобы четко понимать что вот при такой нагрузке нам станет Плохо где-то там решится с точки зрения проработки архитектуры на базе горизонтального масштабирования по адаптерам потокам данных где-то это решится в части Кафки будущей по ресурсному составляющую Но вот это у вас виде должно быть дальше не Хотя людях и стеке человеческий фактор никто не отменял человек может закопаться в этот бедный непонятный стек и сказать Все до свидания Ребята а вы в итоге останетесь если в том числе это не документировали без документации без компетенции и потом это будет очень больно со всеми вытекающими Ну и в том числе при выборе альтернатив пишите опрос листы У вас должны быть четко составленные критерии того что вы хотите решить Ну как вы сами понимаете зачастую при выборе альтернативного инструмента У вас есть понимание Потому что есть из но может быть у вас есть еще какие-то проблемы хотелки которые вы также хотите закрыть за счет этого нового инструмента Составьте опросный риск со всеми участниками команды продумайте его по бренд штормите и вот на базе этих самых критериев уже Идите на research по рынку это вам поможет избежать достаточно большое количество проблем и в том числе у быстрый процесс выбора конкретного решения Ну что На этом У нас все вы видите определенный набор контактов это такой чатик с мануалами когда постоянно публикуется доводства в телеграме мой github на собственно где я всякие интересные скептики публикую в том числе интересные скрипты по авто легкингу из жиры по зафеленному диплои на базе пайтона и прочее Ну и ютубчик канал где публикуются различные выступления в том числе впоследствии появится это собственно подписываемся ставьте лайки колокольчики Спасибо Спасибо большое тебе Ну что мы готовы ответить на вопросы Я уже даже вижу поднятые руки Давайте вот самые дальний пока самых дальних рядов начнем Спасибо за доклад вопрос при переходе был тайм и теряли вы какие-то сообщения То есть как бы построен процесс перехода с одной технологии на другую потому что достаточно тяжелый процесс там на самом деле было намного хуже по сообщениям улетали на старте достаточно большом количестве в детлаторы и это было прям очень больно и из дедлайтеров там очень сложно уже потом что-то достать чтобы это сообщение было живо как Костыль у нас изначально было проработан сначала мануальное потом автоматизированный механизм который позволял вытаскивать из дедлатера сообщение и потом перенаправлять их по собственному по циклу чтобы он это сообщение не терялось собственно впоследствии так вот это самый маленький микросервис такими появился хорошо последующего вопрос новую в дедлайтер именно в кролики до их ловили Кафка вам помогла с этим я так понимаю Ну вот да потихоньку помогает потихонечку потому что еще в процессе Виталий компанизон у меня такой вопрос не подумайте что я такой фанат рэббита просто вопрос просто первый раз Вы рассказывали что отребет отказались из-за того что адаптеры приходилось переписывать да то есть из-за ресурсов Я так понимаю Второй раз когда с кавкой выбор стоял тот тоже как бы из-за этого вроде на там ну Какое отличие нюанс исключительно в компетенции а именно их отсутствием при полном отсутствии компетенции с точки зрения нисколько там аналитики по брокеру очередей Сколько именно Ну поддержка чистая поддержка то есть уметь инструмент заготовить с нуля А поскольку все-таки у нас Казалось бы до полтора года но некоторые вещи если докомпозировать проекты они длились намного более сжатые сроки и резко человека переобучить на тот инструмент о котором он не знает ничего от слова совсем но это тоже дополнительная боль время и не выдержка сроков это тоже Вторая причина почему собственно MQ не зашел Спасибо и маленький еще вопрос можно пора и вас Кафка существовали какое-то время одновременно Да я так понимаю они сейчас архитектура чем это связано что почему нельзя так скажем Ну не одномоментно перейти но не то что прям совсем рубильник Ну то есть вас хранятся сообщения в топиках долгое время или то есть процесс перехода вы не ускоряете потому что Ну как бы никто не отменял историю с ресурсами конкурирующий проект переприоритизации и так далее и здесь основной фактор это именно вот человеческие ресурсы человека часы те самые и то что удается делать в том числе за счет проектов Ну там когда новые появляются потоки это мы делаем А вот когда это становится бэк-логом но тут Сами понимаете это исключительно приоритизация поэтому это всегда растяжение по времени понял спасибо большое ребят еще вопросы рубильник на полтора года цифру которая это невозможно Спасибо за доклад о такой вопрос Когда выбирали в качестве integration фреймворка апачками Вы же одновременно вводили там спринг бутспрингбача а не рассматривались в качестве вот альтернативы плюс опять же семейство спрингл соответственно уже в команде будет экспертиза в одном месте Вот это как вариант там множество из коробок так же самое есть адаптеров расширений и прочего прочего здесь Было определенный нюанс у нас помимо чистой Java существовали батчинг сервисе на Spring Boot и просто переиспользовать опыт бочовых сервисов на спрингбуке было намного быстрее проще чем в integration закапываться просто именно время в микрофон ада лучше дай наверное Может неправильно сформу не бачу альтернатива пачками у нас Принт игры такой вариант Нет мы не рассматривали он у нас по моему по критериям просто не прошел По нашим мы когда просто составляли Ну понятно что я его прям Весь ты сейчас не публиковал это было бы очень громоздко мы брали все те критерии которые нам необходимо вот по всему тому что я сказал То есть это конкретика по ресурсам отказу устойчивость возможность дирпишки всего всего остального Но просто по какому-то из критериев отвалился еще на первой стадии ресерча Да спасибо за доклад Я избираю у нас история Один в один тоже от фьюза с кэмелом Мы в итоге пришли к тоже сейчас есть обе МК параллельно войцекавка и микросервисы камлоски нас примбуте вопрос Вот про сохранение и про рядовивере Вот вы упомянули в начале есть какой-то самописные да микросервис а если вот на уровне Комаровского микросервиса вот справа отвалился то как вы сохраняете сообщение есть такое ну во-первых такая ситуация может быть один или два раза все время было во-вторых у нас остался тот самый поток который я ранее говорил сейчас покажу во-вторых у нас стрелочка периодически в случае необходимости идет от инициатора напрямую Ну то есть в необходимых случаях мы просто также продолжаем миновать транспорт то есть от take you он у нас может ходить также в транспорт так и напрямую в messaging Спасибо Так ну что если вопросов больше нет то настало время выбрать лучший вопрос который тебе показался наиболее релевантным к твоей теме коллега который коллега идите сюда мы вам подарим подарок от Озон тех за самый лучший вопрос про боль и вот это все спасибо большое за вопросы Я всегда люблю чтобы это был не монолог а диалог просьба к вам сделать обратную связь по докладу Да оценивайте доклад нетворкайтесь докладчиком У нас есть отдельная Зона для этого можете ему задать какие-то вопросы если вдруг вы стесняетесь делать это публично Спасибо тебе огромное это всегда сложно и тяжело когда мы берем и избавляемся от чего-то к чему привыкли и перестраиваем практически полностью весь наш продукт для сопротивление может еще быть при этом"
}