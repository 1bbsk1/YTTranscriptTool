{
  "video_id": "e5sBhPlVmEM",
  "channel": "HighLoadChannel",
  "title": "Как мы шли к 5000 RPS на запись / Ян Силов (Ozon)",
  "views": 4210,
  "duration": 2886,
  "published": "2025-01-17T02:35:18-08:00",
  "text": "Всем привет Меня зовут яан Я отвечаю в азоне за разработку систем по контролю и тарификации курьеров в разработке суммарно 7 лет 3 года непосредственно управления Сейчас у меня уже там целое дело На 30 человек собственно приступим сразу к делу что вообще из себя представляет наша система и О чём будем говорить и так далее система - это непосредственный материальный контроль работы курьера То есть то что курьер прямо здесь и сейчас в моменте везёт то есть нас не интересует типа что происходило с предмет интересует что конкретно курьер сейчас везёт как это бизнесов устроено У нас есть склад на складе в течение ночи упорно все стараются подготавливают и формально выглядит как что на складе посылок которые вот ждут когда же их курьеры заберут курьер курьер собственно приезжает на склад забирает посылочку и мы в этот момент такие Ага вот наве на тебя товарную задолженность знае курьер всё это дело развозит по городу где-то что-то вставляет пункты выдачи заказов что-то до клиентов что-то в постоматы закладывает что-то в другие склады достав мостат сго за сва ты нам больше ничего не должен и в зависимости от успешности насколько он правильно это сделал или может быть неправильно то есть мог привести не в тот за которой вы просили Всякое бывает вот ему рассчитывается вознаграждение технически это пример следующим образом выглядит То есть у нас есть сервис агрегатор и некий бизнес-сервис В чём суть агрегатора агрегатор - это просто сервис который читает 15 источников данных может ну плюс-минус короче а читает большое количество источников данных которые никак друг с другом не синхронизированы просто вот система не знаю там пункт выдачи что-нибудь принял какую-нибудь посылочку он сказал мы всё приняли А бизнес-сервис он как раз в себе содержит всю бизнес логику потому а как нам непосредственно что-либо на курьера зачислить А как списать А что делать если вдруг нам пришло зачисление списания до этого не было и куча всякой такой логики и в качестве Как скажем информации для внешнего мира есть топик в который публикуется сообщение о том что что-то произошло внутри то есть что-то навесили что-то списали в чём особенности Да ограничения во-первых все вот эти источники которые нам пишут они вообще никак друг с другом не дружат Точнее они никак друг с другом не синхронизированы то есть никто никого ждать не будет когда там какое-то событие на складе произошло тогда и заслали второй момент - это сам пик нагрузки Он приходится на 1 час в день Ну плюс-минус опять же можно по графику увидеть что в районе 9 утра самый-самый пик что это такое Это вот как раз курьеры со там по всей стране приезжают На склады и им на складах делают как раз выдачу всех посылок что они должны доставлять дальше тоже по графику можно увидеть в течение дня это всё они плюс-минус в каком-то нормальном распределении развозят то есть там вот как раз с 10 до 6 вечера все посылочки доставляются и самая главная особенность то что почти вся нагрузка - это пишущая То есть у нас на чтение нагрузка тоже она есть но грубо говоря там в районе там нескольких РПС там даже говорить не о чем в чём ещё нюанс в том что у нас есть SL SL вообще то есть грубо говоря мы должны моментально по факту обрабатывать то или иное сообщение Но вот самый-самый потолок Для нас это 15 минут Почему такие ограничения есть быстрая доставка Озон Фреш в Москве особо распространена и курьер Уват там даже быстрее чем за 15 минут что доставить до клиента и там могут быть возникать нюансы если мы не успеваем И самое главное с чем собственно и боремся и боролись и будем бороться это Озон достаточно быстро раст а и нагрузка на сервис она растёт кратно от года к году То есть грубо говоря то что мы там готовились например там не знаю сезон год назад сезон распродажи там одиннадцатое одиннадцатое то сегодня это может быть абсолютно обычно повседневностью следующий сезон естественно будет сильно жёстче и сильно объёмнее так кликер кликер Вернись во да Что будет если мы Не справимся Ну во-первых ну Самый наверное минимальный ущерб который может быть это Клиент не узнает кто им везёт посылку что это переведёт Ну клиенты не смогут позвонить курьеру Не узнают кто им везёт не смогут позвонить сказать привези пораньше попозже или я сегодня не могу или выписать Ну основное это выписать пропуск на проезд на территорию дома по что всех там как правило шлк Баума или там это территория организации там требуется заранее выпи пропуска во-вторых значительно повышается вероятность нарушения порядка событий как говорил источники не синхронизированы то есть грубо говоря если нам вперёд приходится выйти о том что курьер посылку приняли на пункте выдачи Мы у себя не находим что эту посылку кто-либо вообще когда-либо Вёз и возникает довольно простая дилемма он её и не должен был вести или всё-таки кто-то её Вёз и мы вот тут вот раньше события поймали изза того что посло миллионы мы не можем тантро имини сверх проблематично вести проще нагрузку держать чем механику такую изобретать и к чему может привести к тому что Ну если у нас будет нарушение порядка курьера может не зачес или неправильно заче выполнение задания то есть у курьера задание Доставь что-нибудь куда-нибудь мы не дождались и можем курьеру просто не заплатить курьеры довольно ревностно относятся к тому сколько они как не зарабатывают поэтому это вот довольно критичная штука Как понять куда мы движемся Ну в первую очередь нагрузочные тест как мы их вообще проводим в первую очередь мы останавливаем все коню все топики мы перестаём читать ждём Когда у нас накопится ла накопится как можно больше сообщений потом всё это дело включаем обратно и наблюдаем смотрим Как ведёт себя система как она всё пережёвывает И как себ чувствует на что котрим здесь нас интересует несколько вещей первое сам само количество необработанных сообщений То есть тут можно увидеть что в 8 утра мы тест нагрузочный начали 8 20 примерно Да всё это дело обратно включили Что всё это дело рассосалась что тут Ключевое Ну во-первых для нас есть некие скажем триггеры пони что наруч ва прошло больше там 10 минут там 15 минут Мы обязаны всё включить обратно второй Триггер - это какой-то лаг то есть грубо говоря если мы понимаем что у нас скопилось слишком много сообщений Это уже имеет риск уйти в Неуправляемый историю Мы тоже всё включаем обратно ну и третий Триггер который ни разу не срабатывал это если вдруг операции вдруг что-то заподозрили то есть где-то в полях кто-то что-то заподозрил для нас всё это плохо Тут нас интересует две вещи первый сам лак его наличие второе Как быстро всё это дело разгребает в идеале должна быть там чуть ли не вертикальная линия вниз если вот она долго долго долго долго разбирается то это уже признак проблем следующее - Это количество реально обработанных сообщений То есть это означает что если у вас в топике Ну вы mess неважно А например миллиард сообщений скопилось и вы это лак разгреби там за 1 секунду То это не означает что вы обработали миллиард сообщений Это означает что вы так скажем офсет сместили миллиард сообщений Но в реальности вы могли там обработать не знаю одно сообщение из этого миллиарда Вот и вас интересовать должно Вот именно количество реально обработанных сообщений а не размер лага а здесь на этом графике нас будет интересовать так называемые полки Что такое полка это так скажем такая воображаемая линия горизонтальная в которое Наш график каким-то образом упирается Ну на данном графике это в районе там 300-400 видим что вот примерно вот такой вот скажем количество сообщений Мы в Пике можем обрабатывать в минуту Ну это ладно следующий момент входящий ПС это мы его засчитывает ходит в бизнес-сервис вот со стороны бизнес сервиса выходящий замеряем тоже одно сообщение не обязательно генерирует один вызов поэтому тут не обязательно прямая и тут опять же Нас интересуют те же самые полки по факту Ну и по факту Ну второй момент то что одна из ключевых наверно метрик за которой мы и смотрим то есть какой у насс бизнес-сервис способен выдерживать то что агрегатор чуть сильно Ну агрегатор сильно легче тюнить дальше Расскажу как и последний график который мы тщательно смотрим это respons Time по кванти что это такое то есть у нас есть жёлтый график там rps и синие графики - это вот ответ по квантили а для нас наверное целевая картинка - это чтобы под высоким rps у нас не происходило значимых деградации системы она в любом случае будет в любом случае respons Time Когда у вас нет нагрузки будет сильно ниже Вот Но тем не менее Нас интересуют Вот как раз какие-то резкие выбросы и ещё один очень важный момент это то что когда вы смотрите на графике rps или респонс таймов и так далее эти все графики они всегда усреднения как вообще снять там не знаю нагрузку там в в Псах и так далее обычно берётся окно в минуту берётся количество сколько у вас запросов за минуту произошло делите на 60 получаете ваш сам этот RS Ну там в какой-то точке если мы на графа это рисуем по секундам естественно никто не рисует сер но слишком часто собирать метрики тяжело и это может породить некоторые искажения то есть что за искажение например вы в течение минуты 10 секунд держали миллиард FS вот пря миллиард миллиард оставшихся 50 секунд просто не было трафика потому что у вас Ну кончились с точки зрения Нару тестирования патроны да или вот просто не было Никой Наки сред будет тельно ниже этого Милли способна это миллиард держать пусть не долго но способно Вот это к тому что когда вы проводите нагрузочный тест важно чтобы э нагрузка точнее вот этих патронов да Или сообщений в Кафки вам хватило вот иначе вы будете видеть не то что хотите или может не то что ожидаете что ещё смотрим это трейс с точки зрения вообще на что опираться с чего начинать проще всего смотреть самые длин по времени чем он длиннее тем интересней в нём мы ищем какие-то узкие места а чтобы узкие места было легче искать надо его делать достаточно подробно чем больше снов Чем больше детализации тем будет вам проще понять Вообще что конкретно тормозит и почему и небольшой дисклеймер ещё всё что мы делали это мы делали в течение лет Нару тестирования то как мы замеряем эффективность и так далее и далеко не все графики у на сохранились поэтому что-то было нарисовано грубо говоря от руки по памяти вот поэтому тут Если что имейте в виду Итак приступим к оптимизации оци у нас Бут через проблемы и проблема первая целый комплекс У нас есть метод на запись да и при его выполнении мы по Спан видим что большую часть времени мы ходим в какой-то другой сервис забываем какую-то информацию как бы мы не игрались с нагрузкой вот эти вот 80 миллисекунд с предыдущего треса Да мы никак побороть не можем Ну то есть грубо говоря с нашей стороны мы нагрузку убрали А кофлет как был Локи так и остался что-то со стороны команды котора нам даёт какую-то информацию Они тоже ничего сделать не могут там под капотом Суровый leg он никак ускориться не может и ещё одна проблемка которая есть это по Метрика база у нас огромное количество transaction Ну тут может быть 39 не выглядит огромным но надо понимать что у нас потолок соединений в районе 4 и вот 39 - это Огромно Что такое Это вы открыли транзакцию под транзакции покат вас ОТК пер по транзакции вычитали какую-то запись сходили узнали у другой системы всё ли окей Не окей а потом уже транзакцию закрываете соответственно пока у вас транзакция открыта у вас занимается коннекшн Никто этим конек воспользоваться не может и Ну если новые запросы приходят они упираются в то что им не хватает коннекшн они такие Ну мы подождём когда кто-нибудь освободится А вы вместо того чтобы делать какую-то полезную работу сидите ждёте Когда вам какой-то там сервис ответно проделали Ну опять же там комплекс проблем решали Да во-первых вынесли всю историю изменений То есть то что курьерам когда-либо выдавали что они либо возвращали и так далее в отдельный микросервис То есть это такой бизнес бизнес сервис аудит условно называется что нам это позволило Ну типа что нам это помогло решить во-первых в самом бизнес-сервис пропала вот эта табличка аудита в ЧМ проблема таблиц аудита они бесконечно растут да Вовы они бесконечно растут вместе с ними растут бесконечные индексы и вместе с ними происходит постепенная деградация инсертов в эти таблички Ну то есть Гру больше аудита дольше вставлять Ну да Аудит в рейме особо никого не интересует Ну вот отдельно живёт следующее это перестали добывать информацию для других истори опять сложилось что кода и так далее А мы так скажем были довольно добры к своим соседям и например вот мы шли добывать информацию там вот курьеру выдали посылку нас интересует что в этой посылке лежит вот мы идём за информацией что в посылке лежит а рядышком лежит информация о каких размеров эта коробка нас говорят Дайте нам размеры коробки Ну вы же всё равно ходите там вот добывает информацию в состав там рядышком размеры лежат Дайте нам Мы такие ну Нам не сложно Вот к чему это привело со временем естественно появилась более упрощенная ручка получить состав более упрощенный метод получить состав А мы не можем на неё переехать потому что мы вот тянем вот этот вот эти габариты зачем они нам нужны и так далее роче по отказывались от этих всех схождений следующее это убрали в методах непосредственно все ходки во внеш сист это выг есть чи сообщение свки как-то это нормализовалось внешних систем и потом только уже делал какую-то реальную обработку всё это перевели к системе следующее что агрегатор читает сообщение обогащает данные только Ну только так как надо вот это вот паре сервисов нормализует уже гото Да бизне ру говоря вне кроме как от самого себя Ну где не смогли убрать по каким-то ограничениям опять же бизнес логики Там просто по вытаскивали всё это дело из под транзакции Ну вот этот комплекс решения нам позволил во-первых респонс ставим сохранить в 2 с по раза Ну и это естественно привело к тому что максимальный ПС который мы стали мочь держать тоже вырос там в д с копейками раз сложность здесь высокая не сколько Инженерная сколько управленческая Ну то есть э самая сложная часть была Договориться с сервисами потребителями наших данных что мы теперь всё Мы для вас никакой информации добывать не будем добывай это всё сами планируйте все задачи ставьте куда-то планируйте Потом ходить говорить А почему вы задачу это не делаете И так далее То есть самая типа большая сложность это вот именно вот следстви от кабалы добывание информации для кого-то так я не понимаю понимаю куда вот вторая проблема деградация на ровном месте что у нас происходит по Метрика респонс таймов Мы видим что под нагрузкой как раз те самые 9:30 утра да А наши немногочисленные Рит меты начинают прямо сильно-сильно деградировать вот на графике видно в три раза это ненормально в тсах начинаю смотреть видим вот такой вот пробе с точки зрения пробела начинаем изучать что там же такое может быть Вроде мы сходили в внеш получили какие-то данные А дальше там не знаю присваивания какие-то создание новых объектов и так далее но больше там ничего и нет ти нечему тормозить назад это дело изд транзакции убирали Какие ваши версии что это может быть сразу дам подсказку это и Второй ответ довольно короткий ещё раз так Колек нет правильно вообще да выделяется с полу потоков он как правило ран количеству виртуальных ядер сколько выделено по работа приложения если какая-то задача приходит ей не хватает потоков в этом ТПУ то он сидит Ну типа dnet сидит ждёт когда же 500 миллисекунд пройдут в течение 500 миллисекунд ни один из потоков не освободился Он такой ну ладно держи плюс о плюс о п вот если вдруг вот этих потоков количество их выросло и очень долго они не утилизируются то соответственно сжимается обратно ну или до минимально необходимого или того что вообще выделялся при старте приложения и самый страх происходит когда у нас приходит слишком много зада последни пингвинчик будет ждать почти 8 секунд когда же он сможет собственно начать выполняться вот эти вот п которые мы видели х раз вот этим всем делом вызвано что мы собственно проделали во-первых У нас не было понимания что это именно он мы долго искали добавили метрики на размер ла для всего озона добавили никто раньше с этим не сталкивался даре раз пока что в какой-то момент времени у нас приходит пик нагрузки и слишком много задач скапливается в таком вот ожидании следующее решение оно было там максимально быстрым и простым есть настройка Вот её задаёшь и всё идеально работает но это есть нюанс нюанс в том что потоки все знаем они не бесплат содержание потоков тоже про на врем при и убрали тюнить руками можно но временно Ну сложность супер низкая Ну потому что дело недолго с точки зрения лечения причин здесь скорее найти причину Но в данном случае у нас ВС выров следующая проблема незначительный релиз сюрпризом как это выглядит выкатили релиз и мы перестали справляться с нием одного из топиков пом всё замечательно никаких проблем там нету ничего не тормозит Всё как было так и есть пон тайму тоже всё замечательно ничего пора от релиза ВС лечит мы потом такие может Случайность накатим ещё раз релиз снова проблема вернулась откатилась снова вернулась откатились спрятано Что случилось мы читали Один из самых-самых таких рнх рнх топиков там по сотни тысяч сообще в минуту идт читаем в этом реста изго для каждого сообщения мы что делаем выва обрабатываем потом говорим комит что такое комит Мы в каку говорим что сообщение обработали давай нам следующее и внезапно оказался не бесплатно операция Вот так да Ну что собственно сделали во-первых добавили метрики на мало того что комит оказался не бесплатной операцие так ещё и операция продюсер тоже ни разу не бесплатный операции и несколько раз мы об это тоже Спока второй момент добавили в Трис всё-таки спанк маленький Вот он А который показывает сколько времени у нас собственно этот комит занимает это нам позволило собственно наглядно видеть что вот он оказывается не бесплатный и в тресе он действительно имеет место быть как лишнее напоминание и увидев что действительно там проблема скорее всего с этими котами мы сделали довольно простую оптимизацию это перешли на режим автокот В чём заключается в том что мы когда вычитали обработали сообщение мы фиксируем это смещение что обработали сообщение локально А сам комит непосредственно в ка который уже сильно дороже проходит асинхронно из коробки поэтому типа сам фикс выглядел как буквально не знаю Три строчки кода поправить чтобы автокот поддержать так я не понимаю куда щёлкать Давайте туда четвёртая проблема число партиции не по нац здесь немножко расскажу как устроен вообще кавка и коминг в НМ есть топик топик - это просто шина данных есть некий продюсер который в топик что-то пишет и комер кото вот топик читает в топике может быть несколько партий партиции - это такие можно сказать логические части внутри топика сообщение попадает в одну из партиции Но типа глобально это всё вот именуется одним током партиции достаточно легко добавлять но к сожалению или Трудно или невозможно удалить удаление обм остановить всех коров да вы должны конв всех продюсеров Все подождите Мы полностью удаляем топик создаём новый с меньшим количеством партиции и только потом всё включаем обратно я думаю по описанию понятно что в продакшене абсолютно никто и никогда этого делать не будет следующая особенность то что читать кто угодно в рамках группы скажем Сообщения типа будут на каждую группу транслироваться следующий момент это в то что у нас внутри одной группы может быть несколько коню как правило точнее так обычно или не знаю как это се сформулировать коню он плюс-минус обычно это вот один под приложение Вот соответственно если у нас коню оказывается меньше чем количество партиции то партиции равномерно размазывается по конс сюра Здесь тоже момент что одной парти может быть только один консьюмер в рамках группы А если у нас соно коню становится слишком много то кто-то из косме начинает бездельничать в чём проблема-то проблема в том что все топики абсолютно разные в каком-то ри в каком-то 10 в каком-то п и так далее соответственно количество поддать довольно тяжело и мы получаем ситуацию что у нас поды нагружены неравномерно то есть грубо говоря что не знаю один под загружен на 100% а второй загружен на 3% просто потому что ему проти не хватило и в любом случае рано или поздно мы начнём опираться в обработку одного сообщения То есть мы вычитывает и чтобы уть пропускную способного довольно примитивную дилемму которая выглядит следующим образом сначала добавить партий потом добавить подов обыч две кнопки одновременно нажимается ти такая лайтовая оптимизация и так далее но сами по себе партиции для брокера глобально не бесплатно и с точки зрения что их расширить расширить можно но потом мы их не сум Если вдруг у нас зна обработка вдруг каким магическим образом ускорится какие симптомы вообще такой проблемы То что вы начинаете упираться именно в то что вас консьюмер не справляется обычно это то что внутри кон сюра есть одна из жирных частей это обогащение данными То есть вы выли сообщение Вот для этого сообщения чем-то обогатились потом пошли обрабатывать собственно чтом как может не дружить с количеством подов Мы научились в рамках одного пода поднимать несколько консьюмер с точки зрения кода довольно очевидное и простое решение но с точки зрения дойти до этого было не всё так быстро и легко а во вторую очередь внедрили чое обработчики как это выглядит вычитывать сразу пачку сообщений всю эту пачку за один раз за одну ходку или там за кратное меньшее количество ходок обогащаем и потом всю эту пачку в параллель пиха в обработку как раз по вид что бизнес все запросы там параллельно улетели и собственно в конце уже когда всю эту пачку обработали сделали там комит самое главное не переду рубо говоря бизнес-сервис надо не задавить Потому что если Вы слишком сильно Задави то от этого вы скорости не выиграете то есть здесь нужен такой баланс между Ути во сложность средняя по причине как раз таки Бах обработчиков кто пытался отписать там довольно скажем фундаментальная проблема то есть вы вычитали пачку сообщений вам важно её всю обработать и потом из этой пачки вы половину смогли обработать а вторую не смогли вопрос что с этой пачкой делать комит типа смещать недо чтото сдела этой вот увше Пай вки dlq и там прочая механика как вам не нарушить консистентность данных и последняя проблема Тут даже с небольшим спойлером но Кокше конеч как это выглядит на графиках Мы видим что под нагрузочный тестом у нас снова это уже пишущие методы под нагрузкой вырастает заметно вырастает по рисам Мы видим что самое типа больше всего времени занимает открытие коннекшн к базе смотрим в метрике базы видим что количество коннекшн у нас в потолке То есть у нас 44 Это для нас потолок Ну в рамках нашей инфраструктуры и вот мы упёрлись что делать что делать Вот Что делать Ну нано оптимизация возможно тут нано оптимизация уровня а как нам сделать так чтобы коннекшн расходовали вот не так уж и много и дошло до Таких вот супер мега нано оптимизации уровня А давайте мы Select Delete и insert закинем в один стейтмент чтобы это всё улетело за один раз в ак чтобы не тратить время на то что мы там по сети гоняем какую-то команду базы по сети ждём от Базы ответ и так далее ну то есть это уровень уже такой-то ове инжиниринга и Бреда других выборов У нас не было собственно что сделали Мы пришли как раз к шардирование одну баску мы одну физическую базу заменили на целый кластер во вторую очередь переписали всю бизнес логику нашего бизнес-сервис по шардирование это даже уже знаю графиках раньше было и выше 5000 Ну да ладно смогли добиться 5000 вот при этом Рен не сильно и вырос с точки зрения сложности это прям архи сложно вот с точки зрения чего с точки зрения того что когда у вас есть одна единственная база и она не шардирование вы этого лишаете И вот здесь уже как раз начинаются всякие и так далее собственно подводим итоги Ну во-первых Мы научились проводить нагрузочное тестирование на запись на продакшене важный момент что на же нагрузочные тесты на запись они тоже позно пое деградировали сколько мы выдержим на проде он никогда не ответит напомню как мы проводим останавливаем коню копим лак всё это спускаем обратно наблюдаем второе вы провели архитектуру взаимодействия систем это наверное самый сложный этап с точки зрения договориться убедить запланировать и так далее внедрили бачо обработчики автокот переписали сервис на шардирование и этим самым получили возможность почти неограниченного масштабирования что хочется сказать напоследок да то есть первое проводите нагрузочные тест ваших систем возможно возможности вашей системы не так высоки Как вам кажется и предел вашей системы не так далёк как вам кажется следующий изучайте трисы в них довольно много полезной информации опте своё время если ваша система выдерживает нагрузку вы прогнозирует не надо в это сильно лезть собственно У меня всё всем спасибо оценивайте доклад задавайте он ещё не закончил уже рука была поднята смотрите на первом ряду Ого либо фанат либо хейтер Но точно неравнодушный кто-то правильно микрофон чтобы голос звучал по-мужски Да добрый день Николаев Павел руководитель инфраструктуры Альфабанка на самом деле очень круто Я фоткал всю презентацию Вот и мы со стороны Эля вот с подобными высоконагруженные фичами начинаем только сталкиваться очень интересно как устроена вообще работа с трессами и вот э постоянная обратная связь посмотрели на трейс поиска узкие места поправили посмотрели снова на трейс Да Как у вас это устроено у нас отдельно у нас сидят нагрузочные тестировщики отдельно псы моли инженеры и вот с ними Ну вот так вот да то есть это не на кончиках пальцев связь Как устроено у вас разрабы являются нагрузочными одновременно а здесь Да но с точки зрения кто это нагрузочное тестирование по утрам проводит мы всё пытаемся в Q это спихнуть но формально этим занимаются разработчики то есть грубо говоря там разработчик раз в неделю встаёт В 8:00 утра чуть раньше обычно да и вместо чашки кофе проводит манипуляции чтобы на проде провести Вот эту нагрузку на запись это первый момент второй с точки зрения кто т изучает и кто за этим бдит по факту тоже разработчик грубо говоря у нас есть таргеты по нагрузочному тест в тагет не прошли и лезем по факту В ресы начинаем изучать А что же там у нас ну типа что тормозит Почему мы не выдерживаем нагрузки и пытаемся уже раз это узкое место искать у рисов есть нюанс в тсах можно увидеть то что ты хочешь увидеть Ну то есть грубо говоря есть девяносто девятый квантиль требования от системы как правило дено дей квантиль должен выполняться там не знаю 100 миллисекунд если один единственный запрос выполнялся 15 секунд нас он не интересует потому что он один но в тсах мы будем роваться Кто у нас больше секунды выполнялся и увидим вот этот самый 15 может быть он действительно покажет какую-то проблему а может быть и не покажет Ну то есть тут надо вот аккуратно короче в это смотреть Спасибо большое пожалуйста первый ряд справа так Привет Ян Спасибо за доклад Очень интересно Много чего вспоминается вот в докладе звучал Утверждение что вы на живой нагрузке экспериментирует то есть Притормози и очередь набрали А потом вы говорите про патроны который это искусственное какое-то нагрузочное тестирование Как именно устроен вот как бы где они в процессе появились Да смотри слово патроны - это так я как говорится образно сказал Да как говорят классики А короче патрон Это скорее просто для общего понимания что это такое для нас Патрон - это те вот самые накопившиеся сообщения в кафке Ну то есть мы их там не генерируем никак Вот они там естественно накопились и когда мы запускаем всё обратно то вот вот эти эти сообщения полетели для нас и не есть патроны и тот момент важный опять же что этих сообщений должно быть достаточно чтобы хотя бы в течение минуты нагрузка держалась если они быстро гребу Но мы ничего не увидим так пожалуйста Да спасибо за доклад у меня такие более организационные вопросы их два Вот вы сказали что вы тестирует на проде Вы в катке там перерыва поток но все мы знаем что баги после релиза они могут только при нагрузке возникнуть и вот Представьте ситуацию что вы топик открываете А водичка не льётся вот была ли у вас такая ситуация если была то что вы делали если не было то наверняка вы оценивали риски такого тестирования что бы вы делали это первый вопрос а второй вопрос перекрыв поток кажется что на нагрузках озона это огромное количество денег потраченных это согласовывали а если как то почему вообще это происходит по поводу то что под нагрузкой баги всплывает в естественном мире нагрузка она не сильно-то и меньше вот так Ну есть грубо говоря у нас нагрузка есть она в люм случае постоянно Если вдруг какие-то баги под нагрузкой есть то мы об Ну опять же с бизнесом у нас договорённость что Фреш курьер который там самый самый самый быстрый вот за 15 минут по идее он быть может ну типа скорее всего не успеет доставить и вот эти 15 минут у нас вот есть а с точки зрения что там да какой-то Супер там флеш курьер успел доставить что-то и выдать клиенту за эти 15 минут Да это какая-то там потеря но потеря будет на том что операции будет руками про что курьер вки доставку выполнил вот один клик операции Ну это рубли в штуках Да вот не в тысячах поэтому тут договорённость такая Спасибо за доклад показалось что продукт важен и как он обеспечивался можно ещ обеспечивался exactly Once А да exactly Once достаточно важен здесь механика Ну как это сказать Ну это давай так на уровне бизнес логики То есть если что-то выдано курьеру например а там не знаю в 11:10 и ещё раз приходит что тот же самый предмет выдаёт курье этому же курьеру в 1110 то Ну для для нас короче не потент работает на том что э мы подобные события уже обработали вот вот если это операцие списание то есть курьер что-то доставил то в первый раз это операция обработает Второй раз мы уже не найдём никакой предмет естественно опять нути второй раз не обработает а второй момент - это то что есть механика специальная табличка которая припоминает за последние там по-моему 10 минут окошко небольшое Вот какие вообще запросы уже обрабатывались для чего это надо это комер любит точнее кавка любит ребалансировка сделал ребаланс и у вас другой Ну типа другой пот начал этот же запрос обрабатывать и вот здесь вот чтобы Ренди не происходило никакого как раз вот и специальная табличка которая припоминает что сейчас обрабатывала недавно Спасибо за доклад Да очень интересный У меня два небольших вопроса Первый э вот когда вы останавливается кон сюмин Что происходит с клиентским опытом и Насколько часто Ну вот такое нагрузочное тестирование проводите А сейчас этот Тест проходит раз в 2 недели а ну то есть в плане раз в 2 недели Ну и из-за того что вот мы там все оптимизации Какие могли вроде бы как сделали вроде всё хорошо себя чувствует вот это раз в д недели проверяем что точно ещё всё хорошо а с точки зрения клиентского опыта а Ну опять же в данным SL клиенты и пользователи нашей системы не успевают заметить что что-то пошло не так а негативно какое-то поведение Но это разве что какие-то операции может быть на складах могут на 15 минут э не знаю что-то заподозрить вот да второй вопрос что вы для сбора метрик используете Ох сейчас это для сбора метрик м в кулары да давайте в ларах я не это я не лично Виктор корейша Запомни это имя будешь потом па назвать Да пожалуйста сечас сечас секун Здравствуйте спасибо за доклад в принципе уже задали вопрос коснулись уже насчёт того что тогда просто комплимент сделай Да да я хотел спросить не думаете ли вы что было бы безопаснее например ночью генерировать нагрузку искусственно чем в сай пиковый момент делать Ну какие-то рискованные операции Да смотри хороший хороший вопрос Да проблема вот как раз-таки с накоплением достаточного количества сообщений а ночью курьеры спят Вот и самое Ключевое Для нас - это выдача курьером Ну то есть не когда курьер там что-то доставил это тоже важно нас сколько интересует когда вот курьеру вот что-то выдали и вот выдают им именно утром и собрать вот такой вот прям объём объём чтобы провести нагрузку У нас есть возможность только вот с утра когда вот им происходит эта выдача Но кажется что это можно симулировать просто тестовыми данными не знаю как-то автоматически Почему нет это по сути для вас это просто какие-то ивенты в системе Слушай взрослые не симулирует просто прозрачная коммуникация и нормально на продакшене тестовыми данными Ну типа можно симулировать Но для того чтобы это симулировать надо где-то подкрутить и сделать это нечестным и тест перестаёт быть честным например там не знаю не отправляет сообщение в кафку типа что у нас что-то произошло вот и А если а вдруг это самое тормозное место вот мы его отрубили для вот во имя тестовыми данными ть погонять вот как-то так хорошо друзья наше время подходит Задай пожалуйста а я пока прокомментируй значит команда озона никуда не уходит и после церемонии закрытия ответить на все ваши вопросы честно и не под запись на грани слива nda вот поэтому уж простите что микрофон Давайте вот Раз два и вперёд Привет Ну поскольку там поиск проблема Это всегда Приключение сколько у вас по времени живут трейс Сколько по времени живут трейс обычно сколько я помню там до оного часа Ну типа про рейсов вообще Тао был доклад по-моему на прошлом хайлоу довольно подробный как трейс вообще пишутся как они хранятся всё остальное Вот но прям по-моему в течение часа их Спокойно можно достать то здесь именно на горячую искать приходится не так что Ой у нас неделю назад что тормозила быстро задавай хорошо быстро А спасибо а в общем вот вы сказали Best практис ом будет переставать отвечать на вопросы которые вам не нужны вашей команде да и как бы Как долго вы ждали пока другие команды изменят свою реализацию и перестанут к вам обращаться в виду наличия некого рычага давления на эти команды мы смогли довольно быстро их убедить Ну то есть грубо говоря вот этот вот перестроение этой всей архитектуры у нас заняло там в районе полугода То есть это не быстро и немедленно вот как-то так Но если это вдруг какие-то супер внешняя внешняя команда то может сильно затянуться именно изза ти продать Это как тех долг или выбить какой-то приоритет вот не 5000 это было в течение там 4 лет оставайся они тебе ещ расскажут много всякого интересного махни рукой все кто задавали вопросы значит кому уходит матрёшка за инвестицию в сообщество вопрос Давайте господин забирает MTS Digital Так давай Digital был вопрос оттуда про Почему мы не не ночью гоняем Почему не безопасни ночью вот он Ага отлично друзья значит что я вам хочу сказать А ещё есть да И кому ваш э приз автомобиль автомобиль да так А пу-пу сейчас пока ты думаешь значит друзья я вам хочу напомнить мы сейчас с вами как в детском лагере загибаем пальцы чтобы не забыть сейчас нет ты всё ещё думаешь Ага я думаю да генеральным партнёром этой конференции феерическое э был МТС и Мы помним Да сотовая связь Кибер бес облака развлечения финансовые сервисы и многое другое и супер поддержка хайлоу в Питере Давайте по аплодирую МТС Спасибо а теперь сейчас подождите подождите праздновать Кому мы дарим автомобиль автомобиль господин синеке госпо А он успел задать вопрос или просто успел задавал оже про это вот тебе тебе пластмассовый на 3D принтере автомобиль езди на здоровье Спасибо за доклад Ты отлично держался и не скаже что первый раз бой Вот это класс друзья а теперь"
}