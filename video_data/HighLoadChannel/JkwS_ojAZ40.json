{
  "video_id": "JkwS_ojAZ40",
  "channel": "HighLoadChannel",
  "title": "Как с помощью event sourcing мы организовали поставку данных и актуализацию структуры/ Никита Рьянов",
  "views": 175,
  "duration": 3147,
  "published": "2024-10-29T03:00:58-07:00",
  "text": "Никита рнов из тимлида ой из Тинькова мли из Тинькова расскажет Как с помощью eving Они организовали поставку данных и актуализацию структур Для более чем 2000 таблиц Никита вперёд Аплодисменты Всем привет да ещё раз представлюсь Меня зовут Никита нов Я работаю в команде которая занимается платформи проектами и и сегодня как раз об одном из таких проектов расскажу с помощью которого мы доставляем данные до хранилища которые у нас построен на Грин ПМЕ и актуализируя данные Для более чем 2.000 таблиц а доклад будет построен следующим образом а Сначала я расскажу а что вообще мы хотели получить Какие цели и задачи перед нами стояли а затем немножко поясню А что у нас уже было на старте Почему в итоге мы всё-таки решили разрабатывать новый проект а не развивать какой-то существующий что И что в итоге у нас получилось На какие грабли наступили и каких результатов достигли собственно исходные цели и задачи которые перед нами стояли первая на самом деле основная задача которая перед нами стояла Это консистентная доставка связанных сущностей в Тиньков очень много данных с которыми Приходится работать аналитиком и редко когда аналитик на самом деле использует в своих запросах какую-то одну единственную таблицу в большинстве своём это какие-то джоны из ДТ и более таблиц которые в конечном итоге представляют на самом деле бизнес сущность с которой Приходится работать соответственно когда мы говорим про актуализацию данных что нам нужно эти данные актуализировать то опять-таки речь всё-таки идёт в большей степени про то что нужно актуализировать все эти таблицы для того чтобы конечный запрос обрабатывал уже информацию по всей бизнес сущности в сумме соответственно если мы будем обновлять эти данные в разное время то это приводит просто к тому что аналитику не очень комфортно на самом деле строить во-первых сам запрос потому что ему нужно выбирать фактически минимальную из максимальных Дат когда эти данные актуальны это пер перво и второе на самом деле это также дополнительная нагрузка на хранилище соответственно первая первоочередная цель которая перед нами как раз ставил это именно добиться консистентной доставки данных по сути бизнес сущности а следующая цель которая также перед нами стояла и она вытекала из предыдущей так как опять-таки данных много все они разнородные то было бы очень здорово иметь какие-то а какие-то данные о том а что вообще из себя представляют используемые данные какой-то мету получить то есть какой-то каталог чтобы в том числе аналитик мог верхний уровня посмотреть на эту мету посмотреть что из себя представляют с точки зрения бизнеса те или иные колонки Ну и какое-то представление в целом получить Ну и вообще это на самом деле очень здорово потому что иногда бывает у аналитиков глаза так сказать заливаются и работа с одним доменом она в итоге приводит к тому что человек не знает про какой-то другой домен и Если есть возможность посмотреть на то что рядом то иногда появляется какая-то новая Синергия и так как мы разрабатываем платформу то было бы очень здорово чтобы у нас был некий инструментарий которы позволил пользователя нашей платформы легко и непринуждённо в идеале с нами интегрироваться собственно что у нас вообще было на руках Перед тем как мы всё-таки приняли решение к разработке нового проекта А на руках у нас было уже некоторое решение которое успешно работало и в целом работает оно построено на основе cdc на данный момент поставляется уже более таблиц умеет читать данные из окла Чере и пос через дези И что самое важное за много лет его работы накоплен действительно большой опыт эксплуатации у пользователей появились определённый определённое понимание гарантий этого проекта что вообще мы поставляем каким образом эти данные доезжают с какой скоростью и с какими дополнительными колонками так или иначе тоже можно поработать соответственно обновлять подобный опыт было бы крайне нелогично поэтому прежде чем с плеча рубить что всё-таки надо вс новый проект разрабатывать мы критически посмотрели на этот проект с точки зрения через призму поставленных целей А с какими задачами собственно проект не справлялся А первое И на самом деле Ключевое - Это то что он построен на основе cdc А cdc по своей природе работает именно с change итами которые в свою очередь представляют изменения какой-то одной таблицы из связки Тех самых бизнес сущностей а то есть мы не можем здесь скажем выстроить однозначно ту связку из нескольких бизнес из нескольких сущностей чтобы превратить её в бизнес сущность по умолчанию соответственно когда данные доходят до хранилища то они могут доходить немножко в разное время или не доходить вообще если где-то нарушены какие-то гарантии соответственно как консистентность мы здесь не получаем И на самом деле На этом бы можно было бы уже остановиться в целом но тем не менее Мы решили всё-таки ещё посмотреть какие ещё проблемы у нас были с сущест проектом Чтобы в дальнейшем их в том числе учесть и следующая На самом деле проблема она также следовала из предыдущей и из природы опять-таки от cdc это то что у нас не было полного контроля над изменениями схем А на источнике а владельцы данных они вольны делать любые изменения со своими данными вплоть до обратно несовместимых если при должен нам умении там выполнить какой-то хитрый Сколь запрос то в целом можно как и сузить типы так и вообще в принципе применить какую-то трансформацию и в конечном итоге а эта миграция она будет на стороне источника нормально работать Однако так как мы сначала получаем вс-таки данные а только затем схему то Для нас это рождалось в проблему что в конечном итоге мы при попытки применить те же самые альтеры которые мы генерируем на стороне хранилища мы не знаем какие на самом деле были правила применены для того чтобы применить те самые обратные совместимые изменения мы ловили определённого рода ошибку получали соответственно получали какую-то ошибку нарушали Гарантии и это требовало ручного вмешательство что было не здорово не для нас потому что дополнительно ру не для пользователей потому что гарантии доставки в том числе нарушались мы нарушали SL а соответственно с этого момента мы и начали думать А как мы можем А собственно достичь поставленных целей а Возможно даже как-то развить существующий всё-таки ВС ещё проект и первая мысль которая у нас была - это Можем ли мы в принципе объединить те самые ченд ивенты чтобы к ним плавно а плавно перейти к бизнес сущностям которые бы объединяли в себе изменения а нескольких каких-то атомарных отдельных таблиц в принципе здесь что первое наверное приходит на ум это так как Мы представляем события изменений каждой таблицы в виде некого потока Наверное мы можем как-то примерно прикинуть что можем их объединить через какой-то н стримов или там используя этот дуализм между стримами и таблицами попробовать снить таблицы и в принципе да действительно такой вариант есть но на самом деле картинка абсолютно и в принципе показывает некоторые варианты жна Ну в данном случае это in Join который нам как раз и нужен для восстановления бизнес сущности а здесь на самом деле есть ряд нюансов с которыми также нужно было поработать и подумать над их решениями и первый на самом деле и самый основной - это А что делать с какими-то запоздавшая нет никакой гарантии что они придут в какую-то разумную Дельту это на самом деле может быть и сутки может вообще никогда не придут Должны ли мы эти данные в итоге хранить Для дальнейшего джоина Или мы должны их отбрасывать если отбрасывать то фактически это потеря данных вряд ли до кого-то устроит если хранить то то сколько Вот и как будем ли мы уверены что вообще когда-нибудь эти данные доедут соответственно альтернативный вариант как раз используя дуализм стримов превратить это обратно в таблицу но по сути мы приходим к тому что мы дублируем данные То есть у нас фактически из потока мы опять получаем таблицу из лого изменений мы снова получаем таблицу которую пытаемся Джони и кажется что это немножко во-первых Странно во-вторых здесь очень серьёзные тоже нагрузки на хранение потому что здесь нельзя гарантировать что количество ключей по которым нужно будет Джони оно будет каким-то разумным это могут быть миллионы и миллиарды соответственно места Нужно достаточно много от этого решения мы а отказались потому что довольно много нюансов которые нужно решить в том числе технических и процессных и решили подумать А что ещё есть за вариант И на самом деле есть а неплохой вариант и Вполне себе тоже классический А который позволяет а несколько бесшовной от cdc к тем самым доменным ивента а от Чен ивентов к доменным ивентум но при этом оставаться в парадигме cdc так называемый outbox pattern А в принципе он используется а не только как transactional outbox pattern его Также можно использовать для подобных вещей потому что мы по сути создаём а некоторую широкую таблицу которая представляет из себя связку Тех самых а простых каких-то сущностей которые в начале мы которыми В начале мы оперируем отдельно А мы в итоге делаем Ту самую широкую таблицу которая в себе агрегирующие Чейн ивенты но с точки зрения уже хранилища это будет не просто Чен ивен это будет на самом деле полноценная бизнес который можно уже оперировать и которая как раз нужна аналитикам а помимо того что есть такой вариант на самом деле он также дублирует по сути данные потому что мы имеем как здесь на примере четыре исходные таблицы плюс ещё пятую агрегат и фактически это дублирование данных пос например предоставляет возможность несколько эксплуатировать логическую репликацию и в принципе мы можем уйти от непосредственной таблицы используя возможность логической репликации самостоятельно посылая в соответствующий ивент который будет представлять из себя тот самый А доменный ивент с которым в конечном итоге бы Мы работали а то есть по сути тоже на тоже за исключением того что мы бы не дублировали данные а от этого решения мы вынуждены тоже отказаться потому что здесь есть определённые нюансы работы во-первых первый процессный вопрос - это А кто должен был создавать Ту самую таблицу Агат или если это не таблица то кто должен был акту тот самый запрос для логической репликации для того чтобы мы получали доменный ивент мы как как платформа или пользователи как владельцы данных если мы то в какой момент как нам интегрироваться в процесс пользовательский чтобы мы как-то желательно незаметно для него внедрились в его стандартный пайплайн или это пользователь если это пользователь То как ему донести что это нужно не просто создавать таблицу агрегат но ещё и дополнительно всё-таки валидировать изменения потому что Они до бы опять-таки обратно совместимыми потому что здесь мы ВС также на самом деле не можем подключиться и запретить ему делать какие-то изменения просто так то есть это должно быть какое-то обоснованное вс-таки видение плюс ко всему у нас нет полноценной возможности в принципе вклиниться в его процессы То есть он миграции там накатывает со своей регулярностью и нам нужно было бы как-то это всё валидировать соответственно на эти вопрос ного Отта см который бы устра длях зало соответственно Мы вс-таки приняли решение что нужно что-то как-то с этим делать по-другому основная идея между этими двумя подходами которые мы рассматривали это то что нам нужно перейти как-то от ивентов которые мы изначально оперировали к доменным ивента чтобы в конечном итоге как раз достичь Ту самую цель для консистентной эту идею Мы пришли к тому что нам нужен который будет оперировать теми самыми домен итами кото с помощю которых мы потом будем наполнять наше хранилище а но первое С чего мы начали это с опять-таки с подхода в cdc нам не очень нравилось то что нам сначала приходят данные а только потом мы получаем информацию по сути о схеме и это приводило к определённому роду проблемам соответственно здесь мы пошли от того что всё-таки сначала нам должна быть известна схема мы должны её проваливать должны на неё посмотреть и только потом сказать пользователю что да эта схема Валид Ты можешь её использовать для того чтобы свои данные отправлять при этом в схеме мы хотели обеспечить возможность описывать те самые связи для связанных сущностей а именно связи один к одному и один ко многим и плюс ко всему для того чтобы нам нужно было иметь возможность создать тот самый каталог данных схема должна быть само документи емо там должны быть должна быть возможность с помою каких-то атрибутов добавить информацию про бизнес ценность собственно что это за сущность в целом что Что из себя представляет каждая отдельная связанная сущность и каждая отдельная колонка в том числе для того чтобы в конечном итоге можно было а аналитикам тоже посмотреть и какие-то выводы сделать А собственно для того чтобы Ту самую схему описать также нужно было бы ещё придумать а а с помощью какого протокола это описывать протокол у нас были свои требования а они Вполне себе естественные это эффективная сериализация данных будет много и данных будет действительно оче много соответственно Чем меньше они будут весить тем лучше для нас потому что все они будут проходить через каку а соответственно нам бы не очень хотелось чтобы нам нужно было сильно раздувать наше хранилище а для брокеров вот Ну и пму это дополнительно нюанс работы с тенно потому что нам нужно в том числе иметь определённого роду гарантии что мы всё-таки успеем Вычитать все данные до того как они потеряются потому что иначе это не имеет никакого смысла и немаловажный пункт это то что было бы хорошо иметь наличие зрелых инструментов удобство пользования и в принципе какую-то кривое обучение должна быть довольно низкой для того чтобы пользователи нашей платформы могли не задавать нам много вопросов по поводу того что А как вообще к нам заехать а всё-таки иметь какой-то простой гайд Установи какой-то плагин сгенерить данные потому этоже было дабы не ходить вокруг до около мы остановились на а связки пги Вполне себе стандарт дефакто для работы с какой а достаточно эффективный протокол сериализации данных он бинарный по бинарным данным при сериализации там довольно небольшой если мы говорим про связку со схемой регистре это дополнительные п байт один из которых это Magic который сес уже используется в том числе для того чтобы понимать и про и со схемой и это собственно глобаль ID схемы плюс ко всему Ары есть возможность гибко настраивать политики совместимости там их сем соответственно нас интересует но в целом с учётом того что есть back for также их пары транзитивные также есть полное отсутствие политик совместимости то тоже очень здорово иметь соответственно Для нас это важно с точки зрения валидации как раз чтобы у нас не было обратно несовместимых изменений Ну и самое главное Ара в принципе Довольно простой с точки зрения описаний есть вариант писание с точки зрения джисона можно кастомные атрибуты добавлять и Можно также использовать специальный avdl формат который более компактный кому-то он удобней Вот Но в основном Как ни странно все пользуются а описанием через Jon а сама схема а выглядит примерно следующим образом а на Верхнем уровне она представляет из себя а рекорд который в свою очередь состоит из трёх обязательных полей это System info key и payload а System info - это наша необходимая Мета которая нам нужна как платформе соответственно мы каждой сущности которая нам отправляет Мы также ожидаем что Бут отправлены определённая Мета информации также есть поле как следует из названия это просто структура с набором колонок примитивных причём которые будут представлять из себя тот самый первичный ключ родительской сущности и далее уже Это непосредственно те самые данные которые нужно доставить до хранилище и здесь на самом деле как раз пример в примере представлена связь один к одному То есть у нас есть некоторая вложенная сущность которая представляет из себя рекорд если бы мы хотели описать один ко многим то это был бы массив рекордов соответственно вложенность может быть любого уровня то есть на самом деле можно здесь вложить рекорд рекорд рекорд рекорд получить очень сложную там структуру некоторым пользователям это надо хотя мы не очень такое одобряем но тем не мене разные люди по-разному экспериментируют с нашими схемами кто-то даже иногда зло Уля этим Вот соответственно формат довольно простым оказался на практике действительно пользователям Вполне себе комфортно с ним работать Несмотря на то что пер на первый взгляд может показаться что не привычно но тем не менее с определёнными гайдами в целом в целом перевариваемая Следующий пункт про кото тоже стоит поговорить Это непосредственно формирование каталога СХ перед тем собственно использовать эти схемы было бы здорово их где-то хранить а соответственно помимо того что нам нужно достичь цели по хранению этих данных и самих схемы и их просмотра также было бы здорово иметь возможность посмотреть на историю изменений этих данных потому что как показала практика спустя какое-то количество времени дней иногда пользователи могут забыть Почему колонка добавилась или удалилась соответственно очень здорово Сказать им что вот смотри у тебя был такой-то Mr такая-то задача А почему вы изменили схему именно так и был бы вообще замечательно если бы ещё была был бы определённый инструментарий для того чтобы можно было внедриться в процесс ревью схем А и сказать что схемы невалидные ещё до того как вообще Они ведут к нам в прод или на тест или на любое другое окружение и пользователю при этом дать какую-то обратную связь А что именно было не так а соответственно здесь на самом деле также какого-то велосипеду мы не изобретали а хранили действительно в ла репозитории обычный репозиторий который просто имеет определённого рода структуру а здесь всё стандартно люди наши пользователи делают C через forb или через наш с-сервис И самое главное мы здесь имеем возможность через gitlab actions в принципе встроить в любой этап CCD и проверить валидна ли схема или нет плюс ко всему на этапе уже непосредственно CD А мы на самом деле за пользователя выполняем очень много тоже автоматики начиная с регистрации непосредственно схемы заканчивая подготовкой топика куда пользователю нужно писать выдачи прав подготовки ещё каких-то других каких-то других систем соответственно пользователю ничего не нужно после этого делать руками то есть единственное что он делает это создаёт схему описывает её проходит ревю и на этом у него после того как всё это ржится заканчивается на са взаимодействие с с репозиторием в дальнейшем он просто использует эту схему и пишет в топик каки как обычно он это делает для любых других задач теперь собственно Стоит немножко углубиться в непосредственно доставку данных раз мы заговорили о том что пользователь в дальнейшем перестаёт уже взаимодействовать с репозиторием а взаимодействовать больше со схемой а то стоит углубиться как раз в доставку данных а первое и самое важное Почему А мы хотели прийти это что каждое событие которое отправляет нам поставщик - это целиком новый стейт соответственно даже если изменилась какая-то одна колонка так как мы хотим построить именно Лог событий Лог изменений в нашем хранилище а то есть по сути сырой такой слой то а пользователь а или поставщик вынужден всё равно отправлять нам а полный объект целиком а в некоторых ситуациях это действительно Бывает избыточно но тем не менее таковы реалии для того чтобы построить а нормальное хранилище а именно сырой слой для него а так как у нас хранилище на Грин ПМЕ то здесь также возникает вопрос А как собственно сохранить а данные в гпм а Даже несмотря на то что это Казалось бы Лок изменений мы строим А всё равно есть определённые нюансы работы с ним а у нас используется ient Only optimized таблицы плюс некоторая кастомная реализация gpf Дива А если максимально упрощённом на схему как это выглядит то выглядит Это примерно следующим образом у нас есть некоторое наше приложение которое читает данные из топика а собирает бач соответственно для того чтобы данные доставлять уже гнп не через мастер сегмент который был бы бутылочным горлышком а всё-таки параллельно в каждый сегмент то мы а по сути в каждом нашем приложении поднимаем некий ИТ gpf дис сервер а потому что gpf дис протокол по сути это на самом деле а а некая некий веб-сервер А с помощью которого можно тоже общаться по хдп запросам соответственно сегменты а отправляют свои запросы после того как мы дали им команду на наш сервер и мы по постепенно отдаём эти данные в параллель каждому сегменту получается довольно быстро эффективно и просто а при этом так как нам нужно это делать консистентной числе делаем Это в рамках одной транзакции соответственно здесь у нас транзакционная сохраняется потому что НП - это транзакционные хранилище и в целом всё очень здорово а выходит а при этом здесь Было бы ещё Здорово а понять тот факт А как собственно формируется связь между сущностями на уровне таблиц Потому что если на уровне схем А это можно проследить что у нас есть какие-то какая-то родительская сущность какие-то дочерние и можно понять что если это там А просто рер то это один к одному если это массив рер то это один ко многим Но нам было бы здорово также иметь эту же информацию и на уровне таблиц для того чтобы аналитики могли в дальнейшем А работать собственно с этими схемами А в своих запросах потому что джоны надо бы как-то делать А через внешний ключ собственно внешний ключ мы как раз и генерируем в конечном итоге это может выглядеть примерно так что у нас есть родительская таблица а от неё есть то есть у нас всегда на самом деле каждая схема представляет из себя одну какую-то родительскую таблицу от которой по сути отходят ветви к дочерним таблицам при этом ветви могут быть совершенно уже разной вложенности соотвественно такой немножко а древо обб структура а при этом а Если обратить внимание то здесь А следующий интересный момент есть это то что у родительской таблицы есть тот самый первичный ключ который мы описываем в структуре кий А И этот же первичный ключ на самом деле в дальнейшем используется в качестве внешнего ключа А в начальных сущностях которые мы автоматически пропа соответственно пользу нашей платформы не нужно это описывать в схеме это уменьшает несколько когнитивную нагрузку на них с точки зрения описания схемы Ну и в том числе упрощает намм работу потому что мы автоматически это просто генерируем и об этом больше не задумываемся а при этом если у нас есть более скажем сложные структуры с точки зрения вложенности Когда у нас дочерняя таблица дочерняя какая-то сущность вложена в другую дочернюю сущность то помимо того что мы проем первичный ключ структуру Мы также пропа первичный ключ дочерней таблицы которая чуть выше уровням соответственно можно Jo строить более сложное это также делается автоматически а но при этом Здесь также ещё стоит поговорить о том как мы работаем с ошибками и с повторными обработками данных а повторная обработка в принципе она неизбежна потому что рано или поздно всё равно какие-то ошибки возникают это может быть какой-то сбой сетевой неважно на самом деле главное что в конечном итоге поток Мы занова вычитаем дубле в хранилище это нужно принять какси это плохо Даже несмотря на то что это сырой слой и Казалось бы его можно дедули с учётом объёма данных любая дедупликация - это дорого соответственно Было бы неплохо здесь как-то эти данные дублировать дедупликация на самом деле здесь происходит довольно просто опять-таки мы здесь старались как можно меньше изобретать велосипед и в целом обычным образом происходит дупликация с за счёт того что мы в общем эксплуатирует зациорский соответственно мы просто работа с офсета как бы это странно не звучало переносим На мето таблицу в Грин ПМЕ нашу системную соответственно когда мы бач закончили обрабатывать то мы просто говорим что вот этот эти апсеты мы закомельская гипотетически мы всё равно вычитаем часть данных которые уже обработали то мы можем спокойно по офсета отфильтровать уже обработанные данные и в итоге таким образом дедули фактически это попытка сделать EX коню А хотя с точки зрения обработки это всё ещё least One потому что и в принципе как некое такое понятие есть eff Processing который говорит о том что у нас доставка действительно будет exactly Once а но с точки зрения процессинга это всё ещё и least А ну давайте теперь немножко всё равно к схеме уже 2.000 таблиц спустя Казалось бы что может пойти не так если у нас есть и валидация и проверка и Маров и схем ещё на этапе ревью и дедупликация сообщения вроде бы как всё здорово но пойти может на самом деле действительно много чего не так а и первое что самое странное - это регистрация заведомо не валидно схемы в схема регистре а Казалось бы как это может произойти на самом деле очень просто и очевидно как теперь уже кажется Хотя до этого нам казалось что это не должно было происходить и причина здесь следующая пользователи когда коннектятся регистре и начинают её использовать у них в их библиотеках есть возможность зарегистрировать в схем схему которая ранее не существовала Соответственно по любым причинам которые нам неведомы может сгенерировать схема таким образом что она будет несколько невалидна относительно того что мы проверили и таким образом мы попадаем в ситуацию что мы опять получаем какую-то невад схему соответственно мы где-то получаем решение здесь оказалось довольно тоже простое мы просто выстроили определенного рода прокси сервер который чуть-чуть умнее чем обычно проксирование запросов мы просто запрещаем пря прямую регистрацию схем для пользователя здесь плюс в том что у него взаимодействие с нами ровным счм никак не поменялось кроме урла которые он использует для подключения всё очень здорово пользу довольны полность избавились от этой про мало кро с уже посло это некорректные данные а под некорректны данными здесь понимаются некорректные данные с точки зрения Грин плама так как у нас грип - Это основное хранилище куда мы сохраняем то с точки зрения авра например некоторые данные могут быть правильными а здесь самый банальный пример Наверное это а тип строки в который пытаются засунуть J носо сформирован например с ошибкой соответственно Мы попытаемся в Грин паме создать сохранить тот самый в конечном итоге при вставке мы получим ошибку что Нели но в авра это было не отсле соответственно здесь мы не можем позволить себе проверять каждое сообщение отдельно потому что поток достаточно большой и наш компьютер тоже не резиновый соответственно Было бы очень здорово переложить эту ответственность на пользовательскую сторону Соответственно что собственно мы и сделали это разработка СДК который встраивается в каждое пользовательское приложение а И помимо того что он упрощает используя наши платформы также здесь были ожидания что мы сможем переложить ответственность за валидацию на непосредственно мощности пользователей такая вот хитрая рокировка Однако на этом тоже определённые сложности не заканчиваются как уже говорил ранее что можно в схеме на самом деле очень много делать сложностей соответственно в принципе мы по умолчанию даже никак это не ограничиваем и в некоторых ситуациях это выходит за какие-то разумные пределы Ну помнить что чем больше сущность на самом деле тем больше возрастают затраты на её обновление потому что каждая дочерняя структура - это в конечном итоге отдельная таблица в Грин плане а соответственно когда мы пытаемся обновить ивенты Мы в одной транзакции начинаем затрагивать как можно больше всё больше больше таблиц Несмотря на то что это ала и Казалось бы там всё довольно оптимально но тем не менее долгие транзакции не очень здорово даже для гпма а соответственно здесь решение на самом деле мы тоже поза у того же самого паттерна в принципе если это какая-то простая структура которая связывает один к одному таблиц то мы просто предлагаем пользователям Почему бы не расширить вам вашу исходную таблицу и сделать её чуть более широкой потому что в любом случае вы её дните соответственно это такого рода денормализация но в пользу того чтобы во-первых упростить схему и во-вторых упростить работу с с ней на уровне хранилища довольно частый кейс на самом деле потому что пользователи действительно а пытаются каждую свою сущность в виде таблицы вынести в отдельную какую-то структуру Но вот такая денормализация позволяет и эффективнее работать с данными уже непосредственно когда мы их обрабатываем и в то же время в хранилище также сохранять достаточно эффективно а Однако с проблемой Когда у нас всё-таки есть один ко многим а такой вариант не работает а здесь проблема с один ко многим заключается в том что опять-таки это следует из той того правила которое мы вы выработали для работы пользователей что один ивент Это полный стейт всей бизнес сущности соответственно если в связке один ко многим например Это пользователи показа реклам и этих показов там миллионы и нужно обновить только одну какую-нибудь строчку или наоборот добавить или удалить то поставщику нужно отправить весь миллион связанных каких-то объектов А это не очень эффективно Как с точки зрения передачи данных так и с точки зрения опять-таки сохранения данных в НП соответственно здесь единственное решение Разумное которое мы сейчас предлагаем - это на самом деле разбивать подобные схемы на разные поставки Да это в итоге мы здесь немножко теряем А в той самой цели А где мы говорим про консистентность также стоит придерживаться здравого смысла что мы и делаем потому что хранилищем пользуемся не только мы но и другие аналитики соответственно будет не очень здорово если мы это хранилище немножко убьём А что в итоге в итоге мы имеем следующее что сейчас проект действительно работает и на данный момент мы поставляем более те самые 2.000 таблиц сейчас уже наверное около 2.400 с копейками А работает это всё довольно успешно и поставляемые в НП довольно много данных здесь на самом деле основная ценность наверное в том что мы не использовали здесь какие-то сверх хитрые технологии какие-то новые не изобретали какие-то новые велосипеды это на самом деле орные какие-то решения за исключением небольших нюансов работы с гпм в виде ГП диста А в остальном это всё действительно построено на стандартных паттернах стандартных подходах что очень здорово и означает что на самом деле При желании это можно повторить а при этом следующий момент на котором тоже хотелось бы остановиться это значит ли это что cdc для доставки данных до хранилищ И вообще наполнения хранили непригоден очевидно Нет это не значит всё естественно как обычно зависит от задачи от контекста в нашем случае cdc для подобных задач не очень подходил соответственно так как у нас есть довольно много бизнес сущностей именно аналитики оперируют именно бизнес сущностями нам было удобнее перейти всё-таки на инсорсинг но при этом инсорсинг также не является каким-то той самой сереной пулей которая подойдёт всем абсолютно потому что исходя из предыдущих двух проблем связанных с бами хема на самом деле инсорсинг Здесь тоже А не то чтобы прям супер эффективен Но опять-таки если у Вас например в задачах есть необходимость доставки данных которая представляют из себя бизнес сущность довольно компактную то в целом да Такой вариант довольно прикольный а удобный и позволяет а достичь а ваших задач на этом а мой доклад на самом деле закончен Всем большое спасибо за внимание а готов ответить на вопросы да м мой микрофон заработал Спасибо большое смотри мы сейчас вопросы будем тебе задавать вы Запомни автора лучшего вопроса он получит за это приз так что всё вперёд О даже два два вопроса мне подсказали вперёд Здравствуйте меня зовут Арм вопросов на самом деле много но Аа Вопрос такой вот больше интересует тех стек вашей платформы она очень на стриминг похоже и второе вы не думали что вы не только можете наполнять базу но собственно сами выступать источником для других потока ваших данных Вот и про pxf хотел спросить почему вы не использовали pxf а вот сами написали какую-то Угу э который бежит по Грин плану э фичу или как там модуль начну наверное с первого это про стек Да у нас действительно по сути стриминговая обработка стек у нас следующий у нас потоковый движок - это apink вокруг него всё это построено навес тоже jaus кавка в качестве шины передачи данных пост для микросервисов ГП в качестве хранилища для распределённых локов также мы используем дополнительно В отрыве от каки Да и вроде бы всё да что из интересного Ну понятно и в целом Да второй вопрос Можем ли мы исполь можем ли мы стать системой источником да на самом деле действительно так и разговоры об этом ведутся и даже уже есть определённо роду подвижки мы действительно можем стать и уже в какой-то степени являемся источником поверх которой строятся другие какие-то проекты У нас есть по сути стриминг etl Вот как раз где мы являемся источником данных для дальнейшей какой-то обработки и трансформации а такая задача есть она действительно сейчас в проработке не скажу что она прямо у нас активно прям мы в эту сторону двигаемся но тем не менее задача есть а что касается последнего вопроса Почему мы не используем pxf а на самом деле а там здесь а такой ответ А чуть чуть наверное будет в общем ситуация в следующем мы не используем pxf Потому что так исторически сложилось что мы на самом деле довольно активно использовали Ди Очень долго и в целом нас он устраивал более чем за исключением того что мы раньше использовали уже готовые какие-то веб-сервера которые Ну по сути бинарник который поднимал дист соответственно мы умели с ним работать у нас уже была экспертиза не только причём работя но и со стороны эксплуатации и администраторов а соответственно А нам бы не очень хотелось от этого отходить плюс ко всему pxf на самом деле на тот момент когда Мы начинали Этот проект а не давал нам некоторых м моментов которые мы хотели получить соответственно а посмотрев О что вообще из себя представляе дист а оказалось что это обычный а протокол а по сути который можно а на основе которого можно реализовать по сути свой собственный небольшой ТП сервер а и сказать сегментам вот общайся с нами А по этому протоколу и обменивайтесь с нами данных данными собственно Вот вот так так А я могу задать вопрос Да просто у меня есть микрофон я решил что Да я тоже из рафайзен банка Я Андрей я может быть не до конца понял хотел уточнить А ты сначала сказал что cdc не очень подходил и вы перешли на доменные ивенты но при этом вы не смогли ну не захотели их читать из постгрес с помощью або таблички и я не Поня А как вы перешли к Ну как вы эти домены ивент отправляете или просите поз отправлять они просто своим приложением пишут ввку окей Тогда вопрос А как вы гарантируете что если пользователь записал транзакции допустим в 10 своих таблиц в постгрес а потом собрал доменный ивент и пишет его в кафку Что делать если там запись в кафку упала например или Ну да он записал в таблицу после этого приложения упал не успел записать в кафку то есть либо потери получаются возникают как такие вещи решаются А здесь э-э А смотри а во-первых а не все на самом деле пользователи А генерируют свои доменные объекты аэ перед тем как или после того как они что-то сохранят у себя в базе то есть на самом деле у нас есть определённый Пласт А приложений которые по сути а являются неким приложением которые вообще по сути ну без состояния фактически то есть они ходят в какие-то IP собирают этот доменный объект а что-то с ним делают и отправляют это дело в кафку то есть здесь а по сути базы в принципе нет Если говорить про ситуацию когда пользователи например получается не успели закоммитить транзакцию но при этом успели отправить в каку или Ну наверное такое может случиться Да если они так сделали такую реализацию сделали такое может быть это по сути генерация дубля с нашей точки зрения это бизнес дубль который действительно может привести к тому что у нас появится Дубль В хранилище DC таких не возникает и там приу боксе и потом Вдруг у вас есть какой-то механизм там не знаю процесс в библиотеке записи в кафку которые ты как-то решает Аа ну у нас именно специфичного прямо процесса нет здесь мы на самом деле также используем Вполне себе а существующий механизм подобной дупликации - это на уровне кавка продюсера То есть в принципе некоторые пользователи эксплуатируют транзакционный продюсера Вот и в принципе частично эта проблема решает не полностью всё равно есть определённые кейсы когда могут быть сгенерирован дубли но тем не менее именно бизнес дубли в данном случае потому что это физически это будут разные сообщения То есть с точки зрения асетов несмотря на наполнение одно и то же Вот соответственно такие дубли мы действительно не умеем сейчас дублировать и как задачу нашей платформы мы не ставили на самом деле потому что это несколько уже такая бизнес всё-таки штука которая А где-то это может быть действительно дубль а где-то нет то есть Нам нужны будут определённые формальные правила которых У нас не было понял я только хотел ещ про когда нет стейта нет базы вы получается каким-то крон Джам периодически собираете этот стейт ну условно этот микросервис бегает по дети другим микросервиса собирает новый доменный ивент и отправляет его правильно это зависит от на самом деле то есть мы здесь являемся платформой и к нам подключаются как раз те самые приложения пользовательские про то как они собирают эти данные мы на самом деле вот стараемся как раз особо не знать потому что мы хотим вот мы предоставим протокол что вот у нас есть топик для вас Вот ваша схема Пожалуйста пишите в ней и собственно всё вот как они там себя формируют мы вот стараемся в это не лезть понял спасибо то есть вы просто даёте гарантию что если вы записали этот топик созданный через gitlab он станет витриной в гнп Ну таблице в ПМЕ табли Спасибо большое за доклад и за ответы Да здравствуйте Спасибо за доклад подскажите а как вы гарантируете консистентность данных внутри базы параллельное чтение неправильно пришли как мы гарантируем консистентность данных записи в хранилище а ситуация следующая у нас один топик - это одна схема соответственно когда мы говорим про какой-то доменный объект или бизнес сущность то все изменения попадают в один топик Казалось бы здесь может быть вопрос е в том числе связан с тем что делать когда у нас много партий в этом топике например и Да здесь мы так немножко хитро Ну на самом деле не очень хитро работаем по сути это который позволяет нам за счёт ки процессора сгруппировать все эти партиции в один какой-то батч и по сути мы получаем уже независимо от того одна там партия 12 или 24 вообще неважно Мы в конечном итоге получаем батч на самом деле который объединяет все за один мы получаем весь этот бач котором собраны разные партиции соответственно в одно акции мы вот этот бач целиком вставляем параллельно никто в эти таблицы не пишет кто-то из них может читать аналитики там что-то делат через свои вьюхино там шеры Лок поэтому это не сильно влияет а именно запись у нас всегда в один поток всегда одним процессом Поэтому консистентность с этой точки зрения у нас сохраняется Спасибо Здравствуйте а Светлана само само картеч Скажите пожалуйста а вы сказали что вы агрегирующие изменилась А в этом случае в таблице в сыре дата и время изменения именно дочерней сущности у вас будет определяться новым событием Либо вы элемент вложенного массива как-то будете сравнивать между собой а если правильно понял вопрос Попробую ответить а нет мы всё равно А создадим по сути новую запись даже если она с точки зрения дочерней сущности никак не поменялась а только поменялась родительская мы всё равно создадим А запись с тем тайм смпом А который будет последний соответственно А в таблице с логами это по сути будет ну две одинаковые колонки в две одинаковые строчки в дочерней сущности но у одной из них будет дата Т1 А у другой Т2 вот даже если даже если они не поменялись потому что мы здесь оперируем а это было бы Возможно если бы мы оперировали патч запросами А мы не оперируем пач запросами у нас Соответственно по сути всегда insert Ну всегда он вот то есть мы получили полный стейт мы его же и вставили А и Да это есть с этим определённого рода проблемы мы думаем над тем чтобы как-то это немножко оптимизировать на самом деле но здесь такой очень сложный компромисс потому что мы можем оптимизировать с точки зрения вставки в гнп Но тогда аналитикам опять получится что им нужно будет получать А что там за минимальная дата где там что изменилось Потому что им нужно в джоне всё это как-то учитывать и они не могут на самом деле просто взять и сказать что сни мне вот таблицу по таким-то ключам потому что мы оперируем таблицами и соответственно на один такой дй мы получим Ну довольно большую связку соответственно им нужно как-то вот уметь выбрать самую последнюю версию вот скажем есть ванат учитываться да да в том числе спасибо Так у нас заканчивается время под вопросы но может быть у нас ещё не закончились вопросы там вот есть рука Там две руки или одна Отлично вот на них заканчиваем окей да Привет Меня зовут Лев Я тоже из фан банка вопрос следующем Какая примерно нагрузка на запись и как было выбрано объём микроба а нагрузка на запись у нас сейчас а ну в среднем к нам приходит около 3 тире 4.000 событий в секунду в целом Бачи у нас копятся а у нас есть три правила накопления Бача Первое - это по количеству записей второе - это по их объёму третье - это если они первые два не сработали то есть есть топики которые редко наполняются и соответственно мы не можем их наполнить там соответственно трешхолд 10 минут допустим а соответственно в среднем у нас Мы около полмиллиона записей Мы копим в баче соответственно если мы полмиллиона не накопили смотрим набрали ли они там свои трешхолд например там гигабайт там 5 гиб Ну в зависимости опять-таки от топика и соответственно если эти параметры сработали то мы начинаем вставлять БАД соответственно мы здесь а скажем пытаемся как раз не убить гнп вот маленькими вставками а всё-таки вставлять достаточно большими объёмами насколько это возможно с учётом стриминга потому что топиков мы обрабатываем много и нам нужно как можно быстрее з вставить данные чтобы читать следующие Вот поэтому так Спасибо за доклад Роман собственно вы сказали что 2000 таблиц потом и схема First звучит так как будто вы на команд поставщики данных переложили кучу новых обязанностей Как вы заставили их это сделать И зачем собственно вы им объяснили это Зачем им это надо Да действительно у нас в том числе была одна из целе где-то немножко инвертировать этот процесс и действительно переложить на а команды поставщиков определённой роли в том числе и описание схемы и А придумывание что там за схемы они хотят описывать А как мы это сделали А вопрос хороший на самом деле мы просто пришли к ним и сказали что вот смотрите есть пользователи которые пользуются вашими данными А им неудобно а потому что вы поставляется таблицы мы мы поставляем сейчас по текущему процессу таблицы в разное время соответственно мы не можем сформировать консистентной самый доменный объект а с которым мы в дальнейшем аналитике работали а это немножко идёт в разрез тому что мы хотим получить от хранилища Давайте с этим что-то думать Вот собственно мы все вместе в том числе и подумали с их подачи То есть у нас были первые адаптеры скажем так А причём это где-то пришло даже на самом деле А как ни странно а от самих же пользователей которые изначально с нами работали через cdc аа они а являлись первыми же пользователями которые на самом деле накладывали определённые требования к новому продукту поэтому а это был очень такой важный и является проект очень важным поэтому Ну где-то можно сказать такая вот немножко монопольно Диктатор ся штучка получилась что вот есть крупный проект теперь под который все тоже подстраиваются У меня бизнес задача на 2 года вперёд вот мне ваши изменения Нафиг мне нужны вот Ну вот у нас вот как с этим боролись Нет я просто это на практике то же самое сейчас происходит это а боролись именно с тем что вы говорите надо проект говорит Ну и что ну мы говорим как Диктатор ся часть в чём состояла Диктатор ся здесь часть в том что А у нас есть просто две альтернативы Либо вы поставляется данные как сейчас уже поставляем Мы через cdc Но тогда у вас будут гарантии определённого рода а и если вас это устраивает можете жить так то есть вы можете ничего не менять у себя в коде там работать как вы сейчас работаете вставлять таблицы как вставляете мы будем через CC это поставлять с теми гарантиями которые уже есть они вам известны если опять-таки устраивают и поставщиков Окей если хотите что-то больше то вот так так вопросы закончились но короткий вопрос Без надеюсь получится пожалуйста короткий сейс микрофон всё и на этом заканчиваю так я постараюсь быть кратким у меня небой разрыв шаблона вот ВМ с всем подходит потому что нужна модель доменная и её надо целиком потом мы пишем что нам нужны схема регистри схема F тут всё классно всё классно А потом мы такие говорим что большая табличка там будет миллионы строчек Это очень дорого много мы так писать не будем и потом переходим к сорса Но их также загружаем ивента по аналогии с cdc на стороне получателя у меня собственно разрыв шаблона Почему у нас были какие проблемы а могут потеряться сообщения могут что-то не прои может что-то не дойти Так мы тоже самое и получаем только у нас теперь есть схема регистре О'кей Ну так мы всё равно целиком доменную модель не присылаем может быть тогда надо было Ну как бы чуть-чуть по-другому пойти то есть не так же вот все таблички расписывать и также их слать и потом опять же также что-то потерять а Например если у нас есть доменная модель Из двух трёх-четырёх таблиц они как-то связаны с Рейна и образуют Ну такой полный контур одной сущности но соответственно Если в ней есть Множественные связи их например нарезать Бача то есть чтобы одна транзакция например задействовал только по тысяче элементов в дох дочерних сущностях на эту самую транзакцию её можно будет закоммитить её можно будет целиком прописать в одну таблицу с вот этим доменным интом и соответственно так так вот пачками постепенно этот миллион собственно проживать Но тогда у нас будет целостность данных на потребителя а так мы получаем всё то же самое что и в начале только у нас схема регистре классно и да и нет А то есть мы получаем всё то же самое что и в начале только вот как раз для тех сущностей где есть те самые связи один ко многим которых у нас э в текущем объёме их небольшой процент соответственно а с ними приходится пока вынуждены работать именно так Как так Как приходится и да фактически они теряют Ту самую консистентность и приходят к тому С чего и начинали для них принципиально ничего не поменялось это правда Однако так как всё-таки основная идея у нас и задача опять-таки исходя из нашего хранилище из нашего пользования опыта того с чем работают аналитики было больше про структуру где всё-таки именно превалирует один к одному Ну так так вышло что вот именно так то для пользователей у которых именно данные на которые мы нацелены их больше и им комфортно с этим работать в конечном итоге но я здесь Да с этим не поспоришь это действительно так что для связи один ко многим если это большие прямо связи ко многим которые Ну те же самые там пользователи показа рекламы здесь у если мы будем это разбивать на разные транзакции Мы на самом деле немножко теряем в том что вот это не совсем уже атомарная тоже вставка то есть есть определённые нюансы с работой Спасибо большое Ну не решили большую проблему но решили классно маленькую Спасибо отлично Я надеюсь то что вопрос этот не самый был лучший потому что из редно не охота ему подарок давать он говорил что постарается коротко у тебя даже ответ долгий был Какие вопросы долгие задаёт Но мы уже закончили твоя задача выбрать два самых лучших вопроса из всех кроме последнего а Наверное мне понравился вопрос про собственно то как приложение непосредственно пишут данные вопрос поднимите у чей-то был вопрос да Ага а Андрея так и на самом деле тоже первый вопрос мне запомнился А я забыл Про что но он ня запол первый вопрос кто задавал первый вопрос Вот вот там хорошо и у нас для тебя кое-что есть сейчас я возьму этот Волшебный пакет Спасибо тебе большое за твой контент я конечно мало что понял но было очень интересно спасибо всё спасибо большое L"
}