{
  "video_id": "6_B8iS7z2BU",
  "channel": "HighLoadChannel",
  "title": "Высоконагруженная распределенная система управления современной АЭС / Вадим Подольный (Физприбор)",
  "views": 2070,
  "duration": 2714,
  "published": "2020-04-27T12:25:51-07:00",
  "text": "еще раз всех приветствую меня зовут довольны московский за прибор прям целый завод мы делаем асутп и начать я хотел бы с того что ну во первых я уже один раз на эту тему докладывал в москве и меня очень попросили не повторятся если прошлый раз я рассказывал про то как вообще устроен освод и псы что там происходит с точки зрения хайло да то сегодня я расскажу не только про то как устроена эта система но и про то что из этой системы можно сделать и что у нас из этого получается и как можно сказать почти военные технологии становятся гражданскими где они применяются поехали я сначала коротко пробегусь потому что мы делаем подробно не буду останавливаться есть видео записано из предыдущего хайло да вы все можете его посмотреть на ютюбе она доступна есть расшифровка значит мы делаем полную гамму продукция для сво т.п. начиная от датчиков заканчивая моделированием то есть мы производим и датчики и шкафы и программное обеспечение системы прогнозирования и системы защиты кибербезопасности и все что связанно с надежностью мы производим с точки зрения датчиков это волоконно-оптические датчики тут вот написан перечень их типов преобразователем и что это такое-то проводишь кабель по рядам железной дорогой и с помощью нейронных сетей в принципе можно идентифицировать один рабочий прошел два рабочих копал лопаты или что-то зарыл сразу соответственно сообщить о нарушении безопасности то есть это интеллектуальные датчики это то будущее что в ближайшем времени всех нас ждет с точки зрения измерений это интеллектуальное измерение контроллеры то есть мы производим вот такого вида компьютера 6 юнита вы блейд-серверы гиперконвергентное на разных видах процессора чтобы можно было на разных операционных системах работать если одна уязвимости реализуется то в другое на не реализуется так мы обеспечиваем надежность безопасность резервирования на наших процессорах работаем чуть подробней про это дальше расскажу делаем особо важные элементы управления на жесткой логике когда на плате раз появляется алгоритм и без паяльника взломать эту историю нельзя то есть если произойдет то что произошло на фукусиме и все с блок избегут то наша автоматика будет продолжать работать будет прокачивать воду через реактор расхолаживает его даже после того когда он остановится то что не происходило вот в последнем инциденте вот так вот примерно работы diverse на и разнообразие вот придет кролик поест всю морковку а если на разных процессорах сделать ну кроме морковки кролик наверно остальное не съест подавиться такой вот нехороших кролик это вот шкафы на тестирование с жесткой логикой моргает вот так вот лампочками все можно найти на видео вот а теперь собственно перейдем к тому что вот я не рассказывал раньше после шкафов вся информация поднимается наверх и оператору очень важно уметь в реальном масштабе времени оценивать ситуацию на современных объектах сигналов очень много полумиллиона в будущем мы прогнозируем и миллионы сигналов они с очень высокой частотой приходят изменения да и вообще в системах автоматизации в индустриальном интернете вещей планируется по моему к двадцать пятом году до 50 миллиардов устройств подключенных к сети и все эти данные нужно уметь обрабатывать собирать что-то с ними делать дальше какой то управляющее воздействие совершать анализировать их координировать эти устройства вот для этого мы разработали базу данных реального времени распределенную базу данных реального времени у которой есть очень любопытные характеристики с точки зрения хайло до назвали мы это корешок вот название пошло от того что активная зона реактора на английском называется коры но на русском корешок звучит так вот весело что нижний уровень у нас называется корешок а верхний уровень вершок что же такое корешок представьте что в сети есть коул зулус часть с этих узлов оптимизирована на запись информации то есть получение информации с датчиков и записи и в базу данных часть этих узлов обрабатывает информацию то есть обеспечивает считывание иногда могут писать и читать все все они находятся в сети в сети может быть миллионы десятки сотни миллионов объектов источников информации на один сервер положить это все конечно же нельзя это нужно класть либо в кластер либо в облака как это не назови но в некот некоторую структуру сопряженных серверов в сеть делать это быстро поэтому мы работаем in memory и в режимах низкой нагрузке спокойно уже сбрасываем эту информацию дальше уже в базу данных обычную мы используем либо позарез либо cliff house я дальше об этом расскажу уже для дальнейшей обработки при бадра при пред обработку мы делаем соответственно кластере а дальше уже разруливает на стандартных решениях и которых вы все знаете есть некоторые тезисы которые перед стартом разработки мы перед собой поставили значит вот тут вот они приведены я думаю что повторяйте зачитывать я это все не буду самое главное что в систему может приходить данных больше чем она в целом способна обработать и эта проблема которая требует управляемый деградация системы где градация это тип потери узлов это потеря данных потеря каналов связи потеря электричество для каких-то и за узлах то есть какие-то сбои в любом случае вот я тут привел некоторые сравнения вот например с финте hamda может я где-то не прав поэтому я не стал заполнять вот у нас можно терять данные винтики если потеряются данные вам не придет зарплата это принципиальное отличие потому что если мне если у нас какой-нибудь датчик температуры там в эту секунду данные не передаст он передаст следующую все равно мы их опрашиваем часто но и так далее чтобы не терять время перехожу дальше да плюс еще надо отметить что у нас есть пара релизом вас в т.п. потому что все датчики и исполнительные механизмы могут работать параллельно и это все можно в информационно управляющей системе вас втб в распределенной системе управления разделить и применять технологии параллельных вычислений и мы эту функцию этот функционал внедрили в нашу распределенную систему управления ядром которой является база данных корешок чтобы это все работало базе нужен отдельный interconnect это вот верхняя картинка он синяя сеть красная сеть это подключение узлов но вас вот и принято скорее вот на аэс например вот в так 2 топология сети когда есть две независимые сети где-то бывает трясите но вот классическая структура вот такая то есть в любом случае кумулятивный interconnect сбора с датчиков не должен превышать по скорости общий interconnect сопряжение кластера ну иначе иначе будут проблемы но сейчас это решается если даже повесить это все на blade server to bleed сервера достаточно будет серверов неплохой interconnect облака можно построить на блейд-серверов можно построить на 200 400 гигабит сейчас я сетевые карты это все в пределах родов сейчас доступна и не является каким-то там дорогим удовольствием следовательно вот такая вот у нас конструкция простейшего подключения объектов тут вот какие то терминальные клиенты указаны что мы подключаем что есть за узлы значит есть управляющая но да есть real-time базы данных есть мета-данные которые мы сейчас во второй версии будем включать внутри нашего база данных реального времени потому что мы построим на них индексы что без метаданных оказалось индексировать очень сложно архивный слой оперативный долгосрочный и аварийный архив аварийные но если что-то случится чтоб потом расследовать инциденты логически слой вычислительное ядро генератор отчетов и нужно как-то узлами управлять регистратор сейчас в качестве аки оркестра таро кубер нить из используется как как большинство из вас ничего особенного и клиентский слой для подключения строятся на серверах и граничных конечных узлов то есть int point некий кластер некое облако в котором некоторые узлы выполняют роль граничных узлов они которые выполняют роль туманных узлов то есть мы можем не только выполнять задачи на граничных узлах куда приходят либо кабельное соединение либо беспроводные соединения но также можем и раздавать задача простая вашему вам и это как раз и есть часть параллелизма то есть так называемых кластерных задач если мы можем выполнять кластерные задачи это отдельные приложения которые занимают отдельные ядра или соответствие части времени я der на узлах кластера также клиентские задачи которые выполняются на and pointing сейчас мы можем примерно двести пятьдесят шесть узлов кластерных обеспечить работу на синхронизацию памяти и где-то 2048 клиентских подключений соответственно каждый клиентский узел каждый in the point знает имеет всю информацию о каждом узле из кластера умеет соответственно сразу из коробки балансировать нагрузку ну и все то что нужно для надежной безопасной работы поддерживаются различные технологии которые на это можно построить сейчас дальше расскажу значит вообще что делает кластер фактически мы синхронизируем оперативную память синхронизируемые несколькими будем говорить так механизмами есть репликация когда на всех узлах или на требуемых узлах есть реплика всех данных или соответственно алгоритмы избыточности и шарден га когда мы данные дробим но еще включается система разграничения доступа которая в зависимости от прав доступа может отдельные узлы закреплять за тем или иным соответственно потребителем то есть мы например можем к кластеру выделить один узел задать разграничение доступа что данный узел имеет только к такому тебя по диапазону данных или к такому-то перечню данных и и уже пользователь который получит доступ к этому серверу он лишние данные читать технически не сможет потому что на этом сервере их просто больше нет а за этот сервер уже клиент просто не сможет зайти или группа серверов соответственно мы обеспечиваем алгоритмы консистентной sti в данных в реальном масштабе времени я дальше расскажу как и самое важное что мы являемся персистенции базах данных то есть что это означает это означает что если будут выходить узлы из строя и мы их потихонечку будем менять то данные будут храниться очень долго я не хочу быть вечно это неправильно но поддерживать данные можно требуемое количество времени и замена оборудования на это не повлияет то есть если будет необходимо например увеличить производительность системы то дополнительный вот узлов просто увеличить эту производительность самое важное что увеличение количества данных в системе не приводит к квадратичном у квадратичной зависимости потребности ресурсов что наблюдается у многих современных баз данных особенно транзакционных вот я дальше расскажу почему мы смогли этого добиться дальше мы на этом строим мультитональность но это как раз возможность изоляции данных коз балансировку нагрузки управляемую деградацию отказывал устойчивость с олей все эти слова вам понятны коз мы строим на том что у нас повторяемые данные мы их умеем фильтровать и если как так как это все-таки асу тп и это или eod то в зависимости от небольшого изменения температуры мы можем например не считать это изменением мы ставим мы их не убираем мы просто такому параметру ставим уменьшаем значимы значения его качество значение его достоверности и если в системе будет лавина то мы затрем данные с наименьшим качеством вот так построен коз это чтобы не терять команда чтобы не терять важные данные и на этом соответственно также и балансировка строится и деградации управляемое и отказоустойчивости вот все остальное значит у нас как в любой базе данных есть крут будем говорить вот так вот значит это не весит база данных то есть там таких that runs эти транзакции у нас нету есть определенный приоритет по транзакциям и и следы лет у нас просто запрещен и мы делаем операцию билет в режиме сервисного обслуживания то у нас есть три различных апдейта есть апдейт положил и забыл есть апдейт тоже асинхронный вероятность на blade который однозначно гарантированно синхронить с определенной долей вероятности синхронизируется по всему кластеру и есть строгий апдейт который ну можно сказать транзакционный то есть мы от каждого узла кластера получим подтверждение о синхронизации это очень тяжелая операция но мы таких операций мы стараемся их снизить обычно эта операция типа квитирования тревоги что оператор взял тревогу под управление и все команды управления там включить выключить насос или что-нибудь еще сделать вот идут такими операциями их бесконечно мала по сравнению со всеми остальными данными поэтому мы можем в связи с тем что у нас вероятностные модели мы можем терять у нас отсутствует квадратичная зависимость мы не делаем транзакцию на каждый апдейт это очень важно вот эта вот штука дает нам вот такие возможности и дальше вот есть табличка где соответственно и указан приоритет операций по важности ну так как соответственно строгий апдейт мы еще можем написать что это минус единица и оппозитный апдейт нолик да то есть они важнее чем соответственно обычный апдейт дальше в связи с тем что у нас память синхронизуйте и мы имеем мы сейчас отбрасываем всю систему разграничения доступа представим что у нас просто на всех узлах одинаковая память а то и в реальном времени на синхронизируется мы говорим о том что мы можем построить параллельные вычисления на этих данных и сделать мы это можем без мапри dios классического то есть у нас отсутствует шахмат у нас отсутствует шаг reviews у нас присутствует флаги которые говорят о том что пора данные собрать и что-то с ними сделать или данные готово для того чтобы начать их параллельно обрабатывать для задачи например гидродинамики параллельные вычисления очень актуальны задача на сетках и так далее вероятностный расчет кинетики частиц ровно так же работают и на этих алгоритмов можно строить прогнозирование которое основано на физике и основано на каких-то векторах то есть если мы например будем подключать tensorflow то данные можно спокойно хранить в этой же базе данных все эти лекторов все матрицы все тендеры вот и их некоторым образом синхронить и самое главное что система весьма принес обычно страдает самый но из-за самого медленного или узла страдает весь расчет мы же можем это перераспределять в онлайне и это некая некая такая положительная характеристика базы но ограничения то что у нас в качестве данных в основном дабл и int у нас нет строку нас нет сложных объектов то есть это некоторое то упрощение которое позволяет нам такую систему построить но по сути по и больше не надо ну вот я привожу картинку какие у нас узлы соответственно слева это у нас кластерный узел справа это наш производимое железо граничный узел и что мы сейчас имеем с точки зрения производительности вот если вот взять четвертинку сервера тазе он опять в 42 630 вот тут написано на 10 мегабитах мы получаем характеристики обновления к мастера почти два с половиной миллиона апдейтов за секунду простых апдейтов при том что вот мы раздробили где-то по 350 тысяч потом изменений параметров в секунду на каждый узел то есть на каждый узел приходят поток они вот так вот разделены аккуратненько чтобы не превысить вычислительную способность каждого узла потому что сейчас мы уперлись в производительность процессора сеть все пропускает процессор сеть сетевой карты спокойно эти данные кушает а вот даже зиона для обработки то есть как бы как для числа дробилки не хватает потому что мы во-первых используем всю память кэш процессора перестает работать нам нужно управлять им самостоятельно что мы имеем сейчас на граничных узлах с гигабитным 075 на соответственно кластерном узле два с половиной и сейчас мы тестируемся в яндекс облаке с двадцатью пятью гигабита me там у них помощнее немножко процессор и мы пока не знаем какая будет скорость потому что мы еще не вышли на стабильную работу час идет тестирование на следующем докладе наверное смогу об этом рассказать и что дальше то есть мы сейчас делаем уже вторую версию ядра мы стаскиваем мета-данные внутрь real-time най базы для того чтобы можно было построить более грамотно индексирование мы наверное повысим процентов на 15 20 производительности и дальше соответственно нам нужен уже в пиджей то есть нам нужно корешок собрать на плесах и прогнозируемой мощность вырастет до 10 миллионов апдейтов в секунду просто вот в россии к сожалению недоступные дион и с альтерн и борту ты является санкционной продукцией а то сейчас бы мы уже бы все бы от тестировали поэтому придется придется использовать то что доступно вот и дальше так же сейчас идут тесты на мило ногти с обычными процессорами но интересно будет сделать это с ft j потому что мне кажется что там будут характеристики еще выше это предстоит выяснить но для современных систем управления таких характеристик должно лет на пять точно хватить как мы разрабатывали сначала мы взяли aerospike на нем мы оплатили все что есть все модели взаимодействия дальше мы разработали свой rest протокол дальше у нас появился корешок опиши на базе shared memory дальше мы сделали свою rt api носи и вот потихонечку потихонечку уже перешли к метаданным на пост греси отказались от спайка сейчас у нас есть режим работы корешка как background worker postgres а и мы можем использовать своим управлять это очень удобно потому что вы не вы не задумываетесь о том что вообще как это устроено это практически становится как кэш пост грессов для реального времени и синхронизации между узлами кластера следующая история это как только был облака приземлились мы также метаданные и архив сделали на кликах aussi вот и дальше что мы делаем я рассказал то есть мы пытаемся метаданные включить внутри можно будет отдельный и после с использоваться или и электрика us postgres у нас сейчас пластеризуется с помощью патроне но остальное я рассказал так но этот про тоже дальше ну у нас уже времени остается мало я уже расскажу что на базе этого мы строим примерно вот так вот выглядят дерево объектов наши база данных реального времени у нас есть интерфейс к которому который разработан на стойке веб-технологий мы используем rest для пока него подключения он может быть может подключаться к узлу кластера или к узлу клиента которых вот может быть вот больше двух тысяч я говорил тревоги корректирование тревог разработка видеокадров у нас ведется формате sve je берете любой редактор вы рисуете то что вам интересно вот с разными отраслями вот железная дорога диагностика контроллера him водоочистка реактора вот у нас сам реактор например на одном видео кадре вот-вот надо нам представлены более трех тысяч переменных для и оператор соответственно спокойно понимает где у него перекосы в зоне мы можем строить эффективные интерфейсы это у нас умный дом безопасность умного дома сейчас у нас есть проект панелей в офисе панели на атомной станции где это всё работает старая версия работает важно отметить и соответственно есть backend это редактор в котором создается база данных набираются элементы делается загрузка sve je привязка к тегу который задается в базе данных реального времени можно это все программировать внутри их победить быть поведение то есть у нас встроены два режима сценариев у нас есть клиентские сценарии для пользовательского интерфейса есть клиентские сценарии для построения отчетов и есть кластерная задача к которым есть api и носи api есть на питон резку и можно подцепить любой язык хоть ли хоть р и сделать сопряжение с любым вычислителем который вам заблагорассудится ну а лучше всего писать носи конечно и компилировать права доступа сделали вот мы еще вот тоже чем мы гордимся для оптимизации положение элементов видеокадров на htc вайф вот такую вот игрушку где мы отслеживаем как оператор крутит головой в зависимости от необходимости посмотреть на тот или иной элемент и потом меняем расположение но это соответственно чтобы ускорить реакцию что происходит сейчас так как мы строим сервисную платформу индустриального интернета вещей и просто интернета вещей мы собираем данные по всем различным технологиям беспроводной связи которые есть в том числе в нефтегазовых проектах мы используем шлюзование с ангстрем am это радиосвязь в том числе которая используется военными на крайнем севере и есть замечательный калининградский проект это муар это мир как лора номер наш стандарт очень перспективная технология мы их платочки к себе встраиваем и тоже конечно устройство выпускаем их синхронизируемые дрона этом работает и самый важный наверное проекта котором вот я уже сказал мы сейчас пилотируем вся в яндекс облаке и возможно возможно если все завершится удачно то история с его там и яндекса у нас будет совместная так что спасибо за внимание задавайте вопросы вроде уложился я должен руль и сюда вопросами я вам помогу у нас заросли очаровательные барышни да спасибо за доклад вот такой вопрос как вообще real-time системы и облака между собой дружат уже ты тоже противоположности но вот но началось облака это метод замены сервера если он у вас сломается стойка ваша вы просто сказали это облако вы используете там все те же технологии которые используются в облаке то есть виртуализация но у вас все локально быстрый сервер гипервизор bare metal просто при необходимости если что-то выгорело вы просто вынули железку выкинули и новые поставили если вам нужно увеличить производительность вы дополнительно сервак поставили а если это проект не асутп а с то это йод но эта и беспроводная связь ну конечно же это облако при том их можно централизовать любимый мой вопрос еще один вопрос что насчет защитной автоматики она обычно локально ставится как вы все еще раз защитная автоматика но я вам рассказал про жесткую логику что у нас самые ключевые системы которые будем говорить называется защитой асу тп или например это называется профессиональный у сбт управляющая система технологической а значит usb управлять системой безопасности технологическая у сбт это 10-20 процентов соединения с пк от системы нормальной эксплуатации если общая система должна для того чтобы обеспечивать хорошую экономику то и повышать эффективность технологического процесса кпд то в случае аварии это все не нужно нужно чтобы не взорвалось поэтому у сбт на себя берет все эти функции и обычно что происходит вот на фукусиме когда у них они реакторы остановили но реактор остановить недостаточность было безопасно происходит от бед бета-распад на остаточное тепловыделение в звезда бета-распад а составляет 0 5 процентов от суммарной мощности виктор этого достаточно чтобы расплавить блок то что и произошло на фукусиме поэтому его нужно прокачивать нужен что обработал резервный дизель генератор или подпитка от нужд собственном другого блока прокачивать водичкой реактор вот если этого не делать то сами знаете что произойдет и у нас эти алгоритмы все распаяны на жесткой логике и резервированный алгоритмами 2 из 3 но про это рассказывал в предыдущем посмотрите там очень подробно спасибо теперь слева у меня очень красивым и ждем сейчас подожди секундочку электронная очередь ваш номер 3 0 2 вопроса скажите пожалуйста как какова длина вашего релизного цикла авторы вопросы правильно понял что графитовые стержни тоже вы знаете я слышу два раза и очень плохо можете без микрофонов и ответ нет нет нет нет не получается вот встаньте сюда здесь волшебное место да давайте потому что я слышу 2 никакого кода только магия так прямо в микрофон скажите пожалуйста какова длина вашего релизного цикла это первая просто второй вопрос я правильно понял что с помощью вашей системы управляются графитовые стержни вряд ли у нас есть ежедневные и еженедельные и ежемесячные релизы все зависит от формата изменения но на блок если что-то ставить или на реальный объект то это возможно сделать исключительно после очень долгих процедур верификации и валидации сертификации и аттестации поэтому вот чтобы куда-то что-то поставить цикл длинный для ионов мы ежедневно выпускаем обновления выходят новые видео кадры новые скрипты новые приложения вокруг это то есть там где нет безопасности серьезной той ежедневной а то что касается что про является стержни реактора ну это все делает автоматика на жесткой логике на очень серьезной физики и оператор видят только диагностику чтобы видеть что у него какие-то перекосы по полю или что то еще то есть это это вот именно то что по реактору вы картинку видели я думаю что из-за нее вопрос это диагностика не более я ответил да паромная спасибо за ваш когда клад вы бы не могли поподробнее рассказать о том как вы собираетесь использовать их пиджей вот вот эту вот часть ну это же числа дробилка в сети вы получаете поток данных этот поток данных нужно с ним сделать простые математические операции сложение вычитание умножение сравнения какие-то условия а обычный процессор работает с задержками когда у вас слишком много информации маленькой то эти задержки в итоге очень хорошо чувствуется на их пиджей задержек нет мы планируем выиграть на отсутствие задержек а какое оборудование вы будете использовать для этого и как вы оценили собственно рост производительности ноутбук первых по опыту пальцем в небо но на числа дробилки в пиджей раза в 3 точно должен быть быстрее ну очень хочется попробовать zeon с alter внутри попробуем однозначной в пиджей современные в пиджи и от интела недоступны и альтеру попробуем отдельно то есть сейчас определенные уже работа есть и на их пиджей во первых у нас есть решение по разбору трафика поэтому мы примерно представляем как это работает то есть ну вот просто приземлить более тяжелое ядро корешка на эту историю немножко немножко более длительный процесс и мы хотели бы чтобы кот кодовая база была едина чтобы внося изменения в код и мы получали все таки код пригодны для заливки в пиджей поэтому это тоже представляет определенную проблему с оптимизации большое спасибо спасибо здравствуйте спасибо за доклад хотелось бы узнать как происходит тестирование особенно критических частей те которые там вот это на атомной станции это далее в плане самого коды и вот если это кот допустим носи носи кодируется ли он каким-то специальным образом там и стандарты там меза россии которые там запрещают определенную вещь у допустим просто все использовать если какие-то транслируете ли этот ход в какой-то более безопасный вариант кодируете ли как-то вычисления и как вы это все тестируете дам фолд injection или что то еще но во-первых мы использовали стандарт 99 года сейчас стараемся от него не отходить вот промышленный стандарт там про безопасность понятно в отличие от 14 и 17 плюсов потому что приходится иногда компилировать и пришел ими компиляторами и в зависимости от системы куда мы это компилируем есть же соответственный linux qnx мы получаем разные результаты понятно что есть история с безопасными компиляторами мы ее не используем мы тестируем значит нагрузочное тестирование мы соответственно обеспечиваем тестирование системы в целых то есть всех компонентов ну понятно что регресс дальше мы исследуем во-первых мы натравливает различные средства анализа кода из кода анализаторы мы применяем вот какие не скажу вот это вопрос как говорится секретный вот что-то даже сами пишем для кода анализа дальше фазе facing в принципе это то что дает нам максимальные результаты потому что facing мы делаем умный диапазонный и отлавливаем но прежде всего мы отлавливаем тормоза с помощью этого то есть про безопасность там в принципе мы так пишем что и так все понятно люди опытные уже кгц собаку на этом съели уже лет тридцать работают некоторые вот и самое важное конечно это как эта система работает при высоких нагрузках то есть нагрузочное тестирование для нас представляет самую большую проблему и тут мы играемся именно вот модель именно распределенная система на получается что она сама обеспечивает повышение эффективности тестирования потому что разница на разных узлах она нам даёт очень много информации это то чего нет в других системах то есть мы во вторых мы встраиваем диагностику внутри самой системы то есть у нас первое там несколько несколько тысяч флотов сигналов это сигналы под диагностику которые самбе заполняются то есть у нас еще диагностика мл онлайне на базе которой мы их делаем и коз и мониторинг вообще состояние здоровья системы ну и вот ну как то так много еще чего еще наверно что-то не рассказал несправедливый мир я имел ввиду что фарин там в 1985 году выпускал такую там книгу в айтал процессор с тестированием вот этим фолтын джексоном когда заряженные там его напарника этого все процессор там как транзисторы там это реагирует именно интересно в этом плане как-то производится тестирование не джо и или это во первых это не нужно это в реактор не ставится да во вторых есть заранее радиационно стойкие процессоры то где их необходимо использовать они просто применяются не ушибся тестированные нам этим заниматься бессмысленно что касается жилья за мы тестируем у нас есть все виды тестирования то есть это вибрация кустика рэп всё кроме соответствия безэховой камере то есть лаборатория на заводе позволяет есть соответ термокамеры железа работает вот минус 60 то есть это она стартует при -60 то есть ну вот так если вы имеете ввиду физику здравствуйте спасибо за доклад у меня вопрос такой вы сказали что вы в какой-то момент уперлись цифру правильно еще раз вы уперлись в цеху в плане производительности почему вам не посмотреть в сторону джипы тем более что у вас в основном int и дабл ор и и математические операции как я понял на джипе это такой же процессор на нем будут те же задержки если вы посмотрите что происходит опять же при обработке там тех же будь то той же математики на шейдерах как сейчас все это делается то по из эффективней вот в данном случае потому что на джипу сетевые карты не делаются обратите внимание вы видели хоть одну сетевую карту наджибулла на плесах почему потому что под поточный разбор информации когда вас поточную очередь тропу то разбирать триплу лучшее в пиджей чем джип джипу это для другого чуть-чуть поэтому нет жопу и не будем туда тоже смотреть это понятная игра она нужно будет тогда когда мы тензор flow подключим для уже соответственно аналитики и вращение векторов вот еще и наверное хватит учили чтобы как нет еще есть время доктора начавшего элбане а кафе j и радиации вот но слушать каша еще еще раз но какая радиация но забудьте вы про радиацию но в современный ares радиация есть только в первом контуре ну и что она внутри реактора спасибо ты не этом через на прощание спасибо за доклад так сказать у вас я вижу железное так сказать крутые программисты не только я но и программисты да извините постаралась вот вы упомянули верификацию вот я некоторое время занимался верификацию формальный можете рассказать какими методами пользуются нет посмотрите что такое валидация и верификация вин 2 есть такая штука как ви модель в атомной энергетике есть стандартный standing документы которые говорят как нужно обеспечивать верификацию валидацию так наверное надо не как мы это делаем а что мы верифицируем его лидируем наверное вот нужно вот так задавать вопрос а на этот вопрос есть ответ верификация и валидация требований то есть есть жизненный цикл разработки который начинается от идеи в требования технического задания технического проекта прототипа кодинга ввода в эксплуатацию эксплуатация планово-предупредительного ремонта переход в него выхода из него еще эксплуатации после pp-r of выхода соответственно язык вывода из эксплуатации и в это очень важный момент в атомной энергетике вывод из эксплуатации и соответственно после вывода из эксплуатации тоже есть фаза и туда закладываются на каждой требование параметры безопасности модели угроз защиты нарушить и дальше эти требования согласуются с тем как получилось и проверяется все ли соответствует на каждом этапе или не соответствует и для этого есть тоже в общем то ну формальные признаки согласно которым обеспечивается проверка и внутри каждого из этих признаков вы найдете тоже стандартные регламентирующие документы которые вот как я озвучил как в инфобизнес или угроз защиты нарушителя то есть вот вот примерно так же как они выполняются то есть например в магатэ появился соответствующие документы на 151 моего как раз писали про то что не только в режиме эксплуатации нужны эти документы ноги на каждом цикле и на переходе из каждого цикла то есть но вот так ну и теперь поклон спасибо большое спасибо"
}