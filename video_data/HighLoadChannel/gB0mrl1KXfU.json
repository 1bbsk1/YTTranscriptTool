{
  "video_id": "gB0mrl1KXfU",
  "channel": "HighLoadChannel",
  "title": "Облегчение моделей виртуальных ассистентов Салют/Ибрагим Бадертдинов ,Александр Абрамов(SberDevices)",
  "views": 5769,
  "duration": 1369,
  "published": "2023-01-19T06:59:52-08:00",
  "text": "всем привет спасибо большое за представление и так сразу же начнем смотрите о чем с ним расскажем в общем действительно мы имеем своем распоряжении огромный core модели которые строятся на архитектуре трансформер всем известный которая сможет нем очень много времени на inference очень много память на то чтобы хранится в памяти мысли расскажем о том как мы обучаем какие хаки мы используем для того чтобы ее облегчить какие хотим использована того чтобы ускорить при этом мы расскажем и про часть связанную с машинным обучением как с pipeline ом для обучения так и с против вещами как мы именно страдавший от ускоряем доклад будет разделён на две части 1 скажу я вторую мой коллега и враги погнали итак у нас имеется некоторой архитектура в которую входят в ассистенте в голосовом ассистенте салют входит множество различных задач их очень много но основные такие что-то распознавание речи это про добро от контекста распознавание всевозможных именованных сущностей распознавание намерение аннотирование и исполнение навыка их самое главное конечно же это все нужно для того чтобы вести осознанный интересный эмпатически эмоциональный диалог при этом на задержку настолько дается не больше одной секунды то есть все эти задачи должны последовательно пройти за секунду и никак не более ok начнем с того как устроена наша модель и на всем на чем это все строится строится у нас как я говорил это над большие трансформеры архитектуре в начале нас это был bird large то есть огромный трансформер занимающий в памяти gpu именно вот как есть около двух гигабайт это без учета что мы что то у чем при этом мы обучим его на огромных наборах данных это порядка миллионов различных диалогов огромное количество специальный постриг разметки именно у иных сущностей намерений различные информации фактурный и так далее то есть определение допустим еще тот раз сентимента это определение тональности текста все это мы закидываем наш bird large сейчас у нас уже роберта скоро мы не расскажем для того чтобы эту модель эффективно представляла наш текст виде какого-то вектора мы используем метре клининг а для того чтобы это все огромным мультитаск am обучать мы используем h вот multi-gpu хоровод очень классный очень классный сервис до того что обучать на как над интерпол какой торт из коробки на низкие gpu одну модельку как он работает мы используем его следующим образом у нас есть несколько to suck сейчас их порядка одиннадцати это вся дотана сортируется блокируется раскидывается на несколько году not который правильно учатся над на одной и той же архитектуре но каждая архитектура у нас инициализируется рандомно независимо друг от друга и в конце каждой эпохи обучение места суммируются тем самым получаем еще более лучшую модель которая была просто мирно из весов к модели которым раскидывали на gpu при помощи хоровода и мы получаем такую zbird причем он не простой спирт значит именно с bird он sentence burton представляет эффективно ваши поисковые запросы и кроме этого ввиду того что кроме этих запросов мы также учим его вести диалоги определять какие то именованные существа так далее он еще имеет сверху несколько to suck и поэтому нам требуется размещать в памяти нескольких и плане 1 группу всю эту информацию при этом кроме того что мы используем вещи на уровне распараллеливания вычисления мы также используем способы облегчения модели при помощи дистилляции сжатие что это такое это когда мы открыто большой модели передаем знание более маленькой минимальным потери качества но при этом оно гораздо быстрее имеющие место в памяти занимает в данном случае мы используем в качестве одного такого из примеров labs модель тоже одна из последних известных моделей с эффективным представлением с интарсом викингов при этом использование вот дистилляция maiden гав нашей labs моделях дистилляции нашей большой модели которым огромное обучили позволяет нам уменьшить размерность примерно в три с половиной раза поискового индекса что при этом нам еще при наследовании свойств добавляет 4 процента качеству к левант насти и к скорости поиска 10 процентов и при этом поисковый индекс который мы используем из векторов размерности 300 он всего лишь мне висит в памяти вашего проекта 400 мегабайт также кроме того что мы сжимаем данные мы также должны как-то думать о том что не только наши модель должна быстро работать быть пожалуй то есть меньше памяти занимать она еще должна сохранять адекватность разговора для этого нам нужно еще продумать то насколько она устойчива к атакам и для того чтобы она была устойчива к атакам мы сделали 2 фреймворка первые the simple а у который используют всевозможные лингвистические замены наших символов в одной фразе на ошибочные символы то есть у нас есть матч ошибок и при этом мы еще используем текст фуллер который заменяет наши фразы не на уровне символов а полностью на похожие по написанию но разные по смыслу таким образом наша устойчивость увеличивается и при этом у нас получается что такая устойчивость не только повышает качество модели то есть релевантность разобрать ответов но еще влияет на скорость сходимости то есть вы не за 10 эпох получаете лучшую модель при обучении а допустим за пять или за 7 то есть это на 5 процентов увеличивать скорость наших вычислений именно конечной модели при этом нужно понимать что так у нас модель большая там много to suck которые мы используем в один бочку у нас в лазе для каждой тоски не больше двадцати четырех 32 сэмплов если мы умножим 3 1 на 10 11 самом большом размере 320 значение если мы учтем что мы используем ди джейкс в 100 размером памяти 32 гигабайт и то еле-еле в них влезает то есть такой маленький бачок мы знаем чем больше batch тем лучше качество у трансформеров как что с этим делать ну для этого мы используем следующий подход во первых так у нас базовый подход он снова на метр клининге у нас да есть вопрос который задает пользователь релевантный ответ и не правильный ответ на этот вопрос мы все время кормим тройками мы раньше мы используем только уникальные тройки положительная пара отрицательная пара но далее мы стали использовать full бочонка что это значит что виду того что у нас почти все пары тематик уникальные мы можем говорить что для каждого сэмпла в этом матче все сэмплы которые не в этой строке находится по yandex они тоже являются не релевантными тогда мы можем использовать всю информацию это значит что у нас и 32 примеров баччан квадрата 32 . ради хотя в памяти хранится только 24 и full бочонка мы используем и на при вычислениях на нашем лоси внутри лосса и это нам позволяет как раз таки гораздо больше информации учесть также это позволяет нам что сделать умещаться в памяти не не используя при этом гораздо больший объем данных увеличить скорость сходимости опять же и увеличить точность то что мы больше негативных и позитивных примеров видим за раз при том что мы не увеличили количество сэмплов которые мы видим но мы смотрим не только на свою пару но и на все далее и вишенкой на торте у нас является подход который позволяет нам очень быстро при поставке нового контента переобучать нашей модели потому что что вам сказал до этого момента мы обучаем от двух недель до месяца на ком-нибудь большом суперкомпьютере например на креста фарида и если у нас появляются новые диалоги нам надо на них тоже до обучиться не можем же мы под каждую новую поставку на месяц-два закидывать в очередь модель нету что мы делаем используем самые популярные в последнее время промт лёнинг или адаптеры когда мы фризе им основную часть нашей модели и только fit форвард слои то есть только определенные модельки маленькие поверх трансформером и перри обучаем тем самым просто смещая распределение каких-то примеров до либо распределение лейблов это нам позволяет очень быстро занять за 1 спринт за один релизный цикл получать модель с обновлёнными данными по контенту далее я хочу предоставить слово моему коллеге ибрагим от единого м или инженеру и моему другу он расскажет уже больше не про обучения а именно провинции нас часть спасибо саша приветствую всех у меня зовут ибрагим я буду рассказывать собственный про то как мы ускоряли и облегчали нашей модели и прежде чем я начну хочу озвучить простая пару концептов во первых когда мы говорим про а скорость у нас есть некая реально верхняя граница полотенце если получается выиграть немного дополнительного времени мы всегда думаем про то как можно здесь за использовать побольше печей или по большая модель потому что это напрямую влияет на наше там онлайн метрики и во вторых когда мы говорим про скорость мы еще подразумеваем скорость процессов то скорость того как быстро можно доставить изменения на продакшен будь то баг фиксы или новые фичи потому что это тоже достаточно и критично и здесь наверное я бы еще хотел упомянуть такой момент что все эти улучшения мы делаем при условии отсутствие деградации по определенным параметрам или принципам который мы тоже для себя разработали это естественно метрики за которыми мы следим это естественно и удобство поддержания это в механизмы и того а вы сможем ли мы вообще в случае чего откатиться на прошлые версии наших моделей и за это мы действительно стараемся достаточно сильно среди здесь я наверное упомянуто как в целом вообще работают виртуальные ассистенты или conversational агенты представьте у вас есть некая фраза от пользователя в первую очередь у вас всегда есть штука которая называется intent рекордной за то что определяет намерение пользователя что он хочет поставить таймер включить музыку или просто пообщаться это некая верхнего уровня в а штука или модуль который router-у и трафик дальше если мы попадаем в модуль собеседника а именно про него мы сегодня рассказываем то есть ты просто общение на свободные темы и здесь тоже есть некий свой внутренний int n 3 как найдер потому что трафик напрямую в бал толку и лечить от модель вы отправлять не будете у вас есть некий набор интентов которые например определяет характер персонажа дать называется библия персонажа то есть там как его зовут как он выделить какие другие ответы или это могут быть какие-то кастомные сценарии или промо-акции приурочены к определенным датам и поэтому есть вот такое разделение и начнем мы говорить наверно из блока который называется на татары мы называем так по сути это просто набор классификаторов поверх векторов контекста предложение или просто токенов которые обогащают запрос и эти предсказаний можно использовать либо для какой-то сценарной логике либо просто как хищник примеру в аранжировке которые уже подбирает наиболее интересную релевантную реплику в диалоге если мы говорим про она татары вот это как раз сто про что а саша говорил у нас есть такой а двухэтапный подход когда есть базовая сильная модель викторе затар который умеет мочить там похожие по смыслу а фразы но разные по написанию примерно в одну точку семантическом пространстве и потом мы используем целую плеяду много-много них очень а простых и легких моделей которые могут собственно решить конкретную задачу например определить является ли текущей контекст провокационным или определи эмоцию или тематику а данного запроса и это намного быстрее чем естественно обучать там 20 огромных моделей это легче потому что сами модели маленький и это главное удобнее то есть это позволяет по сути добавлять новый она татар или задачи очень просто нужно просто небольшой объем данных до обучить эту простую сетку и закинуть и здесь же можно говорить и там право замену модели то есть они разделены но появляется момент за которым нужно следить эта консистентной потому что когда следующая модель напрямую зависит от викторов предыдущий нужно следить за тем что их версия совпадают и если к примеру при этом есть вектор а не сильно разъедутся на каких-то простых функциональных тестов это даже можно не заметить поэтому за этим нужно следить ну и соответственно здесь же можно говорить про ускорение когда есть вот такое выделение базовых викторе zatarra его можно обложить не холодным и горячим кэш он вообще скажу по секрету половина распределения то есть я не знаю там условно возьмите кэш в 40 тысяч разных фраз и вы покроете половину распределению даже в болталки даже мы в жизни на самом деле когда общаемся у нас примерно от не те же фразы могут быть привет нам как дела и все другое поэтому здесь можно использовать каши и не гонять лишний раз а большую тяжелую модель опять же возвращаемся к общей схеме сейчас мы будем говорить про блоки распознавание интентов и chit chat модель вот здесь я наверное упомяну болталки в целом задачу видения просто беседы можно решать нам двумя способами либо это генеративные сети который автор игорь сиона токен зато кентом восстанавливает ответ примеру g5 вот ну и себя используем большое либо это решать как задачу информационного поиска когда у вас есть огромная база из заготовленных реплик и вам нужно отметить и достать наиболее там подходящий под данный контекст реплики из этой базы и потом как-то отранжировать и вот когда мы говорим про подход основанный на поиски и распознавание интентов и там и там по сути задача сводится к тому чтобы викторе заводь текущей контекст и посмотреть какие кандидаты лучше всего подходит решается с помощью быстрого поиска ближайших соседей мы там самым вы начал начали использовать фаиз нашим требования он в целом удовлетворял поэтому мы и дальше с ним и начали двигаться здесь я просто из ридми скопипастил вы ведь что буквально там в три строчки можно у себя а за использовать но здесь упомяну наверное такое а нюанс дело в том что мы по сути быстрый поиск ближайших соседей используем для разных задач это получается что там разные индексы их можно по-разному конфигурировать и так как примеру там в нашем случае у нас нет большой базой там в миллиард условно разных сэмплов мы ее не используем там product квантизация чтобы это все понижать нам просто хватит вот 16 перевести и опять же мы смотрим вот это определенный трейдов между точностью воспроизводимостью и скоростью которой нас будет устраивать поэтому в случае распознавания интентов и болталки основанные на информационном поиске на этапе а генерации кандидатов мы используем вот связку нашего спирта и фрица здесь же естественно заготовлены все каши и для базы здесь же естественно мы там подобрали настройки и наверное упомянул один небольшой момент которого вроде как в ритме нету это то как вы будете загружать в память собственно этот самый индекс можно либо на циpкa либо на gpu у нас часть индексов на gpu и вот здесь можно сохранить весь большой индекс полностью засорились заводь и загружать либо создать яндекс и загружать шар даме и вот во втором случае можно избежать пике по памяти во время старта пода и это может быть достаточно критично потому что если вывалиться из памяти gpu можно просто завязывать в этом бесконечном рестарте и еще один момент это консистентной про который я уже упоминал она больше относится там вообще в целом ко всему проекту они только двухэтапной системе она татаров дело в том что там мы с самого начала тоже когда мы делали какие-то прототипы мы делали просто у нас есть докер контейнер для нашего проекта и моделью код мы хранили там но естественно там не тот путь по которому нужно свет которым нужно следовать и со временем мы разделили отдельно наш код отдельно определенные конфиги и это мы называем статике это нашей модели и крыши почему мы это сделали дело в том что в принципе релизные процесс я думаю всех компаниях кода и просто каких-то статика и лесов модели они отличаются отличаются по скорости поэтому а в нашем случае намного удобнее там а в docker и держать исключительно код нам библиотеки которые исполняются тогда и докер будет полегче это быстрее загружается в реже ствий из тарту а сами модели и каши уже хранить отдельно и на и во время старта пода просто ice bucket а подтягивать это опять же дает удобства того что можно раздельно обновлять модели и быть уверенными что все в целом будет плюс минус хорошо но здесь нужны проверки то есть проверки между совпадениям версия между кодом и стать и коми и второй момент достаточно критичный это совпадение викторе за тары и каши который к нему лежат поэтому нас есть там куча разных проверок которые оценивают вот эту консистентной между векторами в каше и викторе zatarra и вот здесь наверное я бы хотел тоже подытожить в общем момент который мы использовали во первых когда мы говорили про обучение здесь и про inference указанные те пункты которые нам принесли больше всего пользы то есть во время обучения это мультитаск когда модель базово обучается на разные с энергичные задачи то есть те которые не сильно растаскивают им бединге между собой получается получить более сильную модель естественно multi-gpu на хороводят естественно батч побольше это как раз то как мы получаем более сильную базовую модель викторе zatarra второй нюанс так как нам на in firenze а нужно делать поиск га то есть поисковый запрос из которых база нам здесь помогает понижение размерности то есть когда у нас в базе нет там 1000 fat32 на каждое предложение лежит а всего там размерность 300 это здорово нам экономить память здесь же наверное можно отметить прайд версии реала таки мы сначала когда начинали мы и типа не супер думали мы думали так мы сделаем просто по consistent ник опечатка который от людей приходит или к ошибкам с or на самом деле штука оказалась действительно классный это очень дешёвого и действия которое как саша говорил позволяет и сходимость лучше сделать и немного метрики подтянуть и модель сделать более устойчивой вот здесь же расскажу про дистилляцию это тоже больше про этап обучения а если уже мы говорим про inference то здесь это во первых как я уже сказал использовать кашле где вообще это можно в рамках разумного использовать там библиотеку для быстрого поиска ближайших соседей то то что вам больше нравится наладить все и сиди процессы подумать вот про этот двухэтапный подход когда вам просто нужно промотировать или проставить метки для какого-то текущего контекста то есть делать одну базовую модель викторе затар и кучу маленьких которые решают каждое свою задачу это их причем можно принять прямо на этапе мультитаск а можно потом отдельно чисто fine tune в любом случае это хороший подход и ну наверное последние естественно мы все наши модели переводим в какие-то специальные форматы для inference а они благо есть как и у tensorflow такой упорчик который позволяет уже а там не взял от не нужных вычислений во времена принцы и ужимать сами модели это тоже нам здорово помогает в принципе наверное это все сейчас можете сдавать вашей вопрос интереснее я сашу приглашу тоже на сцену с удовольствием ответим и еще хотел бы упомянуть здесь qr-код но дополнительные материалы там как раз ссылки на разные статьи в том числе за нашим авторством и вы можете просто поглубже прочитать про все о чем мы сегодня говорили классный доклад ребят спасибо огромное мне кажется прям яркий мне кажется будет дефилировали этот доклад и кажется довольно рту был длинным остаточных коротких компактных сжата в очередь сути вы в очень по делу просто супер здорово давайте вопросы микрофон привет спасибо большое за доклад у меня вопрос по поводу поиска ближайших соседей почему вы используете в боится ни hns в в общем а здесь упомяну ну во-первых кафе тоже есть функционал во hns в то есть через грабли скидка это первое второе на самом деле по моему в этом еслиб самые типа оптимизированная вот этот быстрый поиском быстрее по метрикам но я не зря там упомянул что мы начали свои со и мы начали работу над ним еще там 3 года назад и так как он удовлетворяет нашим требованиям и это не та бутылочное горлышко который мы упираемся мы типа его не меня просто под свой образ сконфигурировали и нас это устраивает вот спасибо еще могу добавить то что виду того что у нас в первую очередь tf serving используется под bird мы сейчас смотрим только на фаиз есть еще новая либо скн которая для макса dot продукт поиска используется итак у нас основная метрика поиска то мы скорее всего перейдем вот но у нас были бы нч марки машин себе не сильно выигрывал на самом деле уфа из аскона но у них у обоих выигрывать дойч марки можем оценить google search ходить а вот еще вопросы спасибо за доклад реально очень круто очень быстро очень сжато если можно потом повторить может быть в рамках сбера вопрос если можно не продать as a nice а какова доля предобработки что-то как как бы типа мы скриптах и так далее потому что ну сами наверно знаете к чему здесь наверное можно на два момента разделить в общем первое если мы говорим в целом про машинное обучение я тоже скажу сразу мы всегда продумываем про это мы называем какими-то патчами или и в листиками чтобы в случае файла или деградации модель она вроде можно было быстро и уверенно что-то закрыть эвристик их случае conversational агентов эти конкретные вопросы ответы чтобы грубо говоря не пока пить напротив когда что-то всплывает вот это первый момент таких случаев у нас всегда есть организм и для быстрого и безопасного точечного fix а потому что модели переобучать эту долго это первое второе если мы говорим просто про какую-то базовую предобработки средних кейсов вот как раз саша говорил мы не зря там юзаем от версии реал атаки и огромное количество data set of мы как раз это дело в том что для того чтобы максимально в общем понижать количестве до обработки и вот эти даже фразы там нет никакой вот это или мотивации в даже там пунктуации не убираем как это условно был 34 года назад потому что мы все это используем грубо говоря как вещи в самой модели и мы доверяем ей что она сможет собрать какой-то классный embedding если я правильно правильно если я правильно понял вопрос скорее было про то что если чё на крайний случай каким ручные костыли ты есть есть опять же как раз вагин говорил что есть механизм цензоров вот там точно 2 моделей 3 руб поезд правило который если модели нас подведут они будут работать все надежно еще вопросы ничего себе какой крутяк вы прям обескуражили аудиторию и создать уровня спасибо вам огромное"
}