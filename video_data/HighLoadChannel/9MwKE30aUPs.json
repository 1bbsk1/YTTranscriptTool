{
  "video_id": "9MwKE30aUPs",
  "channel": "HighLoadChannel",
  "title": "Теория и практика использования ClickHouse в реальных приложениях / Александр Зайцев (LifeStreet)",
  "views": 18474,
  "duration": 3076,
  "published": "2018-11-20T12:25:27-08:00",
  "text": "кто и такое почему рассказывал кликал с я директор по разработке в компании livestreet которые кликов использует и кроме того сооснователь alte нити эта компания партнера яндекса который клик house продвигает и помогает в общем yandex сделать из этого какой-то успешная успешную штуку вот поэтому я кликал оси немножко знаю и этим знанием готов с удовольствием делиться вот еще я не брат петь и зайцева меня часто спрашивают нет мы не братья мы просто друг друга хорошо знаем но не brazil это 5 зайцева sperg он а вот о чем мы поговорим всем известно что клик house очень быстрый очень удобный и используется в яндексе что это известно всем чуть менее известно а где и как он ещё используется в каких компаниях и каким образом потому что как яндекс используется там более менее понятно я вам расскажу о том зачем где как клика ос используется кроме яндекса о некоторых компаниях более подробно и какие конкретные задачи каким образом решаются при помощи клика last есть каких средствах лик хаоса вы можете использовать для каких-то своих задач и они были использованы в разных компаниях я подорвал три совершенно разных сценария три примера который показывает кликал совсем разных сторон поэтому я думаю что будет интересно первый вопрос зачем то и зачем нужен crack house ну вроде бы вопрос достаточно очевидная но ответов на него больше чем 1 1 ответ это ради производительность школе krause это просто очень быстро и аналитика накликал оси это очень быстро и и его его часто можно использовать там где что-то другое работает очень медленно или очень плохо 2 мотивы мотивировка это стоимость 1 через стоимость масштабирования например версии к совершенно отличная база данных и она очень хорошо работает если у вас не очень много терабайт данных и это не кусается денежками когда речь идет о сотнях терабайт или петабайтах то стоимость лицензии поддержки уже начинает составлять достаточно существенное существенные цифры и это дорого откликалась бесплатный операционная стоимость это чуть-чуть с другой стороны подход redshift вот отличный аналог очень быстро сделать решение расшивки она будет очень хорошо работать но при этом каждый там час каждый день и каждый месяц вы будете amazon достаточно дорого платить потому что это существенно дорогой сервис google big веря тоже если кто-то пользовался он знает что можно там запустить низко запросов и получить счет на сотни на сотни долларов внезапно откликалась этих проблем эти проблем нету где то есть где используется клик house сейчас кроме яндекса используется в куча совершенно разных бизнесов и компании это в первую очередь аналитика в приложении то есть это вот стан и эскиз который пришел из яндекса во вторую очередь немножко связанная с этим многое от тех компаний использует тоже клик house многочисленные компании которым нужно анализировать логе с разных с разных источников операционные лаги лаги безопасности они загружают в клик house делать какие-то отчеты получают какие то какие то им нужные результаты компании начинают использовать финансовом анализе постепенно большой бизнес там подбирается тоже клика усы на это на это смотрят оклад флаер если кто за кликал сам следит то наверняка слышал это слово это это название этой компании это один из таких существенных контрибьютором из комьюнити и у них очень-очень серьезно я кликал инсталляция ну например они сделали кафка кафка engine для клик хаоса из всего прочего телекоммуникационные компании несколько компаний клик house используют либо как пруфов концепта либо либо уже в продакшене компания использует crack house 1 для мониторинга производственных процессов то есть у них есть какой-то кий процесс по микросхемам они достигают микросхемы списывают кучный параметров там порядка 22 2000 характеристик вот и как-то дальше анализирует там партия хорошая плохая или что то типа этого блокчейн аналитика есть такая компания была xinfa российская она анализ и трем сети вот тоже тоже сделали напрягался причем размер не имеет значения есть много компании используется один сервер маленький и он им позволяет решить их проблемы и гораздо больше проблем компании может быть использует большие кластеры из многих серверов или десятков серверов и если смотреть за рекордами то вот известно индекс 500 серверов с лишним 538 как алексей сказал примерно 25 миллиардов записи в день они там сохраняют livestreet примерно 75 миллиардов записей на 60 серверах и сервов меньше записи больше следующий идет клауд flair из известных мне у них еще меньше серверов всего 36 серверов и еще больше данных 200 миллиардов строк в день и сохранять но рекордсменом здесь будет компания такая наверное вы слышали ее название bloomberg вот у них 102 сервера как они сейчас говорят и примерно оценка триллиона записи в день они туда загружают статистике какой то какое они не очень и очень говорят географически это тоже много вот это вот карта она позвала она показывает такой heat map того где кликал используется в мире тут ярко выделяется россия там китай и америка но если сложить европу то есть просто европейских стран мало поэтому там фитна такой маленький вот можно таких вот 44 на самом деле кластера выделить опять же это сравнить на анализ то есть тут не надо искать абсолютных цифр это анализ посетителей англы которые читают англоязычные материалы на сайте ленись и потому что русскоязычных это то там нету и вот россия украина беларусь скажем так русскоязычная часть часть сообщества это все еще самые большие пользователи потом я сша и канада очень сильно догоняет китай то есть там пол года , почти не было а сейчас китай уже обогнал европу и на самом деле растет и старушка европа в общем тоже не отстает в чем лидер по используем клик хауса это как ни странно франция вот то есть зачем это все рассказываю на это так чтобы показать что кликал становится стандартным решением для анализа больших данных и уже очень много где используется так что если вы его используете вы как бы в правильном тренде если вы еще не используете то можно не бояться что вы там останетесь одни и вам никто не поможет уже очень много много кто этим занимается вот теперь перейдем к примерам то есть примеры конкретного реального использования откликался в несколько компаниях первый пример это рекламная сеть миграция смерти кино клик house то есть вот кто то тут то тоже задавал вопрос с чем-то похожие на самом деле несколько компаний я знаю они смерти кинако ликовал силе перешли ли находятся в процессе второй пример транзакционные хранилище накликала это пример построенный на анти паттернах то есть вот все что не надо сделать клик хаусе советом разработчикам здесь сделано но при этом сделано настолько эффективно что это тем не менее работает и работает гораздо лучше чем типичное транзакционные решение и последний пример это распределение распределенные вычисления накликал из то есть вот опять же был вопрос про так как можно кликать интегрировать hadoop и эту систему я покажу пример как компания сделала наклюкался что-то типа on аналог вот такого а мы определись контейнера следят за локализации данных и так далее для того чтобы посчитать очень нетривиально очень тривиальную задачу livestreet компания этих компания который есть все технологии сопутствующие рекламной сети и занимается on оптимизацией объявлений programmatic петтингом много данных порядка 10 миллиардов событий событий в день при этом там события могут на несколько под событий поцелуйте делиться и много клиентов этих данных причем не только люди но гораздо больше различные алгоритмы которые вот занимаются программа think петтингом и компания на самом деле прошла долгий тернистый путь и я о нем рассказывал на холоде несколько раз от разных этапах этого пути сначала livestreet пришла с mais quel а с небольшой остановкой на оракале верте q и можно об этом найти рассказ и все было очень хорошо но как бы достаточно быстро стало понятно что данный растут и vertica это дорого вот поэтому искались различной альтернатива здесь некоторые из них перечислены на самом деле мы сделали пруфов концепт или и иногда прав и в концепт иногда просто performance тестирования почти всех аналитических баз данных которые вот в это в этот период с 13 по 16 год вы были были доступны на рынке и примерно подходили по функциональности и отчасти из них я тоже рассказал на холоде если бы посмотреть как это ты стояла задача мигрировать верти ки в первый сет потому что данные росли что нам не сиделось то их данные росли и росли они экспоненциально несколь от потом они как бы вышли на полку но тем не менее вот и как бы прогнозирую этот рост и прогнозируя that demand который шел из требований бизнеса на объем данных по которым нужно делать какую-то аналитику было понятно что скоро пойдет разговор петабайтах a петабайт и сбыт обед оплатить уже очень дорого поэтому как бы искали искали альтернативу а куда то есть куда уходить и долгое время как бы было уже непонятно куда уходить потому что одной стороны есть коммерческие базы данных они вроде бы работают неплохо некоторые почти также хорошо как vertica некоторые похожи вот но дорогие то есть ничего дешевле не все дешевле найти как бы или лучше не удавалось с другой стороны есть open source решения которых не очень много то есть для аналитики их там можно пересчитать по пальцам и они бесплатные часто или там дешёвые но работают медленно и в них часто не хватает многое-многое нужные или полезной функциональности и вот посередине того чтобы совмещала все то хорошее что есть коммерческих базах данных и все это бесплатно и что есть в конкурс а ничего не было ничего не было ничего не было до тех пор пока неожиданно яндекс не вытащил как кролика фокусника шапки откликалась потому что это было совершенно наверное для внешних людей в не индекса решение неожиданное до сих пор задают вопрос а зачем но тем не менее вот и сразу летом 16 года мы стали смотреть а что же такое клик хаос и оказалось что он иногда даже может быть быстрее вертик и вот здесь вот по по некоторым нашим сценарием мы тестировали разные сценарии на разных запросах и если запрос использовала только одну таблицу то есть без всяких джен of the cliff house был быстрее персики в два раза я не поленился посмотрела все тесты яндекса на днях там тоже самое в два раза у них разница коли house быстрей вертите поэтому они часто об этом говорят но если в запросах есть join это как бы все получается не очень однозначно и увлекалась может быть медленнее версии в два раза а если чуть-чуть запрос подправить или переписать на примерно равные то есть ну неплохо и и бесплатно и получив результаты тестов там посмотрев на с разных сторон а это livestreet поехал на клик house и это был это шестнадцатый год напоминает не сейчас а шестнадцатый год это вот было как в анекдоте про мышей которые плакали кусались но ели кактус и об этом подробно рассказано может быть кто-то видел и там есть видео можно найти всё такое я поэтому не буду подробно об этом рассказывать расскажу только результатах и о нескольких интересных вещах о которых я не рассказывал тогда результаты то что успешный миграция была произведена смерти кино cliff house и больше года система работала оружие уже работает в продакшене кроме того производительность и гибкость она выросла то есть из десяти миллиардов записи которые мы могли позволить себе хранить в день и то недолго теперь livestreet хранить 75 миллиардов записей в день и может это делать три месяца и больше то есть пике если посчитать это два миллиона событий в секунду сохраняется больше миллиона и сколь запросов в день прилетает в эту систему в основном от роботов разных при этом несмотря на то что для клик хауса стали использовать больше серверов чем для vertica экономия иные на железе получилось потому что персики использовались достаточно дорогие счас диски включался sata а почему потому что where текке insert синхронный вот и синхронизация там требует чтобы диски не очень сильно тормозили да и и сеть не очень сильно тормозила то есть достаточно дорогая операция откликался insert асинхронный то есть когда более того можно вообще все локально всегда писать никаких нету на это дополнительных затрат поэтому данные вплеталась можно вставлять гораздо быстрее чем вертик у даже на не самых быстрых дисках она чтение примерно одинаково то есть на самом деле читать чтение cessna на фасад а если это не в рейде сидят это все достаточно быстрый там счас большого проигрыш не будет и конечно же не ограничено лицензии то есть три петабайты данных вот в этих шести серверов а это 60 серверов 3 репликах на самом деле 20 серверов это одна реплика же с триллионам записей фактах и агрегатах ничего подобного на винтике позволить себе конечно не могли вот некоторые важные уроки сейчас я перехожу к практическим вещам в данном примере 1 т фиктивная схема от схемы зависит очень много и вторая генерация эффективного эффективного запросов эффективного и скрыли типичный olap запрос это вот такой то есть select с какими-то часть колонок идет в грубой часть колоннах идет в агрегатные функции есть в которые можно представить как срез куба весь грубой это может быть ставить как проекция то есть поэтому это называется многомерный там анализ данных вот и как бы на не знаете часто это моделируется video star схемы когда есть как бы центральное что-то факт в серединке и характеристики этого факта по странам по лучам и с точки зрения физик физического дизайна как это на таблица ложится то обычно делают нормализованное представление можете нормализовать но это дорого по диску и не очень эффективным по запросам обычно делал делают нормализованное представление то есть таблицы фактов и много много много таблиц измерений но в клик хаосе это работает плохо кто знает почему какие какие версии правильно join a 2 на самом деле две причины первое это потому что кликал осени очень хорошие join и то есть жены есть на неплохие пока плохие вот а второе то что таблице не обновляются обычно вот эти вот таблички которые вокруг с разьемы там надо чего-то иногда менять вот там название клиента что ни будь то есть все все характеристик название компании какой pricing и прочее она все все по сторонам вот и от не работает и выход из этого на самом деле в клика узи есть даже целых два первое это использование масел of external декстрин с это то что помогает ну на 90 процентов решить проблему с состав схемы с апдейтами впрочем а второе то используем массивов может mozilla они тоже помогает избавиться от от джиннов и от их проблем с нормализации то есть чем кларк хороши словари во-первых не нужен join во вторых они обновляем и это две две основные причины причем с марта 18 года появилась возможность недокументированная в документации в об этом не найдете что можно словарь и обновлять частично то есть только те записи которые поменялись практически как как таблица и 3 третья важная штука это что они всегда в памяти поэтому join и в общем-то работают быстрее join и со словарем чем если бы это была таблица которое лежит на диске еще не не факт что она в каше скорее всего что нет тем краше массивы тоже не нужен join да кроме того это компания компактное представление один-ко-многим и на мой взгляд массива сделано для гиков потому что вы если кто здесь вторую презентацию сидит и смотрел то что рассказывает алексей и слушал то вот он и вот эти все эти гиковски и конструкции демонстрировал лямда функции прочее то есть для стилистов прямо можно сказать сделали пили krause и это на это это это дает не а не для красного словца это на самом деле очень мощная функциональность которая позволяет делать многие вещи очень просто и элегантно но типичные примеры которые помогают решать массивы опять же эти примеры я решил что они просто простые и достаточно наглядные поиск по тегам если у вас есть там хэштеги на и вы хотите найти какие-то записи по хэштегу поиск по киви или парам тоже какие то есть атрибуты со значениями или хранение списка ключей который вам нужно перевести во что-то другое заметить назначение все эти задачи можно решить без массивов да то есть ты где можно не знаем кого-то в строчку положить и регулярным выражением выбирать или в отдельную таблицу то придется делать join в массе в кри классе ни чего не нужно делать достаточно описать массив массив стрингов для хэштегов или вложенная структура для систем типики вылью сложная структура на самом деле это не очень может быть удачное название это два массива которые имеют там об общей общую часть в названии и некоторые там еще характеристики связаны и по тегу искать очень просто то есть функциях с который проявляет что в массиве есть элемент все нашли все записи которые относятся к наш конференции поиск по сабаоди чуть сложнее надо нам сначала найти индекс ключа а потом уже взять элемент с этим индексом и проверить что значение такой какой нам нужно но тем не менее очень просто и компактно регулярное выражение которые бы вы захотели написать если все это хранили в одной строчке она была бы вверх коряво во вторых работала скорее всего гораздо дольше чем два массива то есть не скорее всего дольше а другой пример это вас есть массив котором вы храните айди какие-то и вы можете перевести их в имена функция прямо под это типичная лямда функция вот которые вы передаете туда прям до выражение она каждому один из словаря вытаскивает значение имени аналогичным образом можно сделать и поиск передается функция которая indicate и проверяет что элементы чему соответствует то есть вот эти вещи очень сильно упрощают схему и решает куча-куча запросов куча куча проблем но следующая проблема с которой мы столкнулись и который хотел бы упомянуть это эффективные запроса кликал осенит планировщик планировщика запросов то есть вообще нету но тем ни менее сложные запросы планировать надо и в каких случаях если в запросе есть несколько дюймов которые вы заворачиваете в по силе кто-то порядок в которым они выполняются имеет имеет значения и второе если запрос распределенные потому что вас при этом запросе только самый внутренний путь selecta он выполняется распределена а все остальное она передается на один сервер которому вы подключились и выполняется там поэтому если у вас за просто с определенными со многими joy нами то нужно выбирать порядок но даже не только это а в более простых случаях тоже иногда следует выполнять работы планировщика и запросы чуть-чуть переписывать вот пример я не знаю насколько видно на большом экране наверное виднее в левой части запрос о который показывает 5 топ топ fivefingers пять стран с самым высоким каким-то к утром вот и он выполняется по две с половиной секунды а в правой части тот же запрос чуть-чуть переписанные мы вместо того чтобы группировать по строке то есть результат лишь на вызовы словаря эта строка стали группировать по ключу in the woman и это быстрее а потом уже результат который тип пять строчек вернется к мы к результата получите словарь методу сплайн секунд запрос выполняется полторы секунды это хорошо похоже похоже пример с переписыванием фильтров здесь запрос по россия вот он выполняется сколько там пять секунд до вот если мыло перепишем таким образом что будем сравнивать опять не строку числа с каким-то с этом тех ключей которые нам относятся к россии то это будет гораздо быстрее вот таких трюков на самом деле много и они позволяют существенно ускорить запроса который вам кажется что уже работает быстро или наоборот работает медленно их может сделать еще быстрее основные принципы здесь оптимизации запросов это максимум работы распределенном режиме использовать сортировка по минимальным типом вот как отделаны in the патентом если есть какие-то join и словарь это лучше сделать в последнюю очередь в самую последнюю когда у вас уже данные хотя бы частично сгруппированы тогда операция join a или вызов словаря будет меньше 1 вызываться это быстрее и вот этот трюк замены фильтров есть и другие еще техники есть апреля только те которые как бы продемонстрировал их немало и все они позволяют существенно иногда ускорить выполнение запросов так переходим к следующему примеру компания x тоже из сша и что она делает это была задача оффлайн связывания транзакции транзакции связаны транзакций показа рекламы и моделирование этих разных разных моделей связывания в чем состоит как бы задачей сценарий обычный посетитель заходит на сайт скажем 20 раз месяц разных с разных объявлений или просто так иногда приходит без всякого и объявлением потому что помнят этот сайт смотрят какие-то продукты кладет их корзину вынимает из корзины и в конце концов что-то покупает резонные вопросы кому надо проводить за рекламу если надо и какая реклама на самом деле на него повлияло если если повлияло то есть почему он купил и как сделать чтобы люди похожие на этого человека тоже покупали для того чтобы эту задачу решить нужно связывать события которые происходят на веб-сайте правильным образом как как-то между ними выстраивать устраивать связь потом мог передавать уже для анализов какой-то ты the warehouse и на основании этого анализа строить модели кому какую рекламу показывать рекламная транзакции это набор связанных событий пользователя который начинается к показу объявление дальше что-то происходит потом может быть покупка а потом мог быть покупки в покупки например если это какая-то мобильное приложение или мобильная игра то обычно установка приложения бесплатно а если что-то там дальше делать это за это может могут потребоваться денежки и как раз чем больше человек потратят в приложении тем он тем он ценнее вот это надо всю связать есть много моделей связывания самый популярный и the last and traction для interaction а это может быть либо клик либо показ вот бывает first interaction первое что привело человека на этот сайт бывает какая там линейной комбинации модели затуханием и и прочее и как это все работало изначально была такая был runtime да и было кассандра которая использовалась как transaction сторож в них ранее все транзакции уже связаны и когда приходит события какой-то в runtime пока сказать страницы или что-то то делся запрос в кассандру есть такая есть такой человек или нет доставались транзакции которые к нему относятся и производил связыванию то есть если так повезло что есть транзак найди в запросе то это легко обычно не везет поэтому надо было найти последнюю скажем транзакцию или транс секс с последним кликам и так далее вот это все очень хорошо работала пока а трибьют атрибуция было или связаны было к последнему клику потому что кликов скажем 10 миллионов день вот 300 миллионов в месяц если на месяц ставить окно и это сколько в кассандре все в памяти должно быть она работал быстро потому что нам требуется runtime у быстро ответить that enables примерно 10 15 серверов а вот когда захотели к показу привязывать транзакцию то сразу получилось не так весело а почему почему не допил ну да видно что в 300 в 30 раз больше событий надо охранять хранить и соответственно в 30 раз больше серверов и это получается как это астрономическая цифры то есть край держать 300 или там даты до 500 серверов для того чтобы делать связывания при том что в рандами на самом деле серверов существенно меньше это какая-то неправильная цифра и стали думать а что собственно делать и выше накликал а как это делать накликал и сям то есть на первый взгляд кажется что это наборы антипа тарнов транзакции растет то есть мы к ней подцепляем все новые и новые венты то есть она мы это был окликал из в общем не очень хорошо работать с минуты был объектами на потом когда нам приходит посетитель по нам нужно вытащить его транзакции по ключу по его визит ради это тоже point клея к оси так не делают то есть обычно увлекался какие-то большие раньше она тут нам нужно достать низко записей тоже антипатр кроме того транзакция была в три sony исторически так и переписывает ничего не хотели поэтому хотели прямо хранить вот весь этот вот джейсон не структурирована а если надо что-то из него вытаскивать и это тоже анти паттерна то есть набор вот антипа тарнов и тем не менее получилось сделать систему которая уж конечно работала то есть что было сделано появился кликал из в который забрасывались логе разбитые на записи и появился attribution сервис второй такой облачко который получал из этого клик хаоса логин после этого для каждой записи по визит рэнди получал транзакции которая еще там могли быть не до отработаны плюс снапшоты транзакции уже связанных то есть результаты предыдущего работы к этому из них уже отдела логику выбирал правильно транзакцию присоединял новые события опять записывал лук лук уходил обратно в клик house то есть это вот система такая постоянно цикле ru ющая и кроме того уходил уже в data warehouse чтобы чтобы там чтобы там это анализировать вот именно в таком виде она работала не очень очень хорошо поэтому чтобы николасу было проще то когда шел запрос по визит ради группировались эти запросы в блоке по 1000 или по 2000 визит ради вытаскивали для всех этих 1000 или 2000 человек все транзакции и и тогда все это все все заработало есть посмотреть внутрь клик хаоса то на самом деле там всего три таблицы основных которые все это улыбка обслуживает первая таблица в которой заливаются логе нашем блоге вот практически без обработки потом вторая таблица в которую через материала east view из этих блоков вы кусались те которые еще не а трибьют не атрибут от ивенты то есть не связаны опять же через мутило east view в из этих логов вытаскивать из транзакции для построения snapshot а специальном связей устроилась на счет то есть с последнее состояние последнее состояние транзакции накопленная вот здесь написан текст на иску или нам ну вот я хотел прокомментировать ним несколько важных вещей первое это возможность в клик рыси из женщина вытаскивать колонки поля то есть клика оси есть некоторые методы лет работы с джейсоном очень очень примитивное вот видят парам экстракт семейства методов позволяет из женщина вытаскивается атрибута атрибута любые атрибуты то есть первое первое попадание срабатывает и таким образом можно вытащить там транзакций найди или визит ради это раз второе здесь использована такая хитрая материала ест поля что это значит это значит что вы вставляете в таблицу вы его в таблице вставить не можете то есть она никак не вставляется она всегда вычисляется при вставке вычисляется и хранится то есть то есть при при вставке кликал сделает за вас работу и уже вытаскивается джейсона то что вам потом понадобятся второй материала сил в ренне 2 вторая таблица мы в данном случае началась ее этого для необработанных строк она как раз использует вот это вот первую исходную таблицу с практически с сырыми налогами вот и что делает во-первых меняет сортировку то сортировка теперь идет по визит ради потому что нам нужно быстро вытаскивать именно а по конкретному человеку его транзакцию и вторая важная вещь это то что поменялось индекс гранулы рите то есть если вы видели мер 3 там обычно всегда по дефолту 8192 стоит индекс граница что такое что это такое это параметр разреженность индекс откликалась индекс разряженный он никогда не индексирует каждую запись он там каждый 8000 вот и это хорошо когда требуется много данных прочитать на плохо когда немножко потому что большой вверх от и вот если его уменьшать этот индекс длиной граница мы уменьшаем overhead сильно уменьшен до единицы нельзя по нашей может появиться не хватить индексы гдов памяти хранится ослепшего использует еще несколько интересных функций клик хаоса во первых это aggregates 3 алексей не им чуть чуть рассказал вот его кредите 03 хранится агар макс агар макс то есть это аргумент это последняя строчка состояние транзакции соответствующие последнему таймс темпу то есть транзакции все время как бы новые генерятся для данного визита и самые последние состояние вот этого транзакций то есть мы добавили event у нас появилось новое состояние она опять попала в кликал и через агар макс вот в этом матери зова нам представление мы всегда можем получить актуально и актуальное состояние результаты связывания отвязанный от runtime а при этом хранится и обрабатывается до 3 миллиардов транзакции в месяц на порядок больше чем было в кассандре вто в типичной транзакционный системе и все это обслуживается кластер он всего лишь 10 сервов клик хауса пять серверов и каждый сервер имеет реплику это даже меньше чем было в кассандре для того чтобы сделать клик best атрибуция здесь у нас impression best то есть вместо того чтобы увеличивается количество серверов в 30 раз их удалось уменьшить и последний пример финансовая компания y которая анализе анализировала корреляция изменение котировок акций и задача стояла такая есть примерно 5 тысяч акций котировки с каждые здесь 100 миллисекунд известны и данные накапливается 10 лет мы видим на как для некоторых компаний побольше для некоторых по меньше всего примерно 100 миллиардов строк datasette вот и нужно было посчитать изменения корреляции изменений то есть что такое здесь вот есть две какие-то акции нарисованы и их их котировки то есть если одна идет вверх и 2 идет вверх это положительная корреляция 1 растет и 2 растет если одна идет вверх как вот ближе к концу графика а вторая вниз это отрицательная корреляция то есть когда одна раса другая падает анализируя вот эти вот взаимные изменения можно делать какие-то предсказания финансовом рынке но задача сложная то есть что для этого делается то есть вот у нас есть 100 миллиардов записей в которых есть время акция и цена нам нужно посчитать сначала 100 миллиардов раз ранен difference от логарифма брать цены ранен difference эта функция включался которая разницу между двумя строчками последовательную вычисляет вот а после этого посчитать корреляцию причем корреляцию на причиной для каждой пары для пяти тысяч акций пар 12 с половиной миллионов и это много то есть в 12 половиной миллионов раз надо вычислить вот такую функцию корреляции когда там сложная написано и если кто-то там забыл то вот x с чертой игры к черту это матожидания то есть матожидание папа выборки то есть нужно не только как бы там какие-то корни и сумма посчитать а еще в этих внутри внутри сумм еще одни суммы то есть куча куча вычисление надо произвести 12 половиной миллионов раз да еще и сгруппировать почесал а часов тоже у нас немало и успеть это все за 60 секунд это шутка то есть надо было успеть хоть как-то потому что все это работало очень очень медленно и прежде чем пришел кликал потому что они попробовали а не провали это на ходу пи посчитать на спарке на грин пламя и все это было очень медленно или или дорого то есть можно было как-то посчитать но потом это было дорого а потом прижал кликал из и все стало гораздо лучше то есть что они сделали то есть напоминая проблем у нас локальность у данных потому что а корреляция их нельзя локализовать вот эти вот пары их сколько каждый с каждым мы не можем сложить часть данных на 1 часть на другой и посчитает у нас может быть все данные везде и что не сделали то есть изначально данные локализованы они на каждом из серверов храниться данные про pricing 1 одного определенного набора символов и они символ акции и они не пересекаются вот поэтому можно совершенно параллельный независим посчитать вот эти вот логове there a running де фрaнс от логарифма то все все все это происходит пока параллельное распределена дальше дальше что что решили сделать решили это эти данные уменьшить при этом не потеряв выразительности и уменьшить посредством массивов то есть для каждого отсечки времени сделать массив акции и массив цен таким образом занимает гораздо меньше места данные с ними несколько удобней работать это почти масштаб почти параллельно операции то есть мы параллельно эти частичные считают и потом записаны на один сервер посоветовать можно с реплицировать вот буковка r это знает что мы это со реплицировали нас на всех трех серверах сейчас одинаковые данные вот эти вот массивы и дальше специальным скребком вот из этого вот набора 12 с половиной миллионов корреляции который надо посчитать можно сделать пакеты то есть задача потом ответы 2 с плане 1000 задач по 5000 вот этих вот cree led par корреляций и и эту задачу уже вычислять на конкретном кликал сервер все данные у него есть потому что данная одинаково он может их просто последователь начинает вычислять вот еще раз как это выглядит сначала у нас здесь вот все данные есть в такой структуре время символ то есть акции цена потом мы посчитали лук лук ритер то есть данные той же структуры только мисс цены у нас уже лубрикант из логарифм от ран tiffen ссылка от логарифм потом их переделали то есть у нас получилось время и группы рей по по акциям и по прайсом с реплицировали вот и после этого на генетику текучка задачи скормили уже клик хауса чтобы он насчитал и это работает то есть результаты на прав of concept задача это было подзадача то есть взяли меньше данных и на все на всего на 3 серверах вот эти вот первые два этапа в вычисления локтя терна и заворачивание массивы они примерно по часу занимали об отчислении корреляции целых 50 часов но 50 часов это на самом деле мало потому что раньше них это работало неделями это был большой большой успех и так есть причитают примерно 70 1 секунду вот на этом кластере все считалось но самое главное что эта система она практически без узких мест а есть она масштабируется практически линейно я нет проверили успешное и отмасштабировал и там посчитав конце концов какой-то сервис строит который предоставляет вот эти вот данные по корреляции итак основные уроки вот этих вот всех примеров то что правильная схема это половина успеха и правильно схема с использованием всех самых правильных технологий клик хаоса сами нг агрегате hds вот это вот технологии которые позволяют агрегировать или вот считать вот этот snapshot стоит как частные случаи и существенно упрощают многие вещи mozilla ест views позволяют обойти ограничение в один индекс может быть и это не очень четко проговорил но вот когда мы загружали логе в сырые логе были в таблице с одним индексом а она трибьют логе были в таблице то есть те же самые данные почти отфильтрованный только на винкс но индекс был совсем другим одни и те же данные но разная сортировка и малостью позволяет если вам это нужно обойти такое ограничение кликала со гранулярный индекса и главное распределять данные умна максимально локализовать данные внутри сервера и стараться чтобы запросы тоже использовать эту локализацию там где это возможно максимально и резюмируя вот этот вот небольшое выступление кликал сейчас твердо занял вот место территорию и коммерческих баз данных и open source баз данных именно для аналитики замечательно вписался в этот ландшафт и более того он на самом деле потихонечку начинает других вытеснять потому что искре house то вам не нужен не знаю infinite be и даже вертик это может быть скоро будет не нужно если они сделают нормально поддержкой склеили то есть пользуйтесь этим вопросы вопросы спасибо за доклад очень интересно а были какие-то сравнения с вот собачьи phoenix phoenix совместно со space если вот брать клик хауса и вот вот эту базу со сфинксом phoenix apache phoenix но это апофис метр мы я я не слышал чтобы кто-то сравнивал то есть мы стараемся и яндекс травиться отслеживать все сравнения клик хауса с разными база данных потому что если вдруг что-то окажется быстрее коли хауса то лишь мало видов не может спать по ночам и начинает быстренько его ускорять вот я не слышал о таком сравнении может вот смотрите этот apache феникса то сколь зажег на даиш бойцам hbs в основном предназначен для сценария работ типа key и value то есть я со даже можно назвать колонна вариант отсюда то лучше не стоит лучше называть как light can свобода то есть там в каждой строчке может быть произвольное количество столбцов с произвольной именно это можно сказать про такие системы какаешь байс кассандра короче кланы виктор и на них именно тяжелые аналитически запроса нормально работать не будут или вы можете подумать что они работают нормально если у вас не было никакого опыта работа с к хаусом . добрый день уже довольно много интересуюсь этой темой потому что у меня подсистемы аналитическая но сколько бы я не смотрю на кри house у меня возникает ощущение что treehouse очень хорошо подходит для анализа ивентов чего-то имеют оба как бы таких строк и если мне нужно скажем анализировать много бизнес данных там реляционная модель там куча таблиц довольно больших ecли house насколько я понимаю мне не особо подходит особенно если они как бы меняются правильно ли это или есть примеры которые могут опровергнуть это правильно и это на самом деле правда про большинство специализированных аналитических баз данных то есть они заточены на то что есть одна или несколько больших таблиц и код который имеют обл и много маленьких которые там не знаю медленно-медленно изменяющиеся как как как то так то есть кликал с не подходит как как general переброс аналитика solution не за как oracle да то есть куда можно положить все и там строить какие-то сложные очень сложно и очень сложные запросы довольно кликал из эффективно использовать надо схему выстраивать тем образом которые в клик haute хорошо работает то есть избегать излишней нормализации вот использовать словари стараться делать вот меньше вот этих вот для длинных связей сделать там где нормализацию чтобы чтобы этих связей избежать если съем и таким образом выстроить то тогда аналогичная как бы бизнес-задачи наклюкался может быть решена гораздо более эффективно чем в традиционно реляционной базе данных спасибо большое за доклад у меня вот такой вопрос по последнему кейс по моему там финансовый дабл вот вы сделали у них было аналитика надо было сравнить ну как идут вверх вниз там да да да вот этот и я так понимаю что вы именно систему построили именно под эту аналитику если допустим завтра им понадобится какой-то другой отчет по этим данным нужно заново схему строить загружать данные то есть какую-то пред обработку чтобы получить запрос конечно это использование клик хауса для вполне конкретная задача то есть ее в принципе она более традиционно могла бы быть решена в рамках ходу вот если hadoop это как бы идеальная задача на ходу пит очень медленно и моя цель была продемонстрировать то что накликал оси можно решать задачи которые обычно решаются совершенно другими средствами но при этом сделать от гораздо эффективнее это был конкретную задачу так понятно что что если есть задача чем-то похожи то можно ее как бы похожим образом решать а вот скажите вот вы сказали там в моем 50 часов до обрабатывалась это начиная с самого начала когда загрузили данные получили результаты да да хорошо спасибо большое это на 3 северном кластере все еще вопрос а что никто не хочется задать самый интересный вопрос чтобы получить приз от организаторов приветствую спасибо за доклад все очень интересная немножко наверное не про функционал скорее спросить про использование клика уса с точки зрения стабильности то есть случались ли у вас какой-то какие-то датка rovshen приходилось ли восстанавливать как себя там при этом ведет ли house и случалось ли так что у вас вылетала и реплика в том числе это к слову там еще еще к тому что у crack house и мы допустим сталкивались с проблемой когда он вылезает все-таки за свой лимит и падает как конечно идеальных систем нет и у кликал за тоже есть свои проблемы но я не знаю слышали вы о том чтобы яндекс метрика долго не работала наверное нет то есть как бы она работает надежно вот уже сколько там лет в 2012-13 годы наклюкался про про мой опыт и я могу сказать тоже что у нас никогда не бывало полных отказов какие-то частичные вещи могли случаться но они никогда не были критичными которые критичными настолько чтобы серьезно повлиять на бизнес никогда такого не было кликал с достаточно надежен и не падает там случайным образом вот можно об этом не беспокоиться то есть это не старая вещь это вещи production прийти и доказано многими компаниями но у меня еще вопрос есть драться ну вы сказали что нужно сразу думать хорошо про схему данных а если это не случилось вот у меня данные льется льется льется проходит полгода я понимаю да так жить нельзя мне надо перезаливать данные что сми делать в этом момент ну это зависит конечно от системы есть несколько способов ну да каких близким , да есть несколько способов сделать это практически без остановки например вы можете создать материлась фильм в котором сделать друг-друг другую структуру данных если ее можно 1 однозначно скопировать из ваших то есть если она допускает копирование средствами клик хауса вот там знаю как то экстракт каких-то вещей там поменять прайма реки применять против цианирование то можно сделать массе властью туда ложится на данный приписать но новые как бы будет писаться автоматический а потом просто переключиться на использование этом училась в тон переключить запись второй таблице убить вообще без остановки например такой способ спасибо потом алексей рассказал про he calls купер который тоже если очень большая система больших рука восторг так ласково да да который позволяет копировать кластера"
}