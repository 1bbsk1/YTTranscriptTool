{
  "video_id": "8RUnnXA1eHA",
  "channel": "HighLoadChannel",
  "title": "Особое мнение: предугадываем фрод без дата-сайнса / Александр Сальков (Sportmaster Lab)",
  "views": 401,
  "duration": 2738,
  "published": "2023-01-19T07:01:38-08:00",
  "text": "так ну представили доклад давайте знакомиться меня зовут сальков александр вот я являюсь разработчиком в группе компаний спортмастер конкретного сам лобби вот руковожу направлением даты инженерии и больше десяти лет разрабатывая базы данных и все системы которые так или иначе связаны с базами данных вот когда был молод очень хорошо умел ворог написал свой вариант кафки на лорак ли которые делал то же самое что я от кафка только между инстансами oracle а вот но и дополнительно там еще во всяких разных дата-сайентистов вещах участвовал в частности там делал систему которой данте фиксирует людей по ладошкам овен вот ну и много всякого интересного вот давайте познакомимся много вас компании спортмастер вот потому что спортмастер по факту это группа компаний вот и у нас очень много магазинов больше 500 к чему вообще рассказывай потому что дальше у нас там будут небольшие спойлеры у нас примерно столько источников данных вот у нас очень широкая география поэтому если мы говорим про фродо это роды нужно будет во все географии смотреть вот но и плюсом там у нас порядка 200 городов то есть у нас очень очень очень широко и очень много разбросанно магазинов разным точкам миром вот если говорить про sm лап собственно саму компанию то у нас там это эти подразделения группы компаний спортмастер вот у нас там порядка пласт до 1000 сотрудников 5 офисов разных регионах вот плюс удаленка по всей россии вот давайте собственно пойдем поближе к фродо вообще что принято называть фрод какие классы задачек он решает вот но в первую очередь это отслеживание каких-то подозрительных пользовательских транзакций и нетипичное поведение пользователей также есть класс их родов которые отслеживают нарушение технологического процесса вот так же еще есть очень много разных подклассов они тоже являются антиподами но давайте сконцентрируемся на этих двух потому что именно на этих мы и строили свою систему анти фродо вот что вообще хотели когда делали эту систему соответственно у нас есть компании человек которого условно назовем эксперты будем по ходу доклада к нему возвращаться вот собственно этот эксперт очень хорошо разбирается и понимает в соответственно в типовых операциях которые происходят в магазине вот в их метриках которые эти операции должны покрывать вот и в общей массе знает как себя ведет пользователь в системах и вообще в принципе в магазине или не в магазине в интернете ну в общем каком-то нашем информационном пространстве вот соответственно все системы анти фродо они работают с данными вот и эксперты они тоже работают заданными осталось определиться с какими данными конкретно они работают вот была первая задумка это посадить кучу людей чтобы они осматривали 24 на 7 потому что биография очень широкая какие-то видео с камер вот ловить вроде как бы классно прикольно но дорого не работает вот так же есть ещё вариант слушать там всякие разговоры техподдержки обрабатывать аудиопоток и с помощью него делать какие-то выводы анализа и прочее вот вариант прикрутить не рамку она все сделает за вас эксперты за с посадим будем учить работает тоже так себе эксперты сопротивляются вот поняли что у нас есть очень много систем наших внутренних которых есть много данных вот которыми изначально работали эксперты непосредственно на эти данные мы решили посмотреть вот какие данные смотрели собственно чеки все что с ними связано это программу лояльности наша внутренняя и система с данными персоналий и всякие разные движения товаров и мастер данные вот собственно данные довольно такие стандартные они у многих есть вот где они лежат вот со большая проблема в том что география распределенная и все данные которые воды списка мы посмотрели они в основном либо находятся в суде у нас либо распределены по всей россии по сей день по всему миру со своего широкой географии вот есть плюс что у нас очень большой идти и нас почти все системы это быков и системы вот плюс эти системы у нас из-за того чтобы корпус они у нас плюс минус реляционную леди то есть неструктурированных данных как таковых у нас не оказалось вот объем у нас примерно 350 миллионов сутки из тех из точности сущностей которые мы оценили вот и как я сказал у нас порядка пятиста источников этот минус вот потому что не все данные у нас есть вроде соответственно надо минусы все превратить в плюс и поэтому пошли решать задачку какую задачку решать но соответственно у нас есть сот студии есть какие-то мастер данные какие-то дополнительные вещи нужно за всех магазинов затянуть всю информацию к себе вот во всех магазинах есть локальной базы данных мы базу данных любим вот и сделали собственную систему которая берет и собирает все данные со всех магазинов в один центральный оракал который в суде находится вот но это не тема доклада есть хорошая система замечательным названием вот и мы ее собственно реализовали затянули все данные подсос служили рядышком с теми данными которые у нас необходимое то есть уроки географии избавились вот сделали репликации и она работает вот собственно и объем получился не такой уж большой для оракла все переваривать вот в итоге получили что дано у нас имеются горок а у нас есть еще несколько инстансов оракла которые хранят данные мастер данные данные персонал их там и так далее тому подобное вот есть им с успели который под единой схеме лежат и также есть еще какие-то кафки которые через которые общаются наши в кофе системы и которые передают информацию на фронт вы собственно этими данными тоже мы обогащаемся то есть получили некий набор данных которые у нас уже есть соде и классная думаем пора строить пилот какой самый популярный инструмент в области зад инженерией как вы думаете правильно excel взяли затянули все в excel вот построили сырые данные все накидали несколько отчете cove вот и работает получился классный рабочий инструмент апробировали его начали считаться вот и получили следующие выводы примерно по 1 магазина мы считаемся 90 минут чисто затягиваем все данные себя вот потому что их много в excel медленной или мы его не умеем готовить и вот короче получили примерно такое объем вот время подготовки на одну гипотезу гипотеза это такая вещь который проверяет эксперт на предмет того что есть вот здесь вот или нет здесь правда вот примерно 1 минуту не так уж и много то есть классно в принципе быстро можем проверять вот и соответственно потом обработка уже результатов работы этого всего порядка там 15 20 минут как бы решение получилось рабочие но как мы видим медленно и может 500 магазинов плюсом обработать нужно довольно таки много полтора-два часа примерно работать над одним магазином что как-то долговато вот мы не любим такое подумали что даже есть дата like dat лэйте у нас еще есть часть данных вот и пошли в первую оптимизацию что сделали соответственно выделили на входе что у нас есть несколько источников данных они разнородные то кафки это и москве ли это урок вы и так далее вот данных много и мы хотим считать не по магазину а по всей сети собственно определили себе такой вход вот и пошли собственно оптимизировать что хотим на выходе на выходе хотели посчитать на всех сторонах этих какие-то атрибуты дополнительные какие-то характеристики и затягивать уже на сторону батарейка обогащать какие-то информации и отдавать витрины уже непосредственно пользователям вот классно круто пошли делать вот сделали получилось так что мы на всех системах которые внешне на которых мы считаем дублируем нагрузку то есть так как данные все равно сильно связаны для фродо получается что не все данные ну не все данные получается посчитать на стороне и систем источников вот и пришлось их дополнительно затягивать к себе вот за дублировали данные во первых на системах источников во вторых надо клыки вот считаемся на 2 плечах вот плюсом у самой большим минусом этого решение было то что операционной системы которые мы ходим они все-таки рассчитаны больше на лтп на игрушку нагрузку вот а мы к ним ходим с olap нагрузкой по расчету чего-то вот подумали подумали и такие но все-таки оптимизация должна была случиться вот и выделились не такие плюсы то есть мы уже все таки не заставляем эксперт а вот дополнительные какие-то вещи проводить мы всё читаем соответственно у себя и предоставляем пользователю вы готовы и данные эксперт а вот успеет ли 750 часов по всей сети до 6 часов вот что тоже неплохо было результат достигнут какой-то промежуточный оптимизация удалось но не везде вот как мы думали тогда мы ушли и так сильно но нет вот все-таки визуализации было немножечко еще в экселе сделано потому что пользователь привык работать акселем и было за один шаг к воем невозможно было отучить от excel вот подумали минусы не любим любим плюс и пошли в обозначать что мы хотим сделать вот хотим избавиться от pl теперь нагрузки в бэк-офис системах которые рассчитаны на ламп нагрузку на оборот упал от нагрузки в а л типичных системах вот поставили себе задачу вообще чем ходить за данными потому что в том решение которое было она была немножечко примитивная и мы ходили просто затягивали себя с кличками данные какие-то будут лайк и уже там обрабатывали это все вот хотели так чтобы у нас было масштабируемость то есть чтобы мы все-таки гипотезы могли втыкать так как нам нужна они запись на какой-то долгий промежуток времени вот но и мы любим ускоряться потом ли а вдруг получится еще ускориться вот и пошли во вторую оптимизацию и начали как мы подумали самого простого самой простой выделить для себя интеграции то есть ну взяли готовые инструменты типа скупо флюма и прочих попытались техно травить налили тонкие если тебе все это в ходу получилось так тебя вот так себе и сделали такое решение все-таки разделили на два плеча прикидываемся well типичной сессии в когда ходим реляционные базы данных вот загадать закачиваем в 12 потока это всего в кафку и ставки потом в параллель с парком начинаем высчитывать и складывать ходу вот при этом кафка здесь у нас а не нужно отказоустойчивого у нас есть спокойненько реляционной с которой мы можем если что перри поднять вот решение получилось хитрое довольно мы такие обрадовались классно у нас все работает работает довольно таки быстро его 1 регистрируем это все с помощью airflow все эти потоки прикольно но столкнулись с проблемой что не все системы готова на wan давать инкременты по данным и решили не изобретать ничего нового взяли лент архитектуру просто натянули ее на инкремент вот что по сути архитектура мы взяли данные за предыдущий они взяли лог изменений за текущий батч вот накатили определили какие да не обновились в этом блоге изменений и прокатили его дополнительно на всего 100 ной табличку на с тем самым получилось что внутри лета лейка уже применённый инкремент вот прикольно классно но получился вот такой так вот для примера вот этот так вычисляет инкремент на стороне дата лейка а вот такой дак вычисляет когда забирается инкремент на стороне система источника когда мы просто знаем по какому плечу мы можем получать инкремент вот поэтому когда есть возможность не писать и на примет на стороне батарейка не пишите что получилось получилось мы сделали интеграции и смогли сформировать свой сырых данных вот получили примерно 30 сущности вот очень близких 4 нормальной форме вот все сущности партиции они рвали попадать и потому что расчеты у нас тут в рамках да ты либо в рамках диапазона зад вот и плюсом у нас получился объем достаточно скромный как оказалось это всего 10 гигабайт сжатом виде на при одной реплики вот но это 10 гигабайт это так с одной стороны мало с другой стороны когда это начинаешь анализировать там три четыре пять месяцев глубина но немножко расстраивает вот классно интеграции сделали подумали что в интеграция ускориться не получилось ключу инкремент получилось реализовывать такой страшный вот подумали блин ускориться все-таки надо вот и пошли и анализировать общей гипотезы которые эксперты нам предоставили вот пошли анализировать и выделили следующие что есть общие блоки вот общее блоки в этих гипотезах которые просто-напросто взяли и вынесли его слой передали готов вот и начали считать отдельно кредит 1 этапом перед тем как начинать читать гипотезу то есть после того как мы загрузили данные но перед тем как считать гипотезы вот выделили получилось у нас следующее что общее блоки это какие-то атрибуты на чеках какие-то дополнительные активности сотрудников риски на операциях какие-то получив получилось по ранжировать вот и вообще в принципе различные аналитические функции которые строятся на различную глубину вообще в принципе весь слой при так ригатов у нас получился такой что мы берем и просто какие-то агрегационную литические оконные функции считаем на определенную глубину с определенной и широтой вот и тем самым достигается некий промежуточный слой слой которые потом используются дальнейшая гипотез вот спойлер получилось ускориться вот но классно есть сырые данные есть слой предали готов и мы пошли и собственно собирать гипотезу вот с чего себя представляет тепла гипотеза примерно 80 процентов гипотез как оказывается можно покрыть клеем это второй по популярности инструмент в области инженерии вот гипотеза собираются очень просто когда у нас есть все дополнительная информация все внутри деталей к мы просто выбираем данные спрятаны ригатов обогащением их какими-то данными и справочников вот рассчитываем дополнительные атрибуты специфичными для конкретной гипотезы вот и появились тиком каким-то вы помечаем флагами фродо тот ту или иную строчку вот типовая гипотеза собралась круто собираем решение вот так примерно у нас выглядит так зависимости когда мы грузим данные из какого-то конкретного источника вот мы дальше по сенсорам собираем что вот эти источники которые необходимы для предали гатов у нас готовы собранные и мы можем считать слой непосредственно уже стой перед агрегатов вот ну а дальше все просто считаем гипотезы которые зависят от тех или иных слов 5 агрегатов вот что в итоге получили в этой конкретной оптимизации получили что мы реализовали инкременты бычьи получили многослойную архитектуру выделенную в сырые данные слой при до греха-то слой гипотез вот получили так что получилось так что при до грига ты ускорились ускорились за счет перед агрегатов вот добавили сенсора на появление данных ушли от жестких временных границ вот то есть мы не завязан на конкретные time слоты используем сенсоры вот ну и плюс а мы добавили историчность то есть можно посмотреть на глубину какие гипотезы были рассчитаны и какие фразы были вот и на этом уже делать непосредственно какие-то выводы вот плюс анализировать ну и наконец таки пересадили пользователи с акселя всас вот о чем потом пожалели что непосредственно по ресурсам вот слой сырых данных примерно у нас получилось вот такое все зависит конечно от конкретного потока но в среднем получилось следующее что в кафку мы грузим в один поток с одним гигабайтом с одним ядром вот вождей вас уже чуть побольше свой предо грига the phone жирненькие то местные расчеты основные ядра используется но и типовая гипотеза она уже непосредственно joy нет и плюс немножко досчитывает поэтому она чуть послабее вот теперь собственно примеры самое интересное какие же все-таки гипотезы фродо мы считаем вот пример a3 они направлены на разные области действия вот первый пример это собственно эти флот на кассе когда у нас касик возвращает товар который ранее пробил вот проводит товар со своей скидочкой вот а разницу в себе уже в кармашек вот есть такие замечательные до фондовые операции которые проверяются таким фродом такой гек такую гипотезу рода проверяют вот визор визуализация выглядит простым способом то есть у нас есть список магазинов в которых есть светофорчик который выделен вот и на нем желтым зеленым красным обозначены возможные трудовые операции красненькие понятное дело что там скорее всего точно фрод вот если проваливаемся конкретно в него мы увидим что у нас есть чек который был возвращен в 1420 19 вот и заново проведем в 1425 вот при этом цена у него уменьшилась вот собственно это первый фрод первая гипотеза это на кассе вторая гипотеза она касается систему лояльности вот это когда у нас одна и та же бонусная карта которая используется в системе лояльности очень часто применяется либо в одном магазине либо в магазинах с различной географии за определённый промежуток времени вот визуализация вообще просто это просто табличка в которой у нас есть фильтр по которому мы можем отфильтровывать какое количество подозрительных операций мы считаем некачественными ну то есть которой нужно проверять вот и видим что у нас одна карта действительно которая ходит по в рамках одной даты по карточкам не совершается много множественное количество операций вот у нас операции в принципе магазинах не такие частые как допустим в продуктовых магазинах ну то есть для нас подозрительно когда человек один покупает 45 раз в течение суток течение короткого пример длинного промежутка времени но в одном и том же магазине вот поэтому это не бывать зрительно имеет начинаем проверять и собственно анти фрод процессов выполнения ни для кого не секрет что в многих компаниях есть система теперь для продавцов для сотрудников вот собственно и есть еще дополнительно вот у нас такая же и собственно когда гипотеза состоит в том что в магазинах продавцы зачастую подменяя друг друга и пытаются делать оформлять заказы на коллегу который там еще не выполнил свой план продаж допустим какой-то вот очень просто мы здесь анализируем уже источник данных который относится к персоналу вот мы проверяем их рабочие часы и проверяем чеки проверяем на кого эти чеки были вот есть соответственно все это схлопывается в то что у нас некоторые сотрудники в нерабочее время более продуктивной чем в рабочем есть строчка в который сотрудник там двадцать пять операций трубил в рабочее время и 391 в нерабочее время все мы такие сотрудники были да собственно флот обнаружили посмотрели собственно что дальше происходит вот происходит на самом деле не так много событий и первое это какие то не цитируются дополнительные проверки службы внутренней безопасности которая проверяет это все вот либо проводим какой-то в не плановое мероприятие типа инвентаризации либо еще чего-то вот когда это фродо касается бонусных карт мы их блогерам вот чтобы предотвратить какие-то свои потери вот ну и всякие административное воздействие также там применимым вот что в итоге получилось по текущему решению то есть текущая операция это та итерация которая получилась у нас она сейчас работает вот мы убрали дублирующие нагрузку на внешней системы то есть то что мы хотели уйти от нагрузки в лтп системы с олафом мы победили тем что написали свои инкременты сделали регулируемая по точность запор данных вот мы загрузили все данные в интернет лайк а вот получили посещаемое решение за счет того что добавились многослойность и можно различные своей менять по требованию они единым каким-то модулем это все менять вот сократили время расчета то есть те слой перед агрегатор но позволил там в три раза ускориться с 6 часов до двух часов вот но это не предел потому что есть еще старой гипотезы которые в эти два часа эффект от их нужно просто перевести на новый слой перед агрегатов вот в среднем у нас гипотеза 1 от момента когда ее эксперт придумал до внедрения в каком-то виде проект пробации навроде у нас проходит 3 до 5 дней сейчас вот раньше было сильно хуже в цифрах собственной поговорим о цифрах благо рассказывать не получится в абсолютных цифрах но поговорим про процент и вот примерно 75 80 процентов вот тех прудовых операций которые появляются высвечиваются в рамках системы анти фродо они действительно подтверждают фрод то есть система довольно эффективно работает эти гипотезы который эксперт придумал они действительно есть при жизни и мы их отлавливаем вот недостача в магазинах которые выявили примерно 15 20 процентов недостача не действительно подтвердились фродом но недостаточно самом деле могут быть еще по другим причинам не только и за фродо вот на 15 20 процентов недостач покрываются входами вот выяснили что программа лояльности как ни странно самое такое моя система вот ну да наверно у всех вот у нас сейчас порядка 30 гипотез против на этой системе крутится вот и собственно самый главный ключевой почему без достанется но я думаю в течение доклада было понятно почему без запасаемся потому что у нас есть те самые эксперты которые очень хорошо занимаются поведением людей знают поведение людей и они соответственно знают эвристики делают эвристики которые нам помогают в выделении фродо вот собственно поэтому безу без дату солнца планы на будущее то есть у нас есть еще очень много всяких разных областей которые не покрытых родами вот и будем покрывать вот плюсом у нас есть там этап который был на предыдущих слайдах когда у нас подключать административные ресурсы либо другие другие службы начинают проверять это все вот есть идея подключить накатим анти фронтом чтобы был доступ непосредственно либо к записи можно конкретного кусочку видеокамеры чтобы смотреть проверять что действительно там была эта операция либо нет либо уже непосредственно к своим данным которые необходимы для разбора конкретных ситуаций код если вы презентация была в марте то следующего пункта не было бы но сейчас он есть мы переезжаем активно на клик вот для визуализации это блок вот просто потому что это удобно и быстро и мы любим ускоряться то есть там будет сильный быстрее все мы уже папа болен вот у нас порядка 40 гипотезу в баттлоге который еще предстоит реализовать вот и есть такая авантюрная затея так как у нас есть эти листики и мы хотим все таки немножко провод это сайнса то есть идея по генерировать эти листики с помощью за самим создавать их экспертам на проверку и возможно что-нибудь из этого выстрелит собственно о чем можно поговорить дальше в кулуарах или вообще на стенде вот возможно мы что-то делаем не так у нас было три итерации возможно будет 4 точно будет 4 вот если вы знаете как и на мой лучше сделать можно поговорить вот рассказать как у вас устроена система анти входов в компании вот ну вообще если в принципе у вас нет в системах этих родов то вниз компаниях анти фродо хто мужем рассказать как мы к этому пришли что нужно делать какой вектор выбрать вот куда стремиться ул 1 просто рады видеть пообщаться на стенде тоже что ещё можно посмотреть мы активно пишем на наши хабар вот в частности там очень хорошо описана система лояльности но и в принципе у нас есть вакансии на которой можно посмотреть в том числе по разрабатывать систему indev родов вместе со мной вот так же моя контактная информация если будет что-то интересно пишите звоните звонить не надо решить вот всем спасибо так спасибо за доклад поднимайте руки у кого есть вопросы из онлайна у нас пока вопросов нету так первый ряд вопрос да нормально все окей доклад я думаю что вы все делаете так если чуть не удастся пересечь кулуарах 80 процентов всех задачи решаются скриптами как вы начали делать а потом может так ручьева и цитата сайнц неизвестно для чего иногда вопрос не про это там на стадии была такая фраза ранжирование по рискам операции можно поподробнее штуку ранжирование понятно что к и риски операции какие рисковые операции это как раз таки вот те операции которые ну то есть это выделение групп операции которые вот так на примере четко рассказать не могу как это делается но на примере это это те операции которые например вот по одной карте очень часто очень много проходят вот то есть есть критерий по которому вот это ранжирование проводится что вот эти операции не подозрительны и на них строится фронт фрод а все остальные отбрасываются для аналитики это метод отбора тех гипотез которые мы будем релизе метод отбора тех данных на которых нам будем проверять эту гипотезу сила так че образом как я понял просто не все операции не на всех операциях можно напротив так пока давайте второй ряд такой вопрос с какого вот сколько времени это заняло для начала до конца до с какого года на примерно два года это все заняло примерно 2 года и вот вопрос а не маловато ли 30 узкий путь из маловато поэтому 40 еще в докладе просто раньше у пользователя эксперта было очень долгие этап оценки вот сейчас у них 35 дней новая гипотеза 35 дней новая гипотеза они могут это все проверять вот а раньше была типа 750 часов чтобы на всех магазинах проверить гипотезу vitara не поделю но основной сбор данных у вас занимал zanzibar сбор данных был очень тяжелой до сбор данных был точечные по конкретного магазина выгребались и считали на не может конкретной гипотезы и протез вот такой вопрос что вы имеете ввиду под генерацией люди но пока сами не знаем ну я вот затариться чернов промышленность работа со in this data аналитик то есть это мои такие области и представляю как это устроено собственно у нас есть задумка что мы хотим посмотреть то есть а все ли мы более данных покрываем то которое нам необходимо возможно есть еще что то что нужно проанализировать для того чтобы больше гепатит генерировать понятно то есть это на самом деле исследование данных и генерацию гипотез уже такая я просто грешным делом подумал что вы там хотите нейрон q наверно мода в течение прикрутить к чему по названию этого текста гипотеза генерировать с помощью рэнда имела залога вдруг что-нибудь сложиться не алгоритм карте 3 до генерировать текстовый гипотез не но это бизнес дальше руки спасибо за доклад хотелось бы еще уточнить я правильно понимаю что у вас система анти фродо как бы работает как карающая длань то есть она работает в прошлом то есть не в реальном времени да действительно это те операции которые единственное что быстро работает это работает по аналитике по банковским картам вот то есть она близка к задержкам там в три-четыре часа вот все остальное работает на kunena вчера еще хочу уточнить и вот вы собираете данные инкрементом условно за прошлые сутки почему не собирается инкрементом вот по транзакционному словно то есть ваша базу данных ну или сервис который работает ли отставку кафка там уже можно так делать я понял вопрос можно так делать пока нет потребности бизнес не просто ну то есть ей ничего не мешать в текущем решение мы можем это делать прям близко к real-time а но пока нет запроса на это все спасибо так давайте вот оранжевого до вы сделали что вы умеете мерить точность экспертами и как-то приближать полноту через недостачи в магазине что из этих метрик как вызову строить какие-то метрики по двору все это оцениваете есть вот какие теперь понятный еще раз можно вопрос она теперь вопрос какие основные теперь метрики и решения то есть когда вы планируете что вы хотите улучшить их родина год если где цифры которые вы планируете лучше ну как всегда эти факты про деньги оптимизация денежных потерь вот соответственно раз увеличить на этим и денежной потерян но мы ищем наймы мы ищем потери до ищем вводим новую гипотезу смотрим на проверяем насколько ну то есть программ какой-то промежуток времени действительно были виляет это гипотеза достаточное количество фондовых операций вот и сколько они стоят собственно и дальше оптимизируется уже денежная задача все-таки пассива сколько денежных потерь допустимом не скажу что я так и думал"
}