{
  "video_id": "F0Zif5Enbgc",
  "channel": "HighLoadChannel",
  "title": "Тюнинг параметров TCP-соединений в высокоскоростных сетях / Валерий Красников (Сбербанк )",
  "views": 4443,
  "duration": 2729,
  "published": "2021-10-04T02:47:27-07:00",
  "text": "рад приветствовать вас на конференции вот немного о себе меня зовут красников алерии я работаю в пауз бербанк подразделение сбер дата и конечно же я работаю в саппорте поддержки сервисов кибербезопасности вы скажете что саппорт сети но как бы я достаточно давно работаю с linux и unix с 90-х годов поэтому вызываю выступаю как эксперт по сетевым технологиям вот в этом докладе мы с вами рассмотрим небольшую часть связанную с перегрузкой сетей проблемами связанные с этим вне зависимости от масштабов проекта вот мы и что мы можем сделать как разработчики когда сетевики уже не справляются начинаются проблемы в сетях которые начинают нам мешать оборудование не вывозят как говорится и и т.д. и т.п. этот доклад не очередная история успеха какой-то компании или человека давайте немножко сделаем лирическое отступление все знают закон парето когда эмпирическое правило которое в общем виде формулируется так что 20 процентов усилий дают 80 процентов результата так вот сейчас рассказ про остальные восемь процентов потому что результата на выхлопе немного но вам надо понимать нужен вам результат этот или нет и лично мое мнение что успех который достался в результате 80 процентов этой работы и этих усилий очень даже часто стоит тех 20 процентов результата итак что же мы будем и какие темы затронем сегодня это планирование ресурсов оборудования все понимаю планировать надо микро сервисную архитектуру но на самом деле нас не интересует не столько микро сервисы а сколько влияние этой архитектуры но конкретно на сеть то есть как влияет что влияет обсудим аппаратные буфер оборудования на что он влияет насколько важен поймем что такое микро бёрст и вот и по каким причинам он возникает найдем откуда растут ноги юу-тян каста и связанных с этим каскадных задержек вот и также попробуем понять и разобраться в этой проблеме и понять а стоит ли нам ее вообще решать и в каких случаях и стоит решать слегка загни заглянем в теории 10 пакетов совсем слегка и основное тело нашего доклада это тюнинг таймаутов так называемого рапа-нуи что делать дальше если у нас еще не пропал дух авантюризма продолжать это дело итак поехали все мы знаем что необходимо планировать ресурсы оборудования поддерживающий какой-либо проект и рассчитывать потребности сетей в момент старта проекта очень сложно но скажем честно что никто не задумывается как правильно посчитать сети то есть мы все полагаемся на то что они всегда большие вот самый сложный момент наступает тогда когда проект начинает требовать горизонтального масштабирования и мы начинаем разъезжаться по площадям больше чем может придет предоставить наша инфраструктура на которой запущен проект разъезжаемся по нескольким ходом соды разъезжаются по нескольким городам вот наш проект покидает уютный мир однородных сетей и погружается в управляемый хаос арендованных каналов сетевых маршрутизацией каких-то стыков преобразования интерфейсов и т.д. и т.п. вот во всем этом хаосе собственно компоненты нашего проекта всегда знаю только две точки a и b . это адрес исходной откуда надо отправить пакет . б/у да нам надо в сети доставить пакет и в принципе нам не особо интересно какие у нас сети мы знаем что она у нас либо локально либо нелокальное вот и давайте коротко попробуем пробежаться по железной составляющей сети как железо работает с пакетами и борется с перегрузками но коротко это вот чтобы вы понимали совсем коротко на пальцах когда мы проектируем свой продукт мы видим встроенную архитектуру и знаем какой компонента чем взаимодействует мы видим интернет запрос поступает на frontend дальше он идет на шину на backhand backend запрашивает базу все возвращается назад мы думаем что знаем куда перемещаются наши данные мы думаем что все это контролируем но на самом деле это не так взаимодействуем разных компонент на основе движение данных и не равно сетевому взаимодействию это банально это все знают и все обычные то забывают вот для сетевого инженера ваших компонента чаще всего выглядят вот так как макароны и не для кого не секрет что чем больше точнее чем меньше вы нарезаете ваши сервисы по сравнению там допустим с монолитным приложением тем выше требования к надежной доставки потому что начинает возникать can каскадные сетевые эффекты и допустим если проект требует миллисекунды ход кликов или не дай бог синхронной передачи данных то вот эти все тайм-аут и которые заложены в цепи стеки достаточно большие они начинают все больше и больше сказываться в длинные цепочки сервисов вот и посмотрим собственно почему же это начинает сказываться мы живём в том мире в достаточно реальном мире в котором сети всегда перегружены то есть уже скажем так по моему мнению не было ни перегруженных сетей не было никогда и давайте рассмотрим что такое микро-блеск микро перегрузки посмотрим на них при стороне почему они возникают и ну и в принципе что это такое вот сетевой трафик который видит сетевой администратор дежурная смена он обычно измеряется с усреднением использования канала за одну минуту и график получается достаточно гладеньким ну то есть мы видим что у нас все хорошо но на самом деле все знают что усреднение в несколько секунд и даже в течение одной секунды не не покажет нам реальной картины а почему потому что если в течение секунды интерфейс не был загружен на сто процентов то это не значит что в него не влетело какая-то пачка пакетов в течение нескольких миллисекунд вот и фактически если трафик посмотреть с более высокой степенью детализации допустим в миллисекунды то он будет намного более прерывистой или взрывной вот собственно этот взрывной так называемые есть micro бёрст который возникает когда за миллисекунды или доля миллисекунд сетевой интерфейс принимает объем данных больше чем может переварить типичное микро бёрст и но по опыту длятся от 1 до 100 миллисекунд и практически не видны если вот как здесь на графике вы видите миг роберт был 4 миллисекунды speedcam три с половиной гигабита но понятно что в гигабитный интерфейс пакета все не влезли вот и почему же они возникают все помнят эту картинку давайте разберемся пользовательские запросы и ответы от серверов дискретно то есть мы уже давным-давно оперируем большими блоками большими пачками и кроме того услуги которые чувствительны к задержкам они всегда как бы пытаются получить данные как можно быстрее разными изощренными способами это первый момент в которой возникают перегрузки второй момент общая пропускная способность исходящих портов в тех же центрах обработки данных может превышать таковую входящих портов может превышать таковую исходящих как бы стандартно все работают с перри подпиской все экономят деньги вот и третье почему может возникать микро бёрст это собственно сам протокол tcp которые являются за верхний уровнем но не столько протокол скука его механизма заложенной в нем то есть это механизм медленного старта предотвращения перегрузок при медленном старте медленно увеличение скорости передачи и т.д. и т.п. и мы живем в эпоху скоростных интерфейсов когда 10 гигабит это вообще вот как бы норма в садах 40100 это стандарт является то тоже является нормой живем в эпоху в эпоху перегрузок и вы можете сказать ну а как же сетевое там именитые производители всегда умеют бы фрезеровать ну давайте посмотрим как они умеют для чего нам необходим буфер если вкратце буфер необходим чтобы сохранить тело пакета пока коммутатор или сетевая карта обрабатывает его вот обрабатывает заголовок сгладить поток пакета до скорости выходного интерфейса и контролировать перегрузки если интерфейса на интерфейс поступает одновременно слишком много пакетов тут мы не будем рассматривать технологии стал форвард или к утру но лучше сказать что буфер есть как бы это все скучно давайте просто попробуем посчитать а сколько же нам надо буфера для того чтобы мы банально не потеряли пакет который идет вот у каждого сетевого интерфейса неважно это коммутатор сетевая карта либо виртуальный интерфейс есть очередь отправки tx ринг там tx rx ринг который представляет из себя кольцевые кольцевой буфер туда складываются пакеты для отправки сеть туда складываются пакеты которые приходят из сети ну и естественно память у этих буферах конечно и она имеет свойство заканчиваться в этом случае новому пакету уже некуда поступать у нас возникает так называемый тел дроп и мы можем посчитать что допустим чтобы сохранить все пакеты которые придут к нам на скорости гигабит в течение 200 миллисекунд нам на 25 мегабайт вот соответственного 10 гигабит нам интерфейсе нам надо уже 250 мегабайт на порт это много не все оборудование так умеет и не все могут себе позволить оборудование стоимостью в чугунный мост к сожалению поэтому однозначно как бы конечно надо памяти больше но фактически с другой стороны буфер это место где пакета задерживаются и поэтому недостаточно просто буферизации все знают что мы давайте все что надо мы сделаем приоритизации ok приоритезация то есть классифицируем трафик помечаем в отдельные очереди конфигурируем алгоритмы доставки пакетов разным способам это задача пес но бывает часто что не буферизация некроз мне а спасает от жесткой перегрузка сети не спасает от микро бёрст of и не спасает вот ри transmit of вот конечно вот как показано на этой хрестоматийной картинки достаточно старая cisco что есть разные типы очередей можно пропустить пакеты вперед но в любом случае мы когда переформирован все вот эти очереди для того чтобы пакеты шли раньше или позже и буфер закончился мы все равно пакет отбросим вот и давайте рассмотрим еще немножко вернемся на шаг назад почему перегрузки бывают то есть повторимся перегрузки это не редкое явление трафик может идти из более скоростного менее скоростной интерфейс трафик из нескольких портов с одинаковой скоростью может попасть в один исходящий и ещё одна причина по которой перегрузка может возникнуть это собственно это мы обсудили аппаратные именно проблемы и есть программная часть которая еще одной причиной когда пакет и не влезли в буфер приема-передачи оборудования потерялись мы попали на пире повтор и пришел к нам и так называемый in каст когда повторная передача случиться только через время зашито в тесте стеки а именно 200 миллисекунд вот итак мы знаем что сетевые перегрузки эта норма и мы полагаемся на большое количество механизмов доставки пакетов сетях и верхний уровень естественно этот и протокол для нас мы все на него полагаемся и мы все знаем что ты себе был реализован достаточно давно и механизмы предотвращения перегрузок достаточно эффективны для общих случаев но не всегда эффективны для высокоскоростных сетей мы знаем достаточно большое количество настроек которые можно подкрутить это буфер и сетевых карт это размера ты себе окон это какие-то достаточно тонкие параметры в некоторых случаях все равно не спасает потому что но подкрутили буфере zero вали как-то там изменили размер плавающего окна пакет все равно потерялся пришел он к ст от которого не спасает стандартный тюнинг вот давайте немножко еще рассмотрим что такое н каст это такой зверек который является обычно частым гостем на больших распределенных системах такие как бластера ходу которые выполняют задачи мапри deus какие-нибудь распределенные файловые системы как он возникает то есть вы его возникновение есть следующий сценарий сервер шлет запрос кластеру вот в машины обычно средне одинаковые запрос идет многие к одному в обратную сторону и вот этот запрос когда несколько сотен машин начинает отвечать одному серверу приводит к переполнению именно буфера выходного буфера порта на котором сидит этот сервер то есть переполняется не вышестоящий порта вот именно выходной буфер того кто запросил ответ может быть большой мегабайты гигабайт и все эти ответы долетают до коммутатора ну коммутатор как обычно самой обычной как мы помним не стоимостью в чугунный мост и буфера у него небольшой все данные которые туда не влезли будут сброшены возникают 3 transmit и если у нас происходит активное взаимодействие допустим на уровне сетевых файловых систем то этери transmit и могут стать постоянным потоком и мы в какой-то момент можем получить за счет этих ренан смитов падения производительности ниже то есть мы получим скорость ниже чем нам обеспечивает линейная скорость порта придаст она предоставляет коммутатор на своей линейной скорости то есть эффект такой что пришло несколько тысяч ответов и у нас скорость упала с 10 гигабит до гигабита вроде бы канал есть но все работает очень медленно не так что такое он каст это падение пропускной способности при себе которая происходит по мере того как сетевая нагрузка начинает превышать возможности коммутатора буфере zerowatt пакеты it is open начинает с начинаются тайм-аута которые длятся 200 миллисекунд меню в результате начинается серьезная деградация вот еще раз повторюсь возникает латентность система начинает работать медленно что можно применить для решения этих проблем но собственно первый день проблема это решаются железом то есть мы закидываем железом ставим коммутатору которого буфер по большего пытаемся поуправлять как-то потоком не получилось и наконец последнее решение которое у нас может возможно может быть это мы пытаемся уменьшить время рей transmit of то есть так называемое рту чтобы обеспечить более высокую пропускную способность нашим сервисом вот простой и действенный способ ну как казалось бы взять и просто уменьшить рту до 1 5 миллисекунд вот можно ли ну давайте вспомним немножко теорию и все мы помним хрестоматийной у картинку до боли в глазах знакомую как устанавливается здесь присоединения у нас есть пакет сен клиенты выбирают случайное число отправляет сен пакет который содержит дополнительные значение назад уходит sinok сервера выбирает свое собственное число прибавляет единичку и собственно назад отправляется от завершается handshake мы установили соединение картинка красивая но так ли это давайте поподробнее глянем немножко детальнее как это формируется согласно эры фишки 62 98000 ожидает подтверждение ну то есть время подтверждения или рту она расчетное и если подтверждение не получено то соединение считается потерянным но есть проблема что мы когда начинаем передавать данные новый либо вот на низком уровне мы не понимаем потерялись лили у нас данные либо потерялся у нас ответ для это для нас это единое у нас может плавать могут плавать эти значения поэтому важная часть расчета для формирования ри transmit of это определить сколько времени потребуется чтобы сегмент прошел получателю от получателя вернулся ок и вот эта часть будет называться vertu также если рта будет слишком маленьким то мы получим повторный повтор перину как бы повторную передачу пакетов так называемые ложные ри-ре трансмиссию если она будет слишком большим то мы получим медленную реакцию на потери и лаги собственно все таки но я думаю всем стало уже скучно мы слишком ушли в дебри поэтому есть стандартная как бы мурзилка выразившийся 298 как вычисляется и рту заметили что там есть переменные на который она опирается и есть основное правило которые там декларируется что всякий раз когда она вычисляется оно не должно получиться меньше чем 1 значение его не должно быть меньше чем одну секунду но как мы знаем мы все это рекомендации и несмотря на то что ядро linux придерживается rfc правило нужно чтобы их нарушать вот поэтому мы собственно смотрим что же там внутри и не будем ковырять само ядро посмотрим снаружи на него немножко всегда ли linux устанавливает время перри повтора пакета при его потери в одну секунду а если посмотреть в реальную linux систему то конечно же нет вот выше приведен вывод с виртуальной машиной мы увидим что значение не округлилась до 1 секунда была выставлена в 200 одну миллисекунду это связано с тем что васи собственно в линуксах существует минимальные как бы 200 миллисекунд границы и максимальные 120 собственно об этом можно поковыряться в исходниках посмотреть и второй момент да это как бы вычисляемое значение мы не можем просто так взять и поменять это значение для не для машины скажем а для не можем внести это в настройки в какие-то там допустим сиско целью или ещё куда-то virtual за исключением первоначального установления соединения она не статическая на расчетная и следовательно этот динамический параметр который и для каждого тесте соединения он всегда будет расчетным всегда будет меняться и т.д. и т.п. вот что же можно сделать чтобы нивелировать собственно потеряно которые мы наступили видели transmit of можем попробовать покрутить и форта допустим есть такая фича форвард виртуа рекавери или упреждающие восстановление это алгоритм обнаружения таймаутов повторной передачи вот на самом деле там есть достаточно небольшое количество параметров и так как это старая технология те тайм-аута который там заложено они тоже достаточно большие и не подходят для какого-то более или менее глобального кит тюнинга и не дают эффектов вот и в подавляющем большинстве все-таки наши сети и проекта они не будут работать в одной сети мы не можем просто взять и глобального так где-то что-то настроить чтобы у нас было хорошо то есть у нас нет такой кнопки сделать хорошо вот у нас сети которые живут в разных сводах the traffic бегает между разными городами вы скажите а как же там реинкарнация дата-центра вы и сети суть которых fabric of с фабрики коммутации глобального масштаба ну да конечно же крупные организации стараются строить именно такие сети но у них всегда возникают вопросы как это правильно строить строить перепад песка или без вопрос выносится на усмотрение сетевых архитекторов и т.д. и т.п. но оборудования все разное и касательно пропускной способности если не касаться оборудование надо отметить ситуацию что ситуация с микро перстами которые допустим возникают в результате incase то они с ними не справиться не фабрика без при подписке не с перри подписками неважно какая поскольку мы упираемся него пленкy а выходную скорость интерфейсов которым отключены сервера вот режимы мы не можем повлиять на тайм-аута изменить их просто в виде настроек но можем настроить для разных маршрутов давайте попробуем например так может выглядеть таблицы маршрутизации на каком-нибудь из абстрактных хостов ядро linux позволяет установить какой-нибудь какое-нибудь значение рту для конкретного маршрута свои допустим мы вот здесь взяли и сказали что все что ручаться все 10100 у нас будет сортовой 10 миллисекунд таким образом мы можем установить более низкое значение на соответствующих маршрутах и в случае когда у нас допустим в этой сетке прячется какой-то кластер который нам генерирует достаточно большой поток young ace 3 транс speed of ну то есть incase мы в результате нко ст получаем кучу или transmit of мы можем как-то нивелировать это можно попробовать поиграться со значениями начальной оценкой round trip to им но по моему мнению это не так эффективна и менять это наверное трогать не стоит площадок у нас несколько мы конечно же берем начинаем настраивать то есть та которая поближе ставим поменьше рта та которая подальше ставим побольше хорошо ли это возможно да но нам хочется удобней и мы берем и натягиваем все это на динамическую маршрутизацию вот кто можно настроить с помощью того же демона динамической маршрутизации bird раздавая на виртуальной или физические хасты необходимые настройки вот например таким простым способом это просто выдержка из кусочка конфига настраивается фильтр в котором говорится что если нам прилетел вот этот маршрут выставит для этого маршрута virtual 10 миллисекунд насколько удобно не совсем вы скажете о чем она отличается от той же настройки статической ну как минимум тем что маршрут может быть или не быть она все-таки хочется удобней поэтому мы допустим берём и настраиваем минимальное значение рта с помощью быть же пиком камень эти вот вводим понятие функции настраиваем комьюнити и уже нам не надо на каждой из машин держать какие-то предустановленное значения у нас есть разлита стандартная конфигурация r тут зависимости от сетей будут прилетать разные не привожу здесь полностью готовых рецептов потому что реализации на самом деле уменьшение таймаутов они зависят собственно от вашей фантазии то есть со стороны железного оборудования можно настраивать это вдоль и поперек вот какие проблемы мы можем получить при настройках ну понятно что бездумно крутить эти параметры очень сложно ну и достаточно опасно первая проблема которую мы можем получить и наступить на нее в полный рост это ложное повторной передачи то есть мы установили слишком маленькие рту мы не дождались ответа sinok вспоминаем и мы сами создали ситуацию когда у нас произошел перри повтор вот установили рта больше нас не устраивает реакция на потери медленная ну скажем так это достаточно такой эмпирический подбор творческая работа по подбору тех или иных параметров для разных сетей в вашем конкретном случае и она может быть оправдана для допустим больших распределенных систем которые живут в разных судах вот что дальше но собственно этими настройками я хотел показать что мы не полагаемся конкретно на железо а если железо уже не вывезла не хватило буфера не хватило настроек пес и сам собственно сетевой сам администратор который живет на конкретных машинах или используют арендованной инфраструктуру может повлиять как-то на ри transmit и и может как-то улучшить ситуацию для себя вот мы даем разработчикам не обрабатывать сбои программно а переложить как бы вот эту обработку ретро и на сетевой уровень то есть пожалуйста взяли переложили ну не серебряная пуля но дает возможность в несколько раз в некоторых сегментов уменьшить тайм-аута в приложениях и при этом увеличить скорость обмена кто-то спросит ну а почему не смотрите там допустим стороны сиены ли там какой нибудь другого протокола да можно смотреть поддержка протоколов не всегда возможно вот мы стараемся обойтись тем чем есть применить как можно меньше усилий который даст нам 80 процентов результата помним закон парето что 20 процентов усилий требует дают 80 процентов результата вот но все-таки мое мнение что стоит успех который достанется в результате вот этих остальных 80 процентов исследований часто стоит тех 20 процентов которые мы получим меня собственно все вопросы и это означает что у нас 10 минут на вопросы точно ребята задавайте вопросы не стесняйтесь введение добрый день меня зовут алексей пакостный клэп вы рассказали про тюнинг рта да ну вот мне кажется что это но такой worker почему не использовать допустим конечным control механизмы к их в люксовом ядре есть довольно много почему сатану и почему не пойти в эту сторону до села mini rta тюнинг карту на самом деле он имеет достаточно ограниченную скажем так применение и он его можно использовать лишь достаточно узкоспециализированных моментах то есть еще раз повторюсь допустим мы спланировали какую-то архитектуру сетевую и дальше в результате планирования мы знаем что мы хотим нее влезть это у нас достаточно большой кластер но мы не влезли нам надо масштабироваться нам надо когда-то за купить сетевое оборудование но сетевое оборудование закупиться завтра тайм-аут и нам надо убрать сейчас вот поэтому как бы я не призываю использовать именно тюнинг карту но в некоторых паттернах скажем он может быть оправдан в очень достаточно узких паттернах спасибо 1 1 статьи спасибо за доклад большое очень интересная как бы с сетью смутно знаком так как я просто серверный разработчик вот наверное нацелен доклад к да да и вы упомянули увеличение объема буфера сетевого оборудования как очевидное решение проблемы incase то так да совершено сказали что это слишком дорого вот я знаю что за последние там десять лет память сильно подешевела и как бы можно ведь поставить ну более дешевую память в оборудования или поставить два буфера разных скоростей как кэше процессора если бы крупные сетевые компании нацистские это хотели они бы я думаю это смогли реализовать и вот может быть вы видите еще какую то проблему просто в бездумном увеличение объема буфера может она как бы не решит проблему young as the полностью просто ну вот эти всей изворот и сыр того мне показались немного сложными и универсального рецепта все таки нет но дастся я соглашаюсь это на самом деле изврат который не очень часто оправдан но давайте по поводу увеличения буфера сетевое оборудование стандартное она именно ключевое слово стандартная то есть 10 гигабит ссора гигабит 100 гигабит она имеет всегда стандартизированную скорость передачи увеличивая размер буфера сетевого оборудования мы фактически и увеличиваем задержку в которой с которой будет данные сохранятся в этом сетевом оборудовании да мы можем там взять ту же аресту в которой там 4 8 гигабайт памяти и туда вылетит пакет он обработается но вопрос а когда он оттуда вылетит и 2 вопроса устроит ли нас если он пройдет все 8 гигабайт и собственно вылетит там через пару секунд наверное нет я ответил на ваш вопрос спасибо здрасьте лисиченко игнат компаний яндекс продолжая немножко 1 просто дорогам задали а все-таки вы у себя внутри использовали ли какие-то модификации десятикратно полного тестом бибер тисе пи не зная там ведра тоже гуглом внесенный там 10 теперь что же лет 10 назад делали который как раз целятся в то чтобы полечить эту проблему incase то чтобы съедать тот бен дэвис который может давать выходной протокол выходной буфер не при этом не теряя но миссис вас да я я понял вопрос о 10 себе на самом деле это достаточно старая реализация и она достаточно спорная вот она требует именно модификации протокола я хотел донести что необязательно модифицировать протокол а можно просто подтюнить соединение используя стандартные средства то себе стандартные средства маршрутизации и наверное больше вот этот доклад нацелен на людей у которых нету коммутаторов там как я ранее говорил стоимостью в чугунный мост то есть на более средний класс у которого обычный стандартный коммутатор там не обязательно cisco небольшой сетевой буфер и ростом настроен стандартно и не да совершенно верно вот не прибегая к этому заменит а себе стека ну если ответил на ваши вопрос спасибо следующий вопрос она слева первый ряд пожалуйста спасибо за доклад вы приводили сложную формулу расчета можно ли ее упростить и как посмотрите на идею расчета типа ft t + джиттер плюс некий запас вот так вот на коленке немножко будет ли это близко к каким-то бы сплотиться или или это выстрел него получается смотрите на самом деле расчете рта давайте я немножко от листаю нас line а вот эта формула красивая вы имеете виду вот этот да можно ли упростить до плюс 10 ари тически можно но мы и не в идеальном мире живем грехи не просто так написано и достаточно умными людьми то есть есть определенные моменты которые как бы первый момент это начальное значение с которыми нам надо стартануть и второй момент что клиент которому мы подключаемся он тоже имеет определенную вольность когда ему ответить то есть он может ок отправить допустим не через 200 миллисекунд а через пятьсот неожиданно ну потому что вот такой клиент у нас попался и в связи с этим но мы должны учитывать вот такие как бы первоначально установление относительно его уже рассчитывать все следующие значения то есть это постоянно динамически меняющиеся расчетное значение то есть нельзя просто так взять в лоб посчитать песчинок и получить это это спасибо за доклад дмитрий зайцев компании м3 подскажите пожалуйста вот в докладе не услышал о метриках которые нужно наблюдать при тюнинге да то есть как понять что наши махинации там в ту или иную сторону сорта дали эффект это может быть количество ри transmit of может быть просто ппс просто же bandridge это же не только одна метрика но на самом деле вы сами ответили на свой вопрос то есть метрика действительно практически единственное то количество этих гирей transmit of то есть если вы в результате уменьшения рта вы видите что у вас ри transmit и ушли то вы как бы победитель вот если вы видите что вы уменьшили но у вас их стало больше вам не повезло вы сделали слишком короткие рта или transmit его сгенерировали сами собственно вот и все спасибо большое было интересно это конечно blur я правильно понимаю что основная боль которую вы получаете это когда у нас имеется что-то типа gfs а или не дай бог цех или все что-нибудь когда идет много потоков и мы упираемся в буфер на транзитном коммутаторе над об африке всегда в совершенно верно но тут вот присоединяюсь к вопросам что на самом деле было бы классно и немножко рассказать о том как вы поняли что это микро бёрст это обычно внезапно понимаешь когда уже поздно и второе на самом деле мне кажется что то картелу это классно это ронана то время пока чтобы оттянуть момент закупки правильного коммутатора да оставьте не только арист и и не совсем уже так чугунный мостик правильно работаете с вендорами вот если у вас какие-то рекомендации как не попасть вот уже тут человек говорил я не знаю просите вот что какие-нибудь вот так несколько хороших тезисов что человек который не знает просите надо знать когда ему запросить заранее до запуска проекта несколько лунных мостиков виде top of rack не совсем понял вопрос рекомендации про что ещё раз смотрите давайте так снял сразу голос появился есть люди которые умеют программирование но дамы знает что сети они живут вот по этим законам слова микро бёрст это неожиданность вот какие нибудь такие несколько хороших слов скажите как не надо рассчитывать что сеть будет работать чтобы потом мудре сорта неправильную планируй сетевую инфраструктуру как не надо планировать сеть нет наоборот а как на такой ситуации не надо попадать чтобы потом приходилось вот вот вот читать и рыси думать зачем эти цифры может их упростить но на самом деле все очень просто не надо попадать в ситуацию заложников начального планирования то есть если мы распланировали свою сеть и думаем что дальше ее хватит на какой-то конкретный период времени это не правда вот ее надо всегда модифицировать ее надо всегда подтягивать то есть ее надо всегда апгрейдить то есть у вас процесс апгрейда аппаратного обеспечения должен быть ну примерно такой же как процесс обновления или выхода новых релизов но это в идеальном мире конечно идеального мира нет поэтому мы просто смотрим на мониторинг и говорим что вот у нас через некоторое время не хватит ресурсов шеф дай денег чтобы мы проапгрейдили и вот тут как раз вот этот лак успели ли мы поставить новое оборудование или мы все-таки начинаем читать или все идти у нее чертова там параметры и прочее и оборудование приедет но позже я решил все равно читать надо это правда"
}