{
  "video_id": "tIEZ9Cb5nVc",
  "channel": "HighLoadChannel",
  "title": "Тысяча и один бэкап, или Резервное копирование Compute Cloud / Артемий Капитула",
  "views": 967,
  "duration": 3254,
  "published": "2021-10-04T02:42:06-07:00",
  "text": "приветствую всех я работаю в mail.ru и про нас сходит дежурная шутка что у нас корпоративный браузера secu твой корпоративной сетью а браузер амиго вот это неправда браузера у нас не омега среди прочего среди прочих сервисов мы занимаемся в том числе из сервисов публичного облака это наше подразделение mail.ru cloud solutions то есть мэра поддерживаем публичное облако на платформе о пренс ты так почему я говорю о резервном копировании в наше в наше время мы говорим мы все облачные мы все современные у нас все резервированный проблемы заключается в том то что не проблем не проблема задача что резервирование репликация от эти вот все вещи дублирования данных это защита от отказа защита от отказа одного узла она не спасает нас от ошибки и когда случается ошибка а заметьте я говорю не есть ли говорю когда вот у вас повреждаются данные повреждаются синхронно на всех репликах и вам ничего не остается как и и за бэкапом соответственно решение необходимость делать резервные копии она совершенно ортогонально необходимости реплицировать сервисы то есть обеспечить отказоустойчивость & replication защита от h и отказа резервную копию защита от ошибки в принципе задача резервного копирования кажется простой затем исключением что она очень ресурсоемкая потому что каждый раз когда вы делаете резервную копию вы поднимаете огромный объем данных нагружая этим чудовищно диски и сеть затем выйти данные обрабатываете то есть вы рассчитываете контрольные суммы находите тиф и сжимаете упаковываете и так далее и при этом у вас очень жестко нагружается память и процессор вот допустим если что то не так с платформой вас она просто уйдет легко в такой ситуации особенно когда вас от большой действительно большой объем данных для копирования и третья стадия когда вы закладываете сделанную копию на собственно говоря подписать президент на и хранение здесь сеть опять пищит но она пищит нами не так не так громко как когда вы читаете данные но ей тоже очень плохо вот и диском очень ни слова не слабо приходится когда весь поток данных вливается на постоянное на холодное хранение у нас в облаке в принципе есть две группы клиентов первая группа это клауд на этих клиенты это те кто уже адаптировали свою инфраструктуру свой проект к современной и современным требованиям у них как правило вычислительные узлы stateless а весь необходимый state лежит во внешних сервисах в этом в объектном хранилище в базах данных как сервисах очередь сообщение как сервис они используют и так далее то есть у них все стоит а вы и сервисы с выведены наружу и их бэкапом занимается уже собственно их бэкап делается так как средствами инструмента на котором построен соответствующий сервис но в то же время есть вторая группа клиентов не менее рис по ресурсам то есть потребляем и не меньше чем первое это legacy клиенты старые добрые enterprise или просто клиенты которые пока не переехали на новую методику то видно то есть накано контент на вот на стрессовые вещи и соответственно они строят свою инфраструктуру внутри виртуальных машин от облака не беру только инфраструктуру не сервисы и соответственно их ресурсы их виртуальные машины нуждаются в резервном копировании эти клиенты как правило используют eos то есть компьют сторож сеть в отличие от первых клиент первой группы клиентов которые используют cabernet очередь сообщение как база данных firewall лот балансир как база данных но подобные что есть готовыми сервисами используются в случае клауд на этих клиентов у нас собственно говоря большая часть service worker of она живет где-то на компьютер а все остальное managed сервисы на внешнем уровне и соответственно если допустим момент месяц screw у нас на какой-нибудь кафки соответственно bk backup этого дела будут делаться средствами кафки если база данных поиск и заказ буду делать с весами по сброса и тому подобные вещи на всяческие вещи типа балансиры эффект просто не надо бы копить всегда можно просто перри породить заново инстанциировать а вот а у второй группы клиента все намного хуже они сами конфигурируются свою инфраструктуру свои балансиры как привыкли свои сервисы очередей пас виртуальных машин старик старик совой там не которые поднимают свой с 3 в рамках инфраструктуры вот некоторые поднимают файловые хранилища то есть объемы данных об за хранения данных за их организацию они отвечают так же сами и вот для них очень необходим сервис резервного копирования который будет их по возможности консистентная бы копить как у нас выглядит резервное копирование какую нагрузку на порождают но это примерно 500 терабайт в сутки около 15 петабайт в месяц по данным это такой декларативный объем физический объем который реально вычитывается систем хранения 300 триста пятьдесят терабайт все остальное это тонкие волю мы растем и практически двукратно в год все хотят зажатия под эти бэкапы все хотят инкрементальные бэкапы вот ну хотя не для всех инкрементальный сжать и получается эффективно incremental к более менее хорошо подходит всем отжать ее далеко не всем если кто-то хранить большой объем медиа там там просто ничего сжимать она уже зато при так существенный объем этих данных необходимо сдэк опять очень быстро потому что когда делается бэкап эта нагрузка на инфраструктуру проседает производительность если клиент работает по верхней кромке производительности то по верхней кромке ресурсов которыми выдвигает облака то просадка которую берет которую выдает backup в процессе она мурат его сильно за аффекте и никто не хочет этих этой проблемы в рабочее время поэтому грубо говоря с одиннадцати вечера и до 6 утра у нас аншлаг полнейший вот у нас станут диски у нас стонут сеть у нас звенят allure ты что необычно необычные пик потребления вот ресурсов сети диска процессора и так далее то есть все системы звенят но это нормально диски диски которые мы к бэкапе у нас есть две категории дисков при аллоцирование диски они быстрые и в то одновременно но относительно дорогие их мы рекомендуем под базы данных как правило и по чести и под систему если отклик от системы необходим есть тонкие диски севске вот они медленнее чем при аллоцирование вот они заметно более дешевые late in se poate из сего времени обслуживания диска у нас диски разных классов грубо говоря одни дают там от 100 микросекунд латексе вот который лежат совсем на гипервизорах некоторые дают нам 0305 миллисекунды и самые медленные диски но самые дешевые хорошего подходит которые на которых клиенты активно хранят много данных это серовский ssd и hdd там light если может вообще говоря выходить в десяток миллисекунд и это зафиксировано это известно это такой бред это ограничение платформы это ее рабочие рамки у нас диски подключаются разными способами есть риск азии диски которые подключаются на хост с ними очень классно работать средствами операционной системы вообще просто замечательно а я сербские диски которые я операционная система зачастую не может подключить и там их нужно бэкапить каким-нибудь хитрым инструментами то есть там и через youtube space не через youtube space and youth расписные компоненты и взор space не библиотеки для библиотеки клиентские sefa и диски опять-таки у нас есть оффлайновые это те которые вот сейчас лежат не подключены ко всему вот его можно спокойно взять из бэкапе то есть диск подключенные к виртуальной машине там начинается лютый треш и угар что будет если мы будем бэкапить диск на который идет запись ответ после восстановления такого диска данные можно ним хоронить соответствуют проблему надо тоже как-то решать ещё одна очень жесткая задача это уложиться во временное окно у нас она у нас не большое одесские у клиентов у некоторых по весьма существенны имеющееся решение у нас быка пилатом в районе примерно 30 мегабайт в секунду когда-то быстрее когда-то медленнее зависимости от типа диска но смысл в том что 4 терабайтный диск мы бы капель и почти 2 дня и это было неприемлемо сожалению естественно нашей целью было уложиться в наше 6 часовое окно ну или хотя бы восьмичасовой давайте где-то хоть как-то попробуем это сделать поэтому нам надо было просто всего лишь ускорить и резервное копирование в 10 раз и так как выглядит схема резервного копирования самое главное виртуальной машине которую мы копируем в инстансе как мы называем это его диски мы же в облаке мы понимаем что гипервизор может умереть и нас могут зареспаунится другом 10 state памяти вполне себе потеряется это допустимо этапа это понятно если дело дошло до восстановление из резервной копии скорее всего что там было в памяти уже неважно что там было нам на диске это уже critical соответственно нам нужно бэкапить диски простейший вариант оффлайн диск но я просто беру берем диск и льем блок блок в резервную копию возможно сжимая высчитывает div и еще как-то преобразован но эта простая операция вторая операция когда у нас виртуальная машина бежит на одном диске вот и тем ними и как но это диск 1 и виртуальную машину shutdown.exe она при нем пока по что в этой ситуации делать ответ нам приходят на помощь такой инструмент системы как snapshot мы снимаем snapshot с диска он атомарный и потом читаем данные уже и snapshot а это тоже была отдельная попе с тем как мы через это прорывались там было все не так просто я больше я расскажу об этом упомянул и третий заход самый сложный это заход когда у нас несколько дисков виртуальной машине и надо сделать консистентной ее snap сотик вот тогда у нас начинаются проблемы почему ответ потому что снять с на фото процедура не атомарные и если у нас есть база данных которые лежит допустим на двух дисках то при попытке снять что снять snapshot с такой базы у нас snapshots делается в разный момент времени и файлы данных находящийся на разных дисках вообще говоря будут различаться и можно сказать что это копия уже непригодно истока использование постановлению соответственно эту ситуацию надо как-то обходить как это делается ответ это делается с помощью гости агентов гостевой операционной системе вот мы используем в качестве гипервизоров linux sky мушкой и соответственно возле виртуалок у нас какую ему гостевой gentoo ему который может за фризить остановить вот вывод на короткое время для то над пока мы снимаем snapshots диска то есть virtual кать полностью не замораживается останавливаются только и дисковые операции делаются снапшоты диска операции размораживать все пошло это все делается с использованием средств операционной системы windows там этот shadow copy provider в линуксе соответственно это системный вызов с фриз то есть такие вещи это делается срезка операционной системы мы под приживаем вот вывод и делаем делаем снапшоты вот вывод размораживаем клиент продолжает лететь исключение бывают ситуации когда гостевого агента нет на клиент хочет backup тогда мы просто виртуальную машину на 15 секунд притормаживаем полностью суспензии делаем снапшоты дисков раз aspen дивы мы и и она летит дальше то есть мы не рекомендуем такие вещи делают для кластерных систем категорически то есть либо gst гент либо резервирование средствами того ты софта который крутится в этом кластере как угодно вот но пожалуйста нельзя фризить одну ноту кластер а это это доносят до клиентов наши присел и как правило и вот то есть вот красным кругом выделено выделил момент который вызывает больше всего проблем то есть если что-то пошло не так backup дальше может стать некой системы поэтому . очень важно оказалось для нас как делается собственно процесс backup а у нас есть корба сейчас его переписывают называют проект карт 22 это инструмент менеджмента резервных копий этот им скарборо отсылает припав в момент старта задания отправляет инструкцию на гипервизор я хочу сделать подготовь местной под виртуальной машины гипервизор идет гостевую операционную систему он идет в гостевую операционную систему с запросом уэст агенту заморозится пожалуйста после того как вот вывод заморожен гипервизор заходят на control plain система хранения и говорит сделать снапшоты этих дисков на которых лежит виртуалка сходив к делает снапшоты рапортует я сделала снапшоты гипервизор опять гостевую команду в гости операционку команду разморозьте и работая как обычно и после чего рапортуют в карбах о том что у тебя есть напишут в нем вот такой-то ую snapshot виртуальные машины в нем такой-то набор snapshot of дисков после чего корпорат проявляет в backup apk инструкцию сделай мне бэкапы вот этих дисков используя вот эти снапшоты backup apk отправляет это дело на исполнительные наварки да те походят на системы хранения подключают к себе снапшоты лью данные считывают то есть преобразовывают их и заливают холодные сторож холодный сторож это у нас встречка которая не серовская стрижкой вот а своя реализация вот но это большой страшный старридж куда можно много данных валить на холодное хранение то есть вот такая цепочка как правило чаще всего проблемы возникают как раз на этапе между гипервизором и гостевой операционной системой когда q и могу я студент по каким-то причинам отказывается заморозить вот-вот ним или не может в свою очередь после того как бы мы занимались оптимизации той части которая собственно выполняет резервную копию то есть на этом слайде это вот последний флажочек с backup последняя стрелка с backup apk file storage в up and sticky есть системы подсистема бэкапов сентри построена достатка иерархически там есть api шичко в которой приходит клиент запросам сделать бэкап а или кто-то кто хочет сделать backup manager в который попадает запрос который выполняет подготовительные процессы перед бэкапом и драйвер драйвер вычитывает все данные с переданного диска и записывает их на холодное хранение случае суп insteps chemistry мы выбрали мером мы получаем что первое мы сидим во фреймворке ним и вы за мне не мы вызываем что-то нас вызывают и мы вынуждены соответственно работать в жестко в условиях ограничений backup выполняется всегда внутри процесс оси интер а вот это не обсуждается там живут green лет и винт лет и которые очень больно делают нам с вводом-выводом и у нас был еще свой драйвер который не входило в мейнстрим он влил в с3 вот и в нем не работали дифференциальные бэкапы а штатные драйверы которые есть в аптеке это driver backup of например в tivoli gm овский всех на файловую систему nfp нфс или локальную в глостер в google клауд и даже какой-то то есть кнут асаи страшный драйвер да собственно говоря наш так как выглядит в процесс резервного копирования вот так выглядит примерно примерно вызов вызов вызов драйвера внутри фреймворка если здесь по сможешь чтоб что здесь происходит нам передается информация о томе и а бэкапе который мы хотим сделать после чего у нас вызывается процедура открытия этого волюма и нам воде выдается на выход какая-то сущность которой с которой можно читать вот но которая не является в общем случае файловым дескриптором это означает что мы решаемся всех возможностей по асинхронности параллельности и прочему которая могла бы дать нам операционной система и слепота был нормальный файловый дескриптор что мы теперь можем поскольку на обман вынуждены работать по интерфейсу наименьших возможности это последовательно читать оттуда данные и все ниф-ниф параллель читать то его синхронно и чтение там языке прочее для нас все это уже становится под запретом из-за этого мы заперты в 1 поточность мы ничего не можем сделать почти 1 на pineta перед на первой итерации мы попытались все-таки принести какую-то многопоточность туда изделию что у нас был один поток который вычитывал данный один поток который их сжимал преобразовывал и записывал на холодное хранение дамы под мы грубо говоря полтора-два раза ускорились но надо 70 мегабайт он бы нам не хватило наш наша среда что целеполагание было 300 мегабайт секунд и 3 ст и это абсолютный минимум с которой мы можем жить но что делать не укладываемся вот тогда мы начали разбираться где же у нас проблемы прежде всего мы построили я написал такой простенький пример который фактически симулируют то как работает драйвер читаем сжимаем вот тот драйвер который был у нас использовался клип читал блоком по 512 к и что мы видим в данном случае мы видим что первый диск у нас не загружен крайне левой стал крайне правый столбец до 30 процентов утилизация диск от нас есть куда расти и при этом все запросы мы читаем запросам 512 к вот а запросы на чтение у нас пол 128 килобайт 206 это размер блока 512 байт нова получается во-первых нас кто-то на уровне операционной системы режет наши запросы по чтению раз и второе у нас мы судя по всему убираемся во что-то кроме диска но выступ на что можно опираться ответ мы можем упираться здесь только в сжатии компресс да ну выкидываем сжатие что получаем вот выкинуть сжатии мы получаем 290 мегабайт в секунду утилизацию почти сто процентов ответ этот z lip дрова мы это знали самого начала будем заменять за клип мы заменили z лепные lc4 для начала и получили достаточно хорошее ускорение если у нас зак либо вот этот тест выдавал 8 5 мегабайт в секунду то после того как мы заменили компрессию z лего nail за четыре стола 240 мегабайт в секунду уже лучше но все еще недостаточно хотя с этим уже как-то можно было жить единственный как бы но но есть одно ограничение и на нем скажем позже следующим шагом было пытаться понять а что же у нас происходит с диском почему нам не дают читать нашим большим блоком 512 к кто нас лимитирует ответ нас лимитирует такая вещь как ready fit когда мы читаем с диск если мы читаем не в директ и а вот наш запад наш наши за просеки которые прилетают на чтение разбивая как бы читаются кусками площадь драйвером размером в рядах от килобайт это можно посмотреть все sfs там слэш сиз блок имя диска и ряды hed-gp файлик специальный смотрим него 128 килобайт 256 вот эти вот они здесь у нас вырастают ну что ж вот что нам делать ответ а мы можем обойти как-то механизмы вот эти механизмы рядах и да то есть лимитирование кто распущен тест iq накладывает ответ да есть такая вещь как директ и он работает в обход каша подход пейдж каша и в случае если мы используем директ и а я как бы вот второй тест да мы делаем dd из диска в div ну директ и а блоком 4 мегабайта и получаем every шарик веса из 512 килобайт скорость чтения 570 мегабайт в секунду а с этим уже можно работать наверное почему 512 килобайт они 4 мегабайта как мы читали у нас диск этот подключим поискать а все-таки а там 512 килобайт максимальный размер запросу он открыл закон поэтому но все-таки 512 лучше чем 128 количество request a сильно сокращается и сумма на пропускная способность почти двукратно вырастает единственный cup как бы и теперь сводим это воедино если мы берем директ его и добавляем к нему lc4 то мы получаем 370 мегабайт в секунду и мы вроде прыгнули на нашу нас нашу целевую точку проблема заключается в чем проблема заключается в том что у нас есть диски которые не видны как диски в кастовой системе и для них никакого директ и о бы придумать не сможем вообще и вот как жили с этим файла и caged нам мужикам так придется но для того чтобы грубо говоря считать каким-то как-то сколько займет backup в секундах мы извини развернули то я развернул табличку что и из которой то есть понятно то есть у нас есть гигабайт данных есть этапа через который проходит гигабайт данных он читается сколько-то секунд на чтения заходят затем он сжимается сколько-то секунд на сжатие и соответственно все суммируем это время получаем время которое мы тратим на обработку одного гигабайта данных случае ра если раньше мы читали за три с половиной грубо говоря секунды гигабайт так в первом приближении вот сжатие у нас занимала 8 секунд z ли бом соответственно мы получали что у нас 11 с половиной секунд цикл обработки 1 гигабайта после того как мы переключились на директ и а и компрессию пол и 4 у нас чтения начала занимать полторы секунды компрессии одну секунду и соответственно получается две с половиной секунды на гигабайт 400 мегабайт в секунду что примерно коррелирует с тем что мы видели но в случае с сейфом сунита ксев с хдд у нас читает один гигабайт за 30 секунд в последовательном режиме a7s с дочитает гигабайт за пять секунд стан 200 мегабайт в секунду и все мы ничего не можем сделать мы на передают этот файловый объект уже открыты и и мы с него даже в параллель читать не можем и тогда нам пришлось ломать все полностью сначала мы сломали фрейм вот как бы мы решили что отныне мы будем сами открывать переданный нам диск открывать столько раз сколько надо и так как считаем нужным что мы будем обрабатывать данные в много потоков во много процессов чтобы не устать не упираться в гил в винт лет его все прочее мы будем избегать тут копирования данных излишнего и при этом будем хранить данные промежуточные shared memory и чтобы мы в то время записи в с3 не простаивали мы сделаем новый формат который нам позволит в параллель много кусков заливать то есть нас будет непростой стрим который можно взять там и через компрессор прогнать просто через z и получить образ диска но более сложная структура которая зато очень хорошо параллели ться на этапе восстановления в частности на этапе и на этапе записи тоже ну еще нам надо сжимать во много потоков потому что все таки даже слр . и это есть там одна секунда целая на гигабайт как бы уже не то немножко что мы сделаем мы сломали фреймворк если раньше мы доверялись окон стеку на предмет того чтоб он нам открыл волюм теперь мы этого делать перестали мы добавили мы модифицировали интерфейс драйвера добавив там версию и просто говорим если версия драйвера новая то мы отдаем туда ссылку в драйвер на уровне соус равняется волен ян го если версия драйвера старая церковь 90 с легаси драйвера все-таки legacy backup а мы не хотим их терять то мы отдаем до от уже открытый волюм так же как это было сделано раньше фреймворк мы сломали мы получили возможность работы с волюма так как считаем нужным кстати обратите внимание что поломка была небольшая вот и игры очень легко подпортить такое дальше однако процесс ность мы сделали как у нас цикл резервного копирования состоит из множества шагов каждый из которых на выход принимает команду и на на вход принимает команду она выход выдает набор команд для следующих шагов то есть пример характерны да мы она полезная на на вход у нас поступил запрос на допустим чтение 4 4 килобайт а на выходе у нас 4 запросе к каждой из которых на чтение одного килобайта с определенного смещения то есть если у нас там был это после чтение 10 гигабайт мы могли бы раскидать его на 10 чтение по гигабайт у каждой из отдельных смещений и соответственно у нас уже есть возможность если мы сумеем открыть волю много раз в параллель все это за прочитать причем потоки входящих сообщений исходящих сообщений у нас асинхронные мы в агент забрасываем сообщение вот одно за другим не не дожидаясь их конца и просто выгребаем ответы от агента вот когда он их сгенерирует допустим мы можем отправить одну команду на выход нам прилетит 10 ответов или ответят пройти 10 ответов на выходном предмете один мы не ожидаем ничего от агента кроме того что он когда-нибудь даст ответ естественно там есть как команды согласования там началась и начала сессий конец сессии то есть данные будут даны сейчас пойдут данные чуть больше данных не будет но смысл примерно такой так какие агенты у нас есть цикл бэкапы мы разделили на 7 агентов мастер backup а который определяет план как он будет копировать какими блоками читать какого размера то есть на сколько сколько потоков читать тогда он планирует резервную копию риттер reader открывает волюм столько раз сколько считают нужным какие-то волюм открывают четыре раза какие-то восемь раз то есть допустим какие-то читает мы читаем в четыре потока кита 8 потоков и на дальше он получает команду от мастера а чтение данных читает данные складывает их в shared memory отдает следующему агенту агенту которая считывает контрольные суммы агент который ищет контрольной суммы многопоточный он загружает у нас как правило 8-16 ягер на расчет контрольных сумм в несмотря на то что он питоновский зонт нормально делает и после того как контрольные суммы рассчитаны цепочка контрольных сумм передается на следующий этап кстати данных нигде не копируются в прежнему данные вот как reader их прочитал из фарт в мире так они там и лежат вот checksum тексту мы прочитаем дальше div хендлер если у нас backup уровня 1 то мы поднимаем из предыдущего backup а список check sum of блоков который был делаем div сравнивая контрольные суммы и оставляем только те данные которые изменились те данные которые не изменились мы из потока выбрасываем что очень даже удобно затем отфильтрованные данные попадают на компрессор там они сжимаются много потоков вот то есть каждый блок сжимается независимо от другого эти блоки пробрасываю ца надо the right data райтер используя мультипарк во-первых используют агрегацию для того чтобы агрегировать male невозможно маленькие блоки большие сегменты и используют мультипарк для того чтобы закон заложить данные в истре ну и последний фрагмент metadata райтер он финиширует backup то есть он факте он записывает итоговый на борт контрольных сумм метаданные бэкапы вес 3 и все на этом целый цикл backup а завершён все это многопоточное все это как бы каждый каждый из этих шагов в отдельном процессе меж процессор на и взаимодействие через через unix domain сокета строится данные по возможности нигде не копируются единственный этап где данные могут преобразоваться эти два этапа где данные преобразовывается это div хендлер который может выбросить некоторые данные то есть или усечь их и компрессор который собственно данные сжимает остальные куски они zara копи там данные не гоняют из теста если нам надо перегнать 16 туров данных data через сотню них socket это бы это было бы о том полным адам мы бы не смогли это сделать вы уложили бы систему напрочь цикл восстановления намного проще у нас опять таки появляется master master in читает мет метаданные бэкапе который восстанавливает backup составляет план действия backup reader вычитывает данные из с 3 фрагментами и отдает их декомпрессор в параллель декомпрессор в параллель раз сжимает полученные блоки отдает в rider rider пишет полученные данные на диски о параллельности как правило мастер backup а у нас читает а из case вы и диски в четыре потока все папские диски в 8 потоков райтер пишет стоит с тем же коэффициентом то есть рассказе он пишет четыре потока сев он пишет 8 потоков ну и цикл удаления стом коль чисто формальный мы читаем метаданные бэкап и удаляем тибет эти объекты заз 3 которые к этому бэкапы принадлежат так как мы используем по сексу от memory например простейшая ситуация backup мастера придает команду прочти 4 килобайта do the reader читает 4 килобайта в какой-то сегмент shared memory to tie имя этого сегмента записываются следующее сообщение этот сегмент передается в блок расчета контрольных сумм следующему агент тот при первой выпитой контрольную сумму приписывают ее к сообщению это сообщение отстреливается дальше компрессор ну здесь уже ничего не поделать он вычитывает тот блок который сгенерировал reader освобождает его создает собственный блог который вот он синим выделена имя но вы идентификатора записывает в него сжатые данные и отправляет соответствующую команду дальше то есть нас такой flow из команд получается стройный когда каждый шаг может чего-то добавить чего то изменить в команде и выстрелить и дальше на самом деле команды могут меняться то есть какие-то команды могут пропадать чтобы быть агрегированный на выходе какие-то порождаться увеличенных количествах асинхронная обработка если мы отстреливаем даже если мы отстреливаем агент команды в определенном порядке они читают выполняются так как смогут выполнится если какие-то данные прочту ца быстро да допустим первое сообщение она была отработана быстро пришло первое второе сообщение отрабатывалась долго третье сообщение от работалось быстро 4 быстро и только потом в конце пришел ответ на второе сообщение только потом вот это осин вот эта синхронная цепочка она тоже поддерживается с от реализации всех агентов соответственно мы больше не ждем того то есть если у нас backup застрял допустим на чтение стив-о какого-то блока по какой-то причине там может ребаланс может скрупулезно скраб идет может еще что то мы не тормози мы продолжаем собирать данные бэкапить никаких тормозов по возможности 4 терабайта не самый большой волюм у нас одного из клиентов был 20 байт волю частью на backup не ставил слова но мог и так дальше l1 дифференциальный backup поступающие команды могут соответственно быть выброшен и драйва каким-то каким-то участком в нашем случае div хендлера мысли он понимает что этот блок не изменился у дев хэндлера есть новая контрольная сумма блока который только что прочитанный контрольная сумма старого блока старой версии если они совпадают зачем это дело писать соответственно под есть у нас есть четыре потока хочет 4 в 4chan к подряд идет до красный и зеленый чанки не изменились то выходящий поток попадет только 2 только оранжевый и синий чанки то есть два чанка останется всего из 4 причем 4 чанг их можно еще и сжать ну и асинхронность наша как раз подразумевает то что данные в результирующем выхлопе могут идти не в том порядке в котором они писались мы можем переупорядочить соответствующие сегменты единственное надо сделать не забыть сохранить метаданными то данные о том что мы куда положили в какое место вы бы сами результирующего файла это все в метаданных у нас хранится сборка мусора еще наши агенты занимаются сборкой мусора автоматической поскольку мы же все программисты любим ошибаться вот и можем чего он не прохлопать поэтому фреймворк наш внутренний фреймворк обработки сообщений он действует как он смотрит оплетающее сообщение подсчитывает какие шарик на носик менты там были поступили на него какие он выдал и соответственно сегменты которые пропали он сам возьмет и освободиться программиста если программист забудет это сделать то есть в на в первом примере у нас на вход пошел сегмент ридер с префиксом reader на выход у нас выход пришло два сегмента с от компрессора вот значит надо освободить сегмент префиксом предан что наш менеджер ресурсов и делает если программист от забывает ну и второй вариант если у нас на вход пришел сегмент с префиксом reeder она выход риттер и компрессорный молиться там получилось он и говорящего освобождать сегмент перри использована про должно продолжать с ним работать никаких лишних копировать никаких лишних преобразований лег только легкий контроль запросы мы не астане не стали отдавать на откуп программисту работу с ресурсами у нас есть менеджер ресурсов который позволяет запросить все эти вещи вот мы его наверное еще переделаем вот чтоб он был чуть-чуть по эффективней но и даже того что сейчас есть нам уже хватает вот базовая реализация она учитывает ресурсы гарантируют освобождение вот может переиспользовать ресурсы им пока только в рамках процесса есть когда мы вынесем это на уровень и такого внешнего сервиса с которым будем общаться через unix socket но в нир сервис к как внешний сервис другой процесс который будет именно трек эти ресурсы с ним можно его можно будет его можно будет сделать еще более эффективным если это будет нужно главное что мы ввели такую абстракцию сейчас еще мы пытались избежать лишних зависимости все что мы используем это posix shared memory unix domain сокеты джейсон в качестве формата данных то есть никаких там огромных библиотеку папин стеков ских притаскивать общем случае не нужна сложная зависимость которая работает все фом она у нас выкинут а в отдельный в отдельный агент результат этого мы можем бэкапить не только окон стеков ские волю мы мной был металл мы кстати использовать это для тестирования того настроен ли корректно server backup а мы просто говорим вот тебе маленький файлик который будет считать своим диском забыть об его пожалуйста и если как на ком-то или мининфраструктуры кто-то забыл разрешить доступ и прописать маршруты ли что-то такое такая вещь backup падает вот если все нормально то есть он отрабатывает корректным этот сервер работает через все и селе он проходит то есть в инфраструктуру на корректно прописан все нормально ну и можно собственно говоря был металл бэкапить если вставить перед нами станет такая задача то есть мать и кроме того такой маленький кот маленький сервиса станок молить от установления всегда можно отдать клиенту если он захочет напрямую сам поработать со своим бэкапом в этом ничего такого это вполне нормально кто у нас получилось суммарно с ума но у нас получилось сейчас до полутора гигабайт в секунду с одного диска мы поддерживаем l1 хорошо вот до 40 гигабайт в секунду нас проходит потока на тонких дисках как бы там где читать реально не нужно вот 18 гигабайт в секунду это у нас производительность кластерной фермы по чистым данным но мы платим за это большим расходом памяти сейчас когда я делал это когда я делает backup у нас примерно гигабайт памяти на терабайт данных уходил на контрольные суммы и там побольше нами то данные затраты на стерилизацию были то есть если бы копить большой волюм нам надо было много памяти потом мы поработали я здесь уже не стал править вот он у нас сейчас расход памяти упал практически двукратно где-то там два раза где-то в три зависимость от разных этапов там просто оптимизировали работу со строками то есть строки питоновские выкинули сделали одну большую строку из которой сегментарно вычитываем роль на сумму стала многом стало много быстрее много лучше бы память это какая такая некоторая статистика с нашей кластерной фермы это в гигабита в момент пика наш самый слабый сервер зачитывает примерно 18 гигабит в секунду вот на выхлоп выдает примерно пять пять с половиной гигабит то есть сейчас фермы это увеличилась на насколько помню 50 процентов и мы уже не 140 гигабит в секунду съедаем с систем с наших сетевиков а примерно сто восемьдесят двести гигабит и на выхлопа на сто раз отверз не 33 и 34 гигабит на порядка 40 то есть мы подросли достаточно хорошо все сервера достаточно злые они загружены в арене backup а очень очень хорошо настолько же нам пришлось снижать на них степень параллельности степень агрессивности backup а то чтобы он так сильно не на них не коптил поскольку и а лифт и в мониторинге выстреливают из сетей и сетей и сетевики прибегают просьбой прекратить укладывать их канал как правило не жалуется на между это центральный канал вот но и на внутренние сети внутренним сетям тоже надо достать бывает хорошо что нам пришло потребовалось сделать дополнительно первое нам пришлось внести wap инспекторские библиотеки прямой доступ устным счетом сев а его не было в принципе то есть мы залезли в sbrick пилили туда поддержку snapshot of почему это было сделано потому что в резервное копирование сербских дисков создавалась делалось через создание временного диска то сделался clones snapshots a snapshot одесски потом этот диск б копился это this попадал в биллинге клиент был очень недоволен вот ли чтобы вот эту цепочку как-то не крутить ограничение на мир и не просто включили поддержку снапчата в в брик всех и и пошли дальше мы поменяли механизм scheduling а штаб он учитывал нашу схему зоной зонами доступности у нас есть две площадки три зоны доступность то есть либо у нас сер все бэкапы разные равномерным размажутся площадками мы прикладываем между это центровой сеть что очень нехорошо вот либо у нас какая-то надо выдумывать третью зону как ты и располагать общем все было очень и очень печально поменяли механизм scheduler а чтобы он более умный работал и отказались от scheduling avast на восстановление через тот же хвост по умолчанию в open стеки восстанавливается backup на том же костенок которым делся если хост умер надрались базу и править мы это дело выбросили вот и также schedule им восстановления на уровне зон доступность то есть не не на уровне hasta уже интересные факты полу того что получилось сейчас если что-то с бэкапом пошло не так и to preserve попытаться по одной из трёх причин первое гостевой агент не смог за фризить вот вывод второе у клиента пеппа закончилась кого-то на бэкапы или снапшоты и третье у нас какая-то очень большая проблема инфраструктуры например там легла сеть либо у коллег из соседней команда что-то сервисом случилось south возможно сопки фикации с биллингом с чем-то таким то есть у нас прилегла инфраструктура собственно драйвер сам сбоев практически никогда не дает ну она в принципе понятно что там там задача несложная вычитать все правда вычитать очень много сжатия отдать но и объем тестов вот объём тестов там практически в два раза больше чем объем собственно говоря самого кода драйвера это бэкапы там тестируется все вплоть до того что данные ложатся в правильном порядке что они как бы что она байтов paid нужно за то что контрольную сумму все совпали то есть там все цепочки проверяются и объем тестирующих он очень больше чем это объем только юнитов там еще есть естественно интеграционные тесты смоуки вот этой то есть очень много пришлось поработать с тестирование всего-то всего вот этого куска ну и собственно наверное дальше что следую что можно наверное так сказать первое когда мы как механизмы ввода-вывода в операционной системе они классные они замечательные но есть он у них есть один нюанс и они могут снижать производительность на однонаправленных нагрузках если у вас 100 процентно записи или стопроцентно чтение вам стандартные механизм могут подложить свинью жку вот здесь надо уже как правило ходить не то что в обход блочного стек она в обход пэйдж каша уже как минимум приходится заходить фреймворке в приборке тоже добавляют боль иногда много боли вот их приходится ломать это может доставить определенные неудобства вот но зато вы сможете решить задачу эффективно но и когда у вас параллельная обработка активное соответствующие структуры данных вам также предстоит поменять то есть вы не сможете работать с привычным плоским стримом хотя он для вас хоть он про хотя он прост и привычен вот придется что-то придумывать бояться этого не надо поскольку все это подъем на вот эту систему бэкапом и грубо говоря за два человека месяца сделали весь этот драйвер и она заработала спасибо так как обычно по поводу вопросов можно сдавать их здесь можно мы все еще ждем если бы им очень радости кто-то из онлайна нажмет кнопку и подключиться мы выведем человека прям сюда и с ним пообщаемся да можно начинать спасибо за доклад а скажите вот обычно все-таки для клиента же важна скорость ресторане скорость быкова почему вы вот именно бэкапе концентрируетесь скорость скорость ресторан составляет примерно ту же скорость стою backup а они сравнимы по порядку то есть зависимости от того какой бы как мы восстанавливаем то есть l0 или l1 там могут быть конечно варианты там наверное другие технологии ускорения могут быть там можно много чего ускорить но мы пока нет мы пока этого не ускоряли потому что необходимости в этом существенное не было но это же именно то что интересует клинья понимаю что это неудобство вот но опять-таки как правило сейчас у нас волюм размером в 1 терабайт вот восстанавливается примерно минут за 30-40 это вполне себе на с учетом того что у нас нет ограничений на однопоточный восстановление можем даже если у нас у клиента допустим большая машина там местом 3 4 дисков он может в параллель иски восстановить сразу одновременно и как бы у него нет такого такой заморочки что футболу от он уперся в один поток понятно просто там ботаником восстановление большого голема может быть сетевая часть может быть дисковая часть может быть распаковка ошибка может давать много вопросов на самом деле может может быть все сознание трос который хотел спросить у вообще dataframe л за 4 они уже используют чик сумму зачем вам еще чик сумму на ты как бы кекс кекс это checksum ныне в этом не фрей не веткой это checksum и блоков исходных данных которые мы должны иметь возможность dev 0 то есть тот этот блок это не тот кроме того мы не можем гарантии мы у нас агенты независимой и это означает что вообще говоря то как в какими блоками поступают команды на чтение как будет гранулированных чтением и не гарантирует и мы здесь занимается менеджер вот он у него алгоритм может поменяться и тогда грубо говоря у вас у вас насчет за чтение не с того места вот и следовал с какими бы волос вас эти checksum и на уровне там lcl на уровне фрейма фрейма алжирские они просто не пойдут и кроме того наличие ну как бы да они просто не пойду раз таки мне кажется что просто еще тестирование эквивалентности блобов через контрольную сумму в общем случае это рискованное мероприятие из-за коллизий я знаю что это резкого но в данном случае как мы пошли пока по пути копирования того что было сделано в драйверах уп instek а вот я знаю про эту проблему что свёртка всегда отображает пространство меньшего размера в данном случае это прекрасно помню но по парень не было инцидентов вас есть какой-то мониторинг на это то что вы не пропустили в тель то вы на самом деле на самом деле вероятность этого достаточно мало то есть мы знаем мы знаем мы знаем она почему потому что это данные одного клиента но нет пол игры данными используется для дедупликации поэтому у нас нет вероятности того что данные как бы так вот так сильно настолько сильно разойдутся что потери одного пицца это уже критично потери одного бита это критично мне кажется спасибо не все до напоминаю что мои в книжке выручать так что да это какие-нибудь готовые концертные решения рассматривали в частности очень интересуют реттиг оказывается им тоже очень классно можно блочный девайса бы катать повторите пожалуйста идет есть куча готовых open source ных решений для бекапа рассматривали их но может быть не полностью а конкретно на самом на самом деле у нас была проблема связанная с тем что все наши риши все решения которые мы должны использовать мы должны грубо говоря быть интегрированы с окон стеком там был собственный фреймворк backup а то есть singers кпк по который является вообще говоря основные мои стримы и то есть его используя было ну по сути необходимо без вариант а потому что всячески свистопляска всех часть всячески решения которые предлагают там они ориентирован на то что вы отдаете эту дату из групп игры какой-то там какой то какой то какой то блок девайсы туда скан его эти файлы to scan duty либо какую-то конкретную 3 satio здесь мы должны скользить ap инспекторский объект в общем случае да понятно что можно заселить его реальный адрес и отдать потом на резервную копию ну это не то что касается с точки зрения таки страции у нас когда-то был metapod регистратор бэкапов глобальный тоже выбирали какой-то остановились на картер я не знаю почему но для меня я реализовывал социнтерна включает в данном случае вот это как самую агрессивную дата нагружена менеджмент сервис у нас этот компонент описала другая команда к сожалению я не скажу почему они выбрали карпова вот честно я не знаю о нем спасибо йогурт просто можно задать какой очередности андрей стрелковский индекс просто к и правильно понял что подключены клинкера исказить диск вы не можете , то подключенный skazi диск да мы снимаем с него snapshot подключаемся на 500 и b к а то есть для исказить дисков у вас раз ковша то реализована до у нас у нас есть для seahawk ii для из хозяйственных фото всё верно то есть там ну там другая реализация получается а может и в двух словах рассказать или там сложно двух словах как корреляционного тривиально у нас тоже у нас а из case раздается софтовый с линуксов вот там просто создается в маске snapshot это тыл в маске snapshot создается не полного размера вот он создается размером с не ошибаюсь в 20 процентов от размера волюма и подвешивается монитор над ним который этот субсчет когда он начинает заполняться увеличивает по мере необходимости там были определенные расчеты проведены то есть там подобранной границы такие чтобы даже при максимальной загрузке на трудно такой сервер по сети snapshot не переполнился и расширяться успела сработать то есть мы создаем snapshot lvm овский при этом исходный волюм который отдается к в виртуалке с ним ничего не делается его обслуживание продолжается все корректно это все делается в онлайне из на 500 потом расшатывается как валим пояс case для backup server а тот оттуда выгребается данные потом отцепляется и из них суд удаляется понятно спасибо ну еще вопрос про сжатие а вообще вот на реальных данных насколько затем помогает вообще говоря получается что где-то 1 of 2 ну это типа целиком по всем дискам суммарно где-то дед идет где-то порядка половик пола в два раза объем реально получается уменьшен понятно спасибо спасибо за доклад как обычно круто вот у меня вопрос очень понравилась вот эта идея про правления чтения и потому параллельную запись не было идей таким образом ускорять развертывание операционной системы из образов при создании выемки на самом деле идеи такие были вот но они у нас вход пока не пошли мы используем другие механизмы для ускорения создания дисков виртуа для виртуальных машин но механизм параллельного копирования параллельно чтения мы используем для быстрого трансфера дисков между различными скадовском если диск оффлайн раз вот этот механизм параллельного нет а миа миа саму схему параллельного чтения параллельной записи вот активно используем в результате то есть мы вместо привычного трансфера дисков со скоростью там 35 мм 35-70 мегабайт в секунду часах трансферрина 300-700 мегабайт в секунду то есть опыт набраны вот здесь когда мы то есть поднимает a direct его и прочее вот мы потом про экстраполировали перенесли в другие компоненты это мы хорошо запустили сторону прям вот звучит настолько здорово что у себя попробую нечто подобное есть за что бороться"
}