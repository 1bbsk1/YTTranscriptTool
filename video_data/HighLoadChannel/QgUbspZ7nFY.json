{
  "video_id": "QgUbspZ7nFY",
  "channel": "HighLoadChannel",
  "title": "Мифическая миллисекунда, или Как ускоряют движки OLAP баз данных / Роман Кондаков",
  "views": 2331,
  "duration": 2906,
  "published": "2022-03-21T14:10:28-07:00",
  "text": "так всем привет меня зовут рома кондаков я сегодня хочу представить доклад который называется мифической миллисекунда или как ускоряет движки olap баз данных но для начала пару слов обо мне сейчас я работаю в компании квари файл abs мы там занимаемся консалтингом помогаем другим компаниям которые делают свои базы данных делать эти база данных лучше до этого немножечко поработал в яндексе там я занимался индекс говорил and begin а до этого я работал в компании green green где делал ну помогал делать символьный движок для apache игнайт так но обо мне пожалуй все давайте перейдем к сути название доклада фигурировала аббревиатура olap давайте разберемся что такое базы данных делятся на два больших типа это лтп и olap у лтп это когда вас куча параллельных пользователи они делают какие-то маленькие транзакции там например select и апдейты деле ты например это конь какой-нибудь интернет магазин там что-то покупаем что-то там платим за что-то главное что отсюда надо вынести что запросы вал тип типичные запросы в типе баз данных они трогают мало данных буквально там ваш личный кабинет и так далее и типичные представители класса вот этих баз данных это поскорее как ручей ну и прочее довольно известный вещи второй тип баз данных это alup это онлайн транзак она аналитик analytical processing обычно в этих базах данных нет транзакции там ну или лучше бы чтобы их не было они не рассчитаны на огромное количество параллельных запросов там с сидом и ключевое отличие в них что каждый запрос валом данных вала базах он обычно трогает ну или читает огромное количество данных то есть он может прочитать все все все таблицы которые у вас есть этом как-то их с агрегировать все это обычный select и там нету апдейтов но типичный пример это аналитика то есть вот если у вас есть этот магазин то пользователя лапа это не ваши пользователи которая покупает интернет-магазине это аналитики которая работает вашей компании которые там пытаются понять проанализировать пользовательское поведение там пытаются сделать китай отчеты по продажам в общем там все что все что нужно для принятия решений управленческих и представители так класса этих баз данных и так ли cows вертик атомы и прочее доклад со дня будет prolab и что мы х что мы ждем от а лапа во-первых мы ждем чтобы он исполнял сложные запросы там куча join of там я видел что до 4000 джонов бывает в реальных запросов в некоторых статьях об этом говорится что запросто могут быть очень сложная эти запросы должны уметь выполняться на большом объемную объеме данных то есть сотни гигабайт там и более и при этом они должны выполняться за разумное время потому что аналитик вашей компании не хочет сидеть там час день там неделю и ждать пока вот запрос закончится чем быстрее выполняется его запросы тем быстрее он может проверять гипотезы тем быстрее но тем качественнее он работает желательно чтобы olap отвечал за разумное время то есть вы задали запросы там через секунды или лучше миллисекунды к вам ответ вернулся и собственно доклад сегодня будет про то как какие на какие ухищрения идут собственно разработчики баз данных чтобы olap базы данных отвечали на запросы за разумное время в чем проблема проблема в том что если у вас есть много данных например терабайт и он хранится на жестком диске и чтобы прочитать только прочитать эти данные жесткого диска вам потребуются ну при этом средней скорости 2000 секунд то есть это очень много никто не хочешь 73 тысячи секунд ждать пока данные про читаются что мы можем сделать как мы можем улучшить время ну мы можем например по масштабироваться вертикальная и положить данные там на ssd в этом случае время чтения улучшится но все равно она будет оставлять желать лучшего есть ещё вариант можно например попытаться расширится горизонтально например положить данные на рейд или на кластер из нескольких серверов скажем треть таблицы на одном сервере треть таблицы на другом сервере треть на третьем и читать данные параллельно тогда мы в три раза вас курим ся у нас получится 170 секунд как бы уже лучше но все равно он не дотягивает и стает вопрос а всегда ли нам нужно читать все данные например если у нас есть какой-то запрос с предикатом да вот например хотим выбрать продажи по какому-то конкретному региону то нам совсем не обязательно читать абсолютно всю табличку часто прибегают такой техники который называется портишь интрумент где мы разделяем изначальную табличку пока мне быть таким departure ножам ну например по регионам да и в одном в одном портишь они хранится регион 78 во втором там 7727 и так далее и когда мы выполняем запрос по этому региону мы идем только в один конкретный партий шин и читаем только те данные которые релевантны для для нашего запроса мы не читаем ничего лишнего кроме того разделение данных по парте шинам позволяет нам еще сильнее улучшить качество но скорость исполнения запроса благодаря тому что каждый мы можем хранить какие-то метаданные о тех данных которые лежат пар тишины мы можем их вынести в заголовок пар тишина это пример максимальное и минимальное значение каждой колонки или сумма там по каждой колонке ле конт фильтр блума этом все что угодно и глядя на заголовок пор тишина мы можем понимать нужен ли он нам при исполнении этого запроса или нет вот например есть у нас запрос где мы хотим посмотреть там продажи в которых сумма сумма продажи было больше 100 рублей мы можем посмотреть заголовок партии шина и от нуля в partition 1 у нас написано что минимальная сумма продажи было 10 максимальной 50 и с нашим запросам где мы ищем больше ста нам собственно в этом портишь и не делать ничего мы там не найдем релевантных данных поэтому мы его можем пропустить и прочитать только то что нам надо в некоторых случаях партии он позволяет в митаву портишь на позволяет вообще не заглядывать в данные чтобы выполнить запрос мы можем просто ну природа съесть запрос мы хотим посчитать общую сумму продаж вообще у кубка которые у нас были в каждом портишь и не может храниться сумма продаж вот для этого конкретного пар тишина и мы можем просто пройтись по этим суммам сложить их и все мы получим результат запроса очень быстро как мы можем оценить улучшение улучшение чтения при помощи парте шин пару не нга ну скажем если мы читаем только десять процентов от всех данных то может к что на примерно в десять раз мы ускорились то есть было 170 секунд ssd распарить по параллельным мы получили 17 и ту же идею даст что можно не читать все данные можно применить не только к строчка и проецирования но можем применить и колонкам нам не всегда нужно читать все колонки и что здесь надо понимать что классические свободы там пол игры с уроков и так далее они хранят данные по строчкам то есть каждая строка лежи тут в блоке ну в страничке памяти 4 килобайта там обычно это на диске и каждый каждая строка содержит все свои колонки то есть вот у нас бренд регион продажи следующая строка тоже бренд регион продажи и так далее и если мы хотим прочитать например только колонку бренд мы не сможем поднять за диск от только эту колонку нам придется тянуть все колонки и это не очень хорошо потому что мы читаем лишние данные есть другой подход который называется каланча то и хранилище где мы каждую колонку храним в разных местах то есть все данные колонки они находятся последовательно то есть например в одном месте в одной страничке у нас лежит только бренды друг за дружкой на другом только региону в третьем продажи и что здесь прекрасно что мы можем читать только вот именно релевантные колонки то есть если запрос используют две колонки там бренд и и количество продажи мы можем только игре и прочитайте собственное идти исполнять наш запрос здесь тоже можно прикинуть насколько мы улучшились ну например если прочитали 10 колонок то есть нам читать надо будет считать 10 прочитать 10 процентов всех колонок the pale соответственно в 10 раз мы ускорились то есть 17 секунд мы ускорились до двух как бы это уже неплохо но надо помнить что нам надо не только уметь прочитать западных данные с диска нам надо еще уметь этот запрос исполнить то есть нам эти данные надо как то там с джонни с агрегировать отфильтровать ну в общем собственность по лицам запрос чаще всего база данных хранят исполняет запросы которые представлены в виде реляционных деревьев то есть это такие вот деревца типа скан join фильтр project и самый простой способ сделать как раз этот runtime которая собственно считает запрос это представить его в виде модель итераторов или the pale этот тайм или счет называется вулкана модель когда у нас реляционная дерево запросов мы представляем в виде итераторов то есть оператор скан пред превращается в литератор который тире руется ну просто читает данные из таблицы отдает их высшую строчка за строчкой следующий фильтр он берет строчку снизу вычисляет предикат если предикат удовлетворяет то мы передаем эту строчку выше итератор который отвечает за project и он ну и вот собственно поставлять у коту колонку которая запросе было изначально но с этим есть масса проблем на самом деле ну например если посмотреть на интерфейс этого итератора то он скажем так состоит из одного метода который назвается next и когда мы его дергаем он возвращает нам только одну строчку и этот метод получается будет виртуальный то есть мы вас есть какие-то накладным расходам на вызов одного этого виртуального метода но скажем 10 наносекунд может показаться что это мало но так как этот метод вызывается на очень горячим пути скажем если у нас есть 10 миллиардов колонок то просто на дергание от этого next у нас уйдет примерно 10 секунд как с этим можно бороться ну можно сделать так чтобы за один вызов next нам возвращалась ни одна строчка object or строчек то есть целая куча строчек одна за одной типичный размер вот этого батчат этого вектора но там сотня ли там 1000 строчек вообще есть исследование где оптимальное чтобы был этот вектор по чтобы этот вектор помещался в l1 кэш процессора тогда платится будет быстрее всего но здесь тоже есть небольшие проблемы из-за того что мы данные построчно обрабатываем мы получается хранимых памяти вот к каждую строчку там ну та одна строка вторая строка третья строка то есть вот видите у нас там регион бренд регион бренд а если мы хотим вычислить предикат как у нас здесь регион 27 то мы должны взять пошли new циpкa которой лежат все эти данные чтобы вычислить перекат мы должны его загрузить эту кашле не целиком в кэш процессора а а так как а так как предикат вычисляется только по региону то получается что nude нам нужно из этой всей кашле не только там четыре числа всего а мы загрузили там 64 байта непонятно чего всяких разных данные которые нам нужны то есть мы языка из-за того что мы данные храним построчно в памяти мы затаскивает как кажется по очень много лишнего мусора который нам не нужен но грузим шину данных выше шину памяти общем это нехорошо поэтому как а как можно с этим разобраться значит есть два типа размещения данных память это нсм она орестович мода когда мы данные храним вот именно строчка строчка строчка одна за одной есть dsm это когда мы данные в памяти храним сначала одна колонка целиком потом вторая колонка целиком то есть колонки идут последовательно одна закончилась началась вторая этот похоже давно к нам усмотрели простор очках данные хранятся на диске построчный поклоны что это примерно то же самое этот dsm каланчи сохранились в памяти он очень хорошо подходит для колонн снова хранилища на диске потому что вам не надо менять layout данных например если мы храним данные построчно на диске и мы хотим обрабатывать их колонна чно в памяти нам придется переделать колонки строчки на колонке то есть это каждый раз такая накладная операция некоторые базы данных так поступают например как roach д.б. он является строчная базой но тем не менее запрос он исполняет пока ло ночь на то что на так быстрее а вот если осколочная база то вы ничего не меняете вы просто как у вас данные примерно лежали на диске вы их просто под подняли в памяти продолжаете исполнять и с этим ты и сэмом ты шла колите просто превосходен потому что данная колонок упаковано упакованы тесно в памяти тут друг за дружку друг за дружкой и когда вы загружаете кашле new циpкa в кэш собственно вы загружать только те данные которые вам нужны это круто вас не грузится шина данных то есть desam очень полезен как раз для скорости обработки что циpкa он работает так что он максимально утилизирован максимально быстро когда данные однотипные когда они лежат друг за дружкой в памяти когда у нас обработка этих данных ведется в каких-то простых циклах без всяких ветвлений упс без всяких ветвлений и нет минимуме минимум f и филсов за счет чего это происходит во первых вот как к шла колите как мы на предыдущем слайде видели потому что ну не тянем лишние данные в памяти и кроме того все современные процессоры имеют такую штуку как branch prediction epub лапой planning to врач prediction а тогда мы процессор пытается предугадать по какой ветке исполнения вот этого if all so пойдет программа если угадывает то все хорошо если не угадывать то получается он спекулятивно ту работу в которую он сделал ему придется его выкинуть и начать все заново но кроме того существует такая техника как мой мари привлечь young это когда мы заранее ну-ка процесса либо там хардварные как-то предугадывает какие данные нам понадобится преследующих в ближайшее время пытается заранее эти данные вытянуть из основной памяти кэш процессора чтобы не ждать их потом когда они действительно уже понадобится здесь пустятся в процессор он похож на знаете такой комбайн который пшеницу на поле убирает хорошо вот он очень эффективен когда он медленно так звали вальяжно ездит по полю зад вперед собирает это все тогда он крайне эффективная это и крайне эффективная машина но когда скажем мы просим комбайн там съезди к он на одном конце поля убери там 10 квадратных метров на другом конце поля убери там пять квадратных метра тогда company превращается в очень эффективную машину и вот то же самое с срм примерно кроме того процессоры там уже лет двадцать пять умеют в такую штуку который называется сент sengan'en страшно алтиплато то есть то векторные операции например мы можем когда мы складываем два вектора и используем обычные обычно инструкция процессора нам нужно сделать компоненты сложить по очереди одна за одной когда мы используем то есть четыре операции процессора а когда мы используем сим даппи рацию векторизованное векторизованное сложение то все виктора складываются ну просто за одну операцию мы получаем грошев четыре раза ускоряемся вот например этом примере по что мы вектор стал два вектора сложили просто за один такт процессором примерно кроме того многие современные лапа сданных умеют использует очень активно использует компрессию данных но потому что во первых комментам сжатые данные быстрее прочитать диска по что они мест меньше места занимают и также их быстрее передать из памяти в цеху в кажется пум ноги за что надо тратить какие-то так ты какой-то часть времени цикл на рожайте этих данных но это окупается на самом деле и есть еще такая штука для прими зайца runtime а которая называется light материализация когда мы читаем в память только те данные которые нам нужны в том числе эти колонки эти строчки которые нам нужны но например есть у нас запрос мы хотим посмотреть какие бренды продавались таком-то регионе у нас есть предикат что мы делаем мы сначала во-первых идем но при первый шаг это мы идем читаем колонку с нашим нус который используется в предикате регион вычисляем фильтр и получаем на выходе какой то бит вектор садишь никами строк который фильтр прошли то есть это 0 колонка и вторая затем мы с этим детектором снова идем и читаем уже другую колонку бренд который нам нужно на выходе и собственно достаем вот именно вот эти вот нулевую и вторую строчку и отдаем мой клиент то есть мы тут прочитали фактически целиком одну колонку а вторую колонку мы прочитали там на какую-то небольшую часть за счет этого тоже происходит существенное ускорение и вот с этими оптимизации не runtime а их от их вклад оценить ну довольно сложнее чем когда мы пытались оценить как ускоряется чтение томат параллелизма там или замены жесткого диска на ssd вот была такая работа от обойди пятнадцатом году где они сравнивали какой-то roll star слева на на слайде они прогоняли кайтом бенчмарки он этот бочонок занимал какое-то время роуз top показывай сколько времени он занимал и было у них собственно к это колон читая база там академическая и по умолчанию со всеми оптимизация my она работала гораздо быстрее этого роуз тора и они попытались посмотреть что будет если мы если они эти оптимизации отключали вот например отключили вектора и the time время ухудшилось вот на вот эту голубенькую величину отключили компрессию время еще раз ухудшилось отключили там light матери лизой шины в общем мы стали хуже чем роуз top даже работать но это вот примерно так можно прикинуть насколько эти оптимизации приносят вклад симптом тоже можно прикинуть ну нельзя точно померить то что там все случаи уникальны все запросы уникальные каких-то запросах сент улучшит каких-то не улучшит вот я взял прошелся по блогам некоторые системы прикручивали себе этот финт прикручивали отрисована и исполнение и они писали блоге что вот мы там некоторые запросы у нас выросли иван раз но у тебя взял эти как раз мы маркетинговые цифры вот как roach в своем блоге заявлял что в них некоторые операции в 40 раз выросли там pink up в 10 раз у sparco там примерно в восемь раз некоторые запросы но это конечно маркетинга bull sheet на самом деле ну реально цифры катать сент как вы можете ожидать от нескольких процентов до низких сотен процентов прироста скорости ваших запросов кроме того некоторые базы данных использует хардварные к силе рацию то есть они используют какие-то джипе you для исполнения запроса либо делают какие-то кастомные железяки совсем заморачиваться это v&j например но сейчас вот таких систем одна общая боль это то что эта передача данных между хостовой машиной и в этой сверхбыстрой железякой там подпись а экспресс он оказывается очень медленный объемов данных так мы проговорили про runtime как он какого можно ускорить но есть другие моменты например если мы восполняем запрос не эффективны мы исполняем эффективный план то даже очень быстрый runtime нам не поможет вот например смотрите запрос мы джойнер две таблички причем jonim корди картаго произведения то есть у нас получается вот этот на выходе этого sd-карта упражнение быть гигантская табличка огромных размеров и потом мы начинаем просто отфильтровывать лишние строки естественно эффективно эффективнее было бы вот эти все фильтры опустить вниз например отфильтровать строки еще до join of да и сам join переписать декартова произведения на и quick join этим занимаются такими преобразованием занимать занимается оптимизаторы там давным-давно 80-х годах они еще умели только делать как раз простые операции это фильтр фильтр опустить пониже там проекции опустить пониже но со времени стало понятно что это что этого будет недостаточно то есть эвристик никто хватает например самая главная проблема это оптимизация порядка джонов вот но jonim 3 таблички друг дружкой если мы с желанием одним образом тот у нас промежуточный результат этот большой квадратик joy нам он будет например занимать 6 миллионов строчек просто огромная огромная штука у вас память это как этот запрос будет исполняться долго а если мы поменяем порядок джонов чуть-чуть то промежуточные результаты могут оказаться гораздо меньше вот например в этом примере этом десять строчек промежуточный результат естественных запрос выполнится во много раз быстрее и все это делается при помощи на такие вот выбор наилучшего плана происходит при помощи так называемой cost бэст оптимизации когда каждому оператору в плане представиться какая-то стоимость то им и потом оптимизатор генерирует огромное количество альтернативных планов и пытается среди них выбрать тот который имеет наименьшую стоимость но например вот план там имеют еще стоимость 1600 мы опустили фильтр пониже получилось 1100 то есть ну как бы существенно лучше ккц как оценивать операторы то есть что что такое cost обычно чаще всего самый большой вклад в cost оператора привносит то количество строк которые он обрабатывает обрабатывать но это логично чем дольше чем больше строк обрабатывает оператор там фильтр какой-нибудь тем дольше он будет молотить их в процессоре и чтобы понимать сколько стоит каждый оператор но во первых нам нужно понимать сколько у нас данных есть там сколько в строчек в табличках сколько там уникальные значения и так далее и второе это надо научить научиться оценивать селективность предикатов например у фильтр с и нас приходит 100 строчек сколько строчек выйдет из фильтра сколько строчку над кинет это серьезная задача которая ну до сих пор активно обсуждается в академических статьях исследуется и так далее ну например если у нас scan по табличке там у нас есть какая-то какое-то знание быть знание об этой таблички мы знаем что в табличке 100 строчек хранится и что от колонка каста мерки имеет 20 уникальных значений что из этого можем какой вывод сделать что скан вернет передаст фильтр 100 строчек а фильтр ну тут уже сложнее досолите у нас есть 100 строчек и 20 уникальных значений как раз то мерки естественно что можно предположить что каждое значение каста мир киев ключ встречается в среднем 5 раз и вот мы пытаемся при помощи как раз этих статистик предугадать сколько вернется из каждого оператора строк и на основании этого построить как ростка ст модель и оценивать планы выбирать из них наилучший покос без оптимизации тоже есть много статей в блогах книжек и так далее вот здесь я вынес из блоков как раз просто из парка насколько у них улучшилась улучшились планы уст скорость исполнения запросов после того как они добавили к sbs оптимизацию но вот просто заявляет 15 раз выросли некоторые запросы с парк там в районе 8 тоже 10 то есть это довольно таки существенное ускорение можно получить но к сожалению сожалению статистике оценка селективности она может ошибаться то есть мы можем неправильно предугадывать касты у операторов вот есть такая знаменитая статья насколько хорошо optima оптимизаторы и вот здесь мы видим такую картинку скажем там есть такой ленин единичка это значит внушает оптимизатор не ошибся то есть он точно оценил сколько строчек будет на выходе и и тут сравнивается пять баз данных это поиск погряз и несколько коммерческих баз данных и вот по оси получается абсцисс идет все количество join авто есть с от 0 до 6 идет количество джонов которые мы проверялись и как видно из графика все базы данных очень сильно ошибаются там чем больше у нас жернов в запросе есть тем больше оценка оценка к финальной кардинально стиву запроса расходятся с реальностью то с чем дальше отклонения от единички тем хуже вот например мы видим что там 10 10 4 степени то есть в 10 тысяч раз отклонения было оптимизатор ожидал там десять строчек на выходе получил 100 тысяч это очень плохо и получается что ваш оптимизатор ошибся и надо как-то уметь обрабатывать эту ситуацию потому что если об темнота расшибся вы можете уйти в очень долго исполнении запроса есть два варианта это переделывать этот план на лету пересчитывать его как-то и 2 это собирай дополнительную статистику вот мы поняли что у нас там фильтр выдал там не не 10 строк а там миллион строк мы можем запомнить это ага и следующий раз когда мы будем планировать запрос мы будем учитывать наши ошибки эта штука называется адаптивной оптимизация нота пример spark oracal не умеет менять стратегию joiner на лету но здесь показаны примеры из оракла barrocal когда у него в правой части join a мало строчек он может предпочесть мест отлуп join но если мы начнем исполняет запросы фильтр нам вернет вместо 5 строчке вернет тысячу оракал может понять ага так четко нас тут пошло не так давайте переделаем и быстренько меняет стратегию джона нахождение ну и spark примерно тем же образом действует только меня этом распределенная стратегии джона вот ну и второй вариант это красота адаптивная статистика когда мы обновляем статистику кардинально sti то есть у нас рассчитывали на пять строк вернулась в тысячу а по все следующий раз мы это мэтт ошибки не повторим адаптивной оптимизация тоже сильно влияет на скорость но существенно вот например у spark заявляет что некоторые запросы защита за счет только адаптивной оптимизации увеличились на там восемь раз что очень-очень круто так ну и вообще можно задаться вопросом зачем мы вычитаем наши данные исходные данные можно же не их не читать например если у нас есть мы можем создать точнее мать и реализованную в юху скажем например с с продажами по каждому региону и когда у нас есть к это суммарные продажи по каждому региону и когда у нас есть какой-то запрос можно просто посмотреть не мочиться ли этот запрос на какую-то материализованный в юху который у вас уже есть и например просто не пойти и вот посмотреть на предикат и прочитать результат запроса низ таблички дальник нечто и табличку эту здоровенную целиком а просто почитай прочитать одну строчку смотри назад материлась твою грехи и здесь как раз получается что запрос вместо того чтобы сканировать там гигантскую таблицу мы читаем просто одну строчку это фактически запрос можно исполнить за миллисекунды благодаря такой штуки но смотрю за вами фильм тоже есть проблемы например нам надо их создавать кто это будет создавать крутить бы сделать ну либо там тебе и ваш либо некоторая система умеет рекомендовать в you health они смотрят на те к запросы на те паторны запросы которые вас исполняются и говорят слушайте ну вам бы было бы не плохо если у вас те же самые запросы сохраняются те же паттерн и вот такие вещи создатели тогда все будет быстрее гораздо еще большая проблема тоже из которая активно изучается в академических статьях это то как в юхи апдейтить то есть представьте вам пришли новые данные ваш основной таблицу там или что-то из нее удалили эти данные нужно отразить во вью и потому ну чтобы синхронизировать представления это тоже существенная проблема некоторые апдейты синхронизируются хорошо некоторые плохо например если но дюк содержит например минимальное значение какой-то колонки а мы взяли удалили это минимальное значение что надо получается пересчитывать вообще всю руку целиком короче там свои нюансы вот ну и третья проблема с верхами это как выбирать то есть оптимизатор должен понимать что у него такие то в ухе есть под ногами там он видит 100 views и когда к нему приходит запрос он должен уметь выбирать из этих те которые лучше всего подходят для запроса это тоже такая существенная сложная задача ну в общем подводя итог мы посмотрели следующие оптимизации для лап движков 1 вертикальная машет масштабирование когда заменяли жесткий диск на ssd там горизонтальное масштабирование когда добавляли ноды читали данная параллельно потом посмотрели на парте шин burning наколол штор на ко лан тпру ning посмотрели как происходит исполнение запросов и как это в runtime и как от runtime оптимизируется при помощи векторизации при помощи колон ночного представления данных памяти и также глянули на крутой с оптимизацией адаптивно оптимизацию то есть в принципе это такие основные вещи благодаря которым olap баз данных умеет выполняет запрос и ну буквально за секунды до несмотря на то что данных просто гигантские то есть это принципе на самом то деле не никакой не чудо расчудесное шея а просто инженерная такая смекалочка которая пришла на помощь так на у меня все вопросы спасибо большое друзья ну погнали руки их шикарные слышали отлично масочку только надо снять а то получится неразборчивая речь здравствуйте спасибо за доклад еще наверное остался за кадром такой вопрос как м кипит то есть там тоже наверное есть всякие хитрости как можно сделать так чтобы на 1 0 где хранились нужные данные чтобы не передавать большую таблицу по сети есть ли какие то ну такие общепринятые подходы так оптимизировать вот этот разложение там в кольцо и так далее рпп мпп ну общая стратегия здесь это ну-ка локация то есть эти таблички которые вы хотите джоель например друг дружкой вы раскладываете просто по колодцы раваны по этим ключам join авто есть например если у вас там гигантская табличка не знаю там сотрудников и табличка департаментов того просто сотрудников которые хранятся у которой работает департаменте по что вы часто будете женить dive to me to сотрудники в их складывайте на одну ноту и потом все join a выполняется локально то же самое например с агрегатами вы храните вы раскидывайте по нодам таблички так чтобы агрегаты все могли по максимуму исполняться локаль локально на каждой 0 то есть например если перца с той же табличка сотрудников группируйте часто по по департаменту в которых они работает просто табличку сотрудников сортируйте как раз лишних у департамента и вас все работает быстрее ну то есть факты прямо по дмн шоном и раскладывать сразу да да спасибо традиционный юн ши в клетчатом вот он с нами ребята кто смотрит нас в онлайне вы хотите задавайте вопрос ваши лица будут на вот этих огромных экранах пожалуйста привет спасибо было очень интересно и я хотел уточнить про механизм цапли рования то есть когда мы готовы потерять немножечко в точности но вот хотим сильно выиграть производительности какие вообще подходы вы прочтут конкретно запись она должна пасть наш агрегат когда мы хотим записывать одну из 1000 не записи например такая не очень понял все сэмплирование или до земли рования сэмплирование то есть как когда вы выполняете запрос не по всем не по всем данным а вот именно выбирать какой-то сэмпл данных по нему гоняйте ну то есть в классе можно там про что мы хотим работать одну из сотни записей то есть одну сот агрегата ну чаще ну как бы видеть для на raw для продакшена это конечно не годится то есть какие-то реальные отчеты строить с цифровым данным насколько я знаю сэмплирование но чаще всего используется для проверки какой-то гипотезы например всем неохота ждать долго долго вы просто действительно берете прогоните запрос по ком-то подмножеству по сэмплу вот мне лучше вопросу у нас есть разные подходы то есть если мы возьмем первые там 10000 записи и говорим что они падают наш sample это будет не всегда релевантно мы хотим узнать что-то на каждом промежутке времени у нас там какие-то были изменения мы хотим видеть как там пользователь кликнул на кнопку например в течение определял промежутка времени есть какие-то стандартные алгоритмы выбора старт алгоритмы сэмплирование запрос ну там всякие до вероятность алгоритм там не знаю выборка по бернулли там или такое все это да да им нет до по разному бывает и если вы хотите получить результат похожий на правду да вы используйте эти вероятностный алгоритм то есть вы смотрите выбирайте строки и вот именно но и сами только вернули приходит в голову тома штуки то есть схемы а когда вы хотите проверить например ну запрос там сам он тут долго будет считаться не долго там не знаю в тестовом режиме у прогоняйте то можно брать первые десять строчек ничего страшного не произойдет вам нужно просто проверить работоспособные запроса мне кажется это а это зависит от ту какой стратегию выборки использовать зависит от ваших целей либо получить данные похожие на правду либо просто протестировать запрос ноги спасибо спасибо большое следующий и вот есть на первом ряду и на последнем спасибо за доклад такой вопрос если языке шин план может поменяться на лету то это как-то влияет на отображаемый explain запроса если влияет то как какой будет показан explain дуэли после ну это все зависит конечно от от системы конкретной которые это реализуют вот я например могу сказать вот в яндексе в яндекс кого л н бюджетом все это моментально отражалось то если вас поменялся запрос произошла кайт адаптивной оптимизация он сразу в веб-интерфейсе там прилетает по сокету сообщения что вот запрос изменился и все перерисовывается моментально вот как ворог layout не помню честно говоря то есть там как-то как-то тоже это отображается тася вот такие-то минусики выставляю же тут детали не помню но explain как-то отображает что у вас есть альтернативное исполнении ну в общем все зависит от системы как вы сделаете так она и будет работать наверное все спасибо за доклад немного вопрос не совсем в рамках доклада он следующий вопрос пожалуйста у нас было несколько этапов внедрения лап в нашей компании каждый раз презентуем менеджером менеджер смотрят на это и не пользуется и все время спрашивают а зачем нам это мы показываем примеры такие понятно и не пользуются может быть вы как нибудь посоветуете как там примеров может быть тем больше накидать или может быть какое-то об усилении провести то есть как заставить людей пользоваться таким замечательным инструментом потому что там для себя я понимаю чем чем он не может помочь а менеджеры не понимает а им он нужен мне кажется это проблема с менеджером то есть если ему не интересно видеть не знаю какие-то инсайты в своих данных если он знать не хочет марий там как продажи растут и так далее да нет им интересно они просят отчеты но вот почему-то olap не получается засунуть в общем то другому готовить отчеты да там он тут на грант действительно проблема штуки с менеджером мне кажется надо как-то их убедить что вот есть такие прекрасные штуки они там для отображения данных там всякие qlikview там еще бывает табло там букер и так далее как где там менеджер сам без помощи каких-то дополнительных анель аналитиков может там потыкать какие-то галочки там я вот наблюдал по давно еще своей работе что когда менеджеру показали что вот можно короче самому там потыкать галочки посмотреть как там в каких регионах там как что продается там какие продажи падают какие растут загорались глаза и он начал пользоваться то есть видимо надо какую-то грамотную презентацию провести вот и не просто использовать горную систему olap которая просто где-то запросы с ручками печатаешь вот именно попробовать вот эти всякие bio bio инструменты то упал рубио и там табло и так далее мы пробовали excel показатель для подключения к лобку и power бы бей пока не пользуется видимо из страны ну вот очень просто роман интеллигентный человек на я переведу то есть есть менеджер и вот табло и работаем вот менеджера или потом очень человек после удара в нос очень внимательно слушает и и всегда потом делает то что ты рассказал а кто следующего кабинка вот пожалуйста дасти роман просто часть большое спасибо за доклад очень интересная и информацией и главное подборка в конце всех вот этих вариантов оптимизации вопрос меня такой если наша olap база данных территориально распределенные когда узлы находится в различных судах если какие-то стратегии оптимизации применимые именно вот для этого кейса на разных уровнях а то есть вас данные партиции раваны между сотами то есть вас разные копии данных в разных сотах лежат да один это один из вариантов просто у нас как насколько я понимаю это все обычно в лабазе хранится обычная лабаза используется какой-то data warehouse то есть вы подготавливаете какие-то данные от вас есть не на этом под gres стоит где там все но вот типичный базы где там все вся собственно кухне кипит а там продажи и так далее и вы потом из этого полного под gresso при помощи и теле вытягивайте данные подготавливать их переваривать и складывайте вала базу уже подготовлена из ну для удобного быстрого использования и поэтому лап даны лабазе вы догонять и запросы то есть если у вас это alup база данных ляжет там не знаю удалятся все данные с нее то как бы ничего ужасного не произойдет эти данные не критичны потому что они всегда вас тут восстанавливается из условного прогресса поэтому мне кажется что я не заказов для чего вы сортируете данные по разным празднует это центром по судну там принципе тоже лучше удержать всего в одном дата-центре чтобы просто быстрее эти запросы исполнялись пышно score ну а скорость это главное требование лапа и я бы так не делал я бы держал все-в-одном дата-центре да но если он там что-то все все упало то мы просто не знали сделали бы там и те или заново подняли бы все другом дата-центре то есть пока что так исторически сложилось то есть часть данных партиций по определенному ключу лежит в одном центре часть с другом естественно в планах переезда все-таки в один слот но пока что выкручиваем ся так ну вот лучше всего ускорить этот переезд тогда да действительно будет быстрее работать скорее всего хорошо спасибо здравствуйте спасибо большое за доклад у меня такой вопрос у нас есть очень допустим несколько больших таблиц да нам нужны joy нить и все такое классная история смотрю хамидом и этим активно пользуемся и периодически возникает такая мысль создать ненормализованную таблицу где будет просто выборка там из определенного количества колонок и допустимый те или каким-нибудь процесс да и в эту таблицу данные класть периодически до из исходников и source таблиц вместо использования матвей ухе какой-нибудь join me например да в которая будет на ставку чисто пополняться вот что вы думаете про разница между двумя вот этими методами что лучше использовать и оки или это например закрыть глаза на нормализацию данных и сделать отдельную в которой будет повторяться колонки например из разных таблиц и так далее собирать туда спасибо я понял ну вообще тут конечно xp эксперименты нужны то есть что надо смотреть конкретно на ваши данные и что для вас лучше подойдет тут надо мне кажется без эксперимента не как то есть некоторых случаях там от вехи отрабатывает некоторых случаях где нормализации потом фу сканы по этой таблице насколько я знаю гаусса в этом хорош то есть тут эксперимент уже спасибо и видимо финальные сейчас будем выбирать кому подарим книжку и потом пойдем в цифровые клары пожалуйста да роман спасибо за доклад и вы вскользь упомянули про апдейты материала из you down какие вообще есть стратегии рекомендации именно по обновлению этих материалов и без полного их перерасчета допустим удаление происходит какой-то записи там мин допустим или сам то есть если вообще какие-то общие подходы то есть допустим pin есть там absurd еда который позволяет часть реализовать таких сценариев но это я так понимаю их какой-то частный случай может быть есть у вас какие-то рекомендации ну смотрите какое то время назад которыми назад этим вопросом занимался этом из изучал читал статейки и там на самом деле все проще чем кажется то есть там нету каких-то супер пупер этом север секретных алгоритмов там все довольно что называется straightforward когда ну например для вас есть агрегат суммы да там его просто отдайте там из отчетов добавили вы добавили к этой сумме там удалили культа строку вы удалили там евреев там чуть посложнее да потому что вам нужно мыть они там и сумму и количество там 9 1 на другое то есть там самом деле ничего такого кого-то сверхсекретного не то там действительно некоторые агрегаты вот эти сложно когда как вам дельты приходят запросам некоторые ну некоторых сложно некоторые просто и в основном весь как раз research 100 статей очки не как раз об этом говорят что ну как-то не который пытается улучшить какие-то алгоритмы нектара пнул на самом деле все довольно таки просто происходит и зачастую некоторые моторизованные в ухе некоторые агрегаты которые сложно maintain нить которые там требует полного пересчета некоторые базы данных просто говорят что мы так извините мы такое не создаем короче давайте там как-то переделывать то есть нужно довольно не сказать что все радужно то есть тут надо короче каждому каждому агрегату каждый учетом удивило и сделали подход и на этой позитивной ноте спасибо большое спасибо за доклад скажи пожалуйста кому подарим книжку за лучший вопрос а вот парень спросил продуктивной оптимизацию обмакните рукой чтобы вот отлично получить приз тебе тоже памятные призы от хайло да спасибо великолепно"
}