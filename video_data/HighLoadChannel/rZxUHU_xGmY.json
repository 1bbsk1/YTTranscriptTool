{
  "video_id": "rZxUHU_xGmY",
  "channel": "HighLoadChannel",
  "title": "Artisto: опыт запуска нейросетей в production / Эдуард Тянтов (Mail.ru Group)",
  "views": 11266,
  "duration": 3250,
  "published": "2017-04-22T14:48:18-07:00",
  "text": "Меня зовут Эдуард Тянтов. Я занимаюсь машинным обучением в компании MRU Group. И сегодня я вам расскажу про приложение артиста, про технологию, которая лежит в основе этого приложения. Значит, вместо тысячи слов наш проморолик. Здорово, не правда ли? О'кей, давайте я дам пару фактов о нашем приложении. Ну, во-первых, это самое первое в мире приложение для стилизации видео в мире. Да. До этого были только стартапы вроде Departo. Эта компания по сей день существует и предлагает, например, сконвертировать 10 секунд видео за 100 евро. Мы своим пользователям предлагаем это сделать абсолютно бесплатно в онлайне, в отличие от депортио, которые делают это в оффлайне. Во-вторых, у нас уникальная технология стабилизации видео. Мы очень долго над этим работали, потому что если взять технологию для фото и попробовать её применить для видео, там возникают различные проблемы, которые сильно ухудшают качество видео. Вот я об этом подробно расскажу. И третье, мы всего лишь за один месяц, то есть с того момента, как мы сели за разработку технологий и до того момента, когда приложения вышли в сторы, прошёл всего один месяц, что достаточно оперативно. Значит, поднимите руки, кто из вас, э, ну, разбирается, слышал в курсе свёрточных нейронных сетей. О, достаточно много людей, но всё-таки половина, поэтому я на этом более-менее подробно остановлюсь, чтобы все могли понять, что происходит. О'кей. Немножечко история. Значит, учёные достаточно давно пытаются понять, как работает, как устроен наш мозг. Да, и в шестидесятых годах ещё предложили математическую модель под названием многослойный персептрон, где нейроны объединены в слоя и все нейроны, связанные с соседними слоями, связью каждой с каждым. Значит, технические обучение такой сети означает определение этих весов. И, в общем, такая сеть может воспроизводить любые зависимости между данными. А в семьдесят четвёртом году научились их более-менее эффективно обучать методом обратного распространения ошибки. Этот метод используется и по сей день. Вот, естественно, с некоторыми улучшениями. Значит, его суть заключается в том, что мы берём, э, на выходе то, что выдаёт наша сеть, сравниваем с тем, что должно быть с истинным значением на выборке, и эту разницу, на основе этой разницы ошибки мы изменяем веса в последнем слое. И дальше процедуру мы повторяем слой за слоя, как бы распространяя ошибку из конца сети в началу. Вот так эти сети обучаются. Значит, в девяностых годах Ян Ликун, он сейчас руководитель э исследовательской лаборатории фейсбука Фер, э, активно работал над созданием свёрточных сетей. На девяносто восьмом году он выпустил работу, где он рассказал подробно, как с помощью таких сетей можно распознавать рукописные цифры на чеках. И некоторые банки в США применяли эту технологию на практике. Вот. Но тогда эта технология в свёрточных сетей особого популяризации не получило из-за того, что нужно было достаточно долго их и трудно обучать и нужно было много вычислений. Значит, прорыв случился много позже, и об этом я расскажу. Значит, ну давайте подробно обсудим, что такое свёроточная сеть. Она получила своё название за счёт операции свёртки, как ни странно. Значит, её смысл заключается в следующем. Мы берём изображение РГБ, да, или можно, в принципе, GR scale. Это три канала, три матрицы. И мы эти матрицы прогоняем через свёртку. Свёртка, вот как показано внизу на слайде, это такая матрица 3х3 в данном случае, которую мы двигаем по изображению и поэлементно умножаем веса на этой свёртке на числа соответствующие. Складываем и получаем в соответствующих позициях некие числа. Получается, мы входную матрицу преобразовали выходную, и эта выходная матрица называется Fatch Map. Вот. И если рассказать, что графически кодирует свёртка, она кодирует какие-то признаки на изображение. То есть это может быть, например, наклонная линия. Тогда в Фичемэпе мы будем иметь информацию, где, в каких местах эта наклонная линия имеется на изображение. Значит, естественно, в свяршенных нейросетях эти этих свёрток очень-очень много, да, там тысячи, и они кодируют различные признаки. Вот что самое замечательное в этом всём, нам абсолютно не нужно задавать их, то есть они в процессе обучения выучатся сами. Значит, второй важный блок - это пулинг, так называемый, или операция саб-смплинга. Она призвана сократить размерность входной матрицы, да, входного слоя. Значит, как показано на рисунке, мы берём матрицу 4х4 и по четыре элемента берём максимальные, то есть получаем матрицу 2х2. Таким образом, мы существенно сокращаем, а именно в четыре раза все последующие вычисления. Вот это нам также как бонус даёт некую устойчивость к перемещению поворотов объектах на картинке, потому что в свершенных энерсетях в энерсетях там много этих операций пулинга, и в результате к концу сети уже немного теряются пространственные координаты и получается некая независимость от положения. Значит, если мы посмотрим, что свёрточная сеть выучивает, то получится достаточно интересно. Данная сеть училась на лицах. И мы можем видеть, что вот на первом свёрточном слое, э, нейросеть выучивает простейшие примитивы, да, это различные градиенты, линии, часто там цвета. Вот, то есть такие базовые примитивы. Значит, мы точно знаем на сегодняшний момент, что наша визуальная кора головного мозга, да, Visual Cortex работает точно также. Вот, соответственно, сеть эмулирует наш мозг. Вот. Далее у нас получается по мере движения по сети всё усложняется и усложняются те признаки, которые выделяет сеть. Вот на основе первого слоя второй слой уже распознаёт некие части. лица, там глаза, уши и так далее. И уже последний слой строят целый модели лиц. То есть у нас такая получается иерархичность в признаках, да? Чем ближе к концу сети, тем сложнее объекты мы можем детектировать на основе более простых в начале. Значит, ну, основной прогресс в компьютерном зрении происходил в рамках Imagnet Challenge. Imaginnet - это такая база фотографий, размеченная руками. Там их примерно 10 млн. И, значит, она была специально создана для того, для того, чтобы исследователи со всего мира соревновались и прогрессировали, да, в этом в компьютервижене. И самый популярный челлендж, ну, значит, 2010 года проходят ежегодно эти соревнования. Там много разных категорий. Самое популярное - это классификация на 1.000 классов изображения, да, там среди классов много животных, разных пород собак и так далее. И в 2012 году впервые на этом соревновании победила свёрточная сеть. Значит, её автор Кржевский смог поместить все вычисления на ГПУ. Да, их там достаточно много, и благодаря этому он смог обучить пятислойную сеть. Вот на тот момент это было существенным прорывом, как для свёрточных сетей, так и для компьютера Vision, потому что он существенно сократил ошибку. Вот ошибка там измеряется топ-пять, то есть сеть выдаёт пять возможных категорий, что изображено на картинке. И если хотя бы одна совпадает, то считается успехом. Вот что очень интересно. Э учёные замерили, сколько люди ошибаются на этом челлендже. Значит, посадили людей, дали им точно так же размечать данные. И выяснилось, что ошибка порядка 5%. А современные свёрточные сети уже имеют порядка ошибку где-то 3,5%. Вот. Но это не совсем значит, что нейросети работают лучше, чем наш мозг. Вот. Потому что они были заточены под именно этих тыся классов, а люди не всегда там различают породы собак. И если человека посадить и его как бы показывать, учить эти классы, то, естественно, будет лучше. Но тем не менее уже алгоритмы очень и очень близко. Значит, рассмотрим архитектуру. ну, популярную архитектуру. В данном случае это ВГГ. Это один из победителей 2014 года. Она очень популярна. Популярна она потому, что исследователи её обученную выложили в сеть и все ей могли пользоваться. Значит, она выглядит вроде сложно, на самом деле она просто блочная. Значит, каждый блок свёрчный - это несколько слоёв свёрток и потом операция пулинга, то есть уменьшение изображения. И так у нас пять раз повторяется. Значит, у вас происходит пять пулингов. В конце у нас свёрки уже видят изображение там 14 на14. И количество фильтров ближе к концу возрастает. То есть если мы в начале имеем там 32 фильтра, которые распознают примитивы, то в конце сети мы уже видим, у нас уже есть 512 фильтров, которые распознают какие-то сложные объекты. Это может быть там стулья, столы, там животные и так далее. Дальше в этой сети идёт полносвязанные блоки, которые выполняют уже на основе этих признаков чисто классификацию на эти классов. Вот. Но эту сеть, соответственно, предобученную используют во многих-многих задачах, потому что вот эти признаки, которые выделяет сеть, она, ну, эти признаки можно использовать во многих задачах компьютеравиже, в том числе для стилизации фото. Значит, ну, давайте и перейдём к этой теме. Значит, как перенести стиль на фото? Ну, во-первых, что мы хотим, чтобы быть предельно понятным. Значит, мы хотим взять фотографию, в данном случае это город, и взять какой-то стиль, например, nightй Ванго Goга и попытаться перерисовать исходную контентную картинку в этом стиле, да, получить такую pece of art, как говорится. Вот кажется невероятным, но вот в современном мире это возможно. То есть нам не нужен уже никакой художник для этого. Можно это сделать. Немножко истории. Э в в сентябре 2015 году Леон Гатис выпустил статью, которую он рассказал, как это можно сделать, да, придумал. Значит, единственным недостатком было, что этот алгоритм достаточно медленный и из-за этого э широкого практического применения для конечного пользователя на тот момент не получило. Вот. А в марте 2016 года выходят две статьи, э, с идеей, как ускорить всю всю эту обработку, да. И первым из них был наш соотечественник из колтеха Дмитрий Ульянов. Вот он предложил, как это можно делать первым. Ну и дальше через пару месяцев выходит приложение Prзма, получает оглушительный успех. И через полтора месяца запускается винчат ВКонтакте для стилизации фото и от нас артисты для стилизации исключительно видео на тот момент. Значит, как же всё это можно провернуть? Значит, начнём с первой статьи Artistic Style. Как нам восстановить контент? То есть нам надо смешать контентную картинку и стилевую каким-то образом. Для начала нам надо попробовать восстановить контент. Значит, что мы для этого будем делать? Мы возьмём нашу ВГ сеть, о которой я упоминал ранее. и будем использовать её как фичакстрактор, то есть будем рассматривать её как извле, ну, способ извлечения признаков из изображения. То есть мы прогоняем картинку через эту сеть и получаем на выходе иерархическую информацию вот в этих фичемапах, э, которая нам говорит, что расположено на этих на этом изображении, да. Соответственно, можем прогнать любую другую картинку, например, там шум, получить какие-то тоже числа, эти фичмэпа, и между собой мы их уже можем сравнивать, да? То есть от то есть теперь мы можем как бы численно сравнивать картинке. Значит, какой алгоритм предложил Гатис? Он предложил такой оптимизационный алгоритм. То есть мы берём, начинаем с шума. Это как бы наша наш холст, да, на котором мы пробуем нарисовать картинку, которая будет похожа на контентную. Значит, прогоняем её через сеть, получаем признаки и сравниваем, насколько они похожи с контентными признаками нашего целевого изображения. Значит, замеряем ошибку. Ну, естественно, в начале она огромная, да, потому что на шуме там ничего не сработало, нет никаких объектов. Эту ошибку обратным распространением ошибки, да, back propagation алгоритм, гоним по сети, но сеть мы сами никак не изменяем, то есть она у нас фиксировано, а этот ошибка к нам приходит в начало в изображение. И мы изменяем на основе ошибки на выходе само изображение, то есть его перерисовыем. Итак, мы повторяем n итерации, да? И если посмотреть спустя там 1ты000 итераций, как происходит восстановление, да, то мы видим, что достаточно неплохо мы смогли с помощью этого алгоритма восстановить исходное контентное изображение. Вот. Э, видим, конечно, что потерялись цвета. Значит, если посмотреть вот на последние картинки, это последние как раз слои. До этого здесь был взят слой 42. Это один из последних слоёв. Значит, если посмотреть поближе, приблизить эту картинку, то видно, что помимо цветов ещё и потерялись немного пространственные координаты, то есть поплыли границы изображения. Если мы возьмём, значит, это произошло по понятным причинам, потому что у нас в сети идёт пулинг, мы изображение сжимаем, сжимаем, и уже в конце мы знаем, что, например, там расположен дом, но мы только приблизительно понимаем, где он расположен на картинке. Вот если по этим слоям пытаться восстановить, то вот получится так неточно. Если же наоборот воспользоваться ранними слоями, то уже там информация практически никакая не потеряна и э восстанавливается всё очень хорошо. Значит, здесь можно сообразить, что для того, чтобы смешать стиль с контентом, нам лучше пользоваться последними слоями, которые не так точно восстанавливают, потому что нам и не нужно точно восстановить картинку. нам нужно в стиле перерисовать. Так, переходим к самому интересному. Как же всё-таки перенести стиль и что вообще такое стиль? Да, ну, кажется, что стиль - это какие-то мазки, цвета. Вот именно это хочется перенести. Но если мы возьмём точно такой же алгоритм и попытаемся восстановить стиль, то у нас объекты, которые были на стилевом изображении, будут на тех же местах. Соответственно, первое, что нам нужно сделать, ну, по сути, единственное - это избавиться от пространственных координатов. как-то это ну и соответственно, если мы избавимся от координат, то мы уже как-то сможем передавать стиль. Значит, тут есть два варианта, как это делать. Ну, точнее, их несколько. Я расскажу простой для понимания, да, и сложный. Значит, можно просто взять вот fm Map, да, мы зафиксировали какой-то слой, на котором по которому мы хотим восстанавливать стиль, да, допустим, какой-нибудь ранней там из второго блока. И берём просто по пространственным координатам усредняем. Как? То есть получили такой вектор средних. Как это можно понять? Ну то есть это реально будет восстанавливать стиль. Мы сейчас это увидим. Почему это будет восстанавливать? То есть можно, например, предположить, что какая-то из свёрток, например, кодирует признак наличия звезды, да, на изображении. На sterite у нас там n звёзд, ну, допустим, 10. Если у нас, э, в процессе оптимизации там наш алгоритм нарисовал всего две звезды, да, их там 10, то сравнение вот этих средних нам поможет понять, что надорисовать ещё звёзд, да, сделать небо более звёздным. Вот более сложный способ - это взять и попарно перемножить все эти фичимэпа между собой, да, и получить так называемую матрицу ковариации. Вот это такое обобщение дисперсии на многомерный случай. То есть в первом случае у нас было средняя, это можно считать, что дисперсией. И так как в дисперсии содержится информация о взаимосвязях между фильтрами, то вот чисто экспериментально оно работает лучше. И значит, наш алгоритм не сильно меняется. То есть мы также фиксируем какой-то слое, с которого мы хотим э восстанавливать стиль или слои. Прогоняем нашу стилевую картинку через сеть, получаем какой-то фи, из него мы получаем ковариционную матрицу. И то же самое проделываем с с нашим холстом, шумом и сравниваем уже не фипа, а именно эти матрицы. И точно также прогоняем ошибку обратно. И получается неплохо. Значит, что мы здесь видим? Это шум в стиле Вангога, да, какой-то. цвета, мазки, всё передалось. Если мы поиграемся с различными слоями, то мы увидим, что, например, на первом слое там ещё не недостаточно информации о стиле, и там только вот цвета мы видим, там жёлтый, синий, чёрный. И чем дальше мы будем добавлять слои, например, если второй и третий добавим, то там уже, в принципе, очень хорошо передаётся стиль. Если же мы добавим из последних слоёв, которые распознаёт объекты, вот у нас уже начнут вылазить всякие башни из изображения. Ну, они полностью теряют координаты, конечно, но тем не менее очевидно, что предпочтительнее брать из первых слоёв, которые передают именно сам стиль. Значит, теперь, когда мы имеем две технологии восстановления стиля контента, нам бы их как-нибудь смешать, чтобы получиться смиксовать эти два изображения. Значит, как мы это делаем? Значит, опять же, фиксируем стиль, получаем из него корационную матрицу. Запомнили, да? Берём контентное изображение, извлекаем из неё фичима, фиксируем. И теперь наш шум прогоняем, получаем и то, и другое. И в некой пропорции мы сравниваем, да? То есть мы сравнили две коррекционные матрицы, две фичимпов. У нас получилась какая-то ошибка, мы её взвешиваем с какими-то весами, да, допустим, стиль мы говорим, что в 10 раз важнее, чем контент. И будем надеяться, что объекты останутся на своих местах, но перерисуются нужным нам стилем. И у чудо это действительно происходит. Вот эта картинка из статьи, она как бы взбудоражила всю общественность в диплёнинге. Это был прорыв. Вот. И позволяет о нам уже без художника стилизовать любое изображение. Итак, если резюмировать, то вообще нам этот алгоритм позволяет любые вообще две картинки, неважно какие, смешивать хоть два, хоть две фотографии. Вот. И не требует никакого обучения. Тут здесь только оптимизация, поэтому можно достаточно быстро экспериментировать, подбирать нужные параметры и, в общем, очень быстро получать какой-то результат. Значит, код на всех популярных библиотеках для диплёнинга есть. Можно выбрать любой более подходящий. Но самый большой жирный минус это в том, что всё это очень долго вычисляется для, ну, для онлайна. То есть, если на ЦПУ - это 5 минут, если на ГПУ современных, то это порядка 10-15 секунд. Значит, ещё фото можно как-то постараться засунуть в онлайн для видео? Нет. Ну и Дмитрий Ульянов чуть позже предлагает, как можно это всё сильно ускорить. Идея достаточно простая, элегантная. Это просто взять и научить сеть стилизовать любое изображение. То есть не заниматься никакой оптимизацией, а просто подал любую картинку, сеть её как-то переваривает и выдаёт стилизованное изображение. Звучит просто. Значит, как это мы делаем? Опять же, фиксируем стилевое изображение, матрицу ковыряции из него, контентные изображения и его признаки. И теперь у нас здесь отличие. Значит, нам надо стилизовать любое изображение, да? Соответственно, мы берём некий датасет, мы будем обучать сеть. Значит, мы берём датасет какой-то фотографии. Желательно, естественно, для нашего приложения, да, брать фотографии, которые реально будет загружать пользователи, да, не какие-то там абстрактные, например, фотографии людей. Пользователи очень любят фотографировать людей из себя. Вот там они должны обязательно быть, чтобы сеть умела на них корректно отрисовывать. Значит, после этого мы берём какое-то изображение, да, из этого датасета, прогоняем его через вот нашу новую сетьгенератор, так называемую, или трансформационную сеть, которая нам должна отстилизовать изображение. Значит, в начале, когда мы её инициализируем, естественно, она там выдаёт какой-то шум, но со временем она будет обучаться и выдавать какое-то стилизованное изображение. Значит, так как нам сеть надо обучать, мы должны понять, насколько хорошо она стилизует, да? Значит, делаем мы это ровно точно так же. Мы сравниваем, мы прогоняем это стилизованное изображение через сеть, получаем необходимые нам данные и сравниваем с тем контентом и стилем, которые мы хотим, да, опять же, в не в некоторые пропорции, и эту ошибку прогоняем обратно и уже обучаем и уже не оптимизируем изображение, а именно обучаем саму сеть. Итак, мы ходим по этому ДЦСту и там спустя десятки тысяч итерации сеть учится стилизовать изображение. Значит, ну вот это пример из статьи Ульянова, его сетигенератора. Здесь, что интересно, значит, на вход сети подаётся изображение в нескольких разрешениях, дальше прогоняется через набор свёрток, и как бы у нас получается, что сеть усредняет стилизации на разных разрешениях. Вот такая вот интересная у неё идея. Значит, и если резюмировать, то, естественно, огромный плюс это то, что требуется всего один прогон по обученной сети. На ГПУ это достаточно быстро, 20, ну, в район, в десятках миллисекунд это измеряется в зависимости от самой сети или входного разрешения картинки. Вот плати, это, естественно, не бесплатно. Мы за это платим теперь, потому что нам под каждый стиль нужно обучать отдельную сеть. Обучаются сети достаточно долго, но в данной задаче это ещё приемлемо. несколько часов, потому что обычно сеть таже ВГ обучаются там, например, неделю. Ну вот здесь нам для экспериментов, а их нам будет нужно проводить очень много, нам нужно несколько часов на современных картах. Вот. Ну и на момент того, как мы разрабатывали, код был только от Ульянова, он был на Торче. Это фейсбуковская библиотека, она налу, что для диплёнера, да, для специалиста по машинному обучению несколько непривычно, потому что все привыкли к питону, в основном к R. И тут требуется некий входной порог. О'кей. Значит, теорию мы рассмотрели. Всё, всё вроде здорово, всё должно получаться. В статьях отличные результаты. Давайте посмотрим, как оно работает на практике. Но прежде чем перейти к экспериментам, я должен вас познакомить с этой девушкой, которая участвовала во всех наших экспериментах. И наши нейронные сети очень сильно коверкали её прекрасное лицо. Специально для этой конференции я посмотрел, значит, я её выдернул просто из выдачи гугла по запросу девушкам. Это оказывается жена футболиста, её зовут Хофи. Вот она у нас натерпелась. И общая рекомендация не использовать фотографии себя или своих близких для тестирования, потому что дальше они будут попадать в презентацию и про вас будут рассказывать на хайлоуде. Значит, если мы возьмём вот этот медленный алгоритм Гатиса, то есть мы в фазе ресча пробовали всё, естественно, и применим некоторые стили, то мы получим что-то вполне, ну, адекватное, достаточно быстрое. То есть, в принципе, отлично стилизуются, получаются какие-то красивые изображения, здорово передаются все мазки, цвета, вот отлично передаются картины вот типа Ванго Гога. Но если мы будем пытаться добиться чистых изображений, мы об этом ещё позже подробно поговорим, то есть, чтобы лицо девушки, например, было достаточно чистое, а не всё в мазках кисти, да, то этого уже очень тяжело добиться. Мы очень долго пытались с оптимизировать скорость работы этой сети. Нам удалось даже достичь 100 милисекунд на стиль. То есть мы в процессе оптимизации, как начальное приближение используем некую деградированную картинку девушки. И так получается быстрее. Но для каждого стиля это надо подбирать. Очень тяжело. И 100 миссекунд по-прежнему неприемлемо для видео. Вот. Ну итог. Нам этот алгоритм никак не подходит. Значит, дальше мы возлагаем надежды на творение Ульянову, на нейросеть, да, который сразу стилизует. То есть, что мы хотим? Мы взять хотим взять фотографию девушки, значит, взять красивый мозаичный стиль и получить отличную стилизацию. Да, это наши ожидания. Но они разбиваются о суровую реальность. Значит, мы видим, что, значит, как мы не крутим параметры, не стараемся, девушку либо всю мастит текстуры, да, либо ей подбивает глаз, либо вообще какие-то трещины, морщины ей на лице. Ну, то есть и сколько мы не крутили, это ещё самые лучшие результаты, да, были намного хуже, поверьте. Вот, значит, из коробки оно не работает. Ну, то есть не выдаёт красивого ничего по-прежнему, да, из-за того, что обучение идёт несколько часов, очень трудно экспериментировать, да, и мы на это потратили достаточно много времени. Вот. А хороший результат получается только когда тогда, когда взять сеть из второй статьи, э, Джастина Джонсона из Стэнфорда, э, вот, и применить модель генератор оттуда. И она, оказывается, работает как раз уже неплохо. Значит, она похожа, она имеет примерно такую структуру, значит, похожа на энкодер, декодер, который применяется, в частности, для сегментации. То есть мы в первой половине сети, так же, как в ВГГ, у нас свёртки, пулинги, и в конце мы получаем какое-то представление о том, что нарисовано на картинке, да, у нас достаточно уже, ну, в несколько раз меньше разрешения. А потом мы начинаем делать обратные операции, как бы повышая разрешение, уменьшая количество фильтров, перерисовывая изображения. И на выходе мы получаем картинку. Вот. И оказывается, что это в целом достаточно неплохо работает. Там плюс пару патчей танцев с бубнами и получается неплохо. Значит, такой стиль типа огненного уже сразу можно в продакшн. Вот тут ничего не нужно, он трышовый и всё отлично. Вот. Значит, чтобы это перенести, ну, во-первых, нам этот алгоритм, естественно, уже подходит. Он выдаёт какие-то приемлемые результаты. Он достаточно быстрый. Это один прогон в сети. И, соответственно, если мы хотим стилизовать видео, то мы бьём видео на кадры, их отдельно стилизуем и опять склеиваем, получаем видео. Но для видео есть некоторые аспекты, да, э, которые на фото не очень заметны, но для видео уже существенно. Значит, первое - это только не на лицо. Как бы не у нас может быть стиль, который, э, отлично стилизует там вот фотографию Кремля, любые пейзажи, но фотографию человека там опять получаются какие-то артефакты. Можно представить, как если эту девушку на видео снять, да, и как у неё по лицу будут гулять вот эти вот морщины, трещины, не знаю, будет ужас. А пользователи любят очень себя фотографировать, посмотреть в Instagram. Соответственно, как бы ваш фильтр отлично нелизовал пейзажи, там, любые фотографии, самое важное, чтобы он работал на людях. Вот. Иначе пользователи просто не будут этим пользоваться. Вторая проблема - это ряпь. Здесь я привожу видео, которое я взял у Марка Цукенберга. Они сейчас тоже с этим экспериментируют. И вы можете видеть, что там достаточно сильный реб ребит на пустых областях, на стене, на потолке. Значит, связано это с тем, что алгоритму, оказывается очень выгодно в эти пустые области накладывать текстуру. Вот. И текстуру он от кадра к кадру накладывает немножко по-разному и всё ребит. Выгодно ему это делать, потому что, напомним, у нас contentт LOS, да? То есть мы контент сравниваем последние слои, где у нас объекты. На стене объектов никаких нет, да? А стилевое - это ранние слои, где вот какая-то текстура и стиле и энейросеть, удовлетворяя стелевой лос, лепит туда текстуры, не создавая особо никаких объектов. Поздние слои контента, ничего это не видит. И, соответственно, сеть безнаказанно всё мастит текстуры, да, и это сильно ухудшает видео. Значит, мы с этим много боролись, с этими двумя проблемами. Первый из наших наработок - это так называемый hitmap loss. Мы берём опять вот какой-то контентный layer и, значит, у нас получились какие-то признаки, да, мы детектим какие-то объекты. Значит, если мы возьмём все эти 512, да, например, фильтров просуммируем, то мы получим оценку, насколько в той или иной части картины много объектов, да? Если мы вот видим стена, да, она плоская, там ничего нет интересного, синий цвет, там нет никаких объектов, мы их не видим, да? Если мы возьмём лицо девушки, там особенно её волосы, там уже много интересных фич. последние слои много чего интересного видит. Если мы возьмём плохую стилизацию, как снизу, и она начнёт лепить нам текстуру, куда нам не нужно, да, получается совершенно другой хитmap. И мы можем сравнивать вот эти вот тепловые карты между собой как бы добавить ещё один лос и штрафовать сеть, чтобы она не могла эту текстуру мастить там, где нам не нужно. То есть мы управляем обучением сети. Значит, второе. Значит, как выяснилось, разработчик машинного обучения может пару дней потратить на то, чтобы там этот стиль довести, который нам нужен до идеала. А можно взять дизайнера, взять Photoshop и начать управлять стилем путём изменения самой стилевой картинки. То есть мы будем менять по сути эти карты э слоёв, да? То есть мы здесь в данном случае видим красивую текстуру кубов. Если от попробовать стилизовать, то получится очень мелкая текстура, много маленьких кубов. На видео это очень сильно моргает, и мы просто дорисовываем большие кубы, чтобы алгоритму было проще рисовать сплошные пустые области. Значит, ну и второй вариант. Можно просто, у нас есть стилевое изображение, оно нам очень нравится, да, мы хотим таки стилизовать им, но там есть, например, внизу всякие цветы, которые нам не интересны. Нам интересна мозаика, да, и сама девушка. Мы её вырезаем. деградируем всякие ненужные нам текстуры, да, чтобы у нас никаких артефактов не возникало, просто однотонные цвета. Вот и это отлично справляется со всякой рято стандартный подход машинному обучению. К нам эта идея почему-то пришла в самом конце. Вот это при обучении менять входные данные так, чтобы сеть была устойчива к каким-то изменениям. Например, если у нас сеть распознаёт объекты, и вот у нас на картинке собака, да, если мы её повернём на 90°, положим, то она всё ещё собака, да, и сети необходимо быть устойчиво к таким вещам, а там в обучающем множестве просто лежачих собак может не быть, да, и алгоритм может ошибаться. Поэтому мы в процессе обучения переворачиваем собаку так всяк, и у нас она выучивает как бы инвариантно к её положению получается. Вот в случае стилизации. Значит, напомню, что у нас пользователи берут свой телефон, да, идут с ним, снимают, и у нас возникает много шума, меняется освещение, да, и чтобы сеть вот в в разных освещениях не рисовала совершенно разную стилизацию, да, и тогда картинка, чтобы не тряслась, мы в процессе обучения добавляем к обучаемым картинку шум, да, и, значит, но требуем, чтобы она рисовала так же хорошо, как будто шума не было. Да. И таким образом мы получаем некую устойчивость. Это отлично работает и достаточно бесплатно. Вот. Четвёртая вещь, что можно делать, это так называемый superреution. Э, такая задача в машинном обучении, повышение разрешения картинки. Да, она отлично подходит для того, чтобы брать всякий мелкий шум. И здорово есть всякие предобученные уже сети, например, WiFu 2X. Значит, её обучали для улучшения качества анимекартинок, но она отлично подходит и для обычных объектов. Но большой жирный минус в том, что это ещё одна нейронная сеть. Она достаточно тяжёлая и по сути это минимум в два раза увеличение уменьшения скорости. Значит, ну результаты тем не менее для Superрезаution, для этой WiFu очень интересные. То есть у нас картинку, которую мы стилизовали, она получилась немного расфокусированная, да? Мы прогоняем её через Wi-Fу и получаем, то есть у нас BLр убрался. И что интересно, вот, например, перерисовалась шапка, перерисовались зрачки. Вот такой вот эффект. То есть мы можем улучшить картинку, но мы, э, для видео, значит, увеличение в два раза это слишком жёстко по, э, по нагрузке. Для фото, в принципе, это реально. Ну, мы вот это не применяли. И мы взяли вот три из четырёх концепции, их тепловые карты, дизайнеров и агментацию, то есть добавление шума, чтобы сделать наш продукт лучше. Значит, сейчас я вам покажу кусок видео, которые мы сделали для руфера из Нью-Йорка. Вперёд. So far away from you means insane and all my brain so far away from you means insane and all my days Как вы могли убедиться, значит, видео достаточно стабильное. Все объекты на задних фонах, вся текстура, она не мельтешит. Всё достаточно стабильно. Всякие интересные объекты типа татуировки у парня перерисовываются, да, мозаичным стилем. При этом его кожа абсолютно одноцветная, без всяких артефактов. Такой отличный результат у нас получился. Значит, всё, теперь у нас есть технология, мы умеем её готовить. Пришло время поставить генерацию стиля на поток. Значит, тут есть парочку аспектов. Ну, во-первых, параметров много, реально много. На следующем слайде я покажу, насколько. Вот. И вторая проблема, что результат стилизации нужно оценивать исключительно на глаз. Причём не любой глаз, а человек, у которого есть стиль. Ой, чувство вкуса, да, некое. Вот. Потому что там пользователю то, что понравится диплёнеру совершенно может не коррелировать никак. То есть может нравиться, когда происходит отличная трансформация, всё переколбашивает, всё цвета, всё по-другому, а пользователям нужно не это вот, э, соответственно, что было замечено по мере обучения, значит, на наши стилевые контентные лосы, они как бы не очень хорошо коррелируются с красотой изображения. То есть с какого-то момента эти лоссы падают. То есть сеть как бы обучается лучше и лучше, а красота как бы начинает резко ухудшаться. Поэтому надо в какие-то моменты останавливать. То есть надо отсмотреть много результатов. Плюс ещё, когда вы смотрите сотни картинок, глаз очень быстро замыливается, да, и у нас были случаи, когда казалось, ну, когда мы боролись со всякими артефактами на картинке. Вот ты их всех убрал, показываешь, смотрите, какой отличный результат. И тебе говорят: \"Парень, это исходная практически картинка, только от Блюрина, да, там нет артефактов, но ничего не изменилось\". То есть уже кажется, что без стилизации просто лучше. Вот, значит, тут кусок интерфейса. Значит, вглядываться в эти параметры не нужно. Это просто для того, сколько всяких разных параметров, да, мы сделали интерфейс. Я вначале использовал коробочное решение FGLAB на Nodej JS, но оно у меня почему-то глючило, там, теряло некие эксперименты, и мы написали быстренько своё, несложный интерфейс. Вот. И что мы выяснили? что нет смысла брать какой-то стиль, который нам кажется, что будет здоровский, и пытаться с ним стилизовать, да, долго и мучительно, потому что нет никакой гарантии, что с какой-то картинкой вообще что-то выйдет, да, тут у нас ничего не гарантировано, поэтому проще, значит, взять несколько наборов гиперпараметров, которые у нас работали, и к ним подбирать уже стилевую картинку. Вот. И потом уже, когда что-то адекватно из этого всего получается, можно уже руками там менять какие-то параметры и уже дотюнивать хороших кандидатов. Значит, тут мы иллюстрируем, что на каждую там сотую, двухсотую итерацию сохраняется несколько изображений. Надо отсматривать много картинок. Вот сотни там на каждое обучение. Соответственно, какой рецепт успешной штамповки стиля? Значит, мы берём много стилевых изображений, да, все, которые нам нравятся, из надёргиваем из сети. Значит, берём рабочие сеты гиперпараметров, которые у нас на каких-то стилях работали хорошо и давали результат. Дальше, естественно, нам нужны вычислительные мощности, то есть нам нужно много ГПУ, да, мы использовали где-то 20 штук для обучения, да, но так как это требует вмешательства человека, то у нас занято было там штук пять. То есть только в какие-то моменты мы использовали все. Вот необходимым элементом является Red Bull. Без него невозможно отсмотреть такое количество фотографий и не сойти с ума. И, значит, после просмотра мы получаем кучу стилей. Немного о результатах нашего приложения. Значит, выпустились, напомню, мы в конце июля. И в начале октября наше приложение завоёвывает топы USA, то есть в плеймаркете. Мы занимаем второе место. В десятке мы висим там порядка 2 недель. Вот это отличный результат. Достаточно тяжело туда пробиться. Значит, в Apple Store у нас первое место мы уступили только ITSU. Это приложение, которое Apple прибил гвоздями просто на несколько месяцев на первое место. И только недавно его оттуда вытеснил. Вот поэтому проиграли только им. Ну, на этапе лонча через неделю, там две в России мы тоже занимали первое место. По ходу нашего жизни, нашего приложения, да, особенно в момент вирусного эффекта в США, у нас во многих разных там странах, во всяких гаваях там и так далее тоже были там хорошие позиции и много первых мест. Значит, второй такой интересный для нас результат, PR-эффект, это то, что мы специально для NVIDIA разработали с помощью нашего приложения стриAM, и они на своих конференциях GPU Technology, соответственно, демонстрирует возможности новых карт, видеокарт для обработки видео. Да, значит, на картинке там не видно, но президент презентует вот работу нашего приложения. Вот отлично для нас пиар. Значит, немного о userэкспириенсе наших пользователей. Значит, ну, во-первых, пользователям легче работать с фото, чем с видео. То есть мы через какое-то время, там, где-то через месяц после запуска добавили обработку фото, потому что, ну, пользователи банально их чаще делают. Потом им не нравятся всякие трышовые трансформации, как я уже говорил, там, когда ваше лицо перерисовывается цветными макаронами, пользователь от этого не становится счастливее. Ему нравятся очень простые эффекты, там замена цветов, немножко границы. Вот, в общем, ничего такого особенно, но вот это заходит отлично. Значит, потом мы заметили, что пользователи наши, они начали использовать снапчатый маскарад для наложения масок, да, и потом прогонять через артиста, чтобы получить стилизованное фото, значит, или видео, да. И мы видим на, ну, то есть мы взяли, у нас есть ICQ, да, Mailro Group, и у них есть такая технология, мы у них её позаимствовали и впилили в наше приложение. И теперь пользователи могут одновременно делать обе вещи. Значит, мы видим, что маски, когда перерисовывают, значит, когда мы маску накладываем просто на фотографию, видно, что она наложена, да? Если мы перерисовываем каким-то стилем, то уже это практически незаметно. Вот. И получается очень интересные кадры. Да, вы можете видеть меня на одной из этих картинок. Вот. И в заключение скажу, что сейчас мы живём в такое время, когда нам вычислительные мощности, алгоритмы позволяют обучать очень сложную, ну, анализировать данные, обучать сложные сети. Это всё больше и больше проникает в нашу жизнь. В диплонинге сейчас бум, постоянное новое открытие. Значит, очень много сейчас внедрения этих технологий в сферу развлечения, таких как артисты, да, и прочих. Вот. Но уже очень скоро это будет непосредственно в нашу жизнь в виде там плотируемых автомобилей. А я очень жду медицину, когда вот, например, те сети, те архитектуры сетей, которые мы сегодня рассматривали, их можно использовать S для анализа там МРТ, для анализа УЗИ. Вот они там тоже отлично работают. Я думаю, там в ближайшие лет пять уже можно будет заменять дорогостоящих специалистов, которых долго и дорого учить. и они ещё ошибаются часто. Вот такая технология может заменить. Значит, если вас заинтересовали свёрточные сети, то есть очень хороший курс Стэнфорда на эту тему, которую можно прочитать. Там всё коротко и ясно написано. Значит, если вы хотите поэкспериментировать со стилизацией, то можно вбить стайл трансфер в Google. Там будут все статьи, весь код, который можно установить и попробовать как оно работает. Да. Ну, уверен, что в результате моего доклада вы теперь точно знаете, что за этими технологиями свёрточных сетей и, в частности, стилизации стоят достаточно простые, понятные концепции. Да, ничего космического, магического там нет. Всеми этими технологиями, связанными с нейросетями, очень интересно заниматься, увлекательно. И, как вы видели по ходу моей презентации, очень даже весело. Вот у меня всё. Спасибо большое. готов ответить на ваши вопросы. Здравствуйте. Спасибо за доклад, очень интересный. Хотел спросить по поводу рамок. То есть есть интересная задача, обычно картина там в рамках, да? Не пробовали делать рамки и стилизовать с рамками, чтобы плавный переход между изображением и рамкой? Нет, не пробовали. даже не приходила такая идея. Ну, это можно сделать, да. О, ещё вопросы? А, два вопроса. Первый вопрос, а, по поводу архитектуры самой сети. Э, при каких, ээ, наверное, точнее, какая метода есть ээ для выбора архитектуры? То есть, условно, сколько слоёв взять, какие слои взять? И второй вопрос. А не думали ли вы использовать вот такой же стилизованный подход? Ну, аналогичный подход, допустим, для задач а стегонографии, по сути же, что это такое? А стегонография - это, ну, скажем так, методы сокрытия информации в каком-то потоке. Угу. Соответственно, у вас получается видео, а, в котором, например, внедрена какая-то скрытая информация, которую потом опять же использу, да, можно её достать, например, или добавить. Спасибо. Ну, спасибо за вопрос. Значит, отвечаю на первый вопрос, как вообще выбираются архитектуры, а здесь каких-то универсальных рецептов нет, потому что разные архитектуры могут себя проявлять по-разному. В целом, как как выбрать глубину? Ну, берётся там, не знаю, там 10 слоёв. Если работает хорошо, на этом останавливаются. Значит, если работает не очень хорошо, берут 20 слоёв. И так, пока оно не начнёт работать хорошо или у вас не закончатся мощности, да, или оно от сложности самой сети не начнёт переобучаться, там уже с этим надо бороться. По поводу вашего вопроса, ну, очевидно, да, что мы об этом не думали. Коля, я об этом даже не знаю. Вот. Но, в принципе, это возможно, да? сетью можно выделять любые объекты и, в принципе, научиться вполне конкретные вещи там находить и что угодно с ними делать. Ещё один вопрос. Если большое изображение и невозможно его сразу запихнуть в память и хочется набить на сегменты и после этого сегменты как-то, а, стайлтрансфером прогнать и, а после этого их обратно склеить? Вот такую задачу не решали? Нет, не решали. Но у нас для фото разрешения там 24, вот оно несколько картинок влеждает в память. Соответственно, если вы хотите какой-то совсем харес, ну да, и действительно мо, да, можно нарезать таким образом, но не не факт, что на краях как бы у вас будет сойдётся. Хотя, в принципе, если применить наши техники, которые стабилизируют картинку, да, то с высокой вероятностью всё-таки они будут нормально отрабатывать даже на краях. Вот ещё вопрос. Угу. Я а вот в продолжении к предыдущему вопросу. Максимальный размер изображения, какое вообще можно обработать на ГПУ в вашем случае и сколько примерно время это занимает? Угу. А значит, по изображениям, ну, значит, там всё зависит от памяти, да? То есть нам надо в ГПУ память положить модель саму. Значит, после того, как через неё прогоняем картинку, там появляются какие-то активации, на это тратится память и, собственно, само изображение. Значит, у нас с нашей сетью может быть 2.000 на 2.000 поместится в ГПУ карты. Уже больше не поместится. То есть для обработки, ну, 4К, да, надо делить как-то картинку и либо покупать дорогие серверные карты, в которых очень много памяти, и тогда оно влезет. Вот и второй вопрос. Да, вот про нерованные сети тут слушаю, слушаю. У меня стереотип, что это какой-то настольный компьютер. Туда карту Nvido поставили. играемся. А в продакшн-то серверные должны быть какие-то технологии? Я тогда или это следующий доклад? Нет, ну это следующий доклад, но буквально два слова я скажу. Значит, когда мы запускались, мы на лонче ставили PC, потому что тупо не было рынка, на рынке невозможно было купить серверну конфигурации, и мы брали игровые компьютеры и ставили в ДЦ. То есть потом уже спустя там неделю мы добавили серверную конфигурации. Вот. Спасибо. Ещё по поводу видео хотел спросить. Вы просто брали отдельные кадры, дальше соответству, то есть никаких между мы пробовали с этим играться. Значит есть технологии, которые вот для презентационного алгоритма позволяют учитывать как соседние кадры с собой взаимодействуют. Там можно вычислить optical flow, так называемые, как пиксели переходят из одной кадра в другой. Значит, это всё очень долго вычисляется на ЦПУ. Мы пробовали там потратили как бы на исследование там пару недель, пытались нейронной сетью, да, воспроизвести этот opical flow, и у нас не совсем получилось, и мы уже не стали дальше экспериментировать. И ещё один вопрос: а можно это всё запараллелить? Насколько вообще вот именно сам процесс параллелится? То есть если мы не можем, к примеру, обсчитывать быстро на одном, можем ли мы просто добавить ещё больше ГПУ и примеру Да, можем. То есть у нас кадр, ну, для улучшения userэкспириенса у нас, когда видео приходит в приложение, мы его по ключевым кадрам бьём на части и рассылаем в разные обработчики. И там он, соответственно, ускоряется за этот счёт. Здравствуйте. Скажите, пожалуйста, большая ли у вас команда? Вы говорили, там за месяц успели сделать. И какое будущее у вашего приложения? Угу. Значит, ну, у нас это был типа такой стартап внутри компании. Там в начале человек пять-шесть участвовали, значит, ну, по ходу, естественно, у всех возникали свои дела и постоянно и постепенно отсеивались. То есть, в принципе, эффективно там два-три человека в этом участвовали под конец вообще один именно в диплонинге. Ну, а, соответственно, для создания приложения бэкэнда это уже там по несколько человек разработчиков под каждый компонент. А второй вопрос. Какое будущее у приложение артиста? Ну, трудно сказать. Мы пока не решили, да? То есть мы для нас это больше как эксперимент, возможность попробовать свои силы в таких приложениях, попробовать пережить вирусный эффект, который мы пережили, да, и получить некий опыт, да, мы получили отличный пиар. То есть прямо каких-то особых планов у нас на будущее нет. То есть пока мы там дорабатываем какие-то вещи. Вот недавно мы совсем добавили маски. Может быть, нам придут ещё какие-то идеи, мы, естественно, разовьём. Здравствуйте. Подскажите, на слайде, где у вас декодер и энкодера, у вас есть перенос индексов при макспулинге. С какой целью вот он сделан? Так, сейчас. Где же оно? Ага, перенос индексов. Да. Вот. Ну, это я просто на самом деле дёрнул изображение энкодера, декодера, которое визуально показывает, что происходит. Сама сеть она вот, которая Джонсon, она немножко более простая, и там средних слоёв гораздо больше. Переноса индексов там никакого нет. Там идут обратные пулинги, то есть анпулинг идёт, асмплинг и там обратные свёртки. Спасибо. А, здравствуйте. У меня, может быть, глупый вопрос, но мне, если что, не стыдно. А я, как человек, который прошёл только базовый курс по машинному обучению на Курсере, вот сегодня увидел, ну, и вчера много докладов по нейронным сетям. Вот пришёл, хотел узнать, как вот где-то их можно применить. И очень многие говорили об обработке изображений. То есть, ну, в принципе, понятно, у тебя есть входная матрица, и ты через Веса получаешь другую матрицу. А вопрос в том, что можно ли нейронные сети использовать в каких-то других областях? Ну, например, у меня есть большой там лог э действий пользователей и их как-то а классифицировать. То есть об этом просто слов не было. Может быть, вы как-то направите, расскажете. Объясню, да. Ну, это очень активно применяется в компьютервижене, потому что просто до этого не было таких применений. Сейчас, ну, мало каких-то таких продуктов, да, если на основе текстов у нас уже много чего есть, да, то на основе там фото-видео достаточно мало. Поэтому это сейчас так бурное такое развитие. Но свершеные энерсети, которые в первую очередь получили своё развитие на фото, они сейчас постепенно мигрируют в другие области, там, где данные, ну, так называемые слоистые, где есть какая-то структура данных, да, например, это очень хорошо используется в голосе, то есть распознавание голоса. Все ассистенты используют как раз те же самые свёрточные сети. Вот. И для текста тоже сейчас там немножко другая ветка развития, там всякие рекурентные сети, но тоже очень похожи. свёрчные сети там тоже применяются. То есть, в принципе, для вот три таких области больших, где оно применяется на данный момент. Спасибо. Ещё один вопрос. Планируете ли вы делать для своего приложения, чтобы можно было как-то программно его дёргать? Пока нет, но спасибо за идею. Какие возможности обработки изображений видео на мобильных устройствах у вас есть? Ну, вы, наверное, знаете, да, что там PRМА запустила, э, обработку видео именно на мобильном устройстве, соответственно, расплачивается за это пользователь. Он, значит, минуту ждёт, и у него греется очень сильно телефон, потому что всё это переносится на ЦПУ. И, значит, такие модели, которые применяем мы, да, или базовые, которые есть у призмы, они на телефон, ну, не встанут, потому что они очень большие и сложные. Значит, их приходится сильно упрощать. За счёт этого теряется качество картинки. То есть стилизация становится гораздо проще. Вы можете убедиться, если воспользуетесь приложением. Соответственно, телефон сейчас переносится всё только на ЦПУ, да, и каких-то там хороших библиотек для этого нет. То есть надо пилить самим или где-то эту технологию покупать. И на ГПУ ещё там хороших биндингов нет. Там Apple сделал первый шаг к тому, чтобы в их метали добавить обработку сверше сетей, но я пока не видел, чтобы кто-то кто-либо это применял. Ну что, всё тогда, коллеги. Спасибо вам большое за внимание. Если будет вопрос."
}