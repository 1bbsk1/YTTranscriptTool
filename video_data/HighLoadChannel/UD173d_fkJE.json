{
  "video_id": "UD173d_fkJE",
  "channel": "HighLoadChannel",
  "title": "Шардирование: с нуля до Яндекс Диска / Андрей Колнооченко (Яндекс 360)",
  "views": 407,
  "duration": 2953,
  "published": "2024-10-29T03:04:45-07:00",
  "text": "Всем доброе утро Спасибо всем кто проснулся так рано и пришёл на первый доклад Меня зовут Андрей колченко расскажу про шардирование в Яндекс диске как оно развивалось вот с самых первых дней и До сегодняшнего дня немного обо мне Я уже довольно давно в индустрии делал проект для больших компаний там НТА Unilever и прочее а сейчас в бэнде диска отвечаю за ядро файловой системы с точки зрения пользователя диск - это десятки миллионов пользователей еженедельно которые пользуются самыми разными функциями то есть диск - это не а какая-то веб обёртка над файловой системой объектной какой-то это там сотни различных функций по просмотру редактированию данных про интеграции С почтой и публичный доступ но нам интересно интересен диск с технической точки зрения а диск с технической точки зрения - это более 6 различных сервисов которые друг с другом взаимодействуют про все конечно рассказывают заняло бы Целый день я вот на этой большой схеме выделил бы под систему хранения бинарных данных отдельно подчеркнул бы что она вот сбоку и про неё мы сегодня Говорить не будем а будем мы говорить про а диска которые взаимодействует с большой шарди базой данных которая ранит данные о файлах то есть когда они были загружены когда изменены а каков размер файла и так далее как они как эти файлы объединены А в папке сгруппированные Где хранятся и э Сейчас на эту базу данных нагрузка около миллиона запросов в секунду и чтобы держать её нам требуется более сотни шардов пог SQL а плюс есть два дополнительных реплика Сета довольно крупных монгодб которые держат десятки тысяч коннекто а как вот в реплике монгодб так и в шарды постгрес суммарно хранится сотни терабайт метаданных о файлах файлов Понятно гораздо больше и начать историю развития Я хотел бы с самого начала Почему в диске было принято решение о том что нужно шардирование но главное ограничение состоит не в том чтобы на один хост положить большую базу А в том что к этой базе нужно быстро доступа и самое узкое место - это оперативка потому что индекс Ну горячие куски индекса нужно держать в оперативке и это позволяет быстро работать с данными и в качестве первого шага был выбран вариант С шардирование это была крутая хайповая там SQL база данных которая позволяло сконфигурировать шардирование репликацию прямо из конфига Яндекс Диск изначально планировался как довольно большой сервис и что ему нужно расти горизонтально тако вот такая точка отчёта в виде шардирование до третьей версии Это не мы но мы всё понимаем интернет всё помнит и самым важным шагом который определяет всё шардирование наперёд является выбор ключа шардирование он должен равномерно распределять количество данных которые лежат в каждом значение ключа и желательно чтобы он обеспечивал локальность данных внутри изменяющих запросов Ну и чтобы запросы читающие тоже желательно обращались к одному рду для того чтобы более равномерно распределять значени ключей по шардам А ещё дополнительно можно применить хэш функцию но соответственно если у Вас например какой-то ключ в виде инкремента счётчика позволит встроенным механизмам удобнее лучше распределять данные по шардам и в качестве вот такого ключа шардирование были было выбран хэш от ID пользователя тут отталкивались от гипотезы что пользователи более-менее равномерно распределены по количеству данных и также так какк это хранилище персональное то для получения практически любой информации о файлах А можно использовать информацию с одного шарда и например делать сортировки на уровне одного запроса не нужно собирать данные с нескольких шардов и их сортировать что получилось в первой итерации А у нас есть приложения а а воркеры а которые обращаются к локально стоящему монс монс - это роутер запросов монс смотрит в конфиг сервер в котором хранится там распределение где лежат определённые значения ключей и в зависимости от того что отдаёт Кои они направляют уже запрос в нужный Это Лика состоящий из нескольких хостов мого мо - это сама монго здесь монс стоял локально на каждом Хосте воркеров соответственно на одном хасте запущено несколько и это позволяло например в случа каких-то непонятных историй руть вместе с воркера вместе с монго сом и полечить таким образом систему если вдруг что-то идёт не так а потому что иногда монс выносят например в отдельный кластер но в случае с диском такая история а показала себя не очень хорошо вот когда монс Установлен локально работало лучше но диск стал большой и мы стали огребать проблемы первой проблемой которую заметили что если монго большая то с SSD она Точнее с SSD она живёт а сдд живёт плохо 11 лет назад SSD все сервера на SSD поднять было довольно дорого также из того что заметили что при торможении одного шарда начинал тормозить Мос плю пере использовались Хотя уже были в не самом лучшем состоянии и плюс для нас важно чтобы мы могли переносить данные пользователя с одного шарда на другой Например если данных У какого-то пользователя очень много то лучше их перенести на шарт более свежий например с большим количеством свободного места монго из коробки такое практически не позволяет потому что монс - это чёрный ящик конфиг сервер тоже чёрный ящик нужен специальный секретный скрипт который позволит одно значение ключа перенести с шарда на шорт также часть проблем а она основано на том что мы получали очень высокие тайминги ответов от монса Почему Для нас это важно приложение было написано pyon п То есть - это такой SG сервер cg сервер который позволяет под капотом запускать несколько воркеров в соответствии с количеством цпу на каждом Хосте и у нас получалось так что на одном Хосте запущено N воркеров оно ограниченное каждый воркер обрабатывает ровно один запрос и в случае Долгих таймингов ответа от Базы А Все воркеры замирают в ожидании и заканчиваются когда нет воркеров все запросы пользователей выливаются на пол Для нас это было тяжеловато поэтому решили перейти к ручному шардирование таким образом основная идея была избавиться от монса как основного источника проблем у нас и плюс решить проблему как раз с переносом данных между шарда Чтобы не пришлось это делать грязными хака А можно было это сделать в коде понятным механизмом и также это позволило управлять коннекта к каждому рду что для нас тоже было важно и когда есть возможность перейти к ручному шардирование к рду накер то есть вокер сам из какого-то конфига или из какой-то базы данных принимает решение на какой шарт пойти а второй вариант - это когда есть микросервис который отвечает за эту функцию он при передаче ключа возвращает тот тот шарт на который нужно пойти в качестве этапа 2 был выбран Вот первый путь когда на каждом ворке решалось куда зарули запрос естественно база данных уже была не маленькая и переезд занял около года то есть полгода писался код полгода ещё мигрировали данные из старого старой инсталляции в новые шарды что получилось во-первых выделилось ко для общих данных также выделилась System db для того чтобы узнавать в каком на каком рде живут данные пользователя и приложение обращается сначала в System db затем куда ей пойти например ей Ну приложению нужно пойти в первый шарт и дальше уже запрос идт в нужный ша это база с данными например об общих папках о некоторых сущностях которые разделены между пользователями если нужно чтобы состав участников был транзакцию и писать во все шарды о том что Да вот у тебя есть общая папка и в ней вот такие-то участники для избежания транзакций распределённых транзакций вот сделали такую отдельную небольшую базу но база Уже довольно крупная Потому что сейчас держит 30-40 сся одновременных коннекто одной коллекции коллекция содержала дишни пользователя и айдини шарда соответственно она занимает гигабайты индекс помещается в оперативную память для того чтобы отказоустойчивость была высокой потому что это всё-таки у нас единая точка отказам юто падаю отказа во-первых были добавлены большое количество реплик потому что у нас Рей записи он сильно ниже чем й на чтение пользователе гораздо реже добавляют чем читают данные о том на каком рде его данные находятся и плюс у Мон высокая скорость переключение и наконец db которая хранит метаданные о файлах Здесь каждый шарт - это реплика мы называли их юнитами и состоял он из пары читающих реплик мастера одной реплики ин реплика использовалась только для снятия бэкапов туда не шли запрос от пользователей потому что непредсказуемое время ответа возвращаемся к проблемам таймингов не хотим иметь непредсказуемое время ответов Ну и конечно арбитр но диск снова стал большим и мы снова стали огребать проблемы во-первых если тормозили пара шардов то это могло влиять на весь сервис для того чтобы решить проб они количество запросов к каждому рду у по есть баур для того чтобы здесь ограничивать количество коннекто суммарное использовали отдельный компонент назвали его как он работал у нас есть воркеры на каждом Хосте наж процесс держит счётчики активных коннекто и при запросе алике обращается к мастеру и говорит мне нужен конек КР 1 счётчик увеличивается при окончании запросов вокер говорит всё Я закончил счётчик уменьшается и здесь мы уже можем ограничивать то ско мы создадим каждому рду было посчитано что для того чтобы продолжать нормальную работу нужно примерно 2Т свободных воркеров Исходя из этого ограничили максимальное количество и плюс когда достигали максимального количества ещё не сразу начинали отдавать новые коне дожидались пока спустимся То есть если посмотреть на график коннекто то выглядело Это примерно так каком становится не очень хорошо коннекты идут в гору упираются в Верхний лимит больше не водоём дожидаемся пока станет лучше или там за таймаут запросы в общем вернутся тикеты обратно в дожидаемся до того момента пока нем вот нижней планке количество запросов и снова начинаем выдавать тикеты у нас идёт нормальная работа считаем что шарт отпустила и без реку баунс выглядел график ошибок и успешных ответов примерно так что успешные ответы уходили в ноль а пятисот уходили просто в небеса ВС что есть ВС отдаём птит потому что воркеры закончились с введением кубара у нас появилось плато сервис деградировал данные вот пользователей которые были на проблемном Хосте на проблемном рде были недоступны но у всех остальных всё работало таким образом почини проб с один шарт влиял на производительность всей системы но диск снова стал большим и мы снова стали огребать Проблемы тут уже пошли проблемы с инсталляцией Мон что на больших объёмах наблюдали то что память перезагрузкой э самого Демона монг плюс вымывать индексы из памяти И для нас а для нашего применения у монг тогда был не самый оптимальный планировщик запросов и тогда было решено сделать большой переход переход на постгрес А почему собственно говоря подс во-первых у нас данные реляционные и лучше ложились именно на постгрес плюс поддержка транзакций тоже для нас было плюсом у постгрес быстрые рекурсивные запросы у нас а работает с путями и рекурсивные запросы для нас Ну хорошая поддержка чтобы обеспечивать высокую скорость операций например операции перемещения мы можем перемещать па не изменяя все пути по дереву А вот одним апдейта одной строки также была уже большая экспертиза готовая инфраструктура и был готовый координатор шардов Поэтому постс был таким хорошим выбором для того чтобы в дальнейшем расширять если говорить про реляционное У нас есть таблица с пользователями назовём её диск есть у каждого пользователя папки и в папках лежат другие папки и файлы плюс файлы где-то хранятся поэтому есть вот отдельная табличка Stage которая ссылается на объектное хранилище вот для вот этой связи папка которая содержит другую папку и вот используется рекурсивные запросы координатор же это отдельный микросервис который как я сказал уже был готов и он хранит реестр всех шардов и какие Хосты в этот шарт входят вает соединение определяет живость не живость плюс у каждого шарда определяется кто сейчас мастер кто реплика А И у нас таким образом инкапсулированная точка информации о том где Где находятся данные пользователя Ну и уже красной нитью у нас проходит тема про то что очень важно мигрировать данные пользователе переносить между шарда ребалансировка говорят где пользователь жит фактически выдаётся Con STR с указанием что вот мастер здесь реплики Вот они и дальше уже идут запросы в нужный шарт так как у нас снова есть некая единая точка отказа но это уже не только база но и воркеры саму базу важно хорошо распределить и надёжно хранить в каждом дата-центре есть установка база данных координатора шардов среди дата-центров выбирается Мастер и дальше ещё в каждом дата-центре есть дополнительные реплики куда каскадной репликации разливаются новые данные То есть у нас получается надж и быстрое хранилище мы можем в одном дата-центре сделать запрос получить текущие данные и весь запрос о том где живёт пользователь занимает единицы миллисекунд и распределение распределение пользователей подам управляется этим координатором нода случается что на какой-то шарт приезжает больше пользователей или данные пользователей становятся очень большими или количество запросов от какой-то группы пользователей нагружают шар больше чем мы ожидали и поэтому всегда идт отслеживание параметров отслеживается сеть место на Дис э то мы подключаем автоматику для миграции как она выглядит мы создаём табличку в нашем индексом хранилище где указываем что вот нужно взять пользователя с таким-то ID перенести его с шарда о на шар 2 специальная та выва задачи на миграцию которые отложено выполняются параллельно на воркера и в качестве первого шага обычно анализируется активность пользователя мы смотрим когда в последний раз он обновлял диск или заступался к диску а анализируем нагрузку на шарт назначение потому что бывает так что шарт мы только-только ввели и нагрузка за счёт новых пользователей Уже довольно высока И мы не хотим перегрузить шарт подняв его до небес переездом ещё и пользователей которые были на другом рде Исходя из этого выбирается количество потоков миграции и в период неактивности пользователя устанавливается блокировка запи устанавливается как на уровне координатора то есть координатор отвечает по Этому пользователю что сейчас выполняется миграция так и на уровне базы данных устанавливается Триггер а также после этой блокировки анализируются исходные данные там подсчитывается количество записи по for и в какое количество потоков можно перенести эти данные после переноса мы контролируем целостность и если целостность сошлась то переключаем шарт пользователя и разблокируем ему запись вся эта операция она так вот долго расписано но занимает секунды минуты максимум если пользователи данных действительно много У пользователя не было удивлённого лица при после переезда что какие-то данные не перенеслись для нас самое важное - это проран миграцию всех данных а да мы собираем данные по фон ключам плюс анализируем схемы и у источника и назначения Ну потому что новый шарт мало ли на него не накатили все миграции понятно что мы не должны были бы запускать миграцию но Всяко бывает в жизни чтобы такие случаи предотвратить контролируется что схема данных совпадают считаются количество каждого записи каждого типа в каждой таблице и плюс подсчитываются хэш суммы у всего набора данных переносимых блокировка запи на уровне тригера базы данных мало ли мы что-то забыли учесть в коде вот чтобы такого не произошло и пользовательские данные не изменились во время переноса на всякий случай ещё на уровне каждой таблицы базы данных У нас есть проверка что обязательно должен быть триггер на блокировку им обм по переходу фактически от вот коробочного решения от коробочной монг которая шардирование само ручное шардирование тоже прошло много этапов начина уке и тем что сейчас вот есть отдельный микросервис на каждом на каждом шаги решались Ну фактически одни и те же проблемы отказоустойчивость проблема переноса данных шардирование введение новых шардов в в общую базу данных ИЖ по что если бы начали вот попытались бы сразу начать с той инсталляции которая есть сейчас не факт что диск Сейчас бы существовал просто потому что на продуктовое функции на продуктовые фичи просто бы не хватило времени разработки и Да здесь очень пригодился подход и не использовать раньше времени те решения которые пока что не нужны и на этом У меня всё спасибо вам за внимание Буду рад ответить на вопрос так Ну вот здесь ко мне ближе Я поэтому сюда просто приду Спасибо за доклад Меня зовут Владимир какие-то альтернативы не по Ну не знаю там Аля Касандра или ещ что-нибудь Это первый вопрос и второй вопрос Вы описали некую миграцию данных при шардирование А как быть с данными Ну мы же говорим про мето данные правильно метаданные фай Всё верно а соответственно если у нас есть какая-то папка которую Ну не знаю там условно вся компания да то есть мы же говорим какой-то 360 лак туда вся компания Ну пря какая и так далее Вот и все туда много файлов получается постоянно ложат изменяют и так далее То есть один пользователь активность закончил ну к примеру там не знаю во Владивостоке да А в Калининграде начинают активность этим пользоваться как быть с такими данными я уверен что они были спасибо да спасибо за вопрос во-первых про альтернативы конечно альтернативы рассматривались у вс-таки у нас и была монго да как такая документ документная база данных но под Наша задача вот я говорил про задачу перемещения Да когда у нас есть там один запрос и мы меняем одну строку и мы переместили папку Ну например переименовали её или переместили в другую папку вот в случае хранения в какой-то там колоночной базе данных без там рекурсивных запросов без вот такой организации данных получается так что нужно для всех вложенных файлов для всех вложенных папок менять пути и в целом для нас по оказалось Ну решением которое было позволило удобно и хорошо обращаться к таким реляционных папок общих данных такие задачи действительно есть такие сложности бывают и тут важно понимать что данные они принадлежат одному пользователю а всё остальное это уже управляется на уровне прав доступа То есть когда пользователь создат общую папку то она прилежит пользователю просто даёт доступ к доступ другим в случае переноса данных блокируются запросы у пишущий у самого пользователя читающие запросы не блокируются бывает такое что в диске пользователя активность идёт постоянная в этом случае ну во-первых можно есть ССО выполнить данные Перенесу они будут доступны на чтение то есть в Калининграде и во Владивостоке люди смогут прочесть данные но на запись данные заблокирую ещё раз говорю это там операция которая длится минуты в этом случае можно поддать такое время обычно хоных например вот и данные всё-таки перенесут так как э задача Ну отложенная таска которая выполняется то она подгаевский на минуту другую спасибо спасибо Вот сверху фтв Алексей инженер проектировщик У меня вопрос по миграции шардов собственно говоря вот этот механизм который определяет Когда нужно перенести из одного шарда в другой Как часто он происходит то есть это периодически происходит или НонСтоп плюс этот механизм он локально на каждом рде расположе Итон в одном на главном сте Возможно ли миграция сразу нескольких шардов то есть типа там четыре штуки шарда и надо он механизм понимает что надо перенести и вот он переносится или только один может Угу Да спасибо А тут а Суть в том что э миграция данных с шарда - это А ну такая скажем шарт который нужно распределить по другим Ну например перенести половину данных на новые шарды в этом случае ставится задача она ставится вот один раз да создаётся что вот из этой половины мы четверть на Один шар четверть на другой и на остальные два тоже по четверти и в этом случае задачи каждая задача отвечает за перенос данных одного пользователя она будет запускаться до тех пор ну с каким-то интервалом небольшим Пока У пользователя не будет не будут выполнены все условия Вот про которые я говорил что пользователь неактивен что на шарт нагрузка не очень высокая какой-то момент там количество Трайв у этой таски будет расти и мы заметим что данные какого-то пользователя перенести не получается в этом случае там будут приниматься Уже какие-то решения на тему того действительно можно ли их оставить просто на предыдущем рде выбрать других пользователей которые менее активные и их перенести вот бывает так что мы хотим перенести данные какого-то пользователя чтобы у него был чтобы его запросы лучше выполнялись ему лучше жить там же где пользователи с аналогичным профилем нагрузки и в этом случае мы фактически пер пользователь можем создать задачку она будет также трается и со временем выполнится вот растащить шарт чтобы там вообще данных не осталось Это довольно редкая штука гораздо чаще нужно просто снять нагрузку и перенести часть пользователей спасибо Вот Будьте добры Доброе утро Илья Попов ВТБ Спасибо за доклад первый вопрос коллеги сняли с языка он заключался в том Почему не Кассандра поскольку те задачи и проблематику которую вы очертили в принципе Кассандра решает вроде бы как в первом приближении по поводу рекурсивных запросов не знаю это надо смотреть производительность мерить а вот второй вопрос тоже не очень приличный тоже религиозный да А насчёт не думали Спасибо это очень религиозный вопрос да Коле из уже интересовались в нужно понимать что речь идт про переезд был в 2017 году тогда развитие в общем-то ещё особо не было про развитие кассандру Ну знаю что делались бенчмарки не знаю сравнивались ли все база данных вообще которая возмож но в целом бенчмарком хорошие результаты показал именно постгрес И как я говорил на то время уже были готовые решения которые можно использовать А значит вот принцип Simple в нашем случае был именно такой старались максимально простыми способами Итак переезд с документной базы данных в базу данных реляционную был непростым но пово ильз готовую инфраструктуру и на этом уже не заморачиваться Спасибо большое То есть вы тест на секту прошли То есть если бы были настоящей сектой ты бы сразу всё выбросил поставил и впер Да пожалуйста значит смотрите сейчас происходит очень важное внутри ком Яндекса реализуется и на конференциях тоже то есть не знаю Вы знакомы кстати не из команды S3 mds э у нас похожая схема шардирование И мне интересны некоторые цифры детали про вашу схему про вашу схему шардирование сколько rps бывает у самого жирного пользователя пишущей нагрузки Какой rps может приложить шард и какого размера шарды Там сколько Ну не знаю терабайт Вот вот так угу и какого фига у вас не у idb да давай Да у нас ну про конкретные показатели лучше прямо подойти в кулуары А так скажу что дом поговорим да в офисе Да ну Суть в том что размеры шардов порядка там пары четвёрки терабайт вот в плане rps ведь очень сложно сказать что вот какие-то какое-то количество запросов Может взять и сложить шарт а а вот сложные запросы Бывает такое которые выполняются просто очень долго Да у человека Например широкие папки в одной папке там 500.000 файлов что-нибудь такое вот такое может долго выполняться и при высоком Рейтер с похожим профилем нагрузки чтобы а планировщик постгрес правильные ключи выбирал Вот и в этом случае переносятся в МДС конечно так не очень получится наверное у вас другая схема шардирование насколько мне известно вот у нас ещё три вопроса раз два и три и потом будем выбирать кому дарить подарки и остальные вопросы либо в дискуссионной зоне либо э на НД 360 можно прийти и там коллег помучить уже так сказать соборно пожалуйста Добрый день спасибо за доклад Максим а меня интересует Есть ли нюансы вот мы всё перенесли А старые данные надо удалить Вот есть ли нюансы с тем что Ну вот мы сейчас дадим нагрузку на шар в котором это данные или там бэкап надо или ещё какие-то Ну вот когда вот Всё сделано а почисти чистить надо вот сейчас потом вот очень хороший вопрос А чем ты занимаешься скажи коротко распознавание Лис прикольно Это вопрос реально очень хороший Потому что чистка это тоже процесс который нагружает шар довольно сильно поэтому у нас чистка отложенная то есть задача которая перенесла данные она ещё создаёт отложенную задачу на чистку которая выполнится через несколько часов и зача СТ она также анализирует нагрузку на шард и не делает чистку прямо сейчас если нагрузка на рде высокая и таким образом вот этот Круг замыкается Мы всегда учитываем текущую нагрузку и если сейчас вот мы взяли не знаю миллион пользователе перенесли чтобы они не чистились все одновременно поэтому задача на чистку распределены по времени и они смотрят Ага вот сейчас чистить не стоит отложу СКА я ещё там на десяток минут и вот так вот вычищаем сверху у кого микрофон махни вот Ага вот прямо под фонарём Смотри вот лица не видно видно только бейджик Да спасибо за доклад очень интересно чем ты занимаешься скажи компания stls технический директор О нормально а очень интересно но вот вчера послушал доклад про S3 у них под капотом МДС ты сегодня тоже рассказывал не про сами данные А про метаданные и вот интересно что под капотом у Яндекс 360 и есть ли какой-то не знаю roadmap и двигаетесь ли вы в сторону чтобы создать некую синергию между индекс 360 и ещё небольшой вопросик Извини пожалуйста слой каширования есть какой-то над метаданными Окей спасибо Ну тут насчёт первого вопроса про синергию она в общем-то уже более-менее достигнуто потому что бинарные данные хранятся как раз в и А в метаданных хранится ссылка на э хранилище ну на на файл в МДС вот в плане там шардирование как я говорил у МДС немножко другой подход э но а у них свои методы хранения а а второй вопрос по поводу каширования А по поводу каширования У нас есть определённый слой кэширования а он не не везде применяется а Но раньше для него использовалась монго Ну вот в в одной в одном из применений А сейчас там ydb вот используется для кэширования в документах для того чтобы просматривать омот документов Ну он требует содержания кэша вот этих кусочков снх и ссылки на вот эти Рендеры они хранятся в отдельном каширу слое А ты смеешься потому что всё-таки сек Спасибо большое Давай быстро время честно говоря закончилось но пинг-понг и разом короткий вопрос Вы упомянули что в разных Шарх может быть с Да как вы определяете что данные можно смиг вот с этой схемы в ту схему и дополнительный такой вопрос Как вы собственно меняете схему базы данных расселялись бы учить да Нам надо выбрать двоих значит твой приз отдать одному и второй приз от конференции отдать другому сувенир а махни рукой все кто задавали вопросы пожалуйста Угу Вот вспоминай Да а дадада А вот приз от Как тебе не стыдно напустить приз от 360 хотелось бы вручить вот за вопрос про каширу ущ слой а и был очень хороший вопрос про чистку данных вот второй приз от Газпром Да по-моему вот он уже ушёл Да вот Да отлично Спасибо большое Да тебе тоже сувенир от конференции за выступление Спасибо тебе большое"
}