{
  "video_id": "7WT_Rl6m2DU",
  "channel": "HighLoadChannel",
  "title": "Преимущества и недостатки микросервисной архитектуры в HeadHunter / Антон Иванов (HeadHunter)",
  "views": 85641,
  "duration": 3279,
  "published": "2017-06-28T07:10:17-07:00",
  "text": "Я работаю в хантере слидом перед тем как я закончу рассказывать Кто я есть там в хантере Я хочу немножко спойлеров про свой доклад чтобы вы понимали о чём я буду рассказывать И чего от него ожидать первый тезис - это что микросервисы упростят вам жизнь в одном но серьёзно усложнять вам жизнь в чём-то другом причём микросервисы затронут всю компанию а не только отдел разработки но и отдел тестирования и отдел эксплуатации и тут Важно смотреть на эффект для компании в целом а я в своём докладе попытаюсь рассказать о том с какими проблемами мы столкнулись решения будет не очень много Хотя они тоже будут мне интересно рассказать о том чтобы вы понимали с какими болячками С какими трудностями вам придётся сталкиваться для того чтобы переходить на микросервис архитектуру может быть Вы подумаете над тем Стоит ли вам это делать сейчас это сделать потом или не делать вовсе Ну вот обо мне Я 2 года в хантере работал простым разработчиком и в течение этого времени был в общем ярым сторонником микросервисной архитектуры мне хотелось распилить абсолютно всё и мне казалось что наступит светлое будущее потом Я перешёл работать в команду которая нас занимается отка устойчивостью сайта и тут мои представления о том насколько классный микросервис они стали более там взвешенными Я не говорю о том что это плохая архитектура Я говорю что у неё есть Вполне себе ограничения и вы можете извлечь преимущества и недостатки у меня кликер плохо работает с этим можно что-то сделать Окей чтобы вы представляли у нас был Монолит Вполне себе в ше году который был написан на Java на таких фреймворка как strats gsp TK который мы в общем там и половина из этих технологий мы уже поменяли Монолит до сих пор у нас в каком-то виде остаётся он тогда ходил в кш базу данных тогда мы использовали ещё mysql я говорю Тогда мы использовали это по словам меня тогда ещё в хантере не было но тем не менее Вот и потом мы стали от него отпилить куски выделили отдельный сервис который занимается идентификацией пользователя х ID имя фамилия логины пароли авторизации в соцсетях и так далее Всё хранилось там и у него была отдельная база мы выделили отдельно сервис поиска который собственно отвечал за то чтобы по поисковому запросу вернуть список Адик резюме или вакансий мы выделили сервис баннеров у которого тоже своя база была И сейчас он разрабатывается вообще отдельно Кома не просто отдельной командой а отдельным разработчиком который вне штата находится и это Success ST это было здорово сейчас мы пришли к следующей архитектуре у нас появилось Аж три фронтенд нах сервиса они написаны на питоне с использованием Торнадо это сервис AP который отвечает за то чтобы собственно представлять функциональность нашего сайта мобильным приложением или чтобы работодатели интегрировать с нашим сайтом это собственно сервис который отрисовывать Основной сайт хру и сервис который отрисовывать мобильный сайт за него у нас отвечает за мобильный сайт отдельная команда Хотя все они примерно одних и тех же технологиях написаны Python тодо плюс Наш фреймворк фронти у нас выделилось много микросервисов гораздо больше чем помещается на этот слайд среди них там сервис откликов приглашений отклик сервис машинного обучения рекомендаций и так далее там подобное я тут попытался какие-то стрелки нарисовать на самом это далеко не всеки дест между сервисами в какой-то момент мы поняли что у нас появляется очень много интеграционной логики Ну то есть простой пример надо отобразить страничку результатов поиска вакансии надо сходить в сервис поиска поискать адики а потом сходить в сервис вакансий и наложить на это всё данные о вакансиях название описание зарплаты и так далее И у нас появляется сервис интеграционной логики в который могут ходить фронтовые сервисы фронтовые сервисы могут ходить и напрямую вэн если им не нужна никая интеграционная логика Но вот много там крутится такая э трёхуровневая архитектура получилась я этот слайд потом покажу ещё раз в конце мы с вами на него посмотрим что в этом слайде есть хорошего А что в нём есть плохого Какие решения были хорошо приняты Какие решения были приняты не очень здорово а Давайте с вами вначале посмотрим что не то что посмотрим много всего написано есть много докладов о преимуществах микросервисной архитектуры вот я хочу рассказать о том что мы чувствуем от кроной архитектуры че нам в ней нравится Первое - это конечно декомпозиция Монолита понятно что там простые сервисы понима белье чем один большой Монстр Монолит в котором непонятно всё как взаимодействует А тут по крайней мере есть понятная AP и понятные связи между сервисами микросервисы проще тестировать то есть разработчику достаточно там установить какие-то стабы внешних походов и скоуп тестирования Он такой маленький в отличие от Монолита который там ещё куча фикстуры надо развернуть для того чтобы всё протестировать И не сфокусируйся просто на э на своём коде э независимые релизы это кстати очень интересная история э Особенно для нас актуальна была независимость релизов Когда у нас наше монолитное приложение релиз с раз в неделю оно то релиз каждый день но команда получала допуск к релизу в этот Монолит там раз в неделю эти времена уже давно в прошлом и мы релиз се каждый день до сих пор команда отвечает что когда у меня есть собственный сервис за который я отвечаю я могу посреди дня собрать релиз протестировать его и там выложить в с вашим монолитом мне до конца дня придётся ждать Вот Ну вот до сих пор на команда отмечает такое преимущество что ещё круто независимая деградация Ну понятно Если лежит сервиса и он не афект там ве в зависимое масштабирования У нас есть например сервис поиска которому требуется много-много оперативной памяти естественно мы закупаем под него специфичное оборудование под базу данных закупаем специфичное оборудование а под другие сервисы можем закупать другое оборудование Независимо масштабировать те или иные сервисы возможность пробовать новые технологии Ну да просто не возьмёшь и не перепишешь весь Монолит на новую технологию А тут удобно переписал сервис на какую-то технологию понравился не понравилось распространяем дальше этот успех или выпиливаем переписываем обратно и наконец есть такая менеджерские преимущество Я его называю что у сервиса есть команда владелец есть к кому обратиться Кого попросить чего-то исправить более того они сами даже заинтересованы в развити этого сервиса А в поддержке разборе инцидентов и так далее Это хорошая история чего не круто с чем пришлось сталкиваться Ну первое - это когда у вас Монолит у вас в принципе контекст запроса понятен вы смотрите в лог грете там по какому-то идентификатору получаете г Когда у вас много сервисов вы будете грепан запросом во всём этом придётся разбираться эта проблема решаема вы присваивается каждому пользовательскому запросу идентификатор какой-то вы поэтому идентификатору ищете индекси все логи например мы пробовали внедрять глок он там на elastic построен и тут надо понимать что у нас достаточно много логов относительно Конечно вот там больше терабайта в день и на тех машинах которые были выделены под горелок это не были какие-то там виртуалки Непонятно это были железные мощные машины на них было 100% CPU не успевал индексировать просто все логи и разработчики шутили что он скорее иногда находит как правило не находят Никаких никаких данных по по запросам мы могли потратить время для того чтобы там его настроить это это всё не Rocket Science Мы решили что мы перепишем сделаем собст сечас во сейчас заработал которая основа на том что мы ID запросу присваиваем не абы какой А засовы туда тайм начало выполнения запроса Дальше можно увидеть что всё логи упорядочено и в общем-то алгоритм поиска он бинарный просто тыкаем всё в любое место Лога смотрим таймстамп этой строчки больше или меньше и так далее очень быстро находим нужное окно которое нужно просканировать для того чтобы все логи собрать Ну и так по каждому Логу тыкаем и что интересно ну у нас всё равно есть серс сервер на котором на который сливаются все логи Вот И там просто стоит несколько Вот таких простых демонов которые это собственно и делают и вот эта система она не требует Никаких особенных ресурсов А все логи запрос показывает но опять же мы в неё вложились написали и над ней там трудились пара человек а слайд который я долго думал вставлять или не вставлять потому что здесь есть бенчмарк за бенчмарк всегда можно покритиковать да что он устроен непра Вот Но тем не менее я встречал разработчиков которые в общем-то считают что удалённый вызов это очень просто почти ничем не отличается от локального вызова Иван по этому поводу Очень классный доклад прочитал Очень советую его пересмотреть вот я от себя лишь добавлю что Ну какой путь проходит запрос он там пишется в буфер сокета стерилизованные да данные запросы пишутся в буфер сокета в буфер сетевой карты по цепочке сетевых устройств передаются на может быть выделены железные балансировщик может быть нет там раскрывается запрос рути на конкретный сервер и дальше и так далее тому подобное это мы только дослать запрос до удалённой стороны удалённая сторона ещё сделает полезную работу какую-то а дальше начнёт обратно сили зовать ответ передавать всё это обратно по сети вот интуитивно кажется что если у вас сервис выполняет очень мало работы ну там сложение двух чисел например Наверно не стоит это уда поход по сети Да два числа гораздо проще сложить локально Вот и поэтому вот я сейчас приведу график того насколько быстрее медленнее удалённый и локальный вызов метода а метод он на самом деле просто делает какую-то работу по горизонтальной оси как раз отложена работу которую он делает там 1 2 3 10 миллисекунд а график синяя линия - это вот то сколько мы запросов в секунду удря делать напрямую красная линия - это сколько запросов мы удаляемся делать типа делая удалённый вызов на самом деле здесь не совсем удалённый вызов я просто ТП прослойку на локальный компьютер поставил Вот Ну а оранжевая линия собственно показывает наш overhead Ну видно что этот overhead Конечно Чем дольше у вас запрос тем тем он меньше относительно собственно полезной работы которая делается Вот Но всё равно на там когда у вас 1Д 3 миллисекунды занимает этот Ох будет достигать по камере там 20% вот Вот и тут интересно что многие разработчики опять же в Хант очень часто думают что наши запросы занимают там по 50-100 миллисекунд и в общем сетевой оверхед он никакой вот на самом деле я там много раз смотрю вижу центили скорее мы ближе к началу этого графика оди 2Т 1 2 3 миллисекунды у нас многие запросы занимают вот поэтому у нас даже есть там много выделенных серверов которые только занимаются тем что ро балансирует запросы Да Настолько настолько много у нас сетевых вызовов и тут мне возражает ну Антон это всё лирика железо то дешёвое Ну да дешёвое а его обслуживание то есть над этим же работает там служба закупок железа нужно чтобы кто-то его закупил чтобы это железо поставили настроили продолжали обслуживать А это всё уже люди а люди стоят не так дёшево Да вот поэтому вот есть такой баланс Ну интуитивно кажется опять же что если у вас мало сетевых походов то как бы можно этот график не смотреть хочется рассказать это что rpc в принципе сложнее вызова метода что вот для того чтобы сделать удалённый вызов У нас есть целое семейство абстракций например есть там поисковый клиент к сервису поиска Он построен на базе нашего ххш хттп клиента который в свою очередь базируется на базе клиента там открытой открытого синка хттп клиент ранен который в свою очередь между прочем не все знают используют нети под собой и так далее и тому подобное и всё это достаточно много непростого и хорошо когда разработчик от этого всего абстрагировать Но вот у нас например команда S туда заглядывает периодически находит там много нового всякие пулы соединений пулы байт буферов Event лупы хотите ли вы об этом знать или не хотите да Вопрос к вам и второе О чём здесь тоже надо сказать это то что rpc удалённый вызов он сложнее В том смысле что вам прид думать над тем А чего делать когда сервис не ответил и как он не ответи на пятисот также как реагировать на Time Да вопрос иногда открытый у нас всякие баги в проти случались связанные с тем что разработчики написали код Так что считают что Север не ответил или ответил пустым ответом это одно и тоже нет это не одно и тоже там разная бизнес логика зависимости от этого должна была быть распределённой я так просто скажу что вот когда вы уделяете сервисы вы тут же попадаете на все проблемы распределённых систем отказы тайм-ауты ретра дубли и так далее Будьте к этому готовы Я приведу просто простой пример того как мы в сре уем по отстройки всей вот цепочки вызовов сервисов очень простой пример смотрите у нас есть балансировщик который роут запрос на сервис А ну на один сервер или на другой сервер под сервисом А есть сервис перед которым тоже есть балансировщик там есть сервис и так далее заго обходит таку Ну там пользователю сказать через 2 секунды Извини не смог и дальше не хотите напрягать вашу инфраструктуру этим запросом Ну и кажется что вообще-то таймаут от первого балансировщика до сервиса а должен быть меньше чем 2 секунды правильно Ну потому что мы обнаружим что сервис недоступен нам ещё надо время травиться для того чтобы другой сервер его обслужил ну и так недолго думая выставляем ону секунду тут Конечно можно порассуждать на тем что это может быто вране секу до секу не от понимаем что он а потом быстро раем и собствен ответ получаем но предположим о секунду дальше это о секунду мы разбиваем на полсекунды перед сервисом и на там 250 миллисекунд перед сервисом дальше начинаются интересные вопросы Все ли запросы сервиса укладываются в 250 миллисекунд причём это не должна быть там какая-то 99 за вше сивину только потому что у вас чего-то там прило в или в базе отдельные тайма для тяжёлых запросов вы хотите прописывать отдельно в конфиг или вот что вы с ними будете делать как не завалить себя тра во время проблем с базой сервиса у вас база сервиса тупит и у вас там первый сервер не ответил пошла лавина и второй сервис тоже не ответил пому в так чтобы вы понимали что прозой и уже не нужно хотите в это вкладываться что делать если серс а хочет ходить в серс напрямую Как там Тау отстроить Что делать если мы хотим не один й а там Не там три и больше такое тоже бывает необходимо Ну вот просто вот весь этот скоп вопросов вам прид продумывать и даже если вы захотите использовать какое-то решение вы должны будете понимать что оно работает именно так как вам нужно есть некоторая церемония связанная с новым сервисом Да я как считал что я сейчас вот выберу фреймворк на котором я буду писать допишу его под себя всё и он будет в проде Ага упаковываем низи докери настраиваем процедуру выкладки настраиваем ротацию заливку логов настраиваем мониторинг триггеры подключаем систему мониторинга низкочастотных ошибок потому что на мониторинге вы увидите высокочастотное ошибки А у вас ещё случается на эп и выход за пределы массива Вы тоже хотите смотреть на баги в вашем коде и так далее и нет каждая из этих проблем она очень там никакого ке не требует для решения и там например у нас есть система которая автоматизирует процесс сборки любого приложения это просто вопрос добавления сервис Это вопрос добавления конфига в эту систему Вот и так далее но тем не менее решить для себя хотите ли вы в это всё ввязываться или нет поддержка в актуальном состоянии я опять же на примерах вот мы за мониторить походы вш нам мониторинга того который сам ш предоставляет не хватало мы хотели посмотреть А что же с эти происходит на стороне клиента потому что на самом деле очень важная часть любой системы распределённой Это именно клиенты вот Ну что мы выделяем библиотеку хш Клинт идм по 10000 сервисам обновляем их так чтобы в общем там за использовать эту библиотеку давать запрос енп же зажи очере раем пороги срабатывания и все запросы которые приходят на сервис если он понимает что он перегружен он просто отдаёт пятку или вообще рвёт соединение таким образом не заваливать себя ещё больше Окей выделяем библиотеку ХП проходим по 10000 сервисам обновляем здесь нет никакого Кет саса опять же ничего в этом сложного это даже можно автоматизировать потом просто ещё придётся пройти по 10000 сервисом и подстроить которые были неправильно настроены в самом начале В общем мы конечно так никогда не делаем мы выбираем три-пять главных сервиса от которых реально у нас болит а всё остальное отдаём на откуп монтем сервисов ребята Когда вам приспичит обновляйте они естественно Им не приспичит до последнего момента и в последний момент они начинают обновляться долго и мучительно перепрыгивая Через несколько версий и спрашивая нах А что же там понаделали вот в той там предыдущей версии И это тоже проблема решается там административна документация или там фик не знаю обновлений вот есть есть такая боль транзакции Ну иногда хочется но не часто вот так что в общем-то это небольшая боль и тут когда вот я вот набросал да той боль с которой мы столкнулись очень интересно посмотреть на те преимущества которые мы извлекли ли мы их точно или не всегда То есть зависит например вот у нас был Монолит и мы хотели его декомпозировать и вот он устроен как-то непонятно есть пакет в котором Да лежит vacancy даре да User Да который взаимодействует с базой Пакет сервис в котором лежит тоже самое только там с префиксом vacancy res User пакет ресурс который абстрагируйся которые будут соответствовать не технологическим слоям А бизнес фича конечно в предыдущем докладе Сергей правильно говорил что вообще вот эти барьеры они тяжело э то есть инкапсулировать паблик у того метода у того класса который вы хотели инкапсулировать И всё И ваша ваша встроеная система инкапсуляция разрушена Вот но иногда бывают другие контраргументы против этого аргумента говорят Ну этот же разработчик начнут писать потом и Нет ничего хуже чем разработчик с хорошей трудоспособностью он вам перепишет Вполне себе быстро вот модули Вы можете выделить модули Да коллекцию классов с явными зависимостями это тоже по крайней мере на какое-то время вам структурирует ваш Монолит Вот то есть есть способ декомпозировать Монолит Будет ли он работать не будет зависит отм решать Ну да потом бы покажите хо проще сервис выделять Да а не из каждой там пакета дёргать отдельный класс проще тестировать Ну да застал протестировал свой микросервис выложил впро и вперёд но иногда хочется протестировать ВС систему в целом есть конечно подходы когда мы не Тестируем а выкладываем сразу в продакшн Это нормальный подход между прочим Вот Но есть вс-таки есть адеп бое консервативные которые считают сирова лом Нате на сложные стенды в которых нужно развернуть все сервис откорректировать настройки у нас в пр Бер кусок продакшена копируется его с помощью тех же леков с помощью тамб на тестовый стенд просто корректируются какие-то настройки притираются для того чтобы структура тестового стенда она была абсолютно такая же как в проде но за исключением того что в проде там п инстан сервиса А здесь один Ну а это всё на самом деле это достаточно большая работа чтобы чтобы создать такой стенд И поддерживать независимые релизы выше я упоминал Что отлично У нас каждый сервис можно релиз отдельно из-за этого каждая команда которая его поддерживает работает быстрее чем если бы мы выпускали Монолит но Представьте себе что вы хотите изменить IP сервиса C что вам для этого нужно сделать Ну заридить сервис C с совместимым I там будет и старые и новые какое-то время он будет работать и со старым и с новым и сервис B тоже потом зали сервис А переключи его на использование нового IP Ну и потом подальше дальше Идём по сервисам B и сервисам C и выпиливаем старой IP вроде как были независимые релизы Но вот в этом случае они зависимые тут вопрос Насколько часто колбасит ваш AP Насколько часто Вам приходится ради реализации бизнес фичи переписывать IP Хорошо ли она написано или оно может быть написано хорошо просто там бизнес фичи У вас такие что меняют вашу систему достаточно часто и там сервис AB тестирования э много AB тестирований проводится и так далее это всё открытые вопросы тут нету прямо каких-то рецептов независимость градации Да очень круто когда лежит какой-то сервис и пользователи этого не замечают но Представьте себе что у вас лежит сервис поиска Ну то так сайт по поиску работы и поиску сотрудников у него лежит сервис поиска что это за сайт Ну у нас пол сайта читаете лежит вот сервис откликов и приглашений Да работодатели не могут пригласить соискателя на работу а соискатель не может откликнуться считаем ли мы что такой сайт работает нет А если у вас лежит сервис сессии который собственно отвечает за то чтобы узнать пользователя Да его там информацию имя фамилия отчество email и так далее передать во все бэнды то мы считаем что вообще сайт не работает и никто с ним работать не может То есть это вопрос К тому насколько насколько важна ваша функциональность которую вы хотите с деградировать если она настолько важна что ваш сайт перестанет работать то как бы польза того что у вас независимая деградация вы особо не извлечённая там что-то происходит затрагивает сразу весь Монолит и всё падает на практике я видел что скорее большая часть проблем вызывают сетевые походы они падают с большей вероятностью чем что-то происходит внутри Монолита более того вот у нас в хантере до сих пор Монолит сохраняется и Честно говоря я там на своей практике очень редко видел что реально там какая-то проблема в одном месте затронула ВС вот на бывали данта изза того что клиент Плохо работал из-за этого Монолит падал но это не были проблемы там скажем в какой-то бизнес функциональности это были чисто технологические проблемы ИТ ещё вопрос а база данных У вашего сервиса микросервиса отдельная об этом говорили т я хочу подчеркну быть отдельная Лека потому шники чтобы сэкономить на обслуживании там и вообще на инфраструктуре Вполне себе могут установить вашу базу данных на одну и ту же железку и вот у нас в хантере периодически происходят маленькие тайми противные когда один сервис завалил жёсткие диски базы И в общем это привело к падение других сервисов про масштабирование да у вас есть возможность независимо от масштабировать ваши микросервисы или макросе Неважно Что происходило в случае с монолитом вы э брали просто ставили ещё один сервер и ставили своё жирное приложение туда правда очень простое масштабирование У меня правда интересно когда прогонял этот доклад мне э возразила ну подожди твоё монолитное приложение может разрастись до таких размеров что оно просто не влезет на Севера и курьёз оказалось то что это рассказывала меня команда qa которая разработала тестовые стенды на которых стоит весь HeadHunter только мини копия всего Хантера со всеми микросервисами что даже туда же всё влезает в какой момент Монолит станет настолько монолитом что он не влезет на современное железо Я не представляю А что вы делаете с микросервисами Ну да вы берёте микросервис ставите его на железку желательно в трёх вариантах для того чтобы обеспечить отказоустойчивость ну или в двух или в четырёх это там от вашей компании зависит Но ваши сервера не до загружены Да поэтому вы их добавляете туда ещё приложений напивается и ввязываться в историю с виртуалка докеров Там оркестрация и так далее у нас раньше были виртуалки у нас от них болело большой оверхед мы переводим сейчас продакшн на докер у нас многие сервисы в продакшене в докере крутятся и мне кажется что недалёко недалеко то светлое будущее Когда у нас будет оркестрация кубернетес или ещё что-нибудь в этом роде вот на самом деле пользу от независимого масштабирования вы извлекается тогда когда вам действительно нужно независимо от масштабироваться а всё остальное наива вот сервисов в железке это всё там скорее не под независимое масштабирование а про до загрузку оборудования возможность пробовать новые технологии очень классна Когда у вас за каждым сервисом закреплена команда некоторые бизнес фичи в хантере требуют того чтобы изменить в разных сервисах что-то и за это у нас обычно отвечает То есть у нас выделяется команда под бизнес фичу а не под сервис иногда из-за этого болит команда приходит прави сервис чужой видит что там вме или Джус вместо сприн им приходится с этим разбираться в этом нет ничего страшного на самом деле Потому что если понимать подходы то в общем конкретный фреймворк не так важен там начинается зоопарк подходов одна команда написала в одном сервисе так Unit тест другая по-другому и Если у вас нету чёткого разделения команд по бизнес сущностям и соответствующее выделение микросервиса под бизнес сущностью то вы попали потому что вам Бут уходить команда по сервисам и пропилить сквозь эти сервисы свою бизнес фичу гибко но не единообразно команда владелец э в некоторые сервисы пишут все обычно это указывает на то что сервисы плохо выделены мы это понимаем вот есть другая штука что sre например у нас отдельно есть команда которая занимается да Отказ устойчивостью реагирует на проблемы быстрее чем бизнес команда владелец этого сервиса потому что у них свои проблемы у них там деноминации в Белорусии надо срочно переписать весь биллинг вот а там то что Антон у вас там сайт падает это уж там как-нибудь сами разберитесь Вот и в сре просто чисто больше технологических знаний и нам физически просто работать с монолитом потому что мы там что-то что-то там исправляем это сразу всё работает вот а пойти по всем командам ещё донести до них те знания которые мы получили это не простая Задачка мы регулярно в хантере проводим митапы демки Там и так далее тому подобное вообще-то все проблемы микросервисов не требуют Rocket Science для решения и зависит от вас хотите ли вы в это ввязываться или нет как раз На мой взгляд находится в такой в расколбас У нас очень много адептов микросервисной архитектуры которые говорят да Антон давай уже допили всё вот а есть люди которые говорят да ладно Поживём ещё у нас там скорость роста просто технологически не такая большая чтобы там вкладываться в микросервис чтобы мы стали из этого пользу извлекать Вот и тут интересно посмотреть на то что я вам рассказывал и большую часть вы увидели что я рассказывал не про преимущества А про там скажем недостатки на самом деле эти недостатки очень легко нивелировать если пилить на микросервисы правильно Как пилить правильно вот у вас есть Монолит в который приходит запрос он их обслуживает и отдаёт обратно ответы и вы можете его поделить на слои у вас появляется слой фронтенда как у нас есть в хедхантере который там занимается там походом по эндам наложение на ВС это дело Там шаблони и так далее и у вас есть слой энда который собственно отвечает взаимодействие с базой данных слоёв может быть много Да у нас есть ещё интеграционный слой в этом в этом пироге вот что здесь плохо каждый слой он с точки зрения бизнеса не самодостаточен Да в том смысле что вы не можете выпустить бизнес фичу в одном сервисе Вам приходится проливать вашу бизнес фичу сразу Через много слоёв у вас достаточно много сетевых походов на каждый запрос пользовательский от слоя оди наверняка будет один или несколько походов в слой д иногда больше у вас появляется максимум боли От всего этой архитектуры потому что у вас походов много сервис ни за кем не закреплён независимые разработки независимых релизов вы не получаете пробовать новые технологии не можете А зато у вас походов много и система распределённая Вот и в общем минимум профита зато если вы распилить ваш Монолит таким образом чтобы там были достаточно хорошо выделены вертикальные составляющие бизнес сущностей Да чтобы у вас какую-то функциональность на сайте можно было показать просто сходив в один сервис в пределе да тогда вы получаете в общем у вас сервис самодостаточен его разрабатывает команда у вас мало сетевых походов Желательно чтобы их было мало конечно совсем их не станет вы извлекает максимум профита и минимум боле это конечно предельные случаи гибрид и вот например у нас есть Success история когда выделен сервис IP у него есть отдельная команда она его поддерживает да это не самодостаточный слой что I команде IP необходимо ходить в бэнды там иногда там что-то править но в принципе очень хорошая история что он выделен как и с сервисом мобильной версии сайта интеграционный у слой - это на мой взгляд наша боль потому что через него пропили абсолютно вся функциональность это фактически ещё один Монолит или может быть просто отков от изначального Монолита ещ один сервис который там микросервисов коем случае не является просто там инкапсулировать интеграционную логику Должна ли она быть отдельно или её можно было оставить в монолите это ещё там большой вопрос у нас я рассказывал есть сервис банер сервис поиска которые Тоже неплохо выделены Да они там баннер вообще самодостаточная а сервис поиска не самодостаточна Дат адики кому нуж ашки хочется посмотреть на вакансию в лом у нас целая команда которая занимается там в том числе машинным обучением Кет сам которые этот сервис пишет А мы иногда из сре приходим и говорим ребят Вот здесь надо поправить потому что у вас там технологически неправильно Всё сделано Ну вот кажется я рассказал почти всё что хотел если какой-то делать вывод из моей презентации Наверное он будет таким что сервисная архитектура и командная архитектура должна соответствовать прежде всего бизнес фича а не представления красивой архитектуре у меня это всё спасибо Сейчас у нас будут вопросы у меня сразу маленькое техническое замечание У нас есть у всех точнее у вас У нас у всех есть приложение мобильное и всем докладчика будет очень приятно если вы в мобильном приложении будете там ставить пять звёздочек То есть им конечно и за четыре звёздочки будет хорошо и за три можно котики да и все тике а в а в Боте не в мобильном приложении в Боте тоже можно оценивать докладчиков Я забыл вам сказать Ну те были Кто был на предыдущих докладах пожалуйста оценивайте докладчиков Спасибо за доклад за такое практическое изложение проблем и достоинств А у меня такой вопрос Вот вот как раз последние слайды были Значит мы нарезаем приложение горизонтально и вертикально у меня кликер перестал работать а я не могу его показать Ну хорошо там не обязательно вот в общем у нас есть монолитный сервис у него есть монолитная админка вот которая имеет авторизацию как ну как и в обычно в монолитных приложениях Ну один раз логин пароль и весь функционал причём вся админка как построена на базе какого-то фреймворка то есть построена средствами фреймворка то есть н формирует HT CSS выбрасывает то есть кнопочки ВС сделано Да вот и Когда мы будем распиливать всё это дело на микросервисы как быть с админкой То есть если для бизнес фич Мы в апе можем выбросить только пять 10 бизнес методов которые не обязательно крут а только еты там и ничего больше да то админка как правило требует много большего функционала е крут пополнен надо там параметры атрибуты и вот как быть оставлять это написано на фреймворка чудо всё да Или же через AP этим управлять там ннд писать фронтенде а энд Ну Эндер и как-то уже там это взаимодействовать то есть и как это устроено у вас и там соответственно Если какие м Если разная часть админки проблемы с авторизацией вот такой длинный вопрос Да у нас на самом деле устроено и так и так дело в том что у нас есть сервисы которые находятся под одной админкой есть команды которые пишут совсем свои сервисы и они решили свою собственную админку замутить Вот и им это нравится У нас есть сервис который занимается узнавания пользователя прямо когда фронтовый запрос приходит в нашу систему пользователь узнаётся и дальше все сведения об это пользователя о его правах и так далее пробрасывается прямо в заголовки хттп во все бэнды в этом смысле у нас й бэнды Вот и поэтому эту логику Вполне себе можно в БК энда посмотреть Ага Что мне за сессия пришла у него такие-то права он может сделать это не может сделать это и всё и те кто пишут сервис они в принципе не думают об обмин думают только о правах которые предоставлены или не предоставлены Есть ли у вас такая проблема что одному человеку нужно сходить и туда и туда и туда и как это ССО то есть чтобы не раз ну не надо было ему авторизироваться много раз И как вы у нас есть единый сервис авторизации Да вот я технологический стек можете сейчас быстро сказать то есть на основании чего он сделан технологический стек Наших кэндо нет Э вот этого ССО то есть на чём это всё вы сделали руками на коленке самописная хороший код Понятно спасибо Здравствуйте Антон У нас вот интимный вопрос такой тоже про авторизацию Вот вы подняли вопрос независимой деградации вот Меня он тоже очень сильно интересовал Мы хотим микро авторизации сделать Как обойти вот проблему с тем что у нас авторизация ложится И больше никто ничего сделать не может потому что ну как бы проблема понятна проблема в принципе достаточно очевидно но как бы каких-то готовых классных решений Я до сих пор не слышал про неё я не буду говорить как сделать вам я расскажу как сделано у на дру пом Когда у нас лежит сервис авторизации то мы отдаём анонимную сессию просто анонимного пользователя и пользователь который приходит наш сайт имеет возможность работать с сайтом но сайт его не узнает то есть это вот такая контролируемая деградация она в этом заклю естественно всё это есть Триггер естественно мы считаем таймов все права и по крайней мере для старых сессий сохранить Вот все эти заголовки которые прокиды дальше на бэнды Ну вот и надеяться то что этого хватит хотя бы какому-то проценту пользователей Ну такой проблемы не стояло честно говоря там сервис узнавания у нас падает достаточно редко чтобы мы не думали о тем чтобы дальше как-то прокиды права пользователя на фронтенд и чтобы он с фронтенда нам свои права присылал Да ещё нужно думать о секюр чтобы он там их не подделал ну и так далее просто такой задаче не стояло мы с этими проблемами ещ не столкнулись хорошо спасибо Так мы задаём вопросы у нас есть рука ру поднимаем руки раньше тогда мы сможем задавать вопросы более оперативно меня слышно я хотел бы узнать по поводу для каждого сервиса своя база данных вот насколько это распространяется то есть по предыдущим докладам я понял то что база данных именно совершенно различные То есть например кто-то собирает для аналитики чисто базу данных Да и работает именно по аналитике Кто там ну для своей бизнес Вот и у меня такой вопрос то есть именно насколько она распространяется насколько у вас именно база данных разная то есть есть же ну обычно же всегда есть какая-то большая база данных Ну да которой там Ино всё и например сервис может работать с какой-то ну с какой-то это частью Как у вас этот вопрос Рен То есть вы реально делаете именно слепо ба Дан и обновляет его там или какая-то идёт репликация то есть насколько вот этот вопрос отдельной баз данных для каждого сервиса насколько как как он вообще выглядит Как он выглядит в эксплуатации или в тестировании в эксплуатации То есть как какие-нибудь примеры очень просто у нас есть основная база данных в которой лежат основные бизнес сущности вакансии резюме отчасти пользователя и так далее тому подобное высоко нагруженная у неё есть мастер реплика у неё есть несколько реплик с которых можно только читать данные до который данные долетают и бэнды устроены таким образом что стараются брать данные с реплик которые легко отмашка именно в Мастер для изменения запросов А вот так живёт основная база У нас есть много всяких отдельных баз которые для которых просто по степени критичности выделяются разные железки есть там средне критичные ба для них отдельная железка есть там совсем не критичные база они там на другой железке крутятся они тоже имеют возможность масштабироваться в том смысле что мы можем добавлять реплики бэнды про эти реплики знают опять же автоматически переключаются между нужной репликой если одна из них упала У нас есть Касандра которая ну которая мастер не имеет единого мастера имеет каждая нода является мастером фактически вот у нас есть даже два кластера Кассандры один отвечает опять же за критический функционал без которого мы жить не можем второй за какую-то данную помойку где мы храним много данных и так далее Но обычно всё-таки сервисы каждый сервис свой собственный кластер Касандра не разворачивают вот используют обычно общий кластер какой-то Ясно спасибо у нас ещё вопросы есть Спасибо Спасибо за доклад А у меня вопрос как раз по поводу баз данных и реагирования на скажем так инциденты то есть как у вас происходит система Ну вот что-то пошло не так где-то что-то карап нуло как у вас идёт система рол восстановления вот после какой-то деплоя неудачной фичи если таковое бывает на моей памяти Вот что я работаю в хантере капта данных у нас ещё не было мы бэкапы регулярно делаем И даже служба эксплуатации регулярно проводит учение по восстановлению с бэкапов но честно говоря инцидентов связанные с карап Насть данных У нас не было поэтому вот вот так спасибо и можно ещё один вопрос а распределённых то есть Ну наверняка же у вас А сам контент который Ну как файлы какие-то ещё как-то то есть это же распределённое хранилище то есть на чём у вас это работает Ф Что именно Ну контент распределённый Ну не знаю там картинки видео что у вас там ещё есть то есть вы вы же где-то храните распределённое это Да у нас там есть какая-то система из вообще у нас эксплуатацию в своё время написала кассандру на инса фактически Вот они сделали что несколько серверов на каждом устанавливаем который отдаёт статику и определённым образом определёнными скриптами между этими инстанса данные реплицируемый до сих пор Единственное что есть кэширование перед ней на уровне прямо Фронтовых серверов Ну вот как-то так это работает довольно прикольно ну да ну как самописная там на самом деле не так много кода то есть инжин отдаёт данные какой-то скрипт их реплицируемый в общем никакого резона не нужно то есть да то есть какая разница когда она доедет Вот вот и всё Здравствуйте Большое спасибо за доклад вот вопрос он такой объёмный нуно постараюсь разбить чтобы было понятно И мне И может быть кому-то кто тоже задался таким вопросом Всегда ли под микросервисами стоит понимать какую-то одну большую железку на которой разложены либо докер контейнеры либо виртуальные машины то есть э или же нужно держать для каждого сервиса какую-то одну единицу там ну сервер железный там или тут вопрос чего вы хотите да то есть если вы хотите железобетон на отказ устойчивость и так далее наверное у вас будет не микросервис а просто сервис и каждый будет стоять на своей железке с другой стороны если вы понимаете что это экономически неэффективно Зачем Под каждый сервис держать железку которая ещё и недо загружено и требует обслуживания то Вы наверное пожертвует отказоустойчивость там и так далее в пользу чего-то ещё вообще ну там не знаю вся архитектура - это про систему балансов Да вот можно выиграть в одном проиграть в другом микросервис про это же спасибо очень даже понятно и собственно вопрос который вытекает из вот этого у вас на слайде было видно что у вас как раз-таки вот на одном сервере виртуалки и контейнеры Что вы делаете в случае отказа железного сервера гипервизора хостмарк на то чтобы запас по мощности был мы регулярно нагружает прям нагрузочный тестом прям сайт по-живому чтобы убедиться что он при возросших на нагрузках работает вот и пока говорю живём без мигрировал какая-то докер железка Ну и чёрт бы с ней Большое спасибо Так е вопрос хотел бы задать вопрос по вашей мини копии так называемого продакшена то есть вы туда выкатывается сервисы которые сконфигурированы прямо для продакшена или вы делаете отдельные сборку для вот этой тестовой схемы и выкатывается туда мы стараемся использовать все те же скрипты Ну это обычно скрипты которые есть в эксплуатации для того чтобы развернуть сервис мы его разворачиваем в доке контейнере п но конфиги подкладываем собственные даже не столько собственные конфиги Мы в некоторых участках конфигов пропили холдеры что вот здесь использую значение для прода А вот здесь использую значение для тестовой инфраструктуры вот как-то так получается но это те же сервисы абсолютно из той же Деп даже ну то есть доке Image он один и тот же и там и там то есть ну с заменой там там я наверно точно не расскажу об этом гораздо лучше расскажет наш qa вот мы Дело в том что мы сейчас в процессе перехода на доке да то есть у нас какие-то сервисы уже в продакшене крутится в докере вот какие-то сервисы до сих пор де пакетами раскладываются вот не хочу Не не знаю не хочу быть не компетентным не отвечу Лано сли этот образ Здравствуйте спасибо клад вот у меня такой вопрос Вот я Здравствуйте а у вас была диаграмма там были указаны сервисы там АЦ и вот с таймингами которые пробрасывается Как вы вот вот эти тайминги пробрасывается если у вас появились новые сервисы и вот что вы делаете Ну в общем Какая стратегия Ну мы уже не можем продолжать там делить тайминги и вот игнорировать предыдущий серс конечно это я привёл предельный случай Мы конечно так не упаривание более того у нас в некоторых сервисах мы намеренно отключили ретрай мы сказали от них больше боли чем от А чем польза потому что они там во время инцидентов нас заваливают в других эти ретрай настроили сервисы которые не очень критичные для прода мы особенно там выставили какие-то дефолтные тайм-ауты и соображения того что Ну не должен запрос дольше 2 секунд Да там в этом сервисе жить а сервисы которые гораздо более критичны к времени ответу мы там действительно их проанализировали и выставили другие тайм-ауты То есть всё зависит от того какую выгоду Мы хотим извлечь настраиваю все эти тайм-ауты вот и всё А ещё один вопрос по поводу сервис интегратор Вот вы говорили Он там большая боль с ним Вот хотелось бы поподробнее Что именно вот вот у нас есть сервис у нас там есть какие-то скрип чтото Тае которые знат других Ну и вы накатывает сервис Ну вы имеете в виду серс Discovery какой-то Нет ну сервис Discovery Понятно Я имею в виду вот у вас есть сервис Он интеграционный что вы не можете его просто переплава или в чём в чём его проблема вот этого сервиса интеграционного Ну проблема интеграционного сервиса вытекают из того что на самом деле он не делает никакую полезную сущность именно для бизнеса да то есть он с одной стороны принимает запросы которые Впитывает от фронтенда э Особенно с ними ничего не делает Кроме того что прокси ют на кнд и в зависимости от отх ответов кэндо идёт на другие бкд и так далее да Вот то есть у нас фактически получился некоторое такое монолитное приложение которое добавляет сетевого оверхед добавляет оверхед на стерилизацию стерилизацию но при этом а а то есть там бизнес логики никакой нету что ли там бизнес Логика есть она просто минимальна вопрос Нужно ли для этой бизнес-идея именно отдельный deploy Unit или может быть этот небольшой кусок бизнес-холл говорят да Антон но всё-таки он как-то декомпозировать Монолит Да есть сторонники которые говорят что нет Давайте обратный интеграционный слой куда-нибудь сервис который отвечает за кор функциональность Да у на Я как-то построил просто диаграмму сетевого взаимодействия оказалось что 30% запросов у нас всех все запросы то 30% это будет между интеграционным слоем и слоем скоро функциональностью вопрос Вы хотите держать интеграционный слой отдель или может его засунуть в слой Скоро функциональности ко они так сильно связаны Вот Ну а если у нас есть вот B и C а потом нам нужно их сторнировать Ну если что-то пошло не так Нам же всё равно нужен этот интеграционный слой не А почему эту логику нельзя написать в любом другом месте да то есть Почему для этого нужно выделять отдельный пло Юнит отдельный репозитории и так далее и тому подобное эту же логику можно написать в любом сервисе Угу Хорошо спасибо Здравствуйте Меня зовут Дмитрий а у меня такой вопрос по поводу а была Была ли у вас практика прямо в продакшене например именно в реплицировать её структуру Я просто опишу проблему которая у нас в своё время возникла есть например интернет-магазин и есть некоторые карта лояльности которая привязываются к клиентам и в качестве там приморки мы используем как раз номер карты Но вдруг через полгода через год возникает потребность ввести нового вендора и получается что мы не можем использовать первичный ключ и нам продакшене таким хирургическим путём приходилось пере привязывать первичные вторичные ключи вот была ли у вас такая практика проблема особенно когда мы говорим о реплицировать схему базу данных мы регулярно меняем в том числе заменяем одни ключи на другие да бывает необходимо иногда это приводит к данта в том смысле что эти данные приют реплику ше это раскат тако бывало вот ну как-то с этим живём Ну я просто немножко как бы уточню когда просто добавить одно поле это легко когда надо отвязать перепривязать с десяток таблиц вот эти вот первичные вные ключи Но это порой легче застрелиться нам приходилось со вре скрипты прописать на которые автоматически это делают перед этим 10 раз локально протестировать развернуть кластер там протестировать на форке продакшене и уже после вот этого убедившись что ничего там не валится вот мы стараемся делать всё инкремента в том смысле что выпустить какой-то скрипт на прод посмотреть работает выпустить другой на прод посмотреть работает И постепенно шишками приходим к той схеме которая нам нужна бывает для этого требуется несколько релизов Ну то есть у вас в принципе тоже возникают такие проблемы Спасибо деле желательно Добрый день а у меня простой вопрос Чем отличается сервисная архитектура от микросервисной тут как-то вообще в разнице хоть какая-то сейчас есть или она потерялась окончательно Это отличный вопрос я например считаю что микросервисная архитектура это там предельный случай сервисной архитектуры как Монолит тоже предельный случай сервисной архитектуры в котором всего один сервис Да вот я не знаю мне честно говоря никогда не приходило в голову выделять какие-то чёткие критерии просто их в бизнесе особенно неиспользуемым которая такая чёткий микросервис из неё сервис уже не сделать и там нормальным каким-нибудь с дшм сервером который какой-то сервис реализует вот а при этом как я понимаю почти всё что у нас происходит вот в нашем энтерпрайз там в общем это нормальный сервисная архитектура никто не пишет сервисов 10 строчек отдельных хороши полноценные большие ТЛ вот окей Всё Давайте ещё раз поблагодарим докладчика"
}