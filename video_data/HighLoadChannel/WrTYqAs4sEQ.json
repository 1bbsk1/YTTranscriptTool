{
  "video_id": "WrTYqAs4sEQ",
  "channel": "HighLoadChannel",
  "title": "Как строить архитектуру для отказоустойчивой службы такси / Андрей Минкин (Namba Taxi)",
  "views": 945,
  "duration": 2445,
  "published": "2017-04-08T13:50:53-07:00",
  "text": "коллеги у нас стартует секса архитектур встречайте 1 докладчика андрей минкин здравствуйте меня зовут менкен андрей я приехал сюда из киргизии из города бишкек из страны с красивыми горами очень вкусной едой и я представляю компанию нам ботоксе которая является на текущий момент лидером пассажирских перевозок в нашем регионе и буду рассказывать о том как мы добились отказоустойчивые архитектуры и можем потерять любой физических серверов для начала давайте разберем какие вообще есть виды такси черт первый вид это частник или бомбила также бордюрчик у него абсолютно свои правила игры на рынке свои тарифы и он работает без посредников следующая это диспетчерская служба она может быть либо более либо менее автоматизировано использовать либо не использовать в печке или рации и следующее поколение то полностью автоматизированная служба полностью автоматизированный посредник виде убера кто мы мы служба который нужно как можно больше автоматизации при этом мы стараемся догнать uber и за все время работы на нашем рычание у нас сейчас около 300 тысяч довольных клиентов для вас это может быть показаться маленькой цифры но у нас это треть бишкека у нас шестьсот водителей на линии и мы обрабатываем не менее восьми тысяч заказов в сутки при этом суточная нагрузка выглядит так исходя из этого графика мы видим тоже в часы пик у нас их два утром и вечером утром с 6 утра до 10 утра и вечером с 4 до 8 наши сервера обрабатывают до 300 до трех с половиной тысяч запросов в секунду при этом мы успеваем отвечает для водителей 20 миллисекунд и для операторов за две с половиной миллисекунды доклад этот поделен на две части в первой я расскажу как строить саму архитектуру его 2 расскажу про наш опыт работы с выбор тисе и как мы добились хождение звонков пайпе телефонии через эту технологию как все начиналось все началось у нас 2011 году когда пришел к нам силу нашей компании команд нашей команде инженеров и сказал то что чуваки я хочу открыть такси в бишкеке мы сказали ладно окей порекомендовали где брать сервера куда их ставить и порекомендовали ему на тот момент единственная игра игрока на рынке автоматизации службы такси и его решение представляла такую возможность клиент мог либо позвонить либо отправить sms оператор принимает заказ скидывает его водительводитель принимая заказ приезжает на место забирает клиента отвозят вас назначения и по всему этому безобразию менеджеры это чет были следующие фичу насосов те поставщика и ты работай работающий модуль call-центра операторы могли как звонить так и принимать звонки были самос оповещения было более-менее был более-менее автоматизирован workflow и было очень много китайских навигаторов на котором работали водители оповещение псм с были на 2 у нас этапах когда водитель принял заказы когда приехал на место но в жизни так бывает то что партнер и друг друга не устраивают и мы решили отказался по текущим поставщика услуг по ряду причин во-первых когда у него все попадало был очень высокий downtime и служба могла не работать до трех-четырех часов а это очень много это очень дорого по ресурсам ну и плюс к этому мы хотели расти развиваться охватывать новые территории новые рынки новые сегменты и он нам это не предоставлял решили взять и написать свой софт стали определяться с требованиями к системе основное требование так как служба уже проработала год на тот момент на софте поставщика это как можно меньше изменений для людей в процессе работы над заказом также у нас должен быть должна была быть очень гибкая система для разработки водители должны сосна текущих работающих навигаторах на windows и и и также мы должны заложить поддержку android устройств для водителей сделать real time ago обновления в операторской работающую телефонию и когда мы закончим все наши работы у нас должна была быть возможность сразу перейти на свое решение какие у нас были ограничения у нас была очень высокая цена на мобильный интернет и она сейчас остается такой же например у нас сейчас стоимость мобильного интернета такого мы платим примерно 10 долларов в месяц и при этом у нас включена 60 мегабайт трафика в сутки и после этого когда мы черпаем лимит у нас скорость падает просто до безобразия исходя из этого нам нужно экономить на коммуникации между водительским устройством и нашими серверами которые у нас были ограничены и мы сортавале вообще на обычных домашних станциях которые в принципе у вас дома стоят у нас была очень маленькая техническая команда и было полгода на разработку решили это делать web решением по ряду причин во-первых это веб это очень хорошо и быстро можем масштабировать по людям нет никакой привязки них софта не железу и нужен по сути только браузер и решили сделать одно ядро которое бы либо полностью автоматизировала либо предоставлял какие-то методы выпей для работы операторов водители менеджеров и для приема платежей мы выбрали django для ядра редис для механизма publish сам скрипт но джесс для событийного real time ago операторской twisted как соки сервер для водителей руби для работы с sms выбор диси для телефонии спрашивается зачем нам весь этот зоопарк но по ряду причин во-первых у руби есть руби smtp и которая предоставляет нам отличную работу без проблем после протоколу для приема и отправки смсок но джесс потому что есть , который предоставляет по разным видам транспорта реализовать real-time принцип ну просто потому что он клево нам нравился зачем вообще ввязываться в сырой выбор тесея тем более делать на нем телефонию мы изначально хотели построить все решение на свободных технологиях на open source и который бы ни была привязана ни какого нибудь операционной системе или и железу и благодаря этому потому что это все в вебе мы могли экономить рабочие места и оператором была в принципе удобно работать из дома и нам не нужно тратиться на коммутационное оборудование на железке на лицензии очень быстро масштабироваться по людям реализовали первую вещь так у нас это все на джонги водители у нас работать на китайских навигаторах которые общаются сделан он отвезет который свою очередь если водитель авторизован то он обращается к кризису есть и во всех остальных случаях он по пост где запросам общаясь с django django возвращает ответ из это соответственно навигатор то есть вот такой вот простой процесс работы с ума со помещения для клиентов выглядят примерно так у нас они отправляются водителями то есть у нас 3 ст делает запросы джан гута свою очередь на делает на демона руби тот через центр провайдера отправляет клиенту с дальше с это с enter это этот термин будет сейчас а дальше так что запомните также нам нужно принимать с заказы и процесс выглядел так у нас клиент отправляет sms она приходит на центр провайдера мы забираем ее своим демонам наруби и делаем по запросам в но этот заказ можно видеть оператором переходим к операторской у операторов по сути два процесса работы первое это по приему заказов и второе то что мы можем было звонить пользоваться телефоне о телефоне я расскажу чуть позже поэтому переходим к заказам заказы реализованы так все заказы приходят по посту на джангл она сохраняет за каспер кому делает publish на определенный канал в одессе которые суши с демоном на джесс и если вдруг что-то там пришло то делается оборот cosmo всех операторов происходит это все очень быстро очень шустро и операторы довольны и все видят менеджер это самая простая часть нашей системы они по сути джангл она забирает данные sperg он ее выдает их либо в excel либо в браузер то есть ничего сложного вообще на начальном этапе мы заложили небольшой отказоустойчивость для перк он и сделали мастер слоев репликацию и на матери у нас виртуальная печник который мы можем перекидывать случае падения какого-то мастера и делали этому вручную все работы закончили настало время проверять как она вообще работает как поведет себя в реальных условиях запустили все это дело на курьерской службе проверили были готовы из плюсов мы выделяем силища-то что все-таки она работать в реальных условиях и на этом этапе мы в стабилизировали звонки через вайбер тисе настала самая адская боль абсолютно для всех сотрудников в нашей компании так как нужно менять софт и эта боль она абсолютно для всех для начальников для операторов для водителей для клиентов ну и для нас тоже весь этот процесс у нас стояла задача по переносу такая то что нам нужно перенести две тысячи водителей перенести все sms номера работающие телефонию и около 20 операторов делалось это за 7 дней очень маленькая техническая команда и при содействии одного начальника транспортного дела и начальнику колл-центра операторскому сделали так сделали одну точку входа на джинсе и запроектировали джонгу на джесс сам тут все понятно да водителей запускать стали так подняли отдельную виртуалку которая просто донателло на виртуалку с песчаным сделали это специально за возможность дальнейшего масштабирования и также мы обеспечили себя первыми глазами на продакшене использовали центре для сбора exception of абсолютно со всех брендов и ноги узко ли где для мониторинга и сбора метрик тоже все просто на первый день мы перенесли около 100 водителей и 4 операторов все работают операторы скидывают в нашу программу заказы со старой программы на второй день мы принесли уже 250 водителей и около четырех операторов также мы начали переносу наших ас мы сами ров на третий день больше большая часть смены из операторов она уже работать на нашем сайте мы перенесли абсолютно все сам из номера на четвертый день мы занимались переносом телефонии и тут мы столкнулись с первыми проблемами связанные с производительностью у нас водителей не всегда могут взять заказ и у некоторых водители не всегда появляется список заказов в чем может быть проблема тут проблема банальная у нас вываливаются соки таймаут на навигаторов потому что 300 ту нас синхронный и не успевает обрабатывать шквал запросов от водителей делаем так балансируем нагрузку между двумя 3 садами который балансируется еще прокси на пятый день уже все заказы идут через наш софт водители хотят переходить и транспортный отдел зашиваются у них до 150 водителей в офисе и в час и их пиковой нагрузки скорость переноса водители на новый составляет примерно два водителя в минуту при этом это делается около тремя людьми вот и представьте какой там было врал но тут у нас появились проблемы с производительности у операторов связано это с real-time am в том и в бою происходит это так то что у некоторых операторов может быть не обновляться список заказов и допустим если водитель нажал у себя тоже заказ сложный и все операторы не все операторы это видят ну так как у нас за реал тайм от отвечают backend на джесси то по итогу стало ясно то что он не справляется мы его масштабируем и также у нас еще работает django на шестой день мы перенесли уже всю службу и тут у нас начинается самый веселый экшн экшн в том то что у нас водители проблемы у нас и у водителей и операторов у операторов в том то что заказы оформляются медленно у них есть своеобразная карточка они там убивают адрес живут оформить и когда они живут оформить она обычно очень быстро исчезает но тут он исчезает и та же самая проблема как на четвертом дне у водители то есть у них также ловится высоких таймаут на этом моменте у нас проблема с джанкой ну и выход очевиден потому что берется делаем в общем вот так вот ну решение проблемы в лоб и заодно делаем такую первую простую тупую балансировку то есть у нас 13 сот общается с одной джанкой а другой ответит общается с другой джанкой но при этом тут еще не все у нас но je de manana джесс живет порядка двухсот пятидесяти мегабит все эти связано это с тем что мы в принципе использование но джинсы используются китаю на этом графике версия , 098 и кто успел я использовал знают о том то что там little модуль редисом и при работе с средством плодились листа рыб родились connect и в итоге выходила это вот в такой вот ужас контейнер со китая на этот момент занимался активно развитием версии 10 своей библиотеке и эту бабу как-то чинить не хотелось ему но спасибо дать ему чувакам широко , они поправили лик в одессе мы долгое время просуществовали именно на нем пока не вышла версия 13 там какая-то и мы все это дело не обновили на седьмой день у нас все стабилизировалось все работает хорошо вот у нас счастье радость ура мы пережили этот ад и какой у нас итог за время этого переезда семьи дневного мы потеряли порядка восьми процентов заказов суточных ну как бы такое случается там всех стресс но при этом у нас есть еще один плюс мы благодаря нашему софту мы сократили время подачи машины свою минут до 5 ошибки на начальном этапе мы допустили в том то что на 300 синхронной мы не используем трясет как туристы со всеми его крутыми плюшками и вместо сейчас бы вместо использована джесс я бы сделал бы на эрланге или пайта не этот демон и наши нагрузочные тесты были не из реального мира после перехода соответственно когда все стабилизировалось мы начали расти вверх потому что мы быстро подаем машину клиенты довольны все хорошо все радуются но приходит к нам бизнес и говорит чуваки надо больше заказов то где нужно больше заказов нужно расширять охват аудитории и решили мы сделать мобильные приложения в общем она выглядит так у нас есть объект and тоже на чанге чангу мы любим как и поэтому которая делает создает заказ в по по запросу в ядро и дальнейшее статусы заказа хранит в одессе чтобы просто экономить а ненужных степи запросов между двумя брендами и данное клиенте авторизован не авторизован там всякие номера телефонов все это хранится в перкаль наша точка входа на джинсы сало вот выглядит так то есть у нас проектируются и и django на ядре и джанга на автомобильных приложений на джесс ну мы сидим такие довольные наблюдаем рост в заказов инсталлов количество водителей на линии и там где у нас не справлялся 130 теперь не справляется 2 что делать менять всё кардинально использовать трясет правильно висят как 300 совсем его асинхронными вещами при этом у нас неравномерная балансировка ну и заодно просто сделать еще под шумок вот раз начинаем влезать во все эти страшные вещи у нас один сервер нагружении другого потому что у нас одна точка входа на и джинсы и примерно нагрузка на физических серверах она вот выглядело вот так вот вот то есть вообще от водители на этот момент выглядят так у нас точка входа для них это на еще a proxy которая балансирует между двумя тестами который свою очередь общаются либо со мной либо азербайджан гай точка входа на engine си у нас такая же очевидно и мы решаем сделать так мы подымаем 2 джеймс на втором физическом боксе балансируем нагрузку между ними через round robin и делаем ещё на вербе протоколе с помощью демона или linux keep alive все знают эту тему ну кто не знает в общем этот демон очень клевый позволяет делать хороший отказоустойчивостью на этой схеме есть у нас один спадает то 2 джеймс собирается виртуальная печники становится первым джинсу мне ни другое никто этого не замечает также мы разделяем workflow отчеты отдельно обработка заказов отдельно подумаем в общем четыре чан ге даче я объясню почему 4 при этом 2 обрабатывают workflow для принятие заказов и работаю с мастером перк он и две другие они просто выдают отчеты и читаются слова также мы переписываем твистед делаем а синхронную работу с чем со всем чем можно и так же добавляем работу с ранду с рандомными обстрелами в итоге трясу нас умеет работать так в итоге мы пришли к этому у нас одна . на ища прокси которая отказывалась . с помощью демона keep-alive еще прокси балансирует между двумя нагрузка между двумя тави судами которых каждый из них которые балансируют между двумя engine сами отец свою очередь между 1 и 2 джанкой на текущий схеме у нас отказоустойчивые джинсы через happy life is a proxy тоже и благодаря этой схеме можем потерять абсолютно любой компонент из всей этой цепочки вот и все будет продолжать работать при этом чтобы это все работало и не тормозило мы держим на физических серверах порядка 40 50 процентов свободными это нужно для того чтобы все продолжила работать и ничего кисло но это еще не все у нас проблемы с тем самым она не масштабируется и редис у нас не отказывай устойчивый сама демоны работают так то есть у нас django канабиса по обычному тисе пи эс red socket и отправляет смску через monsanto у нас на носу в том то что у нас много коннектов по тисе пи и это очень плохо потому что это вообще бесполезно и используется много ресурсов и у нас нет также масштабируемости выход переместить все это на редис сделать на механизме повлечь субскрайб и в итоге имеем этом django делает publish в нужный канал нарядись и которая случается со мной демоны из нашего то пришла отправляет эту маску в профите следующие в целом понятно отказывал случилось для разница мы делаем с помощью сантино у нас есть сантима который мониторит абсолютно все ноды редиса и есть у нас один мастер с которого синхронно реплицируются данные на славы и если мастер падает то 2 slave что кто-нибудь из остальных своего станет мастером и все будет работать downtime примерно 1 2 секунды также у нас случилась радостная новость вышла версия 5 6 от прикованы мы ее сразу ставим после тестирования и делаем такую стену схему то что у нас мастер мастер репликация по джи ти идеи технологии у нас виртуальный api адрес для мастера один виртуальная пи адрес для слова мастер использован для записи освоив просто для выдачи отчетов при этом мы можем уже сделать отказоустойчивость как мастера так и слова вот это очень удобно но все переключение мы не мотивируем по ряду причин тоже сейчас это нам же дороже будет потому что тема очень сложна и без и особых проблем сейчас это нам не доставляет в итоге мы получили высоком масштабируем продукция сказывалось которая сразу сочи вы и из sepam без лыж а теперь самое интересное как мы делали выбрать эссе работу типа через выбрать эссе стабильным сначала мы купили для тестов одну сип учетку у провайдера и первую реализацию сделали такой мы взяли sepam л в качестве софтфона на фронтэнда поставили выбрать и ситу sip и ко всем видео прокси между си помыл и провайдером вот звонки работают салон все окно проблему такие то есть у нас мы не можем подключать больше номеров и не можем как-то балансе рой звуки и на один дальнейшей номер нам нужно подключать нам нужно подумать ещё один демон выбор тисе решили перепрыгнуть нас риск одиннадцать с половиной сделать такую схему с риск использовать в качестве от с между чтобы он работал с нашими джессом шлюзами с провайдерами оси помол также на фронте нди вот получили плюсы в том то что это от с ее мы получаем все все плюшки полоса над с можем балансировать нагрузку и быстро подключать новые номера но проблемы на 11 астериске 11 5 были в том то что у нас взрывается звонок и когда оператор звонит клиенту он не слышит не гудков ни каких то ответов от о том то что абонент занят либо находится не действие сети также у нас аудио между двумя девайсами большую со очень долго то есть долгое время люди не слушают друг друга и входящий звонок с китайского шлюза мог вполне себе взять и положить asterisk решили обновить до 11 6 итог такой звонок не срывается вас риск работает стабильно в целом аудио бриджеса хорошо но у нас также нет гудков и появилась еще проблема со стороны астериска в том то что у нас у нас иронии аудио то есть клиент звонит оператору и клиент оператора слышит оператор клиента нет или наоборот решили вернуть в эту схему выбрать и ситу сип в качестве медиа прокси между типом или австрийском получили вполне себе работающую телефонию но выбор ti si tu сип начал рандомно крошиться проблему начинаем решать запускать под дебаггер am ставить всякие брэйк поинты потасовок как-то ее починить но не осилили где то за неделю две по вот этим причинам код был плохо задокументирован и было плохое качество кода ну или просто нас мы не осилили эту багу открыли для себя фриспич сделали такую схему вместо с риска и выбрать и ситу сип у нас теперь стоит фриспич плюсы поимели в том то что у нас в целом то все работает но у нас также нет гудков и каких-то служебных ответов от операторов связи так как это все работает мы решили сделать так у нас пресечь при исходящем звонке от оператора поднимает трубку и в итоге все в итоге оператор call-центра слышит абсолютно все водки и сообщения от наш абонент занят или не но у нас проблема в том то что звонок длится теперь не более 2 минут из баги все помыл кто работал с типом а у зная то что мини фиксированная версия этой библиотеке весит порядка одного мегабайта и разбираться с в этой в этом мегабайты него скрипта как-то вообще не хотелось вообще никак решили паре сёрфить какие-то аналоги какие-то замену и нашли джосеф который во первых легковесной из проекта 167 килобайт во-вторых он от авторов офисе работы sepo через выбор тисе и в итоге мы пришли к стабильной схеме в том то что у нас же севка софтфона freeswitch работает с джессом шлюзами провайдерами но осталось все это дело за и чашек и делаем так также используем keep-alive который у нас запущена 2 запущена двоится so free свеча на разных виртуалка и есть вдруг падает у нас мастер то все забирают на себя второй фриспич при этом все хранится в перка они все служебное состоянии также подключенные зеро и все то что нужно при свече ну и благодаря механизмам восстановлением при свечах у нас все работает и никто не замечает этого падения когда мы запустили свое решение для телефонии мы смогли спокойно обрабатывать 25 звонков сутки но в итоге мы благодаря тому то что мы автоматизируем все процессы снижаем какие-то ненужные действия или и как-то помогаем нашим пользователям то есть оператором клиентом или водителем у нас итог проделанной работы вот такой вот звонки мы количество звонков снизили в два раза сократили при этом 30 процентов операторов сокращали их мы очень просто вы просто не брали новых asap ходили вот и в целом мы увеличили количество заказов на 40 процентов какие у нас были вообще подводные камни в проекте было очень много подводных камней в выбор тисе это вообще отдельная тема если кому интересно я могу рассказать о нем подробнее в кулуарах мы не сразу могли сделать отказоустойчивость для редиса и для нескольких других наших компонентов например twice а ты когда мы активно масштабируемся то у нас возникают проблемы с конкуренции проблемы с конкуренции решаются просто если у нас проблема с редисом то используем settings если у нас проблема с целым с конкурентом доступом в базу данных то мы используем атомарные транзакции select правдой русам известной всем практике какие ошибки в целом мы допустили мы не учли такого быстрого раз у нас изначально не была хорошей балансировки наши глаза на продакшен на текущий момент мы вместо nagios ассоль использовал сансу потому что он клёвый централизованный и с ним работать очень удобно вместо коли где мы стали использовать графит new relic мы используем для сбора пи famas metrics демона но джесс используем си profile middleware чтобы пропали джангл и обид используем для сбора performance metrics джангир ну и если у нас по каким-то метрикам все плохо и ленин не удовлетворяют нашим ожиданиям по скорости работы мы все это дело я сам оптимизируем также у нас есть учебные тревоги среди разработчиков и админов мы до 2 раз в неделю проверяем отказоустойчивость любого из наших запущенных сервисов просто взяв и положив его и до 2 1 месяц мы перезагружаем любой из физических баксов и смотрим как все это дело подымется выводы по всему до к докладу сделать могу такие то что архитектура нашего решения должна подстраиваться под бизнес-процессы не нужно боятся менять архитектуру кардинально разные разные workflow большой системы не должны влиять друг на друга и хорошая архитектура растет вместе с вами если у кого остались вопросы то вот мои контакты вопросы я не слушал с пышными кишки здравствуйте раз спасибо за доклад я стич и вот здесь вот в описании доклада была такая фраза что вы расскажите почему не надо использовать но до в продакшене но как то вот я так и не услышал почему ну этот тезис больше для привлечения внимания был было были проблемы в том то что мы использовали ноду немного неправильно были проблемы с мы огребали с тем то что нам подготовили man тренеры библиотека или исходя из этого просто можно сделать такой вывод то что не всегда можно просто так взять и использовать его в продакшен спасибо но тут надо было версия 010 какая-то со китай был 098 и в целом это все происходило где-то в году 2014 в первой декаде вот и тогда мы получили этот немного горький опыт но сейчас мы наблюдаем за версии ну зависимостью используем четвертую версию ноту но ты последний тсс акита у нас 135 по моему еще вопрос на выбор тыс и значит проводили ли вы нагрузочное тестирование free свечей то есть и примерно какой потолок позвонка потому что допустим мы в принципе фиксили as there's да то есть теми же проблемами сталкивались при магнуссон тестирование и значит показала что то есть мы уперлись в магическую цифру 33 тысячи звонков и после которых asterisk просто перестановок переставал обрабатывал мы обрабатывали топ 10 тысяч звонков вот это было на прошлый новый год и все вполне работала ну там короче говоря суть в том то что там реализация и стан протокола то есть там чанки начинали сыпаться и он просто переставал их обрабатывать нос папы тоже алкоголик и и мы просто подумали свой стан сервера ну то есть в итоге то то есть вы freeswitch как-то нагружали то есть ли какой-то потолок есть у него то есть примерно потолка мы не достигли как патолог который у нас сейчас есть это полностью опять или сорок семь замков за сутки спасибо еще такой вопрос а в плане железа сколько серверов вы использовали у нас два брендовых средних брендовых железа и два обычных писю к для не критично то дачу спасибо можно ли на вопрос спасибо за доклад на короткий а сколько человек у вас обслуживает это все ну давайте наверно просто расскажу историю все команды которые она была начинали вдвоем я и ещё один человек который очень быстро слился доводил все это дело до запуска я один после этого перед запуском появился еще один devops который мне помогал мы все это дело запустили взяли еще одно программиста взяли двух мобильных девелоперов и по примерно сразу же взяли около двух админов вот и как-то так спасибо андрею за доклад вопрос такого плана когда упала железкой пропал сервиса все понятно а если сервис у вас поймал скажем клина момент обработки заказа подробнее можно вопрос к вам пришел запрос момент обработки этого запроса одна из ваших лодка то его бра баттлом перестала отвечать то есть она как бы живая все окружающие думают что живая заказ на нее есть но он обработать а не может ну мы такие вещи еще не ловили себя вот и прям точно сказать не могу еще раз спасибо за доклад вот еще такой вопрос я заметил что вы для 300 то использовали h прокси до 1g свыклась реагировали на это понимаю встроенным до кластером нет мы просто их запустили на разных портах и все а у нас никакие данные не хранятся но джесси то есть все идет через редис и поэтому можем сделать так естественно то же самое то есть у вас не часы джинсах нить расход прокси они как-то между собой не возвращается на да спасибо ну надеюсь было интересно спасибо вам"
}