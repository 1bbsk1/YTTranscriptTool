{
  "video_id": "UGbLNJiJmQo",
  "channel": "HighLoadChannel",
  "title": "Как переписать с нуля базу данных личных сообщений ВКонтакте / Дмитрий Егоров (ВКонтакте)",
  "views": 9522,
  "duration": 3448,
  "published": "2017-12-11T01:42:39-08:00",
  "text": "итак всем привет я dmitry egorov я руковожу отделом баз данных компании вконтакте и сегодня вам расскажу об истории эволюции хранилище личных сообщений в нашем сайте а также том как мигрировать между различными версиями без downtime чтобы наши пользователи ничего не заметили и так в первые шитья хотел бы начать с небольшой статистике чтобы вы примерно поняли о тому данных какого масштаба идет речь и первая же самая простая статистика которая всех интересует это сколько же у нас личных сообщений и тут уже встает вопрос что такое личное сообщение если человек посылает его в чат на 10 собеседников это 10 копий одного сообщения или же 10 разных сообщений и в зависимости от того как вы хотите на этот вопрос ответ числа будут разными вы получите либо 5 почти пять триллионов не удаленных живых сообщений если считать копии за разные сообщения и около двух триллионов 300 миллиардов сообщений если считать уникальные при этом каждый день наши пользователи отправляет более двух миллиардов сообщений а получает 6 с половиной опять же за счет этого эффекта с копированием сообщений при этом в пике нашим сервисом пользуются более восьми миллионов человек одновременно здесь мы пользователя читаем онлайн есть но в течение последних 5 минут совершил какое-либо активное действие это может быть примеру просмотр новости которые нам пришло с мобильного клиента он ткнул а кто-нибудь кнопочку на сайте или что нибудь такое в этом случае мужчина им пользуется онлайн и наконец если мы возьмем и соберем все наши личные сообщения все поисковые индексы пони все перекрестной ссылки и даже сожмём их как-нибудь алгоритм обжать у нас равно останется около 320 4 терабайта данных о личной переписке это цифры на сегодняшний момент и так совершенно очевидно что большие нагрузки нужно использовать как можно более оптимальное решение и здесь всегда вступать в противоречие две цели с одной стороны мы хотим использовать как можно меньше железо с другой стороны мы должны не отставать от времени с каждый несколько лет обязательно во всех наших социальных сетях существующих мире появляются какие-то новые фичи которые нужно реализовывать нам чтобы удовлетворять нужды пользователей и давайте мы сначала вернемся на сначала на полтора года назад в начало 2016 года когда за личные сообщения на нашем сайте отвечала около отвечал одна тысяча серверов собирайте были 60 четырьмя гигабайтами памяти для многих из вас откажется какой-то космической цифры но на тот момент нам было необходимо именно такое количество железа и тем не менее мы проанализировали графики и увидели что из-за постоянного роста нагрузок производительность сервисы быстро деградируют и мы еще в первом полугодии прошлого года были вынуждены добавить сервера по 32 гигабайта памяти расширив до 96 гигабайт на север но тем не менее все равно экстраполировать те данные которые мы получили мы заметили что у нас осталось около 1 года на то чтобы решить проблему и с ростом нагрузок на эту базу данных в противном случае наши бы личные сообщения просто бы умерли пути решения есть два либо можем просто добавить еще железы либо же все же заняться оптимизацией баз данных сейчас мы отправимся еще назад в прошлое на 89 лет и подумал бы а чего же вообще начинались личные сообщения вконтакте и сайт в целом самом начале у нас был просто обычный backend написанный на php который обращался к морской лев sky базе данных в которой лежали все сообщения и пока сайт был обществом тусовки небольших студенческих групп это спокойно работала без каких-либо проблем но нагрузки росли и какой-то момент мы написали свое собственное первое хранилища личных сообщений который мы назвали текст енджэ нам текст она основана очень простой концепция актуальны на 78 лет назад что у каждого человека есть некий почтовый ящик которые приходят все сообщения пользователя друг с другом как независимо и различные инстанции движка тоже вся синхронизацию ществляется в php коде и до тех пор пока вам нужно реализовывать исключительно личную переписку двух людей в целом это вполне рабочий подход вам нужно отослать сообщения в два места никаких больших проблем нету но время шло и в какой-то момент появились чат и намного людей здесь начались первые сложности потому что потребовалось создать два дополнительных кластера которые были названы мы обращаться вич от members первый хранил по чатам то какие пользователи в них есть я и дополнительную информацию вроде пригласившего в чат человека в даты приглашение так далее вам обращаться наоборот по пользователям такаки чад у него есть вся синхронизация по-прежнему остается в php коде и мы сразу видим первую проблему как осуществляется отправка сообщений мульти чат в этой схеме php код должен сначала сходить в обращаться чат members достать информацию о том в какие текст engine и нужно послать сообщение разослать их все в текст engine и дождаться ответа и только потом вернуть результат пользователю несложно заметить что для больших чатов это просто работает очень долго кроме того php код вынужден обрабатывать различные ошибки когда один из движков не ответил что усложняет разработку и в целом это не очень надежно у нас бывает еще к примеру райс кондишен и когда несколько человек одновременно пишут в чате в один тут же чат они обрабатываться на разных серверах с бэндом и они рассылают сообщения в текст engine и в разном порядке но это не единственная проблема в какой то момент мы расширили функциональность сайта и дали возможность не только пользователям посылать сообщения между собой но и группам что сразу же привело к проблеме неравномерного распределения нагрузок пока у нас личное сообщение пальца только пользователи они все в принципе довольно похожи друг на друга не один пользователь не может сгнить какую-то очень большую нагрузку которая положит соответствующий ему инстанция текст angel но когда мы говорим о группе above ведь бывает мне только группы но и боты которые у нас зарегистрированы как группы то картина оказывается совершенно и но сейчас у нас самые активные боты посылают несколько миллионов сообщений в день и все они у нас старой схеме приходились бы на один и тот же текст engine если мы так подумаем это то в без кардинального изменения концепции текущие проблемы решить было никак нельзя нужно что то менять абсолютно координально и что же мы хотим получить мы хотим перейти от концепции когда все строится вокруг пользователя и его почтового ящика концепция когда центральная объекта это чат и пользователи в некотором смысле хранят лишь ссылки на него они могут посылать сообщения читать читать сообщение от туда выходить из чатов ходить туда и так далее но центрального объекта вокруг которого строится это теперь будет чат они те сообщения которые есть у пользователя теперь у нас не будет нам в одной единой базы данную мы разделим на две части естественно уличат engine который хранит всю информацию от частых там хранятся все сообщения вторая часть базы данных юзер engine занимается лишь тем что расставляет пользователям ссылки на нехай и боксируют запросы от паха пышного бэг-энда качать engine of ну и в частности занимается некоторым кэшированием и так это как должно быть с чего же начать когда вы пишете какую-то очень большую систему но сначала нужно понять что же вы все же пишите на самом деле личное сообщение тоже ведь не только их посылка чтение к нему там сервис прочитано sti и так далее есть еще три дополнительные вещи без которых невозможно себе представить сервис личных сообщений в социальной сети первое это поиск 2 это real time ago и обновления как на вебе так в мобильных клиентах называемый long полом у нас и последняя часть это кэширование в мобильных клиентов необходимо чтобы пользователи могли читать свои сообщения без интернета и когда интернет появляется они бы не скачивали всю свою переписку которая для активных пользователях миллионов сообщений а лишь погружали некоторые изменения это пережить in history теперь мы поняли что мы хотим написать следующий вопрос как нам реализовать вот такой вот большую задачу первую очередь нужно вытянуть все лишнее нужно оставить лишь тот прожиточный минимум после которых вы сможете поднять вашу базу данных по посылать туда запросы и найти какие-то баги и потом уже постепенно докручивать функциональность и так что же останется если из этой концепции выкинуть все лишнее еще вообще никак нельзя выжить для юзеры джона останется очень простая вещь у каждого юзера джона есть и некий список пользователей которые в первом приближении можно хранить и просто в векторе для которых этот движок будет хранить их ссылки у каждого пользователя есть диалоги у нас называются пирами которое также можно хранить первых уже не просто векторе есть сообщение еще один вектор и а также у каждого диалога есть список сообщений в нем еще один вектор все четыре вектора больше ничего вам не надо это можно написать буквально за один день charging еще проще там просто указывая тоже есть какое-то количество чатов которые есть какое-то количество сообщений в самих сообщениях мы на первом этапе тоже оставляем только текст ну и как необходимо идиш никита чтобы реализовать общение между движками однако эти движки должны между собой общаться синхронизируемых мы при помощи прецедентных запросов которые мы реализовали поверх тисе пи у нас между каждыми двумя движками есть очередь запросов которые мы хотим послать одного к другому соответственно если и нам гарантирует что сообщение будут доставлены в том же порядке в котором мы их посылаемый и в реальной жизни это все работает очень быстро никаких очередей не скапливается если же у нас один из движков а почему то лежит или загруженные может ответить то у всех остальных движков постепенно начинает накапливаться у очередь сообщение к нему а как только лежащий движок поднимется он ее очень быстро разгребешь при этом мы сразу решаем все проблемы с консистенцию за счет этой очереди зафиксирован порядком у нас не могут перепутаться соседние сообщение при такой схеме посылки запросов и следующий важный факт который тут используем то что у нас большие масштабы у нас сейчас работать четыре тысячи юзеров 4000 частные джиннов и поэтому эта лошадь очень хорошо распределена даже если у нас выходит из строя один сервер намертво то скопившуюся очередь из какого к и разумное время не сможет и вообще никак сказаться на производительности остальных движков здесь мы заговорили про президент ные запросы значит у нас появилась какая-то работа с диском работа с диском у нас устроено по большей части следующим образом почти во всех движках мы пишем бинарный лог событий и время от времени генерим индекс зажатое представление о к текущего набора актуальных данных мы здесь используем ещё одну важную особенность наших нагрузок что у нас со подавляющее большинство пользователей живут в московском часовом поясе или близко к нему поэтому в пиковые часы там в 9 10 вечера по москве нагрузку во много раз превосходит нагрузку в четыре-пять утра по москве и поэтому тестер или рака на которые нагрузку близ как предельные в пиковое время ночное время могут выполнять это очень сложные операции оптимизируя структуру данных чтобы в течение следующего дня отвечать на запросы гораздо быстрее типичное устройство снимка такого у нас в ночь в начале файла который может быть размером в десятки а то и сотни гигабайт есть некий небольшой hitter который движок одержит всегда в оперативной памяти и основная часть снимка состоит из большого количества me to файлов это некоторые блоки данных объединенные какой-то одной очень важной особенностью из-за которой они будут использованы более менее в одно и тоже время и мы таки можно оптимизируем походы к диску мы используем наши знания о данных чтобы подгружать ровно то что нам нужно не просто какие-то случайные страницы в частности к примеру для текста шины usi ренджи на один me the file это данные одного пользователя для чат engine а это данные одного чата и типичное использование данной схемы пример такое когда пользователь приходит в онлайн и и начинает ходить по сайту в соответствующих движках очень быстро и все его данные попадают в оперативную память к а если не все то те про которые мы предсказываем что они будут актуальны ему в конкретно в это время они устаревшие на несколько лет соответственно когда после уйдет в офлайн и через какое-то время можем в его данный вытеснить обратным на жесткий диск освободив оперативную память как эту концепцию наложить на наши движки в этом смысле у нас все структуры данных раздваиваются у нас есть отдельная копия данных которых мы храним извинения за текущий день и все пишущие запросы модифицируют структура данных именно те которые находятся в оперативной памяти данные на диске мы не меняем до следующей ночной индексации никак соответственно придется просто мы сначала пытаемся найти нужные нам данные в оперативной памяти если же там найти ничего нам не удалось то нужно поискать в индексных данных если там ничего нет значит к примеру на год какое-нибудь сообщение мы можем сделать уж этого сообщением пользователя не было вообще так реализуется прочистки все запросы довольно легко если же мы говорим про структуры данных которые мы используем кто так как данные на диске не изменяем это мы можем везде заменять виктор она просто массивы фиксированного размера отсортированы и также достичь максимально возможной эффективности по памяти соответственно здесь нарисован пример для юзера джина для отчетная же на все абсолютно аналогично поэтому я не буду тратить на это время отлично мы научились движке общаться между собой написали прозрачным минимум теперь можно начать тестировать мы поднимаем какой-то свой небольшой backend с котором будем посылать тестовые запросов наши небольшой кластер тестовый как будет устроена к примеру посылка сообщение у нас в бэг-энд посылать сообщения в виде рейнджер он его первым делом прокси рует в чат engine который уже сохраняет его на диск запоминает это сообщение и возвращает в качестве подтверждения юзеры джону айдишник сообщения внутри chateau получив achat la cala de выдает ему отличник уже внутри пользователя и называемый узел о кладе выдает в пышный backend у нас еще со времен текст энджи на все включая в первую очередь мобильные клиенты построен вокруг концепции что у каждого пользователя есть своя технику каждого доступного ему сообщения мы от этой концепции можем отказаться как минимум потому что у нас каждый день десятки миллионов пользователей с мобильных клиентов используют наш сервис личных сообщений и мы не можем просто так взять и сломать его для не всех теперь смотрите пользователь уже получил user local айди но при этом я также мог еще не успеет разослать все сообща сообщение остальным пользователям chateau но мы гарантируем что в скором времени мы его доставим чат engine сразу после того как а послал сообщение на рассылку чоколате бьюзи ренджун всем остальным участникам чата рассылает полученное сообщение и оно через нашу систему печатных запросов дойдет обычно мгновенно в случае каких-то нет неполадок тогда когда поднимутся соответствующие движке все просто мы получили консистентной мы научились посылать запросы отлично давайте двигаться дальше давайте подумаем про то как нам переживать старые данные наверное все очень сильно обидится если мы скажем что а давайте-ка вы начните свою переписку в кассу чистого листа мы не хотим импортировать данные за предыдущие 10 лет это не очень реалистично первое что мы хотим заметить то что классным обучаться раздроблен по пользователям он хранение там главный ключ это пользователя по которым хранятся какие то данные о его чатах не очень совместима с концепцией когда у нас все вертится вокруг самих чатов и к счастью мы немножко подумали и нашли способ перенести часть данных в чат members так чтобы в моем бар час на нам уже не нужно было делать никаких запросов в процессе переезда этот кластер еще в течение нескольких месяцев просто жил параллельно из него backend читал данный записывал данные для работы со старым тексты джоном но дальше он нам уже при переезде никак не вмешался мы потом просто его убрали когда окончательно переехали так у нас осталось два важных старых кластеры то текста же начат members первым делом мы перед дроблением чат members так чтобы те чат и которые находятся на карты мульти чат members of в точности соответствовали чтим чатам которые будут храниться на соответствующем чат инженер у нас напомню структура с блогами и генерируемыми снимками и у нас чат engine будет не только писать свой мелок но и читать белок чат members а в практически мгновенно подхватывая все данные об изменении уже существующих чатов теперь ему осталось узнать данные про личную переписку по их структуру ну здесь ничего умного особому не придумали у нас было 4 тысяч extension of each опять же про каждого пользователя спрашивал у каждого текст angel какие есть диалоги у этого текста я же на для этого пользователя соответствующий чат и джину который сделал запрос ну наверно можно было послать запросов и поменьше но в целом это работает обозримое время и мы не стали тут что-то сильно оптимизировать и так после этого учат engine а есть информация о структуре как всех чатов такой и о личных переписках осталось научиться мерча то мир чатов происходит последовательно один за другим каждый чат чат начинает уменьшить с того что запрашивает у всех за действия в тексте же на все сообщения всех участников чата за все время и сохраняет их к себе он их после мира че это она соответственно рассылает все сообщения в userinfo расставляя ссылки и заканчивая мир чатов давайте немного подробнее расскажу про то как же нежить чат и во первых все чат и на самом деле помещаются в оперативную память и современных серверов самые крупные чат и которые вымерли имели порядка 100 миллионов сообщений с учетом кратности мы нашли несколько более больших чатов которые уже у нас не вылезали оперативную память но немножко преследовал что это за данные мы увидели что это откровенный спам и на самом деле просто его удалили вот все а вот все разумные чат и они не превышают 100 миллионов сообщений чтобы вы понимали у нас его было лимит в 50 участников на 15 кроме того у каждого человека есть лимит на 10 миллионов сообщений у в его личной переписке на тот момент он был 10 миллионов соответственно получить чат и больше чем 100 миллионов сообщений суммарно с учетом у копий ну очень сложно и когда мы получали эти до 100 миллионов сообщений мы просто используем то что они отсортированы для каждого пользователя когда мы получались текст engine у них есть естественно сортировка по user local айди кстати нам нужно просто взять и смертен списков кроме того всех сообщений есть дата и мы можем просто пройти стены указателями в каждый раз выбирая самое новое сообщение проверить похож а текста и katachi еще какой-либо дополнительной информации что она совпадает сообща с какими из первых сообщений остальных пользу она совпадает соответственно их склеить в одно единое сообщение и расставить правильно ссылки в общем и целом это работает и есть несколько тонкостей во-первых даты могут быть ним немного разные отличаться буквально на одну секунду за счет как рассинхронизация в php коде еще когда проходит некоторое время между тем как сообщение до ставилась первому человеку в чате пятидесятому поэтому периодически небольшие раз синхронно бывает но честно говоря мы просто подправили время некоторых сообщений на одну секунду потому что ну боже мой мы пишем социальную сеть и покажите мне пользователя который заметит что у какого-то его сообщения три года назад да то изменилось на одну секунду это не очень реалистично вторая проблема что некоторые 4k во время активной переписки случается рейс кондишен и и разным пользу им сообщение приходит в разную по в разном порядке такая проблема бывает женю мы не можем взять и просто так в двух пользователей поменять местами два сообщения на мобильных клиентов все сойдет с ума от того что нам придется как-то поменять user local айди но таких случаев очень мало и мы можем просто склеить их там эти два сообщения и в 2-го к примеру в 3 в чат на инженеру будет там не у каждого сообщения по янцзы лака чуть поменьше как-то они там распределяться неважно вот здесь основная цель как можно сильнее склеить сообщение чтобы хранить меньше данных это задачей так решено практически идеальной и пытаться что-то сэкономить в этом месте не осмыслена отлично мы смешали все чат и начинаем послать видео режим сталкиваемся с одной проблемой раньше у нас были векторов которых всегда юзера la cala de были отсортированы по возрастанию просто когда мы приходим в юзера жены говорим о у нас есть для себя новое сообщение они выдают юзер la cala de в порядке возрастания но когда мы мешен чат и различные чат месяцев совершенно случайном порядке и этот инвариант нарушается поэтому у нас есть два пути первый вариант никак не учитывать их в год запросов но к сожалению после этого сложность тестирование возрастает просто неимоверно потому что самый простой способ протестировать что вас в процессе мир zara биржа все идет хорошо это попросить php разработчика написать код который читает и снова класс стороне из старого выдать домен который будет ходить в новый класс то и смотреть что там происходит что-то адекватное соответственно хочется чтобы за ним порченые сообщение появлялись на этом под нами не как можно раньше тем более что эта проблема решается очень просто можно хранить не в цирке ровном векторе а вот в такой простенькой структур ки состоящий из логарифма отсортированных массивов с точки зрения мымре менеджмента у нас по-прежнему есть вектор но теперь там данный нет сортированы они разбиты налог кусков каждая размером с степень двойки к каждый из этих кусков отсортировано по возрастанию соответственно мы можем делать год запросы за логарифм в квадрате мы просто делаем а логарифм бен поисков соответственно запросы на добавление нового элемента также выполняется очень просто если у нас длина массива было отчетный то мы просто добавляем еще один блок размера 1 если же у нас размер массива был ни чё тут у нас просто несколько последних блоков вместе с последним элементом склеиваться в один тут мы просто их пересортировать да это дает амортизированная оценку и тут энрико кто то может сказать что не знаю отсортировать миллиона чисел а это уже там какие-то миллисекунды как же так и у нас и движок будет висеть но опять же пользователи этого не замечают по факту поэтому так делать можно отлично поборолись с этой проблемой но мы учили пока что не все помимо тех трех пунктов о которых мы сознательно забыли в самом начале есть еще некоторые мелочи во первых когда пользе заходит на страницу личных сообщений у себя на сайте он видит диалоги отсортированы в порядке в последней активности это у нас называется top managers также есть такая же структура данных для on red месседжи и то что называем сабли сами там хранятся просто набора сообщения булочки то особым свойствам к примеру так ранец и спам так охраняться важное сообщение еще есть некоторые служебные под списке некоторые из которых были в некоторых если legacy от очень старых версий приложений от часть из них мы смогли избавиться следующая часть это то что у нас есть огромное количество текста в каждой черте джинни десятки гигабайт и безусловно его нужно как-то сжимать для экономии места также в любой момент времени нужно вспомнить что сообщение стать не только из текста на к примеру там есть от то че там есть имя пославшего его человека который нужно для поиска по отправителю и так далее это все эти мелочи они вкручиваются более меня в любой момент когда у вас появилась на них время они не влияют на картину в целом и последний важная часть это каширование в юзер engine у нас use рожденного в основном работает и такой прокси юбка cheat engine у но активных пользователей у нас ну 8 миллионов от конечно очень много но имеет какой-то количестве какой-то довольно разумная число и не вызывает большого количества обращений к диску джинни ситуация совершенно другая чатов намного больше и чтобы не убить диски у нас наши баз данных здесь работают на hdd на ssd соответственно нужно экономить random access чтение с диска соответственно порежем попытаться запрашивать информацию о новых чатак это решается через кэширование в use реджина обычно пользователь особенно если он активный он вообще не видит сообщение которое мы не знаю там неделя уж тем более год ему нужно переписка с буквально за последние пару дней все эти сообщение можно спокойно кэшировать в use ranger давайте сейчас еще немного углубимся алгоритмические детали к примеру про первые три пункта списке диалогов списке не про считанных сообщений каббалисты это какие-то очень часто изменяемой структура данных тут ничего особо умнее чем дерево не придумаешь вопрос лишь том какое дерево нашу выпал нас play дерево это такое дерево которое основано на идее что перед тем как сделать какую-либо осмысленную операцию с вершиной ее нужно поднять в корень создав тот момент пока ты вы поднимаете вершину в корень происходит балансировка дерево этот опять же амортизированная оценки на скорость работы но здесь важно то что мы в вершине храним не всегда только один диалог одно сообщение мы иногда храним целый range который лежит на диске и в частности когда у нас движок только-только поднялся со снимка после ночной переиндексации все деревья состоят только из одной вершины которая целиком целиком ссылается на весь участок данных на диске соответственно второе очень важное преимущество с плей дерево это его простота в написании у нас есть довольно мерзкая для многих деревьев операция как вставить в середину вершины дисплей дерево оно делается супер тривиально вы просто берете разделяете одну вершину на 2 вставляйте между ними новую все не надо ничего балансировать как только вам по нато какие-то данные внизу дерева дерево сбалансируется самого не нужно писать или лишнего кода не нужно писать лишних багов сжатие сообщение у нас а опять же есть и десятки гигабайт текста на каждом учат энджи не поэтому можем позволить себе охранники это довольно большие вспомогательные структуры данных для сжатия и разархивирования сообщений но есть одна очень важная тонкость что нам нужно уметь и разжимать конкретные отдельные сообщения если мы посмотрим на то как написано типичные архиватор это они используют совершенно другой класс алгоритмов которые пользу тем что часть сортированных данных в отрыве от всех остальных не имеет смысл и поэтому разархивировать и будут только целиком соответственно очень большое количество стандартных алгоритмов жать у нас не применимо а вот алгоритм хаффмана применим но алгоритм хаффмана в лоб это неинтересно можно лучше первый важный эвристик а вытекает из того чтобы опять же а потом с реальным пользовательскими данными у нас есть реальные сообщения и можно заметить что типичные сообщения устроен следующим образом там есть условно можно выделить слова которые чередуются с не словами и к примеру у вас типичное сообщение то слово пробел слова , пробел еще слово . смайлик еще слова и так далее эту информацию можно использовать чтобы улучшить качество сжатия и вторая важная брести к которого можно использовать это то что вероятность встретить какую-то новую букву условная вероятность зависит от того какая буква было предыдущий к примеру в русском языке вы очень редко встретите 2 гласный подряд поэтому наверное у вас предыдущие букв было глаз на то можно следующим глаз на выставку это очень большой кот с 2 согласны случае более частое мнение тоже редки это все можно использовать чтобы улучшить качество сжатия отлично мы вспомнили про деталей из условно самих сообщений остались вспомогательные вещи перес помогать иная вещь это поиск здесь у нас use ranger выступает некоторые про все это как она юзера жену в общем случае ничего не знает про сами тексты сообщений не может искать по ним пусть бывает двух видов бывает пост по всем личным сообщает поиск по конкретному диалогу пусть по всем личным сообщением выполняется как диагональный запросы юзерами жена на все четные же на которых есть и нужные чат и потом и мешать их результаты соответственно поиск в одном частью выполняется еще проще нужно отправить запрос всего лишь на один charger потом просто отфильтровать лишние ничего сложного с точки зрения того что написано в чат инженер опять же там обычно inverted индекс без каких-то особых извращений любой человек который хоть раз открывал к книгу потому как устроена поиск по большому данных на меня в интернете к примеру он с легкостью это напишет никаких потолками тут нету 50 history and paul брезент history это просто список каких-то важных изменений которые есть у пользователя здесь ничего сложного для абсолютно стоит форвард вещи как и собственно long пол это просто вещи нужно не забыть отлично мы вспомнили различные детали давайте теперь подумаем про то что нам мало перелить данные перелив к данных занимает от 3 до 5 дней зависимости от того какие данные мы вытаскиваем о прокате сознательно забываем от того какие мы оптимизацию делаем весь этот процесс у нас занимал от 3 до 5 дней мы его делали несколько десятков раз постепенно допиливать функционал и в течение этого времени нам нужно как-то обрабатывать изменения которые приходят на нужно некоторое проксирование пишущих запросов и так и раньше у нас backend писал в текст engine и в чат members запись в чат момент мы ничего не делаем чат на южных и так подхватывает из-за его для блога а вот запись самих сообщение теперь идет не в текст и женат юзер engine соответствию зерен джиннов первым делом проектируется сообщение в чат начатая жена есть три варианта первый вариант чат сейчас в процессе биржа и здесь должна быть небольшая звездочка в название доклада про down time конкретно в этом месте нас downtime есть и сейчас у вас конкретно сейчас нежиться там и это сообщение отбиваем но мирча то это очень быстрый процесс 100 миллионов сообщений мы бежали за час но это очень маловероятно сценарий даже активные какие-то флудилке где люди ничего не читают а только пишут занимает его ну в районе миллиона сообщений с учетом кратность они множатся в за минуту если же мы говорим о типичных личных переписках то их нужно имеет вообще долю секунды это пользователям заметить очень сложно нам не приходили таких жалоб начну все сделали правильно 2 осмысленных случае таковы если у нас чат либо еще не сложился либо уже съежился и в том и они отличаются только действием самого чат engine а в случае если часы не сможем начать забывает про это сообщение просто отвечать люди ренджи надо хорошо отсылай сообщение дальше если же chat уже был смертин он его запоминает и разошлет потом оставшимся use реншо нам в любом случае он атакует информацию я раньше ну либо информацию о том что делать сообщением что хочешь либо выдача то около иди после этого use ranger прокси рует сообщение в текст engine в ответ ему приходит юзер la cala de который юзере до этого в пуха пышный backend это очень важный момент что мы должны всегда в любой момент времени быть готовым к тому что что-то пойдет не так и нам придется срочно откатиться на старую систему хранения на старый текст engine и в поэтому должны поддерживать данные в текста жив консистентной состояние в частности там должны быть консистентные user local айди отлично мы закончили с режимом проксирование в целом эта схема общее между ушками выглядит как то так теперь какая этого опасались мы сначала подключаем пустой новый класс стр затем юзера печатной же на в чат жена просчитывает весь белок чат мемы затем пускаем проксирование чтобы подхватывать все новые изменения затем запускаемого перелив q данных после того как она через три-пять дней закончится мы получаем полностью два синхронизированных кластера старых и новых движков все на этом мы перелили данные осталось переключить еще нести кстати на no use реджиной отключить проксирование выкинуть старые движки на самом деле на этом слайде описано чуть ли не месяц работы потому что когда вы переключаете чтение текста я же на юзере жданова во первых это делаете не сразу же на всех пользователей а сначала только на сотрудников потом начинаете выкатывать на какой-то процент пользователей и в какой-то момент вам все же присылают какие-то баги либо напрямую либо через поддержку и вы видите что где-то есть проблему нужно все снести начать заново это в целом то что мы делали почти каждую неделю с начала сентября и до первых чисел марта когда мы окончательно переключились мы брали запускали перелив q данных постепенно накатывая на нее новый новый функционал по позже в с режимом проксирование потом уже переводя пользователей на чтение все это мы делали почти каждой недели на протяжении почти полугода . ленте технических вопросов это более менее все на 10 логически вы поняли теперь как это все делать теперь я бы хотел быть немного позаниматься развенчание мифов когда людей спрашивают про то как должна быть устроена какой-то такая большая система люди думают о том что это должны это безумный миллиона строк кода который пишет огромные команды разработчиков но на самом деле чем больше у вас кода это не значит что тем быть что чем больше вас кода тем быстрее он работает скорее всего в нем просто гораздо больше блогов out производительность не обязательно растет на всю эту схему с юзеры джиннами и читать же нас ушло меньше 20 тысяч строк кода всего лишь 20 тысяч здесь не учит анны наши библиотеки для работы с сетью и диском но вся логика там со спреем из-за структур к логарифмам отсортированных массивов с переменными запросами вся логика с учета me long пол апельсин history поиск и так далее все вместе занимает меньше 20 тысяч строк кода и на то чтобы написать эти 20 тысяч строк кода у нас ушло всего 3 разработчика и меньше 10 месяцев работы мы начали начале мая к в первых числах марта мы уже все переключили это мы и периодически отвлекались на другие задачи один из разработчиков на самом деле пришел не в начале мая работа над конкретно этим проектом а в середине июля в общем не нужно писать огромное количество кода огромными командами нужно это делать правильно просто теперь про конкретные результаты которые мы получили результаты бывают двух типов про фичи и про железо профи чем и сразу же улучшили все возможные метрики качества такие как во время ответа и процент повторных запросов первую очередь за счет того что мы вынесли синхронизацию с php кода на движок также мы подняли лимит сообщению пользуясь 10 миллионов до 15 из 10 15 миллионов это на самом деле цифра взяты довольно таки с потолка ну просто нас попросили давайте мы раз уж переехала на движок увеличены ну хорошо давайте увеличим не проблема с поднятием лимита начаты все более интересно раньше 150 человек упирался просто во время работают синхронизации в php коде на большие чат и просто сообщений отправлялась заметно долгое это бы раздражало пользователей мы не делали большие ограничения начато следствие how подняли до 250 на самом деле у нас конкретно в марте этого года еще была некоторая часть связаны с рассылка пушей на прочность стороне но мы над этим тоже работаем мы также автоматически получили гораздо лучше консистентных данных и уже после переезда мы начали с гораздо большей скоростью внедрять новые фичи такие как например поиск по первым сообщением вы просто подумайте насколько сложно его было бы реализовать в старом текст инженер когда у нас нету никакого разумного способа проиндексировать все персоны сообщение потому что текст engine друг с другом никак не связана вся синхронизация в лавках пк и мы даже не знаем как разные копии сообщений у разных пользователей связаны с точки зрения user local айди также есть закрепленное сообщение админ в чате и на самом деле у нас ещё много всего в процессе разработки но что мы ещё не успели выкатить в продакшен вторая часть моя любимая про железо у нас было напомним в начале прошлого года тысячи серверов 96 гигабайтах их не хватало техническая особенность старых движков состояла в том что их удобно раздроблена только в кратное количество раз и в целом мы обычно так и делаем но у не все кластера имеет размер тысячи серверов и делать классов два раза больше чтобы его хватило еще на пару лет не так сложно намного лучше чем париться с тем чтобы как-то рассылать данные и всех старых движков во все новые гораздо проще сделать так чтобы один старый instance раз копировался в несколько новых соответственно альтернатива была либо по 100 тысяч серверов либо же про либо переписать движок и тут важный момент мы переезжали не на тысячу слов она 500 по следующей причине у нас сервера на которых уже крутятся личные сообщения и так в пике загружены до предела мы не можем на них поднять еще копию нового кластера переливать данные по чуть-чуть чтобы чтение работал одновременно собой кустов это какой-то совершенно невозможная задача как мне кажется по крайней уж точно не стоящее того а еще тысячи серверов у нас на самом деле просто не было в тот момент и поэтому нашли 500 серверов и приняли решение приезжайте на 500 серверов тем более что по всем при изначальном прикидкам мы должны были уложиться и так и получилось правда опять же нужно поставить маленькую звёздочку что в телеграмму и сразу поставлю по сто двадцать восемь гигабайт памяти но с другой стороны это перекрывается тем что мою кучу метрик улучшили итак давайте попытаемся все это теперь перевести какие-то деньги делать это можно кучей способов я не претендую на то что мой самый рациональный возьмем просто цены с начала прошлого года когда мы всю задачу планировали тогда один на сервер той конфигурации который мы использовали стоил почти 4 тысячи долларов и его содержание обходил в 30 тысяч рублей в год умножаем на разницу между этими двумя варианта получаем более пяти миллионов 600 тысяч долларов экономии в капитальных расходов и 45 миллионов рублей в год операционных что более чем окупает труд 3 разработчиков чем 10 месяцев и да еще одна звездочка 1 php разработчика который писал backend все равно как мне кажется неплохо на этом у меня все и надеюсь теперь вы будете немного меньше бояться писать это сложные системы не будете думать что нужно обязательно писать огромное количество кода и в целом у нас больше будет появляться интересных с оптимизировать под конкретные задачи более эффективных решений пальцы большая друзья я выяснил что разработчики алисы из индекса приехали чтобы рассказать об этом и мы этим займёмся когда начнется автопати а теперь вопросы то есть буквально через 10 15 минут это произойдет здесь ради алисы можно отложить его на 10 минут вопросы погнали а можно вопрос спасибо за доклад а какой reflection фактор если вот один из 500 серверов умрет то все смотрите есть две возможные репликации сам бывает массе мастер репликация с ней все очень плохо и мы даже не пытались на стадии перееду задуматься об этом изначально репликации у нас фактор фикации был один у сервера на практике лежат не очень часто и стать еще тысячи серверов под горячее реплики мы ну на самом ли просто не могли не мы не так уж и сильно монетизируем ся сейчас с этим все сильно проще с точки технический но мы еще это не реализовали это в планах у скоро будет но что если диски там отвалится или штанину если конкретно с дисками тону ставьте райд и это очень сильно помогает у нашей мы же ведь разрабатываемого соцсеть мы не разрабатываем банковский processing транзакции нам не нужно абсолютно точность нам нужна та точность которая удовлетворит пользователей но при этом которые мы сможем позволить не сдавив их к каким-то громадным количеством рекламы просто если пользователя раз в год не отправится сообщения ему придется там не за пять минут подождать наверно он расстроится меньше чем если мы ему будем давать в два раза больше рекламы нужно находить в это баланс добрый день спасибо за доклад вот здесь вопрос такой как я понял у вас есть напишут есть бинарные логе тардис индексации периодически epilogue актуализирует snapshot добавлять новые сообщения новые данные а вот что произойдет если до переиндексации перезапустится engine получается что он потеряет эти данные о но читать сначала с ним потом подгружать данные сопоставим logo нет никаких потерь глупых нету пожалуйста вот поехали спасибо за раз от вы все меня зовут равиль есть компромисс мой подождите 30 секунд пожалуйста вот там над вами просто есть рука вот поехали так спасибо за доклад ты сказал что считаешь невозможным буферный переезд когда вы используете малое количество серверов и постепенно на них перелазить и я вот посмотрел на математику 500 серверов стоят 2 миллионов долларов они уже были неужели вы не смогли сделать невозможное за 2 миллиона ну вы сделали невозможное , этим мы предлагали сотрудничество телеграмму потому что не хочет схожей технологии они отказались сказали что это слишком сложно так что мой в целом удовлетворены результатом спасибо спасибо ребята задают вопросы приставайте парня здесь у стенки секунду вот здесь здравствуйте у меня два вопроса во-первых вы упомянули зловредные мобильные приложения чаво поддержка старых клиентов стоит много нервов насколько сильные проблемы ну к примеру мой совсем недавно выпили как раз один из образцов который позволил нам сэкономить семь процентов оперативной памяти и на котором жило около миллиона пользователей но они наверное все обновились довольно быстро потому очень старой версии при этом еще ситуаций очень сильно меняется зависимости от iphone и android iphone и гораздо быстрее обновляются андроидов нужные иногда чуть-чуть помогать и второй вопрос вы вначале показывали предсказания как бы что-то на старой системе серверов хватит на год вперед на новой системе сколько время там очень большой простор оптимизации можно за те полгода что прошли их успели вкрутить столько что пока что мы идем гораздо быстрее в плане оптимизации чем растут нагрузки спасибо я здесь у стенки здесь вы сказали вот про сжатия сообщения у меня до этого был вопрос в какой кодировке вы храните и там ну допустим у tf2 байта на русский а если koi8 то один подавляющее большинство текста нас русские поэтому цыпа 1251 так экономичнее нажатии этого то есть я так понял вы там хоффмана используйте то есть по идее кодировка может любая но слушайте акты наверное можно попытаться сделать что-то в другой котировки но я не думаю что это будет сильно лучше а для передачи по сети лучше всегда в 51 так здравствуйте спасибо за доклад вопрос такой вы используете php а не какой-нибудь там высокопроизводительный гол почему вот такая по кендо до на самом деле мы используем си плюс плюс у нас свой компилятор который транслирует php + и и ответим на ваш вопрос еще вопрос потому как вы храните данные это вот бинарный формат это google про табов нет ну просто бинарный формат самописный но зачем для такой мелочи использовавшихся до готовы спасибо спасибо добрый день и спасибо за доклад олег анастасьев одноклассники скажите вот ты упоминал о специальном алгоритмы сжатия для сообщений сколько вот эти все ухищрения далее относительно классического российско но что то в районе 10 15 процентов если его не ошибаюсь по всему вот там сыру спасибо за доклад вы упомянули что сообщение записываете ночью на диске днем они живут в памяти как часто случалось что вы их теряли я сказал не совсем этого мы оптимизируем а хранения данных на диске ночью в бинокль события пишутся сразу же в этом весь смысл берлога спасибо вот привет ты не упомяну рассказал про лол pulling и я хотел узнать почему не websocket и алон пулек спасибо прикольная перегородка просяник еще раз еще раз задать да не сказал про лампу link и я хотел бы узнать почему не websocket алон pulling ну скорее исторически сложилось не хотели делать больше работы во время приезда чем нужна вот эта технология работаем у нее не упирались пока оставили просто может быть изучить basil пожалуйста здравствуйте как вы производите балансировка между тем же нами если я правильно понял вы как-то исторически вас было что там живет читая все понял вопрос на самом деле все очень просто на нас играет условно закон больших чисел что у нас на каждый от той уже сейчас врач 4 миллионов чатов раскидать их просто нужно просто выдавать adige ники так чтобы их хэш был условность случайно распределенный и просто берите по остатку отделениях и шопа кластер союз дальше закон больших чисел все сделать за вас если я правильно понял вы говорили что ты допустим ся учат engine скажем полностью отвалился да просто метеорит попал то есть мы потеряли всю историю сообщений что ним хранилось больше нигде не реплицируется правильно у нас есть backup сорбит лохов нас нет горячих реплик сама история сообщения совсем не потеряется более того за счет кэширования diesel engine на самом деле смог ничего не заметить потому что у них закрышевал сюзерен жениться привет скажи пожалуйста вот берлоге пишите за целый день потом оптимизируете а если вот упали там в конце дня за какое время она все поднимется в память и на катится до того момента как она начнет обслуживать на это очень сильно зависит от движка для конкретно сообщение ну кажется минута что-то такого париками сколько предлогов за день набегает ну на одном сервере я сходу не помню ну типа сотни метров что-то такое так и второй вопрос ты говорил что на время сортировки там миллионного вектора движок встает на единицы миллисекунд на движок однопоточный здесь опять же важно то что у нас очень много данных когда вы используете многопоточность у вас всегда возникают какие-то расходы на сама на самой нужен к этих потоков когда у вас большая часть запросов очень простые это не осмыслена вам не нужно выносить запрос вида достань ее вершинку дерево в отдельный поток вы больше проиграете на этом поэтому у нас большая часть движков однопоточный потому что многопоточном просто не нужно я понял хорошо спасибо привет спасибо за доклад а когда появится кнопка редактировать сообщение что мешает сделать но технически не 6 на появятся когда-нибудь здравствуйте подскажите вот если были какие-то проблемы с cheat engine и в этот момент пользователь пытался написать сообщение при раз клиента то есть сообщения в клиенте записалась но по факту отправиться не смогло и наоборот соответственно сообщение было отправлено пользователя но не до клиента не доехал из этих проблем как эти проблемы решаются насколько со прошлое общение все действительно довольно плохо если лежит в отель jinta не будет подтверждение от него что сообщение записала сегмент вернется ошибкой и это будет как мне доставлено сообщение вы к примеру говорите про клиенты вы наверняка видели такое в красном кружочки белёвская ты знаешь совсем вот будет это просто на самом деле вас не должны очень часто падать базы данных ну эта логика клиента спасибо за доклад вот вообще походу дела возник такой вопрос вы описали кейсы там вот с этим вектором с частично сортированы массивом есть такая структура данных бы дерево с журналом которая реализована почти во всех базах данных то есть вот какой был объектив не использовать куинтин беден супруга бы дерева на дерево это значит структура данных для дискретной оперативной памяти с чего это поиск дольше работает чем же бинарных вы выравниваете страницу на размер кэша процессора услужить и у вас у пользоваться стандартного может быть несколько сотен диалогов какой там выравнивание на страницу где в эту страницу возьми меня странице размер страницы в дереве на потому что вы проводили кен тесты по сравнению допустим скинуть любым embedded ну там там болт db еще и чем любая под база данных которая о сути она реализована и мне просто немного лень кликать в самое начало но вот мы начали кросс маску или и мы как раз когда от него уходили проводили тесты это и были делали еще мои предшественники там очень серьезно вашей производительности он сжигается потому что только мы знаем особенности наших данных только мы можем их максимально эффективно использовать даже несмотря на то что некоммерческий базу данных могут с пользой значительно более сложная структура данных здесь очень важно то что мы знаем особенность обязательно непонятно чем не подошло то есть конкретно в чем в чем был изготовить медленно работы живут больше памяти то мы не тестируем прям совсем все какие-то вещи мы тестировали добрый вечер спасибо за доклад у меня такой вопрос вы сказали что вы потратили 10 месяцев и 3 разработчика а сколько всего разработчиков была потрачена на разработку вконтакте но эта цифра очень сильно меняется со временем я вот пришел в компанию три года назад и тогда у нас было всего что-то меньше 20 разработчиков тех пор мы сильно выросли сейчас у нас около сотни вроде бы конкретно базами данных сейчас занимается 6 человек спасибо за доклад вы сказали что вы сжимаете я здесь не общение по отдельности скажите насколько ваш алгоритм активен на коротких сроках ведь если принимаю правильно большинство сообщения чатах достаточно короткий и планируете ли вы открыть его consort а так пир вопрос про сжать и против и сообщений знаете как работал алгоритм хаффмана ему неважно короткий или длинный сообщением важен общий объем текста мы строим словарь сразу же на все они на конкретные сообщения этом специфика алгоритма хаффмана и на для именно поэтому его и выбрали что он там неважно короткое сообщение длинная что касается open source ну я считаю что это не очень осмысленно по следующей причине у нас все очень сильно завязана на конкретно наше решение они его на админской старания очень сильно используются и человеку в каких-нибудь небольших проектов но не осмысленные поднимать возьмите вы что-то более простое тоже как-нибудь postgres вы на ваших масштабах и какого-то заметного проседание по производительности не по-лу-чит-ся но поддерживать это будет намного проще всего три два один поклон"
}