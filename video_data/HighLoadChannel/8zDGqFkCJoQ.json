{
  "video_id": "8zDGqFkCJoQ",
  "channel": "HighLoadChannel",
  "title": "Yandex Cloud Instance Groups: опыт создания сервиса оркестрации / Василий Бригинец (Яндекс.Облако)",
  "views": 770,
  "duration": 2941,
  "published": "2020-04-14T11:31:32-07:00",
  "text": "да добрый день коллеги привет всем кто смотрит с через экраны мониторов меня зовут василий я работаю в яндекс облаке и моя команда создает инструменты для разработчиков об одном из таких инструментов сервисе instance групп я бы хотел вам сегодня рассказать о том как мы его делали какие желания у нас были какие задачи мы перед собой ставили но перед тем как начать давайте познакомимся с вами примите пожалуйста руки кто вообще хоть раз пользовался каким-либо облачным провайдером довольно много хотя ожидал что будет весь зал а кто пользовался и яндекс облаком хоть раз тоже неплохо спасибо но можете не переживать все что буду рассказывать применимо к любым облачным провайдером любому блоки можно будет строить примерно по тем же технологиям те же с той же архитектурой и в целом же цель моего доклада сегодня за замотивировать вас еще больше использовать облака и выбирать правильные инструменты для построения deployment процессов ну что ж пожалуй начнем доклад мы будет разбит на четыре части сначала я расскажу небольшую историю как мы вообще пришли к этому сервису почему мы решили его делать и как я уже 2 задачи мы для него стояли второй немаловажный момент это архитектура какую мы выбирали архитектуру как мы делали на основе чего принимали решение поговорим как повлиял выбор базы данных на нашу архитектуру но и само собой под конец затронем немножечко про славный на функционал оркестрацию инстансов два года назад в яндекс облаке уже существовала существовала сотни runtime of различных виртуальных машинах и всем этим флотом машин нужно было как-то управлять и в яндексе и в общем то есть сервисы по управление инфраструктурой но мы хотели пользоваться исключительно теми же продуктами чем к которыми пользуются и наши пользователи мы пропагандируем так пудинг то есть стараемся для решения своих задач использовать исключительно свои же инструменты как мне кажется это довольно крутой инстр то я возможность обще в индустрии пользоваться своими продуктами и поэтому мы для нас это был один из немаловажных моментов довольно быстро в общем то мы поддержали основные наши сервисы в терра форме с его помощью довольно легко праве женить инфраструктуру да чё уж там лукавить можно и обновлять или в любой момент инфраструктуру и мы с его помощью настраивали у себя сиди процессы делали безопасный апдейт сервисов без потери связанности без проседания по трафику и вот это требование делать безопасной для нас была основным мы как и любой обычный провайдер в обязаны быть 24 на 7 доступны потому что если какое-то и сервисов будет недоступен в тот момент времени когда вашим приложением нужно будет срочно выкатить hotfix это будет просто файл ну и соответственно мы настраивали все эти процессы мы делали интеграции по слежению за нашими приложениями по реагированию на их недоступность все это делали с какими-то бар скриптами еще какими-то костылями и в общем то довольно быстро поняли что мы так не хотим мы хотим это автоматизировать и самое главное мы не хотим чтобы наши пользователи пострадали также какое-то дело и мы в общем то на основе этого поняли что нужно делать срочно сервис сервис который следит за инфраструктурой сервисы от появился долго уже год назад называется из группы и прежде чем рассказывать про то как мы сделали основной его функционал это безопасно и обновление инстансов под нагрузкой без потери трафика стоит в принципе поговорить о как у нас доставляется трафик от пользователей до наших сервисов но в первую очередь мы не единственный сервис в облаке и логично что мы используем общую инфраструктуру в частности у нас реализован паттерн дизайна пиги твой через который все пользователи до нас попадает конкретно нам как как одному из сервисов облака это дает многофункциональности много возможностей но я бы хотел обратить внимание на 2 которыми мы уже воспользовались и продолжаем пользоваться 1 api guide вей позволяет в любой момент сделать проецирование сервисов можно в любой момент их разделить на несколько либо наоборот объединить да если это требуется и и с группой являются частью компьютер частью большого сервиса компьютер который предоставляет множество различных функционалов там и можно работать сыма джеймса с нам шортами с инстансами его собственный сон с группой в какой-то момент мы поняли что нам проще жить на отдельных серверах на отдельных бы кендо разорвать или наши релизные процесс и потому что где-то мы делаем быстрее где-то они делают быстрее не хотим друг от друга зависеть собственно мы это и сделали пользователи ни один не этого не заметил они как ходили в сервис компьютерные api guide вы и так и продолжают ходить 2 2 неплохая возможность которой мы пользуемся для нас api guide рей является неким фасадом все многие пользователи хотят тот или иной протокол взаимодействия системой и api guide вы сейчас предоставляет просто арест eager песен тогда как внутри облака мы всегда используем только джерси и один и тот же протокол на всех сервисах и нам не нужно думать о том обо всем зоопарке протокола в которой может существовать которые могут потребовать наши пользователи я думаю возможно не стоит рассказывать про важность распределения всех своих сервисов по разным дата-центром по разным зонам доступности но так как это все же влияет на архитектуру и часто очень сильно я бы все же хотел затронуть этот момент в яндексе исторически стараются резервировать все свои сервисы на разных уровнях по разным fall доменом будь то сервера стойки также дата центре логично что и мы распределили по всем возможным зонам доступности свои сервера будь то api guide вы и будь то же конечно конкретной сервисы и что немаловажно конечно же мы а это нам безусловно дает высокая отказоустойчивость о которой я возможно забыл упомянуть для нас критически важно чтобы все наши сервисы были всегда доступны отказоустойчивые поэтому нам необходимо распределять сервисы по разным зонам и что немаловажно внутри зоны внутри зоны всем нашим сервисом мы рекомендуем иметь не менее двух инстансов к в каждом дата-центр в каждой зоне доступности тут можно задать два логичных вопроса почему нужно использовать три зоны они 2 скажем все-таки отказоустойчивость двух зон тоже довольно высокая и почему нужно использовать два инстанции минимально в каждой зоне ответ на первый вопрос довольно прост экономия ресурсов но тут объясню на простом примере если бы моему сервису было достаточно 8 инстансов для обслуживания пиковых нагрузок я бы распределил их по двум зонам то в случае если какая-то зона перестала бы быть доступной вся нагрузка полетела бы на оставшиеся четыре инстанция несмотря на то что выход из строя целой зоны доступности это крайне редкая ситуация и прям маловероятно но тем не менее это происходит буквально четыре дня назад один из крупнейших провайдеров тоже испытывал сложностью своем дата-центре из-за этого были проблемы в крупнейшей социальной сети нашей страны и это прискорбный факт но это да действительно бывает дата-центра выходят из строя не бывают байат и на это нужно закладываться это важный факт так вот если бы мы делали так то мы бы в лучшем случае могли бы деградировать наш сервис в худшем так и полностью его потерять если бы весь трафик полился на оставшиеся четыре инстанции поэтому нам пришлось бы делать в два раза больше инстансов в каждой зоне держать уже по 8 совместно чтобы но можно было обслуживать весь трафик оставшимся одним ин сон сам одной зоны но если добавить еще одну зону то количество инсов требуется уже всего лишь полтора раза больше то есть в нашем примере это было бы 12 ведь если одна из он выйдет из строя то в двух других останется 8 все так просто все логично экономия мне тут очень понравилось как вчера василий пантюхин сказал что это как плата за страховку за строго в ковке мы всегда платим и если оно вам нужно соответственно приходится платить но вот три зоны позволяет немножко сэкономить однако существует автоматическое масштабирование в принципе на него тоже можно полагаться в случае если зона перестанет быть доступны весь трафик перельется в оставшиеся в 1 либо 2 и автоматически и масштабирования будет просто закидывать новых инстансов но тут нужно понимать что трафик то переключается практически мгновенно тогда как инстанции создаются и приложение внутренних стартует уже но за какое то время и в это время вы можете испытывать трудности ваших сервисах и в общем то мы тоже можем и спорт испытывать если бы использовали только автоматическое масштабирование так логично что до для ответа на второй вопрос почему нужно использовать два инстанса в зоне они один стоит разобраться в принципе как устроена нас сеть инстансов у нас много логично что используем сетевые балансировки нагрузки и во всех зонах доступности есть свои балансировки нагрузки которые для каждого сервиса светятся одним и тем же адресом уже во внешней во внешний мир а как по одному адресу попасть в разные места в разные дата-центре но тут нам на помощь приходит всем известные протоколы бюджет и border битвы протокол и и семьи и колкость мультипасс адреса светится с одинаковой стоимостью пути и там уже вся магия равномерно распределяет трафик по всем зонам внутри же балансировщик и распределяет равномерно трафик по по всем инстанциям внутри этой зоны но что будет если в зоне будет 1 инстанции так как анонс анонс адрес этот центр происходит в момент появления первого intenso и снимается в момент когда инсцес зоне пропадает и в целом сам сама снять и анонса и выставление это дело каких-то секунд но тем не менее в это время трафик не будет идти на работающий инстанции или что хуже может наоборот идти в зону в которой нет ни одного intenso да это там какие-то может быть миллисекунды секунды но тем не менее в это время трафик может теряться и если держать зоне хотите хотя бы 2 инстанса то случай если один из нас ушел устроена балансировщик автоматически в зоне переключит весь трафик и потеряется сильно меньше запросов другое дело что мы часто любимом обновлять свои сервисы мы сильно растем быстро и действительно делаем релизы иногда по несколько раз на дню и если бы у нас было по одному ын су мы бы их начинали обновлять в зоне бы выводили из под нагрузки обновили завели обратно перешли ко второй зоне и могло бы казаться так что в одной зоне мы уже считаем что трафик идет а он физически еще не идет во второй зоне мы трафик отключили а и получается весь трафик идет в одну только зону соответственно могли бы здесь испытывать сложности хотя до от этой ситуации можно защищаться если сделать апдейт с помощью расширения группы в расширении ресурсов но в частности добавляем новый incense снова и снова и приложение выводим из под нагрузки 2 и в общем то в в этом случае в зоне всегда будет оставаться хотя бы один работающий инстанции и так все кто ходит к нам все кому ходим мы внутри мы используем как я уже сказала джер pc но мы же не просто какие-то байтики передаем естественно взаимодействуем по некому и пиарю нашем случае это мы выбрали подход ресурсноориентированной то есть все запросы у нас происходит над каким-то объектом в конкретно нашем случае объект это глобальная services группа прославления объект intent группа и по дефолту все операции модифицирующие у нас осин в имеет асинхронную сигнатуру то есть на любой запрос модифицирующие мы возвращаем объекты першин за которым уже клиентский код начинает следить как-то его трек от ждет пока он выполнится и делаем мы это даже если предполагаем что что модифицирующие объект будет исполняться моментально это дает нам возможность любой момент изменить такое поведение и конкретно в нашем сервисе мы уже воспользовались однажды такой возможностью поначалу мы на запросы кредиты апдейт возвращались сразу операцию статусе дан мы считали что ну вот мы уже получили спецификацию от пользователей и в общем то так как incense группа это такой живой объект который всегда находится в процессе дипломанта мы всегда следим за его живостью там если что-то упадёт восстанавливаем авто скейлер может менять размер кластера то есть объект живой и мы считали что до всем успеху получили и будем просто следить за тем чтобы группа пришла к этому состоянию передавая возможность пользователю понять когда для него уже самому важно понимать что группа в крутом или ином состоянии и да этот подход вполне имеет право на жизнь но некоторые пользователи в частности пользователи тира форма начали страдать от такого поведения они хотели завязываться на то чтобы когда операция уже выполнилась в группе были созданы все инстанции которые они запросили или обновлены и как-то их использовать уже в своих дальнейших автоматизация хто дальнейших процессах тут мы довольно быстро это поняли изменили но тем не менее кстати возможно кто-то из вас даже это поведение застал мы довольно быстро it изменили но что самое главное не один пользовательский код не заметил этого изменения они как получали объект операцию так и продолжили получать как следили за ее ходом выполнения так и продолжили следить единство что теперь сама операция стала выполняться вроде бы чуть чуть подольше ну так вот мы получаем модифицирующие запрос на создание на апдейт сохраняем базы данных этот объект операции запускаем какие-то дипломант процессы и мы стараемся делать свои сервисы так чтобы весь расчет происходил исключительно настоятель нас инстанциях то есть мы никакой стоит внутренне засов не храним то что понимаем что в любой момент времени любая железка может в общем то выйти из строя захватив с собой и наши виртуальным машинам ну а весь стоит мы храним соответствие само собой в базе данных оно помогает нам распределять работу и синхронизировать его во всех зонах доступности и буквально вчера мой коллега семён чичерин до рассказывал про эту базу данных про которую мы выбрали это индекс nps а поднимите пожалуйста руки кто присутствовал на его докладе о круто все из вас поняли что такое таблетка понятно ну окей смотрите для тех кто не был индекс дербейс это не sql база данных которая отказоустойчивая катастрофа устойчивый автоматически восстанавливается от любых сбоев на любых своих уровнях она автоматически реплицируют данные она автоматически перекодируют даны перефразирует таблицы случае если это необходимо она поддерживает осетра требования сид на транзакции причем уровень изоляции самый высокий сериала и забыл у нее есть масштабируемость по кнопке в любой момент времени вы можете изменить ваш вычислительные ресурсы причем хороший факт что индекс nps используют для оркестра ции масштабирования сервис енс групп мы их используем их они нас хороший синергия что я ну так вот но почему мы выбрали именно ее казалось бы в яндекс облаке существуют несколько решений по менеджерам данных их довольно много можно по кнопке зависти и пользоваться но а как вы думаете какой бы нам стоило все базы данных за использовать можете предположить вот что у нас существует что могли бы за использовать кассандра вроде у нас нет хотя да наверное могли бы за использовать но в общем то могут подсказать у нас чаще всего используются под колеса и манга по факту реляционная база данных либо но и сколь база данных и да мы могли бы их за использовать но если бы мы использовали реляционные даже не так если бы все наши таблицы были бы одна шарда вы и то в общем то да это был бы хороший выбор все современные базы данных сегодня довольно отказоустойчивые все эти вот мастер слоев системы они тоже справляются вроде бы собой с любым видам нагрузки до чего что миллионы сервисов даже у труден об отдыхе в том числе в яндексе но у нас любой сервис мечтает вырасти многомиллионную аудиторию на мы не исключение я так мечтаю что мы им пользуемся сервисом пользователь пользовались даже есть нужно создать одну виртуальную машину и в самом начале мы уже знали что нам придется свои данные шарнира ватель свои таблицы сортировать а это в общем случае больно и сложно да многие команды это решает да и то есть много гайдлайн of как это делать но мы не хотели тратить свои ресурсы на развитие и поддержку их то баз данных или еще другой инфраструктуры мы хотели писать свой продукт исключительно другой вариант носку или мы тоже отмели довольно быстро в общем-то у нас quelle как правило есть проблем не совсем проблема но есть сложности с консистентной стью она у них и вершили а нам критически важно во всех зонах доступности в любой момент времени иметь последнее согласованные данные и но все-таки мы работаем наш сервис сам по себе глобальный но работаем мы зональными ресурсами и в каждой зоне мы должны следить за вот общем стоит им всегда в один иметь его всегда согласованным другой важный момент что но искры как правило не поддерживают схемы данных и в них нет возможности работать через sql так что в общем то у нас практически не было выбора не успел наше все и в данном случае в яндексе есть только одна насколько знание sql база данных которая предоставляется по кнопке но что стоит учесть вам и с чем столкнулись мы в процессе ее эксплуатации в первую очередь индекс n bass предоставляет самый высокий уровень изоляции транзакций сериала и забу достигается это в числе прочего путем линия реализации выполнения запросов на чердак соответственно вам нужно учитывать и нам пришлось учитывать это поведение и не делать какие-то сложные запросы которые учитывают много данных там tuscan таблиц петабайты данных пытаться прочитать просто по факту эти запросы стерилизуются и начинают исполняться последовательно наши водах а это логично что вызывается задержки задержки на сервисов задержки на запросах но что самое страшное в этом случае могут происходить такие ситуации когда параллельные транзакции начнут модифицировать эти данные и когда в тому моменту когда вы их дочитаете они уже инвалиде руются и вам придется их читать обратно хотя до можно просто понизить уровень изоляции и вроде бы прочитать эти данные но все равно для чтения больших данных я бы рекомендовал использовать не из quelle не лайку или в нашем случае точнее и 5 герпесе герпесе метод retail который по факту это джипси стрим который вычитывает snapshot таблица на какой то момент времени и отдает его вам само собой данные в этой таблице будут консистентной полностью и вы можете вычитать хоть все свои петабайт и более того если поймете что в какой то момент времени все вам хватит вам достаточно просто скажите базе данных все я закрываю этот connect и оно больше не будет нагружаться почем зря и выполняет там чтение как это может еще запросы под собой другой немаловажный момент все модифицирующие все модифицирующие запросы стоят оставлять наконец транзакции связано это с тем что во время выполнения транзакции вы не будете видеть свои изменения и база данных здесь файл hosts там позволяет вам сообщает вам о том что вы пытаетесь прочитать данные которые возможно уже модифицировались тем самым заставляя явно понимать что не стоит так делать и как поступили здесь мы в своем бизнес коде мы выполняем внутри запросы запроса выполняя внутри модификации в разном порядке вид запись рид запись но все модифицирующие запросы мы складируем в кошек и уже когда делаем commit последовательно отправляем в базу данных все эти запросы так довольно просто и еще один факт который выливается из предыдущего нельзя делать два последовательно enter the в одну и ту же таблицу даже не два в принципе больше одного ну связано это с тем что insert по факту это чтение + запись ну чтение потому что нужно удостовериться что вставляем ее данные они уникальны и в момент когда вы будете выполнять вторую такой же insert в ту же таблицу все чтение уже будет не консистентная но тут решение довольно простое нужно просто объединять в один insert все все все ваши все ваши вставки просто передавая внутри велась все нужные значения и мы здесь делали тоже довольно просто когда класс складируем в кэш наши модифицирующие запросы мы смотрим если обращение идёт в одну и ту же таблицу если это insert моего объединяя другое решение тоже довольно простое можно использовать литерал absurd это ее яко или или керол и по факту это просто слепая вставка она либо добавить новую строку либо обновит уже существующую если вы знаете наверняка что у вас точно этих данных в базе данных нет или вы не боитесь их затереть то просто вставляете через absurd еще но это еще и должны она будет быстрее исполняться ну и последний момент на который я бы хотела обратить внимание он в принципе относится не только к yandex bass но и ко всем базам данных которые работают с оптимистичными блокировками и работают несчастным сотами в своей транзакции индекс дербейс гарантирует consistent ность только после выполнения коммита то есть во время выполнения самой транзакции данные могут из разных таблиц быть не консистентными пример вы читаете данные из одной таблицы получаете от туда ссылку на вторую таблицу пытаетесь порезал ведь эти данные и кто-то в параллельной транзакции за к метил изменения удалив вообще эти данные в некоторым коде это может быть проблемой например java вполне можно половить на ул pointer exception в этом в этой ситуации и это нужно учитывать сразу закладывать в своей архитектуре по заставлять разработчиков больше думать писав правильный код но можно пойти и другим путем мы его в числе прочего тоже у себя поддержали если мы ловим какую-то ошибку во время выполнения транзакции and при этом как stack trace можно не не structor с нами тем не менее обращение по индексу которого не существует то мы отбрасываем все свои изменения из кэша мы его просто удаляем и отправляем к иметь базу данных коммент мы тут отправляем на запросы чтения поэтому мы только проверяем консистентной прочитанных данных тем самым и убеждаемся данный консистентной или нет если они не консистентные повторяем все транзакции если консистентная то загораем alert будем разработчика отправляем ученики ну что-же перейдём к основному нашу нашему функционал оркестрация инстансами в целом мы работаем со многими in some они все могут находиться в разных статусах в любой момент времени над ними могут происходить множество различных процессов нам пользователь может прислать опадает спецификации of the healing может сработать и сказать что нужно срочно починить какой то instance of the skill может изменить размер кластера роль бекки опять же изменения размеров от пользователя и нам пришлось учесть все эти состояния все эти переходы статусов в своих процессах по первости мы пытались пойти по пути даже не так мы вдохновлялись идеями тира форма мы пытались строить план и пытаться напитались по нему двигаться но ещё до того как на зарелизили свой продукт мы поняли что это довольно сложно исполнять придется либо учиться делать откат этих планов процессе когда нам прислали новую спецификацию нужно будет понимать что иногда нужно довести наоборот до конца потому что это тупо быстрее чем чем делать разбег все транзакции всей бизнес транзакции конечно и довольно быстро мы перешли к модели когда наш диплом процесс переводит группу инсов и состояния состояния б путем изменения путем инкремент нова изменения состоянии с учетом заданных пользователем ограничений ну то есть если мы сейчас находимся состоянии остановки инстанса как только и солнце реально становится только в этот момент мы будем принимать решение что же делать дальше обновлять instance удалять его или стартовать по новой таким образом в любой момент времени мы как только получаем новую спецификацию сохраняемые в базу данных и в любой из нот которая будет исполняться в любом дата-центре мы в момент когда состояние будет меняться у какого-то объекта мы будем проверять уже а куда его нужно дальше двигать таким образом группа движется максимально оптимальным путём к желаемому состоянию но такое поведение может быть не всегда очевидно в частности можно рассмотреть такой простой пример допустим у нас есть трин солнца мы хотим их обновить и в процессе сделать им узкий лаут ну допустим авто схемам сказал нужно увеличить срочно до четырех intenser если задать ограничение макса новеллы был ноль и макс extension 1 то есть мы говорим что мы хотим чтобы ни одного intenso в кластере не было неработающего и разрешаем из создавать новый интент во время де playman процесса то в этом случае декло инкремент лдпр так мы его называем сначала создаст 2 инстанса почему 2 но думаю понятно потому что желаемое состояние 400 и мы разрешаем увеличить до 1 то есть в общем случае до 5 создаем два инстанса после этого переходим к обновлению 1 к обновлению к другого и только после этого уже к удалению 3 то есть по факту мы создадим два обновим 2 и 1 month и один удалим хотя казалось бы да можно было бы создать один и создать 4 но этот процесс будет сходиться сильно быстрее чем если бы мы заранее строили планы двигались бы по нему более того в процессе создания какого-нибудь солнца можно вообще зависнуть да если например вдруг кто-то закончилась и наш то процесс будет конечно не травить попытку создать эти ресурсы но если бы мы двигались по плану этот бы план тупо завис другой пример те же условия но макса новеллы был один макс меньше 0 то есть мы здесь хотим сделать rolling апдейт разрешаем наоборот выводить из под нагрузки один instance и не разрешаем расширять наш кластер мы сами любим этот паттерн мы сами свои сервера всегда обновляем рынком дей там мы как-то не знаю пригибаем что ли копий адресом их именам инстансов и и любим общем-то сохранять нас в конторе дестких логе если бы делали макс extension to incense с этими логами можно было бы то что потерять но там он был пропал но как если мы используем стел сен-санс и поэтому не боимся в общем-то и такого поведения однако rolling апдейт в этом случае incremental диплом сначала видит что нужно увеличить кластер до четырех желаемого состояния 4 1 не хватает тут же начинает создавать как только создал переходит к обновлению 1 2 если в процессе что-то идет не так он deploy процесс останавливает и сначала пытается восстановить его работоспособность только после этого переходит уже к по продвижению по тёплую жизненный цикл самого incense а тут довольно прост как только мы создаем intent мы ждем чтобы у него внутри приложения стартанула сказав через хавчик что я работаю после того как мы дождались этого передаем информацию сетевой балансировщик о том что instance заработал он там сам ждет своего холсте к он немножко более агрессивных чаще проверяется поэтому все-таки есть рекомендации использовать для целей ожидания старта приложения и подачи подачи балансировщика нагрузки использовать разных расчёте но тем ни менее дожидается балансировщик говорит нам что все окей трафик пошел можете считать что этот intent работающий и мы действительно считаем следим за его жизнью проверяя уже свой ковчег и если в какой-то момент времени мы видим что нужно похитить из нас потому что хавчик перестал отвечать или понимаем что пользователь прислал нам новую спецификацию и горит нужно обновить или авто скейлер сказал что нужно удалить этот intent мы начинаем грейс особенно его останавливать также передаем информацию балансировщик берем что все этот intent больше не нужно поливать трафиком после этого отправляем сюда он сигналов в виртуальную машину ждем пока приложение становится внутри инстанса и переходим к стопу к состоянию стоп после которого тоже как я уже grill принимаем решением куда двигаться дальше но я не стал на этой диаграмме показывать все возможные переходы из состояния в состояние это было бы она просто каша однако нужно понимать что если мы сейчас checking хелси и после того как проверили что приложение работает и понимаем что все этот инцес не нужен мы не пойдем по всей этой схеме мы перейдем сякко перейдем сразу к остановке нсо точно так же если нам пользователь прислал стеку мы начали останавливать трафик остановили трафик и в этот момент нам пользователь прислал предыдущую спецификацию он хочет сделать rollback то мы не начнем устанавливать мы просто опять обратно включим весь трафик и instance будет работать как ни в чем не бывало ну что ж немножечко подытожим мы для себя ставили цели в первую очередь высокая доступность и высокой отказоустойчивости наших приложений мы этим как я уже сказал достигаем с помощью распределения всех наших инстансов по разным зонам по нескольку штук мне здесь очень понравилось как вчера сказал коллега из рядовые с василий пантюхин это покупка страховки за то чтобы сервис сервис и работали надежно мы хотели получить безопасное обновление своих приложений без потери трафика я считаю мы этого добились наш сервис позволяет это делать в автоматическом режиме полностью мы просто отправляем спецификации и даже не практически не смотрим обновилась она или нет дальше смотрим просто по мониторингом и полер там что все хорошо мы хотели наш сервис глобальный мы хотели работать с зональными ресурсами мы поэтому хотели распределенной транзакции мы этого получили с помощью выбора соответствующей базы данных которые по факту создается для нас одной командой и в общем то мы не знаем бед мы не задумываемся о том когда нам нужно данное пересортировать мы не задумываемся о том чтобы их там как-то реплицировать или вообще как-то настраивать мы просто хотим чтобы на взяли на ручке и вот говорили все хорошо пользуйтесь пишите свой продукт с нами дальше мы хотели получить декларативное управление инфраструктурой мы закладывали это самом начале в свою и пьянь мы это поддержали сделали теперь мы всю инфраструктуру всей инфраструктуры управляем исключительно через спецификацию причем старались делать это так чтобы можно было минимальным количеством строк описать крупные отказоустойчивых available сервис и собственно получили возможность хранить спецификацию в детей в для любых других репозиториях а тут соответственно и свои плюсы типа можно проверять перед тем как изменить инфраструктуру чтобы несколько глаз проверили эти изменения безопасникам это прямо особенно сильно нравится мы можем любой момент времени посмотреть историю откатиться и довольно много других преимуществ иметь и от того чтобы хранить описание инфраструктуре в репозитории ну и в конце концов мы хотели писать код а не заниматься настройкой инфраструктуры мы это получили мы используем максимально все сервисы которые можем использовать в яндекс облаке будь то сервисы компьютер и в создании инстансов до создания имиджей будь то балансировщика нагрузки будь то сети будь то индекс nps как я уже говорил и прочее прочее прочее ну а закончить бы я хотел небольшим призывом коллеги пожалуйста пользуйтесь сервисами по управлению виртуальными машинами они есть у всех облачных провайдеров сами по себе эти сервисы не стоит ничего но дает вам дают вам множество возможностей дают вам возможность проще делать ваш де пойман процесса но и присоединяйтесь в наше сообщество мы всегда смотрим и следим за тем что наши пользователи говорят о том какие у них есть более какие есть желания мы приоритизирует на основе этого свои задачи и реализуем в первую очередь то что действительно по хочет комьюнити но в общем то я готов буду ответить в кулуарах на любые вопросы послушать про любую боль которую вы испытываете и сейчас ответить на все вопросы спасибо а теперь все желающие смогут задать вопросы василию но в рамках времени которого у нас сейчас довольно много как мы как происходит процесс ответ на вопросов вы поднимаете руку он мог вам подходит галимов ту надо его микрофон и вы задаете вопрос вот вижу камеру руку первую рука была моя простите луценко станислав в megagroup скажите пожалуйста вопрос такой да спасибо большое за доклад и вы говорили про оптимизацию что пользователь скажем хочет school in сделать и вернее скилов я чувствую так но расширится в общем говоря и хочет обновить конфигурацию и вопрос такой у вас работает какой-то алгоритм который оптимизируют вероятные комбинации этих действий или вы каким-то образом но в корму вас есть набор там plate of до которые уже в ходе забита заранее спасибо за вопрос данислав смотрите мы действительно в кластер если будет там 100 виртуальных машин и пользователь пришлет одну спецификацию потом другую спецификацию 3 то теоретически flattery может образоваться несколько версий но мы всё равно всегда стремимся к самой последней и как я уже говорил мы на первую очередь само собой стараемся приводить к к новой версии самые так скажем самые недавно созданные инстанции если мы у нас есть кластер уже давно работающие мы начали обновлять на одну версию один из нас потом на и приходит другая спецификации мы сначала все же его обновим и потом перейдём уже к остальным и в этом случае стараемся свести к минимуму то количество вариантов которые системе да просто дело в том что у вас могут добавляться новые операции и вы соответственно под новые операции тоже заново начинаете писать что возможно да конечно если у нас появляются новые интеграции какие-то новые статусы появляются естественно мы стараемся да не стараемся учитываемых в своих процессах своих дипломант процессах это довольно огромная стоит машина которую приходится поддерживать это то можно антон для ног java программист и сбербанка василий большое спасибо за доклад я бы хотел спросить по поводу раскатки приложения вторая стратегия low насколько понимают экономичный способ вы показали что у нас сервис какой-то момент был недоступен версия 2 но потом мы поборолись с этой проблемой и успешно раз катались а можете раскатать извинять рассказать что будет при раскатке когда у нас мы не смогли все-таки побороть эту ошибку и надо таким образом переходить обратно на первую версию но смотрите во первых могу и сделать и рассказать спасибо запрос во первых это не канареечно тепло и был я не показал к наречие тёплую сразу расскажу что канареечно это когда мы например можно создать другую sense группу в нее на нее подать трафик с того же балансировщика посмотреть что все хорошо и уже потом переключить и весь трафик на только на нее при rolling апдейте мы действительно если движемся по дипломату и например на первом же инстанс или на каком то по середине останавливаемся потому что приложение не стартанула хавчик не прохаживался то тёплым процесс останавливается и мы действительно ждем либо пока там приложение сама стартанет его пытаться может быть кирич в случае если of the header до сказал что китай эти проблемы но тут уже тут нужно быть конечно реакция пользователя он должен нам явно сказать что хочу откатиться вот на предыдущую версию или прислать еще другую спецификацию на которую мы начнем уже переводить все эти сервисы как то так я правильно понимаю что если пользователь с реакции запоздал то вы будете пытаться доставить вторую версию до тех пор пока он вам не скажет явно откати надо именно так то есть если у нас проходят все хасеке и функциональный и балансировщик нам сказал что хорошо но мы не можем знать что реальное приложение там они работает это уже нужно смотреть за мониторингом и какими-то лентами как вариант может быть однажды и на это тоже завяжемся вполне хорошее было бы решение спасибо антон георгий разработчика додо пицца у меня такой вопрос мне показалось что вы объясняли что state меняется для по очереди например для какой-то одной машины в кластере потом для другой для 3 если кластер большой как это масштабируется влияет лет на latency все их ну там какой-то последовательности транзакций больше как это решается так возможно не до конца понял вопрос но . если вменяется за один раз только одна машина и пока этот стоит не поменяется не делается следующий пернатый правильно да конечно вот ну а как тогда если у нас операцию достаточно большая масштабные но смотрите по ограничения нам задает пользователь то есть можно макса новелла паука за десять до если у вас 100 машин классе лета мы будем сразу по 10 но не по десятке в любой момент времени 10 машин будут обновляться все обновилось одна из них мы перешли к следующей и следующий следующий если все 10 она одновременно обновились перейдем соответственно к следующей десятки в этом случае теплей будет сходиться быстрее но если все 10 и не обновились отвесно мы не перейдем к обновлению следующих если я правильно понял вопрос да понял спасибо георгий ещё вопросы есть даже 2 здравствуйте спасибо большое за доклад хотел уточнить а на каком языке ваш сервис разрабатывается насколько этой идеи но мы конкретно мой сервис в основном на джаве пишем хотя есть различные подсистемы как с которым которые мы то я также пишем поддерживаем там у нас есть и г.и. да вроде все дорогу и второй вопрос небольшое недопонимание возникла я к сожалению не было вчера на докладе по yandex do the bass но возможно вы сможете ответить вот на слайде на котором вы описывали нюансы работы с яндекс это bass указан уровень заняться сера сириал ой забыл вот но при этом вы рекомендуете сначала почитать потом пописать ну то есть это получается даже не рипит и был рид смотрите что-то недопонял тут где-то семья нарядил мгновенно ответить но в общем случае сериала и забил в том плане что вы не увидите повторных действительно чтение вы увидите изменение из других транзакций точнее не так если данный кто-то изменит а не провали и про инвалиде руются и накормить и вам база данных скажет о вы читали не консистентной на моём комменте да но это ведь речь не про изоляцию это речь про consistency то есть когда вы будете коммитить база данных вам с гарантируют консистенция вот а когда вы говорите про изоляцию это вот как раз что вы его в своей транзакции можете ну допустим повторное чтение делать или перемешивать обновлению смотрите сделать повторное чтение кажется у нас можно все-таки и вы прочитаете гарантированно те же данные если их кто-то в другой транзакции изменит они про инвалиде руются там по месяца что эти данные изменились но тем ни менее прочитайте вы все равно те же самые данные которые читали до этого с другой стороны вы не увидите я так понимаю но как минимум точно не увидите еще не закончены и транзакции само собой если какие-то транзакции изменили но как я говорил вы можете не консистенции получить здесь только если вы одновременно читаете из разных таблиц которые в параллели карты видоизменились сначала прочитали одну кто-то в пролезет изменил базы до таблицу вторую и читаете и тут теоретически может быть не континент ность но сама база опять же вам скажет о том что в процессе вашей транзакции данные кто-то менял да даже до того как вы их прочитали во время коммита спасибо но тут оптимистичные блокировки это довольно хороший удар патент разговор на душу и довольно дешево да если нет больших каких-то вставок изменений то это прям вообще супер круто спасибо спасибо начал он вопрос был спасибо за доклад вопрос на одном из первых латов по была битва империй да и там были эти компоненты все собственно сервиса с правой стороны если сервис один из другой нужно поговорить он говорит через тоже не смотря на то что да это можно было бы так делать и в других компаниях я знаю действительно все используют всегда один и тот же я пьян и пеги твое прошу прощения но мы ходим всегда через балансировщик конкретного сервиса уже к нему в данном случае нам приходится конечно настраивать для каждой интеграции свои адреса но у нас вся инфраструктура хранится примерно в одном месте и соответственно если вдруг у сервиса изменится и вот am dm e7 а то он изменит это автоматический у всех сервисов вот с другой же стороны ну да вроде на сапоги твой в итоге и изнутри никто не использую то есть он просто фасад уже для конечных пользователей спасибо спасибо оон еще как минимум с это странно как-то право право для меня часть более активно чем можно еще раз по поводу индекс зато без вот мы передали какой класса систем на используется например там банковские системы это дно как бы файлов немножко другое вот это первый вопрос 2 трансакция начинается какой момент это второй вопрос и третье сверла и забыл говорит о том что если читая данный за одной таблице 2 читаю я получаю как бы данной консистенции вот а здесь получается что я узнаю об этом только в конце если например я камин беллы туда а если я например отправляет ударный куда то еще но смотрите давайте отвечу последовательно первый вопрос для каких подходит больше видов нагрузок клеток правильно понял но глаз система систем базового сама база данных архиерей архитектура изначально составлялась под лтп и olap нагрузки различные но сейчас насколько знаю в основном подарил т.п. используется хотя возможно внутри команд там как-то используют аналитику какую-то на основе инбис транзакции соответственно даже в банковской сфере здесь вполне можно использовать во время исполнения самой транзакций как только вы закончитесь вы будете уверены что вы полностью согласованы консистентной по всем и во всех вообще шар дах которых вы будете уже во всех и дата-центров которых будете обращаться вы будете видеть одни и те же данные касательно того что вы можете читать из двух табличек и они теоретически могут быть не консистентной но не стоит отправлять все таки и во время выполнения транзакции данные куда-то наружу или вообще делать какие-то вызовы во внешней системы тут лучше сначала все-таки сделать какой-то транзакцию сказать что я взял эту работу начал ее делать там прочитали все данные которые нужны как-то их проанализировали отправили в систему сохранили что-то и так далее ну то есть логическая транзакция в базу данных должна отличаться от бизнес транзакции так или иначе вот и еще какой-то был вопрос сначала транзакции начала транзакции есть разные способы мы используем грп цепи индекс n bass у них есть как метод begintransaction и соответственно commit есть возможность прилепить к мид флаг к любому запросу и соответственно begintransaction к любому запросу когда мы выполняем и например на уровне изоляции там readonly мы можем просто отправить один запрос который сразу будет off to commit и но он и начнет транзакций и закончить ее сразу тут разные схемы можно использовать то есть я могу отправить начала транзакции сделать select 1 добиться сделать rect из другой таблицы потом поправить даны в 3 таблицы сделать комет ну скажи что я на втором select и например читал да ну никак не не нет вы когда их читали на тот момент они еще скорее всего могли быть консистентными но до теоретически они могут как правильно сказать но вы действительно только на комете может вы можете быть уверены в том что они консистентной однако если база данных сама видит что те данные которые вы сейчас читаете уже не консистентные по отношению к чему-то другому то она вам скажет не накормите а на самом запросе о том что он инвалид инвалид не инвалид как optimistic лог exception вот поэтому да на этом наши вопросы заканчиваются 8 для вас небольшое задание astra лица конференции чтобы догнать и кобра лучший вопрос а может никто не поможет подскажет их было много и традиционно помнится только самые последние которые задаются здание это сложно может быть проще давайте те кто то вопрос поднимут руку но вот на самом деле коллега мне кажется задал сразу trip вопроса и все более-менее несмотря на то что не совсем про мой сервис но были довольно интересное было интересно отвечать а я к сожаление слева жива и меня коллега который хорошо тогда это это вам быть ребята подойти небольшая книжечка и сувенирной продукции нашего партнера wargaming но и васильева где большая награда тоже от наши конференции highload за то что вы к нам приехали уступили appartement пожалуйста спасибо"
}