{
  "video_id": "wEdb5rzQQxw",
  "channel": "HighLoadChannel",
  "title": "Как HeadHunter удалось безопасно нарушить RFC 793 / Андрей Шорин (HeadHunter)",
  "views": 564,
  "duration": 2468,
  "published": "2017-04-22T14:48:25-07:00",
  "text": "здравствуйте я расскажу как балансировать балансировщик на коммутаторах ночного первых с того как у эволюции проходит проект зачем вообще нужен балансировщик это очень краткий такой курс и когда он становится нужен что происходит потом по мере того как растет проект и приходится балансировать уже балансировщик и между собой чтобы они выдерживали нагрузку и что делать дальше если и этого тоже будет мало во вторых я расскажу как собственно устроена балансировка на коммутаторах и обратить внимание что сети такого объяснить не найдете по крайней мере такого когда ю я для джунипер совершенно точно что в интернете не было готового рецепта чтобы нам это сделать это будет уникальная рассказ и в третьих я расскажу о том как мы гонялись за ошибкой сети которая мучил нас три месяца и стоил нам целого часа downtime а в процессе борьбы с ней это тоже чего пришли в начале проект умещается на один сервер да то есть вот этом месте но когда вы привлекаете туда первых пользователей мощности одного сервера хватать перестает вы начинаете архитектурно выделять какие-то части front and back and выполнения задач сервис отложенного выполнение задач которые не нужно делать тут же пока пользователь смотрит у вас становится несколько сервисов и вы их вы носите на отдельные виртуалке на отдельной железяке они там кроется по мере того как проект набирает популярность получается так что какой-то из этих серверов уже не справляется с нагрузкой не как и чем делить его еще мельче на кусочки вы понимать что выгоднее по ресурсам просто-напросто закидать железом поставить второй такой же сервер и они вдвоем будут в двойную нагрузку выдерживать вот тут и нужен балансировщик кроме этого на этом этапе нужно позаботиться о том чтобы эти два сервера имели одинаковое представление о состоянии клиента но то есть один сервер помнит о том какие языковые предпочтения у человека что у нее в корзине 2 не знает ну то это не консистентной какой-то опыт на сайте будет ну что я говорю вы об этом позаботились раньше еще когда архитектуру я уже конечно да то есть вот эта вот возможность выносить сессию то что называется наружу она уже есть и в конце концов проект развивается он получает все больше больше пользователей там тысячи какие-то запросов в секунду и теперь уже балансировщик не справляется потому что сервисов много добавляется биллинг добавляется баннер к манер нужно таргетировать выбрать по каким-то критериям по в том числе по оплате показать его посчитать его показ возникает отдельно сервис или бэк-офис отдельный сервис для применения переводов отдельный сервис шаблонизатор на в конце концов из кнопка выяснить кто к нам пришел до сервис сессий пользовательский появляется сервис компенсирующий архитектурный не даст обязательно такой появляется вот и в какой-то момент вы не эти вот запросов на то чтобы одну страничку показать на сайте нужно примерно в 5 в 10 раз больше на внутреннем балансировщик и потому что чтобы показать одну страничку нужно очень много всего поделать внутри head hunter а если посмотреть запросов на тех джон иксах которые смотрят в мир по сравнению с теми engine иксами которые обслуживает балансировщик в 10 раз меньше приблизительно у нас этого так устроена их становится все больше и больше и в какой-то момент балансировщика не хватает мощности что это может быть это может быть пропускная способность сети но не умещается трафика в 1 гигабит и это может быть циpкa о чем я говорю на наших внутренних балансировщик ах общий трафик отдачу полтора гигабита в секунду при том что весь сайт наружу вместе со статикой почти умещается в 1 гигабит а циpкa ну какое-то большое количество цифр дают прерывания и хорошо что msx раскидывает эти прерывания по ядрам процессора потому что однажды мы просто уперлись то что одно ядро полностью забита обработкой прерываний а сетевой карты мы это разобрали с помощью и mosaics но дальше существует engine x он отжирает левину и процесса 10 раз меньше берет на себя агент мониторинга он парсит логе в real time мест значит 23 тысячи запросов в секунду вы видели он их разбирать с помощью ребят спав и отправляет метрики на сервер мониторинга чтобы мы могли видеть чтобы мы могли ориентироваться в том что происходит еще чуть меньше берут c пуха прокси и r syslog д в нашем случае версий слог д во-первых пишут логи которым налог аллаха ступаю hype принял от engine.exe потому что индекс пишет логина диск плохо это влияет на его отзывчивость по сети мы отсылаем поедите на свой на локалхосте словно принимать пишут локально и во-вторых отсылает их на сервер локинга где разработчики могут разбираться в этих лохов чтобы искать причины инцидентов и вот при нашей сегодняшней нагрузки внутренние балансировщик и способны почти полностью загрузить сиона 3 процессора xeon и 5650 с частотой 267 гигагерца и 12 ядрами без гипертрейдинг если считает copper 3124 значит чтобы чувствовать себя спокойно мы ставим 10 таких процессоров то есть на 30 процентов примерно каждый из этих процессоров загружены он работают балансировщика пять серверов по два процессора это наши внутренние балансировщика и мы их балансируем на коммутаторе с этой картинкой мы будем работать слева сервер который обращается к некому сервису сервис справа представлен двумя до серверами и вот коммутатор между ними чтобы все это сделать а проект живет в какой-то своей сети 192 168 1/2 4 адрес балансировщика который прописан в конфигурационных файлов сервиса мы указываем из другой сети 10 111 соответственно он не летит напрямую на какой-то сер он летит на маршрутизатор вот так он летит на маршрутизатор потому что чужая сеть есть какой шлюз по умолчанию или он явно прописан и маршрутизатор этот пакет perego пускает и направляет куда-то и представьте что на маршрутизаторе есть статический маршрут и в нем в качестве следующих шлюзов через который будет доступен 10 111 ук а за на наши два сервера то есть эти два сервера шлюзы на самом деле с точки зрения мешать и за трон на них может посылать пакеты если указать обоих и одинаковую метрику то в целом понятно что примерно одинаковое количество пакетов должно достаться одному серверу другому серверу есть этим я могу и и как-то другими словами сказать смысл том что мы привыкли до обычно что есть вот метрика 10 метрика 20 и чем меньше значит от число тем скорее сервер будет выбран а если одинаковый метрик от а будет примерно поровну причем у маршрутизатор есть алгоритм как он выбирает на который сервер придет пакет по умолчанию он учитывает айпи источника айпи назначения и наверное порт назначения меня поправит сетевики если я сейчас сказал неверно любом случае порт исходящий он не учитывает таким образом все запросы с одного какого то сервера клиента будут проходить через один и тот же балансировщик всегда нам это не очень хочется потому что может быть не удачное сочетание айпишник of исходящих серверов и тогда один балансировщик будет перегружен поэтому вы коммутатору дадим команду пожалуйста учитывая в алгоритме балансировки на который сможете заотаров ты будешь на которой с балансировщик в по сути ты будешь направлять пакета учитывай и исходящий тисе пи порт и cisco и jumper это умеют и того пакет приходя пакет выпущенной из сети стоит на 102 168 один на адрес 10 11 11 пойдет на маршрутизатор потому что у другой сети маршрутизатор посмотрим как у меня машук статический и у меня есть два кандидата одинаковыми метриками на какой-нибудь из них он отправит вот в одну сторону пакет проходит вот так есть маленькая тонкость что в адресе назначения этого пакета будет 10 111 а наши серверы балансировщик живут в той же сети в . 102 168 1 и современные дистрибутива linux откидывают такие пакеты то есть в ядре кстати по умолчанию их можно принимать его дистрибутив linux говорят небезопасно них откидывают значит нужным объяснить что ну не обращай внимание не проверять что на этом самом интерфейсе висит адрес на который пришел пакет потому что это адрес у нас есть мы его на локалхосте поднимем 10 111 там будет слушать engine x пакет таки попадет на сервер попадет через ядро потому что она приняла на яндекс он сможет ответить в ядро linux на сказать принимать такие пакеты с адресом который не совсем для тебя не совсем для этого интерфейс так проходит пакеты сен для установления соединений любые против пакеты с данными которые еще тебе запрос из себя представляют потом что же происходит в обратную сторону балансировщик и живут в той же сети да у них есть интерфейс который смотрит сеть стыдно 128 один что у них есть все данные для того чтобы отправить пакет напрямую через коммутатор потому что они живут в том же сегменте они знают arp адрес ethernet овский адрес сервера своей подсети и они могут выпустить такой завернет пакет который коммутатор просто повторит он пройдет через я торгуетесь немножко между логическим уровне вынуждена это делать это уровне оси потому что и коммутаторы и маршрутизаторы то одна железка они вот поэтому обведены на слайде штриховой линией поскольку в адрес и назначения указан адрес сервера стоит на 102 168 12 предположим его arp адрес он может просто через коммутатор просто повторить пакет в нужную дырку в нужный порт и и пакет дойдет при этом исходящий адрес будет 10 111 ну потому что фермер то ожидает от 10 11 11 и опять же linux нужно сказать дать инструкцию о том что когда ты отправляешь этот пакет не обращаем внимание на исходящий адрес 10 111 не отправляй его глупых там никто не слушает , на этот адрес и смотри на адрес назначению выберет тот интерфейс который подключён к сети который ближе к сети назначения вот таким образом проходит в пакет и обратно да значит на сервере на балансировщик который которой наш коммутатор будет в стране коммутатор маршрутизатор будет воспринимать как маршрутизатор тоже через который доступен 10 111 мы поднимем адрес 10 11 один налог на логике там будет engine их слушать когда сервер клиент хочет обратиться он выпускает адрес сторону шлюза сторону маршрутизатора тот выпускает новый пакет в один из балансировщик выбирая один из них и когда индекс отвечает то он пакет напрямую отправляет серверу который сделал запрос собственно говоря вот таким образом мы подготовили балансировщик я про ли потом честное слово и балансировщик балансировщика в готов слайды будут доступны вот как только я закончу а отлично а что делать если один из этих серверов балансировщик of нужно выключить ведь маршрут статический и половина пакетов просто-напросто потеряется половину запросы уйдут никуда а что если мы сохраним статический маршрут а вот список назначений будем менять динамической к счастью и cisco и джунипер умеют отслеживать специфическим образом доступность маршрутизаторов близлежащих причем cisco это делает средневековым методом она берет палочку и тыкает пациента в дырку а живой вот на цивилизованным языке это называется сойти с евой сам насчет на сервере есть engine x открыт 80 порт он такой 72а живой все пакет отправим если ответа нет в течение какого тайм-аута ну помер значит ему больше не отправляем вычеркиваем я пролистаю несколько слайдов что вы понимаете что они там есть и у вас будет откуда скопипастить все это потом потому что нам важно сейчас понять сам принцип вот это вот такой чехол какой-то древний и зонды медицинский вот медицинским зонах проверяют значит если сервера а если же вы и серверы еще и динамический список если сервер выключился пробы не проходит остается какой-то уменьшенный список джунипер помоложе он методов инквизиции не застал поэтому он по молодежным он она что-то открывает приватный чатик и начинает сервером переписываться привет я жива и то там это же привет я живой и и и каждый какой-то заданный интервал они это делают джунипер вот так вот настраивается сейчас важно нам что это это молодые маршрутизатор они такие хипстерской знаете они сидят и строчат вот эти содержательные сообщения друг другу и смотрят если если два или три ну там это настраиваются сколько раз человек не сообщил что он же он сервер не сообщил что он живой значит мы его вычеркиваем это язык называется dvd на котором они общаются в и direction forwarding detection нужен он для того чтобы быстро определять доступность маршрутизатор и соответственно той сети которую можете затар предположительно обслуживает предположительно мы обслуживаем 10 11 один конкретный адрес из джунипер таким определяется стоит ли вот этому конкретному маршрутизатор отдавать пакет для 10 11 1 то есть на сервере нужно что-то такое сделать чтобы джунипер считал его своим братом маршрутизатором но нужно поднять bfd на гитхабе я взял лоб ангел и демон сделал из него пакет под ubuntu простите меня кто другой там где-то в комментах найдется еще и нет де скрипт фразу я не могу обещать что он прям совершенно хорошо работает под ubuntu работает хорошо этого пендель де то есть мы обучаем сервер балансировщик прикидываться маршрутизатором джунипер сможет с ним общаться джунипер nostra настраивается как-то а это вот настройки alpen dvd и при помощи monito мы регулярно проверяем доступен ли джон x и если да мы значит поднимаем об индивиде демон а если нет то мы его запускаем и потом идея в том что cisco вот таким вот образом проверяя до старым способом тыкает и смотрит если реакция а джунипер вот так вот он постоянно болтает сервером для того чтобы понять живой ли он можно ли вам посылать пакеты этого мы получили что у нас есть статический маршрут и динамический список адресов на которые можно посылать пакеты собственно говоря вуаля мы сделали балансировку балансировщик of на коммутаторе у нас готов балансировщик и готовый балансировщик балансировщика теперь уже совершенно точно и ну и собственно все это настроили стали жить-поживать да добра наживать и не было бы этого доклада если бы не приключился один забавный случай ну инцидент не то чтобы на свете инциденты забавляли просто в этот раз сайт сломался каким совершенно необычным образом сайт сломался мы увидели в мониторинге всплеск цикл на балансировщик я бы рассказал чтобы он суров щеки cip его много едят вот это произошло присмотрелись повнимательнее оказалось что это всплеск пакетов нас оказалось что обработка прерываний от сетевой карты съел какое значение циpкa какое-то количество соплом видели слез пакетов на сетевой карте что логично подумать следующем если есть пакета на сетевом интерфейсе то видимо балансировки с кем-то общается натурально мы пошли смотреть на графики коммутаторы вы видели на каких картах есть такой уж без пакетов и вот мы его нашли читать это бесполезно совершенно это скриншот из jira из инцидента где мы все это разбирали я вот приближу одну такую картинку вот в левой части вы видите что пакетов было там 10000 секунда вдруг стало 103 я вернусь где-то было 60 тут вот если удастся рассмотреть то где-то 60 тысяч пакетов в секунду стала где-то 100 тысяч вот такого плана все сорвал взорвалось инцидент продолжался четыре минуты прошел мы ничего не успели сделать но не успел ничего понять такая вот pervi локализация мы такого раньше не видели поэтому стали думать ну как бы сразу же копали логе engine x копали логе приложений или там жалобы на обрыв оси и не на тайм-аут и какие то ну чуть чуть было больше в engine.exe обращение но пользователь уже в 20 раз чаще стали авторизоваться вот на все были последствия ничего из этого не могло объяснить причину и и пришлось думать тем более что за этот месяц пока мы копали логе еще три таких инцидентов случилось ну в общем мы по-другому то думать не стали нет и вот чего придумали мы придумали что у нас недостаточно данных для решения этого инцидента значит будем мог собирать и как собирать мы значит балансировщик и живут у нас на корзинок с блендами такие вот лезвие которые вставляют в корзину очень удобно там есть какой то обще кому тут рядом с одним из балансировщик of который участвовал в этих инцидентах мы сделали сервер на которой стали циркулировать весь трафик от этого балансировщик то есть на коммутаторе который внутри корзины установлен мы настроили зеркалирование порта и стали снимать трафик но не весь трафик netflow если вы интересовались последними законодательными инициативами весь трафик невозможно совершенно снимать мы снимали netflow и что такое дня за три может быть мы хранили но пока места хватало поверить что когда его если у вас дойдет вода разбирать как собирать трафик какую команду flow капчи мы взяли flow тулс мы взяли оттуда команду flow капча и что-то этакое сказали чтобы раз в час она создавала новый файл и писала в лог вы разберетесь очень быстро когда вот такое понадобится это все есть очень легко все эти найти вот и и стали ждать инцидента блага не повторялись и он повторился вот что мы увидели когда это случилось но мы сначала отфильтровали по айпишник у потому что по графикам и смотрели какой серверу часто значит мы выбрали айпишник выйграли порт я подскажу смотрите вот есть у нас 4 нормальных потока netflow потоками называется да и у них гораздо меньше пакетов чему ненормальных которые внизу у наших стоящих в несколько раз в десятке практически раз больше октет авто есть байтов и совсем мало собственно типы соединений то есть одно 20 присоединение совершенно забивали вот пакетами vip эта штука вот то что мы смогли поймать объясняет эффект который видели много пакетов мало трафика но поймите какая штука если вы долго ль мы долго любовались все этим если долго любоваться то можно заметить что вот приближу что средняя длина пакета нормального она вот 1460 а длина пакета который у нас забивал 52 байта в 52 байт поместится но кисеты заголовок u10 заголовками собственные серверы друг друга и бомбили это какие служебные пакеты сен ок растрясет финн и они в этом что-то друг другу что-то делаю там какой именно непонятно потому что netflow нам заглянуть в пакет не дает значит следующий шаг ловить эти самые пакеты длиной 52 байта это условная длина потому что он и flow свое представление длине пакетов одесситы дам по которым мы стали ловить другое представление длине сначала просто в трафике выделили вот в этом зеркале рунам порту выделили скажем syn-ake увидели что эти себе дампа это шесть шесть байт и стали записывать пакеты длиною шестьдесят шесть байт каждую минуту в новый файл 400 мегабайт в минуту в час это двадцать четыре гигабайта или в сутки один диск служебного трафика и стали ждать и однажды случился инцидент и и мы и говорим в мы вращали потому что но один диск чушь меня что-ль каждыми вращать вот и однажды случилось вот эта штука ребята вот это оргазм вообще то что вы видите представляете вы полтора месяца гоняетесь у вас посетили tight какая-то хрень вы за нее гоняетесь и вдруг вы поймали пойти вот два гигабайта хрени на диске это экстаз вообще вот запихивать в vr shark в такие файлы плохая идея этому мы же знаем какие серверы участвуют начала отфильтровали по айпи адреса как тогда по порту и загрузили white shark вот великолепный скриншот все мы любим и забил за это и просить его вершок ну я не надеюсь что вы это разберете я вот чуть-чуть от фильтра вы видите что левый и правый колумб постоянно меняются местами или по-крайней мере длиной вот вот вот так вот если отфильтровать в одну сторону пакеты и я убрал лишние я сделал его черно-белым чтобы можно было читать хоть как-нибудь вот вы видите что одинаковые совершенно пакета это ок это окно лишь идут в одну сторону и примерно такие же пакеты идут другую сторону разница в том что один сервер ожидает пакет с номером 304 920 не пакет с номером последовательность какой-то другой варит меня мы вас сейчас здесь находимся то есть они друг другу безконечно убеждают том что я нахожусь в 10 соединения вот в это если четным не шлешь ура вот вот мы поймали шторм и вы сетевики вы наверно уже понимает в чем дело и я не очень понимаю я уже понял что дело сетью чем более что значит влогах сервер я нашел вот такую вот штуку то есть cisco выкидывала один из балансировщик of на какое то время потом возвращала обратно и инцидент вот этот шторм происходил после того как я стал собирать стенд смотрите какая со стендом вот какая штука это всё виртуалке 2 engine is a который притворяется балансировщик в один клиент который будет у них запрашивать и один адрес общей для них на стенде прокладываю маршрут для общего адресов то 8 6 причем с разные метрикой да на два разных ынджон xa а что дальше я делаю простой запрос стоит на 102 168 2 186 данном случае thanked корень соединения закрыты что осталось на первом engine.exe вот на сто восемьдесят третьем там осталось соединить и hi-fi соединения в состоянии time white я теперь убираю верхней маршрут остается только нижнюю маршрут и начинаю выкачивать длинный файл причем вот с того же совершенно порта это важно потому что там что получалось что сервер у него был соединение и когда трафик внезапно оборвался пакет полетели на другой туда прилетает так много речь и и ничего там о воссоединении быть не могло бы и не всегда с этого порта и весна другой порт на один там он с и заново открыть соединение с тем же самым исходящим портом другу ада трафик это и когда трафика возвращался внезапно дай писала и поднималась он приходит на старый сервер то предположим time with вот ну по крайней мере на этом самом на стенде и согласно протоколу tcp соединение находится в стабильном состоянии если пришел пакет out of о да не по порядку другой секунд на другое окно вич который хочет сервер был обязан ответить вот посмотри на мое окно лишь вот мы где с тобой находимся начни с этого переписывать ок ночь пролетает на другой ферме у него соединение в стабильном состоянии стебли что говорит погоди ты не те пакеты мне шлешь вот они пакеты вот так это происходит вот так работает это так но лишь шторм отлично надо избавиться от той байтов твари сайкл фиск отвел решают вопрос но чтобы time вы это образовался нужно сначала корректно завершить соединение с первым то сервером а трафик переключали внезапно значит там оставались соединений не в тайм выйти до соединение оставались и стебли шт на engine.exe теперь клиент заново установил соединение с тем же портом возвращается из т плюс в establish a и стебли шт как ну то есть получается что наша система балансировки ненадежно ей нельзя пользоваться просто напросто если вот это происходит так долго давайте давайте помню что происходит давайте помнить что происходит можно кое-что подкрутить есть и эфемерные порты почему я про них говорю северный порт это тот набор случайных портов которые сервер выбирает при открытии соединения и он знает что он идет на 80 предположим порт но у себя он выбирает любой случайного списка эти эфемерные порты обычно от 16000 до 65000 но линуксах настроен если увеличить диапазон эфемерных портов с которых устанавливается соединение то вероятность того что клиент успеет установить заново соединение с такого же порта уменьшится вот мы это и сделали то есть нижнюю границу опустились 16 тысяч до двух тысяч то есть было значит 65 -16 около 50 тысяч добавили еще 16 минус 2 14 тысяч не где-то 3 дает чуть меньше тридцати процентов добавили портов снизили вероятность возникновения ситуации стал полегче инцидента продолжают вот стало можно подумать давайте думать establish соединение возвращается на стебле шт чего там есть а вы помните выпадает до выпадает какой-то балансировщика и писала и дауна и ps трек там да вот оказалось что вот на это я на порты повлиял я внизу вероятность хочу чтобы не выпадал у меня балансировщик не могу потому что коммутатора в корзине самопроизвольно перегружает в и мы введём в этот момент переписку с вендором какого чёрта собственно не самопроизвольно перегружает почему это все равно много быть полезна вам потому что есть из админы это великолепные люди с большой инициативы которые на сервере захотят что-то сделать чтобы не повредить клиентам а не скажем опусти топом берди демон да ну потому что ну мало ли что я сейчас на сервер напортачит я пусть я опущу об индивиде демон трафик сюда не пойдет отсчет подкручу и подниму его через минуту ну и вот он инцидент или или сетевой администратор это может быть тот же сисадмин который пойдет нацистскую сделал то же самое скажет но у трек то есть не надо мне отслеживаться сцене этого сервера выкини его из списка что-то поделать и поднимет обратно и опять инцидент может случиться у нас не только из поверьте кроме перезагрузки коммутаторов на корзинах причиной было еще тайм-аут и скажем какие-то да то есть cisco не успела получить ответ за тайм-аут и не то чтобы сервер не ответила у нее циpкa был занят она там что-то все в голове перестраивал поэтому тайм-аут должен отдавать достаточно она может на 5 секунд выкинуть клиент почему все равно в этот вероятностная вич преуспевает заново установить соединение и при возврате трафика на прежний балансировщик опять инцидент порты исходящие подкрутили выпадение не очень получается остается еще и стебли шт третье собственно условия этого дела а что если engine x когда балансировщика выпадает из работы будет закрывать соединения вот было стебли что он причем он как-то принял решение что это соединение пора закрыть и теперь когда трафик на него вернется к нему придет пакет окно лючия в соединении сервер скажет я не знаю такого соединения ресета иди отсюда но все это будет короткий всплеск пять ошибок на сайте и и и все продолжится человек нажмём f5 работа продолжится это не будет по крайне мере downtime таким долгим а вы знаете так как толкайте присоединение могут жить вот вы вечером на работе закрыли ноутбук ушли домой вернулись назад ноутбука открыли а терминальной сессии еще жила был такой вот потому что а чего ну в эти сети придумывали когда модем еще были плохие линии пусть живет и все же отцы-основатели позаботились в такой ситуации в тисе пи протоколе типе есть keep-alive важно отличать его от эти пики пола его это разные механизмы принцип работы такое что если нет трафика на этом типе соединение в течение какого-то времени я буду посылать тестовые пакеты и ожидать на них ответ если в течение если со второго со второй попытки я ответ не получу средний плохое она оборвалась тогда чего получает что если мы настроим задержку на коммутаторе возвращение серверов строй достаточно большую чтобы keepalive на балансировщик и успел отработать тогда мы решили задачу тогда соединение будет закрыта оборвана при возвращении получит ресета они перри договорятся и все это кратковременный стресс и ошибок и и дальше можно собственно отцы-основатели большое вам большое спасибо джон их сумеет пользовать keep alive когда вы говорите лесам говорите за одну секунду одной попытки достаточно и того время реакции 2 секунды пока 2 не прошла cisco умеет задерживать возвращение серверов строй на три секунды 3 секунды все уже уже соединение на балансировки на старом закрыты и джунипер умеет задерживать возвращение серверов строй он более точно в миллисекундах ну и вот то есть мы обошли вот эту проблему когда из-за того что мы даже пакетная балансировка мы не покупали дорогих железок которые умеют потоковую балансировку делать и строили и сами free везде например да с по и финком мы только пакеты главное фирмой и вот таким способом мы обошли вот эту проблему что протокол ip и нарушен и все еще работает вопросы будет микрофон или как-то ну говорите о я буду просто повторять для записи спасибо огромное я повторю вопрос для тех кто записей для тех кто трансляцию смотрит зачем вообще было изобретать балансировку было балансировщик of на коммутаторах изначально если можно нам машине клиенте за балансирует приходите работать нам потому что это следующее это что однажды этих балансировщик of не кончится и вы начнете переписывать приложение свои чтобы она чтобы когда приложение обращается к другому сервиса но она самостоятельно умело балансировать пакеты и или х практик в этих a proxy это усложнения архитектуры раза разобраться в том где чего сломалась то есть еще один слой добавлять ну не хочется а все работало все работало хорошо выбор и не знали до что так если бы коммутатор не стали перегружаться мы потому что все таки с отмена редкого так балуется как я рассказывал вот знаете нам нам как скажет что этот способ дешевле что ли он очень понятный потому что есть коммутатор через который все равно бегают пакеты они постоянно через него бегает он еще и маршрутизатором умеет быть но вот он у нас эти зиру это все эти вот механизм который ослеп доступны сервер они очень дешевые будет сбой но и ладно х прокси в proxy надо поддерживать он тоже циpкa просит у него тоже есть какие-то ограничения ушел один поток живет у него как с ретро имени очень потому что типе соединение установилось все он какие то ищите пи ответа не умеет уже отрабатывать engine x оставить на клиента ну вот вот как то вот это вот способ вот просто в голову не поместился вот-вот нам это правда понятнее вот механизм палочкой тыкать торчать tatis вот работает если это как-то отвечает на ваш вопрос я не уверен зачем тут мы выбрали так почему не стали использовать протокол динамической маршрутизации вам и нести вики на рынке одна из самых дешевых служб эксплуатации и работодатели хозяева это ценят причем они как бы выражают это в деньгах да и но если не работает приложение которое стоит за индексом это же забота engine x как-то отследить от ситуацию до и поэтому нас за engine.exe надо стоит опять который активно с тем потоком запросов которые у нас есть мы всегда учим знаем какой сервер не работает у нас нет такого что он бездействует долго вот у нас было сначала все нас иски потом ушли на джунипер нацистские был 2 циски которые для надежности значит за параллельные через spanning tree все это дело балансировали выяснилось что коммутаторы на болидах не любят spanning-tree перегружается но вот как то вот так вот wonder world of не получилось ними заработка мы потом отключили какой-то watch dog на этих коммутаторах я не успокоились вообще вот циски у нас есть они сейчас на внешнем контуре они там full view вот и вся история джунипер это основной коммутатор маршрутизатор до который мы бэтмен сети собственно есть и то и другое описывает до сих пор работает он внешне индукция балансирует вот собственно с теми же самыми рецептами там на внешних интернетов просто меньшая вероятность того что то же самое клиентов самый порталу стоит их этих клиентов там миллионы вот а внутри этого довольно часто приводил проблем я обещал ссылку на слайды вот внизу ссылка на эти слайды то есть вот это уже можно фотографировать и нужно потому что эта ссылка на слайдер на самом деле там все эти слайды есть вы сможете скопипастить то что там написано и и использовать у себя вот и пишите пишите задавайте вопросы если только у вас нет вопрос прямо сейчас еще тогда спасибо"
}