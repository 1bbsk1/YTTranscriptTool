{
  "video_id": "OhfjB_9gNG4",
  "channel": "HighLoadChannel",
  "title": "Хайлоад, которого не ждали / Дмитрий Самиров (Tarantool)",
  "views": 2479,
  "duration": 2522,
  "published": "2023-01-19T05:55:18-08:00",
  "text": "всем привет меня зовут дмитрий summer of я работаю этим ли дом в tarantul я отвечаю за разработку пилотных проектов рука рука уже командой собственно что вообще такой пилотный проект ну все понимают под этим разное есть такой сын каком веке но зачастую это всегда что-то такое немножко сыровато но рабочие о соответственно у нас в большому счету при разработке таких проектов два главных критерия это нужно сделать максимально быстро но достаточно качественную тут именно с ключевое слово достаточно потому что но мы живем в реальном мире и очень быстро очень качественно не бывает вот бывают очень быстрый и некачественно вот облой точно быстро и достаточно качественно ну что замечает отличие то что зачастую когда мы решаем задачу мы можем пренебречь какими-то условиями которые ну не очень важны для данные для до момента времени например мы решаем что мы хотим чтобы наше решение было очень быстро но не такой уж отказоустойчивого или наоборот мы хотим чтобы у нас было максимально отказоустойчивость но не очень быстро вот и собственно исходя из за таких проектов исходя из таких критериев мы формируем некие именно полу же дальнейшей разработки как мы будем разрабатывать проект на что обращать внимание она что забивать и и собственно сегодня я как раз и хочу рассказать про один из таких наших многочисленных плотных проектов которые внезапно превратился во что-то прямо большое глобальное интересное прикольная вот соответственно однажды к нам пришло просто из ниоткуда примерно вот прям вырвалась можно сказать неожиданно к нам пришли магниты delivery club и они организовывали онлайн доставка продуктов из магнита с пресным приложения delivery club вот соответственно они пришли к нам чтобы ему помогли организовать платформу для сбора остатков товаров чтобы люди которые поиски прежде delivery club видели актуальное состояние того что находится непосредственно магазинных магнита вот средства как правило когда у нас начинается какой-то пилотный проект мы получаем на вход какие-то данные вот зачастую пап каждый раз это по-разному когда-то данных могут когда танк не очень ну вот сейчас у нас данных был очень мало то есть у нас была совсем чуть-чуть понимание бизнесовые части то есть мы примерно понимали зачем мы это делаем вот и было немножко документацией такие доспехи и пили достаточно сыр и вот и там примерное понимание как устроены потоки данных которые нам нужно обработать вот что было еще еще у нас была классная команда потому что действительно вот наша команда это такие очень крутые специалисты которые могут быстро решить практически любую проблему вот на помимо этого у нас было так же классная команда из магнита изделий реклама у нас было 3 стороне такое взаимодействие в котором прям все было хорошо в отличие от того что зачастую люди которые работают с нежными компаниями они ну отношения такое что вот вот если бы не они да мы бы все сделали быстро они настолько тормозят требования постоянно меняют все меняется мы ничего не успеваем все из-за них вот не было бы других мы все сделали идеально вот у нас была к счастью не так у нас прямо было максимально я считаю плодотворное сотрудничество по всем фронтам но вот для примера примерно у нас есть specs было с пикапе которая говорила что вот грубо говоря вы должны организовать две ручки описанные по одной apes на ручке вы должны по нужному иди магазина отдать информацию о том сколько товаров осталось на полках магазина то есть максимально минималистичным данные вот который состоят из эти продукты и количество и вот еще одна ручка которая должна по этому магазину отдать каталог в котором там уже много разных метаданных их достаточно много вот но там название какие то цена и много другое то есть тут все понятно по месяца привел и собственно проблематика основная была в том что у нас был жесткий дэдлайн а именно два месяца то есть буквально если не изменяет память это 1 2 июля к нам пришли с этой задачи сказать что вот надо сделать от первого сентября уже должны были вот но приложение должно бы открыться люди должны были можешь заказать продукты а курьер и должны были ездить по магазинам собирать эти продукты доставлять то есть соответственно ну прям максимально жестко вот документации было очень мало то есть мы опять же понимали какие-то общие вещи но какой-то подробный каких-то спектр того как там где лежит данный у нас не было и в общем там их в процессе создавали также у нас практически отсутствовал полное понимание того на первых порах что мы должны непосредственно какие данные получить к вкусу как преобразовать ком в команде дать это тоже решалась уже в процессе когда мы начинали интеграцию и уже там создавали 1 какие-то решению вот и очень важно что у нас из-за того что это все было очень быстро требование не было я не менялись прямо на ходу очень часто это было прямо абсолютной нормой для этого проекта и у нас к сожалению не было было мало времени на какое-то качественное тестирование что там проверить и функционально нагрузочная интеграционная вот его просто не было нам нужно было поскорее дело разработать а там уже как получится вот соответственно немножко расскажу про тарантул потому что мы использовали для решения именно него что в принципе логично и я же работаю в tarantul и что мы ещё можем использовать на самом деле это не так мы используем только тарантул но сейчас доклад не об этом значит тарантул это пересесть and no i in memory хранилища и все что по большому счету сейчас нужно знать то что тарантул очень хорошо работает когда у нас большая нагрузка с маленькими транзакциями так называемого лтп и очень плохо работает ну на самом деле практически не работает на сценариях нагрузки когда нам нужно получить большое какое-то количество данных как ты их преобразовать а дальше либо там еще что-то с ними сделать не так называла по сценарию вот тарантул не проект значит и мы собственность за то что нужно было все это быстро делать мы это все решили диплом облаке в нашем выкакал solutions и примерно архитектура она получилась вот такая то есть у нас есть наши потребители изначально был только delivery club но потом добавились яндекс и до магниты на самом деле что многие другие вот соответственно мы для них выставили кайта frontend там написали какой-то backend его снова хранилище легла наша платформа транса datagrid это на базе тарантула один из наших продуктов и рядом мы использовали хранилище есть три которые но для картинок чтобы на которые таблица приложение хранишь листы которой у нас есть как именно счас в облаке то есть мы ничего не разрабатывали просто нажали кнопку включить вот это было с одной стороны с другой стороны мы должны были какой-то поток данных получить от магнита и тут была особенность том что поток данных должен быть защищенным то есть мы не могли его пустить через код совсем дикий интернет поэтому мы подняли vpn тоже у нас есть такой спецназ мы тоже там особо ничего не делали вот и через этот weepin' с магнитом мы интегрировались по большому счету по трампа током до первое мы получали от них и получаем поток именно метаданных по продуктам то есть это картинки названием все что общем то мы видели в api вот отдельный поток цен и остатков это прямо буквально каждый магазин магнит а там стоит сервер который там 1 5 не просыпается и посылает нам данные по ценным по остаткам по акциям и много другое вот и отдельно еще был поток фоточек который собственно клались в истре вот чтобы немножко показать то есть это выглядело буквально так то есть у нас на пилотный проект мы должны были стратонов 50 магазинов и вот высшему счету на 50 магазинах стоял какой-то код который пушил нас данные того вот получается что у нас две большие банки кита которые нам отдавали meta-inf ул и непосредственно данные по остаткам по ценам отдавал каждый магазин по отдельности то прям такая хорошая распределенная система которая дает хороший распылен нагрузку и все это снова оборачивать степен канал для наглядности то есть вот если мы посмотрим на карточку продуктов приложение delivery club то у нас все это выглядит так что вот есть картинка это фото каталог есть название что это гранат что есть весов к 500 грамм это у нас ответ на каток продуктов и отдельно у нас информация об остатках об акциях о ценах это непосредственно приходит с каждого магазин вот соответственно мы заранее видели некоторые проблемы потому что ну когда мы начнем что делать принцип потенциально стараемся искать слабые места чтобы быть готовым каким-то проблемам и в принципе в этом проекте мы были готовы к многом ну например у нас есть и pen который но едины . отказа да и то не очень хорошо но как-то быстро жить надо вот быстро нужно стартануть и мы в общем то это просто в уме держали и оставили как есть вот сразу же обратили внимание что по тема пешком который мы делали у нас не была предусмотрена pagination ну так вышло потому что фишка там рекламы использовали там вместе деле реклама на вот есть какая-то общая но вообще мы использовали а большому счету по ней потенциально могут гоняться джейсон и по 50 мегабайт это не преувеличение действительно у нас есть какие-то магазины которые отдают 10 мегабайт не джейсон по запросам каталог вот и еще так называемые мо придет операции а тарантул очень этого не любит вот средстве на что вообще такое что я называемая придется операциями потому что это не совсем верно то что все понимают в общем смысле как это там устроено в комнату google и так далее поэтому я немножко расскажу всю проблематику всей этой истории ну все идет от сортирование то есть когда у нас есть один клиент есть одна база данных то в принципе все просто то есть мы берем клиент делать запрос мы идем базу данных собираем ответ но я даем клиенту все все все легко понятно и так далее а что делаете когда у нас внезапному база данных не хватает потому что на вертикальном конечно можно масштабироваться но не до бесконечности и рано или поздно при росте нашей системы мы должны как то все дела разложить на 2 банки и тут как бы вопрос а куда клиент уходить кто должен решать так далее ну и вот соответственно на примере допустим ну у нас интеграции по всем магазинам и предположим что у нас есть магазин с иди 6 девяток ну и мы должны каким-то образом определить куда этот магазин куда за нами этого магазина отправиться в банку но для простоты один из самых простых алгоритмов кодирования то просто осадок на модулю и предположим что мы ну идем и храним в седана этого магазина в левой банки и собственно как нам вообще не хочется поступать ну вот мы храним данные в левой банки мы получили де магазина мы сходили в этот левую банку и забрали список остатков которые пришли по этому магазину который по факту равняется то что вообще нас магазине доступно и может лежать на полках ведь остатки мы получили а теперь к этим остатком нужно еще добавить метаданные продуктов из каталога и тут как бы возникает проблема то что остатки потом магазины у нас заведомо лежат в 1 в 1 но диодной башке а продукт они не привязаны к магазину они ну они общие для всех и у нас в общем они размазаны его по всему кластера и тут как раз таки чтобы получить метаданные продуктов необходимо сделать запрос сразу на все ноды собственные называемая придется и эта проблема потому что как раз таки тарантул такой не любит тарантул хочет максимально быстро ответить на запрос и продолжить работу дальше а когда мы начинаем грузить весь кластер целиком и ничего хорошего не получается ну естественно после этого мы все это дело мир джим уже и отдаем клиенту и вот такая схема но она не очень классная с точки зрения работы тарантулы в принципе распыленных систем потому что хочется вот так вот то есть мы получили эти магазина сходили в банку все забрали и отдали клиенту все просто то есть нет никаких промежуточных вот этих историй что мы там еще делать это запрос по кластеру напрягаем кластер на прыгаем клиент так далее все забрали дали это вот то к чему мы собственно хотели бы стремиться а теперь вот я рассказал что у нас примерно была в голове где ждать проблемой теперь хочется поговорить о про то какие были реальные проблемы потому что всегда когда тоже дальше 1 на самом деле вот то что ты же точно не случается случается все-все на свете кроме того что ожидал ну например в какой-то мент времени оказалось что ну чтобы корректно собирать заказы сверять что пользователь заказал у delivery club и что непосредственно находится на полках магнита нужно ввести такую сущность как штрих-код вот и соответственно по штрих кодом но они тоже отправляются с магазинов нам сказали но вот нужно добавить штрих-кода но мы там все были в мыле у нас всех был пожаром этом сказали ну нормально сейчас добавим добавили и получили доз потому что ну штрих-кодов внезапно оказалась в 10 раз больше чем остатков если допустим у нас магазин присылает 10000 позиций раз пять минут то штрих-кодов он присылает 100 тысяч позиций раз пять минут вот из того что им увидели еще мы ну не с не настроили некую валидацию и поэтому собственно и в прямом смысле мы как-то раз ну там раньше тестировали лазим в базе этот раз увидели куча разных матерных слов вот как потом оказалось что в магазине штрих-коды добавляются людьми вручную и видимым иногда скучно вот поэтому они как бы делает соответственно ну как мы все это починили какой-то ментам когда мы поняли что штрих-коды мы не можем обрабатывать раз пять минут мы подумали но в принципе это кажется что не такая важная и сущность в том смысле что на меняется редко давайте обрабатывать не раз пять минут рассудке и от равномерно внутри дня чтобы не было прямо какого-то большого потока данных вот ну и настроили нет конечно валидацию то есть мы принимаем только цифры больше вот такого непотребство у нас нету вот только симы саммита в базу не добавим ну какие можно за выводы в том что нужно всегда перепроверять сайдинге то есть мы вот изначальной взяли сайдинге посчитали потом добавили ковать сущность ну и мы в общем забили подумали что-то вроде там не должно быть больше а вот внезапно оказалось что больше вот ну и конечно же нужно настраивать занят айзек данных это принципе такой вы достаточно простой понятный но вот иногда можно запросто забыть вот ну благо здесь ничего плохого не произошло мы просто все посмеялись вот но тем ни менее следующая проблема про которых хотелось бы поговорить это резкий рост вообще всего что мы сделали то есть у нас по плану пилотного проекта должно было быть 50 магазинов и мы действительно запустились 1 сентября там даже вот это один из двух проектов в моей жизни который мы запустили за два дня до дедлайна вот мы запустились там 1 сентября реально курьера пошли собирать заказываем там вот прямо люди с delivery club там сами прям проверяли ездили магазины собирали вот что все работает и это все настолько классно заработала что мы буквально там моргнули три раза и магазина стал 2000 вот при этом ну как то мы в общем то же не ожидали то есть мы думали на 50 ну там сто-двести кто говорил на принципе нормально выдержан вот этот раз их снова внезапно 2000 и так как у нас все магазины в нас полностью погружает весь в этот сет но в общем мы получили holod которого не ждали и вот собственно пример графика у нас в пиках ну пике а не понятно почему потому что с магазина 1 пекин просыпаются этой ну и за то что магазинов нам у нас уже больше 10000 это не возможно как то равномерно размазать внутри дня поэтому общем там как-то сами живут по своей жизни вот неизвестно распить не к нам прилетает такой пик данных не там в пиках вот достигает полтора миллиона рпс на весь кластер вот то есть это ну в общем мягко говоря не то чтобы кто-либо ожидал собственно что вы более вообще понять откуда все это берется давайте посчитаем то принцип математика несложная что такое 1 магазин но в одном магазине в среднем у нас 10000 позиций и 100 тыс штрих-кодов и вся эта история отправиться нас 1 5 минут но штрих-код отправиться рассудке поэтому самом деле не нагрузки по итогам создают не только у нас место занимают и соответственно 1000 магазинов это уже получается в среднем 33 дтп с именно в среднем это если прям все магазин идеально будет равномерно в течение дня все посылать так как этого не происходит по факту это порядка 50000 руб с пиков то есть это действительно такой очень хорошая нагрузка а бывает соответственно на самом деле вот если вы тоже видели наверняка видели у магнита много разных типов магазина магнит-косметик есть магнит у дома которая таки маленькая есть еще такие огромный-огромный гипермаркета типа ашана где там позиции может быть по 50 по 70 тысяч и реально это все та же нас проливается такие магазины они создают гораздо больше нагрузку чем обычно но наша система пользуется не только магнит посылая данный но у нас еще есть наши партнеры the delivery club сам магнит яндексе да и проще которые эти данные собирают они соответственно dolby страна с другой стороны но уже не на записи на чтение и по большому счету у нас со всеми партнерами из договоренность у нас есть некий если что мы по каждому магазину каталог обязаны мочит давать 1 час потому что каталог это к сущность которые очень часто меняется остатке 1 5 минут ну просто потому что мы их раз петь не получаем с на все партнеры хотят тоже максимальной иметь актуальной сладки потому что это влияет на сбор заказов ну и соответственно 1000 магазинов это повышается примерно уже какие-то единиц рпс на каталог и десятки рпс на остатки что в общем-то кажется ну как бы один рпс это что нагрузка что ли вот там 50000 руб с было но здесь я напомню что все это дело у нас гоняются огромные такие джейсон и десятки мегабайт один рпс а те которые 1020 ps это джейсона поменьше но тем не менее тоже общем не три байта и это на самом деле очень сильно нагружает наш кластер когда такие запросы приходят большим количеством собственно ну и к чему вообще привёл рост вся эта история в какой то момент времени у нас все начало жутко деградировать потому что с магазинов который посылает нам даны начал приходить просто x10 трафика вот просто из ниоткуда мы смотрим ну на графике вроде магазинов на не увеличиваясь а трафика стала в 10 раз больше почему ну потому что забыли отключить ретро и но при этом на самом деле мы говорили о том что должен быть моим что или трою быть не должно ну забыли включить с кем не бывает где-то пишет и тигр 1 и магазин начале у нас три травить мы в общем там получилось к это вакханалия мы там всем в ужасе бегали но в целом справились вот и у нас внезапно выросло количество партнеров то есть мы когда стартовали у нас был delivery club потом чуть позже появился yandex вот и ну там мы думали 23 а потом внезапно из-за того что весь проект paper было все очень классно стали подключить многих других и все это дело у нас люди стали очень активно использовать наш api и соответственно какой-то мент можно начали именно страдать полотенце по задержке от ответов на open еще даже проблема в том что партнера ниже между собой не договариваются что там давай ты будешь час дня я буду в два часа дня забирайте не жил со своей жизни и тоже нужно как бы учитывать ну собственно что в итоге сделали ну проблема задаст вам мы решили чем просто мы просто трубили все и сказали кажется у вас что-то происходит недобро давайте разбираться нашли проблему быстро починили и вернулись обратно и соответственно для партнеров по итогам и настроили курортные red limit и потому что действительно начали это мы договаривались на словах типа давайте вот вы там будете столько запросов делать все было классно на какой то момент времени партнеров стало много все вышли из-под контроля не начали долбить вопи как хотят и пришлось настроить жесткие лимиты но с учетом нашего села и который мы должны были обеспечить собственно какие выводы можно сделать не нужно доверять никому то есть в принципе считаю что стратегия на словах договориться с кем-то на рабочий not когда людей становится много к сожалению уже договариваться очень сложно поэтому нужно все таки технические ограничения ну вот собственно пример графика как это выглядело когда у нас внезапно там какой-то партнер загружал культ свой очередной 1000 магазинов у нас очень сильно раз волатильности там буквально там до 1 это 2 минут начнут никуда не годится нагрузочное тестирование на питерском холоде и может там кто-то видел слушала я рассказывал доклад про то как мы классно делаем нагрузочное тестирование про то какие у нас достаточно но я считаю интересные подходы поттер него проведения вот вот в этом проекте все было не так потому что было мало времени и мы накрутим тестирование провели ну из разряда запустили посмотрели что навроде 20000 руб с выдерживает нам вроде больше не нужны и успокоились вот и соответственно использовали код свой генератор он там как-то синтетически geneo данным этом особо им не заморачивались нужно было максимально быстро просто проверить что вообще наш подход он валиден соответственно ну и в итоге мы очень много не учли вот и мы сделали из этого тоже большой воды а именно у нас сейчас есть хороший при продавай кластер который на самом деле держит всю правую нагрузку вот и мы постоянно на нем проводим нагрузочное тестирование продавать данных и чтобы получить продавай трафик мы используем и на вой потому что иного и из коробки умеет трафик зеркалирование так выглядит собственно схему нашего сейчас при про да то есть у нас есть наши байки наши двое который непосредственно получает все данные с магазинов об остатках он получает от проектов прот а дальше настроен мир уже на другой кластер но из интересного у нас между кластерами точно также считаются мета-данные это каталоге это фотки с помощью нас там инструмент трансу за такой отчет мы им синкай два кластера и по факту у нас припрут он абсолютно повторяет про ты там можно к нему делать запросы делать кит опять же нагружать тестирование еретики то фичи пережде чем то есть vici непосредственно на полноценном продавай трафике что в общем достаточно полезна и при этом мире link регулируется то есть допустим если у нас там препод класс стр был поменьше мы бы сделали там но не сто процентов трафика 1020 в общем уносит позволяет давай классный проблемам про которую хотелось бы поговорить это про цены в общем можно заметить что у нас цена по продукту находится в ручке каталога то есть у нас каталог возвращает кучу кучу данных том числе цену томоко ценность признака ценности так далее и собственно в чем вообще проблема в том чтобы партнеру получить ценную по продукту что в принципе кажется ну не то чтобы много инфы там нужные им приходится выгружать огромный огромный каталог его пару сеть и таким образом напрягается вообще все все звенья цепочки и сервер и клиент очень плохо 7 нехорошо никому и поэтому цены очень принципе трудоемко синхронизировать их синхронизирует редко но цены не то чтобы часто меняется скажу но тем ни менее порой задержка все-таки выстреливать каких-то отдельных кейсах и хочется чтобы но такого общем не было никогда чтобы любой партнер но к любой момент выгрузить легко цены и это особо никого не нагрузил а вы собственно ну то есть починили тоже очень просто просто сделали ручку цен вот ну а б н а ш по нашим опытом эволюционирует развиваются и вот мы там довольно модель на ручке и вот например собственного лерочка цен теперь на сцену забирается с другого места какой вывод можно из этого сделать нужно все-таки чаще задумываться о том что мы делаем и для чего делаем потому что ну как бы формально мы задачу выполнили все данные дали но к сожалению на практике это но не очень изобильна и очень интересная тема напоследок это обновление остатков ну в общем в какой то момент времени мы заметили что заказы плохо собираются ну что значит означает что курьер спляшу приходит в магазин за заказом а товары нет на полках но при этом когда пользователи в приложении там заказывает тот же самый гранат он был написано было снова с 5 штук вот а курьер приходит и нету можно конечно подумать что ну возможно пока там курьер бежал товар раскупили ну это прям было массово то есть ну не бывает такого что все товары при массовом раскупают который пользователь хотел заказывать и в этот мент арене начал разбираться и вы сели оказывается магазин не сообщает нам о том что товар закончился то есть ну просто вот он присылает что там осталось десять осталось восемь раз пять а потом в кои-то вин времени просто перестает присылать данные по этим товарам почему так это история дельная но общем так работает и ничего с этим сделать было нельзя именно сторона со стороны клиентов и соответственно это все приводит помочь что весь бизнес расстраивается потому что у него растет недовольство клиентов которые поставить их поддержку пишет что чего там опять мой заказ не собрали полностью ну естественно падает выручка ну в итоге мы собственно пытались понять как это сделать поняли что наклеить это сделать нереально чтобы нам прислали 0 и мы стали обновлять на своей стране выставили некий пришел по времени что если по товару долго ничего не приходит мы считаем что 0 это на самом деле вызвало некоторые сайт эффекты но в совокупности это решение она реально работает работает хорошо и пока ничего лучше в общем у нас нету вот и но после этого графики не сбора заказов прям резко пошли вниз и все обрадовались вот какой вывод нужно чаще задумываться вообще на уровне бизнеса собственно что мы делаем и какие могут быть подводные то есть но проблема с тем и с не сбором заказа она абсолютно стандартный для всех партнеров которые заниматься доставкой продуктов там независимость кого магазина эта проблема очень важна и но тут почему-то мы про это не подумаю заранее ну и собственно подытожив все что рассказал хочется сделать следующие выводы не пытайтесь строить какие-то огромные космолеты я прекрасно понимаю и в принципе я точно такой же всем хочется сделать какое-то решение которое будет максимально вообще идеально решить все проблемы мира но к сожалению реальной жизни пока вы будете это делать возможно вы потратите не десяток лет чтобы это сделать поэтому зачастую достаточно просто чего то более менее рабочего которые решают вашу проблему и а дальше уже смотреть куда это все растает ну как следствие что отказоустойчивость постоянно все расскажут мы рассказываем и все там на холоде рассказывают про то как строить отказоустойчивых приложения и это очень классная вот но зачастую проблема проблемы реальные они никак не связаны с козу стоящего отказоустойчивостью они больше связаны с какими-то не за человеческим фактором вот например ну вот помните я говорил что ты пьяный дин . отказа он вообще нам проблем не причинил вот ни разу то есть у нас чаще люди что-то не то просто выкатывали этого там ломалась что-то чем repens был пьян работает идеально спасибо этому и пену всегда есть риск того что то что вы сделаете к сожалению кому не пригодится и мы это чувствуем особенно хорошо потому что пилотных проектов очень много очень много разных заказчиков ну и как такой некий стартап инкубатор у нас конверсии из пилотов код успешны проекта на но не сто процентов это нормально и поэтому собственность за того что вот мы очень научный тем что мы часто работаем стол мы стараемся максимально экономить ресурсы чтобы не переусердствовать потому что всегда риск есть риск того что ты будешь стараться стараться стараться это никому не будет не нужна люди там банально расстраиваются потому что ну никому не раз работать со все хотят чтобы их работа была нужное и важное вот и вот мы росли высчитываем исходя из этого мы стараемся где того что упростить где-то что-то пренебречь и принципе это рабочий стратегия и важный момент это не нужно бояться какие-то свои прототипы не обкатывать вроде то есть тоже я считаю достаточно большая ошибка он у людей которые вот идут в прод пока у них прям все не будет идеально там логин настроены там метрики все есть вот ты считаешь что этот подход он не годится для того если вы хотите что-то быстро проверить запустить и так далее то есть гораздо проще быстро запустить и на ходу уже разбираться во первых ну может он не выстрелит вещи никому не нужно будет а если он выстрелит но вы конечно будете правильно там уже доводить все до ума и делать все хорошо вот ну и соответственно не нужно всего этого бояться а все потому что мы все ли не роботы мы все люди мы можем что-то забывать и не учитывать и это абсолютно нормально то есть никто не ждет от вас там того что вы сделать что-то идеально это будет идеально работать и это на самом деле не так важно потому что быстро поднятый сервис он не считается упавшим спасибо у меня все спасибо красавец ты сегодня просто пример стресс-теста друзья вопросы под названием ряду вам уже бегут не работает тест микрофона 1 раз это какой-то там 1 обработать веса спасибо дмитрий за доклад вы говорили про то что запускали в облаке ответственно вопрос 1 а сколько там виртуальных машин что была такая нагрузка в пиках полтора миллиона и так понимаю апдейтов до или риплейс авторских обновлений секунду и как это все скелетона чтение и запись то есть зависит ли чтение от записи эффекты для не друг друга и вообще вот как это работает ну в общем да на самом деле мы этот проект масштабировались раза четыре за всю эту историю мы начинали 4 машины сейчас их 18 вот и соответственно основная у нас ты конечно было масштабирование за записи то есть у нас сейчас в кластере больше двухсот not который обрабатывает непосредственно эти полтора миллиона рпс вот и у нас получается еще ну мы жестко разделили чтение и запись то есть мы читаем только с реплик потому что но задержки это милисекундные для клиентов они абсолютно него же не могут им пренебречь соответственно вас четкая одна часть кластер только пишет она читает водные примерно у нас там чтение записи получается в среднем две реплики на один мастер и у нас сейчас все это работает и достаточно это хороший гарантию спасибо друзья еще вопросы добрый день спасибо за год за доклад у меня собственно вопрос такое какой в итоге был алгоритм выбора шарда пойди щеку вот вы говорите то что можно сделать мод ну это самые элементарные а как а у вас а но у нас на самом деле мы используем встроенный в трансфер этой стороны 0 сортирование идет по виртуальным пакетом вот соответственно там есть встроенные функции сортирования ими пользуемся им и шар дерусь а мы как раз по красной книги магазина то есть мы по нему сортируем ся ну можно считать что-то масло так по модулю хотя конечно он что-то с хитрее но в общем суть держимся мы пойдем магазин понял спасибо еще вопросы друзья вот пожалуйста спасибо большое продолжение вот вопроса провал база данных ну про шарды говорили что пришли к схеме где вы за один запрос получаете всю информацию по магазину плюс все продукты там весь каталог продуктов вам пришлось просто копию этих продуктов положить везде или как вы добились вот этой желаемой ситуации но на самом деле я красе мне не говорю что он где допились мы хотим вот на самом деле мы как раз к этому пришли там было много разных вариантов и пока что мы остановились на том что там вопрос крайним копию в ту штанах не очень моды друзья нужен каверзный вопрос вот спасибо может сравнение в пост под весом в бою там что-нибудь такое вот есть да кстати про postgres у нас это я вам сами скажут пожалуйста добрый день аппликациями тарантула и стрижки есть между дат отсюда мире pliko cir & tarantula и стрижки есть да конечно то есть у нас как раз таки вся наша вся наша инфра находится в 200 до репликации с между 100 и в этом плане я из коробки просто вот та торренту мастер slave можно как-то были подробно где как дисплей вы где мастер и как пользователи входят такая задержка да хорош вопрос за счет у нас получается 20 да и весь класс сторон равномерно размазан на всех судах так скажем судя по 9 машину в каждом каждом мужчине одинаково количество нот тарантула соответственно у нас при этом весь кластер делится на рипли кассета которых там ну предположим 100 и в каждом рипли к сети у нас есть мастер репликант шард 1 шард и вот мастера по всем replicas этом они у нас размазана равномерно на 20 до поровну то есть у нас постоянно работает 20 до значит что происходит но при этом нас настроен of the fall over который тоже общем-то стран туре из коробки есть с помощью кессиди то есть у нас есть эти сиди у нас есть 3 коварный тот который как раз таки обеспечивает непосредственно кармане кессиди и авторы lover случае выпадения какой-то ноды он превышает мастера то есть случай вылета дата-центра у нас половина мастеров они перейдут на другой центр это займёт какое-то время там сколько то есть не секунд но это единицы секунд вот и начало скотта времени весь трафик просто будет обсужден ход то есть мы проверяли он выдерживает нагрузку все немножко деградируют не без этого ну а в целом система ставит в рабочем режиме пожалуйста спасибо за доклад о собственно говоря такой вопросик у вас происходит раз пять минут до как и понял приходит джексон на сколько времени джейсон кладется вашу базу данных то есть син 30 не происходит такой момент что приходится следующий джейсон оао данный старые поздно поместились 3 пальца в очередь или еще как то так сколько времени у нас на обработку данных из да ну да у нас назвали там не джейсона про the buff но дверки стену в принципе не так важно у нас действительно вся обработка синхронная мы начали думали о том чтобы сделать обработка синхронной но это нужно было водить доб сущность это не знакомит очередь кафку так далее мы не очень любим говорить доп сущности без необходимости и мы noize француз очень быстрые мы в принципе примерно прикинули обрабатывается синхронно и действительно у нас весь кластер полностью сохран обрабатывает эти полтора миллиона рпс и у нас получается среднем до из один запрос к нам приходит пачка из 1000 записей вот собственно в среднем одна такая пачка обрабатывается примерно 300 500 миллисекунд ну идет 9 проситель бывает скачет до 2 секунд ну всякое бывает вот на в среднем у нас тут за полсекунды 1 т.к. пачеко обрабатывается гарантированно им естественно не не страдаем не копим не где она просто приходит обрабатывается и все спасибо отлично если есть еще один вопрос задайте но мы с тобой отстрелялись красавица спасибо большое бурные продолжительные аплодисменты до пасибо большая"
}