{
  "video_id": "yS-b6PElRPY",
  "channel": "HighLoadChannel",
  "title": "Data Lake MTS / Григорий Коваль (МТС), Михаил Цветков (Intel)",
  "views": 1741,
  "duration": 2722,
  "published": "2020-04-14T11:08:01-07:00",
  "text": "здравствуйте я представляю компанию мтс расскажу сегодня про три основные моменты которые могут быть полезны вам в том числе то есть я расскажу первое это наша инфраструктура как она устроена как произошло становление произошло и установление развития и в каком состоянии она сейчас второе расскажу про важность эксперимента хочу отметить я согласен с предыдущими выступающими максимум единой спасибо им не только мы сталкиваемся с теми же самыми проблемами что эксперимент он достаточно важным то есть если вы не экспериментируйте перед внедрением свои архитектуры значит вы теряете очень большой процент оптимально работу вашей системе будущем и только эксперимент позволяет сделать кастомизированные под ваши кейсы архитектуру и также хочу остановиться на том что компания мтс и не стоит на месте и развивается нельзя сказать что это уже целиком оператор мне вот существуют такие подразделения как наши больших данных и мы используем open source и движемся вперед правим компанию в плане больших данных что позволяет получать достаточно хорошую прибыль структуру моего доклада сегодня такого я расскажу про источники данных какие у нас вообще они есть действительно у нас достаточно много данных с разных источников только вот как пример приведу одна из систем наша который получая данные слышат по всей стране генерирует примерно три с половиной миллиона сообщения в секунду и с этими данными нужно работать и только одна есть еще несколько таких так же расскажу кратко как у нас остро into the lake что что такое для нас до толик из каких компонентов состоит кратко также расскажу про архитектур насколько это возможно вас а сегодняшний доклад и какие-то space способы монетизации и как мы реализовали на нашей архитектуре основные источники данных мтс ну во первых это большая абонентская база мы сейчас мы переходим от абонента клиента то есть у нас много сервисов не только целиком соответственно здесь указана цифра 105 миллионов примерно это не надо думать что это ворота там и другие тонн всевозможные устройства это действительно люди но не все эти люди наши абоненты то есть клиенты других сервисов каких-то а также у нас большая set retail то есть на это около пяти с половиной тысяч точек продаж по всей стране там соответственно генерится операционная нагрузка и все это также является частью больших данных также вот последнее время растет рынок каюте мы наблюдаем за равно рост взрывной рост поступления данных устройств и вот по оценкам на по оценкам наших специалистов мы примерно уже сейчас 45 процентов симка встроенных войти занимаем рынка и также у нас достаточно большое большое количество мобильных приложений которые можно скачать там в android вас использовать с этих мобильных приложений мы также получаем достаточно большое количество данных изначально когда около 4 лет назад bigdata формировалась в мтс было очень много windows ких решений то есть у нас по наследству досталась нам большая система счас в которой были отчеты какие-то преобразования данных очень сложны сначала двухтысячных годов и как бы это до сих пор такой двигайся которая живет и мы избавляемся от него постепенно также потом сформировали такой ландшафтный дизайн можно сказать из разных систем внедрили teradata потому что пользователей становилась мог аналитиков много было и соответственно mtp система она была просто необходимых соответственно очень много было биой система различных то есть что затрудняло работу то есть не было единой системы их было очень много всего всевозможных и sqweel приходилось красным системам обращаться и потом схватить как-то аналитиком ну что-то же тоже неудобно ну и надо сказать что за время за последние несколько лет появилось много вот мобильных приложений уйти так далее данные тоже скапливались возможных небольших хранились всех своих собственных что тоже затрудняло общую аналитику по всем систему ну и вот такой вот вопрос что с этим делать и как жить аналитик она некую аналитику справиться с этим очень тяжело достаточно а теперь перейдём к нашей концепции дата лейка ну как сказали предыдущие докладчики основное то что мы можем теперь хранить неструктурированные данные то есть классическое хранилище не имела такой возможности так что мы можем накапливать данные источников то есть der коллировать так как они есть соответственно если у нас потом какие-то проблемы загрузкой какие-то проблемы с формированием дмитрий но мы всегда можем пересчитать более того эти данные необходимы для дата сайнц потому что тем более сырых данных получат с antis ты тем больше инсайтов они могут сделать ну интеграция данных как я сказал наличии 1 системы которая интегрирует себе данные с разных источников позволяет сделать дополнительную аналитику какие-то дополнительные вещи проявить систему это договор нас то есть у нас есть люди которые занимаются дата гарбуз то есть это по сути дела люди которые обеспечивают жизненный цикл данных накапливать знания вот как раз я потом дальше остановлюсь у нас есть систему которая обеспечивает такую такую радиацию знаний там хранится все известные источники аналитика по ним ну этот сайт то есть за счет того что тот олег он строится на современных технологиях на хату пи он в себя интегрирует очень много библиотек мы достаточно хорошо использовать статуса с утилитами соответственно легко перейти к data science мы конечно самое основное это получение бизнес в эту то есть понять какие у нас данные есть получить способа монетизации этих данных мы изначально вот когда диктата формировалась это около четырех лет назад именно кипя у нас были бизнес то есть мы получали прибыль и принципе за там за пару лет мы полностью оправдали все вложения у нас и достаточно хорошо продвинулись в плане понимания данных сейчас у нас примерно такая архитектура то есть мы оставили mtp потому что действительно на рынке open source mtp решение еще достаточно сыр и с нашей точки зрения поэтому оставили teradata но все остальные хранилища пытаемся заменить на а тут надо толик и в принципе уже сейчас у нас нас мы данных data лека формируется или отчеты и проводится достоинств у нас достаточно большая команда санте став там около 20 человек я думаю и очень много продуктов то есть там тоже десятки продуктов которые приносят демки отзовут а тут когда мы начали вот внедрять большие данные во все батареек в мтс в основном что-то скрывать раньше внедрение таких систем происходило вот for water хольна соответственно на выходе получали не совсем то что хотели там аналитики или там условия какие-то изменились а вот в плане работы с большим с большими данными оказалось что из-за джалла никуда соответственно бизнес формирует нам цель какую-то формируют требования мы выделяем из этого самое важное и делаем именно нужно для бизнеса ну приведу пример например у нас есть такой проект это в принципе замена части dv home которая состоит тоже из нескольких систем и в принципе там заказчику необходимо получить 100 метрик вот им полностью 100 метрик надо из основного количества этих метрик на основе данных они не могут проводить более менее значимый анализ какой-то но тем не менее если мы пошли в этот путь то есть сразу пытались реализовать эти 100 метрик там и на наверное застряли бы напал код в анализе только этих данных потом полгода реализовывали в принципе у заказчика могли измениться требования поэтому при работе с большими данными мы выделяем самое важное формируем продуктовый по их ног backlog продукте приоритизирует и идем дальше в разработку как я сказал кратко остановлюсь на архитектуре это like у нас состоит ну классический в принципе похож на все остальные то-то лайки на основе ходу по то есть мы используем хай spark альфа ну про и рф а вот в предыдущие докладчики сказали достаточно подробно думаю не стоит объяснять и алешин утолишь над как раз система знаний то есть мы внедрили систему для дата гувернеров которая содержит всю информацию о наших сущности в наших таблицах о том как joy нить правильно и вот это очень плодотворно повлияло на работу в батарейкам так что сейчас вот интересный проект на в apache появился этапа chatlas мы его пытаемся внедрить суть его в том что там как раз информация о наших процессов и цель должна содержаться и в принципе там какие то там метки о том что прошел data quality не прошел to the quality идти дальше или нет можно сохранять и процесс может ветвиться вот время принимать какие-то решения у него есть restful api вот сейчас мы пытаемся внедрить также мы написали свой компонент pets it's по сути дела это платформ и толкнул стул со расшифровывается по сути дела это набор плагинов крр flow который мы поддерживаем сами конфигурируем и таким образом получается быстро внедрять какие-то этого процесса так же у нас есть дата ривер что такое тот оливер это streaming платформа то есть внутренние содержится какие-то простые фильтры ну представьте себя есть такой большой поток данных вот три миллиона рпс например соответственно никакая система и when processing а которая стоит на выходе из этот вы не переварит не переварит такое количество сообщений то и все эти сообщения не нужны и нужны только те сообщения которые позволяют извлечь какую-то выгоду которую и соответственно to the river мы используем для группы фильтрации и каких-то простых правил аналитики также статоре верн связаны наш компонент это to the hub можно вот прахов это по сути дела тоже наса наработка то есть система вот группа фильтрации такой простой аналитики на данными соответственно мы используем к и велю хоронили еще для накопительные аналитики то есть например если человек мы знаем что человек прошел несколько вышек вдоль движения аэроэкспресса то мы можем предложить ему какую-то акцию соответственно здесь мы можем на как накапливать эти события и уже конечном счете в комплексом processing system отправлять которые уже какие-то предлагают акции достоинству достанется нас достаточно простой стек технологий то есть нас питон используется обучение в основном на стенд алон таких серверах происходит но также у нас есть до санс кластер отдельные это именно вычислительные ресурсы которые в принципе тоже используется для обучения для скоренко моделей мы используем busting искья boost там нет пустот и так далее также tensorflow и python для глубокого анализа из дивов стыка тут интересные моменты есть то есть мы изначально считали что нам необходимо виртуализация виртуализация нам нужно для того чтобы поднимать тестовые кластер а то есть без тестовых кластеров тяжело достаточно двигаться потому что всегда основной кластер нагрузил но если ты будешь там что-то разрабатывать проверять какие то гипотез это это очень тяжело отражается на всем процессе работы bigdata соответственно у нас достаточно большой облака там мы не соврут это около 40 видов достаточно мощных и там петабайт мы искали и на основе этого мы развернули openstack в котором можно быстро там за час и создать развернуть какой-то кластер который повторяет нам продакшен окружении это очень хорошо влияет на нашу работу то есть ускоряет процесс разработки также мы стараемся до пюре zero во все наши собственные разработки в качестве мониторинг а мы используем про метался графа ну и себя сидели на рынке то есть принципе стандартный стек теперь кратко остановлюсь вот на этом самом пиццы на нашем то есть ну как я сказал в пицце использует фото есть он посадил набор плагинов который так резеро ван который мы можем раскатывать на продакшене может разработчику себя там на ноутбуке поднять что-то разработать и за комитете принципе весь pipeline быстрый то есть у нас релиза происходит каждый день мы не ждем там больших крупных релизов иначе в принципе большая команда будет так работать с перерывом то есть вы всегда ждать так конца недели чтобы проявить какие-то кипоть и со своей проверить гипотезу а также мы не смогут предыдущие тоже the classic говорили что очень хорошие интерфейсу up a cheerful да действительно вам хорошее но когда там тысячи процессов он немножко подвисает соответственно мы допили 305 р плохо сами и используем основы вот того чтобы совместить его с мониторингом ну и получать статусы процессов ну и также от касательно загрузки в ссоры слой мы каждый раз не пишем эти загрузки то есть мы запланировали достаточно описать яму файл и поле того если у нас база данных содержит информацию то этом эту информацию мы можем получить это тоже ускоряет процесс разработки у нас сейчас там ну из разных традиционных бостон около 200 они тоже 2000 наверно до 2000 таблиц грузится и без этой системы это было бы проблематично то есть тогда каждому разработчику пришлось по полностью описывать все сущности так ну и альтернатива конечно есть есть бесплатно этот винтах я например но мы попробовали в самом начале нашего пути оказалось что ну достаточно нестабильно работает у нас было очень много проблем и все равно приходится допиливать поэтому как нам кажется лучше взять конструктор ветер flow который хорошо расчищаем у него хорошая архитектура и допиливать его с помощью плагинов информатику когда-то эдисон пробовали не завелась ну и в принципе вот мы переходим сейчас на 3 х туб как есть только technical preview 3 х дугу хотя он появился уже полтора года назад вот ну пока не видели стабильной версии также урока вот я и можно использовать тут мы даже не пробовали но говорят что хорошо но платно in apache apache вайфай в принципе во многих компаниях его используют тоже мы все три год назад или там больше попробовали он еще не стабилен был но сейчас возможно стоит его рассмотреть как альтернативу каких-то своих плагинов датах up to the hub это вот как раз сердца дата ривера то есть это платформой он processing а как я сказал группой фильтрации и какого-то простого какой-то простой онлайн аналитики она состоит из спад фермин кластер а то есть мы в принципе загоняем поток сказка на спад string кластер и там уже какую-то аналитику и фильтрацию проводим с помощью и выходную кафку выводим поток уже отфильтрованы и таким образом мы его еще и сохраняем вот олег и какие-то системы потребителя и интересно что вот нам потребовалось какое-то время чтобы понять что действительно нужно такие системы дело клиентоориентированный вот как раз спас gres позволяет на ходу менять правила аналитики то есть принципе аналитики из других систем могут задавать правила и они сразу будут применяться ну естественно аналитик должен понимать что-либо его действия могут привести к каким-то эксцессам так ну и вот дальше до the river в процессе построения дата ривера мы столкнулись с выбором то есть какую каверзу базу данных набрать для кэширования того что пользователь потом получали результат и вот это юмон аналитики и попробовали редис и а раз плайк как 2 очевидных таких альтернативы идти со мной что распайку проприетарный реки с открыты ну и получили так что редис на самом деле быстрее то есть он примерно в два раза быстрее отдавал данные чем aerospike но при этом он использовал много циpкa на клиентов то есть клиент когда вытаскивают из него данные там процессе реализации есть которые достаточно хорошо affected производительность соответственно для нас это не совсем прямо ему то есть у нас много потребителей и не хотели нагружать их циpкa и память соответственно мы выбрали aerospike и сейчас продолжаем с ним работать принципе под наши нужды он подошел дальше хотел рассказать про разные кейсы применения big data в мтс то есть вот есть у нас такая система на появилась внутри пик даты в самом начале уже три года развивается мы ее внедрили по всей стране то есть это система управления рабочим временем сотрудников точек продаж то есть действительно достанется алгоритм он расшит рассчитывает оптимальные оптимальные slade и выхода каждого сотрудника временные слоты в тут в точку продажи вот тут идеальную ситуацию нарисована когда каждый сотрудник занят клиентами прошлых дело бывают такие ситуации что сотрудник там один и большая очередь либо сотрудников много клиентов мало таких как раз такие кейсы мы оптимизируем исходя из операционной нагрузки на офис то есть мы видим когда там много пиратское то мало операция по часам также мы анализируем трафик вокруг точек продаж человеческий там по год и многие другие факторы вот тут интересно как мы построили модель расчета то есть модель расчета она использует простую класс функцию с правилами то есть мы правила содержат как в принципе информацию операционной нагрузки так и какие-то законодательные акты которые там способны принимать в расчет и модель у нас ну реализовано на этом стыке мы используем фласка 5 до того что вызвать is back and a rapid and you там накапливает сообщение там есть несколько очередей дело в том что модель можно перечитывать липа массово то есть на целую неделю там месяц по всем офисом или части офисов либо там каждый сотрудник может онлайн пересчитывать все и некоторые слова употребляются что есть по 16 раз пересчитывает он сейчас что-то срочно грузит иногда систему мы используем целый ряд до того чтобы развить вид процесс на несколько потоков и модель запускается на железе результат у нас кладется под gres который потом выдается пользователь и вот тут интересно то есть когда мы попробовали сканировать ну то есть изначально мы там варианте проводили на небольшом количестве офисов то есть все хорошо то есть у нас там быстро модель считалось а потом попробовали обработать ее на данных со многих точек продаж и получается что она там работала семь-восемь часов что для нас не приемлем при этом мы сначала посмотреть как можно тут force он то есть оптимизировать то есть посмотреть сколько нам потребуется железо чтобы посчитать это за час получалось нас серверов но тогда у нас вывели сервера как какие были то есть мы покупаем ну то есть мы изначально бронежилета которое было то есть у нас тогда было по третье поколения c пузе он и вот нас серверов ну что делать ну сначала попробовали быстрым способом оптимизировать вот связали синтолом они нам дали лабораторию где можно было загрузить данные проверить и примерно мы вы считали что от нового поколения процессор intel zeon 5140 нам потребуется всего 4 то есть такая интересная оптимизация а потом еще программно применили тогда вообще ну программной оптимизация у нас за бочек посмотрел что там дантиста наделали программно оптимизировали вообще там 1u сервер достаточно то есть получается организация 16 раз а ну соответственно из за того что у нас еще онлайн пересчет и мы все равно остановились на 4 серверах и достаточно хватает на всю страну то есть такая интересная темизация также вот хочу рассказать про скоринг для банков та же история давняя то есть банки они заинтересованы в том чтобы вы дать хороший кредит то есть тот кредит который вернется стопка человек его выплачивал и соответственно наши данные им тоже интересны то есть у нас that's on this ты также участвуют в этой схеме расчетов первое что мы сделали также попробовали на севере запускать модель которая потом в фласка пьянь банк мог заходить и пересчитывать эту модель ну то есть мы отдавали данные в вопию backend у который там затем выдавал банком по запросу то есть поступал какой-то эти клиента и мы выдавали пойти результат скоренко но оказалось что мы не могли эскалировать то есть всего 10 рпс систему давала осинка вы мы не могли использовать ну такая оптимизация достаточно очевидно потому что у нас только 2 питона переделывать модель у себя на 3 питон было достаточно тяжело попробовали пай fuji мы соответственно backend у нас уже назови был и java 5 попробовали поджи тут интересна такая особенность то что данные у нас хранились по клиентов в дикси на это есть в словаре данных соответственно конвертацию здесь и здесь сценарий питонов ска флешмоб java этим долгое время занимала чему мы пришли мы стерилизуем джейсон фиксируем его и в принципе вот такие тайминг и получаем то есть сканируется горизонтально и цивилизация там в 10 раз менее затратны но это вот такие вот пример оптимизации которые мы проводим ну и оптимизация всей инфраструктуры то есть изначально надо было планировать закупки железа а это очень большие суммы достаточно и как мы к этому пришли то есть опять же провели эксперимент потому что бы язык элемента очень тяжело что-то планировать а вот какие есть тесты на х туб ну это классический ты 1 сорт тест суть его в том что есть скрипты которые генерируют какой-то набор данных достаточно большой но можно задавать тон зависимость от необходимости затем пытаемся его отсортировать несколько потоков на ходу кто ну или там на спарке мы данном случае использовали spark версию а затем мы проходим еще раз по сортированный набор данных пытаемся от вальсировать что отсортировано правильно что это дает тир valeted позволяет понять как система работает фиксированными данными хорошо она кэширует или плохо насколько ускорения получается вот мы взяли опять же наши старые машинки и новые уже с оптимизированы архитектуры про архитектуру потом михаил цветков расскажет он в конце нашей нашего доклада и результат тестировано тоже хорошее то есть ну не в разы но основы архитектуры а еще не сказал здесь еще новый тип памяти использовался то есть так как у нас набора данных большие достаточно они не всегда влезают память и когда они ложатся в диске тогда происходит большой обмен и а вот хорошо было бы кэшировать достаточно большие объемы для того чтобы этого возрастания а ее не происходило вот и у этого есть ответ на это то есть у них новый тип ssd вышел often может слышали он позволяет расширять памяти оперативно она может быть медленнее но вот на работе с большими данных это отражается очень хорошо то есть там ну можно расширять то терабайт в оперативную память а может и больше но при этом тратить меньше денег потому что оперативная память очень дорогая вот ну результата вот такие то есть не в разы но вот на 500 гигабайт уже заметно то есть здесь уже почти в два раза есть возрастание производительности это было terragen тира сорт также в принципе attira валить эту здесь интересно то есть тут кэширования как раз выстрелил то есть здесь в три раза в три-четыре раза улучшения производительности оказалось так и попробовали еще более новое поколение здесь совсем все хорошо то есть там до 16 раз ускорилось производительность на самом деле вот мы сейчас процессе закупки таких цифр для нужд аналитики то есть да конечно мы еще не внедрили над от аноды наши кластеров но вот найти найти уже внедрили там достаточно хорошо нагрузка сканируется то есть мы получаем цифры хорошие нам меньше необходимо вот этих апликэйшен серверов покупать ну тут я изобразил резюмировал то что мы там проводили эксперимент какие но опять же вывод из всего этого то что необходимо экспериментировать перед каким-то принятием решения архитектурным нужно попробовать посмотреть несколько вариантов решения только тогда выбирать необходимо решение физик туре ну и не бойтесь экспериментировать так ну спасибо большое у меня все на сегодня дальше выступит вот михаил цветков сетевого intel россии он расскажет какие там оптимизации применялись в микроархитектуре ну спасибо спасибо григорий действительно эксперименты от коллеги смел экспериментировали с отварным стеком и не менее смело они пустились в эксперименты с аппаратной платформой выбор правильного инструмента он на для высоконагруженных систем критически важен то действительно экспериментировать экспериментировать экспериментировать с одной стороны понятно что надо все тестировать нельзя никому верить как говорил один небезызвестный персонаж но с другой стороны эксперимент эта работа вот в первую очередь они обратились к нам к российскому intel заставили нас хорошо поработать сделать тестовой площадки вот у нас очень большая тестовая площадка в нижнем новгороде там огромный сайт разработчиков наших под тысячу человек есть тестовой площадки в с винтами в британии и они в принципе открыты для экспериментов для тестирования основными архитекторами опять-таки важным фактором здесь было фокус на open source ные продукты intel в его с отварным обличье он является одним из ключевых контрибьютором в open source вот весь т.к. bigdata это области нашей контрибуции linux foundation опубликует регулярно там рейтинге к метров в kernal до торг и там постоянно в топовых строчках именно наши программисты open source а для нас это нечто родное и крайне важно вот о том эксперименте который я говорил здесь шла речь о переходе с предыдущей микроархитектуры поколения там обрадовало на новую микроархитектуру второго и первого поколения zeon скейл был как раз ключевые вещи которые там произошли это переход от ринг арчи которая дирка меж архитектуре межсоединений и такая значительная очень серьезная переработка под системы и о и под системы контроллера памяти вот интересная ситуация возникла в конце нулевых годов с выходом каров sky микроархитектуры когда микроархитектура опередила немного свое время софт который существовал на тот момент он был не готов к тому объему производительности которые на него свалился в платформе нам уровне и мы начали активно инвестировать в battle.net на тот момент в отсутствие нормального хранения нормального инструмента подачи данных и отвод данных от рост производительных ядер и в результате что в результате появился первый intel ssd в 2008 году твердотельных ранения и все это эволюционировало в вуаля вот в такую многослойную тире нглу иерархию хранения когда уже не просто там hdd дальше где-то высоко-высоко по performance у диром и отбрасывал кэшем ступенчатая такой вот гладкая функция возрастание производительности уменьшение задержки и соответственно изменения объемов доступного хранения сначала это sata ssd всем хорошо знакомый nand ssd дальше очень хорошие энви ми которая в отличии от sata нативно используют писяй экспресс протокол инвалид фактически писяй экспресс для хранения и писяй экспресс контроллер сейчас интегрированы прямо на кристалл крест цепью в результате упрощенный протокол sas sata это хорошее заслуженный там десятилетие уже стоящие на вооружении протокол для шпиндельных устройств но тем не менее твердотельное хранение она нативно значит более близко к цифровому писяй экспресса и самое главное вот тут есть классный такой инструмент это вот подсветим почти что самую вершину пирамиды это энергонезависимая память которая сначала появилась в виде н вадимов а потом в значит массовый сервер встала буквально в апреле этого года с пришествием второго поколения intel xscale family это как раз вот intel optane 10 persistent memory в виде ddr4 модулей сейчас вот ddr4 модель которая ставится прямо рядом с обычным де рама идет размерами 128 256 и 512 гигабайт и в отличие от де рама он умеет помнить что в него записали после ребута классическая вот мечта оптимизации потому журналирования у radiohead логом по сохранению тело базах данных сразу в памяти чем эта память может быть как на перво на его адресное пространство операционной системы процесса так и доступное в виде своей собственные dax файловой системы пожалуйста есть великолепный development kit pm дикие присутствует memory 9 кит выложенный на git хоп который как раз и является таким industry стандартом для и пифа для работы приложения нового поколения которое уже знаешь что память бывает разный память бывает не просто забывчивый но память бывают еще и постоянной это не блочное устройство которые там со своими нюансами и со своими проблемами это вот именно память который может адресоваться побайтно в данном случае она адресуется потом кэш line у процессора 64 байта или там 256 байт если в бёрст моде и пожалуйста все соответствующие плюшки по производительности доступны прямо здесь прямо сейчас эксперименты эксперименты и эксперименты и вот intel он занимается кантри бьются и поддержки вот этих вот новых технологий потому что технологии поколения серверов это не только про увеличение там количество ядер частоты и т.д. это важно нужно но более важно что в серверах появляется принципиально новые подходы к работе с данными и вот григорий которые очень активно смотрит по сторонам и планирует ресурсы так что работа работой но обязательно должно быть такой вот звено пас файтинга проверки всего что появляется и все что может быть потенциально релевантно к начата основной задачи он сразу это увидел пришел потребовал и в общем то так если есть требования значит есть и ответ есть возможности прийти измерить показать и потом рассказать самое главное рассказать о полученном поэтому современный сервер он а не только просит ее он в первую очередь и простых хранения которое начинается с де рама модулей и продолжается значит в виде независимый детей помимо persistent memory плашки памяти дальше вниз вами ssd envy me up тайную там и так далее вплоть до совсем уже большие hpi табачного размера сборок из келси nand и все это требует в том числе и изменения на уровне софта поэтому мы как раз и активно кантри beauty в open source начиная от там ядра linux где там 4 версии произошли активные изменения в части сторож под системы где там переписывалась остро режимных прерываний на pulling моду и заканчивая общем-то уже практическими стеками типа sparco big date ходу по но и если в сетевом стыке то openstack это все наверное что я хотел сейчас сказать в условиях ограниченного времени и в заключение хотел бы пригласить всех не стесняться вот у россии есть большое преимущество свой собственный офис интела в москве и нижнем новгороде в общем то писать спрашивать и по возможности будем вам в общем-то помогать пример вот такого сотрудничества когда разработчики четко подходит так своей задач не просто как как унылому production ну а как к настоящему научно-техническому процессу вот он на лицо наверное все григорий позвольте попросить вас выйти на сцену мы выражаем вам благодарность то что вы выступили со своим докладом то что внесли вклад в проведение конференций фил вас послушать спасибо большое вот у нас есть время на вопросы кто-нибудь хочет главное табличка года ну не стеснять добрый день меня зовут никита спасибо за доклад при горем как вам вопрос будет когда вы отлаживайте ваш и вот эти вот все pipeline и по проходу данных и так далее используете ли вы какие-то искусственные продюсеры для данных чтобы как-то генерить их или вы берете просто счастье из продакшна хороший вопрос спасибо действительно приходится брать частиц продаж но потому что нужно искусственных можно разрабатывать но это больше похоже на и не тестирование а вот реально тестирование все-таки уже близко продаж нет тем сэмплом которые используются это была такая такой вопрос то что это интересно с точки зрения безопасности тоже безопасность очень ревниво относится к сэмплом данных с продаж поэтому нас хенны закрытый контур для разработки в том числе соответственно следующий вопрос если у вас какой-то unit тестирование ну или как вы можете отследить регрессии в ваших знаками конечно обязательным и не тестирование то обязательно требования разработчиков окей хорошо спасибо андрей вопрос григорию хотел спросить сколько мтс занял процесс в это внедрение и вот текущий момент насколько это оптимизировано и будет еще кайт оптимизации мы только на самом деле в начале процесса внедрения ну процесс был построен так чтобы сначала работать на том что есть фильма не вдаваться в архитектуру и более менее такое хорошее архитектурное решение появилось в течение трех лет но оптимизировать еще хоть и много есть чего я думаю вот темы для выступления будет достаточно не знаю ответил на ваш вопрос и чтобы конкретно еще кого есть опрос здрасте григорий к вам вопрос график по увеличению производительности видели с ценой но наверняка гол процессом чуть дороже нужно много но да ну действительно сервера получать немного дороже но при этом их надо меньше мы всегда и до паланы фунтов просто речь идет о было пересчете не на сервера о пересчете на функцию то бишь и для выполнения определенного объема задача не важно сколько серверов потому что они могут стоить дешевле могут стоить дороже важно сколько их нужно вот и здесь идет там мощная консолидация в общем-то машинок становится меньше и в результате за на те же или меньшие деньги получается делать больше кредитного так вся полупроводниковая экономика это экономика постоянной продажи мощности дешевле чем раньше ну у нас было три вопроса теперь ваша задача выбрать вопрос который вам понравился больше всего наверно вот вопрос как происходит тестирование человек puffy больше выходите сюда и получите презент спасибо вам"
}