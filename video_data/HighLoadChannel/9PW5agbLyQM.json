{
  "video_id": "9PW5agbLyQM",
  "channel": "HighLoadChannel",
  "title": "VShard - горизонтальное масштабирование в Tarantool/Владислав Шпилевой (Tarantool)",
  "views": 1832,
  "duration": 3065,
  "published": "2018-07-19T13:21:22-07:00",
  "text": "меня зовут шпилевой владислав я вот и франками разработчикам я буду рассказывать сегодня про горизонтальное масштабирование трандлу при помощи модули в шар как будет устроен доклад я расскажу про то зачем вообще нужна масштабирование конкретно sharding какая тут связь шар дом в шар doom как они работают зачем понадобился в шар когда франки и уже давным давно весь sharding на шарик просто шар и как вы sha'n't использовать чем он нужен так зачем масштабировать кластер и покажу на примере допустим у нас есть сервер но хранит какие-то данные принимает запросы чтения записью у него хорошо и компания которая владеет данными на расширяется данной становится больше запросов становится больше клиент количество клиентов растет нагрузка на сервера на сервер возрастает не хочется как-то упростить му работу с этой нагрузкой и для этого есть два способа первый вертикальное масштабирование когда мы увеличиваем мощность сервера мы добавляем новых дисков увеличиваем количество я б я dirt но это тупиковый и дорогостоящей путь поскольку скорее всего количества данных и нагрузка у вас будет расти быстрее чем растет технический прогресс и как альтернативное есть горизонтальное масштабирование когда мы добавляем новые сервера объединяемых кластер нагрузка распределяется по этим сервером и горизонтальное масштабирование бывает масштабирование только по вычислениям когда данные полностью дублируются и масштабирую столько вычисления есть масштабирование когда распределяются и вычисления и данные я немного расскажу про каждый способов зачем они нужны чем может быть нужно масштабирование только вычислений например если у нас данных не много запросов на них очень много на посмотрим более сизо имый пример допустим у нас есть онлайн библиотека и они есть книжки и и вот выходит какой-нибудь бесцельно успехов приходит огромное количество пользователей чтобы скачать прочитать их и чтобы распределить нагрузку с одного сервера мы копируем ее на много серверов раскидываем их пользовании пользователь и такой шар не такое масштабирование называется в репликации второй тип горизонтального масштабирования когда мы хотим освободить место на каждом сервере данных много возьмем вместо книжек например хранилище фильмов в отличном качестве 30 иеговы и здоровые файлы и этот кластер пополняется фильмами нужно как-то раскидывать эти фильмы по серверам без дублирования данных и такой такое горизонтальное масштабирование называется шаринга sharding это довольно общее понятие и типов шарлин га бывает много они делятся по типу шар функции шар функции это способ которым раскидываются записи по физическим узлам кластер а я расскажу про два самых популярных типа шарден га коротко 1 sharding этаж одним карен джами то есть диапазонами смотрим на конкретном кейсе есть кластер который опять хранит фильмы и у фильмов есть год выхода и мы хотим в этом кластере уметь быстро выполняет запрос поиска фильмов вышедших например после какого-то года или между какими-то годами или да то есть запрос который затрагивает диапазон значений некоторые и шарди рование диапазонами позволяет сузить круг поиска физических узлов на которые нужно сходить чтобы выполнить запрос знаете об озон запросе мы можем знать какими узлами физическими пересекается исходить только на них есть еще sharding хэшами это sharding работает быстрее на точечных запросах когда у нас например нужно найти фильм по конкретному названию или по конкретному году выхода найти список фильмов shot функция она считают некоторую ну например хэш вот записи которые мы храним и по этим крышам раскидывает записи по физически музло когда мы хотим что-нибудь найти мы считаем cached ключа идем на тот же узел где хранятся данные и хэш он в принципе реализации проще относительно green j и именно на афишах реализован тарантул шард 1 шард links tarantul и там предельно просто он просто сортирует по хочу от первичного ключа всех стрессов и все и вот однажды у команды тарантула появилась задача которую решить с помощью тарантул шарда оказалось нельзя по трем фундаментальным причинам первое и самое важное tarantul нельзя обеспечить тарантул шарди нельзя обеспечить локальность связанных данных например у нас есть пользователь есть какие то данные связанные с этим пользователями которые хранятся в других таблицах список и мы хотим все эти данные вместе хранить на одном физическом узле то есть данные одного пользователя кучками разбросаны по кластеру они каком-то случайном порядке из тарантул серде такое сделать казалось невозможно и позже скажу почему вторая причина почему не получилось использовать транспорт это медленная и балансировка суть трантон шарда в том что когда мы добавляем в кластер новые узлы у него меняется шар функция она зависит от того сколько узлов физических в кластере находится то есть у нас есть данные которые еще не перенеслись и они могут найтись только по старой шар функции есть уже переехавшие данные на новые узлы которые могут найтись только по новой черт функции в итоге в процессе ришар ленка приходится использовать приходится каждый запрос выполнять пробоем шар функциями это медленно два раза и третья причина это неустойчивость на чтение то есть если отказал мастер например и у нас есть реплики тарантул сарт не всегда способен обратиться к ним даже если запрос readonly посмотрим на более сизо ему примере что я имел в виду в начале под локальность you связанных данных например у нас есть банк есть клиенты этого банка и есть счета которые у клиентов этого банок и связаны связаны эти записи по идентификаторам клиента банк и нам бы хотелось чтобы счета и клиенты хранились вместе физически на одном узле и почему здесь не получается использовать трандл sharp дело в том что он сортирует только по первичному ключу а мы не можем засунуть идентификатор пользователя к первичные ключи счета счета пользователя ведь еще одну пользователя может быть много счетов и тогда нарушится уникальность первичного индекс а если сортировать этот кластер просто так не заботитесь о локальности данных тогда может случится следующее нос пользователь его счет разъедутся на разные реплики чтобы с ними работать придется каждый раз бегать по сети и чтобы решить вот следуйте поставленные три проблемы был создан тарантул в shard тарантул в шар отличается от шарда ключевым образом тем что в нем виртуализованных ранение vertu с пирса рисованные узлы кластера то есть ваш от оперируют не физическими пользователь работает не с физическими узлами с виртуальными которые ваш орден называются бакетами в шар ту же оперируют пакетами они конкретными записями при этом для ваш орда один bucket этой единая и неделимая единица данных то есть она хранится всегда на одном физическом узле полностью и при ришар линки переезжают тоже полностью таким образом задача обеспечение локальности связанных данных сводится к тому чтобы научиться класть их в один bucket как же это делать в шарден посмотрим на примере того же клиента банк его счетов для того чтобы использовать эту схему ваш орден нам придется расширить схему добавив новое поле баки то и де бо китай диета поле которое служит триггером для ваш орда что этот space надо сортировать по кластеру и такие sprees и обрабатывают различными подсистемами нашего шарда включая при балансировщик ну и если у двух записи и одинаковые по китай не очевидно не лежат в одном баке ушах будет гарантировать что мы всегда можем найти вместе на одном физическом узле замечательно то есть чтобы локализовать данные надо у них установить одинаковые по китай дин как же тогда сделать перед этим посмотрим какой будет от этого профит вот за то ли мы одинаковые bucket айди что будет теперь как бы работал транспорт вот у нас есть клиент банка у него есть три счета и есть реплика есть чат есть кластер с тремя физическими узлами мы хотим найти по идентификатору пользователя список его счетов что будет если мы сортируем обычным шаринган него шар дом нас данные раз питаются пара 1 кидаются по разным физическим узлам и чтобы их собрать придется сходить на каждый узел случае кластер из трех узлов это целых три похода на каждый узел придется сходить если мы используем в шахту мы задаем связанных данных одинаковое по либо китай и ваша расположит их в один багет багет положен на один физический узел их все можно будет достать за один запрос ну и как это поле bucket найти значит теперь заполнять ваш арт это делает за нас или это надо делать руками делать надо руками потому что ваш арт не в курсе логике нашего приложения как мы хотим задавать этот макет айден ну и кроме того он оперирует бакетами а не отдельными записями он не вправе что-то в них меня не но заполнение баки той диета довольно простая задача на том же примере клиентов банка их счетов я выбрал бо китай и как crc32 этот индификатор а пользователю здесь можно было выбрать не обязательно crc32 это непринципиально можно было выбрать что угодно например можно было выбрать bucket айди как идентификатор пользователя по модулю числа bucket of единственное что нужно учитывать когда вы выбираете как считать баки найди что вам нужно примерно равномерно распределять ваши данные по баки там покажу на контур примере я вот вернемся к кластеру который хранит фильмы вышедшие в определенном году плохой пример будет взять bucket айди равным номеру равным году выхода фильма почему это будет плохо потому что с каждым годом фильмов снимается все больше и запросов на новые фильмы больше будут писаться много рецензии ставятся оценки то есть на bucket который отвечает за хранение фильмов текущего года будет находиться почти вся нагрузка это плохой пример вычисления bucket айди ладно допустим мы придумали как в нашем приложении считать баки то иди равномерно мы начали заполнять кластер его заполняем заполняем заполняем кластер заполнился и уже хочется куда-то часть данных девать расширять кластер как это делать и что будет самая главная наша функция которая считает баки 39 очевидно что оно зависит от количества баки то не наткнулся именно ту же самую проблему которая половой шар до старого что у него добавился новый узел ашар функция поменялось нет в шар детского произойти не может потому что количество пакетов это константа которую вы выбираете когда создаете кластер самом начале из этого сколько бы вы физически узлов не добавляли у вас количество пакетов то есть виртуальных узлов всегда остается постоянным и функция которая считает баки то иди тоже не будет меняться все время жизни кластера единственное что нужно учитывать что количество bucket of оно должно быть когда вы выбираете не меньше чем максимальное количество реплика сетов у вас в кластере которые когда-либо будет тут не стоит бояться того что количество bucket of constant на ему как-то ограничивает ресур ресурс масштабируемости кластер это не так его можно выбрать каким-нибудь совершенно огромным числом количество repack сетов либо кассеты гады которого никогда не доберутся например 10 тысяч сто тысяч пятьсот тысяч пакетов можно выбрать такое количество реплика сетапов вряд ли вы когда-то можно будет достичь так что это не ограничивает 1 курса масштабируемости кластер и номер баки то должен укладываться в количество пакетов то есть у нельзя сделать больше чем число body если а нет так ну максин просто поставить нельзя потому что количество пакетов будет влиять на на модуль который будет ну я позже расскажу которая отвечает за то чтобы сделать прозрачного прощения к этим сторожам кластера так вот забрали с тем что функция вычисляющую и пакета иди никак не будет меняться во времени жизни класть и вот мы все-таки хотим добавить новые узлы как-то сделать добавление новых узлов ударения существующих узлов выполнения failover и настройка и балансировщика подключения автоматических инструментов вроде ансип лизу keeping который стороны или кстате реализован у нас в открытом доступе находится оно выполняется все через единую точку через два сар башара стоящая который принимает новую конфигурацию чтобы добавить новые узлы в кластер мы стартуем инстансе тарантула пустые описываем каким мы хотим видеть новый реплика сет в конфигурации мы описываем url и и виды можно присвоить имена этим репликам мы указываем кто из них master plus live и разворачиваем эту конфигурацию на кластеры то есть применяемый о на всех узлах на старых на новые но отлично мы добавили узлы теперь надо как-то на них данные перенести нужно ли что-то сделать чтобы переносить данные найти узлы нет делать не нужно ничего шаг сделает все сам полностью автоматический процесс 3 балансировки что происходит когда мы добавляем в кластер новый узел на одном из узлов в шар до просыпается процесс и балансировщик это такой аналитический процесс который не занимается переносом данных но занимаются расчётом того как достичь оптимального баланса в кластере проснувшийся он опрашивает узлы кластере спрашивают у кого сколько баки товар и считает кто из них перегружен кто работает на легкие кто кому сколько bucket of можем отправить чтобы улучшить баланс кластер а то есть он строит так называемый маршруты bucket of и отправляет их перегруженным узлам перегружены и узлы получив маршруты пакетов начинают отправлять bucket и согласно тому куда им сказали это делать мы постепенно данные перенесутся на новые узлы и в кластере будет достигнут баланс помимо числа баки the free балансировщик еще учитывает писарев ли кассетах это такая опция при помощи которой мы можем задавать разницу в емкостях реплика сетов приведу пример например у нас один привлекать счет имеет емкость допустим 100 гигабайт другой рипли косит имеет емкость 200 гигабайт но такая вот разница получилась не успели там железку обновить везде не знаем когда сможем обновить тогда мы задаем ребалансить или что вот такой эта реплика сет может хранить два раза меньше данных чем другой репликацию и ребаланса сделает так что bucket of там станет два раза меньше чем на реках сети у которого больше памяти веса задаются тоже через конфигурацию через два царства очень цифры просто заданием пары чисел и все помимо емкости можно еще использовать леса например если у нас железо на каких-то рипли кассетах просто слабее то есть она не выдерживает меньше нагрузки чем железо других рипли кассету весов и такая абстрактная вещь который можете использовать как угодно то есть можно в качестве весов использовать относительно проценты багетов количество лайков можно использовать конкретные емкости реплика сетов какие и вот в том как выполняется регулировка ваша где кроется того как я решаю при как как ваша от решает проблему одну из поставленных ранее проблем про то что при ришар денги у нас приходится использовать 2 шард функции в старом шарди изучим не причем независимо от того перенесли ему уже данные кто ищем еще нет проблема это решается благодаря тому что в ваш орде перенос данных грануле аррен то-есть багет и переносится каждым рипли кассетам на новое место по одному и все промежуточные результаты фиксируются на диске и какая бы не произошла ошибка в процессе балансировки вплоть до падения мастера файла вера неважно какую ошибки после восстановления ваш арт самостоятельно найдет на каком боккетти он установил ли балансировку посмотрит что с тимба это произошло удалось его перенести не удалось если не у того что почему куда он переносил ваш опцию это автоматически проверит и поймет что в баке там делать перенести его еще раз или просто активировать на новом месте если он все-таки перенес до конца или ничего не делал рассмотрим так сказать под лупой перенос одного пакета чтобы понять каким образом достигается то что таким каким образом достигается гранулярный вот есть реплика сет нем хранится багет и он принимает какие-то запросы на чтения на запись есть другой реплика все просыпается и балансировщик и говорит ему перенести этот пакет на другой реплика сет первое что делает рипли кассет это блокирует запросы на запись чем он это делает затем что если запросы на запись не блокировать я вам начнет переносить это пакет на новое место то во время переноса придут обновления данные поменяются он начнет переносите эти обновления во время переноса придут еще обновления то есть нужно заблокировать пакет на запись на какое то время после блокировки на запись в шар собирает данные этого пакета со всех space of и начинает начинает их отправлять в мое место хранения простого как перенос закончился ваш рот автоматически вычищает bucket вычищает вычисляю вычищает старые данные уже перенесенные со старого реплика сета и активирует перенесенный bucket но новым реплика сеть все это происходит автоматически ну здесь было легко заметить что у нас все-таки есть некоторые проблемы с тем что bucket какое-то время недоступен на запись может так случиться что мы хотим чтобы этого не происходило например наступил вечер народ ломанулся в интернет и мы хотим чтобы у нас шарден который sharding который отвечает за хранение данных которые пытаются пользователю получить вот этим вот вечером ничего никуда не переносила был полностью доступен shot предоставляет возможность защититься от блокировки на запись первый способ это баки пин закрепления пакет за реплика сетом на которому сейчас хранится это выполняется не через три конфигурацию кластера через просто описку скоро же прямо у вас в ходе более того это поддерживаем транзакции то есть попинаем bucket что-то с ним делаем потому ван пины и мы можем быть уверены что пока мы с ним работали он был записан он никуда не уедет у нас usb 1 зачем это может быть нужно кроме как для защиты от 3 балансировщика например мы хотим чтобы у нас на каждом рипли к сети один из bucket of хранил какие-то мета-данные специфичные для этого реплика сету например для регионов котором он находится мы можем запретить или балансиры переносить этот пакет на другой рипли кассет запинок его или нам не понравилось наши шар функции мы вдруг решили ее поменять и у нас часть данных нужно перенести в другие bucket и тогда мы пинаем пару багетов переносим данные из одного в другой потом эти баки ты анпин и вот пара таких простых примеров какой еще есть способ запретить блокировка bucket of на запись блок целого рипли кассета это гораздо более радикальный способ чип-тюнинг одного пакета бог приводит к тому что рипли кассет не может отдать ни одного из своих bucket of другому реплика сет и не может принять ни одного нового боника это нужно примерно по тем же причинам что и пенёк багетов могу привести еще один пример допустим есть страна в которой проходит чемпионат мира по футболу и мы хотим записывать трансляции матчей этого футбола мы хотим и хранить в реплика сети который находится в этой стране и что наши матчи наши записи машине куда они уехали на другие рипли кассеты пока не закончится чемпионат тогда мы блокируем рипли кассет переноса пакетов сохраняем на него все трансляции чемпионат кончаются мы разблокируем рипли кассет или балахир его разгрузит подведем некоторые промежуточные ток у нас было изначально три проблемы для который был придуман в шар и то что не хватало локальности данных стараемся где была проблема что ришар ding выполнялся с двумя шар функциями вообще зависел от количество узлов в кластер и была проблема что нам требовалось стопроцентная доступность на чтение данных даже если мастером упал а реплика еще активно первые проблему мы решили самим фактом существования пакетов как виртуальном условия поверх физических узлов вторую проблему мы решили гранулярный переноса багета что же с третьей проблемой третья проблема решается модулем ваш орда полностью независимым от 100 раджей который называется роутер это прокси специально написана или ваш орда который инкапсулирует в себе логику поиска нужного стажа когда вам нужно кита данные найти посмотрим на пули осязаемом примере как может выглядеть работу с рокером чтобы понять что он вообще такое у нас есть кластер много узлов ну зла хранятся опять клиенты банка считаете их у нас есть цель найти среди всех этих слов счета одного из клиентов что мы делаем мы реализуем функцию поиска клиентов на каждом из этих ста раджей в ходе своего приложения которое выполняет эту задачу локально после чего нам нужно теперь выбрать на каком стороны нужно эту функцию позвать мире нотификатор пользователя из-за нас это сможет сделать рокер что мы делаем чтобы воспользоваться роутером вы вычитаем по китай и и передаем роутеру этот bucket айди функцию которую мы хотим вызвать ее аргументы роутер берет этот баки тайги находит стороны котором хранится этот bucket кто и нам нужен на нем выполняет функцию которую мы ему передали с нужными аргументами возвращает то что эта функция вернулась со стороны то есть вся работа сетью поиск нужного стороны все это скрылась за вызовом кол получился такой как бы нам на бокс таран тайский только работающий ни с одним тарантулом с кластером трампу но это и не единственная автоматика которая делает роутеру не только занимая сро кингом запросов но еще он решает ту самую третью проблему с доступностью данных на чтения когда мастера недоступен делает он при помощи автоматического файла вера рид запрос например у нас есть реплика сет в нем отказал мастер отказала несколько реплик но какие-то реплики еще живы и мы хотим читать из них данные и роутер будет их запросы направлять на те реплики которые еще живы то есть мы помечаем каждый запрос к creed как readonly или как ридера и ты роутер readonly запросы способен направлять на реплики не только на мастеров ну и пользователю предоставляется возможность управлять тем какие реплики рокер должен предпочитать если мастер отсутствует сдается и при помощи присваивания зон репликам и роутером и задание матрица зону матрица зон это просто попарно и расстояние между каждый это расстояние между каждой парой зон смотрим на более конкретном примере как может выглядеть использование зоны и весов ну расстояние между зонами пусть у нас есть три зоны мы хотим задать расстояние между каждой парой мы в конфигурационном в конфигурации задаем эту матрицу весов как обычную таблицу где в ячейках пишем расстояние и запихиваем в конфигурацию и делаем shard роутер цпг такой аналог сторож сафага он принимает в точности точно такую же конфигурацию что и сторож и каждый из них в одну сторону рокер вытаскивает нужные поля ему можно держать один конфликт на весь кластер ну и что здесь может иметься в виду под зонами например самый очевидный вариант это разные дата-центры второй вариант зон это могут быть просто географически удаленных друг от друга в области или зонировать можно реплики с разные производительность например мы знаем что у нас есть какие-то реплики которых очень большой объем но они просто тормознут и они предназначены для приема запросов их держим для бэкапов и на них роутер должен обращаться в последнюю очередь тогда мы засовываем их в одну зону и делаем расстояние до нее огромным чтобы рокер к ним ходил только в крайнем случае ну раз уж я заговорил про failover что шар дас райх фоловерам или иначе говоря со сменой мастеров он не автоматический это не точно не тоже самое что рид failover но но он полуавтоматически можем так сказать то есть задача которую должен решить пользователей это определить что время сменить мастера а саму смену мастера ваша рту же может помочь сделать например у нас есть три реплики одна из них мастер не захотели мастера поменять что мы делаем мы в конфигурации в одной реплики у старого мастера флажок мастера снимаем но в мастера флажок мастера ставим применяем эту конфигурацию на кластере и все на этом ручная работа заканчивается начинают работать автоматикой что происходит при этом очевидно что райт хилого это не просто смена флажка как минимум нам нужно перед тем как нового мастера вводить перед тем как новый мастер начнет принимать рай запросы нам нужно синхронизировать данные которые успели накопиться на старом мастере с остальными репликами и эту задачу красный шарф решает что он делает он принимает новую конфигурацию которой указано кто должен быть новым мастером старые мастера перестает принимать райт запросы по той же причине что и перенос баки мы будем принимать запросы мы никогда не синхронизируемся полностью он перестает принимать запросы синхронизируется с репликами после синхронизации с реплика в силу вступает в новые мастер он принимает все запросы а старый мастер становится своего все это делается автоматически ok получается у нас ваш ордер есть bucket и они могут переезжать туда сюда есть какие-то веса зоны есть роутер и не все вещи ваш эрде автоматизированы например смена мастеров не автоматизирована до конца и хочется как-то следить за всем этим изобилием всяких фич как они работают и триггере какую-то автоматику которой нет ваш hd встроенный делается это при помощи api мониторинга которое присутствует и на роутере на стороны представлен двумя функциями шанс torosyan for cash or draw тиринфа я сейчас подробнее разберу вывод каждый из них начнем со стороны вывод сторож инфо делится на четыре секции встреч в 1 секции сторож info показывается состояние состояние реплика сета к каким слой вам у нас есть connect и есть коннект к мастеру такая задержка репликации и проще аппликационные параметры есть вторая секция которая указана сборная статистика по баки там которые хранит данные реплика сет нем указано сколько bucket of новым способны сейчас принимать ридера и запросы сколько баки то доступны только на редон ли например они сейчас переносятся какие баки ты сейчас находится в процессе отправки в процессе получения сколько bucket of закреплено за этим реплика сетом сколько bucket of уже уехала и сейчас в шар занимается их выпиливание и посколько bucket of может быть очень много по статистику по конкретному баки то можно получить отдельным вызовом наш арт сторону buckets in for есть 3 секции на мой взгляд одна из самых полезных это список предупреждений которые предупреждения проблемах которые вы шанс можно затыкать автоматически например какие могут быть проблемы у рипли кассета слишком маленькой redundancy например у нас меньше трех реплик то есть если еще одна реплика откажет или мастер у нас будет беско ситуация к потере целого рипли кассета и на ритм райн также здесь в этом списке warner сообщается о том что репликация отстает слишком сильно или мастер в конфигурации не указаны удобные вещи ну и четвертая секция состоящая из одного единственного поля она демонстрирует степень более рипли кассета это такая лампочка буквально лампочка она предназначена для того чтобы в gui вывести определенный цвет у этого рипли кассета в каком он состоянии это чисел к 0 до 3 если нового реплика сета все хорошо если три у него все плохо и она вычисляется на основе у всех остальных трех секций например если в рипли кассеты отсутствует мастер то будет двоечка кассету очень плохо если только сердце недоступен вообще целиком тогда ему конец там будет троечка примерно так же выглядит мониторинг роутера у него тоже 4 секции но они немножко отличаются смотрим первую секцию это connect и к connect и к словам и к мастерам роутер когда запускается он устанавливает connect и каждому слою к каждому мастеру в кластере периодических пингует проверяю на проверяя их недоступность и в этой секции он укажет у каких рипли реплик какие реплики недоступны какие мастера недоступны в этой же секции он указывает на каких реплика сетах сколько bucket of он нашел какие из них сколько из них доступна запись сколько доступно на чтение детальная информация каждым рипли к сети и вторая секция у него тоже пакеты но shard оперирует несколько другими вещами чем состоянии баки то типа отправка прием для для роутера важно доступен для baked на чтение доступен на чтение и запись доступен ли он вообще и нашел ли роутеры тот bucket в кластере то есть есть такие четыре состояния пакетов и третья секция тоже предупреждение они уроки немного отличаются от 100 раджа например у него есть автоматически рид failover если the track file аверс работал то есть какой-то мастер пропал то здесь появится вор ник что рокер пользуется репликой они мастера для чтения или например ну рокер он же не может мгновенно узнать расположение всех пакетов кластере и в этом списке он отображает сколько bucket of он еще не смог в кластере найти и заполнить свою таблицу маршрутизации по которым наши запросы раскидываю ну и четвертое поле у него точно такое же это степень более роутера его лампочка который тоже зависит от того какие вардинге он нашел какие connect он не смог установить мы доклад подходит концу что же мы в итоге получаем что такое кошарица зачем он оказался нужен он оказался нужен для задача когда нужно было обеспечить локальность связанных данных то есть когда данные связаны надо хранить на дно физическом узле чтобы обеспечить не только быструю доступностью не нужно было бегать позитивную fiat more ность например гораздо проще использовать транс тульские транзакции на одном узле чем реализовывать собственные двухфазные транзакции на нескольких узлах кроме того оказалось что вы sharp устойчив к ошибкам при балансировке например если при запуске ошибка переносе 1 баки то она не затронет остальные bucket они как мой он предоставляет автоматический фолловерит запросу вот для чего оказался полезен в шар всем привет давайте перейдем к вопросам пожалуйста добрый у меня просто баре балансировки всегда по закону жанра у нас и балансировка происходит час пик у вас это как решается при балансировка ваша где происходит как только происходит конфигурация кластера если вы не хотите чтобы регулировка выполнялась в час пик начнут реконструировать его пользу либо есть еще способ такой например вы хотите поднять новые реплики но и не переносить на них баки то сейчас отложить этот процесс на потом тогда здесь помогут весах реплика сетов вы у новых инстансов которые вы подняли ставить вес в 00 означая что эти инстанции не будут хранить ничего вы их вводите в кластер ваш irda или балансе выведет что у них west вор и ничего на них переносить не будет но при этом брокер будет их видеть он будет их пинговать проверять их состояния да вот ли балансировщик он это процесс который просыпается а случайно во время его начинает что нужно перегладили нет не случайным просыпаются как только вы вызвали конфигурацию пластью уже раскатываете конфликт на все узлы класть рано или поздно он доходит до балансира готовую владели балансер живет там он просто только при перегонке реконфигурации происходит ли балансировка да теперь а как вы что вы делаете когда запись закрыто bucket запросы должна клиентская сторона ретро и смотрите тут есть способ избавиться от блокировки на запись на совсем нельзя но есть способ уменьшить время блокировки на запись до минимума например вы делаете число bucket of огромным тогда каждый bucket будет меньше и переноситься будет быстрее и тогда есть шанс что bucket перенесется быстро вещается и везите что перебиваю чтобы в другим да теперь на вопросах отличается ли ответ сервера при балансировке и вы понять где работает рокси то есть их может быть системе на каждый backend на каждой только угодную проще можно масштабировать отдельно от стран джей в любом количестве их можно сделать один на весь кластер можно сделать и в 10 раз больше чем 100 раджей можно в 100 сколько угодно их можно сделать а не масштабируются полностью независима и когда мастерил ложится то что происходит у вас просто уведомление приходит или во первых на всех роутерах полу появляется alert что мастер не доступен во вторых на каждом уроке рис работает автоматически рид failover то есть хорошая типа а ещё 30 секунд а когда читаем то можем управлять откуда читать осваивались мастера да вы можете у них расстояние веса сконфигурировать если вы не появились два запроса один должен стать только смотря другой мужа да да смотрите я там написал в метод колу на самом деле у роутера трения туда колку лера и color в если вы хотите обратиться к мастеру вы пишете color в то есть кол-ве drive иначе вы пишете колера кол readonly как и от этого зависит куда запрос пойдет спасибо за доклад старая функция шарди рования она вообще еще нужно если вот у меня нет необходимости в локальности данных тогда возможную ваш арт вам и не нужен но он же многие другие проблемы решают мы если они у вас стоят вы можете его взять и как бы он не хуже старого шар danone виртуальные bucket и те же гораздо удобнее мне так понимаю это то же самое что винода кассандре в той же я не скажу за кассандру но это то же самое что баки в коуч поэзию например ещё такой вопрос а зачем нам мастера общем смысле ну какие проблемы он решает но если у вас будет мастер мастер ну вот рассмотрим один реплика cetus вас будет мастер мастер вам придется решать проблему как не допустить конфликтов при записи разных данных на этих мастеров примеру вас прилетела запись на один мастер вы не знаете записался да или нет там не знаю вам пришел тайм-аута запись на самом деле записалась и потом вы ретро ети ее она попадает на другого мастера и они друг друга реплицируют и вступают в конфликт и эта проблема будет поэтому мастер мастер здесь так просто не получается использовать понятно она это за счет того что у нас изменяемые данные спасибо здравствуйте спасибо спасибо за доклад в 1 раз мы заговорили про мастер и реплики расскажите какой протокол используется для обеспечения репликации то есть это что-то известный для обеспечения репликацию используют встроенный таран тульский протокол ну а вот если вы видите пару тройку consist носи availability и портишь intolerance тут такие гарантию на эту вот смотрите если у вас данные локальные то вам обеспечено консистентной просто используете транзакцию работаю странами локально на этом физическом или но вопрос не выжил от транзакции по транзакции мы не говорим то есть меня допустим запись единичного ключа да то есть и не трогаю много ключей вот и соответственно у меня кожа кость хочешь тут у нас будет доклад про sharding про репликацию и давайте эти все вопросы надо хорошо хорошо там вот точно долго человек руку тянет можно здравствуйте а вот у вас здесь есть похожая ситуация у них нельзя менять количество пакетов там их там 16 из 16 а если у высшая до какой-нибудь камин дуем а и количество пакетов для старта вот допустим мы возьмем предположим что мы ничего не знаем и вот какое-нибудь число которое надо взять не думать по умолчанию в вашей где уже стоит количество пакетов 3000 это принципы оптимального смотрите тут весь как бы компромисс если вы делаете больше bucket of у вас каждой папе становится меньше переносится быстрей блокируется на запись меньше данных при переносе но на каждом роутеры у вас будет расти размер таблицы маршрутизации например если у вас 500 тысяч bucket of the эта таблица будет занимать порядка 5 5 мегабайт насколько я помню если у вас всего 3000 bucket of the вряд ли у вас когда-то будет 3000 реплика сетов среднего город придает у среднего сервиса этого хватит чтобы таблица маршрутизации было порядка килобайт of и при этом баке ты переносить с достаточно быстрее недолго блокировались на запись спасибо за доклад скажите что происходит если я меняю конфигурация пытаясь поменять мастера старый мастер при этом лежит если старый мастер лежит то новой просто заработает но у вас данные не получится синхронизировать 2 мастера и в tumbler да на реплики между собой не договариваться у кого нет этого пока нет планируется удар планируется еще вопрос немножко непонятно про реконфигурации balan обязательно ли на балансир выкатывать конфигурацию последними если да то власти обязательно смотрите ваше руб устроен так что не обязательно мгновенно выкатить конфигурацию на все узлы даже если вы будете конфликтовать и целый день там по обновляя по одному узлу в час к стесненность у вас от этого данных не нарушится либо если вы расскать его сначала нари балансира он постучится в другие ноты увидят что у них еще не обновилась и просто замолчит будет ждать пока ну то есть там улыбалась и у него есть два состояния как бы он совсем спит он просыпается раз сейчас просто на всякий случай проверяет что все нормально и есть активное ожидании то есть если он в тех tell что конфигурации изменилась но у него никак не получается провести ребаланс р-ре placing тогда он будет просыпаться не ас в час раз десять секунд и постоянно проверяет что вдруг уже везде конфигурацию обновилась уже можно начать при балансе так он будет тыкаться пока не не запустится а что значит обновилась он сравнивает конфигурация один в один то есть нет он не сравнивать конфигурацию он смотрите если у вас например на новые узлы конфигурации не раз катилась там в шорт вообще не запущен там нету стороже там нету список который хранит эти пакеты если вы шар куда если барибал acer туда придет он там ничего этого не найдет и и получит ошибку и прибудет ретро ведь потом не но мы допускаем что мы поменяли вес существующих реплика сетов то есть везде все есть но конфигурация не консьерж ли вы поменяете только вес только будет достаточно раз катить нари балансер и он без сравнения этого этих лесов с остальными репликами приведет ли балансировки как при этом будет вести себя роутер рокер при любой балансировки неважно это веса или добавление новых узлов или удаления узлов он него есть фоновый процесс который которым он ходит на каждую реплику периодически раз десять секунд и скачиваете нее список пакетов если у вас с которой школа трипле кассета часть bucket of уехал и роутер на неё скоро сходит увидит что они уехали увидят куда уехали обновить свою таблицу маршрутизации с зачем тогда на роутер раскатывать конфигурации давайте у нас тайм-аут срабатывает юрта коридоре получится помучаюсь влада зачем раскатывать на rocker подарите например валидацию видов вот вы завтра вы в конфигурации с дочерью еды какой реплики какой вид и могло случиться так что у вас произошла ошибка конфигурацию вы там случайно опечатались знаю вас реальный вид тарантула отличается того которые вы задали в конфиге и роутер это будет проверять он проверяет валидность конфигурацию помимо всего прочего он когда будет конектится к репликам сверит удивить который он от нее получите который по его мнению и который указан у неё в конфигурации если мы ведь несовпадения тонкой от реплики ходить не будет и выдаст alert в своем мониторинге ну вот например для этого еще скажите пожалуйста на сколько сильно размер таблицы маршрутизации роутере эффект теплота и таллуа поэтому там принципе не очень сильно unaffected кроме того таблицы мешать это конкретный язык смотрите мы скажем там это таблица я могу сказать я могу сказать рпс которые мы мере и было два реплика сета в кластере в каждом по три узла то по моему по три узла был один роутер нем было три тысячи пакетов он выдавал 100000 рпс кластере один роутер при этом столь же не были загружены даже наполовину по моему можно было добавить еще один роутер еще один робот увеличивать вес при этом 3000 это уже довольно много ну и посему вот еще тут блокировка на запись при переезде bucket of master of так далее планируется ли реализации механизма который с ней живет она должна и выполнение что-то для ресниц и ближе эверест до планируется и уже есть план как это делать он состоит в следующем вместо чтобы блокировать запись благ пофиг на запись полностью моризо версию разрешим в него писать и будем параллельно переносить данные за каждый поход и балансировать сюда будет переносить часть старых данных и обновления этих данных который он уже перенес в какой-то момент получится так что он перенес все что было и теперь занимается переносом только обновлений в этот момент перенос баки то все-таки блокируется на запись но здесь получается что напали на гораздо более короткий промежуток времени то есть даже если bucket будет огромен и переносится долго вас блокировка на запись будет только в самом конце они но все время переноса как получаются оптимальный тонируется так делать спасибо спасибо вам за доклад у меня вопрос скорее по поводу терминологии реплика сет содержит в себе какой-то набора багетов ноги отношении master и slave новость марте мастер реплика ну то есть репликация данных она поддерживаться на уровне rip рикошетов фактически есть разные машины которые содержат один и тот же набор пакетов да то есть это реплика сцена когда происходит ли балансировка у нас на запись блокируется при а именно реплика сайт правильно с этим вопрос зачем нужно два уровня абстракции реплика сет и bucket почему нельзя поддерживать отношения масти реплика на уровне пакетов тогда мы бы мы должны были блокировать только один баки на запись единственные bucket и были бы открыты они неправильно понял блокирует в только один baker не весь реплика все то мы переносим один пакет и на это уже рипли к сети еще кучу других bucket of они будут продолжать работать и на чтение назад и все-таки почему двор не абстракция почему вы поддерживаете одинаковый набор пакетов на разных реплик ассетов мы идем нужно разных нет на разных реплика сетах разный набор пакетов одинаковый он внутри рипли кассету но его реплик то есть по сути и приказ а это физическая машина несколько физических машину реплика мы вели это несколько инстансов на одной физической машине скажите а вот и балансировщик который просыпается это какой-то централизованный процесс дает централизованный процесс который живет на одном из мастеров в кластере но то централизованной сразу стираются за словом что этот узел умрет и все пропало ribbon acer не работает но это и балансе стоит лесу свое состояние нигде не хранит может свободных переезжать между узлами сколько ему хочется есть если узел с ре балансиром умер то автоматически назначается удара лицу если он умер вы покидаете вас рипли кассета ребаланс ruby red просыпается на другую музыку это ручная операция выкидывание реплики выкидывания узла изрекли кассета да это надо его из конфигурации удалить времени всю эту конфигурацию и все про это препрег асет забудут спасибо спасибо"
}