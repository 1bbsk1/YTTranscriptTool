{
  "video_id": "TmX8pbM1rUI",
  "channel": "HighLoadChannel",
  "title": "Асинхронный транспорт Cassandra / Вадим Цесько (Одноклассники)",
  "views": 466,
  "duration": 3123,
  "published": "2023-10-06T07:18:34-07:00",
  "text": "так окей Всем привет Меня зовут Вадим цысько Я ведущий разработчик в команде платформы Одноклассников сегодня расскажу вам как мы запилили свой асинхронный транспорт в кассандре и эксплуатируем его уже три года но сначала Пару слов про Одноклассники помимо того что это крупная соцсети которые пользуются десятки миллионов пользователей Одноклассники являются технологической платформой VK и на нашей технологиях работают такие сервисы как ВК видео ВК звонки рустор и некоторые другие у нас много железа в семид это центрах мы отдаем много дробит трафика выжимаем из железа ну или пытаемся максимум храним уже больше одного это данных и 99 процентов кода У нас написано на Джаве и сегодня в докладе будет много Java Ну речь пройдет про транспорт Кассандры немножко Давайте вообще поговорим про то как мы используем кассандру так если в зале пользователя Кассандры так вот Давайте поймем кто использует четвертую кассандру один два человека круто третью так 3 несколько вторую а 06 Так ну вот пользователи очень много было что-то сумму не складывается Ну ладно хорошо мы у нас Кассандра это основной сторож мы вторую кассандру давным-давно и уже столько всего переписали что это отдельный продукт про наши использование Кассандра есть много статей докладов вот рекомендую статьи Олега Настасья во все сноски ссылки в слайдах кликабельны у нас больше сотни кластеров самый большой кластер больше 600 нот Но если просуммировать по всем дата центрам что будет важно в дальнейшем В общем случае у кластеров очень много клиентов тысячи но взаимодействуют кластерами Кассандры а самый нагруженные кластера держат 150 тысяч запросов на ноду это не гет и пуд по ключу это бизнес-запросы с кэшированием сканами и так далее и мы активно используем спекулятивные запросы То есть у нас клиент мы Обращаемся ко всем репликам и ждем кворума самых быстрых ответов Это позволяет летом держать на очень низком уровне А если у нас происходит сбой или отказывают дата-центры то нагрузка на остальные ноды не меняется Мы всегда все надо опрашиваем все нужные реплики Ну чтобы понять проблематику Давайте разберемся как в стриме Кассандры работает взаимодействие между нодами вот у нас есть ринг он состоит из нот кластера есть клиент клиент может обратиться к любой ноде Это нода станет координатором для его запроса но до знает всю топологию кластера Она же участник кластера и она обращается к нужным репликам Ну по ключу вычисляет где данные лежат дальше координатор получает данные с реплик мёджетах в процессе дает клиенту Ну что хорошо в таком подходе Да клиент взаимодействует по внешнему протоколу он абстрагирован от устройства кластера Ну почти а внутри Кассандра взаимодействует по внутреннему протоколу можно ее апгрейдить прозрачно Для клиента обычно Ну и удобно Да вот диплом и все Ну на самом деле на этом преимущество заканчиваются в принципе Давайте про недостатки Это плюс один сетевой хоп это гораздо больший объем данных это стерилизация то есть память потому что координаты он должен расплатить запрос и сравнить их потом стерилизовать ответ клиенту Это задача read Repair который лежат на плечах координатора он мало того что дает данные из диска он еще должен сравнивать ответ от реплик и чинить расхождение и конечно Это несколько сценариев отказа вот мы поставили запрос на координатор в ответ тайм-аут и как-то интерпретировать можно обратиться к другому координатору и поставить запрос но возможно старый молоти данные и новым запросам мы еще больше нагрузим реплики потому что запросы пойдут на те же реплики В общем всё сложнее становится поэтому мы в Одноклассниках используем так называемый паттерн фетклайн у нас клиент является участником кластера и напрямую обращается к нужным репликам клиенту взаимодействует по внутреннему протоколу Кассандры клиенту участвуют в гости он знает Всё топологии кластеры обо всем что происходит внутри кластеры понятно что клиент это прямо Сандры которые не хранит данные задача или дрипы переносится на клиента клиентов много поэтому у них много ресурсов поэтому все хорошо масштабируется ноды занимаются тем что они умеют лучше всего они работать с данными на локальном диске Ну вот если все резюмировать все это нормально работало Но даже в таком подходе есть несколько нюансов про которые хочется поговорить во-первых в общем случае клиент взаимодействует с кучей кластеров Кассандры но внутри нашего клиента работает код Кассандры Потому что клиент является участником кластера А если открыть исходники Кассандры то внутри все построено на глобальных переменных Ну никто там не планировал что в одном процессе будет несколько нот запускать мы не очень понятно как они это тестируют еще то есть везде паблик статики вот везде везде схема Go Super все все втыкано в паблик статиках если вам нужно отправить какое-то сообщение вы берете там сервис глобально и говорите в ту ноду отправляя сообщение ну и когда у нас несколько кластеров У нас есть проблема Ну в лучшем случае все эти кластера с мёджится через этого клиента он же участник сразу нескольких кластеров Ну я честно говоря не пробовал в худшем случае я думаю вообще всё понятно будет как решить эту проблему здесь приходит на помощь кастомные классные Джаве мы можем для каждого кластера завести изолированный класс лоддер и глобальные переменные будут жить вот в этом зарезервированном пространстве и тогда кластера никак друг с другом не смогут пересечься но правда в Аде все это дебажет не очень приятно Они не любят кастомные классные что еще из неочевидного поскольку клиент-участник кластера любая нода может захотеть с ним пообщаться соответственно нужны сетевые доступы в сторону клиента в принципе мы всю инфраструктуру свою контролируем это не такая проблема но было неудобно надев окружение потому что разработчики не могли на своем ноуте запустить клиента Кассандры потому что они обычно за пьяным она там сидят но особенно на удалёнке следующая проблема более важная вот я рисовал там стрелочки какие-то в кассандре Используйте tcp для соединения между нодами и по соединениям бегут сообщения но на самом деле все несколько хуже но до Кассандры умеют использовать соединение только в одну сторону если надо один хочет отправить сообщение ноте два она устанавливает Коннект и пихает в него сообщение если надоед отвечает она установит другое соединение и отвечает через него на самом деле всё ещё хуже А все сообщения по типам делятся на две группы э ну по сути с разными приоритетами для каждой группы выделяется своё соединение Ну пойдем дальше там использовал олдскульный подход Red per Connection каждое соединение с двух сторон обслуживает поток вот outbound incomment tcp Connection это 3D прямо и у нас клиент тоже является участником кластера то есть для него применимо все тоже если вы вспомните какие-то цифры из начала доклада то можно прикинуть что вот у каждого клиента есть по 4 потока для каждой ноды кластер с которым он работает и все симметрично для каждой ноды по четыре потока для каждого клиента и всех других нот Ну помню у нас там тысячи клиентов сотни нот в класть и по сути в каждом процессе работают тысячи потоков И ладно бы эти потоки спали и ничего не делали там для ядра это в общем ничего не стоит Но они все активные реально по этим соединением непрерывно ходят сообщения Мы помним что у нас нужно еще сетевой доступ делать в обе стороны в общем к чему это все мы приходим к целям которые мы ставили при разработке нового транспорта Ну во-первых упростить эксплуатацию перестать ковырять дырки в сторону клиентов то есть соединение только клиентов к нодам и между нотами и вторая цель это упростить разработку чтобы клиенты могли заработать за натами VPN и работа с дефорум либо с продакшеном если у них есть доступы и следующая цель она немного на перспективу соединение Никуда не денутся у нас кластера Вот только растут и кластера Кассандра и кластера клиентов поэтому соединение будет много их нужно научиться обслуживать небольшим потоков Ну цели вы не выливаются в задачи Нам нужно перейти на синхронный вывод тут вариантов Больше нет И нам нужно научиться использовать входящие соединения то есть мы не запрещаем устанавливали устанавливать соединение клиентам Значит нужно использовать соединение которое клиенту установят к нам то есть научиться передавать данные в обе стороны Окей ну внедриться в кассандр легко это вот messaging Service прямо из исходников но я выкинул всякие странные методы что у нас есть можно отправить сообщение на обработку и не важно Каким образом получено это сообщение можно отправить сообщение какой-то ноде она идентифицируется адресом и можно отправить ответ на какой-то Входящее сообщение Вот мы можем перехватить эти методы и перенаправить всю логику в наш транспорт а давайте попытаемся выделить какие-то Логические блоки которые нужно реализовать внутреннее название транспорта Network fabric поэтому везде дальше будет febric для вот обозначения нашего транспорта вот масляный сервис с ним взаимодействовать мы там перехватим методы фабрик должен уметь принимать внешние соединения от других нот и клиентов когда мы получили внешнее соединение мы знаем Remote adress мы можем отправить это соединение на управление соответствующие ремонт ноги это просто логически какие-то куски блока которые куски плода которые по любому будут а соединение Мы хотим использовать в обе стороны будет какой-то код который отправляет данные в соединения и читает из соединения в асинхронном режиме Ну таким образом отправка сообщения из messaging сервиса в соке то она будет идти по такому пути через фабрик нужную ноду по адресу назначения выберем нужный Connection по типу сообщения и отправим Socket входящие сообщения мы как только выпросили можем отправить можно сервис на обработку Ну как выглядит выглядит работа с асинхронными сокетами высоких членолом У нас есть такие вот простые методы Коннект они возвращаются обычно почти мгновенно но мы передаём в них complitionler который вызовет когда Операция завершится С успехом или ошибкой простая такая штука всего два метода есть Ну простота здесь обманчивая вот как только пытаешься в колбеках реализовать например обработку успешных операций сразу начинает сразу возникает множество вопросов вот если мы хотим отправить предыдущая Операция завершилась или нет И вообще можно сокетом работать асинхронным так что операции накладываются хорошо Мы записали но могли не полностью записать может быть надо возобновлять запись потому что интерфейс допускает что запись будет неполной хорошо у нас завершилась какая-то операция Может быть у нас уже есть что ещё отправить это вот всё какая-то логика Ну вот вызвался колбэк он в каком потоке вызвался вообще нужно от кого-то защищаться от кого и Как защищаться Ну и в таких вопросов множество но я бы хотел остановиться на методе фэйл потому что вот в распределёнщине обработка ошибок Это половина если не большая часть кода вам нужно корректно обработать все ресурсы какие-то освободить понять Вообще в каком состоянии А у нас ошибки Это норма потому что например ноду могли стартануть и какие-то старые соединения порвутся она установят новые и нужно Нужен какой-то код который переключится на новые соединения либо вообще ноду могут выключить по питанию Ну и соединение порвутся Это хорошо если порвутся на самом деле потому что они могут повиснуть потому что с той стороны некому типе РСТ послать и мы будем думать что они живы это всё нужно обрабатывать Я долго пытался изобразить какую-то картинку которая более иллюстрирует но потом встретил старого коллегу Диму бугайченко он всё забери работает он привлёк нейронку и она сгенерила картинку по сожалению нашего доклада кажется нейронка что-то знает Вот это синяя Мрачно это ядро Возможно там вот сокет Может быть эти трубы это колбэки какие-то тут буфер явно есть какие-то там монитор может еще что-то в общем что мы на самом деле Хотим мы хотим писать простой последовательный код но при этом обслуживать куча соединений на небольшом пуле потоков в одном из прошлых докладов я рассказывал про реактивные раздатчик музыки которую мы запили в Одноклассниках и там мы решали отдаленно похожую проблему нам нужно было в одном процессе организовать обработку десятков тысяч реактивных стримов не блокирующих и в этом нам очень помогли помогла модель акторов так Кто знаком с моделью актеров так есть такие люди довольно много треть наверное так а Кто использовал её в своем коде в продакшене так процентов 15 отлично Спасибо так тогда для остальных очень быстро пробежимся и вспомним в чем идея такая абстракция сущность ничего общего с потоками это просто сущность у нее есть какой-то внутреннее состояние у актера также есть почтовый ящик вот с фактором можно взаимодействовать только отправкой сообщений и все сообщения которые ему отправляют они могут отправляться скажем не параллельно конкурентно они попадают в почтовый ящик Вот И там они лежат в порядке приходы вот ему отправляли сообщений три сообщения лежит Если есть что обрабатывать актер просыпается и начинает обрабатывать сообщение Строго по порядку при этом обработка сообщений последовательно между обработки соседних сообщений есть кэпинс befo что гарантирует вам что вы внутри Можете писать тупой последовательный код не думать вообще синхронизации сектора такой типичный конечный автомат вот он обрабатывает обрабатывает все больше обрабатывать нечего но динактора это не интересно обычно Они живут в больших системах очень много Ну давайте вот такой пример взаимодействия рассмотрим есть реакторы в общем случае у них сообщение лежат почтовых ящиках вот актер а иц проснулись начали обрабатывать самые старые сообщения и послали сообщение от трубы они легли в почтовый ящик ему уснули предположим проснулся актер б и начал обрабатывать начиная с самых старых но он не может ни на чём обрабатывать на самом деле ему нужен какой-то поток из какого-то Пула потоков на время обработки сообщений вот он первая обработал во время обработки второго сообщения послал сообщения outtour C обрабатывает последнее послал сообщению актера боль делать больше нечего уснул Ну потом проснулись другие актеры и так далее да Ой так не туда в общем да я много рассказывал про модель акторов и Акку на предыдущих докладах если кому интересно но все фича кином здесь не нужны на самом деле нам нужна очень простая лаконичная специализированная реализация для IO мы занимаемся только вводом выводом Ну давайте посмотрим как выглядит кот на примере поиска простых чисел Кто знаком с подходом или что-то схема для поиска простых чист так вот на удивлением а довольно много рук прикольно так ну смотрите квадратики это актеры каждом актере есть простое число он проверяет входящие сообщения на то что они делятся на это простое число если делятся отбрасываем не делятся дальше прогоняем кружочек генератор последовательных чисел вот выглядит примерно так мы знаем что два это простое число Да забудь стропили систему дальше работает генератор он два отправляет актору актер проверяет делятся на двоечку делятся отбросили отправляем 3 проверяем делятся не делятся мы последний актёр в цепочке Значит мы обнаружили новое простое число встраиваемого в цепочку там проверяем 4 Понятно Мы его сразу отбросили потом Пятерочка идет вот она идет по актерам не делится не делятся обнаружили новые простое число Ну и так далее Вот теперь Мы научились искать простые числа Это конечно не самый эффективный подход зато все это работает параллельно Ну конечно нагрузка на первый актеры гораздо больше когда догадываетесь А как это выглядит в коде У нас у каждого делителя есть простое состояние у нас есть простое число на котором пытается поделить и ссылка на следующего актера Ну это вот реальный код из нашей библиотеки Как конструируется актер нужно передать супервизор это код который будет обрабатывать ошибки и остановку текущего актера идентификатор мы используем наше простое число и экзекьютор сервис на котором мы будем обрабатывать сообщения супервизор у него есть всего два метода это обработать ошибку в дочернем акторе при обработке как им какого-то сообщения и возможно что-то сделать если он остановился Ну что будем делать здесь в нашем примере поиска простых чисел подпишем Def Pack если наш дочерняя актёр остановился мы сами себя установим OK взаимодействуют сообщениями Как выглядит протокол вот там всего два типа сообщения есть сообщение чек Проверь кандидата вот помните что по стрелочкам бежала и нам нужно ещё одно сообщение коллеккт которое позволит собрать содержимое простые числа всей этой цепочки в единственный метод который нужны нужно реализовать актером это метод ресив он отвечает за обработку сообщения которые появились почтовом ящике вот тут завозят сейчас модный кленовый паттерн матчинг таскал ещё далеко но уже гораздо симпатичнее выглядит это превью уже можно трогать собственно матчем сообщения и вызываем соответствующий метод обработки Мы уже на картинках Надеюсь помним как мы обрабатываем все если кандидат делится то отбрасываем если не делятся ему последние цепочке то встраиваем нового делителя мы себя указываем в качестве супервизора а Передаем кандидата и тот же диспатчер используем если мы не последние в цепочке отправляем число на проверку дальше Ну иду коллег он примитивный мы Прайм который храним кладем в коллекцию если последнее мы столпаемся это вызывает Каскадный останов всей цепочки помните дефект который мы подписали если не последние Передаем дальше как всю эту систему запустить создаем диспетчер и так за компьютера сервис ну-ка Он дал late Чтобы дождаться становой системы boottripm нашу систему с одним актером запихиваем в эту цепочку кучу чисел по порядку собираем результаты у нас Ну цепочка на последовательная поэтому сообщение коллег то оно будет самым последним ждем когда все завершится вынимаем проверяем Вот это практически весь код там всего 400 строк без зависимости все покрыто тестами но и большей части это комментарии В общем возвращаясь к нашему транспорту у нас каждая логический блок он превращается в актера факторы образуют иерархию давайте рассмотрим функции каждого уровня фабрик он является супервизором для нот он следит за жизненным циклом Кроме того фабрик принимает внешние соединение и роутица просто от мессенсов вниз ноды они управляют соединениями обрабатывают их жизненный цикл переключается в том числе на новые когда отмирают старые Connection и обслуживают соединение если нужно они устанавливают сами новые соединения Но если это не клиент это Бридж к Java Neo то есть они осуществляют асинхронную работу с сокетами Ну как это выглядит на примере Out мы запускаем вот асинхронную запись но чтобы выпрямить код и сделать его простым когда он у нас срабатывает комплит от успешной мы сами себе в mailbox кладем сообщение что операция завершена с успехом если произошла ошибка говорим что соке там что-то плохо и надо делать что-то все дальше обрабатываем последовательно Ну мы все это запилили сразу захотелось по бенчмаркать на одном локал хостел в одном в одной gvm подняли два messaging сервиса полноценных на разных портах первый бенчмарк Давайте отправлять данные в одну сторону отправляем кучу сообщений отправляем пачками и не больше двух пачек в полете потому что генерить можно гораздо быстрее чем они даже через localhost идут ну и получилось примерно 7 микросекунд на сообщение если все время взять и поделить на 10 миллионов Ну просто так Такая синтетика Да в общем непонятно много это или мало но если можно ускорить дёшево то хочется это сделать Давайте вспомним Да сообщения идут от мессендринг сервисов Socket по такому пути оно джавовы синхронные ченнеллы не позволяют операциям перекрываться если у вас запустите запущен Right и compution Hander еще не вызвался при попытке запуска новой операции вы получите райт пентинг эксепшен поэтому как работает актер который отправляет данные в сокет Ну предположим Все началось с самого начала мы знаем что сокет айдол у нас mailbox лежит сообщение которое надо отправить Ну актер знает что соки тайтл запускает асинхронную операцию потом срабатывает колбэк Ну компьютер Мы себе кладем mailbox сообщение что операция успешно завершена автор начинает процессить и у себя в состоянии помечать можно снова запись пихать Окей предположим два пришло надо отправить всё Китай дал отправляем callback не сработал пришло м3 его тоже надо отправить соки то что можем сделать можем только отложить пришло М4 А сокет еще не отпустил тоже откладываем вот наконец у нас срабатывает колбэк и мы узнаем что сокет снова айдола можно его использовать Ну и мы можем взять м3 и засунуть в сокет Да но на самом деле то мы можем сделать гораздо лучше у нас уже пачка сообщений есть мы можем их взять всем скопом стерилизовать и отправить одним большим блоком сокет И тем самым потенциально уменьшит количество си сколов и увеличить сетевые пакеты которые улетают Ну и в принципе возвращать бенчмарку у нас было 7 микросекундное сообщение Если писать пачками по 128 Кб в семь раз быстрее Ну и попутно в на стороне чтением реализовали тот же подход мы вычитываем из сокета сразу все что можно вот сколько возможно большой Блок из него выпарщиваем сообщение и отправляем мастером сервис И снова тут же начинаем пытаться читать из сокета Ну и в случае джавы имеет смысл все это делать через овхи потому что там прячется Одно лишнее копирование памяти Ну в одну сторону не так интересно что если в две стороны вот такой бенчмарк Мы один миллион сообщений отправляем туда-обратно последовательно первое отправили получили ответ второй отправляем и так далее здесь на раунд 3 получилось уже 54 микросекунды Это явно не одна тоже непонятно явно логике здесь больше работает Можно ли это ускорить Окей для этого нужно погрузиться в то как актеры улицы и просыпаются вот что происходит когда мы отправляем актеру сообщение Мы кладем в очередь теперь есть что обрабатывать Нам нужен поток а мы требуем поток если мы еще не за шатулили себя на исполнение Ну зачем еще раз шадулить да Ну вот вспоминай эту схему как вылетает сообщение через сокет Давайте присмотримся вот к цепочке актеров вот Выпиши их отдельно И что происходит Мы хотим Отправить сообщение ноги 10.002 но она должна получить поток проснуться начать обрабатывать сообщение она его она переложит новое сообщение в Connection который выберет по типу сообщения заснет проснется Connection насчет обрабатывает сообщение отправит новое сообщение проснется Out по взаимодействует соки там ну снова заснет потом сокет нам вызовет компрессон хендер Ну видите Да здесь как минимум три контекстных переключения можно их как-то убрать Ну да давайте на кол стайки все делать У нас вот эта схема она не глубокая у нас любое сообщение оно прилетает очень короткий путь Так давайте мы будем пробуждать потоки обрабатывать сообщения на нашем потоке пробуждать актеры обрабатывать сообщение на текущем потоке выглядит примерно так мы берем текущий тренд если мы исполняемся уже в нашем Диспетчере То есть это взаимодействие между актерами мы запустим команду синхронно прямо на нашем трейде если сообщение пришло откуда-то Извне то мы заплатим контекстным переключениям В общем запилили и словили проблему откуда не ждали В общем так есть мелкие длинные но проблема были на операции чтения мы словили с такой верфу что же произошло-то у нас тут Ну довольно тупой подход мы читаем сокеты вызываем мессенсервис тут же начинаем снова читать сокеты но Выяснилось что паттерн популярен в индустрии и внутри Джава Нил применяется ровно такой же трюк с диспетчером если операция Рид например вызвана внутри Да Пула и у нас данные высокие эти уже есть зачем за что-то платить мы вызовем Компрессорный хендер прямо сейчас прямо в этом потоке Ну и понимаете что если бывает данные мы быстро читаем мы на стеке всё это делаем прочитали отправили мастер джеймс-сервис снова запустили чтение сработало так по кругу просто Ну там догадались ограничить количество фреймов мы применим тот же подход граничим колл-стек и теперь наш поток считает глубину вызовов если текущий поток и т.д в Counting Red Мы берем глубину сколько мы уже на этом потоке запустили операции если достигли лимита фролодку то платим контекстным переключениям но нельзя дальше стек увеличивать если не достигли то запускаем прямо на текущем потоке Ну и Counting Red это просто поток со счётчиком В общем пинг-понг бенчмарк было 54 микросекунды с фроуд под один это мы два просто два сообщения позволяем обрабатывать на одном потоке уже 47 То есть нормальное ускорение в полтора раза Ну дальше уже ускорение не сильное мы говорили секунды всякие что с пропускной способностью вот на синтетике в той конфигурации на больших сообщениях через localhost прокачивается легко 20 Гигабит в секунду 40 тысяч сообщений в секунду на небольших сообщениях всего Гигабит в секунду но миллион сообщений в секунду но здесь если математику перемножить видно что есть накладные расходы на передачу каждого сообщения там есть хедеры и кое-что еще про что мы поговорим если поиграть в пинг-понг то можно разогнать до 14 Гб в секунду если маленькими сообщениями то всего 80 мегабит но 30 тысяч сообщений В общем Ну запилили бенчмаркали оптимизнули начали включать Production Ну три года назад здесь график трехлетней давности каждая точка Это один день по вертикали количество сообщений полученных или отправленных через разные виды транспорта в общем не суть что там справа написано нас интересует ярко зеленый и темно-зеленый график Вот ярко-зеленая это старый транспорт темно-зелёный это новые транспорт Ну и при успешном запуске мы как-то ожидаем Что старый транспорт уходят новые появляется и Профит здесь видно что-то шло не так почему-то приходилось возвращать и переключаться на старый транспорт мы поговорим про грабли которые мы встретили по пути а первое видео Это один из самых нагруженных сервисов у нас и это график LED & seedal вертикали в наносекундах то есть мы видим что в среднем запросы работали за одну миллисекунду миллион наносекунд когда включили Новый транспорт подскочило время до 30 миллисекунд не круто В общем по остальным графикам все было в норме CPU в норме там все нормально В чем проблема была непонятно но помню как у нас сообщение вылетают через цепочку актеров У нас у каждого сообщения есть чёткий наборчик поинтов через которые проходит сообщение и мы можем трассировать все то есть вот примеры чекпоинта мы там приняли для отправки поставили в очередь в пачку засунули отправляем и так далее поэтому мы с каждым сообщением запилили трассу мы её тянем трасса включает начальный таймс Темп это глобальное время чтобы можно было их сравнивать Между нодами но ноды Кассандра у нас хорошо синхронизированные иначе начнутся траблы и трасса содержит список чекпоинтов и Дельт между ними относительно предыдущего чекпоинта Дельта специально огрублены кодируется как варви всё всё чтобы вот это максимально ужать и они всегда стерилизованы массив байт никогда не стерилизуется чтобы не полететь всего мы тратим там Пару десятков байт на трассу с каждым сообщением вывели запилили вывели на графике и словили Нежданчик что между событиями записи в соке вот прямо непосредственно перед записью извлечением из соки это проходит 200 миллисекунд В общем поизучали код и поняли что это привет от алгоритма на Игла tcp и в оригинальном транспорте Кассандры которые Отправлял сообщение только в одну сторону сразу после установки исходящего соединения выставлялся tcp но дилей а мы-то теперь входящие соединения тоже используем для отправки сообщений Ну и как только мы после акцепта начали выставлять себе новый дилей все сразу стало норм потому что алгоритм наигла теперь не пытается придержать сообщение чтобы сформировать больше пакеты большего размера В общем если у вас мессенджек поверх Connection Orient транспорта Берите на заметку следующий следующая грабля это флапы координаторов Которые случались каждые 1 или 5 минут здесь имеется ввиду координаторы транзакций в нашем её SQL они отвечают за хранение стейта и управления позиционированными транзакциями их орбите друг друга очень активно если кто-то начинает тупить его выбрасывает другой становится праймери Если старый возвращается то переключаемся снова на него вот здесь нас напрягало что ровно каждый 1 или 5 минут Ну мы включили логикойнтов Джаве и ужас увидели пачки событий боец и балл тревогс В общем каждый не такой большой но они шли подряд их было очень много Пару слов про механизм боец-покинг в Джаве его запилили еще версии 1.2 и подробности можно найти в такой статье там и цифры есть и графики он тогда давал адский Профит Ну на реальной нагрузке А в чем заключается смысл что если у вас есть какой-то Лог Но синхрона из например и он контент то есть его почти всегда берет не почти всегда всегда Берет один поток и отпускает то runtime делает операции над этим блоком бесплатными для этого потока Это только становится баэстон к этому потоку и вот можно лосить бесплатно просто вот ну не дай Бог приходит другой поток который пытается потрогать этот блок тогда происходит событие его к барец мы отзываем Этот байес сам Рибок очень дорогой И мы обрейдим этот Лог до полноценного Лока с которым уже касса операциями работаем а но сейчас появились Канкан коллекции нормальные там Лего сикод народ переписывает железо нормальное runtime усилен и в общем Но нам было интересно как какие операции вообще привели к этим риболкам басом что там происходит кто трогает эти Бест объекты мы взяли любимый наш профайлер от профилировали стол The Word операции но вы можете также сделать всё это опенсе ну и получили такой стек трэйс читать его снизу вверх надо то есть какой-то Бин сервер дёргает методы Оптик Неймар и это вызывает тревогрибас Но вот коричневенько это на эти вход а ну сейчас на самом деле еще проще мы запылили непрерывное профилирование всего в продакшене с помощью асин профайлера и бпфы вот есть ссылка можно выбрать просто любой диапазон времени построить флаим Граф тут же мгновенно и в нем найти любой код и узнать кто в итоге к нему ведет и выключение помогло Просто сразу Но что же произошло у нас есть gmx монитор это внешний сервис который ходит раз в минуту и он трогает счетчики проверяет всякие лимиты и пороги Ну и понятно что он баясит вот эти именно объектов к себе к серверу внутри процесса У нас есть ванлог это компонент который живет внутри каждого процесса он коллектит статистику отправляет его каждые пять минут Ну и когда он ее отправляет он трогает И в этот момент вызываются события а ну и у нас возникают паузы на сотни миллисекунд и координатор выбрасывают Но как только пауза заканчивается Он снова возвращается в кластер пинги там по-моему каждые 100 миллисекунд идут или даже 10 миллисекунд В общем вам волноваться не о чем в 15 Джаве механизм затеплителей и зади заявили по дефолту в 19 вроде бы уже и выпилили но тикет интересен тем что там есть контекст Почему вообще это появилось но звучало как крутая оптимизация почему это сейчас уже можно выпить почему это никому не нужно окей еще одна проблема Ну частый Full gc на стражах возникли на новом транспорте но мы Как водится сняли дам пошли посмотреть кто держит ссылки оказалось что есть хип забит сообщениями которые мы отправляем а ссылки держат комплишер хендлеры вот те самые внутри Java nio В недрах Но это как бы странно было вообще да потому что живых компенсандеров должно быть примерно столько сколько мгновенно происходит операция записи в Socket Ну это вот в любой момент времени не так много Ну на самом деле Проблема была не в нашем ходе Выяснилось что есть бак тогда мы торги делись восьмую одиннадцатый lts там только вышла бак повешен в двенадцатой до следующего теста далеко название будет в заблуждение это касается не только синхронных не только закрытых Ну мы посмотрели патч патч простой нужно всё занулить хендлеры после того как операция завершилась и дать коллектору возможность собрать мусор Окей применим к себе такой же подход вот у нас был батч набор сообщений и на него держалась ссылка из complition но обернем в Atomic reference И как только компьютер этот работает мы его сбросим все ссылок нет изнутри хендлера эти хендлеры копятся но они ничего не занимают но ссылок на батч нет их его можно освобождать Ну с Warcraft там все работало нормально но тут Java чемпиона собрались в Твиттере порешали и Алексей Шепелёв взялся это всё портил в лтсной версии предыдущие поэтому у вас всё будет нормально кому интересны подробности там интересные комментарии в этом тикете с цифрами и примерами как воспроизводятся такие проблемы Ну и последняя грабля на которой мы толкнулись иногда изредка но особенно после каких-то сетевых работ от лица наших ноков некоторые соединения не работают tcp соединение но при этом они рвутся Ну мы начали разбираться это можно воспроизвести локально просто начать дропать соединение в какую-то дропать пакетов какую-то сторону Ну и тут пришло озарение что Ну представьте У нас есть две ноды они установили соединение используют их и вот одна из нот Ну отправляет какие-то сообщения через соединение другой И вдруг она видит что вторая нода начала к ней устанавливать соединение это как бы наводит на мысль что мы возможно чего-то не знаем а то надо что-то знает и вот в таких ситуациях и помогла это эвристика если к нам другая нода устанавливает новые свежие входящие соединения то давайте на них переключаться потому что это не точно работают мы Хотя бы синак смогли сделать а старый Ему больше трогать не будем Пусть они висят может по таймауту там отвалится В общем там было много чего еще пришлось залезть в гости Чтобы ноды перестали пытаться общаться с клиентами выпилили упоротый механизм эко-месадж в кассандре который для каждой ноды которая подключается к кластеру пытается проверить связанность пинг-понгодом для каждой ноды другой Ну это как-то удорожает все весь транспорт включался выключался на горячую без Down time of то есть ну большое внимание уделялось этому Ну и много чего ещё было сейчас у нас всё это в продакшене 95% трафика идёт через Новый транспорт так каждая точечка Это количество сообщений полученных через старые и новые транспорт новый наверху В общем полтрилиона сообщений в день идут через Новый транспорт Ну старые кластера мы постепенно тоже переключаем они просто уже маленькие что у нас получилось на нодах то есть на сторожах мы на два порядка уменьшили количество потоков Ну вот было 7000 стало 8 0 штук время Джесси Мы не ожидали но она тоже уменьшилась два раза Ну гипотеза в том что теперь 80 потоков остановиться в поинте гораздо быстрее чем 7000 сетевые пакеты внезапно тоже уменьшились но благодаря батчингу и это только самое начало мы начали переключать самые крупные кластера и потоки на клиенте начали тоже драматически снижаться вот уже в два раза там переключили но это 2020 год сейчас сильно меньше что в заключение хочется сказать цели достигнуты мы меньшим количеством потоков обслуживаем большое количество соединений и эта работа на будущее опять же кластера будут только расти соединение будет все больше мы не устанавливаем соединение клиентам наши разработчики счастливы особенно на удалёнке могут локально запускать коды подключаться к кластерам неожиданно улучшили метрики сборки мусора что интересно продуктовые метрики не изменились и не улучшились то есть вот эти запросов что говорит о том что на самом деле Connection не так плохо и Ну по крайней мере на тысячах соединений всё это работает нормально но что будет на 10 на десятках или сотнях тысяч соединений непонятно Ну и message такое что если хочется больше производительности приходится лечь лезть в коррные компоненты у транспорт это почти самое корное место ниже только лсм и файловый вот вывод Ну туда Мы правда тоже лазим и призываю всех знать и любить свой стек потому что грабли будут везде мы здесь видели баги GM странное поведение fidgam сетевые проблемы много чего еще что про новую кассандру четвертую которую Почти никто не использует пока там есть такой тикет комплекте челленджинг но уже фикс там транспорт Кассандры переписали но сделали гораздо больше чем мы там сделали транспорта синхронным для этого позвали разработчика найти который переписал Всё на нетти сверху переписали весь слой сделали шейпинг там лимитинг приоритизацию много чего еще если у кого есть опыт использования будет интересно обсудить асинхронный вот вывод поверх авторов One love если у вас будут похожие задачи Дайте актером шанс Все это не было бы возможно без моих коллег из платформы Большое спасибо У нас много интересных задач присоединяйтесь Всем спасибо готов ответить на ваши вопросы здесь есть вау вау Спасибо большое Вадиму классный доклад Давайте тогда начнем задавать вопросы и в зале и просьба кто хочет пишите их в онлайне в чат Да пас Поехали Так у нас Да кто хочет задать вопрос поднимайте руку и к вам подойдут наши прекрасные хелторы давайте начнем отсюда привет Дмитрий очень похоже что сделан General peopos такой транспорт типа два только специфичный вопрос Можно ли где-нибудь библиотека отдельно от Кассандра и вопрос что а так начнем с первой части Смотрите мы протокольную часть не трогали вот туда было страшно влезать у нас мы поменяли именно взаимодействие с сокетами у нас также по два Ну там в оригинальном транспорте было 4 соединения сейчас если клиентка на коннектится к ноде он устанавливает два соединения но до их же используют все что идет сверху это оригинальное сообщение Кассандры вот мы прикладной уровень вообще не трогали Ну если Две ноты взаимодействуют они Прямо одновременно захотят поговорить у них соединений Нет они также установят четыре то есть вот здесь нужно понимать мы только Нижний уровень потрогали самые самые низкий и вместо блокирующих операций делаем не блокирующие И вам легко сделать то же самое если такие задачи есть потому что ну если вы на дживеем стеке вот берёте Java New И сверху 400 строка которые Ну на самом деле вот весь этот транспорт он 4000 строк занимает но как бы тоже немного и большая часть это комменты то есть не выделит такую часть универсальную которую легко использовать здесь именно трассировка какая-то интеграция с mastering сервисом вот в этом специфика Ну надеюсь Я ответила вопрос давайте давайте я повторю тогда или может быть да да ответ на вопрос Если специфика Кассандры в этом решении ответа отрицательный Нет она есть эта интеграция смотрим сервисом Внизу там то есть мы ему пересылаем сообщение мы понимаем что от него приходят у них есть какой-то дедлайн внутри ну то есть мы там это значит помните там в ауте мы откладывали сообщения и потом пачка их отправляли вот там всё сложнее мы когда эту пачку формируем смотрим может быть что-то уже протухло и отправлять например не нужно там довольно тесная интеграция такая мы понимаем В общем что в этих сообщениях и например что можно часть отбросить уже поздно отвечать то есть клиент уже не ждет ответа и не нужно тратить ресурсы например вот второй вопрос Можно ли мы пока я понимаю вас много вопросов и давайте мы ваш вопрос перенесем в Клары А сейчас дадим задать возможность да давайте обсудим Давайте возможность задать вопрос кому-нибудь еще так у нас вот там вот есть вот рука да Кирилл Данилов пока привет Я так понял что вы решили сделать собственную авторную модель вместо того чтобы взять любимую Акку Да а почему Потому что Скала Вот это всё не хотелось там возиться собственный код всегда лучше но насчет аки а это классный продукт Мы его использовали Яндекс вертикалях но у неё много классных фич но два пойнта во-первых это полмиллиона строк пополам скалы 3 точнее момента второе Они поменяли недавно лицензию и там все Довольно интересно если кому Прямо даже неинтересно печально Да ну и третье нам все эти фичи не нужны мы только ее делаем И у нас очень простая фиксированная иерархия на самом деле даже вот эта диспетчер он там можно придраться к справедливости Но поскольку у нас всегда ее идет и у нас от одного там айода другого ограниченное количество вызовов Ну Все работает нормально можно было бы попробовать яку Наверное если у кого-то лицензии есть кто готов ну старая Мне кажется Все работает Да да спасибо Следующий вопрос наверное отдельно тогда окей зала тоже люди очень хотят что-то спросить Спасибо большое Вот вы в начале сказали про плюсы применение толстых клиентов А как толстые клиенты синхронно вместе с самим кластером смотрят на карту расположения данных То есть как они узнают о перестроении шардинги о выпадениях но вот вот как здесь не получается потерять данные Так а здесь Наверное нужно погрузиться во внутренности Кассандры Так этот не умею быстро отматывать на начало Ну смотрите в кассандре есть внутри механизм гости по которой распространяет метаданные топологии кластеры об изменениях топологии в клиенте у нас живет весь код Кассандры и мы через этот код можем понять куда ходить за данными если происходит какие-то операции по ребалансингу кластеру то есть там обстрел или даунс кейл то через госсе все надо узнают об этом и Например если происходит то мы начинаем писать Ну вот утрируя тут очень много подробностей мы все узнают о том что добавляется новые ноды и они начинают писать и на старого владельца диапазона данных и на нового потом через гостях распространяется информация что данные на новую ноду нужно эмигрированы и мы начинаем выводить там старого владельца но с него нагрузку снимать про это тоже узнает клиенты Они слушают все что происходит в топологии Я не понимаю Теперь мы можем читать там только с новой ноды Ну придаунские лет соответственно логично происходит но здесь именно нужно погружаться в вот эти моменты с операцией Кассандры то есть мы вот здесь Новый год не писали теперь просто клиент знает все что внутри кластера происходит это уже было написано потому что ну и так ноды должны были Про это узнавать потому что они роль координатора имплементили раньше так Так у нас остается времени еще на два вопроса вот у нас есть с этой стороны зала Здравствуйте Вадим Спасибо большое за доклад Вот вы упомянули что был оригинальный транспорт где было много соединений всё взяли оптимизировали стало только два Если не ошибаюсь Но в лучшем случае два может быть четыре если одновременно Ну вот собственно вопрос про это как вы с помощью двух соединений Решаете проблемы много сообщений туда-сюда бегает и вот мы не решаем эту проблему это проблема была в оригинальном транспорте она остаётся то есть вот помните там акциям до соединения были в начале Кассандра то есть вспомнить приоритетные Ну вот они так в ходе называют вот через это соединение идут супер короткие быстрые сообщения о том что там например запись завершилась и там вот он именно для коротких быстрых сообщений cmd Это по нему идут данные там ходов флайн блокинг есть но мы эту логику не трогали этим как раз занялись вот в тикете в четвертой кассандре может быть поэтому я три года делали потому что мы это сделали Три года назад она работала это все в работе было они рассмотрели такой вариант что Ну допустим с tcp на какой-нибудь перейти и мультиплексирование делать А ну идея такие были И спасибо за вопрос у нас были правда лет пять назад эксперименты внутренние поверх по поводу rpc поверх usdp И тогда все было неоднозначно на самом деле ему профита не получили А даже где-то у нас просела производительность наверное было бы интересно сейчас что-то попробовать но пока это вот направление будущих следований Спасибо большое так если у нас еще вопросы в зале Если у вас есть вопросы да поднимайте руку есть Так Ну давайте повторим У нас есть как раз на один вопрос время я правильно понимаю что все то что вы пилили это было про открытию кассандру про Вторую про Вторую даже вторую давным-давно Ну тогда второй вопрос А если вообще не релевантный что если Ну выложили в это солнце или нет ну смысла нет Да если фотка да и там мы нас почти всегда спрашивают инфраструктура делить очень сложно нас вот этот не устроил он там тесно с облаком с интегрирован системами метрик мониторинга там ну облако понимает например Какие ноды можно останавливать и когда и чтобы не допустить Дон тайма можно там дать просто одну команду на псклана фоном будет работать кластер будет расширяться то есть там вот супер автома она весь все управление сторожами и вот этот делите отрезать очень сложно чтобы за Open Source Ну я бы это Кассандра может и не называл уже так есть Так Большое спасибо за вопросы А если у кого-то еще остались вопросы то вы можете Обсудить с Вадимом в кулуарах Вадим сложная задача для спикера выбрать вопрос автор которого получит подарок Так у нас даже несколько Хочешь тебе напомню какие есть какие были вопросы про делимость библиотеки вне зависимости от кассандре в принципе ее специфику тракторную модель Почему именно такой был вопрос почему именно такой подход был выбран как толстые клиенты совместимы с кластером смотрят на расположение данных Так что еще проблемы с двумя соединениями и последней про версию Open Source Давай три вопроса да три может выбрать кажется да вот мне точно понравилась про Квик про Ну брак на самом деле и так так третий почему Ну не так звучало Ну давайте на самом деле про вопрос был сейчас про расширение Но об скилл дом скилл И как мы узнаем там кому ходить и корректно работаем Отлично мы тогда всех кого сейчас Вадим назвал просим задержаться в зале подарим с помощью ребят вам подарки А сейчас поблагодарим Вадима за классный доклад Давайте его поаплодируем и подарим небольшой Спасибо"
}