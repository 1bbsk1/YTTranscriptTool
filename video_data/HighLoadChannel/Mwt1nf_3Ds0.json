{
  "video_id": "Mwt1nf_3Ds0",
  "channel": "HighLoadChannel",
  "title": "DPDK в виртуальном коммутаторе Open vSwitch / Александр Джуринский (Selectel)",
  "views": 2603,
  "duration": 2512,
  "published": "2017-04-22T14:48:07-07:00",
  "text": "готовы после обеда вы информации воспринимать а я понял потери сейчас я хочу представить вам александра джуринская из компании select all одного из немногих облачных хостингов на просторах россии наверное самый известный да сейчас надо известный в общем я без лишних слов передам ему руль расскажи его тебе дикий да добрый день собственно сама презентация и мой небольшой доклад будет о типе дикий о его структуре и ну некоторых способов его применения чтобы более подробно добраться де хотелось бы начать с более с простых вещей это с обычного linux т.к. как все наверно хорошо знает приходящий пакет на сетевую карту попадает в первом space где обрабатывается в кольцевом буфере и только потом уже попадает на конечное приложение это самая самая примитивная схема но здесь можно сразу заметить что перед тем как пакет доходит до приложения мы как минимум проходим путь между сетевой карте в kerbal space и squarespace юзер space это все у нас будет завязана на переключение контекста приключения ядра это мы теряем производительность на этом также здесь сама структура linux ластика будет достаточно сильно влиять на производительность из узких мест которые можно при этом заметить это будет выделение памяти на те структуры которые предоставляются при передаче пакетов в данном случае выявленных соус таки это служит socket буфера либо и скобу сокращено какой обычно конфигурирует файлах плюс как указала ранее это переключение контекста процессора чтобы обойти эти узкие места достаточно много уже было попыток у различных разработчиков и и в самом мой линуксом с такие тоже были попытки каким-либо образом ускорить обработку сетевого трафика из основных таких моментов это к примеру него api на пи это сочетание техник прерывания по лизинга в новом ядре это уже после 26 версия по моему если мне не изменяет память и плюс достаточно большая оптимизация со стороны и сетевых карт то есть части обработки трафика сетевой карты берет на себя чтобы разгрузить центральный процессор но все эти способы не дают достаточно большого эффекта чтобы работать с большим объемом трафика поле сеть и обрабатывать в нужном нам приложение как обойти данное ограничение можно просто-напросто не использовать kerbal space можно пойти в обход как это пример вот так ничего страшного мы не будем использовать kerbal space мы просто-напросто сразу наши пакеты будем давать нашему приложению мы сразу убиваем несколько зайцев мы не используем сетевой стыка ядра у нас не будет прерывание ядра процессора и мы будем просто напросто работать сетевой карты напрямую подобная техника именуется сейчас я the wicker man байпас то есть мы просто напросто не используем ядро где это не одно единственное приложение фреймворк который позволяет достичь подобной техники есть нет map f ринг но в большей степени не предназначена именно для обработки пакетов к примеру для анализа всего трафика идущего с раутера уже на коллекторе и дальнейшее анализ полученных данных с де мы уже получаем двухстороннюю связь мы можем организовывать связь между виртуалками делать бугенги polishing cloth балансиры в принципе все что душе угодно через api типе детей если говорить о самом dippy дикий изначально он был разработан компанией intel и данный момент а же ты сильно разрабатывается но уже сейчас это как отдельный проект с сильной поддержкой intel open source ввиду того что это проект intel изначально очень сильно он завязан oriental иски и карты поддерживать к примеру меланоз брат keith другие сетевой карточки но уже с урезанным функционалом основной функционал это все же несут сейчас я intel и он сам проект очень сильно развивается и его интенсивно интегрирует в панели switch виртуальный коммутатор который сейчас используется наверное повсеместно этом во всех проектах в том же самое up and sticky который сейчас есть перед передовой функционал по построению облачных технологий если небольшая схемка которая нам показывает собственно как 9-ке работает это с левой стороны располагается простой принцип работы с обычного стыка когда мы получаем пакеты и через и крыло спасио переносим юзер space 2 5 кея просто-напросто эта связь отсутствует сетевая карта которая доступна а пользователь с консоли просто напросто не будет видно то есть мы систему карту отключен отключаем от стандартного драйвера к примеру там intel и при ключа им на драйвер virtual фэн-шуй фэн-шуй айова либо юзер space это 20 мирных драйвера и десна что virtual факт что он дает больше функций новых которые можно применить к виртуализации к примеру virtual из квн все это сетевой карты переключается благодаря наличию данной функция 8 или драйверов то есть там есть бинт on bing файлы куда мы просто-напросто засыпаем наш адрес и пища и где у нас располагается сетевой карточка rental ну либо же порт сами пакеты в данном случае уже в которые приходят на карточку попадают не в linux выставка идут в users принц где у нас будет работать и попадает fv кольцевой буфер и уже в кольцевом буфере уже будет обработка этих пакетов из этого кольцевого буфера передать пакет и в приложении необходимо будет уже использовать сама a piedi и его библия библиотеки из основных структур которые компонентов которые имеются в 9-ке и хотелось бы отметить это делал это набор драйверов и библиотек который предназначен для создания конечного приложения на основе кипятите под определенную архитектуру процессор и другие параметры там очень много заголовочных файлов где большое количество настроек содержится имеется кольцевого буфера здесь же он именуется как рты ринг который я построен по принципу first thing is to out то есть вошедшие пакеты сетевой карты попадают в цикл в этой категории буфер точнее не сами пакета в кольцевом буфере содержатся именно дескрипторы сами пакеты будут храниться у нас фрукт пейджа который будет визу спейси работать купить сыном в свою очередь позволит хранить практически любое количество данных и уже там обрабатывать оттуда же эти данные будут забираться наше приложение у нас не будет переключение контекста в этом случае между кернов псы юзер space то есть мы именуем уже как минимум один хоп обработки пакетов мем пул здесь служит роль для выделения буферов под те самые пакеты которые мы принимаем либо передаем в м буфер этого своего рода аналог резко буфер который есть в стандартном стеке при этом он пришел в 9-ке и is free бездей то есть основные принципы работы не только м буфер это ринг ампул тоже по моему пришел из free безье разница между стандартным иска буфером и ампулу в том что он не содержит и такого большого количества данных которые сейчас есть в socket буфере изначально socket буфера делался универсальным поэтому сейчас там имеется большое количество информация о всех протоколах ну как можно больше чтобы он был универсальным поэтому он замедляет работу сетевой стыка этой в буфере этого нету он небольшой содержит только пакет информацию о пакете плюс содержит информацию дескрипторы на другие такие же пакета то есть нам это это позволило объединять большое большое количество мелких пакетов в какой-то большой один пакет таким это своего рода техника бочонка то есть бать обработка пакетов то есть мы ни один вальке обрабатываем большой к один большой это перечисленные только основные компоненты и их на самом деле значительно больше там есть и сравнение хэш и различные таймеры и перед чате перечислять их и описывать перестанет работает достаточно большое время займет из основных техник которые позволили 9-ке я стать быстрее обычного стека это batch обработка то есть то что я только что говорил это когда мы объединяем кучу буферов кучу пакетов полученных обрабатываем под векторизации здесь подразумевается что мы раз параллели вам какие-либо действия в программы и выполняемых одновременно привычных нам позволяет здесь производить загрузку кода программы в типу до начала я выполнение то есть мы как бы подготавливаем код для последующего запуска что нам даёт некоторый запас по скорости выполнения еще важный момент который здесь применен а это pulling пулен в данном случае служит для постоянно вопроса устройство сетевой карты и передачи пакетов из кольцово буфера сетевую карту и в обратном направлении pulling нам позволил в первую очередь избавиться от переключения контекста но здесь можно наверное заметил что у нас становится необходимость это постоянно проживать сетевую карту в данном случае у нас как минимум одно ядро если у нас используется один порт сетевой карты по 2 одно ядро процессора всегда будет занято вопросам сетевой карты и если и вы запускали когда-нибудь вы могли заметить что там сто процент они загружены 1 ядро как минимум если два порта вы используете это уже два порта это обуславливается работа буллинга хук печь данном случае это не то чтобы набор техник это даже требования без которых просто напросто не не будет работать ход пусть служит для хранения всех пакетов которые мы обрабатываем принимаю либо отправляем и на его основе собственно работает уже библиотекам мпв де еще это фактор тот что и хук печь и приложение пользователя располагаются в одной плоскости в юзер спейси мы никуда не копируем эти пакеты мы просто-напросто отдаём их приложения напрямую и достаточно остальные техники к примеру 3 и the finite это привязка приложение к ядрам процессора это тоже позволяет уменьшить наши скачки между процессами и ускорить работу приложения зиру копия это это получается уже сочетание различных техник когда мы просто-напросто не копируем пакета из одной области памяти в другую хотя хотелось заметить что не и предоставляет только api и библиотеки с которыми наша надо обучить наше приложение работать то есть мы не получим готовый сетевой ст и где можно там поиграться с адресами настроить сети нет это более низкого уровня приложения идея мы должны будем самостоятельно разбирать или два пакета и наше приложение уже обучайте обработки работать там все семьи либо тисе пи либо http обрабатывать ввиду этого есть некоторое количество проектов которые уже идут вперед и служить таким лайк делитесь и 5 стеком предоставляя приложению вызывая функция ядра подобные стандартному стойку к примеру m tcp в данном случае делает такие же предоставляют почти практически такие вызовы и интеграция к примеру в другие приложения вызывает минимальные действия в исходном коде если сравнивать life растите биде там меньше ста строчек затрагивает наверно исходный код и уже приложение умеет работать с де если это раба произведите без данной прослойки без рамки питоном уже надо будет значительно переписывать приложение и использовать репетицию библиотеки хотелось бы дальше затронуть dippy детей и о панели switch ввиду того что это достаточно распространенное приложение апанасевич и интересное нам самая компания если кто то не знает что такое upon вас фичи это программный коммутатор который позволяет нам очень удобно с большим количеством функционала управлять настройками сети это настройка сети не только на хвосте это предоставление портов различных настроек уже для самих виртуальных машин это в основном применяются ну не только в хостинге в различных проектах к примеру в selectel и это применяется в писе виртуальная приватные облака в нашем дочерним проекте это vestel там тоже все построено на окон весь свече он берёт на себя основную сетевую нагрузку polishing обработка то есть всех сети сетевых настроек поэтому о распространенности и важности up on 8 хостинге я думаю не стоит говорить что это очень актуальная вещь нам в первую очередь нам интересно было тестировать эти с обработки пакетов и парсинга для наших лаборатории мы так скажем брали мы использовали intel процессоры это zeon и 5 у нас большое количество конфигурации поэтому выбор был достаточно большой использовать или сетевые карты intel а и убунта 1604 с последним ядром здесь хочется и что там уже имеется пакета готовый пакет и и пановой свечи которая не надо компилировать вручную ничего пересобирать можно просто-напросто впору командочка поставить и они между собой же дружит единственное ограничение m16 версии это немножечко устаревшая версия панова с вич и она не умеет делать polishing но при этом ипотеки работает стабильно в качестве тестов мы гоняли трафик различными способами это было реперов там другие утилитки с миллионы с генерации миллионов пакетов между виртуалками это не только между валками между виртуальными virtual хостами физическими хостами виртуалками чтобы понять где именно у нас проседает трафик и какой трафик мы получаем при использовании diptyque собственно сами тесты получились во всех случаях практически одинаковые у нас было кратный прирост в 45 раз как минимум это в случае если у нас физически хасты были связаны и оптической сеткой 10 гигабитный вы получали полностью загруженный канал по количеством пакетов это получалось 1 vertu овертон к у нас генерала по-моему до 3 миллионов пакетов если не ошибаюсь сожалению насладить не показал то есть три миллиона пакетов у нас выходило с одной virtual и приходила на другую спокойно обрабатывалась стандартным систем стеки на виртуальной машине нам удавалось там в районе что ли 700 тысяч пакетов или 800 тысяч то есть это значительно меньше мы гоняли ею типичные пакеты там 64 бит моего бати спасибо что заметили он как раз таки дает большой прирост именно с мелкими пакетами уже с большими пакетами разница будет меньше виду того что им тылу все будет упираться это же и тот же тест так уже между физическом концом и виртуалками то есть разница практически одинаково из ограничений которые имеет ли и которые можно ну так скажем сразу же заметить это завязка его на интеловские карты то есть возможно некоторый функционал который вам понадобится он будет только на интеловских картах вы документации 9-ки есть огромнейшая табличка с которыми где можно все это увидеть там поддержка wav и о всё это все описано и полностью по сетевым картам рассказано к примеру нами ланов у нас по какой-то причине не получилось завести должно моего образом опытный switch и передать их виртуальные машины но там имеется mellanox точнее поддерживает уже балансировку и другие компоненты этот список сетевых карточек которые поддерживаются и драйвера и он постоянно изменяется и поэтому возможна поддержка будет тех карточек которые у вас имеются плюс еще это так скажем эхо буллинга по той причине что нам как минимум одним ядром надо жертвовать под его работу не знаю она текла текущий момент на выделенных серверах процессоров много и пожертвовать одним ядром чтобы получить сетевой стать значительно быстрее домой не так страшно из каких-то еще дополнительных минусов это возможна некоторая серого то есть ли и это сложилось такое впечатление это во время тестов к примеру если виртуальная машина зависает и тянет за собой порт который про брошенность и то вешается все остальные машины это что вешается и у них перестает быть доступен все сетевые интерфейсы это серьезная проблема и она решается тоже дополнительным модулем это с п.п. он уже socket по панели если я не ошибаюсь расшифровывается который уже в каждую виртуалку уже отдельный шутит пробрасываю ту же и независимо от и я работает ну точнее он зависит но при этом не будет проблем когда сетевой карты у нас виртуальной машине зависает рассказ можно считать завершенным если у вас есть вопросы возможно дополнение буду рад услышать да да там да там запас огромные разработчики утверждают что и 40 и 10 гигабит он переваривает спокойно но при этом по загрузке процессора не посмотреть потому что она стопроцентно всегда загруженный занят опросам на сетевой карточки да конечно меня довольно странный вопрос потому что скажем так вот то что вы сейчас делаете в некотором смысле противоречит тому как сетевые карты работают с традиционным сетевым стеком значит в традиционном случае насколько я понимаю производители сетевых карт очень любят раскладывать работу сетевого стека на несколько ядер и обрабатывает прерывание соответственно всех ядрах последовательно соответственно у вас в вашем случае вы обрабатываете пакеты на одном ядре и после того как вы обработали пакет вы шлете нотификацию процессу который будет работать с этим пакетом дальше проблема в том что не исключено что вы упреки sniff прерывание сетевой карты а вы упрётесь в интро процессное прерывания для того чтобы будить процесс ну да я вас понял конечно но да вот как раз таки pulling имеет свои плюсы и минусы вот как раз таки вот то что вы указали мы можем упереться да но когда это случится и на каких количествах пакетов еще по моему рано говорить он обработает еще больше да да да тут же поэтому под pulling выделяется 1 а другие советуют полностью исключить его из ядра к там при конфигурации кастовые машины вы в grub опции загрузки просто указывается и заизолировать ты текущие ядра и они будут уже использоваться подтип детей полностью а то есть обычные дрожжи и не будет касаться и какое-то проседание производительность не будет прошу прошения не расслышал уменьшение входящего количество пакетов в сетевой карте когда пакеты клеится большие или ро да бачок batch обработка в данном случае тоже имеются в 9-ке и то есть у них большое количество техника включая привязка к процессору обработка большого количество пакетов одновременно через как раз таки вот эти со стандартной библиотеки в diptyque tam ммм пул и кольцо плюс или как я уже рассказывал то что мы используем 1 socket буфер который там большого размера имеют уже m-bus маленькие и объединяются уже один большое количество пакетов какой-то больше они имеют дескрипторы и указывают друг на друга скажите пожалуйста вы насколько неизвестно в не большое внимание уделяется именно процессу разделения портов между виртуальными машинами вы про этого частично поговорили но вот если можно чуть поподробнее то есть вот как там это работает какие какой реально выигрыш сами ли вы пробовали где ты это в серьезных проектах если пробовали какие сложности то есть от именно разделение реального порта между виртуальными машинами но скажу ждет скажу честно что в больших проектах мы не используем мы только начинаем изучать как раз таки diptyque и упали switch потому что это нам сейчас нужно как раз таки для используя деления вот этого сетевого трафика между виртуальными машинами в случае если мы хотим полностью поделить нашу сетевую карту к примеру упорствую карты между виртуалками мы здесь можем использовать еще технологию srl это когда когда мы даем виртуалке полный доступ к сетевой то есть мы не примем и виртуальной машине не будем иметь virtual виртуальный интерфейс и который нам делает virtual айоу драйвера мы будем иметь полный доступ именно к железу и уже внутри виртов ки мы сможем запустить и то есть виртуалка будет получает весь трафик да да и уже виртуалка должна будет его балансировать условно говоря но если обычно просто как обычно прикладные tomcat и программу но они не справляются с десяткой и поэтому нужен про реже на какой-то трафик нужен балансировщик ну по-моему логично вот запустить эти прикладные программные виртуалка hе балансировать трафик с помощью вот вот как раз вопрос как сбалансировать но тут так скажем целый конструктор на базе a pity можно и балансировщик и свой написать помимо того что просто-напросто перебрасывать весь трафик виртуалка в обычном случае где-то 5 киев пановой switch вы просто получите весь трафик который идет но не весь трафик в обычный гораздо больше ввиду того что он сможет переварить не 700 тысяч пакетов a million 23 спасибо за так вот скажите если у нас есть окон вы спичкам который уже надо пкд пдк работает да и виртуальная машина тоже поверх работает с поддержкой этого фреймворка дополнительные какие-то фичи становятся доступны по производительности возможно еще какой-то интеграции со стороны виртуалке со стороны всей системы в целом я бы так сказал но со стороны системы которая находится на костяк примеру там никаких дополнительных функций не предоставляется там будет там даже этой сетевой карты не будет потому что мы отключим и и весь функционал уже со стороны виртуалке будет просто как ускорение сетевого трафика и в нее вот этот сетевой интерфейс ничем не отличается если мы без diptyque используем то есть мы просто позволяем трафика идти быстрее значительно спасибо просто очень надеялся что zara копия будет озеру копья только со стороны 9-ке и функционал а вот вы сказали что есть проблема с отваривания сетевых интерфейсов есть модуль с п.п. да он эту проблему решает или он и учили каким-то образом ну то есть он и и сохраняет или он как-то ну если решать проблемы то это наверное решать проблему в diptyque надо но она еще не решена поэтому это своего рода прослойка которая избавляет от этой проблемы возможно что будущем и в diptyque изменят ну то есть при использовании модуля этой проблемы не возникает или не вы не возникает просто-напросто отваливаются они под нагрузкой или произвольно тут сожалению от не могу уточнить но я думаю что там уже вдруг у возможно машина зависнет либо большое большое количество трафика были спасибо можно вопросов больше по внутреннему устройству вот насколько я понял приложение для того чтобы получить пакета до которой от детей поступает использует какой-то кольцевой будем за вот по устройству вот например для того чтобы два вопроса не для того чтобы отдать обработанный пакет тоже какой-то аналог кольцевой буфер используется там и на atari кольцевой буфер есть как входящие очередь так и на исходящие то есть обе стороны работает это два разных буфера ну один буфер просто в обе стороны и они как-то разделяются по процессорам я дам то есть на каждый ядро свой бургер и этим какие-то механизмы доступа но сами мы сами буферы располагаются как раз таки в hook пуджа который сам юзер в прессе располагается обработкой этих колец занимается уже pulling которая работает на одном ядре он достает на одном ядре уже потом да а потом уже передает на сетевую карту либо забирая сетевая карта и сразу же в users ps у меня вопрос есть можно давай спасибо за доклад я хотел спросить можно ли избежать например вот ты упомянул реализацию за рамки рула себе стыка и вот ммм тисе пи до кажется очень не можем можете листать вопрос такой вот если я знаю что у меня есть одно приложение например вот я хочу сделать высоконагруженные http сервер который пример отдает какой-то очень мелкую статику что-нибудь такое вот в этом духе я хочу его повесить на один внешний порт от единственный процесс будет который будет висеть на этом там грубо говоря занят порту я хочу дать ему максимальную производительность могу ли я вот если я буду использовать м10 избавлюсь ли я таким образом от максимального количества копирования то есть если у меня будет все в этом процессе вот m tcp он даст мне возможность прямой работы с памятью которого не дает да он дает конечно прямой доступ туда если больше ничего не надо вот кроме кроме вот если ты хочешь права доступа может быть от m tcp еще одной прошло виски стоит избавиться и работать с api и уже тебя теперь это сейчас есть починен джинс джин x который уже работают напрямую с де и там производительность тоже в разы увеличивается спасибо и я понял пожалуйста да-да-да при использовании пули а не будет переключение контекста у нас 1 ядро всегда занята опросам сетевой карта наши запросы без переключения на другой и дроп вот это ядро палит сетевую карту обрабатывает запросы и пишет ответы да на одном ядре вами может ли апликэйшен логика всмысле работать вот вот про что вместе вместе вместе в этом треде с полингом из полевых отдельно только на ядре то есть самый надежный способ заставить это работать максимально быстро это сделать освободить два ядра к одному из них прибить другому прибить applications и все они жили джинс да любой другой ваша пример этому ядру и тогда будет минимальный лотность максимальная производительность но не обязательно к этому ядро но если у вас янович еще к одному ядру прибить да просто раскидать их по ядрам грубо говоря чтоб они не прыгали иная на 1а троне попадали передавать через спину оки туда-сюда ну а не в юзер спеси и вот эта передача данных между вот этими пулами структурами так скажем где хранятся наши пакетов будет минимальна в любом случае я понял идею туч круто спасибо пожалуйста и день не моя конечно но мы ее тестируем здравствуйте спасибо за доклад просто же на иностранный такой вы когда пришли геофизики вы пытались тестировать какие-то свои решения подобных дружбы и смотреть сверху то 9-ке это от ники почин и драйвер для сетевой карты набор библиотек один памяти и прочь вы пытались сделать это самостоятельно но вы имеете ввиду с до какие-то свои приложения нет просто свое решение и отдельно нет ни пытались требуют большого количества человеко-часов как минимум в данном случае у нас есть ли пить и кей который уже хорошо умеет работать со панели switch-ем которые во всей инфраструктуре у нас имеется которые можно использовать и polishing имел тоже вопросик практически вот какой то у молодого человека вы говорили что вы производили тесты на ubuntu 1604 какие настройки вы выполняли при этом то есть например там есть реки бэланс дать абсолютно бесполезное шоу которое 8 очередей повесит на одно ядро как бы ну и смысл теста при этом мне кажется теряется несколько тут он тут он уже не нужен когда ареста работает пулях единственное что мы делали раскидывали по утрам приложение там со стороны с они были некоторые нас троечка ничего сверхестественного из самых xtx очереди вы не перекидывали affinity не делали q алеск не трогали rx ринг тоже не настраивали да то есть вопросы то это голые результаты вот какие из практически то есть а можно добиться больших результатов и посидев там под потеряв более точно спасибо большое"
}