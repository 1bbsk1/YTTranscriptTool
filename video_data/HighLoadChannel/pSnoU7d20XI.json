{
  "video_id": "pSnoU7d20XI",
  "channel": "HighLoadChannel",
  "title": "Охота за аномалиями на графиках / Александр Барановский (Badoo)",
  "views": 1937,
  "duration": 2301,
  "published": "2021-10-04T02:44:45-07:00",
  "text": "давайте поговорим про охоту на аномалию во-первых самый главный вопрос зачем мы это делаем здесь несколько поинтов есть первое это выгодно для бизнеса во первых мы можем видеть эффекты маркетинговых кампаний мы можем контролировать какие-то бизнес метрики развивать это все дальше второе по ходу немножко затягивается но ничего страшного мы с вами разберемся второе это профит для инженеров для того чтобы отслеживать технические метрики этот инструмент тоже будет очень полезен кроме того он может как помогать в исследованиях так и быть их инициатором то есть он этот инструмент который мы с вами сегодня сделаем может вам показать о том что что-то идет не так хорошо или плохо это уже решается человеку и так о чем пойдет речь мы немножечко погрузимся в теорию что это такое аномалия что такое предсказание и так далее мы определим набор инструментов который мы будем использовать сделаем подготовку этого всего приготовим модели предсказания и вычитаем доверительные интервалы выберем лучшую модель и сохраним это все результаты после этого мы научимся работать с ситуацией когда аномалий много и так немножко теории в первую очередь у нас есть данные и the time series у которого через равные промежутки времени есть какие-то цифирки в данном случае это чистая синтетика немножечко измененный синус и так это график он теоретически выглядит нормально но давайте посмотрим немножечко о нем подробнее первое каждый график может обладать какими-то характеристиками где меньше нами так сильно до затягивается у нас презентация собственно это был график график как видите синус как видите все с ним здорово давайте я пока продолжу дальше графики нас догонят die menschen и где меньше на это набор характеристик определяющих природу данных графика к примеру у dimension of может быть страна то есть в какой стране произошли те или иные события устройства пользователя оператор операционной системой и куча куча разных других графиков чтобы понимать более подробно есть такая проблема как комбинаторный взрыв когда вы определяете характеристики фактически вы создаете разные графики каждая комбинация характеристик создает новый уникальный график на приведенном примере у нас 100 стран 50 пользователей 200 операторов и в результате мы имеем один миллион графиков на каждом из которых может появиться какая-то аномалия и 1 миллион это не предел это только три характеристики которые мы определили и так что такое аномалия аномалия в своей природе это отклонение от нормы от общей закономерности какое-то непредсказуемое поведение сейчас у нас появится график и мы его увидим наконец то что такое аномалия так когда у нас появится графе вот появился отлично тот же самый пример синтетический но здесь мы уже видим что есть что-то нестандартное мы видим это глазами вопрос как восславить это технически как славить это на большом объеме первое что нам необходимо это предсказание предсказание prediction это по определению сообщения некоторым событие которое должно произойти в ближайшем будущем в нашем конкретном случае мы будем использовать one step forward for каст это предсказание на одну точку вперед итак сейчас нам опять покажется график тот же самый с нашей аномалий и мы наложили сверху наше предсказание сделаны при помощи какой-то модели как вы видите эти три точки нашей синтетической аномалий очень сильно выбиваются из общего тренда а если присмотритесь остальные точки были предсказаны тоже не идеально и благодаря тому что они предсказал не идеально нам их нужно как-то усреднить для этого мы будем использовать понятие доверительного интервала это интервал который показывает неизвестный параметр заданной надежностью то есть это грубо говоря насколько мы можем ошибиться в нашем предсказании итак сейчас опять покажется график и мы снова увидим это на примере графе и график ладно он нас догонит а пока я продолжу доверительный интервал это по большому счету просто область вокруг наших предсказаний который определяет если . реальных значений попала в этот доверительный интервал значит это не является номоли знать что это предсказуемо и мы это смогли предсказать и как видите здесь на этом примере мы уже обладаем точным пониманием то что эти 3 точки наши конкретно аномалия соответственно на этом мы с теорией закончим и перейдем к туле нгу мы будем использовать базу данных это вся механика будет построена на sql запросах и для того чтобы нам с этим работать нам необходима какая-то аналитическая база данных таких баз много ли house вертик пиквери oracle и так далее в нашем конкретном случае мы будем использовать crack house почему но так исторически сложилось и так подготовка что нам необходимо в первую очередь нам необходимо получение данных то есть нам необходимо откуда-то получить информацию о том какие у нас события уже случились для того чтобы это сделать мы создадим вот такую базу данных и таблицу datasource в которой есть идеи в которой есть список характеристик де ментов есть описание измерения то есть что конкретно мы верим количество пользователей количество уникальных пользователей какой-то пирсинг или или что то еще кроме это у нас есть точка во времени то есть конкретный промежуток как на какой момент мы собрали эти данные ну и собственно реальные значения дальше второй момент подготовки это метаданные поскольку характеристики метрика и все остальное нам по большому счету не нужно в вычислении мы не хотим погружаться в природу данных мы будем эти данные экспортировать в другую таблицу вы сейчас ее как раз скоро должны наверное увидеть собственно мы выносим туда все метаданные сразу не меньше на метрику мы получаем к примеру дополнительные вещи то есть мы можем туда положить fest sls время первого появления метрики и время последнего появления кроме того для того чтобы сделать нашу с тем более автоматически мы будем вы заполняете то каждый раз при анализе каждой новой точки тем самым мы никогда не потеряем данные и за счет ogre гей tenders 3 в клике мы не получим дублей пойдем дальше подготовка 2 это значение самих графиков то есть мы вытащили из data source данные для эта информация но теперь мы будем экспортировать данные сами реальные который мы будем анализировать для этого мы создадим отдельную таблицу уилсон predictions в которой которую мы скоро a mouth ну и и уже видим отлично итак в этой таблице у нас будет идентификатор линии которые точно такой же как в метаданных у нас будет с определяющий точку во времени у нас будет реальные значения и дополним мы это еще несколькими дополнительными полями первую очередь каждая модель которую мы будем строить будет лежать в отдельной колонке для каждой модели мы будем строить строить доверительный интервал и классе и в поле трахал до с соответствующим номером кроме того для будущего нам понадобится поле flex в котором будет храниться битовая маска определяющая каждый определю каждую определенную модель который мы уже посчитали если мы посчитали и данные для нас подходят то в битовой маски появится соответствующий номер модели из нашей map и так следующее мы будем делать уже предсказания для того чтобы сделать эти предсказания мы будем гнать практически один и тот же запрос в котором в под запрос мы будем вставлять конкретную логику нашей модели который будет сейчас предсказывать сейчас появится этот запрос и я объясню некоторые параметры по нему параметров будет 2 1 это нам нам определяет номер нашего номер нашей модели соответственно в какую конкретную колонку мы будем ее ставить кроме этого нам необходимо будет обязательно добавить условий в на то что наш predict фактически был по счетам то есть если у нас не хватает данных или что то еще у модели модель не должна свои данные вставить и поэтому будем там хранить нан и собственно вы видите нашу наш запрос и в модуль ссылок верим и как раз таки будем вставлять необходимые нам запрос предсказание которое нам обязательно должно защищать два поля это айди и prediction дальше мы посчитали все здорово и классно но надо потом эти данные куда-то сохранить и у нас будет просто таблица с историей значений мы не планируем оттуда удалять мы не хотим джон нити и к примеру с нашими данными поэтому там мы будем хранить уже где нормализованные данные со всеми в 70 значений идентификаторами метриками димент шинами всем остальным итак мы подготовились мы уже имеем данные которые у нас лежат мы уже имеем подготовку полную для нашего анализа нам не мешают данные мы уже очень хотим предсказывать поэтому мы пойдем по моделям предсказания так задержка примерно секунд 20 наверное первая модель предсказаний это наивная модель она говорит просто завтра будет так же как есть сегодня сегодня должно быть так же как было вчера она берет предыдущую точку реальных значений и ставит ее как predict на сейчас построение ее довольно простое то есть чтобы ее сделать на иску или нам необходимо прогнать за select в наше значение где мы просто возьмем наш период отнимем у от него разницу в разницу секунд между двумя периодами и возьмем в реальное значение как то что наше предсказание единственное что наивная модель фактически не предсказывает она пытается максимально быстро адаптироваться к тому что есть у нас на реальном графики и поэтому использовать ее именно для предсказания и нахождения на малине совсем корректно поскольку опять же она не предсказывает она пытается адаптироваться но наивная модель очень полезно для ситуация когда вы пытаетесь создать что-то новую какую-то новую модель и если ваша модель на всех исследуемых значениях фактически является хуже чем наивно это видимо что то идет не так определение как конкретном моделью является хуже или лучше мы проговорим чуть позже давайте пойдем дальше вот у нас есть график график уже на реальных значениях это какие то какие то измерения мы не знаем нам это на самом деле и не интересно сейчас мы увидим график графики конкретно четкая аномалия которую мы можем найти глазами и она вот сейчас буквально появится вот появилась аномалия котором наше предсказание которым у поверх этих графиков построен будут выглядеть именно вот таким образом и того что мы будем делать дальше мы будем следовать исследовать другую модель это скользящая средняя чем она может нам помочь мы можем взять несколько точек в прошлом усреднить их и получить предсказание здесь подход может разниться мы можем взять x точек просто в прошлое это один подход мы можем взять точки со смущением примеру минус день и минус 2 дня тем самым мы сможем покрыть дневной сезон то есть когда у нас примеру есть пик и у нас данной повышаются есть of pig и данных минимум и при этом это происходит каждый раз в течение дня примерно в одно и тоже время и и скользящая средняя с правильным смещением в результате даст нам очень неплохие предсказания дальше как будет выглядеть скользящая средняя на нашем примере опять же те же самые данные те же тот же самый график которые нас сейчас появятся так ну он сейчас появится я не могу ничего с этим сделать собственно вот данные и сейчас нам про рисуются уже то как мы будем предсказывать здесь будет интересный момент данные у нас и зональные они нет не обладают явно выраженным трендом то есть у нас нет подъему монотонного его вверх нет подъем монотонного вниз и если посмотреть конкретно сейчас мы видим то что пики и горбы который есть на графике мы видим но аномалию мы словили тем что не поднялись также но скользящие средние используют опять же данные из истории на какой-то периоды для того чтобы предсказывать что-то в будущем и поэтому спустя наш наше смещение мы видим что скользящая средняя попыталась предсказать эту аномалию снова это особенность самой модели и с этим придется жить другой вариант скользящего среднего это немножечко другое смещение как видите это смещение более хорошо укладывается в обычное поведение метрики потому что у него просто другие коэффициентов мы берем просто другие цифры из истории и она из мы взяли с таким смещением чтобы захватывать не просто день а захватывать неделю берем там минус неделя минус 2 недели две точки усредняем и получаем в результате мы захватили не только дневной тренд но и и недельный но в этом случае адаптация модели к тому что происходит будет сильно позже соответственно как минимум неделе ей нужно для того чтобы эти данные в начале влиять на наши предсказания давайте перейдем к еще одной модели это линейная регрессия линейная регрессия позволяет вам очень хорошо угадывать тренд тренды это та когда у нас метрика монотонно повышается монотонно понижается у нас нету явно выраженного сезона или же этот сезон точно также сильно подвержен нашему тренду в этом случае линейная регрессия нам поможет очень хорошо давайте разберем ее запрос здесь много буков здесь очень много логики но сводится у нас все к двум параметрам благодаря математике мы выявляем точку старта то есть на какой высоте у нас будет о вы отобразилось мы определяем на какой высоте мы будем стартовать наш вектор второй угол который фактически у нас будет отображаться после чего мы выявляем точку следующую и прекрасно это все предсказывая в дальнейшем если понадобится я могу это отдельно больше подробно объяснить в кулуарах итак давайте посмотрим как выглядит линейная регрессия на графике мы имеем все тот же самый график с нашей аномалии сейчас он нас отобразится вот и давайте построим поверх этого нашу линейную регрессию как видите сезон она определяет сильной задержкой поскольку она собирает именно данные в прошлое она не берет но си зональные данные и она фактически пытается определить а куда сейчас пойдет наш график конкретно для этого примера который он находится именно с явно выраженной сезонностью линейная регрессия подходит ну не совсем удобно поскольку как вы видите он ошибается довольно часто и так мы разобрали несколько примеров модели но что еще можно делать среди огромного множества моделей предсказание их на самом деле довольно много экспоненциальное сглаживание всякие фреймворке арима prophet к и прочее прочее прочее большинство из них реально реализовать на иск о или если просто взять эту математику и положите ее на sql но сегодня мы на этом останавливаться не будем давайте поговорим теперь дальше про доверительный интервал что как его определяем метод определения по большому счету это вниз кыр среднеквадратичная ошибка мы ее можем посчитать это довольно дешево определение формулы вы видите на грани мы просто берем квадрат разница между тем что мы предсказали и нашем реальном значении берем сумму и усредняем средний соответственно квадрат дальше мы можем взять дисперсию то есть на то насколько наша функция наше реальное значение по большому счету шумные здесь мы будем использовать немножечко другой формат и собственно вы можете посмотреть он на формуле на экране третий вариант это мы можем использовать распределение стьюдента но конкретно в нашем случае мы будем использовать только константные табличные значения которые собственно уже были определены и для определенной доли необходимой вероятности нашим ожиданиям и собственно его будем использовать и так как мы построили этот запрос сейчас он собственно нас отобразится и вот нет еще не отобразился так вот запрос мы будем точно так же вставляем наш в наших школ с определенным номером мы будем использовать разницу в реального значения и предсказания как ошибку и сделаем некоторую комбинацию из того о чем я уже сказал минску рррр дисперсию и собственно распределение стьюдента перед фон третья строчка в селе gti вы уже видите как мы это все объединяем и так давайте посмотрим на пример нас есть к примеру так совсем все плохо о отлично она она эти наши это наше значение среди скользящего среднего давайте посчитаем для него собственно трешхолд выглядит это примерно таким образом что здесь важно если вы заметите сначала калий объем пришол да у нас довольно большой он сужается к моменту аномалии и эта работа нашего распределения стьюдента в зависимость того сколько данных у нас есть прошлом наши табличные значения так или иначе увеличивают доверительный интервал который будет показан пользователю дальше происходит аномалия мы очень сильно ошибаемся наши остальные функции минску я.р. дисперсия начинают взлетать и мы видим то что мы очень сильно расширяем наш пришел наш доверительный интервал пытаясь адаптироваться под то как ведет себя метрика да мы это делаем не сразу и благодаря этому мы видим аномалию и именно эта механика позволит нам справляться с шумными метриками когда ее пресс когда ее поведение уже не совсем предсказуемо когда очень много шума давайте разберем дальше что мы уже на данный момент делаем мы уже умеем работать шумными метриками мы реагируем на аномалии и мы можем работать с малым объемом исторических данных окей мы посчитали гигантское количество моделей у нас это все лежит но как выбрать лучшую модель то есть все они лежат все это цифры человек глазом определить это может но как выбрать все таки именно лучшую модель для начала лучше это та модель который ошибается меньше всего но давайте введем небольшие ограничения ограничения у нас будут следующими мы будем анализировать четко определенное количество точек в прошлом для каждой из моделей второе все модели которые мы ожидаем должны быть посчитаны если какая-то не посчитал ась не успела собрать необходимые данные для того чтобы высчитать или появились какие-то дыры в данных общем что-то пошло не так мы не можем определить лучшую модель потому что возможно это так которая не посчиталась как можно посчитать лучшую модель давайте считать ее ошибку есть несколько способов тот же самое м с моими на абсолют рвы mapi конкретно в нашем случае мы будем использовать именно этот механизм она фактически означает армированную на процент средневзвешенная абсолютно ошибку что позволит нам фактически избежать проблем с мая когда в ограниченных случаях будет выбрана не та модель которая нам нужна итак мы считаем эту ошибку для абсолютно всех моделей и выбираем просто самую модель которая шевелилась меньше всего вот наш запрос сейчас он у нас тоже отобразится по этому запросу мы пройдем по нескольким важным моментом во первых он очень тяжелый он собирает гигантский объем данных для того чтобы посчитать в конкретном в этом примере мы используем 7 дней исторических данных со всеми предсказаниями со всеми трахал даме и все это благополучно собирается в одном запросе для того чтобы посчитать ошибку и так важные моменты для каждый в этот запрос необходимо воткнуть для каждой модели подсчитанные в mapi expression это просто кусок оскал который определяет где конкретно будет лежать prediction то есть именно для подстановки хэша это нам нужно дальше нам для того чтобы мы вообще смогли это все выстраивать необходимо как-то лимитировать нашу выборку в первую очередь мы вспоминаем что у нас был понятие flex куда мы клали бита вую маску каждой нашей модельки именно в этом случае именно сейчас мы начнем это использовать мы будем смотреть какие приди какие модели предсказаний нам необходимо вы определить мы возьмем посчитаем битовым маску для каждой из них и просто посмотрим на наш в нашей маске в нашем флагах то как это будет выглядеть если нам у нас там эти биты все поставлены значит конкретно данная точка в данном в данном промежутке времени у нас было посчитано всеми ожидаем моделями кроме этого если мы возьмем первую точку и отцы чем это не значит что у нас не было разрыва в данных соответственно для того чтобы определить что у нас вообще нигде этого разрывов данных не было и в течение всего промежутка времени нам необходимо выполнить having с тем же самым запросам что количество к ун-тов у нас аккаунт на каждой точке количество точек она в анализе у нас точно равно или больше нашего ожидаемого значения это год на данный момент мы имеем очень много графиков с предсказаниями и доверительными интервалами мы знаем какая модель лучше что дальше дальше мы будем сохранять результаты сохраняем результаты довольно просто у нас уже все данные подготовлены предсказание сделано мы просто возьмем и заполним историю заберем данный is well as in predictions с мертвым их с данными из dimension of заберем какую-то дополнительную аналитику которую нам возможно понадобится это может быть и средние медиана какие-то пирсинг или которые нужны нам для технических действий дальше примерном для фильтрации лишь тут чего-то еще потом мы просто возьмем и вставим это все в хистори но что мы будем делать когда аномалий у нас много вспоминаем проблему комбинаторного взрыва наша система предсказания к примеру может определять девяносто девять и девять процентов всех аномалий так вот в этом случае мы будем иметь целых 10000 аномалий и разобрать с ними руками глазами будет очень сложно так вот давайте с первых получим вариант простой мы берем разницу между и пчелы prediction ом и если по модулю это значение будет больше чем наших школ для нас это будет определением аномалии кроме того мы возьмем это за конкретно определенный период и воткнем наш фильтр который мы о котором поговорим чуть позже нам надо как-то сортировать есть два основных подхода первый самый простой мы будем сортировать по реальным значением то есть мы возьмем легче will you если он больше значит аномалия в этой точке произошедшее будет тоже больше 2 это хитро мы будем строить какую-то логику once новость наших аналитических данных или там на размере ошибки или еще на каким каком-то образе и будем пытаться смотреть ситуацию когда у нас в реальное значение будет гигантским к примеру количество пользователей по всему миру и там есть аномалия на 2 процента или к примеру какую-то страну которая будет на порядок меньше значением но падение у нее будет не нам два а на 50 процентов нам технически первая аномалия когда у нас всего лишь два процента человек на нее посмотрит и скажет да это номально будем жить дальше во второй аномалии то уже кризис катастрофой там реально нужно что то делать что то где то сломалось где-то что-то работать неправильно и с этим определённо надо что то делать так вот 2 хитрый способ призывает именно к тому чтобы подобные аномалии с кризисом как-то аналитически поднять выше но опять же мы их отсортировали их все еще много и с ними все еще нужно что то делать поэтому второй пункт мы будем их фильтровать фильтровать можно тоже абсолютно по-разному по реальным значениям к примеру в нас метаданных лежит страна на мне интересную одна сторона но интересует именно другая потому что конкретно там у нас сейчас запущена маркетинговая кампания поэтому можем фильтровать по параметрам графика точно так же мы можем фильтровать по метрике нам не нас не интересует просители но нас интересует конкретно количество уникальных пользователей мы можем сортировать по значению что к примеру мы не нас не интересует значение меньше тысячи но при этом больше 10000 мы тоже не хотим получать и вот мы получаем какой-то промежуток внутри которого мы будем иметь anomaly кроме этого можем следить за поведением графика к примеру если мы предсказали что должно быть x а получили в два раза меньше то это падение нас могут интересовать только падение потому что пример если уникальных пользователей резко стало больше это замечательно тут а чуть следовать особо не надо мы можем фильтровать по времени то есть у нас есть тест мы можем фильтровать по времени суток по дням недели по конкретному промежутку времени в течение которого мы ожидаем результаты к примеру маркетинговых кампаний так вот наши фильтры это ключ к победе того как можно эту систему заставить действительно работать в первую очередь мы можем эти фильтры объединяет построить сложную логику вокруг этого поставить там и или не и благодаря этому дать возможность из интерфейса людям самим выбирать то что они хотят сделать мы можем комбинировать поведенческие фильтры которые основываются на том как конкретно было построено наша аномалия на то как к примеру она себя ведет что мы хотим только дропы объединим и это все имена с параметрическими то есть мы хотим смотреть только на дроп и на андроиде или пропана и о силе где-то еще и так в результате мы можем обладать возможностью точно предсказать что конкретно мы ищем и дать этой системе чтобы потом отобразилось так вот подведем итоги что мы уже что мы узнали мы можем построить систему поиск аномалий это просто до запросы до сколки но все равно это просто мы можем обрабатывать гигантские объемы данных за счет того что у нас база данных мы по большому счету не строил сложной логики математической это все работает и действительно может а было бы обрабатывать гигантские объемы данных и этим решением могут пользоваться все в команде и это самое главное людям не обязательно знает что такое скользящая средняя система сама покажет им предсказания и мне нужно понимать о том как каким образом она будет построена не нужно думать о том где это хранить и так далее мы просто людям даем возможность предоставить им свои данные системы и все они получат аномалии собственно в этом и профит спасибо большое если у вас есть вопросы прошло задавайте александр спасибо большое за доклад дорогие друзья у нас будет возможность задать вопрос как из зала то есть сидящий здесь так и из оффлайна вот у нас есть некое ограничение на два вопроса вопросы зал один из трансляции вот по результатам вопрос в александр выберет самый лучший вопрос и автор вопроса будет награжден книгой внимание вне зависимости от того находится ли он в зале или смотрит нас удаленно наблюдение андрей adidas горд вы показали условно где большое количество графиков и аномалия по конкретным графиком а корреляцию типа уже дальше математику паса или еще что-то тестировали провале до тестировали это действительно работает и позволяет предоставить какие-то дополнительные данные но опять же возникает небольшая другая проблема для того чтобы это реализовать необходимо внедряться в природу данных чтобы понимать что с чем что из чем коррелировать потому что если мы возьмём опять же тот же самый миллион график 1 корреляция будет миллион в квадрате это дорого ну это еще не говорю собственно суть вопросов вы показывали этот культ house а когда по совы какие писали отдельные микро сервисы или еще что-то или на кликаю все тоже что-то реализуемого такой для корреляции хаусе есть функции корреляции куда вы можете передать один объем данных 2 объем данных и как бы она может из коробки это посчитать то есть вы пока вот этот используете до в большинстве случаев этого достаточно все спасибо и я вас слышу здравствуйте спасибо за доклад а подскажите кубка по ресурсам будет стоить подобная система конкретно в нашем случае у нас все парте цианирования то есть мы просто берем процент от айдишник айдишник это хэш и гоним пачками по 400 тысяч фактически мы ни разу не втыкались предел который мы установили сознательно это 10 гигабайт памяти и по моему 40 секунд на запрос промежутки в 10 минут который на съезд между появлениями данных мы спокойно умещается для подсчета всех абсолютно линии графиков то есть на сколько я понял мы будем хранить там серия с два раза да то есть в крики и где-то еще откуда мы его собирали да нам нужен источник данных если мы хотим сделать системой до патентной то есть грубо говоря любой запрос у вас может упасть если у вас упал запросы вас нету исходных данных или они как-то модифицированы вы не можете сделать ситуацию или патентный вы не можете взять и просто прогнать тот же самый алгоритм повторно в нашем конкретном случае вся иерархия выстроена таким образом что у нас есть история значений куда бы просто источник данных откуда мы можем это дешево забирать и все последующие значения уже позволяют нам убрать проблему модификации данных до в течение периода который необходим для того чтобы все модели могли посчитать данные в таблице уилсон prediction с данной должны лежать включая predict и все остальное потому что опять же определение ошибки требует их но с течением времени они пока благополучно могут испаряться конкретно в нашем случае мы используем 21 день спасибо дан а вы нас слышите доброе утро это у нас вопрос из онлайна поскольку у нас возник какие-то сложности с воспроизведением нет не будет вопрос из онлайна ваша давайте попробовала даем возможность задать еще один вопрос людям которые сейчас находятся в зале вот у молодого человека поднята рука давайте свой микрофон там погонять графон спасибо николай вы показали как ваши модели и вы подаете на вход уже аномальные данные и далее у вас тогда доверительный интервал уже сходит с ума и с этой проблемой как будете использовать мы не считаем это проблемой то есть до доверительный troll сходят с ума и из того что наличие аномалий опять как тот пора ли говори кладно ладно давайте продолжим так вот мы не считаем это проблема мы сделали это по большому счету специально чтобы подобная аномалия не была таковой слишком долго мы считаем что как только случился на малик человек которому она интересна пойдет ее следует ли бо ён и и исправит либо он не исправит но в любом случае дальше и и считать аномалий нет смысла мы увеличиваем доверительный интервал для того чтобы было меньше шума именно в момент после ошибки при этом в дальнейшем он точно также склочница когда как данные будут все устраивать и устраивать я ответил на ваш вопрос"
}