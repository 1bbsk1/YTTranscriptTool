{
  "video_id": "aaJcSzDoZr4",
  "channel": "HighLoadChannel",
  "title": "Особенности построения аналитического in-house data lake в EdTech / Валентин Пановский (ex-Skillbox)",
  "views": 807,
  "duration": 1869,
  "published": "2023-01-19T07:01:39-08:00",
  "text": "скажи сегодня действительно больше правого и тех как так в область достаточно неоднозначно я сиди о компании skillbox поэтому думаю что смогу немножко приоткрыть завесу тайны и в конце концов объяснить почему и тех для аналитиков немножко выглядит иначе как я судный свет накладывает на инфраструктуру и как вообще говоря бродя в компанию где ты первый единственный аналитик построить ее до отдела чтобы все это было удобно прежде всего и для ребят которые работают но безусловно приносил пользу бизнесу поэтому фактически если мы говорим о том что же такое аналитика в общем кажется все очень просто нужны данные данные должны лежать в каком-то хранилище в это хранилище они должны попадать вспоминаем пройти или алки процессы ну в идеале в хорошем мире мы помним провода это gaming значит должно быть настроены все инструменты по discovery документированию или иначе ну и конечно же пока все эти данные лежат в непонятной коробки бизнеса нет к нему доступа вот все остается непонятным и цифрами в отчетах презентациями поэтому в идеале ко всему этому должна была подключена биосистем и действительно на этот этап все выглядит очень стандартно но когда мы говорим о том как аналитика приносит пользу бизнесу о действительном мы не должны делать это отчеты просто в стол у мы не должны делать модельки мыльные которые лежат там крутится где-то на серверах но никак не аффект и бизнес во всем этом есть большое отличие что классические когда мы говорим про использование дает и сайнса аналитика в рамках развития сервиса вспоминаем продуктов и команды все такое в этих есть одна самая главная особенность что помимо продукты технического обычно это и ломать систему управления обучения и что-то сопровождающей маркетинг зависимости от того как устроен бизнес это может быть либо вебинарная платформа либо площадка для проведения онлайн занятий лесу жил основных продуктов получается несколько но образование это во многом контент и при таком грамотном подходе продукт контента тоже нужно развивать если мы начинаем к этому подходить более структурно подключая методистов по большому счету ребята зная как развивать процесс обучения хотят конечно чтобы аналитики массировали таким образом казалось бы в компании которые не должна иметь очень большой штатную численность на аналитику уже накладывается как на нем три функции сопровождения основного продукта для получения образования сопровождающим маркетинговый продуктов и контент на и в этом есть основная проблема что вот для этих аналитик очень часто особенно на этапе становления это отрастет было чем-то не понятно то есть вроде как есть специалисты которые имеют преподавать в конце концов есть вузы который вырубят или прекрасной практике их можно переиспользовать особенно со всей ситуации с коренными ограничениями вузу тоже перешли на удаленку поэтому казалось бы все выглядит очень просто тем не менее по факту небольшая команда но хотела патчи много и в этом самая такая большая травма что изначально получаю какой-то бюджет будущего руководителя этого направления в третьих компании встаёт очень простой выбор как этот бюджет распределить и наверное очень стандартный вопрос как бы делить команду в пропорции инженеры и ребята которые занимаются непосредственно аналитика и by aren сожалению большое количество заказчиков приводит к тому что все таки чтобы бизнес жил и аналитика приносил ему пользу приходится искусственно ну намерены идти на перекос сторону аналитики тем самым это приводит к тому что вы становитесь более ограничены в рамках технического стек а если мы возвращаемся ко всем этим проблемам к чему они приводят с точки зрения устройства самого стыка прежде всего имея несколько найти продуктов можно предположить что разработку будет очень сильно децентрализованное и это децентрализации приводят диверсификации использована технический инструмент то есть даже в этапе становления компании у нас уже было порядка трех таких основных сервисов которые крутились использование разных технологий если где-то как-то попробуй все-таки от консолидируют то фактически la маска это python django по сгрыз на ряд сервисов подключали мангу вебинарная платформа была написана с пользуем печки соответственно майский л и дальше если все это копать появляются какие-то способы обмена данных стекают в центре что-то из игры logo нужную вот почему то решили туда какие-то системные событий уставшие но кафки и таким образом мы получаем очень большую гетерогенность в источник и давайте будем честными все-таки аналитики при правильном распределении зона ответственности они привыкли работать прям что со стилем и для них постучаться вопишь школу собрать данные из горы лагуну это уже что-то находящийся вне зоны классические компетенции и таким образом если принять этот факт что вас фактически вся команда это почти у те ребята которые я упомянул кто-то из аналитиков кому интересно начал изучать питон чтобы какие-то данные повертеть в пандусы в тетрадках юпитера но все равно это не классические за разработчики это не разработчики теперь процесс этот не ребята которыми это отменить какие-то сервисы фреймворке и таким образом когда аналитика переходит уже от стадии что ребятам потушили все пожара ну давайте теперь попробуем на этом пепелище сделать что-то удобно и красиво и а бюджет это уже все потрачены ты начинаешь задумываться как той команды которая тебя есть собрать хороший максимально о тех ресурсов которых есть контур как подключить хорошие система оркестра ции как подготовить их к масштабируемости потому что я думаю каждый из вас может предположить что во времена к видных ограничений был просто огромный бум на онлайн-обучение соответственно нагрузку вырастают до достается больше какой-нибудь небольшой сервер который подходил изначально уже не так ты тянешься запроса ну и конечно же имея ограниченную команду нужно задумываться о том чтобы сервис был максимально поддерживаем в рамках тех компетенциях которые есть чтобы все таки хорошие практики которым нас учат dm бог в плане это это gardens были исполнены ну и конечно же чтобы бизнес получал все результаты своевременно таким образом мы гарантируем ся по факту на аккуратные легкой системе с поддержанием достаточно большого к пасте той команда которая занимается и и утилизации эксплуатации и собственно и реализации поэтому фактически собираясь так мы разбили его на три основных блоков что в рамках этого блока мы изучали прежде всего движок сердце действительно можно до бесконечности был собирать какие-то данные в целевых источников команды разработки слава богу дает реплику можно писать какой запрос скрипка ней забрать данных себе на устройство забрать из другого источника другие данные но и все-таки поднять компетенции ребята использованием питона чтобы они тетрадочками все это сделали возможно да возможно удобно очевидно нет поэтому когда мы все-таки поднимаем вопрос о том что нужно процесс и наращивать тайм ту маркету сесть процесс сделать удобным возникает необходимость поговорить о движке поэтому конечно же есть желание раскатать губу как говорится dream big и когда мы издавали с вопросом какой в итоге основной критерий для движка может быть и какой-то спектр критериев то прежде всего мы исходили опять же из проблем бюджета явно какие-то дорогие про британ ее решения это был не наш кейс мы ориентировались носке скилл им которые были у команды таким образом смотрели чтобы это был либо esquire land синтаксис как основа движка но либо в идеале ansi sql вспоминаем историю пробей что мы не очень не хотим учиться постоянно кем там промежуточным битва им соединяющий движок кобея систему чтобы все это было максимально быстро темпа market наши все но и задаваясь вопросами поддержки было достаточно сложно предположить как весит зоопарк гетерогенных источников будет работать вместе поэтому безусловно подбирай инструмент хочется чтобы он был как минимум хорошо задокументирован а как максимум иметь хорошие открытой комьюнити таким образом мы уже автоматически наверное начали себя внутри готовить к тому что возможно мы будем рассматривать не только классические решения но и походим вокруг а уклон все-таки мир не статичен появляется много сервисов и как покажет практика завод 3 с половиной года мои работы в компании появилась вышла в прод ну как минимум порядка четырех-пяти новых движков поэтому фактически на итоговое рассмотрении у нас ушло порядка там дюжина кандидатов можно увидеть все таки такие большие истории как вверх и как зассал это и облачное решение видео be cleared shift поэтому на самом деле действительно с ребятами проводили очень плотный анализ но все-таки нужно сделать такой небольшой дисклеймер и что то что о чем я рассказываю это путь которой мы проходили вот с лагом получается около трех лет назад потому что повторюсь слава богу мир не статичен системы развиваются они дополняются возможно те недостатки которые являлись существенно тот момент новых версиях уже поправились тем не менее получилась такая здоровенная таблица по которой видно что действительно проприетарные инструменты по факту проигрывают только в одном это не упал сурс это дорого поэтому действительно есть есть бюджет и на внедрение каких-то хороших технологий почему бы им не воспользоваться ее снять себя головную боль особенно если как в моем случае это небольшая команда с инженерными спецификациями то и снять себя головную боль еще не только эксплуатации но и поддержки если мы концентрируем свое внимание все таки на оставшейся части таблицы то по-хорошему здесь пожалуй выигрывают историю дельта дельта лайком dry me u и v чем наверное я заключался наш основной выбор для толик как технология на тот момент было цель с древнего мы тоже в свое время в рамках от тех оказались первопроходцами такой небольшой спойлер в конце концов мы выбрали его если мне память не изменяет на тот момент они с на компании в россии который у вас себя еще внедрил запустил в пруд были ребята из за целом но твой тихий я думаю можно смело сказать что пионером был всё-таки skillbox поэтому с первым блоком закончили дальше появляется регистратор данных должны поступать в систему и я опять же поступать они должны из разных источников весит процесс должен быть удобно мне важно заводить какой-то процесс долго это не должны быть месяце недели поэтому здесь выбор откровенно говоря было попроще гнезда факт на стандартный инструмент в мире инженерии данных и tr flow и появляется тоже вокруг него другие сервисы в нашем рассмотрение были ищут четыре дополнительных инструментом 4 правда со звездочкой но про него немножко позже таким образом мы работали дополнительный с префектом с лайтом и луиджи наверное сейчас я бы сюда ещё такие наука к ним daxter потому что сервис развивается достаточно стремительно уже можно увидеть большую интеграцию существующими инструментами которые тоже становятся стандарт в мире аналитики это и сам кипятим это и средство для тестирования данных фрагов expectations и так далее и тому подобное таким образом править определенный анализ все-таки 5 мы как а регистратор не стали рассматривать потому что как ребят себя позиционируют это все-таки основной кисть его использования это утилита для преобразования данных то самое бук войти лучший инструмент достаточно старый с остальными можем подходили проще сожалению очень важным моментом на котором мы обращали внимание он не звучит технически важно чтобы команде было удобно работать и несмотря на например очевидное преимущество изначально выбор был настроен сторону того инструменты которым аналитиком прежде всего аналитику будет удобно пользоваться и поэтому в этом плане air flow для нас начал проигрывать тоже как покажет практика если человек до этого не занимался разработкой на питоне и он не знаком полноценно с концепция операторов и до этого скажем так вся его работа была направлена на то чтобы просто покрутить повертеть тетрадочки в питере как максимум и рф лол для него apple показала наш практиков был сложнее здесь мы когда выбирали в финальной между этими системами просили аналитиков как просили ставили задачи что вот нужно написать здесь процессы здесь процесс смотрели скорость когда задача уходят на пород но и таким образом собирали впоследствии еще и обратную связь а что в итоге было удобнее изучать потому что откровенно говоря в безумно сложный кейсов когда данные было тяжело получить нас не было действительно мы могли здесь пойти сторону удобство эксплуатации и последний блок на котором акцентировали внимание за счет того что источников было для все-таки не очень большой компании много мы задумались о том каким образом простроить линович потому что имея децентрализованной команда разработки было очень важно понимать что вот окей где-то в табличке поменялись поля появились новые полили таблицу разбили на две у нас за эту кражу за отчета хочется собрать все увидит pipeline от начала до конца и помогает это простроить когда он интегрируется с эти мелкие системой он простоит по факту граф зависимости где вас справа в конце какая-то витрину подключённой биосистеме а слева те самые старые данные и он смотрит как они между собой комбинируется и таким образом подсвечивает скажем так где произошла ошибка если вас упала финальная витрины в этот граф разматывайте к началу и видите а потенциальных там источник проблем их 3 и вы уже не шерстить и все команды не спрашивайте ребята что поменялось и релиз там документации к релизу чтобы разобраться как это эффект и данные таким образом здесь акцентировали свое внимание на 4 инструментов эта история с это хабом атласом омоном и 9 и несмотря на то что опять же видно что наверно из инструментов 9 имеет больше красках таких поинтов там не совсем хорошо политика доступа тоже парадов но тем не менее в финале мы выбрали именно его по старой-доброй причине удобство эксплуатации как инструмент преобразования тибете представляет возможность из коробки подключения микса с квелен ее закажи планирование джинджа таким образом эксплуатируя по сути основной компетенцию команды знание скверы и немного ее расширь его и возможностями шаблонизатора мы получили возможность скажем так вывести полностью из регистратора необходимость реально данные преобразовывать то есть таким образом мы четко разделили в нашем случае этель scheduler регистратор выполнял исключительно функцию регулирования процесса он не преобразовал данные а уже весь контур преобразования данных из сырых в доменную область мы производили за счет инструмента трансформация данных немножко уже наверное в рамках тех или иных спойлеров я рассказывал о том какие инструменты мы выбрали сейчас хочется об этом рассказать немножко побольше и и рассказать об этом не только формате not выбрали такой инструмент потому-то потому-то но и делать с поправкой на фактически порядка трехлетний срок уже эксплуатации всего этого контура как я упоминал в рамках движкам а кстати руль свое внимание на дриме и в итоге для нас основным критерием выбрала 100 стала 2 пойнта прежде всего что dremel способен работать в концепции виртуального 2-х то есть фактически он имеет коннектора к очень большому количеству источников его нужно превосходит только три на бывшей престо но главное что все наши ключевые источники в нем были изначально из-под коробки его действительно было просто легко мониторить ребята очень хорошо качестве подошли к документации есть готовых он чарты который хорошо тянутся и мы могли воспользоваться ресурсов нашей основной команды разработки просто скажем так на их мощностях развернуть класс ты подготовить его автоматически масштабированию на что мы старались также закладываться из недостатков что можно сказать основная проблема которая сохраняется в дриме это то что это не ansi sql него есть ограничение и преимущественно эти ограничения начинают усложнять жизнь аналитиком когда необходимо какие-то сложные агрегации то есть пока это макс мины авичи все такое проблем нет никаких но если вы хотите позаниматься с ранжированием с квантиль амита здесь же придется скажем так возвращаться к сожалению в рот bp больше уклона делается на инженеры составляющей поэтому сложно сказать как быстро этот sql блок будет доработан но тем не менее опять же проблем которым мы бы не смогли решить без усложнений скриптом мы не встречали это действительно радует и отдельно хороший point то что несмотря на то что как и многие средства он распространяется и по free модели и пососу коммюнике открытое большое мы все время находили очень такого заинтересованы разработчика из франции который помогал вам интегрировать краски 9 и dreamy вам поэтому действительно коммюнике прямо потрясающе регистратор в нашем случае был префект основной point как я уже говорил выглядит не технически ненаучно это прежде всего удобство эксплуатации то есть в базовом режиме и в режиме на масштабирование он развертывается легко и просто не требуется незнание сценарий а не требуется необходимость поднятия даст кластера для кит распределенных вычислений и не требуется какая-то глубокая настройка дагов если мы говорим и закомар flow по факту берется какой-то скрип-скрип на скрипнет сверху навешивается декоратора и уже префект его сам автоматически разбивает на тот самый так как мы привыкли называть таким образом решили проблему запуску тех или иных процессов и забор данных и параллельно мы все таки пошли по уже де-факто стандартному пути как инструмент для себя мы выбрали 9 как уже упоминал зная сквер и комбинируй его сын джек просто получает скит космические возможности использования и а циклов внутри запроса истории каких-то mma массах но не говоря уже про макроса получается история прям очень масштабируема с одним лишь наверное -9 имеет меньшее количество коннекторов и фактически если почитать документацию они разделены на три части это те коннекторы которые развивают сама 9 как компания дальше есть ряд коннекторов которые сбивают вендоры лишь грубо говоря клих а у заинтересован в использовании этого инструмента он сам комитет в опыт сур своей открытым коннектор и есть еще прям полноценно upon сурс которые не имеют поддержки большой в плане какого-то вендора не имеют поддержки в лице самого 9 но тем не менее его это блок коннекторов развивает просто группа энтузиастов в нашем случае как раз таки dreamy а не подходит ни под первый кейс не под второй но благо нам повезло опять-таки что тот коннектор который был выброшен его пан source он идеально закрывал все point и позволял всем закрывать блоки за исключением того что уингли ментальная до загрузка данных а из коробки не работала пришлось дописывать и макросами но все-таки бывает вещи похуже поэтому действительно разговариваю о недостатках в нашем случае мы выбирая конкретный движок больше всего ограничивали себя в потенциальных возможностях то есть самое наверное критическое что можно сказать про dreamy о это именно во фри в версии это невозможность реализации сидиси протоколу поэтому если бы на будущее задумалась про какой-то real-time либо не real-time аналитику то боях в этой концепции это был бы мягко скажем сложно кроме того в открытой версии нет абсолютно никакой ролевой модели если это предполагается как решение для большой компании с большим количеством сенситивных данных то я бы просто наверно посоветовал быть крайне осторожным либо раз делять это на уровне инстансов либо задумать все-таки о покупке платной версии потому что это всеми доступ ко всему достаточно грустно и соответственно в открытой версии также сложно разделять ресурсную пулу любой человек имеет равные права если кто-то бахнет большой жирный запрос остальным придется ждать пока этот запрос отработает лимиты навешивать без зале за нею внутрянку достаточно сложно поэтому это все таки не история про гибкую настройку ресурс на тот момент префекта был основной недостаток который опять же можно по решать технические но не решается из коробки это отсутствие сенсоров данных которые как мы знаем есть вырыв лавы и они существенно упрощают случае до загрузки данных когда обмен происходит через по факту файлики либо арк приходит либо паркет ну в конце концов isis и может прилететь фактически 9 те проблемы которые выявились они решаются комбинации с регистратором здесь мы говорим о том что нет возможности авторизации из коробки нет возможности регулирования запуска 9 самого как сервисом но операторы префекта air flow этот эта проблем тоже порешали немножко приходилось похищаются чтобы переплавить документацию случае заведения новых моделей но на общем опять же в контуре это не кажется большой проблемой и уже упоминала что действительно в рамках его интеграции с dreamy блоком криминальной загрузки приходилось допиливать макросами но операция разовая поэтому очень много на это внимание не потратили и действительно подводя какой такой промежуточный итог можно до бесконечности рассуждать о том что произошло но вы мне кажется важным еще и прожив этот опыт задавать себе вопрос пройдя весь этот путь чтобы я поменял для себя и фактически конечно с появлением новых сервисов появляется больше возможностей и появляется не совсем узкоспециализированной наверно такие нишевые сервисы которые могли бы закрыть эту проблему которая была у нас появись они немножко раньше фактически я говорю о том что сейчас бы например мог бы побольше посмотрели в определенная база потому что не все из них требуют наличия звуки пера достаточно просто масштабируется подключается запускается просто новые ноды так например история с cockroach гибели выкроить baby может запускаться достаточно просто наверное мы бы задумались больше о том чтобы получать данные дома генизе равным образом то есть действительно много к аналитике нам пришлось работать с большим количеством источников это сложно и требует больше внимания разных спецификаций разных протоколов обмена данными тем не менее если мы имели какой-то ресурс комбо команды разработки и договорились с коллегами изначально что ребят но вот хотим данные получать в таком-то формате и транслировали это знание во все блоки и при наличии у них ресурсов этого существенно облегчило жизнь получая по условному согласовано протокола данные в кафку мы могли бы разбирать их самостоятельно без задействования каких-то внешних сервисов потому что так или иначе но мир не побоюсь так сказать но подавляющее большинство компаний которые занимать сайте разработку и есть уже настроены к это шины сообщение тот же самый кролик или кафка и аналитики зная как с ней работать но могли бы и просто переиспользовать не тратьте свое время по факту на поддержку этой системы ну и конечно же история с бюджетом имеем больше бюджета на закупку уже пропали тарных сервисов либо использ неудачных ресурсов явно возможность управляется больше есть решив пиквери snowflake можно перечислять набрать и система которой я говорил переводя их в enterprise или счас режим то есть действительно получается прямо в разы больше возможности поэтому наверное здесь уже вопрос больше про управлениями ожидания управлению бюджетов а это уже как говорится совсем другая история поэтому фактически те сценарии эти съемки про которые я рассказывал их можно схематично изобразить таким образом достаточно такая простая схема с попсовым с подпиской на событийную модель можно также было уходить в использовании из 3 совместимых service of dreaming стать в этом плане тоже поддерживать такой вариант имея возможность напрямую к верить файл накручивать на них и сколь синтаксис можно было бы например немножко перекрути сервис не напрямую к нему подключать все источники а просто поднять и какой-то стресс и не снимай хранилище тот же самый меню либо завести bucket of lovers и подключить не мудри new таким образом мы быть добавили небольшое осложнение в точки зрения сетевой пропускной способности потому что приходилось выгонять эти запросы по сети и файлы туда сюда но это тоже был бы проще в поддержке потому что по факту поддерживать тот же самый из 3 сервис явно проще чем полноценные шлиф с кластером поэтому действительным вариантов по на вы их можно комбинировать но как говорится какой путь мы прошли с ним я с вами и делюсь что хочется показать фактически разговаривать о движках можно до бесконечности мне бы хотелось показать тот сценарий с которым в режиме prada отработал уже префект в рамках как я сказал где-то 3 лет фактически прежде всего юань когда я удобный white решает ряд проблем можно действительно видеть какие-то гиппо дают какие потоки отрабатывают в ожидаемое время какие запоздали вообще какой процент успешного запущенных дагов так далее и тому подобное поэтому с точки зрения я префект оказался очень приятным и насколько я помню сейчас ребята планирую достаточно большое обновление так что есть ощущение что станет ещё удобнее если мы говорим конкретно про тот инста за который был поднят в нашем случае мы ограничились всего лишь одним агентом агента читается есть со мастер который запускает процесс и агент который непосредственно силу рабочую функцию на себя берет с нашем случае то эта конфигурация которой я показываю такая небольшая дымка оно работало всего лишь на одном агенте и фактически можно увидеть что на одном а деньги идет сейчас порядка на 15 20 процентов может быть запущены юных нормальный тянет нет приседаний во времени и что самое интересное фактически вот в рамках 8 часовым интервалом можно видишь достаточно много процесса запланированным и таким образом прокидывать нагрузку до она будет меняться она будет увеличиваться либо уменьшать зависимости от времени суток потому что как мы знаем часто аналитику запускают ночью к выпуска ему считаются t минус 1 доступность данных никто не отменял тем не менее система отрабатывала себя хорошо даже на по сути очень такой игрушечной конфигурации и в итоге как третью мира о чем хочется сказать как бы мир не менялся каким решение не появлялись одного универсального решения его так и не существует приходится все также получая новую новую задачу исследовать много технологий чтобы действительность всего этого многообразия как конструктор лего собрать удобный хороший контур и всегда было важным на мой взгляд не ограничиваться только стандартными решениями как я уже много раз сегодня у поменял мир статичен появляются новые сервисы чему бы ни хотя мне проанализировать никто не говорит о том что давайте в прокатить сервиса которых сырые вот там вчера кто-то что-то консульством давайте пробовать нет конечно же но в любом случае изучать это исследовать явно стоит и интересно как мне кажется подход про которой много рассказали все время коллеги есть большой цикл статей его видно на эту тему о том что на самом то деле данные как какой справа я сад это прежде всего тоже продукт и когда ты начинаешь проектировать эту систему именно с точки зрения продукте зации своего технического решения для его последующей эксплуатации голове появляются совсем другие кейсы мне кажется задумываться об этом действительно стоит особенно когда я изначально ты понимаешь что многообразий продуктов которое у тебя базируются на тех данных которые есть в доступе наверно на этом я буду заканчивать огромное спасибо за внимание так что то есть удовольствие отлично вопросам спасибо и ваш вопрос а вот я вижу первый вопрос здесь спасибо за доклад очень интересно было на одном из слайдов вас был критерии оценки это транзакционных зачем да щас вернусь здесь наверное следует упоминать о том что как раз таки та история как найдем его а может и не найдем ну ладно не суть вообще когда мы говорили о том как от система будет эксплуатироваться и мы действительно задумались о том что наверно в какой-то мент за рамки аналитики мы можем быть и в этот самый момент то транзакционные которые вы спросили она у нас и появилась то есть фактически за счет того что данные были разбросаны вечевой команды были ограничены возможностью например не было никакой а возможность отслеживать и посылать какие-то блины пуше людям которые работают в бинарной платформе на основе данных и заломать системы и поэтому предполагалось что в какой-то момент тот движок который будут использоваться он будет не только существовать для обработки данных на том числе и для их обогащения и вот в этом самом контексте нам на будущее в бросили ребята подумать и подаст право традиционностью еще один вопрос просто так вот но он более-менее такой хули горный поэтому ну лучше на него ответить там в двух словах услышал такую мысль то что аналитики девелопера опыт всяких там и примем движков да он говорит что но это утопия вот вот у вас как как просто наверное недаром девелоперы есть девелоперы да со всякой своей культуры и всем таким прочим и заставить аналитикой devil пить как он действительно вопрос очень холивар ный явно это нецелевое использование и те компетенции которые есть у человека но скажем так если человек на мой взгляд хочет учиться он готов к этому мне кажется такой с энергией компетенции и аналитика и разработчика в одном человека не существенно упрощает банально это очень хорошо видно если посмотреть те же самые тетрадки юпитера который пишет просто аналитик и человек со знанием практика разработки это совсем другие вещи и функции оформлены по другому поэтому да это требует больших временных вложений не каждый готов это делать особенно если человек такой я хочу быть аналитиком мне вот ваш этом функция p виртуальных средах мне это не надо но здесь вопрос подбора команды и ее последующего развития спасибо и объемы данных у вас строки терабайт действительно и тех с точки зрения объемов данных это не телеком это не электронная коммерция фактически в пике у нас было порядка пяти тра байтов данных с одной компании тайский бокс это не только одна компания на самом деле холдинг поэтому фактически вот этот пять терабайт можно масштабировать на пять шесть компаний холдинга то есть данный окислительных не так много но наверное с натяжкой можно назвать большими потому что нет огромных сервисных серверных мощностей для их обработки но в целом действительно объемы по сравнению с текущими коллегами по цеху игрушечной можно так назвать"
}