{
  "video_id": "4h73gMfdy8c",
  "channel": "HighLoadChannel",
  "title": "Проксирование данных для Hadoop / Андрей Ильин (Сбер)",
  "views": 1176,
  "duration": 2507,
  "published": "2023-01-19T07:03:35-08:00",
  "text": "я сегодня ну про проксирование данного для ходу другое слово my проксирование виртуализация да то есть как мы обеспечиваем доступ из одного кластера ходу в несколько других и но для чего вообще нужна проксирование чего нужна вообще то виртуализация что она дает и с какими сложностями вы столкнулись при разработке этого решения 2010 2020 год значит нас города там банк с берем сотни кластеров у каждого свои свой владелец своя команда разрабатывать витрина данных моделей выполняют анализирую данные использовать стекла сера здесь пользователям доступ и основные компоненты the hive наш dfs центре но и ranger до для третьего ходу по что нужно этим потребителям прежде всего получить метаданные из других кластеров желательно в онлайн-режиме защитить данные то есть получить данные уже соответствии с централизованным политики политиками сбера кто такой какой сотрудник какие данные может видеть примерно один петабайт в час трафика да точно не знаем потому что он померить довольно сложно и спите есть много очень серверов кластеров и задача померить вообще сколько кто потребляет какого кластер а трафик нетривиальная но и квартира не и статистиков на чтобы потребители могли доступ стоит качать загружать другие кастера и обращение данном через дефис все равно идет получения данных нагрузка на прежде всего лишь dfs на найм ноду но статистика кто куда ходил сколько получил данных и для выявления самых прожорливых значит проблемы трактов двадцатом году удобно для пользователей но сложно управлять требует много железа потому что на тот момент тракт распространения включаю себя промежуточный plaster а то есть берут это с это с помощью специального с называемый супермаркет система супермаркет пользовать заказов данные в портале эти данные копируются на другой общей plaster а где они уже потребляются другими кто кто заказал данные дали бы автоматизированный процесс либо конкретных пользователей все это сложно копировать данные сложно требует много железа доступа доступ ими управлять и управляет capacity вообще этих реплик дано где лежат наши данные этих кластеров сложно метаданные как бы сложно получать потому что это либо процесс через запуск бачей который копирует метаданные через чем с и либо через либо как у предыдущей у нас инструмент это через кафку мы реплицировать данные путем создания винт лиса ника который репетируют свифт объект в кафку дальше его лист фанеры или анализ они raw на каждом классе потребляет записывать в чем с источнику все это проблема метаданные могут достигать в отдельных случаях двух гигабайт на когда много портится многоуровневым портится кафка может ломаться настройки могут плохо настроена будешь нужно настраивать сколько реплицируются куда реплицируется значит в новой архитектуре называем сервис ли рацион доступом так обозвали все свой прокси им одним солнечным именем у нас есть три продукта на 3 сервис-центре праксиологии dance hd фсг твой значит центре прокси просирают привилегии с центре да то есть я ставлю сентри практик подключая свой кластер как будто это центре настраиваю центре прокси говорим cuda нафиг у какие центре сервиса смотреть и как будто эти привилегии появляются на классе среда вокзал dance такая же история только в части и чем с то есть тоже подключаем кого golden это не наш это как бы очень сильно доработанная версия пан собственного продукта по booking доработал мы добавили kerberos поддержку hive 2 high в 3 совместимости и 5 под пережитом нагрузку но еще там много наработок сделали вежлив с great вы в это уже собственно говоря имплементация жди фас api для ходу по тоже поддерживает ее ходу 2 и hadoop 3 см это много немного тоже пришлось нам дело в компе the beauty режим то что опять отличаются значит в чём нужен продукт целом защитить был защитить доступ к репликам источников или витринам данных для новых потенциальных угроз да то есть это означает означает что должен быть как должен бы должна быть какой-то прослойка до которая позволяет манипулировать теме обращениями либо какими данными мы получаем из не источников в централизованным формате да не то что там каждый под каждый потребитель источник какие-то настроечки сделал мы в одном инструменте в одном сервисы всю эту логику централизовано для организации зашиваем доступ метаданным hives . онлайн это понятно потому что портится не пришло время через кладку сообщение развалилась витрина зачиталась ошибкой все витрины пользоваться нельзя все должно быть в онлайне то есть я сижу на кластере даже видеть hive объекта в онлайне централизованного гид запросов тоже важно очень для безопасность посмотреть кто куда ходит зачем сколько уходит на статистику обращений к данным для как раз понимание кто сколько обращает сколько потребляет данных немного цифру 100 гигабайт секунду сейчас у нас мощность власти раз играем расширяем до 500 гигабайт 800 пользователей 30000 это близ на самом деле в большие то там основные сказал триста кластеров hadoop 2 x 3 x важно потому что нам уже совместимость обеспечивать более шести петабайт данных это не включая пользоваться кидаться то и где-то тысяча витрин данных значит основе вызова сервисы должна быть доступна 99 на 5 почему ваш сервис не работает вы подключили кластеров сервису если ваш сервис не работает либо с центре прокси либо в угол dance видео хоть не тез тура прокси либо как hd vs proxy фсг твой все как бы кластер полностью не доступен поэтому всегда должен быть доступен ваш кластера обновление сервис доступен обязательно потому что тех окно невозможно найти то есть никто расчеты витрин которые некоторые по два дня считаются с 0 и для вас специально не будет большие задержки понятными и допустимые большие задержки выдуваются за слой горизонтальная масштабируемость предки нагрузки тоже понятно сейчас про это поговорим совместимость его изоляции источников я уже сказал у архитектура понятно да то есть у нас есть вы gardens да это этим с прокси их на самом деле несколько позже потом расскажу несколько нот и наша виртуально data model ну где-то еще там центре прокси и распределенной кэш прежде всего нам нужен для того чтобы если манипулируем данными паркета и к тому же нам нужно еще блок халате шины для наших потребителей возвращать именно на штат ноды мы должны get кэшировать есть я все не стал рисовать дам здесь только основные система самом деле там еще много-много интеграции с внутри банковским стенами типичное положение spark много экзекуторов много запросов лескова нагрузка для операции с этим местом тент тайм-аут пять минут то есть в gardens там не работает ли ваш сервис про серую hive не работает больше 5 минут не дает ответ мы все как бы пока значит чем мы столкнулись прежде всего с проблемы с работа соединения значит так как в нем надо очень много маленьких запрос и опять же всплески из-за специфики спарка открывать новые шаги не очень дорого зачем ислам тоже сама история но там более терпимо хотя тоже там видели что есть потребитель которую по тысяче по миллионово запросов в пиаре чем с он хоть на это stora делает то есть на обязательно кашированные соединение file descriptor слив то есть не хватает коннектов не хватать дескрипторов за этим надо тщательно следить потому что опять же груз сервис упадет у пользователей выросло нагрузка все пропало да то есть витрина упала это эскалация там куча проблем значит обязательно закроем середине делаем во время нтаб проверяем что мысли мы закрываем соединение а проверяем всею лимиты для систем сети или есть отдельная настройка самом сервис файле и и только она учитывается глобальный учитывать всю эту историю проверяем ошибки в чем с балансируем запрос в несколько метров туров да потому что в хайсмит истории что hadoop втором что в третьем hadoop есть ошибку что много коннектор начинаются проблемы борделе до сих пор не исправлен сейчас мы побеждаем тем что поставили там на каждом кластере по 8 хай-хета споров и балансируем в через google диск все эти запросы обязательно открываем с единицы и пирса нации но это понятно да то есть у четких орловские там или ходу пуске да не используем обязательно пирса неру соединения потоки в java тоже 100 столкнулись когда там делали асинхронный аудит то есть 45 тысяч потоков с живым у нас 18 нормально fair следуем к обязательно потому что если не fair to 1 завис ваш потребитель games парке получил там ответ 0 выгоден для хай-хета stora получил там через несколько минут или даже часов были такие кейсы ну все делать не работает ни рождаем лишь не потоки потоки могут зависать уже сказал да может быть несколько минут или часов даже как будто у вас там что-то не работает при нагрузке тестирование подход 99 иль какой-то отсечка по количеству операций там больше меньше не работает то есть потому что опять же есть таймаут на hive нас парке по умолчанию 5 минут менять под глобального никто не будет поэтому все операции должны решатся определенное время kerberos но тут все понятно все должно быть на kerberos всегда тестируем в kerberos kerberos это боль это на начальном этапе где-то 5-10 процентов вообще у ваших всех проблемы разом разработки все только скира берется digest держите сму обязательно то следим запросами потому что однажды ко мне пришли одни мы сказали что мы там спамим ипатова на а вот сервер мы не хорошие люди тоже затем наследить там учиться разбирать ошибки в с и мешает файл ничего не скажет значит тут делаем ран букв информация в интернет еще мало добавить код библиотеки гсс непосильная задача поэтому гуглим собираем vranduk записан в книжечку какие проблемы были с чем столкнулись и научимся их разбирать совместимости перри и перед матчем с dfs разные вам нужно обеспечить и так и сяк для разных версий поэтому либо сами имплементацию делаем либо какой-то эмуляции имплементации значит проблема при разработке для google denso расспроси рования или виртуализация метаданных мсс чем стоп . текке ходу вообще лишь себе нечасто заточены на какую-то конкретную специфику поэтому мы использовали о спонсорской для теке ходу по даты по большей части сами еще конечно не писали забрали код ходу по и на основе него писали вот как пример хай v konce да вот такой интересный очень обертка вокруг шмап а при создании соединения очень потребляет много happy new roman не могли понять уж такой почему стук памяти кушает вроде там 4000 соединение должно быть хватать not есть такая проблема тяжелая операция понятно что много есть в чем с тяжелых операций и ярко repair in se si iar и тпр он на самом деле кучу внутри себя еще запросов создает все это если большая таблица улетает да и опять же такой как бы спам спам запросов идет ваш сервис ну чтобы доказать что это не вы виноваты дать частая проблема такого промежуточные сервис нужно обязательно понимать смотрите луна учить слоги чем с осмотреть там поле duration какие 5 то можно там статистику выстроить сколько и пятым занимала кто его там называл там и так далее в чем из находить на самом деле под конкретным пользователям использовать или гейшей on talking запас делегация токина создаете connect and connect через энди five и прекрасно им пирса нир уйти там любому пользователю понятную пухаев разрешено все это проксирование для хайло работает только через учет кухайлан да почему потому что ходу пит специальная четкое и можно поменять но на самом деле там ее менять не надо и если у вас стоит центре рейзер то есть выключенным и пирса нация то только с учеткой the high все будет корректно работать поэтому сейчас пока сервис работает при вызове удаленных удаленных объектов допущено надо иногда же писать еще попавшееся общих остро потом пишем метаданные минут используем hive так плохо держит много соединения начинается внутри уже говорил да то есть в девках в хайсмит истории есть дефект можно ходить там по вендорам ничего не править не будут либо быть его почтамта какие-то фиксы свои делают но мы пока поставили несколько типа суворов безопасность не даем менять метаданные источниках изменения метаданных удалили портится это искажение данных на защищаем кита бы нам нужен михайловский этап и поэтому hive китайский этап на суперпользователь поэтому нужно обязательно его корректно защищать у соединение понята переиспользовать нельзя длинным но dat ноды и hive нестора потому что в момент создания они обмениваются как раз этими данными какой пользователь реально том конце нас работает на груше тестирование тоже стoлкнулись есть разные группы и 5 hand of спорим есть бригад на есть отдельные то есть это какие-то получить список чего-то или так далее их отдельно тестируем потому что некоторые из них очень плохие очень много нагрузки дают на самом hive не ту сторону например получение всего списка портится испортится там несколько миллионов это там 2 гигабайт тоже непонятно эмулируем все стандартные тесты теста максим надежно надежность отжать 24 часов 24 часа обычно это время истекания тикета сибирскую поэтому тоже проверяем так полные многие что у нас полный моде с чем мы столкнулись значит очень много простых запросов все они небольшие но их очень много hadoop спроектирован таким образом так в на и моде все хранится в памяти то соответственно ожидает что хайфу spark они быстро всего за нашем случае не так потому что но соединение что-то вызвать и так далее поэтому мы потратили вот на этом самом большее количество времени части дата ноды читаем поблочно до нужно прочитать нужно писать стрим размер блока обязательно выставляем в идеале в такой же как на источниках хотя может там различаться прикидываем какое-то значение тестируем такое на муж данные на дисках с побочным копирование байтов занимает время как ни удивительно но нет не то время которое надо дешифровать шифрование занимает намного больше времени до 10 раз зависимость алгоритмом зато есть тоже надо это уже дать hd fs детали реализации блок такими мы должны хранить потому что мы должны будете отдать модифицированный блок таким сохранить и реальные чтобы созданием потом сходить сейчас хранимого с определенным к шефу кости паркет футера тоже нам хранить нам нужны of свету чтобы работать шифровать потом изменять данные когда мы зато но до читаемых или или пишем у чтение скажешь тоже не бесплатно на так у нас распределенный кэш тоже то на следи за тем что просто класть паркет futura один-в-один не стоит они могут занимать несколько десятков мегабайт для больших файлов поэтому лучшим их класть каком-то обрезанное видео вот одна из больших проблем столкнулись это работа с большим количеством маленьких файлов здесь вот видно на результаты справа у нас проксирование это сколько у нас процентном отношении идет задержка и проксирование еще мы что-то защищаем там за 0 им либо либо шифру им значит здесь у нас ну сейчас у нас цифра большинство кейс в не более тридцати процентов поддержку но вот мы столкнулись тем что как бы если количество файлов очень большое то несколько десятков раз даже без проектирования доступ доступ становится медленнее данным бороться с файлами маленькие можно не специальная утилита типа компактор которые сжимают эти паркетные файлах можно после ваших пакетных задать запускать часто источники или реплики там или витрины и спущены порционирования надо до трех уровней для того чтобы обеспечить больше быстрый доступ тоже с ними надо работать с этим потребителям объяснять что это конечно хорошо там пытается к традиционной pd на яндекс яндекс деньгах сделать но так не работает apache паркета рекомендует оптимальный размер файла один гигабайт но еврогруппы тоже значит то первым apache apache парке колоний формат хранения все в сета хранятся футов самих файлов это что означает это означает мы должны излишнему что-то меняем мы должны отдать footer правильный клиенту мы должны у скачать сначала ты чем пришел запрос на и моду поменять тома все ты перепаковать это все паркетом обратно это занимает время поэтому нужно тут очень хорошо оптимизации заняться да и понимать что чтение future of это вот одна из мой тот в том себе блог vacations это там где вас файлов смогли лежат на кого докладе удаленные файлы лежат нужно учитывать маскирование просто все очень маскирование мы берем записываем забиваемся нулями говорим что размер p&g в паркете вот здесь вот спарка ли какой-то внешней клиент считает до этого места дальше он просто уже не читает здесь проблем нету при изменении и низации данных данные при шифровании либо их изменений как какими каким-то видом чем угодно могут меняться целом мы тестировали до трех раз могут меняться есть конечно отдельная кейс когда вы шифруете без формат призер encryption то есть просто зашифровали этих значения там у вас будет размер соответственно уже зависимости там от размер ключа то есть там получается уже там один байт там превратиться в несколько десятков байт здесь непонятно что делать потому что ну если отдавать как бы 52 раза больше данных каждый раз конечно так не сработает поэтому тоже над понимать что тут такая штука спорная то есть что как вы меняетесь защите данных влияет на производительность нагружать ирвинг производили с реальными данными но на самом деле много кейсов паркета например есть много типов dictionary есть различные типы div dictionary потом могут быть всякие пограничным условия там например пустой паркет просто запишем файл неё нет данных там просто futura любит либо например а другие кейс с чем сталкивались там где то не правильно все посчитали индекс каком-то байте это не работало желательно на реальном в источниках обходной путь которую мы привели к которому пришли и там и про сгенерили паркетные файла или брали синтетику но опять же здесь не только сами данные важны а именно комбинаций типа dictionary в паркете что какие там данные там размеры нулевые ненулевые внутрь все комбинации нужно протестировать так ну все понятно следим в трафик большое на тестовых стендов часто бывает что не хватает значит особенность ктс железо бог отец называем зверев железо значит чем столкнулись нам дали hadoop на стандартную сервера там много дисков много рама нам этого всего не нужно все нам нужно нам нужны stella интерфейса хорошая нам нужно для 5 гигабайт секунду нам хоть четыре ядра если мы ничего не защищаем если защищаем то просто считаем по количеству бойцы а не по количеству полей там далее покрыт именно количество все алгоритмы защита в основном построены на именно количество байт виртуальные сервера теоретически могут подойти но плохо может быть перри подписка может не хватить скорее всего трафика ну вот теоретически степью значит трафик проблема с чем столкнулись трафику сбера то несколько садов до соответственно распределенная архитектура сетевая и канала между сотами они собственно говоря не бесплатные они дорогие и они ограничены до это везде так поэтому сейчас пришли к тому что мы просто своих дата ноды размещаем на прямо рядом с источником и уже в на и многим уши позируем правильным блок таки нам на наши дата надо где расположены в правильном сводим на груш тестирование уже сказал много разных прошли в профиле обязательно есть нюансы в div с есть кэш приводило к странным результатам когда у нас была через проксирование быстрее чем просто напрямую через задевать федерацию поэтому можно с братом сбросить не нашли просто перезагрузили источник кластер источника и проводили тест nt прежде всего джесси потому что паркет мы используем паркетным библиотеку для работы с паркетом она очень потребляет много памяти очень неоптимальное думаем перейти напрямую манипулируем байтами но это на самом деле тоже очень фактически узнать перед писать для тех у паркет еще неизвестно быстрее будет не быстрее тоже продумаем и меряем вот эти параметры да то есть джесси открыто соединение обязательно потому что частоте из какой-то connection алекс все там либо не хватает connect of на самом деле настройки неправильно и кластер у клиента начинает не работать вопрос запросу так взаимодействия с другими командами насчет в части epson с чем столкнулись в берри много кластеров у каждой каждая своя команда свой владелец каждый там печется о своем классе тире чтобы не дай бог там ничего не случился не прорыв потоки поэтому не нужно помогать им да вот эти вот сервис и ваши подключить так как сейчас у нас там человек это делает настройки входов и там в том од пока не разрешает по причинам безопасности то тоже тут нужно учитывать настройки но человек поле для ошибок понятно в ходу пи вообще с настройками беда там миллион этих настроек непонятно какие они должны быть на самом деле есть очень важный которые могут там отключить пирса нацию тогда вообще все запросы будут подходи вам работать доступом ко всему и вся за этим надо следить поэтому автоматизируем я прерываю чего этих сервисов массу будет много для hive проксирование нужно обязательно отдельный экземпляр сервисов потому что каждый кластер а потому что вы должны включаться плакального me the storm ну и в том числе для подобия микро сервисных saturday нами на ней надежности разворачивание настроек много кита бы настройки карба five can't do here газку клиента сертификаты тоже большая боль точки монтирования журналов журнал надо хранить отдельно mount планете чтобы не переполнились началось и расписаний работать структура директорий всего мы все много-много надо настроить поэтому все это руками каждый раз настроить для каждый потребитель очень сложно занимает там неделями поэтому все это автоматизируем затем но для ручной настройки в ряде случаев нужен руб зуба в банке руб зверя рук суббота не выдают должно быть мотивированным значит ну да упс inventory обязательно то есть всегда равны понятно быть какой у вас клиент какому кластеру подключен какие у нее настройки да потому что вы будете обновлять ваш сервис и постоянно и довольно часто примерно раз в неделю там один hotfix и если там представишь надо руками там обновить 300 кластеров 30 русов до 300 кастеров это очень долго поэтому найти как масло настройка и вообще практически невозможно поэтому все это автоматизируем обновление сервисов до из раз в неделю будет проблема потому что новой версии ходу по новые патчи и 5 изменяется реализация этих ли пяо изменяется уследить за этим принципе невозможно где-то что-то будет постоянно взорваться надо уметь будет быстро подправить да и обновить все всех всех ваших все ваши сервисом масло обновлен настроек тоже если сервис настройки например для дев sgp для прокси dfs как источники подключены если источники постоянно новые появляются нужно включать их на лету общем должна быть и софт чтобы автоматически настройки подхватывал и чтобы можно было массово 1 настройки для каждого для раз сервиса мы используем джин пастис on seba принципе нормально работает для разработчика самолетом принципе нко не зная inventory храним wars тоже все всех устраивает хотя есть отдельный инструмент в мире есть сбер да то есть отдельный инструмент для и показывает список кластеров там так далее что же нам помогает среды для каждой среды свой профиль тоже не понятно обязательно что домена carry by rustam другие и так далее все сервисы запускаем тока через систему сидели не знаю кто то может еще не запускает но если кто-то не запускает ужина и запекать простой причине потому что у нас часть сервисов прокси стоит на virtual koch виртуалке обновляется автоматически операционной системы должно обязательно через systems цельсию поднимать так у нас несколько экземпляров кластер подключен к несколько экземпляров сервисов наших мы соответственно файла и рамси нормально на но есть вообще все упадет виртуалке обновляются очереди дай теоретически могут вообще всего все момент обновления быть остановлены поэтому с пользуемся этим стиле ну в общем дальше идет мантра по автоматизация по в дипломе проверки тоже должно быть автоматизировано почему потому что ходу пил много настроек и что не правильно настроить q1 сделали ничего не работает поэтому мы должны обязательно дернуть хотя бы там через hive села или билайном каждый сирле бы я и через парк обязательно для impala и снизу павлов скольку другой инструмент должно убедиться что после изменений настроек все корректно работает хотя примитивные запросы значит все мы сталкивался ч.л. доп тормозит пришлось увеличить время кэшированием причем так у нас запросы там могут валится одновременно несколько но пользователя то могут за спамить то мол дат и поэтому кому-то весело очень в общем и все запросы внешних сервисов и sheeran ну каширу им прежде всего для того чтобы у нас не было ходов как веселые ребята веселятся нас зависание внешний сервер вызову это отказ сервиса поэтому здесь каширу им то что успели уже получить потому что могут быть президент на и сбоем то есть сервис моргнул через пять минут он там откатился ли опять поднялся ну а мы уж все равно запрос от работы пускай шотным что ты взяли ну сами у себя от ограничивать как-то клиентов наших не можем потому что клиент собственно говоря у нас компонента hadoop другого если там ему шутку то шубку вернем нас там сама сам spark приложение sparkly запросто мха и упадет поэтому должны все запросы отрабатывать несмотря на сколько сколько бы их и не валилась функциональным тестированием теста быстро ненадежно проблема с особенно сочи масло очень просто очень много параметров нам доходил до смешного какие-то части параметров по шрифту передавалась там стерилизовали какие-то данные они попадали в виде декады символы в 3 ходу в третьем ходу пятом взрывалась при детализация ошибка там живые мы желаем крошился вот поэтому здесь тоже и не тесты не панацея функциональные тесты очень много времени тестируем с помощью клиентских инструментов пишем с кредитной питания и где в парке или в диване или в хай силой гоняем эти тестовом то есть пишем as the test и функциональное тестирование ну не умеет тепло и понятно без недоступность усов не работает некую клиентам разбираться там делать каких тех окна не будет понятным причинам именно что задачам пакетные они расчеты витрин или модели они занимают много времени трубу надо тестировать обязательно уж много клиентов много конфигурации то есть могут массовый сбор быть и степени тестировать но грифы и вот выбираем кого таков какой то кластером ищем лояльного заказчику которым будем помогать и на нем как бы упражняемся мониторинг значит мониторинг здесь очень важно не настолько важно чтобы вообще в него по рация но очень важно да ты служб как правильно suv написать который не надо мониторить но здесь прежде всего мониторинг джесси потому что опять же работаем с паркетом паркетные библиотеки очень много кушать памяти размер чипа для нас было неважно потому что у нас там hadoop скифу стандартные были hadoop ски глостера сервера там было очень много памяти нам столько не нужно поэтому не кончалось памяти тоже проверяем размер джесси функция там вот скорость роста hippo количество открытых де скриптов обязательно мониторим потому что могут опять же кончится connect connection элите при небольшом ну если свет случилось да то там у всех потребителей ничего не работает количество потоков тоже важно уже говорил да то есть смотрим что у нас много потоков значит вы что-то очень медленно обрабатываете доступно положение понятными свободные места логии файлы приложения обязательно тоже мониторим потому что он для ходу по очень часто много-много-много лобов пишется и забивается место можно настроить архивируем автоматической от насморка биотик пользоваться букву g или look back to чем угодно тоже если есть возможность это надо сделать ну важно для клиента чего клиент вообще смотрят это средние максимальное время отклика 5 потому что ему это важно все остальное для любителя вообще неважно эпигонов отвечал отвечал быстро и работал ну конечно ошибок не должно быть в завете хотя ходу пионер вычищать микстуры бывает что важно для вашего сопровождения сопровождением клиентских серверов кластеров ну вот у нас сам сбере да значит первое это понятно что сервис должны без недоступности сервис если не доступен падают потоки их надо перезапускать или бани автоматически запускать опять это время очень неприятно сервис не должен возвращать ошибки но наверное понятно да то есть но в целом есть хотим едят с парка spark хотя там есть ретро это мы продали донос же просто будет никогда ни один раз но нам несколько раз будет крутиться поэтому если у вас какие то ошибки то извините ваши клиенты не будет вашим сервисом пользоваться потому что ждать пока у них потоки попадают они не хотят rambus типичная ошибками про kerberos уже говорил всю на книжку записываем ходу можно читать скот можно форму читать но лучше утром был завести там все записывать эти проблем в томате зиру и мониторинг тоже pingu им все наши кластера что наши сервисы работают что там через хаев через парк все отрабатывает обучение без них работу hadoop потому что документацию ходов много сопровождении часто не при мальчику он работает разработчики часто не понимает вы единственно люди которые может в общество сколько им читали и хотя там примерно поставляете взаимодействие свежими командами но опять же здесь у вас будет несколько команд для клиентов это все другие сервисы значит либо вы с ними взаимодействовать вам указать информацию он заберет у вас несколько мудрых про них систем откуда мы получаем различной информацию в proxy журналирование там всякие там для безопасности то есть нужно полноценный миграционный стенд понятно сложно собрать но по другому не получается мы просим эти команд помогаем им и просим их выстроить систему мониторинга отзывы и happy скорости отзыва ну иначе сами путь все shakers брать найти кластерах взаимодействие с владельцами кластеров ходу тоже заказчикам нужны отдельные стенды много различных ошибок и нужно уметь быстро понять где ошибка общество ответ такой на потому что только у вас счет не будет работать ваших сервисов заказчик скажу что двор сервиса на лад мы часто будет права а часто они сами не понимают держит заказчик всегда прав опять же он подключается к вашему сердцу а ходу по ожидает что как ходу поем и отрабатывается ценность такой же скоростью с такой же надежностью вот ходу все таки достаточно надежен если не считать там проблем с кучей настроек а так в принципе довольно держу контакты мои с кому то вопросы будут может написать мы в целом если подвести там итоге дата в принципе продукт который мы разработали в потратили довольно много сил прежде всего там изучая код hadoop с самого выходу то есть тот продукт сказал довольно удобно в том смысле что он стандартный api использует комментировать и съема заказчиком удобного кластером к нему подключаться ни одна еще дорабатывать не на там дополнительно защищать в общем кажется всем удобно спасибо спасибо"
}