{
  "video_id": "EvzNBr6toWI",
  "channel": "HighLoadChannel",
  "title": "Алгоритм инкрементальных бэкапов в Apache Ignite / Максим Тимонин (СберТех)",
  "views": 128,
  "duration": 2827,
  "published": "2024-10-29T03:07:33-07:00",
  "text": "сейчас будет доклад А про Open Source от Максима Тимонина из бертека Давайте его тепло встретим Максим пожалуйста Поднимайся на эту сцену А расскажи немножко я правильно понимаю что apach ignite который ты ты разрабатывает apch ignite Ну не я наша команда Понятно Ну ты в том числе да то есть то есть там твою контрибуцию можно зайти и прямо посмотреть Приходите на github сылку могу дать то заходите на github реально есть контрибуция в apch ignite И большинство из вас скорее всего айтом так или иначе пользуются да на самом деле Игнатом пользуются все хотят они этого или нет А по той причине что Сбербанк Банк номер один и всё равно часто слышали эту фразу Переведите Скиньте мне денежку на Сбербанк и соответственно чтобы транзакция выполнила в Сбербанке Она проходит через множество систем внутренних таких как обработка номера карты н антифрод и прочие системы и все они тем или иным способом используют игна в своей работе да всё супер а при за лучший вопрос думайте лучший вопрос в конце будем задавать всё пожалуйста Макс тебе слово спасибо большое так кликер да всем Привет ещё раз А Спасибо что пришли на доклад а Спасибо всем кто смотрит онлайн и всем тем кто посмотрит в будущем Вот я сегодня расскажу вам про алгоритм инкрементальный бэкапов А я работаю в компании из bertech и наша команда разрабатывает и внедряет его в Сбербанке и в других компаниях пару слов о нашем продукте это в ядре масштабируемый Stage который хранит данные в памяти но при этом имеет опцию хранения данных на диске поверх построена Революционная модель с поддержкой распределённых транзакций про которые мы сегодня будем говорить а также наши клиенты используют как комфор став способов написать пользователям лямбды жабы и отправить их непосредственно на узлы чтобы они выполнялись с данными вместе в памяти а Сбербанк достаточно плотненько сидит на игнате всего порядка 300 проектов которые его используют тем или иным способом в том числе Mission Critical системы про которую я говорил в начале используют они не совсем Ваниль Пана это наш for VD построен он на ванильном игнате отличается небольшим набором коммитов мы постоянно удерживаем его в актуальном состоянии и набором плагинов такие как сжатие данных в памяти кэш дампы быстрый cdc и другие у нас команда контрибьютор непосредственно комитет в apch ignite поддерживает сообщество и помогает нашим клиентам А все вот эти наши Mission Critical системы них достаточно жёсткие Требования по отказу устойчивости и по восстановлению после аварий и в игнате есть несколько инструментов которые обеспечивают эти требования один из них Давайте попробуем быстренько разобраться как он работает в ЧМ его особенность на картинке кластер Игната немножко надуманные ну представим что такие тоже существуют четыре узла разнесенных на разные машинки есть табличка с данными которая сбита на восемь партиции у каждой партиции есть хотя бы одна копия приходит системный администратор и отдаёт команду на снятие что под этим подразумевается что данные находится в памяти сбрасываются на диск в определённую директорию процесс этот не мгновенный он занимает длительное достаточно время Поэтому параллельно с ним выполняются пользовательские транзакции Ну например такая транзакция которая изменяет данные в двух ключах в двух разных партиции и вот мы видим что на нашем примере она задействует все узлы в кластере а и задача Игната сделать так чтобы эта транзакция Никаким образом не повредила данные бэкапа чтобы он оставался а как это сделано в игнате а на первом этапе работы бэкапа запускается вводится распределённая блокировка а под который мы дожидаемся завершения всех активных транзакций А все новые транзакции Мы немножко приостанавливают закрой а узел доходит до состояния такой консистентность в котором он создаёт карту данных которая затем используется для построения бэкапа А после чего блокировка отпускается начинаются работать два процесса параллельно копирование данных и пользовательские транзакции если транзакция пытается изменить какие-то данные которые ещё не попали в п то она просто получает копию этих данных и работает с ней А ну из того что я объяснил можно сделать два вывода первое что снятие полного бэкапа достаточно интенсивная операция которая эффекте рама игнайт пытается использовать всю память которая у не есть чтобы разместить там данные Поэтому иногда её даже не хватает И мы эти данные сжимаем в памяти и поэтому дополнительный расход рамы в ран тайме выглядит Ну не всегда приятной идеей и вторая мысль - это что Stop за пауза порождает просадки пон а не сказать что они прям супер большие но если запускать ись раз сутки то наверное не так страшно если мы хотим делать бэкап достаточно часто то это может иметь какое-то влияние на кластер и на Тель скую нагрузку что тоже не очень хорошо а при этом нужно помнить что эп - это Одина из опций по восстановлению кластера после аварии она не основная как правило это План Б Но тем не менее иногда классно иметь там полную копию чтобы быстренько на неё установиться Проблема в том что если мы снимаем бкп редка по вышесказанным причинам то данные которые находятся в Б капе могут сильно отличаться от тех данных которые в продакшене и отставать на несколько часов и это Ну не совсем то что хотят пользователи а пользователи хотят делать бэкапы чаще чтобы в случае какого-то инцидента мы могли восстановиться из бэкапа на как можно больше Ну более поздний момент времени чтобы иметь возможность продолжать работать потеряв там данных хотя бы там за несколько минут но не за несколько часов чтобы сделать ча нужно делать чащ нам нужно решить несколько проблем одна из них - это как раз вот эта распределённая блокировка про которую я говорил А как выглядят вот картины сейчас есть какой-то Тайлан есть какие-то транзакции которые выполняются последовательно и существует чётко очерчена линия когда мы знаем что кластер находится в консистентной это как раз момент когда возведена эта блокировка когда завершены все активные транзакции и мы понимаем что других изменений данных не будет Мы здесь точно знаем что мы можем данные начинать копировать если мы убираем эту блокировку то получается что-то такое мы к сожалению не можем провести Вот эту прямую линию мы единственно что можем просто привести какую-то линию она может быть ломаной кривой и она даёт единственную гарантию что где-то до этой линии существует такая точка консистентность кластера но эту точку нужно найти и посчитать то есть нельзя е просто поймать в момент времени А и получается для того чтобы вот это выполнить нам нужно иметь две вещи первое нужно иметь историю операций и второе нам нужно уметь ориентироваться в этой истории операции таким образом чтобы фильтровать те транзакции которые не должны попадать нам в бэкап А что ж и Давайте попробуем решить а одну из первую из этих задач Как хранить историю у гнати есть две опции как этого можно добиться Первое - это mvcc а для тех кто не знает не помнит mvc - это такой механизм который позволяет хранить несколько версий в данных и за счёт этого позволяет Ну сэкономить в некоторых сценариях на количество блокировок между транзакциями А и здесь всё хорошо то есть на этапе кПа можно зафиксировать какую-то версию относительно этой версии прыгать но здесь есть несколько проблем первая проблема что vcc - это опция то есть пользователи его могут включить а могут не включать и более того что они ещё друг с другом несовместимы с точки зрения хранения данных То есть если вы уже включили mcc для того чтобы его выключить или наоборот вам нужно делать полную миграцию данных мигрировать большое количество клиентов в Mission системах выглядит так себе идеи и вторая Проблема в том что мы к сожалению не можем заставить наших пользователей перейти в accc потому что в нём есть некоторые проблемы Он реализован для всех режимов и имеет некоторые проблемы с производительностью кому если вдруг очень интересно то вот есть ссылка на листе на котором те проблем которые есть сечас в mcc и те которые нам предстоит ещ решить вот эта идея получается нам не взлетает какая у нас есть вторая опция вторая опция хранить историю это хранить её внутри ла - это механизм подающей записи прежде чем данные попадают непосредственно во внутренние структуры данных ината они предварительно записываются во и этот механизм используется для Rec когда узел падает поднимается останавливает сво состояние через механи используется для ребаланса данных с одного узла на другой механизм используется для cdc теперь этот механизм используется для инкрементальные хранить как раз историю данных что нам требуется но здесь тоже есть проблема Проблема в том что в ите очень много conc Ну прям очень много Я вот всегда думал что я разбираюсь в кон до этой задачи Вот после этой задачи я опять так думаю сй зада Ия вмв Вау одной транзакции - это не один рекорд это набор нескольких рекордов вот на в первой линии видно что там транзак транзакция один есть два ключа и мы отдельно их пишем более того здесь какой-то есть дополнительный набор системных рекордов А если мы представим два узла с одинаковым набором партиции с одинаковым набором транзакций которые выполняются на этих на этих партиции на этих узлах у нас нет гарантии что порядок записи в рах логи одинаковый то есть они могут межева друг с другом и это создаёт проблемы как раз про которые мы говорили что нам нужно уметь каким-то образом ориентироваться в этой истории хранить историю мава и каким-то образом фильтровать То есть сейчас мы подошли к тому моменту что для создания инкремента бэкапа Мы готовы использовать и hatw но нам нужен какой-то алгоритм с помощью которого мы можем определять те транзакции которые должны попадать в бэкап и те транзакции которые мы должны оттуда исключать В поисках этого алгоритма мы пошли клам не к нему самому Но к его работам и среди его многочисленных публикаций мы нашли Та которая по названию очень похожа на то что нам нужно в общем эта работа немножко больше чем про нахождение консистентной состояния но в том числе Она имеет в себе алгоритм по созданию по нахождению этого состояния А эта работа достаточно известна она широко представлена на юту Бах можно пойти и в любой интерпретации её послушать посмотреть её преподают в университетах это это алгоритм более того Мой коллега Коля ижи ков сделал перевод статьи на русский язык выложил на Кабр вот Пользуйтесь если кому-то интересно перевод прямо хороший я им пользовался в моменте когда готовился к выступлению А поскольку не будем останавливаться подробнее на этом алгоритме просто посмотрим те свойства и те требования которые этот алгоритм нам Ну Просит чтобы мы его выполнили но первое свойство алгоритма в том что он готов выполняться параллельно вместе с другими вычислениями это как раз то что нам нужно никаких дополнительных блокировок просто какой-то дополнительный алгоритм который работает вместе с транзакциями а процессы не имеют доступа к общим часам это часть тех вещей которые нам нужно будет доработать сейчас у нас такого нет конечное число узлов и каналов между ними Ну кажется это достаточно простое требование хранит данные на фиксированном количестве узлов и для того чтобы изменить это число узлов нужно сделать там достаточно сильные приседания и по этой причине обеспечить там фиксированный набор узлов при выполнении бка достаточно легко Мы можем это сделать вот Следующий пункт немножко нас расстроил а игна алгоритм выставляет требования чтобы каналы у нас были ффа очередью что здесь имеется в виду у вас есть два узла которые обмениваются между собой сообщениями и задача узла приёмника обработать сообщения в том же порядке в котором были они были отправлены и когда мы говорим про распределённые транзакции это выполнить очень сложно а данные транзакции задействуют только набор узлов а транзакции могут пересекаться по набору узлов узлы могут выполнять разные роли в рамках транзакции как хранилище основной копии либо как хранилище запасной копии и во всём вот этом хаосе обеспечить порядок выполнения сообщений Ну нереально вы игнате есть один механизм Один режим работы для которого этот алгоритм бы подходил бы если кому-то интересно Можем потом обсудить В курах или на вопросах но сейчас это выходит за рамки нашего доклада тем не менее читая работу мрта мы находим отсылку другой работе которая возможно нам поможет потому что она обещает что алгоритм другой алгоритм который будет в ней представ рате номе должен работать в случае если порядок событий частично упорядочены в надежде что мы найм там какие-то идеи Я отправился на поиски этой работы но потерпел неудачу есть замечательный сайт на котором представлены все работы Лам и каждый из его работ там их можно скачать они в формате лежат в свободном доступе ко многим из этих работам лежит некоторый комен ково что идея это алгоритма пришла в ампат вместе с его коллегой вечером за ужином но они не смогли доказать этот алгоритм потому что выпили слишком много вина им пришлось доказывать это на следующий день утром а но другой интересный факт что вот эта работа написана в восемьдесят пятом году что лишь в 2012 году кто-то из читателей обратил внимание на эту же ссылку номер семь в надежде её найти на что вам про сказал что он понятия не имеет Что это за работа Откуда она взялась Никогда он её не писал не видел и Вероятно это ошибка вот таким образом мы зашли были заведены в тупик ули вам портом А мы начали поиски заново и на индийских юту Бах мы нашли там многочисленные лекции посвящённые алгоритмам нахождения консистентной один из алгоритмов А вот этих товарищей к сожалению не знаю как правильно их фамилии произнести но тем не менее этот алгоритм лёг в основу нашей реализации Итак что он обещает он обещает что они сделали алгоритм как у апорта но только сняли требования по фо чтобы каналы были фифо и это прикольно теперь можно отправлять сообщения и обрабатывать их в произвольном порядке Лишь бы они достигали за конечное время ещё они обещают что не нужен координатор То есть у все узлы каждый узел Независимо принимает автономное решение о том что они достигли точки консистентность это звучит прикольно на самом деле это не совсем так но в каких-то определённых случаях действительно этого можно добиться и там маленькую свойство этого мы использовали и и третья ещё интересная штука что любой узел Может начать процедуру бэкапа спонтанно Ну не бэкапа а нахождения конного состояния и это то что мы использовали а кажется всё сходится очень похоже на тот алгоритм который нам нужен Судя по требованиям Давайте попробуем его теперь разобрать в оригинальной работе Лесли Ламберта Он попробовал привести метафору в которой пытался объяснить своим читателям А в чём собственно смысл алгоритма в этой метафоре МРТ представил множество фотографов которые фото и пытаются уловить движение птиц мне показалось что это достаточно сложная метафора я попробовал её упростить потому что орнитологов у нас немного поэтому сегодня мы будем считать звёзды я позволил себе эту метафору немножко изменить В общем Представьте что вы находитесь летом где-то в поле над вами Звёздное небо сегодня поток персид и ваша задача посчитать точное количество упавших звд в ЧМ стоит проблема задачи что у вас нет прибора который может охватить У вас есть несколько приборов которые каждый из которых покрывает какой-то сектор и проблема проблематика в том что эти сектора пересекаются Падающая звезда не является точкой а такой отрезок который может попасть в сразу несколько объективов и по этой причине приборы вынуждены друг другом обмениваются информацией о тех звёздах которые они посчитали чтобы избегать дубликатов с точки зрения астрономии возможно немножко намана задача Но я последую другую цель рассказать в ЧМ собственно алгоритма каждый такой прибор может быть представлен в виде микросхемы у микросхема три порта на один порт X вешается собственно наш объектив который считывает звёзды внутри микросхем какая-то логика которая обрабатывает падающие звёзды и в случае если Падающая звезда зачитала то она просто делает внутренний инкремент и при этом отправляет информацию по упавшей звезде куда-то наружу через исходящий каналы out1 А давайте для простоты представим что вся наша Система состоит из двух таких микросхемой которые соединены между собой последовательно здесь интересный момент нужно думать о том что эти микросхемы соединены не проводом по котором порядок сообщений гарантирован А через какую-то сетку и сообщения переданные по этой сети могут приходить в рандомном порядке то есть например если вторая первая микросхем нашла вторую звезду то информация по второй звезде может дойти до Второй микросхемы раньше чем она дола чем дошла информация по первой и скажем в этот момен когда насчитала Две звезды отправила информацию по ним на вторую микросхем вторая микросхем ещё ничего не получила ничего не зарегистрировала пришёл администратор и там или звездочёт и сказал посчитай мне X звёзд и алгоритм даёт достаточно простую формулу Чтобы это сделать Она состоит всего из трёх аргументов первый аргумент - это наше внутреннее состояние это состояние нашего этого внутреннего счётчика который мы считаем а второй аргумент разница А2 ми и1 - это состояние канала взаимодействия между вторыми микросхемой первой то есть вот э стрелочка которая идёт снизу Вот её внутреннее состояние - это как раз вот эта разница А 2 -1 который нам нужно посчитать при в тот момент когда приходит команда на подсчёт X мы фиксируем наше внутреннее состояние S1 фиксируем состояние всех каналов out1 и и1 это происходит в тот самый момент когда пришла команда на снятие бэкапа Ну теперь мы видим если мы попробуем что-нибудь подставить Мы видим что нам не хватает одного аргумента нам не хватает 2 его не но нам необходима для того чтобы посчитать алгоритм умный он говорит так если бы команда пришла на вторую микросхему то формула была бы зеркальной и тот же самый X Прошу обратить на это внимание что X на одной второй микросхеме считается одинаково что у второй микросхемы была бы Зеркальная формула и для расчёта Ей бы не хватало иметь А1 Давайте просто его отправим с первой микросхемы на вторую микро формирует специальное сообщение куда запаковывают все отправленные звёзды которые она находила ранее и отправляет это сообщение на вторую микросхему и допустим для простоты что это сообщение дошло вперёд то есть Несмотря на то что оно было отправлен в третьем после первой звезды после Второй звезды это сообщение пролетело вперёд предыдущих А в тот момент когда это сообщение пролетело вторая микросхем фиксирует своё состояние там всё по нулям мы не посчитали ни одной звезды не приняли ни одного сообщения но при этом у неё есть информация и она может его подставить в формулу и рассчитать количество звёзд Ну получается двоечка То есть получается что в алгоритме вторая микросхема не зарегистрировавший ни одной звезды на данный момент смогла получить правильный результат и она отправляет сообщение А2 в котором пустое множество на первую микросхему если подставить там тоже самое получится та же самая двоечка то есть алгоритм Похоже что работает на что зде стоит обратить внимание на ключевые моменты арим из неко дество действие запуск алгоритма то есть его можно алгоритм запускается либо по команде от администратора либо по специальному маркерно сообщению при приёме сообщения первым делом что мы вообще узел должен сделать - это зафиксировать своё состояние то есть состояние сходящих и исходящих каналов и состояние S и вся проблема Проблема в том что множество out1 и1 и так Дале они не чистятся то есть мы вынуждены хранить всю историю отправленных сообщений всю историю принятых условию алгоритма Вот и кажется это вот самая большая проблема фо описание алгоритма но теперь вот состояние это то что нам нужно найти Итого ЕС подведём небольшой итог для того чтобы создать инкрементальный Нам нужен и нам нужно множество также на узлах нужно множество отправленных сообщений с одного Наго павших звёзд не проблема когда мы говорим о системах у которых десятки сотни тысяч rps то как бы большая проблема хранить отправки этих сообщений особенно если мы хотим делать бэкап там не каждую секунду а Раз в 5 минут то есть количество вот этих данных которые нужно дополнительно хранить оно достаточно велико соответственно Давайте попробуем теперь оптимизировать вот эти две метрики для начала Давайте ещё раз посмотрим на схемку передачи сообщений в рамках алгоритма то есть а которые мы только что обсудили есть четыре узла на Первом узле на самом верхнем происходят события Ну допустим падает та же самая звезда И в этот момент информация по этой звезде отправляется на каждый из других узлов вопрос А откуда узлам знать какие сообщения они получат Ну не откуда то есть сообщение приходит в Случайный момент времени В случайном количестве собственно это является причиной по которой нам нужно отправлять вот этот Out медж чтобы просто сказать узлу что ему собственно ждать как на какой-то определённый момент времени и Слава Богу что вра ция обмен сообщением происходит другим способом в игнатии используется алгоритм двухфазного комита для транзакций и в нём обработка сообщений выглядит немножко по-другому То есть у нас есть сначала фаза prp на которой каждый участник транзакции договаривается о том что какие сообщения ему стоит ожидать и вторая фаза комит на которой собственно непосредственно сами изменения и вносятся а то есть получается что непосредственно изменение в данных происходит в самом конце и сообщение об изменении данных не приходят из ниоткуда они приходят только из тех транзакций которые были предварительно зарегистрированы во множестве prep То есть транзакция попадает во множество prepare через некоторое время прилетает реквест на то чтобы эти данные либо закоммитить либо откатить Ну и кажется это как раз то что нам нужно то есть вот это множество Out - in можно свести к множеству локальных prp транзакций Вот и соответственно теперь наш алгоритм поиска консистентной выглядит таким образом Нам нужен ра headlock и нам нужно трекать а prepare транзакции те транзакции которые у нас должны будут закамини в будущем Отлично Теперь давайте всё попробуем это свести вместе Итак возвращаемся к нашей системе у которой есть а четыре серверных узла четыре узла Игната и Давайте попробуем начать процедуру снятия бэкапа процедуру можно начать по команде от администратора когда администратор на определённом у зле выполняет команду снять бка в тот момент прощения в тот момент когда коман Когда в тот момент когда узел принял сообщение от администратора Что необходимо снять КП в этот момент он начинает отправлять специальные сообщения на все другие узлы То есть если вы помните алгоритм был специальное сообщение маркерное которая себе содержа множество всех отправленных сообщений если это маркерное сообщение нужно передавать это множество А но теперь мы можем просто достаточно передавать один маркер какой-нибудь дишни в котором всего 16 байт на все остальные другие узлы просто для того чтобы нотификат создать эту отсечку по времени в тот момент в который нужно п снять а соответственно Что делать если у нас узел не участвует в тракционное я вам напомню про одно свойство алгоритма что узлы могут начинать процедуру бэкапа спонтанно все узлы вы игнате связаны между собой дополнительным протоколом по кольцу это надёжный протокол который гарантирует доставку сообщений гарантирует последовательность сообщений и по этому протоколу мы можем отправить тот же самый дишни и гарантируя то что он дойдёт до каждого из узлов этот дишни просто прилетает на узел и тот спонтанно начинает работу процедуры бэкапа в Независимости от того каком Ну в каком процессе сейчас находятся все другие узлы а что ж процедуру бэкапа мы начали Давайте попробуем Теперь всё это разметить у нас есть вал в которой мы последовательно пишем рекорды а начинается процедура бэкапа и мы фиксируем наше состояние S это то те по факту состояние S - это тот вал который Мы записали к этому моменту времени то наше текущее состояние то количество звёзд которые вот мы посчитали до этого момента и фиксируем список активных транзакций которые у нас есть как раз это множество Out - in после чего нам Мы должны дождаться завершения этих транзакций да то есть мы должны дождаться для того чтобы эти транзакции внесли изменения в наши данные чтобы мы потом их могли восстановить при восстановлении бэкапа мы дожидаемся завершения транзакций и здесь мы не сразу пишем Finish Record мы на самом деле здесь немножко Ждём когда каждый из узлов закончит ожидания активных транзакций в этот момент мы гарантируем что не будет ни одной транзакции которая должна попасть в бэкап и после чего уже пишем вот этот финиш Яр дополнительный и внутри вот этого финиш Рекорда существует карта того как необходимо транзакции которые у нас присутствуют и записаны внутри вала то есть не все транзакции которые нам попадают это промежуток Между первой и второй рекордом должны в не попасть в п по какой причине например транзакция просто началась после Старт Рекорда а закончилась до финиш Рекорда такая транзакция точно должна быть исключена просто по той причине что ну она нам не было извест на морение тци есть Граф а то есть любую транзакцию можно представить в виде графа а С вершиной ну здесь простой пример есть один одна Reg нода координирующий узел есть там два узла отвечающие за основные копии данных два узла которые отвечают за бэкапы тот узел правый самый с вашей стороны левый тот который является вершиной он принимает решение о том коммитить транзакцию или откатывать её соответственно мы дополнительно повесили на неё ответственность определять Должна ли транзакция попадать в бэкап или нет как она это принимает решение об этом очень просто в тот момент когда приходит момент принятия этого решения узел просто проверяет своё состояние Идёт ли процесс бэкапа или нет Если идёт процесс бэкапа Значит транзакцию она исключает и по фиш месседж который отправляется вот по стрелочкам распространяется информация об этом бка и распространяется информация о том что конкретно эту транзакцию нужно исключить Вот либо если процедура бэкапа ещё не началась то мы отправляем там пустое значение просто Now value то есть ничего не отправляем и это является символом того что транзакция должна попасть в бэкап вот финальной весь алгоритм напомню то что мы пытались добиться Мы пытались добиться оптимизации снятия полного бэкапа потому что это было достаточно дорого с точки зрения памяти достаточно дорого с точки зрения ун и благодаря этому алгоритму нам удалось решить эти задачи то есть снятие бэкапа Никаким образом не аффектив польскую нагрузку алгоритм выполняется совершенно параллельно вместе с транзакциями нигде их не притормаживает и работает автономно и его можно запускать каждые там 5-10 минут в зависимости от тех требований которые есть у системы а алгоритм выполняются очень быстро то есть мы тестировали на узлах состоящих из десяти там машинок достаточно высокой нагрузкой Весь процесс это сам алгоритм плюс там формирование файлив для бэкапов это порядка 300 миллисекунд Ну где-то так в среднем а то есть это достаточно быстро достаточно Здорово а и также мы нигде не расходу Ram Единственное что мы храним в памяти - это набор prp транзакций храним Не сами транзакции а только дишни дишни транзакция там достаточно маленькая величина Даже если за эти 300 миллисекунд там набегает тысячи транзакций Ну это копейки там пара килобайт это прямо в самом таком худшем случае который нам нужно дополнительно копить хранить всё после того как мы запустили процесс выполнили заморозили список транзакций дождались его завершение дождались гарантии что больше транзакций скапа нет мы пишем на этом месте мы отрезаем и формируем там ВК файлики выкладываем лое сегменты в отдельную директорию и здесь мы можем почистить ту коллекцию которую мы хранили хотел рассказать пару слов в завершении нужно сказать что реализовывать алгоритм оказалось достаточно сложно вообще с алгоритмами оказалось очень сложно во-первых их казалось достаточно сложно искать вот видно на нашем примере Их достаточно сложно понимать в алгоритмах оказалось Когда вы читаете бумагу каждое слово имеет значение и мы вместе с моими коллегами когда обсуждали это алгоритм кто-то там слово не понял кто-то предложение которое какое-то не дочитал в итоге ждали совершенно разные концепции и Поэтому собственно я думаю стала причиной то что Мой коллега написал статью на русском языке просто чтобы точно разобраться что имел в виду автор а очень сложно это реализовывать потому что на самом деле очень много специфики в каждой системы Вы игнате в том числе но при этом очень здорово что эта работа осталась не Академической А мы её применили непосредственно в наш в реальном проекте для наших реальных пользователей вот и это здорово всем спасибо за внимание готов ответить на вопрос Да супер Спасибо большое а поднимайте руки да у кого микрофон сразу Вставайте задавайте у нас в целом минут 15 есть на то чтобы пообщаться так микрофоны сейчас секундочку так так так так так да ребята микрофоны Давайте это раскидаем А хотел спросить вы А вот эта история что бэкап снимается просто с отдельные ноды она тут не подходит то есть тут из-за того что распределённая история надо обязательно все ноды у нас а кустра обычно достаточно большие одно нодовый г не имеют никакого смысла Вот то есть она она не владеет полной информации Да ну не то что не вде просто зачем обычный кейс - это когда данных так много что они тупо не помещаются на одну ноду да то есть как минимум нужен кластер дальше возникает вопрос нужна отказоустойчивость нужно партиционирование сегодня вот был хороший вопрос спрашивали типа вот транзакции двухфазный комит это же Достаточно долго Вот соответственно вот эти все должно быть оправдано А куда обычно КП снимается то есть огромное количество данных оно куда льётся дру большое на самом деле те волы которые мы снимаем они достаточно компактные то есть размер именно полного бэкапа он достаточно здоровый То есть это могут быть терабайты данных десятки терабайт данных когда мы снимаем инкрементальный бэкап если мы там снимаем там Раз в 5 минут его размер там мегабайты То есть это файлики валов которые очищены от ненужных реков которые сжатые Они вообще копеечное место занимают Ну просто они хранятся админист имет возможность их перенести спасти куда-нибудь Оке Так ладно вопросы пожалуйста Максим Спасибо за интересный доклад вопросов очень много Я постараюсь задать те которые вот мне самыми интересны показались може по одному задавать первый вопрос Ну про распределённые транзакции Понятно а как быть с теми данными которые пишутся не в распределённые транзакции а затрагивают только одну партиципация приз у нас уже определён вот вы знаете есть вот это понятие таблиц на самом деле оно исторически называется кэш когда ты его конфигурируется в виду что ты выполняется транзакция на уровне там основным хранилищем основной копии и запасными копиями между ними выполняется транзакция здесь а при этом у нас есть такие вещи как то есть двухфазный комит он очень хорошо оптимизирован то есть случае если в транзакции всего один ключ то скорее всего твой узел сразу пойдёт на твой запрос сразу попадёт на узел где этот ключ хранится он там выполнится а и остальные запросы допустим поход на бкп реплицировать оно будет асинхронно то есть у тебя выполнится запрос моментально придёт ответ в п транзакция выполнится чуть подольше это в контролируется настройка здесь можно регулировать то есть достаточно ли тебе что выполнится операция только на основном узле либо если есть требования чтобы она гарантированно записалась на п то тогда там взводится жок и у тебя получается тебе придётся ждать да похода на п Ну неп А вот на копию этой партиции всё понятно второй вопрос Если я правильно понял записани алгоритма может быть такая ситуация что транзакция которая началась с точки зрения пользователя Раньше она не попат ВП а та которая началась позже попат нода сама определяет Какая транзакция попадает ба или нет да Это хороший тоже вопрос А смотри можно включить слайды они работают Они включены Да щёлкай просто назад Ага вот смотри скажем у нас есть клиент который выполняет транзакции последовательно а клиент вот этот та вершина которая у нас в этом транзакции если как только она узнала про том что п начался у неё вводится фжк все последующие транзакции соотвественно в КП не попадут Да я об этом и говорю ноды могут в разное время узнать что начался процесс Сопа они все ноды которые участники транзакции они синхронизируются через двухфазный протокол то есть вот этот маркер сообщения про который я говорил у него есть на самом деле определённая мысль то есть смысл в том что когда отправляется маркер сообщения каждый узел должен первым делом обработать этот маркер и начать процедуру бэкапа и только потом работает сообщение в этом хитрость что благодаря двухфазном вот этому протокол обмену сообщениями то что мы обрабатываем маркер вперёд у нас всегда есть гарантия того что все участники транзакции типа на фазе комит В итоге синхронизируется А на типа решение будут они включать транзакцию в бкп или нет А если у нас сообщение о начале бэкапа придёт по кольцу то есть более позно так намт не транзакций там ничего страшного Ну я понял что лучше в дискуссионной зоне А я говорю лучше в дискуссионной зоне продолжить можно можно ишьюс просто и там уже продолжить Ну нет На самом деле это я вот мне наверное получилось рассказать алгоритм так как будто он выглядит очень легко на самом деле там очень много этих тонких моментов как типа что обработка сообщения вперёд идёт и так далее которые обеспечивают эти нужные гарантии проще показать код просто да и такой 30 минут смотрим так у кого микрофон Давайте вот Ага вот сзади ещё у нас у нас два микрофона всё да следующий возьмите кто будет дальше задавать Здравствуйте спасибо за доклад А подскажите по поводу вот фиксации транзакций То есть это по номеру происходит передаётся какой-то номер транзакции после которого следующие Ну как бы считаются зафиксированными или это просто флаг и по поводу вообще счётчика транзакций то есть там нет проблем с переполнения Так что мы зафиксировали там а потом оказывается что он переполниться и мы смотрим как бы в наше прошлое там или в будущее как зацикливание у нас идёт если сё так на первый вопрос там флаг то есть там ну там типа вот это айдини флаг или номер он он это просто Лота переменные в которых содержится номер но этот номер - это уникальный номер это просто uid который Просто сгенерирует он не монотонно вырастающий То есть каждый бэкап он просто создаёт какой-то дишни А который является вот признаком того дишни просто хранится там на узле он является признаком того что идёт процедура бэкапа когда узлы между собой договорились о том что они зафиксировали свои финишное состояние на самом деле запускается второй круг общения то есть это уже совершенно нно его не видит пользователь соть БПА Но на этом втором круге мы просто зачищаем этот дишни то есть либо есть АШК либо его нет вот такое состояние если аник есть значит п снимается Аника нетка не снимается вот второй вопрос Если я правильно переполнение счётчика транзакций То есть у нас транзакции тикают о там тдх битные чех битные вообще что происходит при заполнении как бы этого сч типа дишни транзакции он чуть Более сложный там типа наша внутренняя структур используется которая типа монотонно возрастающая которая зависит там типа тайм смпа Ну в общем там достаточно длинная там по-моему 322 бита на дишни транзакции Вот И там просто переполнить его ну там физически Мне кажется просто невозможно такое количество транзакций Ну нереально запустить то есть Ну опять же 300 миллисекунд - это то опять же 300 миллисекунд - это в течение которого времени нужно хранить все дишни за 300 миллисекунд там сся транзакций максимум Нужно запомнить это дишни а в рамках mcc он же ВС равно должен ротива говорил что это механизм как бы не использует То есть можно было бы использовать для реализации БПА но мы не использовали потому что обратная совместимость и потому что соответственно его нужно ещё хорошенько так долить А я правильно понимаю что по этой же причини Като конкрет момен вре здесь правильный посыл да то есть вот эти все инкрементальные снапшоты они сохраняются как инкремент который затем при восстановлении накапливается полный сначала полный бэкап мы восстанавливаем затем мы восстанавливаем вот эти инкремента последовательно а здесь можно просто регулировать частоту с которой мы снимаем этот инкремент вот и была изначальная идея что вот этот алгоритм просто запустить чтобы он постоянно в фоне молоти да то есть он типа нашл одну разница между инкремента была доста маленька эти же самые 300 миллисекунд например вот тут можно придумать там другие алгоритмы то есть после восстановления эту точку можно придумать алгоритм который соберёт со всех узлов там все транзакции соберёт их в одном месте примет решение о том какие из них восста каки или нет здесь идея алгоритма в том что ты восстанавливаешь полностью автономно то есть тот момент когда ты запуска у он сам восстанавливается поднимается входит в состав КСТ н а ещё тогда вопрос у вас БК полный то есть с периодичностью какой-то делается или туда просто доливают эти валы и как бы ну они схлопывается скажем так до полного бэкапа Тут нет ничего не схлопывается то есть типа делается снимается полный бэкап потом снимаются инкремента здесь зависит от администратора восстановление инкремента бэкапа не бесплатное то есть восстановление полного бэкапа бесплатно Ты просто файлики подкладывает бэкапа требует какого Като времени да Но типа просто это лы нужно считать их нужно применить и достаточно быстро там вот суточная нагрузка я считал там ну буквально за 2 минуты накатывает вот ну там прям прям очень много данных Вот Но это всё равно время и тут уже трейдов который должен там системный администратор для своей системы принимать решения Ну это же вы писали то есть вот время полного бкп оно позволяет вообще делать или лучше всё-таки что-то Дописать чтобы мы нам не нужен был в принципе кроме одного случая снятия полного бэкапа Вот они потом всё время складывались складывались и образовывали как бы полный бэкап наливали с как бы ну без восстановления скажем так А ну имеется в виду чтобы ну если вы подразумеваете что вы никогда не будете восстанавливаться подразумевается что мы никогда не будем делать полный бэкап кроме первого раза я понял Ну чтобы чтобы потом не никогда не восстанавливаться чтобы его да то есть мы потом пожалуйста делайте Не воспрещается не возбраняется спасибо да давайте дискуссионный дальше продолжим у нас так Ага давайте вот ту третий ряд Да добрый день очень классный доклад Спасибо большое Я думаю придётся ещё раз пересмотреть чтобы вникнуть во все детали вопросы мои будут немножко более простые первый вопрос Вы сказали что восстановление из бэкапа - это План Б Что за план А и В каком случае мне придётся прибегнуть к этому плану б и й Стой давай смотри план А - это использовать cdc асинхронную репликацию когда у тебя ра разных центрами и между ними баланси нагрузка и ска решение когда один кластер отваливается то ты просто переключаешь нагрузку на второй кластер а все данные до него через асинхронную репликацию с каким-то лагом доезжают И это ну план а он самый рабочий потому что о подразумевают Практически никакого простоя вот это все наши имен допустим может не сработать если какая-то багова синхронной репликация Вот это либо случилось какое-то удаление данных да такой там дроп табли который К сожалению там Ну уже сложнее намного Ну востановить из репликации тогда только п я понял спасибо кажется план А у нас уже есть скажи пожалуйста мы можем вашими наработками воспользоваться вы их уже за комети куда-то Да это Open Source Всё спасибо большое угу ну в реквест может да как минимум что-то лежит Нет в смысле это прямо в Мастере это можем пользоваться это рели пром релизе Ага давайте вот второй второй ряд Спасибо большое за доклад Андрей Спасибо за вопрос У меня такой же был Ага ладно Так давайте Тут третий Да спасибо за доклад Я вот здесь Да спасибо не знаю возможно дурацкий вопрос В общем я так понял что Суть проблематики в том что мы должны найти Влах ме мы бекам на всех нода и при этом отфильтровать то в чём мы не уверены Да вот а как но при инкрементальный бэкапа как мы находим откуда начинаем бэкапить то А я понял вопрос на самом деле когда мы снимаем типа полный бэкап мы Вау пишем рекорд там типа специальный рекорд что в этот момент снят бэкап полный и соответственно на инкремента Мы сначала находим вот эту Record Вот и который основной и после него уже начинаем накатывать а то есть инкрементальные бэкапы они всегда только от полного идут Да это требование Да но звучит логично в целом Да но там как я у сказал там ва не сказать что прямо большой потому что в валах основной размер занимает на самом деле не данные а там физические там то есть изначально использовался для Crash Recovery Когда у нас нода допустим у нас та процедура из чекпоинта когда данные сохраняются из памяти на диск если нода свалится во время чекпоинта когда частично данные заехали на диска частично нет то есть вот Для таких случаев В ле хранится очень много вспомогательной информации очень много Это типа в 10 раз больше чем полезной информации и то есть когда ты при этом в инкрементальный бэкапа нам вся эта информация не нужна вот эта физическая на нужно только логические записи считать 10% а потом дополнительно мы это ещё сжимаем то есть ну размер там инкремента бэкапа реально копейки это мегабайты одного Так ладно давайте ещё 5 минуточек У нас есть Ага спасибо за доклад у меня ещё вопрос провала То есть пока инкремент не записан то есть следующий инкремент не записан да Мы обязаны глубину Вало то вала хранить то есть и настолько глубины вала должно это учитывать да то есть из архивов он не читает так я понимаю из архива он Ну когда вал архивируемых сделаем отсечку мы просто собираем все сегменты с момента предыдущего вала Ну с предыдущего инкремента до текущего момента если валы лежат в архиве то как бы О'кей если они уже лежат в архиве сжатые зипо это вообще отлично Мы чего с ними не делаем мы просто их там копируем а вот ещё даже не копируем а ссылку там сделаем другой вопрос вот между двумя отсеками когда мы снимаем состояние вот между ними тогда транзакция успела начаться завершиться она как следующий инкремент попадёт Ну просто за счёт того что она существует Смотри Я я понял Да когда мы смотри мы инкремент восстанавливаем всегда последовательно То есть ты не можешь восстановить Там просто десятый инкремент Ты должен восстановить десятый инкремент Ты должен восстановить с первого по девятый А эта транзакция есть в девятом Просто инкремент Угу её под цеп в следующем инкремент по её как-то идентифика Ну просто мы Никаким образом не фильтруем то есть мы когда накатывает вот эту карту накатки берём только из последнего всё что все Мета информации промежуточных инкремента мы её игнорируем просто потому что по дефолту мы берём все транзакции которые воле лежат к этому моменту времени Спасибо это ещё ещё один вопрос топология кластера влияет на это то есть если у нас кольцо либо либо там с координатора как например не должно я не тестировал до но не должно Да но можно может влияет ну скажем наши клиенты просто ну пользуются кольцом в основном поэтому я на это не тестирую но кажется не должно быть никаких проблем оно же по кокей передаётся эта информацию Да да маркер передаётся по коммуникейшн когда мы отправляем по кольцу это ну через Discovery если там Discovery даёт гарантию доставки то должно быть всё ну не должно быть никаких проблем всё понятно спасибо Так а Всё Давайте закончим на этом дар Да а Выбирай лучший А есть код микрофон сейчас нет нет всё ага ну всё Выбирай лучший вопрос Ну прошу прощения не молодому человеку Константин Константина тебе подарок Макс Тебе спасибо Аплодисменты эти тоже Тебе спасибо большое"
}