{
  "video_id": "xLNvjlQVN3k",
  "channel": "HighLoadChannel",
  "title": "От CRM к DataLake с K8s и микросервисами / Андрей Вильмов (ПерилаГлавСнаб)",
  "views": 195,
  "duration": 2264,
  "published": "2024-10-29T03:00:35-07:00",
  "text": "вильмо Андрей из компании перила глав Снап Встречайте под Ваши аплодисменты спасибо спасибо за представление Ну что ж поехали Я думаю для начала я скажу что рано или поздно в компании наступает такой момент когда либо становится тесно внутри одного Монолита либо дальнейшая судьба компании зависит от непрерывного данных качественных данных и с этой проблемой столкнулись мы и вот Сегодня я расскажу и и покажу как вы на нашем примере сможете в принципе тоже распиливать свой Монолит если это необходимо либо строить далей для непрерывного потока данных нуно думаю для начала более подробно познакомимся Меня зовут вильмо в Андрей я в it работаю уже 10 лет часть из них в перило глав Snap и вот как раз последние 5 лет занимаюсь Дато инженери то есть ну По большей части распиливают на микросервисы и пилю наш dake для того чтобы компания могла развиваться дальше вообще Наша компания из названия Думаю будет понятно занимается в основном перилами то есть мы их производим продаём и устанавливаем Ну что ж начнём как и в любой хорошей истории стоит начать издалека и Пожалуй я тоже начну с самого начала изначально у нас когда-то была система хорошо работа там было не очень много бизнес-процессов К сожалению все бизнес-процессы были реализованы на триггерах в базе сразу небольшое отступление контекст моего доклада будет в рамках mssql Но если кто-то использует другую СУБД там пост грекул Все эти паттерны и подходы о которых я буду сегодня рассказывать они применимы и к любой другой извест мся так вот бы реализованы на триггерах Ну потому что архитектура м системы не позволяла сделать это иначе плюс у нас было определенное количество Excel отчётов для наших топов компании для того чтобы можно было смотреть на определённые метрики и принимать стратегические решения для развития компании эти отчёты формировались несложными SQL скриптами Ну запросами и всё работало база бы нагружена компания развива дитм открываем новые филиалы у нас появляются новые сотрудники мы начинаем автоматизировать ВС Что можно автоматизировать у нас увеличивается количество бизнес-процессов они становятся гораздо сложнее у нас увеличивается количество отчётов которые нужно компании потому что увеличивается и количество метрик которые мы начинаем мерить и сот э отно более сложнее запросы становятся ВС более сложными всё более нагруженные и в итогу мы приходим к такому к Такой страшной картине Когда у нас есть одна м система туда в принципе ходят все Наши сотрудники и в этой же CRM системе генерируются SQL запросы для Excel ОТВ эти запросы содержат в себе и подзапросы и оконные функции вообще они там стали уже уве многоэтажными и мы дошли до того что для выгрузки определённых отчётов для наших топов SQL скрипт мог отрабатывать там почти 2-3 часа и Это так или иначе сказывалось на работе нашей базы плюс на таблицах могло висеть очень большое количество триггеров там было на некоторых таблицах до десятка триггеров и они все тоже не улучшали работу нашей базы то есть там менеджер мог создать счёт и он будет ждать минуты так две пока эта транзакция произойдёт пока выполнятся все эти триггеры И после этого сможет отгрузить клиента и вот здесь вот я думаю уже понятно что что-то могло пойти не так а не так пошло то что к нам пришли блокировки и у нас началась паника надо что-то делать к нам приходит клиент он вот уже сидит Здесь менеджер ему должен отгрузить товар а он не может потому что возникает превышено Время ожидания на блокировку И ладно бы один клиент такой был ну подождём пока блокировки пройдут А их там целое много а их там много и нам нужно было что-то быстро сделать И вот чтобы что-то быстро сделать Мы решили разделить нашу базу на мастер слей на Мастере мы оставили всех тех кто активно работает с базой То есть это все Наши сотрудники триеры мы е переносили это чуть позже но всё остальное что грузило базу это в основном вот эти вот отчёты которые нам непрерывно ходили в базу практически каждые 5 минут и выгружали оттуда какие-то либо Нотис либо Excel отчёты либо ещё какие-то уведомления мы их перенесли на слей потому что они в принципе только читают базу Но появились внешние интеграции компания растёт Мы хотим автоматизировать какие-то процессы мы не хотим пилить свои решения Мы хотим использовать готовые и таких решений мы начинаем подключать со временем всё больше и больше и мы начинаем думать А куда всё это складывать как бы хорошо Можно в РМ систему складывать Но со временем она разра и в рамках одного железного сервера ей будет тяжко и мы начали смотреть А вообще как бы это по-хорошему можно сделать во-первых данных в одной базе будет Мы это нормально дальше слить не сможем нет отказа устойчивости у нас если этот сервер погаснет на котором всё это будет вращаться там у mss есть таск-менеджер который может по расписанию выполнять какие-то запросы свет отключится интернет пропадёт всё ничего не придёт ни отчёты ни данные в дашборды не Выру в общем очень плохо и нет возможности нормально расширять функционал Ну потому что мы опять же ограничены одной железкой здесь нужно что-то думать и нам на помощь приходит а airflow очень удобная штука которая позволяет менеджери задачи То есть у вас есть какие-то задачи которые должны выполняться по расписанию и вот airflow здесь Нам очень сильно помог Ну во-первых у него там есть очень много коннекторов Лич Зага ВД скажу что нам там очень в сво время помог SP отор там есть коннекторы к паре там очень просто дорабатывать те же самые операторы То есть это не очень сложный процесс плюс у него очень удобный интерфейс для того чтобы просматривать А какие вообще у нас задачи выполнились или нет их можно ВМ него Носов из архитектуры мы переехали в такую что здесь принципиально нового Ну во-первых мы развернули airflow он ходит по расписанию в наши интеграции внешней попе забирает эти данные и складывает но уже не ВМ систему А здесь будем говорить та аналитическая база данных пост потому что мы е поднимали просто для того чтобы выполнять там какие-то сложные запросы чтобы не грузить нашу CRM систему и не грузить наш слей А по итогу у нас airflow забирает данные по Репе по какому-то расписанию и складывает это уже в Паг А из pasg мы уже тянем либо в какие-то Excel отчёты либо в наши powerbi дашборды но опять же время идёт компания развивается мы захотели например на сайте вывести А где вообще сейчас едет наш курьер Ну Для клиента чтобы он мог это отслеживать соответственно появились определённые АТ устройство которое мы ей на машины повесили появились скуд устройства для того чтобы можно было проверять А когда сотрудник пришёл появились новые интеграции и данных становилось в общем очень много и наша постр очень быстро разрослась Там до 3 траб и продолжала расти плюс всё-таки постр это ЛТП база данных она предназначена для большого запроса несложных для большого количества несложных запросов А у нас как Мы помним многоэтажные запрос отрабатывают очень долго нужно было какое-то иное решение и мы его нашли в гнм вообще это класс таких параллельных вычислительных систем которые представляют из себя набор различных узлов где каждый узел представляет собой независимую единицу то есть на примере гнм если мы на него посмотрим под микроскопом то каждая нода это будет самостоятельно развёрнутая суд пост Но для нас для айтишников это будет кластер который управляется мастером вообще на рынке существует достаточно много таких систем это и кха и ветика но мы например остановились на Гримм потому что он может быть как ко orent Так и orent плюс если мы на тот же кликхаус посмотрим мы с одной строкой там всё-таки работать проблематично А в гнп мы можем либо удалить ту же строку одну либо её обновить нам это было важно потому что вот мы не очень хотели именно двхм кликхаус А поэтому мы остановились на greenplum плюс там очень удобный технология есть pxf который в дальнейшем нам поможет хранить исторические данные в Ю И в итоге мы уже заменили нашу пост гю на нпм и в принципе мы делаем то же самое только сохраняем уже все наши данные в наш нпм но как Мы помним у нас триггеры ВС равно ещ остались на Мастере как бы блокировок стало меньше Но менеджер всё равно вынужден ждать Там 3-4 минуты пока запись сохранится в базу это нужно что-то делать плюс у нас появились внешние интеграции которые работают через То есть у них нет пи Если чтото вдруг пот немы наме см дан Как только они отдают нам данные Мы в реалтай должны их как-то обрабатывать тоже самое с нашими АТ устройствами нам нужно было проводить какую-то аналитику в Реал тайме нам нужно была какая-то Технология И нам помогла кавка вообще тоже существует много брокеров сообщений это и т и нац но мы остановились на кафке потому что она простая там есть группы коню и их достаточно просто скели плюс сама кавка тоже достаточно просто скели да и по настройкам там тоже особо ничего сложного не было И для нас для не очень-то it компании это была прямо вот серебряная пуля Но вообще можно любые Я думаю брокера сообщения использовать плюс кавка неплохо дружит с Min через кавка connector и в итоге наша CRM си ой наша архитектура превратилась в такую что здесь принципиально нового помним что у нас есть триггеры то что они остались на Мастере Как нам помогла кавка у нас mssql сервер мы включили change dat Cap подключили к этому кавка Connect и теперь любая транзакция из нашей базы переезжает в каку а там наши коню отлавливают эти Новые сообщения эти транзакции и как-то их обрабатывают то есть мы просто перенесли все наши триггеры из mssql на realtime коню При этом если там мы говорим не про mssql А про другую суд то можно использовать deum там если зайти на официальную документацию то у них есть коннекторы в принципе к любой популярной опять же и у них даже к есть конектор номы через cdc пошли тем самым мы разгрузили нашу уже м систему То есть у нас вот эти вот триггеры все обрабатывались как-нибудь асинхронно потом то есть транзакция прошла и потом вот эти триггеры которые там с бизнес-процессами они там что-то делают И они отработают но у на появлятся микросервис вот эти вот рипере сервисы соответственно нам было страшно отключение например света или пропажа интернета э плюс нужно было как-то балансировать нагрузку потому что компания растёт количество данных и количество аналитики которые делается в realtime увеличивается и нужно это было как-то удобно скели а Плюс нужно было далее развивать эту систему Ну вот Ну да скелете повторюсь А и здесь нам приходят кубы в помощь Ну я думаю все да знают что такое кубы В общем это оркестратор докер докер контейнеров в чём их преимущество вернёмся к тому что у нас есть а он развёрнуты у нас на какой-нибудь железке Всё хорошо всё отрабатывает работает И тут вдруг свет гаснет гаснет он так это на день ни один пшк с этим не справляется и сервер гаснет и у нас не выгружается ни один отчёт ни одна интеграция с внешними сервисами не происходит плохо Мы оборачиваем airflow в кубы отдаём кубам три ноды и как бы всё ок одна погасла Ну и Бог бы с ней у меня ещё там несколько в кластере есть оно запустит и продолжит работать и в итоге наша система из такой приехала в такую то есть нам в принципе уже не падение Света не падение интернета не страшно потому что это менеджеры там кубы но данные могут теряться Я уже об этом говорил что у нас там есть некоторые сервисы которые возвращают данные через webhook То есть если мы допустили ошибку в расчётах нам бы по-хорошему взять историю и перевырити как-то её где-то хранить в какой-то промежуточной базе например Плюс нужно хранить историю топика в кафке То есть у нас сейчас есть гнм как мы переливаем данные из CR системы мы до определённого момента делали это очень топорно мы чистили таблицу в гринтом и вставляли это в таблицу в очень топорно так делать плохо и нехорошо не надо не повторяйте наших ошибок нужно было работать только затронуты данными то есть вот менеджер там наплодила какое-то количество транзакций создал новые записи изменил и вот с этим нужно было как-то работать А это Мы помним у нас хранится в кафке соответственно нужно где-то хранить историю этих топ потому что компания начала развиваться мы начали появ получать всё новые новые требования у нас там появилось распознавание первички распознавание лиц распознавание фотографий из чатбот то что вот водитель начал свой рабочий день и закончил свой рабочий день все эти данные нужно было где-то хранить данных было много плюс нужно было где-то хранить логи Ну и забегая вперёд нужно было где-то хранить наши модели и нам на помо пришло - это достаточно удобная веь наверное преимуществом основным преимуществом Почему мы выбрали именно ю было то что это простая система её развернуть И поддерживать достаточно просто и как бы компании повторюсь не очень-то И айтиш поддерживать было го проще дружит с гнп то есть мы через pxf все данные которые старше там двух лет отправляем на Ю и потом также можем их прочитать если это необходимо тот же ML Flow неплохо дружит с mino то есть мы все модельки через ML Flow храним у нас в Min э на Min можно сделать расшарить вольюм в контейнерах это тоже стало достаточно удобно Для нас плюс это делается достаточно просто В общем мы решили остановиться на Ю И в итоге мы из такой системы уже приехали в такую что здесь такого кардинально важного во-первых У нас есть два airflow первый отвечает за то чтобы сходить по реста по расписанию забрать сырые данные и S переложить их в ми то есть сырыми как получил так и сложил туда У нас есть кавка который работает нашей CRM системой куда приходят транзакции из базы и отправляют все эти Новые сообщения в realtime коню при этом все свои топики она сохраняет на mino и появляется второй airflow который уже отвечает за процесс то есть что он делает он ходит в мию забирает либо сырые данные либо историю топиков как-то это преобразовывает и сохраняет уже в ринм из гм мы уже можем с чистой душой забрать либо данные для Excel отчётов либо данные там для powerbi дашбордов И как я хочу это всё подытожить во-первых у нас получается такой конструктор который может решить практически любую поставленную перед нами зада есть возм тоже са когда данных было не очень много я вот не стал включать в презентацию мы использовали Мон то есть мы ходили по Репе забирали оттуда дженки они в принципе неплохо складировать в монго Ну потом у нас появились различные файлики там различные видео для обучения моделей и так далее и Мы перешли на ми То есть если вам не нуж обработка данных там какая-то аналитика или е чтото отсюда можно в принципе выкинуть каку если у вас данных не очень много то есть вот мы гнм разворачивали но будет работать Ладно я думаю все видят Грин мы гнм разворачивали только по той причине что мы не если у вас не этого самого промежуточного хранилища сырых данных то можно выкинуть один Ало который просто будет по забирать данные Таким образом мы получаем конструктор который в принципе может решить любую нашу боль Ну что ж на этом всё спасибо за внимание Я готов ответить на ваши вопросы большой за доклад на самом деле у меня сегодня на этом треке так получается что всё о чём рассказывают Я тем же самым занимаюсь и пока у нас не начался q&a я опять воспользуюсь своим правом ведущего и перехвачу маленький вопрос Почему у вас кавка не поехала в кубы Ну кавка сама по себе неплохо слится у нас поднят кворум у нас всегда в принципе если какой-то сервер отъе у нас будет большинство инстан ка в сети то есть она этот кластер не развалится Ладно справедливо так у кого есть вопросы поднимайте пожалуйста руку какие-то вопросы у нас Ага вижу прямо по центру Здравствуйте Меня зовут Дмитрий концепт разработка по вашему докладу Я так понял что вы ОПС разворачивали на собственном железе Угу но я смотрю ваша инфраструктура разрослась до довольно сложной это кубернетес и прочее а вы не рассматривали варианты использования облака оно довольно сильно снизило бы косты по эксплуатации хотя бы наверное повысил Да мы рассматривали вариант облако но денег у нас не так много нам было легче купить там сервер с дисками который будет обой нам грубо говоря в 300 либо у нас большое количество трафика между серверами идт и в облаках придется за это платить плюс за выделенный гигабитный канал тоже в месяц нужно будет доплачивать и как бы зачем вот в открытую Я сейчас не могу многое сказать но вот в кулуарах я вам обязательно расскажу то что почему мы не можем использовать облака я тоже считал Да я расскажу Спасибо за вопрос ещё есть у нас вопросы Вот первый ряд пока секунду я напомню ребят если у вас нет вопросов У вас всё равно есть что сделать пожалуйста проголосуйте за доклад Оцените его это очень нужно нам для конференции и те кто смотрит нас онлайн прошу задавать вопрос в чат зала тоже если будет вопросы Всё зачитаю спасибо я правильно понимаю что гнп вы развернули для того что там просто очень много данных вы не ратри нной обработки для аналитики потому что мы вот аналогичные проблемы решали не там большой молотилка типа Грин плана а построением распределён распределённой обработки и анализа Нет мы такое не рассматривали нам было во-первых проще поднять какой-то СУБД чем раден ВОВ гораздо проще настроить права доступа И поддерживать Это там одному человеку потому что опять же повторюсь у нас не очень айтишная компания Да со временем если данных будет становиться больше Мы скорее всего не сможем бесконечно докупать железо проблема масштабирования Да всегда актуальна ребят вопросы вопрос Ну да давай Вы рядом Добрый день такой вопрос У вас очень сильно разрослась в принципе по масштабу система какие-то у вас задержки же по идее выросли вы как-то это отслеживали то есть ну какое соотношение получилось к объёму данных на скорость данных задержки Что ты имеешь в виду н какой-то взаимодействи между сервисами Да насколько оно увеличилось вот могу сказать про как раз триггеры основную ту проблему которую мы решали то что изначально там менеджеры могли минуту ждать да у нас задержка есть потому что пока это дойдёт до пока cdc отработает пока кака Connect отработает пока кавка пока Python скрипты отработают и у нас самая большая задержка была в 31 секунду это когда к нам приходили все данные с т устройств и с CRM системы вот когда было всё нагружена А так у нас до 2 секунд нагрузка Ну задержка поближе просто чуть-чуть поближе то есть формально для вас это ну не такие большие задержки в рамках вашей оци потому что как бы он ожидает это в течение 4 минут создания счёта или счёт создаётся и через Максимум 2 секунды прилетает какое-то обновление Там сумма счёта обновляется и второй момент вы говорите у вас несколько нуо потоков ка поднято как вы перекидывает ну на Итак какие функции распределения вы используете То есть вы сами что-то писали Либо вы что-то используете внешнее чтобы перекидывать Ну вот у вас один лёг Да и вы тогда будете на два других например перекидывать если у вас три инстанса поднята не у нас просто мы мы сразу на всю отправляем А там уже внутри как раз наши консьюмер там есть группа группа этих консмед еся и соответственно если один занят то из этой группы получит Второй второй занят получит третий Ну соответственно так далее плюс там можно в самой кафке настраивать количество репликации и сейчас вот не вспомню можно да ну количество репликации мы что хотим избежать потери данных или именно как логика должна быть как логика Я понял хорошо Ну мы просто всей Кафки отдаём а там она уже сама этим рулит хорошо да ещё вопросы так я вижу Ну давай с той стороны сходим для для баланса между читами зал а потом вернёмся к вам хорошо Здравствуйте спасибо за доклад Вопрос такой у вас Судя по вот финальной ахи несколько хранилищ получилось которых там ну либо дублируется либо те же данные хранятся там чуть-чуть видоизменёнными вот вопрос такой как обеспечивается консистентность данных получается во всех трёх хранилищах потому что Ну на каждом из этапов может произойти какое-то сбой не отработать и в итоге там в Грин ПМЕ в финальном у вас чего-то не окажется а там консьюмер быстро работающий вместо триггера уже отработал очень хороший вопрос вокруг этого у нас много много логики вращается я пожалуй расскажу то что как бы могу рассказать во-первых У нас есть отдельные скрипты которые отвечают за то чтобы э везде у нас были те данные которые там необходимы во-вторых у нас э есть storage этот ris который в котором мы просто обновляем куда Какие данные приехали и приехали ли они вообще эта часть не для самых важных данных но она там есть и соответственно есть история тех данных которые были загружены если они соответственно не загружены то История это просто не обновится это если очень поверхностно там вокруг этого вращаются в основном наши наработки то есть получается как вручную вы проверяете да Нет скриптами скриптами Не дай Боже вручную это проверять Ну то есть просто сравниваешь что было на входе потом идёшь смотришь Сколько было на выходе грубо говоря да то есть у нас Мы Либо так либо у нас есть истории то есть мы видим Вот Сходили на мию взяли топик и проверили то что вот этот топик например был загружен загружен всё О'кей мы грубо говоря галочку напротив этого поставили что он загружен и загружен полностью Да по сути это ещё дополнительная операция на чтение это отдельно делается Мы загружаем и потом просто по ночам проверяем не приехали у нас нам на дашборда не нужны лта информацию У нас всегда идут данные задержкой в один день я понял А если я добью вопрос А если в ночью проверили и потом поняли что что-то не доехало Что делается оно обновит те данные которые не доехали понял я ваш вопрос ещё чуть-чуть развил если есть какие дополнительные комментарии есть есть какой-то комментарий ко есть с учётом того что сказали что у вас ещё и K value хранилище там есть а ну не думали ли вы как бы сделать следующую там реинкарнацию архитектуры и как-то оптимизировать количество хранилищ чтобы вот до доп накладные расходы условно в виде там проверки всех хранилищ скриптами сократить да думаем это будет следующий этап Потому что если говорим о уже о развитии этой системы Нам сейчас нужно нормально оптимизировать тот же потому что количество моделей увеличилось нам нужно нормально сделать логирование потому что сейчас это в принципе в ёлку всё складывается но это не всегда удобно и плюс не у всех скриптов ещё настроено там Open телеметрии Ну кто знает про отслеживание взаимодействий микросервисов вот закончим это перейдём к следующему этапу у нас уже готов вопрос С левой части Здравствуйте спасибо за доклад было интересно Скажите пожалуйста сколько у вас людей в команде И сколько занял вот этот переход от одного msq сервера до текущей архитектуры Если вы говорите про людей которые занимались вот этим два человека а сколько по времени полтора месяца если брать последние наработки Если ещё брать то что мы там ф пытались кое-как сейчас развернуть то 2 месяца Моё почтение я Я согласен Да это просто какое-то нереальное Аплодисменты в тему Вижу вижу А я потом вам если что микрофон передам Да потому что у нас тогда мы не попадаем на запись не то чтобы я не хотел давать Вам слово у нас есть вопрос Спасибо за доклад хотел уточнить а вот у вас достаточно как я понимаю такая развёрнутая аналитика проблем с нси Да вот нормативно-справочной информаци то есть вот не планировали вы там введение какой-нибудь МДМ системки вот для того чтобы там управлять более качественной это информации confluence у нас есть confluence если вы про Вики систему Где ведётся вся документация не Вики вот справочники То есть например там в разрезе которых вы там формируете отчёты То есть если например входные данные надо каким-то образом обогатить или вести какие-то аналитические статьи Вот они Где отдельно в эльке ведутся или Пока к сожалению да до этого этапа мы не Добрались хорошо вижу вопрос по второму кругу пошли но у нас время есть Поэтому я думаю можно первый ряд Спасибо вы говорили что вы начинали с репликации всех данных а перешли только на изменения А вот в продолжении этой темы у вас была сегментация ну самое простое закрытие периодов и так далее Нет не было это Оска этим страдает Да я понимаю но чтобы поддерживать там то что называется непрерывность и целостность можно вот закрывать данные иметь гарантии что они уже здесь к сожалению бизнес-процессами у нас выстроены немного не так у нас если закрывать какие-то данные у нас всё равно менеджеры могут к чему-то старому обратиться вполне А а как вы быстрый доступ к этим данным отдаёте Ну как будто бы невозможно Нет ну все те данные которые лежат в CRM системе если не брать данные из внешних интеграций то сама CRM система не очень большая Там она в районе полу тбт и в принципе э РМ система эти данные проглатывает и ещё может как-то просто бывают такие моменты когда в кулуарах Да Вопрос Да есть такое в рамках нашей наших реалий К сожалению это нормально То есть вы можете поменять данные через 2 года да менеджер ошибся поставил другую цену мы потом когда видим это или бы наоборот там не проста себестоимость когда выгружаем аналитику видим себестоимость в этом счёте не проставлена обновить назад Вот ВМ системе нет Потому что когда только всё это развивалось там на пятилетних давност то там вот может не быть той же самой закупочной цены из-за того что превышено время на блокировку то есть там какой-то Триггер не отрабатывал либо транзакции ла и не представлялось нужное нам значение соответственно мы можем изменять старые значения чтобы в дш бордах это сходилось я прошу прощения Давайте тогда в кулуары эту дискуссию перенесём потому что у нас нет микрофона и у нас нету трансляции в запись А ещё вопросы так раз вопрос это последний да Ну наверно если у кого-то появится поднимайте руку Время ещё есть Добрый день Скажите пожалуйста у вас большая часть бизнес логики была организована на триггерах вы перенесли её в докер контейнеры за полтора месяца как отличный финальный вопрос мне кажется я вот боюсь отвечать это вот под запись работал много эно руда Я думаю вот такой ответ какие-то нюансы может потом лично будет уточнить с Что такое ваш докер сейчас вопрос таким образом был это три железных сервера на которым Крутится по пот так а можно можно вернуть в микрофон Да чтобы мы договорили ээ Спасибо А какое приложение э в докер вы упаковали ну то есть какие технологии там использовали а там там разные там и вот эти вот триггеры которые были перенесены на Python скрипты там есть интеграции с с ватсапом на Гонг написанные там есть интеграции с телефонией на плюсах написанное там в общем много всего разного Спасибо есть ли у нас ещё вопросы как будто бы нет Андрей тогда самое приятная часть выбрать самый лучший вопрос их было очень много и мне кажется очень много из них мне прямо лично самому понравились мне очень понравился Про согласованность данных вот мне фонарь мешает Я тоже не вижу лицо человека но я помню этот вопрос Да мы выбрали лучший вопрос вон там Спасибо тебе ещё раз большое за доклад Ребята пожалуйста голосуйте и мы дарим тебе тоже подарок от нашей конференции Спасибо ещё раз спасибо"
}