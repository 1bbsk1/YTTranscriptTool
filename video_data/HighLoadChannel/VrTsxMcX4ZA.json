{
  "video_id": "VrTsxMcX4ZA",
  "channel": "HighLoadChannel",
  "title": "Кластер ClickHouse ctrl-с ctrl-v / Никита Михайлов (Яндекс)",
  "views": 2918,
  "duration": 2371,
  "published": "2021-10-04T02:14:56-07:00",
  "text": "всем привет меня зовут никита я разрабатываю клика us яндексе мой доклад называется клика узкого остро contraption control v и речь в нём пойдёт о том как сделать копию данных вашего с вашего кластера используя для этого примерно не знаю две клавиши сначала давайте поговорим про мотивацию данного доклада в яндексе есть такой отдел яндекс метрика и она использовать ли house как основной сторож для своих данных и вот там был кластер ну в яндекс метрике много кластеров а вот один из них был расположены более чем 500 серверах на этом кластере хранилось таблица визит там хранились сессии пользователей это примерно 10 петабайт данных сжатом виде и 100 петабайт данных несжатом но проблема в том что этот кластер не в облаке он железный и железо разное это не очень удобно когда не знаю у вас разные диски разных объем на каждый но иди кластера поэтому кластер неэффективен было принято решение заменить данный кластер на замене железно данном классе или на более новые снизить его количество но как-то унифицировать эти но до между собой но для этого нужно перенести все данные их у нас немало прям так скажем вот значит моем докладе мы просто пройдем путь как это может как можно придумать утилиту которая копирует кластера сначала мы посмотрим как можно перенести данные сервера с одной машины на другую просто руками еще потом посмотрим как это можно сделать использую стройную функциональность мы подходим прайс quelle и виновница торжества эта утилита click mouse copper ради которой этот доклад был создан так но смотрите вы стартовали сиропе хауса вы должны ему подать на вход конфиг и директории в котором будет хранить ваши данные которые вы не оставите там на самом деле очень много подпапок в этой директории нас интересует 3 это да это метода это и спор давайте посмотрим деталь не что они значит вот ну создадим базу с именем хай-лоу 2021 в этом нет ничего интересного потом зайдем в папку методы и ты увидим там два новых файла харлоу 2021 и хайло 2021 . сквере ну по поводу 2 файла ну по расширению видно что это обычный обычный текстовый файл а если на него посмотреть то это просто искали запрос которым была создана эта база данных файлов а второй файл это 7 link в папочку stor ну пока не очень понятно пойдем дальше создадим в нашей базе таблицу это простая таблица с одной строковой колонкой и теперь вот этот файл рот 2021 это на самом деле директория и в нем появился новый файл партизан . из куина это тоже просто метаданные нашей таблице а это искали запрос которым она была создана теперь мы можем вставить данные в нашей таблицу и посмотреть куда же сохранились наши данные вот но наши данные сохранились в папочку да и то потом имя базы данных имя таблицы мы посмотрим детально этот файл и что это это опять 7 link в папку stor какой можно сделать а если мы пройдем по этому simulink у то мы уже увидим привычную директорию для семейства мечты таблиц что из этого слайда можно вынести на то что калле хауса действительно столбцов и я база данных потому что наша колонка да это она здесь представлен двумя файлами да . бен и да это . и марка 3 вот но какой вывод из этого всего можно получить мы для копирования таблицы с одного сервера на другой мы просто можем скопировать все файлы из этих трех папок с одного сервера на другой по-хорошему еще должны сделать им линки но это мы можем уже сделать руками на сервере после этого можно просто перезапустить сервер бей тогда у нас уже оказывается новая табличка серую этом прочитай а с диска и про инициализируется в подводные камни есть у данного подхода но тут только некоторые из них от которые я смог вспомнить вот например ваш сервер он может быть сконфигурирован по-разному например это может присутствовать multibio уволим сторож это такая функциональность которая позволяет вам хранить данные не на одном диске она нескольких и настраивать там какие-то сценарии перемещения данных или партиций с одного диска на другой вот в этом случае вам придется уже полазать по всем этим диском выудить оттуда файлы и перенести на сервер b выглядит не сложно но проблема могут быть проблемы могут быть из-за того что конфигурации серверов могут отличаться и причем отличаться сильно 2 подводные камни то что версии самого сервер могут сильно отличаться в данном примере мы посмотрели как тихо он хранит данные таблицы в базе данных atomic и и она была включена с версии 23 если у вас например какой нибудь старая версия например 1911 на другом сервере то там уже структуру папа будет отличаться и вам будет сложнее раскладывать файлы по папкам ну и поскольку при хаос это такая продвинутая база данных у нас есть реплицировали сторож из коробки он использует сервис распределенный координации звуки пир для того чтобы в нем хранить всякие метаданные о том что заданный в нем хранятся там на какие куски они разбиты где эти куски лежат вот и понятное дело то что если вы хотите скопировать эту цепную таблицу помимо это данных и данных на самом диске каждой реплике вы ещё должны принести сложную структуру метаданных и пера тоже не тривиальная задача если делать ее руками в целом мы поняли что она есть очень много подводных камней в целом делает так сноу скорее не стоит перед тем как перейти к следующему как бы пункту давайте посмотрим на движок 10 минут вот я уже на прошлом слайде упоминал что есть реплицировали сторож из коробки это например и сторож или пике этот мир 3 но как бы когда вы храните данные в этом старриджа то у вас объем ваши восторженно самом деле не увеличивается то есть он просто становится там отказа устойчивым но больше данных вы за этого хранить там не можете что делать в случае когда вам уже место на репликах не хватает ну вы должны сортировать ваши данные и для этого treehouse предусмотрена вот такая функциональность как таблицы движком дистрибьютор вот она позволяет вам масштабировать источником пьют вашего plaster а почему стоишь это понятно потому что вы добавляете новых серверов компьют потому что при запросу всякие тежело вычисления они частично выполняются на удаленных серверах а потом агрегируются на сервере не центре можно посмотреть на картинку как примерно происходит чтение здесь ребята таблицы вот у нас тут есть два шара да тут цилиндры это одна реплика реплики этот нож 3 и вот у нас есть таблицы distributed она бы расположен на каком-то сервере я этот сервер этим сервером может выступать ваш ноутбук к примеру вот их примеру вы хотите прочитать ее данные что делает таблица она переписывать запрос заменяя в нем свое имя на имя таблица на которой она смотрит и после этого рассылается запросов шарды шарды уже выполняет частично агрегацию данных возможно еще частичную сортировку я даже пересылает запрос обратно на табличку дистрибьютор сама таблица дистрибьютора на данные не хранит она представляет представляет собой view над картами вот вставкой дистрибьютор лицу на по умолчанию синхронное что это значит значит при вставке в 10 минут таблицу данные сначала откладываются на локальный старик а потом асинхронно доезжают во всех орды вот как вообще там создается истребит таблицам давайте для начала создадим лепить едет таблицу на каждом узле нашего кластера и что то можно заметить что то есть вот модификатор у запрос он кластер по нему мы передаем кластеры из конфига и так же у нас есть макросы shard и реплика эти макросы они должны быть определены в конфиге каждого сервера и когда вот такой запрос прилетает на какой-то сервер то эти макросы раскрываются и у вас получается правильное название таблицы и пусть там звуки перри вот мы создали вот у нас есть кластер мы на нем создали на каждой на каждом узле реплей таблицы теперь давайте создадим дистрибьютор лицу тоже на каждом узле зачем это нужно ну для того чтобы можно было обращаться в любой узел нашего кластера иметь доступ ко всем данным нашего кластера вот как это делается просто ем дистрибьютор лицу и говорим что у нас должно быть структура такая же как у таблицы репликейт от дистрибьютор лица на вот в свои параметры принимает имя кластера и там имя таблицы и ключ родирование последний аргумент вот вот так примерно это выглядит тут не хватает стрелочек ну просто для того чтобы не перегружать слайд каждая желтая каждый желтый цилиндр этот дистрибьютор лица рядом с ним rip лететь от таблицы вот теперь давайте вернемся к нашей задаче нам нужно скопировать данные из одного места в другое ну предположим нас просто есть сервер а есть сервер б там какая-то таблица пока не репликейт от и мы хотим скопировать эти данные что мы делаем мы должны сначала создать таблицу на сервере б как мы можем это сделать мы можем в конфиге сервера а указать новый кластеры могут сервер это сервер это кластер который состоит из всего лишь 1 на дэ нашего сервера б вот и вот таким запросам мы получается создаем таблицу на уже другом сервере с такой же структура как и у нас эти оба запрос выполняется на сервере а и после этого мы и должны сделать запрос insert select в мы selecting из нашей соус таблички и вставляем на другой сервер в destination табличку что при этом происходит тут вот видно табличных функций ремоут что эта вещь делает она создает неявный дистрибьютор близко котором мы говорили двумя слайдами ранее и запрос уже выполняется над ней а если он select он просто создает 2 пайпа не вычислений для не сердца для селекции склеивает их там программно по итогу у нас получается то что данные передаются между серверами в найти формате и это достаточно эффективно вот но нам хотелось бы копировать целый кластер а не отдельные маленькие таблички вот у нас есть там source кластер с какими-то надо my destination кластере мы да вот должны перенести данные на 8 класс сторона другой как мы можем это сделать но мы должны воткнуть между ними дистрибьютор лицу тут на слайде небольшая неточность вот желтеньким это на самом деле сервер и вот в этом случае нам на самом деле потребуется 2 дистрибьютор лица 1 который смотрит на source кластера другая который смотрит на 10 нынешних мастер и это создано к вaм другом сервере и потом что мы должны сделать мы должны сделать просто нехер select из одной дистрибьютор блиц и другую дистрибьютором лицу и тогда у нас будет данные читаться с соус кластера и переливаться в destination кластер также можем тут заметить то что если мы создали distributed таблицу на каждой ноге каждого кластера вот то нам совершенно не нужен посредник можем просто выбрать какую-то случайную на душ из одного кластера случайно из другого и что у нас получается у нас получается уже знакома нам задачу нас есть два сервера и там есть таблицы надо перелить данный из одной таблицы в другую что мы должны для этого сделать но мы опять должны сделать enter select в ремонт тут в ремонт указывается адрес сервера на который адрес сервера где расположены дистрибьютор лиц и которые смотрят на destination кластера вот вроде бы все хорошо но в этом подходе есть некоторые проблемы первая проблема что если сеть моргнет и передача данных ну тогда у нас проблемы потому что мы на самом деле не понимаем какую часть данных лекарей лилии какая часть данных реально оказалось destination кластере мы не можем как бы начать с того же там с того же места и продолжить наши скачивание и копирование таким способом оного производится как бы в один поток то есть это не очень эффективно хорошо бы это как-то раз пролились прежде чем переходить следующему к следующему слайду давайте рассмотрим единицы измерения данных ли хаусе как пользователь вы должны иметь дело только с последними пятью измерениями то есть кусок это минимальная единица хранения данных на диске кусок этот отсортированный набор данных и поскольку на старте ровные мы можем эффективно по нему искать то есть делать себе запросы но проблема в том что если ваш select запрос затрагивать данные из множество кусков то мы должны каждый этот кусок потрогать и это может быть не очень эффективно поэтому нужно эти куски сливать воедино и поэтому в мер 3 таблицах есть такой механизм как механизм фоновых слияние этих кусков далее куски они уже логически образуют партиции партиции образуют таблицы важно заметить то что если например вашей таблице нет ключа проти цианирования то все данные таблицы они хранятся в одной партиции с именем пол вот и дальше у нас там есть шар ты кластер вот у нас на привет идут на привет предыдущем слайде было несколько проблем одна из них это то что у нас могут быть ошибки при передаче данных и то что нам нужно как-то раз проверить но давайте запустим отдельные сначала будем раз проливать давайте допустим отдельный сюр select для каждой партиции в исходной таблице если получаем ошибку то мы просто удаляем то что мы скопировали партийцы в destination таблицы и начинаем этот процесс заново вот и вот эти два шага не на самом деле выглядит как алгоритм поэтому этот процесс можно автоматизировать если мы это автоматизируем напишем код то у нас в итоге получится утилита crack house paper это утилита которую входит стандартную поставку crack house ну всем известно что калле хаус это один binary что позволяет это утилита сделать на позволяет скопировать данные таблицы с одного кластера на другой причем сделать это надежно распределены все как мы любим при этом поскольку у нас там используется дистрибьютор блицу мы можем пересортировать пласт что это значит что если например у вас был кластер из трех шагов вы можете его перелить в кластер из десяти шагов или и скольки угодно или например из одного шарда при этом там есть несколько полезных бонусов вы можете поменять движок вашей таблице вы можете как-то поменять структуру вашей таблице например если у вас были колонки числовые с типом и uint8 вы можете заменить там на n16 если был string то можете заменить на науку кардинале те string и всякое такое полезная фича это то что кликал из копий умеет использовал работать параллельно при этом копались вы должны просто запускать crack house copper все остальное он сделать сам клика узко пир конфигурируем с двумя файлами первое это кликал эта конфигурация самого процесса копира что нужно указать в этой конфигурации можно указать класс сразу кипер с которым он будет взаимодействовать а также конфигурацию лагере для того чтобы он писал какие то смысле налоги складывал их файлы и любые дополнительно стройки которые вам нужны дальше есть конфигурация самой задачи город задача это описание того какую то бдительно копировать такого кластера на какой надо копировать вот вы там должны указать также движок здесь конечно таблицы для чего это нужно если у вас здесь навечно таблица не создана to crack house paper попытается ее создать сам при этом как он будет это делать он возьмет посмотрит на колонки которые есть в соус таблицы возьмет их определения и припишет к ним движок из-за конфигурации но таким образом вы можете поменять движок и это еще нужно для внутренних нужд копир также вы можете задать там новых лишь родирование и условия для фильтрации данных для для чтения если вы хотите там как-то чтобы copper копировал не все данные только их часть как это примерно выглядит вот у нас есть два кластера source и destination по 40 в каждом кластере при этом ну количество нот каждый класс ты можешь отвечать потому что мы можем их пересортировать процесс копируем может запуститься на любой ноге из этого кластера и даже на на день и из этого кластера все что ему нужно это сказать каким за кибер он будет взаимодействовать и откуда что копировать запуск слегка узко пир выглядит примерно так что тут нужно ответить это то что в третью строчку нас передается конфиг самого копира и дальше передается пусть перри по которому должна лежать задача для копируем текущей таблице дальше две строчки достаточно интересно это то что если мы не хотим с вами взаимодействие как suzuki пир можешь просто указать файл на вокальном створ g и попросить copper самого загрузить этот файл в руки пир для того чтобы потом возможно другие процессы его оттуда прочитали вот но и так же мы должны указать директорию который copper будет сохранить данные это нужно потому что crack house копёр будет создавать дистрибьютор блице около себя сложности при эксплуатации crack house опера могут быть некоторые проблемы например то что слегка ускорь предполагает что партийцы которым копируют в таблице они не меняются в процессе копирования вот что это значит это значит что в эти в эти партийцы вы уже не можете писать и также что схем и таблиц сохраняется ну вот обойти вторую проблему достаточно просто надо просто менять схему таблиц можно как-то потерпеть а с первым ну возможно как-то тяжело но есть выход из этого вот если зачастую протеане рование таблицы происходит поместится но либо какой-то дате и если мы все время пишем в последнюю партицию затрагивает только одну то предыдущий партиции например предыдущий месяц можно копировать без проблем и при этом данные текущей партиции можно писать уже в оба кластера то есть у нас одновременно копируются данные с одного класса в другой мы пишем сразу в два кластера и по окончании месяца у нас получается так что в destination кластере у нас есть данные за текущий месяц и за все предыдущие поэтому мы можем уже как-то атомарный приключиться наш ихлас старое уже там разбирать кластеры сходный давайте уже теперь детально рассмотрим алгоритм как он работает для начала чтобы понять что нужно чтобы что-то скопировать надо сначала понять какие партийцы нам нужно копировать как это сделать ну давайте для каждого шар до исходной таблице мы создадим локально дистрибьютор таблицу и сделаем примерно вот такой запрос что он делает он выбирает те партиции он просто выбирает все партии цена конкретном шарди таким образом мы для каждого шарда понимаем какие партиции там содержится пользователь может явно сам задать партиции которые нужно копировать ну и это делается с помощью и найду портишь нас конфигурация задача сам процесс копирования он начинается того что нужно создать истребит таблицу и как это делается мы должны сначала сделать понять мы сначала должны найти кредит запрос которым была создана исходная таблиц для этого мы делаем запрос шоу клей pva просто на соус кластер после этого мы должны это запрос модифицировать а именно мы должны просто взять и поменять движок таблицы в этом запросе вот и на всякий случай добавить их но ты здесь потому что полюсе сам мог определить схему destination таблицы и просто сам создать я вот то есть мы сейчас на такой стати мы сделали запрос в каждый шар и поняли какие parties и там находится при этом однопартиец она может быть в более чем одном шарди потому что снова нет каких-либо ограничений на ключ против санирование вот но мы обработаем всякие такие случаи и теперь мы просто должны пролетарий раз по каждой партиции и начать копировать ее копировать мы будем как опять же запросом insert select вот примерно такого вида с условием что у нас на что выбираем данные только из нужные нам партиции при этом соус таблицы она смотрит на конкретный шарф или на множество сортов destination таблица она сможет уже на весь destination кластер вот в этом примере у действительно таблицы достаточно blitz она смотрит на весь кластер и вот и таблице можно задачку лишь родирование таким образом вы можете пересортировать ваш кластер но тут у нас опять подстерегают проблемы потому что копирование по отдельным портится мне всегда хорошо работает почему потому что партиции могут быть очень большого размера например на костре яндекс метрики 1 портится занимает может занимать сотни гигабайт и копируем такого объема данных в один поток она не неизбежно обернется какой-либо ошибкой вот и исходя из нашего алгоритма и записать наши алгоритмы мы что должны сделать случае ошибки мы должны просто удалить все данные которые мы скопировали вот если у вас настроен какой-либо мониторинг то вы будете видеть примерно вот такой график это график потребление диска на узле и теснейшим кластер куда мы копируем то есть что здесь видно что сначала у нас идет копирование вот а после этого резкое падение то что значит это значит сумма удаляем партицию вот мы вообще такой процессу может продолжают бесконечно долго нам нужно найти решение давайте у нас будет копироваться не целый портится и хао отдельной части будем называть их кусочками и как уже было сегодня сказано у разработчик ли хауса реально проблемы с тем чтобы подобрать синоним к слову кусок давайте подумаем что это может быть мог ли бы могут ли бы это быть порты часть партиции на самом деле нет потому что как уже было сказано в мер 3 есть процесс фоново слияний и множество этих портов она может меняться то есть какие-то партии порты могут удаляться какие какие-то сливаться воедино и part сам по себе может быть достаточно большого размера то есть например тоже 100 гигабайт и копируем такого объема данных тоже будет с ошибкой и поэтому получили ту же самую проблему какое решение можно придумать как поделить партию на примерно равные части но мы должны просто посчитать хэш от каждой строчке нашей партиции и взять только то множество строчек нашей партиции у которых хэш дает который одинаково остаток от деления хэша этой строчке но для ускорение мы можем посчитать не крыша с . хэш только первичного ключа и тогда наш select запрос запрос помощью которую мы читаем данные он будет примерно вот таким альтернативы данному методу это что у в качестве атразин можно рассматривать такой подход у таблице можно указать ключ сэмплирование это писали функциональность для того чтобы делать запросы только по части данных и мы можем и воспользуется задавая запрос вот такого вида что тут происходит мы двигаемся окном размера размер партиции поделенное на количество кусков который мы хотим поделите вот мы двигаемся таким куском и каждый отдельный вот кусок этого часть портится которому будем копировать независимо ну и понятное дело то что пользователь может задать количество частей для разбиения в конфигурации задач опять же тут у нас могут быть сложности дело в том что части могут копироваться параллельно вот но при этом никто нас не застрахован от ошибок и вот например что делать если 9 из 10 частей они скопировать успешно а при копировании последней части у нас вылетела ошибка удалять весь объем скопирована данных очень не хочется вот ну какое у нас может быть решение в случае неудачи можем запустить отдельный запрос вида alter ты был дэвид что он делает он должен удалить из наших данных строчки которые подходят под условия то есть мы должны удалить строчки которые на самом деле из последнего кусочка но такой запрос он будет работать очень медленно поскольку нас кусок в клика усе это имут обильная вещь то есть и для того чтобы удалить данные из куска что нужно сделать надо просто перенести из старого куска данные которые не подлежат удалению в новый то есть нас будет одновременно два куска и то есть это не очень эффективно давайте рассмотрим текущие решения отдельные части партийцы мы будем копировать не напрямую в таблицу которую было указано в конфиге как destination мы будем копировать их в небольшие вспомогательные таблички рядом с этой таблицей к примеру если вот мы например хотим разделить нашу партию на десять частей то мы садим 10 магазинов таблиц и то есть и ты кусочек у которого остаток от деления на 10 равен и мы будем копировать в эту таблицу вот так это примерно выглядит то есть желтый тут показано исходная табличка destination около нее мы создали пять маленьких табличек и вот отдельный процесс копира он копирует данные уже в эту маленькую табличку что может тут заметить ну можно еще немножко подумать и понять что вот у этих табличек должно быть и практически идентичная структура как и у destination таблицы но за исключением одного факта эти маленькие таблички они могут уже быть не репетировали потому что в этом месте реплицировать но нет смысла вот после того как мы скопировали все наши кусочки партиции по нашим табличкам что мы должны сделать мы должны как-то объединить наши данные воедино делать мы будем это с помощью этого запроса такой запрос нужно сделать для каждой вспомогательные таблицы и что то можно заметить то что в вспомогательных таблицах у нас портится она имеет тот же тоже имя что и destination таблице они просто открыл отличаются данными как я говорил cliff house используется теперь для синхронизации вот например какие проблемы с помощью него решаются по перу с помощью за кибер можно ограничивать количество одновременно работающих процессов копируем мы можем делать взаимно исключения для того чтобы у нас не было рейс кондишен на копирование каждого кусочка партиции также можем хранить результаты для каждого кусочка тоже закипеть и статуса выполнения тоже для каждой портится для того чтобы можно было the real-time мониторить тут стоит отметить еще нашу замечательную инфраструктуру seo без которой тестирование подобных утилит был бы невозможно кластер crack house у нас поднимается в контейнерах после этого процесса копирование запускаются в произвольных контейнерах из этого им там скармливается какая-то задача и после выполнения проверяют хитрые варианты что еще можно отметить то что копирует тестируется с фолд injections это очень дешевый способ для того чтобы убедиться в откат устойчивость вашей программы например в тестов нас сдается вероятность того то что в какой-то случайный момент времени вас упадет либо копирование у нас будет ошибкой либо на копирование данных либо на объединение этих данных вот и и тестирование происходит нашим seo и постоянно как итог мы получили утилиту это отказываться очень инструмент который позволяет копировать реально большие объемы данных и как мы видим он очень просто настройки вам нужно всего лишь по факту указать 2 конфига и это реально вот настроил и забыл на несколько там недель вот и донатили это было не раз проверено в бою и с помощью неё был скопирован ни один класс яндекс метрики класс троян из метрики все спасибо не кидай большое спасибо за доклад было очень интересно и познавательно теперь мы все с вами знаем как эффективно переливать данные из вас никакой mix 1 но давай игрушечной инсталляция по-настоящему большие кластера время ваших вопросов мы напоминаем что мы принимаем вопросы как из онлайна так и из аудитории автор лучшего вопроса во мнении гиды получат памятный презент не знаю оставьте так меня слышно да такой вопрос после того как house copper отработал стоит ли заморачиваться и выполнять проверку качестве данных то есть так опирался на место другое кликов скопирует работал задача выполнена на долива лидировать что данные действительно консистентной или это гарантированно это не гарантирована это не проверяется потому что у вас может быть сколь угодно сложные задачи для копирования вы можете сами проверить но зачастую это будет просто не нужно нокий понял спасибо и еще мне такой маленький вопрос когда бабочка к этому у нас был опыт использование копира и когда мы его попробовали этом задача было несложно и 2 терабайта данных перелить с одной конфигурации кластера на такую же только менял и железа более мощные там больше диски больше памяти и копируй отрабатывал ну очень медленно но когда мы попробовали использовать вот этот вариант на коленке insert селектором ремоут то все стало достаточно быстро то есть ли какая-то разница между перфомансом копируя и вот варианты на коленке когда ты все организовываешь sun ну разница конечно есть потому что копируя есть и overhead на использование зуки пера и он делает еще достаточно много дополнительных запросов чтобы выяснить структуру вашего кластера чтобы понять какие партиции где лежат чтобы все это не знаю отметить как-то в зуке перри но до вверх от конечно есть и ну копируем был создан для того чтобы его просто настроить и забыть то есть они как-то мониторить этот процесс а просто вот оставить он как как-то сам копирует ну вот поэтому есть играет понял спасибо большое здрасте да скажите а как сильно грузит кластер клика уса процесс копирования возможно ли как-то лимитировать процесс скопировать чтобы наоборот не быстро скопировать а надежно но не повредив и работе основа сервера может он и так высоко нагруженный но опыт показывает то что копирование вот способом разбиения на куски и выделение каких-то кусочков похожу это очень сильно грузит диск вот как-то ограничить ну внутренними средствами treehouse а то есть вы наверное можете делать с ты из какого из-под другого пользователя для него сделать какие-то constraint и может лишь как то получит ведь сейчас стеклышко получается что в данном случае что несмотря что делятся на маленькие кусочки то ресурсы делится равномерно между процессами основным и копированием единиц бишь мы тоже другой процесс нет процесс я понимаю что другой я и говорю что между самим процессом и хаоса и процессом копиром делятся и только единственная возможность получается под разными пользователями надо ограничивать пользователям чтобы не грузить класс церкви хауса можно запустить всего лишь один процесс копира дело в том что копируем внутри параллельный его можно запускать где угодно и сколько угодно процессов на разных серверах то есть можно запустить нам настася рак 100 процессов они будут доставать из букета задачи свои его память эти задачи но сильно грузить ваш кластер а вы можете начать запуск одного процесса я слишком медленно запускайте следующий а если слишком сильно грузят просто пребываете его в любой момент он отказывал стой чего то есть переживает отказывай серверов просто убиваете этот процесс и у вас меньше нагрузка идет вот а кстати вопрос так сколько кусочков разбиения по умолчанию 10 10 это ведь как раз ответ на предыдущий вопрос почему copper тормозит возможно можно поставить один кусочек и тогда будет точно также как insert select ну да если сравнение по производительности медным кусочков и целиком копирование можно же просто запретить мэр жить партицию и спокойненько и копировать на другой шар а потом разрешить мир жить и все это синхронизировать ну запрещать мир ведь партицию это анти паттерн вот и так сравнение какого-то форма сонета давайте последний вопрос заслушаем и затем моники то выберет автора сама вашего вопроса спасибо никита интересный доклад скажите а можно ли этим инструменты может быть есть другие инструменты которые позволяют реализовать что-то типа инкрементальные backup оно какое-то постоянное хранилище без того чтобы иметь параллельно отдельный сервер because столько с целью иметь backup ну-ка торговый и рисует что там раз в день данные измененные просто добавляется какой-то уже существующей backup ну вы про то что вы говорите это совершенно отдельная фича это уже бэкапы греха и скопируем поле соки для другого это вот реально переезд вашего всего кластера со там с одного железа на другое пока по наверно лишь может рассказать"
}