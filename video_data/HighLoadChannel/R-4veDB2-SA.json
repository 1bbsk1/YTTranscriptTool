{
  "video_id": "R-4veDB2-SA",
  "channel": "HighLoadChannel",
  "title": "YTsaurus: опыт эксплуатации хранилища из 180К дисков / Павел Сушин (Яндекс)",
  "views": 3006,
  "duration": 3255,
  "published": "2023-09-01T03:13:23-07:00",
  "text": "да друзья продолжаем Как настроение Да и под этот шум заходит Все кто Хотел зайти а я представляю очередной горячий пирожок из Яндекса звезда предыдущих питерских айлоунов Паша это твой выход что поехали Всем привет Меня зовут Паша сушин Я 15 лет работаю в Яндексе из них больше 10 лет занимаюсь Яндекс инфраструктура конкретно занимаюсь разработкой эксплуатации системы войти заурус большая часть Спасибо всем что пришли принципе я сегодня буду говорить просто систему выйти я буду говорить о том как мы делаем ее удобно для эксплуатации вообще почему важно делать систему для эксплуатации Хотя очевидно но для начала Я хочу начать с небольшой сказки который будет прологом к моему рассказу а жила-была компания и Однажды эта компания решила построить Новый сервис пошла компания к тех Лиду выдал ему тех задание тех лиц сел на печи старый сайт архитектура нарисовал парочку сервисов нарисовал базу данных шину двх а ну и поскольку наш тех лет был проактивный и инициативный он пошел ресерчить пошел ресерчить нашел классный Новый транспорт нашёл транспорт почитал хакеранг отзывы отличные почитал столько отзывы отличная пошел бенчмарки сделал летает вообще приходит огонь администрировать почти не нужно Ну там раз в месяц колеса смазать и может раз в год безопасности пройти но в целом почти ничего не требуется развернули начали писать сервис потихонечку все летит в какой-то момент говорит Слушай Федя а вот у меня есть сообщение Вот важные и вот другие которые не такие важные Ну вот почему не важно мешаются важны можем ничего не сделать в принципе дело нехитрое сейчас поднимем две инсталляции настроим роутинг важное сообщение в один транспорт не важно И в другой и в принципе будет всё снова летать что недельку покодил все работает а прошло еще немножко времени снова приходите к админу и говорит Слушай такое дело заказчик пришел и говорит что надо чтобы вот наш весь сервис Он был желательно совместим справка протоколом потому что мало придется мигрировать Вот ты не знаешь наш транспорт не поддерживает кафку админ призадумался пошел почитал нашел на гитхабе какой-то проектик который делает кавкадаптера так чтобы значит пока протоколу писать в эту вот модно транспорт собрал поднял как изолентой прикрутил Вроде всё работает прошло еще полгода наш сервис запустился стал работать в Облаке прилично данные появились там персональные данные и тут тех Лид приходит ко мне говорит Слушай Федя а ты знаешь что наш транспорт не поддерживает шифрование мы не можем персональные данные в Облаке для шифрования гонять Что же делать тут Федя пригорюнился пошел расчистил начал колдовать что делать надо поддерживать и вот прошел год все раз запущен все хорошо пользователи нагрузку держит выходит новая версия транспорта приходит тех лиц к админу говорит Слушай давай будем все обновлять загрустила админ говорит Ну мне нужно пару джунов нанять я их буду учить как вот все эти наши костыли изоленту которую мы тут наворачивали вокруг твоего транспорта переносить на новую версию Но на самом деле конечно админ бы хотел поговорить с разработчиками этого транспорта и донести до них всю боль которая вызывает эксплуатацию своих продукта в реальном мире в реальной эксплуатации на масштабе в большом количестве инсталляций А дело в том что Федя мечтает не о том чтобы нанять двух джинов он мечтает о том чтобы тот транспорт когда он пользуется поддерживал бы и шифрование и шортирование и желательно адаптеры из коробки это было бы гораздо эффективнее это было бы гораздо дешевле в эксплуатации А и пользоваться этим могли бы все не только Федя на фирмы А все кто пользуется этим честным транспортом а я должен сказать что мне крупно повезло Яндекса есть ресурсы чтобы вкладывать не только в эксплуатацию инфраструктуры но и в развитии и поскольку в Яндексе просторные проекты живут примерно по тем же правила правилам что и обычные сервисы а именно у них довольно быстрый цикл от разработки до внедрения эксплуатации обратной связи и снова разработки то мы стараемся не откладываться надолго костылями изолентой а вносить в основной продукт такие фичи которые позволяют его удобно эффективно эксплуатировать в условиях постоянно изменяющегося мира а ну а теперь немножко про уйти заурус Надеюсь что пройдёт пару лет мне не нужно будет уже эти слайды каждый раз рассказываете Но пока ещё не все я думаю зрители в зале знают о том что это такое Мне нужно немножко погрузить вас в контекст а вот это вот вот это вот Спектр А значит А если говорить на современном маркетинговом языке то выйти замуж это дейта лайтхаус платформа э это система куда все сервисы Яндекса складывают структурированные полуструктурированные неструктурированные данные а потом их разным образом обрабатывают скажем сервисы поиск складывают в войти за а HTML документы видео картинки и потом считают по ним поисковую базу А буквально все сервисы Яндекс складывают в эти за Уру своей логики по которым потом аналитики А считают отчёты строят дашборды или итоги об экстремлентов инженеры используют данные для обучения хранящиеся в эти заурос и ГПУ машины подключенные также кластеры Войти заурус для того чтобы обсчитывать машины Learning модельки в Яндексе развернуто более 20 кластеров батизаурус А в основном Это маленькие ниши как кластера но есть два очень больших основных на которых выполняются основная доля вычислений А самый большой кластер сейчас содержит более 20 тысяч хостов там более одного гигабайта место на жёстких дисках более 1 млн ядер А и более 180.000 дисков было в марте когда я подавал заявку на Хэллоу но сейчас уже по-моему за 200 перевалило а в марте этого года в эти заурус вышел Open Source и теперь доступен всем желающим чтобы попробовать так что не стесняйтесь заходите на базару всех а там можно забронировать онлайн демон самостоятельно составить впечатление о системе Если говорить о возможностях в эти зала в целом то проще всего проводить параллели с этой системой ходу а скажем у нас есть и с разные виды обработки данных вычислений и разные виды сторожа если пытаться найти какие-то ключевые различия я бы сказал что войти заурус гораздо сильнее между собой слои сцеплены скажем в ходупе сервис координации сервис отвечающий за репликацию данных и за метаданные таблицы - это Три разных системы Да это зубкипер это дата нода и это скажем хайв а в эти заурусе всем этим занимается мастерзаурус и это на самом деле довольно э важная часть системы потому что для того чтобы принять таблицу нужно знать её схему для этого нужно взять блокировки А в чанках также есть информация схеме таблицы В общем вот это провязка слоёв она неспростая она даёт а какие-то дополнительные возможности если говорить про архитектуру системы птичьего полёта она тоже похожа на архитектуру ходупа скажем шатулировать эти за ауру занимает примерно тоже место что и ярдов в ходубе а поверх шадулера работы с э подсистемой придиоз которая позволяет писать довольно низкоуровневым он предистый ход есть системы которые позволяют задавать SQL запросы для аналитиков на каком-то высокоуровневом уже диалекте Кроме того У нас есть интеграция со спарком где Spark используется и как где Спарк используют в эти заурус и как источник данных то есть читая данные звоните туда так и среду для выполнения то есть на обычного совершенно стандартном диалекте спарка вы можете написать как план обработки и запустить его в класеретизаурус кстати сразу после моего доклада в пятом зале Лёша Шишкин будет рассказывать про а оптимизации которую мы делали в парке для того чтобы ещё лучше интегрировать его тизарусом а кроме того обращу внимание на интеграцию с греха хаосом которая здесь называются чить Клин Хауса войти А это занимает Ну примерно нишу которая занимает импала пресс или три навыка системе ходуп То есть это быстрый Эмери это хока аналитические запросы во-первых данные которые хранятся в эти за Урус используя движок Хауса а так примерно выглядит компоненты типичного кластера эти за аурус ядром кластер является мастер поскольку кластера У нас сейчас уже очень большие то а мастер у нас не один а он шардирован а-а мастер отвечает за репликацию Да за поддержание всего дерева имеет информацией за контроль доступа а аккаунтинг еще на самом деле множество разных функций транзакция У нас есть шатуллеры которые занимаются распределением ресурсов ноды основные рабочие лошадки кластеры которые хранят данные и на которых запускается собственное вычисления Ну и праксии это точка входа кластера аутентификация пользователей и API которые предоставляет система если спускаться ниже и смотреть только на стороне выйти заурус то можно конечно же найти налоги с такими системами как hdfs не является для hdfs или CF потому что у нас совершенно другие протоколы несовместимые но если говорить про архитектуру и общую идею то многие фичи схожие а также как и скажем всех у нас есть и реже кодирования и а обычная репликация У нас есть поддержка различных кодеков сжатия на уровне отдельных блоков чанков а-а это std xip и некоторые другие а на Нижнем уровне войти заурус оперирует понятием чанка это единица репликация чанки бывают и мутабельные так называемые журнальные которые записываются скворумом и который можно опенить в конец но это все скрыто от пользователя за слоем API пользователь работает с нашим деревом этой информации а также с объектами типа файла и таблицы то есть пользователь непосредственно пишет либо строчки в таблице схематизированные или не смонтизированы или поток Байков файлы или в журнала а ну что ж Давайте теперь на примерах посмотрим как же работать вообще любовь взаимодействия в эти заурус любая работа начинается с того что мы начинаем работать с нашим деревом эта информация которая называется Кипарис и для того чтобы начать работать достаточно выучить всего 5 базовых команд та команда Create Get Set list Remove собственно Вот на этом примере мы создаем папочку в дереве войти заурус Home Project внутренние создаем узел типа Документ и вот вы сразу видите в третьей команде Я обращаюсь к атрибуту нашего нашей папочки для того чтобы посмотреть а сколько же элементов внутри моей папки находят А и все три команды и чтения скажем документы и чтение метода данных выполняется с помощью команды gett э-э Мне не нужно никакого другого IP специального для того чтобы с этим деревом работать забегая вперёд я должен сказать что админу тоже не нужно никаких других команд для того чтобы работать свои эти заурус А ему достаточно вот этих команд работать с кифоризмом потому что всё интроспекцию все это данные Мы в рамках системы стараемся в водительские Борис и предоставляет через тот же API Ну давайте дальше Вот Мы создали папочку хотим теперь создать не таблицу вот есть несколько команд которые создают таблицу со схемой заливаем нее несколько строк данных у нас получилось 4 строки появилась пара новых команд типа ritable но это используется как точки зрения а вот с админской точки зрения админу неинтересно просрочки В таблице обмена Интересно а из чего состоит таблица где данная реально хранятся на классе где данные реально хранятся на дисках а данные реально на кластере хранятся в чанках и чтобы узнать в каких чанков стоит таблица достаточно обратиться к соответствующему атрибуту а чандис вот мы получаем список чанков и мы можем даже получить статистику скажем каким кодеком эти чанки пожаты поскольку мы поставили коды компрессии за этот уровень пять Вот мы видим что все чанки пожатых кодеком т.д уровня 5 Следующий вопрос который админ Должен себе задать Хорошо я знаю с кинг-чанков с таблицей где чанке лежат и тут мы с вами начинаем проваливаться в разные под деревья интроспекции которые предоставляет систему войти заурус чтобы узнать где лежит Чанг нужно пойти всего лишь веточку sichamps и взять атрибут сторис торт репликас А где будет написано на каких нодах находится реплики данного чанка вот мы собственно получили атрибут видим что у нас есть Две ноты что немножко странно потому что вообще-то дефолтный коэффициент репликации в системе три а у нас всего две реплики А как бы нам узнать э-э где третья реплика И вообще не сломалась Эта система Собирается ли она делать реплику Что ж очень просто у Чанг есть также атрибут Джобс в котором мастер рассказывает нам о том если сейчас зашифрованные жабы репликации или восстановление если мы имеем дело реже чанком на какой Надежда запущен А если же вообще не запущен то возможно был предыдущий Джоб и он поборется в какой-то причине всё это есть в нашем деле это информация а нам не нужно Скажем бегать по логам для того чтобы получить это Для дальнейшего анализа А я надеюсь что здесь есть люди у которых было опыт работы с ходу и возможно сейчас вспоминаете свой опыт взаимодействия с командой HD ФСС и надеюсь что вы видите на экране вам нравится чуть больше Окей мы примерно разобрались Как пользоваться интроспекцией Но нам нужно какие-то мониторинге и в общем первое что В подобных системах хочется мониторить это конечно целостность данных что все данные по-прежнему доступны для получения списка данных которые находятся под угрозой в системе войти заврус есть соответствующая ветка в кипарисе который позволяет увидеть какие чанки сейчас недоступны Мы закончили посмотрим У нас целых три Чанг недоступна Как так как бы нам узнать где эти чанки хранились в последний раз очень просто у нас есть атрибут Ласт реплика который позволяет там получить список нот где этот чат был последний раз виден видим что всего на одной ноге был Чанг это очень подозрительно ведь коэффициент репликации три а-а Давайте посмотрим какой таблице чанк и что с коэффициенты аппликации Итак получили таблицу в ТМП кто-то положил таблицу с коэффициентом репликации один что ж в общем его права но есть некоторые проблемы получается что мониторинг который мы построили поверх веточки Lost Chance а будет флапать когда кто-нибудь создаст Таблицы с коэффициентами а проблема Не уникальна скажем так для войти заурус и скажем альтернативные системы предлагают в такой ситуации Ну говорят давайте вы будете мониторить какая доля чанков потеряна если вот будет очень много то тогда будем поджигать А если не очень много то может быть и ладно звучит как не очень получается что если у нас какие-то важные данные стали недоступны но всего в целом данных недоступных не так много то мы совершенно это игнорируем и а-а спокойно дальше живём не поджигаем даже Пока пользователь не прибегают А что ж это не наш путь а наш путь - это на самом деле а подумаешь что надо как-то разметить Какие данные реально важные Какие данные у нас а промежуточные или возможно временное или возможно специально на них был поставлен коэффициент э ребятки фактора один а чтобы они занимали поменьше места и не учитывая их в мониторе ринге для этого мы ввели понятие витальности чанка а которая автоматически выставляется на те чанки которые являются промежуточными рецептами вычислений а витальность false выставляется автоматически на те данные которые можно легко пересчитать или на те данные которые были созданы с коэффициентами развлекаться один Итак Таким образом мы получили новую метку который показывает нам недоступность именно витальных важных критичных чанков Вольтом получили ручку если какая-то табличка какой-то Чанг становится недоступным и мы понимаем что мы по какой-то причине не можем его сейчас восстановить и он не является по факту критичным для пользователя то можем конкретную таблицу поймите как ней витальный и снова потушить наш мониторинг для того чтобы видеть вновь возникающие проблемы Окей базовый мониторингами разобрались Давайте посмотрим на другие админские задачи вот скажем типичная задача администратора ходупа вот новых нот и вообще работами при работе с модами нам важно опять же иметь какую-то интроспекцию для того чтобы делать аналитику следить за скажем нотами которые полностью заполнены а иметь возможность легко проводить мейнтона с операцией скажем замену дисков или А какие-то апдейты работы на оборудование а-а Вот про это сейчас и поговорим Ну во-первых список нот вместе с онлайн и оффлайн нодами можно получить запросы в Кипарис естественно как вы поняли а также про каждую ноду можно получить информацию о том заполнены ли она Сколько места свободно Сколько у неё дисков и так далее а отдельно отмечу что в системе хорошо продуманным API довольно Легко делать аналитику И не только из консоли Ну и поскольку выйти заурус есть здесь достаточно удобный питоновский API то вот буквально несколько строчек я делаю питон ноутбук который позволяет мне найти на кластере те ноды которые полностью заполнены буквально в один фильтр а что же Давайте посмотрим как мы добавляем новые ноды что важно при добавлении новых нот в большом кластере это заботится о Странности данных сохранность данных у нас обеспечивается с помощью разметки доменов отказа А и В системе войти заурус есть два уровня разметки доменов отказа это уровень даты центров и уровень отдельных стоек Если вы когда-нибудь размечали стойки в ходу Возможно вы видели эту инструкцию которая справа в рыжником в квадрате нарисована где нужно подложить значит специальный скрипт потом который разметит ноды потом всё перезапустить чтобы он применился в общем довольно муторная процедура требующая много внимания а для того чтобы разметить но стойками войти заурус достаточно на объект а повесить атрибут с желаемого значением стойки стойку естественно перед этим нужно собрать а создать на кластере и в общем-то все это открывает звучит просто и достаточно тривиально Но это открывает на самом деле новый уровень возможностей в управлении кластером в качестве примера приведу историю как год назад у нас были большие работы в дата-центре на энергооборудование и дата-центр поделен на модули где каждый модуль состоят состоящие из нескольких десятков стоек по очереди тушился для проведения регламентных работ когда модуль тушится то естественно тушится все сервера в нём а после Пауэр сайкла К сожалению далеко не всегда сервера нормально спокойно загружаются есть какой-то заметный процент серверов который после отключения по питанию либо отваливается сбитым диском либо значит зависает на загрузке И это всегда довольно нервный процесс Когда нужно большую часть центра выключить потом обратно включить одно дело стоит сервис а другое дело хранилище как у нас чтобы обезопаситься во время регламентных работ Мы решили перед ними переразметить машины в стойках таким образом чтобы собрать все машины которые уходят в работу в одну такую большую виртуальную стойку и таким образом дать системе сигнал что на этой большой виртуальной стойке можно хранить не более одной реплики каждого чанка с одной стороны звучит какая-то недоработка как будто бы у нас недостаточно выразительности в нашей системе доменов отказа который бы позволила заранее разместить модули с другой стороны за счёт гибкости и разметки машин А мы смогли написать какой-то простейшую автоматизацию которая почти не требовала внимание админов для того чтобы сначала собрать вот этот вот морозный рекор виртуальный Ну а потом по окончании работы его разобрать и в целом работы прошли максимально гладкой Несмотря на то что заметную долю машин не поднялась все данные были доступны а что ж для работы с модами конечно же нам нужны также механизмы для их плавного обновления и для э-э ввода вывода при Main при регламентных работах для замены дисков для а замены памяти апгрейда операционной системы вероятность свечей и так далее а для того чтобы имплементировать ролик апдейт незаметный для пользователя нам нужна механика которая позволяет плавно вводить нагрузку с not кластера желательно при этом не вызывая какой-то избыточной репликации и в общем-то кластер не перегружая для этого у нас есть специальный атрибут который можно установить этот атрибут говорит надежд говорит мастеру и на самой Наде что запись на эту ноду нужно потихонечку сворачивать И никакие новые сессии заливки не открывать имеет такой примитив и возможность интроспекции понять ну да сколько сессии заливки на неё сейчас идёт уже открыто мы можем реализовать логику плавно в апдейта буквально в 10 строк нам нужно взять очередной рек потому что в каждом раке хранится не более одной реки чанка а на всех нодах это враг запретить сессии заливки подождать пока не закончатся убедиться в целостности чанков А после чего выкатить новую версию снять флажок и перейти к следующему а ну механизм дикомиссии в эти заурус в целом известен тем кто работал с ходупом это возможность сказать ноде что она должна увести все чанки потому что мы уводим ее в долговременный мейнтон из какой-то скажем Возможно это будет замена дисков или а замена памяти в общем надо будет недоступна в течение как минимум часов и э-э нет смысла подвергать систему стрессовые до репликации в момент когда много неожиданно уйдёт если можно плавник увести из неё все чанки заранее а собственно реализовав эту механику Насти равной части мы сделали роли на апдейт И как часто бывает одна доделка влечёт за собой другую а мы сделали апдейт и у нас начали во время роллинга загораться мониторинге потому что во время роллинга когда большая да часть данных на кластер хранится в рейдже кодирования во время роллинга то тут то там отдельные реплики недоступны и постоянно возникает необходимость данной восстанавливать А если у нас в течение длительного времени на к власти есть данные которые нужно восстанавливать то это поджигает это конечно ноет потому что мы можем запускать ролинг несколько раз в неделю и всякий раз нам приходится тушить Alert Что же делать на самом деле надо понять что нас беспокоит не то что какие-то данные недоступны тут там она реально беспокоит когда какое-то реже Чанг очень долго не может восстановиться и нам важно понимать а как долго самый старый частично доступный Чанг не может восстановиться и в качестве сигнала для аллюрта использовать именно эту величину Вот на этом графике собственно показано как во время роллинга чанки становятся недоступны По мере того как он идёт реже чанки становится частично недоступны и мы добавили в Мастер новые интроспекцию которая показывает самые старые чанки частично недоступны завязали мониторинг на неё и в целом теперь наши ролики проходят легко и приятно без лишнего шума а Что ж если у нас есть ролик апдейт и мы можем без влияния на пользователей обновлять наши кластера несколько раз в неделю Казалось бы это такой классный механизм который позволяет нам в том числе катать изменения конфигурации или какие-то эксперименты начать нот Да совершенно не влияя на пользователей и на то как они взаимодействуют с кластером это правда только отчасти потому что полноценный ролик на кластере из 20.000 машин занимает от суток до двух примерно и если мы хотим скажем провести какой-то небольшой эксперимент или включить какой-то фьюче флаг Скажем мы добавляем в конфигурацию выкатываем в течение двух суток на весь кластер потом ловим бак и теперь чтобы нам это включить Нам нужен либо балл крестарт который займет 3 часа с влиянием на пользователей Либо мы будем два дня отказывать это обратно поэтому конечно для того чтобы эксплуатировать систему таких масштабов нужно предусматривать какие-то способы динамические конфигурировать на лету и у нас есть подсистема динамическая конфигурация а-а которая построена на модели тегов и выражение на тегом тегах Дело в том что у каждой ноды кластер есть какое-то количество системных а также пользовательских тегов которые можно расставлять всё через тот же наш IP типа Гетто сет А И когда мы укатываем динамическую конфигурацию мы можем указать формулу на тегах которые я определяет множество нот на которых эту конфигурацию нужно выкатить вот в данном случае я э одну ноду отправляю в э одну группу канареечную вторую ногу наречное А все остальные ноды описывают как не попавшие не в одну из первых двух и выкатываю на все эти три группы три разные настройки для того чтобы сравнить и э провести эксперимент принять решение Что лучше работает и потом уже скажем раскатить на всех выкатите откатить подобные изменения конфигурации можно в течение буквально 10 минут а что я потихоньку перехожу к заключительной части нашего такого админского путешествия и по мере роста кластеров мы приходим ситуации когда наши кластеры становятся очень гетерогенные на самых больших кластерах войти сейчас более 40 различных конфигураций хостов они отличаются объемом памяти различными моделями CPU и конечно же количеством и типами дисков работая с такими гетерогенными кластерами нужно хорошо подумать как поддерживать конфигурацию всех этих разных машин и тут у меня есть две темы которые хотел бы обсудить первая тема - это возможно задавать параметры в конфигах в виде относительных величин Дело в том что если у вас есть скажем машина с 50 гигабитной сетью 25 на сетью и 10-битный сетью довольно сложно подобрать одинаковые параметр для скажем троттлер процесс репликации гораздо проще сказать Я хочу чтобы моя репликация занимала не более трети полосы доступной моей машине это Вот пример относительно параметра то же самое скажем с памятью зачастую многие параметры памяти в системах типа войти заурус в сторожах они пропорционально скажем количеству дисков И если мы говорим что мы хотим блок кэш то очень удобно задавать блок Cash пересчете на диск Я хочу чтобы меня на каждый диск было скажем 3 ГБ кэша а но еще более Интересная история конечно с самими дисками Потому что когда у нас возникает кластер гетерогенной с дисками разного типа у нас возникает сразу желание во-первых хранить разные таблицы на дисках разного типа скажем различать горячий холодные данные или изолировать какую-то часть дисков для того чтобы туда опускать журнальные данные которые лейтена секретиков А на другие пускать данные которые скорее ориентированы на максимальные спут и нужна механика позволяющая всё это множество дисков каким-то образом разделить А в ходу пятаками есть она называется storage типа они там более-менее прибиты довольно жёстко их собственное количество и описание а войти завр всё немножко гибче А у нас есть понятие медиума который собственно определяет логический какой-то количество дисков на кластеры и когда мы создаем таблицу мы можем указать На каком медиуме Эта таблица должна находиться создать медиум это полбеды создать медиум это просто команды в Кипарис Но более сложно здесь это разметить сами диски вот примеру две ноды у нас на каждой ноге какое-то количество жестких дисков какое-то количество SSD какое-то количество бешек и 4 Medium и нам нужно нашими админскими какими-то тузами описать Какие диски какого медиум будут относиться сделать для каждой машины более того необходимо предусмотреть чтобы наши диски были равномерно размазаны по рекам будет довольно глупо если мы выделим маленький медиум для экспериментов и он целиком будет расположен на одной машине все диски кажется одной машине никакого отказа устойчивости мы не получим или даже в одной стойке Да это будет максимально как-то неразумно А поэтому нам нужны какие-то инструменты которые позволяли вот эту карту дисков строить регулярно обновлять собственно тип относится диск у нас довольно долго задавался просто в статическом конфиге каждой ноды И если нам нужно было диски разметить это приводил это требовало роллинга очередного приводил довольно массивному переезду чанков по кластеру В общем довольно неприятная процедура а что ж мы не стали с этим мириться что же мы кликер А вот и некоторое время назад мы доработали на Мастере механику для изменения типа медиума для каждой локации Это потребовало довольно большой переделки в Мастере потому что до этого мастер знал просто что на моде есть два диска такого типа такого медиума и скажем 3 диска другого медиума и чанки ссылались напрямую на ноду смысле реплики чанков размещали основной что моя реплика лежит на этой ноте вот на таком-то медиум для того чтобы научить различать медиумы на лету нам пришлось научить мастер знаний об отдельных локациях о каждом отдельном диске и теперь Чанг ссылается уже не на ногу а на конкретный диск и зная про этот диск получается что мастер может за их спозить API для того чтобы типа этого диска поменять конечно же как обычно через атрибуты объектов локаций собственно Можно прийти и поменять тип медиума у диска С дефолтного на какой-то тестовый а имеет такую возможность на стороне мастера мы Как админа можем написать уже сервис который бы поддерживал нашу карту дисков в динамике который бы можно было декларативно описать Какие диски смысле сколько дисков Какого типа мы хотим и чтобы он набирал множество дисков автоматически подобную систему Мы внедрили в прошлом году и это открыло еще один шаг в автоматизации на самом деле потому что это позволило нам не думать больше об разметке дисков когда мы вводим новые машины в кластер теперь вот машина кластер полностью автоматизированный происходит без участия человека потому что а-а мы видим что вот эти машины должны быть добавлены нарезаем там сервис сервис поднимается дальше приходят медиум балансер понимает Какие диски на машине доступны Ну и раздаёт и нужно медиума а это позволило очень дёшево выделять какие-то подмножества дисков для экспериментов и Скажем мы провели эксперимент с переносом всех промежуточных данных мы придётся операции на SSD э это не потребовало никаких сверх усилий по переразметке дисков вручную их в отделение что вот эти диски они будут промежуточные данные вот эти Нет мы сделали это достаточно дёшево буквально э-э небольшим изменениям конфига Ну и напоследок небольшой детектив конечно не все у нас проходит гладко иногда случаются проблемы когда мы внедряли вот эту балансировку медиумов на одном небольших кластеров все спокойно работало Однажды вечерком мы добавили еще одну стойку в этот кластер проверили что данные льются все хорошо живёт вышли спать ночью раздаётся звонок от мониторинга мониторинг говорит что у нас начали падать сессии заливки пошли разбираться по логам видим что проблема локализуется до двух нот стали смотреть графики этих нот там проблема локализуется буквально до двух дисков и на эти диски поток идёт кратно превышающий поток на диске того же медиума на других очень странно потому что вообще-то мастер работает таким образом чтобы равномерно раскидывать задачи сессии заливки а стали смотреть Другие графики по числу собственно сессии заливки видим что на этих двух нодах число сессий заливки во-первых упирается жесткий лимит а во-вторых несколько раз больше чем на других нодах этого Medium а оказалось что Дело было следующим медиум балансер ночью добрался до этой новой стойки и решил что на этих нодах есть диски которые должны попасть в медиум так получилось что это был 16 16 рейк 16 стойка в которой были диски mediumanekoda который мы используем у нас делят чанки на 16 частей как только мастер увидел что на кластере появилась 16 стоек в которые можно разложить чанки он выбрал политику класть на каждый рык не более 1 чанка итоге оказалось что вся нагрузка по заливке реже чанков которые в других стойках приходилось на 10 или 20 дисков здесь свалилась на вот эти два несчастных диска которые были выданы балансером ну распутов эту историю мы подкрутили параметры чтобы разрешить репликатору класть и реже Чан больше реплика реже чанка на каждую стойку и таким образом выровняли нагрузку что ж перехожу к заключению Надеюсь я дал вам представление о том как мы подходим к тому чтобы строить системы которые было бы удобно эксплуатировать понятно что есть какой-то базовый уровень что нужны логи нужны естественно метрики нужны трейсинг Но а не менее важно сталкиваться с какими-то проблемами в эксплуатации не только решать Это скриптами скажем или какие-то вспомогательные инструменты но и э-э забирать потом эти наработки внутрь системы для того чтобы улучшать Как метрики так и механику внутри самой системы позволяющей эксплуатировать её в масштабе А и во множестве инсталляций а Спасибо большое за внимание если вас заинтересовал Давайте то пожалуйста заходите на эти запрос тех вот поэтому qr-коду А там есть онлайн-тема можно записаться получить себе собственный небольшой кластер в эти заурус в своё распоряжение и составить собственное впечатление Да вопросы пожалуйста Паша Спасибо за доклад вопрос про вот в эксплуатацию новых ast of Как работает перебалансировка то есть как вы заполняете новый ход данными ну и соответственно сопутствующий вопрос Вот как раз Чанг балансер не запомнил название Какие алгоритмы балансировки Используйте для того чтобы равномерно распределять нагрузку по кластеру значит Как заполняется новый ход на самом деле практика показывает что среднее время жизни чанка на кластеры довольно невелико что чанки возникают вычислений во-вторых чанки возникают потом складываются в архив отдельные процессы пережимаются и круговорот чанков происходит постоянно у нас есть механика по выравниванию загрузки в Мастер репликатор какое-то долю полосы умеет отводить на то чтобы переносить чанки с более загруженных на менее загружены поработав так несколько лет мы его отключили потому что реально за счет постоянно новые ноды довольно быстро за не выравнивается старыми так про алгоритмы медиумбалансера Ну смотри там есть некоторые мысли там наши настройки которые говорят какой медиум является донором смысле откуда брать диски для mediuma который находится под балансером можно задавать параметры в терминах числа дисков причём там есть Какой гистерезис минимум дисков максимум дисков и э в терминах объёма А смысла Мне нужно минимум столько терабайт для этого медиума максимум тока А ну а дальше о медиум балансера есть знания о том что нужно равномерно по рекам нужно равномерно по машинам на самом деле распределять он старается намазывать медиумы которые под управлением балансера максимально равномерно Спасибо будьте добры Добрый день Сергей Юдина X5 А ну если вы вот эту систему отдаёте его пансорс Наверное вы бы хотели чтобы кто-то её тоже использовал кроме вас а соответственно можете в двух словах э сказать Э именно с точки зрения эксплуатации А какие протоколы доступа к ней есть да то есть кто может ей пользоваться как вам в виде и очевидно что не у всех есть экзабавить данных то есть какой э-э минимальный там логичный а объём а для инсталляции Ну смотрите наши маленькие инсталляции они в принципе это десяток Но вот это там на пяти нотах мы на самом деле разворачиваем смысле 5 хостов да на которых все компоненты в эти зауры разворачиваются не то чтобы это уже тот объем на котором имеет смысл я бы сказал что как только мы говорим хотя бы про тысячу ядер этого уже вполне разумно для того чтобы значит заморачиваться Здесь вопрос именно в количестве но и дисков или в объёме там в терабайтах невозможно осветить абстрактное не понимаю Какие процессы быть там запускать Да и вот скажу как видео В смысле а те проекты которые внутри Яндекса скажем по какой-то причине хотят своих кластера маленькие А обычно речь идёт о хотя бы нескольких сотнях тысяч уже значит люди идут ставить что касается протоколы то э-э у нас есть API на нескольких языках это Go Java э-э c++ и питон а на самом деле это во-первых htp API У нас есть также собственный PCP который реализован без резкой библиотеки довольно сложно пользоваться системой не то чтобы она вот какие-то стандартные даже http можно но лучших мест библиотекой О'кей Спасибо Да Маркет Гуру Денис а у меня следующий вопрос допустим проксмакс есть разметка дисковых пулов Ну все классика такая мини вот если нужно поднимать ятозаврус я правильно понимаю что эта система подразумевает что она сама является платформой для виртуализации своего рода то есть ее надо разворачивать на голых Хоста чтобы прописывать диски с лейблами с атрибутами и уже всю балансировку она скрывает сама за собой то есть тогда нет смысла делать виртуализацию внутри виртуализации смотрите смысл действительно можно но смысл действительно не очень много Хотя можно естественно настроить хранение так чтобы использовать только одну реплику и в принципе там не заморачиваться Особенно с разметкой медиумов можно заморачиваться на самом деле да Если у вас есть надежность скажем сетевые диски реплицированные то наверное вам не нужна репликация в этом плане конечно поверх железо разворачивать Да привет Спасибо за доклад вопрос У меня такой ведь диски любого формата имеют свойство рано или поздно случайно единички сбрасывать в нолики и узнаешь об этом только когда попытаешься с этого сектора считать вы как-то это отслеживаете есть какой-то хилинг просто с которой периодически перезаписывает блоки пересчитывает Ну во-первых суммами В смысле причём как передача по сети так Такая чтение запись диском а скажу честно у нас нету механизма Вот который периодически это сильно отключен есть но которые периодически Случайный Чанг пойдёт почитает чтобы убедиться что у него всё хорошо а в целом не было необходимости в смысле диски довольно сильно загружены в наших кластерах не хочется тратить полосу ещё и на это а есть периодический процесс проверки в целом живости диска что типа он проходит какое-то базовый чек Я записал мегабайт прочитал мегабайт сравнил чек сумму что у меня там всё хорошо вот такое человек с диска мы делаем Ну для ваших нагрузок да Если вы опенсорсите это у людей не у всех постоянные чтение диска но где-то я так понимаю в конфигурации это можно включить Да понятно спасибо передайте вперёд пожалуйста Ага спасибо спасибо за доклад можешь рассказать вкратце я понимаю что там можно ещё один такой класс сделать как бы реально Еще одна история А смотрите я отвечу Так я буквально в мае рассказывал на котфесте в Новосибирске про наш мастер причем там было под запись и запись венчуэли выложится Я пока не могу дать мне qr-код еще но кажется там можно зарегаться на сайте и получить доступ уже сейчас собственно мастер является нашей э точкой координации которая внутри которой есть консенсус который собственно через Форум работает и который позволяет э быть тем единственным сервисом который собственно вот через форум А все остальные используют его для того чтобы так ещё вопрос ваш Привет Сергей Санников надо прямо в микрофон извини пожалуйста не вопрос Сергей Санников ВК безопасность собственно вопрос как у вас безопасностью О классный вопрос а не очень но мы над этим работаем но не очень в том плане что будем честны протоколы сейчас у нас без шифрования причем смысле ХТС конечно у нас есть для внешнего IP Но вот внутри кластер у нас сейчас совсем Всё не шифровано А у нас из Классного У нас очень такая Широкая гибкая система управления правами доступа на уровне э дерева метаданных а в том числе доступ к отдельным колонкам в рамках таблицы можно выдавать а довольно гибкий доступ к объектам разные действия а пользователи группы пользователи или вот это всё реально На мой взгляд на очень хорошем уровне а вот на Нижнем уровне там где мы говорим проживание данных или про такого шифрования протоколов Мы работаем на поддержкой шарований в rpc чтобы внутри кластер гаишифрованный Мы конечно понимаем чтобы интерпразе это как бы must и скажем в облаках это тоже Маст и я думаю что к осени уже будет спасибо спасибо может Сорри за наивно или глупый вопрос я-то заурос Это по сути база данных То есть можно делать таблицы колонки или я неправильно понял Или это только для хранения блобов Ну смотрите это во-первых не реляционная база данных где есть индексы значит связи между таблицами вторичные ключи и так далее это табличная система Но все-таки для большого числа данных и в первую очередь ориентированная на такой Хаус Ну вопрос мой закончу А есть ли противоречие или конфликты интересов например с яд б или это совсем разные ниши использования Я не понял для себя просто А ну давайте в каких местах есть пересечения Ну вот скажем в той части по которой я рассказывал скорее нет потому что ну давайте войди by это в первую очередь база данных Да с возможностью аналитики которые движется в сторону аналитики Но скажем аналитики только Да выйти это система в первую очередь аналитику с возможностью запуска произвольного кода на самом деле но и скверный интерфейсами в целом такая большая платформа которая позволяет свалить туда все данные разными способами в зависимости от ситуации А на как вишенку на торте ещё поставить какой для того чтобы потом отрисовать то что получилось а передай пожалуйста Да спасибо да если на бок положить так кликхаус получается Да живем пожалуйста Спасибо за доклад У меня на самом деле два вопроса вопрос первый на одном из слайдов было написано что поддерживается в iql А ты сказал что SQL как обработчик вот здесь как-то не сошлось давай я широком смысле Да и он не совсем ну смысле что он не комплаент со стандартом ребята довольно много усилий прикладываются в том чтобы поддерживать пост синтаксис и так что если и вдруг еще пока не комплаен то скоро будет хорошо я понял И второй вопрос что по поводу этих директории гербера platform Security Вот это и всей истории Ну частично отвечал на вопрос хотя бы внешняя авторизация скоро да кербера скорее нет но шифрование в протоколе какое-то прям очень хотелось бы крупным интерпрезе это прям я понимаю ничего сразу ну и надо понимать что все-таки у нас не все силы выходят на Source довольно много выходят на поддержку внутреннего но теперь можно на катрибутить хорошо спасибо Повторяй за мной Ну пожалуйста это чаще всего ускоряет Это я послушал этот лайфхак на этим леде это Тим Лиды друг другу рассказывают что если ты пришёл говорит что пожалуйста тогда быстрее разработка идёт Скажи кому мы подарим книжечку за лучший вопрос и кому подарим твой суперприз обнимите руки кто задавал вопросы так чуть-чуть легче Давайте подарим приз вот просто про безопасность и Давай наверное подарим второй подарок за вопрос про минимальную инсталляцию во внешнем мире отлично тебе тоже памятный приз Святого хайлоуида 2023 Спасибо огромное"
}