{
  "video_id": "ZCNcK2fmkBo",
  "channel": "HighLoadChannel",
  "title": "Применение машинного обучения для генерации структурированных сниппетов / Н.Спирин (Datastars)",
  "views": 808,
  "duration": 2959,
  "published": "2018-01-16T12:27:57-08:00",
  "text": "очень очень рад как участвовать на мероприятии хоть и удаленная по визам вопросам соответственно сам не смог присутствовать вот но но надеюсь то что наш значит удаленный сеанс будет полезен и придаст мероприятию какой-то такой международный характер через телемост москва очками вот несколько слов обо мне кирилл уже возможно я коротко представил то есть я закончил mp3 потом сделал печь типа коммерсант здесь winners of illinois urban шампунь работал фейсбуке но и на данный момент соответственно занимаясь доцент consulting company to the stars представляем обучение и разработка различных высокотехнологичных продуктов на основе анализа данных вот но непосредственно по докладу сегодня мы будем говорить о том как генерировать епты для поисковых систем по трудоустройству и чтобы понять о чем же мы будем говорить давайте введём вот это понятие сниппета поскольку людям которые занимаются поиском она может быть знакомо люди которые занимаются другими областями машина обучение применяет машина обучение например на производстве или вскоре нге банковских продуктов это может быть что-то новое в общем что такое поисковый сниппет представим то что вы зашли на поисковик ввели свой запрос как обычно вам будет вывалилась 1020 поисковых результатов каждый результат сопровождается соответственно ссылочкой которая будет называться заголовком и дальше помоги показы какая-то вспомогательная информация в частности показывается краткое резюме о контенте страничке называемой снимка по идею то тут вся информация дополнительно сопровождающая ссылку называется с ней потом но мой гон сфокусироваться именно на вот эту часть которая является развернутым описанием вот и прежде чем соответственно описывать наш подход давайте давайте обозначим проблему вот проблема какая у нас возникает мы посмотрели на интерфейс и существующих поисковых систем по трудоустройству и выяснил что конкретно в этой вертикали job search существуют следующие какие-то такие ogli моменты на данный момент в частности если я введу x дизайнер в поисковик indeed который является самым большим в мире поисковикам по работе у них приблизительно 150 миллионов уникальных пользователей в день вот там увидим то что существует избыточность информации между заголовком и с ними там то есть пользователи должны читать одну и ту же информацию повторно на страничке что соответственно замедляет их процесс взаимодействия с поисковой страничке результатов и таким образом информация и пространство поисков страничке поисковой выдаче используя не оптимально вот другой пример если мы позволим сюда здесь показываться в сниппете какие-то неуклюже приветствия от автора job объявления вот поэтому тоже такую информацию не хотела бы показывать сниппет на хотелось бы сфокусироваться на то что реально полезно соискателем при принять решение на какой же результат все-таки кликнуть вот если мы посмотрим на интерфейс linkedin а то тоже здесь мы видим какие то и хищники из базы данных которые с соискателем не являются полезными вот другая версия lint кубинской интерфейс и после того как они сделали небольшой апдейт она показывает информацию о компании она показывает место положение то есть где будет происходить работа и показывает заголовки носа расширенного с не питали нету вообще плюс если мы посмотрим на заголовке то достаточно сложно различить от 11 результатов другого поскольку у них у всех абсолютно одинаковый заголовок и пользователи если не будут обращать информацию внимание на какое-то дополнительно помогать информацию им будет сложно понять куда же ты хоть но и они будут вынуждены прокликивать самого верха поисковой страничке все вниз опять же теряя время просматриваю возможно не релевантные результаты вот аналогичный проблем существует у монстр ew.com проблема у job.ru тоже показывается только только заголовки и какая-то очень примитивная информация компании но нет развернутых сник так вот то есть если мы если мы резюмируем то существует следующий проблемы с ней появляется не информативными сложно различать один результат от другого избыточность информации присутствуют между с головками и текстом себе то на данный момент а вот иногда показывать какая-то бесполезная информация но и а непосредственно в контексте это трудоустройства иногда бывает по заголовку достаточно сложно определить является вакансии релевантной например а существует стартапы и особенно на ранних стадиях когда softer инженеры это какая-то такая генеральная генеральная вакансия a soft инженер подразумевается будет решать какие-то вопросы под это сайнс работать с данными и так далее и хотелось бы как-то больше понимать что же это за 100 пипсов турнира в конкретной компании то есть какая то дополнительная информация возможно было бы полезно вот но и соответственно на основе этого возмущение то есть сформировав проблему поняв какой-то такое боль которую может возникать у соискателей у нас возникает вопрос что ж показывать сниппет то есть как бы если мы не удовлетворены тем что в на данный момент существует на поисковой страничке давайте подумаем из каких-то таких базовых предпосылок проанализируем поймем что же кандидатом будет полезно чтобы они оптимизировали свой процесс поиска и делали клике на результаты более вдумчиво вот но и для того чтобы это ответить на этот вопрос мы провели и x исследования то есть как бы весь доклад построен так что мы рассказываем под еда от идеи и до конкретные уже до внедрения 2b тестов то есть полный цикл поэтому несмотря на то что доклад про машины обучение мы рассмотрим вот этот промежуточный исходный шага который мотивировал соответственно потребностью применять машины обучения вот то есть я их исследование мы провели и использовали три различных методов анализа сбора требований а от пользователей для того чтобы произвести то регуляции каким-то образом верифицировать результаты которые мы за счет одного конкретного метода а юзер исследование получили вот первый метод был вербализация то есть мы попросили пользователей которые приходили к нам в лабораторию поработать с конкретной поисковой системой и после этого в процессе поиска мы просили их высказывать свои комментарии вслух мы соответственно записывали делали транскрибацию их мыслей и после этого кодирова ли анализировали какие же ключевые мысли идеи возникают они в голове в процессе поиска и непосредственно с уклоном на то какую же информацию им было полезно бы посмотреть на страничке а поисковых результатов соответственно второй метод исследования был основан на разметки мы после того как они поискали с помощью поисковика мы попросили отобрать несколько вакансий которые они считают для себя релевантными по их предметной области и в каждой из вакансии мы попросили выделить ту информацию которую они считают важный для принятия решения откликнуться на эту вакансию либо нет вот соответственно после этого мы проанализировали какие же секции какие блоки вакансии они посвятили и таким образом тоже поняли характер информации которая людям интересно ну и дальше мы соответственно провели третий метод исследования это был опросник мы проанализировали достаточно большой корпус вакансий а и выделили атрибуты которые упоминаются о вакансии в и и соответственно тексте apis контент контент локонте проанализировали вот например есть такие вещи как год основания тип тип работы соответственно требуемый опыт опыт работы и так далее индустрия и дальше мы попросили пользователи отранжировать но поставить каждому из атрибутов оценку от одного до пяти и таким образом мы поняли является ли данное 3 будет полезным либо полезно либо не полезным для принятия решения о а вот клике на конкретную вакансию вот и здесь какое важный момент но два вопроса два вопроса мы сделали первый опрос был сфокусирован на страничку поисковых результатов 2 опрос был сфокусирован на детально страничка вакансии то есть вопрос ставился следующим образом какие из атрибутов вам интересные для того чтобы принять решение просматривать ли данную вакансию более подробно это для страничке поисковых результатов а уже на страничке поисковых результатов вопрос ставился так что насколько какие атрибуты вам полезны для того чтобы решить откликнуться лет на этого канча то есть после более детального анализа но и соответственно там небольшие различия были в ответах но в целом было консистентной как бы выборг атрибута соответственно какие результаты мы получили в на основе вербализации компании навыки профессии обязанности опыт работы были полезны как мы уже знаем а на основе анализа существующих интерфейсах поисковых систем по трудоустройству показывается компания показывается профессии то есть заголовок например да то станете стиле разработчик вот но при этом не показывать навыки не показывать обязанности не показывается опыт работы в годах либо показывается только очень ограниченного числа поисковиков соответственно вот inside такое что навыки обязанностей это хорошая информация которую можно было бы показывать чтобы помочь нашим соискателем дальше соответственно если мы какие-то рассмотрим такие отдельные уникальные анекдотические комментарии которые пользователи сгенерировали в процессе исследования что мы видим то что люди из используют мне информацию о требованиях как необходимую информацию и если она не удовлетворяется то они просто начинают пропускать конкретную вакансию вот другой пример соответственно третий пример маст хав критерия то есть я обращаю внимание именно на эти атрибуты если они не выполняют что я дальше пропускаю вот соответственно после на основе разметки также были выявлены такие вещи что требования частности навыки образования опыт работы в годах важны обязанности важны город но город как бы он уже сейчас показывается место работы но этот соответственно топ-5 трибутов на основе разметки ну и третье соответственно на основе опросов у нас также есть тип деятельности компания профессия навыки обязанности вот но и если резюме как бы резюмировать то мы видим то что хорошо было бы еще показывать обязанности и требования на страничке поисковых результатов вот то есть расширить наше представление сниппета этой информацией ну и последним как бы шагом последней каплей как бы мотивирующий наш новый формат сниппетов стала статья объем research и которая была написана в 2012 году на одной из топов конференцией по hi-min hi-min компьютер interaction вот там если проводил следующее исследование сравнивал с две версии юзер интерфейса для поиска экспертов и сотрудников в внутрикорпоративного интернета и рассматривался одна версия когда показывается фоточка и какая-то очень базовая информация про сотрудника например его позиция и возможно департамент а в другом случае рассматривалось развернутой информации где еще показывались скиллы какие-то предыдущие проекты и так далее вот мы и на основе утопи статьи выяснилось то что развернуто информация позволяет более быстро находить очных экспертов и а качество этих аспектов была выше чем при наличии только вот этой информации вот но и соответственно наша идея это показывает на страничке результатов обязанности и требования то есть приблизительно вот такой мокап сниппетов мы предлагаем здесь как бы я сделал такой небольшой комментарий что когда новый какое-то понятие или inside в презентации возникает то здесь будет появляться такая звездочки кубик но и мы соответственно за время презентации будем билде те действует наш как бы арсенал новых инсайтов кубиков которые возникают в докладе а то есть первая идея это вот показывать обязанности и требования на страничке поисковых результатов соответственно дальше у нас возникает два под вопрос а как извлечь тогда это информа свою обязанностях и требования из произвольной вакансии поскольку вы знаете то что вакансии пишется достаточно различным образом какие-то компании имеют шаблоны какие-то компании пишут очень свободной форме и извлекать информацию может быть проблематично вот а мы хотим создать алгоритм который для любой вакансий будет стандартизировать и переводить в такую жесткую форму а типа джейсона который мы будем иметь секции требования в требования обязанности условия и какую-то другую такую информацию с явной семантикой вот но и дальше у нас вопрос такое хорошо мы предложили расширить сниппеты этой информацией мы научились допустим даже извлекать эту информацию и вакансии будут ли пользователи рады этому да то есть как бы на основе исследований мы построили гипотезу то что да это да это может быть интересно но конечно хотелось бы проверить это в продакшене ну соответственно мы для этого проведем об тесты и ответим на этот вопрос на данный момент давайте сфокусируемся на первый из этих вопросов как извлечь необходимой информации постановка задачи изучения информации очень простая дано предложение из вакансии либо это просто при текст либо it rich темалл так который содержит внутри себя текст и задачи присвоить метку класса обязанность требования либо другое вот но и соответственно российском интернете еще очень важно секции условия в вакансиях поэтому для русского интернета мы еще использовали класс условия то есть назвал 4 класса вот качестве примера здесь приведена вакансия произвольно вакансия и в результате алгоритм идеальный должен произвести такую разметку то что часть предложений у нас покраситься в один цвет часть предложения в другой часть предложения останется нерелевантный если мы посмотрим на структурированную вакансию то здесь в принципе ничего не меняется мы опять должны разукрасить это правильным образом вот ну и для этих целей чтобы производить вот эту автоматизированную разметку мы применяем машины обучения как бы я здесь не буду повторяться и углубляться в описание того что же такое машина обучение поскольку секции все-таки про машины обучение я подойду как бы считаю то что достаточно знакомая аудитория с данной идеей вот совсем быстро я освещу эту как бы в методологию для людей которые возможно только только начинает внедрять машина обучения себя то есть состоит процесс весь из трех там четырех стадий вначале нужно собрать данные для обучения модели в нашем случае разметить какой-то корпус вакансий вручную и присвоить каждому приложению нужную секцию дальше соответственно на основе вот этого размеченный датасет а мы обучаем модель в этот момент мы генерируем признаки выбираем модель алгоритмов настраиваем различные параметры вот ну и дальше как бы необходимо создать дополнительный корпус данных который мы никогда не показываем алгоритму про них проводим на нем только оценку то есть от называется fold to set вот и мы сравниваем предсказание алгоритма разметку алгоритмы с разметкой асессоров либо экспертов для того чтобы понять насколько качественно алгоритм предсказывает то что мы на самом деле видим она вакансии будучи людьми вот но и дальше как бы нет предела совершенству мы повторяем шаги 1 и 2 до тех пор пока результат на конечным и болеющим дата сайте у нас не были творят нашим стандартам вот давайте сфокусируемся соответственно на 1 на 1 вопрос на первый под задачу в процессе машину обучения сбор данных соответственно здесь у нас возникает дилемма то есть либо мы можем размещать вручную этот корпус либо мы можем подумать да то есть традиционный подход тогда у нас не существует размечена во корпуса данных мы соответственно нанимаем асессоров сажаем их лабораторию даем скрипт по размер и говорим соответственно следуйте вот этому конкретному набору шагов набор правил и для каждого объекта нашей как бы задачи ставьте метку например мы можем сказать то что ну если предложение содержит ta ta ta ta ta ta представьте и как в метку требования если она содержит какие-то другие слова представьте метку обязанности но и соответственно таким образом разметьте для нас корпус вот на этот процесс дорогостоящий и особенно важно в нашем контексте что если поисковик по трудоустройству работает на международных рынках то есть например indeed работает там с в 100 странах по моему где там 5 50-100 страны такой диапазон и поддерживать 20 плюс языков то не хотелось бы для каждого из языков делать свою разметку поскольку это все-таки процесс дорогостоящий и требует значительного времени вот соответственно если это банковский какой-то бизнес это обычно нет генерируется самим бизнес-процессам то есть пользователь приходит оставлять заявки на кредитные как бы кредитные выдачи и дальше нас получается метка вернул не вернул кредит вот в этом случае как бы тогда разметка вообще не требуется но это опять же не наш случай вот мы и наш случай подумать соответственно мы не хотим размечать руками мы хотим применить какие-то наши упал и автоматизировать процесс непосредственно разметки данных вот и для этого мы сделали наблюдение следующее то есть если мы посмотрели глазами на какой-то подмножество вакансий и выяснилось то что какой то какой то подмножество пенсии имеет достаточно явно визуальную структуру то есть мы как бы существует заголовок потом идет какое-то перечисление вот и возникла идея мы может быть сможем достаточно простыми регулярными выражениями вытаскивать вот эти вот нужные предложения соответствующие точно какой-то секции их использовать в качестве обучающего множества для нашего алгоритма соответственно мы проанализировали html-код date of вакансии именно подмножество и выделили такое группу под вакансий подгруппу вакансий которые у нас существует какой-то header там верхнего уровня то есть один и два и так далее течь 7 там вот дальше у нас идет о darth и леонард лист вот и внутри существует лист элемент который содержит по средств уже текст предложения который соответствует нужны для нас секции в данном случае это требование вот и дальше мы сгенерируем вот такой простой extraction паттерн с другой стороны он является очень точным то есть как у на себя что на себя представляет он из себя представляет непосредственно слова который соответствует секции дальше на существует вот эта обёртка из ю л л а и и вот то что находится в действии вместо звездочки и это как раз-таки то предложений которые соответствуют нашей полезной секции вот но и дальше метод генерации данных для обучения состоит из четырех шагов для каждой секции мы задаем по одному слову за головку например для обязанностей это может быть слова обязанности или это может быть там ваши задачи для требованиям непосредственно слабо требований или у вас там например как бы у вас вы там выпускник хорошего вуза у вас есть опыт работы вот такой-то тематики и так далее да то есть но вот именно важно что заголовок он каким-то конкретным таким словом которые часто употребляются для наименования этой секции с этим связан вот дальше мы соответственно собираем большой корпус вакансий и на основе вот этого правила шаблона извлекаем очень много предложений для каждой из секций то есть для обязанности для требований для условий и генерирует также класс другое посредством сэмплирование предложения либо из какого-то произвольного текста либо из предложений которые явно не связаны с секциями да то есть это может быть какая-то базовая информация до перечисления требования обязанности либо это может быть какая-то информация после вот но и дальше как бы мы собираем очень много примеров эвристик а такая существует машина обучение что 1000 1000 + объект на класс хорошо бы иметь для того чтобы алгоритм качественно обучался в одном и на для финальной версии которые мы использовали мы собрали в общей сложности 10 миллион предложений то есть приблизительно по две с половиной тысячи предложений на каждый класс обязанности требования условия и другое вот но это вот соответственно у нас 2 квадратик появляется то есть это вот метод генерации данных описан общей те которые вы можете использовать в других проектах да то есть создать создается какое-то очень точно про его извлечений как бы это правило извлечение использовать для порождения чистого обучающего множества а потом уже те те объект обучения которые вы из большого с реально очень большого корпуса насобирали вы используете их для обучения алгоритма и гипотеза такая то что в текст майнинге поскольку все таки язык такой достаточно вариативный как бы объект то с большого корпуса мы сможем выучить машины алгоритм который будет аккуратно предсказывать для произвольной как бы вакансии для текста который происходит с произвольного источника вот соответственно таким образом че после этого процесса у нас появляется обязанность требования другое вот обучающий множество и наша задача обучить маргарита машину обучения здесь уже чист дело техники то есть мы использовали линейный разъём с кэшированием признаках на основе моделей богов worlds то есть используя 7 граммы и граммы триграммы а также мы использовали part of speaking то есть определение частей речи и над ними также и не граммы и граммы триграммы вот и дальше мы соответственно обучали линейная swim по каждому из этих признаков представлений дополнительно к этим трем там 6 моделям мы добавляли признаки над предложением капитализация текста длина предложением словак символах число и чтим албегов внутри конкретного под блока вакансии вот и это у нас составляла признака в описании и дальше соответственно мы строили алгоритмическую композицию которая объединяет предсказание вот этих базовых алгоритмах первого уровня и признаки которые мы над генерирую над предложением то есть дальше обучается опять второго уровня модель машинного обучения и задача как мы ее поставили для нового предложения зги не сгенерировать признаки а применить наши алгоритмы получить предсказание метки класса то есть является ли это требованием обязанностью условиями и либо другим классом вот то есть это достаточно стандартная операция после того как уже сформировано обучающий множество поэтому мы как бы дальше углубляться в эту штуку не будем перейдем непосредственно к эксперименту эксперимент 1 ставился следующим образом мы в одном случае в качестве bass line а разметили ручками вакансии вот выделили предложений которые принадлежат соответствующей секции и потом обычный алгоритм на основе этого обучающего множества то есть этот называется меню annotation а такая же модель алгоритмов использовалась как и для финальной модели вот вторая как бы модель это вот викой супер вас то проще который требует задачи всего лишь одного заголовка на секцию потом использует эти высокое информативные точные шаблоны из или катары которые позволяют нам создать обучающие множество очень быстро и большого масштаба ну и дальше мы сравнивали качество предсказаний требований и обязанностей и используют две ключевые метрики это точности и полнота соответственно точность важна для того чтобы мы реально качеством информацию показывали восьми битах и извлекали нужные предложения а полнота нужно чтобы мы эти точные как бы предложение извлекали с большого репрезентативного сэмпла вакансии могли покрыть всю предметную область job search вот и на этом графике интересными существуют как две две вещи первое то что наш алгоритм практически не уступает алгоритма сменил annotation и то есть здесь как бы не статистически значимая разница вот при этом наш алгоритм достигает большей полноты поскольку он обучается на значительно большем обучающий множестве которые просто физически создать нельзя поскольку соответственно размечать миллион миллион там лететь миллион предложений ручками будет ну просто невозможно тем более хочу отметить то что невозможно это сделать ляг вы из языков таким образом как бы процесс позволяет как бы поймать двух зайцев сразу же да то есть и гарантировать хорошую точность такой же как и при ручной разметки а и большую палату мистика второй эксперимент который мы провели то есть результат того же эксперимента чуть-чуть с другой стороны на него посмотрели мы сравнили точность извлечения соответственно требований и обязанностей для случайного в случайной выборки различных профессий для того чтобы понять что наш алгоритм извлекает информацию качественно из из разных вещей да то есть как бы что он не не является хорошо работающим только там скажем для вакансий собственно инженер или продакт-менеджер и например работой хорошо войти индустрии а шофёров или поваров как бы он не обрабатывать то есть мы специально сразу посмотрели и качество там консистентная по по всему datasette у да то есть по крайней мере видно то что там эту в районе 08 как бы это в течение происходит но если докручивать в гори там тому же до 09 доход дальше соответственно до крутка алгоритмы какая если использовать одну одно слово на каждый на каждой из секций на извлечение и задачи вот этого алгоритма для генерации обучающего множества то мы все-таки не все вакансии можем накрыть и размер нашего обучающего множества может быть не такой большой соответственно для того чтобы увеличить размером чаще у множества что мы можем сделать мы можем больше число шаблонов экстракторов создать и таким образом накрыть большее число вакансий а из них извлечь ненужный там нам предложение ну и соответственно здесь как мы видим где виде меньше на реке раз да то есть чем больше правило мы добавляли тем как бы замедлялся рост накрытие числа вакансий из которых мы можем на сам полировать предложение но и понятно дело то что он как бы асимптотически стремится к ста процентам но они достигают этого момента потому что всегда существует какие-то непонятные вакансии очень непонятного формата во-первых которые совсем не присутствие не содержит секции и разбивку в html текст ну и плюс люди формулируют совсем по-разному заголовки вот ну и дальше соответственно мы предложили 2 стадиями метод обработки вакансии вместо того чтобы использовать только машина обучение это соответственно 3 inside 3 идеи предлагаются для того чтобы реально повысить качество финальный production модели сделать композицию в начале мы используем те же самые правила которые мы использовать для излечения наших предложений и для обучающего множества поскольку они очень точные и гарантированно извлекает нужную информацию а дальше если ни одно из этих правил не отрабатывает на вакансии в момент извлечений из новой то мы используя машинное обучение как печь пол решения то есть как бы такую страхующий систему которая извлекает уже на основе искусно интелекта и присваивают метки на основе того как вы с теми показателями качества которые мы достигли здесь вот ну и соответственно мы научились извлекать информацию давайте теперь перейдем ответа на вопрос как пользователи реагируют структурированы сниппеты да как является ли это полезным все наши усилия вот но и прежде чем мы ответим на этот вопрос еще возникает под вопрос такой у нас существует структурированы вакансия уже формате джейсон у нас есть секция требования и там предложения которые мы смогли извлечь машиной у нас есть секция обязанности и предложения которых извлечь смогли извлечь машины вопрос как же мы генерируем сниппет на основе этого структурирована представление поскольку всю информацию мы не можем выгрузить выгрузить на страничку поисковых результатов все-таки 12 никита ограничено и нам нужно как-то повысить информативность нашего снейп это путем умной селекции вот этих предложений из каждой из секций и для этого сделали мы использовали принцип максимума джонс да то есть здесь как бы такая страшная формула давайте лучше разбить на примере у нас есть запрос например кто-то в поисковой машине вел datasource из в качестве вопроса и у нас допустим есть секции обязанностям и у нас есть три предложения кандидата здесь мы видим то что предложение номер два она содержит это source is a друг третье предложение содержит только дейта первое приложение не содержит ничего согласно принципу маг максимум markzware даунс что мы делаем мы тогда берем самые релевантные приложение первым вторым предложение мы берем предложение которые также является ремонтом но отличается от предыдущего но поскольку здесь все приложения достаточно отличается то мы берем второе предложение вот ну и таким образом последовательно мы набираем предложение сад удовлетворяют балансирует два критерия первое то что каждое предложение должно быть и релевантный исходному запросу второе то что предложение каждое предложение следующее отличается от всех предыдущих предложений вот таким образом мы повышаем информативность при этом мы сохраняем релевантность нашего сниппет а вот ну и дальше перейдем непосредственно к pt100 мы сделали apts течение двух недель на 500 тысяч пользователей с большой поисковой системой по трудоустройству и у нас соответственно рассматривалось две версии сниппетов это простой сниппет без какой-то информации развернутый и сниппет в котором показывают требования обязанности и условия вот хотя мы могли провести абед stf которым тестировались только например пока с требование либо только обязанности либо там комбинации требования dance мы решили такой самый грубый первичный с перемен поставить сравнить версию когда ничего не показывать и когда показывается вся информация для того чтобы польза ли могли принимать более вдумчивые решения вот но и соответственно архитектура решения достаточно стандартная здесь я не буду акцентировать поскольку нас секции все-таки про машина обучение они про распределенной системы коротко упомянуто что мы как бы сад из из из из интернета краулер веб-странички о вакансиях дальше у нас соответственно существует машина обучение блок также у нас существует config files вот этими чтобы маме если катарами правилами который позволяет генерировать обучающий множество дальше мы их раскидываем соответственно по машинам у нас существует уже обученная модель i can fix правилами все это лежит под cash-in для того чтобы мы могли эффективно обрабатывать новые входящие запросы и производить машины обучения только если к нам попадает новую вакансии пора вакансия запрос вот но и дальше все это было прокси равана под индексом вот дальше соответственно результат эксперимента 1 inside который мы нашли это то что число кликов на запрос на странице с результатом стало меньше то есть это означает что пользователи биф свой запрос и когда они смотрят на страничку поисковых результатов они более вдумчиво делают клики и только на те вакансии которые им реально и которые их реально цепляют на основе информации сниппетов да то есть но наш такая интерпретация вот дальше соответственно соотношение откликов просмотром вакансий стала выше что это означает то что когда пользователи попадают уже на детальную страничку вакансии после поисковой странички и смотрят на нее то они откликаются с большей вероятностью да то есть они заведомо попадают на те вакансии с которой потом конверсия последующей тоже выше то есть это означает то что они изначально выпирают более релевантные для них вакансий то есть это тоже хорошая вещь с точки зрения юзер экспириенс а дополнительные метрики которые были собраны в формате abr теста качество это соответственно offline метрика для финальной модели которому сильно докрутили для русского языка для bts то вот с точки зрения бизнеса немножко такие как бы неоднозначные результаты то что уменьшилось число запросов за сессию и уменьшилось число просмотров да но это как бы обычно эти цифры хотят увеличить для того чтобы если бизнес модель построена у поисковика на числе показов страниц поисковых результатов там какая-нибудь реклама подобное конечно хотелось бы больше показывать этих страниц но если мы сделаем акцент на user experience и хотим помочь пользователям более вдумчиво и быстрее находить нужные результаты то как они были зации этих метрик как может быть оправдан вот и дальше очень интересная как бы мысль о том что общее число откликов тоже была увеличена то есть совокупный как бы эффективности marketplace а то есть митинг сыскать и компаний он был увеличен за счет этого алгоритма вот и таким образом можем сказать то что даже несмотря на то что как бы мы уменьшили число просмотров уменьшим число запросов на сессию общее число эффективной системы подросла до то есть если действительно система трудоустройства фокусируются на то чтобы удовлетворить пользователей помочь им надо найти как бы нужны результаты то это хороший результат вот так же как вы видите число откликов после просмотра как я уже упомянул увеличилась на 13 процентов число коротких кликов то есть кликов которые выглядят следующим образом пользователь вводит свой запрос нажимает на какой-то результат а потом сразу же мгновенно уходит назад нажимает бег или что-то подобное число таких как бы паттернов уменьшилась на пять с половиной процентов ну и энтропии критиков тоже уменьшилось то есть пользователи а разные пользователи на один и тот же запрос на одну и ту же поисковую страничка выдачи стали кликать приблизить более одинаково более консистентная поскольку у них появилась вот эта дополнительная информация на страничке поисковых результатов позволяющий оценить более аккуратно релевантность вот то есть такие как бы общее наблюдение за б теста но и давайте перейдем к заключению мы представили 5 ключевых инсайтов идей первое то что нужно оказывает требования обязанности второе то что мы можем собирать данные для машину обучения в руки как бы на основе точных шаблонов а потом уже обучать машинное обучение дальше то что имеет смысл комбинировать подход на основе обучения с точными правилами использовать машина обучения в качестве такого katch пол решение уже в продакшене для того чтобы обрабатывать какие-то сложные случаи которые не покрываются правилами это позволяет нам достичь высокой точности при этом гарантировать тоже хороший полноту вот дальше соответственно принцип максимальной общей полезности можно использовать для того чтобы набирать информативный и при этом релевантный сниппет но и все аккуратно тестируем в в тестах этого в принципе уже мне кажется является таким стандартам индустрии вот контакты пожалуйста задавайте вопросы по почте в социальных медиа с удовольствием буду рад ответить всем всем новом как бы контактам очень рад вот но и по вопросам дополнительно как не касающимся презентации консалтинг проекты и либо обучение под это санс потеплел link пожалуйста тоже пишите с удовольствием обсудить готов вот и в качестве бонуса да то есть завершая завершая наш как бы презентацию поскольку мы работаем как бы в рунете ну то есть это данной конференция посвящена проектом из рунета а то хочется отметить что супер ток индекс i had hunter уже какой-то степени интегрировали структурированы сниппеты вот в качестве примера я могу привести здесь юзер интерфейс мы видим то что есть обязанности требования и показывается эта информация кандидатом дальше соответственно яндекс тоже самое здесь показывается только секции про требования и возможно здесь как бы желание уменьшить чуть-чуть размер каждого отдельного результат а потому что если показывать как superjob там слишком много информации возможно пользователь будет сложно сканировать там вверх вниз вот то есть здесь как бы я так сделал такой трейдов фокусируется на самую важную информацию которая позволяет именно сделать такой троллинг пространства поиска данное счас на основе users лет юзер экспириенс исследовали мы тоже выяснили то что а требования является более важной информации чем обязанности для кандидатов 1 х х хантер здесь не полностью показывает структурированность но все же показывает информацию про требования в своих снега так вот ну и соответственно дальше там какие то дополнительные слайды про юзер исследования это уже людям которым непосредственно будет интересно более детально ознакомиться с методами вот так что все коллеги теперь готов ответить на ваши вопросы спок у нас осталось времени у нас еще 15 на вопросы спасибо отлично отлично отлично 20 минут мы значит вовремя успели вопросы никита привет спасибо за доклад скажи пожалуйста ты говорил что какая то часть сниппетов обрабатывается правило веркит какая-то часть обрабатывается машинным обучением а можно вот рассказать про соотношение то есть скольки процентам сниппетов в итоге применяются машина и обучения сколько процентов сниппетов просто от по таким правилам о его листиком показывается соответственно это качественный вопрос очень да если мы посмотрим вот на этот слайд точные цифры я сказать не могу да но примера это зависит это зависит от размера числа шаблонов которые мы задаем да то есть если посмотреть на эту кальку то мы видим то что там допустим если у нас есть 20 патронов на каждой из секций да то есть на каждой секции мы создаем папку по определенным числом паттерна то у нас на накрывается скажем 90 процентов вакансий ставим этом 35 но это как бы условный вещь это зависит от чего зависит от секции зависит от корпуса данных да то есть вот языка например в российском интернете там достаточно стандартный как в формат вакансий в америке и там он тоже достаточно стандартный если мы посмотрим на какие-то другие страны тут там ну ахтунг может быть то что очень сильно падает вариативность большая от вакансии вакансии это может быть число патронов больше требуется да но вот в целом а именно на качественном уровне кривуль к она будет выглядеть вот так вот туда то есть и зависит от того сколько вы патронов на генерируете ну скажем скажем так я дума наверно процент такой пары это правило да то есть где то там 80 процентов вакансий накрываются правилами там 20 процентов уже обрабатываются машинному обучению ok и еще один вопрос а вот это правило они только для определенного языка или существуют правила именно для определенных профессий например у программистов там одни шаблоны вот там бухгалтеров другие шаблоны а здесь как бы и где с чем заключается то что мы создаем вот эти вот генеральные паттерн и которые применяются на язык в целом как бы на каждый язык по хорошему самым-самым таком экономичном случае что мы должны задать мы должны задать всего лишь по одному вот этому слову о названии sex и и потом использовать вот этот как бы поттер на основе форматы и читы на страничке вот то есть там нету как бы привязки к программистам или бухгалтером или и к докторам или к шоферам вот то есть такая вечности 3 генеральный правило но для каждого языка оно должно быть свое например для английского вместо вот этой слова обязанности там у нас должно быть слово responsibilities да то есть требования там должно быть слово рекламе на немецком языке я не знаю там как будет это слово но там будет вот слова эквивалентные с точки зрения перевода но вот нашему слову требования нот вот так и спасибо же никиту меня вопрос а почему взяли своим ну как бы здесь какая вещь и swim с точки зрения как бы работают с текстом достаточно хороший алгоритм да то есть как бы он там работает с различными признаками очень хорошо на на модели backwards да то есть вторая вещь то что здесь именно не было не было акцента на то чтобы сделать самую самую самую точную модель здесь была как бы больше акцент посмотреть на то и минут на качественном уровне сможем ли мы сделать этот pipeline котором мы можем генерировать обучающим множеству этим автоматизированным способом и потом показывать нужную информацию в степи тогда то есть как бы если мы потом сделаем акцент на машина обучение там начнем применять какой-нибудь бегу forts увеличим число признаков начнем какой-то качественный модуль инжиниринг да то есть тюнинг гипер параметров я думаю то что качество машина обучения можно еще там процентов ну на 5 повысить то то есть от того что мы здесь имеем а но как бы желание был сделать именно такой первый первый такой более менее качественно алгоритм и проверить весь pipeline вот ну то есть такой ответ ну и потом там как бы еще есть вот эту вещь которая связана с хеширование признаков там как бы я могу а просто в поисковой машине можно поискать там либо google школа там не либо яндекс либо google а что-то типа с вами switcher caching да то есть и такой как бы запрос ключевые слова и это алгоритм просто он позволяет значительно ускорить из вен с точки зрения как бы и тренировки и предсказание в дальнейшем вот то есть это хорошее такое решение было с ограниченными ресурсами вычислительными обучить достаточно качественную модель вот если есть опять же как бы вычислить на ресурс на кластере там какой нибудь сделать истребитель к машин лёнинг на большой коллекции то можно можно повысить как бы качество я понял скажи пожалуйста проверяли влияние таких сниппет of seo что значит влияние на seo на поисковое продвижение может быть такие страницы выше ранжируется но здесь я на самом деле не могу ответить на этот вопрос поскольку на мой взгляд как бы но если поисковая система общего назначения типа яндекс или гугл а занимается так называемым дип веб троллингом да то есть когда она формирует запрос к поисковой странички и получает страничку поисковой выдаче а потом и и индексирует и кладет к себе в яндекс то как бы на этот вопрос имеет место быть но я на самом деле не могу точно сказать делают ли это поисковые системы на данный момент индексирует ли они страничке поисковых результатов вот поскольку контент и структура предметной области вот этой трудоустройства она сильно динамичная да то есть вакансии умирают и буквально там через месяц до то есть через две недели то есть там правил африку рекрутёров там за 21 день закрыть вакансии то есть это ну как бы такой стандарт хорошего recruiter насколько мне известно и если индексировать эти странички то в каком-то плане достаточно холостая работа для индекса потому что потом их нужно будет выбрасывать через какой-то период времени вот и ну не знаю как бы насколько это оправданно вот я знаю только то что поисковики общего назначения индексирует как бы новости для не алиса специально как бы так называемый fast индекс да то есть как бы fresh индекс который именно по социальным медиа и по новостным источником про и спрыгну проводят очень часто индексацию если рынок трудоустройства и поисковики не по работе являются тоже таким важным важным аспектом то наверное есть есть как бы актуальные вечном индексировать эти странички но яд я сомневаюсь вот что касается ну допустим теперь как бы что все-таки они индексируются да тогда я думаю ответ на этот вопрос будет положительный то есть это с точки зрения сила должно улучшать потому что на основе такого ну а об общей логике допусти я занимался антиспам в свое время в понте спамом чем больше ключевых слов на страничке тем лучше как бы для до ранжирования да то есть как бы тем тем больше актуального контента и семантически связанного да то есть запрос и выдача плюс какая-то дополнительной информации сниппетов тем для машины это должно но быть быть приятнее вот но здесь как бы уже нужно экспериментировать как бы заранее говорить сложно поскольку даже эти алгоритма ранжирования страничек у поисковиков общего назначения постоянно меняются кто-то там будет ссылочное ранжирование кто-то его убивает кто-то чисто на машину обучение делать акцент но и так далее вот я понял спасибо еще вопросам вопросов больше нет тогда ники спасибо больше за доклад было интересно ну все хорошо удачный удачного подключения конвенции было приятно подключиться и чуть-чуть почувствовать себя частью частью сообщества рунета издалека в все всем всем удачи спасибо пока"
}