{
  "video_id": "UC7l8Y0sx24",
  "channel": "HighLoadChannel",
  "title": "Основы велосипедостроения при репликации данных между дата-центрами  / Евгений Кузовлев",
  "views": 1970,
  "duration": 3013,
  "published": "2020-04-14T10:50:38-07:00",
  "text": "на первой нашей сессии на первом нашем докладе этого потока в этом зале мы сегодня открываем конференцию я надеюсь вы все со свежим со свежими головами к нам пришли еще не затуманенными еще никто вам туда не налил никакой информации поэтому я буду первый как это право первого доклада собственно поговорим мы сегодня про репликацию данных между дата центрами том таких почему я тут и вообще кто я такой меня зовут кузовлев евгений я работаю в компании и компаний в группе компании компов подразделения тишь нам естественным конференция технику все-таки чем мы занимаемся занимаемся мы проведение платежей отъемом денег у населения скажем так помогаем нашим клиентам юридическим лицам осуществлять этот процесс наименее безболезненно для конечного пользователя который собственно это деньги платит и мы все это дело процессе проводим отправляем в банке отправляем в различные платежные системы и наверное еще одна такая особенность что мы являемся таков настоящий степени финн тех компаний по той простой причине что мы эксплуатируем система написанные нами внутри у нас есть только одна система и она не в основном флотом сбоку стоит которая написана не нами а весь наш processing это наш собственный house разработка на которой мы собственно работой мы зарабатываем денег мы большей степени международная компания нежели российская на российском рынке мы тоже представлена но меньше чем в европе и вообще в мире достаточно большой объем транзакций проводим и все что мы делаем это все связано с деньгами как следует из моего рассказа но так как у нас раз тема достаточно быстро у нас вот например там уже два года как появились так называемые терруар клиенты это клиенты очень крупные очень большие которые которым важен индивидуальный какой такой подход когда там под них запиливаю этой миссии сильно важно но им важно еще такая вещь как надежность для них это такой краеугольный камень которой без которого как бы они просто к вам не придут а вы соответственно не сможете предоставлять определенному кругу клиентов ваши услуги и ваш сервис и надежность она в какой-то момент упирается в вопрос собственного масштабирование когда вы растете и растете растете и в какой-то момент вы понимаете что все вы уперлись ваш предел вот про это мы сегодня и с вами поговорим давайте начнем с такой дискуссии вот представьте себе что вы как технические специалисты работаете над каким-то для начала стартапов ваш стартап будет расти и перед вами в первый раз замаячила замаячил вопрос масштабированию что вы сначала делаете вот какой первый шаг масштабирования да да да действительно меняем железо на борту и мощно это первый шаг самый простой у вас что-то пошло не так выкинули железо снова все хорошо какое-то время дальше не помогает уже этого кидаете там выступили допустим самый мощный сервер который был на рынке дальше что у что какие следующие шаги у вас есть второй сервер то есть вы по факту добавляете не так с ней не только железо ну вы добавляете вы включаете понятие линейного масштабирование добавляете серверов приложений до примерно так дальше ну после того как вы добавили сервер приложений у вас наверное уже с приложение работают отдельно от базы данных потом потом третьим шагом вынесли базу данных и в какой-то момент вы уперлись в эту базу данных что тогда вы делаете в слышу прям хорошие слова шарди рования репликации это хорошо но репликацию вы можете использовать только тогда когда у вас много запросов на чтение если у вас много запросов на запись что как бы это уже вопрос multimaster баз данных это такой краеугольный камень священный грааль который все ищут ну до сих пор мы особо никто не нашел да хотя мы немножко про это поговорим о на более высоком уровне сегодня вы сортируете базу данных вы разделяете так чтобы у вас там не знаю было 10 баз данных по 10 процентов данных лежала в каждой базе а дальше вы подбираетесь к некому такой некой точки невозврата когда вы начинаете ловить странное у вас уже это далеко уже не в стартап вы уже обслуживаете сотни и десятки тысяч из них сотни тысяч клиентов и ваши клиенты начинают жаловаться что вы где-то опять тормозите а у вас по всем метрикам мы все хорошо во всем предмет рекам приложения у вас база данных не тормозит приложение тармо не тормозит все хорошо но вот клиенты жалуются и вы сами вот когда ставите внешние метрики приложения вы тоже понимаете что он действительно тормозит и вы переходите на нике более такой уровень высокие просветления вы понимаете что есть сеть например и вы упираетесь в сеть допустим у вас там не знаю если вы работаете не в облаке работать и со своими серверами у вас гигабитные интерфейсы и вы просто уперлись гигабит интерфейса чем в этом случае делает но тоже самое что с железом да ну только сетью вы берете там меняете сетевую схему взаимодействия или там не знаю самой приколы самые такие хардкорные ребята берут там не знаю приходит к вендоры говорят о дайте мне два switch-а уровня дата-центра с пропускной способностью 100 гигабит на порт им говорят да не вопрос вот вам счет примерно как построить еще одну москва сити а не грех не вопрос заверните и у них там 100 гигабит на порт тоже подход но опять же если мы говорим про масштабирование дальше мы как было опять рано или поздно упремся и не все могут позволить себе москва сити и что делал дальше и дальше вы подходите к некой точки невозврата которую по-настоящему всегда является вопросом а как нам будет дальше вы приходите к мысли что нам надо ставить прям полную копию приложение где то еще но возникает вопрос связи данных между приложениями да то есть условно поставить полный instance по факту развернуть 2 дата-центр о чем мы сегодня будем говорить и когда вот про это говорим вот мы например задачу решали я понял что тут есть такой некий информационный вакуум крупные компании terlan и сайте не знаю там условно индекс он домой задачу решил они про него забыли для них это рутина а когда компания перешагивает этот шаг для нее это всегда проблема как это решать какими методами как с этим бороться и вот в этом случае как бы информации практически нет собственно чем вот мы сегодня и поговорим да что мы начинаем разворачивать еще одну установку систему еще одна копия система почему бы и нет плюсы этого они достаточно очевидны они классные да мы получаем не только масштабирования но мы получаем еще возможность авто failover а если у нас одна площадка упала мы можем переключить клиентов на другую замечательно и кроме эту мы получаем совершенно дикую возможность сокращения времени обработки пользовательских запросов не за счет увеличу улучшение системы потому что зачастую вашей системы уже отполированы так что любой код может позавидовать но и за счет того что вы просто приближаете географически систему непосредственно к пользователю например кто знает какая сетевая задержка между москвой лондоном вот прям вот во временном лаге в два раза ошибся но усредненно если у вас хорошие коннекта на магистральных провайдеров то действительно потеряете где-то 50-90 миллисекунд на connect ну вот в целом да вот диапазон примерно такое соответственно представьте себе что вы проводите не знаю финансовую транзакцию в которой 4 запроса ну вот а пользователи идет соответственно полсекунды теряете только на сетевом взаимодействии если вы можете у вас есть техническая возможность приблизить это все к непосредственно конечному пользователю и получается что все достаточно просто давайте давайте поднимем еще одну копию система в чем сложность вообще о чем вообще здесь говорим о сложность вот в этих вот 3 наверно окнах нужно обеспечить репликацию данных потому что но если мы полностью изолированную копию системы извините полностью изолированную копию системы поднимем то это какой то наверное не очень хороший случай нам нужно сделать так чтобы люди которые работали системы они работали с неким единым информационным полем нам нужно обеспечить согласованность данных чтобы они не отличались в этих системах и естественно если мы масштабируемся и сделаем так что у нас не знаю время отклика наших систем станет два раза хуже то бизнес вас не поймет скажете зачем вы это сделали нам стало хуже давайте сделаем лучше мы говорили про лучше они про хуже и вот как решить собственно эти задачи как мы их решали мы об этом поговорим но прежде чем перейти к практике для того чтобы понимать вообще почему мы так решали мы немножко поговорим про теории начнем с краеугольного камня это так называемая cup теорема или теорема breuer а кто не знает про неё подкованная аудитория классно правда это уже так как классика классика все стесняются да есть три показателя это согласованность данных нет у даны у вас в разных географических распределенных инстансах одинаковые это доступность что у вас если один из инстансов падает что общая система продолжает работать это пользователя это не влияет и устойчивость к разделению это когда у вас пропадает связь между вашими инстансами систему тоже не должна это не должно это приводить к отказу в обслуживании и теорема она гласит что вы можете выбрать любые две вещи из этих трех а очень условно как бы классическим подходом считается что мы всегда выбираем против intolerance это просто де-факто потому что если связь между центрами она бывает моргает это нормальная ситуация и если в этом случае он будет а центра операбельной но ваш системы не работают на фиг не нужна такой масштабирование никому не вашим клиентам него мне бизнесу ни кому-либо еще доступность ну наверно это краеугольный камень и вообще в принципе масштабирования поэтому все обычно жертвуют согласованностью как жертвует вводится такое понятие как согласованность конечном счете то есть и венчал консистенции что это значит это значит что данные будут согласованы в ваших системах но когда-нибудь то есть идет некий асинхронный поток взаимодействие вот это репликации данных и в этом асинхронном потоки пользователь взаимодействует а параллельно данные доезжают другой дата-центра если вот пользовательский поток остановить на условном через минуту данные будут консистенции может быть через 10 секунд может быть через секунду зависит от архитектуры ваших систем обычно в те жертву так но зачем нам позже ртуть попробуйте ответить для себя на этот вопрос вот как бы мы когда строили систему нашим и вот этот вопрос себе задавали ни один раз доступностью пожертвовать нельзя согласованностью тоже плохо потому что в рамках проведения одной транзакции нам согласованность сто процентов нужно у нас бывает так вот бы транзакции когда проводите вы покупаете что-нибудь в интернете типичный пример что вы делаете два запроса к провайдеру это запрос когда вы инициируете транзакцию и запрос 2 когда вы после проведения вот ввода 3d skoda в банке которую мы с маской приходит если между этими запросами данные будут не согласованы просто просто после 2 запроса вы получите ошибку а деньги с вас спишутся ваше недовольство будет очень велико я бы сказал крайне очень так аккуратно поэтому вот здесь есть 4 смешной вариант 4 смешная опция товарищ из этой картинки и очень удивляется но не все так плохо есть расширение cup теоремы теорема bios и эльсе который бы раз сформулированного словно в десятом тире двенадцатом году девяностом году оно было опубликовано сформулирован в десятом который говорит что в принципе есть не 34 понятия это устойчивость к разделению доступность согласованность и четвертое понятие это задержки вернее отсутствие этих задержек и в принципе мы можем к нашим двум основным понятиям которые мы выбирают обычно все это партия tolerance and ability добавить третье это либо отсутствие задержек либо согласованность в реальной жизни это как реализуется если мы добавляем согласованность у нас появляются задержки то есть мы в синхронном режиме принимая запрос от пользователя его реплицирует на один или несколько других дата-центров допустимо это или нет зависит от контекста и от ситуации вашей но тут есть нюанс что это работает только при штатной работе всех компонентов если у вас в системе произошел какой-то failover один из дата-центров упал прилег отдохнуть вы на нем проводите плавные работы вот этот дополнительный параметр который стоит после л.с. вы жертвуете у вас остается только два базовых на этом наверно со скучной теории все и переходим к практике к тому как мы с этим работали какие системы мы масштабировались и соответственно как мы это решали как мы задачи решает всего наверное мы масштабировать и по-настоящему четыре системы про одну я рассказывать не буду потому что иначе с ворую хлеб у своего коллеги который выступает завтра это рис control system если интересно приходите на этот доклад а я расскажу про три других систем и это дельта вэ хаус это хранилище данных для аналитической системы для личных кабинетов клиент это payment for processing наш непосредственно моему назван пор ядро и адаптер платежных систем так исторически сложилось что мы ему почему-то называем плюс никто не уже не помнит почему называем соответственно вот в рамках этих трех платежных систем в рамках этих трех компонентов мы будем вместе наш разговор первая система про которую мы поговорим это дает в хаос хранилище данных что это такое но это большая условно можно сказать что это набор баз данных это действительно так которые хранят данные по каждой транзакции которые мы проверим при этом данных огромное количество для понимания объема этих данных мы храним на данный момент по моему почти 400 полей по каждой транзакции эта дата начала дата окончания через какую платежную систему через какой банк какая карта такой клиент из какой страны какой стране магазин в какой стороне карты в общем тысячи таких параметров по этим параметрам у нас строится достаточно сложное аналитическое аналитические расчеты наши клиенты юридические лица получают свои отчеты и одни видят оттуда соответственно информацию своих личных кабинетах зачем мы вообще может абирова ли эту систему масштабирование и по двум причинам первое это повысить отказоустойчивость для понимания вот ну вообще почему вы пришли к этому в этом году у нас случилось два отказа пони нашей вине ну условно пони наши вине они случились потому что наш провайдер немножко прилег отдохнуть в одним в одном случае вообще произошло комичная ситуация там экскаватор ехал и там дорога была глиняная и дождик прошел он подскользнулся и съехал вниз а внизу кабель лежал они в одной трубе реджи экскаватор трубы победил пока завели генераторы пока кинули значит такую штучку значит которая за там соединит эти кабели в общем 40 минут мы немножечко нехорошо отвечали нашим клиентам на проведение транзакций мы поняли что как ты не масштабирую нашей системы не повышая надежность внутренних систем внутри атму все равно есть некие внешние факторы да и нужно избавляться от этих нужно работать с этими внешними факторами если ты хочешь обеспечить надежность а у нас например наше село надежность это четыре девятки при понимании четыре девятки это господи этапом меньше часа в год то есть включая сервисные работы включая все вот-вот в год там имеем право там не оказывать сервис нашему нашим клиентам 36 минут что такое меньше соответственно это первый краеугольный камень он будет звучать во всех абсолютно наших системах что мы масштабируем отказоустойчивость ну и второе это снижение время доставки данных и локальных клиентов как как вообще данный получает наши data warehouse он получает пакеты данных из процессинга каждый пакет о каждом изменении состояния транзакции транзакция меняет состояние на своему the flow жизни примерно 10 раз соответственно примерно 10 пакетов получает девич получает в асинхронном режиме и потом их как-то обрабатывать у себя внутри тоже в асинхронном режиме если условно у нас клиент стоит в другом регионе то без приближения девич к нему у него растет задержка данных эти вот такую рассмотрим некую ситуацию гипотетическую что вы клиент предположим у вас небольшой онлайн магазинчик в котором вы продаете австралийском кенгуру сено за биткоины и вы работаете не только в австралии но и решили выйти на новый рынок вы решили продавать тоже цену за те же биткоины но сибирским кенгуру и значит как провайдер ваших услуг строим новый дата-центр в сибири но почему ну чтоб сибирским кенгуру на полсекунды быстрее все на продавать вот кенгуру счастливы а вы не очень потому что вы эти транзакции в своем личном кабинете видите задержки в две минуты и для того чтобы сделать не только игру счастливых но и вас соответственно мы приближаем девич конечному пользователю то есть к вам ваш регион там где вы обрабатываете транзакции условную сибирь для сибирских кенгуру соответственно это наша задача может обернуть девич требования которым предъявлялись к этому масштабирование и вообще где вещь которую мы должны соответствовать здесь нам немного просто у нас время появления в данных девич до пяти минут это прописано наших целей в реальности ну там до минуты если она стоит в том же регионе где собственно вы работаете устойчивость к разделению естественно обязательно это маст хэв работоспособность при отказе одного до c огромным загрузка ну здесь вот немножко вспоминаем теорию до из первого прямо утверждение сразу становится все просто чем мы можем пожертвовать собой не вариант техподдержкой пожертвовать злые вы не добьетесь техподдержку программистов сохраняем да а может и стиранием по жертву нет нельзя ладно не будем привет тестированием ну да действительно мы можем здесь пожертвовать consistent насти ну то есть мы переходим в режим и венчал consist расти когда мы просто вот исходя из первого утверждения мы говорим что мы данные согласованные будем получим не прям сразу как вот нам данные пришли из наших систем а там не знаю через пять минут форме вот то что здесь написано соответственно как это достигается вот эта схема условно представим себе что у нас есть два это центр один в сибири другой в австралии и в любой из дата-центров пролетает пакет в каждом пакете это обусловлено требуем это практически единственное требование на передачу данных девич у нас есть метка времени это метка времени она с точностью до микросекунд для того чтобы как бы обеспечить достаточно высокую точность и когда этот пакет пролетает дата центр на центр смотрит на эту метку времени и смотрит на дополнительную служебную метку которую исходная система никогда не ставит если этой метке нету тогда это значит это пакет от исходных систем и его просто в асинхронном режиме добавляя метку времени мы реплицирует на другой дата-центр другой дата-центр об о метку дата-центра да спасибо другой этот центр получает этот пакет видят ту же самую метку времени видит уже служебную метку дата-центра понимать что это пакет данных реплицировать не надо и соответственно его записывает здесь возникает вопрос что когда вы работаете в синхронном режиме то у вас данные которые были сгенерированы на некое шкале времени последовательно не в той же последовательности будут доставлены до конечных систем и вот этот эту проблему позволяет нам решать метку времени мы записываем данные с меткой времени там условной т1 к нам допустим потом пролетает более ранней пакет t0 мы сравниваем эту метку времени и более равный по ранней пакет просто отбрасываем потому что у нас уже записаны более свежие данные старые данные нам не совсем актуально при этом система работает multimaster режиме то есть в любой момент по той же самой транзакции теоретически данные могут прийти в другой дата-центр и мы их запишем добавив соответственно опять же ту самую метку дата центра и отправив данный в другой этот центр вот таким вот образом это наверное самая простая система которые мы реализовали к репликации мы собственно и сделали просто центру аппликацию для нашей системы девич следующая система про которую мы поговорим вторая наша система это ядро процессинга ядро процессинга эта система несколько более сложной в данном контексте потому что у нас нет таких допущений у нас нет допущении что мы там условно мы должны согласовывать за пять минут при этом мы должны проводить эти транзакции мы их обязательно должны проводить как можно быстрее и при этом мы должны обеспечивать отказоустойчивость из этого у нас сформулированы требования этот газ устойчивость как я уже сказала она будет везде увеличение мощностей нашего процессинга мы в какой-то момент нашего роста так я сказал сам начале мы столкнулись ситуации что мы убирали сетевую производительность одной площадке у нас внутри нашего процессинга ходит столько много данных такой большой поток что мы в какой-то момент просто уперлись в сеть 1 сетевые интерфейсы поняли что ну как бы масштабироваться в рамках одного дата-центра нам слишком тяжело для понимания в одном дата центре у нас сейчас примерно 10 стоек оборудования то есть это действительно большой объем информации который там ходит и мы вот именно это была одной из причин по которой мы начали расти дата центрами сначала мы уперлись в 1 гигабит потом в десять гигабит сейчас у нас на самых нагруженных участках 40 гигабит там где можно мы оставили подешевле потому что ссора гигабит это очень дорого вот соответственно мы начали вот разворачивали 2 до центра мы увеличиваем мощности и мы сказали что мы начинаем приближаться географически пользователю так как условно те самые тир-1 клиенты они очень требовательны к времени проведения транзакций например один очень одна очень известная таксомоторной компании вернее компания агрегатор который наверно часть из вас своим сутра пользовались для того чтобы сюда доехать у нее например вот если транзакция проводятся за время более трех с половиной секунд начинается небольшое такой шевеление она начинает нервничать и эти нервы они естественно спускаются на провайдеру платежных систем то есть в общем на нас они говорят ребята давайте мы сделаем побыстрее а побыстрее насколько ну они такие ну в идеале чтобы вы проводили платеж на мгновенно вот вот так пальчиками щелк и деньги на нашем счету все маги не бывает ну что ж вы так вам деньги платим его варить маги не бывает поэтому мы всеми доступными способами приближаем как бы сервис клиенту от когда мы запустили второй до центра это замечательное событие случилось буквально месяц назад для процессинга скорость проведения транзакций для пользователей которые приближены к второму до центру снизилась суммарно по вот этому замечательному агрегатор у примерно на четыре десятка секунды то есть условно мы проводили эти транзакции там за 3 1 3 2 секунды начали их проводить за 27 28 секунд что для них является очень существенным это почти 15 процентов во времени проведения транзакций требования которые предъявлялись к масштабированию данной системы это естественно устойчивость к разделению работоспособность при отказе от центру согласованность данных и недопустимость времени обработки транзакций то есть вот вот здесь вот вообще без вариантов выбор пожертвовать только собой но мы подумали прикинули что если мы пожертвуем собой ту тогда вообще чуть никакого не случится и сделали определенные допущения и эти допущения очень важны это значит что по-настоящему в каждом конкретном случае когда вы будете делать такие задачи вы будете решать то у вас тоже будут вроде бы изначально противоречивые условия при которых нельзя построить такую систему но вы начинаете думать начинаете осмыслять бизнес-процессы понимаете что ну вообще-то можно если сделать ограничением эти ограничение сделали работоспособность при отказе дата-центра мы сказали что да мы работаем за исключением тех транзакций которые у нас были в процессе то есть если у нас падает дата-центры при этом транзакция была ну вот в моменте проведение например пошел и нет ни запрос на проведение транзакции и мы там начали общаться с банком для того чтобы предоставить клиенту 3ds страницы в этот момент а центр падает многое что до эту транзакцию мы не завершим клиент получит decline ну то есть у него отобразится отказ на его веб интерфейсе и он просто нажмет повторить еще попадет на другой дата-центр по которому он попал с файлами рам и у него получится как показывает статистика если у клиентов такое случается 99 процентов клиентов нажимает попробовать еще и все хорошо то есть это допустимая практика а согласованность данных опять же мы сказали что только в рамках одной транзакции и это нам дало сразу большое ли преимуществом потому что а если мы согласовываем данной в рамках одной транзакции нам не надо обеспечивать согласованность по всем дата-центром у нас одна транзакция проводится в одном дата-центры соответственно между дата центрами нам подходит модель та самая венчал консистенцию как мы это делали для этого надо немножечко понять из каких данных состоит наш processing вот очень схематично это вот такие вот ботинки две основные ботинки это processing и биллинг в процессинге содержатся все настройки наших клиентов там такие вещи как например в какой банк мы отправим какой платеж это так так называемый роутинг платежей в биллинге содержатся баланса клиентов с которых мы списываем зачитаем комиссию за тебя за те платежи которые видим эти данные об обязательном порядке должны быть репрессирован и между дата центрами потому что у одного клиента эти данные могут быть в разных ну транзакции могут вести в разных дата-центрах и нам важно отслеживать этот баланс в режиме приблизь приближенного реального времени и куча баз данных шар дав которые содержат непосредственно транзакционные данные эти транзакционные данные нам по факту не очень-то нужны в других дата-центрах потому что это уже проведенная транзакция зачем она нам как бы в другом до центре для того чтобы проводить новые транзакции отсюда рождается вот такая схема что нам нужно в принципе реплицировать две базы данных но двусторонний репликации вот вроде бы опять тот самый священный грааль священный грааль двусторонняя репликация multimaster система до берем какую-нибудь стандартную систему которая умеет мастер мастер они же все умеют и все хорошо некую платформу по-настоящему мы изначально у нас базу данных sql мы работаем на пир коне и вроде бы там есть такая штука как геркона класть рада там у успели как он называется да спасибо большое за подсказку коллировать клавы вылетел но галера тоже не умеет делать магии она тоже работает с вами парадигме и галера для того чтобы синхронизировать данные между центрами повышает задержку а мы не можем себе позволить повысить задержку до пользователь черт подери опять магии не случается есть системы класса которые реализуют репликацию синхронной парадигме это к вопросу почему например мы не выбрали какую-то платформу и на ней не построили данные например условная кассандра kassandra это на узкое решение а мы тут у нас наглухо архитектура завязано на sql и в принципе если вы есть определенные сложности построить систему на новость о решениях вы внутри когда ее будете строить выпал сталкивались куча ограничений что там поиск по множественным полям он либо недоступен либо он работает очень медленно это все еще плохо то есть там по первичному ключу то все хорошо . поэтому мы изобрели свой собственный велосипед велосипед работает вот таким образом перед тем как записать каждое изменение данных в базу данных мы просто берем и иску или запрос кладем файлик и формируем так кучу таких файликов для каждая система для каждой базы данных потом эти файлики передаем у друга этот центр и там отдельным процессом мы просто их прогоняй получается такой по такое подобие right ahead logo и по факту это такая statement bass трипле catia но у этой схемы есть несколько существенных недостатков кто мне может назвать хотя бы один ordering решается по-настоящему очень просто мы можем выставить на каждом дата-центре свое смещение тогда ordering у нас не будет накладываться там и помнить полный крутым сортом триплет например недопустимая операция апдейт после до лета но это решается фоловерам репликации да это мы учли есть одна овечка вот с которой мы решили вот что мы не будем бороться мы просто на уровне логики и запретим нет но это обычный флобер если у нас какой то процесс отказывает если мы вал записали а данные не доехали но это просто резервирование на уровне компонентами еще раз не очень понял там проблематику тоже опять же да изменение данных в разных дата-центрах это значит что мы не можем делать апдейты по абсолютному значению если мы реализуем multimaster схему и изменяем одну и ту же строку в обоих дата-центрах то мы можем либо делать инкремент либо делать декремент к чему это приводит это приводит к тому что у нас по факту мульти мастером в постоянном режиме является только биллинг давай топ 9 меняются баланс и там мы изменяем биллинговые баланс и одновременно во всех дата-центрах но мы там построили такую логику что мы изменяем баланса не на абсолютное значение а на дельту на дельту той транзакции которые мы провели либо в плюс либо в минус в зависимости от того что надо а если такие апдейт у нас есть по абсолютному значению они у нас есть они у нас есть например когда изменяем настройки настройки мы должны изменять в одном дата-центр и в принципе в наши парадигме это вполне сработала потому что саппорту в принципе удобней изменять настройки в одном дата-центре нежели сразу в 2 3 4 5 в 10 и просто вот они поменяли условный там не знаю набор роутинга проекта это набор роутинга реплицироваться во все дата центры и встал все хорошо вот таким образом мы например решили задачу репликации основной нашей системы и последнее просто мы сегодня поговорим это про адаптер платежных систем первый вопрос как бы адаптер что это такое вообще вот у нас давайте мой в конце я запомнил ваш вопрос конце отвечу на него на соответственно если вообще адаптер платежных систем зачем он нужен почему там можно репликация вроде бы адаптер это такая вещь которая стоит лишь а стоит ли с вещи реплицировать на множестве на дата-центры очень легко и просто мы просто берём ставим множество инстансов и там нету состояния и все хорошо но есть обычные платежные системы в которой мы отправляем запрос на троне получаем ответ есть платёжная система в которой мы отправляем запрос и потом у них спрашиваем статус а есть примерно 10 процентов платежных систем несколько странных они не позволяют спрашивать статус они возвращают калмыки но при этом они вроде такие прогрессивные синхронные возвращают кубики они не позволяют в запросе и нет нам на проведение транзакций указать куда вернуть callback вот ты приходишь платежную систему мы хотим с вами работать они окей куда возвращать калмыки дайте нам один адрес а у нас 10 дата-центров не волнует сколько у вас дата-центров дайте нам один адрес и начинаешь думать как бы а вот как с этой системы бороться и вот тут-то вот собственно нам и нужно обеспечить эту проценту репликацию соответственно требования для адаптера они все те же устойчивость к разделению работоспособность при отказе до центра и согласованность данных потому что callback приходящий на одну точку к сожалению мы не можем его строитесь на конкретный дата-центр сразу потому что мы не очень понимаем какой конкретный до центра относится а формат кубиков он абсолютно разные и позвонил мы не можем это решить там на точки входа поэтому нам нужно прокинуть это кал бег до приложение и обеспечить согласованность данных между всеми приложениями здесь чем мы можем пожертвовать в этом случае из у нас такие требования загрузил уже да мы можем поддержать вы пожертвовать задержками по-настоящему так я сказал что 10 процентов только таких платежных систем для них мы реализовали синхронную систему репликации данных между дата центрами и да эта система получилась класса портишь intolerance availability and consistency то есть букву л мы убираем мы убираем отсутствие задержек как это работает эта штука репликант называется у нас с вечера мы отправляем запрос платежную систему как только мы его в платежной системе зарегистрировали мы отправляем запрос банк и говорим вот и это транзакция принадлежит этому дата-центра этот switcher sync ронни параллельно отправляет запросы на все наши дата центра ну чтобы не сильно увеличивать сетевую нагрузку у нас там стоит сетевой тайм-аут 100 миллисекунд если за 100 миллисекунд мы не получили ответа социальных дата-центров мы пишем файла верную очередь которая потом реплицируется в асинхронном режиме мы сначала подумали что может быть 100 миллисекунд маловато и как бы нам этого не хватит но по-настоящему как показала практика 90 примерно восемь процентов укладывается в 100 миллисекунд остальные два процента очень быстро реплицируется 8 хроне быстрее чем приходит калмыки действительно работает соответственно когда к нам приходит callback платежный шлюз отправляет нам этот кубик данный падает случайно до цента до центра смотрят что кубик принадлежит не ему и перенаправляет в тот дата-центр который нам нужен итоге всего этого дела основной итог самый главный она работает на этом да можно заканчивать да все все но та мысль до который я хотел до вас донести нашими примерами нет единого правильного решения все зависит от тех требований и ограничений которые вы выставите есть платформы по которой я упомянул тоже кассандра который умеет без задержки latency реплицироваться но это платформа накладывает ограничение что вы работаете с новой сколь базы данных есть галера кластер который работает со сколь базам данных но она добавляет lafense и не факт что она подойдет нам не подошла ни одна из платформ в итоге мы реализовывали собственные велосипеды поэтому вот велосипеды нам помогли в наших конкретных случаях с нашими конкретными решением если вы будете решать такие задачи естественно советую сначала посмотреть на платформы потому что ну не надо будет ничего строить а если все-таки платформа вам не подходят то подумайте над теми допущениями которые вы можете сделать на этом наверно все и мы переходим дискуссии вот сразу там был еще по ходу доклада вопрос про консистентных данных рассогласованность вопрос следующим я для всех повторю что все услышали что произойдет если у нас я так понимаю вопрос по процессингу да давайте я вернусь на слайд с процессингом вот тут что произойдет если вот этот пункт 3 у нас оборвался дата-центра операбельной но пункт 3 как бы не фурычит по-настоящему в данном случае мы наглухо синхронно поэтому файлы передачи файлов это отдельная синхронный процесс если связь прервалась мы просто файл будет передан чуть попозже а так как у нас выполняются вот эти два требования к нам в принципе в принципе все равно когда мы сделаем and abel да мы теоретически допускаем что в случае нарушения портишь на tolerance для наших балансов мы можем на чуть-чуть уйти в минус мы этот риск закладываем в некий вот наши касты что если у нас клиент вдруг уйдет в минус это вот ну какие тут нашего трески и наши касты пока такого слава богу не случалось ответил как вы потом будете эти две базы склеивать но какая разница если у нас апдейты происходит по инкрементальные decree ментальному значению то в любом порядке их можно применять от того что мы условно у нас есть начало -5 потом плюс 3 потом -10 потом плюс 20 от того что вы в математическом выражении поменяете местами эти значения у вас результат не поменяется хорошо тогда такой вопрос а вы не думали но это просто в качестве вопроса иметь какую-то форму на площадку которая будет выбирать кто из них прав и прокурор да наверное смотрите про кворум система кворума они достаточно сложны в своей собственной реализации во первых corum накладывает ограничение в том плане что нам нужно сразу нечетное количество площадок и некое подтверждение либо мы должны выбирать мастер площадку всегда и тогда вопрос сразу а что произойдёт при ее падение пока мы пришли к тому что мы не будем реализовывать механизмы кворума по той простой причине что серьезные тему технологические накладки и мы реализуем экологическими ограничениями но если у нас вдруг бизнес к нам придет и скажет что вот ну невозможно и у нас эти логически ограничение теперь надо поддерживать мы конечно будем реализовать корм знаете получили здравствуйте меня зовут сергей спасибо заклад у меня предложение к вопросу предыдущему а вот допустим при таком sweetbox перед грей не если у клиента недостаточна баланс и допустим не было пятьдесят монет я в одном дата-центре списал тридцать монет и в другом доценты 30 у меня произошло два декремента и стала -10 как вы такие ошибки именно но бизнеса решаете это некий риск который мы закладываем спасибо то есть условно для понимания процентов этого риска относительно там условно дохода бизнеса это там даже не тысячная доля процента сильно меньше этой схеме апдейт только числовые да никаких сильных по факту мы работаем с финансами традиционными данными там только числа то есть это то есть эти механизмы в этой только биллинг и все все все остальное все остальное это двусторонняя репликация по факту там двусторонняя поддерживается но на уровне логическом это некий фолловер нам не нужно реплицировать данные настроек так как мы в одном дата-центре их изменили они применялись во всех сразу же мы практически сразу задержка there's цикл от того что у нас задержка в 30 секунд при роутинга это вполне сопоставимы с человеческим фактором когда он исполнил us apart пойдет руками будет менять давайте у цели стороны вопросы спасибо за доклад а вы говорили что у вас я бы ты происходит по дельте а если там происходит над полями которые не принадлежат флоту и энту и так далее вторая строка все спасибо вот здесь вот руки подняты вижу давайте вы я голосом повторю если что а можно я попрошу прошу прощения вот человек который от нас уходит вот буквально сейчас а можно задержаться до конца вопросов у меня просто есть подарок и пока вот вы лидер в этом подарке вопроса первый раз про провалили вы это все запустить больше ключи на 2 dc какие сложности были и второй вопрос вы используете сиквел а не было желание написать свой дессау чтобы больше были гарантии того что резистентности до этого будет всегда ну что вот например тут есть стенд рядом за дверью там ребята из мой sic volo сделали ну сиквел интерфейс для себя то смогли бы написать там свой идеал и который бы вам гарантировал лучше компетентность апдейтов эскадрилью первый вопрос 3 дата центры 4 дата центра сейчас у нас production 2 дата-центра на тесте их у нас уже больше и мы уже сейчас работаем над тем чтобы дуб и запустить 3 4 до центра и в тестах этапа отработать мы не видим никаких проблем пока про свой собственный diy цель мы непосредственно как бы наверное небольшие ретрограды мы всегда с одной стороны открыты новому нас там если мы говорим про технологии там кафка например есть у нас есть другие модные технологии но как показывает практика то что хорошо в теории не всегда хорошо заходит на практике и про свой собственный dsl когда у нас были некие попытки и парадигма и обсуждать это мы всегда находили какие-то недостатки которые блокировали ну то есть они не видел до сих пор решение которое бы хорошо бы ложилась здесь отлично здесь в догонку опять первого вопроса чем отличается синхронизация от обычной стандартной асинхронной репликации с типом репликации ролл тем что она была в данном случае работает в режиме смотрите у нас если мы посмотрим вот на эти базы данных конечно мозг мускуле включить житья идеи роба из трипле кации 2 в этом средством для базы поставить multimaster но как показывает практика если не дай бог где-то произошло не мгновенное переключение здесь данные добегают а сюда начали писаться уже до момент переключения там и потом эти данные не соберем а в этой схеме это некий другой уровень абстракции который позволяет нам с этими ошибками значительно проще работать но если мы выставляем тепло у нас влогах те же самые запросы мы можем просто продолжить точно также как если за только если у нас не прошел апдейт по одной и той же строке здесь у нас отдает по одной и той же строке для биллинга например ходит кисть следующий вопрос возвращаясь начал вы про то как водят пакеты вы сказали что у вас там в районе 400 полей и получается согласно ваших рыбка потерями вы также теряете в скорости нет не теряем потому что мы доставляем асинхронно это отдельная очередь у нас все синхронно и вы получается гоняйте каждый раз целиком весь объект не один и поля целину магнит смотрите у нас не обязательно объект приходит целиком у нас master system достаточно много которой каждый из которых продуцирует только набор из полей и только одна и ну то есть условно 1 master system пуля не знаю отправляет 20 полей мы эти 20 палитре плюс сыром другая master system другие 30 полей мы эти 30 более реплицирует спасибо спасибо спасибо за доклад у меня скорее не вопрос любопытный hawk из мира фей тех и как не дать клиенту уйти в минус в тот момент когда у нас разваливаются кластер а мы просто делим доступный остаток на сколько осталось до и тогда клиента не можем не дадите в минус при этом спасибо за подсказку на по-настоящему мы думали про некий неснижаемый положительный остаток но мы пришли к выводу то есть мы обсуждали такие возможные варианты мы пришли к выводу что он проще нам давать в некоторых случаях уйти минус давайте ещё два вопроса а дальше перейдет перед перейдем дискуссию поля когда клиентов отправляете в ближнее дата-центры как вы это организуете со стороны клиента о это вопрос наверное один из самых краеугольных и один из самых сложных дело в том что во первых у нас собственно автономная система и бюджеты до соответственно бюджет и маршруту определяет ближайшую точку рисового ближайшую минусом самый оптимальный маршрут на нам можно настроить таким образом соответственно но там есть еще нюанс что у нас условно есть два дата-центр которые для некоторых клиентов равноудаленные и в этом случае мы стараемся такого рода клиентов прибивать gotta центру и это мы делаем тоже на своей стороне то есть условно на точки входа у нас есть некая логика которая позволяет нам строить на точки входа мун клиентов нужно дата-центр есть некая логика по умолчанию где вот по маршрутам это делается а есть логика роутинга когда мы прям прям специально прибиваем конкретного клиента конкретному доцент давайте падение во сколько сколько будет ждать клиент при падении до центра в среднем наверно отмена куда 3 эти последний вопрос я здесь другой стороны я прошу прощения панель уже перейдем да потом да здравствуйте у меня вот вопрос и у вас падает мы все понятно то есть когда у вас биллинг близится все окей а если не черт ее дата-центра развалились связь то что произойдет а insert у нас происходит в данном случае только при настройках по настоящему когда мы настраиваем админ панель и это происходит только в одном в одном над центре если это до центра падает нас админ панель переключается на другой до центра и еще тогда один вопрос у вас же реплицируется только биллинг соответственно сами транзакции потом вообще только в одном дата-центре хранятся или они все-таки как-то догоняют они тоже очень хороший вопрос который мы можем подробнее обсудить в кулуарах по-настоящему нас есть копии транзакционных данных каждом дата-центре но они только рядом ли это есть если хочет посмотреть всю историю по транзакциям то ее собрать достаточно сложно нет у нас есть девич всем спасибо большое друзья у меня тут есть небольшой подарок за лучший вопрос я бы хотела отдать вот такая вот тяжелая книга ей можно капусту квасить базы данных томас канале каролин бег за офигенную дискуссию порожденные вопросам уходит нашему замечательному человеку спасибо большое"
}