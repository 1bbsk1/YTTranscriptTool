{
  "video_id": "vkMts1jp6yU",
  "channel": "HighLoadChannel",
  "title": "Одна платформа, чтобы править всеми / Михаил Кабищев (Ozon)",
  "views": 4413,
  "duration": 3149,
  "published": "2023-04-28T06:19:52-07:00",
  "text": "Привет меня зовут Миша Я работаю в озонтехе я руковожу направление базовых сервисов платформе И сегодня я хочу вам рассказать про платформу озона то какая она есть но прежде чем начать рассказывать про платформу хочется немножко вернуться в историю потому что Озон достаточно взрослая компания Но все что вы слышите и видите последние несколько лет оно появилось не так давно И на самом деле в восемнадцатом году произошла достаточно сильные изменения и до того как они случились можно сказать что вот озона образца восемнадцатого года Это был такой достаточно большой но онлайн магазин книжек и я так думаю многие мои друзья тоже так думали а-а это было не очень большая компания с точки зрения идти нас их на тот момент было меньше чем 100 человек технический стайк был чистый Windows соответственно был Windows на серверах были огромные монолиты msqlos какими-то тысячами хранимых процедур были десктопные приложения на Delphi для того чтобы этим управлять и какие-то прочие прелести жизни скажем так и восемнадцатом году Один мой друг и коллега Женя он тоже где-то здесь думал переходить Мы работали с ним вместе Он хотел переходить в Озон Я очень сильно отговаривал потому что говорю ну серьезно Женя ну как бы ну вот это вот всё как бы вот оно тебе надо вот этот вот там Windows Нет ты будешь что-то переписывать Зачем Ну вот спустя 4 года Женя здесь Я здесь И мы строим вместе большую классную платформу если говорить про зону образца 22 года то из вот этого огромного магазина книжек мы превратились в одного из лидеров ekom мы стали огромным маркетплейсом который продаёт всё что угодно и Кому угодно мы выросли почти в 40 раз уже нас сейчас порядка 4 человек у нас несколько дата-центров тысячи серверов огромное количество наша система разрослась из нескольких больших монолитов в огромную распределенную систему которая стоит из нескольких тысяч сервисов и совокупно на текущий момент Они обрабатывают порядка трех миллионов запросов в секунду весь весь бэкент О чем сегодня хочу поговорить Я хочу говорить про платформу вообще что это такое зачем она нужна и детальнее рассказать про некоторые аспекты того как мы относимся к платформе это некоторые стандарты которые мы вели которые используем и на основе которых строится жизненный цикл разработки диплои и обслуживание сервиса давайте начнем с платформы Зачем вообще она нужна и зачем она нужна в крупной компании а мы хотим чтобы команды вообще разработкой которая есть компании продуктовая разработка чтобы она занималась продуктовыми задачами мы не хотим тратить их время на решение каких-то технических задач особенно связанных с инфраструктурой Мы хотим чтобы сложная инфраструктура использовалась одинаково во всех уголках нашей компании и хотим предоставлять максимально удобное представлять достаточно верхний уровень абстракции одним Мы хотим предоставлять решение для различных популярных кейсов Если одна команда хочет сделать что-то еще 10 команд хотят сделать то же самое мы как платформа можем предоставить Общее решение чтобы все его использовали также мы занимаемся стандартизацией всего и вся начиная от железа заканчивая библиотеками и кодом Мы внедряем у нас есть механизм он постепенно зарождается для крупных изменений чендж-менеджмент опять-таки с точки зрения инфраструктуры и кода и все совокупно это нужно в большой компании для того чтобы снижать тайм-ту Market чтобы наши разработка наша продуктовая разработка работала быстрее и мы как бизнес продолжали дальше развиваться концептуально всю нашу платформу весь наш департамент можно представить виде нескольких слоев сам внизу находится железо это такая условно базовая инфраструктура это железо сеть это те самые дата-центры сети это также некоторые огромные складские комплексы которые у нас есть а там же ребята занимаются так называемой хардарт wabay мы Тестируем разные новые железки Тестируем разные новые процессоры для того чтобы понимать Какое железо наиболее эффективно использовать На текущий момент на основе этой базовой инфраструктуры строятся компьютер свой это вычислительные мощности это виртуалки и соответственно контейнеры cuber и так далее У нас несколько у нас несколько дата-центров несколько кластеров убирай соответственно часть весь бэкент все приложения запускаются в клубе виртуалки мы используем для вас еще для ряда задач которые могут потребоваться выше над компьютер находится сервисный слой это различные сервисы базы данных ши очереди что-то еще Чем могут пользоваться какие-то прикладные сервисы в своей работе потому что сервисы не существует вакууме где-то нужно хранить данные как-то обмениваться ими выше уровнем идет Application слой это библиотеки и фреймворкин основой которых строятся все приложения Ну и выше соответственно находятся все микросервисы все компоненты которые мы строим внутри и можно сказать что платформа это некий технологический фундамент на основе которого строится вообще весь Озон если двигаться по тому плану то первое чем я хочу сказать это конвенции стандарт почему это вообще почему важно большой компании с сложной архитектуре это делать есть вот такая вот цитата неких великих людей я не знаю кто и автор но она достаточно популярная лучше безобразно но однообразно в целом Это конечно шутка но однообразие безумно важно и всегда нужно находить правильный баланс между тем самым однообразием и какой-то увеличением энтропии для того чтобы система не превращалась в хаос вас не было огромного количества технологий языков чего-то еще и для того чтобы снижать вот эту энтропию У нас существует один из основных документов он появился 4 года назад он называется конвекция по микросервисам условно между собой шутим что этого некоторого рода Конституция разработчика вазоне и Этот документ рассказывает о том как мы должны писать сервисы как мы должны их делать как нужно называть сервисы Как нужно версионировать API в них как они должны конфигурироваться используя различные параметры как вообще они должны общаться между собой мы используем gpc для общения между сервисами наружу все сервисы предоставляют http свой а кто-то использует Граф кивель и все вот эти вот пункты что можно использовать в каком случае они описаны в этом документе и также он покрывает очень важную часть связанную с обсервабилити он говорит о том какие какую телеметрию каждый сервис должен отдавать как он должен писать влаги в каком формате какие там должны быть обязательные поля Какие метрики он должен репортить В каком виде как должен как должен работать рейтинг и так далее это супер супер важно потому что без такого документа опять-таки все это превращается в хаос и во время какого любого технического спора стороны не могут договориться но такой документ он всегда позволяет сказать Вот смотри как бы в конвенции написано вот так вот поэтому давай делать Вот так вот и все естественно это не что-то высеченная в камне это достаточно подвижный документ он со временем меняется появляются новые версии мы добавляем и расширяем его помимо конвенции по микросервисам есть более детальная конвенции по языкам и технологиям которые в некотором роде расширяют и добавляют основную конвенцию по сервиса и в этих конвенциях написано какие-то правила относящиеся к языкам программирования например какие версии языков мы можем использовать или компиляторов у нас есть механизм который позволяет ограничивать набор библиотек и их версии каких-то внешних зависимостей и автоматически их обновлять сервисы также мы для некоторых языков есть правило про то как нужно писать код потому что это же любимое дело разработчиков в споре Давай поставим скобочки вот так или вот так а здесь нужны табы здесь пробелы Вот это всё описано в этих документах опять-таки можно очень легко апеллировать Ну и в целом мы можем рекомендовать Давайте использовать например для генерации моков вот такую библиотеку а для тестирования такую на основе этих двух конвенций по микросервисам и по конкретному языку мы можем начинать создание сервиса У нас есть мы используем несколько языков на которых вообще пишем Back and принципе в компании их 5 но только на первых из трех мы пишем те сервисы которые скажем так обрабатывают пользовательский запросы в основном мы используем Go C Sharp и Java JavaScript и поэтому существуют в компании они используются в некоторых нишевых местах но глобально весь бэкента зона написано первых трех языках и для этих языков У нас есть языковая платформа которая представляет из себя генераторы сервисов при запуске которого вы можете скормить ему протофайл потому что мы используем gpc соответственно любой сервис используя имеет свое описание виде прота и после генерации проекта мы получаем правильную структуру проекта мы получаем все нужные системные файлы настройки для кубера g2b еще ряда наших внутренних компонентов У нас есть методы заглушки скажем так это базовая реализация имплементация этих методов у нас полностью сгенерирован весь rpc свой есть же PC есть gpc geedway для того чтобы предоставлять htp свой для goednet Это встроено непосредственно в сами в непосредственно в бинарник в случае Java мы умеем генерировать сайт Car рядом есть свагер для документации есть всякие прикольные штуки можно настраивать Trade limiter Breaker и по всяким умным правилам есть поддержка онлайн конфигурации и есть телеметрия на все случаи жизни то есть любое вообще внешнее взаимодействие покрыто десятками или сотнями онлайн конфигурация это очень классная штука а существуют одно из достаточно популярных мнений о том что давайте использовать 12 факторов Давайте конфигурировать все через переменные но это не очень удобно на наш взгляд потому что такая конфигурация получается статической А некоторые вещи как даже системные какие-то настройки Мы хотим Иметь Иметь хотим иметь возможность модифицировать в онлайне для того чтобы это сделать мы написали небольшую систему которая позволяет описать все нужные вам параметры в неком манифесте это простой Ямал файл причем разработчик у нас как платформа есть набор наших параметров которые мы хотим распространять на все языки так и разработчики могут добавить какие-то свои параметры для их бизнес-логики могут быть какие-то коэффициенты тумблеры что-то еще в итоге этот манифест с помощью простой утилитки загружается видите сиди или волк если нужно хранить секреты и сервис подписывается на эти сети и получает апдейты о том что какой-то параметр поменялся и разработчик может написать очень простой callback и что-то сделать внутри своего приложения а для управления этим и генерируем автоматические такой вот интерфейс в котором есть эти параметры здесь на скриншоте переведён пример для одного из гоночных сервисов есть ряд параметров которые относятся непосредственно к рантам настройки и еще настройки сервера и так далее если разработчик добавляет что-то свое кастомное у него точно также появляется новая строчка можно написать какой-то хелпер можно задать какие-то правила валидации в общем очень классно удобная вещь сервисы не живут вакууме как я уже говорил они должны общаться между собой и для того чтобы это делали то чтобы это общение случилось нам нужно понимать как вообще какие методы есть у другого сервиса Как как взаимодействовать с ним и нам нужны клиенты к ним Да мы используем gpc Да они Легко генерируются но здесь возникает очень серьезный вопрос Если кто-то использовал gpc то вы скорее всего или неважно даже из флагером Скорее всего сталкивались в такую историей Том А где вообще хранить контракты вот у нас 3000 сервисов 3.000 контрактов где их хранить и кто из них должен и каким образом должен генерировать клиенты и четыре года назад когда Мы начинали все это делать Да и даже три года назад было все очень печально мы особо не занимались этой историей разработчики делали так и хотели и была примерно такая картина нас вообще не было никаких правил потому как писать порту файлы люди использовали и писали разделяли их на множество файликов использовали на экспрессии не использовали кто-то генерировал клиенты самостоятельно и мог сделать это Например для гошного сервиса сгенерировать клиентов только на Гоа то как будет жить какая-нибудь команда которая пишет О тут знаете их это мало волновало кто-то копировал протофайлы из чужого репозитория к себе соответственно он даже не то чтобы фиксировал а просто сохранял какой-то стоит на какую-то дату и и все И как бы сервис мог обновляться естественно сервис клиент никак не узнавал про эти обновления и не работал gpc reflection достаточно важная штука потому что в общем все против файлы которые были в сервисе там был полный бардак соответственно reflection не собирался не работал и в двадцатом году мы поняли что сервисов стало так много и это стало такой большой проблемой что мы начали стандартизировать написание proto файлов и генерацию клиентов мы написали достаточно простую на тот момент простую сивая утилиту которая занималась примерно следующим она делала вещи похожие на то как работают менеджеры зависимость для каких-то языков она парсила proto файл понимала какие там есть зависимости можно было специально файлики указать что вот я хочу этот файл из такого репозитория такой-то версии она дальше все это рекурсировано рисовала сформировала локально все нужное дерево протофайлов и дальше занималась генерация уже на основа прота си или буфа тут это зависело уже от команды и также соответственно Этот шаг позволил нам внедрить первые стандарты того как нужно все это делать здесь было несколько проблем потому что вся эта логика была реализована на клиенте А вообще вот проблема зависимости протофайлов она до сих пор не очень хорошо решена и есть какие-то инструменты от убираем самбов пытается что-то сделать но это пока всё такая очень открытая ниша соответственно логика в этой утилите меня у вас достаточно часто потому что находились всякие сложные очки исы и приходилось постоянно ребят Простите обновить эту библиотеку ребята нам жаловались о том что вот что-то там не работает и так далее и так далее и мы решили сделать следующую вещь Мы решили Ага вот у нас как бы это сложный кусок связанный с реализованным Давайте вытащим его из клиента и сделаем максимально сложный клиент сделаем максимально простым сделаем сложный сервер и написали серверную реализацию этого портагена который работает очень просто она принимает есть очень простой тонкий клиент он берет нужный протофайл отправляет на сервер сервер делает все сложную логику и отправляет обратно архив с каталогом proto файлов уже правильно собранным правильных версий который точно сгенерируется нужными нужными плагинами Очень классно и удобно мы до сих пор правда находимся в на этапе его внедрения ещё не все команды на него переехали вот поэтому кажется что это было однозначно правильный шаг и мы будем возможно когда-нибудь штуку тоже потому что я повторюсь что это проблема мало кем решалась после того как мы научились общаться с другими сервисами И вообще как написали бизнес логику нам нужно собрать из-за диплоид наш сервис потому что мы хотим чтобы он делал что-то полезное мы для таких вещей мы используем gitlab мы используем супер активно У нас есть сложные большие пайплайны для всех языков здесь на скриншоте проведен пример пайп Лайна для гошных приложений здесь видно что есть разные шаги мы запускаем линтера мы запускаем тесты мы запускаем проверку некоторых системных файликов мы собираем образы можем задефоить в девелопмент окружение после того как все хорошо мы можем задеплоить его в Production для того чтобы это сделать у нас есть еще более обширный pipline который делает еще более сложные штуки он повторно прогоняет может тестов также он занимается очень важными вещами он пытается найти секреты в коде потому что периодически команды могут оставить какой-нибудь пароль или положить какой-нибудь ключ внутри репозитория и забыть про это закоммитить вот эта штука находят такие вещи говорит Извини вот пожалуйста Сначала удали этот пароль и потом мы только потом мы будем тебя диплоить также она проверяет Security Scan проверяет внешние зависимости на наличие каких-то уязвимостей и наличие вот этих вот штук очень сильно нам помогло весной этого года в свете понятных событий Когда нужно было очень быстро и аккуратно ограждать себя от внешнего мира для того чтобы не скачать каких-нибудь неправильных зависимостей для того чтобы не положить весь продакшн и мы достаточно легко пережили этот период благодаря вот этим вот штукам от команды безопасности вообще про всю эту историю связанную с нашим диплом нашими И общими принципами как доставляется Production будет доклад от моего коллеги Дании он будет завтра приходите послушать там будет интересно очень-очень подробно про многие куски поэтому я в этом докладе особо туда не буду погружаться это все оставлю Даня перейдем к следующей вещи сервис mesh сервис мешает достаточно интересная штука можно даже сказать что это отчасти как молодежный секс все про него говорят но как бы мало кто это действительно делает или все это делают сильно по-разному и в случае озона это тоже У нас есть некая своя реализация наше видение сервис мешает в далеком восемнадцатом году когда мы только начинали строить нашу инфраструктуру у нас появилась uber мы начали смотреть А что есть на рынке и один из перспективных проектов который был на тот момент был линкер д Это прикольная штука твиттера которая позволяет делать запросы глобально это выглядит Так что вот у нас есть пользователь он отправляет какой-то запрос в клубе он попадает через НГС попадает в какой-то первый сервис и дальше сервисы общаются не напрямую друг с другом а через вроде бы все классно здорово но были проблемы мы его крутили и так и так у нас никак не получалось мы получали очень большие задержки эта штука написано на Джаве мы не тепло или ее принципиально виде сайтаров Потому что когда у тебя есть маленький гошные приложения которые занимают десятки мегабайт оперативки рядом вот такая вот жалость история кажется что это не самый лучший вариант Мы пытались сделать не все что могли выкинули его Ну и как бы вот сделали такой вывод что да как бы штука классная его многие любят но мы на самом деле не очень и после того как мы выкинули стал вопрос как бы О'кей что делать потому что сервисы всё равно надо общаться между собой и подумали Окей как бы у нас есть tengress да это встроенный функционал в кубер соответственно Давайте подумаем Так ну как бы условно какая разница откуда пришел запрос от пользователя он пришел в Ingress или другого сервиса и переехали вот на такую схему что есть пользователь он отправляет запрос в НГС попадает в сервис и дальше сервисы ходят в такие же Ingress для того чтобы пойти нужно сервис это все работает из коробки очень классно Здорово некоторые задержки есть потому что есть промежуточные хопы но нас они устраивают жены сработает мы используем gmx для Ingress он работает лучше чем linker D как бы давайте ничего дополнительно делать не будем как бы все классно но команда губерманов была с нами не согласна сказала что как бы ребята ведь не для этого да он для того чтобы правильно приземлить трафик в клубе Всё После этого мы начали разные эксперименты с другими проектами они по разным причинам не увенчались успехом И постепенно стало появляться проблема с задержками работы между сервисами и нужно было ее решать и мы решили поступить следующим образом решили не тревожить команду кубермэнов или решили написать сбоку свою систему Discovery назвали Вардан глобально она очень простая выглядит просто выглядела на тот момент у нас есть сервис Он подписывается на кучу событий с кубера получается из него апдейты строят глобальную карту того где и как задефоин каждый сервис в каком-то центре в какой стойке какая у него версия он предоставляет очень простой API для других сервисов где сервис один может подписаться на изменение сервиса 2 ему будут прилетать Вот теперь сервис 2 на таких-то айпишниках таких портах все очень классно и в двадцатом году мы сделали пилотную реализацию этого проекта запустили ее Она завелась мы поняли что Классно Здорово надо эту штуку развивать что мы получили мы перестал воспользоваться как нечто какой-то промежуточный балансировщик он используется как конгресс у нас нет дополнительных задержек потому что нету лишних хопов трафик летает напрямую между полами и у нас нет никаких сайдкаров которые опять накладывают какие-то какой-то overhead потому что количество контейнеров в случае с аль-карами на каждой машине увеличивается докер установится хуже и так далее так далее В общем вся эта логика она реализована в виде библиотеки которая работает со фишкой вардена и на текущий момент Варда большая сложная история она вышел за пределы только сервисов он предоставляет единый искал Discovery вообще для всего что есть для вас Для различных можно Discovery кластера кавки Он поддерживает разные алгоритмы балансировки можно использовать в этот раунд Робин p2c у разработчиков есть возможность задать какой-то свой кастомный алгоритм если они хотят и на основе вардана и вот этих вот всех плюшек у нас строится канареечные дипломы мы можем очень удобно вручную протестировать новую версию сервиса расскажу про это чуть позже мы вокализуем трафик мы можем настроить систему таким образом что если трафик прилетает какой-то дата-центр то все запросы в другие сервисы будут отправляться в пределах этого дата-центра Либо если ты если команда хочет поступить Иначе она может делать Так что трафик будет гулять между всеми дата центрами и также мы решаем некоторую проблему большого количества сервисов Я тоже про него Расскажу чуть позже к наречный диплои выглядит очень просто соответственно есть сервис 1 есть две версии сервиса 2 мы можем настроить трафик Так что чтобы 90 процентов шло в версию а 10 процентов чего версию B при этом с точки зрения API отдается айпишник Я у каждого дается соответствующий вес поэтому сервис 1 библиотека внутри сервиса 1 знает куда отправить сколько трафик также есть очень классная штука которая позволяет с помощью специального заголовка называется X3 mesh вершин это некоторые Legacy но в общем Она живет с нами уже четыре года мы можем но скорее всего не будем его менять название вообще не принципиально смысл но смысл заключается в следующем что в случае какого-то стандартного запроса трафик идет сервис 1 сервис 2 в дефолтную версию и потом сервис 3 мы можем сказать что вот для моего конкретного запроса Я хочу использовать версию а версию B и я могу отправить этот заголовок и тогда мой трафик пойдет через другую версию этого сервиса Таким образом мы можем протестировать можем выложить новую версию на продакшн и не включая канарейку то есть не раскатывая на какое-то процент живых пользователей потестировать все это дело сами и в том числе мы имеем также делать в том числе из мобильных приложений эта схема может выключить сложнее и она работает на взаимодействие вообще на любом уровне То есть если у нас Граф вызовов или цепочка вызовов стоит из десятка или двух можем на любом уровне сказать Вот пожалуйста в какой-то 15 сервис пойди пожалуйста по альтернативному пути и также мы варден умеет делать сапсеттинг здесь приведем пример когда у нас есть много несколько клиентов и много копий сервиса 2 и на самом деле в большой системе когда их Конечно не единица десятки или сотни нет никакого смысла устраивать такой full-mash когда каждый клиент соединяется с каждым сервером это очень-очень накватно и достаточно каждому клиенту отдать какое-то подмножеством сервиса два для того чтобы они взаимодействовали между собой глобально кажется что очень простая задача потому что их здесь девять там их три как бы поделил одно на другое и всё как бы никаких проблем нет На самом деле алгоритм достаточно сложный непростой потому что это все нужно делать очень аккуратно нужно гарантировать что на каждый сервис 2 будет одинаковая нагрузка и не всегда одно делится на другое Поэтому в бывают некоторые нюансы но эта штука действительно очень полезная потому что любой тип соединения если у вас 100 клиентов и 100 инстансов вы получаете 10 тысяч соединений которые вам на самом деле не нужны а каждое соединение независимо от того На каком языке вы пишите требует какие-то накладные ресурсы в случае Go Это несколько грузин в случае в других языков Всё равно всё равно появляются какие-то абстракции Хотя бы даже память на то чтобы обслуживать это соединение про Discovery мы поговорили Про Сервис поговорили Давайте теперь я расскажу про общеробились Мы очень любим abserbility и мы очень много ресурсов и времени вкладываем в это потому что система очень сложная И несмотря на то что нас так много работает в Озоне Никто из нас не представляет как вся эта система функционирует Вот вот в общем потому что компонентов очень много и ни один человек да вот даже Антон там стоит улыбается дом тоже наверное не до конца понимает как вся эта штука работает каких-то нюансах поэтому нам нужно предоставлять большое количество информации которая поможет разработчикам и нам самим понять Как это всё работает что мы вкладываемся достаточно стандартный набор тем это метрики трейсинг который все больше больше входит в такой стандартный набор того что компания делают многие и континиус профайлинг давайте начнем с метрик что мы снимаем в первую очередь Конечно мы снимаем системные метрики со всех серверов Неважно какая там операционная система мы снимаем метрики с кубера с контейнер D в общем любой любое Все что до чего можно дотянуться руками мы это сохраняем также мы если переходить к приложениям и сохраняем метрики из runtime какого-то языка неважно это город net Java можно накопать и получить огромную информацию о том что то что там происходит внутри сколько потоков Сколько памяти Как часто работает Джесси и так далее так далее У нас есть супер подробные метрики про весь rpc свой то есть про взаимодействие сервис как входящие исходящие как сервис общаются с любыми внешними любыми внешними зависимостями на основе этих метрик за счет того что они унифицированы и описаны вот в тех самых конвенциях мы строим общий дошборды которыми очень легко пользоваться я покажу чуть позже И также у нас есть большое количество базовых алертов которые позволяют командам разработки не думать о том как какую метрику правильно взять или как написать самовыражение им нужно только поставить правильные пороги сказать Вот пожалуйста присылаем мне уведомления если мы сервис не знаю к примеру отвечает дольше 50 миллисекунд или у меня теперь такое здесь приведем пример одного самого дашборда он называется сервисы review это тоже такой Legacy название которое мы когда-то давно 4 года назад придумали здесь показан пример одного сервисов с видно количество РПС количество РПС времена ответов в различных фантилях И вообще этот дашборд он просто гигантский там огромное количество там несколько десятков разных вкладок на все случаи жизни там можно посмотреть грамм зависимости и понятия кто приходит в мой сервис куда хожу Я вам вообще в Какие сервисы можно Посмотреть детальную информацию по хендлерам понять что Ага вот на самом деле вот этот этот хэндлер используется там на него приходят тысячи РПС на все остальные 10 можно посмотреть время и ответа по ним можно понять как вообще сервис работает в разных дата-центрах потому что в разных дата-центрах может стоять разные железо потому что мы их запускаем не одновременно и на самом деле мы тоже сталкивались такую историей что современное железо работает быстрее и в каких-то дата-центрах сервисы работают быстрее чем в других есть информация по каждому протоколу взаимодействия под grpc по htp точно также есть есть метрики из runtime непосредственного языка количество груди запуски Горбач коллектора количество потоков можно посмотреть сколько сервис используют ресурсов CPU памяти сети Как в общем так и в разрезе по повода В общем много много всего есть детальная информация по внешним запросам как мой сервис видит взаимодействие с другим сервисом Ну и в общем-то на основе всех этих метрик строятся вот этот дашборд на который можно медитировать очень большое количество времени потому что там много-много полезной информации и также как я уже говорил у нас есть вот эти базовые базовые ленты как практически по всем слоям они если команда эксплуатации которые следят за тем как стать опечатка которая пропустил затем как работает система как работают сервисы и разработчикам нужно просто в яму файле поменять сказать Ага вот у меня есть аллер на количество грудин и я хочу чтобы он срабатывал когда грузин будет 30.000 или я хочу чтобы альярдное количество ошибок срабатывал когда их будет 2% и так далее В общем обсервабилити это огромный кусок нашей платформы который позволяет понять Вообще как вся эта система взаимодействует трейсинг Мы очень любим трейсинг Я самым долго занимался в Озоне и в 2018 году Мы начинали с ванильного егеря наверное многие из вас за эти годы успели поработать с ним но мы достаточно быстро поняли что как бы Егерь прикольный Но из коробки он скорее как такая некая красивая игрушка поэтому он не удовлетворял Всем нашим потребностям и мы решили переписать его бэкэнд в девятнадцатом году мы целиком переписали бэкент от егеря и оставили от него только интерфейс в двадцатом году Мы перешли на opentley с точки зрения протокола бэкент остался всё также нас и что мы умеем Теперь мы храним абсолютно все трейсы которые происходят все запросы вообще которые происходят на бэкэнде а зона в течение 10-15 минут мы умеем делать умные сэмплирование то есть сохранять эти трейсы по разным параметрам которые могут настраиваться и умеем считать критический путь и многое другое прикольные статистике здесь приведена ссылка на доклад но двадцатого года я рассказывал как раз про то как Мы строили вот на тот момент наш инфраструктуру трейсинга Кому интересно можете посмотреть что мы добавили поверх того что есть из коробки егеря в с точки зрения интерфейса многие из вас я думаю видели Вот эту вот картинку с колбасками вам она может показаться привычным Но из интересного здесь есть вот эти вот черные полосочки внутри которые показывают так называемые критический путь запроса то есть то на что уходило наибольшее время в рамках выполнения всего пользовательского запрос тут переведено формальное определение что э какой-то Спан находится на критическим пути в момент времени T тогда только тогда когда уменьшение его длины в момент времени T уменьшит общее время запроса если говорить очень простым языком то вот на этой картинке Если вы уменьшите время работы D значит весь запрос станет быстрее это очень классно и в понять понять Вот это критический путь на самом деле не всегда просто то есть не строя вот в таком вот виде то Как выполняется ваш запрос Скорее всего вы даже не поймете что вам в первую очередь нужно оптимизировать можете пойти не в нужную в неправильном направлении но смотреть вот такие вот штуки на каком-то одном конкретном трейсе не так интересно поэтому ребята из команды трейсинга решили сделать такой Шаг вперед и построили супер сложный дашборд который представляет критические пути в виде файм графов Он умеет их сравнивать за разные периоды и давать возможность понять Ага вот мне нужно по оптимизировать такую-то такую-то операцию тогда вот все эти запросы станут быстрее он действительно сложный и для того чтобы научиться работать с этой штукой нужно немножко въехать в то как вообще все это работает но это достаточно важная и полезная история и также мы делаем Continuous профайлинг мы собираем регулярно с разных приложений внутреннюю информацию о том как оно работает сколько CPU она тратит Сколько памяти она тратит на что некоторые языки это делают очень просто опять таки Go это предоставляет вообще из коробки поэтому это нужно просто брать сохранять и пользоваться и мы немножко там подшаманили с нашими фреймворками для этой Java для того чтобы тоже это сделать мы раз в 10 минут или в 5 минут уже собираемся всех приложений все эти профили сохраняем их и предоставляем интерфейс который позволяет посмотреть Ага вот значит в таком-то сервисе В такое время э-э CPU тратился вот на такие вещи Это стандартные интерфейс с правой стороны того что предоставляет гошный тулинг В каких случаях вообще это полезно это полезно Когда вы хотите ответить на вопрос А что случилось с моим сервисом на 3 часа ночи когда он упал по он потому что естественно вы спали ничего не осталось он просто при стартился и все Или А что было Вот ко мне пришла большая нагрузка мой сервис очень медленно отвечает на что посепил он уперся в полку и как бы что вообще происходит и вот благодаря этому инструменту можно всегда отмотать историю в прошлое открыть такой профиль и посмотреть что там происходит и у нас есть планы скорости эту историю поэтому мы про это правда говорим уже пару лет но я думаю что это случится Поэтому ждите и последняя большая тема про который хочу поговорить Это менедж сервиса это то чем мы занимаемся последний год что активно вкладываемся какое-то время назад пару лет ходили такие слухи что вот инфра да как часть нашего департамента называли инфрой говорят вот они все очень делают долго и бывали случаи когда команда готовит какой-то большой релиз им нужна база данных и они приходят и говорят А вот ребят у нас релиз завтра запустите нам пожалуйста базу А для того чтобы запустить базу команде нужно было поставить тикет это тики должен был подхватить какое-то дежурный соответственно Он должен был пойти руками создать эту базу а-а скинуть кредиты и так далее так далее В общем как бы было очень-очень много ручной работы и с одной стороны нам не хотелось заниматься это ручной работой и мы хотели сделать так чтобы разработчикам было жить лучше и мы решили давайте сделаем всё по кнопке хочу пост значит я могу нажать кнопочку сделай пожалуйста мне происходит какая-то магия он запускается Ну в общем-то родилась концепция Давайте делать что-то за сервис Неважно Что что это значит что для разработки уменьшается время реагирования Нет какого-то живого человека который занимается твоей задачей разработкой получает мониторинг аллерты такие же красивые дошборды по любой технологии Можно контролировать доступы очень удобно потому что можно делать а для нас из плюсов можно отметить стандартизацию то есть мы как бы уверен что вот вы хотите использовать пост например Используйте его вот так вот мы его представляем именно вот так вот есть различные конфигурации Но их число ограничено и за счёт того что мы именно мы это всё контролируем то мы это делаем начинает железо до конечных библиотек которые умеет работать с какой-то технологией а визуально это выглядит примерно следующим образом у нас есть что-то что мы хотим автоматизировать есть сервисы которые хотят им пользоваться нам нужно написать Control plain который позволяет запускать создавать это что-то а пользователь взаимодействует через itc Its это наш внутренний интерфейс для управления всем облаком так называется Он взаимодействует уже с этим controline очень простая схема как бы ничего вообще нового Мы в целом не придумывали и спустя несколько лет мы на текущий момент умеем предоставлять несколько технологий редисменты предоставлять как просто сингл но дей мы умеем предоставлять шортированные с репликацией сейчас мы готовим планы потому как мы будем запускать персистентный рейдис также в клубе похожая история чуть проще можно запускать создавать топики в Кафки А можно создавать и так далее соответственно добавление ресурсы выглядит крайне просто есть для тех кто работал с какими-нибудь облаками интерфейс может показаться очень знакомым вы выбираете план который говорит сколько ресурсов вы хотите потратить Ну и всё нажимаете кнопочку создать ждете несколько секунд это что-то запускается в данном случае я создавал конечно Ну и все И как бы можно пользоваться берем библиотеку для любого языка говорим и Как называется мой кластер она сама все на Discovery и все все классно работает каждый такой ресурс что важно когда у нас не было всей этой системы и мы делали все вручную все эти тикеты терялись люди которые заводили эти тикеты увольнялись переходили в другую команду у нас вообще не было в какой-то момент мы начали терять контроль о том а кто вообще отвечает за вот такой-то Бакет за счет всей этой системы мы теперь однозначно понимаем что вот этот ресурс он лежит такому-то сервису этот сервис лежит такой-то команде вообще в целом у этой команды есть определённый квоты на запуск каких-то ресурсов благодаря этому мы можем считать бюджет как не в каких-то ядрах трамвая так в чём-то ещё так и в принципе в деньгах и можем заниматься копать и планированием на будущее а для разработчиков потому что огромный плюс что они все это делают супер быстро за несколько секунд вообще без нашей помощи очень классно Ну и заключение что я хочу сказать что для нас как для озона большой компании Да и в целом наверное для многих компаний платформа может быть очень важным фундаментом для всей разработки потому что именно на основе платформы строится все информационные системы которые есть название но при этом не нужно забывать что разработка это главные клиенты все что мы делаем мы делаем не просто потому что мы где-то прочитали это прикольно А потому что мы хотим упростить жизнь всем остальным разработчикам для того чтобы всё это работало проще можно было создавать такие вещи очень важно описывать какие-то стандарты и правила как вы хотите это использовать чтобы это всё не превратилось в хаос Ну если есть какое-то большое желание какое-то команды внедрить что-то новое и как бы вот они это давят давят давят и у вас не получается победить явно заставить использовать что-то другое то вы должны понять что ага ну как бы кажется что всё-таки технология она нужна и давайте мы ее заодно Оптима предоставим точно такой же ноги потому что кажется что это действительно нужно Ну и на этом все большое спасибо что вы послушали мой доклад время вопросов Смотри руки в небо первый вопрос из чатика как при к наречном тестировании происходит миграция базы данных и версии они базы данных а вообще эти процессы они живут условно параллельно значит когда вы хотите поменять какую-то схему изменения схемы базы оно вообще не совсем связано с изменением кода сервисов Если вы хотите например удалить какую-то колонку из базы то вам сначала нужно выпустить новую версию приложения которые перестает работать с этой колонкой раскатить ее целиком на сто процентов что все хорошо только после этого вы можете уже потом удалять саму колонку из базы Ну то есть как бы здесь в целом ничего нового Как бы особенного не используем это просто набор каких-то правил то есть в любой момент времени должен понимать что в своей схемой может работать как старая старая версия приложения так и новая Поэтому нужно очень аккуратно к этому относиться Спасибо здравствуйте Михаил Большое спасибо за доклад у меня такой банальный вопрос А вот когда выбирали сервис-меж Почему не посмотрели сторону истину лир Крид насколько я знаю немножко помпа раньше вышел но допустим 20 году уже можно было но это Control playme Да нужно что дата Plane в случае есть чаще всего и на тот момент времени использовали вой существовала большая проблема так как мастер большой в котором запущено много сервисов немного инстансов то каждый инвой истину накачивал каждый инвое знание обо всем класть это не нужно потому что один сервис общается с каким-то множеством и получалось такая история что конфиг для инвой остановился огромный и опять-таки сайт Car мог занимать больше ресурсов чем реальная соседнее полезное приложение и ребята из команды клубера очень не хотели сильно увеличивать количество контейнеров которые у нас будет потому что у нас достаточно большие тачки на которых мы не используем виртуалки под куберна железные сервера и мы количество контейнеров большое А чем больше контейнеров тем блокеру хуже от этого становится того чтобы за этим следить вот поэтому мы поэкспериментировали с в том числе с инвоем из истину но пошли по пути написания своего решения можно ещё вопрос я правильно понимаю что Варда - это библиотека Да это то есть не сервис это библиотека Она говорит это сервисы библиотека есть серверная часть которая накачивает в себя информацию из кубера из э контроля плейна баз данных она discoveryt всякие штуки прокавку знает про всё всё и соответственно сервис библиотечные реализация оно же ходит в варден для того чтобы подписаться на изменение оттуда данных понял спасибо Будьте добры Добрый день компания RV Vision Сергей я в своей компании Сейчас занимаюсь примерно Тем же чем вы в Озоне но у нас этот процесс только начинается и мой вопрос не совсем технический он скорее такой организационный вы сказали что одна из основных вещей это основная задача это облегчение жизни разработчикам Но что если разработчики как-то не очень рады тому что вот мы пришли сейчас будем облегчать им жизнь то есть мы сталкиваемся иногда с таким скепсисом э иногда даже саботажем то есть там ну прям Широкая такая грань Как вы к этому подходите были у вас есть интересные если решения Ну смотри тут можно подходить с разных сторон значит ВС и такие решения их некоторые решения действительно тяжело внедрять потому что количество людей большое все со своим мнением но тем не менее это нужно внедрить Здесь можно с одной стороны включать некоторую такую социальную инженерию То есть ты как бы ты выпускаешь какой-то апдейт на который Людям нужно потратить какие-то усилия потому что обратно совместимость сломалась что-то еще но при этом ты добавляешь какую-то новую классную фишку фреймворк которую действительно хотят ты не бы портишь её в старую версию добавляешь только в новое Тем самым ты мотивируешь людей естественным образом перейти на неё А это работает не всегда естественно в нашем случае тоже У нас бывает ситуация когда в разных сервисах глобально есть большой хвост зависимости Да у нас есть фреймворк он обновляется обновляется обновляется Ну и могут быть сервисы которые используют какую-то супер старую версию на такой случай У нас есть механизм который позволяет обрубить этих версий ты банально Не сможешь собрать свой проект на какой-то старой версии мы иногда чаще всего мы стараемся делать это очень лояльно у людей всегда есть большое время на обновление Иногда приходится делать достаточно быстро потому что мы находим какой-то баг он достаточно критичный и поэтому нужно всех принудительно обновить здесь опять-таки для того чтобы простить жизнь разработчикам У нас есть таком зачаточном состоянии система который можно сказать пожалуйста найди все сервисы которые используют вот такую вот библиотеку и обнови в них на новую версию он эта штука умеет создавать тикеты делать Джей квесты и разработчикам нужно просто замёрзть и из-за диплоить версию вот ну то есть как бы либо ты занимаешься социальной частью либо включаешь какие-то технические ограничения но они должны быть обоснованы что Да вот мы делаем вот так вот так всё как бы вы по-другому просто не сможете иногда это понимание приходят у команды разработки сейчас через какое-то время что да вот как бы на самом деле так было правильно сделать и Да мы тогда там по ворчали но глобально от этого все выигрывают только вот пожалуйста и потом дальше в кулуары и надо выбрать Кому мы подарим эти сувениры кто владелец конвенции и как происходит ее обновление как вообще происходит решение о том что мы там используем новую технологию обновляемся что такое Значит у нас есть несколько таких органов внутри компании они называются комитеты эти комитеты профильные по разным технологиям есть комитеты по городу Джаве францука и так далее входят в эти комитеты входят представители всех департаментов и крупных вообще кусков компании каждый такой комитет отвечает за свою конвенцию Например я отвечаю за всю историю связанную с Гоа и глобально все все руководители всех этих комитетов отвечают за всю большую конвенцию то есть кто-то изнутри Может предложить какое-то изменение если мы устраиваем обычный mergy квест мы смотрим комментируем обсуждаем это Ну и всё и потом принимается там нету таких жёстких правил пока что вот Нужно две трети голосов там или что-нибудь такое Ну вот оно как-то так всегда достаточно логично и просто решается уже дали микрофон поддержать то мы отобрали пожалуйста Дело в том что Миша По большей части ответил мне кажется на мой вопрос я хотел спросить как вы ограничиваете версию о пенсионной библиотек и состав аппенсации то есть следить или вы за тем чтобы там не образовался зоопарк когда кто-то хочет именно эту библиотеку а не соседнюю такую же классный вопрос Мы последние несколько месяцев супер активного это вкладывались и сейчас чтобы притащить новую версию библиотеки или вообще какую-то новую библиотеку нужно получить обруху и башников и они проверяют по есть много это процессов достаточно автоматизирован но они проверяют большому количеству бас больше конечно внимание уделяется каким-то вообще новым библиотекам которые нужно затаскивать А в случае обновления Возможно это чуть-чуть проще Вот и после того как получается получаем опрос тогда архив с этой библиотекой попадает в зависимости от языка во внутренней репозитории условно у разработчиков нет возможности BIOS система она не может получить доступ в Интернет она ничего не может скачать ни С гитхаба не откуда Извне Вот она скачивается только из внутренних репозиторий А вы не следите за версию этой чтобы все были на какой-то современной У нас есть механизм У нас есть система где можно сказать что вот такое-то пакет а пожалуйста вот мы хотим чтобы он был у всех не ниже чем такой-то версии соответственно если у нас когда добавляется такое правило туда можно добавить дедлайн а Ребята а будут при попытки собрать такое э приложение пока не наступил до 2 будет прилетать уведомление Что вот тогда-то больше будет нельзя это делать А когда-то давай наступил все ты просто не можешь собрать свой проект друзья Спасибо сразу справа на выходе из зала дискуссионная зона где спикер будет памятные призы спикеру и скажи кому значит мне понравилась последний вопрос про версии Так ну потому что да Теперь у тебя не сетевого определяет какой Open Source ты можешь использовать обезопасить и давайте мне понравился вопрос Вот про От молодого человека как вообще заставлять разработчики все делать отлично Спасибо большое спасибо"
}