{
  "video_id": "csZ6IGYs2Kc",
  "channel": "HighLoadChannel",
  "title": "Как построить видеоплатформу на 200 Гбитс / Вячеслав Ольховченков (Integros)",
  "views": 1095,
  "duration": 2558,
  "published": "2017-04-10T02:23:50-07:00",
  "text": "добрый вечер я расскажу сегодня о том как мы это россия умудряемся раздавать 200 гигабит видео ну можно сказать с 9 серверов как у нас тут крепится как я сказал что пики мы умудряемся сейчас раздавать 210 бит в секунду с одного сервера в пике опять же умудряемся раздавать 40 мегабит за сутки раздаём один петабайт это 6 миллионов просмотров видео балансер собственно говоря которой определяет как бы что именно у нас будет смотреть пользователи откуда написан на ruby на рельсах и как это ни удивительно с единственного сервера отдает 1000 запросов в секунду это как и по ним судя потому против да за кладом которые до этого слышал это гораздо быстрее чего на всяких нотах у которых существенно меньше но это получается 60 миллионов запросов в сутки с одного сервера основные компоненты нашей архитектуры это балансер сервера раздачи видео менеджер энкодеры сервера хранения сервер статистики также у нас плеер и имеется собственный то есть клеток может прийти со своим player может воспользоваться нашим который имеет возможность бронированная наш плеер имеет возможность показывать рекламу в наиболее растрате роллах форматах также у нас имеется api для автоматизации встраивания в клиентские сайты структурно это выглядит примерно так можно заметить что на самом деле компонентах очень много и связи между ними достаточно сложный но к раздаче больше большая часть этих компонента отношения не имеет когда клиент приходит он обращается к балансиру если своего player у него нету то есть я сделал стоит на сайте с нашим player он получает с балансира плеер получает манифесты для просмотра треков и идет серверу раздача получает видео по окончании просмотра на сервер статистики отдается информация о том насколько была просмотрена как статистика смотрелось ну и информация о качестве балансира это такая конструкция из собственно 2 и проксей и сервера с аппликациям наруби и не корм postgres rides in ginex все в принципе достаточно просто балансира отправляет статистику о том что происходило на серы мониторинга и статистике это позволяет выбрать g и и иметь информацию о том что у нас происходит с видео основные функции балансира это геотаргетинг статистика выбора сервера раздачи предоставление player клиенту защита от того чтобы всякие хитрые деятели не воровали наши мне тесты и не смогу не могли смотреть видео несанкционированно скажем так тем кто это видео предоставлял также балансиры для плееров которые умеют показывать рекламу можете передавать информацию о том что собственно говоря следует показать балансер крутится на вот такого собственно говоря не очень мощные машинки все 8 гигабайт памяти один-единственный процессор в качестве диска ssd цикл пике утилизируется всего на 75 процентов то есть у нас имеется еще запас при этом насколько прямо это очень высокий результат . рпс на таком железе на рельсах для того чтобы таких результатов достичь мы применили некоторые хитрости наиболее горячие до данные из адреса были перемещены в родис руками были произведены некоторые оптимизации по запросам в некоторых местах пришлось место арман описать вскоре ручками остальные оптимизации уже давали гораздо меньший эффект первые две позволили поднятия производительности где-то раза в три все остальное это уже в общем то можно сказать на стабильность результата следующая компонент основной это серый раздачи когда я собственно говоря в проект пришел сервер раздачи имел вот такую конфигурацию тоже в общем не очень-то мощная и позволял раздавать 5 гигабит в секунду собственно говоря никаких настроек исходно на нем не было все было по дефолту и первое же наблюдение выдала что основная проблема у нас это в том что почему-то при раздаче видео у нас на диске идут на диске идет интенсивная запись что очевидно в общем-то совершенно неразумно и быть так не должен все оказалось очень просто по дефолту стоит а тайма выключаем at times и получаем сразу 8 гбит в секунду но хочется все больше и больше внимательно рассмотрим что мы собственно говоря делаем мы раздаём видеоформате форматах hdr и hls это куча небольших файлов по 300 500 килобайт то есть kings который собственных и раздает вынуждена постоянно открывать и закрывать файлы валики небольшие привлечь он не успевает разогнаться файловая система не может догадаться о том что пусть файл с именем и сегмент один 365 hls мы будем открывать файл 367 л.с. такого интеллекта ним какой файловой системе я общем-то не знаю поэтому очевидное решение это объединить сегменты мелкие сегменты один большой файл у нас сразу будет работать пресечь endings нужно будет открывать гораздо меньше файлов на длинном видео какой-то фильмах часовой трехчасовой это там две три тысячи сегментов и у нас уже 8 громита систему хотите гигабитная ставим следующую еще одну сетевую карту просто иначе мы откроемся носите уху и мы получаем 2 датский для бит в секунду все равно мы в ашдод и упираемся опираемся мы физику в особенности обычных жестких дисков время выполнения запроса жестким диском можно разделить на три части это время позиционирования головки с дорожки на дорожку время ожидания пока у нас диск подойдет под головку и время на собственно говоря передачу данных первое время можно посмотреть даташите винчестера для практически всех тестеров и десктопных серверных на 7 200 р п м три с половиной дюйма это примерно 90 секунд если диск 21 дюйма и будет поменьше 7 вестер п половина оборота это четыре с небольшим или секунды время на передачу это собственно варя размер блока который куска который мы будем читать на 160 мегабайт цикл 106 коми секунду это мы смотрим в даташите типичный трансфер для современных винчестеров то есть получаем что у нас 13 секунд и время трансфера если мы читаем с диска кусками восток глаз 8 килобайт то время трансфера 08 миллисекунды всего у нас магические 72 и abs а про который наверное все знают что и об сорта сатанинские 72 но никто не знает почему почему вот это простая арифметика на уровне четвертого класса 72 и акса и то есть небольшие мегабайт секунду это опять-таки магическая цифра которую все знают что если у нас одесского и up some , в трэша то у нас с него транспорт 10 мегабайт место 160 которая может быть опять таки это не магия это легко читается а если мы будем с диска читать кусками по мегабайт он да у нас время трансфера будет больше 6 небольшой миллисекунд и опусов будет меньше примерно 54 но каждый раз у нас будет гораздо более эффективной мы получим пятьдесят пять мегабайт в секунду почти в пять раз больше с тех же самых дисков ни чего не вкладывая в железо мы можем просто иначе читая получить в пять раз больше производительностью но достичь этого не так уж ты реально дтп с помолчу на тот момент максимальный размер блока имел 128 килобайт архитектурно ctfs может иметь блоки до 16 мегабайт в пол собственной версии 128 килобайт закрытой oracle омской это было о раковом независимо сделано да собственно говоря и того момента еще но публично это никому если не в solaris не солярисе которая тарако платный это недоступно поэтому мы связались с мэтью ареса и по сначала мы конечно сами попробовали на коленочки сделали хак хак убедились что это и сильно работает это действительно дает нужный прирост производительности и мэтью сделал патч который позволяет зад офисе иметь блоки 16 мегабайт размером это было интегрированного при беге номера ревизий для hiit из ты его имеются на слайде ставим мегабайт ные блоки да конечно надо будет переписать всю коллекцию видео заново чтобы у нас блоки стали чтобы у нас файла стали менее размазанными по дискам выключаем ctfs ты привлечь полностью он нам теперь не нужен вообще он только мешать будет и получаем прирост производительности здесь написано 20 копеек мы эти 20 копеек получили не сразу когда мы дошли примерно до 16 мегабит выяснилось что у нас имеются тормоза в ядре в рыбе езде мы стали их исследователи выяснили что во-первых нужно отказаться от гипертрейдинг а энто очень любит продавать гипертрейдинг smt это все в общем примерно одно и то же это судя по всему для плохих компиляторов хорошие компиляторы ресурсы аллу самостоятельно умеют использовать на полную катушку на второе гипер трейдерская и дроу чем ничего не остается scheduler пытается его использовать а там ресурсов не о том гипертрейдинг лучше выключить дальше выясняется что лучше всего размести обработку собственно прерываний на одно на один sucked у нас система 200 лет на то в процессор ну а собственно весь applications иметь на другом и 3 после этого обнаруживаем еще недостаточно оптимизацию параметров их тоже ликвидируем и вот после этого получают 20 газеты и тут уже мы полностью имитируем ся собственно говоря тем что железо не очень мощная память сто 1333 процессора уже устаревший и вообще исходно не очень мощные мы хотим выбрать собственно говоря конфигурацию на следующую итерацию сервера раздачи мы можем выбирать между е3 и пять и три на самом деле нам однозначно не подходит там 30 книга по это лимит архитектурными если хотим раздавать 40 мегабит у нас будет 50 тысяч клиентов мегабайт на буферный клиент 50 гигабайт буферов е3 у чиркаем 1 процессор ная система я 5 или 2 процессор на до процессора тупо дешевле и на 2 processor она будет просто больше официальных салатов для того чтобы подключить периферию больше создав больше дисков и сетевых карт мы уже втыкались то что у нас hdd дает недостаточный транспорт может быть имеет смысл перейти полностью на ssd все хорошо 1 и создал данным даже на данный момент а тогда еще больше 8 раз дороже и одна из sdk в 4 раза меньше по объему то есть если мы хотим иметь тот же объем нам нужно будет в четыре раза больше аж башнях флотов четыре раза больше посадочных местных корпусе и заплатим этой восемь раз дороже кстати приходим к вот такой конфигурации дух процессом они очень мощные xi на 128 гигабайт памяти просто-напросто под буфера и под кэш и 40 гигабайтная карта челси ну просто на тот момент у интела 40 дебитная карта в продаже вообще не было она в было как бы в разработке но выкупить ее было нельзя и 8 ssd под l2 арк ctfs кэша и собственно говоря действительно модель stick получаем 40 гигабит вот полочка упираемся это 50 тысяч клиентов и любят спрашивать сколько у вас пакетов в секунду 5 миллионов для кого-то это доз а для нас это обычная нормальная работа может быть на насосы был но вот этот нож должен быть больше чем 5 мегабит в секунду как минимум несколько раз тогда мы его будем замечать пока не было вторую безо всяких гипертрейдинг а отдельные процессора не горы а именно от камня отдельный камень приложение отдельная камень потолстела карту просто проверь по-разному проверяли вот такой про такая комбинация дает гораздо большей производительностью и когда вот мы на батарейке перешли это третья версия в промежутке было еще во 2 выяснилось что если в bios не отличить numa то производительностью будет процентов на 30 меньше если отключить магические производительности вырастает процентов на 30 не знаю почему это философская настройка что там автора биоса наворотили как это отражается на конфигурация никто не знает но факт и стал через некоторое время мы выяснили что те сервера которые 40 бит отдавали они перестают давайте руки габит ничего не менялось настройки не менялись но вот перестают отдавайте все стали опять исследователь и увидели что у нас опять очень много очень большая нагрузка на hdd почему в первый момент было непонятно потом сумели догадаться у нас проект к этому времени уже имел достаточно большую историю там достаточно много то копилось видео с видео какая ситуация вот его залили его посмотрели первое время его смотрят активно какое-то видео смотрят очень активно какое-то менее активно но в любом случае со временем видео смотрят все менее и менее активно то что было давно уже никого не интересует всех этих только свежие все неважно что но свеженькая то есть у нас на дисках хранится контента имеющий разных характеристик есть тоже только что залитая горячая есть только что залитая не очень популярная есть залитое давно бывший по очень популярным но сейчас уже потерявшая популярность и вот то что у нас популярность потеряла но еще достаточно активно смотрится она мешает этому контенту который вот совсем горячий совсем популярный который особенно говоря основной трафик и делает если мы этот контент с сервера раздачи куда-то в другое место переместим где он не будет особом мешать то у нас performance существенно возрастет но как мы можем этот контент собственно говоря определите выделить у нас с сервера раздачи в сутки там но не знаю 50 тысяч единиц может осмотреться а всего на нём хранится больше сотни тысяч единиц контента не индивидуально же его тома packard выискивать вот можно сделать следующим образом по оси x у нас отложены единицы контента которые кто-то смотрит они отсортированы по тому сколько за сутки этот один конкретный день тут выведен из них было отдано байт чем pipe больше о дано тем ближе она к нулю а дальше ну вот единица контента 300 килобайт на 300 килобайт сдвинулись гигабайта были сделаешь а по вертикальной оси у нас отложено сколько доля того сколько раз его свой собственный размер был отдан наружу в интернет самое популярное единица контента за сутки себя отдала 32 тысячи 800 раз ну на самом деле специально выбрал день когда был залит очень популярный контент и вот этот идите популярного контента она вот как раз такой популярности имела начнем смотреть просто непрерывно с утра до ночи несколько потоков и так далее а самая непопулярная единица контента но и это только тронули тут же закрыли то есть на вот один на 10 минус 5 это вообще как то не серьезно и в промежутке много чего логарифмических координатах имеется такая замечательная криво им до всего у нас на этом сервере раздача 143 гигабайта контента за сутки вообще хоть как-то потрогали только 28 143 терабайта за сутки потрогали сегодня 28 терабайт то есть еще 14 терабайт потрогали он всего у нас 28 терабайт то есть почти 10 то работник то вообще даже не под не подумал их посмотреть за эти сутки они просто там лежат и занимают места вот наиболее интересные кусок предыдущего графика чуть покрупнее здесь можно много интересного увидеть у нас вот есть какие сгущения дело то что у нас единица контента хранятся и отдаются в разных разрешениях с разным битрейтом и от их можно здесь увидеть также тут можно увидеть две горизонтальных прямых одна идет на уровне единицы то есть у нас имеется упорное желание большое число единиц контента посмотреть ровно один раз есть еще чуть выше она на уровне дойки почему тут один и два раза достаточно часто смотрят даже очень редкие видео но вот их и смотрят сначала до конца больше всего там это вот то что один раз посмотрели что там смотрят по два раза это я не знаю почему кому-то этот наверно может пригодиться и сделать из этого могут какие то наверно очень хорошие интересные выводы например то что сначала там жена посмотрела видео дома в нее но что потом муж пришёл с работы они посмотрели вдвоем или еще что то вот тот хвостик который у нас ниже единицы находится но не очень низко нужно то что там совсем внизу она в общем практически не мешает но что такое на взяли потрогали и не серьезно то что у нас чуть ниже единицы она вот как раз нам мешает его от него трафика не очень много но вот она при этом и еще не кэшируется то что вот его взяли посмотрели и все если мы его выделен и переместим на другой сервер вот зелененькое то мы сейчас вот так вот его выделяем мы еще совершенством алгоритм поиска этого теплого контента то у нас производительности восстановится при этом по гигабайтом поттера поэтом это не очень большая доля по трафику это вообще 10 процентов или меньше производительность просаживает в разы после того как мы это сделаем вот у нас сверху суточный график суммарный 210 и есть 18 гигабит в пике и за неделю то есть можно убедиться что это не какой-то там специальный там напряжение то достаточно стабильная для нас нагрузка еще один вариант повысить отдачу это использовать или технологии point-to-point основанную на какой ли вы портите здесь мы сотрудничаем с нашими французскими партнерами с на его рот они работают не только с нами они еще работают с телевещанием ими сетями франции по их данным вот для конкретного суточного дня есть суточный график синяя часть это то что идет с наших серверов красная часть это то что идет с клиентов между клиентами то есть видно что можно выиграть 67 процентов это довольно неплохо как это работает клиент который имеет возможность воспользоваться в процессе после 1 кации на нашем балансире получает с player ещё инфа возможности обратиться extreme рот трекеру и получить смело список перов где он может получить по и процессе сегменты которые хочет посмотреть если эти сегменты он успевает получить поппер rtc то он их и смотрит если не успевает если у него есть заряжающий буфер на чтение он слишком быстро исчерпывается инет через wifi процессе пополняться он этот сегмент запрашивать с наших серверов то есть никакого никакого того случая что вера прт сил ничего не получила и ничего не посмотрел быть не может в крайнем случае он посмотрит все по старинке с наших серверов имеются некоторые проблемы в августе когда это дело в очередной раз испытывалось получалось так что при замечательно в общем-то в центе of law до часть клиентов не могли просмотреть все таки видео объяснил что это проблема на стыке flash player и java script которых собственно и реализует обращение по веб rtc на данный момент эта проблема пофикшен а java script интерфейсе но лучше всего flash все-таки окончательно ликвидировать и перейти на html5 player player варианты имеются они должны вроде как полностью быть своими схемами и реализовывать все нужные нам функция но пока еще у нас не проверен на это наши партнеры имеют такие планы на 2016 год практически полная поддержка любых устройств и еще большая эффективность по отводу на данный момент по их данным и firefox и опера свет rtc совместимы если ты не только хром но и вашего основные доступные плееры в будущем году еще и все мобильные устройства и всякие телевизионной приставки если кто интересуется у нас имеется еще доклады записи можно посмотреть также как можно к нам обращаться наши мои коллеги находятся в зале узнать можно по футболки можно писать на почту если у вас имеются вопросы я с удовольствием на них отвечу такой немножко сисадмин ский вопрос вы много сказали про оптимизации на серверах и про 40 гигабитные карточки это все хорошо что по поводу сетевого оборудования дата-центре не знаю ваш неважно то центр как там вот какое хорошее оборудование выдает такие результаты том дата-центре в котором мы стоим оборудование джунипер пой моя смертная специальность это сетевой инженер в принципе такое же такие же результаты будут и с оборудованием cisco и с оборудованием huawei у них у всех примерно одинакового качества оборудования вопрос то к чему больше привык у кого лучше скидки на покупку какого оборудования россия спасибо у меня такой вопрос как вы думаете еще насколько реальна реализация пир toupper видео обмена между мобильными устройствами наши коллеги считают что реально не тронет использование промежуточного сервера там промежуточный сервер работает исключительно трекером он просто поддерживает он как федором как торрент он просто имеет список клиентов у которых есть эти сегменты трафик через него не идет хорошо а что является основной проблемой в реализации на сегодняшний момент почему это следующем году хотя вроде как у нас там уже 2015-м l5 там все уже тут я к сожалению не могу сказать потому что эта часть полностью разрабатываются ими вероятно тут какие-то проблемы совместимости my вероятно тут проблема в том что не все мобильные браузеры еще и реализовывать в процессе это мои чисто предположения не секрет что firefox в общем то где то только по моему с 39 40 версии в процессе и включил в дефолтную поставку стабильных релизов поэтому он с этим тут тоже может быть вопрос а если не через браузер а через одели мобильное приложение с поддержкой уже всех необходимых функций наша модель такая что мы не требуем установки мобильного приложения я понял спасибо здравствуйте о контингенте лицо и искать дать ей using записать достаточно от езды от проекта и изотов с using далтон сложен и с нею со стока про песни вершин зато в духе это не stability чувствует готовы пафосным новости было video streaming сау и делим cookie jam on the way to do you сам caldav да это ты тупике еще деткам про швы у языка из индекса tab и note нет did applications and don't ask об решено видео том compressed and all content edited on a blood это можно немножко не по теме вопрос я здесь на первом ряду кто вам дает такие каналы 200 гигабит чтобы просто раздавать или этого разные дата центры и разные рэнс плиз название и оспа и на голландия на на русском может да просто я посмотрел на рельсы и это голландский to the center is визе . имеется на доске объявлений здравствуйте спасибо за доклад ваши сервера раздачи они на free bsd дам можно ли было бы достичь таких же результатов на линуксе если да то почему выбор при везде если нет то понимаете когда мне говорят linux и диск я сразу вспоминаю магические цифры 12 309 вот это 1 момент 2 момент у меня есть коллега который пробовал за tfs на линуксе он до сих пор она это плюется я не думаю что другие системы могли бы вот мы как минимум kit для 95 процентов остальных систем и не думаю что я сумел бы получить от 1 мегабайт ные блоки по моему xfs еще того может в любом случае с этой сессии еще есть совершенно замечательно работающий арк кэш благодаря ему мы от 10 т тут все гигабит напрягаемся отдавать прямо из памяти сэр горячий контент которая считается памяти при этом никак не вымывается менее горячим контентом насколько я знаю linux такого не позволяет а интеграция ctfs и система памяти приобрести гораздо лучше чем в linux здравствуйте я из компании rutube и мы можем дать до 700 гигабит и вопрос в общем такой чем вы сегментируйте видео то есть вот хвс хдс и все такое у нас пресс сегментация с помощью early видео ну плюс sonic можно еще раз повторить продукт макса лапшина коммерчески да не уверен это лучше спросите моих коллег коммерческая версия там или нет а д м вы какой-нибудь используете на данный момент нет в планах есть обсуждается но пока более точно сказать не мало после большая да здравствуйте меня еще один вопрос от моего коллеги которые не прямо сейчас здесь но вот прям заинтересовался уже вот на в момент балансировки нагрузки до там применяется какая-то вот я не знаю п.ф. пока шеффилд рынка или другая система вот именно управлением балансировкой нагрузки балансировка нагрузки у нас идет исключительно выбором это сервера то есть у нас нету точки через которого проходит трафик после ja от эджа он идет прямо на клиента все-таки 200 гигабит пропускать через одну единственную железку это как-то слишком дорогая железка должна быть то есть вот мы выбрали сервер который будет отдавать контент то что они могут отдавать до 40 гигабит позволяет достаточно либерально относиться к тому как мы собственно говоря выбираем сервера если вот мы немножко томата промажем мы их не насытим небольшими ошибками а вот такая вот балансировка через пакет фильтра и так далее она как мне кажется имеет смысл только когда нам нужно очень точно балансировать когда мы можем просто выбрать сервера и считают что вот у нас там 5 10 гигабит туда-сюда нас и не выбьют из рамок мы можем просто клиентов сразу отправлять на нужный сервер чтоб они оттуда перебрали тогда вопрос вдогонку во что произойдет если сервер умрет на данный момент у нас сервер они умирают я понимаю вашу и запущенности мы планируем просто имейте копию видео на нескольких серверах и просто отправлять на другие"
}