{
  "video_id": "tfGavlTZRyo",
  "channel": "HighLoadChannel",
  "title": "TopGun = архитектура терабитной DPI-платформы / Леонид Юрьев (Positive Technologies)",
  "views": 315,
  "duration": 2986,
  "published": "2017-04-22T14:47:45-07:00",
  "text": "миллионами пакетов в секунду в одну сторону на одном канале. Что делать, если этих каналов много? Что делать, если нам там ничего не надо потерять? Как надо травить туда рой программный соответствующего размера? чтобы поддержать все сессии. И в общем-то нам тоже кажется, что это настоящий хайлот, только относящийся не к конечным сервисам, не к конечным сервисам, а к, собственно, сетям передачи данных пакетной. И я думаю, будет интересно. Тут вот такая вот картиночка Big Brother. Вот Леониду тут организаторы пошутили написали в качестве компании Big Brother Matrix Randab. На самом деле мы представляем компанию Pтерсервис, которая давно на телеком рынке оперирует, но мы будем в основном не про себя рассказывать, а про решение. Конференция у нас техническая. Я пока вот так вот вас развлекаю. Пока официальное время доклады подходят, подходят, подходит. И уже, наверное, подошло. Надеюсь, все заняли свои места. И передаю слово докладчику. Леонид Юрьев будет. Пусть вам будет и страшно, и интересно. Всё, у меня есть. А, ну хлопайте. А всех приветствую. Значит, я буду со шпаргалкой, потому что мне так проще, информации много, и надеюсь, так мозг не вынесу или вынесу, но меньше. А значит, доклад об архитектуретной платформы, о DPI. Значит, тема DPI такая немножко скользкая, непонятная. Все думают, что это большой брат и ничего больше. А мы отчасти с этим согласны, но рассказ будет о платформе. И, собственно, я тоже затрону бизнес-кейс и покажу, что в DPI полезного, кроме, а, большого брата. Итак, ещё раз. Наша цель небольшой брат, хоть у нас картинка такая эпотажная. Наша цель платформа, которая даёт много-много плюсов именно бизнес-кейсов с точки зрения применения DPI. А значит, пару слов о себе. Ну, зовут меня уже представился. 20 лет я программирую, что-то изобретаю. Получается, иногда не велосипеды, есть координаты для связи на слайде. Значит, если хотите по похантить, посмотрите ценник, ужаснитесь, ну и останемся друзьями. А о компании Pтерсервис, значит, чтоколько Питерсервис? примерно 20 лет хайлода, решение для крупных операторов связи, а разработчиков, ну, наверное, больше, чем в этом зале сейчас. Ну, остальное на слайде вы видите. Не хочу больше углубляться. Приходите к нам, нас будет интересно. А тема доклада, в смысле, структура доклада будет примерно следующая. Будет некое введение. Затрону некие автопики. Они вот будут помечаться таким штампом. Поясню, зачем и почему мы решили делать топга, с каких целей мы хотим достигнуть. Потом будет основная часть, а, и завершающая. Всё довольно просто. Значит, первый автопик. Что такое DPI? А, ну, Вики даёт ответ. Повторяться не буду. Не все, наверное, в теме. Вот кратко схема такая представлена. Значит, это некая коробочка, которая устанавливается оператора. Ну, коробочка, конечно, в кавычках. Она мониторит проходящие пакеты с какой-то целью, что-то делает, что-то делает полезное. В принципе, работает она сырыми пакетами, ни с чем-то. И, ну, что ещё? Есть две схемы включения в разрыв, и тогда коробочка может контролировать трафик. А, либо она включается в копь в режиме на копию и тогда просто наблюдает. А, ну и вот такой термин есть flow. Значит, он часто используется. Нужно просто понимать, что это такое. Значит, Flow - это последовательность пакетов, которая примерно, грубо соответствуют соединению TCP там или DP в зависимости от контекста. А кому и зачем нужен DPI? Вообще кейсов очень много. Это тоже Автопик. О нём мы будем говорить отдельно. Вот кратко, что бы я хотел сказать, что как только мы смотрим в пакет больше адресов, то есть посмотрели адреса и что-то ещё, это на самом деле уже DPI. И когда мы говорим вот о защите, да, то есть то же самое вот атаки, защита от доз, антивирус у вас домашний, который там что-то смотрит, это DPI, значит, а фильтрация какая-то имеется в виду детский, домашний интернет, то есть родительский контроль. Это тоже DPI. Значит, а, ну, шейпинг, шейпинг. Все тоже есть такая плохая у него как бы карма. Значит, все считают, что вот операторы связи, они там зажимают трафик, не дают нам жить. На самом деле дело не так. Надо воспринимать правильно. У нас есть рынок, у нас есть бизнес. И, в принципе, шейпинг - это как бы возможность получить гибкие тарифы. Ну, то есть, например, торенто работают, но не мешают. И вот таких кейсов, на самом деле, их масса. Вот здесь представлены, значит, такие более-менее понятные. Повторяться я больше не буду. Пойдём дальше. Насколько законно, этично? А, ну уже говорил, что наша позиция - это если что мы называем DP DP. То есть если мы смотрим на на данные на подходящие пакеты больше, чем просто на атрис, то это уже DPI. И самое главное, с какой целью мы это делаем и как мы используем результат. Обсуждать этичность вот этого дела нужно именно по результату. С какой целью это делается? Значит, здесь тоже кратко представлено, что и как. А, ну, наша наш тезис, наша позиция, давайте не вешать ярлыки. Всё зависит прежде всего от того, с какой целью мы это делаем. Это вот крайне важно, потому что, ну, есть такое негативное отношение, да, и меня тоже троллили, спрашивали: \"Вот куда ты полез?\" То есть дело в бизнес-кейсах, дело с целью, которой всё это делается. А сложность задач. Значит, давайте кратко поймём, из чего всё это может складываться, из чего складывается задача, из чего складывается её сложность. А, ну, во-первых, у нас Speed, то есть все пакеты у нас идут на скорости проводов. Мы подключаемся к инфраструктуре оператора. Это значит примерно 15 млн пакетов на каждые 10 гигабит трафика. А канал у нас дуплексный, значит, данные идут в каждую сторону. Ну, соответственно, на самом деле 14 млн там с копейками - это когда пакеты маленькие, когда какая-то досатака, но на это нужно рассчитывать. То есть вот такой темп пакетов мы должны суметь обработать. Естественно, что реальные каналы там не 10 гигабит, а гораздо больше. Поэтому всё это нужно домножать. То есть 30 млн умножить на что-то. А далее отказоустойчивость. Всё ломается, оборудование выходит из строя, проливается кофе. А если DPI система начинает сбоить и что-то с ней происходит, то вообще полный фейл происходит. Отключается интернет, интернет не работает, большая проблема для бизнеса, потеря денег. Совершенно понятно, что система должна быть отказоустойчивость. Отказоустойчивой. То есть, что бы не происходило с её железом, она должна работать. Дальше трафика, как я говорю, у нас будет много. То есть вот этих N по 10 гиб будет именно N, а потребуется какая-то какое-то масштабирование, балансировка. И для полноты масштаба, для понимания нужно вот до конца осознать, что это не просто пакеты, да, а это те самые флоу, те самые соединения. Их много, их миллионы будут уже. Десятки тысяч будут новых в секунду, то есть будут появляться, закрываться. И вот со всем этим ужасом нам что нужно сделать? Нам, например, нужно реализовать защиту от ДОС, а, то есть промониторить, посмотреть, что это за соединение, что это за протокол, чтоТП, нетп, принять решение, что с ним делать. Родительский контроль, то же самое, посмотреть, что у нас внутри. А, урлы профильтровать. А, ну, тарифы, вот торренты фоне, да, то есть тоже нужно понять, что это торренты там или ещё что-то. Сделать шейпинг. Опять-таки представим, что у нас этих флоу там 10 млн идёт, там 30, 50, 100 млн пакетов в секунду. Огромное количество флоу. Нам нужно для них сделать шейпинг. Если кто в Линуксе с этим сталкивается, они понимают, что эта задача вот так вообще не решается. А мы это умеем делать. А готовность к разным задачам. Вот важный момент, потому что нужно понимать вообще, с какой целью мы всё это делаем. Это важный пункт. Попытаюсь его сейчас чуть чуть развить. Но если пафосно, всё меняется, возникают новые задачи, значит, мы должны быть к ним готовы. А если сказать попроще, тот код, который мы написали, та наша платформа, которая сделана, мы не должны её каждый раз переписывать. То есть мы должны что-то получить, что некую модульность, некое повторное использование. Это, на самом деле, большая проблема в мире DPI, потому что таких систем фактически нет, забегая вперёд. Ну и вот куча ещё всего, в принципе. полный кошмар, но это челлендж, это такой хайлот реальный. Поэтому на самом деле задача интересна. И опять-таки скажу, кейс насчёт этичности, всего прочего зависит от бизнеса, от применения. Поэтому мне, как инженеру, как такому вот маньяку техническому, это очень интересно, и я этим занялся. Поехали дальше. Зачем нужен топ первый DPI? Почему мы решили сделать, значит, нашу платформу? С чего мы начали? Но если кратко, потому что у конкурентов есть проблемы. Какие проблемы? Вот если избавиться от мелочей, то их в основном две. Значит, зависимость от оборудования. А вндерлок, ну, прежде всего хардверный, нетиражная электроника. Это вроде бы всем понятно. Значит, стоимость, риски незаменимости, уникальности. А, но есть такая ещё проблема, как закладки. Вот откуда берутся закладки и что это такое. Вообще отдельная тема. Не могу сейчас в неё погружаться. Хочу донести такую мысль, что если у вас стоит оборудование и вы не можете его поменять, ну, потому что вот целиком решение к нему привязано, то вероятность наличия в нём закладок гораздо больше, потому что есть смысл их поставить. А следующий пункт, значит, вот нет готовности к разным задачам. Чуть немножко разовью, поясню, что о чём идёт речь. Ну, если кратко, много есть различных решений DPIх. И, как правило, их невозможно адаптировать под новые какие-то задачи. Вот хороший кейс, там написано, это когда есть некий акселератор аппаратных регулярных выражений, что на самом деле внутри себя представляет многие DPI решения. И вот когда нужно к нему добавить, значит, обработку распаковка кзипа. А, как правило, это приводит к тому, что всё нужно переделывать, невозможно ничего сделать. А следующий пункт, нет эластичности, вычислительной сложности. Это вот большая отдельная тема. Дальше сейчас поясню, что мы об этом подразумеваем. А здесь хочу вот пометить, что что вот про эти проблемы вам никто не скажет. То есть никто не сознает, что они есть. Никакой вендер никогда ничто вам про это не скажет до тех пор, пока вы лоб не не расшибёте. Уже после того, как деньги заплатили, конечно. И ещё один тезис. Значит, аппаратный DPI - это ерунда. Его нет. Это ерунда из девяностых. Многие люди заблуждаются. Если хотите, потом аргументированно в кулуарах докажу, что, ну, аргументами закатаю в асфальт, что это вот именно так и есть. А нет эластичности вычислительной сложности. Вот что мы под этим подразумеваем. Значит, ну, мы говорим про софтверный DPI, как вы поняли, да? То есть софверное решение. И де-факто мы имеем два варианта вот реализации таких подхода. А old school, условно я назвал. Что это такое? Это у нас некий generic интерфейс. Большой оверхд получается на обмен данными огромный часто. И большой количество пакетов здесь никак не обработать. Оверхд большой. В итоге мы как бы можем что-то добавить, сделать удобно для себя, да? Так и делаем. А получается в итоге код, который уже никогда не сможет работать быстро. То есть его нужно переписать. В целом это решение приходит подходит для задач, где много неких вычислений. То есть вот некий анализ нужно сделать, в котором много вычислений, а пакетов мало, иначе это будут сплошие накладные расходы. А другое решение - это вот следующая как бы волна. А, а теперь PPS, я назвал его условно. А в результате осознания проблем, которые есть на в старой школе, значит, пытаемся их решить. Работаем на низком уровне с неким DM рингом, например, то есть кольцо ДМА дескрипторов. Равихеда у нас практически нет. Считаем так. рекордный PPS. В общем, такой пулемёт. А, всё хорошо, но трудно очень это кодировать. Много ошибок, просто сложно, дорого. Значит, получается, что, в принципе, это работает, но подходит для задач, где мало вычислений, а много пакетов. Если пропорция другая, мы как бы стреляем с пушки по воробьям, наши все усилия уходят в песок, то есть мы экономим на спичках. А, ну и вот там попугаи в секунду, да, это очень условно изображено. Ну, имеется в виду пакеты в секунду примерно, которые мы можем обработать. Вот эти цифры сильно зависят от того, что и как, на какой платформе, какие сетевые карты. Ну, примерно порядок вот так. То есть 500.000 в секунду на обычном Linux хосте каком-то крутом, ну, может быть, миллион или там 50 млн. Вывод какой отсюда, что на самом деле, когда начинаем какую-то разработку, нужно выбрать один из этих вариантов и не посягать на задачей другого. Вот суть этого слайда была в этом. Поехали дальше. Наши цели, что мы хотим сделать, чего хотим достигнуть, на какую луну мы хотим сесть. Вот в тезих это выглядит так: свободно, без привязки к конкретному железу на оборудовании, на любом оборудовании. А эластично вот решить те самые все проблемы, которые я обозначил. уметь адаптироваться под новую задачу, просто добавляя серверо сервера в кластер. Взяли, подключили, оно работает эффективно. Значит, мы условили, что за всю универсальность и красоту мы вот должны платить не больше 10%. Ну, имеется в виду производительность. Может быть меньше, может, чуть больше, но примерно 10%. Ну, дальше там масса параметров. И в принципе мы хотим получить платформу, то есть некую, некий фреймворк, некий повторно мо используемый модульный код. Вот это всё довольно сложно и круто. А, но мы попробовали, у нас получилось. Вот рассказываем, значит, следующий такой, а, собственно, что сделано, каков у нас продукт. Если краткий, краткий ответ, мы запилили прототип и пришли рассказать, поделиться, как у нас всё хорошо, какие мы умные. Topgun - это не конкретная какая-то разработка, не продукт. Это прежде всего resarch с выходом в продуктовую разработку. Поэтому мы сейчас не готовы раскрывать продуктовые детали, открывать все карты. Приходите, мы вам расскажем детально, но, в принципе, это автопик сейчас. Но с другой стороны какие-то цифры назвать надо. Вот мы их можем назвать какие-то параметры. Значит, ширина канала, ну, сколько потребуется. То есть берём, подключаем. Хотите рабит, пожалуйста, терабит. Хотите 10, пожалуйста, 10. пакета в секунду, потому что иначе смысла нет вообще. Задержка, мы говорим, имеется в виду задержка, это задержка на обработку пакета, кото вот когда он входит в нашу систему, что-то с ним происходит и получается, значит, выходит наруже. Мы говорим про а примерно 100 микросекунд, то есть 1 миллисекунды. Такие верхняя граница сверху на деле будет меньше. И на самом деле сильно сильно зависит от бизнес-кейса, от того, что нужно сделать. И есть такой параметр: загрузка на выход с ядра. То есть сколько пакетов у нас одно ядро может обработать такой показатель внутренней эффективности? Мы говорим, что 90% от рекордов примерно. Рекорды имеется в виду, что есть Intel DPDK. Кто в теме знает, значит, довольно крутая вещь. И мы говорим, что у нас на 10% хуже без прифеча. Почему без прифеча? Потому что отдельный разговор. Спрашивайте, расскажу. Фух. Значит, вот прошли мы как бы в некую вводную часть. Подходим к сути. Ну и девист Обгана, собственно, из фильма. Я хотел ставить сюда логотип. Ну, копирайт нельзя. Если кто помнит, был фильм такой с Том Крузом, и там они пели песенку. Скоуп платформы. А такой вводный тоже слайд. Хочу пояснить, о чём мы будем говорить, а что оставляем за скобками. Значит, схематично показана вот такая схема установки DPI. Сверху у нас, а, инфраструктура оператора, схематично показаны, как ходят пакетики. Есть там такой фьюз предохранитель. Значит, его задача входит что если у нас система сбоит или её нужно обслуживать, он включается, включает сквозной канал, включает байпас. Вот. Кроме фьюза этого предохранителя, мы не будем рассматривать ещё мониторинг управления. Это слева. И некие внешние системы, интерфейс с ними, например, билинг. То есть мы это не рассматриваем, просто говорим о красном прямоугольнике в центре. Что внутри? Что вот о чём могу смогу, успею рассказать ещё? Или основные слагаемые. Значит, плюшек на самом деле больше. Вот то, что мы успеем распробовать. Я выделил четыре, то есть три некие компонента основные основные идеи и некий транспорт. У нас есть инфраструктурный компонент. Значит, информации всё равно много. пришлось утоптать. Надеюсь, что будет понятно. Переходим к сути. Анатомия скелета. Что куда подключено? Такая схема, которая поясняет грубо, э, ну, что с чем у нас соединено. А вот этой схеме 50 лет примерно или даже больше, точно сказать не могу. А переключатель опять инфраструктура оператора ниже две копии системы. Значит, сразу поясню, почему две копии, чтобы не подумали плохого. Вторая копия нужна только тогда, когда нам нужна полная гарантия отсутствия потерь на случай там ядерной войны или брошенной гранаты. Если второй копии нет, что может произойти? Ну, вырубилось питание, выдернули провод, топором попали в блейд. Значит, всё, что летело, вот все пакетики, которые были в проводах или что у него было в памяти, вот это мы потеряем. Потом всё восстановится, всё заработает, но какие-то потери минимальные будут. Если мы хотим избавиться и от этого, вот тогда нужна вторая копия. Именно поэтому она здесь нарисована. Ну и, собственно, как работает переключатель в этом случае? Значит, тот трафик, который поступает к нам на обработку, он поступает параллельно в две копии, а результат забирается с активной копией системы. И вот переключатель это именно делает. Ну, дальше теперь пойдём сверху вниз. У нас есть некий модуль условно распределения трафика. Сейчас про него расскажу. Трафик распределили. Потом есть Dataп Plane. Dataп - это такая шина данных. Ну, классическое название. У нас это коммутаторы, в принципе, любые, лишь бы работали. Дальше блейды обрабатывающие. Не обязательно блейды какие-то, серверы. Обычно корзины, конечно, удобно. На на блейдах крутятся наши обработчики. обработчики - это некие такие процессы Линукса, а, которые просто молотят данные молотилки. Ниже ещё, а, Ctrlplay - это тоже switch. Control Plan - это сигнальная шина. У нас здесь управления и репликации. Дальше поясню. Ещё ниже супер супервайзер. Что делает супервизор? Значит, это такая штука, которая следит за ситуацией и в итоге командует переключателем, какую систему включить. А значит, в реальности всё будет чуть сложнее. Вот эта схема, которая изображена, она имеет изъян. Здесь сам переключатель и супервайзер - это как бы потенциальная точка отказа. Рассматривать вопросы мы не будем. Значит, если мы у нас есть понимание, как эти вопросы решить, но они связаны с тем, как мы будем подключаться к в инфраструктуру оператора. А на практике мы этого не делали, есть только соображения, поэтому как бы говорить, наверное, рано. Ну и что? Вот прошу вернуть слайд. Ага. Седьмой пункт. Чем вот наша Что мы в эту схему привнесли принципиально нового? Вот там такой перешек снизу, циферка7 красный. Вот это наше добавление. Именно этот перешек позволяет нам реплицировать реплицироваться между копиями системы и позволяет достичь полной отказоустойчивости. Пойдём дальше. Что есть балансировка? Значит, балансировка в нашем понимании - это распределение трафика. Да. Плюс управление этим распределением. Вот мы разбили эту задачу на две. И этот слайд иллюстрирует важный принцип того, что чему нужно следовать DPI. Значит, нужно вначале подумать, что делать, а потом быстро-быстро фигачить. Вот это именно так происходит. То есть мы управление, где мы думаем, вынесли отдельно вот распределение, где мы фигачим. Итак, начнём распределение. Значит, распределение мы делаем за счёт макроврайта и распределяем трафик коммутаторами. Значит, в чём как бы идея? Идея в том, что для того, чтобы балансировать, нужно порезать трафик на некие полосы и потом эти полосы распределить через управляющими блейдами. Что мы делаем? Буквально магия. Считаем хэш от IP. Берём два байта, пишем в конец мака, остальное заполняем константой. Ну, это всё условно. Вы поймите, что не буквально так, конечно. В итоге мы получаем вот если как буквально как написано, получаем 65.000 сегментов трафика полос. А и каждая уже связано с отдельным макадресом. Дальше, что мы делаем? Мы логически вот эти отдельно отдельно вот эти сегменты логически распределяем через распределяем между нашими обрабатывающими серверами, между блейдами, раздаём им, а потом говорим блейдам: \"Пингуйте-ка вы наш датап от имени этих макдресов\". Собственно, всё, схема заработала. Значит, что происходит? В коммутаторах заполняется ассоциативная таблица макна порт, и они начинают трафик разруливать. Всё замечательно работает. Всё. Надеюсь, всё понятно. Будут вопросы. Если будут вопросы, задавайте в конце. А какие тут плюсы? Какие бонусы? Ну, вообще их масса, я их забываю часто, что их так много. Значит, молоненосная управляемость. Вот эта схема, она, в принципе, всё статично, но легко управляется. То есть любой блейд отправляет какой-то пин. У нас всё поменялось, поменялась матриц коммутации. Мгновенный файловер. То есть, если у нас где-то отвалился кабель и выключился блейд, коммутатор переходит в bradcast режим, это выйдет другой блейд, который потенциально стоит файлове, да, то есть ждёт в резерве. он этот трафик принаправляет на себя и всё это занимает какие-то там доли миллисекунды. Ну и на нас работает весь тоже довольно много там всего есть. То есть агрегация там вот фреймame тоже интересная вещь. На самом деле, если бade завис, например, нуel pic, да, у нас произошёл, то что что получится? сетевая карта увидит, что процессор не выбирает, не обслуживает DMН, кольцо дескрипторов, отправит в сторону коммутатора пауфрейм, коммутатор увидит, что, например, этот пауфрейм стоит там, ну, там десятую долю миллисекунды и скажет всёдау и тоже всё переключится. То есть вот эта вся схема целиком, она очень проста, элементарна и работает на ура. Причём для того, чтобы её расширить, нужно просто прийти, подключить новый коммутатор, втыкнуть провод, и у нас всё работает. Ну, дальше. А в этой бочке мёда есть ложка дёкт. Вот операция макроврай, она тут самая бы такая неудобная. Значит, сама по себе операция простая, да, но никакого оборудования стандартного готового вот тиражного для того, чтобы её выполнить, нет. Мы нашли три способа, как это можно сделать. А первый способ - это Openflow. Ну, что такое Openflow? Аopic рассказывать не буду. Есть интернет. Посмотрите, какие там минусы. Значит, фишка в том, что операция установить установки макандреса есть не во всех коммутаторах с поддержкой Openflow. Это не написано часто в документации. И если позвонить даже в саппорт, саппорт не понимает, о чём вы спрашиваете. Вот так вот такой такая есть магия. А дальше хэш операции как таковой там нету. Вот эту нашу marре пош, то есть таблицу се, прошу прощения. Значит, иллюзию хэширования мы делаем через определение табличной функции. То есть нужно задать много-много правил. Чтобы это сделать, потребуется аl, потребуется кампамять, скорее всего. И в принципе фишка в том, результат в том, что вместо вот этих 65к сегментов мы в большинстве случаев получаем 1.000 или даже меньше. А, но это сильно зависит, крайне сильно зависит от модели коммутатора. И, в общем, всё меняется. Ну, следующий пункт. Есть наши друзья изористы, у них есть application switch и есть ещё всяческие решение на FPGA. Работает это на урач хорошо. А единственная проблема это против нашей религии, как бы против вот есть вен. В остальном всё замечательно. И третий способ, так называемый ПОВ. Это, пожалуй, серебряная пуля. Значит, что это такое? Это расширение, предложение по расширению Open Flow. И там есть такой хак. Можно посчитать контрольную сумму, например, от IP, записать в Mac и получить вот то, что нам нужно. А резюме какое? Значит, продержаться можно. Ожидаем подкрепления. В принципе, проблема решаемая. Дальше мне надо немножко ускоряться. Значит, интеллект роя - это наш подход. В чём суть? Перовой интеллект - это не какой-то конкретный алгоритм, это эффект. эффект поведения децентрализованной системы. А его бонус в том, что достигается именно децентрализация и некая самоорганизация. Много есть принципов, на основе которых он строится. Я вот здесь выписал три такие наиболее релевантные. Зачитывать не буду, дальше они будут проиллюстрированы. Никакой магии здесь нет, никакого ноухау нет, ничего нового. известно, лет всё это 20-30, но на самом деле при правильном применении это даёт порождает системе новое качество. Вот она, они именно рождаются. А балансировка, а точнее управление балансировкой, управление распределением трафика у нас идёт через рой. Значит, как мы это делаем? А, ну довольно просто. Пункт первый. Мы оцениваем обработчики. То есть на каж для каждого обработчика мы считаем некую метрику метрику здоровья. Сколько у него там памяти, есть ли признаки проблем. Дальше мы оцениваем его связанность с роем. То есть мы проверяем, входят ли данные, какие задержки. А дальше вот это всё мы делаем перекрёстно. То есть оценка происходит перекрёстно. Грубо говоря, каждый сервер в рой, да, составляет такую таблицу перекрёстную, насколько хорошо он слышит или видит там всех других. В итоге у нас получается такая перепись. Получается перепись. Значит, есть много тонкостей. Не хочу сейчас их как бы озвучивать. Мы все проблемы, думаю, решим до конца. Значит, следующий пункт. Что как у нас происходит балансировка? Балансирует на самом деле каждый. То есть у нас получается на каждом сервере получается некий локальный список роя, список всех отдельных его соседей. И дальше каждый сервер независимо берёт и выбирает себе сегменты, те сегменты, с которыми он хочет работать. Алгоритм таков, что если сервер в принципе знает всех остальных других, то никогда не будет конфликта, они никогда не подерутся. Даже если будет конфликт, то на самом деле он не страшен, потому что у нас вот вся схема управления такая, что всё будет хорошо. Ну, это можно углубляться, лучше потом отвечу на вопросы. А, ну деталей, пару деталей. Значит, хэшринг мы не используем, конситентное хэширование просто потому, что сегментов мало. Это ни к чему. И балансируем мы не по ядрам, не по серверам. Ну, потому что тоже всё может быть меняться. Какие какие плюсы? Вот плюсы здесь тоже масса их. Во-первых, у нас получается постоянно самодиагностика. То есть система постоянно себя автопилотит, и мы всегда видим карту того, что внутри. Отказоустойчиво, децентрализовано. То есть никакой точки отказа нет. Э, с этим всё сказано. И всё это легко мониторить. Для того, чтобы это мониторить, на систему не нужно воздействовать. То есть мы просто берём, подключаемся в любую точку контролплейна и видим всю ситуацию. То есть мы не инвазивно, не оказывая воздействия, легко моментально мониторим вот просто подключившись. Это тоже важно. Далее, таб табло для роя. Значит, что такое табло? Ну, это на самом деле такое оперативное, вообще доступное хранилище. Значит, это простой KV value, а, но все записи в нём версионированы. То есть есть некий номер версии какой-то, например, просто можно использовать номер пакетов, счётчик пакетов, но есть тонкости, это зависит от задачи. В этом как бы ноу-хау. Но в итоге мы записи версионируем. Дальше это заточено всё подрокопи, там в ша в шаренной памяти и кфри. Много технических деталей. Это работает. У нас есть отдельный транспорт для этого. Дальше мы всё это табло репрецируем. Ну как мы поддержим реплики? То есть буквально просто обработчик делает обновление, одновременно идёт некий анонс уведомления по контроплейну, а все остальные это видят, слышат, загружают реплику. Если она пришла, загружают уведомление, если версия пришла более новая, чем хранится, то обновляется. Ну, в общем, такой простейший мультимастер, только своими руками. Никаких проблем тут нету, потому что, э, линейное версионирование идёт. То есть сложностей никаких нету. Сложности именно с версиями, а не с смто самим таблом. Э, поехали дальше. А значит, рой и обработка. Ой, прошу прощения, по-моему, я пере Да. Обработка роя. Значит, что у нас? Как это всё работает? Вот для чего табло сейчас станет понятно. Значит, как мы делаем, точнее говоря, что вот как мы приходим к этому моменту. Значит, у нас трафик мы распределили, нужно его обработать. Значит, первое, что мы делаем, мы трансформируем задачи. Значит, мы их сводим, а, к модели виртуальной микромашины. А, ну опять-таки, DPI система что делает? Идут пакетики, мы их принимаем, обрабатываем, делаем что-то полезное. Мы надо смотрим по-другому. У нас есть виртуальная машина, которая получает события, меняет состояние, в результате обработки получается новое состояние. Вот мы всё, что мы делаем, мы берём, значит, это состояние, представляем в виде KV value и версионируем его. Опять-таки версионируем связи с событиями, с пакетами. Самый простейший способ, способ, как сказал, выработать номер версии, просто пакеты пронумеровать. Но деталей много, отдельное ноухау именно здесь. Дальше что мы делаем? Обработчики мы приводим в stat. Они работают очень просто. Значит, они хранят состояние на табло и, в принципе, работают в цикле. Получили события, посмотрели наше табло, локальную реплику, что там хранится, вытащили оттуда, обновили, ну, обработали, вытолкнули состояние назад. Основная идея в этом. В итоге обработчики не хранят ничего ценного внутри. Они работают по принципу роя. Если обработчик срубает по какой-то причине, то что-то остаётся в реплике, и часто это именно самая последняя версия. Вот кратко работает примерно так. А какие здесь как бы есть компромиссы? Здесь плюсы, минусы. Ну, во-первых, те те, кто с такой схемой работал, скажу, сразу скажут, что трафик по Ctrl Panel будет огромный и вообще вы умрёте. Всё решаемое. Значит, мы просто можем регулировать этот трафик, оповеща, отправляя повещение о репликациях реже. Ну, не каждое, там каждое второе или как захотим. А при этом как бы мы увеличиваем временной лак на репликацию. Если что-то упадёт, то мы потеряем больше. Есть такой баланс, опять-таки. А дальше объём реплик. Объём реплик вот в тех масштабах DP, да, то есть когда у нас 100 млн там соединений, он будет огромен. Поэтому памяти, на самом деле, даже если она дешёвая, будет очень нужно много. Поэтому объём реплик мы тоже, на самом деле, легко уменьшаем. Как мы нам не нужно хранить всё в локальной реплике, мы должны хранить только своё и в принципе то, что нам нужно для файловера. Ну то есть есть первая, а первая линия балансировки, вторая на случае отказа первая и так далее. Достигается это при, а не достигается, точнего, а распределяются эти линии, скажем так, такие этажи при раздаче сегментов. И таким образом нам достаточно хранить там первый, второй этаж. Ну если сильно хочется, то третий, но объём реплики при этом уменьшается там на порядке. Обычно там примерно в 100 раз. Какие тонкости? Значит, TCP окно, ну, приходится носить этот чемодан для того, чтобы собирать TCP-се. Есть тонкости, мы это делаем. В итоге всё равно нужно его носить. TCP секвенсы, значит, они нам как раз-таки дают два измерения, две линейки версии в каждую сторону, потому что поток данных дуплексный. А, и у обработчика, на самом деле, есть локальный такой вбкэш через отдельную очередь в реплику. Ну вот примерно как на схеме показано. Не хочу грузить деталями, могу то могу пояснить. А это вот до этого я рассказал три большие вещи. Три большие вещи. Дальше у нас есть свой транспорт. Такой инфраструктурный компонент - это такой бульон, в котором всё плавает. Значит, почему свой? Почему транспорт? Вообще, что это такое? Значит, если посмотреть на нашу задачу, что нам потребовалось? Нам потребовался некий обмен сообщениями, но мегаэффективный. Вот. И в принципе он ориентирован на обмен через разделяемую память. Плюс, значит, а какая-то бесшовная такая лёгкая интеграция с такими специфическими вещами, как Intel, DPDK, Netmap, MPI и так далее. MPI - это Infin Band, например. Ну, кроме этого ещё хочется обмениваться процессорами, там по сети ЗРМQ. Ну, в принципе, основной кейс - это как бы суперэффективный обмен сообщениями в разделяемой памяти. Вот оттуда мы как бы начали дизайнить. А значит, Zero Copy, шаренная память, log free и PI - это наследование приоритетов. Это вот как бы основные тезисы, на которых всё построено. А дальше есть некий интерфейсный фасад, под которым мы подвели несколько, значит, транспортных библиотек. А, ну тот же самый DPDK, MPI там, так далее. и так далее. Интерфейсный фасад на самом деле у нас очень простой, потому что как такового транспорта нет, потому что всё, что буквально из транспорта - это просто пересылка сообщений. Интегрируется всё легко с поправкой на то, что реальные сложности возникают, когда нужно клеить буфера, а локаторы, управления буферами, разделяемой памятью. Ну вот это правда наш конёк. Что у нас ещё есть? У нас есть мегаэффективные операции со строками и итераторами, именно со строками данными, с чанками. примерно то, как это сделано внутри BSD. В этом очень сделано эффективно. И за счёт этого мы получаем именно те бонусы. То есть мы можем, получаем ту самую эластичность. Что ещё там есть? Есть всякие фишки. Например, мы, конечно же, можем включить контроль а этих буферов наших в самом различном варианте. на тот случай, что если кто-то записал мимо памяти или, значит, ошибся с подсчётом ссылок, потому что там, конечно, разделяемая память, нужно считать ссылки на объекты, ссылки на разделяемого буфера и так далее. Вот это всё контролируется. Дальше есть save mode. Что такое save mode? Это мы можем а взять и выключить обмен через разделяемую память. То есть мы говорим, что прямой при помого доступа нет, пусть будет копирование, зато будет безопасно и надёжно. Это тоже баланс. То есть таким образом мы можем изолировать ошибки. И такой девиз: \"Сложности в отдельный процесс\". Что имеется в виду? А, например, в ЗРМQ есть свои очереди, там ещё они где-то есть свои треды, масса каких-то других вещей, с которыми сложно интегрироваться. Мы говорим: \"А мы не будем это делать. У нас есть разделяемая память, у нас общие буфера\". Поэтому все эти сложности, они пусть там живут отдельно в отдельном процессе по своим правилам, что угодно там происходит, а мы работаем с данными, у нас всё круто. И вот это спасает по полной. А транспорт, что у нас внутри? Ну, много я уже рассказал. Там есть офсет ПТР, концерт буфера как куска памяти, значит, fre. Ну, в общем, на самом деле много деталей, там схема нарисована. Но когда готовлся к докладу, я в итоге понял, что вот это всё рассказывать нельзя и не надо. Я хочу сделать вот так. А, то есть действительно есть идея сделать доклад на на этот по этому транспорту через год. И более того, мы хотим, по крайней мере, пока есть намерение, выступи выпустить его под LDPL лицензии или какой-то другой открытой лицензии. И вот тогда будет как раз-таки всё в кассу, всё в тему. Можно будет и рассказать более подробней, будет документация, будет код, поиграться там и так далее, и так далее. А, ну, собственно, о чём мы ещё думаем? Мы думаем о том, что можно подключить это карлангу. Правда, там всё равно есть потеря эффективности, но главное там уже не эффективность, главное в удобстве. Можно подключить к другим языкам, ко всяческим. А это хорошо мо можно будет подключить к различным акселераторам. Ну X или Xion, как правильно назвать, файлера, есть процессоры, там матрица целая и так далее. То есть это, в принципе, всё подключается, легко работает. Мы, в принципе, легко стыкуемся, с дымая диалогией. У нас offset PTR, поэтому интеграции с процессорами, когда у нас другое адресное пространство, у нас нет проблем. Это если будут какие-то вопросы. Ну, немножко подходим уже подходим к завершению. У нас получается круто, короче говоря, господа, значит, масштабируемость. То есть я хочу подвести некие итоги того, что у нас внутри вот по этим по этой технической части. Значит, распределяем трафик коммутаторами. говорить ограничениях производительности бесполезно. То есть всё, что можно, любой трафик, который есть, он всё равно идёт через коммутаторы, мы сможем его обработать. Поэтому, значит, макрой есть трудности, но они все преодолимы. А управление у нас зацентрализовано, узких мест мы не видим, единой точки отказа мы не видим. То есть мы считаем, что у нас круто, у нас всё хорошо. А значит, дальше эластичность и эффективность. очень много всё зависит, упирается в транспорт и гибкость вот достигается за счёт тех самых 10%, ну, условно, да, которые мы готовы заплатить. Мы можем в результате при нехватке серверов, при нехватке мощностей просто подключить сервера. Вот просто буквально взять их, подключить, и всё будет работать. А, ну и доступ, вот там есть другие варианты, что включить safe mode, балансировать между скоростью и надёжностью. То есть это это есть это та самая эластичность. баланс между надёжностью и стоимостью. Ну, мы считаем, что у нас нет единой точки отказа с поправкой вот на переключате или на супервизор, но опять-таки это всё решается, и просто не хотелось бы сейчас об этом говорить. Мы можем, значит, балансировать ещё между стоимостью и надёжности, уменьшая трафик по Ctrl Panel по Ctrl Plan, а за счёт этого как бы снижая стоимость оборудования. И мы можем уменьшить объём реплики и снизить стоимость за счёт того, что нам нужно меньше памяти. С другой стороны, если нужна полная надёжность без потерь, пожалуйста, гарантия. Ставим зеркало, всё хорошо. Ну и платформа, у нас есть понимание, как её строить. Есть некая вот идеология, есть определённые трудности, есть определённый ноу-хау, но в целом, опять-таки, мы видим путь, по нему идём, ну, и считаем, что у нас всё круто. А техническая часть доклада как бы закончена основная. Значит, в самом начале я говорил, что много есть смежных тем, юзкейсы, бизнес и законность, много-много всего. Поэтому, а, мы решили всё это вынести отсюда, сделать отдельное мероприятие. Значит, программу мы сейчас его готовим. В декабре будет семинар, он будет. Детали, пожалуйста, вот там должны быть какие-то уже пояснены. Если есть предложение по программе, хотите выступить с докладом или просто прийти, поучаствовать, послушать, welcome, мы вас ждём. Далее чекпоинт. Совсем чекпоинт перед вопросами наши выводы. Значит, DPI нужен. Он нужен как бизнесу, так и государству. Проблемы этики здесь стоят отдельно. Это юзкейсы. А топга у нас интересная, сложная задача. У нас получается всё круто. Значит, у нас будет масштабирование, у нас есть эластичность, у нас есть, значит, баланс между стоимостью и надёжностью. Мы знаем, как его развивать. Вопросы? Амин Александр. Вопрос вопросов два. Первый. Леонит лаг используете? Если да, в каком объёме и как собираетесь отказываться? Значит, вообще был ещё вопрос отдельный. А пробовали мы на пирабите? Заранее поднимайте руку, я к следующему буду подбегать. Наверное, вопрос я его как бы раскрою, расширю. Значит, а на терабите буквально мы не не включались. Почему? Потому что нужно собрать стенд нехилый, потому что просто это дорого. Вот поэтому мы этого не вот терабитни мы не включали. А дальше мы просто исходим из того, что нет у нас привязки к лагу, да? То есть а вот какая-то конкретная схема внедрения будет, сколько это конкретных каналов, там всё, пожалуйста. О'кей. Привязки к лагу нет. Тогда второй вопрос. Скажите, что предпочитаете? Много процессоров, мало ядер, либо много ядер, мало процессоров. И почему? Без нума. Безнумы и мипс в перспективе. Добрый день. Я прямо за колонной от вас с правой стороны. Смотрите сюда. А, сюда. Прошу прощения. Зеркально по Хотел спросить такую вещь. А как вы решаете конфликты, когда дво два обработчика хватаются за один сегмент одновременно? Ну, смотрите, что будет. Во-первых, такого такая ситуация довольно маловероятна, потому что они вступают в балансировку, когда уже получили список роя, синхронизировались, там есть тайм-стемпы. Ну то есть вероятность того, что будет такая коллизия, в принципе, условия для неё, они крайне ничтожно мала. Дальше, даже если коллизия будет, то что у нас получится? А у нас один сервер пинганёт, потом второй пинганёт. Вот на самом деле работают-то коммутаторы. Тот, кто пинганул чуть позже, тот и остане. У того и тапки будут. Это как бы принцип управления коммутаторами. В общем, проблемы нет. Скажите такой вопрос. Вот самого интересного хотелось бы услышать. Я здесь. Угу. А какие модули у вас делают расшифровку трафика-то, собственно? А что вы подразумеваете под расшифровкой? SSL и всяческое шифрование. А, ну, смотрите, во-первых, SSL - это как бы уже M in the midle или там занимаются этим какие-то спецрганы, образно говоря, это не наша задача. То есть мы не занимаемся взломами шифров и не занимаемся какими-то там каким-то подглядыванием. Это вот хотелось бы, чтобы это тоже донести. Нет такой задачи. То есть SSL есть. Максим, что нам нужно от него сделать - это сказать, что это SSL. Его нужно определить в какую-то полосу или посчитать или ещё что-то с ним сделать. Может быть, вообще не пустить, но это вряд ли, понимаете, как бы. Но это задача бизнеса, не а не платформы. Спасибо, что ответили. Я прямо примерно то же самое про ССЛЬ хотел спросить. А, и ещё один вопрос. Вы сказали, что у вас есть собственный транспорт, да, между нодами. Попозже вы о нём расскажете. Ну, хотелось бы узнать, а поверх какой вообще протокол он построен? Поверх IP или ещё ниже? Нет никакого IP. Использование сокетов это вообще там не проходит никак, потому что накладные расходы будут огромные. То есть на самом деле мы работаем практически поверх dpdk. Может быть отдельно нужно пояснить, что это такое dpdknetmap. То есть это фактически прямой DMAnet из из usersрса вот буквально. То есть без копирования, без ничего. Остаётся 1на минута. Вот два вопроса надо в неё утоптать. Давай. Да, здравствуйте, Алексей Плутахин. Ну я не про Патроха спрошу, а вот про, наверное, внешнюю сторону дела, потому что не исключаю, что придётся столкнуться на практике. А там много компонентов, как я заметил. А будет ли какой-то, ну вот с точки зрения администратора, как это будет выглядеть? Какой-то как ну какое количество компонентов будет управляться из какого-то единого окна? Это вот та подсистема. В начале я за скобки вынес управление мониторинг. Вообще идея всего конструктора следующая - это как бы нулевой поднимите ещё одну руку. Нулевая стоимость владения. То есть, образно говоря, у нас сетевая загрузка. Максимум мы втыкнули флешку в блейд в новый для того, чтобы он просто знал, кто он такой. Ну там номер его записан. Включили его врnetт куда-то и всё. Всё остальное автопилотно. А что касается вот бизнес-задач, ну это как бы вот платформа так управляется. А есть же бизнес-задачи. У нас там есть какие-то протоколы политики, что делать, шейпить. В общем, вот на самом деле это большая огромная тема. Есть такая штука, как атрибутное дерево, как руль, их куча всяких, да? То есть это тема уже Питросервиса и связи с бизнес-логикой. Ну, образно говоря, у нас там у пользователя есть тарифные планы, есть какие-то деньги, они расходуются, да, там целый ещё огромная гора, она как бы вот просто совсем в не в не теме этого доклада, конечно. Она будет костомная под каждого оператора или там что-то одно своё? Вы знаете, я даже не могу сказать, потому что, наверное, сильно зависит от оператора. Вот это вот это уже совсем блиш ближе к бизнесу. Приходите на наш семинар, это там будет. Хорошо. Спасибо. Организаторы, покажите, пожалуйста, слайд, где записываются, если можно. Здравствуйте. Спасибо за доклад. Вопрос пользовательский. Как подразумевается расширение вот этими внешними компонентами? То есть использование вашей платформы. Вы говорите, что вы предоставляете инструмент для э получения чего-то. Вот как с этим чем-то работать? Вот использовать в юзкейсах, как вы говорили. А, ну смотрите, это прежде всего разработка-то заказная. То есть транспорт мы откроем, а какой-то совсем уж а или конструктор опубликовать наружу мы, наверное, не будем. Ну я не могу точно сейчас сказать, что будет. А значит, сейчас минутку я пытаюсь прощёлкать. У меня были Нет, прошу прощения, нет дополнительных слайдов. Да, ну есть отдельные слайды, там можно смотреть. Извините, покажу, если хотите, на самом деле. Давайте поблагодарим докладчика за интересный доклад. Время наше сейчас истекло."
}