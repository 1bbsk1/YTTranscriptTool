{
  "video_id": "FUXimVbe8t0",
  "channel": "HighLoadChannel",
  "title": "Хранилище для Почты / Виктор Могилин (Почта Mail.ru, VK)",
  "views": 394,
  "duration": 2926,
  "published": "2023-10-06T07:19:41-07:00",
  "text": "Да я тоже был на предыдущих докладах Сегодня у нас клуб любителей объектных староджей собрался Вот и я тоже сюрприз расскажу про объект мистера Вот Но я расскажу какие-то более глубокие детали о том как он у нас работает устроен должно быть интересно значит так сказали Я Магеллан Виктор я занимаюсь староджами в почте в Облаке почтовые стороны такая большая база данных которые мы храним письма информацию и для того чтобы клиенты такие как браузеры и приложения могли отрисовывать красивенькую такую картинку с папками письмами кратко напомню о том что такое письмо это такое стандартизированный Файлик у него есть заголовки какая-то структура портов бывают вложения та же музыка например или документы можно прикладывать письму и сразу начну с проблематики довольно давно у нас сторож был устроен таким образом что почтовый ящик юзера это была Папочка на диске вот Папочка на диске есть внутри этой папочки лежат все его письма и индексы проблема которая перед нами определенный момент встала это вертикальное масштабирование горизонтально масштабировались легко доставляли дисков Сколько нам нужно а большие диски использовать не могли и почему почему что мы упирались дисков есть такой ограничение магнитных дисков Это довольно маленькие цифры современные диски примерно вытягивают и При таком раскладе когда мы на диске имеем много-много папочек много-много писем у юзеров То есть у них бывает и сотни тысяч писем бывают пользователей за миллион переваливают у нас каждый дисковая операция буквально становится дорогой то есть чтобы нам почитать чего-то нам нужно прочитать метаданные директории dentry cash заполнить загрузить Ай ноду в кэш файловой системы тоже грязное чтение на диск только потом мы приступаем к чтению Ну а записи файлов и апдейты индексов это просто Рандомные постоянно Рандомные сети в диск это то что магнитные диски вообще не любят соответственно вот мы сидели сидели На таких вот маленьких дисках масштабировались горизонтально как могли и в конце концов до масштабировались для того что у нас на проде было 3000 примерно серверов с данными это уже никуда не годится То есть это и дорого содержать и неудобно обслуживать и занимает кучу места в дата центрах соответственно задача которую мы решали это сократить количество этих сторожей желательно порядок как я говорил письма есть какая-то структура и разными частям письма очень сильно разные паттерн доступа и вот эта история сокращением количества серверов она на самом деле превратилась не фильм Большой а Целый сериал из трех серий и 1 серии этого сериала Мы оптимизировали хранение вложений это самое не нагруженная часть Они вообще никакой нагрузки фактически по сравнению с другими частями не привносят оптимизировать нет смысла был смысл там заморачиваться с дедупликацией вот ну про это есть уже доклад на хайлоде пару лет назад Андрей сумме нам об этом уже рассказывал в конце моей презентации будет ссылочка на этот доклад и правда Чья говорить не буду сегодня и значит вторым шагом мы стали уносить с наших саражей письма более подходящее для этого хранилище то есть в итоге хотели такую картинку получить пока что оставляем только индексы на сторожах письма носим внешнее хранилище соответственно два два варианта где их хранить можно поставить sdmi и не заморачиваться с юбцами у этих дисков там огромные запасы десятки тысяч йотов они вытягивают но есть такая проблема что сильно много сервер не запихнешь мы хотели вот как прошлом докладе один рассказывал у них там 100 дисков каждый диск 18 ТБ то есть там в сумме один сервер даёт очень больше петабайта хотели использовать что-то такое нас немножко другой сетап Но похоже поэтому остаемся на HD Поэтому будем решать проблему айопсов и замечаем что письмо Оно хоть имеет какую-то структуру но структуру мы эту парсим на входе то есть при получении письма структуру партов мы кладем себе в индексы а письмо само тело письмо больше не трогаем то есть мы его один раз положили на стороне потом какое-то количество раз читаем но иногда письма удаляем то есть Можем рассматривать письмо как блог как какой-то бинарные данные без всякой структуры но соответственно в принципе по которым стоит хранить бинарные данные блога неизвестные они называются объектами мы начали делать свое свое объектный объектное хранилище с этого начинается история нашего объектного хранилища называется оно зевта и она достаточно простое я расскажу об основных принципах на которых она строится поглубже шкаф но в какие-то моменты то что кажется в прошлом докладе назывался называлась шарт у нас называется Бакет в аналогичных системах еще можно встретить такие понятия как и начали Воли в них в этих бакетах Мы храним свои данные и зевта Она состоит по большому счету из трех компонентов это зептопрокси точка входа в кластер сам кластер статус сервис база данных для база данных для метаданных и вот эти сами бакеты с которыми происходит общение через backet Service и весь кластер имеет по большому счету три команды то есть положить запись достать эту запись и удалить вот здесь же уже начинается небольшое отличие от стандартных схемы прокси У нас у нас схема не предполагает какого-то выделенного слоя пи выделенного роутера zeptoproxy мы ставим прям клиентам на тачку Тем клиентам которые которые сохраняют данные и их получают из хранилища таким образом можно поменьше данных по сети гонять и применять еще некоторые оптимизации а теперь поговорим вот о бакете что это такое На физическом уровне это файл файл этот не модифицируется мы можно дописывать его только только в конец и когда мы создаем Бакет мы делаем на него Кейт это такой си скол который поддерживается В современных файловых системах современных ядрах который нам гарантирует что если он выплатился успешно то пространство для файла выделена максимально последовательными блоками на диске и что мы эти два гигабайта точно в файл запишем система не скажет посередине записи закончилось место Бакет состоит у баки то есть начале некоторые служебный заголовок и потом дальше идут пользовательские записи одна за другой заканчивается всегда специальной записью эндо-файл EOS пользовательская записи мы пишем страничками по 4 килобайта и у польской записи каждый тоже есть Фидер небольшой заголовок там перечислена всякая там сжатом не сжат какой у него размер и рекорд ID значит что такое рекорд ID записи у нас это как раз таки такой Есть отличия от других аналогичных подходов рекорде записи это смещение от начала Бакета до начала записи стоит Значит надо взять Бакет сделать на 100 байт можно читать пользоваться запись на этом же слайде вводится важное понятие версия байке то я буду несколько раз оперировать в дальнейшем этим понятием версия Бакета это смещение до эндо файл специально для записи когда мы используем версию например тогда когда хотим закрыть Бакет на запись мы не хотим писать его бесконечно Мы хотим Бакета держать там двух гигабайтными чуть больше Поэтому когда версия Бакета переваливает за 2 ГБ мы такой байк Закрываем на запись больше в него не пишем 4 килобайтные страничке они снабжены сердце кодом и мы проверяем сердце и на каждом чтении И периодически нас есть процесс который ползает баки там сверяет сердце суммы чтобы ходить в бакеты чтобы из них читать Чтобы в них писать поднимается байкат сервис над ними на диске и через этот Бакет сервис с ними осуществляется все общение таким образом сервер у нас выглядит таким образом сколько-то дисков над каждым диском поднят Бакет сервис у них от всех одинаковые разные порты Таким образом мы можем обратиться к любому внутри этого сервиса и для того чтобы выполнять всякие фоновые компакты создания бакетов отдельные фоновый процесс его часто лимитирует у нас через группы чтобы он не кушал там лишних процессов Он выполняет всякие служебной информации статус сервис важная часть системы база данных для метаданных он опрашивает все Бакет сервисы периодически делает это раз в минуту время первого запроса он получает всю информацию потом дельтами получает изменения и внутри себя в памяти Он держит карту этих байкетов то есть Где он находится какая у него версия сломана Например если сердце там у него не сошелся он будет сломан или здоров и тому подобное также статус сервис предоставляет API для zepto Proxy это два основных запроса либо Дай мне любой багет Куда я могу записать либо вот у меня есть багет из которого я хочу почитать Дай мне адрес где он находится также важным Важной задачей статус сервиса является поддержка в кластере какого-то количества доступных для записи бакетов и пока мы далеко от физического слоя не ушли пару слов Почему такой подход к хранению он более более бережный к диску и к файловой системе требует меньше у нас файлы довольно таки большие она разделе допустим из двух терабайт таких файлов получается не очень много там тысяча допустим файлов взято прокси все эти бакеты хранит в одной папочке то есть мы имеем одну папочку и внутри тысячи файлов нас дендере кэш никогда не вымывается практически всегда за кэшированы поэтому Рид реально выполняется фактически почти всегда за одну операцию в рай ты такой системе они фактически бесплатные то есть они всегда идут в конец открытого уже заранее какого-то Бакета у которого провоцированные блоки есть на диске и его здесь тратятся разве что когда диску нужно перепрыгнуть с одной дорожки на другую теперь поговорим о том за счет чего достигается надежность хранения тут нет никакого Откровения надежное сравнение достигается также за счет репликации хоть багет это физически просто файл но система работает с бакетом как с логической некоторой сущностью и Бакет логическом смысле это объект у которого есть айтишник Бакет ID и набор этих реплик сколько там 2 может быть три набор реплик физических значит когда создается Бакет создание Бакета инициирует статус сервис когда он понял что не хватает он сгенерировал новый айдишник он послал две команды на создание бакетов два разных центра там пакет сервисы с Бакет компактными создали новый Файлик если все прошло успешно статус Сервис К себе в память записал Ага у меня появился новый багет у него айдишник 42 у него версия 0 То есть можно писать начиная с нулевого смещения и реплики лежат вот таким вот там айпишником когда прокси получает баки для записи она получает все адреса всех реплик и выполняет вороную запись во все адреса которые получили факторы репликации которые мы поддерживаем это X2 X3 X полтора X2 X3 Ну там все понятно просто либо две реплики у каждого байке то либо три реплики у каждого байкета их вороная запись либо в два места либо в три места x-полтора это вот тот самый реже кодинг про который тоже рассказывалось на прошлом докладе но у нас он тоже своеобразный такой тоже упрощенный значит как у нас устроен режим кодинг используем к свар кодирование и когда мы создаем багет он создается в трех экземплярах в трех разных дата-центрах но третий багет для начала Он является заглушкой А запись идет в два Бакета когда байкер закрыт на запись приходит специальный такой процесс мы называем его полтарирование и полторы на одном диске он оставляет четное Бакета на другом диске нечетные А на третьем четных нечетных за счет этого достигается а вот X полтора фактор репликации то есть 100 байтный файл он будет занимать в таком пластере 150 байт и Если на байтах это не очень кажется что решает то вот хранится 55Б вместо 200 петабайт это тоже очень хороший результат минус такого полтарирования понятен он чтобы прочитать файл нам нужно всегда прочитать двух дисков слепить файл обратно в памяти и потом только можем отдать файл так и того надежность в нашей zept заключается за счет обеспечивается за счет репликации за счет постраничного црц который мы всегда проверяем и бакеты они у нас не такие уж и большие там 23 гигабайта сломанный Бакет всегда можно удалить довольно быстро перезаписать Со здоровой реплики поговорим теперь о доступности доступностью подразумевается возможность чтения и записи систему в случае вылета до центра из строя нас есть два уровня на котором нужно обеспечить доступность это статус сервис и слой хранения без статус сервиса вообще ничего работать не будет потому что только он знает где бакеты лежат может выдать Бакет для записи Ну и через него получается Бакет на чтение поэтому статус сервиса нужно как-то дублировать и тоже место такое очень вариативное разное решение я встречал кто-то здесь ставит традиционную базу данных организовывает синхронную репликацию кто-то ставит eventualcon Systems систему какую-то наверное наворачивает рафт выбор мастера Но это и другое оно подразумевает некоторые Down Time в случае если мастер все-таки вылетел Они хотели и сделали тупо Независимо друг от друга то есть в первом приближении Они вообще а друг о друге ничего не знают работают Независимо каждый опрашивает Все бакеты каждый У каждого есть карта бакетов в памяти Вот Но есть проблема генерации айдишников для этих бакетов то есть нужен какой-то там глобальный что ли автоинкремент Вот Но это проблема Мы решили просто мы просто потому что натуральных чисел пошарнировали там по остатку отделения и каждый статус сервис соответствии со схемой шаринга генерирует ID для бакетов внутри своего диапазона вот мы такой статус сервис который регенерирует внутри своего диапазона называем мастером для вот этого диапазона то есть информация о бакете можно получить из любого статус сервиса Но если какой-то статус сервис упадет то что мы получим мы получим меньше врата был бакетов системе вот если пакт негативный будет то есть это не совсем бесплатно но там тайма не будет сможем Продолжить писать теперь откуда статус сервисы узнают куда им вообще надо ходить куда нам надо ходить и опрашивать статусы опрашивать Бакет сервисы Вот это по сути единственная такая информация которая персистится в зевте даже все остальное можно выстроить с нуля потеряв там все данные всех ошибок желательно не терять штуку эта схема добавления дисков кластер она у нас задается жестко фиксировано при инициализации кластера и она описывает то как должны быть соединены диски между собой при добавлении в кластер диски мы добавляем группами и здесь примерчик для фактора репликации X2 и 3 Вот это центров в таком случае минимальный набор дисков для группы это 6 штук Они организовывают между собой пары таким образом чтобы было три пары диски внутри каждой пары находится в разных центрах То есть у нас есть три Бакета по сути который можем писать и дисков 6 штук Да как значит работает файловер в данном случае как работает газоустойчивость Если третья дата-центр вылетает то у нас остается одна пара в которой мы можем продолжать записи данные мы можем читать реплик потому что реплики остались живые в первом во втором центре для репликации X3 и 5 дата-центров я сильно уже подробно рассказывать не буду просто там группы уже состоят минимальные группы тоже 15 дисков связи соответственно больше надежность повыше такое схеме можем потерять два дата-центра и сохраним доступность на чтение и даже и даже Возможно если в такой ситуации еще 104 вылетит у нас по-прежнему останется одна тройка в которой мы сможем писать маленькая лирическое отступление по поводу вот такому поводу значит я до этого говорил диски диски что мы Бакет сервис у нас на диске работают на самом деле не над дисками А над партициями Вот Но это не ограничивает общности нисколько но стоит все-таки наверное об этом рассказать потому что если реплицировать диск диск И вот у нас диск вылетел то все риды которые могли бы на этот диск прилететь прилетят на реплику таким образом реплики может стать не очень хорошо Но если диск разбить на например на традиции по 2 ТБ организовать группы из этих партиций то соответственно при вылете диска Вот здесь например есть диск 3 на картинке он вылетел и часть чтения которая пошли бы на него если бы он был живой улетают на один дата-центр часть чтений летают на другой центр таким образом нагрузка немножко размазали в принципе есть уже вся необходимая информация чтобы понять как работает у нас вставка детально понять как работает вставка данных получения их удаления чтобы вставить в кластер какие-то данные клиент идет Рокси рандомном порядке опрашивает нас нет каких-то сложных балансировок о которых было на прошлом докладе у нас практически говорит с помощью команды Lock пытается получить байкер для записи значит какой-то статус сервиса ему отвечает Вот тебе такой табакет значит реплики лежат там версия у него такая То есть пиши начиная с этого байта в этот момент Бакет ловил лучится на запись внутри этого статус сервиса больше клиенты этот пакет не получат пока лук не будет снят параллельно записывает данные в во всей реплике и если все успешно снимает Лог говорит я записал Спасибо тебе можешь выдавать дальше кому-то ещё и в результате у клиента на руках появляется айдишник записи То есть он у нас генерится прямо в приставке внутри внутри кластера айдишник этот это багет айди и рекорд ID то есть Бакет ID и прям смещение внутри этого Бакета здесь может что-то пойти не так фактически на любой стрелочке но здесь везде работает ретрай то есть не получилось взять Лог в одном статус сервиса пошли в другой не получилось записать кворам на во всей реплике пошли пошли с нуля все начинать пошли в другой статус сервис Причем есть всякая разная логика выбора бакетов из других центров чтобы не пытаться тоже это центр сохранять мне удалось закоммитить запись тоже бросаем все Начинаем начинаем просто сначала когда-нибудь процесс сходятся и значит то что у нас айтишник вот таким вот образом генерится это тоже интересное такое Мне кажется решение которое тоже отличает нас от других объектных хранилищ то что это чистилка с ней удобно работать мы эту чиселку должны хранить в индексах а индекс Мы хотим чтобы были максимально компактными максимально небольшого размера чистилку удобно Она сама по себе не очень много весит еще можно кодировать всякими воротами зигзагами она немного весит с ней работать удобно и нам не нужен никакой индексный файл здесь зачастую делается какой-то индексный файл в котором есть соответствие то есть айтишник какой-то записи в Облаке индексный файл читаем информацию значит оказывается у этого ID вот такое смещение Такой размер Там и так далее у нас света хранится прямо в бакете а смещение уже закодировано в ID записи Вот Но и здесь у меня заготовлена шутка да если кто смотрел прошлые доклад там было мульти экзо-байтные блок storage вот у нас он ровно просто их забайт потому что бак Это идея рекорд ID это 4 баттные числа соответственно мы можем адресовать до 264 байт это ровно забайт Вот это тоже особенность нашего сторожа он конечный него нельзя складывать данные бесконечно вот ну другой стороны у нас писем пока что всего лишь 15 так что мы не переживаем что он закончится Get выглядит довольно просто по ритм все точно так же клиент передает этот прокси из него извлекается Бакет ID делается запрос на статус сервис на любой в рандомном порядке получаем айпишники где лежат реплики Бакета читаем из одного бег от сервиса рандомного если что-то пошло не так но мы какое-то количество байт прочитали мы начинаем с уже с следующий остаток байт можем дочитать из второй реплики что касается делитов тоже вариативное место такое делают делитые через внешняя база данных делают через журнал удалений у нас делит это специальные такие записи которые в общем-то представляют собой обычные записи то есть делит у нас это пуд делит у нас это вставка специальная запись в Бакет причем такие делиты они вставляются даже и в закрытые бакеты вот удаление тоже как и у всех происходит не мгновенно То есть если в бакете был 3 записи и первые два из них удалились этот на картинке изображено какое-то время в таком виде будет существовать Ну потом приходит компакт понял полностью перезаписывает Бакет Бакет сервис Этот новый перезаписанный Файлик перечитает и таким образом данные удаляются физически если мы удалили две записи из Бакета А у нас рекорд ID это смещение внутри Бакета то серия карт иди очевидно поехали Теперь они не соответствуют реальным положению дел чтобы эта ситуация обрабатывать у нас появляется индекс удалений в бакете он там хитроустроен Чтобы поменьше весть Ну грубо говоря это мапка специальная в которой удаленным рекорд ID удаленным записям соответствует их офсет и мы с помощью этой матки корректируем доступ байке теперь о отказоустойчивости делитов значит если Бакет сервис Ну там диск почему-то недоступен а нам нужно удалить данные из него как мы действуем в таком случае мы положим удаление в любой доступный врайта был Бакет и компакт когда доберется до этого доли то он поймет что это делит от другого Бакета принадлежит другому бакету будет бесконечно пытаться доставить такой долить до адресата вот такие билеты мы называем фарендолиты в компакты которые которые значит обрабатывают удаление они у нас запускаются и как по времени так и порогу если Бакет какой-то накопил достаточно много удалений то в нем тоже запустится компакт что побыстрее освободить место Ну и того что касается Если говорить о доступности то значит за счет чего она у нас работает за счет жесткой схемы кластера где диски разбиты под центром еще три траев на всех уровнях Ну или ты умеют умеют удалять умеет применяться значит не только своим родным баке там где лежат данные Но просто в любые Рандомные можно положить применяться когда-нибудь для управления кластером нас есть такая штучка называется наземптоциль помощью нее задается и схема кластера при инициализации и схема шортирования статус сервисов и тоже на прошлом докладе было несколько слов значит об этом У нас эта штука называется мувин когда мы добавляем новую группу из чистых сторожей из чистых дисков смысле в кластер то на эти диски действительно будет Больше записи нас тоже есть приоритет мы пишем в недозаполненные диски в первую очередь вот если значит много писать много делать записей эти диски то потом получим перекос пакетом потому что действительно к письму которые пришли недавно обращаются гораздо чаще чем письмам которые пришли уже давно вот поэтому у нас вот масштабирование выполняется то есть операция чтобы выполнить такое масштабирование но у нас нет весов там какие-то сложных мы просто отслеживаем средние диски по кластеру и по этому по средней заполненности В общем выравниваем новые диски тоже эта штука по вибрировала как будто бы она скоро сядет Надеюсь что не сядет до конца наклады значит можно есть некоторые язык запросов такой простенький можно по selected эти бакеты локальные кластерок поднял грохнул пару данных значит и вот чтобы просто показать как она выглядит значит есть какие-то сломанные бакеты значит пары бакеты эти стали не в рай Это был уже можно посилектить все можно посилектить под этот центру можно Селект с определенным статусом и так далее можно запустить компакт или фикс и поломанные бакеты переедут из здоровых реплик все станет зелененьким красивым в принципе по устройству наверное все И перехожу к такой более уже итоговой завершающей стадии значит вынесли Значит мы в первой на первом этапе мы вынесли вложения на внешнее хранилище в этом внешнем хранилище у нас был такой сетап то есть сервер у него 12 дисков обычных подключается SATA полка которой можно подключить еще 60 дисков 18 терабайтные диски на этот же кластер мы выносим zepto делим значит как-то место между вложениями и зевтой и оно все живет в общем на этих 18 терабайтных дисках они хорошо утилизируются по месту и по йопсам там получается совсем довольно таки мало то есть есть еще запас на каждом диске можно видимо будет использовать и диски большего объема Если и когда они появятся по фактору репликации которыми мы используем для писем это у нас X2 и Казалось бы 15 должно быть должно превращаться в 30 но мы также используем сжатие а письма очень хорошо жмутся поэтому после сжатия 35 обратно превращается в 15 значит что касается исходной задачи с уменьшением количества серверов у нас было 3000 мы хотели сократить на порядок Ну Действительно получилось сократить то есть мы унесли письма со сторожей на сторожах остались только индексы мы индексы хорошо уплотнили там два раза но задача еще не была решена поставленная и с этого момента начался началась третья серия сериала уже собственно касающиеся индексов и их оптимизации и хранения на текущий момент несколько лет назад работы завершены и мы действительно весь сторож уместили там примерно в 250 серверов но про индексы тоже будет видимо другой доклад где-то В будущем я надеюсь он будет Вот по итогам касающимся именно построении объектных хранилищ ну Да хотя хотел сказать что блог бы это не только там фоточки и файлы можно в принципе достаточно много придумать примеров когда подходит этот паттерн хорошо какие-то архивы блоги например старые которые жалко выкидывать но хочется еще похоронить можно письма например в почте тоже можно рассматривать как Блоб принципы по которым строятся объектное хранилище она недостаточно простые понятные и я сегодня о них попытался рассказать и попутно рассказал за счет чего можно объектное хранилище сделать таким чтобы оно было доступным и надежным получилось оно то есть у нас нет такого значит какой-то мегахранилище в котором в котором хранится все у нас сейчас пока что другая ситуация мы используем маленькие более маленькие так скажем независимые друг от друга например есть кластер в Облаке там мы используем как раз таки полторашку есть кластеры в других разделах почты например в поиске по почте есть еще и другие по цифрам Если говорить о кластере с письмами то там средний доступ он в общем-то сравним получается с доступом в диск вот если говорить про полторашку там большие объекты и вот такая особенность этой полторашки там уже цифры поскромнее в общем зависит это еще и от дисков которые под Под этой стоят и вот фактор репликации Да здесь у меня есть Lights точками роста и вот гибридная схема репликации как раз то о чем рассказывали тоже на прошлом докладе хотелось бы тоже такую штуку нас просят но пока не умеем значит То есть быть как бы в режиме репликации X3 но при этом не переходить в redonalis потери одного диска переходить как бы в режим X2 а для старых данных автоматически начинать хранить в x-полтора вот нас пока такой автоматики нет хотя что-то подобное мы делаем внешними компонентами там с помощью внешних инструментов каких-то удаления У нас сделано достаточно прикольно но из-за того что они все-таки летят не в рай Это был бакеты а в целевые какие-то бакеты это все-таки порождает трендом и можно большим количеством удалений кластеру сделать больно Вот пока что пока что такую проблему не решили вот ну и много значит хотелось бы конечно больше в принципе автоматики то есть мы сейчас умеем много чего лерть о сбоях диска о там побитых царство Сумах во всяких там сбоях внутри кластера Вот но сами Мало чего чиним Ну принципе с одной стороны это правда на боязнь админов всяких боязнь админов что автоматика там сойдет с ума и Патриот данные вот поэтому они вот смотрят свои мониторинги и вызывают уже операции вручную Ну что-то можно было на самом деле и автоматизировать как обещал ссылка на доклад по нашим вложениям про их дедупликацию и на этом У меня пожалуй все Спасибо Виктор спасибо тебе за рассказ про еще один объектный storage мы здесь просим всех кто слушал в онлайне или сейчас доклад оставлять обратную связь Я думаю это и тебе важной организатором и Давайте перейдем к вопросам Я надеюсь они накопились да и у нас значит как всегда есть вопросы зала и вопросы из онлайна Кто смотрит в онлайне У вас есть специальная кнопка и я вижу что у нас уже первый вопрос из зала готов задавайте Виктор спасибо за доклад У меня вопрос про схему хранения ты сказал что используется схема X3 X2 X полтора Вопрос Из каких соображений выбирается талии или иная схема для конкретной инсталляции сторожа как выбираете ну их у нас не так много этих кластеров в принципе могу про каждый фактически рассказать X3 у нас используется на почтовых индексах их потерять будет очень больно фактически точно не будет работать максимально надежность нужна поэтому там X3 и с другой стороны их мало там всего два петабайта этих индексов облачные он в Облаке используется полтора схема репликации потому что там файлики которые мне обращались юзеры в течение года уже Ну и наверное вряд ли обратятся в ближайшее время там очень маленький рейд запросов и нас даже настолько маленький что в принципе нас не сильно парят то что нам мы вынуждены читать двух дисков там лепить этот Файлик в памяти такие файлики нам нельзя удалять естественно мы должны их по-прежнему хранить надежно но как бы можем уже ослабить требования и здесь выбираем их полтора Вот но X2 такая золотая середина наверное там где не нужно супер дешево хранить но и не нужно супер надежно там можно X2 оставить как-то так а не задумывались о варианте например схемы сырейша кодингом когда блок разбивается на большее количество реплик там количество частей строится большее количество блоков избытостей таким образом можно достичь той же стоимости хранения типа x-1,5 но при этом достичь больше отказоустойчивости даже чем случае X3 и если у вас сценарий Когда это было бы имеется ввиду что блоки файл разбился на 10 блоков эти 10 блоков на разные диски легли на 10 разных дисков это плохая схема и смотреть Почему давайте рассмотрим 6 дисков если у нас есть дисков в кластере всего то они образуют 15 пар То есть первый со вторым 1 3 1 4 2 3 4 и так далее 15 пар дисков лежит на двух дисках то вероятность его потерять на 15 если на трех дисках то пытались его потерять 3/15 вы предлагаете в схеме когда файл разбился на 10 кусочков каждый кусочек положить Значит на 10 дисков то ну при вылете любой пары фактически мы потеряли файл Ладно я понял что кажется у вас более сложная схема используется Просто у нас хранилище как раз мы развиваем на 8 частей плюс 4 и в целом дается чуть больше устойчивости но я так понял В вашем случае это связано как раз с особенностью того чтобы берете дальше группы и не получается Ну вроде того я потом еще рыжим кодинг эта штука сложная хитрая да я согласен Спасибо подтянулся у нас онлайн Сейчас посмотрим это говорят смотрю много в телефон Давай дадим вопросы зала так хорошо тогда вот у нас здесь человек рук тянул Давайте его спросим Привет Спасибо за доклад скажи ваша схема хранения данных подразумевает что у вас видимо нету шифрования над ними это так шифрование У нас есть но исторически оно сделано на уровне админов то есть нас специальные специальные материнке там ТП модуль и не дает небольшой там Кост плюс пять процентов по процу но зато прикладные программисты этим не заморачиваются То есть у вас достаточно небольшие Касты на то чтобы расшифровать один блок интересно спасибо так хорошо Кто хочет задать вопрос поднимайте руку Так а у нас уже человек с микрофоном Да Виктор спасибо за доклад был интересно соответственно я продолжу тему фактор репрекации Когда вы говорите О Факторе 1.5 вы говорите что Используйте коды коррекции ошибок а именно к 40 почему именно к ссор приводили какие-то тесты сравнения Ну так скажу просто наверное потому что это вот это нужно было сделать побыстрее попроще да Обычно используют риды Соломона в этом месте и на самом деле мы тоже хотим сделать до этого Пока не дошли руки его просто было сделать просто реализуется его просто понять он просто имплементируется Понятно спасибо так хорошо Еще я видел руки Вот вот у нас здесь Добрый день спасибо еще раз за доклад такой вопрос насколько я понимаю ваша схема хранения предполагает что компакт работает сразу же на всех трех репликах Бакета Правильно я понимаю Нет смотрите про это был доклад у нас сервер устроен таким образом что там есть сколько-то дисков над каждым диском Бакет сервис и есть один процесс на весь сервер на весь сервер bucket compactor который выполняет компакты Он выполняет контакты только внутри своего только внутри своего сервера Насколько я понял По докладу у вас порядок записи важен во всех реках Ну при компакте тогда получается меня происходит Вы имеете ввиду что на одном может удалиться запись сервис Это скорректирует получается на одном допустим не успел произойти значит информация об этом в дело индекс не попала а на другом там где компакт успел произойти информация об этом попала в индекс удалении и Бакет сервис скорректирует соответственно соответствующим образом понятно и обратной собственный вопрос был похожий ответ будет прикормной записи как вот уже обеспечивать порядок там трех репликах не услышал Извините прикормной записи как обеспечивать порядок записи в трех репликах допустим Ну в один раз я об этом тоже был слайд значит когда мы получаем баки для записи статус сервиса valoged а то есть вы пишете только всегда здоровые баки туда то есть когда все три реплики допустим доступно То есть если у нас в X3 схеме один Бакет помечен как недоступный или у него там не так то такой баре становится редонли А что будет если произошло во время записи вы просто повторяете записи на уровне клиента в другие какие-то баки все начинается сначала причём э-э зачастую из поиска доступных для записи бакетов исключается вот эти вот это центры на которых вот эти вот сбои происходили О'кей хорошо спасибо Так кто еще можно вот ага я да да вот у меня мелких вопросов несколько просто уточняющих по смыслу вот говорили что Бакет сервис Каждый на своем порту то есть смысле каждый байкеры ста один процесс да хорошо то есть если у вас там условно 18 каких-нибудь хордов то типа на сервер дофига довольно может быть этих процессов Да смотрите 22 диска 72 диска на каждом по 6 ТБ отведено на партийцы по 2 терабайта То бишь на 72 200 процессов конец пишется если нужно Ну вот из ецэшного если удалить надо еще раз Ну из x-1,5 надо удалить то есть когда он у вас конвертирован уже в полтора Хороший вопрос но как-то сходу как-то сходу наверное даже на него отвечу надо будет подумать Ну ладно хорошо вот еще вы говорили про нагрузку что можно вот перекос почти не получить потом как сказали что просто у вас по заполненности выравнивается и все то есть по нагрузке все-таки не выравнивается вот то что весов нет ничего подобного нет нет И последнее это SATA полка Это она правда сатана или она сад я общался с админами они говорят Сата Понятно Ладно понял Осина синхронизация по смещениям Да ну то есть размещение этого идентифицирует позицию то есть синхронизация наверно по ним Да синхронизация чего Ну реплика допустим там лежал диск ее надо догнать То есть она вернулась она может хвост файла просто к себе скопировать Нет не синхронизируем бакетами целиком синхронизация всегда полная понял Спасибо у нас небольшие легко сейчас два гигабайта или терабайта гигабайта А ну если гигабайта тогда у вас 18 тролливый диск это же больше будет это у вас два терабайта до 6 терабайт получается да подожди 2 ТБ Все понял Спасибо отлично так кажется здесь вы кто-то хотел еще задать вопрос вот нас рука да давайте дадим возможность Спасибо большое все-таки не могу не задать вопрос почему не хранилище Одноклассников Почему не цеф который насколько я знаю mail.ru также эксплуатирует Ой ну Хороший вопрос значит цифр это вообще распределенная насколько я знаю файловая система она здесь излишне совершенно нам не нужна А но она умеет больше чем имеет объектное хранилище нам не нужно то что умеет файловая система распределенная а почему не хранилище Одноклассников честно говоря не знаю это как бы исторический доклад Да это не то что мы делали вчера всю эту работу Мы закончили наверное там году 18-19 уже было закончена на тот момент не было еще такой у нас тесной интеграции сейчас у нас да Вектор в компании на переиспользовании технологий там друг друга Ну тогда ещё не было такого Так у нас есть время еще на один вопрос кто еще что хочет что-то спросить Ну как будто мы удовлетворили интерес есть вопросы из онлайна как раз у нас времени на один Да Максим Бабича спрашивает как будет происходить сохранение файла больше двух гигабайт как потом прочитать этот файл такие файлы мы сохраняем таким образом они делятся на записи по 64 мегабайта пишутся в Бакет последовательно причем в таком случае мы разрешаем баки то расти до 3 ГБ а Но если 3 ГБ наступает то тогда этот значит остаток дописывается в новый Бакет а в конце в самом конце когда все весь байк от улегся кладет специальная запись в которой перечислены зевта ID всех кусочков А из которых получился вот этот большой который содержит большой файл И вот именно зеп Отойди вот этой последней служебной записи отдается клиенту прокси умеет такие штуки обрабатывать прозрачно Для клиента то есть клиент не будет знать о том что его файл как-то разбился на кусочки Лег в разные баки вот так Да ну что у нас идеальный тайминг настало время выбрать лучший вопрос У тебя есть список Нужно ли Напомнить лучше вопрос однозначно Про удаление из полторашки Ну честно скажу Наверное я момент написал поэтому поэтому не знаю точно как он устроен Вот Но у нас здесь зале есть разработчики можно будет в кулуарах обсудить вот этот вопрос я думаю Самый лучший был вопрос идентифицируйте себя вот автор вопроса Про удаление из полторашки помашите рукой Вам помогут Спасибо большое за вопросы Что Виктор отличный доклад Большое спасибо для тебя тоже есть прекрасный сувенир"
}