{
  "video_id": "eL3dkh-WaSU",
  "channel": "HighLoadChannel",
  "title": "Deep Learning vs common sense: разрабатываем чатбота / Владислав Блинов, Валерия Баранова (Тинькофф)",
  "views": 1843,
  "duration": 2476,
  "published": "2019-12-05T13:03:00-08:00",
  "text": "но я занимаюсь сегодня мы скажем такой можно получить двигаюсь в поле пустым к более сложным задачам разработки чирков но для начала давайте поймем зачем вообще подколка в этот розовый бумажник залогом нашего планки и миллионы клиентов и периодически нужно различного рода помощь поэтому для того чтобы эту помощь оказывать у нас в техподдержке на 1000 сотрудников естественно это порождает назначь на высокую стоимость обслуживания канала какими популярные музыку и заводили можно отмечать их романтических помощи от года его на какие-то отвлеченные темы java и что нужно на основах его любая чего же хочет мой пользователь и понять его больше пользователей называете поэтому все модель которые будем показывать подходы мы будем показывать на примере деньги болею сейчас тестировать их качество мы будем проверять их на тестовой выборке которая состоит из реальных диалогов с пользователями там более 30000 размеченных примеров более 170 интентов всего представлено смотрим пару примеров интентов там есть intent и похода в кино intent поиск ресторанов некоторые банковский intent такие как открытие вклада и закрытии вклада и так далее но также у олега есть на многое свое мнение например вот он знакомых сибири у него пока нет а с лучший язык программирования для его для него это си плюс плюс ну то есть он и просто поболтать с вами тоже может первое что можно сделать задачи классификации интентов это просто использовать какой-то слова например у нас есть фраза переведи сотку лерия и если слово переведи встречаются фразе пользователя мы считаем что денежный перевод абсолютно простой метод можно сделать несколько каких то ключевых фраз например закинь денег или сделать денежный перевод но мы можем померить качества в принципе наше тесто выборки для вот такого подхода тоже ну да чтобы не быть голословными давайте вот тема посчитаем какое качество будет у так называемого классификатора а если он просто по слову переведи будет назначать о намерении пользователя денежный перевод на самом деле качество достаточно высокая точность 88 процентов при этом полнота низкая 23 процента ну оно и понятно слова переведи явно не описывает всех возможностей сказать а переведи кому-то деньги более одного несмотря на это у этого подхода все-таки есть плюсы во первых нам не нужна никакая размеченная выборкам ты будешь шутить даром не нужна выборка я сама не учи модель а вы можете получить высокую точность если вы хорошо составите свои словари а при этом эти словари все еще нужно составлять поэтому на это тратится время и ресурс плюс скорее всего у вас будет потенциально низкая полнота так как все вариации какого-либо класса описать иногда тяжело более того вот есть например контрпример когда у вас был intent переведи лири сотку и появился второй интер переведи на оператора что такое intent переведены оператора ну не все пользователи любят общаться с голосовыми помощниками иногда они хотят с реальным человеком пообщаться поэтому когда мы добавляем новый интент перевода на оператора то видно что качество резко падает на точность резко падает на 18 пунктов при этом естественно полнота не растет и здесь мы уже понимаем что хочется какой-то более продвинутый подход попробовать но прежде чем мы перейдем к машин лингу нужно понять что мы будем делать с текстом поскольку мы шли лирник модели работают с какими-то числами они не могут просто взять и начинает касаться intent и по тексту поэтому будем переводить их виктора данном случае у нас есть фразу олег почему тебя назвали олег мы хотим получить для нее какой-то вектор и самый простой подход который можно применить это использует fdf вектор tf и defector это вектор который учитывает встречаемость каждого слова фразе пользователя и еще учитывает общую встречаемых слов коллекции те слова которые встречаются часто в разных текстах они имеют меньший вес в этой модели можем обучить на от f&d в представлениях вашу любимую линейной модели на svm ли логистическую регрессию в данном случае цифры для логистической регрессии и видно что у нас полнота я резко взлетает при том что точность она примерно соизмеримо с использованием словаря то есть мы модель уже сама разбирается какие слова важны для какого интента вам не нужно ничего придумывать здесь самим и их мера вот здесь то и взвешенно гармоническое среднее между между точностью палаты да она тоже резко возрастают при этом наверно скучно на одном intense смотреть примеры поэтому давайте увеличим число интентов дано прижим чем мы это сделаем нам хочется визуализировать наши данные то есть понять как в принципе выглядят наши intent и насколько хорошо они группируются в пространстве мы не можем просто взять и визуализирует fdf представлении поэтому будем использовать здесь метод сжатия размерности именно тсн я причем мы будем использовать такой метод жать и размерности которые при переводе в двумерное пространство сохраняет относительное расстояние между объектами то есть мы в принципе структуру увидим все равно можем посмотреть как это выглядит вот на этом слайде то есть у нас здесь представлены топ-10 интентов по встречаемости в нашей коллекции здесь есть зеленые точки это не какой-то и 10 кластеров которые помечены цветом это уже разные intent и видно что некоторые из них прям очень хорошо группируются в кусты ра и слева показана взвешенная f1 мера на вот этих 10 интентах она равна 0 90 бум что в принципе достаточно высоко и с этим уже можно работать то есть другими словами если мы используем эмаль модельном во первых не нужно уже мы у нас по мото получается гораздо выше чем при использовании словаря при соизмеримой точности при этом нам не нужно придумывать какие слова какому intent у соответствует но у нас во первых ограниченный словарь мы можем получить весам модели только для тех слов которые присутствуют тренировочной выборки при этом мы никак не учитываем перефразирование и никак не учитываем порядок в котором слова встречались интенцию давайте остановимся подробнее на перефразирование зачем же она нам нужна и что чего мы хотим добиться ну например у нас есть какие-то два текст на между которыми общих слов нет напомню что ты f&d фактор он как бы близкими назначить только те тексты которыми которые которые пересекаются по словам а что же такое близкие тексты ну например чтобы посчитать близость между двумя векторами можно посчитать косинус угла между ними а давайте посчитаем косинус ную близость по векторному представлению tf для конкретных примеров вот первые хочу в ресторан с высокими ценами поесть бы в дорогом месте и пойдем мажорный бар видно что для векторного представления твд в принципе это не очень близкий текст на хотя они для нас это один и тот же intent и один и тот же класс что можно с этим поделать но например можно в место числа представлять слова целым вектором это называется embedding слово такое embedding может быть построен на основании например контекста в котором это слово употреблялось первой моделью которая была предложена в 2013 году она достаточно простая и широко используется с тех пор называется во рту век и обучается работает она примерно следующим образом мы берем текст купи билеты на rocketman вечерком берем какое-то слова из контекста и выбрасываем ее выбрасываем его вот слова rocketman затем мы берем какое-то случайное слово из контекста например вечерком представляем оба слова ваншот векторами подход вектор это вектор по размерности словаря где на индексы слова единичка если это и есть это слово далее мы обучаем простую однослойную нерону сеть без активации на внутреннем слое предсказывать следующее слово в контексте то есть по слову кидман предсказывать слова и вечерком здесь мы для каждого слова из нашего словаря назначаем какую то вероятность быть следующим словом мы конечно же знаем какое слово было в действительности считаем ошибку наказываем нашу модель обновляем веса и вот эти обновленные веса которые выучили мы на основании нашей выборки и являются имбилдингом а нашего слово спасибо спасибо является имбилдингом нашего слова какие же у этого плюса но во-первых мы уже получаем embedding который зависит от контекста в котором это слово употреблялось ну популярные примеры да там например трамп и путин близки по во рту веку потому что они оба президенты и часто употребляются например в текстах вместе и для тех слов которые модель внедрила при обучении например для слова билет вы просто берете матрицу митингов берете по индексу слова его вектор получайте вектор сказала бы все хорошо помимо того что некоторых слов в вашем вот эта вот ваши матрицы может не быть просто их модель не видела при обучении поэтому для того чтобы бороться с этой проблемой out of capillary или проблемой незнакомых слов в следующем году в четырнадцатом придумали небольшую модификацию во рту века который называется фас текст работает оно следующим образом если мы слова в словаре не имеем тогда мы разбиваем его на триграммы то есть рог окей так далее м н и берем для каждой триграммы его обеде нг из уже матрицы берингов триграмм который мы обучаем подобно тому как во рту века обучаем усредняем получаем вектор отлично теперь мы получили виктора для ослов которых у нас нет то есть мы учитываем похоже слов мы получаем виктора где для похожих слов они будут близки по этому же самому косинус нам у нас наблюдатели косинусом расстоянию кроме того у нас есть и бединге для незнакомых слов и что довольно важно здесь обученные модели для русского языка для английского языка для китайского например facebook а у проекта де павлов так что можно очень быстро подключить такую штуку в свою свой пайплайн но при этом мы во первых эта модель она не используется для получения вектор текста целиком то есть она строит именно виктора для конкретных слов поэтому чтобы получить общий вектор текста нужно тут что-то придумывать например усреднять их или усреднять их с перемножением найди офиса и там в разных задач и кто может по разному работать кроме того мы все еще не получаем разные виктора для слов когда это слово при использовании модели встречаются в разных контекстах поскольку во рту века фастекс они обучают один вектор слова для любых контекстов которых это слово встречалась там будет обобщенный контекст если у нас есть слово многозначная например язык это часть тела или это лингвистики язык у него будет один и тот же вектор хотя он может в разных смыслах там разных текста встречаться но у нас косинус на близость пафос текст она уже выше чем косинус на близость pdf pdf даже несмотря на то что в этих фразах пересечения только по стоп-слов в и но если посмотреть на график на котором показано вас текст песни и разложению видно что здесь костра уже не так явно выделяются и качество хуже чем на tf и df использовать только фастекс и мы еще сделали эксперимент объединяли dfs dfs икс-фактора получалось качества абсолютно такой же как при tf и df это не во всех задачах так бывают задачи где объединен то f&d фастекс она работает лучше чем просто fdf или где force текст работает лучше tf это нужно экспериментировать и пробовать конкретно у нас видимо просто обобщающая способность линейной модели повертев pdf и фас текст она уже ограниченное выше чем 092 не поднимется но в принципе вот модель которая линейная модель поверх dff нам уже не плохое качество дает 92 процентов для 10 in that off ну а давайте просто увеличим количество интентов потому что ну на самом деле у нас там их 170 штук возьмем топ-30 интентов видно что качество резко падает аж на 7 пунктов при этом кластер и хоть некоторые из них и сохранились в основном как бы мы теперь не видим вот такой выраженный кластерной структуры а давайте посмотрим на примеры текстов которые начали путаться то есть что произошло у нас в к нашим int n там добавились еще intent и которые пересекаются семантические по словам ну например вот у нас есть текст а если открыть вклад как какие проценты по нему и а я хочу открыть вклад под семь процентов но казалось бы очень похожие тексты но это разные интенты в первом случае человек хочет узнать условия по вкладам а во втором случае человек хочет открыть вклад вот для того чтобы разделять уже такие текста на разные классы нам понадобится что-то более сложная и сейчас мы рассмотрим более сложные модели и в частности мы хотим получить например диктор текста и в частности вектор слова которые уже будет зависеть от контекста про который говорил влад в котором это слово употребляется стандартным способом получить такую штуку является получить вот этим бединге из языковой модели что такое за кого я модель но она решает тогда дам задача языкового моделирования что-то за задача у нас есть последовательность слов например буду говорить только в присутствии своего и мы пытаемся предсказать следующее слово в последовательности языковая модель которая может быть любой их много мы рассмотрим пример чуть попозже выдает нам контекстным бединге мы получили эти контекстные бединге или виктора для каждого слова и зачем и затем используете а виктора предсказываем вероятно следующего слова здесь у нас все точно так же у нас есть вектор размерности словарям и каждому слову назначаем вероятность быть следующим конечно мы же конечно же мы знаем какое слово было в действительности а и считаем ошибку между вероятностью и действительным словом обучаем модель да и на самом деле моделей вот таких языковых моделей довольно много в прошлом году бы вообще бум на языковые модели было предложено очень много разных архитектур было много статей начале года вышла статья про модель который называется эмма тогда это был стоит азарт и идея модели в том что мы сначала строим для каждого слова в нашем тексте по символьный bedding этого слова а потом для них применяемого stm сей таким образом что нас получается уже им бединге учитывающий контекст которым слово встретилось давайте сначала посмотрим как получается по символьным бединге допустим есть слово олег мы разбиваем его на символы для каждого символа мы применяем embedding слой и получаем матрицам беден гав причем она обычно довольно маленькая когда речь идет о символах например размерности 32 потом к этой матрице им беден гав мы применяем одномерную свёртку как обычно делается в нпс макс пудингом в конце и получаем один вектор и к этому вектору еще дополнительно применяются двухслойная так называемая highway сеть это вариация на тему density у нее только есть gate очень похожи на его стоим в каком-то смысле и получаем общий вектор слова причем у этого есть большой плюс что здесь неважно встречалась вошло в обучающей выборке или нет в любом случае вам о модели для него построить какую-то гипотезу и м'бенга после того как мы получили контекстные получили по символьные бединге для каждого слова мы к ним применяем его stm сеть двухслойную и вот на этом шаге постой как мы применили эту слой на ustream сеть обычно просто берется a hidden стоит и берутся хидан стоит и последнего слоя и считается что это контекстные бединге новый алма есть две особенности первое это то что там есть три сечел connection так называемый между входом первого слоя вас темы и выходом то есть мы берем вход в stm и прибавляем к выходу это нужно для того чтобы избегать проблем и исчезающих градиентов кроме того авторы элма статьи предлагают объединять embedding по символьный для каждого слова выход первого слоя вас tm и выход второго слоя was then select с некоторыми весами которые подбираются для каждой задачи это нужно для того чтобы мы учитывали как низкоуровневые признаки именно по символьные бединге так и более высокоуровневые признаки которые дают нам 1 2 свои уста наши задачи мы использовали просто усреднение этих трех беден гав и получая таким образом контекстный bedding для каждого слова ну и что же нам даёт обучение вот такой языковой модели во-первых мы получаем контекст слово в зависимости от того где это слово употребляется то есть мы получим разные dict виктора текстов для слова язык как часть тела и для текста где язык употребляется как лингвистический термин более того так же как для во рту века есть много обычных моделей например для русского языка де павлов их предоставляет то есть можно взять готовую модель попробовать на своей задаче потом нам больше не нужно задумываться о том как усреднять виктора слов так как просто если мы возьмем вектор последнего слова так как это рекуррентная сеть то на как бы аккумулирует в себе вектор всего текста поэтому можем не заморачиваться и получать сразу виктора текстов более того мы можем добыть эту языковую модель для своей задачи есть различные способы для этого например алан fit про который мы приложим ссылку можно будет почитать но единственный минус который сейчас для нас остался это то что данный языковая модель не дает нам гарантии того что текста которые относятся к одному классу то есть к тому интента будут действительно в пространстве располагаться рядом давайте посмотрим сначала на то как у нас по имени поменялось косинус на я близость если мы используем виктора который мы получили из алма то есть видим что хочу were стран с высокими ценами поездку в дорогом месте пойдём мажорно барт действительно стали ближе друг другу и если мы посмотрим на кластер и которые получаются при допустим там так то видно что кластеры тоже более выражены то есть мы уже видим чётко раз-два-три-четыре 10 кластеров при этом мы получаем прирост на один пункт по сравнению с моделью над fdf для топ-30 интентов здесь все еще сохраняется кластерная структура то есть все еще видно что какие-то какие-то тексты которые относятся к одному intent у лежат рядом и мы точно так же получаем прирост по качеству на один пункт но как я уже сказала нет гарантий того что текст и вот а если открыть вклад какие проценты по ним и а я хочу открыть вклад под семь процентов будут далеко друг от друга и соответственно близко к текстам из своих классов то есть мы хотим чтобы какие условия по моему вкладу а если открыть вклад были рядом и а я хочу открыть вклад под семь процентов и открыть вклад были соответственно тоже близко от рука друга эти классы были далеко элма нам этого не дает так как мы просто учим языковую модель если семантически текста похоже то они будут близко ну то есть мы не знает ничего пока что о наших классах вот какой трюк можно применить чтобы вот так раздвинуть как бы векторов а текста в пространстве а можно взять любимую вашу архитектуру нейросети здесь может быть действительно любой архитектур которая вам нравится ну для примера возьмём атом о которых мы рассказали почему нет берем intent то есть какой-то текст и получаем его invading то есть вектор этого текста точно так же делаем для другого en tanto мыс взглянуть монна для другого intent-а и затем когда мы получили для двух текстов их им бединге или векторные представления мы просто посчитаем косинус на старой расстояние между ними косинус на расстоянии это просто единице минус косинус най близость которой мы ранее как познакомились еще до называется сиамской сетью то есть что мы получаем мы хотим чтобы тексты из одного класса например сделай перевод и закинь денег от 1 intent лежали близко в пространстве то есть расстояние между их векторами было как можно меньше в данном случае 0 сделай перевод и дать человека это тексты из разных классов из разных намерений мы хотим чтобы они как можно дальше друг от друга лир лежали вот теперь а а функция потеря том как обучать тусить lotro скажет да то есть мы не можем просто взять и обучают наши нейросеть предсказывать курс на расстояние которое мы ожидаем здесь очень хорошо работает следующий трюк мы будем использовать тройки объектов они называются 3 плиты и соответственно функция потерь который мы используем будет называться 35 лосс вот здесь на слайде изображен триплет там есть м корр объект это синий кружочек позитив объект это зеленый кружочек и негатив объект это красный кружочек негатив объекты encore лежат в разных классах а позитив объекты анкор лежат в одном классе чего мы хотим здесь добиться мы хотим добиться чтобы позитивный объект он был ближе к encore учим негативный после обучения для того чтобы это сделать мы считаем косинус на расстояние между интером и позитивным объектом между интером и негативным объектом и вводим еще гипер параметр который называется морджим это то расстояние которое мы ожидаем что будет между позитивным и негативным объектом и считаем нашу функцию потерь как максимум из нуля и марджан плюс расстояние тонко рода позитивного объекта минус расстояния от n corrado негативного объекта другими словами мы в ходе обучения добиваемся чтобы позитивный объект был ближе к м корр учим негативный хотя бы нам arjan если это произошло то функция потеряны этой тройки будет равна нулю и мы заканчиваем учения если это не так то функция потерь будет положительной и мы будем ее минимизировать в ходе обучения после того как мы обучили такую модель мы еще не получаем классификатор это просто метод для получения митингов таких что и объекты которые лежат в одном intent и скорее всего будут иметь близкие виктора после того как мы получили такую модель мы можем использовать здесь другой метод классификации поверх этих им беден гав и очень хорошо здесь работает кнн поскольку мы получаем и бединге с ярко выраженной кластерной структурой напомню как работает кнр мы берем новый текст получаем для него embedding переводим его вот в это векторное пространство и затем смотрим кто его сосед среди соседей считаем самой частой классы говорим что вот этому классу этот объект новый и принадлежит поскольку у нас а бединге большие размерность 300 и у нас тренировочной выборки около 500 тысяч объектов мы не можем использовать стандартные методы поиска соседи быстро например пол 3 или к д 3 поэтому мы использовать метод который называется и чины sw сначала расскажу что такое nsw это navi гейбл small world это такое граф связные в котором присутствует очень малое количество длинных связей и очень большое количество коротких ребер нашем случае длина ребра будет регулироваться косинус нам расстояние соответственно такой графон строится случайно на наши тренировочные выборки то есть мы между всеми интента мы считаем расстояние потом выбрасываем случайным образом очень большие расстояния так чтобы граф всё ещё оставался связанным но это еще не все после этого собственно и а мы разбиваем этот граф на уровне из это как раз первая буква здесь то есть хайр арки кого мы на каждом уровне только подмножество вершин содержит причем так что каждый следующий уровень он содержит все вершины которые есть на предыдущем и на каждом уровне мы проводим вот этот случайный выбрасывание ребер так чтобы игра всё ещё был связным затем мы берем нашу вершинку для которой мы хотим найти соседей то есть наш новый текст мы берем какую-то случае и на вершину на первом уровне смотрим кто из ее соседей ближе всего к той точки для которой мы ищем соседей и двигаемся вот в соседу случайно вершины затем перемещаемся на следующий уровень повторяем этот процесс до тех пор пока мы не наберём нужное нам количество соседей таких что они скорее всего будут самыми близкими к зеленой точке в данном случае то есть той вершине для которой мы соседи ищем но этот метод он приближенный то есть здесь нет гарантий что мы найдем прямо самых самых близких соседей в каждом случае но у него очень хорошая палата около там 95 99 в зависимости от настроек более того когда вы используете используйте метод ближайших соседей если вы добавляете новый интент и он не слишком пересекается по лексике с по словам теме in тентами которые у вас уже есть то вам не обязательно даже всю модель переобучать вы можете просто кинуть как бы новые точки и у вас получится автоматом добавлен новый класс вы просто переводите векторное пространство добавляете вам даже ничего переобучать ненужном случае если у вас действительно не нужно переобучать с у вас не сильно слова поменялся давайте посмотрим теперь на то как использование сиамской сети какое прироста издает использование сиамской сети ну во первых давайте обратим внимание на график то есть здесь явно видно что сейчас все intent и образуют отдельные кластера причем эти кластера дествительно отодвинут друг от друга за исключением небольших выбросов а это нам позволяет даже линейной модели гораздо лучше разделять пространство то есть мы получили прирост на два пункта по сравнению с моделью с языковой моделью с ума для 30 интентов это кластерная структура все еще сохраняется то есть мы все еще видим ярко выраженный кластером примерно 30 штук с небольшими выбросами и мы опять же получаем прирост на один пункт давайте подведём небольшой река небольшие итоги подведем самый первый метод шаблоны словари он все еще используется и вполне себе допустим когда у вас немного интентов когда они хорошо разделимы действительно описываются какими-то словарями или шаблонами или тогда у вас мало примеров в классе ну то есть действительно бывают такие классы ну в нашем случае намерение пользователи который потом больше двадцатью тридцатью предложения мениск но и нет смысла от никакие модели для них делать можно просто по словарями обойтись если же у вас уже добавляется больше интентов но при этом и не все еще хорошо разделяются то есть у них мало пересекающихся слов можно обходиться стандартными простыми линейными моделями поверх dff то есть не заморачиваться очень быстро в принципе такая такая модель обучается с ходу получается хорошее качество дальше вы уже там крутите выборку например какие-то параметры ну чуть-чуть растете если же вам хочется перефразирование учитывать стоит все еще посмотреть в сторону во рту век фас текста то рабочей модели несмотря на то что они в нашей конкретной задачи прироста не дали так как скорее всего там просто сложнее зависимости между словами и действительно этой модели простой но она не сможет уловить эту разницу но вашей задачи например в вашей классификации текстов может помочь и стоит попробовать тем более мы считаем что то все так же быстро как и простая модель потому что есть куча при добыче ных и эта сетка учится очень быстро даже на своих данных чуть чуть больше времени скорее всего займет обучение элма если вы будете его до обучать на своих данных потому что вам понадобится чуть больше данных и нужно будет разобраться немножко с тем как это загружать и так далее но в принципе тоже не супер долго и прирост по качеству уже значительны можно получить тем более если у вас например текста classic классификация текстов очень похожи на на семантические особенности действительно у вас к этому классу относятся просто по смыслу одинаковые предложения но если вас уже что-то более сложную да там пересекаются очень сильно по смыслу тексты из разных классов тогда вам действительно нужно уже там более продвинутые штуки использовать в dip лёнинг и так далее но в нашем случае мы сейчас рассказывали про а сиамскую сеть да действительно этот метод займет больше времени потому что во первых вот для сиамской сети и в случае конкретно для одессы сам в принципе dsm и the sims к сети разновидность dsm для обучения такой сети нужно очень много примеров в каждом классе то есть каждый на в случае намерение intent должен быть представлен большим количеством позитивных и негативных примеров плюс вам нужно будет придумать вот эту саму нейронную архитектуру протестировать несколько это все обучить но вы скорее всего получите очень хороший прирост по качеству и стоит это использовать давайте еще раз посмотрим на графики на то как у нас росло качества и выделялись глостера при усложнении модели на топ 30 интентах то есть видно что для dvd разделение наших этих топ-30 интентов это сложная задача и исправляется мягко скажем эта модель не очень но если мы используем уже языковую модель то кластером начинают выделяться и мы прирастаем на один пункт по качеству при использовании сиамской сети кластер а уже четко выделены и мы еще один пункт по качеству получаем при этом а в конце прошлого года вышел birth это супер но сейчас де-факто state of the art нлп поэтому при использовании его с некоторыми модификациями а вот в настоящее время мы можем получить еще а прирост на два пункта и вот если сравнить с предыдущей моделью то видно что просто сиамская сеть дает какую-то кластерную структуру но и использование bird от вообще от отодвигает krutch кастеры друг от друга и получается прямо четко красивая картинка все ссылки которые на материалы которые мы упоминали на все статьи будут приведены по вот этому qr коду да там же ссылка на презентацию то сможете посмотреть обзор на фотографирует у нас все спасибо за внимание ваш вопрос большое спасибо за доклад теперь можно задавать вопросы привет меня зовут антон хотел задать несколько вопросов первое вот-вот казалинск а моделей вот задача вас грубая продукт это это диалоговый год по сути правильно вопрос как вы будете с проблемы диалогового контекста к примеру вы сказали отправки или решетку а потом выйти и павлу как вы боретесь с этим это отдельная задача в рамках данного доклада мы ее не показывали это в рамках данного доклада мы просто классификацию текстов показывали у нас отдельная модель учитывает контекст окей выговоришь что вы использовались тоже модели такие как берт и просушим про холод как вы боретесь лотность то что допустим burton там же 3 гигабайта 3 гигабайта есть оперативки в его памяти и при этом там есть он не запускается на gpu у него там очень очень долго идут mb деньки как вы с этим боретесь так как у вас там тысячи миллионы клиентов но у нас есть на самом деле класса пью то есть мы на нём на самом деле запускаем bird запускаем и так далее поэтому у нас в принципе но там где-то я думаю около 100 миллисекунд на один имбилдинг тратит свои номер тийной алма вот в принципе это нам нам этого хватает чтобы выдержать нагрузку текущую окей и последний как вы боретесь с короткими и длинными фразами то что bird изначально ты павловский и обучены у гугла обучался на длинных текстах а там задач маскирование продолжаем логических логического продолжения фраза как боролись с тем что на коротких фразах и на две фразы абсолютно разные подбор но мы его до обучали на наших конкретно текст нас на самом деле очень-очень много генерируется текста в чате клиентами поэтому у нас там есть ну не знаю порядка там десятков миллионов в общем текстов дроидами оси коротко мы давайте своем доносите до то что надо обучение на см10 потом соответственно еще до обучения на задач классификацию то есть на задач разделения не беден гав через омску сеть на 100000 и вообще редко бывает такое что ты берешь уже обученная но схода работает то есть конечно же на своем домене нужно до обучать ну как с бертом который de pollo есть принципе это работает тоже там его можно применять задачах сразу он довольно хорошее качество дает везде как выбирается куда классифицировать какое-то слово там если он находится на границе кластер а то есть вот какая то это века эта вероятность на я мера и как вот выбирается этот порог что вот допустим там слова язык отнесется к классу тело языку мы конкретно классификацию тексту делаем точно там есть обычно несколько слов в тексте i am как мы используем кнм то есть вот метод джек соседями может переутомился у него же четкое разделение там нету вероятностями кайта семантики я почему там есть есть способ получить из кнн вероятности и на самом деле мы делаем отсечку по вероятность то есть если она очень низкая из всех пороговая функция да ну да это как бы порог он динамически высчитывается или это какой-то статические параметры на какой на вход вода дома динамическими счетам нам нам нужно больше первую очередь престижем потому что у нас ну то есть если будет не правильно отвечать от плохо поэтому мы в первую очередь по престижным подгоняем пороге если а если мы не уверены мы переспрашивать но странные вопросы вот если люди как бы общается и ищут например там фраза из фильма то есть фраза не относится вообще не не не относится ни к операции не относится к кому-то поиск каких-то услуг а просто это как бы человек ищет фильм или песню какую-то то есть это по сути ну какой то текст который мы не понять не понятно что искать вот как в такой ситуации это какой-то выброс получается на самом деле у нас несколько моделей и отдельная модель отвечает за поиск фильмов в которой мы индекса соответственно обновляем то есть каждый четверг выходит фильм и перед четвергом мы загружаем автоматически подгружаем фильмы явно соседка зале раз обновляется до ну вот которой обучающий для фильмов но отдельно им не имею обучающая выборка но не вы че там да там не обучающая выборка но и сами индекса мы обновляем по фильму сего так привет у меня сначала комментарий к комментариям к вопросу по нашему опыту embedding калмана цпу сотня 150 миллисекунд поэтому ничего страшного там нет но от количества ядер еще зависит а ну пошли сигнал вот а и вы частично ответили прямо сейчас на мой вопрос на я все равно его задам то есть у вас весь доклад был поток как подогнать mb денги под решение зачем методом кнр в том числе да ну скорее доклады про про то что не обязательно всегда использовать конечно беда и к иным это говорил и да ну просто этом понятного вам кучный костров нужно именно для этого до вопрос почему вы остановились на кнн с чем сравнивали какие результаты были сравнили на самом деле совсем и с линейными моделями и пробовали слои лука и нам просто есть очень большой плюс который довольно важно когда intent и постоянно добавляются то что мы можем добавлять новый интент там просто получаете вам бединге сразу же классифицировать но в целом да некоторые модели они могут отдавать на один пункт выше чем каин данная задача потому-то зависит от того какие вас intent и как чувствуем то динамично для нас выбор кнр в том числе встал мы выбрали к нам в числе потому что мы могли быстро добавлять новый интент все они на самом деле для фильмов другая модель немножко работает конкретно для фильмов здрасте спасибо а скажите ваше мнение по поводу то таких яд вот платформ типа вид и прочих уже существующей понятно что они наверное для такого использования наших не в тему просто ваше мнение на эту тему что как вообще вы поделите я прямо чтобы по формуле не пользовался могу на примере раз рассказать это библиотека открыта до 1 а на самом деле в плане настройки допустим довольно удобно то есть в этом за ближе или intent и вам даже про машины лирник ничего практически знать не надо чтоб это использовать но она чисто с инженерной точки зрения на мой взгляд не очень удачно написано поскольку ее надо очень долго заводить чтобы она прям реально заработала в принципе диалога и платформа это удобно когда он небольшой бизнес и вам нужно что-то подключить когда у вас нет им большой команды разработки которые делать он прятал что финьков у нас довольно большая команда этим занимается поэтому нас в этом диалоговый движок используйте когда у вас нету 170 интентов плюс ну да если контентов там 510 допустим тоже отлично можно сколько до сценарии достаточно понятны и да нормальный подход хороший пользоваться готовыми не нужна своя писать дорого бессмысленным перед этими спасибо за доклад такой вопрос вот вы сказали что во сто семьдесят интентов да правильно но обычно на самом деле больше многие здесь неважно набор этих антенн он плоский или есть какая-то иерархия все таки есть йерархии да но я могу привести пример пользователь говорит я не почетом лимит до лимит просто говорит вот это может быть кредитный лимит может быть лимит на снятие наличных например или еще что-то там соответственно доска структуру получаются вот это вот решается то из пользователь горит лимит и вы не понимаете это вот один intent или другой интернет спрашиваем его научным показывает подсказку вы хотите лимит по карте по снятию либо который вот примыкают к этому вот промежуточному int n туда и перечисляете задавая пользовать там вы имели виду вот это вот это прямо по кабак покажем подсказок кредит мы уверены что этого дойти до но там чтоб не получилось бесконечного списка конечно понятно спасибо большое меня зовут артем спасибо большое за доклад очень интересно у меня вопрос следующий на жизнь менять постоянно да и там сколько ты лет назад распространенная история была сказать 50 рубли руи мне пожалуйста да то есть это мемы словарные появляются так далее вот как вы мониторите что ваш вот с этим справляется и какую-то обучаетесь на вот таких вот мемасиков изменениях речь спасибо большое ну во-первых мы переключаем модели часто то есть мы добавляем новые данные как пользовательские так и принципе из интернета и перри обучаем но у нас такая область что очень часто появляются новые модели и их есть смысл пробовать прошлом году нельзя было еще таких результатов такого кончик качество получить там как концу вот прошлого года например да когда появился bird поэтому мы и данные обновляемой модели перри обучаем и вот мы берем и пользовательские данные более того нам еще как какие-то конкретные примеры прямо скидываю то есть вот вот этот пример нужно определять то есть это происходит просто регулярно они там какая-то метрика есть где-нибудь там графа не если график метрика конечно метрики есть да тоже есть но мы регулярно размечаем то есть у нас там условную раз в неделю появляются новые данные которые можно уже использовать спасибо большое за доклад такие вопросы 1 вот вы рассказали что у вас не сеть вот эту она молчит и минусами intent мы определяем что он хочет пользователь перевести кому-то а как вы выделяете вот именно слова кому это отдельная модель начала первый шаг выбираем это потом уже они прям отдельные модели для каждого да вот все под пар диалог отдельно на самом деле хорошо пасе большой и не второй вопрос это вот есть уже спрашивали про чат-бот модель платформы есть еще 1 л флаг угла есть объем в абсент а вы не пробовали с ними сравнивать грубо говоря у них загрузить да и сравнить насколько они лучше хуже матч от это все дело прямо так не пробовали на я думаю что хуже поскольку они когда есть что то такое очень общее что можно в разных задачах использую для разных акцентов и так далее обычно но хуже работает поскольку нет специфики области у нас область довольно такая специфично именно банковский там финн тех и так далее тут тексты очень специфичной которые пользователи пишут поэтому у нас ну прям заточена под эту модель на самом деле вот влад прям завтра читает доклад где он чуть ботан орос идеал под не банковскую тему но все равно и там сходу все равно то только на другой конференцию поднастроить сафонов после больше вообщем да мы пробовали но не для наших данных ещё вопросы есть хорошо тогда мы попросим давайте поблагодарим еще раз богаче к и можете выбрать лучший вопрос не на самом деле первый самый понравился 2 уже там было три вопроса сразу складу так вроде все по делу"
}