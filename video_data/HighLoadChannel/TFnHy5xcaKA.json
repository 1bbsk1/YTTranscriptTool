{
  "video_id": "TFnHy5xcaKA",
  "channel": "HighLoadChannel",
  "title": "Magnit Tech: сервисы остатков и цен на Go. Как справиться с большими потоками данных / Д.Жаворонков",
  "views": 276,
  "duration": 2208,
  "published": "2024-10-29T02:48:12-07:00",
  "text": "сейчас на сцену выйдет Дмитрий Жаворонков из магтек и расскажет как они справлялись с большим потоком данных на примере сервиса остатков и цен поприветствуем раз раз Да добрый день коллеги Всем привет начать доклад хотел с народной поговорки веселились посчитали прослезились э поговорка актуальна для торговли в онлайне а Конкретно для проблемы остатков и цен собственно кликер Переключайся кликер не переключается раз-раз собственно так в рамках доклада хочу рассказать как мы делали систему остатков и цен С какими сложностями мы сталкивались как мы проектировали архитектуру как тюли нашу базу данных как разрабатывали собственно непосредственно приложение как мы всё это дело тестировали И как мы измеряли наше приложение как Мы понимали что оно работает что оно в порядке что оно вывозит нужную нам нагрузку поехали этот доклад будет полезен разработчикам архитекторам системным аналитикам всем кто обм к архитектуре начнём вообще Давайте начнём с того что такое остатки и цены вообще да то есть что за термины Я думаю каждый из нас когда-либо заказывал товары в онлайн-магазине и возможно сталкивался с ситуацией когда ему звонит сборщик и говорит Извините вашего товара нет в наличии или же Извините на этот товар другая цена Ваше лицо в этот момент выглядит примерно Вот так и скорее всего вы недовольны в этот момен это недовольство стараемся измерять и измеряем с помощью двух метрик это идеальность корзины и комплектность корзины собственно не в давая скучные подробности комплектность корзины - это отношение привезённых вам позиций к количеству заказанных вами позиций идеальность корзины считается чуть посложнее она учитывает конкретное количество каждого товара привезённого вам собственно определившись с метриками Давайте введём некоторый глоссарий и определимся с основными терминами самое базовое магазин Да это обычный магазин офлайн в нашем случае магазин магнита да А продукт - это любой товар в рамках конкретного магазина и он обладает одновременно остатками то есть фактическое значение количество в магазине как раз влияет на идел баскет и цена - это фактическая стоимость товара в рамках конкретного магазина и собственно она влияет на маржинальность А определившись с глоссария Давайте поймём как у нас стояла задача то есть что было перед нами было в районе 30.000 магазинов Магнит в каждом из них было порядка 10-20 000 ску то есть товарных позиций и была уже готовая система на базе Тарантула как выглядела загрузка данных в неё То есть каждый магазин буквально кажды на каждом магазине стояло железо раз в 8 минут отправлял полный слепок данных по jpc во внешнее Решение вот на базе Тарантула далее Это внешнее решение уже обслуживала синхронных клиентов то есть там приложения какие-то сервисы корзины сервисы поиска и так далее Собственно решение-то уже есть оно работает оно уже приносит деньги в чём проблема-то Зачем нужно что-то с нуля делать а была проблема то есть во-первых начали ухудшаться бизнес-мероприятий SL То есть под ростом нагрузки и самое главное а у нас не было технологического суверенитета То есть это внешнее решение мы за него платим деньги оно хости в другом контуре и Ну мы понимаем что суверенитет - это важная часть технические с да то есть Как выглядели графики отдачи остатков В девяносто девятом девяносто пятом Тиле соответственно Вы можете наблюдать И даже иногда это выглядело вот так собственно с такой картиной мы пришли в Магнит и перед нами стала задача Да Нам нужно поддержать текущую архитектуру То есть то что каждый магазин раз 8 минут отправляет полный слепок своих данных нам нужно учесть текущей А мы должны учитывать то что у нас ожидается увеличение трафика на один порядок и как всегда это приправленные да А ещё было одно специфичное требование от нашего сио как он сказал мы не должны просто скопировать внешнее решение мы должны построить гибкую платформу гибкую цифр а напоминаю у нас было в районе 30.000 магазинов в каждом из них по 10-20 000 товаров но мы берём по верхней планке по 20.000 и каждая каждый товар содержал в себе информацию Приблизительно на 150 байт то есть что туда входило дишни магазина дишни товара две цены базовая акционная а фактическое значение остатка и какая-то другая информация которая потенциально полезного для бизнеса Ну там место хранения в магазине может быть или номер на весах Ну обычный тевсе штуки в общем несложными математическими вычислениями мы получаем 85 ГБ это тот объём данных который пролетает через сеть каждые 8 минут напоминаю далее переходим к выбору архитектуры классический выбор между монолитом и микросервисами да выбирая между ними нужно помнить что Понятно каждый из них обладает своими плюсами и минусами Но самое главное вы балансируют в самом начале и параллельностью возможностью параллели разработку Дине и в конце вашего проекта собственно также когда вы выбираете микросервисы у вас появляется такая возможность как graceful degradation напоминаю да То есть вы можете не полностью деградировать на системе а выборочно падать по маленьким микросервиса при этом как бы основное приложение будет жить и отдавать какую-то информацию собственно Мы выбираем микросервисы и переходим к выбору а как нам нужно загружать наши данные да то есть загружать в нашу систему синхронно или асинхронно это тоже технологический трейдо и как всегда у нас есть свои плюсы свои минусы напоминаю Да что при синхронном подходе нам достаточно просто разрабатывать нам достаточно просто отлаживать эту систему очень легко ловить стейт а при асинхронной архитектуре это делать сложнее но у вас появляется некий демфер демфер который может контролировать пиковые нагрузки Например если у нас магазины сойдут с ума и начнут слать тамм в 10 раз больше информации мы этим демфер можем выдержать нашу нагрузку выдержать всплеск И когда вы выбираете асинхронную архитектуру Если вы её выбираете Не нужно забывать об eventual consistency да то есть у нас согласованность в конечном счёте Что это значит Это значит что в какой-то момент ваша система Может быть разбалансированный но в конечном итоге она придёт туда обычно конечном итоге имеется в виду ближайшее время Это понятное дело не года и не дни а также вам это нужно учитывать при разработке и вашему бизнесу нужно учитывать при эксплуатации вашей системы А мы выбрали асинхронную архитектуру тем самым выбрав Надёжность нашей систе при пиковых нагрузках далее пошл выбор хранилища Когда вы выбираете хранилище когда мы выбирали хранилище вы выбираете исходя из различных критериев таких так производительность транзакционные экспертиза да то есть мы неплохо умели готовить постс и похуже действительно похуже умели готовить мон и собственно Нест Лукава мы дальше мы выбирали брокер Да брокер - это тоже своего рода хранилище поэтому критерии для выбора у него похожие собственно мы выбрали кавка и перешли к выбору протокола взаимодействия то есть мы обсудили как мы будем загружать информацию в нашу систему как её будут получать наши клиенты да синхронные клиенты Ну собственно стандартом проколы Ира что jpc как обладает большим перформанса за счёт прото бафа но при этом у нас есть фронтенд у нас есть старые контракты по сту и мы не тащим это всё дело туда собственно Мы выбираем оба этих протокола взаимодействия просто реализуя два разных транспортных уровня выбор языка да Когда вы выбираете язык Вы тоже выбираете определённые критерии например Нам нужен был производительный умеющий достаточно простой язык и современным кони поэтому мы выбрали Всем спасибо нет шутка конечно да поэтому мы выбрали Гоше и технологический стек выглядел следующим образом у нас были микросервисы асинхронные в качестве базы данных был по в качестве брокера кавка jpc на синхронных клиентов и в качестве языка го такая классическая вакансия Наре перейм к реализации как выглядела загрузка данных в нашу систему То есть каждый магазин то есть на каждом магазине расположен железный сервер на нём расположена база данных который раз в 8 минут отправляет полный слепок данных по grpc во внешнее решение в Тарантул Ну первым делом Нам нужно вклиниться в этот поток забрать этот поток на себя и разделять его на две версии одну мы отправляем во внешнее решение в Тарантул чтобы поддержать обратную совместимость и вторую часть вторую копию потока мы отправляем в нашу будущую систему наша будущая система начинается свка а это и есть тот демфер который будет выдерживать пиковую нагрузку от магазинов Если вдруг они начнут слать что-то другое или их количество увеличится кратно а далее мы поток разделяем на два разделяем на поток отдельно остатков и отдельно поток цен А мы осмысленно это сделали потому что это две разные сущности остатки меняются чаще в физическом мире у них разные бизнес правила их даже разрабатывать потенциально могут две разные команды это два разных домена и далее каждый из этих доменов дется на уже маленькие микросервисы общий из них который - это микросервис постфиксом да то есть и Price это как раз интересный нам сервис который отвечает за высокодозной с архитектурой мы переходим к тюнингу базы мы с самого начала понимали что база данных для нас будет м скорее всего и озадачили нам подойти к масштабированию этого дела нам нужно было выбрать шардирование или партицирование какая у нас будет репликация какое будет железо как мы под тюним наш автовакуум начнём мы не пошли в шардирование потому что погр в него не умеет из-под коробки А время знакомиться с ПГ шаманом или чем-то подобным У нас не было и экспертизы не было собственно мы пошли в партицирование и выбрали определённое количество партий 10.000 штук это достаточно много и чем мы чем мы руководствовались при выборе количества да то есть ну во-первых это была документация постгрес которая говорила нам о накладных расходах на выбор партиции и это был профиль нагрузки то есть профиль нагрузки у нас все операции и запись и чтение происходят в рамках конкретного ST ID конкретного магазина поэтому мы выбираем большое количество партиции А партиции по хэш ST ID то есть натурально берётся хэш функция от идентификатора магазина делится на количество партиции в нашем случае это 10.000 и просто по остатку отделение выбирает нужную партиции и при записи и при чтении и ставший уже стандартом декларативный подход описания партиции репликация выглядела достаточно стандартно То есть у нас вся запись идёт на мастер А чтение мы кидаем на словы словы масштабируется достаточно просто горизонтально добавляются новые реплики репликация асинхронная через вал вакуумирование да Мы помним у нас пос pog - это mvcc mvcc - это блонг таблицы индексов а собственно подробно все настройки здесь не имеют смысла рассматривать скажу только то что мы подняли 32 воркера и достаточно строго настроили трешхолд то есть а Триггер для запуска авто вакуума у нас стоял достаточно строгий то есть небольшое количество изменений требовало а запуска процесса вакуумирования и о чём ещё стоит поговорить да то есть для улучшения перформанса работы а нужно использовать Connection Pool да то есть напоминаю он помогает утилизировать запросы у него есть failover из-под коробки и он умеет балансировку по словам Connection Pool работает в трёх разных режимах то есть тоже напоминаю Да в сессион трансакционным и statement разница между которыми только в как раз строгости триггера возврата коннекта обратно в пул то есть в случае сессии - это конец сессии транзакция конец транзакции сй конец запроса он такой специфичный для P Проси используемый а напоминаю у нас профиль нагрузки - это большое количество маленьких транзакций Мы выбираем режим Рак пунга и в сухом остатке работа с pog у нас выглядит следующим образом Да у нас пос пй версии в качестве конек Пула мы используем осей 32 ядра 64 оперативы у нас 10.000 партиции один Мастер и два слова словы накидываются достаточно просто Если нужно будет масштабировать чтения мы доем их брокер да то есть как готовится брокер Мы тоже начинаем с выбора количества партиции а формула достаточно простая вы берёте желаемую вами пропускную способность напоминаю у нас 85 Гб в раз 8 минут - это максимальная пропускная способность которую мы всё равно делим ещё на два но берём по верхней планки а скорость партиции сам кавка советует брать 10 Мб в секунду понятно что это зависит от железа от сети но в среднем по больнице можно брать 10 Мб в секунду А собственно математические вычисления мы делим одно на другое и получаем приблизительно 18 партий накидывает 30% и выбираем 24 партиции ещё одна характеристика топика - это ретеншн да то есть время жизни или время или вес который отвечает за хранение данных в топике по нашему профилю нагрузки напоминаю раз в 8 минут каждый магазин отправляет полный слепок нам не нужно было классическое использование Кафки Когда вы пишете такой Лог последовательный и в случае потери данных вы полностью восстанавливались с какого-то офсета нам нужно было некое такое хранилище которое держит оперативные данные мы выбрали четыре часа - Это то время которое мы позволяем себе лежать и не и при этом поднявшись не потерять ни одного сообщения а минимальные синхронизированные реплики да то есть тоже если бы мне показали такой слайд Я бы наверное А спросил Чувак я не помню всех этих характеристик поэтому подробно можно поговорить позже напомню что у нас было три брокера и реплицировать мы только на два из них при записи но дожидались ответа только одного лидера то есть мы тем самым выбрали доступность и скорость взаимодействия в пользу консистентность то есть мы выбрали доступность потому что у нас есть такая защита от дурака у нас есть возможность когда магазины полностью отправят ещё один слепок И даже если мы что-то потеряли какую-то небольшую порцию данных мы ожидаем её что она через 8 минут придёт и это никак не повлияет на бизнес А собственно ТТХ Кафки выглядит следующим образом это три брокера 24 партиции 4 часа топи 2 и одна синхронизированная реплика при записи железово ядер 32 оперативы го да перейдём к го го как говорил ро Пайк - это простой язык и поэтому мы ничего сложного на языке го Не решали потому что какая бы ваша система не была сложная всё равно это всё сводится к тому что вы откуда-то берте Джейсон Ну в нашем случае проба записываете его в баску потом доста из Баски и отдаёте его наверх собственно Давайте поговорим про эти операции а Первое - это работа с кавка да то есть выбирая между драйверами мы сразу же отключили confluent потому что он нёс в себе в зависимость си и выбирали между сама и кавка Go на тот момент в Магните уже использовалось кавка Go и были проведены некоторые бенчмарки сравнения производительности нам показалось что этой производительности достаточно и интерфейс взаимодействия с кавка в кавка Go был собственно интереснее мы выбрали кавка Go и как и любой другой драйвер Он позволяет настраивать Да работу с кавка первая интересная настройка - это bch sze то есть дефолтном в единицу Это означает что при каждом записи используя драйвер вы отправляете сетевой вызов кавка А это не нужно делать при больших нагрузках вам нужно накопить некий буфер Да и отправить пачкой сообщения собственно изменив его до 1000 Вы прям буквально на три порядка увеличиваете пропускную способность драйвера а Контер параметр Да который говорит что если вы не набрали эту пачку но через какое-то гарантированное время вам нужно сделать сетевой вызов а эмпирическим путём мы выставили его 200 миллисекунд и в купе Это дало хорошую пропускную способность А Мы помним что у нас есть партиции да в кавка и нам нужно как-то партицирование в одну партиции чего а не прочитали в разном порядке Мы использовали в качестве балансиров балансировщика Прошу прощения хэш и настройка коню - это собственно максимальное количество байт при подтягивании данных тут всё просто мы выбрали 10 Мб работа с pog начиналась тоже с выбора драйвера И на самом деле выбора здесь не было потому что Li PQ он уже запри а и мы выбрали pgx Ну точ Он с нами был сразу и собственно Что интересного можно рассказать про pgx на самом деле не знаю что можно интересного рассказать а во-первых Используйте трассировку запросов да то есть pgx предоставляет интерфейс Когда вы можете подсунуть свой трассировка а напоминаю трассировка запросов поможет вам дебажить ваш код в случае чего если вы хотите узнать где у вас расположен botle Neck это позволяло нам посмотреть на примере конкретного запроса сколько вот эти верхние колбаски - это весь таймлайна одна маленькая - это запрос в базу собственно м вы можете посмотреть А как М как долго выполняется ваш запрос а протокол выполнения запросов Да напоминаю у нас транзакционный режим пуле и те кто работали с Go по-любому сталкивались вот с такой ситуацией да то есть у нас классическое выполнение запроса вы отправляете первый сетевой вызов на так называемую предпо готовку запроса Да при парку и получаете некий идентификатор этого запроса и далее в другом коннекте вы отправляете запрос на выполнение и получаете ошибку типа statement ID does not exist что-то подобное А как решить эту проблему драйвер pgx позволяет это сделать из-под коробки выставив query Simple прокол в предпочтительный способ выполнения запросов и мы не используем РМ да то есть тоже холивар най вопрос возможно не холивар для Хайда да то есть при всех своих плюсах РМ как бы имеет один большой жирный минус то что он проигрывает по перформанс сырому выполнению данных э суровому выполнению запросов собственно мы не используем РМ и вся работа с pog у нас сводится к тому что мы используем pgx мы трассирующий себе стресс-тестирования а выбирая между инструментами мы сразу отказались от Яндек Танка У нас с ним был плохой опыт и мы не выбрали К6 потому что он всё равно заставлял нас писать какую-то логику на языке программирования мы выбрали самый интересный вариант - это самописный МОК сервис А собственно наш самописный МОК сервис А был написан за один вечер на коленке и Он умел притворяться магазинами То есть он изображал нужно конфигурирован количеством магазинов да Он отправлял в какое-то заданное количество потоков информацию с заданной задержкой и ещё и заданным батч сайз и всё это визуализировать мы с помощью классики граан графана и прометеус а Первая Попытка барабанная дробь и мы падаем по CPU то есть мы падаем по CPU и не понимаем в чём дело на самом деле Проблема была прозаичный дефолтном месте где мы хостилис коннекторов для Паг у pasg каждый конект это достаточно дорогостоящая операция это и мы выставляем лимит коннекто в 400 что тоже немало на самом деле но каждая операция У нас - это небольшая транзакция которую в принципе быстро возвращает Connect в пол а попытка номер два пробуем ещё раз и у нас растёт кою lck мы не можем обработать всю информацию которую ожидали То есть у нас большое количество партиции у нас независимая работа с разными магазинами Ну в рамках конкретной партиции и мы всё равно не успеваем в чём дело а всё оказалось тоже весьма прозаично мы забыли добавить балк операции то есть мы в классическом исполнении инсорсинг Мы пытались обработать каждый Сток и каждую цену отдельно по одному записать в баску Ну конечно При таком объёме данных это было невозможно мы стали обрабатывать по 1.000 штук то есть мы стали натурально брать 1.000 штук которые прислал нам магазин и записывать в базу данных стараться записать там insert on Conflict do update собственно Попытка номер три и у нас получается мы можем обработать нужное намм количество данных и наш коню падает гораздо быстрее чем растёт мы справляемся с потоками данных сохраняем всё в базу данных и максимальный рейт на запись который мы добились он был 150.000 150.000 строк в секунду мы вставляли или оберти в пагр и это достаточно хороший результат нам показался собственно проведя стресс-тестирования в эксплуатацию нашу систему нужно было придумать а как мы будем мониторить нашу систему да то есть как мы будем понимать что она Здорова что она что она работает так как мы планировали и собственно переходим к плавно к алер и к аналитике а итеративный путём да мы а выбрали удобный для нас интерфейс Когда Вы заходите в систему и видите некое overview то есть на дашборде вы видите как чувствует себя ваша система это ошибки варнинг может быть Не дай Бог паники А мы хотим измерять транспорт Мы хотим смотреть Кто в нас входит С какой скоростью С С какой частотой Как часто Как быстро мы отвечаем Мы хотим трекать базу Мы хотим смотреть как долго или как быстро выполняются наши запросы повесить некоторые алерты да на трешхолд вые значения Мы хотим трекать базу с точки зрения железа и мы хотим мерить нашу кафку да то есть мы хотим смотреть как быстро мы справляемся с входящим потоком Когда у нас происходят всплески с чем они связаны может быть Конечно мы хотим видеть логи то есть отдельно все и детерминировано по ошибкам и по варнин определившись техническими метриками и введя уже систему в эксплуатацию к нам начали приходить с вопросами бизнес стал приходить А почему вы показали цену 2 дня назад Вот такую Почему вы показали остаток неделю назад Вот такой ведь он должен был быть другой и нам понадобилась аналитика да то есть понадобилась бизне аналитика мы хотели иметь возможность отвечать на такие вопросы напоминаю у нас в системе есть кавка и это из-под коробки позволяет вам поднять консьюмер на каком-то аналитическом хранилища А мы выбрали Клик House и Подняли там специальный коню с типом кавка кавка Engine который с помощью специальные матюх прям натурально материальное представление которое берёт данные из таблицы свка и перекладывает их в Таргет ную таблицу merge 3 обыч merge 3 для визуализации каких-либо метрик что что это было Ну это было например вот так то есть мы могли посмотреть как у нас менялся остаток в рамках конкретного ID и товара мы могли посмотреть какую-то другую информацию полезную для бизнеса то есть количество там товаров в магазине да это количество заблокированных не заблокированных магазинов и собственно Когда мы уже внедрили нашу аналитику нам нужно было понять как работать с ошибками да то есть есть такой паттерн Zero Error policy который говорит что у вас не должно быть нормального уровня все ошибки то есть любой вот деплой да Когда у нас что-то катится и все говорят Ну 100 ошибок это нормально это такого быть не должно все ошибки имеют критический уровень и если ошибка не критическая то вам нужно понижать её северити но этот пункт под звёздочкой потому что мы все понимаем что в сложных системах в распределённых системах такого быть не может То есть это Утопия И поэтому это не то чего нужно достичь но то к чему нужно стремиться Подводя итоги хочется заметить что мы ски зрения техники построили систему на микросервисах круто почали пог добились рейд на запись 150.000 чего Мы не ожидали Но это было очень интересно и респонс Тай на чтения Мы уменьшили в 10 раз с точки зрения бизнеса Мы тоже повлияли на Л Бакет он увеличился порядка 10% и самое главное у нас появился технологический суверенитет А мы как команда разработки получили крутой опыт и собственно на этом У меня всё Спасибо Буду рад ответить на ваши вопросы коллеги задаём вопросы Ох как вас много как бы вас всех успеть вроде успеваем Привет Спасибо я первый я первый Меня зовут Надир Я из магнита Это неподготовленный вопрос правда интересно Я видел как это всё развивается мне стало интересно ты рассказывал про аналитику про мониторинг но про те системы которые вы сами сделали там вот с ST R Price R а но при этом у вас есть утилит на магазинах 20.000 магазинов 20.000 утилит Так вы их как-то мониторить или нет Ну да спасибо за вопрос Надир А да что мы мониторим мы мониторим просто ситуацию во-первых если они перестали нам отправлять сообщения То есть вот отрубилась связь мы выставили артин на это дело то есть произошла авария на магазинах и мы смотрим примерный объём который был на прошлой неделе и на этой неделе то есть если он отличается критично значит что-то произошло либо магазины как бы стали отправлять другую информацию либо добавилось какое-то количество магазинов или ушло какое-то количество магазинов то есть с этих двух сторон мы измеряем наше решение Спасибо за доклад Меня зовут Михаил у меня пара вопросов первый это Неужели вы гоняли все цены остатки а токо Изе чем вы объединили цены и остатки в один поток а потом его разделили Если знаете что цены меняются раз в сутки условно а остатки Раз в 5 минут спасибо за вопрос ответ будет на самом деле один на оба вопроса То есть когда мы пришли систе то есть на каждом магазине расположен определённый софт который уже умеет так работать то есть это было leg да какое-то и первый вопрос который мы задали был именно такой А почему не слать дельты да то есть вот за 8 минут изменилась только маленькая часть и вот такой Рей на самом де Ну пожи но при этом сказа вы как бы абсолютно правы Приходите через год Возможно мы что-то сделаем то есть разработка софта на магнитах на магнитах на магазинах да то есть она ограничена железом и ограничена каким-то капасити по разработке то есть Нам нужно было решать проблему Здесь и сейчас но вот такой подход он в таргете То есть как только так сразу Спасибо большое спасибо за доклад У меня вопрос на каки если вам нуж только оперативные данные храните последних 4 часа Почему не хранить просто последние сообщение по там retention polic Compact Да спасибо за вопрос смотри 4 часа было выбрано вот прям просто на шару на самом деле скажу честно то есть мы выбирали Какое количество нам хотелось бы считать А то есть когда мы ещё не знали как быстро мы сможем обрабатывать данные Мы думали что ну 4 часа - это то время да которое мы можем полежать условно и через 4 часа полностью восстановить й даже если вдруг изменилось только там ну вот представим себе ситуацию какой-нибудь старый там Магнит в деревне да где изменился остаток например в 12:00 и больше там ничего не менялось за эти 4 часа Вот мы через 4 часа поднимемся и всё равно не потеряем это изменение собственно всё здравствуйте Дмитрий Спасибо большое за доклад правда интересно тут в конце доклада сказал что удалось ускорить получение данных из базы А вот мне интересно бы ли проблем с получением данных если они были по каким-то условиям с сортировка именно из-за того что партиции по хш устроились У спасибо а напоминаю Да что весь профиль нагрузки то есть и получение цен и получение остатков они всегда происходят у нас в рамках конкретного ID А по факту Когда мы уже запаролить по хешу от ST ID у нас уже работает некий такой индекс Да при выборе партиции который достаточно быстро работает а далее внутри него уже есть какие-то индексы по работе конкретный Store ID Good ID собственно а специфика работы в том что мы отдаём конкретные цены и конкретные остатки вот в разрезе ST ID Good ID сортировки цен у нас нету полнотекстового поиска у нас нету и чтение было Вот всегда по так сказать предпо готов данным которые удобненько лежат мы их достали очень быстро и отдали Здравствуйте спасибо за доклад меня Александр зовут Я из X5 DIR и мы очень похожи задачу просто решаем поэтому очень интересно А вопрос такой вот когда вы ценны остатки получаете соответственно вот из магазинов Да так как вы полную выгрузку получаете то Вам наверное нужно сверять их Да с тем что в базе лежит чтобы не задалбываю там не В тупую апдейт туда какие-то там insert он апдейт там генерить там А ну как бы доставать из базы по магазину всё то что там есть сверять это э на основе этого там инсерты апдейты какие-то генерить вот Ну вы так делаете вот вопрос и Если да то там по цпу это наверное загружено то есть для вот этих вот сервисов которые цено остатки обрабатывают Ну сколько они у вас там железа жрут сколько там инстан сов у вас там вот на Go который это обрабатывает О спасибо за вопрос очень классный вопрос да то есть мы действительно в докладе не сказал мы действительно перед тем как сохранять базу мы чекаем Ну понятное дело in у себя и делаем Это мы с помощью простых запросов в пагр но с маленьким хаком как бы возможно не очень красиво мы делаем это распределённое и во все словы то есть мы считаем что у нас этот реплика лак достаточно низкий мы отправляем равномерно запрос на получение актуального остатка пастору в Мастер и в словы и далее сравниваем в коде и обсер только то что изменилось на самом деле но в случае если вдруг реплика отстала да то у нас всё равно на защите работает insert on Conflict до update Вот это наверное на первую часть вопроса А вторая была по железу сейчас на самых нагруженных сервисах работает один мастер два слова как я сказал 32 ядра оперативы и ой 32 ядра и 64 оператива я понял да инстанс уровня приложения да то есть коллеги дебаты и дискуссию мы можем продолжить чуть-чуть поже ко отвечу коротко да да спасибо 24 инстанса где-то по одному или полутора гигабайтам оперативы и по одному ядру наверное вот так Андрей Наумов ядро Спасибо за доклад Дмитрий у меня такой вопрос что вроде как изначально Проблема была то что у нас не консист То есть клиент видит что допустим у нас вроде есть какой-то остаток заказывает оно не приходит это влияет собственно на корзину но тут и там в решении мы жертвуем консистентность и в кафке в реплике и в том что у нас в паре асинхронное взаимодействие асинхронная репликация как бы вот такой вот какой-то поит мой Да спасибо кто же хороший вопро консистентность да На каждом из которых мы можем что-то потерять Но на самом деле как оказалось с точки зрения бизнеса это не самая главная проблема то есть с точки зрения бизнеса самая главная проблема - это проблема офлайна то есть кто-то положил товар в корзину или кто-то украл его то есть на базе магазина Он числится на нашей базе тоже числится в реальности его нет но если Отвечая прямо на вопрос про консистентность А мы не теряем сообщения и ну единицы Да какие-то и если происходит какая какое-то запаздывание Да мы реализуем это это упущение с помощью бизнес логики то есть мы там осознанно уменьшаем какое-то небольшое количество остатка например да В случае остатков там на определённое значение с точки зрения Ну каких-то бизнес правил вот возможно я не ответил на вопрос можно ещё раз чуть-чуть попробовать задать или в кулуарах да Меня зовут Никита соответственно Мы в здравсити решаем очень похожую проблему и хотелось бы спросить почему изначально не писать бы цены остатки именно в кликхаус вместо по греса на merch 3 и оттуда соответственно читать актуальные данные Угу спасибо Никита мы как как ты наверное видел мы пишем в кха но пишем только на аналитику Почему Потому что транзакционная поса она выше а нам нужно быть консистентные всё-таки то есть в случае с остатками нам нужно быть менее консистентные в случае с ценами более консистентные то есть лиха поддерживает меньшую консистентность чем Паг Спасибо так если у вас остались какие-то вопросы-ответы сейчас будет отличная возможность встретиться со спикером в кулуарах А сейчас приятный этап спикер выбирает аж не один не два а три лучших вопроса так первый вопрос хороший был про дельты Почему мы обслуживаем полный слепок второй вопрос был про делаете ли вы Селект вот молодой человек из X5 по-моему Да делаете ли вы селекты в это тоже очень актуальный вопрос и про консистентность вот спрашивал молодой человек да вот подходите подходите сцена не кусается Мы тоже всё спикер подожди пожалуйста сейчас подарочки вручим так А где подарки подарку у тебя и один подарок туда вот два два подарка Угу Да санта-клаус приехал пораньше СБО спасибо спасибо за доклад спасибо спасибо Угу спасибо знаком красиво да спасибо Так ну а теперь ещё более этапный этап это подарок Лично тебе за отличный доклад Угу спасибо всем спасибо Приходи к нам ещё мы будем очень и очень рады тебя видеть"
}